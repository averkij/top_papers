{
    "paper_title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
    "authors": [
        "Zizhao Tong",
        "Di Chen",
        "Sicheng Hu",
        "Hongwei Fan",
        "Liliang Chen",
        "Guanghui Ren",
        "Hao Tang",
        "Hao Dong",
        "Ling Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. FIDELITY-AWARE DATA COMPOSITION FOR ROBUST ROBOT GENERALIZATION Zizhao Tong1,5 Di Chen3 Sicheng Hu5 Hongwei Fan2,5 Liliang Chen3 Guanghui Ren3 Hao Tang4 Hao Dong2,5 Ling Shao1 1UCAS-Terminus AI Lab, University of Chinese Academy of Sciences School of Computer Science, Peking University of Multimedia Information Processing, School of Computer Science, Peking University 5PKU-Agibot Lab 4State Key Laboratory 3Agibot 2CFCS, 5 2 0 2 9 2 ] . [ 1 7 9 7 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is common approach to introduce diversity, it presents subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), framework that treats data composition as an optimization problem. CIFT uses practical proxy for Information Fidelity based on the feature-space geometry of dataset. This enables the identification of phase transition, termed the Decoherence Point, where training stability degrades. The framework includes generative engine, Multi-View Video Augmentation (MVAug), to synthesize causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as Ï€0 and Diffusion Policy improves OOD success rates by over 54%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots."
        },
        {
            "title": "INTRODUCTION",
            "content": "Training large-scale, data-driven generalist policies is central approach in modern robotics. VisionLanguage-Action (VLA) models are prominent example, which demonstrate the capacity for performing tasks in unstructured environments (Brohan et al., 2023; Black et al., 2025; Firoozi et al., 2025; ONeill et al., 2024). The premise is that broad capabilities emerge when models learn statistical patterns from datasets that have high fidelity to the real worlds causal structure. However, this premise is often not met in practice. The significant cost and complexity of acquiring comprehensive real-world data lead to training sets with inherent statistical biases, for example, limited backgrounds, textures, and lighting. These biases can foster low-fidelity statistical cues, such as spurious correlations between an action and background texture. This divergence between the correlations in the training data and the true causal relationships of task creates data-fidelity gap. This gap can drive policies toward shortcut learning (Geirhos et al., 2020), where they exploit these low-fidelity, spurious cues over more predictive (Ribeiro et al., 2016; Beery et al., 2018), core causal ones (Singla & Feizi, 2022; Hermann et al., 2024). The result is policies that generalize poorly and are prone to exhibit failures on specific subgroups of data where learned shortcuts become invalid, known challenge for out-of-distribution (OOD) generalization (Sagawa et al., 2020). common strategy for the data-fidelity gap is to use generative models to create synthetic augmentations (Bowles et al., 2018). The goal is to increase visual diversity (e.g., by changing backgrounds or textures) to prevent policies from relying on spurious correlations. However, unprincipled data *Indicates equal contribution. Correspondence to: Hao Dong (hao.dong@pku.edu.cn) and Ling Shao (ling.shao@ieee.org). 1 Preprint. Under review. Figure 1: The CIFT framework pipeline. Given small seed dataset, our generative engine, MVAug, synthesizes large pool of augmented data. CIFT then analyzes this pool to select suitable data mixture that maintains information fidelity. The resulting curated dataset is used to train robust policy that generalizes to novel environments. mixing can be counterproductive (Cubuk et al., 2019); it presents trade-off where the diversity from synthetic data can come at the cost of the information fidelity of real demonstrations. (De Haan et al., 2019; Park et al., 2021) An excessive amount can dilute the original learning signal, leading to unstable training or decline in performance. The central challenge is therefore not just the synthesis of varied data, but the principled composition of the final training dataset (Bansal et al., 2024). This work proposes method for systematic data composition, as overviewed in Figure 1. The proposed framework integrates generative engine, Multi-View Video Augmentation (MVAug), with composition algorithm, Coherent Information Fidelity Tuning (CIFT). CIFT determines mixing ratio by analyzing learning dynamics, with the objective of improving generalization while maintaining performance on the original task distribution. Our main contributions are: 1. Multi-View Video Augmentation (MVAug): video-to-video augmentation engine for synthesizing multi-view consistent, causally disentangled robotic demonstrations. 2. Coherent Information Fidelity Tuning (CIFT): data composition framework guided by proposed metric, Information Fidelity, to optimize the data mixing ratio and ensure training stability. 3. Extensive empirical validation: demonstration that CIFT improves the OOD success rate of widely-used policies by over 54% by mitigating shortcut learning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Generalist Robot Policies. Robotics research increasingly centers on training high-capacity, generalist policies on large-scale datasets (Reed et al., 2022; Walke et al., 2023; ONeill et al., 2024). This approach has led to the development of various architectures, from transformers (Zitkovich et al., 2023; Brohan et al., 2023; Driess et al., 2023) to vision-language models (Kim et al., 2024). The performance of this paradigm, however, is often constrained by data acquisition. The significant cost and complexity of collecting diverse real-world data can result in training sets that are visually homogeneous, characteristic linked to the fragmentation of aggregated datasets (Dasari et al., 2020; Xing et al., 2025). This can create data-fidelity gap, where the training distribution does not fully capture the causal structure of real-world environments (Chebotar et al., 2019). This gap is contributing factor to poor out-of-distribution (OOD) generalization, especially on coherent data subgroups where spurious correlations fail (Sagawa et al., 2020). Shortcut Learning in Robotics. Shortcut learning is primary consequence of the data-fidelity gap, where models adopt decision rules that perform well on standard benchmarks but show poor generalization to new environments (Geirhos et al., 2020; Ye et al., 2024). Policies trained on biased data may learn to exploit spurious features (Baker et al., 2019; Izmailov et al., 2022; Singla & Feizi, 2022), such as background textures that are predictive in the training set (Xiao et al., 2021; 2 Preprint. Under review. Luo et al., 2021; Tobin et al., 2017). Such features are often learned because they are highly available, meaning they are easy for model to extract. This reliance on spurious correlations is known characteristic of deep nonlinear models, which can prioritize feature availability over causal predictivity (Hermann et al., 2024). Applying certain training paradigms, such as distributionally robust optimization (DRO), may be insufficient without careful regularization (Sagawa et al., 2020), and some methods like adversarial or contrastive training may even increase background sensitivity (Moayeri et al., 2022). This issue is particularly relevant in robotics, where dataset fragmentation can foster the learning of shortcuts (Xing et al., 2025), presenting barrier to deployment. Data Augmentation for Generalization. To address the challenges of data scarcity and shortcut learning, data augmentation has become widely used strategy. Recent work has advanced data synthesis for creating varied robotic demonstrations. This includes methods for background randomization (Chen et al., 2023; Teoh et al., 2024; Yuan et al., 2025), semantically conditioned modifications (Chen et al., 2024), video-to-video translation (Agarwal et al., 2025; Liu et al., 2025), and object-aware debiasing (Mo et al., 2021). This progress in synthesis, however, highlights the challenge of principled data composition. The literature often relies on ad-hoc heuristics, and lacks formal methodology for navigating the trade-off between visual diversity and information fidelity. Our work addresses this challenge by formalizing the principled integration of synthetic data."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "This work addresses shortcut learning, where policies exploit spurious correlations in training data instead of learning generalizable causal relationships. To ground our solution, we adopt causal framework, detailed in Appendix B, to formalize this failure mechanism and the corresponding debiasing task. 3.1 SHORTCUT LEARNING AS CAUSAL MODEL MISSPECIFICATION To formalize shortcut learning, we model an observation as composite of core causal feature and shortcut feature (see Definition B.1 for details). For instance, in robotic picking task, could represent the objects geometry and pose, while might be background texture consistently paired with the object in the training data. Shortcut learning arises when is easier for model to learn (highly available) than u, even if is more predictive of the correct action (Hermann et al., 2024), and spurious correlation exists between and in the training data (Assumption B.1). An ideal policy, Ï€, should achieve causal invariance by basing its action solely on the causal feature (Arjovsky et al., 2019; Pearl, 2009). This is formally expressed as conditional independence from (Definition B.2): (au, v) = (au). (1) policy exhibiting shortcut learning fails to achieve this invariance (Bengio et al., 2013), instead developing dependency on (Definition B.3). 3.2 DEBIASING AS CONSTRAINED OPTIMIZATION This causal model allows for precise definition of generalization settings. The in-distribution (ID) setting mirrors the training statistics, where the spurious correlation between and holds. The out-of-distribution (OOD) setting comprises environments where this correlation is broken (e.g., appears with novel v) (Koh et al., 2021). The debiasing strategy is to train the policy on composed data distribution, Pfinal, which is convex combination of real data (Preal) and synthetic data (Psynth) controlled by mixing ratio Î» [0, 1] (see Definition B.4) (Zhang et al., 2018a). This strategys objective is to find an optimal mixing ratio Î» that navigates the Diversity-Information Fidelity trade-off (Definition B.5) (Tsipras et al., 2019). This goal is formalized as the following constrained optimization problem (Problem B.1): Î» = arg max Î»[0,1] POOD(Ï€Î¸(Î»)) s.t. PID(Ï€Î¸(Î»)) PID(Ï€Î¸(0)) Ïµ, (2) 3 Preprint. Under review. Figure 2: An overview of the MVAug architecture, latent diffusion transformer for multi-view video synthesis. The model generates new multi-view video conditioned on three inputs: the original multi-view footage, an edited initial frame from primary viewpoint, and guiding structural prior. key component, the Periodic Cross-View Attention mechanism (detailed in Section 4.1), ensures the resulting video is fully consistent across all viewpoints. where Ï€Î¸(Î») is the policy optimized on the data mixture defined by Î». Directly solving Equation 2 is often intractable, as it requires evaluating performance on the true OOD distribution during training. This motivates the need for practical proxy to guide the selection of Î», which our work provides."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Our methodology addresses the optimization problem in Eq. 2 with two-stage framework called Coherent Information Fidelity Tuning (CIFT). First, generative model synthesizes candidate pool of diverse data. Second, composition strategy selects data mixture from this pool, guided by proxy for information fidelity. 4.1 GENERATIVE DISENTANGLEMENT VIA MULTI-VIEW AUGMENTATION The generative engine of CIFT is Multi-View Video Augmentation (MVAug), latent diffusion transformer tasked with synthesizing controllable spectrum of causally disentangled training data from multi-view robot demonstrations (Rombach et al., 2022; Peebles & Xie, 2023). As illustrated in Figure 2, MVAug processes tokenized video chunks from multiple camera perspectives. The models controllability stems from its conditioning mechanism. The generation process is guided by structural prior, provided as Canny edge map from the source video (Canny, 1986; Zhang et al., 2023), to maintain motion fidelity. To introduce novel visual contexts, it is also conditioned on an appearance prior. This prior is an edited image generated by the first-frame editing model FLUX.1-Kontext-dev (Labs et al., 2025), which we adopt for first-frame editing based on textual prompts such as new backgrounds or lighting conditions (Figure 3) (Molad et al., 2023). key architectural feature of MVAug is its handling of multi-view data. To ensure the generated video is coherent across different camera perspectives, we introduce periodic cross-view attention mechanism. This strategy modulates the behavior of the transformers self-attention layers. Most layers perform intra-view self-attention, processing the features for each view independently. However, at periodic intervals, the model executes global cross-view self-attention operation, where tokens from all views are jointly processed. This periodic information fusion allows the model to build globally consistent representation while managing computational complexity (Kitaev et al., 2020). The model is trained with standard denoising diffusion objective (Ho et al., 2020): L(Ï•) = Ez0,ÏµN (0,I),t,ccond (cid:2)Ïµ ÏµÏ•(zt, t, ccond)2(cid:3) , (3) 4 Preprint. Under review. where zt is the noised latent and ccond comprises all conditioning inputs. Detailed architectural specifications, pseudo-code for the attention mechanism, and training hyperparameters are provided in Appendix A. Qualitative examples of videos synthesized by MVAug are presented in Appendix C.6. Figure 3: Generation of the appearance prior for MVAug. For multi-view video input, we only edit the first frame from the primary camera view (e.g., the head camera). Given this source frame (left), we use the image-edit model FLUX.1-Kontext-dev (Labs et al., 2025) to generate an edited version according to various textual prompts (e.g., dusk, cinematic). This single edited frame serves as the global appearance prior, guiding the MVAug engine to synthesize consistent visual style across all camera views for the entire video sequence. 4.2 PRINCIPLED COMPOSITION VIA INFORMATION FIDELITY The MVAug engine generates large pool of synthetic demonstrations, denoted Dsynth. The subsequent challenge is to determine how to best compose this synthetic data with the original real dataset, Dreal. simple mixture of the two can be suboptimal. The CIFT framework provides method to determine suitable mixing ratio, Î», for this composition. Our composition strategy is guided by the concept of Information Fidelity, defined as the constructive alignment of learning signals between real and synthetic data. As direct gradient-based measurement is intractable, CIFT uses practical proxy based on the feature-space geometry of the combined dataset (Heusel et al., 2017). This proxy, which we term the Feature-Space Signal-toNoise Ratio (SNR), is formally defined as follows. Let Dfinal(Î») be the composed dataset for given mixing ratio Î», and let FÎ» = {f (x)x Dfinal(Î»)} be the set of feature vectors extracted by pre-trained model () (see Appendix C.3 for backbone analysis). Let w1 be the first principal component of the covariance matrix of FÎ» (Jolliffe & Cadima, 2016; Balakrishnan et al., 2018). The Feature-Space SNR is then: SNR(Î») = Ef FÎ»[f w1] (cid:112)Varf FÎ»[f w1] . (4) The full calculation protocol for this metric is detailed in Appendix A.1. We use this SNR as our proxy for Information Fidelity. Analysis shows non-monotonic relationship between the mixing ratio Î» and the SNR. As Î» increases, the SNR may reach peak and then decline. We term the critical phase transition the Decoherence Point, Î»dc, which we define as the mixing ratio at which the Feature-Space SNR reaches local minimum, indicating collapse in feature coherence (Tishby & Zaslavsky, 2015; Alemi et al., 2017). The CIFT procedure simplifies the optimization problem in Eq. 2 by finding the ratio Î» that maximizes this data curation proxy, 5 Preprint. Under review. (a) Physical robot setup for on-robot evaluations. (b) Robustness Score (RS) vs. mixing ratio for different augmentation methods. Figure 4: Experimental platform and ablation study results. (a) The physical dual-arm setup used for all on-robot, closed-loop evaluations. (b) The non-linear relationship between the data mixing ratio and the policys Robustness Score (RS) across different augmentation methods. while operating within this coherent regime: Î» = arg max Î»[0,Î»dc) SNR(Î»). (5) The ratio Î» is then used to compose the final training dataset."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Our experiments validate the CIFT framework across three axes. We first confirm that our SNR proxy predicts open-loop policy stability, then conduct ablation studies on data synthesis and composition, and finally evaluate the end-to-end frameworks effectiveness on physical robotic tasks. 5.1 EXPERIMENTAL SETUPS Tasks and Platforms. Our experiments utilize range of robotic tasks and platforms. We conduct the open-loop stability analysis on the dual-arm cloth folding task (Ross et al., 2011; Raval et al., 2024). This task was selected specifically because it requires the policy to generate actions across the full 14-dimensional output space of the Ï€0 Policy (Black et al., 2024), providing comprehensive basis for evaluating prediction stability. For generative model comparisons, we use the Agibot World dataset (Bu et al., 2025). For on-robot closed-loop evaluations on our physical dual-arm setup (Figure 4(a)), we selected two tasks to represent distinct manipulation challenges: picking up toy (a representative single-arm task) and folding clothes (a complex dual-arm task). Baselines. Our evaluation is structured around two sets of comparisons. For the ablation and stability studies, we compare different data augmentation strategies against non-generative and generative baselines (Chen et al., 2020). For the end-to-end on-robot evaluations, we compare baseline policies, Ï€0 (Black et al., 2024) and Diffusion Policy (Chi et al., 2023), trained on real data only against CIFT-trained counterparts. The performance for this evaluation is measured by Task Success Rate under both in-distribution (ID) and out-of-distribution (OOD) conditions (Koh et al., 2021). Evaluation Metrics. We evaluate performance on two fronts: generative model quality and downstream policy performance. Generative quality is assessed using standard metrics; detailed definitions are provided in Appendix C.1. For policy evaluation, we use two metrics. Open-loop stability is quantified by our proposed Robustness Score (RS). This score is based on the Mean Squared Error (MSE) between policys predicted action trajectory and the ground-truth robot actions, evaluated on held-out ID and OOD video observations. detailed description of the open-loop evaluation protocol, the RS formulation, and the corresponding results is provided in Appendix C.2. The score is calculated as: (cid:18) (cid:18) RS(Î») = max 0, 1 (cid:19)(cid:19) MSEOOD(Î») MSEOOD(0) 100 (cid:18) MSEID(0) MSEID(Î») (cid:19) . (6) Preprint. Under review. (a) Robustness Score (RS) and Feature SNR as function of the data mixing ratio. The left axis corresponds to the RS of the trained policy. The right axis corresponds to the SNR of the datasets features prior to training. Shaded regions denote different data composition phases. (b) The evolution of the feature distributions first and second moments. The y-axis shows the standard deviation (Ïƒ, noise) of the features first principal component. The size and color of each data point correspond to the magnitude of the mean (Âµ, signal). Each point represents different data mixing ratio. Figure 5: Experimental validation of the Feature SNR proxy. (a) The relationship between the pretraining SNR and post-training policy robustness (RS). (b) The underlying changes in the feature distributions mean (Âµ, signal) and standard deviation (Ïƒ, noise). Closed-loop on-robot performance is measured by the Task Success Rate. Implementation Details. The open-loop stability analysis and primary on-robot evaluations are based on the full fine-tuning of Ï€0 foundation model (Black et al., 2024). For each data configuration, training this model on our dataset of 200 real-world, multi-view video episodes, each with an average of 2000 frames at 30 FPS, required approximately 50 hours on 8 NVIDIA H100 GPUs. To validate that our CIFT framework is model-agnostic, we also performed an on-robot validation using three-view Diffusion Policy. For this, we trained baseline policy on real data only and corresponding policy using the CIFT-composed dataset, with each training run for the Diffusion Policy requiring approximately 80 hours on 16 H100 GPUs. All policy inference for both the open-loop analysis and the on-robot evaluations was conducted on workstation equipped with an NVIDIA RTX 4090 GPU. 5.2 VALIDATING SNR AS PREDICTOR OF OPEN-LOOP STABILITY The central hypothesis of this work is that static, pre-training analysis of datasets feature geometry can predict the open-loop stability of policy subsequently trained on that data. This section validates this hypothesis by demonstrating that our Feature-Space Signal-to-Noise Ratio (SNR) serves as an effective proxy for the post-training Robustness Score (RS). The concept of representing distributions quality via the ratio of its mean (signal) to its standard deviation (noise) is foundational principle in information theory (Cover, 1999). Our motivation for employing this proxy is further detailed in the open-loop analysis in Appendix C.2, which shows that naively increasing synthetic data leads to non-linear stability response. Analysis of Correlation and Feature Dynamics. We test our hypothesis using the open-loop evaluation protocol detailed in Section 5.1 and Appendix C.2. The complete quantitative results are presented in Table 1, while Figure 5 provides visual analysis of the relationship between the pre-training SNR and the post-training RS. As shown in Figure 5(a), the SNR (dashed line) peaks at 100:100 mixing ratio, preceding the peak of the final RS (solid line) at 100:200. The sharp decline in SNR at the 100:300 ratio serves as leading indicator of the corresponding collapse in policy stability, validating its utility for identifying the decoherence point. Figure 5(b) provides mechanistic explanation for this phenomenon by visualizing the underlying feature dynamics. It illustrates that the decoherence point corresponds to geometric shift where the 7 Preprint. Under review. feature signal (Âµ, bubble size and color) collapses while the noise (Ïƒ, y-axis) increases. This collapse in open-loop stability is qualitatively visualized in Figure 6, which contrasts smooth trajectory from the CIFT-selected data mix (b) with catastrophic failure from the decoherence point (c). (a) Original Point (b) Optimal Point Figure 6: Qualitative visualization of open-loop rollouts. The trajectory generated at the CIFTselected optimal point (b) is smooth and accurate, whereas the trajectory at the decoherence point (c) exhibits catastrophic failure. (c) Decoherence Point Mixing Ratio (Real:Synth) Feature-Space SNR (Âµ/Ïƒ) OOD MSE ID MSE Robustness Score (RS) 100:0 (Baseline) 100:100 (CIFTs Choice) 100:200 (Peak RS) 100:300 (Decoherence Point) 100:400 100:500 0.1423 0.2171 0.1644 0.0097 0.0588 0.1448 0.0700 0.0010 0.0010 0.0242 0.0015 0.0018 0.0021 0.0036 0.0034 0.0037 0.0037 0.0048 0.00 56.37 60.29 36.56 53.91 41. Table 1: Quantitative validation of the SNR proxy. The data shows positive correlation between the pre-tuning Feature-Space SNR and the final post-training policy stability (RS). The 1:3 ratio corresponds to drop in both metrics. 5.3 ABLATION STUDIES We conduct ablation studies to analyze the contributions of data synthesis quality and the composition strategy (Table 2, Figure 4(b)). full report on our generative model, including detailed ablation results  (Table 7)  , discussion of quantitative metrics, and human evaluation study, is provided in Appendix C.4. Effect of Synthesis Quality. The results first show the effect of synthesis quality on augmentation. As shown in Table 2, MVAug obtains more favorable scores on generative quality metrics such as FVD compared to the baselines. This is achieved with significant computational efficiency, as detailed in our performance analysis in Appendix C.5. This improvement in synthesis quality corresponds to higher open-loop stability, as illustrated by the peak Robustness Score (RS) values in the line chart in Figure 4(b). The chart shows that augmentations from MVAug achieve higher peak RS (60.29) than the baselines (RoboEngine: 35.2, RoboTransfer: 22.4, and BG Replace: 14.8). Effect of the Composition Strategy. The results also show the effect of the composition strategy. As illustrated in the chart in Figure 4(b), for all augmentation methods, the RS exhibits non-linear dependence on the mixing ratio. The score initially improves with the addition of synthetic data but then degrades after certain point. For instance, the curve for MVAug peaks at 1:2 ratio (60.29) and drops at 1:3 ratio (36.56). This non-linear response suggests that increasing the quantity of synthetic data does not guarantee improved performance and motivates the need for data-driven method to identify suitable mixing ratio. 5.4 ON-ROBOT GENERALIZATION PERFORMANCE This section evaluates the end-to-end, closed-loop performance of our CIFT-trained policies against baselines on physical robotic platform. Evaluation Protocol. We evaluated each trained policy under two distinct sets of conditions. Indistribution (ID) evaluations were conducted in environments visually congruent with the original real-world data collection setup. Out-of-distribution (OOD) evaluations introduced visual shifts 8 Preprint. Under review. Method Realism View Consistency Temporal Coherence Text Align. FVD FID CVFC MVDC Ewarp T-LPIPS TCJ CLIP Score RoboEngine RoboTransfer MVAug (Ours) 1463.49 2854.5 545.7 221.5 323.5 104.6 0.7658 0.8278 0.8023 0.6001 0.3960 0.6318 212.5 9.2 3.7 652.3 242.1 10. 3.713 1.649 0.218 22.42 21.07 22.89 Table 2: Quantitative comparison of generative model quality. Detailed metric definitions are in Appendix C.1; our proposed CVFC metric is computed using CLIP features (Radford et al., 2021). Citations for other metrics include: FVD (Unterthiner et al., 2018), FID (Heusel et al., 2017), MVDC (Ranftl et al., 2020), Ewarp (Lai et al., 2018), T-LPIPS (Chu et al., 2020), TCJ (HuynhThu & Ghanbari, 2006), and CLIP Score (Hessel et al., 2021). Arrows indicate whether higher () or lower () scores are better. Architecture Task Method ID Success OOD Success (%) (%) Lighting Distractors Background Texture Ï€0 (Black et al., 2024) Picking up toy Folding clothes Diffusion Policy (Chi et al., 2023) Picking up toy w/o CIFT w/ CIFT w/o CIFT w/ CIFT w/o CIFT w/ CIFT 40 70 60 80 55 70 35 70 50 80 0 30 65 45 75 0 70 5 85 10 85 0 5 85 10 85 0 85 Table 3: On-robot generalization performance. Success rates (%) are averaged over 20 trials, comparing baseline policies (w/o CIFT) with those trained using our framework (w/ CIFT). across four axes: lighting variations, the presence of novel object distractors, changes to the scene background, and novel table textures. Results and Analysis. The on-robot performance for both the Ï€0 and Diffusion Policy architectures is summarized in Table 3. The results reveal consistent trend: policies trained solely on the original real data exhibit significant performance degradation under OOD conditions, particularly when faced with semantic shifts such as novel backgrounds and textures. For example, the baseline Diffusion Policys success rate on the toy picking task plummets from 55% (11/20) in the ID setting to 0% when encountering both background and texture shifts. In contrast, policies trained with data composed via the CIFT framework demonstrate substantially improved OOD robustness across all tested tasks and architectures. The CIFT-trained Diffusion Policy, for instance, achieves an 85% (17/20) success rate under the same challenging semantic shift conditions that caused the baseline to fail completely. This result indicates that the benefits of our data composition framework are not specific to single model architecture, but rather provide more general mechanism for enhancing robustness. Qualitative visualizations illustrating this improved performance are provided in Appendix C.7."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work frames shortcut learning in robotics as problem of principled data composition, rather than one of synthesis alone. We introduce Coherent Information Fidelity Tuning (CIFT), framework that identifies Decoherence Point, predictable phase transition where naively increasing data diversity degrades the stability of policy training. The framework leverages computationally tractable feature-space proxy to identify this transition during the data curation phase, enabling the principled mitigation of shortcut learning and improving the out-of-distribution robustness of learned policies. The approach is constrained by the fidelity of the underlying generative model. Artifacts and physically implausible dynamics can introduce new spurious correlations, and the computational cost of large-scale video synthesis remains practical concern. further limitation is the temporal coherence of current models over long horizons. However, this limitation aligns with the current paradigm 9 Preprint. Under review. in robot learning, where foundation models like Visual Language-Action (VLA) models are trained on large corpora of short video clips. primary direction for future work is to scale the CIFT methodology to augment and debias the large-scale, heterogeneous datasets used for pre-training foundation models, offering principled approach to addressing inherent dataset biases at their source. Other avenues include the development of online adaptation, where an agent synthesizes CIFT-tuned dataset upon deployment to new environment, and interactive, goal-conditioned synthesis to enable self-correcting training paradigms. Finally, extending the composition principle to other sensory modalities, such as synthesizing plausible tactile data to accompany visual augmentations, could lead to the development of more robust, multi-modal agents."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. In International Conference on Learning Representations (ICLR), 2017. Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations (ICLR), 2019. Guha Balakrishnan, Amy Zhao, Mert Sabuncu, John Guttag, and Adrian Dalca. An unsupervised learning model for deformable medical image registration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. LLM augmented LLMs: Expanding capabilities through composition. In International Conference on Learning Representations (ICLR), 2024. Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), 2018. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):17981828, 2013. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. Ï€0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Galliker, et al. Ï€0.5: vision-languageaction model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Hammers, David Alexander Dickie, Maria Valdes Hernandez, Joanna Wardlaw, and Daniel Rueckert. Gan augmentation: Augmenting training data using generative adversarial networks. arXiv preprint arXiv:1810.10863, 2018. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics In Proceedings of Robotics: Science and Systems transformer for real-world control at scale. (RSS), 2023. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 10 Preprint. Under review. John Canny. computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8(6):679698, 1986. doi: 10.1109/TPAMI.1986.4767851. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In IEEE International Conference on Robotics and Automation (ICRA), 2019. Qiuyu Chen, Sho C. Kiami, Abhishek Gupta, and Vikash Kumar. GenAug: Retargeting behaviors to unseen situations via generative augmentation. In Proceedings of Robotics: Science and Systems (RSS), 2023. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML), 2020. Zoey Chen, Zhao Mandi, Homanga Bharadhwaj, Mohit Sharma, Shuran Song, Abhishek Gupta, and Vikash Kumar. Semantically controllable augmentations for generalizable robot learning. The International Journal of Robotics Research, pp. 02783649241273686, 2024. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pp. 02783649241273668, 2023. Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via self-supervision for gan-based video generation. ACM Transactions on Graphics (TOG), 39(4):751, 2020. JJ Collins and CJ De Luca. The effects of visual input on open-loop and closed-loop postural control mechanisms. Experimental Brain Research, 103(1):151163, 1995. Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: In Proceedings of the IEEE/CVF Conference on Learning augmentation strategies from data. Computer Vision and Pattern Recognition (CVPR), 2019. Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. In Conference on Robot Learning (CoRL), 2020. Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2019. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning (ICML), 2023. Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 44(5): 701739, 2025. Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. Katherine Hermann, Hossein Mobahi, Michael Curtis Mozer, et al. On the foundations of shortcut learning. In International Conference on Learning Representations (ICLR), 2024. 11 Preprint. Under review. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems (NIPS), 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NIPS), 2020. Quan Huynh-Thu and Mohammed Ghanbari."
        },
        {
            "title": "Impact of jitter and jerkiness on perceived video",
            "content": "quality. In Proc. Workshop on Video Processing and Quality Metrics, 2006. Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew Wilson. On feature learning in the presence of spurious correlations. In Advances in Neural Information Processing Systems (NIPS), 2022. Ian Jolliffe and Jorge Cadima. Principal component analysis: review and recent developments. Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. In Conference on Robot Learning (CoRL), 2024. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations (ICLR), 2020. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: arXiv preprint Flow matching for in-context image generation and editing in latent space. arXiv:2506.15742, 2025. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In Proceedings of the European Conference on Computer Vision (ECCV), 2018. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow In International Conference on Learning Representations matching for generative modeling. (ICLR), 2023. Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang, and Zhizhong Su. Robotransfer: Geometry-consistent video diffusion for robotic visual policy transfer. arXiv preprint arXiv:2505.23171, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learning of background for few-shot learning. In Advances in Neural Information Processing Systems (NIPS), 2021. Sangwoo Mo, Hyunwoo Kang, Kihyuk Sohn, Chun-Liang Li, and Jinwoo Shin. Object-aware contrastive learning for debiased scene representation. In Advances in Neural Information Processing Systems (NIPS), 2021. Preprint. Under review. Mazda Moayeri, Phillip Pope, Yogesh Balaji, and Soheil Feizi. comprehensive study of image classification model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. Journal of Artificial Intelligence Research, 70:13731411, 2021. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In IEEE International Conference on Robotics and Automation (ICRA), 2024. Vardan Papyan, XY Han, and David Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40): 2465224663, 2020. Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin, Jinwoo Shin, and Tie-Yan Liu. Objectaware regularization for addressing causal confusion in imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2021. Judea Pearl. Causality. Cambridge university press, 2009. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual In International Conference on Machine Learning models from natural language supervision. (ICML), 2021. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):16231637, 2020. Vedant Raval, Enyu Zhao, Hejia Zhang, Stefanos Nikolaidis, and Daniel Seita. Gpt-fabric: Folding and smoothing fabric by leveraging pre-trained foundation models. arXiv e-prints, pp. arXiv 2406, 2024. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. Transactions on Machine Learning Research, 2022. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should trust you? explaining the predictions of any classifier. In Proceedings of the ACM SIGKDD international conference on knowledge discovery & data mining (KDD), pp. 11351144, 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 13 Preprint. Under review. Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations (ICLR), 2020. Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In International Conference on Learning Representations (ICLR), 2022. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Eugene Teoh, Sumit Patidar, Xiao Ma, and Stephen James. Green screen augmentation enables scene generalisation in robotic manipulation. arXiv preprint arXiv:2407.07868, 2024. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pp. 15. Ieee, 2015. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations (ICLR), 2019. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In International Conference on Learning Representations (ICLR), 2021. Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, and Jingkuan Song. Shortcut learning in generalist robot policies: The role of dataset diversity and fragmentation. In Conference on Robot Learning (CoRL), 2025. Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, and Aidong Zhang. Spurious correlations in machine learning: survey. arXiv preprint arXiv:2402.12715, 2024. Chengbo Yuan, Suraj Joshi, Shaoting Zhu, Hang Su, Hang Zhao, and Yang Gao. Roboengine: Plugand-play robot data augmentation with semantic robot segmentation and background generation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025. Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 14 Preprint. Under review. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018b. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning (CoRL), 2023. 15 Preprint. Under review."
        },
        {
            "title": "A MVAUG ARCHITECTURE AND IMPLEMENTATION DETAILS",
            "content": "Base Architecture and Modifications. Our MVAug model adapts the Cosmos-Predict2-2BVideo2World foundation model (Agarwal et al., 2025), 28-layer transformer. We modify its input layer to process multi-modal conditioning scheme: VAE video latents, Canny edge map for structural guidance, and padding mask. To enforce multi-view consistency, we introduce two modifications. First, the periodic cross-view attention mechanism interleaves global cross-view self-attention with standard intra-view self-attention. Specifically, every third transformer block jointly processes tokens from all views to facilitate information exchange. Second, we introduce set of learnable view embeddings, which are fused with the timestep conditioning signal to provide each view with unique identity. The pseudo-code for the attention mechanism is provided in Algorithm 1. Q, K, Linear(X) if (mod ) = 0 then Algorithm 1 Periodic Cross-View Attention 1: procedure PERIODICATTENTION(X, i, ) Require: Per-view hidden states R(BN )LD. Require: Current block index and attention period . 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: end procedure end if return Output Qcat, Kcat, Vcat ReshapeToBatch(Q, K, V) Acat ScaledDotProductAttention(Qcat, Kcat, Vcat) Output ReshapeToViews(Acat) Output ScaledDotProductAttention(Q, K, V) else Global Cross-View Self-Attention Intra-View Self-Attention Training and Inference. We fine-tune all model parameters for 100,000 steps using flowmatching objective (Lipman et al., 2023) and the 8-bit AdamW optimizer (Loshchilov & Hutter, 2019), managed via DeepSpeed ZeRO Stage 2 (Rajbhandari et al., 2020). The model is trained on 30 FPS video segments processed in 25-frame chunks, with each chunk autoregressively conditioned on the four preceding frames. At inference, video generation is performed by numerically integrating the learned probability flow ODE using first-order forward Euler method. The generation is guided by Canny edge map and generic negative text prompt to improve visual quality. Detailed hyperparameters are listed in Table 4. Table 4: Fine-tuning hyperparameters for the MVAug model. Hyperparameter Value Base Model Fine-Tuning Scheme Total Training Steps Learning Rate LR Scheduler LR Warmup Steps Weight Decay Global Batch Size Gradient Accumulation Steps Max Gradient Norm Mixed Precision Optimizer Training Resolution Video Chunk Length Conditional Frames Seed Cosmos-Predict2-2B-Video2World Full Parameter Update 100,000 1e-4 Constant with Warmup 1000 5e-5 4 1 1.0 bf16 8-bit AdamW 384x512 pixels 25 frames 4 42 16 Preprint. Under review. A.1 CIFT: THEORETICAL FOUNDATIONS OF THE SNR PROXY This section details the theoretical and methodological foundations of our Feature-Space Signalto-Noise Ratio (SNR) proxy. We first establish the information-theoretic basis for using SNR to evaluate feature distributions. We then describe the step-by-step protocol for its calculation. Information-Theoretic Foundation. The core principle of CIFT is to quantify the quality of composed dataset by analyzing its geometry in learned feature space. This approach is grounded in foundational concepts from information theory and signal processing, which model information as combination of deterministic signal and random noise (Shannon, 1948; Cover, 1999). We adapt this principle to evaluate the quality of feature representation for robotic task. An ideal feature representation should be highly sensitive to task-relevant causal factors (e.g., object pose, gripper status), which constitute the signal, while remaining invariant to task-irrelevant distractors (e.g., lighting, background textures), which constitute the noise. In this context, we can model the distribution of the features primary component as: The Signal (Âµ): The mean of the distribution, representing the consistent, average activation that captures the core essence of the tasks state. strong, non-zero mean indicates that the feature representation is discriminative and consistently identifies task-relevant information. The Noise (Ïƒ): The standard deviation of the distribution, representing the features variability in response to task-irrelevant nuisance variables. low standard deviation suggests that the feature is robust and invariant to visual distractors. Therefore, maximizing the Signal-to-Noise Ratio (SNR), defined as the ratio Âµ/Ïƒ, is equivalent to searching for feature distribution that exhibits high feature coherence: the representation is simultaneously discriminative (high signal) and robust (low noise). The idea of using statistical proxies to evaluate and filter datasets is an active area of research, with related approaches seeking to quantify label noise for data cleaning (Northcutt et al., 2021). SNR Calculation Protocol. The following protocol details the step-by-step procedure for computing the Feature-Space SNR for given data mixture. 1. Feature Extraction and Projection. For each data mixing ratio, we first extract framelevel features from all videos using pre-trained Inception-v3 model. We then apply Principal Component Analysis (PCA) to this collection of feature vectors and project them onto their first principal component. This process reduces the high-dimensional feature space to single dimension that captures the largest variance. The validity of such lowdimensional projection is supported by findings that deep neural networks often learn representations that occupy low-dimensional subspace (Papyan et al., 2020). 2. Statistical Modeling. We fit univariate Gaussian distribution, (Âµ, Ïƒ2), to these onedimensional projections. The assumption of normality is justified by the Central Limit Theorem, which suggests that the aggregation of numerous underlying factors (as captured in deep feature vector) will tend towards normal distribution upon projection. 3. SNR Computation. We compute the mean Âµ and standard deviation Ïƒ of the fitted Gaussian distribution. The Feature-Space SNR is then calculated as the ratio Âµ/Ïƒ. The complete statistical results for this analysis across two different tasks are presented in Table 6. The strong empirical correlation between this pre-training, information-theoretic proxy metric and the post-training open-loop performance (as shown in Section 5.2) forms the basis of the CIFT framework. It allows us to select an optimal data composition by maximizing for feature coherence before undertaking the computationally expensive process of full policy training."
        },
        {
            "title": "B THEORETICAL FOUNDATIONS AND PROOFS",
            "content": "This appendix provides mathematical formulation for the problem addressed in this paper. First, we establish causal model to define shortcut learning. Second, we provide proofs analyzing the 17 Preprint. Under review. sources of data and model bias that can lead to this phenomenon. Finally, we frame the debiasing task as constrained optimization problem and provide the theoretical motivation for our proposed solution. B.1 CAUSAL MODEL OF SHORTCUT LEARNING Our analysis begins with causal model of the data generation process. Definition B.1 (Core and Shortcut Features). Following prior work (Hermann et al., 2024; Xing et al., 2025), we model an observation as being generated from two latent features: core feature and shortcut feature v. The core feature represents the set of causal factors necessary for the task, characterized by high predictivity of the optimal action. The shortcut feature represents set of non-causal factors that are spuriously correlated with the core feature in the training data. This feature is often characterized by high availability, meaning it is easily extracted by the model architecture. Given these features, an ideal policy would depend only on causal information. Definition B.2 (Ideal Causal Policy). An ideal, robust policy Ï€ is invariant to the shortcut feature and bases its actions solely on the core feature u. This causal invariance is expressed as: Ï€(ax) = (au, v) = (au) (7) Shortcut learning arises when specific conditions in the data and the model are met. Assumption B.1 (The Shortcut Condition). Shortcut learning can occur when the training dataset, Dtrain, satisfies the shortcut condition, which comprises two components: 1. Spurious Correlation (Data Bias). The core and shortcut features are spuriously correlated in the data distribution, i.e., Ptrain(u, v) = Ptrain(u)Ptrain(v). 2. Availability Bias (Model Bias). The shortcut feature is more available to the learning algorithm than the core feature u, due to inductive biases in deep nonlinear models (Hermann et al., 2024). This leads to the formal definition of shortcut learning. Definition B.3 (Shortcut Learning). policy Ï€Î¸ exhibits shortcut learning if, when trained on dataset satisfying Assumption B.1, it learns to depend on the more available shortcut feature v. Formally, the policy violates causal invariance: The degree of this dependence can be quantified by the conditional mutual information IÏ€Î¸ (a; vu) > 0. The objective of debiasing is to learn policy that minimizes this quantity. PÎ¸(au, v) = PÎ¸(au) (8) B.2 ANALYSIS OF SPURIOUS CORRELATION FROM DATA STRUCTURE We now provide formal basis for the Spurious Correlation component of the Shortcut Condition (Assumption B.1), adapting the framework from (Xing et al., 2025). We consider dataset composed of uniform mixture of sub-datasets, {D1, . . . , Dm}. Within any sub-dataset Di, the core and shortcut features are assumed to be independent, pi(u, v) = pui(u)pvi(v). The correlation thus arises from the mixing process. Proposition B.1 (Spurious Correlation from Low Diversity). Given two sub-datasets, D1 and D2, with disjoint feature supports, the normalized mutual information between and is inversely related to the total intra-dataset diversity: I(u, v) = 2I(u, v) H(u) + H(v) = 4 Cdiversity + 4 (9) where Cdiversity (cid:80) i{1,2}(H(ui) + H(vi)) is the sum of entropies. 18 Preprint. Under review. Proof. The entropy of the mixture distribution p(u) = 1 2 [pu1(u) + pu2(u)] with disjoint supports is H(u) = 1 2 (H(u1) + H(u2)) + 1 (using base-2 logarithms). similar expression holds for H(v). The mutual information I(u, v) = 1 because observing either feature uniquely determines the subdataset of origin. Substituting these into the definition of normalized mutual information yields the result, showing that lower intra-dataset diversity (a smaller Cdiversity) leads to higher degree of spurious correlation. Proposition B.2 (Mitigation via Data Overlap). As the degree of feature overlap between subdatasets (Cinterleave) increases, the upper bound on the spurious correlation tightens towards zero. Proof. The proof involves establishing lower bound for the total entropy and an upper bound for the mutual information, both as functions of the overlap quantity Cinterleave. Combining these bounds yields the result I(u, v) 1 , which shows that increasing overlap reduces the maximum possible spurious correlation. Cdiversity Cdiversity+4Cinterleave B.3 ANALYSIS OF AVAILABILITY BIAS IN LEARNING DYNAMICS We now provide mechanism for the Availability Bias component of the Shortcut Condition (Assumption B.1). Proposition B.3 (Disparity-Induced Learning Bias). In linear model trained with gradient descent, the initial learning dynamics are biased towards the feature with greater variance across the mixed dataset. Since inter-dataset disparity contributes to this variance, model can learn to depend on feature with high disparity, even if it is non-causal. Proof. Consider linear policy Ï€Î¸(x) = Ï‰T + with MSE loss. At initialization, the gradients are Ï‰u = Cov(y, u) and Ï‰v = Cov(y, v). The variance of feature in mixture of two sub-datasets is Var(u) = 1 4 (Âµ1(u)Âµ2(u))2. The term (Âµ1(u)Âµ2(u))2 is the squared disparity. Higher disparity increases the features total variance, which generally leads to larger covariance magnitude and thus larger initial gradient. Therefore, the feature with higher inter-dataset disparity will influence the initial stages of learning more strongly. 2 (Var1(u)+Var2(u))+ 1 + Ï‰T B.4 DEBIASING AS CONSTRAINED OPTIMIZATION Our debiasing strategy is to construct new training distribution, Pfinal, by composing real and synthetic data. Definition B.4 (Composed Data Distribution). The final training distribution Pfinal is convex combination of the original real data distribution Preal and synthetic, causally disentangled distribution Psynth, controlled by mixing ratio Î» [0, 1]: Pfinal(x; Î») = (1 Î»)Preal(x) + Î»Psynth(x) (10) The choice of Î» governs fundamental trade-off. Definition B.5 (The Diversity-Information Fidelity Trade-off). The quality of the composed dataset is governed by trade-off between two competing properties: information fidelity and diversity. Information fidelity is the preservation of the core learning signal from Preal, necessary to maintain performance on in-distribution tasks. Diversity is the introduction of novel (u, v) pairings from Psynth that break the spurious correlation, necessary to improve out-of-distribution generalization. This trade-off leads to the formal definition of the optimal data composition problem. Problem B.1 (Optimal Data Composition). Let POOD(Ï€) and PID(Ï€) be the OOD and ID performance of policy Ï€. Let Ï€Î¸(Î») be the policy trained on Pfinal(x; Î»). The optimal data composition problem is to solve: Î» = arg max Î»[0,1] POOD(Ï€Î¸(Î»)) (11) where Ïµ 0 is tolerance for ID performance degradation. Directly solving this is intractable. The CIFT methodology provides practical proxy for this optimization problem. s.t. PID(Ï€Î¸(Î»)) PID(Ï€Î¸(0)) Ïµ, 19 Preprint. Under review. B.5 THEORETICAL MOTIVATION FOR THE CIFT FRAMEWORK This section provides formal analysis that motivates the CIFT methodology. We first analyze the learning dynamics to establish why the alignment between real and synthetic data signals is important. We then present statistical model that links this dynamic to the geometric properties of the feature space, thereby justifying our use of the Feature-Space SNR as predictive proxy. The effect of composing synthetic with real data is non-monotonic. As the mixing ratio Î» increases, the learning dynamics can transition between constructive and destructive interference. For small to moderate Î», causally-disentangled synthetic data can act as regularizer, where gradients from real (greal) and synthetic (gsynth) data are largely co-linear, reinforcing the learning of causal features. However, beyond certain ratio, the synthetic data signal can overwhelm the real data signal. The gradients may conflict, leading to training instability and harming performance. This behavior can be formalized by analyzing the gradient of mixed data batch. Proposition B.4 (Gradient Interference). Let the loss on mixed mini-batch be Lfinal = (1 Î±)Lreal +Î±Lsynth, where Î± is the proportion of synthetic data. The squared norm of the final gradient gfinal is: gfinal2 = (1 Î±)2greal2 + Î±2gsynth2 + 2Î±(1 Î±)grealgsynth I(Î¸, Î») (12) where I(Î¸, Î») = greal,gsynth grealgsynth is the Information Fidelity. Proof. The proof follows from the definition of the squared norm of vector sum: + b2 = a2 + b2 + 2a, b. Equation 12 shows that when Information Fidelity is positive, the gradients interfere constructively. When it is negative, they interfere destructively, which can reduce the magnitude of the learning step. This destructive interference can be seen as symptom of an underlying geometric misalignment in the feature space. Proposition B.5 (Feature-Space Collapse). Assume the real and synthetic feature distributions are approximately Gaussian along principal dimension, with means Âµreal and Âµsynth. If these means are opposed, there exists critical mixing proportion Î±dc at which the mean of the mixture distribution collapses toward the origin. Proof. Consider 1D feature space with Âµreal > 0 and Âµsynth < 0. The mixture mean is Âµfinal(Î±) = (1 Î±)Âµreal + Î±Âµsynth. Setting this to zero yields critical proportion Î±dc = . This corresponds to mixing ratio Î»dc = Âµreal/Âµsynth. Âµreal ÂµrealÂµsynth This analysis provides mechanism for point of decoherence. The collapse in Information Fidelity, manifested as destructive gradient interference, can be consequence of geometric collapse in the feature space. This provides rationale for employing geometric proxy, our Feature-Space SNR, to empirically identify and avoid this unstable regime."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL DETAILS",
            "content": "C.1 METRIC IMPLEMENTATION DETAILS Generative model evaluations were benchmarked on long-horizon table-wiping task. Source videos are approximately 80 seconds long (2400 frames at 30 FPS). We uniformly sample 300 frames from each generated video for all metric computations. All metrics are computed independently for three synchronized camera views (head, left hand, right hand), and we report the mean and standard deviation across these views. The distributional metrics (FVD, FID) measure the Frechet Distance between the feature distributions of real (Pr) and generated (Pg) data, defined as: d2((Âµr, Î£r), (Âµg, Î£g)) = Âµr Âµg2 The following provides details for each metric used. 2 + Tr(Î£r + Î£g 2(Î£rÎ£g)1/2) 20 Preprint. Under review. Frechet Video Distance (FVD). This metric (Unterthiner et al., 2018) applies the Frechet Distance to spatio-temporal features extracted from pre-trained I3D model (Carreira & Zisserman, 2017). Frechet Inception Distance (FID). This metric (Heusel et al., 2017) applies the Frechet Distance to spatial features from pre-trained Inception-V3 model (Szegedy et al., 2016) to assess per-frame image quality. Cross-View Feature Consistency (CVFC). This metric measures semantic alignment across views. For each timestep t, we extract image features using CLIP (Radford et al., 2021) for each view (f ) and compute the temporally-averaged pairwise cosine similarity. , lh , rh Multi-View Depth Consistency (MVDC). This metric evaluates geometric coherence across views using the MiDaS depth estimation model (Ranftl et al., 2020). Ewarp. This metric (Lai et al., 2018) measures frame-to-frame stability via the reconstruction error between frame It and the previous frame It1 warped by the optical flow Ftt1. Temporal LPIPS (T-LPIPS). This metric (Chu et al., 2020) assesses perceptual similarity between adjacent frames using the LPIPS model (Zhang et al., 2018b). Temporal Consistency Jitter (TCJ). This metric (Huynh-Thu & Ghanbari, 2006) quantifies instability as the variance of cosine similarities between consecutive CLIP features. CLIP Score. This metric (Radford et al., 2021; Hessel et al., 2021) measures the cosine similarity between the CLIP text embedding of the prompt and the CLIP image embeddings from the generated video frames, averaged over time. C.2 OPEN-LOOP STABILITY ANALYSIS AND ROBUSTNESS SCORE (RS) Evaluation Protocol. To analyze the effect of the data mixing ratio, we conducted an open-loop analysis (Collins & De Luca, 1995) on the dual-arm cloth folding task using the Ï€0 model. fixed pool of augmented data was generated using five visual prompts. Separate policies were then trained for various mixing ratios of real to synthetic data, from 100:0 to 100:500. Performance was quantified by the Mean Squared Error (MSE, scaled by 106) between the models predicted action vector at each timestep and the ground-truth action vector recorded from the robot. The evaluation used held-out test set partitioned into two subsets: an in-distribution (ID) set with videos visually congruent with the training data, and an out-of-distribution (OOD) set with videos featuring novel visual styles. Robustness Score (RS) Formulation. The Robustness Score is computed from these MSE values to provide single normalized metric for open-loop stability. For policy trained with mixing ratio Î», the score is defined as: (cid:18) (cid:18) RS(Î») = max 0, 1 (cid:19)(cid:19) MSEOOD(Î») MSEOOD(0) 100 (cid:18) MSEID(0) MSEID(Î») (cid:19) . (13) Here, MSEOOD(Î») and MSEID(Î») denote the average MSE over the OOD and ID test sets. The term (1 MSEOOD(Î») ) quantifies the relative improvement in OOD performance compared to the baseline MSEOOD(0) policy (Î» = 0). The final term, ( MSEID(0) MSEID(Î») degrades relative to the baseline. ), acts as penalty factor if the policys ID performance Results and Analysis. The detailed MSE results for this analysis are presented in Table 5. For ID trajectories, performance remained relatively stable across mixing ratios. For OOD trajectories, the baseline policy (100:0) exhibited high MSE (6900). Mixing ratios of 100:100 and 100:200 reduced the OOD error to approximately 100. At the 1:3 ratio, the OOD MSE increased to over 2200. These results show that (1) data composition can improve robustness to visual shifts without degrading ID performance, and (2) the effect of the mixing ratio is non-linear, with excessive augmentation degrading performance. Preprint. Under review. Table 5: Open-loop trajectory prediction MSE (106) on the cloth folding task. ID-Seen/Unseen refer to evaluation on trajectories from the original visual distribution; OOD conditions use trajectories with novel visual styles. Columns represent policies trained with different mixing ratios. Evaluation Condition / Mixing Ratio 100:0 100:100 100: 100:300 100:400 100:500 ID-Seen (Original) ID-Unseen (Original) OOD (dusk) OOD (romantic) OOD (tangerine right) 47 363 6993 6998 7117 119 598 100 98 166 504 105 101 112 103 631 2547 2286 3122 216 528 162 141 206 227 735 171 183 236 Feature-Space Geometry. To analyze the mechanism behind the performance degradation, we examined the geometry of the composed datasets in feature space. We extracted frame-level features using Inception-v3 and applied PCA to project them onto their first principal component. We then fit univariate Gaussian distribution, (Âµ, Ïƒ2), to these 1D projections. The results in Table 6 show that the distributions mean Âµ shifts with the mixing ratio. We compute the ratio Âµ/Ïƒ as proxy for the Feature-Space Signal-to-Noise Ratio (SNR). For both tasks, this SNR metric reaches minimum at the 100:300 mixing ratio, which corresponds to the point of performance degradation observed in the open-loop analysis. This correlation forms the basis of the CIFT framework, which uses SNR during the data curation phase to determine an optimal data composition. Table 6: Gaussian statistics along the first principal component for different data mixing ratios. The mean Âµ of the original data (100:0) is aligned to be non-negative for comparison. Folding clothes Picking up toy Ratio Âµ Ïƒ Âµ/Ïƒ 100: 100:100 100:200 100:300 100:400 100:500 100: 100:100 100:200 100:300 100:400 100:500 0.79 5.55 0. 1.17 5.39 0.2171 0.85 5.17 0.1644 0.05 5.18 0.0097 0.30 5.10 0.0588 0.73 5.04 0.1448 0.98 3.33 0. 0.76 3.84 0.1979 0.26 3.89 0.0668 0.05 3.84 0.0130 0.25 3.94 0.0635 0.37 3.78 0.0979 C.3 VALIDATION OF SNR METRIC ACROSS FEATURE BACKBONES Experimental Design. To evaluate the dependence of the SNR metric on the feature extractor, we computed it using three different backbones: Inception-v3 (Szegedy et al., 2016) (supervised), CLIP (Radford et al., 2021) (vision-language), and DINOv2 (Oquab et al., 2024) (self-supervised). For each backbone, we extracted frame-level features from datasets with varying data ratios and computed the SNR. Results. The results in Figure 7 show consistent trend across all backbones. The SNR value follows non-linear curve, reaching minimum at approximately the 1:3 real-to-synthetic data ratio. This consistency suggests the performance degradation point is systemic property of the data mixture. However, we observed differences in stability. CLIP showed task-dependent sensitivity. DINOv2 was sensitive to low-level noise. Inception-v3 provided stable response across the tested tasks. Consequently, it was selected for the primary analyses in this work. C.4 SUPPORTING ANALYSES FOR GENERATIVE MODEL Detailed Ablation Study. We provide component-wise analysis of our ablation studies  (Table 7)  . Removing periodic cross-view attention (Single-View Agg) lowers the MVDC score, indicating that multi-view context is important for geometric coherence. Replacing dynamic Canny edge guidance (Canny, 1986) with random noise increases FVD by approximately 400%. Using static Canny edges from the first video chunk results in high FVD, showing the necessity of dynamic structural guidance. Replacing our backbone with Qwen-Image-Edit (Wu et al., 2025) results in general decline in generative fidelity, validating the choice of FLUX.1-Kontext-dev (Labs et al., 2025). 22 Preprint. Under review. (a) Dual-arm: Folding Clothes (b) Single-arm: Wiping Table Figure 7: Comparison of SNR curves for three feature backbones on two tasks. All backbones exhibit U-shaped trend with minimum near the 1:3 ratio. Inception-v3 shows the most consistent response. Table 7: Ablation study on video generation quality. All metrics are averaged across the three views. indicates lower is better, and indicates higher is better. Model / Setting Ours (Full Model) Ablations on Model Design FVD FID CVFC MVDC Ewarp 103 T-LPIPS 103 TCJ 103 545.7 22.1 104.6 2. 0.8023 0.6318 3.7 1.3 10.1 6.1 0.218 Single-View Agg Canny to Random Noise Canny to Fixed First Chunk Backbone to Qwen-Image-Edit 609.1 106.7 2714.2 323.3 836.7 105.1 1400.4 148.2 112.3 9.6 483.1 36.1 159.1 19.8 355.6 35.5 0.7915 0.9321 0.7938 0.8244 0.5863 0.5592 0.5936 0.6103 4.4 1.3 19.2 0.45 3.6 1.0 4.8 1.3 13.3 8.2 174.1 22.0 8.63 4.50 17.3 10. 0.436 0.699 0.411 0.256 Ablations on Inference Strategy Unit-based Relighting 847.9 190.0 177.1 10.6 0. 0.6147 5.32 1.18 18.6 10.8 0.751 Discussion of Quantitative Generative Metrics. The CVFC score for our model is lower than that of RoboTransfer. We hypothesize this is related to RoboTransfers synthesis strategy, which separates the object from static background. This approach can increase feature similarity across views due to the near-identical backgrounds, but may produce unrealistic object contours. Metrics such as FVD and FID, which evaluate the entire image distribution, show more favorable results for our method. Human Evaluation. We conducted user study to evaluate perceptual quality. 20 participants viewed 30 video pairs in blind, randomized trial, with each pair containing video from our method and one from baseline. Participants rated each video on 5-point Likert scale across four criteria and selected an overall preferred video. The results  (Table 8)  show user preference for our method. Results were found to be statistically significant (p 0.01) via two-tailed paired t-test. Criterion Quality Smoothness Consistency Fidelity Overall Preference Ours RoboTransfer Preference for Ours (%) 4.5 0.6 4.3 0.7 4.5 0.5 4.6 0.4 3.2 1.0 2.8 1.1 2.9 1.1 3.7 0.9 89.5% 91.3% 92.1% 88.3% 90.3% Table 8: Human evaluation results comparing our method to RoboTransfer. Scores are mean SD on 1-5 Likert scale. Preprint. Under review. Method Task (384x512 pixels) Inference Time VRAM Utilization (%) First-Frame Generation FLUX.1-Kontext-dev (Our Base) Qwen-Image-Edit First Frame Synthesis First Frame Synthesis 3 min 25 min 33 sec Video-to-Video Inference RoboTransfer RoboEngine MVAug (Ours) 2129 frames @ 30 FPS 300 frames @ 30 FPS 2129 frames @ 30 FPS 100 min 20 min 20 min 97 97 88.5 95 97.9 Table 9: Inference performance for 384x512 video generation on single NVIDIA RTX 4090 GPU. C.5 COMPUTATIONAL PERFORMANCE Inference Performance and Resource Utilization. To provide transparent overview of the computational requirements, we benchmarked our method and related baselines on single NVIDIA RTX 4090 GPU (24GB VRAM), with all video inference conducted at resolution of 384x512 pixels. The results, detailed in Table 9, highlight the practical efficiency of our approach, particularly in memory-constrained scenarios at this resolution. For first-frame generation, our FLUX.1-Kontext-dev base model (Labs et al., 2025) is highly efficient, requiring 3 minutes, substantially faster than the 25 minutes needed by Qwen-ImageEdit (Wu et al., 2025). In the video-to-video synthesis comparison, the hardware limitations of baselines become apparent. RoboTransfer, for example, is memory-intensive and encounters outof-memory errors when attempting to generate long video sequences at 384x512 resolution on this GPU. We therefore benchmarked it on 2129-frame sequence that runs within the 24GB VRAM limit, task which took 100 minutes. In contrast, our MVAug pipeline completed the identical task in approximately 20 minutesa five-fold speedupwhile maintaining stable, high VRAM utilization. While our inference time is comparable to RoboEngines, our method generated over seven times more frames in that period (2129 vs. 300), indicating significantly higher throughput. Figure 8: MVAug synthesis example 1. Sampled frames from the three generated camera views, conditioned on the textual prompt Relight with vibrant tangerine glow emanating from the left side. 24 Preprint. Under review. Figure 9: MVAug synthesis example 2. Sampled frames from the three generated camera views, conditioned on the textual prompt Transform the lighting to include blazing yellow stage-like lighting from above. C.6 QUALITATIVE ANALYSIS OF THE MVAUG ENGINE This section visualizes the capabilities of our MVAug synthesis engine, which forms the foundation of the CIFT framework. We first showcase its ability to generate high-fidelity and diverse data augmentations, which are critical for exploring the data composition space (Figure 8, 9,10,11,12,13,14,15,16,17,18,19,20,21). Following this, we present visual ablation study of the generative model to provide insight into our key design choices and their impact on synthesis quality (Figure 22). Figure 10: MVAug synthesis example 3. Sampled frames from the three generated camera views, conditioned on the textual prompt Spotlight effect, soft dusk lighting, warm yellow glow, centered illumination. 25 Preprint. Under review. Figure 11: MVAug synthesis example 4. Sampled frames from the three generated camera views, conditioned on the textual prompt Transform the lighting to include blazing yellow stage-like lighting from above. Figure 12: MVAug synthesis example 5. Sampled frames from the three generated camera views, conditioned on the textual prompt Replace the background with green grass. Figure 13: MVAug synthesis example 6. Sampled frames from the three generated camera views, conditioned on the textual prompt Replace the background with brown floor. 26 Preprint. Under review. Figure 14: MVAug synthesis example 7. Sampled frames from the three generated camera views, conditioned on the textual prompt Recolor the plate to soft pink-blue shade. Figure 15: MVAug synthesis example 8. Sampled frames from the three generated camera views, conditioned on the textual prompt Add warm lighting to the vegetables in the scene. Figure 16: MVAug synthesis example 9. Sampled frames from the three generated camera views, conditioned on the textual prompt Replace the background with brown floor. Preprint. Under review. Figure 17: MVAug synthesis example 10. Sampled frames from the three generated camera views, conditioned on the textual prompt Apply purple finish to the oven. Figure 18: MVAug synthesis example 11. Sampled frames from the three generated camera views, conditioned on the textual prompt Change the lid of the ice maker to purple. Figure 19: MVAug synthesis example 12. Sampled frames from the three generated camera views, conditioned on the textual prompt Recolor the lid to cyan tone. 28 Preprint. Under review. Figure 20: MVAug synthesis example 13. Sampled frames from the three generated camera views, conditioned on the textual prompt Replace the background with green grass. Figure 21: MVAug synthesis example 14. Sampled frames from the three generated camera views, conditioned on the textual prompt Add warm orange-yellow glow inside the ice maker. 29 Preprint. Under review. (a) Ours (Full Model) (b) Single-View Agg (c) Canny Fixed First Chunk (d) Backbone Qwen-Image-Edit Figure 22: Qualitative results of the ablation study. These visuals confirm the quantitative findings in Table 7, showing degradations such as loss of consistency or structure in ablated models. 30 Preprint. Under review. C.7 REAL-WORLD POLICY EVALUATION Building on the generative capabilities shown above, this section demonstrates the real-world effectiveness and out-of-distribution (OOD) robustness of policies trained with the CIFT framework. We begin by illustrating direct comparison between our policy and the baseline in challenging semantic OOD scenario (Figure 23). To analyze the policys robustness in greater detail, we focus on the complex dual-arm cloth folding task. For this on-robot evaluation, we systematically introduced variety of OOD conditions, including changes in the cloths color and size, different surface textures and lighting conditions, and alternate starting orientations. Figures 2426 visualize several successful on-robot executions under these diverse conditions, showcasing the policys broad generalization capabilities. In addition to these successes, we also present failure case to transparently analyze the boundaries of the systems robustness (Figure 27). We hypothesize that this failure is not primarily due to limitation in the learned policy itself, but rather to hardware-induced stochasticity. During on-robot inference, we observed occasional jitter and instability in the grippers, form of real-world noise that can disrupt the execution of long-horizon, high-precision tasks like cloth folding. This analysis highlights critical challenge in real-world robotics and defines boundary condition for the policys performance, where physical hardware stability becomes limiting factor. Figure 23: Qualitative on-robot comparison. The CIFT-trained policy (bottom) succeeds despite significant change in surface appearance, challenging OOD scenario where the baseline policy (top) fails. 31 Preprint. Under review. Figure 24: Example A: Successful Execution in Low-Light Conditions. Key stages from successful on-robot trial demonstrating robustness to challenging lighting. Conditions: Unlit environment, white table surface, and front-oriented, dark green cloth (size 160). Outcome: The policy successfully completed the folding task. Figure 25: Example B: Robustness to Novel Texture and Task Orientation. successful execution under different set of OOD variations, highlighting generalization. Conditions: Direct lighting, textured wooden surface, and back-oriented, blue cloth (size 160). Outcome: The policy successfully completed the folding task. 32 Preprint. Under review. Figure 26: Example C: Generalization to Novel Object Size. Another successful trajectory, showcasing the policys ability to adapt to different object properties. Conditions: Unlit environment, wooden table surface, and front-oriented, purple cloth (size 120). Outcome: The policy successfully completed the folding task. Figure 27: Example D: Challenging Scenario Resulting in Failure. trial illustrating limitation of the current policy under specific combination of factors. Conditions: Direct lighting, white table surface, and front-oriented, blue cloth (size 160). Outcome: The policy failed to complete the task, highlighting boundary condition for its robustness."
        }
    ],
    "affiliations": [
        "Agibot",
        "CFCS",
        "PKU-Agibot Lab",
        "School of Computer Science, Peking University",
        "State Key Laboratory",
        "UCAS-Terminus AI Lab, University of Chinese Academy of Sciences"
    ]
}