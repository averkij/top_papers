{
    "paper_title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
    "authors": [
        "Ben Liu",
        "Zhen Qin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow's sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 4 8 2 1 . 5 0 5 2 : r Accelerate TarFlow Sampling with GS-Jacobi Iteration Ben Liu 1,2 and Zhen Qin 1 1TapTap, Shanghai, China 2Zhejiang University, Hangzhou, China"
        },
        {
            "title": "Abstract",
            "content": "Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlows sampling process is extremely slow. In this paper, we demonstrate that through series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: small number of blocks play major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53 in Img128cond, 5.32 in AFHQ, 2.96 in Img64uncond, and 2.51 in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow."
        },
        {
            "title": "Introduction",
            "content": "Image generation models have been widely applied in various scenarios. As an instance, normalizingflow-based models, from the original NICE [Dinh et al., 2014] model, to improved RealNVP [Dinh et al., 2016] and Glow [Kingma and Dhariwal, 2018] models, offer unique advantages through their invertible architecture that applies sequence of lossless transformations to noise vectors, but show limited performance in the generation of high solution and complex images. Recently, Transformer Autoregressive Flow model (TarFlow, [Zhai et al., 2024]) introduces stacks of autoregressive Transformer blocks (similar to MAF [Papamakarios et al., 2017]) into the building of affine coupling layers to do Non-Volume Preserving, combined with guidance [Ho and Salimans, 2022] and denoising [Bigdeli et al., 2023], finally achieves state-of-the-art results across multiple benchmarks. However, TarFlows sampling efficiency suffers from critical bottleneck: the causal attention structure within each autoregressive block forms nonlinear RNN. This forces strictly sequential computation during sampling, where each step must wait for the full update of preceding key-value caches, resulting in significantly reduced computational efficiency for large-scale image generation tasks. liuben0330@gmail.com zhenqin950102@gmail.com Preprint. Under review. In this paper, we try to solve this. We first transform the nonlinear RNN in the TarFlow sampling phase into diagonalized nonlinear system, then we can employ iteration-based solvers such as Gauss-Seidel-Jacobi (GS-Jacobi) iteration [Ortega and Rheinboldt, 2000] [Song et al., 2021] [Santilli et al., 2023]. However, naively applying GS-Jacobi iteration leads to generation failure (see 1st and 2nd row of Figure 4). Through detailed analysis, we discover that blocks in the TarFlow model have varying importance: small number of blocks play major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is appropriate. Leveraging these two metrics, we present sampling algorithm that substantially reduces calculation amount required. As summary, we list our contributions as follows: Transform the sampling process of the TarFlow model into diagonalized nonlinear system, and apply Gauss-Seidel-Jacobi hybrid iteration scheme to it, providing corresponding error propagation analysis and convergence guarantees; To identify the non-uniform transformation patterns in TarFlow blocks and control the number of iterations, we propose the Convergence Ranking Metric (CRM) to evaluate and measure them; To control the stability of iteration initial values to avoid numerical overflow, we propose the Initial Guessing Metric (IGM); Comprehensive experiments demonstrate 4.53 speedup in Img128cond, 5.32 in AFHQ, 2.96 in Img64uncond, 2.51 in Img64cond sampling without measurable degradation in FID scores or sample quality."
        },
        {
            "title": "2 Related Work",
            "content": "Normalizing Flow Based Models In the field of image generation, numerous methods have been proposed. From PixelRNN ([Van Den Oord et al., 2016]), GANs [Goodfellow et al., 2020], to DDPM [Ho et al., 2020], Stable Diffusion ([Podell et al., 2023]). Diffusion models seem to dominate this field, but normalizing flows still offer unique advantages, including exact invertibility [Whang et al., 2021] enabling precise density estimation [Trippe and Turner, 2018], single-step sampling for efficient generation [Grcic et al., 2021], and structured latent spaces that support interpretable manipulation [Bose et al., 2020]. Normalizing Flows learn an invertible model that transforms noise into data x, such that = (z). The key to build invertible models is accessible inverse function with Jacobi determinant easy to calculate, series of flow models accomplishes this through coupling layers. NICE [Dinh et al., 2014] introduced the additive coupling layers. To enhance the non-linear capability, RealNVP [Dinh et al., 2016] integrated scaling and shifting to the non-volume preserving transform as the affined coupling layer. Glow [Kingma and Dhariwal, 2018] improved the images generation by introducing invertible 1 1 convolution, and Flow++ [Ho et al., 2019] included attention mechanic. The most significant advantage of these models is that the inverse function is explicit and Jacobi matrix is lower triangle. This can avoid the complex calculation in the general invertible ResNet framework proposed in [Behrmann et al., 2019]. However, overly simple structure makes these flow models less nonlinear. To improve this, normalizing flows are combined with autoregressive models. IAF [Kingma et al., 2016] pioneered dimension-wise affine transformations conditioned on preceding dimensions to improve variational inference. MAF [Papamakarios et al., 2017] utilized the MADE [Germain et al., 2015] to create invertible autoregressive mappings. NAF [Huang et al., 2018], which replaced MAFs affine transformations with per-dimension monotonic neural networks to enhance expressivity. T-NAF [Patacchiola et al., 2024] augmented NAF by integrating single autoregressive Transformer, whereas Block Neural Autoregressive Flow [De Cao et al., 2020] adopted an end-to-end autoregressive monotonic network design. TarFlow [Zhai et al., 2024] proposed Transformer-based architecture together with set of techniques to train high performance normalizing flow models and show SOTA in many fields, thus becomes the sampling object of this article. 2 Figure 1: Simple intuition diagram of GS-Jacobi sampling. First pass forward small batch of images to compute Initial Guessing Metric (IGM) and Convergence Ranking Metric (CRM) for each block. When sampling, the initial iteration value (0) is determined by IGM; for blocks whose CRM is non-dominant, parallel Jacobi iterate X; for CRM-dominant blocks, segment into small modules Xg, parallel Jacobi iterating within modules, then serially deliver to next module. Linear/nonlinear systems refer to fi(x) = 0, Rn, Parallel solving of linear/nonlinear systems where fi, = 1, . . . , is linear/nonlinear function. The parallel solution of these systems is an important problem in scientific computing. [Saad, 2003] established methods such as Jacobi, Gauss-Seidel, successive over-relaxation (SOR), and Krylov subspace techniques for linear systems. Block-Jacobi iterations [Anzt et al., 2015, 2016, Chow et al., 2018] use GPU parallelization to solve linear/nonlinear equations. As special case, when fi takes the form fi(x) = xi gi(x<i), it is called an autoregressive system. Lots of approaches have been proposed to accelerate autoregressive computation. [Oord et al., 2018] introduced probability density distillation for transferring knowledge from slow autoregressive models to faster computation. MintNet [Song et al., 2019] developed specialized Newton-Raphson-based fixed-point iteration method to speed up autoregressive inversion. Similar theoretical concepts were earlier explored by [Naumov, 2017] without empirical validation."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Jacobi Mode Fixed Point Iteration Sampling Let denotes the noise direction and denotes the image direction, both with size (B, T, C), where B,T,C represent batch size, patchified sequence length, and feature dimension, respectively. For TarFlow model, an autoregressive block can be written as: Forward: Inverse: xt = exp(s(x<t))zt + u(x<t). zt = exp(s(x<t))(xt u(x<t)), (1) for = 1, , and x1 = z1. x<t := {xi}t1 i=1 denotes the history before time t, s(x<t), u(x<t) generated from causal attention block. In forward direction, all xt are given, so all s(x<t), u(x<t) can be calculate in parallel. But in inverse direction, xt can only be computed serially after x<t has been solved. In Table 3a, it takes about 213 seconds for such serial sampling to generate 100 128128 images with single A800 GPU. Denote exp(s(x<t)) = σ1 , u(x<t) = ut, the former process can be written in matrix form: σ1 1 z1 z2 ... zT = σ1 2 . . . σ1 3 x1 x2 ... xT u1 u2 ... uT . (2) Figure 2: The distance between (k) (up to 150 times) and target of all 8 blocks in four models. Most blocks converge within iteration times << , with each model exhibiting only one or two slowly descending curves. with σ1 = 1, u1 = 0, then the transform from to can be seen as an non-linear system: Forward: = Σ1(X)(X µ(X)), Inverse: = Σ(X)Z + µ(X). (3) For the inverse process, we can view the target as the fixed point of the nonlinear system g(X) = Σ1(X)Z + µ(X), and then solve it using the non-linear Jacobi iteration [Kelley, 1995]: (k+1) = Σ(X (k))Z + µ(X (k)), zt + u(k) x(k+1) = σ(k) , parallel for = 1, . . . , T. (4) with an initialized (0). We propose Proposition (1) to explain the convergence and error propagation of Jacobi mode iteration under this nonlinear system. See Appendix for detailed discussion. Proposition 1 (Converge and Error Propagation). For fixed point iteration (4), let ε(k) = (k) be the error after iteration, et be its t-th component, (k) zt u(k) = x(k) σ(k) , then: Equation (4) converges strictly after 1 times, e(k) (cid:80)t1 i=k+1 γ(k) ti e(k) , with γ(k) ti = ft xi (cid:12) (cid:12)X (k), + 2. This iteration method involves two components: the initial value (0) and the maximum number of iterations. As shown in Figure 4, different initialization strategies lead to different convergence effects, with poor strategies causing model collapse. Also as shown in Figure 2, different blocks converge at varying speeds, some blocks converge quickly while others slowly, suggesting that we should employ different iteration strategies for different blocks. We propose Initial Guessing Metric and Convergence Ranking Metric to address these two issues, respectively. 3.2 Initial Guessing Metric common choice for initialization is to take (0) = = [z1, z2, . . . , zt], i.e, the output of the former block, with intuition TarFlow transform images \"gradually\" (Zhai et al. [2024]), which means that the difference between adjacent TarFlow blocks maintain stable. As shown in Figure 3, the change from noise to image in most steps is gradual, and locates in the neighbor of , which 4 Figure 3: The trace of the sampling in four models. From top to bottom: Img128cond, Img64cond, Img64uncond, AFHQ. From left to right: noise, Block 7-0, denoised image. Figure 4: The influence of different initial value and iteration times of an Img64cond sample. From top to bottom: Set all (0) = Z, Jacobi 30 times; Adaptive initialized according to IGM, Jacobi 20 times; Adaptive by IGM, Jacobi 30 times; GS-Jacobi [0/7-16/8-10/13-6] . can be good initial guessing. However, in practice, we find that take all (0) = cause numeric collapse in Img64 models in Block0, as shown in the 1st row of Figure 4. An alternative workable guessing is (0) = Z0 = [z1, 0, . . . , 0], since pixel value ranges from -1 to 1 and centers in 0. natural strategy is comparing and Z0 and choose the better one. Since the worst inflation occurs at first few iteration, we define following \"Initial Guessing Metric\": IGM(X (0)) = Σ(X (0))Z + µ(X (0)) 2. (5) to measure rough distance with (0) chosen from {Z, Z0}. In Appendix D, different norms showed similar results, spectral norm is little better and we use it in this paper. We can treat IGM as model property which is determined once training completed. So we dont need to repeatly calculate (5) each time sampling and instead use the following steps: Select batch of images from the training set, patching to size (B, T, C), which is ; Forward passing through TarFlow blocks to get = Σ1(X )(X µ(X )); Calculate the residual Σ(X (0))Z + µ(X (0)) with both {Z, Z0}; Calculate the mean of residual in the dim B, calculate the norm of the (T, C) matrix. 3.3 Convergence Ranking Metric When sampling with (4), although all blocks converge strictly, some get nice solution with very small k, while others need near 1. As shown in Figure 2, Block6 of Img128cond, Block7 of AFHQ, Block0 of Img64cond, Block6 of Img64uncond behave worse compared to other blocks. To measure this difference, we propose the following Convergence Ranking Metric: CRM = Σ1(X)X2Ws2 + Wu2 with Ws, Wu the weight matrix of the project out layer of s(x<t), u(x<t). Ws measures the change of variance; Wu measures the mean, and Σ1(X)X measures the non-volume-preserving. See Appendix for detailed derivation. CRMs can be calculated with the following steps: (6) Extract the project out parameters for each TarFlow block, calculate Ws2, Wu2; Select small batch of images from the training set, go through the forward process, get Σ1(X)X with size (B, T, C), take means over dimension to get (T, C) size matrixs; Calculate Σ1(X)X2 then CRM for each block. This metric doesnt strictly measure the convergence rate, only represents the relative convergence ranking among TarFlow blocks, therefore we call it ranking metric. In Appendix D, different matrix norms behave similarly in relatively ranking, and we use spectral norm. By CRM, we can know whether block can converge rapidly or slowly, thus roughly determine the iteration times of (4). Blocks with dominant CRM values in Table 2 converge slowly in Figure 2. In practice, although only very few blocks in TarFlow converge slowly, this severely affects the speed and effectiveness of the Jacobi iteration method: For \"tough\" blocks, fewer iterations result in poor generation quality (see 2nd row of Figure 4), while more iterations improve the model but simultaneously lose the speed advantage. As shown in Table 3a 3b, the Jacobi-30 strategy exhibits significantly inferior performance, while Jacobi-60 shows measurable improvement with much more time cost. 3.4 Modular Guass-Seidel-Jacobi Iteration For (B, T, C) tensor, (4) updates all units in parallel, while \"For\" iteration updates 1 unit time, serially run 1 times. Naturally, an in-between method is to update set of units in parallel (with Jacobi) in one iteration, and serially go to another set, thats so-called Guass-Seidel-Jacobi iteration. Let := {xt}T g=1 an non-decrease segmentation for time-step index 1 : , Xg := {xtt Gg}G i=1 Xi, and similar defination for {Z, zt}, {Σ, σt}, {µ, ut}. Then the concept of modular GS-Jacobi method can be shown in Figure 1, and detailed algorithm is shown in Appendix E. g=1, X:g := (cid:83)g t=1, {Gg}G All the analysis of Jacobi mode iteration is applicable to the modules of GS-Jacobi sampling. We point out that GS-Jacobi can effectively improve the solution for blocks with large CRM: The probability of numerical overflow due to initial guessing value is greatly reduced. The size of error matrix (7) is smaller thus the error cumsum (8) reduced; The convergence of each sub-Jacobi will be accelerated, since the modules closer to the back will have more accurate initial value; An appropriate GS-Jacobi strategy (select Gg and maximum Jacobi iteration times) can achieve both accurate and fast solution. We segment the tough blocks into 8 equal modules and apply GS-Jacobi iteration in Figure 5. In Figure 2,pure Jacobi iteration requires between 50 to 150 times to converge for tough blocks, whereas in Figure 5, the GS-Jacobi method reduces this number to approximately 30, and usually only module1 suffer more difficult trace. So, proper strategy can take advantage of such modular iteration method. Ideally, IGM and CRM should be calculated for each GS-Jacobi modules to judge it is tough or not. Then for every modules, allocate more iteration to large CRM and vice versa. This can be seen as an adaptive strategy. In practice, equal-size segmentation and same Jacobi times is usually enough. Then strategies can be denoted in the format [Stack-GS-J-Else]. Stack indicates the tough blocks should be segmented; GS 6 Figure 5: The distance between GS-Jacobi iteration and target tend to converge within 30 iterations, and the 1st module suffer more difficult trace. of four tough blocks. All modules indicates the number of equal size segmentation with length //GS; indicates the maximum Jacobi times of each module; Else indicates the maximum Jacobi times for other blocks with small CRM. To determine the stacked blocks, select blocks with large CRM one by one until there are no dominant blocks in the remaining set. By Table 2, we stack Block6 in Img128cond, Block7 in AFHQ, Block0&6 in Img64uncond, Block0&7 in Img64cond."
        },
        {
            "title": "4 Experiment",
            "content": "We train four models given by [Zhai et al., 2024]: TARFLOW [4-1024-8-8-N (0, 0.152)] for Conditional ImageNet 128128 (Deng et al. [2009]); TARFLOW [8-768-8-8-N (0, 0.072)] for AFHQ 256256 (Choi et al. [2020]); TARFLOW [2-768-8-8-N (0, 0.052)] for Unconditional ImageNet 6464 (Van Den Oord et al. [2016]); TARFLOW [4-1024-8-8-N (0, 0.052)] for Conditional ImageNet 6464. The first three = 1024, the last = 256, and all four models have 8 TarFlow blocks. For convenience we will refer to them as Img128cond, AFHQ, Img64uncond and Img64cond. 4.1 Initial Guessing Metric We first calculate IGMs for four models with 128 training images, as shown in Table 1. We find that there are not significant difference between two initializations in Img128cond and AFHQ, while the sampling of Img64cond and Img64uncond will collapse if initialize (0) = for all blocks. This is evident in Table 1 since IGMs of Block0 in Img64cond and Img64uncond are pathological with (0) = Z, while set (0) = Z0 can release this. As shown in Figure 2, IGM is highly correlated with the potential maximum value occur during the iteration. We find that Img64cond and Img64uncond are more sensitive to the initial value. This may be because low-resolution images are more prone to mutations between pixels, which causes huge fluctuations in the attention layers parameters. In practice, the GS-Jacobi segmentation can greatly improve the problem of numerical overflow, so it is sufficient to simple initialize with Z, Z0 by IGM. 4.2 Convergence Ranking Metric We calculate CRMs for four models with 128 images in Table 2 the same time with IGMs. Detailed components are shown in Appendix C. Table 2 is consistent with Figure 2, following the simpe rule: The larger the CRM, the more Jacobi times required for convergence, and vice versa. 7 Table 1: Initial Guessing Metric for initialization with Z, Z0 for four models. Blocks (0) Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 Img s128cond 12.06 14.19 3.35 10.33 9.04 14.78 53.42 11.00 Z0 14.15 3.55 5.66 14.23 29.26 26.89 42.03 39.67 Img64cond Z0 Img64uncond Z 5.41e5 7.23 6.19 4.99 10.64 5.55 4.31 23.64 9.89 6.92 5.92 6.40 13.45 24.23 20.18 13.92 1443.13 10.49 5.89 6.65 3.13 2.74 34.98 23.11 9.98 5.64 6.25 3.50 8.47 5.46 5.64 31."
        },
        {
            "title": "AFHQ",
            "content": "Z 21.81 12.48 11.88 34.36 15.58 28.61 13.91 124.80 Z0 26.28 9.81 8.73 40.80 46.59 48.51 51.48 134.86 Table 2: Convergence Ranking Metric of four TarFlow models, with dominant blocks bolded."
        },
        {
            "title": "Models",
            "content": "Img128cond CRM Percent"
        },
        {
            "title": "AFHQ",
            "content": "Img64uncond CRM Percent CRM Percent Img64cond CRM Percent Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 6.52 7.03 3.08 13.63 9.66 9.17 70.54 5.05 5.22 5.63 2.47 10.93 7.74 7.35 56.57 4.05 51.85 51.45 66.76 64.98 73.77 84.05 76.64 348. 6.33 6.28 8.16 7.94 9.01 10.27 9.36 42.60 22.29 1.06 1.01 1.48 0.77 0.58 14.78 1.95 50.71 2.42 2.29 3.38 1.77 1.33 33.62 4.44 141.22 9.25 1.36 1.82 7.68 5.08 3.08 19.81 74.58 4.88 0.72 0.96 4.05 2.68 1.62 10.46 An important property is, only very few blocks in TarFlow model have relative large CRM. This may be because TarFlow, or other normalizing-flow based generative models are over-determined, which means that the amount of parameters is redundant relative to the generative capacity, and many blocks dont modify the images drastically, only carefully crafted. As shown in Figure 3, visually, many middle blocks have no obvious changes, which provides the possibility of GS-Jacobi acceleration. To identify such \"tough\" blocks, we just need to repeatedly select the block with the largest CRM until there is no dominant block in the remaining blocks. So for Img64cond, we first select Block0, but Block7 with CRM 10.46 is still dominant in remaining, so Block7 is included. 4.3 Quantitative Evaluations with FID We tune the hyperparameters cfg (classifier free guidance), lr (denoise learing rate), attntemp (attention temperature), sampling 50000 images with \"For\" iteration to restore the FIDs results in [Zhai et al., 2024]. Treat it as the target FIDs, then keep the hyperparameters consistent, sampling with different GS-Jacobi strategies, recording the FIDs, relative error (%), running time (100 s) and accelerating rate. Rates with relative error less than 1% are bolded. The strategy is as stated above [Stack-GS-J-Else]. All samplings are performed on 8 A800 GPUs with 80G memory. In Table 3a Img128, keep \"For\" iteration for tough Block6, just few pure Jacobi for other blocks are enough to get good FID, like [6-1024-1-10], speeds up 3.67. Then we fixed the total Jacobi times for Block6 with 128 and 256, and try different [GS-J] pairs. We found that simple strategies, like [6-8-32-10] can achieve results with relative error < 1% and surprising speed-up. Similar results occurred in the sampling for AFHQ, as shown in Table 3c. Since the two models both have just one tough block, the acceleration rate behave similarly. For Img64 models, the situations are quite different. As shown in Table 3b Img64uncond, acceleration rates are not as high as single tough block models because it stacks both Block0 and 6, but still speeds up about 3. In Img64uncond, we treat two tough blocks equal since the CRMs have no absolute gap. For Img64cond, we first stack both Block0 and Block7 to original \"For\" loop, get the rate 2.31. Then we keep Block0 unchanged, try different strategies for Block7, the rate can be improved to 2.42. From 8 Table 3: FIDS of different GS-Jacobi strategies for four Models, with relative error <1% bolded. Strategy time (rate) FID (rel) FID (rel)"
        },
        {
            "title": "Strategy",
            "content": "time (rate) Original Jacobi-30 Jacobi-60 [6-1024-1-8] [6-1024-1-10] [6-1024-1-20] [6-1-128-10] [6-2-64-10] [6-4-32-10] [6-8-16-10] [6-1-256-10] [6-2-128-10] [6-4-64-10] [6-8-32-10] [6-16-16-10] 5.06 10.36 6. 5.07 (0.20) 5.04 (0.00) 5.04 (0.00) 5.50 (8.70) 5.19 (2.60) 5.22 (3.20) 5.40 (6.72) 5.30 (4.74) 5.10 (0.79) 5.05 (0.00) 5.09 (0.59) 5.16 (2.00) 133.19 (1.00) 58.16 (2.29) 114.24 (1.17) Original Jacobi-30 Jacobi-60 33.11 (4.02) 36.31 (3.67) 52.98 (2.51) 48.46 (2.75) 34.08 (3.91) 27.59 (4.83) 24.37 (5.46) 78.93 (1.69) 48.75 (2.73) 35.64 (3.74) 29.41 (4.53) 26.38 (5.05) [0/6-1024-1-10] [0/6-1024-1-20] [0/6-1024-1-30] [0/6-2-64-20] [0/6-4-32-20] [0/6-8-16-20] [0/6-16-8-20] [0/6-2-128-20] [0/6-4-64-20] [0/6-8-32-20] [0/6-16-16-20] [0/6-32-8-20] 14.67 25.66 17. 15.27 (4.10) 14.77 (0.68) 14.72 (0.34) 15.22 (3.7) 15.18 (3.5) 16.44 (12.1) 21.16 (44.2) 15.03 (2.50) 14.81 (0.95) 14.80 (0.89) 15.17 (3.40) 17.47 (19.0) 109.05 (1.00) 45.60 (2.39) 92.06 (1.18) 38.17 (2.86) 47.65 (2.29) 57.16 (1.91) 51.11 (2.13) 36.89 (2.96) 28.11 (3.88) 26.14 (4.17) 76.41 (1.43) 50.06 (2.18) 36.84 (2.96) 31.38 (3.47) 29.86 (3.65) (a) Img128cond with cfg=1.5 lr=0.97 (b) Img64uncond with cfg=0.2 attn=0.3 lr=0.9 Strategy original [7-1024-1-10] [7-1024-1-20] [7-1024-1-30] [7-1-128-10] [7-2-64-10] [7-4-32-10] [7-8-16-10] [7-1-256-10] [7-2-128-10] [7-4-64-10] [7-8-32-10] [7-16-16-10] FID (rel) 13.60 13.61 (0.07) 13.60 (0.00) 13.60 (0.00) 14.70 (8.08) 14.27 (4.92) 14.15 (4.04) 15.52 (14.1) 14.21 (4.48) 14.07 (3.45) 13.82 (1.62) 13.73 (0.96) 14.12 (3.82) time (rate) Strategy FID (rel) time (rate) 109.24 (1.00) Original 4.42 12.16 (1.00) 26.58 (4.11) 37.69 (2.90) 48.70 (2.24) 33.36 (3.27) 23.56 (4.54) 19.06 (5.73) 16.83 (6.49) 53.62 (2.04) 33.98 (3.21) 24.81 (4.40) 20.54 (5.32) 18.49 (5.91) [0/7-256-1-6] [0/7-256-1-8] [0/7-256-1-10] [0/7-256/1-1/64-6] [0/7-256/2-1/32-6] [0/7-256/4-1/16-6] [0/7-256/8-1/8-6] [0/7-256/4-1/24-6] [0/7-256/8-1/13-6] [0/7-16/8-8/13-6] [0/7-16/8-10/13-6] [0/7-16/8-12/13-6] 4.42 (0.00) 4.42 (0.00) 4.42 (0.00) 4.41 (0.00) 4.40 (0.00) 4.54 (2.71 5.35 (21.0) 4.38 (0.00) 4.41 (0.00) 4.50 (1.81) 4.43 (0.23) 4.42 (0.00) 5.26 (2.31) 5.81 (2.09) 6.37 (1.91) 6.78 (1.79) 5.51 (2.20) 4.86 (2.50) 4.59 (2.65) 5.34 (2.28) 5.03 (2.42) 4.63 (2.63) 4.85 (2.51) 4.97 (2.45) (c) AFHQ with cfg=3.4 lr=1.4 (d) Img64cond with cfg=2.0 lr=1.0 Figure 5 and Table 2, we notice that Block0 behaves much tougher than any other blocks, so we segment Block0 into more modules and get 2.51 speed up. Based on all above experiments, we can conclude that, the fewer the blocks with dominant CRMs and the longer the time step after patching, the more significant the acceleration can achieve by GS-Jacobi sampling. This is consistent with intuition. GS-Jacobi achieves acceleration by iterating batches of equations in parallel, avoiding repeated serial updates of the kv caches in the \"For\" loop."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we comprehensively optimize the sampling process of TarFlow models. By identifying the non-uniform transformation patterns across TarFlow blocks and proposing IGM and CRM, we effectively address the problems of initial value chosen and convergence rate differences. The introduction of the GS-Jacobi iteration and its in-depth error propagation analysis provides practical and efficient solution for TarFlow sampling. The experimental results on multiple TarFlow models show the superiority of proposed methods. The GS-Jacobi sampling achieving speed-ups of 4.53 in Img128cond, 5.32 in AFHQ, 2.96 in Img64uncond, and 2.51 in Img64cond without degrading sample quality, which is of great significance for the application of TarFlow models. 9 There are still some aspects that can be further improved. The strong assumption in calculating CRM needs more theoretical verification. In addition, the current method of determining GS - Jacobi parameters is relatively simple, and more intelligent and adaptive strategies are expected to be developed in the future."
        },
        {
            "title": "References",
            "content": "Hartwig Anzt, Edmond Chow, and Jack Dongarra. Iterative sparse triangular solves for preconditioning. In Euro-Par 2015: Parallel Processing: 21st International Conference on Parallel and Distributed Computing, Vienna, Austria, August 24-28, 2015, Proceedings 21, pages 650661. Springer, 2015. Hartwig Anzt, Edmond Chow, Daniel Szyld, and Jack Dongarra. Domain overlap for iterative sparse triangular solves on gpus. In Software for Exascale Computing-SPPEXA 2013-2015, pages 527545. Springer, 2016. Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual networks. In International conference on machine learning, pages 573582. PMLR, 2019. Siavash Bigdeli, Geng Lin, Andrea Dunbar, Tiziano Portenier, and Matthias Zwicker. Learning generative models using denoising density estimators. IEEE Transactions on Neural Networks and Learning Systems, 2023. Joey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, and Will Hamilton. Latent variable modelling with hyperbolic normalizing flows. In International conference on machine learning, pages 10451055. PMLR, 2020. Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains, 2020. URL https://arxiv.org/abs/1912.01865. Edmond Chow, Hartwig Anzt, Jennifer Scott, and Jack Dongarra. Using jacobi iterations and blocking for solving sparse triangular systems in incomplete factorization preconditioning. Journal of Parallel and Distributed Computing, 119:219230, 2018. Nicola De Cao, Wilker Aziz, and Ivan Titov. Block neural autoregressive flow. In Uncertainty in artificial intelligence, pages 12631273. PMLR, 2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International conference on machine learning, pages 881889. PMLR, 2015. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Matej Grcic, Ivan Grubišic, and Siniša Šegvic. Densely connected normalizing flows. Advances in Neural Information Processing Systems, 34:2396823982, 2021. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 10 Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. In International conference on machine learning, pages 27222730. PMLR, 2019. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive flows. In International conference on machine learning, pages 20782087. PMLR, 2018. Carl Kelley. Iterative methods for linear and nonlinear equations. SIAM, 1995. Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. Durk Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016. Maxim Naumov. Parallel complexity of forward and backward propagation. arXiv preprint arXiv:1712.06577, 2017. Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast high-fidelity speech synthesis. In International conference on machine learning, pages 39183926. PMLR, 2018. James Ortega and Werner Rheinboldt. Iterative solution of nonlinear equations in several variables. SIAM, 2000. George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. Advances in neural information processing systems, 30, 2017. Massimiliano Patacchiola, Aliaksandra Shysheya, Katja Hofmann, and Richard Turner. Transformer neural autoregressive flows. arXiv preprint arXiv:2401.01855, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with masked convolutions. Advances in Neural Information Processing Systems, 32, 2019. Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 97919800. PMLR, 2021. Brian Trippe and Richard Turner. Conditional density estimation with bayesian normalising flows. arXiv preprint arXiv:1802.04908, 2018. Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. Jay Whang, Erik Lindgren, and Alex Dimakis. Composing normalizing flows for inverse problems. In International Conference on Machine Learning, pages 1115811169. PMLR, 2021. Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024."
        },
        {
            "title": "A Convergence and Error Propagation",
            "content": "Indeed, (4) is an equivalent form of the diagonal Newton method. Let (X) = g(X), to find its root, the iteration of diagonal Newton method is: (k+1) = (k) D1 (X (k))f (X (k)) (X (k)) the diagonal of Jacobi matrix Jf (X) = /X. Because g(X) is causal designed (X (k)) = I. Then the iteration formula of diagonal Newton method is again with D1 in dimension, D1 (4). Since D1 Jf is strictly lower triangle matrix with zero spectral norm, the fixed point iteration (diagonal Newton method) is superlinear convergence in the neighbor of . Indeed, this iteration can converge strictly after 1 times: x(k+1) x(k) = (σ(k) σ(k1) )zt + (u(k) Initially, 1 = z1, for = 2, one iteration can get the accurate absolutely accurate solution, since the difference between x(k) = x(k) Let ε(k) = (k) be the error after iteration, (k) ) u(k1) 2, and so for and σ(k) zt u(k) . But we dont need reduced after each iteration. , for t-th component: e(k+1) = e(k) (k) (cid:88) e(k) i=1 ft xi t1 (cid:88) i=1 = ft xi (cid:12) (cid:12)X (k)e(k) (cid:12) (cid:12)X (k)e(k) with the obtained by first-order Taylor expansion at . Denote ft xi (cid:80)t1 , and the error recursion can be written in matrix form: i=1 γ(k) ti e(k) (cid:12) (cid:12)X (k) as γ(k) ti , then e(k+1) ε(k+1) = Γ(k)ε(k), Γ(k) = 0 γ21 ... γT 1 (k) 0 . . . . . . γT,T 1 . . . 0 . (7) Since ε(k) = (cid:81)k1 one unit afer each iteration, so the error will go to 0 after 1 iteration. j=0 Γ(j)e(0) and e(0) 1 = 0, the product of Γ(k) will move down the lower triangle part Thus for each component et: e(k) = t1 (cid:88) i=k+1 ti e(k) γ(k) (8)"
        },
        {
            "title": "B Derivation for Convergence Ranking Metric",
            "content": "To measure the convergence difference between TarFlow blocks, direct method is to calculate the norm of error recursion matrix (7). But, analytically, zt γti = σt xi ut xi s(x<t) xi the derivative of σt, ut is hard to calculate since they are generated by series of attention layers, thus we propose simple but vaild alternatives. Since s(x<t), u(x<t) consist of series of attention layers and project out layer: u(x<t) xi = σtzt then: s(x<t) = Wsattn(x<t) + bs u(x<t) = Wuattn(x<t) + bu γti = (σtztWs + Wu) attn(x<t ) xi Γ = (Σ(X)ZWs + Wu)JAttn(X) Now we make very strong assumption: the norm of Jacobi matrix of attention layers between TarFlow blocks behave similar, or in coordination with the previous item, which can be dropped out. Its hard to strictly verify this but it holds in the experiments. Intuitively, this may be because the attention layers add various regularizations thus make the values bounded, and secondly, the changes in attention (i.e. derivatives) of TarFlow blocks with large changes are also large. In practice, we can replace the non-volume-projection parts with Σ1(X)X to calculate it at the same time with IGM in the forward direction. Intuitively Σ(X)Z and Σ1(X)X measure the same property from symmetric direction. Then the simplified norm of error can be written as: CRM = Σ1(X)X2Ws2 + Wu"
        },
        {
            "title": "C Components of Convergence Ranking Metric",
            "content": "The components of CRM for Non-Volume-Preserving Σ1X, variance Ws and mean Wu are shown as Table 4. Table 4: Components of Convergence Ranking Norm, with dominant CRMs bolded. Imagenet128 Ws Σ1X 20.06 14.77 13.65 33.65 43.65 53.86 131.84 26.08 Σ1X 53.48 8.00 8.55 13.79 8.36 8.06 40.50 13.97 Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 0.31 0.46 0.19 0.40 0.21 0.16 0.53 0.18 0.41 0.11 0.08 0.09 0.06 0.05 0.36 0.12 Wu CRM Σ1X 0.24 0.18 0.37 0.17 0.26 0.26 0.19 0. 52.78 39.32 34.70 73.39 118.90 159.91 153.54 286.83 6.52 7.03 3.08 13.63 9.66 9.17 70.54 5.05 Wu CRM Σ1X 0.10 0.12 0.24 0.12 0.25 0.13 0.13 .26 22.29 1.06 1.01 1.48 0.77 0.58 14.78 1.95 82.72 14.26 4.53 7.61 25.09 27.44 12.26 56.91 Img64uncond Ws"
        },
        {
            "title": "AFHQ",
            "content": "Ws Wu"
        },
        {
            "title": "CRM",
            "content": "0.96 1.28 1.90 0.87 0.61 0.51 0.49 1.21 0.86 0.75 0.73 0.65 0.94 0.96 0.94 0.93 Img64cond Ws Wu 1.70 0.59 0.16 0.16 0.29 0.17 0.21 0.34 0.42 0.78 0.59 0.53 0.33 0.38 0.45 0. 51.85 51.45 66.76 64.98 73.77 84.05 76.64 348.51 CRM 141.22 9.25 1.36 1.82 7.68 5.08 3.08 19."
        },
        {
            "title": "D IGM and CRM with Different Norm",
            "content": "We also computed the IGM and CRM under both the Frobenius norm and the 1-norm, shown in Table 5a 5b. The numerical values differ among the Frobenius norm (F), 1-norm, and spectral norm (Table 1 2), while the relative ranks of each block remain entirely consistent. Notably, the spectral norm exhibits more dispersed distribution in its measurements. Table 5: IGM and CRM for four models in both F-Norm and 1-Norm Img128cond Img64uncond Img64cond"
        },
        {
            "title": "Models",
            "content": "F-norm Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 12.40 14.83 3.56 10.27 9.37 14.88 53.36 10.27 1-Norm Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 137.76 167.96 88.66 106.1 97.83 151.26 634.4 118.41 Z0 14.61 3.92 6.50 13.94 29.58 27.92 41.41 39.87 Z0 198.3 53.21 99.38 161.9 551.18 357.02 454.57 449. 4.21e5 7.95 6.55 5.00 10.63 5.39 4.54 24.04 3.51e6 39.2 35.58 49.86 95.01 48.15 38.66 236.01 Z0 10.13 8.66 7.66 7.42 13.86 26.29 19.83 14. Z0 39.77 49.39 45.56 42.7 156.55 173.57 228.86 77.94 1416.83 10.67 9.39 6.47 4.02 3.86 33.96 24.98 14915.71 204.01 143.53 188.01 61.84 68.53 1028.43 524. Z0 8.29 7.08 7.83 5.30 10.19 7.82 7.74 35.99 Z0 156.51 116.13 98.95 99.36 169.69 112.22 174.63 633.18 21.38 12.79 13.12 35.07 16.39 30.00 14.99 134. 94.23 71.84 204.35 302.48 158.54 271.19 99.23 1698.14 Z0 23.24 11.23 11.69 44.06 53.62 54.26 60.40 145.32 Z0 148.39 113.24 69.05 277.35 1024.6 429.51 766.3 1370. (a) IGM with Frobenius Norm and 1-Norm Models Img128cond Img64cond Img64uncond AFHQ F-Norm CRM Percent CRM Percent CRM Percent CRM Percent Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block 9.35 10.89 5.72 17.12 16.50 19.34 83.75 7.17 5.50 6.41 3.37 10.08 9.71 11.38 49.30 4.22 150.29 14.62 3.66 4.59 13.09 10.82 6.59 33.25 63.42 6.17 1.54 1.93 5.52 4.57 2.78 14.03 19.23 2.12 2.25 3.12 1.68 1.43 21.99 2.75 35.22 3.88 4.13 5.72 3.08 2.62 40.27 5. 60.31 74.53 103.02 82.16 104.05 133.83 149.97 473.30 5.10 6.30 8.72 6.95 8.80 11.33 12.69 40.07 1-Norm CRM Percent CRM Percent CRM Percent CRM Percent Block0 Block1 Block2 Block3 Block4 Block5 Block6 Block7 99.98 194.08 84.62 666.29 160.53 345.13 1253.95 137.96 3.39 6.59 2.87 22.64 5.45 11.72 42.61 4.68 799.34 274.96 15.43 33.05 97.42 144.68 52.84 349.06 45.24 15.56 0.87 1.87 5.51 8.18 2.99 19. 219.04 46.72 25.37 64.74 15.79 23.56 341.51 133.23 25.17 5.37 2.91 7.44 1.81 2.70 39.25 15.31 1320.45 948.31 1085.57 2364.8 1799.76 4486.86 2396.26 14507.4 4.56 3.28 3.75 8.18 6.22 15.52 8.28 50.18 (b) CRM with Frobenius Norm and 1-Norm GS-Jacobi Sampling The complete algorithm of GS-Jacobi sampling is as follows: Algorithm 1 Guass-Seidel-Jacobi Sampling Input: Well trained TarFlow model containing blocks {Blockl := Σl, µl}L training samples X, batch of noise Z, other hyperparameters. l=1, batch of Output: Generated images of the same size as Z. Preprocessing: 1: Patchify into size (B, T, C) 2: for Blockl, in 1 : do 3: 4: 5: end for 6: Record the initial guessing mode of l-th block to or 7: Determine the GS modules numbers {Gl}L Calculate IGMl with equation (5) Calculate CRMl with equation (6) to CRMl, with JL //Gl 0 according to IGMl l=1 and Jacobi times {Jl}L l=1 for each blocks according Sampling: 1: Patchify into size (B, T, C) 2: Set 1 = Z, ebound = 108 3: for Blockl, in 1 : do for in 1 : Gl do 4: 5: 6: 7: set = 0, = 1000 while < Jl and > ebound do )Z X l(k+1) = l(k+1) = + 1 = Σg(X l(k) :g l(k) 8: 9: 10: 11: 12: 13: end for 14: return Unpatchified L+1 end while l+1 = l(k+1) end for + µg(X l(k) /(B C) :g ) An intuition Figure is: Figure 6: Intuition diagram of Gauss-Seidel-Jacobi sampling in single block. The horizontal long dashed line segment (4) into subgroups. The red rotating arrow denote the in-group Jacobi iteration with kg card(Gg) times. Then, the solution closed to will be delivered to next subgroup serially, this is the Gauss-Seidel part which denoted by blue rotating arrow."
        },
        {
            "title": "F Visual Comparison of Different Methods",
            "content": "We used the original \"For\" loop and GS-Jacobi strategy with FID relative error within 1% for each model under the same guidance and denoise, and visualized the sampling results in Figure 7. (a) AFHQ Original Sampling (b) AFHQ GS-Jacobi [7-8-32-10] (c) Img128 Original Sampling (d) Img128 GS-Jacobi [6-8-32-10] (e) Img64 Original Sampling (f) Img64 GS-Jacobi [0/7-16/8-10/13-6] Figure 7: Visual Comparison of Different Methods"
        }
    ],
    "affiliations": [
        "TapTap, Shanghai, China",
        "Zhejiang University, Hangzhou, China"
    ]
}