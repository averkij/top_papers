{
    "paper_title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance",
    "authors": [
        "Lixuan He",
        "Jie Feng",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \\textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 2 4 4 9 6 0 . 8 0 5 2 : r AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance Lixuan He, Jie Feng, Yong Li Department of Electronic Engineering, Tsinghua University, Beijing, China. helx23@mails.tsinghua.edu.cn, {fengjie,liyong07}@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of implicit rewards, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), novel single-stage algorithm that learns the optimal balance between SFTs implicit, path-level reward and RLs explicit, outcome-based reward. The core of AMFT is meta-gradient adaptive weight controller that treats the SFT-RL balance as learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFTs stability, sample efficiency, and performance, offering more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT."
        },
        {
            "title": "Introduction",
            "content": "Post-training fine-tuning is critical stage for adapting Large Language Models (LLMs) to complex downstream tasks, such as multi-step reasoning and nuanced human preferences [27, 2]. The prevailing paradigm has been sequential, two-stage pipeline: first, Supervised Fine-Tuning (SFT) on high-quality demonstrations, followed by Reinforcement Learning (RL) to optimize for specific reward signal. While this SFTRL pipeline has produced state-of-the-art models, it suffers from fundamental tension. SFT excels at teaching models to imitate expert patterns but is confined to its static dataset, leading it to memorize\" rather than truly learn,\" thus failing to generalize to out-of-distribution scenarios [4, 29]. Conversely, RL enables exploration beyond demonstrations, discovering novel solutions and generalizing better [13]. However, RL is notoriously sample-inefficient and unstable, especially under sparse rewards, and its on-policy nature limits its ability to learn beyond the base models capabilities [39, 22]. Most critically, the abrupt objective shift between stages often leads to catastrophic forgetting, where the RL stage overwrites the structured knowledge acquired during SFT [3, 4]. Preprint. Under review. This dilemma has spurred research into unified, single-stage frameworks that integrate SFT and RL more tightly. Recent approaches combine both objectives within single training loop, using adaptive mechanisms to balance the two. These mechanisms, however, are fundamentally reactive, relying on short-term, heuristic signals. For instance, SRFT uses policy entropy [11], while SuperRL, SASR, and DyME employ dynamic switches based on reward density, gradient norms, or generation correctness, respectively [22, 3, 20]. While these methods confirm the promise of hybrid training, they leave crucial question unanswered: how can we find the optimal balance between imitation and exploration in principled, dynamic, and forward-looking manner, rather than reacting to noisy, local signals? Figure 1: Overview of AMFTs motivation and framework. In this paper, we reframe this challenge through the lens of implicit reward learning. Recent theoretical work has established that SFT is not merely distribution matching; it can be formally understood as special case of RL that optimizes an implicit reward function encoded in expert demonstrations [35]. This provides powerful unified view: both SFT and RL are forms of reward optimization. SFT optimizes for an implicit, dense, path-level reward that encourages human-like reasoning structures, while RL optimizes for an explicit, often sparse, outcome-based reward that targets correctness. The challenge, therefore, is not to balance two different learning paradigms, but to learn the optimal combination of two complementary reward signals. Building on this insight, we propose Adaptive Meta Fine-Tuning (AMFT), novel single-stage algorithm that addresses this challenge directly. The core of AMFT is an adaptive weight controller that treats the balance between the SFT and RL objectives as learnable parameter, µ. Unlike prior methods, our controller employs meta-optimization strategy, updating µ using meta-gradients computed with respect to long-term validation objective. This forward-looking approach effectively learns dynamic training curriculum that maximizes final task performance. Intuitively, the controller learns to prioritize SFTs implicit reward when the policy is unstable, anchoring it to sound reasoning patterns. As the model gains competence, the controller shifts focus toward the explicit RL reward, encouraging exploration to discover higher-performing solutions. Our contributions are as follows: We propose AMFT, novel single-stage fine-tuning algorithm whose core innovation is metagradient-based adaptive weight controller. This controller learns the optimal, dynamic balance between imitation (SFT) and exploration (RL) by directly optimizing for final task performance, moving beyond the reactive, heuristic-based mechanisms of prior work. We conduct comprehensive evaluation on diverse benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points [44]), and vision-language navigation (V-IRL [41]), demonstrating that AMFT consistently sets new state-of-the-art. Our analysis of the training dynamics confirms that the meta-controller enables more stable and efficient learning process, leading to superior generalization, particularly in multi-modal and sparse-reward settings."
        },
        {
            "title": "2 Related Work",
            "content": "Our work is situated at the intersection of supervised fine-tuning (SFT), reinforcement learning (RL), and meta-learning for LLM alignment. We contextualize our contributions by reviewing these three key areas. Table 1: Systematic comparison of adaptive SFT-RL integration methods. Our proposed AMFT is the first to use meta-gradient controller that directly optimizes for long-term performance, rather than relying on reactive, short-term heuristics. Method Granularity Limitation Core Mechanism SFTRL (Baseline) SRFT [11] SuperRL [22] SASR [3] DyME [20] LUFFY [39] Sequential Training Single-Stage Weighted Sum Adaptive Switch Adaptive Integration Dynamic Mode Selection Off-Policy Data Mixing Stage-level Continuous Step-level Step-level Step-level Step-level Catastrophic forgetting, inefficient. Heuristic; entropy may not correlate with performance. Binary switch; not ideal for intermediate reward densities. Heuristic; gradient norm is proxy for learning stability. Binary switch; tailored for small models and sparse rewards. Relies on fixed ratio of off-policy data. AMFT (Ours) Single-Stage Meta-Learning Continuous Principled, forward-looking optimization of the trade-off."
        },
        {
            "title": "2.1 SFT: Anchoring Policy via Imitation",
            "content": "Supervised fine-tuning (SFT) on curated datasets is the foundational step for aligning pretrained LLMs with desired behaviors. Through large-scale instruction tuning [5], SFT teaches the model domain-specific knowledge and stylistic patterns. For instance, fine-tuning on step-by-step exemplars was key to the success of Minerva, which achieved state-of-the-art results on mathematical reasoning benchmarks by learning to generate detailed solutions [18]. Works like LIMA have even suggested that alignment can be achieved with surprisingly few high-quality examples, positioning SFT as powerful format teacher\" [46]. However, the limitations of SFT are well-documented. As it optimizes next-token prediction objective, the model primarily learns to imitate the training distribution. This often leads to memorization\" rather than the acquisition of generalizable principles, causing poor performance on out-of-distribution (OOD) task variants [4, 29]. SFT lacks mechanism to learn from its own mistakes or to explore solutions superior to those in its static dataset. As shown by recent theoretical analysis, this imitation process can be viewed as optimizing an implicit reward function encoded within the demonstrations, but often with missing KL regularization term, which can lead to policy drift [35]. While SFT provides strong initialization, it is insufficient on its own for robust reasoning. 2.2 RL: Generalizing Policy via Exploration Reinforcement Learning (RL) has become cornerstone of LLM alignment, enabling models to optimize directly for desired outcomes. The RL from Human Feedback (RLHF) paradigm, central to models like ChatGPT and InstructGPT, uses rewards learned from human preferences to train helpful and harmless agents [27, 33]. Beyond subjective preferences, RL with Verifiable Rewards (RLVR) has shown immense promise for reasoning tasks. Programmatic rewardssuch as the correctness of final mathematical answer or the successful execution of codeallow models like DeepSeek-R1 and Kimi-1.5 to significantly enhance their problem-solving abilities through exploration [14, 34]. Unlike SFT, RL can discover novel solutions not present in any initial dataset. However, RL for LLMs faces significant hurdles: Instability and Inefficiency: On-policy algorithms like PPO [31] are resource-intensive and prone to high variance. Moreover, studies suggest on-policy RLVR often amplifies pre-existing capabilities rather than creating new ones [45, 42], leading to policy collapse if not carefully regularized [47]. Sparse Rewards: In complex reasoning tasks, rewards are often sparse (e.g., single +1 for fully correct multi-step solution), making it difficult for an agent to receive learning signal, problem known as advantage collapse\" [20, 22]. Catastrophic Forgetting: When applied after SFT, the RL stage can overwrite the structured knowledge learned during imitation, undermining the benefits of the two-stage pipeline [3, 9]. 3 These challenges highlight that while RL enables generalization, it requires significant guidance to be effective and stable. 2.3 Hybrid Paradigms: The Path to Integration The complementary strengths and weaknesses of SFT and RL have naturally led to the development of hybrid paradigms. As summarized in Table 1, these approaches seek to unify the two, moving beyond simple sequential training. Initial hybrid methods focused on simple interleaving schedules or static, single-stage loss combinations [25, 21, 24]. However, fixed trade-off is rarely optimal. This led to new wave of adaptive frameworks that dynamically adjust the balance between SFT and RL based on heuristic signals reflecting the models learning state. As detailed in Table 1, these methods rely on various control signals: SRFT uses policy entropy [11]; SuperRL and DyME employ binary switches based on reward density or generation correctness [22, 20]; and SASR uses the gradient norm [3]. While powerful, these heuristic-based approaches are fundamentally reactive. They adjust the SFT-RL balance based on short-term, proxy signals that may not perfectly correlate with the ultimate goal of maximizing performance on unseen data. crucial gap remains for method that learns the balancing strategy in more principled, forward-looking manner. Our work, AMFT, fills this gap by treating the SFT-RL weighting factor µ as learnable parameter instead of relying on heuristics. Its meta-gradient controller directly optimizes µ to maximize long-term validation objective, an approach inspired by meta-learning for hyperparameter optimization [10]. This allows AMFT to autonomously learn an optimal training curriculum, offering principled, forward-looking solution to the imitation-exploration trade-off that is more automated and theoretically grounded than prior adaptive methods."
        },
        {
            "title": "3 Methodology",
            "content": "We propose AMFT, single-stage algorithm that unifies supervised fine-tuning and reinforcement learning. Our approach is grounded in modern theoretical perspective that reframes the fine-tuning problem itself: rather than balancing two disparate learning paradigms, we see it as optimizing single policy against two complementary reward signals. 3.1 Unified Objective for Imitation and Exploration Our framework begins by formalizing SFT and RL under common lens. We consider policy πθ that generates trajectory τ (e.g., reasoning chain) given an input x. This policy is guided by two distinct but valuable sources of information: Explicit Outcome-Based Reward (from RL). The first signal is an explicit, verifiable reward Rexplicit(τ ), which evaluates the final outcome of trajectory (e.g., +1 for correct answer, 0 otherwise). The standard RL objective is to maximize the expected value of this reward, encouraging the model to explore the solution space to find high-performing strategies: JRL(θ) = Eτ πθ [Rexplicit(τ )]. (1) In practice, we use policy gradient loss, denoted LRL(θ), such as the PPO-clip objective [31], to perform gradient ascent on JRL(θ). Implicit Path-Based Reward (from SFT). The second signal is derived from dataset of expert demonstrations, DSFT. Building upon the theoretical framework of implicit rewards [35, 7], we interpret the standard SFT loss (i.e., negative log-likelihood) not merely as imitation, but as the optimization of an implicit reward function, Rimplicit(τ ), which is high for trajectories that faithfully replicate the expert demonstrations. The SFT objective encourages the policy to align with these desirable paths: LSFT(θ) = E(x,ydemo)DSFT[log πθ(ydemox)]. This objective promotes imitation, anchoring the policy to distribution of human-aligned, structurally sound reasoning. (2) 4 The Unified Loss Function. AMFT elegantly merges these two objectives into single, dynamically weighted loss function: Ltotal(θ; µ) = (1 µ) LRL(θ) + µ LSFT(θ). (3) Here, the adaptive weight µt [0, 1] acts as dynamic dial, controlling the relative influence of exploration versus imitation at each training step t. The central novelty of our work lies in how we learn the optimal schedule for µt. 3.2 The Adaptive Weight Controller: Meta-Learning the Optimal Balance fixed or manually scheduled µ is unlikely to be optimal. AMFT addresses this by introducing an adaptive weight controller that learns the optimal schedule for µ online, using principled meta-optimization strategy. This elevates the balancing act to bilevel optimization problem: the inner loop optimizes the policy parameters θ given fixed µ, while the outer loop optimizes µ to improve long-term performance. Long-Term Strategy via Meta-Gradient. To ensure the learned schedule for µ is directly aligned with our ultimate goal, we treat µ as learnable parameter and optimize it via meta-gradient. We define utility function (θ) as the expected explicit reward on held-out validation set Dval: (θ) = E(x,τ )πθ(Dval)[Rexplicit(τ )]. (4) The controller periodically estimates the gradient of this utility with respect to µ, effectively asking: how will small change in the current SFT/RL balance affect long-term validation performance?\" This forward-looking signal is computed using the chain rule: θt µ µU (θt) = θU (θt) (5) . While computing the full Jacobian-vector product θt µ is expensive, we approximate it efficiently by differentiating single step of the inner optimization update, θt θt1 αθLtotal(θt1; µt), common technique in meta-learning [10]. This efficient one-step approximation nudges µ toward values expected to yield better future rewards. Synergizing Long-Term and Short-Term Control. The meta-gradient provides globally optimal direction for µ but is computationally expensive and estimated infrequently. It is therefore insufficient to handle immediate, step-level training instabilities. To address this, we supplement it with fast-acting heuristic based on policy entropy, H(πθ), which serves as robust proxy for the models uncertainty and stability. High Entropy (H(πθ) ): Indicates policy uncertainty or chaotic exploration. The controller increases µ to strengthen the stabilizing influence of the SFT loss. Low Entropy (H(πθ) ): Suggests the policy is becoming too deterministic, risking overfitting or policy collapse. The controller decreases µ to encourage exploration. The target entropy is hyperparameter, initialized based on the average entropy of the warm-up SFT policy. The final update rule for µ synergistically combines these two signals: µt+1 = clip (µt + ηµµU (θt) + ηH (H H(πθt)), µmin, µmax) , (6) where ηµ and ηH are learning rates for the long-term meta-gradient and short-term entropy heuristic, respectively. This dual-mechanism controller allows AMFT to pursue long-term optimal strategy while deftly navigating short-term instabilities. 3.3 The AMFT Algorithm in Practice Algorithm 1 outlines the complete single-stage training loop. The process begins with brief SFT warm-up phase, which provides stable and instruction-aligned initialization. In the main loop, each update step uses mixed batch of data from both the SFT dataset and on-policy rollouts. The controller first updates the weight µt. This new weight is then used to compute the unified loss Ltotal, which in turn updates the models parameters. This tight, single-loop integration ensures that every gradient step is informed by an up-to-date assessment of the optimal balance. More implementation details are in AppendixB. Algorithm 1 The AMFT Algorithm Require: Pretrained model πθ; Demonstration data DSFT; Environment env; Initial weight µinit Ensure: Fine-tuned model π θ 1: Initialize µ µinit; Initialize value function ϕ 2: Warm-up Phase: Train πθ on DSFT for steps using LSFT. 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: return πθ Sample SFT batch {(xi, yi)}m Sample RL rollouts {τj}n // Update adaptive weight µt Compute meta-gradient gµ µU (θt) on validation batch (periodically) Compute entropy heuristic gH H(πθt) µt+1 clip(µt + ηµgµ + ηH gH , µmin, µmax) // Update model parameters Ltotal (1 µt+1) LRL + µt+1 LSFT Update policy θt+1 and value function ϕt+1 by descending Ltotal and Lvalue j=1 πθ in env and compute LRL using PPO objective i=1 DSFT and compute LSFT 3.4 Theoretical Grounding Our adaptive framework is grounded in established theoretical principles. The SFT loss term, µ LSFT, can be interpreted as proxy for dynamic Kullback-Leibler (KL) divergence penalty, DKL(πdemoπθ), that regularizes the policy πθ against deviating too far from the expert demonstration distribution πdemo. We provide an intuitive sketch of this connection here and defer the full mathematical derivations to AppendixA. The SFT objective minimizes the negative log-likelihood: minθ Eτ πdemo[log πθ(τ x)]. This is equivalent to maximizing the log-probability of the demonstration data. Given that the entropy term Eτ πdemo [log πdemo(τ x)] is constant with respect to the model parameters θ, this maximization is mathematically equivalent to minimizing the KL divergence DKL(πdemoπθ). Thus, the SFT loss acts as data-driven KL regularizer. In this view, our adaptive controller is effectively learning the optimal, time-varying Lagrange multiplier (µ) for this KL constraint, making it more principled and responsive than the fixed KL penalty commonly used in RLHF [47]."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Training Datasets and Implementation Details. We fine-tune Qwen2.5-Math-7B [40] for mathematical reasoning and LLaMA-3.2-Vision-11B [1] for visual tasks. For math, we use the OpenR1Math-46k-8192 1 [39] dataset, leveraging its prompts for RL rollouts and its high-quality solutions for SFT-based objectives. For visual reasoning, we use the official training splits from the General Points [44] and V-IRL [41] benchmarks. To ensure fair comparison of convergence, all RL-based methods are trained for 500 steps using the PPO algorithm with 8 rollouts per prompt. Further details are in the AppendixC. Evaluation Benchmarks and Metrics. Our evaluation is extensive and multi-faceted. For mathematical reasoning, we report in-distribution (ID) performance on five benchmarks: AIME24 [19], AMC [19], MATH500 [16], Minerva [18], and OlympiadBench [15]. Generalization is measured on three out-of-distribution (OOD) benchmarks: ARC-C [6], GPQA-D [30], and MMLU-Pro [36]. For visual reasoning (General Points and V-IRL), we evaluate both ID and OOD performance, where OOD variants test generalization to novel rules or visual features. During inference, we set the generation temperature to 0.6 and maximum sequence length of 8,192 tokens. We employ Math-Verify for reward computation during training and the OAT-Grader [23] for final evaluation. Baseline Methods. We benchmark AMFT against comprehensive suite of baselines using Qwen2.5-Math-7B. 1https://huggingface.co/datasets/Elliott/Openr1-Math-46k-8192 Standard Paradigms: We include SFT-only on the training data; RL-only (GRPO) trained from scratch; and the sequential SFTRL pipeline. State-of-the-Art Hybrid Methods: We compare against leading single-stage frameworks: LUFFY [39], mixed-policy GRPO approach using off-policy data; ReLIFT [26], which interleaves RL with online fine-tuning on hard questions; TAPO [37], which integrates external knowledge as \"thought patterns\" into GRPO; and SRFT [11], which unifies SFT and RL through entropy-aware weighting. 4.2 Results on Mathematical Reasoning Quantitative Performance. As presented in Table 2, AMFT consistently establishes new state-ofthe-art. It achieves the highest average accuracy on both the five in-distribution (ID) math benchmarks (61.3%) and the three out-of-distribution (OOD) general reasoning benchmarks (63.3%). This demonstrates superior overall performance and generalization. This balanced excellence is directly attributable to the meta-gradient controller, which learns to retain sufficient SFT guidance to preserve general knowledge (preventing catastrophic forgetting on OOD tasks) while still aggressively optimizing in-domain reasoning performance through exploration. reasoning performance via exploration. Table 2: Results on mathematical reasoning (in-distribution) and general reasoning (out-ofdistribution) benchmarks. All models are trained from Qwen2.5-Math-7B. Accuracy (%) is reported. * indicates results are taken from the corresponding paper. In-Distribution Performance Out-of-Distribution Performance Model AIME24 AMC MATH500 Minerva Olympiad Avg. ARC-C GPQA-D MMLU-Pro Avg. Qwen2.5-Math Qwen2.5-Math-Instruct Supervised Fine-Tuning SFT SFTKL Reinforcement Learning RLGRPO [32] SimpleRL-Zero* [43] PRIME-Zero* [8] OpenReasoner-Zero* [17] SFT and RL SFT RL LUFFY [39] TAPO* [37] ReLIFT* [26] SRFT* [11] AMFT (ours) 11.5 12.4 31.0 12.9 24.1 27.0 17.0 16.5 32.0 29.4 33.3 28.2 35.3 36.1 31.6 48. 62.4 45.1 61.5 54.9 54.0 52.1 66.9 65.5 77.5 64.8 74.3 77.9 46.7 80.4 84.9 69. 79.0 76.0 81.4 82.4 84.1 87.2 83.4 85.0 89.8 89.5 7.9 33.0 39.0 26.4 32.9 25.0 39.0 33. 34.1 37.3 38.2 37.1 39.7 40.9 15.8 39.2 53.1 36.0 47.1 34.7 40.3 47.1 56.2 57.2 46.2 54.9 58. 62.1 22.7 42.6 54.1 38.0 48.9 43.5 46.3 46.2 54.6 55.3 55.7 54.0 59.5 61. 18.0 70.1 76.1 33.1 75.2 30.2 73.3 66.2 76.3 80.1 81.6 74.9 85.3 84.1 11.1 24. 25.7 22.2 30.9 23.2 18.2 29.8 37.8 39.5 37.9 40.9 46.4 47.5 16.7 34.1 45.1 30. 41.7 34.5 32.7 58.7 49.6 52.6 49.6 51.9 55.9 58.3 15.2 42.8 49.0 28.5 49.3 29.3 41.4 51. 54.6 57.4 56.4 55.9 62.5 63.3 Analysis of Training Dynamics. The learning trajectories in Figure 2 and Figure 3 reveal why AMFT succeeds. Sequential SFTRL methods are highly sensitive to the SFT duration, manuallytuned hyperparameter that is difficult to optimize (Figure 2). In contrast, AMFTs adaptive weight µ (red dash-dotted line) autonomously learns the optimal curriculum, smoothly transitioning from an SFT-dominant to an RL-dominant phase. Furthermore, Figure 3 shows that while pure RL quickly suffers from policy collapse (indicated by rapidly decreasing entropy and stagnating rewards), AMFTs controller injects stabilizing SFT guidance to maintain high entropy. This sustained exploration allows AMFT to operate in high-reward, high-entropy space, leading to more robust and higher-performing final policies. Please refer to Appendix for further studies on the AMFT Controller. 4.3 Results on Visual Reasoning and Generalization To validate AMFT in multi-modal settings, we test its ability to resolve the SFT Memorizes, RL Generalizes dilemma [4]. In-Distribution vs. Out-of-Distribution Performance. We evaluated each method on ID and OOD versions of the General Points and V-IRL tasks. As presented in Table 3, the results are conclusive. The baseline methods clearly illustrate the fundamental trade-off: SFT-only performs reasonably in-distribution but its performance collapses on OOD tasks. Conversely, RL-only shows 7 Figure 2: AMFT learning dynamics vs. sequential baselines on math benchmarks. The left y-axis shows validation accuracy (%), while the right y-axis shows the adaptive weight µ. AMFT (solid blue) achieves superior learning curve by dynamically adjusting µ (red dash-dotted), avoiding the difficult and manually-tuned trade-offs of sequential SFTRL methods. Figure 3: Comparative analysis of training dynamics between AMFT and pure RL-only (GRPO) baseline on mathematical reasoning tasks. The main 3D visualization (left) plots the learning trajectories across training steps, outcome rewards, and policy entropy. For clarity, 2D projections for policy entropy (top right) and outcome rewards (bottom right) are provided. The plots reveal two distinct behaviors: the RL-only policy rapidly converges to low-entropy state (policy collapse), limiting its reward potential. much stronger OOD performance but lags behind SFT on ID tasks. The two-stage RL-from-SFT and off-policy LUFFY offer progressively better compromises. However, AMFT consistently achieves the best performance across all conditions. It not only sets the highest score on ID tasks but also exhibits the most robust OOD generalization, with the smallest relative performance drop. This demonstrates that AMFTs adaptive controller learns to leverage SFTs structural guidance to build strong in-distribution foundation while seamlessly transitioning to RL-driven exploration to learn the underlying task logic required for OOD success. It does not just Table 3: In-distribution (ID) and Out-of-distribution (OOD) performance on visual reasoning tasks. Win/Success rates (%) are reported for both rule and visual generalization. The data reflects the principle that SFT excels in-distribution while RL generalizes better out-of-distribution, with AMFT achieving the best of both. General Points (Visual) V-IRL Navigation Method ID Win% OOD (Rule)% OOD (Visual)% ID Success% OOD (Rule)% OOD (Visual)% SFT-only RL-only (from scratch) RL-from-SFT (two-stage) LUFFY [39] AMFT (ours) 22.5 41.2 55.0 62.3 72.1 5.6 14.2 25.5 35.2 45.8 13.7 41.2 52.0 61. 70.3 88.0 85.0 92.5 94.0 95.2 2.5 45.0 55.1 64.8 71.4 11.1 65.0 77.8 82. 85.2 Table 4: Ablation study results across all task domains. Performance is reported as the primary metric for each task (%). All components of the AMFT controller are shown to be critical. AMFT Variant Math Reasoning Acc. (%) General Points (Visual) V-IRL Navigation ID Avg. OOD Avg. ID Win% OOD (Rule)% OOD (Visual)% ID Success% OOD (Rule)% OOD (Visual)% AMFT (Full) w/o Meta-Gradient (Entropy-only) w/o Entropy Heuristic (Meta-only) w/o SFT Warm-up 61.3 57.0 55.0 56.2 63.3 60.5 57.5 58. 72.1 68.1 65.4 62.3 45.8 41.5 38.2 35.6 70.3 66.2 62.1 58.5 95.2 92.3 88.5 85.1 71.4 67.8 62.1 60.5 85.2 81.0 75.3 72. Table 5: Computational and sample efficiency analysis on OOD (Visual) benchmarks. We report the resources required to reach target performance (60% win rate on GP-Visual, 70% success rate on V-IRL). AMFT demonstrates significant gains in both computational (fewer steps) and sample efficiency (fewer expensive RL rollouts). Peak performance is reported for methods that failed to reach the target. General Points (Visual) Target: 60% Win Rate V-IRL Navigation Target: 70% Success Rate Method # Training Steps # SFT Samples # RL Rollouts Peak Perf. (%) # Training Steps # SFT Samples # RL Rollouts Peak Perf. (%) Baselines SFT-only RL-from-scratch 480 150,000 0 Two-Stage & Hybrid Methods 420 400 RL-from-SFT LUFFY [39] 60,000 (fixed) 32,000 AMFT (ours) 310 49,600 0 30,720 21,760 22,400 15,840 *DNC: Did Not Converge. Performance plateaued without reaching the target. 13.7 60.0 60.0 60.0 60.0 450 120,000 0 50,000 (fixed) 36,000 54,400 0 >90,000 >75,000 44,800 30, 11.1 48.2 (DNC*) 68.5 (DNC*) 70.0 70.0 combine SFT and RL; it learns the optimal curriculum for integrating them, thereby achieving the best of both worlds. Ablation Study and Efficiency Analysis. To dissect AMFTs core components, our ablation study  (Table 4)  confirms that all parts are essential and synergistic. Removing the meta-gradient (w/o Meta-Gradient) causes consistent performance drop, underscoring that the forward-looking, performance-driven signal is crucial for discovering an optimal curriculum. Meanwhile, removing the entropy heuristic (w/o Entropy Heuristic) leads to even greater degradation and training instability, confirming its necessity as reactive regularizer for short-term stability. Furthermore, eliminating the initial SFT warm-up (w/o SFT Warm-up) significantly harms performance, proving that stable, instruction-aligned starting point is vital for effective exploration, particularly in complex visual domains. Computational Cost and Sample Efficiency. To quantify AMFTs efficiency, we measured the resources required to reach demanding performance thresholds on the challenging Out-ofDistribution (OOD) Visual variants of our multi-modal benchmarks: 60% win rate on General Points and 70% success rate on V-IRL Navigation. We assess efficiency via computational cost (# Training Steps) and sample efficiency, distinguishing between inexpensive SFT samples and costly RL rollouts. The results in Table 5 confirm AMFTs superiority. While baselines like SFT-only and RL-from-scratch fail to reach these OOD targets, AMFT converges with the fewest training steps. Most critically, it dramatically reduces the number of expensive RL rollouts by intelligently substituting them with cheaper SFT updates via its adaptive controller. This principled resource management confirms that AMFT provides more practical and scalable path to robust generalization. Please refer to the appendix for more details on the experimental results and analysis."
        },
        {
            "title": "5 Discussion",
            "content": "Conclusion. We introduced Adaptive Meta Fine-Tuning , single-stage algorithm that unifies supervised fine-tuning and reinforcement learning. By reframing the fine-tuning challenge through the lens of implicit rewards, AMFT moves beyond reactive heuristics. Its core innovationa meta-gradient adaptive weight controllerlearns the optimal, dynamic balance between SFTs path-level imitation and RLs outcome-based exploration by directly optimizing for long-term task performance. Crucially, our work demonstrates that the optimal fine-tuning curriculum is not static recipe to be discovered, but dynamic strategy to be learned, paving principled path toward more autonomous model alignment. AMFT consistently achieved state-of-the-art performance on diverse reasoning benchmarks, with its learned curriculum preventing catastrophic forgetting and policy collapse to yield superior robustness and sample efficiency. Limitations and Future Work. The primary limitation of AMFT is the computational overhead of the meta-gradient, trade-off for its principled optimization. Its performance also depends on high-quality validation set and introduces new controller hyperparameters. Future work will therefore focus on developing more efficient meta-gradient approximations and investigating AMFTs application to other critical alignment challenges."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card. 2024. [2] Yuntao Bai, Andy Jones, Kamaldeep Bhasin, Amanda Askell, Anna Chen, Sam Bowman, Avital Vardis, Tom Brown, Ben Mann, Nelson N. DasSarma, Dawn Drain, Stanislav Fort, Zac HatfieldDodds, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, T. Hume, Scott Johnston, Tomotake L. J. Kaplan, Anna Goldie, Nova DasSarma, Jared Kaplan, Sam McCandlish, L. Olah, Catherine Olsson, Dario Amodei, Nova McGreggor, Danny Hernandez, Dustin Li, Chris Olah, and Eli Tran-Johnson. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. [3] Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, and Xiao Wang. Step-wise adaptive integration of supervised fine-tuning and reinforcement learning for task-specific llms, 2025. [4] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. [5] Hyung Won Chung, Le Hou, Shayne Longpre, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [7] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, and et al. Process reinforcement through implicit rewards, 2025. [8] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [9] Heshan Fernando, Han Shen, Parikshit Ram, Yi Zhou, Horst Samulowitz, Nathalie Baracaldo, and Tianyi Chen. Mitigating forgetting in llm supervised fine-tuning and preference learning, 2025. [10] Luca Franceschi, Paolo Frasconi, Saverio Salzo, et al. Bilevel programming for hyperparameter optimization and meta-learning. ICML, 2018. 10 [11] Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning, 2025. [12] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, Matthieu Geist, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation, 2022. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [15] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [17] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-Reasoner-Zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [18] Lewkowycz et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. [19] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [20] Jiazhen Liu, Yuchuan Deng, and Long Chen. Empowering small vlms to think with dynamic memorization and exploration, 2025. [21] Mingyang Liu, Gabriele Farina, and Asuman Ozdaglar. Uft: Unifying supervised and reinforcement fine-tuning. arXiv preprint arXiv:2505.16984, 2025. [22] Yihao Liu, Shuocheng Li, Lang Cao, Yuhang Xie, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, and Dongmei Zhang. Superrl: Reinforcement learning with supervision to boost language model reasoning, 2025. [23] Zichen Liu, Changyu Chen, Xinyi Wan, Chao Du, Wee Sun Lee, and Min Lin. OAT: researchfriendly framework for LLM online alignment. https://github.com/sail-sg/oat, 2024. [24] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning, 2024. [25] Lu Ma, Hao Liang, Meiyi Qiang, et al. Learning what rl cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. [26] Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. [27] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. 11 [28] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to q: Your language model is secretly q-function, 2024. [29] Neel Rajani, Aryo Pradipta Gema, Seraphina Goldfarb-Tarrant, and Ivan Titov. Scalpel vs. hammer: Grpo amplifies existing capabilities, sft replaces them, 2025. [30] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [33] Stiennon, Ouyang, Wu, et al. Learning to summarize from human feedback. NeurIPS, 2020. [34] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [35] Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, and Xipeng Qiu. Implicit reward as the bridge: unified view of sft and dpo connections, 2025. [36] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [37] Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, and Jianhua Tao. Thought-augmented policy optimization: Bridging external guidance and internal capabilities. arXiv preprint arXiv:2505.15692, 2025. [38] Markus Wulfmeier, Michael Bloesch, Nino Vieillard, Arun Ahuja, Jorg Bornschein, Sandy Huang, Artem Sokolov, Matt Barnes, Guillaume Desjardins, Alex Bewley, Sarah Maria Elisabeth Bechtle, Jost Tobias Springenberg, Nikola Momchev, Olivier Bachem, Matthieu Geist, and Martin Riedmiller. Imitating language via scalable inverse reinforcement learning, 2024. [39] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. [40] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. 12 [41] Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. V-IRL: Grounding virtual intelligence in real life. In European conference on computer vision, 2024. [42] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. [43] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRL-Zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [44] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [45] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. [46] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. [47] Daniel Ziegler, Stiennon, Jeffrey Wu, et al. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Theoretical Foundations of AMFT",
            "content": "This section provides the rigorous mathematical derivations that underpin AMFTs core premise: reframing SFT and RL as the optimization of complementary reward signals. We build upon the theoretical framework established by [35] to formalize the connection between imitation learning (SFT) and implicit reward optimization. A.1 Full Derivation of SFT as Implicit Reward Optimization The goal of this section is to formally demonstrate that the standard Supervised Fine-Tuning (SFT) objective is special case of broader reinforcement learning framework aimed at optimizing an implicit reward function [35, 7]. This unified perspective is central to the design of AMFT. Our derivation begins with general objective for imitation learning: minimizing the divergence between the state-action distribution of policy π (µπ) and that of an expert (µE). In the context of LLM post-training, it is crucial not to deviate excessively from the knowledge and linguistic priors of the base model, πref. Therefore, we modify the standard entropy regularizer to Kullback-Leibler (KL) divergence term against the reference policy [35], yielding the objective: min π Df (µπµE) + βDKL(ππref) (7) where Df is chosen f-divergence and β is the regularization coefficient. Following the principles of non-adversarial imitation learning [38, 12], this objective can be transformed into an equivalent min-max problem using the convex conjugate function, . This allows us to re-express the divergence minimization problem as reward optimization problem: Df (µπµE) + βDKL(ππref) max {Eµπ [g] EµE [f (g)]} + βDKL(ππref) (8) where : is an arbitrary function. By substituting = to align with the standard RL reward maximization convention and leveraging the saddle-point property of the objective, we can swap the min and max operators [35]: (cid:104) min EµE [f (r)] + max π (cid:105) (Eµπ [r] βDKL(ππref)) (9) 13 min π = min π The inner maximization problem, maxπ (Eµπ [r] βDKL(ππref)), is standard regularized RL objective. As demonstrated in prior work [28], this problem has closed-form solution where the value of the objective function at the optimal policy π is the optimal value function at the initial state, (s0). Furthermore, the relationship between the optimal policy π and the corresponding reward function is given by [28]: r(x, y) = β log + (s0) (st) (10) π(yx) πref(yx) where st denotes state in the trajectory y. To complete the derivation and connect this general framework to SFT, we select specific fdivergence. Following [35], we choose the **Total Variation (TV) distance**. The convex conjugate for the TV distance is the identity function, (t) = t. Substituting this into Eq. 9 and focusing on the inner maximization over π, the objective becomes: max π,r = max π,r [EµE [f (r)] Vπ(s0)] [EµE [r] Vπ(s0)] (11) Now, we substitute the reward in Eq. 11 with its policy-dependent form from Eq. 10. This yields: (cid:20) β log max π EµE = max π E(x,y)DSFT π(yx) πref(yx) (cid:20) β log π(yx) πref(yx) (cid:21) + Vπ(s0) Vπ(st) Vπ(s0) (cid:21) Vπ(st) (12) Since πref and Vπ(st) are constant with respect to the optimization of π(yx) at each step, maximizing this objective is equivalent to maximizing E(x,y)DSFT[log π(yx)]. This is precisely the objective of minimizing the standard SFT loss (negative log-likelihood): LSFT(θ) = E(x,y)DSFT[log πθ(yx)] This derivation formally establishes that SFT is special case of implicit reward optimization, where the implicit reward function being learned is identical in form to that of preference learning methods like DPO [35]. This provides the theoretical basis for our unified view of SFT and RL as optimizing complementary reward signals within single framework. (13) A.2 Theoretical Justification of SFT Loss as KL Divergence Proxy In this section, we expand on the theoretical grounding of our AMFT framework, specifically elucidating how the Supervised Fine-Tuning (SFT) loss term in our unified objective (Eq. 3 in the main paper) can be formally interpreted as dynamic, data-driven Kullback-Leibler (KL) divergence penalty. This perspective is crucial for understanding why learning the balance parameter µ is more principled approach than using fixed KL penalty, as is common in traditional RLHF pipelines [47]. Our goal is to demonstrate that minimizing the SFT loss is mathematically equivalent to minimizing the KL divergence between the expert demonstration distribution, which we denote as πdemo, and the models policy, πθ. Decomposing the KL Divergence. We begin with the formal definition of the KL divergence from the expert distribution πdemo to the policy distribution πθ: (cid:20) (cid:21) DKL(πdemoπθ) = E(x,y)πdemo log πdemo(yx) πθ(yx) By the properties of logarithms, we can decompose this expression into two distinct terms: DKL(πdemoπθ) = E(x,y)πdemo [log πdemo(yx)] (cid:125) (cid:124) (cid:123)(cid:122) Term 1: Negative Entropy of Expert E(x,y)πdemo[log πθ(yx)] (cid:123)(cid:122) (cid:125) Term 2: Negative SFT Loss (cid:124) 14 (14) (15) Analyzing the Components. We now analyze each term in Eq. 15 with respect to the optimization of the model parameters θ: Term 2 is precisely the expectation of the log-likelihood of the demonstration data under the models policy. This is, by definition, the negative of the standard SFT loss function: E(x,y)πdemo [log πθ(yx)] = LSFT(θ) (16) Term 1 is the negative entropy of the expert demonstration distribution, H(πdemo). The crucial insight here is that the expert distribution πdemo is defined by the static, pre-existing SFT dataset (DSFT). Therefore, its entropy is fixed constant value with respect to the model parameters θ that we are optimizing. Establishing Equivalence. Substituting these observations back into Eq. 15, we get: DKL(πdemoπθ) = H(πdemo) + LSFT(θ) (17) This reveals direct linear relationship between the KL divergence and the SFT loss. When we seek to find the optimal parameters θ that minimize the SFT loss, we are performing the following optimization: θ SFT = arg min θ LSFT(θ) = arg min θ (DKL(πdemoπθ) + H(πdemo)) (18) Since H(πdemo) is constant with respect to θ, minimizing the SFT loss is mathematically equivalent to minimizing the KL divergence from the expert distribution to the policy distribution. Implication for AMFT: Principled, Adaptive Regularizer. This theoretical connection provides deeper justification for the AMFT framework. The SFT term, weighted by µ, in our unified loss function: Ltotal(θ; µ) = (1 µ) LRL(θ) + µ LSFT(θ) (19) is not merely an imitation objective. It functions as principled KL divergence penalty that regularizes the policy πθ against deviating too far from the expert demonstration distribution πdemo. This contrasts sharply with the fixed KL penalty commonly used in RLHF, which typically regularizes against the base model πref. While regularizing against πref prevents catastrophic forgetting of pretrained knowledge, it can be overly restrictive, penalizing the model for learning novel, desirable behaviors present in the expert data. In AMFT, the meta-gradient controller learns the optimal, time-varying schedule for the weight µ. From this theoretical standpoint, the controller is effectively learning the optimal Lagrange multiplier for the imitation constraint (DKL(πdemoπθ)) at each stage of training. It learns when to strongly pull the policy towards the expert distribution (high µ) for stability and structured reasoning, and when to relax this constraint (low µ) to allow for reward-driven exploration. This makes AMFTs approach to balancing imitation and exploration more principled, dynamic, and directly tied to the optimization landscape than methods relying on fixed KL penalties or reactive heuristics. A.3 Assumptions and Limitations of the Meta-Gradient Approximation The computational tractability of our meta-gradient controller hinges on one-step approximation of the inner optimization loop, technique well-established in the meta-learning literature [10]. While powerful, this approach rests on several key assumptions and entails certain limitations that are important to acknowledge for complete understanding of AMFTs behavior. This section details these theoretical and practical trade-offs. The Core Assumption: One-Step Lookahead as Valid Proxy. The fundamental assumption of our approximation is that the effect of change in the balancing weight µ on the model parameters θ after single gradient descent step is sufficiently informative proxy for its long-term impact. As detailed in Appendix B.2, we approximate the full Jacobian θt by considering only the most µt recent update: θt θt1 αθθLtotal(θt1; µt). This assumption holds most reliably under two 15 conditions: (1) sufficiently small inner-loop learning rate αθ, which ensures that single updates do not drastically alter the loss landscape, and (2) relatively smooth loss landscape for Ltotal with respect to θ. If the landscape were highly chaotic, single gradient step would not be representative of the optimization trajectory, and the one-step lookahead would provide noisy and unreliable signal for updating µ. Our empirical results suggest that for the LLM fine-tuning scenarios we study, these conditions are adequately met to allow for stable meta-optimization. Limitation I: Inherent Myopia and Long-Term Credit Assignment. direct consequence of the one-step approximation is its inherent myopia. The meta-gradient is calculated based on the immediate, next-step impact on the validation utility (θ). It cannot capture complex, long-term dependencies where particular choice for µ might lead to temporary dip in validation performance but unlock more promising optimization trajectory several steps later. This represents classic trade-off between computational feasibility and optimization foresight. While full unrolling of the optimization history would provide more accurate gradient for µ, its computational and memory costs are prohibitive. AMFTs design acknowledges this trade-off, using the efficient one-step approximation to provide principled, forward-looking signal that, while not perfectly prescient, is significant advance over the purely reactive signals used in prior work (e.g., current-step entropy or reward density). Limitation II: Dependence on the Validation Set. The quality and representativeness of the validation set, Dval, are critical to the success of the meta-learning controller. The meta-gradient µU (θt) is computed with respect to performance on this set, meaning the learned schedule for µ will be optimized to produce model that excels specifically on data distributed similarly to Dval. If the validation set is small, noisy, or poorly aligned with the final test distribution, the controller may learn suboptimal or even detrimental schedule for µ. This highlights the importance of curating high-quality, representative validation set, prerequisite shared by many meta-learning and hyperparameter optimization techniques. Limitation III: Potential for Instability and the Role of the Entropy Heuristic. The metagradient calculation involves second-order information (as seen in the Hessian-vector products implicitly computed when differentiating through gradient step). In optimization landscapes that are not sufficiently smooth, these higher-order derivatives can be noisy, potentially introducing instability into the updates for µ. This is primary motivation for the hybrid nature of our adaptive weight controller. The short-term, reactive entropy heuristic (ηH (H H(πθt)) in Eq. 6) acts as crucial regularizer and stabilizer. It provides fast-acting, robust signal based on the immediate stability of the policy (its entropy), complementing the long-term, forward-looking (but potentially noisy) meta-gradient. Our ablation study in Table 4 of the main paper, which shows significant performance drop when removing this heuristic (w/o Entropy Heuristic), empirically validates its critical role in ensuring stable and effective learning process. Despite these limitations, the one-step meta-gradient provides principled, forward-looking optimization signal for the imitation-exploration balance, representing significant advance over the purely reactive, heuristic-based mechanisms employed in prior adaptive frameworks."
        },
        {
            "title": "B AMFT Algorithm Implementation Details",
            "content": "This section provides detailed, step-by-step description of the AMFT algorithm, designed to supplement the high-level overview in the main paper and facilitate replication. Our goal is to offer transparent and comprehensive guide to the implementation of the core mechanisms, including the SFT warm-up, the main adaptive training loop, and the meta-gradient-based weight controller. B.1 Fully Annotated Pseudocode Algorithm 2 presents the complete, annotated pseudocode for Adaptive Meta Fine-Tuning (AMFT). To offer clear and in-depth understanding of its operational flow, we first elaborate on the rationale behind its three-phase structure. This design is intentionally crafted to address the well-documented instabilities of pure RL and the catastrophic forgetting issues of sequential SFTRL pipelines [3, 9]. 16 Algorithm 2 The AMFT Algorithm (Detailed Version) Require: Pretrained model policy πθ with parameters θ. Require: Value function Vϕ with parameters ϕ. Require: SFT demonstration dataset DSFT. Require: Validation dataset for meta-objective Dval. Require: Environment env with reward function Rexplicit. Require: Hyperparameters: SFT warm-up steps , total training steps , initial weight µinit, controller learning rates ηµ, ηH , policy learning rate αθ, value function learning rate αϕ, metaupdate frequency K, weight clip range [µmin, µmax]. Ensure: Fine-tuned model policy π θ . 1: Initialization: 2: Initialize policy parameters θ0 from pretrained model. 3: Initialize value function parameters ϕ0. 4: Initialize adaptive weight µ0 µinit. 5: // Phase 1: SFT Warm-up 6: for = 1 to do 7: 8: 9: 10: end for 11: Compute target entropy meanxDSFTH(πθW (x)). {Set target entropy based on post-SFT Sample batch {(xi, yi)}m Compute SFT loss LSFT(θw1) using Eq. 2 (main paper). Update policy parameters: θw θw1 αθθLSFT(θw1). i=1 DSFT. policy.} 12: Initialize main loop policy θ0 θW . 13: // Phase 2: Main Adaptive Training Loop 14: for = 0 to 1 do 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: // Data Collection Sample batch of demonstrations {(xi, yi)}m Generate batch of on-policy rollouts {τj}n // Loss Computation Compute LSFT(θt) on the SFT batch. Compute rewards, advantages, and LRL(θt) on the RL rollouts. // Adaptive Weight Controller Update gµ 0. if (mod K) == 0 then i=1 DSFT. j=1 πθt in env. Compute meta-gradient gµ µU (θt) on validation batch. {Meta-gradient is updated periodically. See Appendix B.2 for the full derivation.} end if Compute policy entropy Ht meanxDSFTH(πθt(x)). Compute entropy heuristic gH Ht. Update adaptive weight using Eq. 6 (main paper): µt+1 clip(µt + ηµgµ + ηH gH , µmin, µmax). // Policy and Value Function Update Compute the unified loss using Eq. 3 (main paper): Ltotal(θt; µt+1) (1 µt+1) LRL(θt) + µt+1 LSFT(θt). Update policy parameters: θt+1 θt αθθLtotal(θt; µt+1). Update value function parameters ϕt+1 using its own loss (e.g., MSE on returns from rollouts). 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: end for 36: return πθT SFT Warm-up. The algorithm begins with mandatory, albeit brief, warm-up phase consisting solely of Supervised Fine-Tuning. This stage is critical for two primary reasons. First, it serves as \"format teacher\" [46], aligning the base models outputs with the required structural conventions (e.g., reasoning chains, answer formats). This initial alignment is essential for the subsequent RL phase, as it ensures that the policy can generate syntactically valid trajectories from which meaningful reward signal can be extracted. This directly mitigates the \"advantage collapse\" problem, where chaotic initial policy fails to produce any reward-bearing outputs, leading to null learning signal [20]. Second, this phase establishes stable and competent policy whose average token entropy can 17 serve as reliable target, , for the entropy-based heuristic in our adaptive weight controller. This provides data-driven anchor for what constitutes \"stable\" level of policy uncertainty. Main Adaptive Training Loop. Following the warm-up, AMFT transitions to its core single-stage training loop. Unlike sequential methods that create hard switch between objectives, this loop continuously integrates both imitation and exploration signals. At each step, the algorithm processes mixed batch of data, comprising both expert demonstrations from DSFT and on-policy rollouts generated by the current policy. The central innovation lies in how these two signals are balanced: the adaptive weight controller (lines 8-11 in the pseudocode) updates the balancing parameter µ before it is used to compute the unified loss for the policy update. This ensures that every gradient step taken by the policy is guided by the most current, principled assessment of the optimal imitationexploration trade-off. The controller itself synergizes forward-looking meta-gradient with reactive entropy-based regularizer, mechanism designed to pursue long-term performance while maintaining short-term stability. This stands in contrast to prior heuristic-based methods that rely solely on reactive signals like reward density or gradient norms [22, 3]. B.2 Meta-Gradient Approximation for the Adaptive Weight Controller central innovation of AMFT is its ability to learn the optimal SFT-RL balance rather than relying on heuristics. This is achieved by treating the balancing weight µ as learnable parameter, which is updated via meta-learning. This section provides detailed derivation of the one-step meta-gradient approximation used in our adaptive weight controller, as first introduced by [10]. Problem Formulation: Bilevel Optimization. The core task is bilevel optimization problem. In the inner loop, we update the policy parameters θ to minimize the unified loss Ltotal for given weight µ. In the outer loop, we aim to update µ to maximize long-term utility function (θ) evaluated on separate validation set. Let θt1 be the policy parameters before an update. The inner-loop update for one step is: θt(µt) = θt1 αθθLtotal(θt1; µt) (20) where we explicitly denote θt as function of µt to highlight the dependency. The outer-loop objective is the validation performance: (θt) = E(x,τ )πθt (Dval)[Rexplicit(τ )] Our goal is to compute the meta-gradient µU (θt), which tells us how to adjust µt to maximize this long-term utility. (21) The Meta-Gradient via the Chain Rule. Using the chain rule, the gradient of the outer-loop objective with respect to µt is: µU (θt) = θU (θt) (cid:125) (cid:123)(cid:122) (cid:124) Outer Gradient θt µt (cid:124)(cid:123)(cid:122)(cid:125) Jacobian (22) This equation decomposes the meta-gradient into two components: The Outer Gradient (θU (θt)): This is the gradient of the validation utility with respect to the updated model parameters θt. It indicates the direction in parameter space that improves validation performance. The Jacobian ( θt µt ): This term captures how the model parameters θt change in response to an infinitesimal change in the balancing weight µt. Computing the full Jacobian is computationally prohibitive, as it requires unrolling the entire training history to account for how µt influences all subsequent parameter updates. The One-Step Approximation. To make this computation tractable, we adopt widely-used onestep approximation from the meta-learning literature [10]. We approximate the updated parameters 18 θt using only the single, most recent gradient descent step from Eq. 20. We can then differentiate this one-step update rule with respect to µt: θt µt µt (θt1 αθθLtotal(θt1; µt)) = αθ µt θLtotal(θt1; µt) (23) Since θt1 does not depend on the current weight µt, its derivative is zero. Now, we substitute the definition of Ltotal = (1 µt)LRL + µtLSFT: θt µt αθ µt θ [(1 µt)LRL(θt1) + µtLSFT(θt1)] = αθθ µt [(1 µt)LRL(θt1) + µtLSFT(θt1)] = αθθ [LRL(θt1) + LSFT(θt1)] = αθ (θLSFT(θt1) θLRL(θt1)) (24) Here, we can swap the order of differentiation because the losses LRL and LSFT are evaluated at θt1, which is constant with respect to µt. The Final Meta-Gradient Formula and its Intuition. By substituting this tractable approximation of the Jacobian back into the chain rule (Eq. 22), we obtain the final formula for the meta-gradient: µU (θt) αθθU (θt) (θLSFT(θt1) θLRL(θt1)) (25) This final expression is computationally efficient, requiring only three gradient calculations: one on validation batch for the outer gradient, and two on the training batch for the SFT and RL gradients (which are already computed for the main policy update). The intuition behind this formula is powerful. The term (θLSFT θLRL) represents the \"disagreement vector\" in the parameter space between the imitation and exploration objectives. The meta-gradient is the projection of the validation performance gradient (θU ) onto this disagreement vector. If increasing µ (i.e., moving more towards the SFT gradient direction) would result in new parameters θt that better align with the direction of long-term improvement θU (θt), the meta-gradient will be positive. This will increase µ in the next step, favoring SFT. Conversely, if moving more towards the RL direction (decreasing µ) better aligns with long-term improvement, the meta-gradient will be negative, thus decreasing µ and favoring RL. This mechanism is explicitly forward-looking: the decision on how to balance SFT and RL at step is based on its approximated effect on performance at future step. This allows AMFT to learn principled training curriculum that directly optimizes for the final task objective, moving beyond the reactive, proxy-based heuristics of prior work."
        },
        {
            "title": "C Experimental Setup",
            "content": "This section provides comprehensive overview of the experimental configurations used to evaluate AMFT and all baseline methods. Our goal is to ensure full reproducibility by detailing the datasets, models, hyperparameters, and evaluation protocols. C.1 Dataset Details Our evaluation spans three distinct reasoning domains: mathematical reasoning, general reasoning (for out-of-distribution testing), and visual reasoning. The selection of these datasets is intended to provide rigorous and multi-faceted assessment of each fine-tuning paradigms ability to foster both specialized competence and broad generalization. 19 Mathematical Reasoning Datasets (In-Distribution). For fine-tuning and in-distribution evaluation of mathematical reasoning, we use combination of primary training dataset and five standard evaluation benchmarks. Training Dataset (OpenR1-Math-46k-8192): As stated in the main paper, all math-focused fine-tuning originates from this dataset [39]. It consists of 46,000 mathematical problems with high-quality, step-by-step reasoning solutions (CoT) generated by the DeepSeek-R1 model. This dataset serves dual purpose in our framework: its problem statements are used as prompts for RL rollouts, and its detailed solutions are used as expert demonstrations for the SFT objective. Evaluation Benchmarks: We evaluate performance on five challenging, competition-level mathematics benchmarks to measure in-domain reasoning capabilities: AIME24 and AMC [19] for competitive math, MATH500 [16] and Minerva [18] for broad mathematical problem-solving, and OlympiadBench [15] for problems requiring exceptional insight. General Reasoning Datasets (Out-of-Distribution). To assess how well mathematical reasoning skills generalize to other knowledge-intensive domains, we evaluate all models on three OOD benchmarks. These tasks require reasoning but do not fall into the domain of pure mathematics, thus testing for catastrophic forgetting of general knowledge. ARC-C [6]: The AI2 Reasoning Challenge (Challenge set), consisting of difficult, grade-schoollevel science questions that require commonsense and scientific reasoning. GPQA-D [30]: The \"Diamond\" subset of the Graduate-Level Google-Proof Q&A benchmark, containing expert-level questions in biology, physics, and chemistry designed to be resistant to simple web searches. MMLU-Pro [36]: more robust and challenging version of the Massive Multitask Language Understanding benchmark, covering 57 diverse subjects and designed to test deep knowledge and its application. Visual Reasoning Datasets and Generalization Splits. For visual reasoning, we use the General Points and V-IRL benchmarks, which are specifically designed to test generalization across both textual rule changes and visual variations. We follow the experimental design of [4] to define our ID and OOD splits. General Points: An arithmetic reasoning task where the model is presented with four playing cards (either as text or an image) and must generate mathematical expression that equals target number (24). ID vs. OOD (Rule Variation): This split tests the models ability to apply arithmetic principles under changing rules. * In-Distribution (ID): Models are trained and evaluated using the rule where face cards J, Q, and all count as the number 10. * Out-of-Distribution (OOD): Models are evaluated on an unseen rule where J, Q, and are interpreted as 11, 12, and 13, respectively. This forces the model to generalize its arithmetic operations rather than memorize specific number combinations. ID vs. OOD (Visual Variation): This split tests the models visual recognition capabilities, specifically its invariance to cosmetic features like color. * In-Distribution (ID): Models are trained exclusively on images of cards with black suits (spades and clubs ). * Out-of-Distribution (OOD): Models are evaluated on images of cards with red suits (hearts and diamonds ). V-IRL (Vision-Language Navigation): spatial reasoning task where the model must navigate real-world environment based on visual observations and textual instructions. ID vs. OOD (Rule Variation): This split tests whether the model learns abstract navigational concepts or memorizes specific action vocabulary. * In-Distribution (ID): Models are trained using an absolute orientation action space (e.g., turn_direction(north), turn_direction(west)) . * Out-of-Distribution (OOD): Models are evaluated using relative orientation action space (e.g., turn_direction(left), turn_direction(slightly right)). 20 Figure 4: General Points: An example of the sequential revision formulation with verifier. The illustration is from [4]. ID vs. OOD (Visual Variation): This split tests the models ability to generalize its spatial reasoning and landmark recognition to novel environments. * In-Distribution (ID): Models are trained on navigation routes collected exclusively from New York City. * Out-of-Distribution (OOD): Models are evaluated on the VLN mini benchmark, which contains routes from various other cities worldwide, such as Milan, London, and Hong Kong. Figure 5: V-IRL: Demonstration of one navigation task. The navigation procedure is shown at the top, with the navigation instructions displayed below. Visual observation-related information is highlighted in green, while action-related information is marked in orange. The illustration is from [4]. 21 C.2 Model Details The selection of base models is critical component of our experimental design, chosen to rigorously test the AMFT frameworks effectiveness across both specialized and general-purpose reasoning domains. We provide detailed descriptions of the models used for mathematical and visual reasoning tasks below. Our choice of these specific, publicly-available models ensures that our results are transparent and reproducible. Qwen2.5-Math-7B for Mathematical Reasoning. For all mathematical reasoning experiments, we use Qwen2.5-Math-7B as the base model [40]. This model is part of the Qwen2.5 series developed by Alibaba and has been specifically optimized for mathematical tasks through continued pre-training on extensive math-related corpora [39, 7]. Its strong foundational capabilities in arithmetic, algebra, and logic make it an ideal and challenging baseline for evaluating advanced fine-tuning methods. By starting with model that already possesses strong innate reasoning abilities, we can more accurately assess the additional value and sample efficiency provided by our AMFT paradigm compared to other state-of-the-art fine-tuning techniques. LLaMA-3.2-Vision-11B for Visual Reasoning. For the multi-modal reasoning tasksGeneral Points and V-IRLwe employ LLaMA-3.2-Vision-11B [1]. This model is state-of-the-art, opensource Vision-Language Model (VLM) developed by Meta. It is known for its robust visual understanding and strong instruction-following capabilities, which are essential prerequisites for tackling complex, multi-step visual reasoning problems [4]. The models 11-billion-parameter scale provides sufficient capacity for the nuanced demands of both spatial navigation (V-IRL) and symbolic visual reasoning (General Points). Using this powerful, general-purpose VLM allows us to test AMFTs ability to resolve the \"SFT Memorizes, RL Generalizes\" dilemma in multi-modal context where both visual perception and logical deduction are intertwined. C.3 Baseline Implementation Details To rigorously evaluate the performance of AMFT, we established comprehensive suite of baseline methods. This section provides detailed description of the implementation and hyperparameter configurations for each baseline. Our goal is to ensure fair and transparent comparison by not only listing the parameters but also explaining the rationale behind their selection, grounding our choices in established best practices from the cited literature. All experiments were conducted using the same underlying computational framework and base models as AMFT to isolate the effects of the training paradigm itself. SFT-only. This baseline represents the standard supervised fine-tuning paradigm. It serves to quantify the effectiveness of pure imitation learning on the reasoning datasets and acts as the foundational first stage for the sequential SFTRL pipeline. Objective and Rationale: The model is trained exclusively by minimizing the standard crossentropy loss (negative log-likelihood) on the high-quality demonstration data from the respective training sets (DSFT). The objective is to directly imitate the expert policy (πdemo) encoded in the dataset. This approach is foundational for teaching the model domain-specific knowledge and, critically, the structural and stylistic patterns of the desired reasoning format [46]. As observed by [4], this initial format teaching is often prerequisite for successful RL. Implementation Details: We conducted full-parameter fine-tuning to provide the model with maximum flexibility to adapt to the complex reasoning structures present in the data. The training was run for 3 epochs, standard duration found in related works to achieve good balance between sufficient exposure to the data and the risk of overfitting on large-scale instruction datasets [39]. Hyperparameter Configuration: The chosen hyperparameters reflect common practices for effective SFT. Learning Rate (5 105): This is conventional, relatively high learning rate for SFT, designed for rapid adaptation of large pre-trained model to new data distribution. This contrasts sharply with the much lower learning rates required for stable RL, key difference between the optimization dynamics of the two paradigms [29]. 22 Scheduler (Cosine Annealing): We used cosine learning rate scheduler with warm-up ratio of 0.1. This allows for stable initial updates, followed by smooth decay of the learning rate, which has been empirically shown to improve convergence and lead to more robust final models. Batch Size (128): This value was chosen as trade-off between gradient estimation accuracy (which improves with larger batch sizes) and the memory constraints of the available hardware. RL-only (GRPO). This baseline is designed to isolate the effect of reinforcement learning by applying it directly to the base model, reflecting the \"RL from scratch\" or \"zero RL\" paradigm [43]. It is critical baseline for assessing whether RL can instill reasoning abilities without supervised warm-up. Objective and Rationale: We employ Group Relative Policy Optimization (GRPO), state-of-theart policy gradient algorithm for RLVR popularized by [32]. GRPO is particularly well-suited for reasoning tasks as it computes advantages by normalizing rewards within group of self-generated trajectories for given prompt. This eliminates the need for separate, learned critic network, reducing algorithmic complexity and computational overhead. The policy is optimized solely based on the explicit, outcome-based reward signal Rexplicit. Implementation Details: The model learns exclusively from on-policy rollouts. At each step, 8 candidate trajectories are generated per prompt to form the group for advantage calculation. The model is trained for fixed 500 optimization steps to maintain consistent computational budget across all RL-based comparisons. Hyperparameter Configuration: The RL hyperparameters are chosen with strong emphasis on training stability. Learning Rate (1 106): very low learning rate is crucial for the stability of on-policy RL algorithms like GRPO. Higher rates can cause the policy to change too drastically between updates, which violates the assumptions of importance sampling and leads to trust region collapse and divergent training [29]. KL Coefficient (β = 0.0): While traditional RLHF uses KL penalty to prevent the policy from deviating from the reference model, recent works on complex reasoning have found it can be overly restrictive [39]. Forcing the model to stay close to base policy that cannot produce long, coherent reasoning chains can stifle the very exploration needed to acquire this skill. We therefore follow this modern practice and omit the KL penalty. Discount Factor (γ = 1.0): For episodic reasoning tasks with sparse final reward, there is no need to discount future rewards. value of 1.0 ensures that the credit for correct final answer is fully and equally propagated back to all steps in the reasoning trajectory that produced it. Sequential SFTRL. This baseline represents the conventional two-stage fine-tuning pipeline, widely adopted de facto standard for aligning powerful LLMs. It serves to benchmark the benefits and drawbacks of sequential training, particularly the issue of catastrophic forgetting that AMFT aims to solve. Objective and Rationale: The two-stage process is designed to leverage the distinct strengths of SFT and RL sequentially. Stage 1 (SFT) provides strong initialization by teaching the model the required reasoning format and aligning it with distribution of high-quality solutions. This pre-conditioning is vital for making the subsequent RL stage tractable, as it provides policy already capable of producing some reward-generating trajectories, thus avoiding the severe sample inefficiency and potential collapse of starting RL from naive policy [4]. Stage 2 (RL) then refines this policy, exploring variations and optimizing for correctness beyond what is present in the static SFT dataset. Implementation Details: For maximum fairness and to isolate the effect of the sequential paradigm itself, our implementation is direct composition of the two preceding baselines. 1. We first perform the complete SFT-only training and select the checkpoint with the highest validation performance. 2. We then initialize the RL phase from this best SFT checkpoint, using the exact same hyperparameters and 500-step training budget as the RL-only baseline. 23 This controlled setup ensures that any performance difference between this baseline and AMFT can be directly attributed to the training paradigm (sequential vs. unified) rather than to differences in initialization or optimization settings. Other State-of-the-Art Hybrid Methods. To ensure that AMFTs performance is benchmarked against the current state-of-the-art, we implemented several leading single-stage hybrid frameworks. For each method, we adhered as closely as possible to the methodologies and critical hyperparameter settings described in their original publications. This approach guarantees that our baselines are not just strawman implementations but are faithful, strong representations of these advanced techniques. LUFFY [39]: This method enhances on-policy RLVR with off-policy guidance from stronger models. Core Mechanism: LUFFY augments the on-policy rollout batch with high-quality, off-policy expert demonstrations and uses Mixed-Policy GRPO objective to balance imitation and exploration. Implementation Details: In our implementation, for every batch of 8 rollouts per prompt, we used 7 on-policy rollouts generated by the current policy and 1 off-policy expert trace from the DSFT dataset, as recommended by the authors . Key Hyperparameters: * The off-policy guidance is integrated by setting the behavior policy probability πϕ = 1 for computational efficiency, which avoids tokenization mismatches and the need to re-compute probabilities for the expert model . * The PPO-clip operation was omitted for the off-policy objective, as the standard clipping becomes imbalanced when πϕ = 1 . * The policy shaping function (x) = x/(x + γ) was applied to the off-policy importance sampling ratio, with the hyperparameter γ set to 0.1, the optimal value found in their ablation studies . ReLIFT [26]: This approach is characterized by its strategy of interleaving RL with targeted online fine-tuning. Core Mechanism: ReLIFT alternates between standard RL steps and SFT steps. Crucially, the SFT updates are performed specifically on expert demonstrations corresponding to the \"hardest questions\"those for which the policy failed to generate correct response during recent rollouts. Implementation Details: Our implementation followed schedule where after every 50 RL training steps, full SFT training step was performed on batch of samples collected from the failure cases of the preceding RL phase. This cyclical process allows the model to continuously patch knowledge gaps identified during exploration. TAPO [37]: This framework enhances RL by incorporating high-level, structured guidance in the form of \"thought patterns.\" Core Mechanism: TAPO abstracts reasoning strategies (thought patterns) from successful demonstrations and adaptively integrates this external knowledge into the GRPO framework. This provides higher-level form of guidance than raw token-level imitation. Implementation Details: We implemented simplified version of this principle by first extracting structured reasoning steps from the expert trajectories in DSFT. During RL rollouts, these abstracted patterns were prepended to the prompt as form of structured guidance to steer the models exploration process. SRFT [11]: This method proposes single-stage, unified loss function that balances SFT and RL signals using policy entropy as dynamic indicator. Core Mechanism: SRFT does not switch between SFT and RL but rather combines their losses in every step, with weights that are dynamically adjusted based on the current policys entropy, H(πθ). Implementation Details: We implemented the unified loss function as specified in their work. For each batch, we mixed samples from DSFT and on-policy rollouts. Key Hyperparameters: The weights for the different loss components were calculated dynamically at each step according to the authors formulations: 24 * For SFT loss on demonstration data: wSFT = 0.5 exp(H(πθ)) . This gives more weight to imitation when the policy is uncertain (high entropy). * For RL loss on positive-reward self-generated samples: wRL = 0.1 exp(H(πθ)) . This encourages exploration when the policy becomes too deterministic (low entropy). By meticulously implementing these diverse and powerful baselines according to their original designs, we provide robust and challenging context in which to evaluate the unique contributions and superior performance of our AMFT framework. C.4 AMFT Hyperparameter Settings This section provides comprehensive specification of the hyperparameters used for all AMFT experiments and key baselines discussed in the main paper. Our goal is to ensure full reproducibility and provide clarity on the configurations that led to the reported results. The parameters were chosen based on combination of preliminary experiments, established best practices from the cited literature, and the specific requirements of our proposed algorithm. We have maintained consistent settings across all comparable methods to ensure fair and rigorous evaluation. General Training and Model Parameters. Table 6 details the general hyperparameters applied across all training paradigms, including SFT, RL, and our AMFT framework. These settings relate to the model architecture, optimizer, and learning schedule, and are aligned with established practices for training large reasoning models. Table 6: General training hyperparameters for all experiments. Hyperparameter Value and Rationale Base Models Optimizer AdamW β1 AdamW β2 AdamW ϵ Weight Decay Policy Learning Rate (αθ) Value Function Learning Rate (αϕ) Learning Rate Scheduler Warmup Ratio Total Training Steps (T ) SFT Warm-up Steps (W ) Global Batch Size Per-Device Batch Size Gradient Accumulation Steps Precision Max Sequence Length Qwen2.5-Math-7B (Math), LLaMA-3.2-Vision-11B (Visual) AdamW 0.9 0.95 1 108 0.1 1 106 5 106 Cosine annealing with warmup 0.1 500 (for RL-based methods) 50 128 4 4 bfloat16 8,192 AMFT Adaptive Weight Controller Parameters. The core novelty of AMFT lies in its metalearning controller, which introduces new set of hyperparameters. These parameters, detailed in Table 7, govern the behavior of the adaptive weight µ. They were tuned on small, held-out portion of the training data to find configuration that yields both stability and strong performance. The use of policy entropy as stabilizing signal is inspired by the analysis in related works, which identify entropy as crucial indicator of training effectiveness. 25 Table 7: Hyperparameters for the AMFT adaptive weight controller. Hyperparameter Value and Rationale Initial Weight (µinit) Weight Clip Range [µmin, µmax] Meta-Gradient Learning Rate (ηµ) Entropy Heuristic Learning Rate (ηH ) Meta-Update Frequency (K) Target Entropy (H ) 0.5 (Neutral starting point, balancing SFT and RL) [0.05, 0.95] (Prevents either objective from being fully ignored) 1 104 (Small rate for stable meta-updates) 5 104 (Allows faster reaction to policy instability) Every 20 steps (Balances cost and controller responsiveness) Data-driven; set to the average policy entropy after the SFT warm-up phase (see Algorithm 2) These detailed configurations, grounded in practices from leading contemporary research, provide transparent foundation for our experimental results and are intended to facilitate direct replication and further extension of our work by the research community. C.5 Computational Infrastructure This section provides the technical specifications of the computational environment utilized for all experiments presented in this paper. Our goal is to ensure full transparency and facilitate the reproducibility of our results by detailing the hardware, software, and core frameworks that underpinned our research. The described infrastructure was designed to be directly comparable to the high-performance environments used in state-of-the-art research, such as the SRFT study, and was consistently used for training and evaluating AMFT and all baseline models. Hardware Setup. All large-scale training and evaluation experiments were conducted on highperformance computing cluster designed for demanding LLM workloads. The specific hardware configuration was as follows: GPU Nodes: The primary computational workload was handled by cluster of 8 nodes, with each node equipped with 8 NVIDIA A100 80GB GPUs. This provided total of 64 A100 GPUs. Interconnect: The GPUs within each node were interconnected using high-bandwidth NVLink, and the nodes themselves were connected via 200 Gbps InfiniBand network. This setup is crucial for ensuring efficient communication and synchronization during large-scale, multi-node distributed training. Software Environment. To maintain consistent and reproducible software stack, all experiments were run within containerized environment. The key software components and their versions are listed below: Operating System: Ubuntu 22.04.2 LTS (within Docker container). NVIDIA Stack: CUDA Version 12.2, cuDNN 8.9, and NVIDIA Driver Version 535.104.05. Core Libraries: PyTorch 2.2.1 Transformers 4.41.2 TRL 0.8.6 Accelerate 0.29.3 DeepSpeed 0.14.2 vLLM 0.4.1 (for efficient RL rollout generation and evaluation) Training Framework and Orchestration. Our experimental pipeline was built on top of established open-source frameworks to ensure robustness and scalability, directly following the toolchain mentioned in our primary reference study. Primary Framework: The implementation of our AMFT algorithm and all RL-based baselines was built upon the verl framework. This choice was made to align our methodology with the SRFT study, ensuring that differences in performance can be attributed to the algorithmic innovations rather than framework-specific optimizations. 26 Distributed Training: Multi-GPU and multi-node training across the 64-GPU cluster was orchestrated using Hugging Face Accelerate in conjunction with DeepSpeed, configured with the ZeRO Stage 3 optimization to efficiently manage memory and scale training. Rollout and Evaluation Engine: To maximize sample efficiency during the reinforcement learning phase and ensure fast evaluation, on-policy rollouts and final evaluations were conducted using the highly optimized vLLM inference server. This practice is consistent with several state-of-the-art RLVR frameworks for its speed and efficient memory management."
        },
        {
            "title": "D Additional Experimental Results and Analysis",
            "content": "This section provides deeper, more granular evidence to support the claims made in the main paper. We extend our analysis of training dynamics to the visual reasoning domains and present qualitative case studies that offer concrete examples of AMFTs superior reasoning capabilities compared to baseline methods. D.1 Training Dynamics Visualizations for All Domains The main paper (Figure 2) illustrates the learning dynamics of AMFT compared to baselines on mathematical reasoning tasks. To demonstrate the cross-modal robustness and consistency of our meta-learning controller, this section provides equivalent visualizations for our two visual reasoning benchmarks: General Points and V-IRL Navigation. These plots track both task performance and the trajectory of the adaptive weight µ over the course of training, offering clear window into how AMFT autonomously learns an effective training curriculum in multi-modal settings. General Points (Visual Arithmetic Reasoning). The General Points task presents dual challenge: it requires not only accurate visual recognition of card values from an image but also robust symbolic reasoning to construct valid mathematical expression. Figure 6 visualizes how different fine-tuning paradigms navigate this complex, multi-modal problem space. AMFTs Learned Curriculum. The trajectory of AMFT (solid blue line) exemplifies the effectiveness of its forward-looking optimization. The adaptive weight µ (red dash-dotted line) orchestrates clear, data-driven training curriculum. In the initial phase (approx. 0-150 steps), with µ held at high value ( 0.9), training is dominated by the SFT objective. This forces the model to rapidly master the tasks rigid output format and imitate fundamental arithmetic patterns from expert data. This imitation-first approach provides crucial scaffold that prevents the chaotic, low-reward exploration that often plagues pure RL in its early stages, directly mitigating the risk of policy collapse. As training progresses, the meta-controller, observing consistent performance gains on the validation set, systematically reduces the weight of µ. This transition signifies that the model has acquired competent base policy, allowing the optimization focus to safely shift from imitation towards exploration. In this RL-dominant phase, the model moves beyond merely replicating seen solutions. It begins to refine its strategies, learn from its own mistakes through the explicit reward signal, and discover novel solution paths not present in the static SFT dataset. This ability to explore and generalize is what ultimately allows AMFT to break through the performance plateaus observed in imitation-heavy baselines. Baseline Performance Analysis. The limitations of simpler paradigms are clearly illustrated. The SFT-only model (dot-dashed green line) learns the task format quickly but its performance saturates at low level, classic exhibition of the \"SFT memorizes, RL generalizes\" dilemma. It successfully imitates the training datas style but fails to learn the underlying, generalizable arithmetic principles. The RL-only baseline (dotted orange line) confirms the challenge of exploration without guidance; it learns slowly and exhibits high variance, showcasing the severe sample inefficiency of pure exploration in complex, sparse-reward environment. The sequential SFTRL approach (dashed purple line) is strong baseline, leveraging the SFT initialization to achieve respectable performance. However, it is ultimately surpassed by AMFT, suggesting that the hard switch between training stages is suboptimal compared to AMFTs continuous, adaptive fusion of the two complementary learning signals, which better mitigates catastrophic forgetting. 27 Figure 6: Learning dynamics on the General Points benchmark. The left y-axis shows the ID Win Rate (%), while the right y-axis shows the adaptive weight µ. AMFT (solid blue) demonstrates superior and more stable learning trajectory by dynamically adjusting µ (red dash-dotted) to learn an optimal curriculum, starting with SFT-dominance (high µ) and smoothly transitioning to RLdominance (low µ). V-IRL (Vision-Language Navigation). similar, compelling pattern emerges in the V-IRL navigation task, domain characterized by sequential decision-making, spatial reasoning, and the critical need to ground textual instructions in visual observations. As shown in Figure 7, AMFT once again achieves the highest final success rate with the most stable learning trajectory, confirming its effectiveness in distinct reasoning domain. In this task, the SFT-only baseline AMFTs Adaptive Strategy in High-Baseline Scenario. performs strongly out of the gate, as the highly structured nature of navigation instructions (e.g., \"turn left,\" \"walk forward\") is well-suited to imitation learning. However, its performance quickly plateaus, as it struggles to adapt to novel visual scenes or slightly ambiguous instructions not perfectly represented in the training data. AMFT, by contrast, leverages this strong initial performance. Its controller begins with high SFT weight (µ) to efficiently absorb this foundational knowledge, matching the initial SFT trajectory. Then, mirroring its behavior on General Points, the meta-controller autonomously and gradually reduces µ, increasing the influence of the explicit RL reward. This transition is critical: it allows the model to move beyond imitating static paths and start learning true, generalizable navigation policy. Through RL-driven exploration, the model refines its ability to ground textual commands (e.g., \"The Dutch on your right\") to specific visual landmarks in dynamic environments, correcting its own errors and ultimately pushing its performance beyond the imitation-based ceiling of the SFT and SFTRL baselines. The trajectory of µ is again clear testament to the controllers ability to learn an effective curriculum, moving from phase of imitation to one of exploration and refinement. Contrasting with Baselines. The performance of the baselines reinforces the narrative. While RL-only eventually learns decent policy, its initial exploration is far less efficient than simply learning from the SFT data first. The SFTRL pipeline is again strong contender but its rigid, twostage nature proves less effective than AMFTs dynamic balancing act. The smooth and principled transition orchestrated by AMFTs meta-controller avoids the potential instabilities of hard switch and leads to more robust and higher-performing final agent. 28 Figure 7: Learning dynamics on the V-IRL Navigation benchmark. The left y-axis shows the ID Success Rate (%), and the right y-axis shows the adaptive weight µ. AMFTs learned curriculum, visualized by the red dash-dotted µ curve, enables it to build upon strong SFT foundation and leverage RL to achieve the highest final performance, surpassing all baselines. Collectively, these visualizations across two distinct multi-modal domains provide strong evidence for the general effectiveness and robustness of the AMFT framework. The meta-controller consistently learns sensible curriculumstarting with imitation to build stable foundation and progressively shifting to exploration to achieve superior performancewithout any task-specific manual tuning. This demonstrates that our forward-looking, meta-learning approach provides more principled and effective solution for balancing the SFT-RL trade-off than the reactive heuristics used in prior work. D.2 Qualitative Analysis: Case Studies While quantitative metrics provide high-level summary of performance, qualitative analysis of model outputs offers invaluable insight into the behavioral differences instilled by each fine-tuning paradigm. This section presents case studies from our three reasoning domains to provide concrete examples of the phenomena discussed in the main paper, such as SFTs brittleness, RLs instability, and AMFTs balanced and effective reasoning. Case Study 1: Mathematical Reasoning. This case study uses moderately complex algebra problem. The goal is to assess whether the model has learned the underlying algebraic principles or has simply memorized solution templates. Problem Statement : Let (x) = x2 + 6x + 7. Find the vertex of the parabola. Your answer should be in the form (h, k). Correct Reasoning and Answer: The vertex of parabola (x) = ax2 + bx + is at = b/(2a). Here, = 1, = 6. So, = 6/(2 1) = 3. To find k, we evaluate (h): = (3) = (3)2 + 6(3) + 7 = 9 18 + 7 = 2. The vertex is (-3, -2). Figure 8: Problem statement for the mathematical reasoning case study. 29 Table 8: Model outputs for the mathematical reasoning case study. SFT-only Output RL-only (from scratch) Output AMFT (ours) Output To find the vertex, we complete the square. The function is (x) = x2 + 6x + 7. We rewrite this as (x) = (x2 + 6x + 9) 9 + 7, which simplifies to (x) = (x + 3)2 + 2. From the vertex form (x) = a(x h)2 + k, the vertex is (h, k). So the vertex is (3, 2). Analysis: Exhibits brittle memorization.The model correctly recalls the \"completing the square\" method. However, it makes critical sign error when mapping (x + 3)2 to the vertex form (x h)2. This demonstrates pattern imitation without deep understanding of the underlying principles. vertex is The vertex is the point where... point = 1, = 6, the... So 3. = Then calculate x... x... f(x)... (-3, 7). The vertex is Analysis: Shows clear signs of policy collapse. The model fails to produce coherent reasoning chain, getting stuck in repetitive and incomplete phrases. To find the vertex (h, k) of (x) = ax2 + bx + c, we can use = b/(2a). In this case,a = 1 and = 6. So = 3. = (3) = (3)2 + 6(3) + 7 = 9 18 + 7 = 2. Therefore, the vertex of the parabola is (-3, -2). Analysis: Demonstrates robust and principled reasoning. The model correctly identifies and applies the standard vertex formula, more direct and less error-prone method. Each step is logically sound and mathematically correct. Case Study 2: Visual Arithmetic Reasoning (General Points). This case study assesses the models ability to ground its reasoning in visual input. The task requires both correct card recognition and subsequent arithmetic calculation. Problem Statement (General Points): You are observing these four cards in the image. Note that J, Q, and count as 10. Your goal is to output formula that evaluates to 24. Correct Reasoning and Answer: The cards are Queen of Spades (10), 8 of Hearts, 3 of Clubs, and 2 of Diamonds. The numbers are 10, 8, 3, 2. The correct solution is 10*3-8+2=24. Figure 9: Problem statement for the visual arithmetic reasoning case study. Table 9: Model outputs for the visual arithmetic reasoning case study. SFT-only Output RL-only (from scratch) Output AMFT (ours) Output {\"cards\": [\"Q\", \"8\", \"3\", \"2\"], \"number\": [10, 8, 3, 2], \"formula\": \"(10 + 2) * (8 / 4)\"} [\"Q\", {\"cards\": \"8\", \"K\", \"2\"], \"number\": [10, 8, 10, 2], \"formula\": \"10+8+10+2=30\"} {\"cards\": [\"Q\", \"8\", \"3\", \"2\"], \"number\": [10, 8, 3, 2], \"formula\": \"10*3-8+2=24\"} Exhibits pseudoAnalysis: thinking and poor grounding. The model correctly identifies the cards and their values. However, it hallucinates the number 4 in its formula. Analysis: Demonstrates both poor visual perception and policy collapse. The model incorrectly identifies the 3 of Clubs as King. It then proceeds to perform simple, incorrect addition. Analysis: Shows strong visual grounding and effective reasoning. The model correctly identifies all four cards. It then successfully explores the solution space to find valid, non-trivial formula that equals 24. 30 Case Study 3: Vision-Language Navigation (V-IRL): Instruction History: 1. First, turn left to face east. 2. Move forward until you reach the next intersection where Hotel 32One is on your right behind. 3. Turn right to face north. ... (remaining steps) Action History: A_1: Current State: The agent is at state O_4, observing an intersection via the 2x2 grid of street views. turn_direction(east), A_2: forward(), A_3: forward() The agent is facing east after the last turn, so \"right behind\" corresponds to the \"back\" view relative to its initial orientation, but from the current intersection, the key is simply to identify the landmark. Correct Reasoning and Answer: The agent has completed instruction 2 by reaching the intersection with \"Hotel 32One\" visible. The next step is to execute instruction 3. Action: turn_direction(north). Figure 10: Problem statement for the revised vision-language navigation case study, based on the provided high-resolution visual grid. Table 10: Model outputs for the revised vision-language navigation case study. SFT-only Output RL-only (from scratch) Output AMFT (ours) Output {\"current observation\": \"I see an intersection with several buildings.\", \"current instruction\": \"Turn right to face north.\", \"action\": \"stop()\"} {\"current observation\": \"I have reached the intersection and can see Hotel 32One in the right-side view.\", \"current instruction\": \"Move forward until you reach the next intersection where Hotel 32One is on your right behind.\", \"action\": \"forward()\"} Analysis: Exhibits critical failure in state tracking and logical transition. The model correctly performs visual perception, identifying \"Hotel 32One\". However, it becomes fixated on the current instruction (\"Move forward until...\") and incorrectly repeats the associated action (forward()), failing to recognize that the terminal condition of that instruction has been met. This demonstrates classic SFT pitfall: it has memorized the association between an instructions text and an action, but has not learned the underlying statemachine logic required for multistep plan execution. Analysis: Demonstrates severe lack of visual grounding and policy coherence. The model provides generic, ungrounded observation (\"several buildings\") and fails to identify the key landmark. Although it happens to guess the next instruction textually, its chosen action (stop()) is completely unrelated and nonsensical. This exemplifies policy collapse, where the absence of stable, formatfollowing policy from SFT leads to an inability to connect perception, planning, and action, resulting in random or useless behavior. {\"current observation\": \"I have arrived at the intersection. can confirm the presence of Hotel 32One in the right-side view of the visual grid. This fulfills the condition of the current instruction.\", \"current instruction\": \"Turn right to face north.\", \"action\": \"turn_direction(north)\"} Analysis: Displays accurate state tracking, robust visual grounding, and correct logical execution. The model first grounds its action in the visual evidence, explicitly confirming \"Hotel 32One\". Crucially, it then correctly deduces that this observation satisfies the condition of instruction 2. This triggers successful state transition, causing it to identify instruction 3 as the new goal and execute the correct corresponding action (turn_direction(north)). This showcases AMFTs ability to build sophisticated and reliable decision-making policy that seamlessly integrates perception, reasoning, and planning."
        },
        {
            "title": "E Further Studies on AMFT Controller",
            "content": "This section dissects the components of the AMFT adaptive weight controller to demonstrate that its specific design choices are crucial, well-justified, and robust. We conduct series of studies investigating the sensitivity of the models performance to the controllers key hyperparameters, including its learning rates, the frequency of meta-updates, and the target entropy setting. E.1 Sensitivity to Controller Hyperparameters (ηµ, ηH ) The AMFT controllers behavior is governed by two primary learning rates: the meta-gradient learning rate, ηµ, which controls the influence of the long-term, forward-looking validation signal, and the entropy heuristic learning rate, ηH , which dictates the strength of the short-term, reactive stability signal. This study investigates how the final model performance varies across different settings of these two parameters. The goal is to demonstrate that while the performance is sensitive to these values, there exists reasonably wide range where AMFT performs well, indicating that the method is robust and not prohibitively difficult to tune. Experimental Design. We conducted grid search over range of values for ηµ and ηH , centered around the optimal configuration used in our main experiments (ηµ = 1 104, ηH = 5 104). For each pair of hyperparameters, we ran full 500-step training process on the mathematical reasoning 32 task and evaluated the final checkpoint. Performance is reported as the average accuracy across the five in-distribution mathematical reasoning benchmarks (AIME24, AMC, MATH500, Minerva, and OlympiadBench). Results and Analysis. The results of our sensitivity analysis are presented in Table 11. The data reveals several key insights into the controllers dynamics: Table 11: Ablation study on the AMFT controller learning rates ηµ and ηH . Performance is reported as the average accuracy (%) on the five in-distribution mathematical reasoning benchmarks. The configuration used in the main paper is highlighted in bold. Meta-Gradient Learning Rate (ηµ) 2 104 1 104 5"
        },
        {
            "title": "Entropy\nHeuristic\nLearning Rate",
            "content": "1 104 5 104 1 103 60.1 60.7 60.5 59.8 61.3 61.0 58.7 60.2 59.5 Existence of an Optimal Region: There is clear performance peak at (ηµ, ηH ) = (1 104, 5 104), which validates the hyperparameter choice for our main experiments. Importantly, the performance degrades gracefully around this peak rather than collapsing, with several neighboring configurations achieving strong results (e.g., > 60.0% accuracy). This demonstrates the robustness of the AMFT framework. Impact of Meta-Gradient Rate (ηµ): The influence of the long-term signal is critical. When ηµ is too low (5 105), the controller adapts too slowly, failing to fully capitalize on the forwardlooking signal to escape the suboptimal regions that heuristic-only methods might settle in. When ηµ is too high (2 104), the meta-updates to µ become too aggressive and potentially noisy, causing instability in the learned curriculum and slightly degrading final performance. Impact of Entropy Heuristic Rate (ηH ): The short-term stabilizer is equally important. When ηH is too low (1 104), the controller cannot react swiftly enough to policy entropy fluctuations. This makes the training less stable, especially if the meta-gradient is also high. Conversely, when ηH is too high (1 103), the controller becomes overly reactive to transient entropy changes. This excessive regulation can dampen the long-term signal from the meta-gradient, preventing the policy from engaging in the necessary exploration, thus leading to slightly suboptimal outcome. In conclusion, this ablation study confirms that the performance of AMFT is dependent on synergistic balance between its long-term, meta-learning objective and its short-term, stability-ensuring heuristic. The results show that while careful tuning is beneficial, the method is not overly sensitive to minor variations in its controllers learning rates, making it robust and practical approach for fine-tuning LLM reasoners. E.2 Impact of Meta-Update Frequency (K) The meta-gradient controllers update frequency, denoted by the hyperparameter K, is critical factor that balances the controllers responsiveness against the computational cost of training. The meta-gradient µU (θt) provides principled, forward-looking signal, but its computation requires additional forward and backward passes on validation set, making it more expensive than the main policy update. This ablation study investigates the impact of varying (the number of training steps between each meta-gradient update) on final model performance and overall training efficiency. Experimental Design. We trained several AMFT models from the same SFT warm-up checkpoint, varying only the meta-update frequency K. We tested values ranging from very frequent updates (K = 5) to very infrequent updates (K = 100). All other hyperparameters, including the controllers learning rates, were held constant at their optimal values as determined in Appendix E.1. Performance was measured by the average accuracy on the five in-distribution mathematical reasoning benchmarks after 500 training steps. We also report the total number of meta-gradient computations performed during training as proxy for the additional computational overhead. Results and Analysis. The results, presented in Table 12, clearly illustrate the trade-off between controller responsiveness and training efficiency. The data reveals three distinct operational regimes for the meta-controller based on the update frequency. Table 12: Ablation study on the meta-update frequency K. Performance is the average accuracy (%) on in-distribution math benchmarks. The configuration used in the main paper is highlighted. Update Freq. (K) Avg. Acc. (%) Total Updates Analysis and Observation 5 10 50 100 60.9 61.1 61.3 60. 59.3 100 50 25 10 High-Frequency: Performance is slightly single-batch metahindered by noisy, gradients. Near-Optimal: Strong performance with halved computational cost compared to K=5. Optimal Trade-off: Effectively balancing controller responsiveness with computational and gradient stability. Low-Frequency: The learned curriculum starts to lag behind the policys needs. Very Low-Frequency: The controller updates too infrequently, resulting in ineffective curriculum. High-Frequency Updates (K < 20): When the meta-gradient is computed very frequently (e.g., every 5 or 10 steps), the controller is highly responsive. However, this comes at significant computational cost, as indicated by the high number of total meta-updates. Furthermore, these frequent updates can introduce instability, as each meta-gradient is estimated from single, potentially noisy validation batch. This can cause the adaptive weight µ to oscillate unnecessarily, slightly hindering the model from settling into its optimal learning trajectory and resulting in final performance just below the peak. Optimal Trade-off Region (K 20): The best performance (61.3%) is achieved at = 20, the value used in our main experiments. At this frequency, the meta-updates are frequent enough to steer the training curriculum effectively, allowing µ to adapt to the policys evolving needs in timely manner. The interval is also long enough to average out some of the noise from single-batch gradient estimates and to significantly reduce computational overhead. This setting represents an empirically validated \"sweet spot\" that maximizes performance while maintaining practical training efficiency. Low-Frequency Updates (K > 20): As the update frequency decreases (K = 50 or = 100), the controller becomes progressively less responsive. The adaptive weight µ is updated too infrequently to keep pace with the policys rapid learning in the inner loop. The learned curriculum becomes \"stale,\" failing to provide the right balance of SFT and RL when the model needs it. This leads to graceful but clear degradation in final performance. In the limit of , the training would be equivalent to using fixed, manually-tuned µ, forfeiting the benefits of dynamic, forward-looking adaptation. In conclusion, this study validates our choice of = 20 as principled and effective compromise. It demonstrates that while the meta-learning component of AMFT is vital driver of performance, its benefits can be realized without incurring the prohibitive computational costs associated with overly frequent updates."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, Tsinghua University, Beijing, China"
    ]
}