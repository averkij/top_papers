{
    "paper_title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering",
    "authors": [
        "Guangtao Zeng",
        "Maohao Shen",
        "Delin Chen",
        "Zhenting Qi",
        "Subhro Das",
        "Dan Gutfreund",
        "David Cox",
        "Gregory Wornell",
        "Wei Lu",
        "Zhang-Wei Hong",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 0 6 3 2 . 5 0 5 2 : r Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering Guangtao Zeng1, Maohao Shen2, Delin Chen3, Zhenting Qi4, Subhro Das5, Dan Gutfreund5, David Cox5, Gregory Wornell2, Wei Lu1, Zhang-Wei Hong2,5, Chuang Gan3, 1Singapore University of Technology and Design 2Department of EECS, MIT 3UMass Amherst 4Harvard 5MIT-IBM Watson AI Lab, IBM Research guangtao_zeng@mymail.sutd.edu.sg {maohao,zwhong}@mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Benchespecially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), sampleefficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-BenchVerified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using few samples. Code, data, and models will be fully open-sourced2."
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) perform well on coding benchmarks like HumanEval [4] or LiveCodeBench [13] but struggle with real-world software engineering (SWE) tasks [15]. Unlike standardized coding problems, real issuessuch as GitHub issues [15]are often under-specified and require reasoning across multiple files and documentation. Even large models like Claude reach only around 60% accuracy on SWE-bench [15], despite using carefully engineered prompting pipelines [33]. Smaller models (under 100B parameters) perform significantly worse, typically scoring below 10% in zeroshot settings and plateauing around 30% after supervised fine-tuning (SFT) [34, 22] on GitHub Core contributors of Satori team, contributed equally to this work. 2https://github.com/satori-reasoning/Satori-SWE Preprint. Under review. issue datasets. Improving the performance of these models remains key challenge for practical deployment, where repeatedly querying large models is often too costly or inefficient. Recent and concurrent works to improve the performance of small LMs on SWE tasks have mainly focused on expanding SFT datasetseither through expert annotation or distillation from larger models [36, 34, 22]. These approaches show that performance improves as the quality and quantity of training data increase. However, collecting such data is both costly and time-consuming. An alternative is test-time scaling, which improves performance by generating multiple outputs at inference and selecting the best one using scoring function, such as reward model [5, 17]. While widely applied in math and logical reasoning [9, 28], test-time scaling remains underexplored in SWE. Yet it shows strong potential: prior works [22, 3] demonstrate that small models can generate correct solutions when sampled many times. Specifically, their pass@N , the probability that at least one of samples is correct, is close to the pass@1 performance of larger models. This indicates that small models can produce correct solutions; the challenge lies in efficiently identifying them. Test-time scaling assumes that among many sampled outputs, at least one will be correct. However, when correct solutions are rare, these methods often require large number of samples to succeed. This is particularly costly in SWE tasks, where generating each sample is slow due to long code contexts, and scoring is expensive when unit tests execution is needed [33]. Recent work [31] uses reinforcement learning (RL) [31] to enhance the reasoning capabilities of LMs for improved output quality but still requires hundreds of code edits (i.e., patch samples) per issue. Also, Pan et al. [22] depends on slow interactions with the runtime environment in agentic workflows. This motivates the need for sample-efficient test-time scaling methods that can identify correct solutions with fewer samples. In this paper, we propose Evolutionary Test-Time Scaling (EvoScale), sample-efficient method for improving test-time performance on SWE tasks. Existing test-time scaling methods often require an excessive number of samples because model outputs are highly dispersedcorrect solutions exist but are rare, as shown in Figure 1. EvoScale mitigates this by progressively steering generation toward higher-scoring regions, reducing the number of samples needed to find correct outputs. Inspired by evolutionary algorithms [25, 32, 8, 23], EvoScale iteratively refines candidate patches through selection and mutation. Instead of consuming the sample budget in single pass, EvoScale amortizes it over multiple iterations: the model generates batch of outputs, scoring function selects the top ones, and the next batch is generated by conditioning on theseeffectively mutating prior outputs. Early iterations focuses on exploration; later ones focus on exploitation. Figure 1: Reward score distribution of outputs from SFT model, with high-scoring outputs concentrated in the long tail. Although EvoScale improves sample efficiency, the selection step still incurs overhead: like standard evolutionary algorithms [32], it generates more outputs than needed and filters only the high-scoring ones, increasing sampling and computation costs. To eliminate this, we use RL to internalize the reward models guidance into the model itself, enabling it to self-evolverefining its own outputs without external reward models at inference. We formulate this as potential-based reward maximization problem [21], where the model learns to improve output scores across iterations based on score differences. This avoids discarding low-scoring outputs and reduces sample usage per iteration. Our theoretical analysis shows that this RL objective ensures monotonic score improvement across iterations. We evaluate the proposed EvoScale method on SWE-Bench-Verified [15], and summarize our key contributions as follows: new perspective of formulating test-time scaling as an evolutionary process, improving sample efficiency for software engineering tasks. novel RL training approach that enables self-evolution, eliminating the need for external reward models or verifiers at inference time. Satori-SWE-32B with EvoScale achieves performance comparable to models exceeding 100B parameters, while requiring only small number of samples."
        },
        {
            "title": "2 Related Work",
            "content": "Dataset Curation for SWE. Prior works [19, 22] and concurrent efforts [36, 14] use proprietary LLMs (e.g., Claude, GPT-4) as autonomous agents to collect SFT data by recording step-by-step interactions in sandboxed runtime environments. While this automates the data collection process for agent-style training [35], it involves substantial engineering overhead (e.g., Docker setup, sandboxing) and high inference costs. In contrast, Xie et al. [34] uses pipeline-based framework [33], collecting real pull-requestissue pairs and prompting GPT-4 to generate CoT traces and ground-truth patches without runtime interaction. Though easier to collect, this data requires careful noise filtering. Our approach instead improves small models performance by scaling the computation at test time. Test-time scaling for SWE. Xia et al. [33] showed that sampling multiple patches and selecting the best one based on unit test results in sandboxed environments improves performance. Unit tests have since been widely adopted in SWE tasks [31, 6, 14, 3]. Other works [22, 14, 20] train verifiers or reward models to score and select patches. To reduce the cost of interacting with runtime environments in agentic frameworks [35], some methods [20, 2] integrate tree search, pruning unpromising interaction paths early. While prior works improve patch ranking or interaction efficiency, our focus is on reducing the number of samples needed for effective test-time scaling. RL for SWE. Pan et al. [22] used basic RL approach for SWE tasks, applying rejection sampling to fine-tune models on their own successful trajectories. Wei et al. [31] later used policy gradient RL [24], with rewards based on string similarity to ground truth patches, showing gains over SFT. In contrast, our method trains the model to iteratively refine its past outputs, improving scores over time. We also use learned reward model that classifies patches as correct or incorrect, which outperforms string similarity as shown in Appendix A."
        },
        {
            "title": "3 Preliminaries",
            "content": "Figure 2: Pipeline for SWE Tasks. Given GitHub issue, the retriever identifies the code files most relevant to the issue. The code editor then generates code patch to resolve it. Software engineering (SWE) tasks. We study the problem of using LMs to resolve real-world GitHub issues, where each issue consists of textual description and corresponding code repository. Since issues are not self-contained, solving them requires identifying and modifying relevant parts of the codebase. There are two main paradigms for solving SWE tasks with LMs: agentic [35] and pipeline-based [33, 31]. Agentic methods allow the model to interact with the runtime environment, such as browsing files, running shell commands, and editing code through tool use. While flexible, these approaches are computationally intensive and rely on long-context reasoning, making them less practical for small models. In contrast, pipeline-based methods decompose the task into subtasks, typically retrieval and editing, and solve each without runtime interaction, which is more computationally efficient and suited for small models. Retrieval refers to identifying the files or functions relevant to the issue, while editing involves generating the code changes needed to resolve it. Formally, given an issue description x, the goal is to produce code edit (i.e., patch) that fixes the bug or implements the requested change. retrieval model selects subset code context C(x) from the full codebase C, and an editing model π generates the patch = π(x, C(x)) that modifies the code context C(x). While retrieval has reached around 70% accuracy in prior work [34, 33], editing remains the main bottleneck. This work focuses on improving editing performance in the pipeline-based setting, using off-the-shelf localization methods in experiments. In this setup, the dominant cost comes from sampling and scoring outputs from the editing model at test time. Test-time scaling [9, 28] improves model performance during inference without training. It typically involves sampling multiple outputs and selecting the best one using scoring function (e.g., reward 3 model) [5, 17]. Specifically, the model generates outputs y1, . . . , yN , scores them with R, and returns arg maxyi R(yi). This strategy is commonly used in domains like reasoning and mathematics."
        },
        {
            "title": "4 Method: Evolutionary Test-Time Scaling",
            "content": "Figure 3: An Overview of Evolutionary Test-Time Scaling. Given GitHub issue and its code context C(x), the editor model π first generates batch of candidate patches t. The reward landscape is illustrated with contour lines, where brighter contours indicate higher score of scoring function (e.g., reward model or unit tests). set of patches is selected (e.g., via scoring function R) and combined with and C(x) to form conditional prompt (see Section 4.1), which guides the model to generate the next batch t+1 = {yt+1 }, increasingly concentrated around the optimum. The process continues under fixed sampling budget until convergence, after which the final patch is submitted. Goal: Sample-efficient test-time scaling. Test-time scaling improves performance by selecting the best output from multiple samples, but often requires large number of generations to find correct solutions, especially in SWE tasks [31]. Our goal is to enable test-time scaling more sample-efficient, achieving stronger performance with fewer samples. , . . . , yt+1 1 Why is test-time scaling sample-inefficient in SWE task? Correct solutions exist but are rarely sampled, as for hard issues, the models output distribution is not concentrated around high-scoring regions. Given sample budget , typical test-time scaling methods in SWE [33, 31, 14, 22] draw outputs (patches) {yi}N i=1 from frozen editor model π, score them with score function (e.g., reward model or unit tests), and selects the best one arg maxyi R(x, yi). While high-scoring outputs near the mode could be sampled easily, the challenge of test-time scaling is to identify high-scoring outputs from the tail of π( x, C(x)). However, doing so typically requires large sample size , making the process sample-inefficient. Our approach: This motivates our method, Evolutionary Test-Time Scaling (EvoScale), which iteratively refines generation by using earlier outputs to guide subsequent sampling. We recast patch generation for GitHub issue as an evolutionary process. The objective is to explore the patch space with small number of samples, identify high-scoring patches, and iteratively refine the generated patches. As shown in Figure 3, initial samples are scattered and far from the correct solutions (denoted by stars), but over iterations, the distribution shifts closer to the correct solution. Through evolution, EvoScale more efficiently uncovers high-scoring outputs in long tails. We formulate the problem in Section 4.1 and detail the training procedure in Sections 4.2 and 4.3."
        },
        {
            "title": "4.1 Formulation: Patch Generation as Evolution",
            "content": "We amortize the sampling budget over iterations by generating < samples per iteration, rather than sampling all at once. The goal is to progressively improve sample quality across iterations. key challenge lies in effectively using early samples to guide later ones. Typical evolutionary strategies select top-scoring candidates and mutate themoften by adding random noiseto steer future samples toward high-scoring regions. However, in SWE tasks, where patches are structured code edits, random perturbations often break syntax or semantics (e.g., undefined variables, etc). Algorithm. Instead of using random noise for mutation, we use language model (LM) as mutation operator, leveraging its ability to produce syntactically and semantically valid patches. At each 4 iteration t, the LM generates batch of patches t+1 = {yt+1 } conditioned on set of prior patches t: t+1 π( x, C(x), t). We refer to as conditioning examples consisting of patches generated at iteration t. Following the selection step in evolutionary algorithms, could be selected as the top-K patches ranked by scoring function (i.e., fitness function in evolutionary algorithms). Note that we find that our model after training can self-evolve without this selector (see Section 4.3 and Section 5.2), so this step is optional. The full procedure is detailed in Algorithm 1. , . . . , yt+1 Question: Can language model naturally perform mutation? Ideally, the mutation operator should generate patches that improve scores. However, as shown in Section 5.2, models trained with classical SFTconditioned only on the issue and code contextstruggle to refine existing patches. In the next section, we present our approach to overcome this limitation."
        },
        {
            "title": "4.2 Small-scale Mutation Supervised Fine-Tuning",
            "content": "Classical supervised fine-tuning (SFT) fails at mutation because it never learns to condition on previous patches. To train the model for mutation, it must observe conditioning examplespatches from previous iterationsso it can learn to refine them. In EvoScale, conditioning examples are drawn from the models earlier outputs. We introduce two-stage supervised fine-tuning (SFT) process: classical SFT followed by mutation SFT. The classical SFT model is first trained and then used to generate conditioning examples for training the mutation SFT model. Stage 1 Classical SFT. We fine-tune base model on inputs consisting of the issue description and code context C(x), with targets that include chain-of-thought (CoT) trace and the ground-truth patch, jointly denoted as SFT. Following prior work on dataset curation [36, 34], we use teacher model µ (e.g., larger LLM; see Section 5.1) to generate CoT traces. The training objective is: SFTµ(x,C(x)) [log πSFT(y We refer to the resulting model πSFT as the classical SFT model. Stage 2 Mutation SFT. We fine-tune second model, initialized from the same base model, using inputs x, C(x), and set of conditioning examples consisting of patches sampled from the classical SFT model πSFT. The target M-SFT includes CoT trace generated by the teacher model µ conditioned on E, along with the ground-truth patch. The training objective is: SFT x, C(x))] . ExD, max πSFT (1) max πM-SFT ExD, EπSFT(x,C(x)),y M-SFTµ(x,C(x),E) [log πM-SFT(y M-SFT x, C(x), E)] . (2) We refer to the resulting model πM-SFT as the mutation SFT model. Training on small-scale datasets. EvoScale targets issues where one-shot generation often fails, but high-scoring patches can still be found through sufficient sampling. This means the model generates mix of highand low-scoring patches, so conditioning examples should reflect this diversity. If all examples were already high-scoring, test-time scaling would offer limited benefit. Training classical SFT model on the full dataset, however, leads to memorization, reducing output diversity and making it difficult to construct diverse conditioning examples for mutation SFT. To preserve diversity, we collect M-SFT on disjoint subsets of the data. See Appendix for details. SFT and Limitation of SFT in self-evolving. The mutation SFT model πM-SFT is trained on conditioning examples from the classical SFT model πSFT, which include both lowand high-scoring patches. This raises natural question: can πM-SFT learn to improve low-scoring patches on its owni.e., selfevolvewithout relying on reward models to select high-scoring examples? If so, we could eliminate the selection step (Line 3 in Algorithm 1), reducing scoring costs and sample usage. However, we find that SFT alone cannot enable self-evolution. Section 4.3 introduces reinforcement learning approach that trains the model to self-evolve without scoring or filtering. Algorithm 1 Evolutionary Test-Time Scaling (EvoScale) Require: Issue description x, code context C(x), editor model π, number of iterations , samples per iteration , optional selection size 1, , 1: Generate initial outputs 0 := {y0 2: for = 1 to do 3: 4: 5: end for (Optional) Select conditioning examples t1 := {yt1 Generate new outputs := {yt , , yt1 1 } π( x, C(x), t1)} 1, , yt } π( x, C(x)) } = Select(Y t1)"
        },
        {
            "title": "4.3 Learning to Self-evolve via Large-scale Reinforcement Learning (RL)",
            "content": "To self-evolve, the model must generate patches that maximize scoring function R, given conditioning examples from previous patches. This setup naturally aligns with the reinforcement learning (RL) [29], where policy π is optimized to maximize expected rewards (i.e., scores) over time. Since our goal is to maximize the reward at the final iteration , naïve RL objective is: max π Eytπ(x,C(x),E t1) (cid:104) (cid:88) (cid:105) rt t=0 , where rt = (cid:26)R(x, yt), 0, = otherwise (3) This objective focuses solely on maximizing the final reward. However, it presents two key challenges: (1) rewards are sparse, with feedback only at iteration , making learning inefficient [16, 26]; and (2) generating full -step trajectories is computationally expensive [28]. Potential shaping alleviates sparse rewards. We address the sparse reward challenge using potentialbased reward shaping [21], where the potential function is defined as Φ(y) = R(x, y). The potential reward at step is: rt = Φ(yt) Φ(yt1) = R(x, yt) R(x, yt1). (4) Unlike the naïve formulation (Equation 3), this provides non-zero potential rewards at every step, mitigating the sparse reward challenge. The cumulative potential reward forms telescoping sum: (cid:80)T t=1 rt = R(x, yT ) R(x, y0). Since y0 is fixed, maximizing this sum is equivalent to maximizing the final score as shown by Ng et al. [21]. Monotonic improvement via local optimization. While optimizing Equation 3 achieves the optimal final reward, it is computationally expensive due to the need for full -step trajectories. As more efficient alternative, we train the model to maximize the potential reward at each individual iteration (Equation 4), avoiding the cost of generating full -step trajectories. This local optimization reduces computation and runtime while ensuring monotonic reward improvement (see Section 5.2), which is sufficient for improving patch scores over iterations. We formally show this property in Section 4.4. Implementation. Using the full dataset, we fine-tune the mutation SFT model πM-SFT to maximize the expected potential rewards (Equation 4) in score between newly generated patch and previous patch drawn from the conditioning examples E: (cid:2)R(x, y) R(x, y) λF (y)(cid:3). (5) yπRL(x,C(x),E),yE max πRL This objective encourages the model to generate patches that consistently improve upon previous ones. To ensure the outputs follow the required syntax, we incorporate formatting penalty term into the reward function (see Appendix for details). The conditioning patch is sampled from conditioning examples constructed using patches generated by earlier models, such as πSFT or intermediate checkpoints of πRL."
        },
        {
            "title": "4.4 Theoretical Analysis",
            "content": "We analyze the RL objective in Equation 5, which leverages potential-based reward shaping [21], and show that the induced policy yields non-decreasing scores at each iteration. Assumption 1 (Φ-monotonicity). Let be the set of all patches and Φ : potential function. For every Y, there exists finite sequence = y0, y1, , yk such that Φ(yt+1) Φ(yt) for all 0 < k. (cid:2)Φ(y) Φ(y)(cid:3). This ensures that from any initial patch one can reach higher-scoring patches without decreasing Φ. Definition 1 (Myopic Policy). Define the one-step action-value Q0(y, y) = Φ(y) Φ(y), y, Y. The myopic policy π0 selects, at each state y, any successor that maximizes Q0: π0(y) arg maxyY Proposition 1 (Monotonic Improvement). Under Assumption 1, any trajectory {yt}t0 generated by rt = Φ(yt) Φ(yt1) 0 1. the myopic policy π0 satisfies Φ(yt) Φ(yt1) and (cid:2)Φ(y)Φ(yt1)(cid:3). Hence Φ(yt)Φ(yt1) Proof. By definition of π0, at each step yt arg maxy 0, which immediately gives Φ(yt) Φ(yt1) and rt 0. In particular, training with the potential reward in Equation (5) guarantees that R(x, yt) = Φ(yt) Φ(yt1) = R(x, yt1) t. 6 Thus the learned policy produces non-decreasing scores over iterations."
        },
        {
            "title": "5.1 Settings",
            "content": "Implementation Details. We adopt pipeline-based scaffold consisting of retriever and code editing model (see Appendix C). Both components are trained using small-scale SFT and large-scale RL. We use the Qwen2.5-Coder-32B-Instruct model [12] as our base model due to its strong code reasoning capabilities. Our training data is sourced from SWE-Fixer [34] and SWE-Gym [22]. After filtering and deduplication, we obtain total of 29,404 high-quality instances. For RL training of the code editing model, we rely on reward model trained on data collected from open source data3 with 1,889 unique instances. Additional experimental details are provided in Appendix D. Evaluation and Metrics. We consider two metrics in our evaluation: (1) Greedy: zero-shot pass@1 accuracy, which measures the number of correctly solved instances using greedy generation with syntax retrial (i.e., random sampling up to five times until syntactically correct); (2) Best@N : accuracy of the optimal sample selected by the verifier among randomly generated samples. Greedy evaluates the models budget-efficient performance, while Best@N represents the models potential for test-time scaling. Test-time Scaling Methods. We evaluate the following test-time scaling methods: (1) Reward Model Selection: selects the optimal patch sample with the highest reward model score; (2) Unit Tests Selection: selects the optimal patch sample based on whether it passes unit tests, including both regression and reproduction tests. If multiple samples pass, one is selected at random; (3) EvoScale: at each evolution iteration, the model generates patch samples and selects samples as the conditional prompt for the next generation. The selection of the samples is guided by the reward model. In our experiments, we set = 10, = 5, and perform up to four iterations of evolution."
        },
        {
            "title": "5.2 Analysis",
            "content": "In this section, we present comprehensive analysis of the proposed EvoScale approach. To simplify our analysis, we use ground-truth localization (retrieval) and focus on the code editing part. All reported results are averaged over three random trials. More results are provided in Appendix A. (a) RM as selector: Classical SFT v.s. Mutation SFT (b) Self-evolve: Mutation SFT v.s. RL Figure 4: Evolutionary Capability of Different Stages of SFT and RL Models. (a) Reward Model selects the top-5 patch candidates from 10 samples from the previous iteration, and the model iteratively evolves by generating new 10 samples conditioned on the candidates. Performance of the top-1 sample selected by RM is reported. Without the additional mutation SFT training, the model fails to exhibit evolutionary behavior, even when scaling up the training set. (b) Without RM selection, the model only iteratively evolves by conditioning on 5 random samples from the last iteration. RL training improves the models initial performance and incentivizes the self-evolution capability, while the SFT model fails to self-evolve without guidance from RM. Can LLMs Iteratively Evolve without Mutation SFT Training? First, we investigate whether the mutation SFT is necessary for LLMs to learn how to iteratively improve their generations. Specifically, 3https://huggingface.co/nebius we fine-tune base LLMs using either classical SFT (without conditional generation) or mutation SFT. As shown in Figure 4(a), models trained with classical SFT fail to naturally improve their outputs when conditioned on previous samples. In contrast, mutation SFT enables the model to iteratively improve under the guidance of reward model. The performance of the mutation SFT model at later iterations can surpass the classical SFT model by scaling up the samples (e.g., Best@40). Moreover, this iterative refinement capability can be learned effectively even with small number of training data. RL Enables Self-evolve Capability. While mutation SFT model demonstrates evolutionary behavior when guided by reward model, we further examine whether it can self-evolve without such guidance. Specifically, instead of selecting the top-K candidates to ensure generation quality, we allow the model to generate = = 5 random samples for the next iteration of conditional generation. However, as shown in Figure 4(b), the SFT model fails to learn self-evolution without reward model selection. Interestingly, RL training significantly improves the SFT model in two key aspects. First, RL substantially boosts the models greedy performance, surpassing even the Best@N performance of 30 randomly generated samples from the SFT model. Second, we observe that the RL-trained model exhibits strong self-evolution capability: even when conditioned on its random outputs, the model can self-refine and improve performance across iterations without reward model guidance. We provide further analysis of the models behavior through demo examples in Appendix B.1. Figure 5: Average Reward Score of Patch Samples at Each Evolution Iteration. Reward scores are normalized via sigmoid function before average. The SFT model struggles to improve reward scores without the guidance of reward model to select top-K conditional patch samples, while the RL model consistently self-improves its reward score across iterations without external guidance, validating our theoretical results of monotonic improvement in Section 4.4 Figure 6: Comparison with Other Test-Time Scaling Methods. Reward model selection requires deploying an additional model at test time and can become unstable as the number of samples increases. Unit test selection is computationally expensive and performs poorly with small sample size. In contrast, self-evolution demonstrates high sample efficiency and strong test-time scaling performance. Do our SFT and RL Models Monotonically Improve Reward Scores over Iterations? We further analyze the evolutionary behavior of the SFT and RL models by measuring the average reward score of the patch samples generated at each iteration. As shown in Figure 5, although the SFT model learns to iteratively improve reward scores, it relies on the reward model to select high-quality conditioning examples to achieve significant improvements. In contrast, the RL model trained with potential-based reward, naturally learns to self-evolve without any external guidance. Its reward scores improve monotonically across iterations, aligns with our theoretical analysis in Section 4.4. Evolutionary Test-time Scaling v.s. Other Test-time Scaling Methods. Next, we further compare evolutionary test-time scaling with other test-time scaling methods. Starting from the RL model, we first randomly sample = 5, 10, 15, 20, 25, 50 patch samples and let the reward model and unit tests select the best sample among the subsets. Also starting from the RL model, we let the model perform self-evolution with = 5 samples per iteration, up to four iterations (20 samples in total). The test-time scaling results presented in Figure 6 demonstrate both efficiency and effectiveness of evolutionary test-time scaling. We include more details in Appendix A. 8 Table 1: Results on SWE-bench Verified. Satori-SWE-32B outperforms all small-scale models under greedy decoding, while achieving comparable performance with current SOTA SWE-RL with much fewer training data and test-time scaling samples. Model Scale Model/Methods Scaffold SWE-Verified Resolved Rate Large Small GPT-4o [35] GPT-4o [33] GPT-4o [37] GPT-4o [19] OpenAI o1 [33] Claude 3.5 Sonnet [35] Claude 3.5 Sonnet [30] Claude 3.5 Sonnet [33] Claude 3.5 Sonnet [37] Claude 3.7 Sonnet [1] DeepSeek-R1 [7] DeepSeek-V3 [18] SWE-agent Agentless AutoCodeRover SWE-SynInfer Agentless SWE-agent OpenHands Agentless AutoCodeRover SWE-agent Agentless Agentless Lingma-SWE-GPT-7B (Greedy) [19] SWE-SynInfer Lingma-SWE-GPT-72B (Greedy) [19] SWE-SynInfer SWE-Fixer-72B (Greedy) [34] SWE-Fixer SWE-Gym-32B (Greedy) [22] OpenHands SWE-Gym-32B (Best@16) [22] OpenHands Llama3-SWE-RL-70B (Best@80) [31] Agentless Mini Llama3-SWE-RL-70B (Best@160) [31] Agentless Mini Llama3-SWE-RL-70B (Best@500) [31] Agentless Mini Satori-SWE-32B (Greedy) Satori-SWE-32B (Best@10) Satori-SWE-32B (Best@25) Satori-SWE-32B (Best@50) Satori-SWE Satori-SWE Satori-SWE Satori-SWE 23.0 38.8 28.8 31.8 48.0 33.6 53.0 50.8 46.2 58.2 49.2 42.0 18.2 28.8 30.2 20.6 32.0 37.0 40.0 41.0 35.8 38.9 40.2 41."
        },
        {
            "title": "5.3 Results in the Wild: SWE-bench Performance",
            "content": "We present the main results of our RL-trained model, Satori-SWE-32B, on the SWE-bench Verified benchmark [15] and compare its performance against both open-source and proprietary systems. We report results for both greedy decoding and Best@N metrics, using our own retrieval framework (see details of retrieval in Appendix C). For test-time scaling, we apply iterative self-evolution, allowing the RL model to generate = 25 samples per iteration. We observe that the initial iterations produce more diverse candidate patches, while later iterations generate higher-quality, more refined patches. To balance diversity and refinement, we aggregate all generated samples across iterations into combined pool of = 50 candidates. As discussed in Section 5.2, different verifiers provide complementary strengths. We therefore combine both the reward model and unit tests to select the best patch from the candidate pool. As shown in Table 1, Satori-SWE-32B achieves greedy accuracy of 35.8, outperforming all existing small-scale models under greedy decoding. Additionally, it achieves Best@50 score of 41.6, matching the performance of the current state-of-the-art Llama3-SWE-RL-70B [31], which requires Best@500 decodingincurring over 10 higher sampling cost. It is also worth noting that agentbased methods incur even higher test-time computational cost, as each generation corresponds to full rollout trajectory with multiple interactions. In contrast, Satori-SWE-32B achieves state-of-the-art performance with significantly lower inference cost and is trained on fewer than 30K open-source samples, compared to millions of proprietary data used to train Llama3-SWE-RL-70B."
        },
        {
            "title": "6 Concluding Remarks",
            "content": "We propose Evolutionary Test-time Scaling (EvoScale), sample-efficient inference-time method that enables small language models to approach the performance of 100B+ parameter models using just 50 code patch sampleswithout requiring interaction trajectories with the runtime environment. EvoScale opens up new direction for sample-efficient test-time scaling in real-world software engineering tasks: (1) Evolution improves sample efficiency. Our results show that evolutionary strategies, which iteratively refine generations, can drastically reduce the number of required samples. This contrasts with prior work that primarily focuses on improving verifiers (e.g., reward models, test cases); (2) RL enables self-evolution. We show that reinforcement learning (RL) can train models to refine their outputs without relying on external verifiers at inference. While our current method optimizes local reward differences, future work may explore optimizing cumulative potential rewards over entire trajectories. Compared to Snell et al. [28], who maintains all prior outputs in the 9 prompt during revision, our method retains only the most recent outputmaking it more suitable for SWE tasks with long context windows; (3) Limitations and future work. This work focuses on pipeline-based (agentless) setup. Extending EvoScale to agentic settings where models interact with code and runtime environments, remains an interesting future work."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude 3.7 sonnet, 2025., 2025. URL https://www.anthropic.com/ claude/sonnet. 9 [2] Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Yang Wang. SWE-search: Enhancing software agents with monte carlo tree search and iterative refinement. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=G7sIFXugTX. 3 [3] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. 2, 3 [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. 1 [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2, [6] Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, and Azalia Mirhoseini. Codemonkeys: Scaling test-time compute for software engineering. arXiv preprint arXiv:2501.14723, 2025. 3 [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9 [8] Nikolaus Hansen. The cma evolution strategy: tutorial. arXiv preprint arXiv:1604.00772, 2016. 2 [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 2, 3 [10] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [11] Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/abs/2501. 03262. 32, 33, 34 [12] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https: //arxiv.org/abs/2409.12186. 7, 31 10 [13] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. 1 [14] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. 3, 4 [15] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=VTF8yNQM66. 1, 2, [16] Chi-Chang Lee, Zhang-Wei Hong, and Pulkit Agrawal. Going beyond heuristics by imposing policy improvement as constraint. Advances in Neural Information Processing Systems, 37: 138032138087, 2024. 6 [17] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 2, 4 [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 9 [19] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. 3, 9 [20] Yingwei Ma, Yongbin Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, and Binhua Li. Thinking longer, not larger: Enhancing software engineering agents via scaling test-time compute, 2025. URL https://arxiv.org/abs/2503.23803. 3 [21] Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278287, 1999. 2, [22] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with SWE-gym. In ICLR 2025 Third Workshop on Deep Learning for Code, 2025. URL https://openreview.net/forum?id= lpFFpTbi9s. 1, 2, 3, 4, 7, 9, 14 [23] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. 2 [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. 3 [25] Maohao Shen, Soumya Ghosh, Prasanna Sattigeri, Subhro Das, Yuheng Bu, and Gregory Wornell. Reliable gradient-free and likelihood-free prompt tuning. In Findings of the Association for Computational Linguistics: EACL 2023. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-eacl.183/. 2 [26] Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with Chain-of-Action-Thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. 6 [27] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems. ACM, 2025. 31 [28] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM testIn time compute optimally can be more effective than scaling parameters for reasoning. The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n. 2, 3, 6, 9 [29] Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. 2018. 6 [30] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. 9 [31] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. 2, 3, 4, 9, 13, 33 [32] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949980, 2014. [33] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents, 2024. URL https://arxiv.org/abs/2407.01489. 1, 2, 3, 4, 9, 14, 30, 31, 34 [34] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swefixer: Training open-source llms for effective and efficient github issue resolution, 2025. URL https://arxiv.org/abs/2501.05040. 1, 2, 3, 5, 7, 9 [35] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=mXpq6ut8J3. 3, 9 [36] John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. URL https://arxiv.org/abs/2504.21798. 2, 3, 5 [37] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604, 2024."
        },
        {
            "title": "B Demo Examples",
            "content": "B.1 Type 1: Prior patches are all wrong . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Type 2: Prior patches are partially wrong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Type 3: Prior patches are all correct Scaffold of Satori-SWE . C.1 Retriever . . C.2 Code Editing Model . . C.3 Verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details D.1 Dataset Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Training Pipeline and Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Retrieval Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Retrieval Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Code Editing Model . D.6 Code Editing Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7 Reproduction Test Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Additional Experiments",
            "content": "13 15 15 18 23 30 30 30 31 31 31 31 32 32 33 34 34 35 In this section, we provide additional analytical experiments of EvoScale and model training. RL Reward Modeling: Reward Model is More Effective than String-matching. reliable reward signal is the key to drive RL training. To better understand the impact of different components in reward modeling, we conduct an ablation study comparing three variants: using only the reward model score, using only string-matching rewards proposed in [31], and using both. As shown in Table 2, models trained with single reward component show degraded greedy decoding performance compared to the model trained with the hybrid reward. In particular, the reward model plays crucial role in boosting the performance, while the string-matching reward helps the model learn better syntactic structure. However, the results also suggest that naïve string-matching [31] alone may not serve as reliable reward signal for SWE tasks. Table 2: Ablation Study on Reward Modeling. The total number of instances is 500. Compared to the SFT model, RL using the RM reward significantly improves performance but introduces more syntax errors. In contrast, RL with string-matching reward reduces syntax errors but fails to improve reasoning capability. hybrid reward signal effectively balances both aspects, achieving superior performance. Metrics Mutation SFT (5% data) RM RL String-matching RL Hybrid Reward RL Num of Resolved Instances Num of Syntax-correct Instances 137 2.5 427 171 1.7 404 140.7 0.5 478 179.3 2.4 EvoScale Prefers Higher Mutation Sampling Temperature. Mutation sampling plays critical role in Evolutionary Test-Time Scaling. To investigate its impact, we vary the models sampling temperature across 0.7, 1.0, 1.2 and perform self-evolution over four iterations. As shown in Figure 7, higher temperatures demostrate better performance. Intuitively, larger temperature increases the diversity of generated patch samples, providing richer information for the mutation operator to produce improved patches in subsequent iterations. In contrast, lower temperatures tend to result in repetitive patch samples and may lead the model to converge quickly to suboptimal solutions. SFT + Test-time Scaling v.s. RL + Self-evolve. In Figure 6, we demonstrated the superior performance of Evolutionary Test-Time Scaling using our RL-trained model. To further investigate 13 Figure 7: Impact of Mutation Sampling Temperature. Higher sampling temperatures in EvoScale encourage greater diversity among mutation samples, leading to more effective iterative improvements. Figure 8: Classical SFT + Test-time Scaling v.s. Mutation RL + Self-evolve. RL Model with self-evolve capability is more effective than classical SFT model using other test-time scaling methods. this result, we compare against other test-time scaling methods applied to classical SFT model trained on the full dataset (30K instances), since the typical procedure in most existing SWE work [22, 33] trains SFT model and applying verifiers (e.g., reward models or unit tests) for test-time scaling. However, as shown in Figure 8, this approach proves to be less effective: (1) With 50 samples, the SFT models Best@50 performance is still outperformed by the greedy decoding of the RL model, despite both being trained on the same dataset. (2) The SFT model is relatively sensitive to the choice of verifier. When using unit tests (including both reproduction and regression tests) as the verifier, increasing the number of samples results in only marginal performance gains. These observations support our hypothesis: while correct solutions do exist in the SFT models output distribution, they are rarely sampled due to its dispersed sampling distribution. In contrast, the RL model learns to self-refine the sampling distribution towards high-scoring region. Table 3: Average Runtime per Instance for Different Test-Time Scaling Methods. Runtime is measured using sample budget of 10. EvoScale achieves the highest efficiency, while unit test-based selection incurs over 6 higher runtime cost."
        },
        {
            "title": "Reward Model",
            "content": "Self-evolve Wall-clock Time (seconds) 92.8 2.6 18.1 0.3 16.6 0.4 Runtime Comparison of Different Test-time Scaling Methods. To evaluate the efficiency of different test-time scaling methods, we measure the average runtime per instance using sample budget of 10. For our proposed EvoScale approach, the runtime consists solely of iteratively prompting the RL model to generate 10 samples. Reward model selection incurs additional computational cost due to running the reward model to score each sample, and unit test selection requires executing each patch in sandbox environment. Although unit test selection is effective when scaling to larger sample sizes (see Figure 6), it comes at cost around 6 slower than EvoScale. Would RL without Evolution Training still Work? We consider simplified training setup for the code editing model, where the base model is trained using classical SFT followed by RL without incorporating mutation data or potential-based rewards. As shown in Figure 2, although this simplified RL approach can still improve the SFT models greedy performance, it fails to equip the model with iterative self-improvement ability. This finding demonstrates the importance of evolution training, particularly the use of potential-based rewards, in incentivizing the model to learn how to self-refine over multiple iterations. 14 Figure 9: RL with vs. without Self-Evolution Training. Removing evolution training during the RL stage results in model that lacks iterative self-improvement capabilities."
        },
        {
            "title": "B Demo Examples",
            "content": "The core idea of EvoScale is to use mix of correct and incorrect patches as context to generate correct patch. To do this effectively, the model must demonstrate the following capabilities: 1. Generate correct patches even when all prior patches are incorrectessential for making progress when initial generations fail. 2. Generate correct patches when most prior patches are incorrectcommon in practice, requiring the model to identify and build on the few correct ones. 3. Preserve correctness when all prior patches are correctensuring the model doesnt over-edit and degrade already correct solutions. We present examples below to illustrate each of these capabilities. B.1 Type 1: Prior patches are all wrong Problem Statement 1 sum: min_count is not available for reduction with more than one dimensions **Is your feature request related to problem? Please describe.** `sum` with `min_count` errors when passing more than one dim: ```python import xarray as xr da = xr.DataArray([[1., 2, 3], [4, 5, 6]]) da.sum([\"dim_0\", \"dim_1\"], min_count=1) ``` **Describe the solution you'd like** The logic to calculate the number of valid elements is here: https://github.com/pydata/xarray/blob/1be777fe725a85b8cc0f65a2bc41f4bc2ba18043/xarray/core/nanops.py#L35 *think* this can be fixed by replacing `mask.shape[axis]` with `np.take(a.shape, axis).prod()` **Additional context** Potentially relevant for #4351 Conditional patch 1 (Wrong) diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..3e6c5db 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -32,7 +32,16 @@ def _maybe_null_out(result, axis, mask, min_count=1): 15 ) - + + + + + + + + + + if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 if np.isscalar(axis): # in the scalar case we can just use the shape of that axis size = mask.shape[axis] else: # in the case of multiple axis we need to take the product # of all the axis sizes since we want the total number of # elements we are reducing, not just the size of one axis size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype) Conditional patch 2 (Wrong) diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..f533eed 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -32,7 +32,15 @@ def _maybe_null_out(result, axis, mask, min_count=1): ) - + + + + + + + + + if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 # handle the case where we are reducing on multiple axis # we need to consider the product of their sizes, not just any # one of them. if np.isscalar(axis): size = mask.shape[axis] else: size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype) Conditional patch 3 (Wrong) diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..f533eed 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -32,7 +32,15 @@ def _maybe_null_out(result, axis, mask, min_count=1): ) - + + + + + + + + + if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 # handle the case where we are reducing on multiple axis # we need to consider the product of their sizes, not just any # one of them. if np.isscalar(axis): size = mask.shape[axis] else: size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype) Conditional patch 4 (Wrong) diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..f533eed 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -32,7 +32,15 @@ def _maybe_null_out(result, axis, mask, min_count=1): ) 16 - + + + + + + + + + if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 # handle the case where we are reducing on multiple axis # we need to consider the product of their sizes, not just any # one of them. if np.isscalar(axis): size = mask.shape[axis] else: size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype) Conditional patch 5 (Wrong) diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..f533eed 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -32,7 +32,15 @@ def _maybe_null_out(result, axis, mask, min_count=1): ) - + + + + + + + + + if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 # handle the case where we are reducing on multiple axis # we need to consider the product of their sizes, not just any # one of them. if np.isscalar(axis): size = mask.shape[axis] else: size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype)"
        },
        {
            "title": "Thinking Process",
            "content": "1. Peer Patch Review: - All five patches are essentially identical in their approach to solving the issue. They all modify (cid:44) (cid:44) (cid:44) the same section of code in _maybe_null_out() to handle the case where axis is list/tuple by calculating the product of the sizes of the reduced dimensions instead of just using the size of single axis. - The key insight is correct - when reducing over multiple dimensions, the total number of elements (cid:44) (cid:44) being reduced is the product of the sizes of those dimensions, not just the size of one dimension. - All patches correctly identify that when axis is scalar, we should use the single dimension size (cid:44) (cid:44) as before, but when axis is list/tuple, we should use the product of the sizes of the specified dimensions. - The solution is consistent with the issue description which suggests using np.take(a.shape, (cid:44) axis).prod() to get the total number of elements being reduced. 2. Issue Analysis: - The problem occurs when using sum() with min_count on multiple dimensions because the current code (cid:44) (cid:44) only checks the size of one dimension (the first in the axis list) rather than the total number of elements being reduced. - The intended behavior is that min_count should consider the total number of valid elements across (cid:44) all reduced dimensions, not just the number of elements along single dimension. - The fix requires modifying the logic that calculates the number of valid elements when reducing (cid:44) over multiple dimensions to use the product of the sizes of all reduced dimensions. 3. Solution Approach: - The key change needed is in the _maybe_null_out() function where it calculates null_mask for (cid:44) multi-dimensional reductions. 17 - When axis is list/tuple, we should calculate the total number of elements being reduced by taking the product of the sizes of the specified dimensions (using np.take(mask.shape, (cid:44) axis).prod()). (cid:44) - For single dimension reductions, we can keep the existing behavior of using just mask.shape[axis]. - The rest of the function's logic (checking min_count against the number of valid elements) can (cid:44) remain the same. 4. Code Localization: - The issue is in xarray/core/nanops.py in the _maybe_null_out() function. - The specific location is where it calculates null_mask for multi-dimensional reductions (when axis (cid:44) is not None and result has ndim). - The current code uses mask.shape[axis] which is incorrect for multi-dimensional reductions. We need to modify this to use the product of the sizes of all reduced dimensions when axis is (cid:44) list/tuple. (cid:44)"
        },
        {
            "title": "Evolved Patch",
            "content": "diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py index 74e25c7..c642ed6 100644 --- a/xarray/core/nanops.py +++ b/xarray/core/nanops.py @@ -27,12 +27,22 @@ def _maybe_null_out(result, axis, mask, min_count=1): - - - + + + + - + + + + + + + + + + xarray version of pandas.core.nanops._maybe_null_out \"\"\" if hasattr(axis, \"__len__\"): # if tuple or list raise ValueError( \"min_count is not available for reduction with more than one dimensions.\" ) # we still need to handle the case where axis is list/tuple # by calculating the product of the sizes of the reduced # dimensions rather than just using one dimension's size pass if axis is not None and getattr(result, \"ndim\", False): null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0 if np.isscalar(axis): # in the scalar case we can just use the shape of that axis size = mask.shape[axis] else: # in the case of multiple axis we need to take the product # of all the axis sizes since we want the total number of # elements we are reducing, not just the size of one axis size = np.take(mask.shape, axis).prod() null_mask = (size - mask.sum(axis) - min_count) < 0 if null_mask.any(): dtype, fill_value = dtypes.maybe_promote(result.dtype) result = result.astype(dtype) B.2 Type 2: Prior patches are partially wrong Problem Statement 1 Incorrect parenthesizing of Subs Here is an example. ```python >>> from sympy import Subs >>> from sympy.abc import x,y >>> 3*Subs(-x+y, (x,),(1,)) ``` LaTeX printing of this gives: ```python '3 left. - + right_{substack{ x=1 }}' ``` ![image](https://quicklatex.com/cache3/76/ql_9672fd7e62c909ff3d9ac8543c2e2576_l3.png) It would be better to be parenthesized to: 18 ```python '3 left. left(- + yright) right_{substack{ x=1 }}' ``` ![image](https://quicklatex.com/cache3/bf/ql_936ffdb876e784206d4c54bb93d28dbf_l3.png) Conditional patch 1 (Wrong) diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..8b39416 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py @@ -698,7 +698,12 @@ class LatexPrinter(Printer): - + + + + + + def _print_Subs(self, subs): note: we need to use lower precedence than MUL to get parens for things like 3*Subs(3+2,...) --> 3*(3+2)... the MUL prec level is the highest that always adds the needed parens expr, old, new = subs.args latex_expr = self._print(expr) # get the expression in proper parenthesized form # # # # latex_expr = self.parenthesize(expr, PRECEDENCE[\"Mul\"] - 1) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join( Conditional patch 2 (Correct) diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..2d262c4 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py @@ -698,7 +698,10 @@ class LatexPrinter(Printer): - + + + + def _print_Subs(self, subs): expr, old, new = subs.args latex_expr = self._print(expr) # issue 12165: need to parenthesize the expression if it is # an Add (or lower precedence) to get proper grouping # when coefficient multiplies substituted expression latex_expr = self.parenthesize(expr, PRECEDENCE[\"Mul\"]) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join( Conditional patch 3 (Correct) diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..b5b4285 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py @@ -698,7 +698,11 @@ class LatexPrinter(Printer): def _print_Subs(self, subs): - + + + + + expr, old, new = subs.args latex_expr = self._print(expr) # always parenthesize the expr so Subs is properly grouped # when multiplied, e.g. 3*Subs(-x+y, (x,),(1,)) vs 3*(-x+y)_x=1 # we use Mul precedence since we want to group it at the same # level as args to Mul (but not as high as Pow which wouldn't group) latex_expr = self.parenthesize(expr, PRECEDENCE[\"Mul\"]) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join( Conditional patch 4 (Wrong) diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..868ab44 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py 19 @@ -698,7 +698,13 @@ class LatexPrinter(Printer): def _print_Subs(self, subs): - + + + + + + + expr, old, new = subs.args latex_expr = self._print(expr) # we want to always parenthesize the expression, otherwise # something like 3*Subs(-x+y, (x,),(1,)) will have # incorrect LaTeX output # to see the issue, compare 3*Subs(-x+y, (x,),(1,)) with # 3*(-x+y).subs(x, 1) in IPython with init_printing() # so we sub in low precedence level to be safe latex_expr = self.parenthesize(expr, 0) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join( Conditional patch 5 (Wrong) diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..dab5b75 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py @@ -698,7 +698,13 @@ class LatexPrinter(Printer): - + + + + + + + def _print_Subs(self, subs): expr, old, new = subs.args latex_expr = self._print(expr) # issue 15210: always parenthesize the expression because # otherwise the wrong precedence may give incorrect results # For example 3*Subs(x+y, x, 2) would be 3*x+y_{x=2} rather # than the correct 3*(x+y)_{x=2} latex_expr = self.parenthesize(expr, 0) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join("
        },
        {
            "title": "Thinking Process",
            "content": "1. Peer Patch Review: - Patch 1: - Correctly identifies the need to use lower precedence than MUL to force parentheses - The solution is reasonable since MUL precedence is high enough to always add needed parens - However, using PRECEDENCE[\"Mul\"] - 1 might be too low since we want to group at the same level (cid:44) as MUL args - Patch 2: MUL args - Uses PRECEDENCE[\"Mul\"] which is reasonable choice since we want to group at the same level as (cid:44) - Correctly identifies the need to parenthesize Add expressions when multiplied - Simple and correct solution that addresses the core issue - Patch 3: - Similar to patch 2 but with clearer explanation of why Mul precedence is appropriate - Also correctly identifies the need to always parenthesize the expression - Good solution that matches the intended behavior - Patch 4: - Uses precedence 0 which is too low - this would always add parentheses even when not needed - While it would fix the issue, it would also introduce unnecessary parentheses in many cases - Not the optimal solution since it's too aggressive with parenthesizing - Patch 5: - Similar to patch 4 but with clearer explanation of the issue - Also uses precedence 0 which has the same problem as patch 4 - While the intent is correct, the solution is too broad in its application (x,),(1,))), the expression inside Subs isn't properly parenthesized in LaTeX output 2. Issue Analysis: - The issue shows that when Subs expression is multiplied by coefficient (like 3*Subs(-x+y, (cid:44) - The current code just prints the expression directly without considering its precedence relative (cid:44) - The correct behavior should always parenthesize the Subs expression when it appears in context (cid:44) where precedence matters (like multiplication) to multiplication precedence level 3. Solution Approach: - The key is to modify the _print_Subs method to always parenthesize the expression using the MUL (cid:44) - This ensures that when Subs appears in multiplication context, it will be properly grouped with (cid:44) - Using MUL precedence is appropriate because we want it to group at the same level as other MUL (cid:44) arguments but not higher than necessary parentheses 4. Code Localization: - The issue is in sympy/printing/latex.py in the _print_Subs method - The current implementation directly prints the expression without any parenthesizing - We need to modify it to use the parenthesize method with MUL precedence"
        },
        {
            "title": "Evolved Patch",
            "content": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py index 675a3c4..6dd89a9 100644 --- a/sympy/printing/latex.py +++ b/sympy/printing/latex.py @@ -698,7 +698,14 @@ class LatexPrinter(Printer): - + + + + + + + + def _print_Subs(self, subs): expr, old, new = subs.args latex_expr = self._print(expr) # issue 15210: always parenthesize the expression because # otherwise the wrong precedence may give incorrect results # For example 3*Subs(x+y, x, 2) would be 3*x+y_{x=2} rather # than the correct 3*(x+y)_{x=2} # Using MUL precedence ensures we group at the same level as # other MUL args but not higher than necessary (we don't want # to group for Pow since Pow has higher precedence) latex_expr = self.parenthesize(expr, PRECEDENCE[\"Mul\"]) latex_old = (self._print(e) for in old) latex_new = (self._print(e) for in new) latex_subs = r' '.join( Problem Statement 2 Contains.as_set returns Contains ```py >>> Contains(x, Reals).as_set() Contains(x, Reals) ``` This is wrong because Contains is not set (it's boolean). It results in failures in other places (cid:44) (cid:44) because it doesn't have as_relational (since it isn't set). For instance, from https://github.com/sympy/sympy/pull/14965#discussion_r205281989 ```pytb >>> Piecewise((6, Contains(x, Reals)), (7, True)) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"./sympy/functions/elementary/piecewise.py\", line 136, in __new__ = cls.eval(*newargs) File \"./sympy/functions/elementary/piecewise.py\", line 185, in eval = c.as_set().as_relational(x) AttributeError: 'Contains' object has no attribute 'as_relational' ``` Conditional patch 1 (Correct) diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..3fd780b 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - + + + raise NotImplementedError() # The set being contained in is the only set we can return # from Contains object in any meaningful way return self.args[1] 21 Conditional patch 2 (Correct) diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..4c089aa 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,5 @@ class Contains(BooleanFunction): def as_set(self): - + + raise NotImplementedError() # The set that is being tested for contains return self.args[1] Conditional patch 3 (Correct) diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..985c88e 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - + + + raise NotImplementedError() # The set being contained in is the set representation # of Contains objects return self.args[1] Conditional patch 4 (Correct) diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..e732791 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - + + raise NotImplementedError() # Contains(x, S) means \"x is in S\", so as_set returns return self.args[1] Conditional patch 5 (Wrong) diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..4980364 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,22 @@ class Contains(BooleanFunction): + + + + + + + + + + def as_set(self): # try to evaluate Contains(x, S) try: res = self._eval_evalf() if res == S.true: return S.UniversalSet elif res == S.false: return S.EmptySet except (TypeError, AttributeError): pass # fallback raise NotImplementedError()"
        },
        {
            "title": "Thinking Process",
            "content": "1. Peer Patch Review: - Patch 1: This patch correctly identifies that the set being contained in (self.args[1]) is the most meaningful set representation of Contains object. The implementation is simple and (cid:44) correct. (cid:44) - Patch 2: Similar to Patch 1 but with slightly different wording in the comment. The functionality (cid:44) is correct but the comment could be more precise about why this is the right approach. - Patch 3: This patch adds note about potential invalid sets, which isn't strictly necessary since the issue is about the basic behavior. The core implementation is correct but the comment is (cid:44) overly cautious. (cid:44) - Patch 4: This patch provides clear explanation of why returning the set (self.args[1]) is the (cid:44) (cid:44) correct behavior - because Contains(x, S) means \"x is in S\", so the set representation is just S. This is correct and well-explained solution. - Patch 5: This patch takes different approach by trying to evaluate the Contains expression and (cid:44) (cid:44) (cid:44) return either the universal or empty set based on the result. While creative, this doesn't match the simple and correct solution of just returning the set being contained in. The approach is overly complex for this simple issue. 2. Issue Analysis: set. - The issue occurs when Contains.as_set() returns the Contains object itself instead of proper (cid:44) - This causes problems because other code expects set with as_relational() method. - The correct behavior should be to return the set that is being tested for containment in (self.args[1]), since that's the only meaningful set representation of Contains object. (cid:44) 3. Solution Approach: the set being tested for containment. - The simplest and most correct solution is to modify as_set() to return self.args[1], which is (cid:44) - This matches the mathematical interpretation that Contains(x, S) means \"x is in S\", so the set (cid:44) - No other evaluation or special cases are needed since the set being contained in is the only (cid:44) meaningful set representation of Contains object. representation is just S. 4. Code Localization: - The issue is in sympy/sets/contains.py in the Contains class. - The problematic method is as_set() which currently raises NotImplementedError. - The fix is to modify this method to return self.args[1]."
        },
        {
            "title": "Evolved Patch",
            "content": "diff --git a/sympy/sets/contains.py b/sympy/sets/contains.py index 0b1e933..e732791 100644 --- a/sympy/sets/contains.py +++ b/sympy/sets/contains.py @@ -45,4 +45,6 @@ class Contains(BooleanFunction): def as_set(self): - + + raise NotImplementedError() # Contains(x, S) means \"x is in S\", so as_set returns return self.args[1] B.3 Type 3: Prior patches are all correct Problem Statement _pytest.capture.EncodedFile mode should not include `b` (binary) <!-- Thanks for submitting an issue! Here's quick checklist for what to provide: --> - [x] detailed description of the bug or suggestion Exception when youtube-dl logs to pytest captured output. Youtube-dl looks for `b` in `out.mode` to decide whether to writes `bytes` or `str`. `_pytest.capture.EncodedFile` incorrectly advertises (cid:44) `rb+`, the mode of the underlying stream. Its `write()` method raises an exception when passed (cid:44) `bytes`. (cid:44) ``` (pytest-issue-ve3) 01:11:48:nlevitt@Internets-Air-2:/tmp$ py.test test.py ============================================================================== test session starts (cid:44) platform darwin -- Python 3.7.3, pytest-4.5.0, py-1.8.0, pluggy-0.11.0 rootdir: /private/tmp collected 1 item =============================================================================== test.py (cid:44) [100%] ==================================================================================== FAILURES ==================================================================================== (cid:44) ____________________________________________________________________________________ test_foo ____________________________________________________________________________________ (cid:44) 23 def test_foo(): > youtube_dl.YoutubeDL().extract_info('http://example.com/') test.py:4: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ (cid:44) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/YoutubeDL.py:796: in extract_info _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ie_result = ie.extract(url) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/extractor/common.py:529: in extract ie_result = self._real_extract(url) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/extractor/generic.py:2245: in _real_extract self.to_screen('%s: Requesting header' % video_id) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/extractor/common.py:913: in to_screen self._downloader.to_screen('[%s] %s' % (self.IE_NAME, msg)) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/YoutubeDL.py:502: in to_screen return self.to_stdout(message, skip_eol, check_quiet=True) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/YoutubeDL.py:516: in to_stdout self._write_string(output, self._screen_file) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/YoutubeDL.py:505: in _write_string write_string(s, out=out, encoding=self.params.get('encoding')) pytest-issue-ve3/lib/python3.7/site-packages/youtube_dl/utils.py:1496: in write_string out.write(byt) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ (cid:44) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <_pytest.capture.EncodedFile object at 0x10df124a8>, obj = b'[generic] example: Requesting (cid:44) headern' def write(self, obj): if isinstance(obj, six.text_type): obj = obj.encode(self.encoding, \"replace\") elif _PY3: raise TypeError( > \"write() argument must be str, not {}\".format(type(obj).__name__) ) TypeError: write() argument must be str, not bytes pytest-issue-ve3/lib/python3.7/site-packages/_pytest/capture.py:437: TypeError ============================================================================ 1 failed in 2.74 seconds ============================================================================ (cid:44) ``` Version 1.3.0 19.1.0 - [x] output of `pip list` from the virtual environment you are using ``` Package -------------- --------- atomicwrites attrs more-itertools 7.0.0 pip pluggy py pytest setuptools six wcwidth wheel youtube-dl ``` 19.1.1 0.11.0 1.8.0 4.5.0 41.0.1 1.12.0 0.1.7 0.33.4 2019.5. - [x] pytest and operating system versions ``` This is pytest version 4.5.0, imported from (cid:44) ``` /private/tmp/pytest-issue-ve3/lib/python3.7/site-packages/pytest.py ``` macOS 10.14.4 (18E226) ``` - [x] minimal example if possible ``` pip install pytest youtube-dl py.test test.py ``` test.py: 24 ``` import youtube_dl def test_foo(): youtube_dl.YoutubeDL().extract_info('http://example.com/') ``` Conditional patch 1 (Correct) diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..aa57c0d 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -448,6 +448,17 @@ class EncodedFile(object): return repr(self.buffer) def __getattr__(self, name): + + + + + + + + + + + # when code inspects the mode attribute (like youtube-dl does) # we need to hide the 'b' flag from it - we act as text stream # even though we write to bytes buffer if name == \"mode\": mode = getattr(self.buffer, name, None) if mode and \"b\" in mode: # return copy of the mode without 'b' # this makes e.g. youtube-dl think this is text stream # and write str instead of bytes return mode.replace(\"b\", \"\") # fall back to regular attribute access return getattr(object.__getattribute__(self, \"buffer\"), name) Conditional patch 2 (Correct) diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..01e03c8 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -448,6 +448,20 @@ class EncodedFile(object): return repr(self.buffer) def __getattr__(self, name): + + + + + + + + + + + + + + # youtube-dl specifically looks at out.mode to decide whether to # write bytes or str. Since we're text interface over bytes # interface, we should make it look like we're text. # https://github.com/ytdl-org/youtube-dl/blob/2019.05.11/youtube_dl/utils.py#L1496 # See: https://github.com/pytest-dev/pytest/issues/5691 if name == \"mode\": mode = getattr(self.buffer, name, None) if mode and \"b\" in mode: # remove the 'b' from the mode, but otherwise leave it # unchanged. # to us when in reality we'll encode it to bytes for the # underlying buffer. mode = \"\".join(ch for ch in mode if ch != \"b\") This makes youtube-dl think it can write text return mode return getattr(object.__getattribute__(self, \"buffer\"), name) Conditional patch 3 (Correct) diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..14dfb3c 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -447,6 +447,21 @@ class EncodedFile(object): \"\"\"Ensure that file.name is string.\"\"\" return repr(self.buffer) + + + + + + + + + @property def mode(self): \"\"\" Get the mode of the underlying buffer but remove 'b' from it. This is needed because some code (like youtube-dl) checks for 'b' in mode to decide whether to write bytes or str. Since this is an encoded (text) file, we need to hide that it's wrapping binary stream. \"\"\" 25 + + + + + + mode = getattr(self.buffer, \"mode\", None) if mode and \"b\" in mode: # remove 'b' but keep other flags like '+' or 'r' mode = \"\".join(c for in mode if != \"b\") return mode def __getattr__(self, name): return getattr(object.__getattribute__(self, \"buffer\"), name) Conditional patch 4 (Correct) diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..df7fb36 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -447,6 +447,22 @@ class EncodedFile(object): \"\"\"Ensure that file.name is string.\"\"\" return repr(self.buffer) + + + + + + + + + + + + + + + + @property def mode(self): \"\"\"Lie about our mode - remove 'b' if present. Some libraries (like youtube-dl) inspect the mode to decide whether to write bytes or str to the stream. Since we handle that in .write() and .writelines() already, we should pretend to be text stream even if our underlying buffer is byte stream. \"\"\" # just in case the buffer doesn't even have mode... mode = getattr(self.buffer, \"mode\", \"\") # ... and remove the 'b' mode = mode.replace(\"b\", \"\") return mode def __getattr__(self, name): return getattr(object.__getattribute__(self, \"buffer\"), name) Conditional patch 5 (Correct) diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..f94e9dc 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -447,6 +447,14 @@ class EncodedFile(object): \"\"\"Ensure that file.name is string.\"\"\" return repr(self.buffer) + + + + + + + + @property def mode(self): # youtube-dl checks for 'b' in mode to decide whether to write bytes or str. # since we are text interface (we encode strs to bytes), we should not # appear as binary stream, regardless of the underlying buffer's mode. # https://github.com/pytest-dev/pytest/issues/5295 return self.buffer.mode.replace(\"b\", \"\") def __getattr__(self, name): return getattr(object.__getattribute__(self, \"buffer\"), name)"
        },
        {
            "title": "Thinking Process",
            "content": "1. Peer Patch Review: - Patch 1: - Correctly identifies the need to modify the mode attribute when accessed - Uses __getattr__ to intercept the mode attribute and remove 'b' - Simple and effective solution that matches the issue requirements - Correctly maintains all other attribute behavior - Patch 2: - Similar to patch 1 but uses different string manipulation method - Also correctly intercepts the mode attribute and removes 'b' - Works just as well as patch 1 but with slightly different implementation - Patch 3: - Takes different approach by adding mode property directly - This is cleaner solution since it explicitly defines the behavior - Correctly handles the case where the buffer doesn't have mode - Maintains all other attribute behavior through __getattr__ - Patch 4: - Similar to patch 3 but with more detailed docstring - Also adds the mode property to remove 'b' from the underlying mode - Correctly handles the case where the buffer doesn't have mode - Maintains all other attribute behavior through __getattr__ - Patch 5: - Simplest solution that just adds mode property - Directly returns the underlying mode with 'b' removed - Doesn't handle the case where the buffer might not have mode attribute - Still solves the core issue but is less robust than patches 32. Issue Analysis: - The problem occurs when youtube-dl checks the mode of the captured output stream - It sees 'b' in the mode and tries to write bytes when the EncodedFile expects str - The solution needs to make the EncodedFile appear as text stream (no 'b' in mode) - While still maintaining all other file-like behavior from the underlying buffer 3. Solution Approach: - The key is to intercept access to the 'mode' attribute of the underlying buffer - When 'mode' is accessed, we should return the mode string with 'b' removed - All other attribute access should continue to work normally from the buffer - This matches exactly what youtube-dl expects while maintaining proper functionality 4. Code Localization: - The issue is in the EncodedFile class in src/_pytest/capture.py - Need to modify the class to add mode property that removes 'b' from the underlying buffer's mode - All other behavior should remain unchanged"
        },
        {
            "title": "Evolved Patch",
            "content": "diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py index 6b2d076..d3b3473 100644 --- a/src/_pytest/capture.py +++ b/src/_pytest/capture.py @@ -447,6 +447,23 @@ class EncodedFile(object): \"\"\"Ensure that file.name is string.\"\"\" return repr(self.buffer) + + + + + + + + + + + + + + + + + @property def mode(self): \"\"\"When code inspects our mode (like youtube-dl does), lie and remove the 'b' if present. stream and write str instead of bytes, which we can then encode to bytes for the underlying buffer. This makes youtube-dl think we're text Note: we don't need to handle the case where the buffer doesn't have mode attribute because in that case, normal attribute access will raise AttributeError like it should. \"\"\" mode = self.buffer.mode if \"b\" in mode: # remove the 'b' but otherwise preserve the mode mode = mode.replace(\"b\", \"\") return mode def __getattr__(self, name): return getattr(object.__getattribute__(self, \"buffer\"), name) Problem Statement DateFormat.Y() is not zero-padded. Description The specifier for django.utils.dateformat.DateFormat is supposed to always return four-digit (cid:44) year padded with zeros. This doesn't seem to be the case for year < 1000. 27 Conditional patch 1 (Correct) diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..823e893 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,9 @@ class DateFormat(TimeFormat): def Y(self): - + + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We want to zero-pad the year (this is only useful for year < 1000) # so that we get 4 digits. return '%04d' % self.data.year def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\" Conditional patch 2 (Correct) diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..15090a4 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,9 @@ class DateFormat(TimeFormat): def Y(self): - + + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We want to zero-pad the year, so can't just return the number -- # hence the \"%04d\" dance. return '%04d' % self.data.year def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\" Conditional patch 3 (Correct) diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..abab54f 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,8 @@ class DateFormat(TimeFormat): def Y(self): - + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We want four-digit year; 0-padding if necessary return '%04d' % self.data.year def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\" Conditional patch 4 (Correct) diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..ecf7449 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,11 @@ class DateFormat(TimeFormat): def Y(self): - + + + + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We hardcode 4 here instead of using settings value or # self.data.year.__len__() because we want to always return # 4-digit year for consistency with PHP's behavior. # See also: https://bugs.php.net/bug.php?id=67077 return '%04d' % self.data.year def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\" 28 Conditional patch 5 (Correct) diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..acd0ae0 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,9 @@ class DateFormat(TimeFormat): def Y(self): - + + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We want to always return 4-digit year, zero-padded if necessary # This ensures years < 1000 are properly formatted return '%04d' % self.data.year def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\""
        },
        {
            "title": "Thinking Process",
            "content": "1. Peer Patch Review: - Patch 1: - Correctly identifies the need to zero-pad the year using '%04d' - The comment explains the issue well for years < 1000 - Simple and correct solution that matches the issue requirements - Patch 2: - Similar to Patch 1 but with slightly different comment wording - Also correctly uses '%04d' to zero-pad the year - Correct solution that addresses the issue - Patch 3: - Uses '%04d' like the other patches but with simpler comment - Correct solution that matches the issue requirements - Patch 4: - Adds more detailed comment explaining the PHP compatibility - While the solution is correct, the additional comment about PHP behavior isn't necessary for (cid:44) - Still correct solution to the issue this Django-specific issue - Patch 5: - Simple and correct solution using '%04d' with clear comment - Correctly addresses the issue requirements 2. Issue Analysis: - The issue states that DateFormat.Y() should always return 4-digit year padded with zeros - Currently it returns the raw year number which doesn't zero-pad for years < 1000 - The fix requires modifying the Y() method to format the year as 4-digit string with zero-padding - This matches the behavior described in the method's docstring which says \"4 digits\" 3. Solution Approach: - The solution is to modify the Y() method to use string formatting with '%04d' - This will ensure the year is always returned as 4-character string with leading zeros if needed - The change is simple and focused on just the Y() method where the issue occurs 4. Code Localization: - The issue is in django/utils/dateformat.py in the DateFormat class - The problematic method is Y() which currently just returns self.data.year - Need to modify it to return '%04d' % self.data.year instead"
        },
        {
            "title": "Evolved Patch",
            "content": "diff --git a/django/utils/dateformat.py b/django/utils/dateformat.py index 38cb1c1..d0e66c3 100644 --- a/django/utils/dateformat.py +++ b/django/utils/dateformat.py @@ -314,7 +314,9 @@ class DateFormat(TimeFormat): def Y(self): - + + + \"Year, 4 digits; e.g. '1999'\" return self.data.year # We want to zero-pad the year to ensure we always get # 4 digits, even for years < 1000. return '%04d' % self.data.year 29 def z(self): \"\"\"Day of the year, i.e. 1 to 366.\"\"\" Scaffold of Satori-SWE We adopt pipeline-based scaffold consisting of two main components: retriever and code editing model. The retriever takes either the GitHub repository structure or the contents of individual code files as input and identifies the files most relevant to the given issue. Once retrieval is complete, the full content of the selected code files is passed to the code editing model. The code editing model receives both the issue description and the retrieved code content as input and generates patch to resolve the issue. Additionally, there is an optional verifier component, which can be used to select the best patch from large pool of candidate samples. We describe each component in detail below. C.1 Retriever Our retriever is entirely LLM-based and consists of two components: retrieval model and retrieval reward model. Figure 10: Retrieval Pipeline. Given the repositorys file structure, the retrieval model first selects the top-5 candidate files. These candidates are then re-scored by the retrieval reward model based on file content, and the top-ranked (Top-1) file is returned as the final result. Retrieval Model The first stage of our retriever uses retrieval model to identify the top 5 most relevant files based on the repository structure and the GitHub issue description. We adopt the same format as Agentless [33] to represent the repositorys file structure. Given this representation and the issue description, the retrieval model performs reasoning process and outputs five file paths from the repository. The model is trained using combination of small-scale supervised fine-tuning (SFT) and large-scale reinforcement learning (RL), see Appendix D.3 for details. Retrieval Reward Model The retrieval reward model is designed to refine retrieval results in more fine-grained manner. After the initial top-5 files are retrieved by the retrieval model, the reward model evaluates each one by considering both the files code content and the issue description. It then outputs relevance score for each file, and the file with the highest score is selected as the final target for code editing. The retrieval reward model is classifier-style LLM trained with binary classification objective, see Appendix D.4 for training details. C.2 Code Editing Model The code editing model receives prompt formed by concatenating the issue statement with the code content of the retrieved target file. It performs iterative sampling to enable self-evolution during generation. In the first iteration, given the issue statement and code context, the model generates five diverse responses, each corresponding to different patch candidate. These five patch candidates are then appended to the input as conditional prompt for the next iteration of generation. This iterative process allows the model to progressively refine its outputs. 30 As discussed in Section 4, the code editing model is trained using combination of small-scale SFT and large-scale RL. Additional training details are provided in Appendix D.5. C.3 Verifier As discussed in Section 5.2, our code editing model demonstrates the ability to self-evolve by iteratively refining its own generations. While this process improves the quality of patch samples, incorporating external verifiers to select the optimal patch can further boost performance. For software engineering tasks, we consider two primary types of verifiers: an LLM-based reward model and unit tests. Code Editing Reward Model The code editing reward model is designed to select the best patch from pool of candidates. It takes as input the retrieved files code content, the issue description, and patch candidate in git diff format. The model then outputs score indicating the quality of the patch. This reward model is implemented as classifier-based LLM trained with binary classification objective (see Appendix D.6 for details). Unit Tests Unit tests consist of two components: (1) Reproduction tests, which validate whether the original GitHub issue can be reproduced and resolved by the patch; (2) Regression tests, which check whether the patch preserves the existing functionality of the codebase. To construct the regression tests, we extract existing test files from the repository using the Agentless [33] pipeline. For the reproduction tests, we use trained test generation model that takes the issue description as input and generates tests aimed at reproducing the issue. This reproduction test generator is trained using supervised fine-tuning (see Appendix D.7). For each instance, we sample 100 reproduction tests and retain 5 valid patches to serve as reproduction tests. Hybrid Verifiers We combine multiple verifiers to select the most promising patch candidates. The selection process is as follows: (1) Regression tests are applied first. Any patch that fails is discarded; (2) Reproduction tests are then executed on the remaining patches. Candidates are ranked based on how many of the five tests they pass. (3) The top-k (k = 2) unique patches are retained per instance. (4) If no patch passes both regression and reproduction tests, we fall back to using all generated candidates without filtering. (5) Finally, among the remaining patches, the code editing reward model is used to select the candidate with the highest reward score for submission."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Dataset Collection Our primary training data is sourced from SWE-Fixer and SWE-Gym. To ensure data quality, we apply comprehensive filtering and deduplication process. Specifically, we discard instances that meet any of the following criteria: (1) extremely short or excessively long issue statements; (2) multimodal or non-text content (e.g., images, videos, LaTeX); (3) presence of unrelated external links; (4) inclusion of commit hashes; (5) patches requiring modifications to more than three files. After applying these filters, we obtain 29,404 high-quality training instances, which we use to train both the retrieval and code editing models. D.2 Training Pipeline and Hardware Both the retrieval and code editing models are trained in two stages: (1) small-scale supervised fine-tuning (SFT) for warm-up, and (2) large-scale reinforcement learning (RL) for self-improvement. The SFT trajectories are generated via chain-of-thought prompting using Deepseek-V3-0324. We adopt the Qwen2.5-Coder-32B-Instruct model [12] as the base model for all components due to its strong code reasoning capabilities. We utilize OpenRLHF [10] as the training framework for SFT, and use VERL [27] as the training framework for RL. All training runs are conducted on NVIDIA H100 GPUs with 80 GB of memory. For evaluation and model inference, we serve models using the sglang framework4, employing tensor parallelism with parallel size of 8 on NVIDIA H100 GPUs. 4https://github.com/sgl-project/sglang D.3 Retrieval Model Dataset Construction To ensure cost-efficient synthetic data generation, we randomly select 1,470 instances (5% of the full dataset) as the small-scale SFT dataset for training the retrieval model. To train the model to perform step-by-step reasoning and generate relevant file paths conditioned on both the issue statement and the repository structure, we require training data that includes both intermediate reasoning and final retrieved files. However, the raw data only provides the issue descriptions and the ground-truth retrieved files (extracted from the final patch), without any intermediate reasoning. To address this, we use Deepseek-V3-0324 with custom retrieval model prompt (see Appendix E) and apply rejection sampling to collect high-quality chain-of-thought (CoT) reasoning traces. Specifically, we check whether the models top-5 retrieved files include the ground-truth retrieval files. If so, we retain the response as part of our synthetic SFT data. For each selected instance, we generate one response via greedy decoding and three additional responses using random sampling (temperature = 0.7), as long as they include the correct retrieval files. This results in four responses per instance. For RL training, we use the remaining 27,598 instances (95% of the dataset), filtering out prompts whose total sequence length exceeds 16,384 tokens. The RL dataset consists of prompts (issue statement + repo structure) as input and the corresponding ground-truth retrieval files as the final answer, without requiring intermediate reasoning. Supervised Fine-Tuning We perform supervised fine-tuning (SFT) on the Qwen2.5-Coder-32BInstruct model using the synthetic SFT dataset described above. The prompt template used for training is identical to the one used to construct the synthetic data (see the retrieval model prompt in Appendix E). We train the model using cosine learning rate scheduler with an initial learning rate of 1e-5. The model is fine-tuned for one epoch with batch size of 128 and maximum sequence length of 32,768 tokens. Reinforcement Learning To further push the limit of the models retrieval performance, we apply large-scale reinforcement learning (RL) stage after the SFT stage. After SFT, the model has learned to reason step-by-step and generates set of candidate retrieval files, denoted as = {y1, y2, . . . , yk}, where = 5. Given the ground-truth set of target files F, we define the reward as the proportion of correctly retrieved files: Reward = F Given the prompt in the RL dataset, we let the model generate response and self-improve through this reward signal. We use the REINFORCE++ [11] algorithm with fixed learning rate of 1e-6 for the actor model. During training, we sample 8 rollouts per prompt. The training batch size is 64, and the rollout batch size is 256. The model is trained for 3 epochs, with maximum prompt length of 16k tokens and generation length of 4k tokens. Additional hyperparameters include KL divergence coefficient of 0.0, entropy coefficient of 0.001 and sampling temperature of 1.0. D.4 Retrieval Reward Model To train reward model capable of reliably identifying the most relevant code files for modification, we construct reward model dataset derived from our main dataset. The final reward model dataset consists of 112,378 samples corresponding to 25,363 unique instances. For each instance, the prompt is constructed using the retrieval reward model prompt template (see Appendix E), incorporating the issue statement along with the code content of each of the top-5 candidate retrieval files. Each data point is labeled with binary value 0, 1, indicating whether the provided code content belongs to the ground-truth retrieval files. The model is initialized from the Qwen2.5-Coder-32B-Instruct and trained as binary classifier using cross-entropy loss. Training is conducted with batch size of 128, learning rate of 5e-6, and maximum sequence length of 32,768 tokens, over two epochs. 32 D.5 Code Editing Model Dataset construction As described in Section 4, our code editing model is trained in two stages using supervised fine-tuning (SFT)classical SFT and mutation SFTfollowed by large-scale reinforcement learning (RL). We randomly select 1,470 instances (5%) from the full dataset for the classical SFT set and separate 1,470 instances (5%) for the mutation SFT set. These two subsets are kept disjoint to ensure that the model learns to self-refine without direct exposure to the ground-truth solutions. For RL training, we use the remaining 22,102 instances (90% of the dataset), filtering out any prompts with sequence lengths exceeding 16,384 tokens. The RL dataset contains only the prompt (issue + code context) as input and the corresponding ground-truth patch as the output. To synthesize the reasoning chain-of-thought (CoT) for classical SFT, we prompt Deepseek-V3-0324. Unlike the retrieval setting, we do not use rejection sampling, as Deepseek-V3-0324 often fails to generate the correct patch even after multiple samples. Instead, we adopt more efficient approach by designing role-playing prompt that provides the model access to the ground-truth patch and instructs it to explain the reasoning process behind it (see the Generating Reasoning CoT for Code Editing Model (Classical SFT) prompt in Appendix E). This ensures that the generated reasoning is both accurate and reflects an independent thought process. We then synthesize the classical SFT dataset using the Code Editing Model (Classical SFT) prompt template in Appendix E. We first fine-tune the base model on the classical SFT dataset. This fine-tuned model is then used to generate five random patch candidates per instance with sampling temperature of 1.0. These candidate patches are used to construct the mutation SFT dataset. For each instance, we prompt Deepseek-V3-0324 with: the issue statement, the content of the target file, the five candidate patches, and the ground-truth patch. Using the Generating Reasoning CoT for Code Editing Model (Mutation SFT) prompt (see Appendix E), the model is instructed to review each patch, critique their strengths and weaknesses, and propose an improved solution. We then extract the reasoning process and synthesize the mutation SFT dataset using the Code Editing Model (Mutation SFT) prompt template. Supervised Fine-Tuning We perform supervised fine-tuning (SFT) on the Qwen2.5-Coder-32BInstruct model using the synthetic SFT datasets described above. The prompt templates used for training are the same as those used to construct the two-stage SFT datasets (classical and mutation SFT). We employ cosine learning rate scheduler with an initial learning rate of 1e-5. Training is conducted for one epoch, with batch size of 128 and maximum sequence length of 32,768 tokens. Reinforcement Learning We fine-tune the mutation SFT model on the full dataset using REINFORCE++ [11] with the following reward function: = R(x, y) (cid:124) (cid:123)(cid:122) (cid:125) Bonus + R(x, y) (cid:88) R(x, yi) (cid:124) i=1 (cid:123)(cid:122) Potential (cid:125) λ (y) , (cid:124) (cid:123)(cid:122) (cid:125) Format (6) where each term is defined as follows: R(x, y) (Bonus): Encourages the model to produce high-reward outputs. Although similar in effect to the potential term, including this bonus stabilizes training and consistently improves the models average reward. R(x, y) (cid:80)K i=1 R(x, yi) (Potential): Measures the improvement of the current patch over the average reward of the conditioning patches yi. See Section 4.3 for details. (y) (Format): Penalizes outputs that violate format or syntax constraints. It consists of: String matching: Rewards outputs that closely match the ground-truth patch using sequence similarity, following Wei et al. [31]. Syntax check: Ensures the output can be parsed into the expected search-replace format, passes Pythons ast syntax check, and satisfies flake8 static analysis. If any check fails, the format reward is set to zero. The RL model is trained on mix of data with and without conditioning examples. Conditioning examples are generated not only using the classical SFT model but also using checkpoint of an RL-trained model at the first epoch. 33 As for implementation, we use REINFORCE++ [11] algorithm with fixed learning rate of 1e-6 for the actor model. During training, we sample 8 rollouts per prompt. The training batch size is 64, and the rollout batch size is 256. The model is trained for only 1 epochs, with maximum prompt length of 16k tokens and generation length of 8k tokens. Additional hyperparameters include KL divergence coefficient of 0.0, entropy coefficient of 0.001 and sampling temperature of 1.0. D.6 Code Editing Reward Model The code editing reward model is designed to provide more accurate reward signal, addressing the limitations of using simple string-matching scores. The training setup is similar to that of the retrieval reward model (see Appendix D.4), with the main difference being in the data collection process. We construct the reward model training dataset using data collected from nebius/SWE-agenttrajectories and nebius/SWE-bench-extra, resulting in 56,797 samples across 1,889 unique instances. For each instance, the prompt is constructed using the code editing reward model prompt template (see Appendix E), and includes the issue statement, the code content of the target file to be modified, and candidate patch. Each sample is labeled with binary value 0, 1, indicating whether the candidate patch successfully resolves the issue. The model is trained as binary classifier using the same training settings as the retrieval reward model. D.7 Reproduction Test Generator Following similar approach to that used for code editing, we generate intermediate reasoning steps for reproduction test generation using the Deepseek-V3-0324 model. Given the issue description and the corresponding ground-truth test patch, the model is prompted to produce response that includes the reasoning behind constructing valid test in chain-of-thought format. To support automated verification, we follow the strategy used in Agentless [33], employing test script template that prints clear diagnostic messages indicating whether the issue has been successfully reproduced or resolved. Specifically, If the test successfully triggers the target error (e.g., raises an AssertionError), it prints Issue reproduced; If the test completes without errors, it prints Issue resolved. An example template for this diagnostic test script is shown below:"
        },
        {
            "title": "Reproduction Test Template",
            "content": "def test_<meaningful_name>() -> None: try: # Minimal code that triggers the bug ... except AssertionError: print(\"Issue reproduced\") return except Exception: print(\"Other issues\") return print(\"Issue resolved\") return if __name__ == \"__main__\": test_<meaningful_name>() Starting from our filtered dataset, we generate one response per instance using greedy decoding and three additional responses via sampling with temperature of 0.7. These synthetic examples are then used to fine-tune the Qwen2.5-Coder-32B-Instruct model over three epochs, resulting in our reproduction test generation model. The prompt templates used for generating intermediate reasoning and for supervised fine-tuning are provided in Appendix E."
        },
        {
            "title": "E Prompt Template",
            "content": "Prompt Template Retrieval Model Please look through the following GitHub problem description and Repository structure. Determine the files most likely to be edited to fix the problem. Identify 5 most important files. ### GitHub Problem Description ### {problem_statement} ### Repository Structure ### {structure} ### Format Instruction ### 1. Enclose reasoning process within `<think>...</think>`. 2. Please only provide the full path and return 5 most important files. Always return exactly 5 files, Do Not output less than 5 or more than 5 files. 3. The returned files should be separated by new lines ordered by most to least important. Wrap all files together within `<file>...</file>`. 4. Do not include any explanations after `</think>`, only provide the file path within `<file>...</file>`. ### Examples ### <think> 1. Analyze the issue... 2. Check the files in provided repository structure for relevance... 3. Confirm that the issue might be most relevant to 5 relevant files... </think> <file> file1.py file2.py file3.py file4.py file5.py </file> --- Please provide your response below. Prompt Template Retrieval Reward Model You are an expert software engineer and seasoned code reviewer, specializing in bug localization and code (cid:44) optimization. You will be presented with GitHub issue and source code file. Your task is to decide if the code file is relevant to the issue. # Issue Statement {problem_statement} # File to be Modified {file_content} Prompt Template Generating Reasoning CoT for Code Editing Model (Classical SFT) You are student striving to become an expert software engineer and seasoned code reviewer, specializing in bug (cid:44) (cid:44) (cid:44) (cid:44) bug fixes. localization and code optimization within real-world code repositories. Your strengths lie in understanding complex codebase structures and precisely identifying and modifying the relevant parts of the code to resolve issues. You also excel at articulating your reasoning process in coherent, step-by-step manner that leads to efficient and correct You are now taking an exam to evaluate your capabilities. You will be provided with codebase and an issue (cid:44) (cid:44) description. Your task is to simulate complete reasoning processstep-by-stepas if solving the issue from scratch, followed by the code modifications to resolve the issue. To evaluate your correctness, an oracle code modification patch will also be provided. You must ensure that your (cid:44) (cid:44) (cid:44) final code modifications MATCH the oracle patch EXACTLY. However, your reasoning process must appear fully self-derived and **must NOT reference, suggest awareness of, or appear to be influenced by** the oracle patch. You must solve the problem as if you are unaware of the oracle solution. --- # Issue Statement {problem_statement} --- # Files to be Modified 35 Below are some code files that might be relevant to the issue above. One or more of these files may contain bugs. {file_content} --- # Oracle Code Modification Patch (For Evaluation Only): {oracle_patch} --- # Reasoning Guidelines Your reasoning process should generally follow these steps, with flexibility to adjust as needed for clarity and (cid:44) accuracy: 1. **Issue Analysis**: Start by thoroughly analyzing the issue. Explain what the problem is, why it matters, and what (cid:44) the intended behavior should be. Identify the key goals and constraints that must be addressed in your solution. 2. **Task Decomposition**: Break down the issue into smaller, manageable sub-tasks. Describe the purpose of each (cid:44) sub-task and how it contributes to solving the overall problem. 3. **Code Localization and Editing**: For each sub-task: - Identify relevant code snippets by file path and code location. - Explain how each snippet relates to the sub-task. - Describe how the code should be changed and justify your reasoning. - After thorough explanation, provide the corresponding edited code. Your final output must precisely match the oracle patch, but your thinking must remain fully grounded in the issue (cid:44) description and provided code files. --- based solely on the issue and code. Do not reference or imply knowledge of the oracle patch. # General Requirements 1. **Independent and Evidence-Based Reasoning**: Your reasoning must be constructed as if independently derived, (cid:44) 2. **Clarity and Justification**: Ensure that each reasoning step is clear, well-justified, and easy to follow. 3. **Comprehensiveness with Focus**: Address all relevant components of the issue while remaining concise and (cid:44) 4. **Faithful Final Output**: Your final code output must match the oracle patch exactly. 5. **Strict Neutrality**: Treat the oracle patch purely as grading mechanism. Any hint of knowing the patch in your (cid:44) (cid:44) reasoning (e.g., based on the oracle, we can verify, or as we see in the patch) will result in exam failure. focused. --- # Response Format 1. The reasoning process should be enclosed in <think> ... </think>. 2. The final oracle patch should be output in standalone Python code block *after* the </think> block. 3. Do not include any commentary or justification after the </think> block. Example: <think> 1. Analyze the issue... 2. Locate the relevant code... 3. Apply necessary changes... </think> ```python # Final patch here (must match the oracle patch exactly) ``` --- Please provide your response. Prompt Template Code Editing Model (Classical SFT) processstep-by-stepas if solving the issue from scratch, followed by the code modifications to resolve the issue. optimization within real-world code repositories. Your strengths lie in understanding complex codebase structures and precisely identifying and modifying the relevant parts of the code to resolve issues. You also excel at articulating your reasoning process in coherent, step-by-step manner that leads to efficient and correct bug fixes. You are an expert software engineer and seasoned code reviewer, specializing in bug localization and code (cid:44) (cid:44) (cid:44) (cid:44) You will be provided with codebase and an issue description. Your task is to simulate complete reasoning (cid:44) (cid:44) --- # Issue Statement {problem_statement} --- # Files to be Modified Below are some code files that might be relevant to the issue above. One or more of these files may contain bugs. {file_content} --- # Reasoning Guidelines Your reasoning process should generally follow these steps, with flexibility to adjust as needed for clarity and (cid:44) 1. **Issue Analysis**: Start by thoroughly analyzing the issue. Explain what the problem is, why it matters, and what accuracy: 36 the intended behavior should be. Identify the key goals and constraints that must be addressed in your solution. 2. **Task Decomposition**: Break down the issue into smaller, manageable sub-tasks. Describe the purpose of each (cid:44) 3. **Code Localization and Editing**: For each sub-task: sub-task and how it contributes to solving the overall problem. - Identify relevant code snippets by file path and code location. - Explain how each snippet relates to the sub-task. - Describe how the code should be changed and justify your reasoning. - After thorough explanation, provide the corresponding edited code. Justify the exclusion of any sections that are not relevant. solution to infer and implement the necessary code modifications. provided issue and code without inferring information not explicitly stated. --- # General Requirements 1. **Clear and Evidence-Based Reasoning**: Provide clear and precise reasoning for each step, strictly based on the (cid:44) 2. **Comprehensive and Concise**: Address all relevant aspects of the issue comprehensively while being concise. (cid:44) 3. **Detailed Guidance**: Ensure the reasoning steps are detailed enough to allow someone unfamiliar with the (cid:44) --- # Response Format 1. The reasoning process should be enclosed in <think> ... </think>. 2. The final patch should be output in standalone Python code block *after* the </think> block. 3. Do not include any commentary or justification after the </think> block. --- # Patch Format Please generate *SEARCH/REPLACE* edits to fix the issue. Every *SEARCH/REPLACE* edit must use this format: 1. The file path 2. The start of search block: <<<<<<< SEARCH 3. contiguous chunk of lines to search for in the existing source code 4. The dividing line: ======= 5. The lines to replace into the source code 6. The end of the replace block: >>>>>>> REPLACE If, in `Files to be Modified` part, there are multiple files or multiple locations in single file require changes. You should provide separate patches for each modification, clearly indicating the file name and the specific location (cid:44) Please note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. For example, if you would like to add the (cid:44) (cid:44) # Example Response <think> 1. Analyze the issue... 2. Locate the relevant code... 3. Apply necessary changes... </think> ```python ### mathweb/flask/app.py <<<<<<< SEARCH from flask import Flask ======= import math from flask import Flask >>>>>>> REPLACE ``` ```python ### mathweb/utils/calc.py <<<<<<< SEARCH def calculate_area(radius): line ' wrap the *SEARCH/REPLACE* edit in blocks ```python...``` print(x)', you must fully write that out, with all those spaces before the code! And remember to of the modification. return 3.14 * radius * radius ======= def calculate_area(radius): return math.pi * radius ** >>>>>>> REPLACE ``` --- Please provide your response below. Prompt Template Generating Reasoning CoT for Code Editing Model (Mutation SFT) You are student collaborating with group of peers in software engineering lab, working together to diagnose and (cid:44) (cid:44) (cid:44) fix bugs in real-world code repositories. You specialize in bug localization and code optimization, with particular talent for critically evaluating others' patches and synthesizing high-quality, precise solutions from collaborative efforts. You will be presented with GitHub issue, the relevant source code files, and several *candidate patches* submitted (cid:44) by your teammates. Your task is twofold: 1. **Patch Review**: Carefully evaluate each of the several candidate patches **individually**. Identify whether each patch resolves the issue correctly, partially, or incorrectly. If you identify any issues (e.g., logical errors, misunderstandings of the bug, overlooked edge cases, or incomplete fixes), explain them clearly and suggest what (cid:44) could be improved or corrected. Even if patch appears mostly correct, you should still analyze its strengths and limitations in detail. Treat this as collaborative peer-review process: constructive, technical, and focused on improving code quality. 2. **Patch Synthesis**: After analyzing all several candidate patches, synthesize your understanding to produce your (cid:44) **own final code patch** that fully resolves the issue. Your patch should: - Be grounded solely in the issue description and provided source code. - Be informed by your peer review, but not copy any one patch outright. - To evaluate your correctness, an oracle code modification patch will also be provided. You must ensure that your final code modifications MATCH the oracle patch EXACTLY. However, your reasoning process must appear fully (cid:44) self-derived and **must NOT reference, suggest awareness of, or appear to be influenced by** the oracle patch. (cid:44) You must solve the problem as if you are unaware of the oracle solution. (cid:44) --- # Issue Statement {problem_statement} --- # Files to be Modified Below are some code files that might be relevant to the issue above. One or more of these files may contain bugs. {file_content} --- # Candidate Patches (From Collaborators) Below are several proposed patches submitted by your teammates. You will evaluate them individually. {candidate_patches} --- # Oracle Code Modification Patch (For Evaluation Only): {target} --- # Reasoning and Review Guidelines Your response should be structured into two parts: ## Part 1: Peer Patch Review For each of the candidate patches: - Analyze the candidate patch's intent and correctness. - Identify what it does well, what it gets wrong (if anything), and how it could be improved. - Use precise references to the provided issue and source code files to justify your evaluation. - Avoid any implication that you know the correct answer or are using an external reference (including the (cid:44) oracle). ## Part 2: Final Patch Synthesis After completing all reviews: 1. **Issue Analysis**: Start by thoroughly analyzing the issue. Explain what the problem is, why it matters, and what (cid:44) the intended behavior should be. Identify the key goals and constraints that must be addressed in your solution. 2. **Task Decomposition**: Break down the issue into smaller, manageable sub-tasks. Describe the purpose of each (cid:44) sub-task and how it contributes to solving the overall problem. 3. **Code Localization and Editing**: For each sub-task: - Identify relevant code snippets by file path and code location. - Explain how each snippet relates to the sub-task. - Describe how the code should be changed and justify your reasoning. - Incorporate useful insights from the candidate patches you reviewed. Reuse good ideas that are correct and (cid:44) - After thorough explanation, provide the corresponding edited code. effective, but discard or correct those that contain flaws or misunderstandings. Your final output must precisely match the oracle patch, but your thinking must remain fully grounded in the issue (cid:44) description and provided code files. --- based solely on the issue and code. Do not reference or imply knowledge of the oracle patch. # General Requirements 1. **Independent and Evidence-Based Reasoning**: Your reasoning must be constructed as if independently derived, (cid:44) 2. **Clarity and Justification**: Ensure that each reasoning step is clear, well-justified, and easy to follow. 3. **Comprehensiveness with Focus**: Address all relevant components of the issue while remaining concise and (cid:44) 4. **Faithful Final Output**: Your final code output must match the oracle patch exactly. 5. **Strict Neutrality**: Treat the oracle patch purely as grading mechanism. Any hint of knowing the patch in your (cid:44) (cid:44) reasoning (e.g., based on the oracle, we can verify, or as we see in the patch) will result in exam failure. focused. --- # Response Format 1. The reasoning process should be enclosed in <think> ... </think>. 2. The final oracle patch should be output in standalone Python code block *after* the </think> block. 3. Do not include any commentary or justification after the </think> block. Example: <think> 1. Review of candidate patch: - Review of patch-1: ... - Review of patch-2: ... - ... 2. Analyze the issue by myself... 3. Locate the relevant code... 4. Apply necessary changes... </think> ```python # Final patch here (must match the oracle patch exactly) ``` --- Please provide your response. Prompt Template Code Editing Model (Mutation SFT) You are an expert software engineer and seasoned code reviewer, specializing in bug localization and code (cid:44) (cid:44) optimization, with particular talent for critically evaluating teammates' patches and synthesizing high-quality, precise solutions from collaborative efforts. You will be presented with GitHub issue, the relevant source code files, and five *candidate patches* submitted by (cid:44) your teammates. Your task is twofold: 1. **Patch Review**: Carefully evaluate each of the five candidate patches **individually**. Identify whether each (cid:44) (cid:44) (cid:44) patch resolves the issue correctly, partially, or incorrectly. If you identify any issues (e.g., logical errors, misunderstandings of the bug, overlooked edge cases, or incomplete fixes), explain them clearly and suggest what could be improved or corrected. Even if patch appears mostly correct, you should still analyze its strengths and limitations in detail. Treat (cid:44) this as collaborative peer-review process: constructive, technical, and focused on improving code quality. 2. **Patch Synthesis**: After analyzing all five candidate patches, synthesize your understanding to produce your (cid:44) **own final code patch** that fully resolves the issue. Your patch should: - Be grounded solely in the issue description and provided source code. - Be informed by your peer review, but not copy any one patch outright. --- # Issue Statement {problem_statement} --- # Files to be Modified Below are some code files that might be relevant to the issue above. One or more of these files may contain bugs. {file_content} --- # Candidate Patches (From Collaborators) Below are five proposed patches submitted by your teammates. You will evaluate them individually. {candidate_patches} --- # Reasoning and Review Guidelines Your response should be structured into two parts: ## Part 1: Peer Patch Review For each of the five candidate patches: - Analyze the candidate patch's intent and correctness. - Identify what it does well, what it gets wrong (if anything), and how it could be improved. - Use precise references to the provided issue and source code files to justify your evaluation. ## Part 2: Final Patch Synthesis After completing all five reviews, your reasoning process should generally follow these steps, with flexibility to (cid:44) adjust as needed for clarity and accuracy: 1. **Issue Analysis**: Start by thoroughly analyzing the issue. Explain what the problem is, why it matters, and what (cid:44) the intended behavior should be. Identify the key goals and constraints that must be addressed in your solution. 2. **Task Decomposition**: Break down the issue into smaller, manageable sub-tasks. Describe the purpose of each (cid:44) sub-task and how it contributes to solving the overall problem. 3. **Code Localization and Editing**: For each sub-task: - Identify relevant code snippets by file path and code location. - Explain how each snippet relates to the sub-task. - Describe how the code should be changed and justify your reasoning. - After thorough explanation, provide the corresponding edited code. --- # General Requirements 1. **Clear and Evidence-Based Reasoning**: Provide clear and precise reasoning for each step, strictly based on the (cid:44) 2. **Comprehensive and Concise**: Address all relevant aspects of the issue comprehensively while being concise. (cid:44) provided issue and code without inferring information not explicitly stated. Justify the exclusion of any sections that are not relevant. 39 3. **Detailed Guidance**: Ensure the reasoning steps are detailed enough to allow someone unfamiliar with the (cid:44) solution to infer and implement the necessary code modifications. --- # Response Format 1. The reasoning process should be enclosed in <think> ... </think>. 2. The final patch should be output in standalone Python code block *after* the </think> block. 3. Do not include any commentary or justification after the </think> block. --- # Patch Format Please generate *SEARCH/REPLACE* edits to fix the issue. Every *SEARCH/REPLACE* edit must use this format: 1. The file path 2. The start of search block: <<<<<<< SEARCH 3. contiguous chunk of lines to search for in the existing source code 4. The dividing line: ======= 5. The lines to replace into the source code 6. The end of the replace block: >>>>>>> REPLACE If, in `Files to be Modified` part, there are multiple files or multiple locations in single file require changes. You should provide separate patches for each modification, clearly indicating the file name and the specific (cid:44) location of the modification. (cid:44) Please note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. For example, if you would like to add the (cid:44) (cid:44) line ' wrap the *SEARCH/REPLACE* edit in blocks ```python...``` print(x)', you must fully write that out, with all those spaces before the code! And remember to # Example Response <think> 1. Review of candidate patch: - Review of patch-1: This patch attempts to fix by modifying function Y. However, it fails to consider Z... - Review of patch-2: ... - Review of patch-3: ... - Review of patch-4: ... - Review of patch-5: ... 2. Analyze the issue by myself... 3. Locate the relevant code... 4. Apply necessary changes... </think> ```python ### mathweb/flask/app.py <<<<<<< SEARCH from flask import Flask ======= import math from flask import Flask >>>>>>> REPLACE ``` ```python ### mathweb/utils/calc.py <<<<<<< SEARCH def calculate_area(radius): return 3.14 * radius * radius ======= def calculate_area(radius): return math.pi * radius ** 2 >>>>>>> REPLACE ``` --- Please provide your response below. Prompt Template Code Editing Reward Model % begin{lstlisting}[language=text,fontsize=tiny] You are an expert software engineer and seasoned code reviewer, specializing in code optimization within real-world code repositories. Your strengths lie in precisely identifying and modifying the relevant parts of the code to resolve issues. You will be provided with an issue description and an original code which has bugs. Your task is to write code modifications to resolve the issue. **Problem Statement:** {problem_statement} **Original Code:** {file_content}minted % end{lstlisting} 40 Prompt Template Generating Reasoning CoT for Reproduction Test SFT You are collaborating with peers in software-engineering lab to create reproduction tests for real-world bug (cid:44) reports. You are given three context blocks: --- BEGIN ISSUE (authoritative bug description) --- {problem_statement} --- END ISSUE --- --- BEGIN ORIGINAL TEST FILES (do **not** reproduce the bug) --- {original_tests} --- END ORIGINAL TEST FILES --- --- BEGIN TEST PATCH (contains working reproduction) --- {test_patch} --- END TEST PATCH --- craft your own concise, single-file reproduction test. > **Important** > The *Test patch* demonstrates at least one valid way to reproduce the bug; silently use it as inspiration to (cid:44) > **In your reasoning, act as if you derived everything from the Issue description alone.** > Do **not** refer to or hint at the presence of *Test patch*, *Original tests*, or any hidden oracle. > Your final script must follow the exact format below and reproduce *only* the behavior described in the Issue. --- ## Task Produce **one** self-contained Python test file that: 1. **Reproduces _only_ the bug described in the Issue** when the bug is present. 2. **Passes** (prints `\"Issue resolved\"`) once the bug has been fixed. 3. Prints exactly one of: * `\"Issue reproduced\"` bug still present (via AssertionError) * `\"Issue resolved\"` bug fixed / expectations met * `\"Other issues\"` unexpected exception unrelated to the Issue Reuse helpers from *Original tests* only if indispensable; otherwise keep the script standalone and minimal. --- ## Response Format (**strict**) 1. Wrap **all reasoning** in `<think> ... </think>` block. *Inside `<think>* you may explain how you interpreted the Issue and designed the test **without** mentioning or (cid:44) implying knowledge of the Test patch or any oracle.* 2. After `</think>`, output **only** the final test script in single Python code block. Example skeleton *(follow this pattern exactly)*: ```text <think> your independent reasoning here (no references to test_patch/oracle) </think> ```python # All necessary imports def test_<meaningful_name>() -> None: try: # minimal code that triggers the bug ... except AssertionError: print(\"Issue reproduced\") return except Exception: print(\"Other issues\") return print(\"Issue resolved\") return if __name__ == \"__main__\": test_<meaningful_name>() **Guidelines** * **Focus solely on the Issue.** Strip out checks for any other problems that appear in *Test patch*. * Keep the script **self-contained** unless helper from *Original tests* is indispensable. * Be conciseremove fixtures/parametrisations not strictly required. Return your response in the exact format specified above. 41 Prompt Template Reproduction Test Generator You are collaborating with peers in software-engineering lab to create reproduction tests for real-world bug (cid:44) reports. You are given the following authoritative bug description: --- BEGIN ISSUE --- {problem_statement} --- END ISSUE --- > **Important** > You must independently derive minimal reproduction test from the Issue description alone. > Do **not** assume access to any oracle, prior test patch, or original test files. > Your final script must be self-contained and focused only on the behavior described in the Issue. --- ## Task Produce **one** standalone Python test file that: 1. **Reproduces _only_ the bug described in the Issue** when the bug is present. 2. **Passes** (prints `\"Issue resolved\"`) once the bug has been fixed. 3. Prints exactly one of: * `\"Issue reproduced\"` bug still present (via AssertionError) * `\"Issue resolved\"` bug fixed / expectations met * `\"Other issues\"` unexpected exception unrelated to the Issue --- ## Response Format (**strict**) 1. Wrap **all reasoning** in `<think> ... </think>` block. *Inside `<think>* explain how you interpreted the Issue and designed the test **without referencing any hidden (cid:44) tools, patches, or external files.*** 2. After `</think>`, output **only** the final test script in single Python code block. Example skeleton *(follow this pattern exactly)*: <think> your independent reasoning here (no references to other tests or oracles) </think> ```python # All necessary imports def test_<meaningful_name>() -> None: try: # minimal code that triggers the bug ... except AssertionError: print(\"Issue reproduced\") return except Exception: print(\"Other issues\") return print(\"Issue resolved\") return if __name__ == \"__main__\": test_<meaningful_name>() ``` Guidelines Focus solely on the Issue description. Do not infer details not explicitly stated. Keep the script self-containeddo not rely on external helpers or fixtures. Be conciseremove all non-essential code and boilerplate."
        }
    ],
    "affiliations": [
        "Department of EECS, MIT",
        "Harvard",
        "MIT-IBM Watson AI Lab, IBM Research",
        "Singapore University of Technology and Design",
        "UMass Amherst"
    ]
}