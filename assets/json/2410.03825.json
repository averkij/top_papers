{
    "paper_title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion",
    "authors": [
        "Junyi Zhang",
        "Charles Herrmann",
        "Junhwa Hur",
        "Varun Jampani",
        "Trevor Darrell",
        "Forrester Cole",
        "Deqing Sun",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 2 8 3 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MONST3R: SIMPLE APPROACH FOR ESTIMATING GEOMETRY IN THE PRESENCE OF MOTION Junyi Zhang1 Charles Herrmann2, Forrester Cole2 Deqing Sun2, Ming-Hsuan Yang2,4, Junhwa Hur2 Varun Jampani3 Trevor Darrell1 1UC Berkeley 2Google DeepMind 3Stability AI 4UC Merced Figure 1: MonST3R processes dynamic video to produce time-varying dynamic point cloud, along with per-frame camera poses and intrinsics, in predominantly feed-forward manner. This representation then enables the efficient computation of downstream tasks, such as video depth estimation and dynamic/static scene segmentation."
        },
        {
            "title": "ABSTRACT",
            "content": "Estimating geometry from dynamic scenes, where objects move and deform over time, remains core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating pointmap for each timestep, we can effectively adapt DUSt3Rs representation, previously only used for static scenes, to dynamic scenes. However, this approach presents significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results, source code, and trained models will be available at: https://monst3r-project.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Despite recent progress in 3D computer vision, estimating geometry from videos of dynamic scenes remains fundamental challenge. Traditional methods decompose the problem into subproblems Project lead, Equal contribution"
        },
        {
            "title": "Preprint",
            "content": "such as depth, optical flow, or trajectory estimation, addressed with specialized techniques, and then combine them through global optimization or multi-stage algorithms for dynamic scene reconstruction (Luiten et al., 2020; Kumar et al., 2017; Bˆarsan et al., 2018; Mustafa et al., 2016). Even recent work often takes optimization-based approaches given intermediate estimates derived from monocular video (Lei et al., 2024; Chu et al., 2024; Wang et al., 2024b; Liu et al., 2024; Wang et al., 2024a). However, these multi-stage methods are usually slow, brittle, and prone to error at each step. While highly desirable, end-to-end geometry learning from dynamic video poses significant challenge, requiring suitable representation that can represent the complexities of camera motion, multiple object motion, and geometric deformations, along with annotated training datasets. While prior methods have centered on the combination of motion and geometry, motion is often difficult to directly supervise due to lack of annotated training data. Instead, we explore using only geometry to represent dynamic scenes, inspired by the recent work DUSt3R (Wang et al., 2024c). For static scenes, DUSt3R introduces new paradigm that directly regresses scene geometry. Given pair of images, DUSt3R produces pointmap representation - which associates every pixel in each image with an estimated 3D location (i.e., xyz) and aligns these pair of pointmaps in the camera coordinate system of the first frame. For multiple frames, DUSt3R accumulates the pairwise estimates into global point cloud and uses it to solve numerous standard 3D tasks such as singleframe depth, multi-frame depth, or camera intrinsics and extrinsics. We leverage DUSt3Rs pointmap representation to directly estimate geometry of dynamic scenes. Our key insight is that pointmaps can be estimated per timestep and that representing them in the same camera coordinate frame still makes conceptual sense for dynamic scenes. As shown in Fig. 1, an estimated pointmap for the dynamic scene appears as point cloud where dynamic objects appear at multiple locations, according to how they move. Multi-frame alignment can be achieved by aligning pairs of pointmaps based on static scene elements. This setting is generalization of DUSt3R to dynamic scenes and allows us to use the same network and original weights as starting point. One natural question is if DUSt3R can already and effectively handle video data with moving objects. However, as shown in Fig. 2, we identify two significant limitations stemming from the distribution of DUSt3Rs training data. First, since its training data contains only static scenes, DUSt3R fails to correctly align pointmaps of scenes with moving objects; it often relies on moving foreground objects for alignment, resulting in incorrect alignment for static background elements. Second, since its training data consists mostly of buildings and backgrounds, DUSt3R sometimes fails to correctly estimate the geometry of foreground objects, regardless of their motion, and places them in the background. In principle, both problems originate from domain mismatch between training and test time and can be solved by re-training the network. However, this requirement for dynamic, posed data with depth presents challenge, primarily due to its scarcity. Existing methods, such as COLMAP (Schonberger & Frahm, 2016), often struggle with complex camera trajectories or highly dynamic scenes, making it challenging to produce even pseudo ground truth data for training. To address this limitation, we identify several small-scale datasets that possess the necessary properties for our purposes. Our main finding is that, surprisingly, we can successfully adapt DUSt3R to handle dynamic scenes by identifying suitable training strategies designed to maximally leverage this limited data and finetuning on them. We then introduce several new optimization methods for video-specific tasks using these pointmaps and demonstrate strong performance on video depth and camera pose estimation, as well as promising results for primarily feed-forward 4D reconstruction. The contributions of this work are as follows: We introduce Motion DUSt3R (MonST3R), geometry-first approach to dynamic scenes that directly estimates geometry in the form of pointmaps, even for moving scene elements. To this end, we identify several suitable datasets and show that, surprisingly, small-scale fine-tuning achieves promising results for direct geometry estimation of dynamic scenes. MonST3R obtains promising results on several downstream tasks (video depth and camIn particular, MonST3R offers key advantages over prior work: era pose estimation). enhanced robustness, particularly in challenging scenarios; increased speed compared to optimization-based methods; and competitive results with specialized techniques in video depth estimation, camera pose estimation and dense reconstruction."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Limitation of DUSt3R on dynamic scenes. Left: DUSt3R aligns the moving foreground subject and misaligns the background points as it is only trained on static scenes. Right: DUSt3R fails to estimate the depth of foreground subject, placing it in the background."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Structure from motion and visual SLAM. Given set of 2D images, structure from motion (SfM) (Schonberger & Frahm, 2016; Teed & Deng, 2018; Tang & Tan, 2018) or visual SLAM (Teed & Deng, 2021; Mur-Artal et al., 2015; Mur-Artal & Tardos, 2017; Engel et al., 2014; Newcombe et al., 2011) estimate 3D structure of scene while also localizing the camera. However, these methods struggle with dynamic scenes with moving objects, which violate the epipolar constraint. To address this problem, recent approaches have explored joint estimation of depth, camera pose, and residual motion, optionally with motion segmentation to exploit the epipolar constraints on the stationary part. Self-supervised approaches (Gordon et al., 2019; Mahjourian et al., 2018; Godard et al., 2019; Yang et al., 2018) learn these tasks through self-supervised proxy tasks. CasualSAM (Zhang et al., 2022) finetunes depth network at test time with joint estimation of camera pose and movement mask. Robust-CVD (Kopf et al., 2021) jointly optimizes depth and camera pose given optical flow and binary masks for dynamic objects. Our approach directly estimates 3D structure of dynamic scene in the pointmap representation without time-consuming test-time finetuning. Representation for static 3D reconstruction. Learning-based approaches reconstruct static 3D geometry of objects or scenes by learning strong 3D priors from training datasets. Commonly used output representations include point clouds (Guo et al., 2020; Lin et al., 2018), meshes (Gkioxari et al., 2019; Wang et al., 2018), voxel (Sitzmann et al., 2019; Choy et al., 2016; Tulsiani et al., 2017), implicit representation (Wang et al., 2021a; Peng et al., 2020; Chen & Zhang, 2019), etc. DUSt3R (Wang et al., 2024c) introduces pointmap representation for scene-level 3D reconstruction. Given two input images, the model outputs 3D point of each pixel from both images in the camera coordinate system of the first frame. The model implicitly infers camera intrinsics, relative camera pose, and two-view geometry and thus can output an aligned points cloud with learned strong 3D priors. However, the method targets only static scenes. MonST3R shares the pointmap representation of DUSt3R but targets scenes with dynamic objects. Learning-based visual odometry. Learning-based visual odometry replaces hand-designed parts of geometry-based methods (Mur-Artal et al., 2015; Mur-Artal & Tardos, 2017; Engel et al., 2017) and enables large-scale training for better generalization even with moving objects. Trajectory-based approaches (Chen et al., 2024; Zhao et al., 2022) estimate long-term trajectories along video sequence, classify their dynamic and static motion, and then localize camera via bundle adjustment. Joint estimation approaches additionally infer moving object mask (Shen et al., 2023) or optical flow (Wang et al., 2021b) to be robust to moving objects while requiring their annotations during training. In contrast, our method directly outputs dynamic scene geometry via pointmap representation and localizes camera afterwards."
        },
        {
            "title": "Preprint",
            "content": "Monocular and video depth estimation. Recent deep learning works (Ranftl et al., 2020; 2021; Saxena et al., 2024; Ke et al., 2024) target zero-shot performance and with large-scale training combined with synthetic datasets (Yang et al., 2024a;b) show strong generalization to diverse domains. However, for video, these approaches suffer from flickering (temporal inconsistency between nearby estimates) due to their process of only single frame and invariant training objectives. Early approaches to video depth estimation (Luo et al., 2020; Zhang et al., 2021) improve temporal consistency by fine-tuning depth models, and sometimes motion models, at test time for each input video. Two recent approaches attempt to improve video depth estimation using generative priors. However, Chronodepth (Shao et al., 2024) still suffers from flickering due to its window-based inference, and DepthCrafter (Hu et al., 2024) produces scale-/shift-invariant depth, which is unsuitable for many 3D applications (Yin et al., 2021). 4D reconstruction. Concurrently approaches (Lei et al., 2024; Chu et al., 2024; Wang et al., 2024b; Liu et al., 2024) introduce 4D reconstruction methods of dynamic scenes. Given monocular video and pre-computed estimates (e.g., 2D motion trajectory, depth, camera intrinsics and pose, etc.), the approaches reconstruct the input video in 4D space via test-time optimization of 3D Gaussians (Kerbl et al., 2023) with deformation fields, facilitating novel view synthesis in both space and time. Our method is orthogonal to the methods and estimate geometry from videos in feed-forward manner. Our estimates could be used as initialization or intermediate signals for these methods."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 BACKGROUND AND BASELINES Model architecture. Our architecture is based on DUSt3R (Wang et al., 2024c), ViT-based architecture (Dosovitskiy et al., 2021) that is pre-trained on cross-view completion task (Weinzaepfel et al., 2023) in self-supervised manner. Two input images are first individually fed to shared encoder. following transformer-based decoder processes the input features with cross-attention. Then two separate heads at the end of the decoder output pointmaps of the first and second frames aligned in the coordinate of the first frame. Baseline with mask. While DUSt3R is designed for static scenes as shown in Fig. 2, we analzye its applicability to dynamic scenes by using knowledge of dynamic elements (Chen et al., 2024; Zhao et al., 2022). Using ground truth moving masks, we adapt DUSt3R by masking out dynamic objects during inference at both the image and token levels, replacing dynamic regions with black pixels in the image and corresponding tokens with mask tokens. This approach, however, leads to degraded pose estimation performance (Sec. 4.3), likely because the black pixels and mask tokens are out-of-distribution with respect to training. This motivates us to address these issues in this work. 3.2 TRAINING FOR DYNAMICS Main idea. While DUSt3R primarily focuses on static scenes, the proposed MonST3R can estimate the geometry of dynamic scenes over time. Figure. 1 shows visual example consisting of point cloud where dynamic objects appear at different locations, according to how they move. Similar to DUSt3R, for single image It at time t, MonST3R also predicts pointmap Xt RHW 3. For pair of images, It and It , we adapt the notation used in the global optimization section of DUSt3R. The network predicts two corresponding pointmaps, Xt;t , with confidence map, Ct;t The first element in the superscript indicates the frame that the pointmap corresponds to, and indicates that the network receives two frames at t, and that the pointmaps are in the coordinate frame of the camera at t. The key difference from DUSt3R is that each pointmap in MonST3R relates to single point in time. and Xt;t and Ct;t Training datasets. key challenge in modeling dynamic scenes as per-timestep pointmaps lies in the scarcity of suitable training data, which requires synchronized annotations of input images, camera poses, and depth. Acquiring accurate camera poses for real-world dynamic scenes is particularly challenging, often relying on sensor measurements or post-processing through structure from motion (SfM) (Schonberger et al., 2016; Schonberger & Frahm, 2016) while filtering out moving"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Training datasets used fine-tuning on dynamic scenes. All datasets provide both camera pose and depth, and most of them include dynamic objects. Dataset PointOdyssey (Zheng et al., 2023) TartanAir (Wang et al., 2020) Spring (Mehl et al., 2023) Waymo Perception (Sun et al., 2020) Domain Synthetic Synthetic Synthetic Real Scene type # of frames # of Scenes Dynamics Ratio Indoors & Outdoors Indoors & Outdoors Outdoors Driving 200k 1000k 6k 160k 131 163 37 798 Realistic None Realistic Driving 50% 25% 5% 20% objects. Consequently, we leverage primarily synthetic datasets, where accurate camera poses and depth can be readily extracted during the rendering process. For our dynamic fine-tuning, we identify four large video datasets: three synthetic datasets - PointOdyssey (Zheng et al., 2023), TartanAir (Wang et al., 2020), and Spring (Mehl et al., 2023), along with the real-world Waymo dataset (Sun et al., 2020), as shown in Tab. 1. These datasets contain diverse indoor/outdoor scenes, dynamic objects, camera motion, and labels for camera pose and depth. PointOdyssey and Spring are both synthetically rendered scenes with articulated, dynamic objects; TartanAir consists of synthetically rendered drone fly-throughs of different scenes without dynamic objects; and Waymo is real-world driving dataset labeled with LiDAR. During training, we sample the datasets asymmetrically to place extra weight on PointOdyssey (more dynamic, articulated objects) and less weight on TartanAir (good scene diversity but static) and Waymo (a highly specialized domain). Images are downsampled such that their largest dimension is 512. Training strategies. Due to the relatively small size of this dataset mixture, we adopt several training techniques designed to maximize data efficiency. First, we only finetune the prediction head and decoder of the network while freezing the encoder. This strategy preserves the geometric knowledge in the CroCo (Weinzaepfel et al., 2022) features and should decrease the amount of data required for fine-tuning. Second, we create training pairs for each video by sampling two frames with temporal strides ranging from 1 to 9. The sampling probabilities increase linearly with the stride length, with the probability of selecting stride 9 being twice that of stride 1. This gives us larger diversity of camera and scene motion and more heavily weighs larger motion. Third, we utilize Field-of-View augmentation technique using center crops with various image scales. This encourages the model to generalize across different camera intrinsics, even though such variations are relatively infrequent in the training videos. 3.3 DOWNSTREAM APPLICATIONS Instrinsics and relative pose estimation. Since the intrinsic parameters are estimated based on the pointmap in its own camera frame Xt;t , the assumptions and computation listed in DUSt3R are still valid, and we only need to solve for focal length to obtain the camera intrinsics Kt. To estimate relative pose = [RT], where and represent the cameras rotation and translation, respectively, dynamic objects violate several assumptions required for the use of an epipolar matrix (Hartley & Zisserman, 2003) or Procrustes alignment (Luo & Hancock, 1999). Instead, we use RANSAC (Fischler & Bolles, 1981) and PnP (Hartley & Zisserman, 2003; Lepetit et al., 2009). For most scenes, where most pixels are static, random samples of points will place more emphasis on the static elements, and relative pose can be estimated robustly using the inliers. Confident static regions. We can infer static regions in frames t, by comparing the estimated optical flow with the flow field that results from applying only camera motion from to to the pointmap at t. The two flow fields should agree for pixels where the geometry has been correctly estimated and are static. Given pair of frames It and It , we first compute two sets of pointmaps and Xt;t t, Xt;t t. We then use these pointmaps to solve for the camera intrinsics Xt;t (Kt and Kt ] as above. We then compute the optical flow field induced by camera motion, Ftt cam , by backprojecting each pixel in 3D, applying relative camera motion, and projecting back to image coordinate, ) for each frame and the relative camera pose from to t, Ptt = [Rtt , Xt;t Ttt cam = π(Dt;t Ftt Kt Rtt Kt1 ˆx + Kt Ttt ) x, (1)"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Dynamic global point cloud and camera pose estimation. Given fixed sized of temporal window, we compute pairwise pointmap for each frame pair with MonST3R and optical flow from off-the-shelf method. These intermediates then serve as inputs to optimize global point cloud and per-frame camera poses. Video depth can be directly derived from this unified representation. where is pixel coordinate matrix, ˆx is in homogeneous coordinates, π() is the projection operation (x, y, z) (x/z, y/z), and Dt;t is estimated depth extracted from the point map Xt;t . Then we compare it with optical flow (i.e., Ftt ) computed by an off-the-shelf optical flow method (Wang et al., 2024d) and infer the static mask Stt (cid:104) α > Ftt via simple thresholding: (cid:105) Stt (2) = cam Ftt L1 est est , with threshold α, L1 for smooth-L1 norm (Girshick, 2015), and [] for the Iverson bracket. This confident, static mask is both potential output and will be used in the later global pose optimization. 3.4 DYNAMIC GLOBAL POINT CLOUDS AND CAMERA POSE Even short video contain numerous frames (e.g. 5-second video with 24 fps gives 120 frames) making it non-trivial to extract single dynamic point cloud from pairwise pointmap estimates across the video. Here, we detail the steps to simultaneously solve for global dynamic point cloud and camera poses by leveraging our pairwise model and the inherent temporal structure of video. Video graph. For global alignment, DUSt3R constructs connectivity graph from all pairwise frames, process that is prohibitively expensive for video. Instead, as shown on the left of Fig. 3, we process video with sliding temporal window, significantly reducing the amount of compute required. Specifically, given video = [I0, . . . , IN ], we compute pointmaps for all pairs = (t, t) within temporal window of size w, Wt = {(a, b) a, [t, . . . , + w], = b} and for all valid windows W. To further improve the run time, we also apply strided sampling. , Xt;t Dynamic global point cloud and pose optimization. The primary goal is to accumulate all pairwise pointmap predictions (e.g., Xt;t ) into the same global coordinate frame to produce worldcoordinate pointmap Xt RHW 3. To do this, as shown in Fig. 3, we use DUSt3Rs alignment loss and add two video specific loss terms: camera trajectory smoothness and flow projection. We start by re-parameterizing the global pointmaps Xt with camera parameters Pt = [RtTt], Kt i,j := Pt1h(Kt1[iDt and per-frame depthmap Dt, as Xt i,j]), with (i, j) for pixel coordinate and h() for homogeneous mapping. It allows us to define losses directly on the camera parameters. To simplify the notation for function parameters, we use Xt as shortcut for Pt, Kt, Dt. First, we use the alignment term in DUSt3R which aims to find single rigid transformation Pt;e that aligns each pairwise estimation with the world coordinate pointmaps, since both Xt;t and Xt;t are in the same camera coordinate frame: i,j; jDt i,j; Dt Lalign(X, σ, PW ) = (cid:88) (cid:88) (cid:88) Ct;e (Xt σePt;eXt;e)1, (3) te where σe is pairwise scale factor. To simplify the notation, we use the directed edge = (t, t) interchangeably with t. iW eW"
        },
        {
            "title": "Preprint",
            "content": "We use camera trajectory smoothness loss to encourage smooth camera motion by penalizing large changes in camera rotation and translation in nearby timesteps: Lsmooth(X) = (cid:88) t=0 (cid:16)(cid:13) (cid:13)Rt (cid:13) Rt+1 (cid:13) (cid:13) (cid:13)f + (cid:13) (cid:13)Rt (cid:13) (Tt+1 Tt) (cid:17) , (cid:13) (cid:13) (cid:13)2 (4) where the Frobenius norm is used for the rotation difference, the L2 norm 2 is used for the translation difference, and is the identity matrix. We also use flow projection loss to encourage the global pointmaps and camera poses to be consistent with the estimated flow for the confident, static regions of the actual frames. More precisely, given two frames t, t, using their global pointmaps, camera extrinsics and intrinsics, we compute the flow fields from taking the global pointmap Xt, assuming the scene is static, and then moving the camera from to t. We denote this value Fglobal;tt , similar to the term defined in the conficam dent static region computation above. Then we can encourage this to be close to the estimated flow, Ftt est according to the global parameters: , in the regions which are confidently static Sglobal;tt (cid:88) (cid:88) Sglobal;tt (Fglobal;tt cam Ftt est )1, (5) Lflow(X) = iW ttW where indicates element-wise multiplication. Note that the confident static mask is initialized using the pairwise prediction values (pointmaps and relative poses) as described in Sec. 3.3. During the optimization, we use the global pointmaps and camera parameters to compute Fglobal cam and update the confident static mask. The complete optimization for our dynamic global point cloud and camera poses is: ˆX = arg min X,PW ,σ Lalign(X, σ, PW ) + wsmoothLsmooth(X) + wflowLflow(X), (6) where wsmooth, wflow are hyperparameters. Note, based on the reparameterization above, ˆX includes all the information for ˆD, ˆP, ˆK. Video depth. We can now easily obtain temporally-consistent video depth, traditionally addressed as standalone problem. Since our global pointmaps are parameterized by camera pose and perframe depthmaps ˆD, just returning ˆD gives the video depth."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "MonST3R runs on monocular video of dynamic scene and jointly optimizes video depth and camera pose. We compare the performance with methods specially designed for each individual subtask (i.e., depth estimation and camera pose estimation), as well as monocular depth methods. 4.1 EXPERIMENTAL DETAILS Training and Inference. We fine-tune the DUSt3Rs ViT-Base decoder and DPT heads for 25 epochs, using 20,000 sampled image pairs per epoch. We use the AdamW optimizer with learning rate of 5 105 and mini-batch size of 4 per GPU. Training took one day on 2 RTX 6000 48GB GPUs. Inference for 60-frame video with = 9 and stride 2 (approx. 600 pairs) takes around 30s. Global Optimization. For global optimization Eq. (6), we set the hyperparameter of each weights to be wsmooth = 0.01 and wflow = 0.01. We only enable the flow loss when the average value is below 20, when the poses are roughly aligned. The motion mask is updated during optimization if the per-pixel flow loss is higher than 50. We use the Adam optimizer for 300 iterations with learning rate of 0.01, which takes around 1 minute for 60-frame video on single RTX 6000 GPU. 4.2 SINGLE-FRAME AND VIDEO DEPTH ESTIMATION Baselines. We compare our method with video depth methods, NVDS (Wang et al., 2023), ChronoDepth (Shao et al., 2024), and concurrent work, DepthCrafter (Hu et al., 2024), as well as singleframe depth methods, Depth-Anything-V2 (Yang et al., 2024b) and Marigold (Ke et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Video depth evaluation on Sintel, Bonn, and KITTI datasets. We evaluate for both scaleand-shift-invariant and scale-invariant depth. The best and second best results in each category are bold and underlined, respectively. Sintel Bonn KITTI Alignment Category Method Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 Single-frame depth Marigold Depth-Anything-V2 Per-sequence scale & shift Video depth NVDS ChronoDepth DepthCrafter (Sep. 2024) Joint video depth & pose Robust-CVD CasualSAM MonST3R Per-sequence scale Video depth DepthCrafter (Sep. 2024) Joint depth & pose MonST3R 0.532 0.367 0.408 0.687 0.292 0.703 0.387 0.335 0.692 0.345 51.5 55. 48.3 48.6 69.7 47.8 54.7 58.5 53.5 56.2 0.091 0.106 0.167 0.100 0.075 - 0.169 0. 0.217 0.065 93.1 92.1 76.6 91.1 97.1 - 73.7 96.4 57.6 96.3 0.149 0. 0.253 0.167 0.110 - 0.246 0.104 0.141 0.106 79.6 80.4 58.8 75.9 88.1 - 62.2 89. 81.8 89.3 Table 3: Single-frame depth evaluation. We report the performance on Sintel, Bonn, KITTI, and NYU-v2 (static) datasets. MonST3R achieves overall comparable results to DUSt3R. Sintel Bonn KITTI NYU-v2 (static) Method Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 Abs Rel δ <1.25 DUSt3R 0.424 MonST3R 0.345 58.7 56.5 0.141 0.076 82.5 93. 0.112 0.101 86.3 89.3 0.080 0.091 90.7 88.8 We also compare with methods for joint video depth and pose estimation, CasualSAM (Zhang et al., 2022) and Robust-CVD (Kopf et al., 2021), which address the same problem as us. This comparison is particularly important since joint estimation is substantially more challenging than only estimating depth. Of note, CasualSAM relies on heavy optimization, whereas ours runs in feed-forward manner with only lightweight optimization. Benchmarks and metrics. Similar to DepthCrafter, we evaluate video depth on KITTI (Geiger et al., 2013), Sintel (Butler et al., 2012), and Bonn (Palazzolo et al., 2019) benchmark datasets, covering dynamic and static, indoor and outdoor, and realistic and synthetic data. For monocular/singleframe depth estimation, we also evaluate on NYU-v2 (Silberman et al., 2012). Our evaluation metrics include absolute relative error (Abs Rel) and percentage of inlier points δ < 1.25, following the convention (Hu et al., 2024; Yang et al., 2024b). All methods output scaleand/or shiftinvariant depth estimates. For video depth evaluation, we align single scale and/or shift factor per each sequence, whereas the single-frame evaluation adopts per-frame median scaling, following Wang et al. (2024c). As demonstrated by Yin et al. (2021), shift is particularly important in the 3D geometry of scene and is important to predict. Results. As shown in Tab. 2, MonST3R achieves competitive and even better results, even outperforming specialized video depth estimation techniques like DepthCrafter (a concurrent work). Furthermore, MonST3R significantly outperforms DepthCrafter (Hu et al., 2024) with scale-only normalization. As in Tab. 3, even after our fine-tuning for videos of dynamic scenes, the performance on single-frame depth estimation remains competitive with the original DUSt3R model. 4.3 CAMERA POSE ESTIMATION Baselines. We compare with not only direct competitors (i.e., CasualSAM and Robust-CVD), but also range of learning-based visual odometry methods for dynamic scenes, such as DROIDSLAM (Teed & Deng, 2021), Particle-SfM (Zhao et al., 2022), DPVO (Teed et al., 2024), and LEAP-VO (Chen et al., 2024). Notably, several methods (e.g., DROID-SLAM, DPVO, and LEAPVO) require ground truth camera intrinsic as input and ParticleSfM is an optimization-based method that runs 5 slower than ours. We also compare with the DUSt3R with mask baseline in Sec. 3.1 to see if DUSt3R performs well on dynamic scenes when ground truth motion mask is provided."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Evaluation on camera pose estimation on the Sintel, TUM-dynamic, and ScanNet. The best and second best results are bold and underlined, respectively. MonST3R achieves competitive and even better results than pose-specific methods, even without ground truth camera intrinsics. Category Method ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot Sintel TUM-dynamics ScanNet (static) Pose only DROID-SLAM DPVO ParticleSfM LEAP-VO 0.175 0.115 0.129 0.089 Robust-CVD Joint depth CasualSAM 0.360 0.141 & pose DUSt3R w/ mask 0.417 0.108 MonST3R 0.084 0.072 0.031 0.066 0.154 0.035 0.250 0.042 1.912 1.975 0.535 1.250 3.443 0.615 5.796 0.732 - - - 0.068 0.153 0.071 0.083 0. - - - 0.008 0.026 0.010 0.017 0.009 - - - 1.686 3.528 1.712 3.567 1.217 - - 0.136 0.070 0.227 0.158 0.081 0. - - 0.023 0.018 0.064 0.034 0.028 0.017 - - 0.836 0.535 7.374 1.618 0.784 0.545 requires ground truth camera intrinsics as input, unable to estimate the depth of foreground object. Benchmarks and metrics. We evaluate the methods on Sintel (Butler et al., 2012) and TUMdynamics (Sturm et al., 2012) (following CasualSAM) and ScanNet (Dai et al., 2017) (following ParticleSfM) to test generalization to static scenes as well. On Sintel, we follow the same evaluation protocol as in Chen et al. (2024); Zhao et al. (2022), which excludes static scenes or scenes with perfectly-straight camera motion, resulting in total 14 sequences. For TUM-dynamics and ScanNet, we sample the first 90 frames with the temporal stride of 3 to save compute. We report the same metric as Chen et al. (2024); Zhao et al. (2022): Absolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot), after applying Sim(3) Umeyama alignment on prediction to the ground truth. Results. In Tab. 4, MonST3R achieves the best accuracy among methods to joint depth and pose estimation and performs competitively to pose-only methods even without using ground truth camera intrinsics. Our method also generalizes well to static scenes (i.e., ScanNet) and shows improvements over even DUSt3R, which proves the effectiveness of our designs (e.g., Eq. (6)) for video input. 4.4 JOINT DENSE RECONSTRUCTION AND POSE ESTIMATION Fig. 4 qualitatively compares our method with CasualSAM and DUSt3R on video sequences for joint dense reconstruction and pose estimation on DAVIS (Perazzi et al., 2016). For each video sequence, we visualize overlayed point clouds aligned with estimated camera pose, showing as two rows with different view points for better visualization. As discussed in Fig. 2, DUSt3R struggles with estimating correct geometry of moving foreground objects, resulting in failure of joint camera pose estimation and dense reconstruction. CasualSAM reliably estimates camera trajectories while sometimes failing to produce correct geometry estimates for foreground objects. MonST3R outputs both reliable camera trajectories and reconstruction of entire scenes along the video sequences. 4.5 ABLATION STUDY Table 5 presents an ablation study analyzing the impact of design choices in our method, including the selection of training datasets, fine-tuning strategies, and the novel loss functions used for dynamic point cloud optimization. Our analysis reveals that: (1) all datasets contribute to improved camera pose estimation performance; (2) fine-tuning only the decoder and head outperforms alternative strategies; and (3) the proposed loss functions enhance pose estimation with minimal impact on video depth accuracy. Discussions. While MonST3R represents promising step towards directly estimating dynamic geometry from videos as well as camera pose and video depth, limitations remain. While our method can, unlike prior methods, theoretically handle dynamic camera intrinsics, we find that, in practice, this requires careful hyperparameter tuning or manual constraints. Like many deep learning methods, MonST3R struggles with out-of-distribution inputs, such as open fields. Expanding the training set is key direction to make MonST3R more robust to in-the-wild videos."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Qualitative comparison. Compared to CasualSAM and DUSt3R, our method outputs both reliable camera trajectories and geometry of dynamic scenes. Refer to Fig. A5 for more results. Table 5: Ablation study on Sintel dataset. For each category, the default setting is underlined, and the best performance is bold. Variants ATE RPE trans RPE rot Abs Rel δ <1.25 Camera pose estimation Video depth estimation Training dataset No finetune (DUSt3R) w/ PO w/ PO+TA w/ PO+TA+Spring w/ TA+Spring+Waymo w/ all 4 datasets Training strategy Full model finetune Finetune decoder & head Finetune head Inference w/o flow loss w/o static region mask w/o smoothness loss Full 0.354 0.220 0.158 0.121 0.167 0.108 0.181 0.108 0.185 0.140 0.132 0.127 0.108 0.167 0.129 0.054 0.046 0.107 0.042 0.110 0.042 0. 0.051 0.049 0.060 0.042 0.996 0.901 0.886 0.777 1.136 0.732 0.738 0.732 0.860 0.903 0.899 1.456 0.732 0.482 0.378 0.362 0.329 0.462 0.335 0.352 0.335 0. 0.339 0.334 0.333 0.335 56.5 53.7 56.7 58.1 54.0 58.5 55.4 58.5 55.7 57.7 58.7 58.4 58."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We present MonST3R, simple approach to directly estimate geometry for dynamic scenes and extract downstream information like camera pose and video depth. MonST3R leverages per-timestep pointmaps as powerful representation for dynamic scenes. Despite being finetuned on relatively small training dataset, MonST3R achieves impressive results on downstream tasks, surpassing even state-of-the-art specialized techniques."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We would like to express gratitude to Youming Deng, Hang Gao, and Haven Feng for their helpful discussions and extend special thanks to Brent Yi, Chung Min Kim, and Justin Kerr for their valuable help with online visualizations."
        },
        {
            "title": "REFERENCES",
            "content": "Ioan Andrei Bˆarsan, Peidong Liu, Marc Pollefeys, and Andreas Geiger. Robust dense mapping for large-scale dynamic environments. In ICRA, pp. 75107517, 2018. 2 Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. naturalistic open source movie for optical flow evaluation. In ECCV, pp. 611625, 2012. 8, 9 Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. LEAP-VO: Long-term effective any point tracking for visual odometry. In CVPR, pp. 1984419853, 2024. 3, 4, 8, Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, pp. 59395948, 2019. 3 Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-R2N2: unified approach for single and multi-view 3D object reconstruction. In ECCV, pp. 628644, 2016. 3 Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. DreamScene4D: Dynamic multi-object scene generation from monocular videos. arXiv preprint arXiv:2405.02280, 2024. 2, 4 Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, pp. 5828 5839, 2017. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 4 Jakob Engel, Thomas Schops, and Daniel Cremers. LSD-SLAM: Large-scale direct monocular SLAM. In ECCV, pp. 834849, 2014. 3 Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE TPAMI, 40(3): 611625, 2017. 3 Martin Fischler and Robert C. Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24 (6):381395, 1981. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11):12311237, 2013. 8 Ross Girshick. Fast R-CNN. In ICCV, pp. 14401448, 2015. 6 Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh R-CNN. In ICCV, pp. 97859795, 2019. 3 Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into selfsupervised monocular depth estimation. In ICCV, pp. 38283838, 2019. 3 Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In ICCV, pp. 89778986, 2019. 3 Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3D point clouds: survey. IEEE TPAMI, 43(12):43384364, 2020."
        },
        {
            "title": "Preprint",
            "content": "Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 5 Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. DepthCrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 4, 7, 8 Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, pp. 94929502, 2024. 4, 7 Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4):1391, 2023. Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, pp. 16111621, 2021. 3, 8 Suryansh Kumar, Yuchao Dai, and Hongdong Li. Monocular dense 3D reconstruction of complex dynamic scene from two perspective frames. In ICCV, pp. 46494657, 2017. 2 Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. MoSca: Dynamic gaussian fusion from casual videos via 4D motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 2, 4 Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. EPnP: An accurate O(n) solution to the PnP problem. IJCV, 81:155166, 2009. 5 Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning efficient point cloud generation for dense 3D object reconstruction. In AAAI, 2018. 3 Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, and Junhui Hou. MoDGS: Dynamic gaussian splatting from causually-captured monocular videos. arXiv preprint arXiv:2406.00434, 2024. 2, 4 Jonathon Luiten, Tobias Fischer, and Bastian Leibe. Track to reconstruct and reconstruct to track. IEEE Robotics and Automation Letters, 5(2):18031810, 2020. Bin Luo and Edwin R. Hancock. Procrustes alignment with the EM algorithm. In International Conference on Computer Analysis of Images and Patterns, pp. 623631, 1999. 5 Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 39(4):711, 2020. 4 Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning of depth and egomotion from monocular video using 3D geometric constraints. In CVPR, pp. 56675675, 2018. 3 Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In CVPR, pp. 49814991, 2023. Raul Mur-Artal and Juan D. Tardos. ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras. IEEE Transactions on Robotics, 33(5):12551262, 2017. 3 Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. ORB-SLAM: versatile and accurate monocular SLAM system. IEEE Transactions on Robotics, 31(5):11471163, 2015. 3 Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, and Adrian Hilton. Temporally coherent 4D reconstruction of complex dynamic scenes. In CVPR, pp. 46604669, 2016. 2 Richard Newcombe, Steven J. Lovegrove, and Andrew J. Davison. DTAM: Dense tracking and mapping in real-time. In ICCV, pp. 23202327, 2011."
        },
        {
            "title": "Preprint",
            "content": "Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for RGB-D cameras exploiting residuals. In IROS, pp. 78557862, 2019. 8 Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In ECCV, pp. 523540, 2020. 3 Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, pp. 724732, 2016. 9 Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 44(3):16231637, 2020. 4 Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pp. 1217912188, 2021. 4 Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David J. Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. NeurIPS, 2024. 4 Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 2, 3, 4 Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. 4, 7 Shihao Shen, Yilin Cai, Wenshan Wang, and Sebastian Scherer. DytanVO: Joint refinement of visual odometry and motion segmentation in dynamic environments. In ICRA, pp. 40484055, 2023. 3 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In ECCV, pp. 746760, 2012. 8 Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. DeepVoxels: Learning persistent 3D feature embeddings. In CVPR, pp. 24372446, 2019. 3 Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of RGB-D SLAM systems. In IROS, pp. 573580, 2012. 9 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, 2020. 5 Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. arXiv preprint arXiv:1806.04807, 2018. 3 Zachary Teed and Jia Deng. DeepV2D: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605, 2018. Zachary Teed and Jia Deng. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D cameras. NeurIPS, pp. 1655816569, 2021. 3, 8 Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. NeurIPS, 2024. 8 Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In CVPR, pp. 26262634, 2017."
        },
        {
            "title": "Preprint",
            "content": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2Mesh: Generating 3D mesh models from single RGB images. In ECCV, pp. 5267, 2018. 3 Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021a. 3 Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024a. 2 Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. GFlow: Recovering 4D world from monocular video. arXiv preprint arXiv:2405.18426, 2024b. 2, 4 Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3D vision made easy. In CVPR, pp. 2069720709, 2024c. 2, 3, 4, 8 Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. TartanAir: dataset to push the limits of visual SLAM. In IROS, pp. 49094916, 2020. 5 Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. TartanVO: generalizable learning-based VO. In CoRL, pp. 17611772, 2021b. Yihan Wang, Lahav Lipson, and Jia Deng. SEA-RAFT: Simple, efficient, accurate RAFT for optical flow. In ECCV, 2024d. 6 Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video depth stabilizer. In ICCV, pp. 94669476, 2023. 7 Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Bregier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jerˆome Revaud. CroCo: Selfsupervised pre-training for 3D vision tasks by cross-view completion. NeurIPS, pp. 35023516, 2022. 5 Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. CroCo v2: Improved cross-view completion pre-training for stereo matching and optical flow. In ICCV, pp. 1796917980, 2023. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, pp. 1037110381, 2024a. 4 Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything V2. arXiv preprint arXiv:2406.09414, 2024b. 4, 7, 8 Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram Nevatia. Every pixel counts: Unsupervised geometry learning with holistic 3D motion understanding. In ECCV workshops, 2018. 3 Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 204213, 2021. 4, 8 Zhoutong Zhang, Forrester Cole, Richard Tucker, William Freeman, and Tali Dekel. Consistent depth of moving objects in video. ACM Transactions on Graphics (ToG), 40(4):112, 2021. Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William T. Freeman. Structure and motion from casual videos. In ECCV, pp. 2037, 2022. 3, 8 Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. ParticleSfM: Exploiting dense point trajectories for localizing moving cameras in the wild. In ECCV, pp. 523542, 2022. 3, 4, 8, 9 Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. PointOdyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023."
        },
        {
            "title": "A MORE QUALITATIVE RESULTS",
            "content": "A.1 DEPTH For more thorough comparisons, we include additional qualitative examples of video depth, comparing our method against DepthCrafter, concurrent method specifically trained for video depth. We include comparisons on the Bonn dataset in Fig. A1 and the KITTI dataset in Fig. A2. In these comparisons, we show that after alignment, our estimates are much closer to the ground truth than those of DepthCrafter. Figure A1: Video depth estimation comparison on Bonn dataset. Evaluation protocol is persequence scale & shift. We visualize the prediction result after alignment. Note, in the first row, our depth estimation is more aligned with the GT depth (e.g., the wall) compared to DepthCrafters. A.2 CAMERA POSE We present additional qualitative results for camera pose estimation. We compare our model with the state-of-the-art visual odometry method LEAP-VO and the joint video depth and pose optimization method CasualSAM. Results are provided for the Sintel dataset in Fig. A3 and Scannet dataset in Fig. A4. In these comparisons, our method significantly outperforms the baselines for very challenging cases such as temple 3 and cave 2 in Sintel and performs comparable to or better than baselines in the rest of the results like those in the Scannet dataset. A. JOINT DEPTH & CAMERA POSE RESULTS We present additional results for joint point cloud and camera pose estimation, comparing against CasualSAM and DUSt3R. Fig. A5 shows three additional scenes for Davis: mbike-trick, train, and dog. For mbike-trick, CasualSAM makes large errors in both geometry and camera pose; DUSt3R produces reasonable geometry except for the last subset of the video which also results in poor"
        },
        {
            "title": "Preprint",
            "content": "Figure A2: Video depth estimation comparison on KITTI dataset. Evaluation protocol is persequence scale & shift. For each case, the upper row is for input frame and depth prediction; the lower row is for ground truth depth annotation and error map. Prediction result is after alignment. pose estimation (highlighted in red); and ours correctly estimates both geometry and camera pose. For train, CasualSAM accurately recovers the camera pose for the video but produces suboptimal geometry, misaligning the train and the track at the top right. DUSt3R both misaligns the track at the top left and gives poor camera pose estimates. Our method correctly estimates both geometry and camera pose. For dog, CasualSAM produces imprecise, smeared geometry with slight inaccuracies in the camera pose. DUSt3R results in mistakes in both the geometry and camera pose due to misalignments of the frames, while our method correctly estimates both geometry and camera pose. A.4 PAIRWISE POINTMAPS In Fig. A6, we also include visualizations of two input frames and estimated pairwise pointmaps, the direct output of the trained models, for both DUSt3R and MonST3R. Note, these results do not include any optimization or post-processing. Row 1 demonstrates that even after fine-tuning, our method retains the ability to handle changing camera intrinsics. Rows 2 and 3 demonstrate that our method can handle impossible alignments that two frames have almost no overlap, even in the presence of motion, unlike DUSt3R that misaligns based on the foreground object. Rows 4 and 5 show that in addition to enabling the model to handle motion, our fine-tuning also has improved the models ability to represent large-scale scenes, where DUSt3R predicts to be flat."
        },
        {
            "title": "Preprint",
            "content": "Figure A3: Camera pose estimation comparison on the Sintel dataset. The trajectories are plotted along the two axes with the highest variance to capture the most significant motion. The predicted trajectory (solid blue line) is aligned to match the ground truth trajectory (dashed gray line). Our MonST3R is more robust at challenging scenes, temple 3 and cave 2 (the last two rows). Figure A4: Camera pose estimation comparison on the Scannet dataset. The trajectories are plotted along the two axes with the highest variance to capture the most significant motion."
        },
        {
            "title": "Preprint",
            "content": "Figure A5: Qualitative comparison on Davis. Compared to CasualSAM and DUSt3R, our method outputs both reliable camera trajectories and geometry of dynamic scenes."
        },
        {
            "title": "Preprint",
            "content": "Figure A6: Qualitative comparison of feed-forward pairwise pointmaps prediction."
        },
        {
            "title": "Preprint",
            "content": "Figure A7: Qualitative comparison of static/dynamic mask. We visualize both the continuous error map (upper row) and binary static/dynamic mask (lower row). The threshold α is fixed. A.5 STATIC/DYNAMIC MASK We present the result of static/dynamic mask estimation based on the optical flow, as discussed in Sec. 3.3. As shown in Fig. A7, our MonST3R achieves overall plausible results while DUSt3R fails to provide accurate motion mask due to error in camera pose and depth estimation."
        },
        {
            "title": "B MORE QUANTITATIVE RESULTS",
            "content": "B.1 ABLATION ON TRANING/INFERENCE WINDOW SIZE In this section, we present additional quantitative results on the impact of different training and inference window sizes, as shown in Tab. A1. The results demonstrate that performance generally improves as the inference window size increases, up to the size of the training window."
        },
        {
            "title": "Preprint",
            "content": "Table A1: Ablation study on different training/inference window sizes on the Sintel dataset. Each cell displays two values: ATE / Abs Rel , corresponding to camera pose and video depth estimation, respectively. The cells where the inference window size exceeds the training window size are highlighted in grey. The default setup is underlined, and the best results are in bold. GPU memory consumption for each inference setup is listed in the leftmost column. Training window size Memory (GB) Inference video graph Size 5 Size Size 9 17.3 20.1 17.2 23.5 19.9 29.5 23.2 Window size 3 Window size 4 Window size 5 (stride 2) Window size 5 Window size 7 (stride 2) Window size 7 Window size 9 (stride 2) 0.191 / 0.442 0.178 / 0.431 0.145 / 0.439 0.139 / 0.406 0.180 / 0.409 0.174 / 0.389 0.177 / 0.380 0.182 / 0.413 0.166 / 0.406 0.140 / 0.411 0.132 / 0.399 0.140 / 0.372 0.136 / 0.351 0.156 / 0.387 0.163 / 0.383 0.148 / 0.362 0.137 / 0.367 0.133 / 0.355 0.121 / 0.359 0.113 / 0.346 0.108 / 0. Moreover, our proposed stride-based sampling provides better trade-off between window size and computational cost. For instance, for the training window size of 7 or 9, the inference configuration Window size 7 (stride 2) outperforms Window size 4 while consuming similar amount of memory (19.9 GB vs. 20.1 GB). Additionally, for training window size of 9, Window size 9 (stride 2) achieves better performance than Window size 7 while reducing memory consumption by 20%, highlighting the efficiency of our design. B.2 ABLATION ON LOSS WEIGHT SENSITIVITY Table A2: Ablation study on loss weight sensitivity. The table shows the effect of varying the loss weights wsmooth and wflow on camera pose and video depth estimation. The default setup is underlined, and the best results are in bold. Camera pose estimation Video depth estimation wsmooth wflow ATE RPEtrans RPErot Abs Rel δ <1.25 0.01 0.01 0.01 0.01 0.01 0.001 0.005 0.01 0.05 0.1 0.001 0.005 0.01 0.05 0.1 0.01 0.01 0.01 0.01 0. 0.118 0.109 0.108 0.115 0.118 0.110 0.109 0.108 0.127 0.138 0.045 0.042 0.042 0.044 0.049 0.048 0.044 0.042 0.045 0.049 0.716 0.715 0.732 0.831 0.838 0.869 0.844 0.732 0.779 0. 0.335 0.336 0.335 0.329 0.330 0.332 0.334 0.335 0.342 0.346 58.2 58.2 58.5 59.5 59.3 58.4 58.3 58.5 57.9 57.3 In the main paper, our global optimization objective (see Eq. (6)) is combination of three different loss terms, controlled by two hyperparameters. Here, we evaluate the sensitivity of the results to variations in these two weights. From the results in Tab. A2, it can be seen that the weight of the optical flow loss (wflow) does not significantly impact the overall performance. Varying the flow loss weight sometimes leads to slightly better results in other metrics. The weight of the camera trajectory smoothness constraint (wsmooth) exhibits more noticeable effects. When it is set to lower value, the difference in performance remains small, though the RPE performance drops noticeably. However, when the weight is set too high, performance degrades, likely due to the over-constraining of the camera trajectory."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Stability AI",
        "UC Berkeley",
        "UC Merced"
    ]
}