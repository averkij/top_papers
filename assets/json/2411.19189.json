{
    "paper_title": "Video Depth without Video Models",
    "authors": [
        "Bingxin Ke",
        "Dominik Narnhofer",
        "Shengyu Huang",
        "Lei Ke",
        "Torben Peters",
        "Katerina Fragkiadaki",
        "Anton Obukhov",
        "Konrad Schindler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io."
        },
        {
            "title": "Start",
            "content": "Bingxin Ke1 Dominik Narnhofer1 Shengyu Huang1 Lei Ke2 Torben Peters1 Katerina Fragkiadaki2 Anton Obukhov1 Konrad Schindler1 1ETH Zurich 2Carnegie Mellon University 4 2 0 2 8 2 ] . [ 1 9 8 1 9 1 . 1 1 4 2 : r Figure 1. The RollingDepth model takes an unconstrained video and reconstructs corresponding depth video. Unlike methods that rely on video diffusion models, it extends single-image monodepth estimator such that it can process short snippets. To account for temporal context, snippets with varying frame rate are sampled from the video, processed, and reassembled through global alignment algorithm to obtain long, temporally coherent depth videos. Depth is colour-coded from near far."
        },
        {
            "title": "Abstract",
            "content": "Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled renewed interest in video depth. However, naively applying single-image depth estimator to every frame of video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take step back and demonstrate how to turn single-image latent diffusion model (LDM) into state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) multiframe depth estimator that is derived from single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io. 1. Introduction Inferring 3D scene structure from video stream is fundamental capability of vision system. Besides its scientific relevance as an elementary building block of machine perception, it has broad range of applications, including mobile robotics and autonomous driving, augmented reality, media production, and content creation. Traditionally, video would be converted into 3D world model by recovering the camera trajectory with structure-from-motion (SfM) techniques [17, 54], then applying multi-view reconstruction based on either stereo triangulation [15, 71] or, more recently, inverse volume rendering [25, 43]. That approach has the attractive property that it delivers full 3D scene model in common coordinate frame. The price to pay is that it is only feasible under narrowly defined conditions: the camera motion must be just right, and the scene must have static background with cooperative texture and lighting conditions. In practice, both SfM and multi-view reconstruction fail more often than not when applied to in-the-wild videos. This is where video depth comes in. Not all applications require full-scale 3D reconstruction, and it turns out that information about the scene structure can be recovered much more reliably if one aims for more modest goal: augment every video frame with dense 2.5D depth map, in such way that those depth maps are consistent through time. The past years have witnessed tremendous progress in depth estimation from single image, sidestepping camera pose estimation (and often also calibration of the focal length) [5, 24, 50, 69]. common thread is that recent methods build on foundation models trained on internetscale data, such as DINOv2 [45] or StableDiffusion [52], and fine-tune them for depth estimation, often using predominantly synthetic RGB+depth image pairs that can be generated in large quantities and have accurate depth. The underlying, rich visual priors afford these depth estimators excellent zero-shot generalization across scene types, imaging, and lighting conditions. In general, applying single-image depth estimator to video frame-by-frame does not yield satisfactory results, but leads to depth flicker and drift. These artifacts are caused by multiple factors. Most obviously, neither the model training nor the inference procedure have any notion of temporal coherence between adjacent frames. Moreover, monodepth estimation requires scene understanding, which may also suffer from the lack of temporal context (e.g., when partially visible object only becomes recognizable after zooming out). What is more, in video the depth range between nearby and distant scene parts may change all of sudden (e.g., when foreground object enters the viewfield, or when the camera pans to window), making consistent monodepth estimation difficult. Some authors [23, 56] have explored the idea of repurposing generative video models like Stable Video Diffusion [4] for depth prediction. These methods enable information exchange along the time axis and acquire strong flow and motion prior during training, hence they achieve excellent local consistency through time. On the downside, video LDMs besides being computationally demanding are trained for fixed, short sequence lengths and cannot be applied directly to uncurated footage of varying lengths. To be practically useful, the diffusion routine must be wrapped into partitioning scheme that splits the input video for processing and stitches the depth estimates back together, often resulting in low-frequency flickering and gradual drift. We also find that current LDM-based video depth models tend to be less accurate on distant scene parts. Rather than design more refined video LDMs, which require huge resources for training, we take step back and re-examine how far one can take video depth estimation with augmented single-image LDMs. We design set of measures that, taken together, extend per-image monodepth framework like Marigold [24] in way that enables it to handle video input. Importantly, these measures greatly improve local and global consistency across time, while maintaining constant memory footprint such that one can process long sequences. Specifically, we employ rolling inference with sliding window of few frames (typically three, but other numbers are possible). Those snippets are sampled from the video with varying spacing, i.e., they can be immediately adjacent but also dilated along the timeline to cover long-range context. They are then fed into multiframe LDM fine-tuned from single-frame model, with modified cross-frame self-attention mechanism to enable information exchange. To reassemble the snippets, we propose robust optimization-based global co-alignment, followed by averaging the aligned frames. Optionally, the resulting video can be degraded with moderate random noise and denoised again with the same per-snippet LDM to further refine spatial details. To summarize, our approach estimates accurate and temporally consistent video depth without resorting to cumbersome video diffusion models. To that end, we contribute: 1. an LDM for monocular depth estimation in video snippets of few frames, adapted from the Marigold [24] single-frame model but able to capture temporal patterns across frames via self-attention; 2. rolling inference scheme that operates on snippets with multiple different (temporal) resolutions and enables efficient propagation of contextual information through video sequences of arbitrary length (up to minutes); 3. global alignment procedure, based on robust optimization, to recompose the snippets into depth video whose depth values remain consistent over long time periods; 4. an optional refinement of the final output with another round of multi-frame diffusion, where the same LDM is applied starting from moderately degraded video. 2. Related Work 2.1. Monocular Depth Estimation Monocular depth estimation is dense regression task. The pioneering work by Eigen et al. [12] showed that metric depth values can be recovered from single sensors. Successive advancements include including various parameterizations (ordinals, bins, planar guidance maps, piecewise planarity, CRFs, etc.) [2, 13, 30, 32, 38, 44, 47, 75], switching CNN backbones to vision transformers [1, 3, 33, 66], considering camera intrinsics [19, 22, 48, 74], and patchwise processing [5, 35, 36]. To handle in-the-wild settings, extensive internet photo collections are used for training [31, 73]. MiDaS [50] improves the generality by training on mixture of multiple datasets. Depth Anything [68, 69] takes data scaling to the next level by relying on DINOv2 [45], foundational model trained on 142M images in self-supervised manner, and subsequently training with 62M pseudo-labels, 1M real depth annotations, and 0.5M synthetic ones. Recent trends leverage generative models, particularly diffusion models [21, 57], for depth estimation [11, 53, 80, 80]. Marigold [24] proposed to finetune Stable Diffusion [52], generative text-to-image latent diffusion model (LDM) trained with LAION-5B [55], towards affine-invariant depth using 74k samples. This approach has been improved in many aspects including fewer steps [16, 18, 20, 65], finer details [77], and more modalities [14, 20]. 2.2. Video Depth Estimation Video depth estimation calls for dedicated mechanisms to ensure smoothness of adjacent frames, and correct handling of varying depth range. Existing approaches can be grouped into three main categories: test-time optimization, feedforward prediction, and diffusion-based. Test-time optimization methods [7, 28, 42, 78] often rely on camera poses or optical flow and perform optimization for each new video during inference. While these methods can produce depth estimates that are temporally consistent, their dependence on camera poses and long processing time hamper their application to open-world video scenarios. Feed-forward prediction methods estimate depth sequences directly from input videos [34, 59, 61, 72, 76]. For example, DeepV2D [59] integrates camera motion estimation with depth prediction, MAMO [72] adopts memory attention mechanisms, and NVDS [62, 63] introduces stabilization network as postprocessing module. However, the generalization of these methods to in-the-wild videos is often constrained by the limited diversity of training data and model capacity. Very recently, concurrent with our work, several authors have investigated the use of video diffusion models, in particular SVD [4], for video depth. ChronoDepth [56] DepthCrafter [23] and DepthAnyVideo [67] all modify video diffusion for conditional generative depth prediction. From the underlying video diffusion model they inherit high training and inference costs, and restriction to short video clips of at most 100 frames. In contrast, in RollingDepth we explore how to turn an image diffusion model into temporally consistent depth estimator, which can handle long videos of 1000 frames or more. 2.3. Image Diffusion Models for Video Tasks Image diffusion models have been employed in various video inverse problems, such as video generation, inpainting, and super-resolution [10, 29]. large amount of work [49, 70, 79] focusses on video editing, either by finetuning text-to-image diffusion models on video data [39, 64] or through training-free approaches using cross-frame attention and latent fusion [6, 26]. However, these works [6, 39, 64] predominantly address video-to-video translation tasks, where both the input and output reside in RGB space. In contrast, our approach leverages image diffusion priors to generate consistent depth videos, with the additional challenge to accommodate large variations of the depth range, as the near and far planes change often suddenly due to camera and object motion. Implementation tricks when using single-image models, like fixing the initial noise or blending consecutive latent representations, can somewhat mitigate the lack of knowledge w.r.t. temporal coherence, but do not solve it [24]. 3. Method Let RNF 3HW be an RGB video of length NF , the goal of monocular video depth estimator is to predict depth video RNF HW . All frames in that depth video should share common depth scale and shift, i.e., depth values should not drift unless the associated pixel moves relative to the camera. In the following, we present our RollingDepth framework for predicting from x. The proposed approach is based on per-snippet LDM, test-time depth co-alignment, and an optional refinement of the resulting video, as illustrated in Fig. 2. 3.1. Marigold Monocular Depth Recap Several recent methods [11, 24, 53], including our base model Marigold [24], cast monocular depth estimation as conditional image generation, where pre-trained LDM is retargeted to generate the depth map given the input image. To that end, the model progressively adds noise to depth samples di and learns to reverse that degradation, to approximate the conditional distribution p(dixi). In detail, the model is trained to predict the added noise ϵ at each step by minimizing the objective L(θ) = (di 0,xi)Pdi,xi ,tU ,ϵN (cid:104)(cid:13) (cid:13)ϵ ϵθ(di t, xi, t)(cid:13) (cid:13) 2(cid:105) . At inference time the model starts from the input xi and pure Gaussian noise di (0, I), and gradually maps the latter to depth map di 0 by iteratively applying the learned denoising step. For computational efficiency, the denoising process operates low-dimensional latent space Z, with an auto-encoder to map images to latent embeddings, and depth maps back to image space [52]. 3 is ith frame), we construct NT Figure 2. Overview of the RollingDepth Inference Pipeline. Given video sequence (with overlapping snippets using dilated rolling kernel with varying dilation rates, and perform 1-step inference to obtain initial depth snippets ). Next, depth co-alignment optimizes NT pairs of scale and shift values to achieve globally consistent depth throughout the ( full video. An optional refinement step further enhances details by applying additional, snippet-based denoising steps. 3.2. Extension to Snippets Inspired by multi-view diffusion models [27, 40], we extend Marigold [24] to handle multiple frames by modifying its self-attention layers. In each self-attention block, we flatten tokens from all frames in snippet into single sequence, such that the attention mechanism operates across frames and captures spatial and temporal interactions. Unlike video diffusion models with factorized spatial-temporal attention, this approach can handle frames with varying temporal spacing, which makes it possible to sample snippets at lower frame rates and capture long-range dependencies, an advantage when processing long videos. The original Marigold model predicts (affine-invariant) depth between image-specific near and far planes. This parametrization poses problems for video depth estimation, where the depth range can vary over time. We therefore retrain Marigold to predict inverse depth (like several other monodepth estimators [50, 69]), which is less sensitive to such variations, particularly in the far field. 3.3. From Snippets to Video Our multi-frame depth estimator operates on short snippets frames, where NF . As snippets are processed independently, each has its own scale and shift which are arbitrary in the case of affine-invariant methods [18, 24, 50] including Marigold, but will in practice not be aligned perfectly even when using metric depth estimator [36, 48, 74]. To address that ambiguity, we construct overlapping snippets with different dilation rates. The frames shared between different snippets are subsequently used to align all depth predictions to common scale and shift. Dilated Rolling Kernel. We construct multi-scale snippets using the dilated rolling kernel. For instance, for 3-frame snippets with dilation rate (frame spacing) and stride h, the kernel picks frames (xig, xi, xi+g) from the input video, where {g + 1, + 1 + h, + 1 + 2h, . . .}. By varying the dilation rate, we sample snippets with different frame rates, in order to capture temporal dependencies at different time scales. For each snippet of frames, we then predict depth using the multi-frame LDM, to obtain corresponding n-frame depth snippet. Depth Co-alignment. At this stage we have generated NT depth snippets. Each of them has its own scale and shift parameters {(sk, tk), 1 . . . }, which are shared across its constituent frames. Our goal is to jointly compute the NT scale and shift values such that they optimally align all snippets into consistent video. At given frame xi, there are different individual depth maps {di j, = 1 . . . i} originating from different snippets, where can vary from frame to frame. Let k(i, j) be an indexing function that retrieves the snippet index for the j-th depthmap at frame i. To estimate the best alignment we minimize the L1 loss over all individual depth predictions across depth predictions, NF(cid:88) (cid:88) i=1 j=1 min sk>0,tk (cid:12) (cid:12)sk(i,j)di (cid:12) + tk(i,j) di , (cid:12) (cid:12) (cid:12) with the mean depth di = 1 i (cid:88) j=1 (cid:0)sk(i,j)di + tk(i,j) (cid:1) . (1) (2) The solution to eq. (1) is found with gradient descent, stabilized by putting more emphasis on snippets with high di4 malized on per-snippet basis, using the 2nd and 98th percentiles for robustness. We found it important to jointly normalize the values within each snippet rather than normalizing each frame individually. In this way, the same frame is normalized differently depending on the context it appears in, and normalized depths remain comparable within snippet, enabling the model to understand and correctly handle rapid changes in the depth range, which routinely appear in longer video sequences. 4. Experiments 4.1. Implementation Details Training Datasets. To finetune the snippet LDM, we use TartanAir [60], synthetic video dataset with various (indoor and outdoor) scenes, styles, and camera motions. We visually inspect the scenes and select 18 scenes consisting of 369 sequences. Training snippets are randomly sampled from sequence, with minimum overlap ratio of 30%. To increase the diversity of scenes and avoid significant sim-to-real gap we additionally also use Hypersim [51], photorealistic single-image dataset containing 365 diverse scenes, treating images as 1-frame snippets. Training Settings. Training images are resized to 480640 for efficiency, with random horizontal flipping as data augmentation. To align with the refinement setting, we employ depth range augmentation, where we randomly squeeze the normalized depth snippets to smaller range and then slightly rescale and shift the depth range in each frame. As optimizer, we use AdamW [41] with learning rate of 3 105 and exponential decay. Training is run on four Nvidia A100 GPUs with batch size of 32 and takes approximately 18k iterations or two days to converge. Inference Settings. During inference, we fix the snippet length to = 3, with three different dilation rates {1, 10, 25} to capture shortto mid-range temporal relations. For each snippet we perform 1-step inference. Long-range temporal relations are covered by the depth coalignment, which is initialized with sk = 1 and tk = 0 and optimized with 2000 steps of gradient descent, using the Adam optimizer. For the optional refinement, we start at timestep /2 of the diffusion trajectory and perform 10 denoising steps, gradually reducing the dilation rate from 6 to 1. Input images are resized to maximum side length of 768 pixels. For evaluation, the final result is up-sampled to match the original resolution in the dataset. 4.2. Evaluation Evaluation Datasets. We evaluate RollingDepth on four datasets that include both static and dynamic scenes with varying camera and scene motions: PointOdyssey [81] is synthetic dataset with individually animated characters that move independently, designed for long-term tracking. Figure 3. Depth Refinement encodes the co-aligned depth video into latent space, contaminates it with moderate amount of noise, then denoises it with series of reverse diffusion steps with decreasing snippet dilation rate. After each step, overlapping latents are averaged to propagate information between snippets. lation rates, and additional regularization. Once the depth snippets have been aligned in common frame with the estimated scale and shift values, the depth maps at every frame xi are obtained by taking the pixel-wise mean di, resulting in single, consistent depth video with one depth map per frame. See Appendix A.1 for further details. Depth Refinement. To enhance visual quality and capture finer details, we optionally apply diffusion-based refinement step to the merged depth video d, as illustrated in Fig. 3. The video is again encoded into latent space frame by frame, and contaminated with moderate amount of noise, corresponding to step /2 of the diffusion schedule, halfway between the clean latent and pure noise. The degraded video is again split into snippets with the dilated rolling kernel and each snippet is denoised individually with the same LDM as above. To integrate information across overlapping snippets, the latent embeddings of every frame are averaged after every denoising step. We find that this partial (reverse) diffusion works best when applied in coarse-to-fine manner in time, starting with large snippet dilation rate and gradually decreasing it along the denoising process. The refinement process enhances high-frequency detail without altering the global scene depth layout, at the cost of increased inference time due to the additional round of denoising diffusion. 3.4. Multi-Frame Training We exploit the flexible design of the multi-frame selfattention mechanism to fine-tune the model with varying snippet lengths. Training snippets are randomly picked to have one, two, or three frames, making sure that the motion between frames is small enough to have overlapping view frustra. To fully utilize the value range of the diffusion model for best performance, inverse depth values are nor5 Table 1. Quantitative comparison of RollingDepth with baseline methods on zero-shot benchmarks. Bold numbers are the best, underscored second best, numbers in the bracket after each dataset denote video sequence length. RollingDepth demonstrates superior performance across both short and long video sequences, despite being an image-based model. PointOdyssey (250) Abs Rel δ1 g e f Marigold [24] DepthAnything [68] DepthAnythingv2 [69] i NVDS (DPT-Large) [62] ChronoDepth [56] DepthCrafter [23] RollingDepth (ours, fast) RollingDepth (ours) 14.9 16.3 14. 26.6 51.7 36.3 9.6 9.6 80.4 76.0 81.4 68.2 71.2 75.0 90.4 90.5 ScanNet (90) Bonn (110) Abs Rel 14.9 12.9 13.3 18.5 16.8 12.7 10.1 9.3 δ1 78.3 84.0 82.6 67.7 73.8 84.3 89.7 91.6 Abs Rel 10.5 9.9 10.5 10.5 10.9 6. 7.9 7.9 δ1 86.7 89.4 87.4 88.1 86.9 96.7 93.6 93.9 DyDToF (200) δ1 Abs Rel DyDToF (100) δ1 Abs Rel 25.3 25.4 24.8 24.7 26.9 22.1 17.7 17.3 55.5 54.3 55.9 56.0 53.2 60. 69.6 71.7 16.4 16.4 16.0 18.8 19.9 16.2 12.7 12.3 73.5 75.6 76.6 69.3 66.5 74. 81.6 83.0 Inverse depth version, retrained with the original training code. Run at half-precision (fp16), with dilation rates {1, 25}, without refinement. Figure 4. Qualitative comparison between different methods. RollingDepth excels at preserving fine-grained details (cf . the chandelier in the first sample and the tripod in the third sample) and recovering accurate scene layout (cf . the far plane in the second sample). We filter out overly simplistic toy scenes and retain 35 sequences. For each sequence, we follow the videodepth literature and exclude frames with camera zoom, then select the first 250 frames in each sequence. ScanNet v2 [8] is an indoor dataset of static scenes recorded with the Kinect RGBD sensor. We use its test set of 100 sequences, downsample the frame rate by factor 3, then keep the first 270 frames of each sequence. Bonn RGBD [46] is an RGB-D dataset of moving people in indoor spaces, captured with the OptiTrack Prime13 sensor. Following [23], we use frames 30140 from five different dynamic scenes. DyDToF [58] is photorealistic synthetic dataset featuring moving objects including people and animals. It has several videos per scene, we always take the first video and create two subsets of different lengths from it, by clipping frames 50-250, respectively frames 50-150. Evaluation Protocol. We extend the affine-invariant depth evaluation protocol [50] to videos, i.e., depth predictions ˆd are aligned to the ground truth with scale and shift found with least squares fitting, where we fit one pair of transformation parameters per video, i.e., all frames in video share common scale and shift. We quantify the depth estimation accuracy with two standard metrics [23, 24, 50, 62]: the absolute mean relative error (AbsRel), defined as j=1 ˆdj dj/dj, where is the total number of 1 pixels; and the δ1-accuracy, which measures the fraction of pixels for which max( ˆdj/dj, dj/ ˆdj) < 1.25. Metrics are always given as percentages. (cid:80)M 4.3. Comparison with Other Methods for We compare RollingDepth against six state-of-the-art methods zero-shot monocular depth estimation: Marigold [24], DepthAnything [68] and DepthAnythingv2 [69], which are single-frame methods; as well as NVDS [63], ChronoDepth [56], and DepthCrafter [23], which are video-based approaches. 6 Figure 5. AbsRel error over time: The line plot (left) shows the depth error at every individual frame, end-of-line numbers are the average error across the video. The images (right) display error maps (low high) for two specific frames. RollingDepth achieves the lowest error overall, competing methods recover scene layout less faithfully and tend to be biased towards the foreground or the background. Figure 6. Qualitative comparison of depth predictions (near far) from in-the-wild videos. To graphically show temporal consistency, we display temporal profiles (red box) for fixed column (marked with red line). RollingDepth picks up subtle details like accessories and wrinkled cloth, and mitigates spurious depth discontinuities (cf . background in temporal profile of the first sample) in time. As Quantitative Comparison. shown in Tab. 1, RollingDepth outperforms both single-frame and videobased approaches across multiple datasets and different sequence lengths, often by considerable margins. We attribute this to its ability to combine the accuracy of imagebased models with the temporal coherence afforded by our snippet-based inference and global depth co-alignment. On PointOdyssey, which includes many challenging scenes with highly variable depth ranges, RollingDepth achieves by far the best result. Methods based on video models struggle on this dataset, and are in fact even unable to match the performance of single-frame methods. We observe that the performance of video models drops especially in scenes with sudden, large changes in the depth range (e.g. hand gesture in front of the camera). We hypothesize that the underlying video prior is too rigid and prevents correct adaptation to the rapid change, see Appendix B.1 for details. Also on DyDToF, RollingDepth greatly reduces the error compared to other methods, again underscoring its ability to handle dynamic scenes and variations of the near and far planes. Still, the good performance is not limited to dynamic scenes with strong depth variations. RollingDepth also performs well on indoor data, reaching the lowest error on the static ScanNet scenes and the second-lowest error on the Bonn data. Here DepthCrafter shines we observe that it generally tends to do well in scenes dominated by foreground objects, particularly humans. Qualitative Comparison. To make our findings more tangible, we provide qualitative comparisons both on evaluation data and on in-the-wild examples. Fig. 4 confirms that RollingDepth consistently produces high-quality depth maps that preserve fine detail, both near the camera in the distance. DepthCrafter and ChronoDepth produce locally smooth videos with little frame-to-frame flicker, but have 7 tendency to distort the overall scene layout in way that certain objects are segmented well but placed at incorrect (relative) depths. Single-image estimators are seemingly more accurate in that respect, but suffer from flickering and lack of temporal coherence. We further illustrate these trends in Fig. 5, where we plot per-frame errors, as well as per-pixel errors for selected frames. To demonstrate generalization to real-world video clips, Fig. 6 shows depth predictions for videos collected from the internet. Also in these cases, RollingDepth accurately recovers fine details and maintains long-term coherence. To better illustrate the evolution of the depth estimate over time, we extract temporal profiles for fixed image columns. They exhibit no significant high-frequency variations along the time axis that would indicate frame-to-frame flicker. We also do not observe drift or unwarranted jumps in the depth values that would indicate systematic biases. DepthCrafter for the most part also recovers plausible depth, but misses depth variations within the main segments and sometimes exhibits instabilities along the time axis. Chronodepth recovers depth boundaries rather well, but delivers billboardlike, layered depth maps. 4.4. Ablation Studies We validate our main hyper-parameters and design choices on subset of 10 sequences from the PointOdyssey test set and 20 sequences from the ScanNet test set. Dilation of Initial Predictions. We start by ablating the arguably most crucial hyper-parameter of RollingDepth, the dilation rate for snippet sampling, see Tab. 2. The base setting uses only dilation rate {1} for minimal information exchange and smoothness between adjacent frames. Having high dilation rate {1,25} gives the model access to longerterm motion patterns on the order of 1 second and greatly stabilizes the co-alignment step, which in turn reduces the AbsRel error by >6 percept points on PointOdyssey and by >2 percent points on the (static) ScanNet. This is what we use in our fast setting (c.f. Tab. 1), which takes 81s for 768432 video of 250 frames (ChronoDepth: 121s, DepthCrafter: 284s). An additional, intermediate dilation rate {1,10,25} further intensifies the information exchange across time. This further boosts the quality of the estimated depth maps, but as expected yields diminishing returns. Effectiveness of Co-Alignment and Refinement. We further isolate the effect of the RollingDepths components, see Tab. 3. The snippet diffusion step is mandatory to obtain any depth estimates at all and cannot be left out. For the experiment we switch on and off the two remaining steps, co-alignment and refinement, and test all combinations. Simply merging overlapping latents without prior alignment proves to be insufficient, i.e., their individually estimated depth ranges are too inconsistent to average them into coherent sequence. The refinement step cannot fix Table 2. Ablation of dilation rates for snippet prediction. We report values before the optional refinement step. The minimal base setting uses only dilation rate 1. Adding high dilation rate 25 brings marked performance gain. Yet another dilation rate 10 gives further, smaller boost. Dilation rates Abs Rel {1} {1, 25} {1, 10, 25} PointOdyssey δ1 75.5 89.5 89. 16.7 10.2 10.2 ScanNet Abs Rel 12.8 10.6 9.9 δ1 83.2 88.8 90.1 Table 3. Ablation of components. Depth co-alignment is crucial functionality for the snippet-based strategy of RollingDepth, whereas the additional refinement has only small effect on the performance metrics, despite visibly enhanced image detail. Co-Alignment Refinement Abs Rel PointOdyssey δ1 84.4 84.6 89.6 89.8 13.0 13.0 10.2 10.2 ScanNet Abs Rel 12.4 12.3 9.9 9. δ1 84.3 84.8 90.1 90.2 that problem. Conversely, the co-alignment does the heavy lifting to fuse depth snippets with different scales and shifts into coherent video and contributes the lions share of the improvement. Subsequent refinement of the aligned video only results in marginal increase of the performance metrics, but visibly improves the result by recovering sharp details that have been missed or blurred in the preceding steps. 5. Conclusion We have introduced RollingDepth, novel method for monocular video depth estimation that is derived from single-image (latent) diffusion model. The core components of our method (i) are monodepth estimator for short snippets, sampled at various frame rates to capture temporal context at different time scales; (ii) an optimization-based co-alignment procedure that optimally registers all snippets of video into common depth range; and (iii) an optional refinement step, again based on the same denoising diffusion scheme for snippets, that enhances fine details in the depth video. RollingDepth strikes favorable balance between accurate per-frame depth prediction and temporal coherence, and can process long video with hundreds of frames. It empirically delivers best-in-class performance across multiple datasets, also outperforming alternatives derived from full-blown video diffusion models. That being said, the RollingDepth framework is flexible and offers the possibility to replace individual components. For instance, an interesting avenue for future work would be to swap out the snippet-based refinement and replace it with generative video model or flow-based method for even better motion reconstruction. 8 Acknowledgments. We thank Yue Pan, Shuchang Liu, Nando Metzger, and Nikolai Kalischek for fruitful discussions. We are grateful to redmond.ai for providing GPU resources."
        },
        {
            "title": "References",
            "content": "[1] Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat Kaur, and Bingbing Liu. Bidirectional attention network for monocular depth estimation. In ICRA, 2021. 3 [2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using adaptive bins. In CVPR, 2021. 3 [3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. ZoeDepth: Zero-shot transfer by combining relative and metric depth. arXiv:2302.12288, 2023. 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv:2311.15127, 2023. 2, 3 [5] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv:2410.02073, 2024. 2, 3 [6] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In ICCV, 2023. 3 [7] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In ICCV, 2019. [8] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 6 [9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 12 [10] Giannis Daras, Weili Nie, Karsten Kreis, Alex Dimakis, Morteza Mardani, Nikola Borislavov Kovachki, and Arash Vahdat. Warped diffusion: Solving video inverse problems with image diffusion models. NeurIPS, 2024. 3 [11] Yiqun Duan, Xianda Guo, and Zheng Zhu. DiffusionDepth: Diffusion denoising approach for monocular depth estimation. arXiv:2303.05021, 2023. 3 [12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. NeurIPS, 2014. 2 [13] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In CVPR, 2018. 3 [14] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In ECCV, 2024. [15] Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: tutorial. Foundations and Trends in Computer Graphics and Vision, 9(1-2):1148, 2015. 2 [16] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. arXiv:2409.11355, 2024. 3, 12 [17] Carsten Griwodz, Simone Gasparini, Lilian Calvet, Pierre Gurdjos, Fabien Castan, Benoit Maujean, Gregoire De Lillo, and Yann Lanthony. AliceVision Meshroom: An openIn ACM Multimedia, source 3d reconstruction pipeline. 2021. 1 [18] Ming Gui, Johannes Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast monocular depth estimation with flow matching. arXiv:2403.13788, 2024. 3, 4 [19] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus, , and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In ICCV, 2023. 3 [20] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv:2409.18124, 2024. 3 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [22] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv:2404.15506, 2024. 3 [23] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv:2409.02095, 2024. 2, 3, 6, 12 [24] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 2, 3, 4, 6, 12 [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d Gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4):1391, 2023. 2 [26] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In ICCV, 2023. [27] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In CVPR, 2024. 4 [28] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 3 [29] Taesung Kwon and Jong Chul Ye. Solving video inverse problems using image diffusion models. arXiv:2409.02574, 2024. 3 [30] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv:1907.10326, 2019. 3 9 [31] Zhengqi Li and Noah Snavely. MegaDepth: Learning singleview depth prediction from internet photos. In CVPR, 2018. 3 [32] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. arXiv:2204.00987, 2022. 3 [33] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. Machine Intelligence Research, pages 118, 2023. [34] Zhaoshuo Li, Wei Ye, Dilin Wang, Francis Creighton, Russell Taylor, Ganesh Venkatesh, and Mathias Unberath. Temporally consistent online depth estimation in dynamic scenes. In WACV, 2023. 3 [35] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchfusion: An end-to-end tile-based framework for highIn CVPR, resolution monocular metric depth estimation. 2024. 3 [36] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchrefiner: Leveraging synthetic data for real-domain highIn ECCV, resolution monocular metric depth estimation. 2024. 3, 4 [37] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In WACV, 2024. 12 [38] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, and Luc Van Gool. VA-DepthNet: variational approach to single image depth prediction. In ICLR, 2023. 3 [39] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, 2024. [40] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv:2309.03453, 2023. 4 [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 5 [42] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics, 39(4), 2020. 3 [43] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [44] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying output space of visual tasks via soft token. In ICCV, 2023. [45] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193, 2023. 2, 3 [46] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019. 6 [47] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and Luc Van Gool. P3depth: Monocular depth estimation with piecewise planarity prior. In CVPR, 2022. 3 [48] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth: In CVPR, Universal monocular metric depth estimation. 2024. 3, 4 [49] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: FusIn ing attentions for zero-shot text-based video editing. ICCV, 2023. 3 [50] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 2020. 2, 3, 4, [51] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synIn thetic dataset for holistic indoor scene understanding. ICCV, 2021. 5 [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [53] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv:2302.14816, 2023. 3 [54] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In CVPR, 2016. 1 [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. 3 [56] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv:2406.01493, 2024. 2, 3, 6, [57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [58] Zhanghao Sun, Wei Ye, Jinhui Xiong, Gyeongmin Choe, Jialiang Wang, Shuochen Su, and Rakesh Ranjan. Consistent direct time-of-flight video depth super-resolution. arXiv:2211.08658, 2022. 6, 12 [59] Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. In ICLR, 2020. 3 [60] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, 2020. 5 [61] Yiran Wang, Zhiyu Pan, Xingyi Li, Zhiguo Cao, Ke Xian, and Jianming Zhang. Less is more: Consistent video depth In ACM MM, estimation with masked frames modeling. 2022. 3 [62] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video depth stabilizer. In ICCV, 2023. 3, 10 [78] Zhoutong Zhang, Forrester Cole, Richard Tucker, William Freeman, and Tali Dekel. Consistent depth of moving objects in video. ACM Transactions on Graphics (TOG), 40(4):1 12, 2021. 3 [79] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards consistent video editing with text-to-image diffusion models. NeurIPS, 2024. 3 [80] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. arXiv:2303.02153, 2023. 3 [81] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. 2023. 5, 12 [63] Yiran Wang, Min Shi, Jiaqi Li, Chaoyi Hong, Zihao Huang, Juewen Peng, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Nvds+: Towards efficient and versatile neural stabilizer for video depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3, 6 [64] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023. [65] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv:2403.06090, 2024. 3 [66] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformer-based attention networks for continuous pixel-wise prediction. In ICCV, 2021. 3 [67] Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, and Tong He. Depth any video with scalable synthetic data. arXiv:2410.10815, 2024. 3 [68] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 3, 6 [69] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 2, 3, 4, 6 [70] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia, 2023. [71] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth inference for unstructured multiview stereo. In ECCV, 2018. 2 [72] Rajeev Yasarla, Hong Cai, Jisoo Jeong, Yunxiao Shi, Risheek Garrepalli, and Fatih Porikli. Mamo: Leveraging memory and attention for monocular video depth estimation. In ICCV, 2023. 3 [73] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv:2002.00569, 2020. 3 [74] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3D: Towards zero-shot metric 3d prediction from single image. In ICCV, 2023. 3, 4 [75] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. NeWCRFs: Neural window fully-connected CRFs for monocular depth estimation. In CVPR, 2022. 3 [76] Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, and Youliang Yan. Exploiting temporal consistency for real-time video depth estimation. In ICCV, 2019. 3 [77] Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, and Christopher Schroers. Betterdepth: Plug-and-play diffusion refiner for zero-shot monocular depth estimation. NeurIPS, 2024."
        },
        {
            "title": "Appendix",
            "content": "This supplementary material includes additional implementation details and experimental results. A. Implementation Details A.1. Depth Co-Alignment As discussed in Section 3.5 in the main paper, let k(i, j) denote an indexing function that returns the snippet index corresponding to the j-th depthmap of i-th frame. To make the optimization more robust, we include an additional loss term in depth space while predicting inverse depth. We further scale the loss terms by their respective mean absolute value per frame to increase the numerical stability. Additionally, soft constraints on sk, tk are applied: min sk>0,tk 1 (cid:99)di (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) NF(cid:88) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + λ1 max(0, 1 sk(i,j))2 + λ2tk(i,j), (cid:101)di (cid:101)µi di (cid:98)µi (cid:99)di (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) j=1 i= + (3) = sk(i,j)di where (cid:99)di inverse depth are defined as + tk(i,j). The mean depth and mean di = 1 i (cid:88) j=1 (cid:99)di (cid:101)di = 1 i (cid:88) j=1 1 , (cid:99)di (4) with the corresponding mean absolute values per frame given by (cid:98)µi = 1 HW HW (cid:88) (cid:12) (cid:12) (cid:12)di (cid:12) (cid:12) (cid:12) (cid:101)µi = 1 HW HW (cid:88) (cid:12) (cid:12) (cid:12) (cid:101)di (cid:12) (cid:12) (cid:12) . (5) We found that λ1 = 101, λ2 = 101 work well in practice. A.2. Additional Training and Inference Details During training, we apply gradient accumulation to increase the effective batch size, to 32. To better mix the samples with varying snippet lengths, every mini-batch is sampled randomly and can have different snippet lengths. For the initial depth prediction, we apply the same random Gaussian noise to all frames. When applying refinement, the same noise is used to perturb the (encoded) coaligned depth sequence. The denoising process then starts from timestep /2. A.3. Evaluation Datasets PointOdyssey [81] contains several sequences that feature overly simplified toy scenes, as well as some with smoke, for which depth estimation is ambiguous (cf . Fig. S1). We exclude these sequences from the test dataset, detailed list of selected frames will be provided with the code. For evaluation, pixels on windows are excluded due to inconsistent depth labels. In ScanNet [9], the RGB images and depth labels include thin black border. Following DepthCrafter [23], we crop the RGB images by removing 8 pixels from the top and bottom and 12 pixels from the left and right. Similarly, we crop the depth maps by removing 4 pixels from the top and bottom and 6 pixels from the left and right. For DyDToF [58], we exclude depth values beyond 23m, corresponding to less than 1% of the depth values. Figure S1. Examples of PointOdyssey toy scenes (left) and scenes with smoke (right). A.4. Baseline Methods We evaluate baseline methods using their recommended default settings. For DepthCrafter [23], the inference is performed with 25 diffusion steps, using an overlap of 25 frames for videos longer than 110 frames. For ChronoDepth [56], inference comprises 10 denoising steps, with window size of 10 (referred to as num-frames in the code) and stride of 9 (referred to as denoise-steps in the code). For Marigold [24], we retrained an inverse depth version using the trailing scheduler setting [16, 37]. Under this configuration, 1-step inference with single model achieves performance comparable to the original configuration with multi-step inference and ensembling, so we utilize the former, more efficient setting. B. Additional Experiment Results B.1. Failure cases of video models on PointOdyssey We provide further examples from the PointOdyssey dataset where video-based methods struggle. Fig. S2 features scenes with large depth changes, such as hand gestures in front of the camera or objects entering the near field. These sudden changes require rapid alterations of the depth range, both before and after the event. Video models tend to produce incorrect overall scene layout in such cases, we hypothesize that they try too hard to equalize the depth range throughout the scene. B.2. Failure Cases of RollingDepth While our proposed method handles changing depth range more robustly than video models, it also has certain limitations. Two examples are shown in Fig. S3. RollingDepth sometimes misjudges the depth of cloudy skies. Another source of error is transparent surfaces such as glass windows, where subtle variations of transparency or reflections may cause the depth to oscillate between the glass and the scene behind it common issue of depth estimators. Figure S2. Examples of PointOdyssey samples that challenge video models. In the cases above, the (inverse) depth range varies significantly across frames. The arrows highlight situations where video models yield distorted depth maps. In the first two rows, this occurs in regions where the depth deviates significantly from the surrounding scene. In the last row, the depth predictions get drawn towards the near plane to match the object close to the camera, biasing the depth in the far field. Figure S3. The two samples on the left show incorrect depth predictions in the cloudy sky. The two samples on the right show inconsistencies between different frames of the same video, where the depth at the glass windows fluctuates between the solid and transparent states."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "ETH Zurich"
    ]
}