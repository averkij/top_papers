{
    "paper_title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning",
    "authors": [
        "Yu Li",
        "Zhuoshi Pan",
        "Honglin Lin",
        "Mengyuan Sun",
        "Conghui He",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 1 5 7 1 . 7 0 5 2 : r Can One Domain Help Others? Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Yu Li1,2, Zhuoshi Pan1,2, Honglin Lin1,2, Mengyuan Sun1,2, Conghui He1,2, Lijun Wu1,2 1OpenDataLab, 2Shanghai Artificial Intelligence Laboratory Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). Existing research has predominantly concentrated on isolated reasoning domainssuch as mathematical problemsolving, coding tasks, or logical reasoning. However, real-world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning (RL) remains poorly understood. To bridge this gap, we present systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactionsincluding mutual enhancements and conflictsthat emerge during combined cross-domain training. (3) To further understand the influence of supervised fine-tuning (SFT) on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors (e.g., Chinese vs. English datasets). Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs. Date: July 24, 2025 Correspondence: Lijun Wu, wulijun@pjlab.org.cn Code: https://github.com/Leey21/A-Data-Centric-Study Equal contribution: Yu Li, Zhuoshi Pan, Honglin Lin"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) [42, 19, 35, 20, 44], exemplified by DeepSeek-R1-Zero [8], have demonstrated that Reinforcement Learning (RL) can substantially enhance the reasoning capabilities of Large Language Models (LLMs) even without relying on supervised fine-tuning (SFT) [46, 6, 3]. This approach has revealed emergent reasoning capacities, notably through length-dependent performance improvements. Later, multiple studies building upon this framework have validated the effectiveness of RLVR across specialized reasoning domains. For instance, Logic-RL [38] has significantly advanced deductive reasoning, while OpenReasoner-Zero [11] has set new performance benchmarks in mathematical reasoning tasks via RLdriven methods. These successes highlight the broad versatility and effectiveness of RLVR as post-training framework to enhance reasoning skills across diverse domains. 1 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Despite these breakthroughs, existing research has largely concentrated on reasoning tasks within isolated domains, such as mathematical problem-solving [45], code generation [18], or logical reasoning [22] tasks individually. In practice, however, comprehensive reasoning [16, 24] often demands the seamless integration of multiple cognitive skills. Crucially, the interactions among these reasoning skills under RLVRparticularly regarding how domain-specific training influences cross-domain generalization, training dynamics, reward structures, curriculum strategies, and training languageshave remained underexplored. comprehensive, systematic investigation of these multi-domain interactions is thus essential to understand and optimize RLVR for holistic reasoning applications. In this paper, we conduct systematic study of multi-domain reasoning under the RLVR paradigm, explicitly focusing on three critical reasoning domains: Math, Code, and Puzzle. Leveraging the Group Relative Policy Optimization (GRPO) [30] algorithm and the Qwen-2.5 [27] model family, (1) we first examine the impacts of single-domain training on in-domain performance and cross-domain generalization. (2) Then, we identify complex interactions, including mutual enhancements and conflicts, that emerge when integrating multiple domains during training. (3) To further elucidate the role of supervised fine-tuning in enhancing RL effectiveness, we systematically analyze performance differences between base and instruct models. (4) Moreover, our analysis explores critical training strategies, such as curriculum learning, variations in reward design, and language-specific effects (e.g., Chinese versus English training datasets). Through rigorous experimental evaluation, we uncover nuanced insights into domain interactions, revealing fundamental mechanisms that influence both domain-specific expertise and generalized reasoning capabilities. Our contributions provide valuable guidelines for future research aiming to refine RL methodologies, ultimately fostering more robust and integrated multi-domain reasoning in LLMs. The primary findings of this study are summarized as follows:"
        },
        {
            "title": "Overall Takeaways",
            "content": "Puzzle and math data provide mutual support. Logical reasoning and mathematical capabilities complement each other and enhance overall model performance. Code reasoning has mixed cross-domain effects. It strengthens reasoning transfer for the instruct model but may constrain the base models reasoning capacity. Cross-domain data leads to more robust performance. Combining diverse data often results in stronger or more balanced model capabilities, but requires more sophisticated design to address conflicts that may arise between different domains. SFT boosts the effectiveness of RL. Incorporating an SFT stage before RL leads to substantial improvements in model performance. Template consistency is critical. Misalignment between training and evaluation templates can significantly degrade performance, which also indicates that the robustness of RLVRs generalization ability is challenged when trained on specific domains. Policy refresh Benefits. Periodic updates to the reference model and optimizer state in curriculum learning can somewhat improve model stability and performance. Reward design should adapt to difficulty. Tailoring reward settings to how the model performs on the training data can improve learning efficiency. RLVR is language-sensitive. Models trained in Chinese underperforms that trained in English with consistent performance gap. Besides the above overall takeaways, more detailed and throughful observations are illustrated in specific sections in the following studies. 2 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "2 Experimental Configuration",
            "content": "This study aims to explore the models fine-grained reasoning capabilities from data-centric perspective, through various training approaches including single-domain data training, cross-domain data combination, curriculum learning, different reward settings, and training languages."
        },
        {
            "title": "2.1 Multi-Domain Training Setup",
            "content": "We categorize our reasoning domains into Math, Code, and Puzzle. To support multi-domain training, we curate domain-specific datasets for these areas, as detailed in Table 1. (1) For the Math domain, we select the popular DeepScaleR (DSR) [21] and CountDown (CD) [22]. (2) In the Code domain, our experimental data consists of CodeR1-12k [18], which includes 2K reliable LeetCode [36] data and 10K verified data filtered from 26K TACO [15] data. (3) For the Puzzle domain, we focus on two main categories: Knights-and-Knaves (KK) [37] and Logic Puzzle Baron (LPB) [2]. Since the LPB dataset lacks ground-truth answers, we utilize DeepSeek-R1 [8] to annotate 2.4K easy-level puzzles, treating these annotations as pseudo ground truth answer for our RL training. For consistency across domains, we randomly sample larger datasets like DSR (40.3k) and CD (490k) to 10k samples, equalizing the data scale for subsequent training. Table 1: Datasets for multi-domain training."
        },
        {
            "title": "Math",
            "content": "DeepScaleR (DSR) [21] CountDown (CD) [22]"
        },
        {
            "title": "Code",
            "content": "CodeR1-12k [18]"
        },
        {
            "title": "Puzzle",
            "content": "Knights-and-Knaves (KK) [37] Logic Puzzle Baron (LPB) [2] 10k 10k 12k 5.4k 2.4k Binary 0-1 Binary 0-1 Binary 0Binary 0-1 Proportional 0-1 For the reward, we design task-specific schemes based on careful analysis of each dataset and the models initial performance. The LPB dataset stands out for its higher difficulty: models often fail to produce correct answers in single attempt at the start of training. To address this, LPB uses proportional 01 reward based on the fraction of correctly predicted cells, while all other datasets adopt simpler binary 01 reward based solely on final answer correctness, without additional format checks. More details on reward design are provided in Section 7. For model selection, we adopt the Qwen2.5-7B-Base and Qwen2.5-7B-Instruct models as starting points for training. Notably, as emphasized in [20], training templates play crucial role in both the training and testing phases. Our experiments demonstrate that template consistency is crucial; hence, we standardize the use of the R1-template  (Table 2)  [8] during training. In testing, we also adopt the R1-template to ensure consistency between training and testing phases. We observe that mismatched templates severely degrade model performance, with detailed analysis provided in Section 5. Table 2: Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question. R1-template: conversation between the User and Assistant. The User asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process internally and then provides the User with the answer. The reasoning process and the answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant: For the optimization algorithm, we adopt Group Relative Policy Optimization (GRPO) [30] as the core RL algorithm. Compared with Proximal Policy Optimization (PPO) [29], GRPO dispenses with 3 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning the traditional value model. Instead, it evaluates the advantage of different responses by assessing the quality differences among answers within rollout group. GRPO formally maximizes the following objective: LGRPO(θ) = Eτπθ min (cid:34) (cid:16) rθ(τ)A(τ), clip(rθ(τ), 1 ϵ, 1 + ϵ)A(τ) (cid:35) (cid:17) , (1) where τ denotes the response sampled from the current policy πθ, and rθ(τ) = πθ (τ) πold(τ) represents the probability ratio between the current policy and the previous policy before each actor update. Unlike PPO, the advantage in GRPO does not depend on critic model. Instead, it estimates the advantage by calculating baseline directly from the rollout groups scores {Ri}iG(τ): A(τ) = Rτ mean({Ri}iG(τ)) std({Ri}iG(τ)) . (2) For Experimental Framework, all experiments are conducted using the veRL framework [31]. Both training and testing are conducted on cluster equipped with 8 A100 GPUs."
        },
        {
            "title": "2.2 Evaluation Settings",
            "content": "To ensure comprehensive evaluation of model performance, we employ representative benchmarks across three key domains: mathematical reasoning, code generation, and logical problem-solving. Math Domain: We evaluate in-domain mathematical reasoning using MATH500 [10], AIME241, and CountDown [22]. Notably, our implementation of MATH500 adopts strict 0-shot evaluation, without providing any prior examplesthis contrasts with many existing works where the number of shots is often unspecified. For CountDown, we follow the dataset split defined in TinyZero [22] and augment it with 24-game dataset (1.36k)2, which we also evaluate under 0-shot setting. Code Domain: We utilize HumanEval [4] and MBPP [1] to evaluate code generation proficiency. For MBPP, we employ 3-shot prompting, while HumanEval is evaluated in 0-shot setting. Puzzle Domain: We assess performance using test sets derived from KK (the datasets own test set) and ZebraLogicBench (Zebra) [17], which provide diverse scenarios for evaluating logical reasoning abilities. These benchmarks are evaluated in 0-shot configuration. Evaluation Details: All evaluations are performed using OpenCompass [7] toolkit, conducted with consistent hyperparameters: temperature = 0.7, top-p = 0.95, and maximum output length of 8,192 tokens. We emphasize that many existing studies report inconsistent results when reproducing baseline models or conducting new evaluations, often due to misaligned templates or unspecified few-shot configurations. To promote reproducibility, we provide complete details of our prompt templates and few-shot examples in Appendix C, and encourage future work to maintain similar transparency in benchmark reporting."
        },
        {
            "title": "3 Performance with Single-Domain Data",
            "content": "In this section, we evaluate the performance of models trained via RL using single-domain data. Our goal is to investigate the impact of single-domain training on both in-domain and out-of-domain (OOD) benchmark performance, providing insights into the models generalization capabilities. Additionally, 1https://huggingface.co/datasets/AI-MO/aimo-validation-aime 2https://huggingface.co/datasets/nlile/24-game 4 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning findings from single-domain training will guide the design of subsequent experiments involving combined-domain data. In all experimental results, Blue denotes positive improvements, while Orange indicates decline relative to the baseline model. For clarity in the subsequent analysis, we adopt the notation Base-DSR and Instruct-DSR to represent models trained on the DSR dataset using the Base model and the Instruct model, respectively. The same naming convention applies to other datasets."
        },
        {
            "title": "3.1 Math Domain",
            "content": "Next, we focus on the math domain, conducting all experiments under identical settings to ensure fair comparison. Key training hyperparameters are detailed in Table 3. Given that mathematical tasks often require longer Chains of Thought (CoT) for reasoning [39, 34, 26, 23], we set larger max token for training. For brevity, Batch Size is abbreviated as BS. Table 3: Key hyperparameters for math domain."
        },
        {
            "title": "Max Token Rollout BS Mini BS",
            "content": "8,192 256 128 LR 1"
        },
        {
            "title": "Rollout Times Epochs",
            "content": "8 12 As shown in Table 4, our results highlight two key findings: (1) RLVR enhances in-domain performance. Across all math domain experiments, RLVR consistently improves average model performance. For instance, the Base-DSR model increases MATH500 accuracy by 19.60 over the base models 56.40, while the Base-CD model boosts CountDown accuracy by 75.56, far surpassing the base models 1.05. Similar improvements are observed with instruct models. However, both the base and Base-DSR models perform poorly on the CountDown dataset, achieving only 1.05 and 0.04, respectively. Analysis of model outputs reveals that the base model struggles to meet the task requirement of using all numbers exactly once, highlighting its limited instruction-following capabilities without specialized training. (2) Math training improves puzzle performance but impairs coding skills. Math training improves puzzle-solving performance, with Base-DSR and Base-CD models increasing puzzle averages to 24.08 and 21.13, respectively, from 9.07 (base). However, coding performance declines significantly; for instance, the Base-CD models code performance drops to 29.59 from 67.46. This suggests that while math training enhances puzzle-solving capabilities, it may hinder coding skills due to differing reasoning requirements. Data Base DSR CD DSR&CD Instruct DSR CD DSR&CD Table 4: Model performance (%) after training in the math domain. Math Code Puzzle MATH500 CountDown AIME24 Avg. HumanEval MBPP Avg. KK Zebra Avg. 56. 76.00 67.20 72.00 69.00 72.60 72.40 74.60 1.05 0.04 76.61 53.77 24. 35.33 66.89 64.79 10.00 13.33 13.33 16.67 13.33 10.00 20.00 13.33 22. 29.79 52.38 47.48 35.56 39.31 53.10 50.91 5 70.12 82.00 23.78 66. 82.93 78.05 79.88 80.49 64.80 39.20 35.40 62.00 62.80 53.60 61.20 54. 67.46 17.86 0.27 60.60 29.59 64.23 26.71 21.40 26.43 21.46 21.11 18. 72.87 10.14 31.50 65.82 70.54 67.44 25.00 24.29 26.86 29.66 29.36 24. 9.07 24.08 21.13 22.42 20.82 27.33 26.82 25.75 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "Takeaway for Math RL",
            "content": "Math training boosts mathematical performance. Math-focused training significantly enhances model performance on mathematical tasks. Math skills aid puzzles but hinder coding. Math training improves puzzle-solving abilities through shared logical reasoning but often reduces coding performance."
        },
        {
            "title": "3.2 Code Domain",
            "content": "In the code domain, RL typically focuses on generating executable code from user-provided instructions and verifying correctness using predefined test cases. This verification requires secure sandbox environment to safely run generated code and enforce strict execution time limits to prevent timeouts. The key hyperparameters and sandbox configurations used in our experiments are summarized in Table 5. We highlight several key observations below: Table 5: Training hyperparameters for code domain. Max Token Rollout BS Mini BS 4,096 128 LR 1 106 Rollout Times Epochs Sandbox Timeout(s) 5 15 FireJail (1) Improvement in in-domain performance. The in-domain performance after RL training on code data is presented in Figures 1 and 2. Both the base and instruct models exhibit substantial improvements on Humaneval and MBPP, demonstrating the efficacy of code data for RL training. The base model, in particular, reveals significant untapped potential, with its Humaneval score surging from 70.12 to 80.49 (+10.37). On MBPP, the base model improves from 64.80 to 67.40. Despite these gains, the instruct model, enhanced by SFT, consistently achieves the highest performance. On Humaneval, the instruct model reaches superior score of 84.15, improving by 1.83. On MBPP, it overcomes an initial deficit (62.80 vs. 64.80) to attain 68.40, surpassing the base models 67.40. This consistent outperformance across both benchmarks underscores the critical importance of SFT in unlocking the full potential of RL training, enabling the instruct model to outperform the base model despite the latters remarkable progress. Figure 1: Performance on HumanEval. Figure 2: Performance on MBPP. (2) Distinct cross-domain effects of code reasoning. To examine how enhanced code reasoning influences performance in other domains, we report OOD results in Table 6, using checkpoints after 6 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning 240 training steps. The results reveal contrasting cross-domain effects between the base and instruct models. For the instruct model, improved code reasoning generally brings gains across most OOD benchmarks (except for CountDown). In contrast, the base model shows performance drops on most OOD tasks, except for Zebra. Further analysis of Base-CodeR1 outputs suggests that the rigid structure of code training data can constrain the base models output flexibility, leading to format inconsistencies that hinder correct answer extraction in non-code tasks. Table 6: Model performance (%) after training in the code domain."
        },
        {
            "title": "Data",
            "content": "Base CodeR1 Instruct CodeR"
        },
        {
            "title": "Puzzle",
            "content": "MATH500 CountDown AIME24 KK"
        },
        {
            "title": "Zebra",
            "content": "56.40 50.80 69.00 72.00 1.05 0.04 24.35 22.59 10.00 6.67 13.33 16. 17.86 13.85 10.14 17.57 0.27 31.24 31.50 32."
        },
        {
            "title": "Takeaway for Code RL",
            "content": "Coding ability enhancement. Code RL effectively improves the models ability to handle coding tasks. Code reasoning has mixed cross-domain effects. It strengthens reasoning transfer for the instruct model but may constrain the base models reasoning capacity."
        },
        {
            "title": "3.3 Puzzle Domain",
            "content": "Logic reasoning puzzles require complex logical deduction and multi-step reasoning, posing unique challenges for RL models due to their need for sequential decision-making and pattern recognition. We evaluate RL performance in the puzzle domain using base and instruct models under identical training conditions for fair comparison. Key training hyperparameters are summarized in Table 7. Table 7: Key hyperparameters for puzzle domain."
        },
        {
            "title": "Max Token Rollout BS Mini BS",
            "content": "4,096 128 64 LR 1"
        },
        {
            "title": "Rollout Times Epochs",
            "content": "5 25 The results in Table 8 reveal the following key findings: (1) Puzzle task performance is substantially enhanced. Training on puzzle-specific datasetsnamely KK and LPBsignificantly boosts performance within the puzzle domain. Exclusive KK training achieves outstanding KK accuracy (94.29 for the base model, 99.14 for the instruct model), while LPB training notably raises Zebra scores (34.60 for the base model, 36.20 for the instruct model). Combining KK and LPB produces more balanced but slightly lower peak performance, with average puzzle accuracies of 61.98 (base) and 59.96 (instruct), indicating that mixed-dataset training offers limited gains beyond single-source specialization. (2) Cross-Domain Generalization from Puzzle Reasoning to Math Tasks. Training on puzzle datasets often enhances the mathematical reasoning ability of models, demonstrating effective transfer of logical skills across domains. For example, training on the KK dataset boosts the base models scores to 68.40 on MATH500 and 20.00 on AIME24, approaching the instruct models original scores of 69.00 and 13.33, respectively, indicating strong cross-domain generalization. In contrast, the Instruct-LPB Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning model exhibits sharp performance drop on Countdown, falling from 24.35 to 2.47. This decline may stem from LPBs relatively fixed problem format, which imposes significant constraints on the problem-solving process for Countdown. Tracking this drop, as shown in Table 21, reveals trend of initial improvement followed by decline, suggesting that fixed data formats during training can lead to overfitting, thereby limiting the models out-of-domain performance. (3) Limited code domain impact. Puzzle training has an inconsistent effect on coding performance. Training on individual datasets often leads to reduced coding scores, but combining KK&LPB helps mitigate this decline, yielding Code averages of 71.35 (base) and 71.25 (instruct). This is likely due to the mismatch between the fixed format of puzzle data and the requirements of coding tasks. However, the increased data diversity from combining both datasets helps reduce the performance drop seen with single datasets. Table 8: Model performance (%) after training in the puzzle domain. Data Base KK LPB KK&LPB Instruct KK LPB KK&LPB Math Code Puzzle MATH500 CountDown AIME24 Avg. HumanEval MBPP Avg. KK Zebra Avg. 56.40 68.40 69.00 67.60 69. 73.20 69.40 72.40 1.05 19.36 7.40 10.81 24.35 33.95 2.47 30.30 10. 20.00 10.00 10.00 13.33 23.33 13.33 23.33 22.48 35.92 28.80 29.47 35. 43.49 28.41 42.01 70.12 60.37 74.40 78.70 82.93 74.39 70.70 80.49 64. 51.80 61.60 64.00 62.80 62.80 63.80 62.00 67.46 17.86 0. 9.07 56.09 68.00 71.35 72.87 62.60 67.25 71.25 94.29 16.60 89.29 30.69 34.60 34. 62.49 25.60 61.98 10.14 31.50 20.82 99.14 14.00 83.29 17.91 36.20 36. 58.53 25.10 59."
        },
        {
            "title": "Takeaway for Puzzle RL",
            "content": "Puzzle tasks enhance logical reasoning for math tasks. Puzzle tasks improve logical reasoning, leading to better performance on mathematical tasks. However, this effect does not extend to coding tasks."
        },
        {
            "title": "4 Performance with Combined-Domain Data",
            "content": "In this section, we reorganize the experimental results of cross-domain RL and provide systematic analysis of performance across different domain combinations and their interaction patterns. To facilitate clear examination of how various combinations influence the models generalization capability and domain-specific task performance, we divide the analysis into two subsections: dualdomain combinations and triple-domain combinations. Given that many prior studies [11, 41, 45, 22, 38] primarily focus on base models, we employ the Qwen2.5-7B Base as the foundation for continued training. For experimental hyperparameters, whenever mathematical data is included, we adopt the same configurations as those used for math tasks  (Table 3)  , in order to accommodate the higher token requirements inherent to math-specific training."
        },
        {
            "title": "4.1 Combinations of Dual Domains",
            "content": "We first examine the models performance under pairwise domain combinations, specifically Math + Puzzle, Puzzle + Code, and Math + Code. The results for these configurations are directly compared against their respective single-domain baselines. From the domain perspective, the outcomes are summarized in Table 9. 8 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Table 9: Performance (%) of the RL model with compared to Base."
        },
        {
            "title": "Training Data",
            "content": "Math Avg"
        },
        {
            "title": "Value",
            "content": "Code Avg"
        },
        {
            "title": "Value",
            "content": "Puzzle Avg"
        },
        {
            "title": "Value",
            "content": "All Avg"
        },
        {
            "title": "Math\nPuzzle\nCode",
            "content": "Math + Puzzle Puzzle + Code Math + Code 22.48 47.48 29.47 19.17 49.72 32.06 47.22 +25.00 +6.99 -3. +27.24 +9.58 +24.74 67.46 64.23 71.35 73.95 44.90 74.88 75.06 -3.23 +3.89 +6. -22.56 +7.42 +7.60 9.07 22.42 61.98 22.55 49.78 55.15 25.34 +13.35 +52.91 +13. +40.71 +46.08 +16.27 31.50 45.11 50.72 35.78 48.36 50.89 48.92 +13.61 +19.22 +4. +16.86 +19.39 +17.42 Key observations are summarized as follows: (1) Joint training with specific domain pairs can lead to clear synergistic benefits. For example, when training with the combination of Math + Puzzle, the models performance on Math improves to 49.72, surpassing the Math-only performance of 47.48. Similarly, for Code tasks, both additional Puzzle and Math data lead to improvements in code-related tasks when compared to Code-only training. These findings indicate that joint training can facilitate beneficial transfer of knowledge across domains in certain settings. (2) Adding an extra domain does not always lead to better performance and may introduce new generalization challenges. For the Puzzle task, all configurations involving additional domains perform worse than the Puzzle-only setting, suggesting that increased data diversity can hinder the models ability to specialize in solving puzzles. This also reflects the high degree of specialization required by the Puzzle task. Notably, in the Math + Puzzle configuration, the models performance on Code tasks drops significantly, falling below both the Math-only and Puzzle-only baselines. This may be due to the unique characteristics of the Code task, which differs structurally and linguistically from Math and Puzzle, making generalization more difficult when training data is dominated by other domains. Among all combinations, only Puzzle + Code achieves overall strong performance, with an overall improvement of 19.39. These results highlight that incorporating more domains does not guarantee universal improvements and may sometimes impede the models ability to adapt to tasks with distinct forms or representations. In summary, these results show that dual-domain training can yield non-trivial benefits and better task balance in specific settings, but its effectiveness depends on how domains are combined and how training data is allocated. Careful design choices are necessary to leverage synergy and mitigate potential negative interactions."
        },
        {
            "title": "4.2 Combinations of Triple Domains",
            "content": "Furthermore, we investigate the impact of training with data from all domains (Math + Code + Puzzle), comparing this setting to the previous optimal configuration (Puzzle + Code) in Figure 3. Additionally, Figure 4 illustrates the trends in overall performance as domain combinations are progressively expanded. The following key takeaways emerge: (1) Combining data from all three domains further enhances overall performance, though negative transfer occurs on specific tasks. As shown in Figure 3, joint training on data from the Math, Code, and Puzzle domains yields an overall average performance of 56.57, surpassing the previous best configuration (Puzzle + Code, 50.89). While overall accuracy improves and Math tasks reach their highest performance (49.75), the performance on Puzzle tasks drops to 49.73, notably lower than the Puzzle + Code setting (55.15). This supports our earlier observation that the Puzzle domain demands 9 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Figure 3: Performance comparison of tripledomain and optimal dual-domain data. Figure 4: Overall model performance under different data combinations. high degree of specialization, and that incorporating out-of-domain data can negatively impact its results. These results indicate that while broader domain coverage improves overall performance and generalization, the inclusion of Math data fails to yield positive effects on Puzzle tasks and instead leads to performance degradation. This outcome likely stems from the increased complexity introduced by the additional domain, which adversely impacts specific tasks. (2) Enhanced data diversity contributes to further model performance improvements. As shown in Figure 4, excluding the outlier (50.72) observed with the Puzzle-only data, the models overall performance exhibits positive trend as domain combinations increase. However, the unusually high value in the Puzzle task is primarily due to the models exceptional performance on the KK task (94.29), which disproportionately elevates the overall score. This outlier does not necessarily reflect balanced performance improvements across all tasks, but rather is driven by particularly strong result in one specific sub-task. (3) The triple-domain combination improves performance balance across tasks compared to certain dual-domain combinations. Unlike some dual-domain configurations (e.g., Math + Puzzle), which experience significant performance degradation on Code tasks (22.56), the triple-domain approach maintains more balanced performance across all tasks. While some task-specific specialization may be slightly reduced, the inclusion of Code data ensures that performance on the Code task remains strong and consistent. These results indicate that expanding domain coverage can mitigate the performance collapse on specific tasks, achieving more stable and generalized performance across tasks. In summary, the triple-domain combination exemplifies the balanced advantages of multi-skill training. Although incorporating additional domains may lead to modest reduction in peak performance for individual tasks, this approach achieves the highest overall performance while maintaining competitive results across all subtasks. These findings underscore the substantial potential of carefully designed multi-domain training strategies in building versatile models with broad adaptability. Takeaway for Dual-Domain and Triple-Domain Combinations Multi-domain training improves overall performance. Combining multiple domains generally leads to better overall performance, with the triple-domain combination showing moderate gains. Multi-domain training improves task balance and overall stability. By providing broader coverage, multi-domain setups help maintain consistent performance across tasks, preventing extreme drops in any single area and promoting more robust, generalized models. 10 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "5 Evaluating Template Variations in Reinforcement Learning",
            "content": "A commonly overlooked issue in RL is the mismatch between templates used for training and those applied during testing [9, 32, 13]. Such discrepancies can significantly degrade model performance during evaluation. For instance, training data may utilize an R1-style template, while testing might employ Qwen template (the default for many Qwen-series models [40]) or no template at all, as some evaluation tools, such as OpenCompass [7], default to blank template for base model testing (see Table 10). In this section, we train base and instruct models on the KK dataset using the R1-style template and evaluate their performance with various templates. The results, presented in Table 11, assess the impact of mismatched training and testing templates, highlighting the models sensitivity to template alignment. Qwen Template: <im start>systemnPlease reason step by step, and put your final answer within boxed{}.<im end>n<im start >usern{question}<im end>n<im start> assistantn Base Template: {question} Table 10: Template for qwen and base model. question will be replaced with the specific reasoning question during training. Table 11: Model performance under different templates. Model Template Math Code Puzzle MATH500 CountDown AIME24 HumanEval MBPP KK Zebra Base Instruct Base Template Qwen Template R1 Template Base Template Qwen Template R1 Template 72.80 69.20 68.40 3.20 1.80 73. 0.00 20.79 19.36 0.63 0.29 33.95 6.67 13.33 10.00 3.33 0.00 23.33 60.98 63.78 60.37 51.22 32.93 74. 3.00 64.40 51.80 1.60 42.60 62.80 31.29 94.00 94.29 41.57 62.71 99.14 16.18 0.56 30.69 21.22 9.84 17. As shown in Table 11, mismatch in templates significantly impacts the models performance. We can break this down into the following points: (1) Mismatched Templates Significantly Impact Model Performance. Mismatched templates substantially reduce the performance of base and instruct models across diverse tasks. For the base model, mismatched base template lowers scores to 0, 3.00, and 31.29 on Countdown, MBPP, and KK, respectively, while mismatched instruct template decreases the Zebra score to 0.56. Similarly, the instruct model experiences performance drops to 1.80 on MATH500 and 0.29 on Countdown with mismatched template like Qwen, among other tasks. These declines highlight the models sensitivity to template mismatches. Figure 5: The average test performance of base and instruct models on different templates. (2) Matched Templates Typically Achieve Optimal Model Performance. Matched templates consistently enhance the average performance of base and instruct models across benchmarks. As shown in Figure 5, the R1 template produces scores of 47.84 and 54.56 for the base and instruct models, respectively, surpassing mismatched conditions. Although no single template excels in every task, the superior performance of matched templates underscores their critical role in ensuring stable and effective outcomes, especially for intricate datasets like KK and Zebra. 11 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "Takeaway for Template",
            "content": "Template consistency is critical. Mismatched templates degrade model performance on certain tasks, highlighting the current lack of robustness in RLVR."
        },
        {
            "title": "6 The Role of Curriculum Learning in Reinforcement Learning",
            "content": "While curriculum learning is well-established in SFT [5, 14, 12], its application in RLVR remains insufficiently explored [25, 33]. To address this gap, we systematically investigate curriculum learning strategies in the Puzzle domain, leveraging the KK dataset. This dataset features clearly defined difficulty variations based on the number of sub-questions per problem, enabling us to effectively categorize data by difficulty levels. This facilitates focused evaluation of the generalizability of our approach within specific cognitive challenge. Difficulty stratification and curriculum design: Effective curriculum learning hinges on accurately quantifying task difficulty. For the puzzle task, difficulty is defined by the number of sub-questions per problem (PPL), ranging from 3PPL to 8PPL across six levels. Training progresses sequentially from easier to harder tasks, enabling the model to build proficiency in simpler reasoning before tackling more complex problems. We propose novel policy refresh strategy, where after each training stage (175 steps), corresponding to specific difficulty level, the reference model is updated by replacing it with the latest actor model. Additionally, the optimizer state is reset to prevent overfitting to prior difficulty levels. This strategy provides the model with fresh starting point at each new difficulty level, facilitating stable learning and better adaptation to progressively challenging tasks. The corresponding experimental results are presented in Figure 6. The analysis is as follows: Figure 6: Model performance on the KK dataset with different curriculum settings. The x-axis represents the KK difficulty levels, and the y-axis shows the training data sequence from 3PPL to 8PPL. (1) Curriculum learning improves the upper bound of model performance. Figure 6 demonstrates that curriculum learning under both settings effectively enhances the models upper performance bound, achieving accuracies of 97.29 and 99.71, respectively, which significantly surpass the 94.29 accuracy obtained under mixed training. This improvement highlights the key advantages of curriculum learning, including more structured and progressive learning patterns that enable the model to better capture complex task dependencies and enhance generalization capabilities. Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning (2) Policy refresh further improves the performance and convergence rate of curriculum learning. The right panel of Figure 6 reveals that policy refresh accelerates convergence while delivering superior final performance. Beginning from the second stage, models incorporating policy refresh consistently outperform standard curriculum learning, achieving 97.43 accuracy at 6PPLalready exceeding the latters final score of 97.29. Remarkably, the policy refresh model nearly reaches perfect accuracy, even surpassing the final result of the instruct model under mixed training (99.14)."
        },
        {
            "title": "Takeaway for Curriculum Learning",
            "content": "Curriculum learning demonstrates effectiveness and achieves additional improvements through policy refresh. Staged training raises the models performance upper bound, while periodic reference model updates further accelerate convergence and enhance final results."
        },
        {
            "title": "7 Impact of Reward Styles on Model Performance",
            "content": "In this section, we investigate how different reward styles affect model performance using the KK and LPB datasets, selected for their complex problem structures involving multiple interdependent entities. Unlike typical Math and Code datasets, which often feature problems with single correct answer and rely predominantly on binary reward schemes, the KK and LPB present unique challenges. Each problem requires filling multiple blanks, resembling cloze task, enabling evaluation of diverse reward strategies. We compare two primary schemes: binary reward (R1), granting credit only for fully correct responses, and partial reward (R2), based on the fraction of correctly filled blanks. Additionally, we explore format reward (R3) using the <think> tag to promote intermediate reasoning, and rescaled reward (R4) that extends the reward range to [1, 1] to penalize incorrect responses. (1) Formally, for the KK dataset, we define the reward function R(response) as follows: R(response) = Nc/N Nc/N Nc/N + format reward 1 if Nc = N, 1 otherwise (Binary Reward, R1) (Partial Reward, R2) (Format Reward, R3) (Rescaled Reward, R4), where Nc denotes the number of blanks correctly completed by the model and represents the total number of blanks in the puzzle. For the LPB dataset, the reward functions differ slightly from those defined for the KK puzzle dataset. Specifically, the forms of the binary reward (R1) and the partial reward (R2) remain identical. However, two modifications are introduced: (1) the format reward (R3) is modified from Nc/N + format reward to Nc/N + format reward, and the rescaled reward (R4) is defined by linearly scaling the partial reward (R2) to continuous range from 1 to 1, leading to 2 (Nc/N 0.5). (2) Finally, the reward function for the LPB dataset is defined as follows: R(response) = Nc/N (Binary Reward, R1) Nc/N (Partial Reward, R2) Nc/N + format reward (Format Reward, R3) 2 (Nc/N 0.5) (Rescaled Reward, R4), The rationale behind these adjustments is supported by empirical results on the LPB dataset, where the partial reward (R2)which measures the proportion of correctly filled blanksyields the best 13 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning performance, whereas the binary reward (R1) results in the poorest performance. Consequently, we convert the first component of the format reward (R3) from binary to partial, and adjust the rescaled reward (R4) into continuous form to avoid using discrete rewards. To systematically evaluate the impact of different reward styles, we conducted comprehensive analysis across both in-domain and out-of-domain settings. 7.1 Impact of Reward Models on In-Domain Performance To investigate the impact of different reward styles on model training, we conduct experiments where only the reward varies, training for total of 800 steps (or fewer if collapse occurs). We compare performance across the KK and LPB, with results shown in Figures 7 and 8. These comparisons reveal that reward efficacy is highly dataset-dependent, as summarized in the following key findings: (1) Binary reward excels on KK but fails on LPB due to sparsity differences. On KK, the simplest R1 achieves the best final performance, outperforming more nuanced alternatives by providing clear, direct signals. This aligns with the proportional reward trap phenomenon [28] in programming tasks, where R2 introduce noise. In contrast, R1 leads to consistent training collapse on LPB, where extreme reward sparsity arises because the model rarely predicts all puzzle cells correctly, yielding few positive signals. We limit R1 training on LPB to 200 steps to avoid unnecessary computation. This observation underscores that binary rewards are well-suited for relatively easier tasks, like KK, where the base model can achieve complete success sometimes, but are untenable for harder ones like LPB. (2) Partial reward underperforms on KK but offers viable baseline on LPB, though with limitations. On KK, R2 shows no advantage over R1 and ultimately degrades performance by injecting noisy learning signals. Conversely, on LPB, R2 emerges as feasible alternative to R1s collapse, delivering initial promise with peak accuracy of 38.63 at 200 steps. However, its gains are not sustained, as it fails to accurately penalize the specific erroneous cells in the response, leading to slight decline. This highlights R2s utility in sparse settings but exposes its inadequacy compared to more sophisticated reward mechanism. Figure 7: Performance on KK. Figure 8: Performance on LPB. (3) Format reward and rescaled reward excel on LPB, while falling short on KK. On KK, despite early gains from format correction or error suppression, both R3 and R4 yield inferior final performance to R1, indicating that their added complexity does not pay off in domains favoring binary signals. In contrast, on LPB, R3 and R4 initially trail R2 but eventually surpass it, benefiting from more informative signals: R3 stabilizes training via well-formed outputs, and R4 amplifies behavioral differences for better optimization. Overall, these contrasts demonstrate that optimal reward design is not universal but critically tied to dataset characteristics like sparsity and task complexity. These factors must be carefully considered when designing RLVR training. 14 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning 7."
        },
        {
            "title": "Impact of Reward Models on OOD Generalization",
            "content": "Figures 9 and 10 illustrate the effects of various reward schemes on OOD tasks, highlighting how reward efficacy varies across datasets. Our key observations reveal some insightful findings: (1) In mathematical reasoning tasks, reward schemes yield different outcomes depending on the training data. For KK, all rewards produce similar performance, with the R3 offering no clear advantage over the base model. This contradicts prior claims [38] about the benefits of structured reasoning incentives. In contrast, for LPB, the choice of reward significantly impacts performance: R2 achieves the highest accuracies (e.g., outperforming others on both AIME24 and MATH500 benchmarks), while R1 leads to substantial declines (e.g., MATH500 accuracy dropping from 56.4 to 41.8). Figure 9: KK-impact of reward configurations (base model shown with dashed lines). (2) In code generation tasks, different datasets exhibit distinct reward sensitivities. For example, training on KK is relatively sensitive to reward design, with the performance of different rewards varying considerably. Most rewards generally degrade performance compared to the base model, though R3 effectively mitigates this negative impact and provides modest improvements. In contrast, training on LPB is less sensitive to reward design: most rewards (excluding R1) perform similarly, yielding gains on the HumanEval benchmark but experiencing drops on the MBPP benchmark. R1, however, suffers from significantly worse performance on both HumanEval and MBPP, which further aligns with the observed in-domain training collapse on LPB. (3) significant limitation lies in current reward mechanisms. These comparisons expose critical limitation in current reward mechanisms: they operate at the response level rather than the cell level. This means they fail to accurately penalize the erroneous predicted cells but treat all cells equally within response. This issue is particularly evident in KK, where even R2 led to poor outcomes, aligning with challenges noted in [28]. Overall, the absence of universal reward strategy emphasizes the task-dependent nature of reward design and the essential role of data diversity in RL training. Optimal rewards should be tailored to specific dataset characteristicse.g., finer-grained, cell-level schemes for datasets like KKto overcome the limitations of current response-level rewards. Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Figure 10: LPB-impact of reward configurations (base model shown with dashed lines)."
        },
        {
            "title": "Takeaway for Reward",
            "content": "Reward design should match task complexity. Binary rewards work well for simpler tasks, while partial rewards are more suitable for complex tasks. Current partial rewards lack precision. Designing more fine-grained partial reward signals is promising direction for further improvement."
        },
        {
            "title": "8 Influence of Training Language",
            "content": "To investigate the impact of training data language [43], we translate the DeepScaleR dataset into Chinese using GPT-4.1-nano3 and performed RL training with the identical hyperparameters to those used for the English version. To ensure the model uses Chinese for reasoning during training, we employ the langid4 package to detect the language of each rollout trajectory. reward of 1 is given only when the language is Chinese and the final answer is correct. If the language is not Chinese, even if the final answer is correct, the reward is 0. This strict reward function is necessary, as we observe that without it, the model would default to reasoning in English even when the questions are translated into Chinese. Figure 11 illustrates the impact of different training languages. It is clear that models trained to reason in Chinese consistently perform worse than their English counterparts. This inefficiency in Chinese language reasoning highlights the need for more advanced post-training Figure 11: The effect of different training language. 3https://openai.com/index/gpt-4-1/ 4https://pypi.org/project/langid/ 16 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning algorithms capable of improving cross-lingual generalization for complex reasoning tasks."
        },
        {
            "title": "Takeaway for Training Language",
            "content": "RLVR is language-sensitive. Models trained in Chinese underperforms that trained in English with consistent performance gap."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we focus on the reasoning capabilities of large language models, with data-centric approach. We classify reasoning data into three main domains: Math, Code, and Puzzle. detailed discussion is provided on the effects of domain-specific data in reinforcement learning and the generalization to out-of-domain data. Through cross-domain data combinations, we reveal the potential interactions between different model capabilities, including both assisting and conflicting phenomena. Additionally, we thoroughly discuss the impact of various factors on model performance in Reinforcement Learning, including model template, curriculum learning, reward styles, and training languages, offering some insights from multiple perspectives. For future work, we believe it would be beneficial to further refine the categorization of reasoning capabilities. For example, introducing data from the science and general reasoning domains would allow for more detailed discussions on data combinations. Furthermore, due to hardware limitations, this paper primarily focuses on the base and instruct models of Qwen 2.5. Future research could expand on the impact of different datasets on models like Llama and DeepSeek. We also believe that RLVR will become significant milestone in the development of large language models, with data remaining the cornerstone of any training process. We hope that future work will delve deeper into exploring the impact of data on RLVR. 17 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning References [1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. [2] Oleg Bask. The logic puzzle baron dataset. https://huggingface.co/datasets/olegbask/LogicPuzzleBaron, 2024. [3] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [5] Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piche, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint arXiv:2505.14970, 2025. [6] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [7] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance?, 2024. URL https://arxiv.org/abs/2411.10541. [10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [11] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [12] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59015910, 2020. [13] Fengqing Jiang, Zhangchen Xu, Luyao Niu, Bill Yuchen Lin, and Radha Poovendran. Chatbug: common vulnerability of aligned llms induced by chat templates, 2025. URL https://arxiv.org/abs/2406.12935. [14] Yajing Kong, Liu Liu, Jun Wang, and Dacheng Tao. Adaptive curriculum learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 50675076, 2021. [15] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. 18 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning [16] Yu Li, Qizhi Pei, Mengyuan Sun, Honglin Lin, Chenlin Ming, Xin Gao, Jiang Wu, Conghui He, and Lijun Wu. Cipherbank: Exploring the boundary of llm reasoning capabilities through cryptography challenges. arXiv preprint arXiv:2504.19093, 2025. [17] Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning, 2025. URL https://arxiv.org/ abs/2502.01100. [18] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. [19] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. [20] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [21] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Surhttps://pretty-radio-b75.notion.site/ Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. passing o1-preview with 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Deepscaler: [22] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [23] Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, Vicky Zhao, Conghui He, and Lijun Wu. Lemma: Learning from errors for mathematical advancement in llms. arXiv preprint arXiv:2503.17439, 2025. [24] Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, and Lijun Wu. Rest: Stress testing large reasoning models by asking multiple problems at once, 2025. URL https://arxiv.org/ abs/2507.10541. [25] Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. [26] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. Mathfusion: Enhancing mathematical problem-solving of llm through instruction fusion. arXiv preprint arXiv:2503.16212, 2025. [27] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [28] Abhinav Rastogi, Albert Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910, 2025. [29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 19 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning [32] Shijian Wang, Linxin Song, Jieyu Zhang, Ryotaro Shimizu, Ao Luo, Li Yao, Cunjian Chen, Julian McAuley, and Hanqian Wu. Template matters: Understanding the role of instruction templates in multimodal language In ICLR 2025 Workshop on Navigating and Addressing Data Problems for model evaluation and training. Foundation Models, 2025. URL https://openreview.net/forum?id=aDAaoRhYW4. [33] Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, and Wentian Zhao. Dump: Automated distribution-level curriculum learning for rl-based llm post-training. arXiv preprint arXiv:2504.09710, 2025. [34] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [35] Jialong Wu, Shaofeng Yin, Ningya Feng, and Mingsheng Long. Rlvr-world: Training world models with reinforcement learning. arXiv preprint arXiv:2505.13934, 2025. [36] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms, 2025. URL https://arxiv.org/abs/2504.14655. [37] Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning. 2024. URL https: //arxiv.org/abs/2410.23123. [38] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [39] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. [40] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [41] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [42] Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025. [43] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: tale of diversity and bias, 2023. URL https://arxiv.org/abs/2306.15895. [44] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [45] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [46] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros aha moment in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. 20 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "Appendix",
            "content": "A Step-Level Performance Results for Each Training Dataset In the following figures, we present detailed performance results for the base and instruct models across different training datasets. The results show performance at each evaluation checkpoint, illustrating the models progression and how different training data influence overall performance. DeepscaleR: Figures 12 and 13 illustrate the training dynamics on the DeepscaleR dataset. The results show that the models mathematical reasoning ability improves consistently and partially generalizes to logical reasoning tasks, while its code reasoning capability drops significantly. Figure 12: Base models detailed performance on DeepscaleR. Figure 13: Instruct models detailed performance on DeepscaleR. CountDown: Figures 14 and 15 present the training dynamics on the CountDown dataset. Although the model initially underperforms on CountDown, domain-specific training significantly improves its results. Meanwhile, CountDown training degrades the base models coding ability, whereas the instruct models coding performance remains relatively stable, indirectly demonstrating the positive impact of SFT on the robustness of RL training. Figure 14: Base models detailed performance on CountDown. 21 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Figure 15: Instruct models detailed performance on CountDown. CodeR1: Figures 16 and 17 show the training dynamics on the CodeR1 dataset. While code data substantially improves the models coding reasoning ability, it also introduces negative effects on mathematical reasoning, with the impact being more pronounced for the base model. Figure 16: Base models detailed performance on CodeR1. Figure 17: Instruct models detailed performance on CodeR1. Knights-and-Knaves: Figures 18 and 19 show the effects of training on the KK dataset. Targeted training significantly boosts KK performance, with this improvement in logical reasoning generalizing well to mathematical tasks but negatively impacting coding ability. Figure 18: Base models detailed performance on KK. 22 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Figure 19: Instruct models detailed performance on KK. Logic Puzzle Baron: Figures 20 and 21 show the effects of training on the LPB dataset. Similar to the KK setting, the models performance on the ZebraLogicBench improves rapidly with targeted training. Figure 20: Base models detailed performance on LPB. Figure 21: Instruct models detailed performance on LPB. Detailed Performance Results for Cross-Domain Composition Puzzle + Math: As shown in Figure 22, under this setting, the models performance on math and puzzle tasks shows steady improvement. However, for HumanEval, there is slight improvement followed by significant decline, which also demonstrates the catastrophic forgetting phenomenon in RLVR. Math + Code: As shown in Figure 23, the model exhibits stable improvements across all aspects in this setting, both in-domain for math and code tasks. Additionally, the extra math data further enhances performance on code tasks. The model also generalizes to unseen puzzle-solving capabilities. Puzzle + Code: As shown in Figure 24, although code performance shows gradual improvement, the in-domain puzzle performance does not exhibit significant improvement compared to other settings, indicating that code data has an additional negative impact on puzzle performance. 23 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Figure 22: Base models detailed performance on Puzzle + Math domain data. Figure 23: Base models detailed performance on Math + Code domain data. Figure 24: Base models detailed performance on Puzzle + Code domain data. In addition, we have summarized the specific performance of all cross-domain combinations in Table 12: Table 12: Model performance during cross-domain data composition. Training Data Base Math Puzzle Code Math + Puzzle Puzzle + Code Math + Code Math + Puzzle + Code Math Code Puzzle ALL MATH500 CountDown AIME24 Avg. HumanEval MBPP Avg. KK Zebra Avg. Avg. 56.40 72.00 67.60 50.80 72.80 60.60 69.60 73. 1.05 53.77 10.81 0.04 66.35 22.25 62.07 52.33 10.00 16.67 10.00 6. 10.00 13.33 10.00 23.33 22.48 47.48 29.47 19.17 49.72 32.06 47.22 49. 70.12 66.46 78.70 80.49 25.00 84.15 82.32 78.66 64.80 62.00 64.00 67. 64.80 65.60 67.80 68.60 67.46 17.86 0.27 9. 64.23 71.35 73.95 44.90 74.88 75.06 26.42 89.29 13.85 68.57 75.00 24.00 18.42 34.66 31.24 30.99 35.30 26. 22.42 61.98 22.55 49.78 55.15 25.34 31.50 45.11 50.72 35.78 48.36 50.89 48.92 73. 68.14 31.31 49.73 56.57 24 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"
        },
        {
            "title": "C Prompt Formats for Each Benchmark Evaluation",
            "content": "Example C.1: MATH500 Now the user asks you to solve math problem. After thinking, when you finally reach conclusion, clearly state the answer within <answer> </answer> tags. i.e., <answer> (boxed{}) </answer>. {problem}n Example C.2: AIME24 Now the user asks you to solve math problem. After thinking, when you finally reach conclusion, clearly state the answer within <answer> </answer> tags. i.e., <answer> (boxed{}) </answer>. {question}n Example C.3: CountDown Now the user asks you to solve math problem. After thinking, when you finally reach conclusion, clearly state the answer within <answer> </answer> tags. i.e., <answer> </answer>. Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>. Example C.4: HumanEval Now the user asks you to solve code problem. After thinking, when you finally reach conclusion, clearly state the answer within <answer> </answer> tags. i.e., <answer> </answer>. Complete the following python code:n{prompt}n Example C.5: Knights-and-Knaves (KK) Now the user asks you to solve logical reasoning problem. After thinking, when you finally reach conclusion, clearly state the identity of each character within <answer> </answer> tags. List the identity of each person one by one, for example, <answer> (1) Zoey is knight (2) Oliver is knight (3)... </answer>. New question: {prompt} 25 Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Example C.6: MBPP You are an expert Python programmer, and here is your task: Write function to find the similar elements from the given two tuple lists. Your code should pass these tests: assert similar elements((3, 4, 5, 6),(5, 7, 4, 10))==(4, 5) assert similar elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4) assert similar elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14) [BEGIN] def similar elements(test tup1, test tup2): res = tuple(set(test tup1) & set(test tup2)) return (res) [DONE] You are an expert Python programmer, and here is your task: Write python function to identify non-prime numbers. Your code should pass these tests: assert is not prime(2) == False assert is not prime(10) == True assert is not prime(35) == True [BEGIN] import math def is not prime(n): result = False for in range{ 2,int(math.sqrt(n)) + 1 }: if % == 0: result = True return result [DONE] You are an expert Python programmer, and here is your task: Write function to find the largest integers from given list of numbers using heap queue algorithm. Your code should pass these tests: assert heap queue largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3} == (85, 75, 65) assert heap queue largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2) == (85, 75) assert heap queue largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5) == (85, 75, 65, 58, 35) [BEGIN] import heapq as hq def heap queue largest(nums,n): largest nums = hq.nlargest(n, nums) return largest nums [DONE] You are an expert Python programmer, and here is your task: {text} Your code should pass these tests: 26 {test list} Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning Example C.7: ZebraLogicBench (Zebra) # Example Puzzle There are 3 houses, numbered 1 to 3 from left to right, as seen from across the street. Each house is occupied by different person. Each house has unique attribute for each of the following characteristics: - Each person has unique name: Peter, Eric, Arnold. - Each person has unique favorite drink: tea, water, milk ## Clues for the Example Puzzle 1. Peter is in the second house. 2. Arnold is directly left of the one who only drinks water. 3. The one who only drinks water is directly left of the person who likes milk. ## Answer to the Example Puzzle { reasoning: Given Clue 1, we know Peter is in House 2. According to Clue 2, Arnold is directly left of the one who only drinks water. The person in House 3 cannot be on the left of anyone, so Arnold must be in House 1. Thus, Peter drinks water, and Eric lives in House 3. Then, according to Clue 3, Eric drinks milk. Therefore, Arnold drinks tea., solution: { House 1: { Name: Arnold, Drink: tea }, House 2: { Name: Peter, Drink: water }, House 3: { Name: Eric, Drink: milk } } } # Puzzle to Solve {puzzle} # Instruction Now please solve the above puzzle. Present your reasoning and solution in the following json format: {json template}"
        }
    ],
    "affiliations": [
        "OpenDataLab",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}