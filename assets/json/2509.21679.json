{
    "paper_title": "ReviewScore: Misinformed Peer Review Detection with Large Language Models",
    "authors": [
        "Hyun Ryu",
        "Doohyuk Jang",
        "Hyemin S. Lee",
        "Joonhyun Jeong",
        "Gyeongman Kim",
        "Donghyeon Cho",
        "Gyouk Chu",
        "Minyeong Hwang",
        "Hyeongwon Jang",
        "Changhun Kim",
        "Haechan Kim",
        "Jina Kim",
        "Joowon Kim",
        "Yoonjeon Kim",
        "Kwanhyung Lee",
        "Chanjae Park",
        "Heecheol Yun",
        "Gregor Betz",
        "Eunho Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either \"weaknesses\" in a review that contain incorrect premises, or \"questions\" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 7 6 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "REVIEWSCORE: MISINFORMED PEER REVIEW DETECTION WITH LARGE LANGUAGE MODELS"
        },
        {
            "title": "Doohyuk Jang\nKAIST",
            "content": "Hyemin S. Lee MIT"
        },
        {
            "title": "Yoonjeon Kim\nKAIST",
            "content": "Kwanhyung Lee KAIST, AITRICS"
        },
        {
            "title": "Heecheol Yun\nKAIST",
            "content": "Gregor Betz KIT Eunho Yang KAIST, AITRICS"
        },
        {
            "title": "ABSTRACT",
            "content": "Peer review serves as backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either weaknesses in review that contain incorrect premises, or questions in review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce REVIEWSCORE indicating if review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from weakness. We build human expert-annotated REVIEWSCORE dataset to check the ability of LLMs to automate REVIEWSCORE evaluation. Then, we measure human-model agreements on REVIEWSCORE using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. thorough disagreement analysis further supports potential of fully automated REVIEWSCORE evaluation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Peer review is an evaluation of academic work by experts to assess its originality, significance, and validity before publication (Kelly et al., 2014; Taylor & Francis, 2025). In AI conferences, as the number of submissions is exponentially increasing, required number of reviewers is also exploding. As result, review quality is degraded, which undermines the integrity and reliability of peer-review system (Stelmakh et al., 2021; Cortes & Lawrence, 2021; Shah, 2022; Kim et al., 2025). Due to the importance of the issue, previous works propose criteria for evaluating review quality. However, we observe trade-off between applicability to reviews and specificity of rubrics. Goldberg et al. (2025) and Du et al. (2024) proposes criteria that could apply to nearly every review but those are quite vague and subjective. Guo et al. (2023), Sadallah et al. (2025), Purkayastha et al. (2025), and Ou et al. (2025) introduce specific and objective criteria but those target narrow scope of reviews. If we could reliably measure review quality based on specific yet applicable criteria, we could filter out misinformed reviews or give feedback to reviewers before the reviews are released. To resolve this issue, we introduce two specific yet applicable criteria of review quality: unanswerability of questions and factuality of weaknesses. To select the criteria, we recruited group of 1We will make the source code and dataset publicly available."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of REVIEWSCORE. Each review point in review is categorized into question and weakness. We further categorize weakness into claim and argument by the presence of supporting reasons. Based on an appropriate knowledge base, if question is answerable by the paper, claim is factually incorrect, or an argument contains factually incorrect premises, then the review point is misinformed. For arguments, to extract all explicit and implicit premises, we also introduce an automatic argument reconstruction engine. human experts and let them independently analyze small subset of ICLR reviews2. Specifically, each of them decomposed review into review points, which are formally defined as follows. Definition 1 (Review Point). review point is single, self-contained unit of evaluation or inquiry in revieweither weakness or questionthat stands on its own semantically. Each human evaluated quality of review points, and they discussed trustworthy criteria to detect low-quality. Based on the discussion, we formally define misinformed review point as follows. Definition 2 (Misinformed Review Point). review point is misinformed if and only if question stated in review can be already answered by the paper, or weakness stated in review is incorrect or contains incorrect premises regarding the paper. In this work, the human annotation shows that 26.4% of questions and 15.2% of weaknesses are misinformed, which means that the current AI conference reviews contain considerable amount of misinformed review points.3 We also note that these criteria aligns with reviewer guidelines of major AI conferences. For instance, ACL 2023 Peer Review Policies indicate that before writing negative review, check whether your questions are already answered. (Boyd-Graber et al., 2023), and NeurIPS 2025 reviewer guidelines indicate that reviewers should minimize the chance of misunderstandings during the reviewing process (NeurIPS 2025 Program Chairs, 2025). Based on these observations, we define REVIEWSCORE that indicates if review point is misinformed (Section 3.1). First, we define BASE REVIEWSCORE by directly applying the definition of misinformed review points in 5-point scale. However, we discovered human-annotated factuality often diverges. This is because weakness often contains both correct and incorrect explicit premises or incorrect implicit premises, which hinders humans to reliably annotate factuality. To resolve this issue, we further define ADVANCED REVIEWSCORE as an aggregation of premise factuality scores. If weakness has no supporting reason, we call it as claim, and if weakness consists of more than one premises, we call it as an argument. We adopt two aggregation methods, logical conjunction, following the literature of logic (Beall et al., 2024), and weighted average, maintaining 5-point scale (Figure 1). Before evaluating ADVANCED REVIEWSCORE, we have to extract all explicit and implicit premises from an argument. This process is called argument reconstruction in logic and critical thinking (Brun & Betz, 2016; Groarke, 2024; Dowden, 2024), and we construct an engine that automatically does this process (Section 3.2). To fully automate misinformed review detection with LLMs, we should verify if model-estimated REVIEWSCORE align with human-annotated REVIEWSCORE. To do that, we construct human expert-annotated REVIEWSCORE dataset based on ICLR reviews (Section 3.3). We recruited 15 experienced graduate students and they dedicated total 244 hours for trustworthy human annotation. 2A group of three graduate students studying AI analyzed reviews of ICLR submissions available in OpenReview. Detailed process of this group work is described in Appendix B. 3A group of 15 graduate students annotate reviews of 40 works submitted to ICLR 2021-2023. Detailed process of this human annotation is described in Section 3.3."
        },
        {
            "title": "Preprint",
            "content": "We verify the reliability of automatic evaluation of REVIEWSCORE by measuring human-model agreements (Section 4). We use eight current state-of-the-art models, including five proprietary and three open-sourced models, and confirm moderate agreements with the human experts. ADVANCED REVIEWSCORE clearly outperforms BASE REVIEWSCORE, which proves the effectiveness of premise-level factuality scoring. We also conduct thorough human-model disagreement analysis and observe that models sometimes misunderstand the meaning of review points or predict scores that minority human annotators give. To summarize our contributions: 1. We introduce REVIEWSCORE, novel evaluation criteria that detects misinformed review points (i.e., questions that can be answered by the paper or weaknesses with incorrect premises). 2. To evaluate the factuality of premises, we propose an automatic argument reconstruction engine that generates valid and faithful set of premises and conclusion. 3. We construct trustworthy human expert-annotated dataset to measure the reliability of automatic evaluation of REVIEWSCORE. 4. We validate the reliability of automatic REVIEWSCORE evaluation by using eight current stateof-the-art LLMs and conducting comprehensive human-model disagreement analysis."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Peer review evaluation. Previous works studied how to evaluate the quality of peer reviews. Goldberg et al. (2025) introduces four criteria, understanding, substantiation, coverage, and constructiveness. However, they focus on evaluating review as whole, not each individual review point, which makes difficult to clarify which specific points are misinformed. They also do not use LLMs to automate evaluation. Guo et al. (2023) specifically focuses on evaluating substantiation. However, they evaluate whether each review point has supporting evidence, not whether the supporting pieces of evidence are factually correct, leaving it as future work. Our works factuality criteria exactly covers this part. Sadallah et al. (2025) proposes four criteria, actionability, grounding & specificity, verifiability, and helpfulness. However, they only focus on evaluating weaknesses, excluding questions, and as in Guo et al. (2023), they only evaluate whether weaknesses are grounded and supported, not whether those are factually correct. Purkayastha et al. (2025) defines lazy thinking in the context of NLP paper reviews and introduces heuristic-based rubrics. However, their criteria is only applicable to NLP domain which is hard to be generalized into other domains. Argument evaluation. In logic and critical thinking, an argument is list of statements, one of which is the conclusion and the others are the premises (Dutilh Novaes, 2022; Lau & Chan, 2025). To evaluate an argument, we need to follow two steps. First, we have to identify and reconstruct the argument into set of premises and conclusion, which is called an argument reconstruction (Brun & Betz, 2016; Groarke, 2024; Dowden, 2024). Then, we evaluate whether each premise is factually correct. An argument reconstruction should both be valid, which means premises deductively imply conclusion, and faithful, which means premises and conclusion accurately and completely represents an original argument (Brun & Betz, 2016; Betz & Richardson, 2021). Previously, Betz & Richardson (2021) trains T5 model for argument reconstruction. However, the training datasets are either synthetic or polished and the reconstruction do not require any additional context information. In contrast, our work targets peer reviews, which include real-world unpolished arguments, and the reconstruction requires an entire paper to fully understand the context of arguments."
        },
        {
            "title": "3 REVIEWSCORE",
            "content": "We newly define REVIEWSCORE that measures how misinformed review point is (Section 3.1). To evaluate REVIEWSCORE, we also introduce an engine that automatically extracts every explicit and implicit premises from weakness (Section 3.2). Lastly, we construct human expert-annotated dataset that evaluates LLMs ability to evaluate REVIEWSCORE (Section 3.3)."
        },
        {
            "title": "3.1 DEFINITION",
            "content": "Our goal of defining REVIEWSCORE is to detect misinformed review points. Following this goal and the review quality criteria discussion in Section 1, we first define BASE REVIEWSCORE as factuality of weaknesses and unanswerability of questions. The following definition formally describes it. Definition 3 (BASE REVIEWSCORE). Let be review point (either weakness or question) about submitted paper S. Define FactualityS : {1, 2, 3, 4, 5}, UnanswerabilityS : {1, 2, 3, 4, 5}, where and are, respectively, the sets of weaknesses and questions appearing in review of S4. We considered score 12 as Misinformed and score 35 as Not misinformed for binary classification setup. The BASE REVIEWSCORE of is: ReviewScorebase(x) = (cid:40)WScore(x) = FactualityS(x), if (weakness), QScore(x) = UnanswerabilityS(x), if (question). However, during the group discussion in Section 1, we discovered that human annotators evaluations on factuality diverge if weakness contains both factual and nonfactual premises or nonfactual premise is implicitly presumed. It happens since the human annotators implicitly weigh the importance of underlying premises of weakness and then decide the final factuality score. To resolve this issue, we further define ADVANCED REVIEWSCORE. We categorize weaknesses into arguments and claims based on whether there are supporting reasons or not. We keep the definition of WScore to evaluate claims, but further develops finer-grained score to evaluate arguments. Following the literature of critical thinking (Brun & Betz, 2016; Groarke, 2024; Dowden, 2024), we reconstruct an argument into premise-conclusion structure and then define ADVANCED REVIEWSCORE for arguments as (an aggregation of) factuality of premises. The following definition formally describes it. Definition 4 (ADVANCED REVIEWSCORE). Let be review point about submitted paper S. Let C, A, and denote, respectively, the sets of claims, arguments, and questions in review of S. For A, let its (explicit and implicit) premises be P(x) = {p1, . . . , pk} with {p1, . . . , pk} for the conclusion of x. Let be the set of knowledge bases available for factuality judgments (i.e., S, annotator knowledge, referred papers), and let Factuality : {1, 2, 3, 4, 5}, := (cid:91) xA P(x), be 5-point scoring function for claims and premises given knowledge base. Define selector KB : that chooses the knowledge base used for each item (for claims C, KB(x) = S; for premises pi, KB(pi) = KBi K). For given KB(x) K, we notate the factuality function as FactualityKB(x)(). Agg is an operator that aggregates list of scores to single score. The ADVANCED REVIEWSCORE of is: ReviewScoreadv(x) = ClaimScore(x) = FactualityS(x), ArgScore(x) = Agg(cid:0)FactualityKBi(pi)piP(x) (cid:1) , if (claim), if (argument), QScore(x) = UnanswerabilityS(x), if (question). Aggregation methods. To aggregate premise factuality scores as single ArgScore, we introduce two aggregation methods: logical conjunction and weighted average. Following the literature of logic, an argument is true if and only if all premises are true. We define premise is true if and only if it has factuality score 35, and otherwise, it is false. We dubbed this aggregation as logical conjunction, which follows the binary classification setup. However, if an annotator mistakenly evaluates one of premises, then the error propagates to the entire argument. To alleviate this issue, we also aggregate by weighted average. Since it is difficult to measure the importance of premises, 4Detailed rubric is described in Appendix H."
        },
        {
            "title": "Preprint",
            "content": "(a) (b) Figure 2: (a) Overview of an automatic argument reconstruction. Given an argumentative review point with paper, model first generates reconstructed argument (i.e., set of premises and conclusion). To check its validity, model translates NL reconstructed argument into FOL formulas, and then SAT solver judges if it is valid. To check its faithfulness, model translate FOL formulas back into the NL domain, and model judges if the reconstruction is faithful. If one of two criteria does not met, then corresponding NL feedback is given to the generator model. (b) representative example. We sample review point of Dosovitskiy et al. (2020) and its reconstruction along with corresponding formulas and keys. we instead weighted scores by untrivialness of premises (with scale of 02). This is intended to simply filter out trivially true premises by measuring their importance as 0."
        },
        {
            "title": "3.2 AUTOMATIC ARGUMENT RECONSTRUCTION",
            "content": "To evaluate ArgScore, we have to extract (explicit and implicit) premises P(x) from an argument A. Since human experts require significant amount of time and costs to do this, we automate it using LLMs. First, we check if model could directly reconstruct an argument by giving detailed instructions. To preserve the context of an argument, we also give the model submitted paper S. However, it mostly fails to generate valid and faithful reconstructions6. To resolve this issue, we add two feedback loops to ensure validity and faithfulness of an argument reconstruction (Figure 2a). An LLM alone often falls short in ensuring validity, so we include SAT solver which could automatically judges the validity of set of premises and conclusion without any logical errors. To do that, an LLM translates set of natural language (NL) premises and conclusion into first-order logic (FOL) formulas. Then, SAT solver decides whether the premises deductively implies the conclusion. If the reconstruction is valid, then an LLM translates formalized premises and conclusion back into NL domain. This process is called logical streamlining, which means to rephrase NL premises or conclusion in order to make their logico-semantic structure more transparent (Bowell & Kemp, 2014; Brun & Betz, 2016; Betz & Richardson, 2021). We then pass these streamlined NL premises and conclusion to the subsequent faithfulness feedback loop. However, if the reconstruction is invalid, then we feed rule-based reward signal to the argument reconstructor to regenerate the reconstruction. There are two types of reward signals, one is naive signal that says the formalized premises do not imply the conclusion, and the other one tells that the proof is circular. 5Since we reconstruct every argument as valid (i.e., set of premises deductively implies conclusion), there are often conditional premises that make an argument valid but are trivially true. Detailed rubric of untrivialness is described in Appendix H. 6Detailed numerical results and qualitative analysis are reported in Appendix C."
        },
        {
            "title": "Preprint",
            "content": "Although the reconstruction is now valid, it should also faithfully represents the original argument. To ensure that, we prompt an LLM to judge whether the reconstruction is faithful or not and justify its decision. If the reconstruction is faithful, then we stop iterating the loop. If the reconstruction is unfaithful, then we feed LLM judges justification to the argument reconstructor to regenerate the reconstruction. To minimize model calls in practice, we connect these two loops in series so that only valid reconstructions are judged for their faithfulness. Also, we limit the total number of loop iteration to 10 and return the last reconstructed argument if the loop fails to generate valid and faithful reconstruction. We provide an example reconstruction in Figure 2b, and details on feedback loops and model prompts are described in Appendix and H, respectively. Quality of argument reconstruction. We measure validity and faithfulness of reconstructed arguments using SAT solver and human annotators, respectively. With claude-sonnet-3.7 as base LLM, every reconstruction is valid, and average faithfulness score is 4.5 / 5. Detailed analysis including comparison with direct reconstruction is described in Appendix C."
        },
        {
            "title": "3.3 DATASET CONSTRUCTION",
            "content": "Our final destination of proposing REVIEWSCORE is to automatically filter out misinformed review points using LLMs. To verify LLMs ability to do that, we build human expert-annotated dataset to measure an agreement between humans and LLMs on REVIEWSCORE evaluation. Our dataset contains total 657 annotated review points, consisting of 143 questions, 92 claims, and 422 arguments. Specifically, 1,748 premises of the arguments are manually annotated 7. Human-review matching. We recruit 15 graduate students studying AI as human annotators, and they annotate total 40 papers submitted to ICLR 202120238. Specifically, we first make five groups by their research interests. For each group, three human annotators discuss which papers to annotate and select eight papers that are relevant with all three. Then, each human annotates selected eight papers review points which are preprocessed from OpenReview. Data curation process. We collaboratively use an LLM and humans for REVIEWSCORE data curation, where an LLM preprocesses reviews and then humans annotate those. Given review, an LLM extracts independent review points. For each review point, an LLM automatically annotates the type (i.e., claim, argument, or question) and human verifies it. If the review point is question, then human scores if the question is (un)answerable by the paper in 5-point scale and justifies it if needed. If the review point is claim, then human scores if the claim is true in 5-point scale and justifies it if needed. If the review point is an argument, then human scores the arguments factuality same as in claims (i.e., BASE REVIEWSCORE). To annotate ADVANCED REVIEWSCORE for arguments, we first run automatic argument reconstruction engine (Section 3.2) to extract underlying premises of the argument. After that, human scores if the reconstruction is faithful in 5-point scale. If the faithfulness score is less than 4 (i.e., faithful, but one or two minor changes recommended), then they skip the subsequent annotations. Otherwise, they judge the factuality of premises. For each premise, they first select knowledge base (i.e., submitted paper, annotator knowledge, or referred papers), score if the premise is true based on the knowledge base in 5-point scale, and score if the premise is (un)trivial in 3-point scale. They justify any of three decisions if needed. Trustworthiness of human annotation. Three humans independently annotate reviews (i.e., we take median value as final human label), and they are allowed to use any related materials or tools (e.g., discussion between authors and reviewers in OpenReview, web search, etc.). We also provide annotators detailed guidelines and an hour-long online orientation session. Furthermore, we highly encourage them to communicate with us through group chat so that we could give them instant responses to their questions and share with all, which builds global consensus among human annotators. Thanks to these efforts, despite the difficulty of the work, we obtain median 0.301 and highest 0.428 inter-annotator agreement in Krippendorffs Alpha (Hughes, 2021) across different annotator groups. Detailed results are described in Appendix E. 7Details of the dataset construction and statistics are described in Appendix D. 8Since ICLR 20242025 submission drafts are not opened to public, we exclude these years."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Human-model agreement on REVIEWSCORE evaluation. Model F1 Kappa F1 Kappa F1 Kappa F1 Kappa ClaimScore ArgScore QScore REVIEWSCORE Proprietary models claude-sonnet-3.7 claude-sonnet-4 gpt-4o gpt-5 gemini-2.5-flash Open-sourced models qwen3-235b-a22b llama-3.3 deepseek-v3 0.160 0.125 0.000 0.125 0.091 0.231 0.133 0. 0.137 0.130 0.119 -0.004 0.106 0.094 0.153 0.165 0.458 0.329 0.329 0.426 0.447 0.403 0.250 0.276 0.380 0.266 0.239 0.334 0.366 0.149 0.107 0. 0.542 0.507 0.538 0.543 0.512 0.531 0.551 0.597 0.387 0.357 0.340 0.300 0.225 0.234 0.284 0.228 0.448 0.367 0.405 0.441 0.439 0.436 0.409 0. 0.355 0.333 0.362 0.310 0.318 0.243 0.355 0."
        },
        {
            "title": "4 RELIABILITY OF AUTOMATIC EVALUATION OF REVIEWSCORE",
            "content": "To evaluate LLMs ability to evaluate REVIEWSCORE, we describe an experimental setup (Section 4.1) and show human-model agreement results (Section 4.2). We also compare the effectiveness of BASE and ADVANCED REVIEWSCORE (Section 4.3). We further analyze human-model disagreements and the effect of providing authors response to models (Section 4.4)."
        },
        {
            "title": "4.1 SETUP",
            "content": "Given score rubrics in 5-point scale, an LLM evaluates REVIEWSCORE according to knowledge base it selects. We only provide submitted paper to model since we assume that the model has pretrained human-level internal knowledge and has general understanding of referred papers. Detailed model prompts are described in Appendix H. Language models. To measure LLMs ability to automatically evaluate REVIEWSCORE, we perform experiments on eight current state-of-the-art LLMs that achieve significantly high alignments with humans. We include five proprietary models, claude-sonnet-3.7 (Anthropic, 2025a), claude-sonnet-4 (Anthropic, 2025b), gpt-4o (OpenAI, 2024), gpt-5 (OpenAI, 2025), and gemini-2.5-flash (Gemini Team, Google, 2025), and three open-sourced models, qwen3-235b-a22b (Yang et al., 2025), deepseek-v3 (DeepSeek-AI et al., 2024), and llama-3.3 (AI@Meta, 2024)9. To get consistent and reliable scores from LLM judges, we set low temperature (i.e., 0) and select the highest probability response from the model (Liang et al., 2022; Liu et al., 2023; Gu et al., 2024). We exclude reasoning models as LLM judges since REVIEWSCORE mostly depends on grounding and evidence, not longer or smarter chains of thought. Evaluation metrics. We use different sets of metrics for two types of problem formulations, binary classification and 5-point scale scoring. For the binary classification, since majority of humanannotated labels are Not misinformed, we mainly use F1 score which is robust to class imbalance. For the 5-point scale scoring, since majority of human-annotated scores are 4 and 5, we mainly use Quadratic Weighted Kappa (Warrens, 2012), variant of Cohens Kappa (Cohen, 1960), that is robust to the skewed data distribution. Hereinafter, we call this metric Kappa for simplicity. To provide more comprehensive results, we additionally use Precision and Recall for the binary classification and Pearson rank correlation and Gwets AC2 (Gwet, 2001) for the 5-point scale scoring."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "We empirically validate the alignment of human-annotated and model-estimated REVIEWSCORE using different models and evaluation metrics in Table 1. Most models show 0.40.5 F1 score and 0.30.4 Kappa score, which indicates moderate agreement between humans and models on REVIEWSCORE evaluation. However, there are differences in human-model agreements for three 9Details of model specifications are described in Appendix"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Comparison of human-model agreement of BASE vs. ADVANCED REVIEWSCORE. Model Metric Base Advanced w/o Agg Advanced claude-sonnet-3. gpt-5 gemini-2.5-flash deepseek-v3 F1 Kappa F1 Kappa F1 Kappa F1 Kappa 0.185 0.153 0.243 0.223 0.167 0.138 0.203 0.120 (b) (a) 0.262 0.280 0.316 0.315 0.349 0.330 0.163 0.202 0.458 0.380 0.426 0.334 0.447 0.366 0.276 0.196 Figure 3: Human-model disagreement analysis. Comparison of disagreement types of claude-sonnet-3.7 and claude-sonnet-4. (a) Types of human-model disagreements. (b) types of review points. Regardless of the models, questions show the highest agreement, arguments follow subsequently, and claims show the lowest agreement. Specifically, for claims, some models show zero F1 score or negative Kappa score. We analyze human-model disagreements thoroughly in Section 4.4 and conclude that since claims lack supporting evidence and are often value-laden, models often misinterpret the intended meaning of the claims or judge differently than humans. For proprietary models, claude-sonnet-3.7 shows the highest human-model agreements types of review points. One thing to note is claude-sonnet-4 conacross different sistently shows lower agreement its because than its preceding model. We find out claude-sonnet-4 sometimes overthink and more frequently predicts the minority humanannotated scores than claude-sonnet-3.7 does. In contrast, comparing F1 scores of gpt-4o and gpt-5, the advanced model shows higher agreement than the preceding model. that For open-sourced models, qwen3-235b-a22b shows best human-model agreements in terms of F1 score. Open-sourced models show lower human-model agreements than proprietary models, but the gap is not significant. Open-sourced models sometimes show better agreements than proprietary models, for instance, llama-3.3 outperforms claude-sonnet-4 on REVIEWSCORE evaluation. However, open-sourced models consistently show lower agreements on evaluating ArgScore. More results and qualitative analysis of model evaluation are described in Appendix F."
        },
        {
            "title": "4.3 BASE VS. ADVANCED REVIEWSCORE",
            "content": "We compare human-model agreements of BASE and ADVANCED REVIEWSCORE using different models and evaluation metrics. Since the only difference is in defining scores for arguments, we compare REVIEWSCORE for arguments in Table 2. To verify the effectiveness of aggregation methods, we additionally include ADVANCED REVIEWSCORE for arguments without aggregation (i.e., factuality of premises). Regardless of models, ADVANCED REVIEWSCORE clearly shows higher agreements than BASE REVIEWSCORE. For claude-sonnet-3.7, ADVANCED REVIEWSCORE performs 2.48 higher F1 score and 2.48 higher Kappa score than the BASE REVIEWSCORE. The aggregation methods also contribute to higher agreements. For claude-sonnet-3.7, ADVANCED REVIEWSCORE performs 1.75 higher F1 score and 1.36 higher Kappa score than the one without aggregation."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Effect of providing Authors Response (AR) to model for REVIEWSCORE evaluation. F1 Kappa w/o AR w/ AR w/o AR w/ AR WScore ArgScore w/o Agg ArgScore QScore ClaimScore REVIEWSCORE 0.211 0.230 0.329 0.507 0.125 0.367 0.286 0.283 0.447 0.513 0. 0.449 0.178 0.271 0.266 0.357 0.130 0.333 0.212 0.312 0.340 0.324 0. 0."
        },
        {
            "title": "4.4 ANALYSIS",
            "content": "REVIEWSCORE Distribution. We present model-estimated REVIEWSCORE distribution and compare with humanannotated ones. Specifically, following the binary classification setup, we report the proportion of misinformed review points in Table 3. There is clear trend that models more frequently decide review points as Misinformed than humans. Models decide 31.2% of reviews points as Misinformed in average, where humans decide only 17.8% as Misinformed. The discrepancy is the largest in claims where models decide 22.8% as Misinformed and humans decide only 5.9% as Misinformed. To understand these results more deeply, we continue our analysis on the following human-model disagreements. Proportion of misinTable 3: formed review points estimated by models and humans. Model Human Claims Arguments Questions Review points 22.8 30.1 40. 31.2 5.9 17.7 26.4 17.8 Human-model disagreements. We analyze types of human-model disagreements in Figure 3a. Across all review points, there are two common types of disagreements, which are models misunderstand or incorrectly reason and models predict scores that are minority of human-annotated scores. These two types comprise nearly all disagreements on arguments and questions, and about half of disagreements on claims. For claims, we observe that 36% of disagreements are caused by either claims are value-laden or lack supporting reason. These make the factuality judgment of claims subjective, leading to low human-model agreement. Practical limitations such as incorrect pdf parsing and not providing appendix to models cause nontrivial portion of disagreements. We also analyze the reason why claude-sonnet-3.7 shows higher human-model agreements than claude-sonnet-4 in Figure 3b. The advanced model shows smaller number of misunderstanding or incorrect reasoning, however, it shows higher number of overthinking10 and minority human-annotated score predictions, leading to lower human-model agreement. Helpfulness of authors response. We study whether providing authors responses of reviews to model benefits the automatic REVIEWSCORE evaluation using claude-sonnet-4 in Table 4. Providing authors response clearly shows higher human-model agreement than not providing it. Specifically, the agreement of REVIEWSCORE increases by 22.3% in F1 score and 13.2% in Kappa score by incorporating authors response. Note that claude-sonnet-4 with authors response achieves the highest agreement compared to all models without authors response. Furthermore, the agreement of WScore and ArgScore without aggregation (i.e., premise factuality) also increases significantly by incorporating authors response. These observations show that authors response largely benefits detecting misinformed reviews by providing additional cues for model judgments."
        },
        {
            "title": "5 LIMITATIONS",
            "content": "We acknowledge three types of limitations of our work. First, there are technical and practical limitations in the automated REVIEWSCORE evaluation. Throughout Section 4, human-model agreement seems low even with authors response. However, compared to Du et al. (2024) which measures human-model agreement on Deficient review detection and reports the best F1 score as 0.220, our 10We judge model overthinks if its justification of score is significantly longer than another model. We also manually verify if model overanalyzes review point."
        },
        {
            "title": "Preprint",
            "content": "work reports the best F1 score as 0.449, indicating more than 2 agreement. To save API calling costs, when the paper is given to LLMs, we only provide text and tables of the main paper, where any figure or appendix is excluded. We also observe nontrivial amount of incorrect pdf parsing. Second, there are technical limitations in the automatic argument reconstruction. The reconstruction output is not always perfect since it depends on the capability of the base model. However, we observe considerable output quality improvement when we upgrade the base model to claude-sonnet-4, indicating that the reconstruction engine would perform better as the model improves in general. Lastly, there are practical limitations in the dataset construction. Since human annotators are graduate students with varying skillfulness and the annotation requires significant amount of cognitive load, there are unavoidable noise in human annotation. Furthermore, manual selection of papers by human annotators might introduce any kind of unintended biases. However, to collect the most reliable annotation under our limited budget, we inevitably choose this method."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce REVIEWSCORE, new evaluation of peer review quality, focusing on detecting misinformed review points. To automate REVIEWSCORE evaluation using LLMs and validate it, we also construct trustworthy human-annotated dataset. The results show moderate human-model agreement, and further comprehensive disagreement analysis supports the potential of fully automated REVIEWSCORE evaluation with current state-of-the-art LLMs. We hope our work could contribute to mitigate the issue of low-quality reviews in AI conferences."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. The llama 3 herd of models. 07 2024. URL https://arxiv.org/abs/2407. 21783. Family report for Llama 3; used to cite Llama 3.3 70B Instruct. Anthropic. Claude 3.7 sonnet system card. Technical report, Anthropic PBC, 02 2025a. https://assets.anthropic.com/m/785e231869ea8b3b/original/ URL claude-3-7-sonnet-system-card.pdf. System card / technical report. Anthropic. System card: resonnet 4. URL https://www-cdn.anthropic.com/ port, Anthropic PBC, 05 2025b. 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. System card / technical report. Claude opus 4 & claude"
        },
        {
            "title": "Technical",
            "content": "Simran Arora, Avanika Narayan, Mayee Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. Ask me anything: simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=bhUPJnS2g0X. Jc Beall, Greg Restall, and Gil Sagi. Logical Consequence. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2024 edition, 2024. Gregor Betz and Kyle Richardson. Deepa2: modular framework for deep argument analysis with pretrained neural text2text language models. arXiv preprint arXiv:2110.01509, 2021. Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. LanIn The Eleventh International Conferguage models are realistic tabular data generators. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= cEygmQNOeI. Tracy Bowell and Gary Kemp. Critical Thinking: Concise Guide. Routledge, 2014. Jordan Boyd-Graber, Naoaki Okazaki, and Anna Rogers. Acl23 peer review policies. ACL 2023 Blog, February 2023. URL https://2023.aclweb.org/blog/review-acl23/. Program Chairs blog post. Georg Brun and Gregor Betz. Analysing practical argumentation."
        },
        {
            "title": "In The argumentative turn in",
            "content": "policy analysis: Reasoning about uncertainty, pp. 3977. Springer, 2016. Jacob Cohen. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746, 1960. Corinna Cortes and Neil D. Lawrence. Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. arXiv preprint arXiv:2109.09774, 2021. doi: 10.48550/arXiv.2109.09774. URL https://arxiv.org/abs/2109.09774. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: DiffusionIn The Eleventh International Conferbased semantic image editing with mask guidance. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= 3lge0p5o-M-. Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large In The Eleventh International Conferlanguage models for interpretable logical reasoning. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= 3Pf3Wg6o-A4. Leonardo De Moura and Nikolaj Bj√∏rner. Z3: An efficient smt solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 337340. Springer, 2008. DeepSeek-AI, Aixin Liu, Bei Feng, et al. Deepseek-v3 technical report. 12 2024. URL https: //arxiv.org/abs/2412.19437. Technical report; applicable to the V3-0324 checkpoint lineage."
        },
        {
            "title": "Preprint",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Bradley Dowden."
        },
        {
            "title": "Logical",
            "content": "reasoning (dowden), 2024. URL https://human. libretexts.org/Bookshelves/Philosophy/Logic_and_Reasoning/ Logical_Reasoning_(Dowden). Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, et al. Llms assist nlp researchers: Critique paper (meta-) reviewing. arXiv preprint arXiv:2406.16253, 2024. Catarina Dutilh Novaes. Argument and Argumentation. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2022 edition, 2022. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=yf1icZHC-l9. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=NAQvF08TcyG. Gemini Team, Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Technical report, Google DeepMind, 06 2025. URL https://storage.googleapis.com/deepmind-media/gemini/ gemini_v2_5_report.pdf. Technical report; covers Gemini 2.5 Pro and 2.5 Flash. Alexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice Oh, Alekh Agarwal, Danielle Belgrave, and Nihar Shah. Peer reviews of peer reviews: randomized controlled trial and other experiments. PloS one, 20(4):e0320444, 2025. Leo Groarke. Informal Logic. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2024 edition, 2024. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, and Jian Guo. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. URL https://arxiv.org/abs/2411.15594. Recommends temperature=0 to reduce randomness and stabilize judge outputs. Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, and Chloe Clavel. Automatic analysis of substantiation in scientific peer reviews. arXiv preprint arXiv:2311.11967, 2023. Kilem Gwet. Handbook of inter-rater reliability. Gaithersburg, MD: STATAXIS Publishing Company, pp. 223246, 2001. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=_CDixzkzeyb. Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model In International Conference on Learning compression with weighted low-rank factorization. Representations, 2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9."
        },
        {
            "title": "Preprint",
            "content": "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve, 2023. URL https://openreview.net/ forum?id=NiEtU7blzN. John Hughes. krippendorffsalpha: An package for measuring agreement using krippendorffs alpha coefficient. arXiv preprint arXiv:2103.12170, 2021. Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun KIM, Stanley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language In International Conference on Learning Representations, 2022. URL https: models. //openreview.net/forum?id=vfsRB5MImo9. Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. Tailoring language generation models under total variation distance. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=VELL0PlWfc. Jacalyn Kelly, Tara Sadeghieh, and Khosrow Adeli. Peer review in scientific publications: benefits, critiques, & survival guide. Ejifcc, 25(3):227, 2014. Jaeho Kim, Yunseok Lee, and Seulki Lee. Position: The ai conference peer review crisis demands author feedback and reviewer rewards. arXiv preprint arXiv:2505.04966, 2025. Yoonjeon Kim, Hyunsu Kim, Junho Kim, Yunjey Choi, and Eunho Yang. Learning input-agnostic manipulation directions in styleGAN with text guidance. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= 47B_ctC4pJ. Anna Kukleva, Moritz Bohle, Bernt Schiele, Hilde Kuehne, and Christian Rupprecht. Temperature schedules for self-supervised contrastive methods on long-tail data. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=ejHUr4nfHhD. Joe Lau and Jonathan Chan. What is an argument? Critical Thinking Web, 2025. URL https: //philosophy.hku.hk/think/arg/arg.php. Module A01: Argument analysis. Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=bVuP3ltATMz. Kevin Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. Mix{kd}: Towards efficient distillation of large-scale language models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=UFGEelJkLu5. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. URL https://arxiv.org/abs/2211. 09110. Standardizes eval settings; many HELM runs use temperature=0 for reproducibility. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using GPT-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 25112522. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023. emnlp-main.153/. Uses low temperature / deterministic decoding for stable evaluator outputs. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DHyHRBwJUTN."
        },
        {
            "title": "Preprint",
            "content": "Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=aBsCjcPu_tE. Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, and Jinwoo Shin. STUNT: Few-shot tabular learning with self-generated tasks from unlabeled tables. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=_xlsjehDvlY. NeurIPS 2025 Program Chairs. Neurips 2025 reviewer guidelines. NeurIPS 2025 website, 2025. URL https://neurips.cc/Conferences/2025/ReviewerGuidelines. Guidelines page. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partiallycorrect solutions. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4D4TSJE6-K. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth In The Eleventh International Confer64 words: Long-term forecasting with transformers. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= Jbdc0vTOcol. OpenAI. Gpt-4o system card. Technical report, OpenAI, 08 2024. URL https://cdn.openai. com/gpt-4o-system-card.pdf. System card / technical report. OpenAI. Gpt-5 system card. Technical report, OpenAI, 08 2025. URL https://cdn.openai. com/gpt-5-system-card.pdf. System card / technical report. Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, William Jurayj, Miriam Wanner, Shaobo Liang, Candice Morgan, et al. Claimcheck: How grounded are llm critiques of scientific papers? arXiv preprint arXiv:2503.21717, 2025. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=FjNys5c7VyY. Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, and Iryna Gurevych. Lazyreview dataset for uncovering lazy thinking in nlp peer reviews. arXiv preprint arXiv:2504.11042, 2025. Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi. Progressive prompts: Continual learning for language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=UJTgQBc91_. Laura Eline Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktaschel, and Edward Grefenstette. Large language models are not zero-shot communicators, 2023. URL https: //openreview.net/forum?id=WgbcOQMNXB. Abdelrahman Sadallah, Tim Baumg AIrtner, Iryna Gurevych, and Ted Briscoe. The good, the bad and the constructive: Automatically measuring peer reviews utility for authors. arXiv preprint arXiv:2509.04484, 2025. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4."
        },
        {
            "title": "Preprint",
            "content": "Shah. Nihar B. peer doi: challenges-experiments-and-computational-solutions-in-peer-review/. in 2022. https://cacm.acm.org/research/ Challenges, Communications computational 65(6):7687, solutions June 10.1145/3528086. experiments, and ACM, of URL review. the Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can CLIP benefit vision-and-language tasks? In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=zf_Ll3HZWgy. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= fR3wGCk-IXp. Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled In International Conference on Learning Representations, 2021. URL https: time series. //openreview.net/forum?id=4c0J6lwQ4_. Satya Narayan Shukla and Benjamin Marlin. Heteroscedastic temporal variational autoencoder for irregularly sampled time series. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Az7opqbQE-3. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, In The Eleventh International Conferand Lijuan Wang. Prompting GPT-3 to be reliable. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= 98p5x51L5af. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-avideo: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= nJfylDvgzlq. Ivan Stelmakh, Nihar B. Shah, Aarti Singh, and Hal Daume III. novice-reviewer experiment to address scarcity of qualified reviewers in large conferences. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 47854793, 2021. doi: 10.1609/aaai.v35i6.16610. URL https://ojs.aaai.org/index.php/AAAI/article/view/16610. Taylor and Francis. Understanding the peer review process. what is peer review? guide URL https://authorservices.taylorandfrancis.com/ for authors, 2025. publishing-your-research/peer-review. Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. MICN: Multiscale local and global context modeling for long-term series forecasting. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=zt53IDUR1U. Matthijs Warrens. Some paradoxical results for the quadratically weighted kappa. Psychometrika, 77(2):315323, 2012. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR. An Yang, Anfeng Li, Baosong Yang, et al. Qwen3 technical report. 05 2025. URL https:// arxiv.org/abs/2505.09388. Covers the Qwen3 family including the 235B A22B series (e.g., 2507 checkpoint)."
        },
        {
            "title": "Preprint",
            "content": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided network for irregularly sampled multivariate time series. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Kwm8I7dU-l5. Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vSVLM2j9eie. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5NTt8GFjUHkr. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting In The Eleventh International Conferenables complex reasoning in large language models. ence on Learning Representations, 2023a. URL https://openreview.net/forum?id= WZH7099tgfM. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/ forum?id=92gvk82DE-."
        },
        {
            "title": "A FUTURE IMPACT",
            "content": "We expect automatic REVIEWSCORE evaluation could greatly benefit different roles of the current peer reviewing system. For authors, by providing reconstruction of argumentative review points, it helps to understand or clarify the reviewers intention and to formulate rebuttal. For reviewers, by providing REVIEWSCORE of their review points, it allows them to verify the review quality by themselves and helps reviewers to better understand the paper. For metareviewers, by providing REVIEWSCORE of each reviewer, it could assist their final decisions. To summarize, it could serve as an automated system for managing the review quality."
        },
        {
            "title": "B REVIEW QUALITY CRITERIA DISCUSSION",
            "content": "First, we recruit group of three graduate students studying AI and NLP and let them independently analyze reviews of ten common manually selected submitted papers from ICLR 2021202311. Specifically, we guided the group to: decompose review into several independent review points, evaluate quality of the review points by 5-point scale with their own criteria, and justify their scores that ends with meta-sentence that does not involve papers context which is used for our further analysis. During human evaluation, we minimize our effort to provide detailed guidelines to facilitate bias-free human analysis. Then, we categorize these justifications in order to find common features of low-quality review points. There are five common types: (1) questions that can be already addressed by the paper, (2) comments that reflect misunderstanding of the paper, (3) out-of-scope remarks, (4) observations pointing out minor details, and (5) unclear points. However, for the last three types, we observe that majority of review points are agreed or argued by single human annotator. In other words, given review point, only single human argues it is out-of-scope, whereas other two humans argue it is within scope and could be potential drawback of the submitted paper. Following this pattern, single human argues review point is addressing minor details but others argue it is major point, and single human argues review point is unclear whereas others do not agree with this. In contrast, the first two types of review points (i.e., answerable questions and misunderstood comments) are mostly agreed by two or more human annotators, indicating that these two types have more objective and trustworthy criteria for detecting low-quality reviews. Based on these observations, we focus on evaluating review points based on the first two criteria. Lastly, we share some meaningful insights from this group discussion that strongly motivates our work. Based on the analysis of score justifications, given review point, some (sub)sentences are high-quality or factually correct but others could be misinformed or factually incorrect. This later motivates our premise-level factuality evaluation (Section 3.1). Furthermore, during the group discussion, we observe that human annotators sometimes struggle which parts in review point they should weigh more to evaluate the review point. This later motivates our aggregation methods which include logical conjunction and weighted average by untrivialness (Section 3.1). We also discover that human evaluations are sometimes incorrect. We leave this as limitation of our work. However, to alleviate this issue, we recruit three human annotators for every instance and further ensure the annotation quality by providing them careful guidelines and actively communicating with them to build global consensus on the evaluation criteria (Section 3.3). 11Since those papers include papers written by authors of our work, we do not share the full list of target papers to keep anonymity. We will uncover the full list after the paper is published."
        },
        {
            "title": "C AUTOMATIC ARGUMENT RECONSTRUCTION DETAILS AND RESULTS",
            "content": "C."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We elaborate components of automatic argument reconstruction engine (Section 3.2, Figure 2) in the following. We also refer the corresponding prompt used in each step if it exists. 1. Given an argumentative review point, an LLM extracts verbatim conjecture and its verbatim reason statements (Figure 23). 2. Given verbatim conclusion and reasons of the argumentative review point and corresponding paper parsed from pdf file, an LLM reconstructs the argument into premise-conclusion structure. At the same inference, the model also translates (or formalizes) NL premises and conclusion into corresponding FOL formulas and generate keys which assign NL meaning to variables and predicates. To facilitate the model to generate valid formalization, the model generates deductive proof using formalized premises and conclusion at the end (Figure 24). 3. Given FOL premises and conclusion with keys and deductive proof, an LLM extracts necessary FOL premises for the deductive proof, write python program using Z3 (De Moura & Bj√∏rner, 2008) that automatically checks the validity of the necessary FOL premises and conclusion, extracts final FOL conclusion that is used in the python program, and judges whether the proof is circular (i.e., whether the final FOL conclusion is included in one of necessary FOL premises.) (Figure 25, Figure 26). 4. If the proof is circular, then NL feedback indicating circularity of the proof is sent to Step 2, and the model re-generates an argument reconstruction. Otherwise, we run the python program that checks the validity of the reconstruction. However, if the program returns an error, the model takes this error message and re-generate the python program that fixes the error (Figure 27). If the reconstruction is invalid, then NL feedback indicating invalidity of the reconstruction is sent to Step 2, and the model re-generates an argument reconstruction. Otherwise (i.e., if the reconstruction is valid), we proceed to the next step. 5. To check faithfulness of the reconstruction, the model first translates FOL premises and conclusion (one of the outputs of the model in Step 2) with keys (one of the outputs of the model in Step 1) into NL premises and conclusion (Figure 28). This process is called logical streamlining in logic and critical thinking (Bowell & Kemp, 2014; Brun & Betz, 2016; Betz & Richardson, 2021). 6. Lastly, given an original argumentative review point (or an argument) and the streamlined NL premises and conclusion, the model judges whether the reconstruction is faithful with justifications (Figure 29). If the reconstruction is unfaithful, then NL feedback including the models justifications is sent to Step 2, and the model re-generates an argument reconstruction accordingly. Otherwise, since the reconstruction is valid yet faithful, the feedback loop is finished and the streamlined NL premises and conclusion become final argument reconstruction."
        },
        {
            "title": "Preprint",
            "content": "C.2 QUANTITATIVE RESULTS To verify effectiveness of the feedback loop, we report the argument reconstruction performance with and without feedback in Table 5. We mainly measure average validity and faithfulness of the reconstructed arguments using SAT solver and human annotators, respectively. We provide score rubric for evaluating faithfulness in Figure 4. We also include the pass rate, which indicates whether the (last) reconstructed argument fulfill the validity and faithfulness criteria in the feedback loop. Furthermore, we report the average number of loop iterations to check if the feedback loops are actively used. In Table 5, we verify that the proposed method (i.e., w/ feedback) achieves perfect validity and nearly perfect faithfulness and pass rate, whereas the performance of the direct reconstruction (i.e., w/o feedback) largely lags behind that. We observe that the average number of loop iterations is 3.09, indicating active usage of feedback signals, but the number varies lot depending on the clarity of logical structures of the arguments. Table 5: Performance of Automatic Argument Reconstruction. w/o feedback w/ feedback Validity Faithfulness Pass Rate # of loops 0.895 2.91 0.369 N/A 1.00 4.47 0.935 3.09 2.73 Faithfulness Score Rubric for Argument Reconstruction 5: Definitely faithful, no change is essential 4: Faithful, but one or two minor details in premises need to change in order to fully expres the original context 3: Faithful, but more than two minor details in premises need to change 2: Not faithful, one or two major error exist 1: Definitely not faithful, more than two major errors exist Figure 4: Score rubric for evaluating faithfulness of argument reconstruction by human annotators. C.3 QUALITATIVE RESULTS We present three representative examples of model-generated argument reconstruction and compare outputs with and without the feedback loop. We observe the direct reconstruction (i.e., w/o feedback) often generates either invalid or unfaithful reconstructed arguments, however, the proposed method (i.e., w/ feedback) generates valid yet faithful reconstructed arguments. We provide detailed analysis in Figure 57."
        },
        {
            "title": "Preprint",
            "content": "Input: Argument (Review Point) & Paper Paper: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (Zhou et al., 2023a) Argument (Review Point): The paper lacks insights on key questions. Does the model have ability to decompose questions very different from demo examples into correct sub-problems? Even the model solves each sub-problem correctly, what makes the model compose the final solution correctly? Output: Reconstructed Argument Direct Reconstruction (w/o feedback): P1: The paper does not demonstrate whether the model has the ability to decompose questions very different from demo examples into correct sub-problems. P2: The paper does not explain what makes the model compose the final solution correctly even when it solves each sub-problem correctly. P3: If paper fails to provide insights on key questions about proposed method, then the paper lacks insights on key questions. P4: Questions about decomposition ability and solution composition are key questions for the least-tomost prompting method. C: The paper lacks insights on key questions. Proposed Method (w/ feedback): P1: The paper does not demonstrate whether the model has the ability to decompose questions very different from demo examples into correct sub-problems. P2: If the paper does not demonstrate whether the model has the ability to decompose questions very different from demo examples into correct sub-problems, then the paper fails to provide insights on the key question about decomposition ability. P3: If the paper fails to provide insights on the key question about decomposition ability or fails to provide insights on the key question about solution composition, then the paper fails to provide insights on key questions. P4: If the paper fails to provide insights on key questions, then the paper lacks insights on key questions. C: The paper lacks insights on key questions. Analysis - For the direct reconstruction, P1, P2, P4 semantically implies an intermediate conclusion that The paper fails to provide insights on key questions about the proposed method., but this connecting premise is not explicitly reconstructed. Therefore, this reconstruction is invalid []. However, if we consider that this connecting premise is implicitly presumed, then the reconstruction faithfully [] represents the original argument. - For the proposed method, the reconstruction is logically valid [] yet faithfully [] represents the original argument. Figure 5: Example #1 of automatic argument reconstruction."
        },
        {
            "title": "Preprint",
            "content": "Input: Argument (Review Point) & Paper Paper: ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2023) Argument (Review Point): ReAct alone works fairly well for some tasks, but self-consistent Chain of Thought often outperforms ReAct alone, and leads to fairly dramatic increases in performance. The best models use smart combination of both CoT and ReAct, using heuristics that likely maximize performance on the dataset(s) (see ReAct CoTSC strategy), which makes the performance of ReAct by itself slightly less impressive (though still think this is useful contribution for the community) Output: Reconstructed Argument Direct Reconstruction (w/o feedback): P1: The best models use smart combination of both CoT and ReAct. P2: Models use heuristics that likely maximize performance on the dataset(s) (see ReAct CoTSC strategy). P3: If models use heuristics that likely maximize performance on the dataset(s), then the performance of ReAct by itself is less impressive. P4: If the best models use combination of both CoT and ReAct rather than ReAct alone, then the performance of ReAct by itself is less impressive. P5: The performance of ReAct by itself being less impressive still means it is useful contribution for the community. C: ReAct alone works fairly well for some tasks, but self-consistent Chain of Thought often outperforms ReAct alone, and leads to fairly dramatic increases in performance. Proposed Method (w/ feedback): P1: ReAct alone works fairly well for some tasks. P2: Self-consistent Chain of Thought often outperforms ReAct alone. P3: The best models use smart combination of both CoT and ReAct. P4: Models use heuristics like the ReAct CoTSC strategy that maximize performance. P5: If Self-consistent Chain of Thought often outperforms ReAct alone, and the best models use smart combination of both CoT and ReAct, and models use heuristics like the ReAct CoTSC strategy that maximize performance, then the performance of ReAct by itself is slightly less impressive. P6: If ReAct alone works fairly well for some tasks, and the performance of ReAct by itself is slightly less impressive, and technique can be useful contribution even if its performance is less impressive, then ReAct is still useful contribution for the community. P7: technique can be useful contribution even if its performance is less impressive. C: The performance of ReAct by itself is slightly less impressive, and ReAct is still useful contribution for the community. Analysis - For the direct reconstruction, both pairs of premises, P1 & P4, and P2 & P3, deductively imply an intermediate conclusion that The performance of ReAct by itself is less impressive.. However, there is no logical connection between this intermediate conclusion and the final conclusion C, which indicates the reconstruction is invalid []. Furthermore, the final conclusion does not correctly represent the original argument, which means the reconstruction is unfaithful []. - For the proposed method, the premises deductively imply the final conclusion (i.e., valid []), and the reconstruction correctly yet completely represents the original argument (i.e., faithful []), including the correct final conclusion. Figure 6: Example #2 of automatic argument reconstruction."
        },
        {
            "title": "Preprint",
            "content": "Input: Argument (Review Point) & Paper Paper: Automatic Chain of Thought Prompting in Large Language Models (Zhang et al., 2023) Argument (Review Point): Finally, if clustering was the way to go, the authors could have tried to cluster samples into n-clusters, see where the test sample is fitting, and could have tried to use just one prompt instead of different ones at each time. This might have reduced the compute cost and would be interesting to see the performance vs resource trade-off. Output: Reconstructed Argument Direct Reconstruction (w/o feedback): P1: The authors could have tried to cluster samples into n-clusters and see where the test sample is fitting. P2: The authors could have tried to use just one prompt instead of different ones at each time. P3: Using one prompt instead of different ones would reduce the compute cost. P4: If method reduces compute cost, it would be interesting to see the performance vs resource trade-off. P5: If the authors could have done something that would be interesting to see, then they should have done it. C: The authors should have tried to cluster samples into n-clusters, see where the test sample is fitting, and use just one prompt instead of different ones at each time. Proposed Method (w/ feedback): P1: An alternative approach would be to cluster samples into n-clusters, see where the test sample is fitting, and use just one prompt instead of different ones. P2: If an alternative approach would use just one prompt instead of different ones, then the alternative approach would reduce compute cost. P3: If the alternative approach would reduce compute cost, then examining the performance vs. resource trade-off of the alternative approach would be interesting. P4: If an alternative approach would use just one prompt instead of different ones and examining the performance vs. resource trade-off of the alternative approach would be interesting, then the alternative approach would have been interesting to evaluate for its performance vs. resource trade-off. C: The alternative approach would have been interesting to evaluate for its performance vs. resource trade-off. Analysis - For the direct reconstruction, P3 and P4 imply an intermediate conclusion that Using one prompt instead of different ones would be interesting to see the performance vs resource trade-off.. Let us call this IC1. IC1 and P5 imply another intermediate conclusion that The authors should have tried using one prompt instead of different ones.. Let us call this IC2. Lastly, P1, P2, and IC2 imply the final conclusion C, indicating the reconstruction is valid []. However, the final conclusion is stronger than what the original argument states. The original argument suggests an alternative approach, but the reconstructed final conclusion obligates authors to try it, meaning the reconstruction is unfaithful []. - For the proposed method, the reconstruction is valid [] and faithful [], including the correct degree of strength of the final conclusion. Figure 7: Example #3 of automatic argument reconstruction."
        },
        {
            "title": "D DATASET DETAILS",
            "content": "D.1 TERMS OF USE AND LICENSE All papers and corresponding reviews used in our work are crawled from OpenReview12, and our work is consistent with the Openreview terms of use13. Following these terms, we will release our work with CC-BY 4.0 license. D.2 PILOT DATASET Before we construct the main dataset in Section 3.3, to ensure trustworthiness and reliability of human annotation, we conduct pilot study. We recruit three graduate students studying AI as human annotators, and let them choose total five papers submitted to ICLR 2021202514. Specifically, each human annotates reviews of three papers in OpenReview, where only one paper is authored by themselves and the other two non-authored papers are assigned in common. To ensure trustworthiness of non-authors annotation, three humans annotate reviews and then we take median value as final human label. D.3 DATASET STATISTICS We present the REVIEWSCORE dataset statistics including the pilot and main subsets in Table 6. We include number of instances and percentage of misinformed labels. Table 6: REVIEWSCORE dataset statistics. Pilot Main Total ICLR Years # Papers # Review(er)s 20212025 5 20212023 40 155 - 45 174 Number of instances Review Points Questions Claims Arguments Premises 84 22 5 57 227 573 121 87 365 1, 657 143 92 422 1,748 D.4 FULL LIST OF PAPERS We provide full list of papers used in the main subset of REVIEWSCORE dataset in Table 7. As explained in Section 3.3, eight papers are selected by each human annotator group with common research interest."
        },
        {
            "title": "E ANALYSIS OF HUMAN ANNOTATORS",
            "content": "E.1 HUMAN ANNOTATOR EXPERTISE We report human annotators expertise on REVIEWSCORE evaluation in Table 8. Specifically, for each annotator, we indicate number of publications in AI/ML (including arXived works) and an averaged paper relevance score across eight assigned papers. We also indicate averaged numbers and scores for each group. The results show that an average number of publication is 3.93 and average paper relevance score is 4.06 / 5, indicating highly-experienced and relevant experts conduct human annotation process. However, we observe that there are inter-group gaps in human expertise. Specifically, Group 2 and 4 shows significantly higher number of publications and paper relevance 12https://openreview.net 13https://openreview.net/legal/terms 14Unlike the main dataset, we include ICLR 20242025 since authors can access to their own submissions."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Full list of ICLR submitted papers used in the REVIEWSCORE dataset. Group Topic Paper Title 1 3 4 5 Image Generation Lipman et al. (2023), Hertz et al. (2023), Couairon et al. (2023), Singer et al. (2023), Meng et al. (2022), Gal et al. (2023), Poole et al. (2023), Kim et al. (2023) Time Series ML Shukla & Marlin (2021), Zhang et al. (2022), Borisov et al. (2023), Nie et al. (2023), Nam et al. (2023), Zhang & Yan (2023), Wang et al. (2023), Shukla & Marlin (2022) LLM Reasoning / Hsu et al. (2022), Ji et al. (2023), Lu et al. (2023), Ni et al. (2023), Compression Fu et al. (2023), Liang et al. (2021), Ruis et al. (2023), Arora et al. (2023) LLM / VLM Hu et al. (2022), Kukleva et al. (2023), Wei et al. (2022), Sanh et al. (2022), Shen et al. (2022), Zhou et al. (2023b), Si et al. (2023), Shi et al. (2023) LLM Prompting Zhang et al. (2023), Yao et al. (2023), Zhou et al. (2023a), Razdaibiedina et al. (2023), Li et al. (2022), Jang et al. (2022), Creswell et al. (2023), Huang et al. (2023) than other groups. Detailed score rubric for paper relevance is described in Figure 8, and we note that there is no instance where paper relevance of any human annotator is less than 3 (i.e., Moderate relevance). Table 8: Human annotators expertise on REVIEWSCORE evaluation. bold indicates the highest number/score across different groups, and an underline indicates the second highest. Group Annotator ID # AI/ML Pub Paper Relevance Avg # Pub Avg Paper Relevance 1 2 3 4 5 anno 11 anno 12 anno anno 21 anno 22 anno 23 anno 31 anno 32 anno 33 anno 41 anno 42 anno 43 anno 51 anno 52 anno 53 Total - Paper Relevance Score Rubric 2 3 4 6 3 7 0 1 3 1 6 13 3 1 - 3.50 4.13 4.38 4.50 4.38 4.25 4.00 4.00 3.88 4.00 3.88 4.25 3.13 4.13 4. - 3.00 5.33 1.33 6.67 3. 3.93 4.00 4.38 3.96 4.04 3. 4.06 5: Direct expertise Works in the exact subtopic and can judge nuanced claims, methods, and datasets. 4: Strong relevance Adjacent/overlapping subtopic with regular use of the papers methods or domain; can evaluate technical choices with minimal ramp-up. 3: Moderate relevance Same broad area (e.g., NLP NLP; CV CV) but different subtopic or methods; will understand contributions but may miss edge-case nuances. 2: Low relevance Only tangential connection (e.g., general ML experience while the paper is domain-specific) and limited familiarity with core methods or domain. 1: No clear relevance Outside the field; would require substantial background reading to assess claims/methodology. Figure 8: Score rubric for evaluating paper relevance of human annotators."
        },
        {
            "title": "Preprint",
            "content": "E.2 INTER-ANNOTATOR AGREEMENT To ensure trustworthiness of human annotation, we report inter-annotator agreement in Krippendorffs Alpha (Hughes, 2021) on REVIEWSCORE evaluation in Table 9. Overall, REVIEWSCORE shows 0.301 Krippendorffs Alpha, indicating moderate inter-annotator agreement. Specifically, ClaimScore shows the highest agreement, QScore follows subsequently, and ArgScore shows the lowest agreement. By comparing inter-group agreements, Group 2 and 4 show significantly higher REVIEWSCORE agreement than other groups. In contrast, Group 1 and 5 include negative inter-annotator agreement, which is critical concern for reliable human annotation. By manual disagreement analysis, we confirm that most disagreements come from human annotators with low paper relevance (i.e., anno 11 and anno 51 in Table 8). To summarize, we observe that the inter-annotator agreement aligns with the annotator expertise  (Table 8)  , and we (unsurprisingly) confirm that human annotations could be more reliable if their research interests become more relevant to the assigned papers. Table 9: Inter-annotator agreement (Krippendorffs Alpha) on REVIEWSCORE evaluation. bold indicates the highest agreement across different groups, and an underline indicates the second highest. Group ClaimScore ArgScore QScore REVIEWSCORE 1 2 3 4 5 Median 0.399 0.328 0.400 0.375 0.203 0. -0.023 0.329 0.146 0.361 -0.023 0.146 0.245 0.350 0.436 0.465 -0.078 0.350 0.125 0.428 0.301 0.385 -0.041 0."
        },
        {
            "title": "F ADDITIONAL RESULTS OF AUTOMATIC REVIEWSCORE EVALUATION",
            "content": "F.1 QUANTITATIVE RESULTS We additionally report human-model agreement on REVIEWSCORE evaluation using different evaluation metrics in Table 1013. Specifically, we report agreement on ClaimScore evaluation in Table 10, agreement on ArgScore evaluation in Table 11, agreement on QScore evaluation in Table 12, and agreement on REVIEWSCORE evaluation in Table 13. For the binary classification setup, we use Precision, Recall, and F1 Score, and for the 5-point scale setup, we use Pearson rank correlation, Gwets AC2 (Gwet, 2001), and Quadratic Weighted Kappa (Warrens, 2012). Table 10: Human-model agreement on ClaimScore evaluation. Model Precision Recall Pearson AC2 Kappa Binary 5-point Scale Proprietary models claude-sonnet-3.7 claude-sonnet-4 gpt-4o gpt-5 gemini-2.5-flash Open-sourced models qwen3-235b-a22b llama-3.3 deepseek-v3 0.091 0.091 0.000 0.083 0.059 0.136 0.091 0.000 0.667 0.200 0.000 0.250 0.200 0.750 0.250 0.000 0.160 0.125 0.000 0.125 0. 0.231 0.133 0.000 0.192 0.172 0.130 -0.014 0.146 0.135 0.144 0.195 0.035 0.048 0.064 -0.057 0.054 0.137 0.130 0.119 -0.004 0.106 0.045 0.122 0. 0.094 0.153 0."
        },
        {
            "title": "Preprint",
            "content": "Table 11: Human-model agreement on ArgScore evaluation. Model Precision Recall F1 Pearson AC Kappa Binary 5-point Scale Proprietary models claude-sonnet-3.7 claude-sonnet-4 gpt-4o gpt-5 gemini-2.5-flash Open-sourced models qwen3-235b-a22b llama-3.3 deepseek-v3 0.345 0.250 0.483 0.353 0. 0.381 0.344 0.267 0.679 0.482 0.250 0.536 0.679 0.429 0.196 0.286 0.458 0.329 0.329 0.426 0.447 0.403 0.250 0.276 0.425 0.258 0.280 0.368 0. 0.281 -0.014 0.238 0.369 0.239 0.221 0.329 0.358 0.012 0.124 0.188 0.380 0.266 0.239 0.334 0.366 0.149 0.107 0.196 Table 12: Human-model agreement on QScore evaluation. Model Precision Recall F1 Pearson AC2 Kappa Binary 5-point Scale Proprietary models claude-sonnet-3.7 claude-sonnet-4 gpt-4o gpt-5 gemini-2.5-flash Open-sourced models qwen3-235b-a22b llama-3.3 deepseek-v3 0.571 0.462 0.447 0.500 0.386 0.394 0.409 0. 0.516 0.563 0.677 0.594 0.759 0.813 0.844 0.821 0.542 0.507 0.538 0.543 0.512 0.531 0.551 0.597 0.394 0.387 0.395 0.320 0.283 0.325 0.375 0. 0.380 0.335 0.298 0.285 0.154 0.117 0.196 0.101 0.387 0.357 0.340 0.300 0.225 0.234 0.284 0.228 Table 13: Human-model agreement on REVIEWSCORE evaluation. Model Precision Recall F1 Pearson AC2 Kappa Binary 5-point Scale Proprietary models claude-sonnet-3.7 claude-sonnet-4 gpt-4o gpt-5 gemini-2.5-flash Open-sourced models qwen3-235b-a22b llama-3.3 deepseek-v3 0.350 0.291 0.417 0.370 0.324 0.351 0.358 0.328 0.448 0.367 0.405 0.441 0. 0.436 0.388 0.382 0.380 0.360 0.389 0.327 0.369 0.336 0.004 0.361 0.339 0.308 0.359 0.300 0.296 0.161 0.322 0.278 0.355 0.333 0.362 0.310 0. 0.243 0.316 0.310 0.622 0.495 0.393 0.543 0.678 0.576 0.424 0."
        },
        {
            "title": "Preprint",
            "content": "F.2 QUALITATIVE RESULTS We present thorough qualitative human-model disagreement analysis on REVIEWSCORE evaluation in Figure 914. For each review point type, we demonstrate two representative examples which include model-based scores and corresponding justifications using claude-sonnet-3.7 and claude-sonnet-4. Input: Claim (Review Point) & Paper Paper: Towards Continual Knowledge Learning of Language Models (Jang et al., 2022) Claim (Review Point): The experimental findings in this paper are somewhat trivial. Human-annotated Label (5-point Scale) 3, 4, 4 (Median: 4) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 2 Human-model Disagreement Type: Value judgment Justification: The proposition states that the experimental findings in this paper are somewhat trivial. To evaluate this claim, need to assess the novelty, significance, and depth of the experimental findings presented in the paper. Key experimental findings in the paper include: . . . These findings are not trivial because: - They establish new problem formulation and benchmark for an important real-world challenge - They provide novel insights about how different methods perform in this setting - They identify key factors affecting knowledge retention and acquisition - The findings have practical implications for developing ever-changing language models Model: claude-sonnet-4 Score: 2 Human-model Disagreement Type: Value judgment Justification: To evaluate whether the experimental findings are somewhat trivial, need to assess the novelty, significance, and non-obviousness of the results presented in the paper. **Novel contributions and non-trivial findings:** . . . **Overall assessment:** While some high-level expectations (like catastrophic forgetting occurring) are predictable, the specific findings about which methods work best, the quantitative trade-offs, the multi-phase degradation effects, and the detailed analysis of what causes forgetting provide substantial non-trivial insights. The systematic benchmarking and the discovery that traditional CL methods dont transfer well to this domain are significant contributions. The experimental work is comprehensive, the findings have practical implications for maintaining upto-date language models, and several results challenge conventional wisdom from traditional continual learning. Figure 9: Example #1 of ClaimScore evaluation."
        },
        {
            "title": "Preprint",
            "content": "Input: Claim (Review Point) & Paper Paper: Temperature Schedules for self-supervised contrastive methods on long-tail data (Kukleva et al., 2023) Claim (Review Point): The improvement is quite limited compared with other long-tailed learning methods. Human-annotated Label (5-point Scale) 2, 2, 3 (Median: 2) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 3 Justification: The proposition states that The improvement is quite limited compared with other longtailed learning methods. To evaluate this claim, need to examine how the paper compares its proposed temperature scheduling (TS) approach with other long-tailed learning methods. Looking at the paper: . . . Given that the paper shows consistent improvements over the baselines and SDCLR, but doesnt compare with broader range of long-tailed learning methods, and the improvements are modest (1-3%), the proposition that the improvement is quite limited compared with other long-tailed learning methods is partially supported but not fully substantiated. Model: claude-sonnet-4 Score: Human-model Disagreement Type: Value judgment Justification: To evaluate this proposition, need to examine the experimental results and compare the improvements achieved by the proposed temperature scheduling (TS) method against other long-tailed learning methods. Looking at the experimental results: 1. **Comparison with SDCLR**: In Table 3, the paper compares SimCLR+TS with SDCLR (a specialized method for long-tailed contrastive learning). The improvements are modest. 2. **Absolute improvements**: The improvements over baseline methods are generally in the range of 14% across different metrics and datasets. 3. **Limited baseline comparisons**: The paper primarily compares against standard contrastive learning methods (MoCo, SimCLR) and only one specialized long-tailed method (SDCLR). It doesnt compare against other established long-tailed learning approaches from supervised learning or other self-supervised long-tailed methods. 4. **Scope of evaluation**: The evaluation is limited to relatively small datasets (CIFAR variants and ImageNet-100) and doesnt demonstrate performance on larger-scale long-tailed datasets that are commonly used in long-tailed learning literature. 5. **Method simplicity vs. improvement**: While the method is simple (just scheduling temperature), the improvements are incremental rather than substantial breakthroughs. The paper acknowledges this limitation implicitly by positioning the method as simple yet effective and emphasizing the zero additional computational cost, rather than claiming state-of-the-art performance improvements. Figure 10: Example #2 of ClaimScore evaluation."
        },
        {
            "title": "Preprint",
            "content": "Input: Premise of Argument (Review Point) & Paper Paper: Automatic Chain of Thought Prompting in Large Language Models (Zhang et al., 2023) Argument (Review Point): Firstly, when mentioning that the method automatically finds prompts using chain of thoughts (Auto CoT) approach, this does not imply finding the best samples from the dataset and using the same method as before. In other words, the proposed method does not find the best chain of thought prompts as claimed but rather believes that lets do this step by step is the best zero-shot-CoT prompting strategy and the method merely uses that with different prompt samples. The title and the abstract are highly misleading in that way. Premise: The method uses lets do this step by step with different prompt samples. Human-annotated Label (5-point Scale) 5, 5, 5 (Median: 5) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 5 Justification: The premise states that the Auto-CoT method uses lets do this step by step with different prompt samples. need to check if this is accurate based on the paper. Looking at the paper, in the DEMONSTRATION SAMPLING section, the author describes how they construct demonstrations: prompted input is formulated as: [Q: q(i)j. A: [P]], where [P] is single prompt Lets think step to step. This formed input is fed into an LLM using Zero-Shot-CoT (Kojima et al., 2022) to output the reasoning chain consisting of the rationale r(i)j and the extracted answer a(i)j. . . . So the premise is accurate - the Auto-CoT method does use lets think step by step (or very similar variant Lets think step to step) with different prompt samples selected through their clustering and sampling approach. Model: claude-sonnet-4 Score: 2 Human-model Disagreement Type: Overthinking Justification: To judge the factuality of this premise, need to examine what the Auto-CoT method actually does according to the paper. From the paper, can see that: 1. Auto-CoT consists of two main stages: (i) question clustering and (ii) demonstration sampling 2. In the demonstration sampling stage, the method uses Zero-Shot-CoT to generate reasoning chains for selected questions 3. The paper explicitly states that Zero-Shot-CoT uses the prompt Lets think step by step (not lets do this step by step) . . . So the premise is mostly accurate in describing the approach (using Zero-Shot-CoT with different samples), but contains factual error in the exact wording of the prompt. The paper consistently uses Lets think step by step rather than lets do this step by step. Figure 11: Example #1 of ArgScore evaluation."
        },
        {
            "title": "Preprint",
            "content": "Input: Premise of Argument (Review Point) & Paper Paper: Automatic Chain of Thought Prompting in Large Language Models (Zhang et al., 2023) Argument (Review Point): Secondly, this is very obvious to anyone that the prompts must be representative of the test samples (as much as possible). This work takes the dataset, clusters them, finds the most representative sample from each cluster, and uses that as prompts. There is nothing extra-ordinary about this approach and is merely small improvement over the randomly sampling strategy. This can also be seen from marginal improvement in the results over random sampling strategy. Premise: The approach is obvious (i.e., prompts must be representative of test samples). Human-annotated Label (5-point Scale) 4, 4, 5 (Median: 4) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 2 Human-model Disagreement Type: Misunderstanding / incorrect reasoning Justification: Ill examine whether the paper itself acknowledges or implies that prompts should be representative of test samples as an obvious approach: . . . The papers main contribution is showing that diversity in demonstrations is important, not just representativeness. The paper states: diversity-based clustering may mitigate misleading by similarity and diversity of demonstration questions is the key. This suggests that the premise oversimplifies the papers findings. The paper doesnt claim that prompts must simply be representative it argues for balance of representativeness and diversity. Model: claude-sonnet-4 Score: 4 Justification: To judge the factuality, need to determine whether its true that prompts must be representative of test samples is an obvious principle. This is general statement about machine learning methodology and prompt engineering principles that would be known to AI practitioners before 2023. The idea that training/demonstration examples should be representative of test data is fundamental principle in machine learning that predates this specific paper. This principle is well-established in the field and doesnt require knowledge of this specific paper to evaluate its truth. The paper does implement this principle through clustering and selecting representative samples, but the general principle itself is established knowledge in the field. Figure 12: Example #2 of ArgScore evaluation."
        },
        {
            "title": "Preprint",
            "content": "Input: Question (Review Point) & Paper Paper: ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2023) Question (Review Point): Is there way to add self-consistency to ReAct, such that it samples multiple reasoning traces and actions and uses the majority action, for example? SC seems extremely effective for Chain of Thought tasks, and it seems like something similar could work here. Human-annotated Label (5-point Scale) 2, 2, 5 (Median: 2) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 4 Human-model Disagreement Type: Minority score prediction Justification: The question asks about adding self-consistency to ReAct, specifically whether its possible to sample multiple reasoning traces and actions and use the majority action, similar to how selfconsistency works for Chain of Thought tasks. Let me check if the paper discusses this concept or something similar: . . . However, the paper does not explicitly discuss applying the self-consistency technique directly to ReAct itself (i.e., sampling multiple ReAct trajectories and taking the majority action). It only discusses combining ReAct with CoT-SC as separate methods, not applying the self-consistency sampling technique to ReActs own trajectories. Model: claude-sonnetScore: 4 Human-model Disagreement Type: Minority score prediction Justification: The question asks about adding self-consistency to ReAct, similar to how its used in Chain of Thought (CoT) tasks. Let me examine what the paper says about self-consistency and whether it discusses or implements this for ReAct. Looking through the paper, can see that: . . . However, the question specifically asks about applying self-consistency directly to ReAct itself sampling multiple ReAct reasoning traces and actions and using majority voting. While the paper discusses CoT-SC and hybrid approaches that combine ReAct with CoT-SC, it doesnt appear to directly implement or discuss ReAct-SC approach where multiple ReAct trajectories are sampled and majority voting is applied to the ReAct outputs themselves. Figure 13: Example #1 of QScore evaluation."
        },
        {
            "title": "Preprint",
            "content": "Input: Question (Review Point) & Paper Paper: Large Language Models Can Self-improve (Huang et al., 2023) Question (Review Point): Can you check if baseline self-consistency with more examples can match LMSI or not? Human-annotated Label (5-point Scale) 4, 5, 5 (Median: 5) Output: ClaimScore Evaluation Model: claude-sonnet-3.7 Score: 2 Human-model Disagreement Type: Misunderstanding / incorrect reasoning Justification: The paper includes hyperparameter study in Figure 4(b) that shows the effect of varying the number of sampled reasoning paths for self-consistency, both with and without LMSI. This directly addresses whether increasing the number of examples in baseline self-consistency can match LMSI performance. Model: claude-sonnet-4 Score: Justification: Looking at the experimental setup and results: - The paper uses m=32 reasoning paths for generating training data - Figure 4(b) shows accuracy vs number of sampled reasoning paths for both w/o LMSI and LMSI - The experiments seem to use consistent numbers of paths for comparison The paper doesnt systematically explore whether dramatically increasing the number of paths in baseline self-consistency (e.g., to 100+ paths) could match the LMSI performance. While Figure 4(b) shows diminishing returns after 1520 paths, it doesnt explore very high numbers of paths that might be computationally equivalent to the LMSI training process. Figure 14: Example #2 of QScore evaluation."
        },
        {
            "title": "G MODEL DETAILS",
            "content": "2025a) including five proprietary models and claude-sonnet-4-20250514 (Anthropic, We measure reliability of automatic REVIEWSCORE evaluation with eight current stateof-the-art LLMs, and three open-sourced models. For proprietary models, we use Anthropics claude-3-7-sonnet-20250219 (Anthropic, 2025b), OpenAIs gpt-4o-2024-08-06 (OpenAI, 2024) and gpt-5-2025-08-07 (OpenAI, 2025), and Googles gemini-2.5-flash (Gemini Team, Google, 2025). Since the REVIEWSCORE evaluation does not require longer chain of thought, we exclude reasoning models. Following this rule, we use Anthropics claude models without thinking modes and OpenAIs gpt-5 model with minimal reasoning effort. For open-sourced models, we use Alibabas qwen3-235b-a22b-2507 (Yang et al., 2025) with 235B model parameters, DeepSeeks deepseek-v3-0324 (DeepSeek-AI et al., 2024) with 671B model parameters and Metas llama-3.3-70b-instruct (AI@Meta, 2024) with 70B model parameters. We call the models by their APIs."
        },
        {
            "title": "H PROMPTS",
            "content": "We list every prompt used for automatic REVIEWSCORE evaluation (Section 4), automatic review point type classification (used in preprocessing stage of the dataset construction in Section 3.3), and automatic argument reconstruction (Section 3.2). Figure 15 Figure 20 indicate prompts for automatic REVIEWSCORE evaluation and related score rubrics. Figure 21 and Figure 22 indicate prompts for automatic review point type classification, where the type is one of claim, argument, and question. Figure 23 Figure 29 indicate prompts for automatic argument reconstruction. In the prompts, capital letters with double square brackets (i.e., [[XYZ]]) would be replaced by the corresponding material, and words with curly bracket (i.e., {abc}) would be replaced by the corresponding score rubric (Figure 15 Figure 17) or python code snippet in Figure 26. Unanswerability Score Rubric 1: Definitely Answerable by the paper 2: Probably Answerable by the paper 3: No Verdict 4: Probably Unanswerable by the paper 5: Definitely Unanswerable by the paper Figure 15: Score rubric for evaluating unswerability. Factuality Score Rubric 1: Definitely False 2: Probably False 3: No Verdict 4: Probably True 5: Definitely True Figure 16: Score rubric for evaluating factuality. Untrivialness Score Rubric 0: Definitely Trivial 1: Probably Trivial 2: Definitely Not Trivial Figure 17: Score rubric for evaluating untrivialness."
        },
        {
            "title": "Preprint",
            "content": "Prompt for evaluating QScore (unanswerability of questions) # Paper [[PAPER]] # Question [[QUESTION]] You are given paper submitted to an AI conference and question regarding the paper. Judge if the question is answerable or not by the paper. You should scale 1-5 to indicate unanswerability as follows. {Unanswerability Score Rubric} If you score the question 1 or 2, then indicate which knowledge source you have grounded to (i.e., indicate corresponding section(s) and verbatim sentence(s)) and answer the question. Your output should be the following. ### Reasoning [think step-by-step] ### Unanswerability [1 or 2 or 3 or 4 or 5] ### Source [corresponding section(s) and verbatim sentence(s) if the score is 1 or 2, otherwise None] ### Answer [answer the question if the score is 1 or 2, otherwise None] Figure 18: Prompt used for evaluating QScore in 5-point scale. Prompt for evaluating ClaimScore or WScore (factuality of weaknesses) # Paper [[PAPER]] # Weakness [[WEAKNESS]] You are given paper submitted to an AI conference and weakness regarding the paper. Judge if the weakness is true or not based on the paper. You should scale 1-5 to indicate factuality as follows. {Factuality Score Rubric} Your output should be the following. ### Reasoning [think step-by-step] ### Factuality [1 or 2 or 3 or 4 or 5] Figure 19: Prompt used for evaluating ClaimScore or WScore in 5-point scale."
        },
        {
            "title": "Preprint",
            "content": "Prompt for evaluating ArgScore (factuality of premises) # Paper [[PAPER]] # Weakness [[WEAKNESS]] # Premise [[PREMISE]] You are given paper submitted to an AI conference, weakness of the paper, and one of premises of the weakness. Your task is to judge the factuality and untrivialness of the given premise. First, judge the factuality of the premise. To do that, choose an appropriate knowledge source from: 1. given paper 2. annotator knowledge before the year [[YEAR]] (more precisely, before the paper is publicized) 3. other paper(s), and then judge the factuality of the premise based on the knowledge source. You should scale 1-5 to indicate factuality as follows. {Factuality Score Rubric} Here are guidelines you should follow: - Main purpose is to distinguish *given paper* and *annotator knowledge*. - Select *other papers* only if the premise refers specific paper. - Note that you should separate judging the factuality of the premise from understanding the semantics of the premise. It does not matter whether *given paper* is needed or not to understand the semantics of the premise. The knowledge source is *given paper* only if *given paper* is needed to judge the factuality of the premise, otherwise, the knowledge source is *annotator knowledge*. For example, although you need the papers context in order to understand what the premise means, if you do not need the papers knowledge to judge if the premise is true or not (e.g., logical assessment), then you should choose *annotator knowledge* as knowledge source and judge the factuality accordingly. - For premises that are conditionals (If then B), you should presume that the antecedent (A) is always true even if the antecedent does not align with the papers knowledge. (Because the antecedent is always true, the knowledge source should only be determined while judging the factuality of the conseqeunt.) Then, choose an appropriate knowledge source to judge if the consequent (B) is true or not and judge the factuality accordingly. Next, decide whether the premise is trivially true or not based on the common knowledge of CS/AImajoring undergrad students before the year [[YEAR]] (more precisely, before the paper is publicized). For premises that are conditionals (If then B), you must assume that the antecedent (A) is true and judge if the consequent (B) is trivially true or not. You should scale the score to 0-2 as follows. {Untrivialness Score Rubric} Here are guidelines you should follow: - If the knowledge source is *given paper* or *other papers*, then untrivialness should always be 2 unless the premise factuality could also be determined by *annotator knowledge*. - If the knowledge source is *annotator knowledge*, then untrivialness could be 0-2. Your output should be formatted as below. ### Reasoning [think step-by-step] ### Source [given paper or annotator knowledge or other papers] ### Factuality [1 or 2 or 3 or 4 or 5] ### Untrivialness [0 or 1 or 2] Figure 20: Prompt used for evaluating ArgScore in 5-point scale."
        },
        {
            "title": "Preprint",
            "content": "Prompt for classifying review point types (is argument) # Review Point [[REVIEW POINT]] You are given an AI conference review point. Is this an argument or not? Your response should follow the format below. ### Reasoning [think step-by-step] ### Response [Yes or No] Figure 21: Prompt used for deciding if review point is an argument or not (is argument). Prompt for classifying review point types (is question) # Review Point [[REVIEW POINT]] You are given an AI conference review point. Decide if this is question or simple claim. Your response should follow the format below. ### Reasoning [think step-by-step] ### Response [Question or Claim] Figure 22: Prompt used for deciding if review point is question or claim (is question). Prompt for Argument Reconstruction (extract verbatim conclusion reason) ### Review Point [[REVIEW POINT]] Given an AI conference review point, consider it as an argument, and then returns its verbatim conjecture in the source text and verbatim reason statements of that conjecture in the source text. The output format should be as following. ### Conjecture [main conjecture in the review point] ### Supporting Reasons [list of supporting reasons for the conjecture] Figure 23: Prompt used for extracting verbatim conjecture and reason statements in an argument (extract verbatim conclusion reason)."
        },
        {
            "title": "Preprint",
            "content": "Prompt for Argument Reconstruction (argument reconstruction) # Paper [[PAPER]] # Review Point ## Conclusion [[CONCLUSION]] ## Explicit reasons [[REASONS]] You are given paper submitted to an AI conference and review point by peer reviewer. review point consists of conclusion and its explicit reasons. Reconstruct an argument (i.e., review) with premise-conclusion structure where premises deductively imply the conclusion. The reconstructed argument should be deductively valid, using formal logical patterns like modus ponens (e.g., Premise1: A, Premise2: If then B, Conclusion: B). Add implicit premises and intermediate conclusions if needed. Your output should composed of two parts, argument reconstruction and its formalization. In the first part, list premises, intermediate conclusions, and conclusion, and indicate their logical connection (i.e., which propositions deductively implies which). In the second part, first define variables and/or predicates, then formalize premises, intermediate conclusions, and conclusion, and then generate deductive proof. The output format should be as following. # Argument Reconstruction ## Premises [list of explicit and implicit premises] ## Intermediate Conclusions [list of intermediate conclusions (if intermediate conclusions are not needed, then write None.)] ## Conclusion [a conclusion] ## Logical Connections [list of logical connections] # Formalized Argument ## Defined Variables/Predicates [definition of each variable and/or predicate] ## Formalized Premises [formalization of premises using definition] ## Formalized Intermediate Conclusions [formalization of intermediate conclusions using definition (if intermediate conclusions are not needed, then write None.)] ## Formalized Conclusion [formalization of conclusion using definition] ## Deductive Proof [deductive proof using formalized premises] Figure 24: Prompt used for reconstructing an argument (argument reconstruction)."
        },
        {
            "title": "Preprint",
            "content": "Prompt for Argument Reconstruction (streamlining) ## Defined Variables/Predicates [[DEFINITION]] ## Formalized Premises [[PREMISES]] ## Formalized Conclusion [[CONCLUSION]] ## Deductive Proof [[PROOF]] First, determine necessary formalized premises for the given deductive proof. This includes: 1. Add any missing formalized premises that are necessary to prove conclusion but cannot be dervied from the formalized premises. 2. Remove any unnecessary formalized premises that are not necessary to prove conclusion but present in the formalized premises. You should format these premises into python dictionary where keys and values are python strings. Second, write python program using z3 that inputs the necessary formalized premises and formalized conclusion and outputs: 1. Their validity, formatted as python string of either valid or invalid. 2. smallest subset of necessary formalized premises to prove the formalized conclusion, formatted as python list of keys of the python dictionary of the necessary formalized premises. You should therefore print two things (a python string and python list) separately. Please use the below python code snippet. {Code snippet for checking validity} Third, return the final formalized conclusion that is used in the python program in step 2. Lastly, judge whether the formal proof using the necessary formalized premises (in step 1) and the final formalized conclusion (in step 3) is circular or not. If there is single necessary formalized premise that is the same as the final formalized conclusion, then return N/A. Your response should be as following. ### Necessary Formalized Premises python { [Symbol of premise #1]: [Formalization of premise #1], [Symbol of premise #2]: [Formalization of premise #2], ... } ### Python Program python [a python program] ### Final Formalized Conclusion [Formalized conclusion in the python program] ### Proof Circularity [Yes or No or N/A] Figure 25: Prompt used for streamlining formalized reconstruction (streamlining)."
        },
        {
            "title": "Preprint",
            "content": "Code snippet for checking validity python from z3 import * import itertools ################################# ### Write down your code here ### ################################# # Check validity of the argument def check validity(premises dict, conclusion): = Solver() s.add(list(premises dict.values())) s.add(Not(conclusion)) if s.check() == unsat: return valid else: return invalid # Find minimal set of premises def find minimal premises(premises dict, conclusion): for subset size in range(1, len(premises dict) + 1): for subset in itertools.combinations(premises dict.keys(), subset size): subset premises = [premises dict[key] for key in subset] if check validity(subset premises, conclusion) == valid: return list(subset) return list(premises dict.keys()) validity = check validity(premises, conclusion) print(validity) minimal premises = find minimal premises(premises, conclusion) print(minimal premises) Figure 26: Python code snippet for evaluating validity of reconstruction. Prompt for Argument Reconstruction (program debugging) When execute the python program, got the following error: [[ERROR]] Fix the error and generate revised python program. Your response should be as following. ### Reasoning [explain why and how to fix the program] ### Revised Python Program python [a python program] Figure 27: Prompt used for debugging python programs that evaluate validity of reconstruction (program debugging)."
        },
        {
            "title": "Preprint",
            "content": "Prompt for Argument Reconstruction (deformalization) ## Defined Variables/Predicates [[DEFINITION]] ## Formalized Premises [[PREMISES]] ## Formalized Conclusion [[CONCLUSION]] Given definitions of variables and/or predicates, generate natural language (NL) descriptions of formalized premises and conclusion. Your response should be as following. ### NL Premises [list of premises in natural language] ### NL Conclusion [conclusion in natural language] Figure 28: Prompt used for translating FOL formulas with keys (i.e., defined variables/predicates) to NL reconstructed arguments (deformalization). Prompt for Argument Reconstruction (check faithfulness) # Argument [[ARGUMENT]] # Argument Reconstruction ## Premises [[PREMISES]] ## Conclusion [[CONCLUSION]] For an argument, its reconstruction as premise-conclusion structure is given. Your task is to judge whether the construction is faithful or not. You should judge the faithfulness according to the following two criteria: - **Accuracy & Charity.** The reconstruction should keep the authors intended meaning while eliminating irrelevanciesi.e., obey the principle of charity and prefer the strongest sensible reading of ambiguous passages. - **Completeness.** All explicit premises, the main conclusion and any indispensable implicit premises must be included. The output format should be as following. # Reasoning [Explain step-by-step] # Faithfulness [Yes or No] Figure 29: Prompt used for evaluating faithfulness of reconstruction (check faithfulness)."
        }
    ],
    "affiliations": [
        "KAIST, AITRICS",
        "KIT",
        "MIT"
    ]
}