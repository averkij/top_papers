{
    "paper_title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "authors": [
        "Xiangbo Gao",
        "Renjie Li",
        "Xinghao Chen",
        "Yuheng Wu",
        "Suofei Feng",
        "Qing Yin",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 7 7 2 8 0 . 2 0 6 2 : r PISCO: Precise Video Instance Insertion with Sparse Control Xiangbo Gao1, Renjie Li1, Xinghao Chen1, Yuheng Wu3, Suofei Feng4, Qing Yin2, Zhengzhong Tu1 1Texas A&M University 2Visko Platform 3KAIST 4Stanford University Abstract. The landscape of AI video generation is undergoing pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, the core requirement is the ability to perform precise, targeted modifications. cornerstone of this transition is video instance insertion, which requires inserting specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction (e.g., shadows and reflections), and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify single keyframe, start-andend keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project Homepage: https://xiangbogaobarry.github.io/PISCO/ Date: February 10, 2026 Contact: Xiangbo Gao (xiangbogaobarry@gmail.com), Zhengzhong Tu (tzz@tamu.edu)"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in large-scale video generative models [1] are able to generate videos with high visual fidelity and realistic motion. As these models continue to evolve, the focus of video generation is shifting beyond producing visually plausible content toward highly controllable video generation and editing [2], with the long-term goal of enabling Hollywood-grade AI-assisted filmmaking [3, 4]. In this new paradigm, the priority is no longer to cherry-pick barely acceptable output from numerous generations, but to reliably achieve precise user intent with minimal iteration. particularly demanding yet under-explored capability in this context is Precise Video Instance Insertion: the ability to insert specific object into an existing video at user-specified spatial location and temporal position, while preserving the identity and dynamics of the original footage. In professional visual effects and post-production workflows, instance insertion is not merely about adding visually plausible object, but about achieving precise and reliable control with minimal iteration. In this setting, precision entails several tightly coupled requirements: ➊ Instance-level controllability, enabling users to explicitly specify when and where an object appears; ➋ Physically plausible temporal propagation, where the inserted object automatically evolves over time with coherent pose and motion, following physically reasonable dynamics; Figure 1 PISCO enables precise video instance insertion with arbitrary sparse keyframe control. Given clean input video and few user-provided instance cutouts at selected timestamps, PISCO inserts the instance with coherent temporal propagation and physical effects while preserving the original background dynamics. ➌ Physically consistent scene adaptation, in which the surrounding background is appropriately adjusted to account for insertion-induced effects such as shadows, reflections, illumination, water ripples, and others; ➍ Faithful preservation of background scene dynamics, ensuring that pre-existing motions, identities, and temporal patterns in the original video remain unchanged and temporally consistent after insertion; ➎ Low-effort user interaction, where achieving the above goals does not require dense per-frame annotations or manual editing across the entire video. Despite the strong generative capacity of diffusion-based video models [5], these requirements remain largely unmet. Existing solutions can be broadly categorized into several classes, each addressing only subset of the above requirements. Video inpainting methods [6, 7] enforce spatial consistency through dense per-frame masks, but require exhaustive annotation of object shape and trajectory, making them impractical under sparse user control (➎). Moreover, their copy-and-fill nature limits the ability to adjust the surrounding scene in physically consistent manner, such as modeling realistic shadows or illumination changes (➌). Reference-guided video-to-video editing approaches [8, 9] propagate appearance information over time, yet lack fine-grained spatial and temporal controllability, making it difficult to precisely specify object placement and timing (➊). Agentic pipelines that combine image editing with subsequent image-to-video generation [10, 11] allow flexible object manipulation at the image level, but discard most temporal information from the original video, often leading to background drift and disrupted scene dynamics (➍). Geometry-heavy reconstructionbased approaches [12, 13] explicitly model scene structure and spatial relationships, but incur substantial computational overhead, rely on fragile geometric cues, and struggle to robustly model complex or previously unseen object motions (➋). Together, these limitations prevent existing methods from providing practical and flexible solution for precise video instance insertion under sparse user control. To overcome these limitations, we propose PISCO (Precise Instance insertion with Sparse COntrol), video diffusion framework designed for professional-grade instance insertion with minimal user effort. PISCO allows users to specify sparse instance conditionssuch as single keyframe, start-and-end keyframes, or arbitrary keyframes at any timestampsand performs video instance insertion by propagating appearance, motion, and interaction in scene-consistent manner. Our framework builds upon the Wan video diffusion backbone and augments it with multi-channel context adapter that ingests instance RGB, mask, depth, and an explicit 2 availability signal indicating when instance guidance is provided, enabling flexible sparse keyframe control within unified model. Sparse control introduces additional challenges, as naïvely masking missing frames can cause severe distribution shift when processed by pretrained temporal video VAEs, resulting in flickering, miscoloring, and incomplete instance rendering. PISCO addresses these challenges through set of dedicated mechanisms, including Variable-Information Guidance (VIG) for modulating conditioning strength during training to enable robust guidance, Distribution-Preserving Temporal Masking (DPTM) to stabilize sparse conditioning under temporal VAEs, and geometry-aware conditioning to maintain physically plausible interactions and scene consistency. To evaluate precise video instance insertion under sparse control, we construct PISCO-Bench, curated benchmark derived from BURST [14] with carefully verified instance annotations and paired clean background videos generated using side-effect-aware instance removal model [15]. We evaluate performance using both reference-based metrics (FVD, LPIPS, PSNR, SSIM) and reference-free perceptual metrics (VBench [16]). Experimental results show that PISCO consistently outperforms strong inpainting and video editing baselines under sparse-control settings, and further demonstrates clear and monotonic improvements as additional sparse control frames are provided, validating its scalability with respect to control signal density. Our contributions are summarized as follows: We introduce PISCO, the first video diffusion framework that enables precise video instance insertion under arbitrary sparse keyframe control, allowing users to insert instances at any desired timestamps with minimal annotation effort. We propose set of dedicated mechanisms for sparse instance control, including Variable-Information Guidance and Distribution-Preserving Temporal Masking, which jointly address temporal propagation and distribution shift in pretrained video diffusion models. We construct PISCO-Bench and demonstrate through extensive reference-based and reference-free experiments that PISCO significantly outperforms state-of-the-art video instance insertion baselines, exhibiting consistent performance improvements as additional control signals are provided."
        },
        {
            "title": "2.1 Video Inpainting for Video Instance Insertion",
            "content": "Video inpainting [6] refers to masking out certain part of the video and leveraging models to recover the masked region. It serves as core technique for object removal, insertion, replacement, video restoration, and outpainting. Early methods employed 3D Convolutional Neural Networks (3D-CNNs) to learn spatio-temporal coherence [17, 18, 19]. To provide stronger motion priors, subsequent approaches incorporated optical flow guidance [20, 21, 22, 23, 24, 25]. With the advent of Transformers, attention mechanisms were adopted to better capture long-range spatio-temporal correlations [26, 27, 28, 29]. Recently, with the development of diffusion models [5] and video generative foundation models (e.g., Wan [1], Flux [30]), video diffusion models have been extensively applied to video inpainting [31, 32, 33, 7, 34, 35, 36, 37, 38]. significant advantage of these models is that training data can be obtained by simply masking out target regions in original videos, relieving the reliance on high-quality paired data which is difficult to acquire. However, despite their generation quality, applying standard inpainting to instance insertion faces critical bottleneck: it necessitates dense segmentation annotations (masks) for every frame during inference. Obtaining per-frame masks for dynamic object that does not yet exist in the video is labor-intensive and impractical for general users. In contrast, our proposed method requires only sparse instance mask signals to control the video instance insertion process."
        },
        {
            "title": "2.2 Reference-Guided Video-to-Video Editing",
            "content": "Reference-guided video editing focuses on conditional video generation, using references such as instance images or text prompts to guide the synthesis process. Large-scale frameworks like UniVideo [8] and VACE [9] integrate multimodal controls to unify generation and editing tasks. Similarly, methods such as VideoDirector [39] and ContextFlow [40] utilize text-to-video models to modify scene content via prompt engineering. Specific 3 to object manipulation, VideoAnydoor [41] and Wan-Animate [42] focus on high-fidelity object insertion or replacement with motion control, while Animate-a-Story [43] retrieves assets for storytelling. The primary limitation of these methods is the inability to achieve fine-grained spatial and temporal control. Most current approaches function as general video-to-video translation, which modifies the global style or replaces existing objects where the motion trajectory is pre-defined. They struggle to satisfy precise user intents, such as inserting specific instance at an exact position, with an exact posture, starting at an exact timestamp. Our work specifically addresses this need, enabling precise instance-level manipulation while maintaining the integrity of the original video."
        },
        {
            "title": "2.3 Image-to-Video (I2V) Generation and Propagation",
            "content": "A distinct paradigm involves generate-then-animate workflow: editing single keyframe using mature image editing tools and then animating it via Image-to-Video (I2V) models. Recent advancements in image editing, such as Step1x-edit [44], Z-Image [45], and Qwen-Image [46], allow for precise object insertion in static images. To extend this to video, I2V models [10, 47, 48, 11, 49] and transition generation methods [50, 51] predict future frames based on the edited image. Specialized architectures like MotionStone [52], MotionPro [53], and Through-The-Mask [54] attempt to decouple motion from appearance to improve controllability. However, relying solely on I2V models for insertion introduces fundamental issue: background hallucination. I2V models are trained to generate motion for the entire frame and often fail to strictly adhere to the unedited background pixels of the original video. This leads to severe temporal drift and inconsistency between the edited foreground and the static background. While Generative Video Propagation [55] attempts to mitigate this, ensuring seamless blending without altering the non-object regions remains significant challenge for pure I2V approaches. Our approach overcomes this by leveraging the original video context to strictly preserve background consistency. 2.4 4D Reconstruction and Manipulation To address challenges related to occlusion and geometric consistency, recent works have proposed lifting 2D video into explicit 3D or 4D representations. Methods like Place Anything into Any Video [12] and Anything in Any Scene [56] utilize depth and flow to physically place objects. Most notably, InsertAnywhere [13] bridges 4D scene geometry with diffusion models to handle complex occlusions during insertion. Despite their theoretical correctness, these geometry-heavy approaches suffer from inherent flexibility limitations compared to 2D generative models. First, they are constrained by data scarcity, as high-quality 4D-labeled data is rare, leading to poor generalization across diverse real-world scenes. Second, they struggle with dynamic object insertion; inserting moving character into 4D scene typically requires pre-existing dynamic object, whereas 2D diffusion models can infer realistic motion from single or few images. Our method bypasses these heavy reconstruction requirements, offering flexible, data-efficient solution that supports dynamic object insertion with natural interaction."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "t=1 contains foreground instance and = {Vt}T Given paired videos { ˆV , } of length , where ˆV = { ˆVt}T t=1 depicts the same scene without that instance, our goal is to synthesize an edited video in which the instance is naturally inserted into with spatially accurate placement, temporally coherent motion, and geometrically plausible occlusion relationships. , We represent the instance-side conditions as an RGB instance clip = {It}T t=1 t=1 and the instance depth DI = {DI,t}T . Here, each It contains only the segmented foreground instance on t=1 constant zero-valued background, and Mt indicates the corresponding foreground region. In addition, we compute background depth map DV from to provide geometric cues for depth ordering and occlusion reasoning. Details of how these signals are constructed are described in Sec. 4.1. , its spatial mask = {Mt}T 4 Figure 2 Overview of PISCO pipeline. We train conditional video diffusion model with sparse keyframe control via Variable-Information Guidance (VIG), and stabilize sparse conditioning under pretrained temporal VAEs using Distribution-Preserving Temporal Masking (DPTM): pixel-space nearest-frame interpolation followed by token-level masking, with spatial mask and availability signals injected alongside RGB/depth conditions. Variable-density user guidance via availability mask. User guidance can range from single keyframe to dense , where At {0, 1} per-frame supervision. We model this variability with an availability mask = {At}T indicates whether instance-side information is available at time t. During training, we sample pγ(A), where the density ratio γ [0, 1] controls the expected fraction of available frames as well as their temporal placement. The mask is applied exclusively to instance-specific signals: t=1 = I, = M, DA = DI , (1) while the background depth DV remains unmasked. This design reflects practical scenarios: while user typically specifies instance-level details sparsely, the background scene geometry is often fully observable. Conditional video diffusion with context adapter. Let z0 denote the latent representation of the target video obtained using pretrained temporal video VAE, and let zt be the noisy latent at diffusion step t. We train denoising network ϵθ to predict the added noise conditioned on the background video and the masked instance-side signals, ending up with the training objective = Ez0,ϵ,t, Apγ (cid:2)(cid:13) (cid:13)ϵ ϵθ (cid:0)zt, t, V, DV , A, DA , A, A(cid:1)(cid:13) (cid:13)2 (cid:3) . (2) In our implementation, ϵθ uses the Wan video diffusion backbone [1] with VACE context adapter [9] for injecting multi-channel video conditions. To accommodate our condition format, we modify the first linear layer of the VACE adapter to match the new input dimensionality, while reusing pretrained parameters for the remaining layers. Training details are provided in Sec. 4.2."
        },
        {
            "title": "3.2 Variable-Information Guidance via Dynamic Contextual Dropout",
            "content": "Variable-Information Guidance (VIG). Precise video instance insertion requires flexibility regarding user input, which can range from single keyframe to dense per-frame annotations. To ensure robustness across varying levels of control, we introduce Variable-Information Guidance (VIG), dynamic contextual dropout strategy that samples an availability mask during training. This approach exposes the model to diverse supervision regimes, encouraging it to propagate instance information under sparse guidance while maintaining appearance fidelity and spatial alignment under dense guidance. 5 Figure 3 Visualization of the depth-aware insertion. Conditioning on depth improves depth ordering and occlusion handling, reducing foreground/background blending artifacts compared to depth-agnostic variant. We employ hybrid sampling strategy that covers the spectrum of annotation densities. This includes an extreme sparsity regime where only single frame is retained, varying levels of sparse and dense sampling to simulate different user efforts, and an anchor regime that provides full-frame supervision. This strategy enables smooth trade-off between temporal propagation and fidelity: under sparse conditions, the model learns to infer plausible motion by leveraging background context, while dense regimes prevent identity drift and preserve detailed appearance."
        },
        {
            "title": "3.3 Distribution-Preserving Temporal Masking (DPTM)",
            "content": "Modern video generative models [1, 57, 58] often rely on pretrained temporal video VAEs [59, 60, 61, 62] that compress the temporal dimension by factor Ct. If we naïvely mask missing frames using in pixel space, the temporal VAE receives out-of-distribution inputs, which leads to flickering, miscoloring, and incomplete instance rendering as illustrated in Figure 4. To address this, we propose Distribution-Preserving Temporal Masking (DPTM) to decouple distribution preservation from information masking, while staying fully consistent with the sampled availability mask A. Pixel-space temporal completion. As shown in Figure 2, given masked instance clip A, we first perform nearest interpolation in pixel space. Concretely, missing frames are filled by propagating the temporally nearest available instance frame forward and backward. This produces temporally complete clip with natural video statistics, which helps keep the input distribution compatible with pretrained temporal VAEs. Token-space masking. We then encode the interpolated clip into video tokens. Tokens corresponding to the originally unobserved frames are masked out in the latent space. This explicit masking enables the model to distinguish valid signals from interpolated fillers, thereby accelerating convergence. Availability channel aligned to token resolution. To distinguish observed frames from interpolated ones, we introduce an availability channel. Let = /Ct be the compressed token length and (H , ) be the spatial resolution. We reshape the binary frame-level availability mask {0, 1}T HW into: RCtT W . This transformation moves the local temporal window Ct into the channel dimension, preserving finegrained observability patterns within each compressed token. The resulting tensor is concatenated with other conditions and processed by the VACE context adapter. Combined with pixel-space completion and token-space masking, this strategy enables precise sparse conditioning and minimizes temporal artifacts, as evidenced in Figure 4. Figure 4 Effectiveness of DPTM under sparse guidance. We compare results given segmented instance inputs only on odd frames. Naïve masking leads to distribution shifts and temporal artifacts, whereas DPTM preserves encoder input statistics and significantly improves temporal stability."
        },
        {
            "title": "3.4 Geometry- and Appearance-Robust Training",
            "content": "We further incorporate three complementary strategies to improve geometric plausibility and appearance robustness: depth-aware conditioning, occlusion-aware completion augmentation, and relighting augmentation. Depth-aware insertion. Purely 2D video editing lacks explicit geometric reasoning, making scale, depth ordering, and occlusion ambiguous. We therefore condition the model on two depth signals: the background depth DV computed from , and the instance depth DI extracted from ˆV using . These mask and masked by availability as DA depth cues are provided alongside RGB, spatial mask, and availability signals, enabling the model to reason about relative depth ordering and produce physically plausible layer compositing. As shown in Figure 3, Depth conditioning helps the model respect depth ordering and occlusion relations during insertion. instance augmentation. During inferAmodal ence, users typically provide spatially complete (amodal [63, 64]) instances, expecting the model to resolve occlusions based on scene geometry. However, training instances extracted directly from segmentation are often fragmented by scene occlusions. To align training with this inference requirement, we introduce an amodal augmentation strategy. Specifically, we reconstruct missing regions to generate complete pseudo-amodal instance as the input condition, while supervising the model with the original occluded video. This pairing forces the model to learn the compositing logic required to correctly occlude complete input instance according to depth cues. Details of the completion construction are provided in Sec. 4.1. Some examples of amodal completion results are shown in Figure 5. Figure 5 Visualization of amodal instance completion and relighting augmentations. We complete occluded instance cutouts to form pseudo-amodal inputs and apply moderate relighting to improve illumination compatibility with the target scene while preserving keyframe appearance controllability. Figure 6 Effectiveness of relighting augmentation. Without relighting, the model over-preserves the input instance appearance, causing illumination mismatch after insertion. Relighting augmentation encourages lighting adaptation, yielding more coherent blending with the surrounding scene. In video instance insertion, strictly preserving the provided instance Instance relighting augmentation. appearance can lead to incompatible lighting between the instance and the background scene. To improve automatic lighting adaptation, we augment training with relighted instances generated by IC-Light [65]. For each instance clip, we synthesize relighted version under randomly sampled background lighting conditions. The relighted instance is then used as additional training data for instance conditioning. Figure 6 shows the qualitative results of the relighting augmentation."
        },
        {
            "title": "4.1 Data Preparation",
            "content": "We construct training tuples of the form ( ˆV , V, I, M, DV , DI ), where ˆV is the target video containing the instance, is the paired background video, (I, ) are the segmented instance clip and its mask, and (DV , DI ) are depth maps computed for background and instance signals. 7 Data sources. We use real-world videos from ROSE [24], VPData [34], MOSE [66, 67], and DAVIS [68, 69]. We filter the segmentation results to favor clips with clean and logically removable instances and discard cases with ambiguous or non-detachable masks. After filtering, we obtain 16,642 video clips with corresponding foreground segmentation masks, each with at least 49 frames. Background video generation. To construct paired videos { ˆV , }, we train side-effect-aware video instance removal model on ROSE [15]. Given target video ˆV and an instance mask , we extract the masked instance from ˆV and apply the instance removal model to obtain the paired background video . This procedure produces training pairs that share the same scene dynamics while differing in the presence of the target instance, which matches the insertion objective. Depth estimation. We compute DV by running Depth Anything V3 [70] on the background video . We compute DI by running it on the target video ˆV and then cropping the instance region using . Amodal Completion Data Construction. To support amodal instance augmentation, we develop completion model that reconstructs the full amodal appearance = F(I) from an occluded modal segment I. We construct paired training data by curating collection of fully visible instances from our dataset to serve as amodal ground truth. Corresponding modal inputs are synthesized by randomly overlaying additional instance cutouts onto these samples, creating mapping from occluded views to complete appearances. We employ pre-trained image editing framework as the base generator, fine-tuning it via LoRA to specialize in amodal recovery. This model is applied frame-wise to generate pseudo-amodal instance clips. Finally, the amodal masks are obtained through thresholding, while the amodal depth DI is synthesized using interpolation from available depth values."
        },
        {
            "title": "4.2 Training Schedule",
            "content": "Training Hyperparameters. Our models are built upon two pretrained backbones: Wan2.1-VACE-1.3B and Wan2.2-VACE-Fun-A14B, denoted as PISCO-1.3B and PISCO-14B, respectively. To ensure stable adaptation to our multi-condition setting while preserving pretrained generative priors, we adopt staged training strategy with progressively increased model flexibility. All stages prior to Stage are conducted at spatial resolution of 832 480 and temporal length of 49 frames to stabilize optimization and reduce computational cost. Stage I: Adapter input warm-up. We replace the input projection of the VACE context adapter to support multi-channel conditioning and train only this newly introduced layer while freezing all other modules. This warm-up stage stabilizes condition injection before finetuning deeper components. Stage II: Adapter finetuning. We finetune the full VACE context adapter while keeping the diffusion backbone frozen. This allows the adapter to better align pretrained representations with task-specific conditioning without disturbing the backbone priors. Stage III: Joint finetuning. We jointly finetune the context adapter and the diffusion backbone. This step improves the coupling between conditioning signals and generation dynamics. Stage IV: Augmented training. We introduce occlusion-aware completion and relighting augmentations and continue joint training using parameter-efficient LoRA finetuning. This enhances robustness under challenging visibility and appearance variations. Stage V: Resolution and temporal extension. We further extend the model from generating 49-frame videos at 832 480 resolution to 120-frame videos at 1280 720 resolution. All experiments are trained using AdamW on NVIDIA H100 GPUs. Wan2.2 adopts dual-denoiser architecture with separate high-noise and low-noise denoising stages; therefore, for PISCO-14B, the two denoisers are trained independently using the same schedule and hyperparameters."
        },
        {
            "title": "5.1 Evaluation Data.\nWe introduce PISCO-Bench, a curated evaluation benchmark constructed on top of the BURST dataset [14].\nPISCO-Bench is designed to assess instance-level video editing under diverse real-world conditions, including\nurban driving scenes, scripted movie clips, and in-the-wild internet videos.",
            "content": "8 Starting from BURST, we carefully select 100 videos with broad scenario coverage and no overlap with our training data. For each video, we manually inspect the instance segmentation annotations and correct missing or low-quality masks to ensure reliable conditioning inputs. To obtain clean background videos, we remove the target instances using ROSE [15], side-effect-aware instance removal model. Throughout this section, in the context of video instance insertion, we refer to these processed background videos as clean videos and the videos with instances as target videos."
        },
        {
            "title": "5.3 Baselines.",
            "content": "Our goal is precise video instance insertion under sparse user control, where the user provides only small number of segmented instance frames to specify the appearance and placement of the instance. Since there are no directly comparable open-sourced models that support the same sparse conditioning interface, we compare against representative approaches that can achieve similar functionality via different pipelines. Specifically, we consider three categories: (1) Agentic pipeline: image editing + image-to-video (I2V) generation. This pipeline first edits the first frame (and optionally the last frame) using the reference instance and mask, and then generates the full video conditioned on the edited frame(s) with an I2V model. This is commonly adopted recipe in recent editing data generation workflows [71, 72]. In our experiments, we use Nano-banana-Pro [73] for image editing, taking the unedited frame(s) together with the segmented reference instance image and mask as input. We then use Wan2.2-Fun-A14B-InP [1] for I2V generation under both first-frame and first & last-frame control. (2) Video inpainting models. Video inpainting methods typically require dense masks over all frames, along with text prompt describing the inserted object and the target scene. In this experiment, we use Qwen3-VL-32B-Instruct to generate detailed textual description of the segmented reference instance image as the prompt. We adopt CoCoCo [7] and VideoPainter [34] as strong inpainting baselines. Both take the text prompt and the masked video as input, and this setting is also widely used in video editing pipelines [71, 72]. Table 1 Input conditions for all compared methods. We indicate whether each approach uses the clean video, reference instance frames (first/last), and instance masks (sparse or dense), as well as an optional text prompt. Notably, several baselines require dense per-frame masks, whereas PISCO operates with sparse keyframe masks (first-only or first&last)."
        },
        {
            "title": "Video First Last First Last All Text",
            "content": "Image Editing + I2V Generation"
        },
        {
            "title": "Method",
            "content": "First First&Last (3) Reference-guided video-to-video editing models. Another alternative is generic video-to-video editing, which edits the entire video in an end-to-end manner using multiple conditions such as refIn this erence image, text prompt, and masks. category, we evaluate VACE [9] and UniVideo [8]. VACE takes the unedited video, segmented reference image, text prompt, and the full mask video as input. UniVideo does not support mask conditioning; it uses only the unedited video, segmented reference image, and text prompt."
        },
        {
            "title": "CoCoCo\nVideoPainter",
            "content": "Video-to-Video Editing VACE (14B) UniVideo First First&Last PISCO 1.3B (Ours) PISCO 14B (Ours) Table 1 summarizes the input conditions for all methods. we note that several baselines requires dense per-frame masks (the full mask video), whereas our models work with sparse masks (first frame or first & last frame). This comparison reflects the design philosophy of PISCO: minimizing user effort while maintaining controllability and output quality. First First&Last 9 Table 2 Quantitative Results (FVD, LPIPS, PSNR, SSIM). Comparisons on Whole Video and Foreground Region. Blue and red indicate best and second-best among standard settings. Gray rows indicate the Five-Frame reference setting (not included in ranking)."
        },
        {
            "title": "Method",
            "content": "First Only First + Last"
        },
        {
            "title": "CoCoCo\nVideoPainter",
            "content": "VACE14B UniVideo First Only First + Last Five Frames First Only First + Last Five Frames"
        },
        {
            "title": "Foreground",
            "content": "FVD LPIPS PSNR SSIM FVD LPIPS PSNR SSIM 826 624 590 524 371 485 398 269 337 204 136 Image Editing + I2V Generation 15.47 16.44 0.55 0."
        },
        {
            "title": "Video Inpainting",
            "content": "23.62 23.11 0.80 0.78 Video-to-Video Editing 25.55 19.22 0.88 0.61 PISCO 1.3B (Ours) 25.35 27.01 28.53 0.87 0.88 0.89 PISCO 14B (Ours) 24.81 26.58 28.01 0.88 0.89 0.90 297 398 384 273 310 243 171 104 222 138 75 0.451 0.392 0.191 0. 0.103 0.211 0.121 0.103 0.089 0.116 0.097 0.084 0.030 0.030 0.031 0.035 0.028 0. 0.029 0.024 0.018 0.029 0.022 0.015 30.24 30.38 30.26 29.27 30.55 29.21 30.51 32.99 35. 30.61 33.58 35.94 0.97 0.98 0.97 0.97 0.98 0.97 0.98 0.98 0.98 0.97 0.98 0. Consider that different pipelines support different video resolutions and frame length, we keep the experimental comparison with 832 pixel width, 480 pixel height, and 49 frames."
        },
        {
            "title": "5.4 Quantitative Experiments.",
            "content": "Reference-based Video Quality Assessment. For the Precise Video Instance Insertion task, we use the PISCO-Bench datasets ground-truth pairs (V, ˆV ) to conduct reference-based evaluation. The generated videos are assessed using four standard metrics: FVD [74], LPIPS [75], PSNR [76], and SSIM [77]. To ensure comprehensive analysis, we evaluate at two levels: (1) Whole-video assessment calculates metrics between and ˆV to measure global consistency and the accuracy of physical effects like shadows or reflections. (2) Foreground assessment isolates the inserted instance using mask , computing Metric(V M, ˆV ). This directly verifies whether the generated object adheres to the reference conditions and intended trajectory. Quantitative results in Table 2 show that PISCO consistently outperforms all baselines. In whole-video metrics, the Image Editing + I2V pipeline performs worst due to severe background hallucination, as reflected by its low PSNR and SSIM. While Video Inpainting models like CoCoCo and VideoPainter preserve unmasked regions better, their high FVD scores indicate poor temporal coherence. VACE emerges as the strongest baseline, yet PISCO-14B (First & Last) surpasses it significantly, reducing FVD from 371 to 204 and LPIPS from 0.103 to 0.097. In foreground assessment, PISCO-14B (First & Last) achieves Foreground FVD of 138 and LPIPS of 0.022, both substantially better than competing methods. These results demonstrate that our framework generates instances with high visual quality and strict temporal alignment. Moreover, the superior performance of the First & Last configuration over First Only confirms that endpoint constraints effectively stabilize complex instance dynamics. We also evaluated scalability using Five Frames setting, incorporating the first, last, and three random intermediate frames. Unlike existing baselines that lack the flexibility for arbitrary multi-frame conditioning, PISCO leverages these additional signals to further refine quality. For example, PISCO-14B with five frames improves whole-video FVD to 136 and Foreground LPIPS to 0.015. This performance boost underscores our frameworks unique ability to effectively scale controllability with additional sparse inputs. Reference-free Video Quality Assessment. To evaluate perceptual quality and temporal consistency without pixel-aligned ground truth, we employ VBench [16]. However, standard VBench consistency metrics compute feature similarity over entire frames, mixing foreground and background signals. This is suboptimal for 10 Table 3 VBench Quantitative Comparison. We compare PISCO against recent baselines. To ensure precise evaluation for instance insertion, Background and Subject Consistency are computed using masked regions to isolate foreground/background signals. Blue and red indicate the best and second-best results among standard settings. Gray rows indicate the Five-Frame setting. Method Background Consistency Subject Consistency Aesthetic Quality Imaging Quality Motion Smoothness Overall Consistency Temporal Flickering Temporal Style Average First Only First + Last 91.43 92.28 CoCoCo VideoPainter 94.63 94.51 VACE14B UniVideo First Only First + Last Five Frames First Only First + Last Five Frames 94.21 94.04 93.72 94.07 94. 93.84 94.20 94.42 83.86 85.09 89.55 89.14 90.29 89.88 87.16 91.33 91.45 87.26 91.57 91. Image Editing + I2V Generation 48.83 49.47 59.01 59.71 97.89 98.18 Video Inpainting 47.81 47. 55.76 57.68 98.67 98.97 Video-to-Video Editing 48.69 49.41 60.95 60.84 98.90 98. PISCO 1.3B (Ours) 60.67 61.18 62.09 98.85 98.86 98.86 PISCO 14B (Ours) 60.80 62.00 62.87 98.79 98.79 98. 47.70 49.48 50.01 48.24 50.08 51.45 15.11 15.58 14.61 13.85 14.87 15.51 14.92 15.58 15. 15.14 15.64 15.82 95.89 96.26 15.11 15.58 63.39 64.02 97.69 97.77 14.61 13. 64.17 64.20 97.56 97.01 97.54 97.51 97.67 97.26 97.21 97.34 14.87 15.92 65.04 65. 14.92 15.58 15.61 15.14 15.64 15.57 64.43 65.45 65.89 64.56 65.64 66.04 instance insertion, where object consistency and background preservation should be assessed independently. Therefore, we utilize instance masks to isolate foreground and background regions, extracting CLIP/DINO features solely from these areas to compute consistency scores. Other metrics, such as Aesthetic Quality and Motion Smoothness, follow the official implementation. The results in Table 3 show that PISCO demonstrates superior performance across most perceptual dimensions. Notably, in Subject Consistency, PISCO-14B (First & Last) achieves score of 91.57, significantly outperforming VACE (90.29) and inpainting models (89). This indicates that our model maintains the identity and appearance of the inserted instance more faithfully over time. PISCO also secures top spots in Imaging Quality and Aesthetic Quality, suffering less from artifacts compared to baseline pipelines. While inpainting baselines naturally achieve high Background Consistency by copying pixels, PISCO remains highly competitive (94.20) while offering superior motion and instance fidelity. Following the reference-based evaluation, we also report PISCOs performance using the Five Frames setting. Adding three intermediate control frames further boosts metrics, with PISCO-14B reaching 91.98 in Subject Consistency and 51.45 in Aesthetic Quality. This consistent upward trend across both reference-based and reference-free benchmarks reinforces our conclusion: PISCO is uniquely capable of leveraging additional sparse visual prompts to refine generation quality, flexibility not supported by current state-of-the-art baselines."
        },
        {
            "title": "5.5 Qualitative Experiments",
            "content": "We qualitatively compare generated frames in Figure 7 to assess different methods. The results highlight several baseline limitations: agentic pipelines (combining image editing with I2V) often regenerate the entire sequence, severely degrading background consistency. Inpainting models like CoCoCo and VideoPainter, lacking reference instance inputs, frequently hallucinate blurry or incorrect objects. Meanwhile, reference-based V2V models such as UniVideo and VACE struggle with long-term spatial control; they often fail to maintain correct perspective (e.g., the oversized car in UniVideo) or lose the target instance entirely in later frames. In contrast, our PISCO approach demonstrates superior controllability. While first-frame control alone effectively captures appearance, the trajectory may slightly drift over time. However, as shown in the PISCO First & Last results, providing sparse control frames easily resolves this ambiguity, ensuring precise alignment with the target trajectory throughout the sequence. 11 Figure 7 Qualitative Comparison. Visual results of instance insertion in two dynamic scenes (A) and (B). Existing methods struggle with background preservation (Agentic), instance identity (Inpainting), or scale consistency (UniVideo/VACE). Our method, PISCO, achieves superior visual fidelity and spatiotemporal alignment, particularly when utilizing sparse \"First & Last\" frame control. 12 Figure 8 Broader applications beyond instance insertion. Leveraging the same instance-level conditioning and temporal propagation, PISCO supports diverse instance-centric video edits and simulation under controllable spatiotemporal guidance: Background Change, Instance Repositioning, Speed Change, Instance Rescaling, and Dynamics Simulation."
        },
        {
            "title": "6 Beyond Instance Insertion: Broader Applications of PISCO",
            "content": "While PISCO is designed for precise video instance insertion under sparse keyframe control, the same instancelevel conditioning and temporal propagation machinery naturally generalizes to broader set of controllable video editing and simulation tasks. Figure 8 illustrates representative use cases enabled by PISCO. In particular, PISCO can (i) perform background change by re-rendering the surrounding scene while preserving the foreground instance identity and motion; (ii) support instance repositioning and instance rescaling by adjusting the instance location and size while maintaining scene-consistent interactions (e.g., occlusions and shadows); (iii) realize speed change by temporally subsampling instance-related conditions to induce faster or slower motion; and (iv) enable dynamics simulation by providing partial instance-related conditions to create counterfactual trajectories for stress-testing downstream perception and planning systems. These extensions highlight PISCO as general-purpose, instance-centric video editing framework rather than single-task solution. We encourage future researchers and developers to further investigate these directions and extend PISCO to additional instance-centric editing and simulation scenarios."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented PISCO, video diffusion model for precise video instance insertion under sparse user control. By introducing Variable-Information Guidance and Distribution-Preserving Temporal Masking, together with geometry-aware conditioning, PISCO effectively resolves the distribution shift and temporal instability issues that arise when applying sparse conditions to pretrained video diffusion models. Extensive experiments on the proposed PISCO-Bench demonstrate that PISCO consistently outperforms strong inpainting, video editing, and agentic baselines, while exhibiting clear and monotonic performance gains as additional sparse control frames are provided. These results position PISCO as practical and scalable solution for professional-grade video editing, and key step toward highly controllable, low-effort, AI-assisted filmmaking."
        },
        {
            "title": "References",
            "content": "[1] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang et al., Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [2] A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang et al., Movie gen: cast of media foundation models, arXiv preprint arXiv:2410.13720, 2024. [3] L. Manovich, The rise of generative media, Manovich.net, 2023, from the collection \"Artificial Aesthetics\". [Online]. Available: http://manovich.net/content/04-projects/166-visual-generative-media/visual_generative_media.pdf [4] J. Moore, Why 2023 was ai videos breakout year, and what to expect in 2024, jan 2024, accessed: 2024-05-20. [Online]. Available: https://a16z.com/why-2023-was-ai-videos-breakout-year-and-what-to-expect-in-2024/ [5] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang, Diffusion models: comprehensive survey of methods and applications, ACM computing surveys, vol. 56, no. 4, pp. 139, 2023. [6] W. Quan, J. Chen, Y. Liu, D.-M. Yan, and P. Wonka, Deep learning-based image and video inpainting: survey, International Journal of Computer Vision, vol. 132, no. 7, pp. 23672400, 2024. [7] B. Zi, S. Zhao, X. Qi, J. Wang, Y. Shi, Q. Chen, B. Liang, R. Xiao, K.-F. Wong, and L. Zhang, Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 10, 2025, pp. 11 06711 076. [8] C. Wei, Q. Liu, Z. Ye, Q. Wang, X. Wang, P. Wan, K. Gai, and W. Chen, Univideo: Unified understanding, generation, and editing for videos, arXiv preprint arXiv:2510.08377, 2025. [9] Z. Jiang, Z. Han, C. Mao, J. Zhang, Y. Pan, and Y. Liu, Vace: All-in-one video creation and editing, arXiv preprint arXiv:2503.07598, 2025. [10] Y. Hu, C. Luo, and Z. Chen, Make it move: controllable image-to-video generation with text descriptions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 21918 228. [11] Y. Jin, Z. Sun, N. Li, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, Pyramidal flow matching for efficient video generative modeling, arXiv preprint arXiv:2410.05954, 2024. [12] Z. Liu, J. Yang, M. Gao, and F. Zheng, Place anything into any video, arXiv preprint arXiv:2402.14316, 2024. [13] H. Jin, H. Jang, J. Kim, J. Hyung, K. Kim, D. Kim, H. Choi, H. Kim, and J. Choo, Insertanywhere: Bridging 4d scene geometry and diffusion models for realistic video object insertion, arXiv preprint arXiv:2512.17504, 2025. [14] A. Athar, J. Luiten, P. Voigtlaender, T. Khurana, A. Dave, B. Leibe, and D. Ramanan, Burst: benchmark for unifying object recognition, segmentation and tracking in video, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2023, pp. 16741683. [15] C. Miao, Y. Feng, J. Zeng, Z. Gao, H. Liu, Y. Yan, D. Qi, X. Chen, B. Wang, and H. Zhao, Rose: Remove objects with side effects in videos, arXiv preprint arXiv:2508.18633, 2025. [16] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., Vbench: Comprehensive benchmark suite for video generative models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21 80721 818. [17] C. Wang, H. Huang, X. Han, and J. Wang, Video inpainting by jointly learning temporal structure and spatial details, in Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 52325239. [18] Y.-L. Chang, Z. Y. Liu, K.-Y. Lee, and W. Hsu, Free-form video inpainting with 3d gated convolution and temporal patchgan, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 90669075. [19] Y.-T. Hu, H. Wang, N. Ballas, K. Grauman, and A. G. Schwing, Proposal-based video completion, in European Conference on Computer Vision. Springer, 2020, pp. 3854. [20] R. Xu, X. Li, B. Zhou, and C. C. Loy, Deep flow-guided video inpainting, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 37233732. [21] D. Kim, S. Woo, J.-Y. Lee, and I. S. Kweon, Deep video inpainting, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 57925801. [22] A. Li, S. Zhao, X. Ma, M. Gong, J. Qi, R. Zhang, D. Tao, and R. Kotagiri, Short-term and long-term context aggregation network for video inpainting, in European Conference on Computer Vision. Springer, 2020, pp. 728743. [23] C. Gao, A. Saraf, J.-B. Huang, and J. Kopf, Flow-edge guided video completion, in European Conference on Computer Vision. Springer, 2020, pp. 713729. [24] X. Zou, L. Yang, D. Liu, and Y. J. Lee, Progressive temporal feature alignment network for video inpainting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 16 44816 457. [25] Z. Li, C.-Z. Lu, J. Qin, C.-L. Guo, and M.-M. Cheng, Towards an end-to-end framework for flow-guided video inpainting, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 17 56217 571. [26] R. Liu, H. Deng, Y. Huang, X. Shi, L. Lu, W. Sun, X. Wang, J. Dai, and H. Li, Fuseformer: Fusing fine-grained information in transformers for video inpainting, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 14 04014 049. [27] , Decoupled spatial-temporal transformer for video inpainting, arXiv preprint arXiv:2104.06637, 2021. [28] J. Cai, C. Li, X. Tao, C. Yuan, and Y.-W. Tai, Devit: Deformed vision transformers in video inpainting, in Proceedings of the 30th ACM international conference on multimedia, 2022, pp. 779789. [29] S. Zhou, C. Li, K. C. Chan, and C. C. Loy, Propainter: Improving propagation and transformer for video inpainting, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 10 47710 486. [30] B. F. Labs, S. Batifol, A. Blattmann, F. Boesel, S. Consul, C. Diagne, T. Dockhorn, J. English, Z. English, P. Esser et al., Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space, arXiv preprint arXiv:2506.15742, 2025. [31] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai, Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, arXiv preprint arXiv:2307.04725, 2023. [32] Z. Zhang, B. Wu, X. Wang, Y. Luo, L. Zhang, Y. Zhao, P. Vajda, D. Metaxas, and L. Yu, Avid: Any-length video inpainting with diffusion model, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 71627172. [33] D. Green, W. Harvey, S. Naderiparizi, M. Niedoba, Y. Liu, X. Liang, J. Lavington, K. Zhang, V. Lioutas, S. Dabiri et al., Semantically consistent video inpainting with conditional diffusion models, arXiv preprint arXiv:2405.00251, 2024. [34] Y. Bian, Z. Zhang, X. Ju, M. Cao, L. Xie, Y. Shan, and Q. Xu, Videopainter: Any-length video inpainting and editing with plug-and-play context control, in Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, 2025, pp. 112. [35] X. Li, H. Xue, P. Ren, and L. Bo, Diffueraser: diffusion model for video inpainting, arXiv preprint arXiv:2501.10018, 2025. [36] M. Lee, S. Cho, C. Shin, J. Lee, S. Yang, and S. Lee, Video diffusion models are strong video inpainter, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 4, 2025, pp. 45264533. [37] Z. Wan, C. Qi, Z. Liu, T. Gui, and Y. Ma, Unipaint: Unified space-time video inpainting via mixture-of-experts, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025, pp. 18611871. [38] Y. Guo, C. Yang, A. Rao, C. Meng, O. Bar-Tal, S. Ding, M. Agrawala, D. Lin, and B. Dai, Keyframe-guided creative video inpainting, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 13 00913 020. [39] Y. Wang, L. Wang, Z. Ma, Q. Hu, K. Xu, and Y. Guo, Videodirector: Precise video editing via text-to-video models, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 25892598. [40] Y. Chen, X. He, X. Ma, and Y. Ma, Contextflow: Training-free video object editing via adaptive context enrichment, arXiv preprint arXiv:2509.17818, 2025. [41] Y. Tu, H. Luo, X. Chen, S. Ji, X. Bai, and H. Zhao, Videoanydoor: High-fidelity video object insertion with precise motion control, 2025. [42] G. Cheng, X. Gao, L. Hu, S. Hu, M. Huang, C. Ji, J. Li, D. Meng, J. Qi, P. Qiao et al., Wan-animate: Unified character animation and replacement with holistic replication, arXiv preprint arXiv:2509.14055, 2025. [43] Y. He, M. Xia, H. Chen, X. Cun, Y. Gong, J. Xing, Y. Zhang, X. Wang, C. Weng, Y. Shan et al., Animate-a-story: Storytelling with retrieval-augmented video generation, arXiv preprint arXiv:2307.06940, 2023. [44] S. Liu, Y. Han, P. Xing, F. Yin, R. Wang, W. Cheng, J. Liao, Y. Wang, H. Fu, C. Han et al., Step1x-edit: practical framework for general image editing, arXiv preprint arXiv:2504.17761, 2025. [45] H. Cai, S. Cao, R. Du, P. Gao, S. Hoi, S. Huang, Z. Hou, D. Jiang, X. Jin, L. Li et al., Z-image: An efficient image generation foundation model with single-stream diffusion transformer, arXiv preprint arXiv:2511.22699, 2025. [46] C. Wu, J. Li, J. Zhou, J. Lin, K. Gao, K. Yan, S.-m. Yin, S. Bai, X. Xu, Y. Chen et al., Qwen-image technical report, arXiv preprint arXiv:2508.02324, 2025. 15 [47] K. Namekata, S. Bahmani, Z. Wu, Y. Kant, I. Gilitschenski, and D. B. Lindell, Sg-i2v: Self-guided trajectory control in image-to-video generation, arXiv preprint arXiv:2411.04989, 2024. [48] X. Wang, B. Zhou, B. Curless, I. Kemelmacher-Shlizerman, A. Holynski, and S. M. Seitz, Generative inbetweening: Adapting image-to-video models for keyframe interpolation, arXiv preprint arXiv:2408.15239, 2024. [49] Z. Li, H. Luo, X. Shuai, and H. Ding, Anyi2v: Animating any conditional image with motion control, arXiv preprint arXiv:2507.02857, 2025. [50] Z. Yang, J. Zhang, Y. Yu, S. Lu, and S. Bai, Versatile transition generation with image-to-video diffusion, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025, pp. 16 98116 990. [51] W. Wang and Y. Yang, Tip-i2v: million-scale real text and image prompt dataset for image-to-video generation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025, pp. 14 89814 908. [52] S. Shi, B. Gong, X. Chen, D. Zheng, S. Tan, Z. Yang, Y. Li, J. He, K. Zheng, J. Chen et al., Motionstone: Decoupled motion intensity modulation with diffusion transformer for image-to-video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 22 86422 874. [53] Z. Zhang, F. Long, Z. Qiu, Y. Pan, W. Liu, T. Yao, and T. Mei, Motionpro: precise motion controller for image-to-video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 27 95727 967. [54] G. Yariv, Y. Kirstain, A. Zohar, S. Sheynin, Y. Taigman, Y. Adi, S. Benaim, and A. Polyak, Through-the-mask: Mask-based motion trajectories for image-to-video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 18 19818 208. [55] S. Liu, T. Wang, J.-H. Wang, Q. Liu, Z. Zhang, J.-Y. Lee, Y. Li, B. Yu, Z. Lin, S. Y. Kim et al., Generative video propagation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17 71217 722. [56] C. Bai, Z. Shao, G. Zhang, D. Liang, J. Yang, Z. Zhang, Y. Guo, C. Zhong, Y. Qiu, Z. Wang et al., Anything in any scene: Photorealistic video object insertion, arXiv preprint arXiv:2401.17509, 2024. [57] S. Chen, C. Ge, Y. Zhang, Y. Zhang, F. Zhu, H. Yang, H. Hao, H. Wu, Z. Lai, Y. Hu et al., Goku: Flow based video generative foundation models, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 23 51623 527. [58] J. Liu, J. Han, B. Yan, H. Wu, F. Zhu, X. Wang, Y. Jiang, B. Peng, and Z. Yuan, Infinitystar: Unified spacetime autoregressive modeling for visual generation, arXiv preprint arXiv:2511.04675, 2025. [59] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [60] J. Wang, Y. Jiang, Z. Yuan, B. Peng, Z. Wu, and Y.-G. Jiang, Omnitokenizer: joint image-video tokenizer for visual generation, Advances in Neural Information Processing Systems, vol. 37, pp. 28 28128 295, 2024. [61] Z. Zheng, X. Peng, T. Yang, C. Shen, S. Li, H. Liu, Y. Zhou, T. Li, and Y. You, Open-sora: Democratizing efficient video production for all, arXiv preprint arXiv:2412.20404, 2024. [62] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., Cosmos world foundation model platform for physical ai, arXiv preprint arXiv:2501.03575, 2025. [63] J. Ao, Q. Ke, and K. A. Ehinger, Image amodal completion: survey, Computer Vision and Image Understanding, vol. 229, p. 103661, 2023. [64] J. Ao, Y. Jiang, Q. Ke, and K. A. Ehinger, Open-world amodal appearance completion, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 64906499. [65] L. Zhang, A. Rao, and M. Agrawala, Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport, in The Thirteenth International Conference on Learning Representations, 2025. [66] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai, MOSE: new dataset for video object segmentation in complex scenes, in ICCV, 2023. [67] H. Ding, K. Ying, C. Liu, S. He, X. Jiang, Y.-G. Jiang, P. H. Torr, and S. Bai, MOSEv2: more challenging dataset for video object segmentation in complex scenes, arXiv preprint arXiv:2508.05630, 2025. [68] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and L. Van Gool, The 2019 davis challenge on vos: Unsupervised multi-object segmentation, arXiv, 2019. [69] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung, and L. Van Gool, The 2017 davis challenge on video object segmentation, arXiv:1704.00675, 2017. 16 [70] H. Lin, S. Chen, J. Liew, D. Y. Chen, Z. Li, G. Shi, J. Feng, and B. Kang, Depth anything 3: Recovering the visual space from any views, arXiv preprint arXiv:2511.10647, 2025. [71] H. He, J. Wang, J. Zhang, Z. Xue, X. Bu, Q. Yang, S. Wen, and L. Xie, Openve-3m: large-scale high-quality dataset for instruction-guided video editing, arXiv preprint arXiv:2512.07826, 2025. [72] Y. Chen, J. Zhang, T. Hu, Y. Zeng, Z. Xue, Q. He, C. Wang, Y. Liu, X. Hu, and S. Yan, Ivebench: Modern benchmark suite for instruction-guided video editing assessment, arXiv preprint arXiv:2510.11647, 2025. [73] Google Cloud, Blog, Nov. nano-banana-pro-available-for-enterprise Nano banana pro: [Online]. Available: 2025. State-of-the-art image generation for enterprise, Google Cloud https://cloud.google.com/blog/products/ai-machine-learning/ [74] I. Skorokhodov, S. Tulyakov, and M. Elhoseiny, Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 36263636. [75] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586595. [76] B. Jähne, Digital image processing. Springer, 2005. [77] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE transactions on image processing, vol. 13, no. 4, pp. 600612, 2004."
        }
    ],
    "affiliations": [
        "KAIST",
        "Stanford University",
        "Texas A&M University",
        "Visko Platform"
    ]
}