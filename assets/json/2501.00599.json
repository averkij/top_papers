{
    "paper_title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
    "authors": [
        "Yuqian Yuan",
        "Hang Zhang",
        "Wentong Li",
        "Zesen Cheng",
        "Boqiang Zhang",
        "Long Li",
        "Xin Li",
        "Deli Zhao",
        "Wenqiao Zhang",
        "Yueting Zhuang",
        "Jianke Zhu",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 9 9 5 0 0 . 1 0 5 2 : r VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM Yuqian Yuan1,2*, Hang Zhang2, Wentong Li1, Zesen Cheng2, Boqiang Zhang2, Long Li1,2*, Xin Li2, Deli Zhao2, Wenqiao Zhang1, Yueting Zhuang1, Jianke Zhu1, Lidong Bing2 1Zhejiang University 2DAMO Academy, Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce multi-agent data engine to meticulously curate largescale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities. 1. Introduction Multi-modal Large Language Models (MLLMs) [2, 8, 19 22, 27, 35] have demonstrated remarkable general-purpose capabilities for open-world image understanding through language-based dialogues over the past year. In constant, *Work is done during internship at DAMO Academy, Alibaba Group Corresponding author extending their capabilities to the video domain presents unique challenges, as videos comprise dynamic sequences that not only showcase visual content but also convey the timing and relationships among various events and objects. Currently, existing Video Large Language Models (Video LLMs) [9, 15, 17, 26, 50, 54] primarily focus on holistic scene understanding. Unfortunately, these approaches often fall short in capturing the nuanced elements of video content. For instance, they often struggle to focus on userspecific objects, such as accurately describing particular object. Fig. 1-(a) illustrates typical example from general VideoLLaMA2 [9]. The ability to discern such finer details in video content is crucial for applications that require precise object description, detailed event analysis, and predictive reasoning in dynamic environments. To achieve fine-grained object understanding, numerous efforts have been devoted to image-based MLLMs, such as GPT4RoI [52], Ferret [44, 51] and Osprey [46]. These methods typically utilize region encoder to obtain objectlevel embeddings, adapting them to LLMs for static image region-text alignment. In contrast, research on video-based object understanding remains limited. Some works [41, 45] directly convert the bounding box coordinates of object from specific frames into textual prompts to assist the LLM in identifying referred objects within the video. However, these methods are plagued by impractical object referring and suffer from imprecise regional understanding. Alternatively, Artemis [32] employs an external RoI tracker to capture an object across the video and extract box-level features for aligning with the LLM. However, as illustrated in Fig. 1- (b), it primarily focuses on single-object referencing using coarse box-level representations, which restricts its capacity to handle complex tasks, such as analyzing relationships among multiple objects and performing intricate reasoning. Therefore, developing an interactive Video LLM that facilitates comprehensive understanding of objects within video represents nontrivial research challenge. 1 Figure 1. Comparisons with previous general and specialized MLLMs. Our VideoRefer excels in multiple fine-grained regional and temporal video understanding tasks, including basic video object referring, complex video relationship analysis, and video object retrieval. In this work, we revisit the design of the Video LLM for finer-level video understanding. We contend that achieving this necessitates three essential components: largescale dataset containing high-quality object-level video instruction data, an architecture that integrates object embeddings with temporal cues, and thorough benchmark for performance assessment. To this end, we introduce VideoRefer Suite, designed to empower Video LLMs with spatialtemporal object comprehension. Dataset. Firstly, to achieve regional alignment between video content and language embeddings, we meticulously curate large-scale region-text video instruction dataset named VideoRefer-700K. Specifically, we present multi-agent data engine to create high-quality video-based mask-text description pairs. This data engine leverages several expert models that excel in various tasks, collaborating meticulously to produce diverse range of object-level instruction data for each object across the video. Our curated VideoRefer-700K comprises descriptions and multi-round QA pairs covering basic questions, complex reasoning and future predictions. Model. Next, we introduce an effective Video LLM, named VideoRefer, that enables fine-grained perceiving, reasoning and retrieval for user-defined regions at any specified timestamps. To accommodate both single-frame and multi-frame region inputs, we propose versatile spatialtemporal object encoder. Specially, Spatial Token Extractor is developed to generate accurate object-level encoding at any frame, leveraging unified pixel-level mask representation to allow arbitrary free-form input regions. We then propose an adaptive Temporal Token Merge Module, which captures temporal contextual information across Benchmark. multiple frames while producing flexible, enriched regional representations. The image-level and object-level embeddings are interleaved with language instructions to form the input sequence for the LLM, facilitating detailed object understanding of the input video. Furthermore, to evaluate the regional video understanding capabilities of Video LLM comprehensively, we develop benchmark named VideoRefer-Bench, which consists of two subVideoRefer-BenchD, which focuses benchmarks: on description generation from four and VideoRefer-BenchQ, which emphasizes multiplechoice question answering across five aspects. VideoReferBench thoroughly assesses the models performance across various timestamps and objects, evaluating the abilities in comprehensive captioning and reasoning, complex multi-object relationships, and future predictions. aspects, As illustrated in Fig. 1, our VideoRefer unlocks range of advanced finer-level video understanding capabilities, including basic video object referring, intricate relationship analysis among objects and object retrieval tasks, maintaining user interactivity. In particular, VideoRefer can be seamlessly integrated with the off-the-shelf SAM 2 [34] to further enhance user interactivity by enabling comprehensive understanding of everything user click on. Extensive experiments conducted on VideoRefer-Bench and general video understanding benchmarks, yield compelling results and demonstrate the efficacy of our approach. Notably, VideoRefer not only significantly surpasses the state-of-theart methods in regional video understanding across temporal, sequential and relationship reasoning, but also advances the general video understanding abilities. 2 2. Preliminary 2.1. Background and Video-referring Task. To attain precise regional comprehension, MLLMs can be incorporated with instance-level visual representations. This integration allows models to generate semantic understandings that focus on specific regions. As for image-based MLLMs, recent researchs [46, 11, 13, 33, 39, 42, 44, 46, 47, 49, 51, 52, 55] has demonstrated significant trend to enable the image referring with spatial visual prompts. In contrast, research focused on video-based regional understanding across sequential scenes is relatively limited. The video referring task involves comprehending userspecific regions at designated moments or time periods within video [32, 41, 45]. The basic video referring task focuses on captioning, while more complex tasks involve reasoning about the relationships between objects, and inferring their future states or interactions. Video referring tasks can significantly enhance the functionality and applicability of video analysis for Video LLM across multiple domains, such as navigation, surveillance, and interactive robotics. 2.2. Task Formulation. For basic video object referring, the model processes questions phrased as What is <object> doing in this video?, where the <object> is specified by the user at specific moment or over duration of time. In more complex scenarios involving various object relationships, the model requires multiple user-defined regions, such as <object1>, <object2> and <objectK> along with the corresponding questions, like How do <object1> and <object2> interact with each other?. To address these nuanced regional and temporal tasks, we provide unified problem formulation. For given video RN HC, where , , H, denote the frame number, height, width and channels, respectively. We define all the <object> as R, where = {R1, R2, . . . , Rn}. Here, represents the total number of objects specified by the user. Rj is expressed as Rj = {rij }, with rij representing region within single frame, and being set containing one or multiple timestamps. For Video LLM, the model optimization process aims to maximize the log-likelihood of generating text conditioned on , R, and text-based prompt across the entire training dataset to produce the desired output: (cid:88) = (V,R,x,y) log (y V, R1, ..., Rn, x), (1) where denotes the ground truth label. 3. VideoRefer Suite Our VideoRefer Suite consists of three crucial components: comprehensive dataset, VideoRefer-700K, containFigure 2. multi-agent data engine for the construction of our VideoRefer-700K. ing high-quality instruction-following object-level annotations; Video LLM, VideoRefer, capable of pixel-level regional and temporal comprehension; and an evaluation benchmark, VideoRefer-Bench, developed to evaluate models across various video referring tasks. 3.1. VideoRefer-700K Dataset 3.1.1 Multi-agent Data Engine We develop an automatic multi-agent data engine to create VideoRefer-700K, large-scale and high-quality object-level video instruction-following dataset. Specially, we utilize off-the-shelf expert models that excel in tasks such as captioning, detection, segmentation and summation as collaborative agents to carefully create diverse types of object-level instruction data. As illustrated in Fig. 2, our curation pipeline involves five components: (i) Analyzer for noun extraction; (ii) Annotator for object-level caption generation; (iii) Segmentor for mask generation; (iv) Reviewer for correspondence verification; and (v) Refiner for summarization&refinement. This multi-agent data engine effectively eliminates noisy or irrelevant contexts, ensuring that the data maintains its accuracy and relevance. Analyzer for Noun Extraction. Considering that most available video datasets contain the short scene-level caption, we begin by analyzing the raw captions to accurately capture the nouns within the sentences, i.e., objects occurred in the video scene. To achieve this, we employ an Analyzer to extract nouns, encompassing both subjects and other relevant nouns. The Qwen2-Instruct-7B model [43] serves as our analytical tool in this process. Annotator for Object-level Caption Generation. To obtain detailed descriptions of the extracted nouns, we employ general video understanding model as an annotator. We prompt the model to provide comprehensive descriptions focused specifically on the objects, rather than the holistic narrative of the whole video. To enhance accuracy and detail, we query the model twice: emphasizing dynamic actions&movements, and highlighting static appearances&states, respectively. Specifically, we filter out static actions related to the subjects to maintain variability and 3 dynamism in the videos. The open-source InternVL2-26B model [8] serves as our annotator. Segmentor for Mask Generation. To acquire pixellevel masks as object-level region representations for each extracted noun, we first select random frame from the video and extract the bounding box using GroundingDINO [23] through open-set grounding, with the extracted noun serving as the input text prompt. Subsequently, HQSAM [14] is employed to generate the high-quality mask based on the corresponding box prompt. To accommodate multi-frame input, we further generate masks for each video frame using SAM 2 [34]. Reviewer for Correspondence Verification. To address potential errors and mismatches in this data construction pipeline, we introduce Reviewer to verify the correspondence between masks and descriptions. Initially, we employ Osprey [46] to provide region-level description for specific frame. The Reviewer then assesses whether the descriptions from Osprey and the Annotator refer to the same object. After this filtering process, we retain only 40% of samples to ensure accuracy. Qwen2-Instruct-7B model [43] is chosen as the Reviewer for this task, due to its efficiency and suitability for handling the complexity of this process. Refiner for Summarization&Refinement. Finally, we utilize reliable Refiner, GPT-4o [28], to summarize and refine the temporal and appearance-related captions generated by the annotator. This process aims to further eliminate repetition and hallucinations, ensuring coherent and accurate final object-level instruction-following dataset. 3.1.2 Data Characteristics By leveraging our multi-agent data engine, we meticulously create three primary types of object-level video instruction data: detailed captions, short captions, and multi-round question-answer (QA) pairs. Object-level Detailed Caption. We utilize subset of large-scale Panda-70M [7], which has short caption for each video. We generate 125K high-quality object-level detailed captions through our full multi-agent data engine. Object-level Short Caption. To generate short captions, primarily for aligning object-level encoder with the LLM for pre-training, we employ portion of the pipeline, which only includes the Analyzer and Segmentor. Specifically, in the Analyzer, we extract only singular subject nouns, enabling the reusing of raw captions for short descriptions. Using this approach, we produce 500K short captions. Object-level QA. To generate instruction data that explicitly specifies particular objects or their relationships, we collect MeViS [10], Ref-YouTube-VOS [36] and A2DSentence datasets. Both provide reliable short descriptions with mask annotations for each object region. By utilizing these short descriptions and masked videos, we first employ Annotator to generate object-level descriptions for each region, and then employ Refiner to generate QA pairs related to the objects within the videos, using variety of prompts. Three types of region-based QA data have been created: (i) Basic Questions: These cover object types, attributes, ac- (ii) Reasontions, locations, and interactions over time. ing Questions: These require reasoning and background knowledge to explain events without relying on specific visual details. (iii) Future Predictions: These involve anticipating future actions or events related to given object. We generate 75K QA pairs in total. 3.2. VideoRefer Model 3.2.1 Overall Architecture In this section, we introduce the VideoRefer framework, which ensures the next token predictions of Video LLM, enabling fine-grained mask-level comprehension at any specific regions and any timestamps for given video. Given that the current Video LLM already exhibits strong general scene-level video understanding capabilities, we develop our model upon well-established Video LLM, VideoLLaMA2.1 [9]. Our primary innovation is to introduce versatile and unified spatial-temporal object encoder to obtain object-level representations across video scenes. The overall architecture of our framework is illustrated in Fig. 3. VideoRefer adopts visual encoder and STC connector [9] to encode the global scene-level visual representations, pretrained text tokenizer to capture the language embeddings, and an instruction-following LLM for language decoding. To achieve video referring, we present versatile and unified spatial-temporal encoder, denoted as REnc, to derive object-level representations. For specific object Rj R, we define Rj = {rij }, where each rij represents unified 2D binary mask designed to accommodate free-form input regions, assigning value of 1 inside the region and 0 outside. The set of objects R, along with the image feature map extracted from the shared visual encoder, is then fed into the introduced object encoder REnc, which generates enriched objectlevel tokens, expressed as TR = REnc(R, Z). Finally, the interleaved scene-level tokens TZ, object-level tokens TR and linguistic tokens Tx are sent to the LLM to obtain the fine-grained semantic understandings , formulated as = Φ(TZ, TR, Tx), where Φ denotes the LLM. 3.2.2 Versatile Spatial-Temporal Object Encoder To support various spatial-temporal video understanding tasks, our presented object encoder not only captures masklevel spatial features within the single frame at specific timestamp, but also aggregates temporal information across multiple frames over duration of time. Consequently, we devise two modes for our object encoder: single-frame and 4 Figure 3. Model architecture of our VideoRefer for spatial-temporal video object understanding. multi-frame. For the sake of brevity for better illustration, we use single object Rj as an example. If multiple objects are specified by the user, we adopt the same manner to extract features for each object individually. Single-Frame. For single-frame mode, the input consists of randomly selected frame along with the corresponding regions specified by the user in that frame. Here, contains only randomly chosen timestamp. To generate the object-level token representations, we present the Spatial Token Extractor. In detail, the image feature is initially extracted by the shared visual encoder to generate the global image feature FI R1HI WI DI , where HI , WI , DI denote the height, width and dimension of the image feature, respectively. Each binary mask of an object is then resized to match the shape of the image feature. We utilize the Mask Pooling operation upon image feature to extract object-level spatial feature FO R1DI for each mask, which pools all features within the region to generate an object-level representation. Finally, an MLP layer is employed to adapt and produce the object-level token R1C for each object region. Multi-Frame. In the multi-frame mode, the input consists of list of selected frames from the video, accompanied with their respective object regions, i.e., contains list of timestamps from the video. The frame-level feature is extracted using the shared visual encoder to generate the image feature FI RkHI WI DI , where represents the number of selected frames. We then employ the Spatial Token Extractor to generate the object-level tokens for each frame. Hence, we obtain the object tokens RkC. To aggregate distinct temporal object-level representations across multiple frames over time duration while minimizing redundant tokens, we propose the Temporal Token Merge Module, which is designed to effectively capture essential object-level tokens across the temporal dimension. Specifically, starting with spatial object tokens RkC, we first compute the cosine similarity between each pair of adjacent tokens, formulated as: Sm,m+1 = Om Om+1 Om Om+1 , 0 m<k. (2) Subsequently, we select the top similarity scores from S, where is predefined constant. The corresponding pairs of tokens are then merged into single union, resulting in unions. For each union, we apply straightfoward average pooling to produce single distinct representative token. Ultimately, tokens, represented as RuC, are generated following an MLP layer for each object, ensuring both spatial integrity and temporal coherence without disrupting spatial structure. 3.3. VideoRefer-Bench To comprehensively evaluate the models capability on video-based regional comprehension, we have developed benchmark named VideoRefer-Bench. This benchmark assesses the models in two key areas: Description Generation, corresponding to VideoRefer-BenchD, and Multiple-choice Question-Answer, corresponding to VideoRefer-BenchQ. Fig. 4 and Fig. 5 provide exemplar visual illustrations and data characteristics, respectively. 3.3.1 VideoRefer-BenchD We introduce sub-benchmark, VideoRefer-BenchD specifically designed to evaluate the description generation performance of video-based referring models. The bench5 Figure 4. Exemplar visual illustration of VideoRefer-Bench. mark comprises total of 400 curated data entries. We curated the test set based on Panda-70M [7], employing the pipeline described in Section 3.1, followed by meticulous human check. Furthermore, we developed an evaluation pipeline utilizing the GPT-4o model. This pipeline rigorously assesses various capabilities of the model by assigning scores to the generated predictions on scale range from 0 to 5 across the following four dimensions: Subject Correspondence (SC): This dimension evaluates whether the subject of the generated description accurately corresponds to that specified in the ground truth. Appearance Description (AD): This criterion assesses the accuracy of appearance-related details, including color, shape, texture, and other relevant visual attributes. Temporal Description (TD): This aspect analyzes whether the representation of the objects motion is consistent with the actual movements. Hallucination Detection (HD): This facet identifies discrepancies by determining if the generated description includes any facts, actions, or elements absent from reality, like imaginative interpretations or incorrect inferences. 3.3.2 VideoRefer-BenchQ The other sub-benchmark VideoRefer-BenchQ is designed to evaluate the proficiency of MLLMs in interpreting video objects. We meticulously curated dataset comprising 198 videos sourced from various datasets, including DAVIS-2017 [31] and the test set of MeViS [10]. To facilitate robust evaluation, we annotated set of 1,000 high-quality multiple-choice questions. These questions are crafted to assess different dimensions of understandincluding Basic Questions, Sequential Questions, ing, Relationship Questions, Reasoning Questions, and Future Predictions. The annotations were performed by researchers with extensive research experience in visionlanguage learning. Importantly, each QA pair is required to be explicitly linked to specific video region. This ensures that the MLLMs cannot provide answers without actually analyzing the video or the designated object. Figure 5. Data characteristics of VideoRefer-Bench. 4. Experiments 4.1. Implementation Details We adopt siglip-so400m-patch14-384 [48] as the vision encoder, Qwen-2 [43] as the LLM. The AdamW [24] is used as the optimizer and the cosine annealing scheduler [25] is used to adjust learning rate. We use hybrid strategy including both single-frame and multi-frame modes during training. We leverage progressive training scheme, which consists of image-text alignment pretraining (Stage 1), region-text alignment pre-training (Stage 2), high-quality knowledge learning (Stage 2.5) and visual instruction tuning (Stage 3) stages, respectively. Please refer to the Appendix for detailed introduction to each stage. At the first and second stages, we set global batch size to 256 and learning rate to 1103 for one epoch. In stage 2.5 and stage 3, the learning rate is reduced to 2105 with global batch size of 128 for one epoch. Unless otherwise specified, all models adopt the 7B LLM. 4.2. Main Results To evaluate the efficacy of our VideoRefer model, we conduct experiments on both video referring tasks and general video understanding tasks to demonstrate its capabilities. 4.2.1 Video Referring Tasks VideoRefer-BenchD. We compare our approach on VideoRefer-BenchD with the previous generalist models, including GPT-4o [28], GPT-4o-mini [28], InternVL226B [8] and Qwen2-VL [43], and specialist models for object-level understanding, including image-level Osprey [46], Ferret [44], and video-level Elysium [41], Artemis [32]. Both single-frame and multi-frame modes are adopted for evaluation. In the single-frame mode, we select the first frame that contains the specific object with its aligned boundary for the generalist models. For imagelevel region understanding models, we utilize random frame along with the corresponding region prompt as input. In the multi-frame mode, we uniformly sample 16 frames with mask contours for generalist models. For image-level methods, we obtain the description frame by frame and then generate summary using GPT-4o. For Method GPT-4o [28] GPT-4o-mini [28] InternVL2-26B [8] Qwen2-VL-7B [43] Image-level models Osprey-7B [46] Ferret-7B [44] Video-level models Elysium-7B [41] Artemis-7B [32] VideoRefer-7B Single-Frame Multi-Frame SC AD TD 3.34 3.56 3.55 2.97 3.19 3. 2.35 4.41 2.96 2.85 2.99 2.24 2.16 2.01 0.30 3.27 3.01 2.87 2.57 2.03 1.54 1. 0.02 3.03 HD Avg. Generalist Models 2.95 2.92 2.84 2.39 Specialist Models 2.50 2.38 2.25 2.31 2.45 2.14 3.59 2. 2.34 2.19 1.57 3.42 SC AD TD HD Avg. 4.15 3.89 4.08 3.30 3.30 3.20 3.42 4.44 3.31 3.18 3.35 2.54 2.66 2. 1.34 3.27 3.11 2.62 3.08 2.22 2.10 1.97 1.39 3.10 2.43 2.50 2.28 2.12 1.58 1. 2.90 3.04 3.25 3.05 3.20 2.55 2.41 2.23 2.26 3.46 Table 1. Performance comparisons on VideoRefer-BenchD. The best results are bold and the second-best results are underlined. means that the model does not support the certain input form. Grey entries denote cases where the original method cannot accomplish the task; for these tests, masks of the targets were overlaid on the original video (the same below). Method GPT-4o [28] GPT-4o-mini [28] InternVL2-26B [8] Qwen2-VL-7B [43] Osprey-7B [46] Ferret-7B [44] VideoRefer-7B Basic Questions Sequential Questions Relationship Questions Generalist Models Reasoning Questions Future Predictions Average 62.3 57.6 58.5 62.0 45.9 35.2 75.4 74.5 67.1 63.5 69.6 47.1 44.7 68. 66.0 56.5 53.4 54.9 Specialist Models 30.0 41.9 59.3 88.0 85.9 88.0 87.3 48.6 70.4 89.4 73.7 75.4 78.9 74. 23.7 74.6 78.1 71.3 65.8 65.0 66.0 39.9 48.8 71.9 Table 2. Performance comparisons on VideoRefer-BenchQ. Note: Video-level specialist models, including Elysium [41] and Artemis [32], do not have the ability to handle multi-choice questions on VideoRefer-BenchQ. Elysium [41] and Artemis [32], we adhere to the official settings provided in their respective papers. For our VideoRefer, we randomly select single frame and uniformly sample 16 frames as inputs for the single-frame and multiframe modes, respectively. Table 1 presents the comparison results. Our approach achieves the leading average performance in regional-temporal video understanding compared to previous methods in both single-frame and multi-frame modes. Notably, VideoRefer attains top scores of 4.41, 3.27, and 3.03 for Subject Correspondence (SC), Appearance Description (AD), and Temporal Description (TD) in single-frame mode, and scores of 4.44 and 3.04 for SC and Hallucination Detection (HD) in multi-frame mode. Fig. 6 illustrates typical visual comparison. VideoRefer-BenchQ. We then compare our VideoRefer against the previous methods on VideoRefer-BenchQ. Here, we set single-frame mode following settings of VideoReferBenchD. As shown in Table 2, our VideoRefer achieves the best average performance with 71.9, which significantly outperforms the previous regional methods. Especially, our approach excels in basic questions, relationship questions, reasoning questions and future predictions with 75.4, 59.3, 89.4 and 78.1 scores with best or second-best places, respectively. These results clearly demonstrate the superiority of our method in spatial-temporal video understanding. Previous Video Referring Metrics. Following the previous state-of-the-art video referring approach, Artmis [32], we further conduct experiments on the test set of HCSTVG [38]. Table 3 presents the comparison results. Our approach outperforms Artmis [32] by +1.0%, +0.7%, +1.6%, +15.4%, and +2.9% on BLEU4 [29], METEOR [3], ROUGE [18], CIDEr [40] and SPICE [1] metrics. These results demonstrate the superiority of our VideoRefer. 4.2.2 General Video Understanding To demonstrate the capabilities of our method, we conduct performance evaluation on general video understanding tasks. As shown in Table 4, VideoLLaMA2.1 [9] achieves scores of 54.9% on Perception-Test [30], 57.3% on MVBench [15], and 54.9%/56.4% on VideoMME [12]. Based on that, our VideoRefer exhibits performance gains Figure 6. Visual comparisons between our VideoRefer with general GPT-4o and regional video-level Elysium and Artemis. Here we provide detailed illustrations on VideoRefer-BenchD. Method BLEU@4 METEOR ROUGE CIDER SPICE Merlin [45] Artemis [32] VideoRefer 3.3 15.5 16.5 11.3 18.0 18. 26.0 40.8 42.4 10.5 53.2 68.6 20.1 25.4 28.3 Table 3. Exprimental results on video-based referring metrics on the HC-STVG [38] test set. Method Perception-Test MVBench VideoMME VideoLLaMA2 [9] VideoLLaMA2.1 [9] Artemis [32] VideoRefer 51.4 54.9 47.1 56.3 54.6 57.3 34.1 59.6 47.9/50.3 54.9/56.4 28.8/35.3 55.9/57.6 Table 4. Exprimental results on general video understanding tasks. of +1.4%, +2.3%, and +1.0%/+1.2%, respectively. In contrast, Artemis demonstrates subpar performance. These results clearly indicate that our approach not only excels in object-level analysis, but also enhances the ability of general video understanding. 4.3. Ablation Study Single-frame vs. Multi-frame. We first validate the impacts on the single-frame and multi-frame modes, i.e. with or without Temporal Token Merge (TTM) module to encode the multi-frame sequences during the inference. As shown in Table 5, our approach utilizing multi-frame mode exhibits improvements over the single-frame mode in both VideoRefer-BenchD and VideoRefer-BenchQ across all metrics. Notably, for sequential relation-based metrics, including Temporal Description (TD), Sequential Questions (SQ), and Relationship Questions (RQ), as well as hallucination-related metrics such as Hallucination Detection (HD), multi-frame mode showcases the superiority. Ablation on VideoRefer-700K Dataset. Table 6 summarizes the ablation results for various data types in the constructed VideoRefer-700K dataset. The results indiMode VideoRefer-BenchD VideoRefer-BenchQ TD HD Avg. SQ RQ Single-frame 3.03 2.97 3.10 3.04 Multi-frame 3.42 3.46 68.3 59.1 70.6 60.5 Avg. 71.9 72.1 Table 5. Results using different modes during the inference. Here, SQ and RQ are Sequential Questions and Relationship Questions. Method BenchD BenchQ MVBench 0 w/o Regional data 1 + Short description 2 + QA 3 + Detailed description 2.43 2.45 3. 68.3 71.7 71.9 57.9 58.0 58.4 59.6 Table 6. Ablation results on various data types in VideoRefer700K dataset. Bench denotes VideoRefer-Bench for simplicity. cate that using short description yields score of 2.43 on BenchD and 68.3 on BenchQ, along with an MVBench score of 58.0. Incorporating question-answering (QA) data improves the performance to 2.45 for BenchD and 71.7 for BenchQ, while maintaining an MVBench score of 58.4. Notably, the method employing detailed descriptions achieves the best results, with scores of 3.42 on BenchD, 71.9 on BenchQ, and 59.6 on MVBench. These results demonstrate that the inclusion of more comprehensive data significantly enhances overall performance. Impacts of Different Union Numbers in TTM. The Temporal Token Merge (TTM) Module is designed to capture essential object-level tokens across the temporal dimension in multi-frame mode. Fig. 7 visualizes the similarity scores between adjacent object token pairs. It is evident that most adjacent tokens exhibit high similarity, making it necessary to merge those tokens with significant similarity. We conducted ablation experiments using temporal and sequential metrics to investigate the effects of varying numbers of token unions u. The experimental results are deUnion 32 16 8 4 1 VideoRefer-BenchD VideoRefer-BenchQ TD 3.17 3.20 3.18 3.10 3. HD 3.01 2.99 3.02 3.04 2.98 SQ 68.7 69.3 69.6 70.6 68.9 RQ 58.1 58.5 57.8 60.5 60. Table 7. Temporal and sequential performance comparisons for various union in the TTM module under multi-frame mode. tailed in Table 7. Notably, with = 4, VideoRefer achieves the best performance in Hallucination Detection (HD) and Sequential Questons (SQ), and ranks second in Reasoning Questions (RQ). We adopt = 4 to strike balance between performance and token costs in our approach. 5. Related Works 5.1. Video Large Language Models Large Language Models (LLMs) have revolutionized the field of artificial intelligence by proving their capability to tackle diverse tasks related to language comprehension and generation. To fully leverage the potential of LLMs for visual understanding, researchers have increasingly turned their attention to image-based Multimodal Large Language Models (MLLMs) [2, 8, 16, 1922, 27, 53], which integrate language and visual data within unified feature space. This integration has emerged as significant area of research focus. In parallel, Video Large Language Models (Video LLMs) [9, 17, 26, 50, 54] have garnered increasing attention fueled by advancements in image-based MLLMs. Most Video LLMs primarily follow the trend of utilizing pre-trained vision models to extract sequence-based information from videos, which is then interleaved with textual embeddings for LLM to generate responses [37]. Despite their promising results, current Video LLMs still face challenges in fine-grained regional and temporal understanding. 5.2. Regional Understanding with MLLMs To attain fine-grained regional object-level comprehension, MLLMs can be incorporated with instance-level visual representations. This integration allows models to generate semantic understandings that focus on specific regions. In the context of image-based MLLMs, recent researchs [4 6, 11, 13, 33, 39, 42, 44, 46, 47, 49, 51, 52, 55] has demonstrated significant trend to enable the image referring with spatial visual prompts. In contrast, research focused on video-based regional understanding across dynamic sequence-based scenes is relatively limited. Merlin [45] first explored video-based referring and future reasoning by employing three manually selected frames as visual input, which limits the models ability to comprehend longer and more intricate scenes. Elysium [41] introduces million-scale dataset for object-level tasks in videos; however, the provided descriptions tend to be quite simplistic. Another reseach work is Artemis [32], but it primarily emphasizes basic single object descriptions, thereby constraining its capacity to analyze relationships among various objects or perform more complex tasks on specific objects within dynamic sequences. Moreover, Artemis utilizes sparse bounding box representation, which inadequately captures the nuances of the objects. Compounding these challenges is the lack of large-scale, high-quality regionlevel video instruction data and benchmarks for thorough evaluation, which further hampers progress in this domain. To address these issues, we introduce the VideoRefer Suite to advance spatial-temporal understanding. 6. Conclusion In this work, we introduced the VideoRefer Suite to empower Video LLM for fine-grained spatial and regional video understanding. Three key components have been proposed: 1) VideoRefer-700K: large-scale, high-quality region-level video instruction data curated by developed multi-agent engine; 2) VideoRefer: Video LLM equipped with versatile spatial-temporal object encoder that includes Spatial Token Extractor and an adaptive Temporal Token Merge Module to enabling precise sequential regional representation; and 3) VideoRefer-Bench: comprehensive benchmark that thoroughly evaluates model performance across multiple aspects, ensuring holistic assessment of spatial-temporal capabilities. Extensive experimental results and analyses have demonstrated the efficacy of our VideoRefer Suite, substantially advancing finer-level video understanding and analysis."
        },
        {
            "title": "References",
            "content": "[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pages 382398, 2016. 7 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv:2308.12966, 2023. 1, 9 [3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. 7 [4] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In CVPR, pages 1291412923, 2024. 3, 9 [5] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Position-enhanced visual Maosong Sun, and Yang Liu. 9 instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. [6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3, 9 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In CVPR, pages 1332013331, cross-modality teachers. 2024. 4, 6 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 4, 6, 7, 9 [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 4, 7, 8, 9, 12 [10] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In ICCV, pages 26942703, 2023. 4, [11] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. In NeurIPS, 2024. 3, 9 [12] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 7 [13] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In CVPR, pages 1379613806, 2024. 3, 9 [14] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In NeurIPS, 2023. 4 [15] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. 1, 7 [16] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. 9 [17] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2023. 1, [18] Chin-Yew Lin. Rouge: package for automatic evaluation In Text summarization branches out, pages of summaries. 7481, 2004. 7 [19] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. 1, 9 [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [21] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. 12 [22] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. https: // llavavl.github. io/ blog /20240130llava-next/, 2024. 1, [23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 4 [24] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 6 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6 [26] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2024. 1, 9 [27] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. 1, [28] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 4, 6, 7 [29] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, pages 311318, 2002. 7 [30] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2024. 7 [31] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [32] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye, and Yunjie Tian. Artemis: Towards referential understanding in complex videos. In NeurIPS, 2024. 1, 3, 6, 7, 8, 9 [33] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, pages 1300913018, 2024. 3, 9 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 2, 4 10 [49] Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. 3, 9 [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1, [51] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 1, 3, 9 [52] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on regionof-interest. arXiv preprint arXiv:2307.03601, 2023. 1, 3, 9 [53] Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, et al. Hyperllava: Dynamic visual and language expert tuning for multimodal large language models. arXiv preprint arXiv:2403.13447, 2024. 9 [54] Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model. https: // llavavl.github. io/blog /20240430llava-next-video/, 2024. 1, 9 [55] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474, 2023. 3, 9 [35] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [36] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In ECCV, pages 208223. Springer, 2020. 4 [37] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. 9 [38] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. TCSVT, 32(12):82388249, 2021. 7, 8 [39] Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. Chatterbox: Multi-round multimodal referring and grounding. arXiv preprint arXiv:2401.13307, 2024. 3, 9 [40] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 45664575, 2015. 7 [41] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level perception in videos via mllm. In ECCV, pages 166185. Springer, 2024. 1, 3, 6, 7, [42] Shiyu Xuan, Qingpei Guo, Ming Yang, and Shiliang Zhang. Pink: Unveiling the power of referential comprehension for multi-modal llms. In CVPR, pages 1383813848, 2024. 3, 9 [43] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 3, 4, 6, 7 [44] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024. 1, 3, 6, 7, 9 [45] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms In ECCV, pages 425443. Springer, with foresight minds. 2025. 1, 3, 8, 9 [46] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In CVPR, pages 2820228211, 2024. 1, 3, 4, 6, 7, 9 [47] Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, Xingjian He, Gang Xiong, Yisheng Lv, and Jing Liu. Sc-tune: Unleashing self-consistent referential compreIn CVPR, pages hension in large vision language models. 1307313083, 2024. 3, 9 [48] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pages 1197511986, 2023."
        },
        {
            "title": "Appendix",
            "content": "A. More Qualitative Results We provide additional visualization results to emphasize performance across variety of tasks, such as single-object referring, video relationship analysis, complex reasoning, future prediction, and video object retrieval. Besides, we present the examplar cases to demonstrate the capabilities in general video understanding and image object understanding. Fig. 12 showcases these visual examples. randomly selected mask along with its corresponding frame is used as the region input. B. Additional Implemental Details B.1. Training Stages The training pipeline of our model is structured into four distinct stages. Fig. 8 presents the data distribution for each stage. Stage 1: Image-Text Alignment Pre-training. In this initial pre-training phase, we utilize the same dataset as employed in the first stage of VideoLLaMA2.1 [9]. During this phase, the parameters of both the vision encoder and the large language model are frozen, and training is conducted solely on the STC connector [9], enabling the alignment of image and text modalities. Stage 2: Region-Text Alignment Pre-training. This stage further incorporates the Object Encoder to capture object-level features based on the weights obtained from Stage 1. The training focus is exclusively on the spatialtemporal Object Encoder to ensure the alignment of intricate object-level features with corresponding language embeddings. We use the generated 500K region-level short descriptions, along with video and image referring segmentation datasets as the training data. During this stage, all the data are processed in single-frame mode to focus solely on alignment. Stage 2.5: High-Quality Knowledge Learning. At this intermediate stage, the weights of vision encoder remain frozen, while the STC connector, Object Encoder, and LLM undergo fine-tuning. This stage aims to infuse the model with high-quality captioning data, utilizing diverse dataset that includes 118K image-caption pairs, 30K videocaption pairs, 79K image-level region caption data, and 125K video-level region caption data, inclusive of the detailed descriptions we curated. For object-level video data, we employ balanced approach, using half in single-frame mode and half in multi-frame mode. Stage 3: Visual Instruction Tuning. The training configuration for this stage closely mirrors that of Stage 2.5. The primary objective is to enhance the models ability to accurately interpret user instructions and tackle complex Manually True Manually False Reviewer True Reviewer False 88 (TP) 36 (FN) 12 (FP) 64 (TN) Table 8. Confusion matrix of the randomly sampled 100 items in the Reviewer evaluation. object-level understanding tasks. For video-level data, we utilize the same dataset segments as those used in VideoLLaMA2.1 [9]. For image-level data, we employ the datasets from LLaVA [21]. In addition, we incorporate 294K imagelevel region data and 115K previously constructed videolevel region data to further strengthen the models capabilities. We also employ balanced approach using half in single-frame mode and half in multi-frame mode in this stage. C. More Details of VideoRefer-700K Dataset and Benchmark C.1. Human Evaluation on Reviewer In our muliti-agent data engine, we introduce the Reviewer to address potential errors and mismatches, thereby ensuring the quality of our VideoRefer-700K dataset. To assess the effectiveness of the Reviewer, we conducted manual evaluation of its outputs. We define the evaluation metrics as follows: TP (True Positives): Items that the Reviewer identified as relevant and accurate, which are confirmed to be true upon manual inspection. TN (True Negatives): Items that the Reviewer discarded as irrelevant or inaccurate, which are indeed false according to the manual check. FP (False Positives): Items that the Reviewer considered as true, but are found to be false during manual verification. FN (False Negatives): Items that the Reviewer discarded as false, but are actually true upon manual review. We randomly sampled 100 items each from both the data discarded and retained by the Reviewer. The detailed results are represented in Table 8, and the corresponding metrics are calculated as follows: Accuracy = + P + + + = 0.76, (3) Precision = P + = 0.88, Recall = P + = 0.71, (4) (5) F1 Score = 2 Precision Recall Precision + Recall = 0.79. (6) 12 Figure 7. Visualizations of similarity among adjacent object-level token pairs across the temporal dimension. Here, we use cosine similarity as the measurement. Figure 9. Data distributions of our VideoRefer-700K dataset, encompassing five different data types. Figure 8. Visual illustrations of the data distribution for each training stage. The precision value stands at 88%, indicating that the majority of samples identified as positive by the reviewer are indeed positive, thereby ensuring the datas quality. C.2. Example Illustrations We provide typical example to better exhibit the construction pipeline of our multi-agent data engine, as shown the data distribution of our in Fig. 11. Additionally, VideoRefer-700K dataset is illustrated in Fig. 9. Fig. 13 further showcases the additional visual samples from the VideoRefer-700K dataset. C.3. More Benchmark Visualization We present more visualizations of our benchmark, VideoRefer-BenchD and VideoRefer-BenchQ, as shown in Fig. 14. These visualizations aim to provide deeper understanding of benchmarks structure and content. VideoReferBenchD focuses on detailed description tasks, facilitating 13 Figure 10. Visual illustrations of human check process. TP, TN, FP and FN are introduced for the assessment on Reviewer. Figure 11. detailed illustrative example of the construction pipeline in our multi-agent data engine. the analysis of nuanced object references and relationships within videos. Meanwhile, VideoRefer-BenchQ is designed for question-and-answer scenarios, capturing the essence of interactive video comprehension. 14 Figure 12. Visualization results of VideoRefer across various tasks, including single-object referring, video relationship analysis, complex reasoning, future prediction, video object retrieval, as well as general video understanding and image object understanding. 15 Figure 13. Visual samples from our VideoRefer-700 dataset, typical including short descriptions, detailed descriptions, and QA pairs. D. Limitations In this work, our VideoRefer is designed on object-level spatial-temporal video understanding, without the abilities on grounding. This limitation may affect the applicability of our method in real-world scenarios, which requires identifying and associating objects within their dynamic contexts. In the future work, we will address this gap by integrating grounding abilities into our framework, extending our dataset and benchmark to improve the systems overall utility in practical applications. 16 Figure 14. Visual examples of our VideoRefer-Bench, including VideoRefer-BenchD and VideoRefer-BenchQ."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Zhejiang University"
    ]
}