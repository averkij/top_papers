{
    "paper_title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "authors": [
        "Haoran He",
        "Jiajun Liang",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Ling Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch."
        },
        {
            "title": "Start",
            "content": "Test-Time Evolutionary Search Haoran He1,2 Jiajun Liang2 Xintao Wang2 Pengfei Wan2 Di Zhang2 Kun Gai2 Ling Pan1 1 Hong Kong University of Science and Technology 2 Kuaishou Technology haoran.he@connect.ust.hk 5 2 0 2 3 ] . [ 1 8 1 6 7 1 . 5 0 5 2 : r Figure 1: We propose Evolutionary Search (EvoSearch), novel and generalist test-time scaling framework applicable to both image and video generation tasks. EvoSearch significantly enhances sample quality through strategic computation allocation during inference, enabling Stable Diffusion 2.1 to exceed GPT4o, and Wan 1.3B to outperform Wan 14B model and Hunyuan 13B model with 10 fewer parameters. 1 Abstract As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website tinnerhrhe.github.io/evosearch."
        },
        {
            "title": "1 Introduction",
            "content": "Generative models have witnessed remarkable progress across various fields, including language [1, 24, 32], image [16, 38], and video generation [7, 36, 78], demonstrating powerful capabilities to capture complex data distributions. The central driver of this success is their ability to scale up during training by increasing data volumes, computational resources, and model sizes. This scaling behavior during the training process is commonly described as Scaling Laws [30, 33]. Despite these advancements, further scaling at training time is increasingly reaching its limits due to the rapid depletion of available internet data and increasing computational costs. Post-training alignment [71] has been proven to be effective in addressing this challenge. For diffusion and flow models, these approaches typically include parameter tuning via reinforcement learning [6, 19] or direct reward gradient backpropagation [11, 52]. However, they suffer from reward over-optimization due to their mode-seeking behavior, high computational costs, and requirement of direct model weight access. Alternative methods [2, 94] propose directly optimizing initial noise, as some lead to better generations than others, but demand specialized training and struggle with cross-model generalization. Recent advances in large language models (LLMs) have expanded to test-time scaling (TTS) [8, 83], showing promising results to complement traditional training-time scaling law. TTS [92] allocates additional computation budget during inference, offering novel paradigm for improving generation quality without additional training. However, diffusion and flow models present unique challenges for test-time scaling, since they must navigate the complex, high-dimensional state space along the denoising trajectory, where existing methods in LLMs struggle to transfer effectively. Current approaches of test-time scaling for diffusion and flow models include (i) best-of-N sampling [44, 48], which, despite its simplicity, suffers from severe search inefficiency in highdimensional noise spaces; and (ii) particle sampling [35, 61], which, while enabling search across the entire denoising trajectory, compromises both exploration capability and generation diversity due to its reliance on initial candidate pools. These simple heuristic designs lack fundamental adaptability to the complex generation pathways, leading to sample diversity collapse and inefficient computation. In this paper, we aim to address the above critical challenges and develop general and efficient test-time scaling method that is versatile for both image and video generation across diffusion and 2 flow models without parameter tuning or gradient backpropagation. To enable test-time scaling of flow models, we transform their deterministic sampling process (ODE) into stochastic process (SDE), thereby broadening the generation space, which paves the way for unified framework for inference-time optimization. Through systematic analysis of latent spaces along the denoising trajectory, including both starting Gaussian noises and intermediate states, we find that neighboring states in the latent space exhibit similar generation qualities, suggesting that high-quality samples are not solely isolated. Based on this insight, we propose Evolutionary Search (EvoSearch), novel test-time scaling method inspired by biological evolution. EvoSearch reframes test-time scaling of image and video generation as an evolutionary search problem, incorporating selection and mutation mechanisms specifically designed for the denoising process in both diffusion and flow models. At each generation, EvoSearch first selects high-reward parents while preserving population diversity, and then generates new offspring through our designed denoising-aware mutation mechanisms to explore new states, enabling iterative improvement in sample quality. The key insight of EvoSearch is to actively explore high-reward particles through evolutionary mechanisms, overcoming the limitations of previous search methods that are confined to fixed candidate space. To optimize computational efficiency, we dynamically search along the denoising trajectory, progressing from Gaussian noises to states at larger denoising steps, thereby continuously reducing computational costs as we approach the terminal states. Through extensive experiments on both text-conditioned image generation and video generation tasks, we find that EvoSearch achieves substantial improvements in sample quality and human-preference alignment as test-time compute increases. We summarize our key contributions as follows: (i) We propose EvoSearch, novel, generalist, and efficient TTS framework which enhances generation quality by allocating more compute during inference, unifying optimization for both diffusion and flow generative models. (ii) Based on our observations of latent space structure, we design specialized selection and mutation mechanisms tailored to the denoising process, effectively enhancing exploration while maintaining diversity. (iii) Extensive experiments show that Evosearch effectively improves generative model performance by scaling up inference-time compute, outperforming competitive baselines across both image and video generation tasks. Notably, EvoSearch enables SD2.1 [13] to surpass GPT4o, and allows the Wan 1.3B model [78] to achieve competitive performance with the 10 larger Wan 14B model."
        },
        {
            "title": "2.1 Diffusion Models and ODE-to-SDE Transformation of Flow Models.",
            "content": "Both diffusion models and flow models map the source distribution, often standard Gaussian distribution, to true data distribution p0. forward diffusion process progressively perturbing data to noise, defined as xt = αtx0 + σtϵ, where ϵ (0, I) is the added noise at timestep [0, ], and (αt,σt) denote the noise schedule. To restore from diffused data, diffusion models naturally utilize an SDE-based sampler during inference [66, 69], which introduces stochasticity at each denoising step as follows: xt1 = αt1 (cid:18) xt 1 αtϵθ(xt, t) αt (cid:19) (cid:113) + 1 αt1 σ2 ϵθ(xt, t) + σtϵt. (1) In contrast, flow models learn the velocity ut Rd, which enables sampling of x0 by solving the flow ODE [69] backward from = to = 0: xt1 = xt + ut(xt)dt, (2) leading all xt1 drawn from xt identical. This restricts the applicability of test-time scaling search methods like particle sampling and our proposed EvoSearch in flow models [34], since the sampling 3 process lacks stochasticity beyond initial noise. To address this limitation, we transform the deterministic Flow-ODE into an equivalent SDE process. Following previous works [3, 34, 47, 51, 60], we rewrite the ODE sampling in Eq. (2) as follows: σ2 2 log pt(xt) dt + σtdw, ut(xt) dxt = (3) (cid:19) (cid:18) where the score log pt(xt) can be computed by velocity ut (see Eq. (13) in [60]), and dw injects stochasticity at each sampling step."
        },
        {
            "title": "2.2 Evolutionary Algorithms.",
            "content": "Evolutionary algorithms (EAs) [9, 37] are biologically inspired, gradient-free methods that found effective in optimization [21, 23, 75], algorithm search [12, 55], and neural architecture search [54, 65, 88]. The key idea of EAs is mimicking the process of natural evolution [4], by maintaining population of solutions that evolve over generations. EAs begin with the initialization of population of candidate solutions, often generated randomly within the defined search space. Subsequently, EAs employ fitness function to evaluate and select parents, prioritizing those with higher scores. Once parents are selected, genetic operators such as crossover (recombination) and mutation (random changes) are applied to create offspring that constitute the next generation. Through iterative generations, the population evolves toward optimal or near-optimal solutions. Due to the diversity within populations and the mutation operations, EAs have strong exploration ability. As result, compared to conventional local search algorithms like gradient descent, EAs exhibit better global optimization capabilities within the solution space and are adept at solving multimodal problems [40, 75]."
        },
        {
            "title": "3 Related Work\nAlignment for Diffusion and Flow Models. Aligning pre-trained diffusion and flow generative\nmodels can be achieved by guidance [13, 69] or fine-tuning [18, 39], which aim to enhance sample\nquality by steering outputs towards a desired target distribution. Guidance methods [5, 10, 26, 29, 67,\n68] rely on predicting clean samples from noisy data and differentiable reward functions to calculate\nguidance. Typical fine-tuning methods involve supervised fine-tuning [18, 39, 82], RL fine-tuning [6,\n19, 20], DPO-based policy optimization [43, 46, 77, 87, 91], direct reward backpropagation [11,\n52, 85],stochastic optimization [14, 89], and noise optimization [2, 17, 25, 70, 94]. These methods\nrequire additional dataset curation and parameter tuning, and can distort alignment or reduce\nsample diversity due to their mode-seeking behavior and reward over-optimization. In contrast, our\nproposed EvoSearch method offers significant advantages through its universal applicability across\nany reward function and model architecture (including flow-based, diffusion-based, image and\nvideo models) without requiring additional training. Moreover, EvoSearch complements existing\nfine-tuning methods, as it can be applied to any fine-tuned model to further enhance reward\nalignment.\nTest-Time Scaling in Vision. Several test-time scaling (TTS) methods have been proposed to extend\nthe performance boundaries of image and video generative models. These methods fundamentally\noperate as search, with reward models providing judgments and algorithms selecting better candi-\ndates. Best-of-N generates N batches of samples and selects the one with the highest reward, which\nhas been validated effective for both image and video generation [44, 48]. More advanced search\nmethod for diffusion models is particle sampling [35, 41, 42, 59, 61], which resamples particles over\nthe full denoising trajectory based on their importance weights, demonstrating superior results than\nnaive BoN. Video-T1 [44] and other recent works [44, 49, 84, 86] propose leveraging beam search [64]\nfor scaling video generation. However, in the context of diffusion and flow models, we remark that",
            "content": "4 beam search represents specialized case of particle sampling with predetermined beam size, as both methodologies iteratively propagate high-reward samples while discarding lower-reward ones in practice. Furthermore, Video-T1 is constrained to autoregressive video models, limiting its applicability to more advanced diffusion and flow generative models. All existing search methods rely heavily on the quality of the initial candidates, failing to explore new particles actively, while our proposed method, EvoSearch, leverages the idea of natural selection and evolution, enabling the generation of new, higher-quality offspring iteratively. EvoSearch is also generalist framework with superior scalability and extensive applicability across both diffusion and flow models for image and video generation, contrary to previous methods that are constrained to specific models or tasks."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "In this work, we investigate how to efficiently harness additional test-time compute to enhance the sample quality of image and video generative models. Given pre-trained flow-based or diffusionbased model and reward function, our objective is to generate samples from the following target distribution [42, 72, 73, 80]: ptar := arg maxpEx0p[r(x0)] αDKL[pppre 0 ], which optimizes the reward function while preventing ptar from deviating too far from pre-trained distribution ppre 0 , with α controlling this balance. The target distribution ptar can be re-written as: (4) ptar = 1 ppre 0 (x0) exp (cid:19) (cid:18) r(x0) α , (5) where denotes normalization constant [53, 72]. Notably, directly sampling from the target distribution is infeasible: the normalization factor requires integrating over the entire sample space, making it computationally intractable for high-dimensional spaces in diffusion and flow models."
        },
        {
            "title": "4.2 Limitations of Existing Approaches\nTest-time approaches to sampling from the target distribution ptar in Eq. (5) employ importance\n0 ∼ ppre\nsampling [50], which generates k particles xi\n0 (x0) and then resamples the particles based on\nthe scores exp(r(x0)/α). A straightforward implementation of this concept is best-of-N sampling,\nwhich simply generates multiple samples and selects the one with the highest reward. A more\nsophisticated approach, called particle sampling [35, 62], searches across the entire denoising path\nτ = {xT , · · · , xk, · · · , x0}, guiding samples toward trajectories that yield higher rewards. However,\nboth of these methods suffer from fundamental limitations in their efficiency and exploration\ncapabilities. Best-of-N only resamples at the final step (t = 0), taking the entire distribution\n0 (x0) = (cid:82) (cid:81)\nppre\n(xt−1|xt)}dx1:T as its proposal distribution. This passive filtering approach\nis computationally wasteful, as it expends a large amount of computation generating complete\ntrajectories for samples that ultimately yield low rewards. In contrast, particle sampling can search\nand resample at each intermediate step along the denoising path, using ppre\n(xt−1|xt) as its proposal\ndistribution at each step t. However, it is still constrained by the fixed initial candidate pool,\nstruggling to actively explore and generate novel states beyond those proposed by ppre\n0 during the\nsearch process. This limitation becomes increasingly restrictive as the search progresses, which leads\nto restricted performance due to limited exploration and reduced diversity.",
            "content": "t{ppre To better understand these inherent limitations more concretely, we visualize the behavior of different approaches in Fig. 2. As shown, re-training methods, including RL (DDPO [6]) and reward 5 Figure 2: Visualization of test-time alignment experiment. We train diffusion model with 3layer MLP on Gaussian mixtures (pre-trained distribution), with the goal to capture multimodal unseen target distribution, where reward r(X, ) = 2 + 2 4. EvoSearch achieves superior performance, capturing all the modes with the highest reward (-0.74). backpropagation [11], struggle to generalize to the unseen target distribution, largely due to their heavy reliance on pre-trained models and mode-seeking behavior. While test-time search methods (best-of-N and particle sampling) achieve higher rewards than re-training methods, they still fail to capture all modes of the multimodal target distribution, converging to limited regions of the solution space. These findings highlight the need for novel test-time scaling framework capable of effectively balancing between exploitation and exploration while maintaining computational efficiency for scaling up. In the following sections, we introduce how our EvoSearch method overcomes these fundamental limitations, which achieves the highest reward with comprehensive mode coverage as shown in Fig. 2."
        },
        {
            "title": "4.3 Evolutionary Search",
            "content": "We propose Evolutionary Search (EvoSearch), novel evolutionary framework that reformulates the sampling from the target distribution ptar in Eq. (5) at test time as an active evolutionary optimization problem rather than passive filtering. EvoSearch introduces unified way for achieving efficient and effective test-time scaling across both diffusion and flow models for image and video generation tasks. The overview of our method is provided in Fig. 3. EvoSearch introduces novel perspective that reinterprets the denoising trajectory as an evolutionary path, where both the initial noise xT and the intermediate state xt can be evolved towards higher-quality generation, actively expanding the exploration space beyond the constraints of the pre-trained models distribution. Different from classic evolutionary algorithms that optimize population set in fixed space [9], EvoSearch considers dynamically moving forward the evolutionary population along the denoising trajectory starting from xT (i.e., Gaussian noises). Below, we introduce the core components of our EvoSearch framework."
        },
        {
            "title": "4.4 Evolution Schedule",
            "content": "For typical sampling process in diffusion and flow models, the change between xt1 and xt is not substantial. Therefore, performing EvoSearch at every sampling step would be computationally wasteful. To address this efficiency problem, EvoSearch defines an evolution schedule = {T, , tj, , tn} that specifies the timesteps at which EvoSearch should be conducted. Concretely, EvoSearch first thoroughly optimizes the starting noise xT to identify high-reward regions in the Gaussian noise space, establishing strong initialization for the subsequent denoising process. After high-quality xT is obtained, EvoSearch progressively applies our proposed evolutionary operations to intermediate states xti at predetermined timesteps ti . This cascading way enables each subsequent generation beginning directly from the cached intermediate state xti obtained from the previous generation, instead of repeatedly denoising from xT , eliminating the redundant 6 Figure 3: Overview of our method. EvoSearch progressively moves forward along the denoising trajectory to refine and explore new states. denoising computations from xT xti. In practice, we implement this evolution schedule using uniform intervals between timesteps, which significantly reduces computational overhead."
        },
        {
            "title": "4.5 Population Initialization.",
            "content": "Following the evolution schedule , we introduce corresponding population size schedule = {kstart, kT , , ktj , , ktn}, where kstart denotes the initial size of sampled Gaussian noises, and each kti specifies the children population size for the generation at timestep ti. This adaptive approach enables flexible trade-offs between computational cost and exploration of the state space (please find Appendix B.1 for further analysis on ablation of K). The initial generation of EvoSearch begins with kstart randomly sampled Gaussian noises {xi i=1 at timestep = , which serve as the first-generation parents for the subsequent evolutionary process. }kstart"
        },
        {
            "title": "4.6 Fitness Evaluation.",
            "content": "To guide the evolutionary process, EvoSearch evaluates the quality of each parent using an off-theshelf reward model at each evolution timestep ti: R(xti) = Ex0p0(x0xti ) [r(x0)xti] , (6) where the reward model can correspond to various objectives, including human preference scores [28, 81, 85] and vision-language models [27, 44]. Note that previous methods typically rely on either lookahead estimators [41, 49] or Tweedies formula [10, 15] to predict x0 from noisy data for reward calculation in Eq. (6), which can induce significant prediction inaccuracies and approximation errors. In contrast, we evaluate the reward directly on fully denoised x0 (e.g., clean image or video), thereby obtaining high-fidelity reward signals."
        },
        {
            "title": "4.7 Selection.",
            "content": "To propagate high-quality candidates across generations while maintaining population diversity, EvoSearch employs tournament selection [22] to sample parents from the population of size kti through cycles. Specifically, each cycle picks tournament of < kti candidates at random and selects the best candidate in the tournament as parent."
        },
        {
            "title": "4.8 Mutation.",
            "content": "Recent works [2, 94] have shown that different initial noises yield varying generation quality. Intuitively, this property extends naturally to intermediate denoising states. While this phenomenon 7 serves as basis for making best-of-N and particle sampling useful, it raises more fundamental question: do these noises and intermediate states possess other exploitable patterns or structural regularities that can be leveraged to enhance inference-time generation quality? To investigate this critical question, we visualize the latent states at different denoising steps using t-SNE [74]. Our findings, as shown in Fig. 4, reveal that neighboring states in the latent space exhibit similar generation qualities, suggesting that high-quality samples are not solely isolated. Building upon this discovery, we develop specialized mutation strategy that leverages this exploitable structure in the reward landscape of diffusion and flow models. Specifically, we preserve elite parents (those with top fitness scores) at each generation to ensure convergence, where kti. For the remaining ktim parents, we mutate them to explore the neighborhoods around selected parents to discover higher-quality samples. This approach avoids premature convergence to narrow region of the denoising state space, facilitating effective exploration of novel regions while maintaining population diversity. To align with the characteristics of the underlying SDE sampling process, we develop different mutation operations for initial noises and intermediate denoising states. Figure 4: t-SNE Visualization of latent xt from SD2.1 model at different steps, colored by their corresponding ImageReward scores. At denoising step 0, xt is Gaussian noises. Top row: Results from Stable Diffusion 2.1 model. Bottom row: Results from Flux.1-dev. Initial noise mutation. For the initial noise xT , which is sampled from Gaussian distribution, the corresponding mutation operation is designed to preserve the Gaussian nature of the noise based on xchild = (cid:112) 1 β2xparent + βϵT , ϵT (0, I), (7) where β is hyperparameter that controls the strength of added stochasticity to the parents. The first term ensures that the mutated children preserve the high-reward region density, while the second term encourages exploration. Intermediate denoising state mutation. For intermediate states xt, the mutation operation defined in Eq. (7) is not applicable since xt is no longer Gaussian due to the denoising process. To synthesize meaningful variations while preserving the intrinsic structure of the latent state xt, we propose an alternative mutation operator inspired by the reverse-time SDE: xchild = xparent + σtϵt, ϵt (0, I), (8) 8 where σt is the diffusion coefficient defined in reverse-time SDE, controlling the level of injected stochasticity. This mutation operation effectively generates novel xt, enabling exploration of an expanded state space while preserving the inherent distribution established during the denoising process. In the next generation of EvoSearch, we sample x0 p0(x0xchild ) based on the new offspring xchild , and repeat the above evolutionary search process, including evaluation, selection, and mutation. We highlight that EvoSearch provides unified framework that encompasses both best-of-N and particle sampling as special cases. Our method degenerates to best-of-N when setting = {xT }, and reduces to particle sampling upon elimination of both initial noise search and mutation operations. We refer to the pseudocode of EvoSearch in Alg. 1 and Alg. 2. t Algorithm 1 Overview of EvoSearch 1: Input: Pre-trained model pθ, population size schedule = {kstart, kT , , ktj , , ktn}, evolution schedule = {T, , tj, , tn} 2: Initialize population list = [ϕ for _ in ]. 3: Initialize reward list = [ϕ for _ in ] 4: Sample initial Gaussian noises xT with population size kstart 5: Initialize generation = 0 6: for = T, 1, , 1 do 7: 8: 9: 10: 11: 12: end for xt, P, = evosearch_at_denoising_states(pθ, xt, P, R, , K, g) + end if xt1 = denoise(pθ, xt, t) if in then // Alg"
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate the efficacy of EvoSearch through extensive experiments on large-scale text-conditioned generation tasks, encompassing both image and video domains."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "5.1.1 Image Generation. Tasks and Metrics. We adopt DrawBench [57] for evaluation, which consists of 200 prompts spanning 11 different categories. We utilize multiple metrics to evaluate generation quality, including ImageReward [85], HPSv2 [81], Aesthetic score [58], and ClipScore [28]. ImageReward and ClipScore are employed as guidance rewards during search. Please refer to evaluation details in Appendix A.2. Models. We employ two different text-to-image models to evaluate EvoSearch and baselines, which are Stable Diffusion 2.1 [56] and Flux.1-dev [38], respectively. SD2.1 is diffusion-based text-to-image model with 865M parameters, while Flux-dev is rectified flow-based model with 12B parameters. For both models, we use 50 denoising steps with guidance scale of 5.5, with other hyperparameters remaining as the default."
        },
        {
            "title": "5.1.2 Video Generation.",
            "content": "Tasks and Metrics. We take the recently released VideoReward [45] as the guidance reward to provide feedback during search. VideoReward, built on Qwen2-VL-2B [79], evaluates generated 9 Algorithm 2 EvoSearch at Denoising States 1: Input: Pre-trained model pθ, starting states xt, population list P, reward list R, evolution schedule = {T, , tj, , tn}, population size schedule = {kT , , ktj , , ktn}, generation g, elites size m. if in then end if xt1 = denoise(xt, t) P[idx] = cat(P[idx], xt) idx idx + 1 2: Set idx = 3: Set population size = K[g + 1] 4: for = t, 1, , 1 do 5: 6: 7: 8: 9: 10: end for 11: Calculate rewards via fully denoised x0 in Eq. (6) 12: for = g, , len(R) 1 do 13: R[i] = cat(R[i], r) 14: end for 15: Select elites = P[g] [topk(R[g], m)] 16: Select parents from P[g] via tournament selection [22] 17: if g=0 then 18: Mutate parents = (cid:112)1 β2 + ϵ β, ϵ (0, I) 19: else 20: Mutate parents = + σt ϵ, ϵ (0, I) // σt is the diffusion coefficient in the SDE denoising process 21: end if 22: Get children cat(e, p) 23: Output: Children c, P, videos on multiple dimensions: visual quality, motion quality, and text alignment. To measure the generalization performance to unseen rewards, we utilize both automatic metrics and human assessment for comprehensive evaluation. For automatic evaluation, we employ multiple metrics from VBench [31] and VBench2 [93], which encompass 625 distinct prompts distributed across six fundamental dimensions, including dynamic, semantic, human fidelity, composition, physics, and aesthetic. For human evaluation, we hire annotators to evaluate videos on 200 prompts sampled from VideoGen-Eval [90]. Evaluation details are in Appendix A.2. Models. To evaluate the scalability and performance of baselines, we utilize two widely adopted video generative models: HunyuanVideo [36] and Wan [78]. Given the computational intensity of video generation compared to image generation, we specifically use the 1.3B parameter variant of Wan for practical evaluation. Each video comprises 33 frames, with other hyperparameters following default configurations."
        },
        {
            "title": "5.1.3 Baselines.",
            "content": "As we evaluate the scalability of both diffusion and flow models across image and video generation tasks, we benchmark EvoSearch against two widely-used search methods that are applicable to our experimental settings: (i) Best of samples multiple random noises at beginning, assign reward 10 values to them via denoising and evaluation, and choose the candidate yielding the highest reward. (ii) Particle Sampling maintains set of candidates along the denoising process, called particles, and iteratively propagates high-reward samples while discarding lower-reward ones. Implementation details of EvoSearch and baselines are provided in Appendix A.1. To ensure fair comparison, we employ the same random seeds to generate videos for each method."
        },
        {
            "title": "5.2 Results Analysis",
            "content": "To evaluate EvoSearchs versatility and practical performance, we include image generation on diffusion model (SD2.1) and flow model (Flux.1-dev), video generation on flow models (HunyuanVideo and Wan) for comprehensive empirical analysis. Question 1. Can EvoSearch consistently yield performance improvement with scaled inference-time computation? We measure the inference time computation by the number of function evaluations (NFEs). As shown in Fig. 5, where we evaluate performance using both ImageReward and ClipScore, EvoSearch exhibits monotonic performance improvements with increasing inference-time computation. Notably, for the Flux.1-dev model (12B parameters), EvoSearch continues to demonstrate performance gains as NFEs increase, whereas baseline methods plateau after approximately 1e4 NFEs. Qualitative results in Fig. 1 show that both SD2.1 and Flux.1-dev generate images with progressively improved prompt alignment as inference computation (i.e., NFEs) increases. Question 2. How does EvoSearch compare to baselines for scaling image and video generation at inference time? Figure 5: Scaling behavior of EvoSearch and baselines as inference-time computation increases on DrawBench. Top: SD2.1. Bottom: Flux.1-dev. (a) and (b) use ImageReward and ClipScore as guidance rewards, respectively. (b) (a) For image generation tasks, as evidenced in Fig. 5 and Fig. 7, EvoSearch demonstrates consistent superior performance over all baseline methods across varying computational budgets, for both diffusion-based SD2.1 and flow-based Flux.1-dev models. For video generation tasks where VideoReward serves as the guidance reward, EvoSearch continues to obtain the highest score across different generative models compared to the baselines. Quantitative results in Fig. 6 (top row) show that for the Wan 1.3B model, EvoSearch outperforms bets-of-N and particle sampling by 32.8% and 14.1%, respectively. When applied to the larger HunyuanVideo 13B model, EvoSearch demonstrates improvements of 23.6% and 20.6% over best-of-N and particle sampling, respectively. Results on the prompts sample from Videogen-Eval [90], as illustrated in Fig. 6 (bottom row), further corroborate these findings, with EvoSearch showing improvements of 22.8% and 18.1% compared to best-of-N and particle sampling, respectively. Qualitative assessment in Fig. 8 reveals that only EvoSearch successfully generates images with both background consistency and accurate text prompt alignment. In contrast, particle sampling fails to comprehend the complex text prompt, while best-of-N produces results of inferior visual quality. More qualitative results are provided in Appendix B.2. The superior performance of EvoSearch can be attributed to its active exploration and refinement within the denoising state space, whereas best-of-N and particle sampling are limited to local candidate pool. 11 Figure 6: VideoRewards on VBench & VBench2.0 (top) and VideoGen-Eval [90] (bottom). Figure 7: EvoSearch can generalize to unseen metrics. Top row: DrawBench results on SD2.1. Bottom row: DrawBench results on Flux.1-dev. Table 1: Evaluation results across multiple metrics from both Vbench and VBench2.0. Methods Dynamic Semantic Wan 1.3B +Best of +Particle Sampling +EvoSearch (Ours) HunyuanVideo 13B +Best of +Particle Sampling +EvoSearch (Ours) 13.18 15.38 +2.2 13.18 +0.0 16.48 +3.3 8.79 6.59 2.2 6.59 2.2 7.69 1.1 16.83 13.67 3.16 12.67 4.16 15.51 1. 16.11 12.84 3.27 11.00 5.11 14.92 1.19 Human Fidelity 82.98 87.58 +4.6 86.13 +3.15 86.84 +3.86 90.28 91.31 +1.03 93.17 +2.89 94.63 +4.35 Composition Physics Aesthetic Average 38.08 44.71 +6.63 39.43 +1.35 51.57 +13.49 47.89 50.53 +2.64 36.67 11.22 51.37 3.48 64.44 56.10 8.34 56.41 8.03 57.5 6.9 56.10 47.62 8.48 54.29 1.81 61.54 +5. 64.01 64.84 +0.83 64.54 +0.53 64.35 +0.34 66.31 66.28 0.03 65.55 0.76 66.75 +0.44 46.59 47.04 +0.45 45.39 1.2 48.71 +2.12 47.58 45.86 1.72 44.55 3.03 49.48 +1.90 Question 3. How does EvoSearch generalize to unseen reward functions (metrics)? As demonstrated in recent work [48], reward hacking [63] can significantly impair test-time scaling performance, where the model exploits flaws or ambiguities in the reward function to obtain high rewards. Such mode-seeking behavior results in reduced population diversity and ultimately leads to mode collapse as the computation increases. However, our method, EvoSearch, can mitigate the reward hacking problem to some extent since it maintains higher diversity through the search process, effectively capturing multimodal modes from target distributions. We evaluate the generation performance on unseen (out-of-distribution) metrics in Fig. 7, where ClipScore is used as the guidance reward. EvoSearch still showcases superior scalability and performance across different models and metrics. For o.o.d. metric Aesthetic, which is not aligned with ClipScore (as Figure 8: qualitative example showing that EvoSearch generates videos with superior visual quality, enhanced background consistency, and improved semantic alignment with the input text prompts. 12 demonstrated in Fig. 8 of [48]), EvoSearch shows less performance degradation compared to particle sampling. For video generation tasks, we include 9 different unseen metrics spanning 6 main categories to evaluate EvoSearchs generalizability to unseen rewards. From the results shown in Table 1, we observe that EvoSearch consistently gains more stable performance improvements compared with baselines. Notably, even for metrics that are not aligned with VideoReward (e.g., Semantic), EvoSearch maintains robust performance with minimal degradation. For the physics metric on HunyuanVideo, EvoSearch even achieves distinctive performance improvements while both best-ofN and particle sampling exhibit significant degradation. Figure 9: Human evaluation results. Figure 10: For the same prompt, EvoSearch generates more visually diverse images. Question 4. How does EvoSearch perform under human evaluation? To validate EvoSearchs alignment with human preferences, we conduct comprehensive human evaluation study employing professional annotators. The assessment focused on four key dimensions: Visual Quality, Motion Quality, Text Alignment, and Overall Quality. As illustrated in Fig. 9, EvoSearch achieves higher win rates compared to baseline methods across all evaluation dimensions. Question 5. Can EvoSearch remains high diversity when maximizing guidance rewards? Method Reward Diversity Table 2: Results of reward and diversity. EvoSearch demonstrates superior capability in sampling diverse solutions through its continuous exploration of novel states during the search process. We randomly select 10 prompts from DrawBench, and generate 10 images per prompt using EvoSearch and baselines under 100 scaled inference-time compute. After generation, we evaluate the quality of the generated images by ImageReward, and evaluate the diversity of these images by the L2 distance between their corresponding hidden features extracted from the CLIP encoder. We observe in Table 2 that EvoSearch obtains the highest reward while achieving the highest diversity. Qualitative results in Fig. 10 further support this finding, revealing that EvoSearch generates text-aligned images with notably greater diversity in backgrounds and poses compared to baseline methods. Question 6. Can EvoSearch enable smaller-scale model outperform larger-scale model? Best of Particle Sampling EvoSearch (Ours) 0.16 0.13 0.18 0.62 0.94 1.34 Table 3: EvoSearch scales Wan 1.3B to have the same inference time as Wan 14B. Results are evaluated on 625 prompts from VBench and VBench2.0. In image generation tasks, as illustrated in Fig. 5, SD2.1 achieves superior performance compared to GPT4o with fewer than 5e3 NFEs, requiring only 30 seconds of inference time on single A800 GPU. Qualitative results presented in Fig. 1 further demonstrate how EvoSearch enables smaller models to exceed GPT4os capabilities Methods VideoReward Wan 14B Wan 1.3B + EvoSearch (ours) -1.24 -0.15 through strategic inference-time scaling. For video generation tasks, we allocate 5 inference computation to Wan 1.3B, ensuring equivalent inference time with Wan 14B on identical GPUs. Results documented in Table 3 show that the Wan 1.3B model with EvoSearch achieves competitive performance to its 10 larger counterpart, the Wan 14B model. These findings highlight the significant potential of test-time scaling as complement to traditional training-time scaling laws for visual generative models, opening new avenues for future research."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose Evolutionary Search (EvoSearch), novel, generalist and efficient testtime scaling framework for diffusion and flow models across image and video generation tasks. Through our proposed specialized evolutionary mechanisms, EvoSearch enables the generation of higher-quality samples iteratively by actively exploring new states along the denoising trajectory. Limitations and Future Work. EvoSearch has demonstrated significant effectiveness in exploring high-reward regions of novel states, which opens promising directions for future research. The exploration ability of EvoSearch relies on the strength of the mutation rate β and σt. higher mutation rate will effectively expand the search space to find high-quality candidates, while low mutation rate can restrict the exploration space, which represents trade-off. In addition, we rely on Gaussian noise to mutate the selected parents. While this approach provides robust exploration across diverse image and video generation tasks, developing more informative mutation strategies with prior knowledge can further improve the search efficiency. The inherent complexity of interpreting denoising states makes it an interesting open research question. Our findings also suggest promising future directions in understanding the shared structure between golden noises and intermediate denoising states, which may provide valuable insights for future test-time scaling research in the context of visual generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Jaewon Min, Minjae Kim, Wooseok Jang, Hyoungwon Cho, Sayak Paul, SeonHwa Kim, Eunju Cha, et al. noise is worth diffusion guidance. arXiv preprint arXiv:2412.03895, 2024. [3] Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [4] Ping Ao. Laws in darwinian evolutionary theory. Physics of life Reviews, 2(2):117156, 2005. [5] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 843852, 2023. [6] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 14 [8] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [9] Thomas Bäck. Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms. Oxford University Press, 02 1996. [10] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023. [11] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. [12] John Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Quoc Le, Sergey Levine, Honglak In International Lee, and Aleksandra Faust. Evolving reinforcement learning algorithms. Conference on Learning Representations, 2021. [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [14] Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. [15] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [17] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. ReNO: Enhancing one-step text-to-image models through reward-based noise optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [18] Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. In International Conference on Machine Learning, 2023. [19] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. [20] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. [21] David E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. AddisonWesley Longman Publishing Co., Inc., USA, 1st edition, 1989. [22] David E. Goldberg and Kalyanmoy Deb. comparative analysis of selection schemes used in genetic algorithms. volume 1 of Foundations of Genetic Algorithms, pages 6993. Elsevier, 1991. 15 [23] John Grefenstette. Genetic algorithms and machine learning. In Proceedings of the sixth annual conference on Computational learning theory, pages 34, 1993. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-to-image diffusion models via initial noise optimization. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93809389, 2024. [26] Yingqing Guo, Hui Yuan, Yukang Yang, Minshuo Chen, and Mengdi Wang. Gradient guidance for diffusion models: An optimization perspective. arXiv preprint arXiv:2404.14743, 2024. [27] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. [28] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [30] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [32] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [33] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [34] Jaihoon Kim, Taehoon Yoon, Jisung Hwang, and Minhyuk Sung. Inference-time scaling for flow models via stochastic generation and rollover budget forcing. arXiv preprint arXiv:2503.19385, 2025. [35] Sunwoo Kim, Minkyu Kim, and Dongmin Park. Test-time alignment of diffusion models without reward over-optimization. In The Thirteenth International Conference on Learning Representations, 2025. [36] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 16 [37] John R. Koza. Genetic programming: on the programming of computers by means of natural selection. MIT Press, Cambridge, MA, USA, 1992. [38] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [39] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [40] Jian-Ping Li, Xiao-Dong Li, and Alastair Wood. Species based evolutionary algorithms for multimodal optimization: brief review. In IEEE Congress on Evolutionary Computation, pages 18, 2010. [41] Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, and Shuiwang Ji. Dynamic search for inference-time alignment in diffusion models. ArXiv, abs/2503.02039, 2025. [42] Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024. [43] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2(3), 2024. [44] Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025. [45] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, and Wanli Ouyang. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. [46] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. [47] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [48] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [49] Yuta Oshima, Masahiro Suzuki, Yutaka Matsuo, and Hiroki Furuta. Inference-time text-to-video alignment with diffusion latent beam search. arXiv preprint arXiv:2501.19252, 2025. [50] Art Owen and Yi Zhou Associate and. Safe and effective importance sampling. Journal of the American Statistical Association, 95(449):135143, 2000. [51] Zeeshan Patel, James DeLoye, and Lance Mathias. Exploring diffusion and flow matching under generator matching. arXiv preprint arXiv:2412.11024, 2024. 17 [52] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients, 2024. [53] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [54] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 47804789, 2019. [55] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: Evolving machine learning algorithms from scratch. In International conference on machine learning, pages 80078019. PMLR, 2020. [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [59] Anuj Singh, Sayak Mukherjee, Ahmad Beirami, and Hadi Jamali Rad. Code: Blockwise control for denoising diffusion models. ArXiv, abs/2502.00968, 2025. [60] Saurabh Singh and Ian Fischer. Stochastic sampling from deterministic flow models. arXiv preprint arXiv:2410.02217, 2024. [61] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models, 2025. [62] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. [63] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. [64] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [65] David So, Quoc Le, and Chen Liang. The evolved transformer. In International conference on machine learning, pages 58775886. PMLR, 2019. [66] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [67] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023. [68] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 3248332498. PMLR, 2023. [69] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [70] Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, and Tsung-Hui Chang. Inference-time alignment of diffusion models with direct noise optimization. arXiv preprint arXiv:2405.18881, 2024. [71] Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, et al. survey on post-training of large language models. arXiv preprint arXiv:2503.06072, 2025. [72] Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based fine-tuning of diffusion models: tutorial and review. arXiv preprint arXiv:2407.13734, 2024. [73] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Finetuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194, 2024. [74] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [75] Pradnya A. Vikhar. Evolutionary algorithms: critical review and its future prospects. In 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC), pages 261265, 2016. [76] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. [77] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [78] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, 19 Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [79] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [80] Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. Advances in Neural Information Processing Systems, 36:3137231403, 2023. [81] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [82] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20962105, 2023. [83] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth International Conference on Learning Representations, 2025. [84] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [85] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [86] Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, Junjun He, Zongyuan Ge, and Imran Razzak. Scalingnoise: Scaling inference-time search for generating infinite videos. arXiv preprint arXiv:2503.16400, 2025. [87] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89418951, 2024. [88] Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, and Chang Xu. Cars: Continuous evolution for efficient neural architecture search. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18291838, 2020. [89] Po-Hung Yeh, Kuang-Huei Lee, and Jun-Cheng Chen. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. [90] Ailing Zeng, Yuhang Yang, Weidong Chen, and Wei Liu. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. [91] Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. 20 [92] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025. [93] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [94] Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, and Zeke Xie. Golden noise for diffusion models: learning framework. arXiv preprint arXiv:2411.09502, 2024."
        },
        {
            "title": "A Experimental Details",
            "content": "A."
        },
        {
            "title": "Implementation Details",
            "content": "A.1.1 Implementation Details of EvoSearch Evolution schedule . Evolution schedule can be flexibly defined based on the available amount of inference-time compute. If the inference-time computation budget is sufficient, we can perform EvoSearch at more timesteps; otherwise, we can deploy EvoSearch at several timesteps. In our implementation, we set to have uniform intervals. Population size schedule K. Population size = {kstart, kT , , ktj , , ktn}. can be flexibly defined based on the available amount of inference-time compute. We can increase population size as inference-time computation increases. In our implementation, we assign 2 larger population size at the first generation of EvoSearch, while keeping the population size at the remaining generations the same. This means that kstart is twice as large as the other population sizes. is defined as schedule Stable Diffusion 2.1. We set the guidance scale as 5.5, and set the resolution size as 512 512. We employ the DDIM scheduler from the diffusers library [76] for inference. We set the mutation rate β = 0.3, with σt following the default DDIM configurations. Flux.1-dev. We set guidance scale as 5.5, and set the resolution size as 512 512. We employ the sde-dpmsolver++ sampler in FlowDPMSolverMultistepScheduler [76] for inference in SDE process. We set the mutation rate β = 0.3, with σt following the default sde-dpmsolver configurations. Wan. Following the official codes [78], we set the resolution size as 832 480, with video consists of 33 frames. We set the guidance scale as 5.0. For transforming the ODE denoising process in Wan to SDE process, we leverage the sde-dpmsolver++ sampler in FlowDPMSolverMultistepScheduler [76] for inference. Hunyuan. Following the official implementation [36], we set the resolution size as 544 960 to ensure the generation quality, with video consisting of 33 frames. The guidance scale is set at 1.0 as suggested, and the embedded guidance scale is 6.0. For transforming the ODE denoising process in Wan to SDE process, we leverage the sde-dpmsolver++ sampler in FlowDPMSolverMultistepScheduler [76] for inference. To save computation for large number of experiments conducted in this paper, we set the inference steps to 30. We refer to the pseudocodes of EvoSearch in Alg. 1 and Alg. 2. At the beginning of EvoSearch, we denote the size of randomly sampled Gaussian noises as kstart. The implementation of EvoSearch is provided in the supplementary material, ensuring reproducibility. A.1."
        },
        {
            "title": "Implementation Details of Baselines",
            "content": "Best of N. Best of generates batch of candidate samples (images or videos), from which the highest-quality sample is selected according to predefined guidance reward function. In practice, we use the same guidance reward for EvoSearch and all baselines to ensure fair comparison. Particle Sampling. Particle-based sampling methods have demonstrated significant effectiveness in enhancing the generative performance of diffusion models during inference. For our implementation, we leverage the generalist particle-based sampling framework proposed by [61], utilizing 22 their publicly available codebase. Their approach introduces flexible methodology that accommodates diverse potential functions, sampling algorithms, and reward models, leading to improved performance across broad spectrum of text-to-image generation tasks. We adopt the Max potential schedule for resampling at intermediate states, which empirically demonstrated superior performance in the original study. Other hyperparameters, such as the resampling interval, are carefully tuned to establish robust baseline performance. A.2 Evaluation Metrics Image Evaluation Metrics. (i) ImageReward is text-to-image human preference reward model [85], which takes an image and its corresponding prompt as inputs and outputs preference score. (ii) CLIPScore is reference-free evaluation metric derived from the CLIP model [28], which aligns visual and textual embeddings in shared latent space. By computing the cosine similarity between an image embedding and its associated text prompt embedding, CLIPScore quantifies semantic coherence without requiring ground-truth images. (iii) HPSv2 is preference prediction model that reflects human perceptual preferences for text-to-image generation [81]. (iv) Aesthetic quantifies the visual appeal of images, often independent of text prompts [58]. Video Evaluation Metrics. (i) Dynamic evaluates models ability to follow complex prompts and simulate dynamic changes (i.e., color, size, lightness, and material). This evaluation metric includes prompts of Dynamic Attribute form VBench2.0. Scores are calculated following the original codes [93]. (ii) Semantic evaluates the models ability to follow long prompts, which involve at least 150 words. This evaluation metric includes the prompts of Complex Plot and Complex Landscape from VBench2.0. (iii) Human Fidelity evaluates both the structural correctness and temporal consistency of human figures in generated videos. This evaluation metric includes the prompts of Human Anatomy, Human Clothes, and Human Identities from VBench2.0. (iv) Composition evaluates the models ability to generate complex, impossible compositions beyond real-world constraints. This evaluation metric includes the prompts of Composition from VBench 2.0. (v) Physics evaluates whether models follow basic real-world physical principles (e.g., gravity). This evaluation metric includes the prompts of Mechanics from VBench2.0. (vi) Aesthetic evaluates the aesthetic values perceived by humans towards each video frame using the LAION aesthetic predictor [58]. This evaluation metric includes the prompts of Aesthetic Quality from VBench."
        },
        {
            "title": "B Additional Experimental Results",
            "content": "B.1 Ablation on Population Size Schedule To ablate the effect of population size schedules under the same inference-time computation budget, we set different population size schedules for the Stable Diffusion 2.1 model with approximately 140 50 inference-time NFEs. Here, 50 is the length of the denoising steps for each generation. We report the DrawBench results in Fig. 11. We observe that different population size schedules perform similarly with little reward difference. The most significant factor is the value of kstart, which represents the population size of the initial Gaussian noises. larger value of kstart benefits strong initialization for the subsequent search process, while small value of kstart would affect the performance lot. B.1.1 Ablation on Evolution Schedule We further ablate the effect of the evolution schedule. From the results shown in Fig. 12, we find that the evolution schedule exhibits less significant influence compared to the population size schedule 23 Figure 11: Ablation study on the population size schedule K. We denote the population size schedule = {kstart, kT , , ktj , , ktn}, where kstart is the size of the initial sampled Gaussian noises. We use Stable Diffusion 2.1 to conduct EvoSearch on DrawBench, employing ImageReward as the guidance reward function during search, and the denoising step is 50. From left to right of the x-axis, the population size schedule is configured as: 0) {60, 40, 50}; 1) {70, 30, 50}; 2) {80, 20, 50}; 3) {62, 62, 20}; 4){58, 58, 30}; 5) {54, 54, 40}; 6) {46, 46, 60};7) {40, 60, 50}; 8) {30, 70, 50}; 9) {20, 80, 50}, where we maintain the evolution schedule as {50, 40}. K. Our analysis demonstrates that an evolution schedule with uniform intervals yields superior performance. Additionally, larger initial population sizes kstart help increase the performance. Figure 12: Ablation study on the evolution schedule . We use Stable Diffusion 2.1 to conduct EvoSearch on the DrawBench, employing ImageReward as the guidance reward function during search. We denote the evolution schedule = {T, , tj, , tn}. From left to right of the x-axis, the evolution schedule is 0) {50, 30}; 1) {50, 20}; 2) {50, 10}; 3) {50, 30}; 4) {50, 20}; 5) {50, 10}. To keep the same test-time scaling computation budget across different evolution schedules, each population size schedule is adjusted as 0) {60, 50, 50}; 1) {70, 50, 50}; 2) {80, 50, 50}; 3) {55, 55, 50}; 4) {60, 60, 50}; 5) {75, 75, 50}. 24 Figure 13: EvoSearch can generalize to unseen metrics, where ImageReward is set as the guidance reward function during search. Top row: DrawBench results on SD2.1. Bottom row: DrawBench results on Flux.1-dev. B.2 Qualitative Results We present extensive qualitative results for both image and video generation as follows. B.2.1 Results for Image Generation Please refer to Fig. 14, Fig. 15, and Fig. 16 for comparison between EvoSearch and baselines. These examples clearly demonstrate that EvoSearch significantly enhances image generation performance while requiring lower computational resources. B.2.2 Results for Video Generation Please refer to Fig. 17, Fig. 18, Fig. 19, and Fig. 20 for comparison between EvoSearch and baselines in the context of video generation. We find that EvoSearch outperforms all the baseline with higher efficacy and efficiency. Please refer to Fig. 21, Fig. 22, Fig. 23, Fig. 24, Fig. 25, Fig. 26, and Fig. 27 for comparison between Wan14B and Wan1.3B enhanced with EvoSearch. The results demonstrate that by increasing the test-time computation budget of Wan1.3B to match the inference latency of Wan14B, the smaller model outperforms its 10 larger counterpart across diverse range of input prompts. 25 Figure 14: Comparative analysis of test-time scaling methods for Stable Diffusion 2.1. EvoSearch demonstrates consistent improvements in image quality and text-prompt alignment as NFEs increase, achieving accurate interpretations of the challenging prompt with high computational efficiency. In contrast, Best-of-N fails to produce semantically correct results even with increased NFEs, while Particle Sampling introduces semantic ambiguity at higher NFEs (e.g., confusing wine glasses and eyeglasses). Notably, EvoSearch further enables SD2.1 to outperform GPT4o. 26 Figure 15: Results of test-time scaling for Flux.1-dev. EvoSearch demonstrates significant exploration ability, enabling the generation of images with diverse styles, while both Best-of-N and Particle Sampling generate images with reduced diversity. 27 Figure 16: Results of test-time scaling for Flux.1-dev. EvoSearch can even achieve accurate spatial relationship interpretation with only 10 scaled computation budget, while consistently improving image quality through higher NFEs. 28 Figure 17: Results of test-time scaling for Hunyuan 13B. The denoising step is 30, and we scale up the test-time computation by 20. Only EvoSearch generates high-quality video aligned closely with the text prompt. Figure 18: Results of test-time scaling for Hunyuan 13B. The denoising step is 30, and we scale up the test-time computation by 20. EvoSearch successfully follows the text prompt while the baselines fail. 29 Figure 19: Results of test-time scaling for Hunyuan 13B. The denoising step is 30, and we scale up the test-time computation by 20. EvoSearch demonstrates superior text alignment and higher-quality generation compared to baselines. Figure 20: Results of test-time scaling for Hunyuan 13B. The denoising step is 30, and we scale up the test-time computation by 20. The video generated by EvoSearch demonstrates better temporal consistency and text alignment. 30 Figure 21: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. Qualitative results demonstrate that EvoSearch enables Wan1.3B to outperform Wan14B, its 10 larger counterpart. 31 Figure 22: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. EvoSearch enables smaller models to achieve not only competitive but superior performance compared to their larger counterparts. 32 Figure 23: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. EvoSearch demonstrate superior text-alignment performance. 33 Figure 24: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. EvoSearch enhances Wan1.3Bs capability in dynamic-attribute video generation. 34 Figure 25: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. EvoSearch enhances Wan1.3Bs capability in handling challenging prompts, outperforming Wan14B given the same inference time. 35 Figure 26: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. The video generated by EvoSearch follows the text instruction more closely, exhibiting improved logical consistency. 36 Figure 27: We scale up the test-time computation of Wan1.3B by 5, ensuring equivalent inference times between Wan14B and Wan1.3B+EvoSearch. EvoSearchsignificantly improves the generation quality with superior semantic alignment."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Kuaishou Technology"
    ]
}