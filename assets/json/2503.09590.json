{
    "paper_title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
    "authors": [
        "Md Mohaiminul Islam",
        "Tushar Nagarajan",
        "Huiyu Wang",
        "Gedas Bertasius",
        "Lorenzo Torresani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 0 9 5 9 0 . 3 0 5 2 : r BIMBA: Selective-Scan Compression for Long-Range Video Question Answering Md Mohaiminul Islam1* Tushar Nagarajan2 Huiyu Wang2 Gedas Bertasius1 Lorenzo Torresani2 1UNC Chapel Hill 2Meta AI https://sites.google.com/view/bimba-mllm"
        },
        {
            "title": "Abstract",
            "content": "Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides general solution for sequence modeling, but it has prohibitive cost when applied to massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient statespace model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple longform VQA benchmarks, including PerceptionTest, NExTQA, EgoSchema, VNBench, LongVideoBench, and VideoMME. Code, and models are publicly available at https: //sites.google.com/view/bimba-mllm. 1. Introduction Large Language Models (LLMs) [1, 5, 11, 15, 31, 54, 55, 65] have revolutionized the field of artificial intelligence and produced significant changes in various fields. Building on LLMs, Multimodal Large Language Models (MLLMs) have recently enabled significant gains for image [2, 4, 9, 13, 25, 3537, 44, 46, 53, 78, 89] and video understanding problems, typically reformulated as language generation from video [7, 39, 40, 43, 47, 51, 61, 69, 82]. However, most video MLLMs have been applied to short *Work done during an internship at Meta. video inputs spanning few seconds. In this case, simple but effective strategy involves extracting tokens from the individual frames of the video using pretrained image encoder to produce outputs aligned to the space of an LLM [35, 36, 46, 53]. The concatenation of the tokens extracted from the frames is then passed to the LLM for semantic processing and language generation [3, 36, 66]. This strategy breaks down when the video is very long, e.g., when it spans minutes or hours, because of the quadratic cost of the self-attention operation needed to process the extracted visual tokens. In addition, naive concatenation of frame-level tokens for long video would flood the LLM with an exceedingly long and redundant sequence of features. For example, the LLAMA-3.2 [53] image encoder outputs 1600 6400 tokens per image. Therefore, encoding just 128 frames would produce sequence of 205K 820K tokens, which is impractical to process even by modern GPUs. To address these issues, most long-form video MLLMs adopt compression techniques to reduce the number of visual tokens. For example, the methods in [51, 76] apply simple spatial/temporal pooling to frame-level tokens to shorten the sequence passed to the LLM. However, the pooling operation discards important spatiotemporal information. An alternative strategy is to use convolution-based modules to simultaneously perform sequence compression and temporal modeling [43, 48]. However, convolutionbased models lack long-range modeling ability since convolutional kernels can only capture local short-range dependencies. Recent work [82] reduces the number of output visual tokens by applying self-attention with smaller number of query keys [30]. However, these systems trade efficiency at the expense of cross-frame analysis, which is essential for long video understanding. In this work, we propose Bidirectional Interleaved Mamba for Better Answers (BIMBA), which provides an efficient and effective alternative for compressing long videos into short, information-rich sequence of tokens for video question-answering by LLMs. Our system is inspired by the success of state-space models (SSMs) [19 1 23] for processing long documents in NLP [18]. SSMs inherently possess long-term modeling ability while requiring linear computation costs with sequence length instead of the quadratic cost of self-attention. In particular, Mamba [18] further improved SSMs with selection mechanism that allows the model to select relevant information in an inputdependent manner. We design video architecture based around the Mamba module to compress the sequence of spatiotemporal tokens by more than one order of magnitude (e.g., from 102K tokens to 6.4k tokens) while retaining essential long-range dependencies. The selective-scan mechanism [18] of our token selector module allows the model to selectively propagate important tokens and discard redundant ones from the highly redundant video content typically present in long videos. Although SSM and Mamba models have been used before for image analysis [8, 57, 57, 90], image-language modeling [26, 34, 59, 86] and video understanding [6, 27, 28, 41, 50, 77], our proposed architecture differs from these prior works by introducing few simple but effective contributions. First, we introduce an SSM-based compression module called spatiotemporal token selector, which can take large number of spatiotemporal video tokens as input and output significantly smaller number of compressed tokens only containing important information by utilizing the selective-scan layers inside the module. We achieve this by introducing smaller number of visual queries and jointly modeling them with the spatiotemporal video tokens. Second, we propose an improved method for concatenating visual and spatiotemporal tokens by interleaving them at equal intervals. The standard approach of appending tokens at the end can introduce positional bias, where end-sequence tokens disproportionately influence query tokens. Our interleaved positioning mitigates this bias, influencing queries more uniformly across the entire video sequence. Finally, we use bidirectional selective-scan mechanism [41] that is more effective in capturing 2D/3D spatiotemporal structures in video compared to the standard selective scan, which is better suited for 1D sequence modeling. We experiment on several long video question answering datasets including PerceptionTest [56], NExT-QA [73], EgoSchema [52], VNBench [87], LongVideoBench [72], Video-MME [16], and show state-of-the-art results on all of these datasets. 2. Related Work 2.1. Video Multi-Modal Large Language Models Recently, we have witnessed many advances in video MLLMs. Video LMMs process videos by encoding video frames using image or video encoders and passing the extracted features to the LLMs. Unlike images, videos usually generate large number of frame-based features (tokens), which causes computation and context limitations for LLMs. Therefore, many existing models can only process limited number of video frames [3, 10, 35, 39, 84]. Other methods utilize compression techniques to reduce the number of tokens before passing them to the LLMs. For example, Video-ChatGPT employs pooling modules to reduce data dimensions, MovieChat [64] and ChatUniVi [32] adopt memory-based mechanisms, LLaMAVID [42] learns condensed representations using dual tokens. Most existing token compression techniques are based on pooling, perceiver, or Qformer operations. These operations lack the ability to perform long-range modeling and capture effective information from long input sequences. In contrast, we introduce Mamba-based token compression method that efficiently models long-range spatiotemporal dependencies, selectively capturing essential information while discarding redundancies to create more effective compressed video representation. 2.2. State Space Models Inspired by classical state space theory [33], state space models (SSMs) have been proposed for building deep learning models that efficiently handle long-sequence data [19 21]. Afterward, several methods have been proposed for long-sequence modeling using SSMs, including Structured State Space (S4) Models [21] and Diagonal State Space (DSS)[23]. Recently, Mamba [18] has advanced S4 by adding time-varying, input-dependent selection mechanism, providing efficient scaling for NLP and achieving transformer-level performance in long-sequence tasks. Furthermore, SSMs have been successfully applied to several image [8, 57, 57, 90] and video [6, 27, 28, 41, 50, 77] analysis tasks, as well as to image MLLMs [59, 86]. Inspired by this work, we propose to leverage SSMs/Mamba to develop effective long-form video MLLMs. 3. Background: Mamba Model The Mamba model (also known as Selective Scan Structured State Space or S6) [18] is recently introduced sequence-to-sequence model that offers distinct advantages over existing approaches like convolutional networks (CNNs), recurrent networks (RNNs), and self-attention models (Transformers). Traditional models, such as convolution and recurrence, are computationally efficient but struggle to capture long-range dependencies within sequences. In contrast, self-attention excels at modeling longrange dependencies but at much higher computational cost, which scales quadratically with sequence length. The S6 model combines the best of both worlds: it has the capacity to model long-range dependencies in sequences while maintaining computational efficiency, achieving linear computational cost and memory usage. 2 Figure 1. Our proposed BIMBA model uses Mamba-based Spatiotemporal Token Selector to select reduced number of salient tokens from long sequence of features extracted via pretrained image encoder. The token selection is optionally conditioned using the textual query to identify the features that are most informative for answering given question. Finally, the selected and transformed tokens are passed to large language model with tokenized version of the input question to generate the answer. Theoretically, the State-Space Model (SSM) represents continuous, time-invariant system that maps an input signal x(t) RL to output y(t) RM through hidden state h(t) RN . Mathematically, SSMs can be described using set of linear ordinary differential equations (ODEs): h(t) = Ah(t) + Bx(t), y(t) = Ch(t) + Dh(t). (1) Here, RN represents the state matrix of the system, RN , RN are projection matrices, and R1 is the skip connection for state size . To apply SSMs to real-world data, the continuous ODE (1) is first discretized using the following equation: ht = Ahk1 + Bxk, yt = Chk + Dxk. (2) Using this formulation, an efficient implementation of SSM, called Structured State Space (S4), has been proposed [21]. S4 represents the state matrix as diagonal and low-rank, which facilitates efficient computation. The S4 model is Linear Time-Invariant (LTI) system with parameters that remain constant and are independent of the input. This can be suboptimal, especially for modeling long-term dependencies in sequence containing redundant and unimportant information. Recent work [18] proposes Selective Scan State Space (S6) to develop Mamba LLM [18]. In S6, the matrices B, C, and are made input dependent and derived from input using linear layers as follows. = LinearN (x) = LinearN (x) (3) Such selective-scan approach allows the parameters to interact dynamically with the input across the sequence, enabling the model to retain important information and filter 3 out redundant data. As result, S6 models are better suited for applications that require efficient information compression, such as ours. 4. Technical Approach We present BIMBA, multimodal large language model (MLLM) designed for long-range video question answering. Figure 1 illustrates the overall architecture. Our model consists of standard image encoder [14], spatiotemporal token selector, and an LLM decoder [53]. First, we encode each input video frame independently using the image encoder and extract patch-level features (tokens) from each frame. Then, we use spatiotemporal token selector, which reduces the sequence of spatiotemporal input tokens by more than an order of magnitude (e.g., 16) for efficient processing by the subsequent LLM. Specifically, we utilize the selective-scan structured state-space model [18] for designing our token selector. Our spatiotemporal token selector efficiently models the long-range spatiotemporal dependencies in the input video and selects only the most relevant tokens from the vast input set. Finally, the compressed token sequence is passed to the LLM along with the language question to generate the response. In the following subsections, we provide detailed discussion of each component of our model. 4.1. Image Encoder We use standard vision transformer [14] to encode each video frame independently. Let = (V1, ..., Vt, ..., VT ) RT 3HW be the input video consisting of frames Vt R3HW where the 3 channels encode color in RGB format, is the height, and is the width of each frame. First, we divide each frame into non-overlapping patches of size (p p). Then, an image encoder fI is applied to extract Figure 2. (a): Architecture of our Spatiotemporal Token Selector. (b): Traditional selective scan with queries appended at the sequences start or end introduces positional biases that often lead to suboptimal performance. (c) We propose to interleave the queries uniformly to capture interactions between spatiotemporal tokens across the video more evenly. (d) Furthermore, we introduce bidirectional selective scan (forward and backward) operation to improve the long-range modeling further. spatial features (tokens) zt Rhwd from each frame Vt: zt = fI (Vt). (4) where = H/p, = W/p, and is the feature dimension. Afterward, the features of each frame are concatenated to produce the spatiotemporal token sequence RT hwd. We denote the total number of tokens by = w. 4.2. Spatiotemporal Token Selector The spatiotemporal token selector has two main objectives: (1) capturing long-range dependencies from the sequence of space-time tokens produced by the image encoder and (2) selecting the most relevant tokens from the highly redundant information typical in long videos. This task is challenging due to the large number of space-time tokens generated by the image encoder throughout the video. Using conventional self-attention would be computationally prohibitive due to its quadratic cost relative to the sequence length. To address this problem, we use selective scanbased token selector [18], which (1) efficiently models longrange temporal dependencies with linear computational cost and (2) filters out redundant tokens, retaining only the most relevant ones from the long input sequence. Figure 2(a) illustrates our spatiotemporal token selector. First, we initialize sequence of visual queries RT hwd using an adaptive 3D average pooling layer applied to spatiotemporal tokens Z. Here, = w is the number of queries, which is significantly smaller than the number of input spatiotemporal tokens = w, i.e., << L. In our implementation, the input consists of 64 40 40 = 102, 400 tokens, while the number of queries is 16 20 20 = 6, 400. Thus, the selector applies 16 compression ratio. Although the pooling operation provides good initialization for visual queries, it does not capture long-range spatiotemporal dependencies or fine-grained details from the input video. To address this gap, we apply selectivescan mechanism. First, we concatenate the visual queries with the spatiotemporal tokens Z, producing combined token sequence RLd, where = + . We then apply layer normalization to followed by selective-scan layer to capture long-range dependencies and critical finegrained information from input spatiotemporal tokens. residual connection is added after the selective-scan layer. Finally, the output queries are extracted from the combined token sequence and passed to the LLM. These operations are expressed using the following equations: = [Z; Q], Zres = Z = Selective-Scan(LN(Z)) = Zres + Q = Extract(Z) (5) Furthermore, our token selector module incorporates simple but effective design adaptations to make it suitable for processing long video inputs, as illustrated in Figure 2(c,d) and described next. 4.2.1. Interleaved Queries straightforward way to concatenate queries with the space-time tokens is to append the queries at the end, as shown in Figure 2(b). However, this approach can introduce biases as it positions the queries closer to tokens from the later portion of the video, potentially limiting interaction with earlier frames and thus missing context from the beginning of the video. To address this, we interleave the queries among the spatiotemporal tokens at regular intervals, as shown in Figure 2(c). By evenly distributing the queries across the sequence, our design enables interaction with tokens from all parts of the video, supporting more balanced and comprehensive representation. 4 4.2.2. Bidirectional Scan 5.3. Our Model Variants The original selective scan developed for 1D sequences is suboptimal for vision tasks as it lacks spatial awareness. To address this limitation, we apply bidirectional scan (Figure 2(c)) [41], which performs forward-and-backward pass through the visual tokens. This forward-and-backward scan through the visual tokens enhances the models ability to capture spatiotemporal structure, improving its effectiveness for video modeling tasks. 4.2.3. Question-Conditioned Token Selection We also explore variant of the token selector where we prepend the extracted tokens from the textual question to the spatio-temporal tokens before passing them to the token selector. This variant enhances the models ability to select the most relevant tokens by taking into account the context provided by the question. The operation is expressed as: = [X; Z; Q], (6) where represents the question tokenized by the LLM. Then, the combined token sequence is passed to the token selector as described in Section 4.2. 4.3. LLM Decoder We use an LLM decoder fL to generate textual answer from the concatenation of the tokenized input question and the set of visual queries generated by the spatiotemporal token selector: = fL([X; Q]). (7) 5. Experimental Setup 5.1. Training Data For our default models, we trained our model on 370K instruction tuning videos aggregated from collection of including YouCook2 [88], Ego4D-HCap [29], datasets, NExT-QA [74], IntentQA [38], CLEVRER [79], Ego4D [17], STAR [71], and Perception Test [58]. We did not use any data generated by ChatGPT or GPT-4, in accordance with the OpenAI terms of use1 and our internal legal policy. In addition, to speed up ablation experiments, we construct smaller instruction tuning training set of 70K videos from NExT-QA [74], IntentQA [38], Ego4D [17], and Ego4D-HCap [29]. The ablations are evaluated on NExT-QA [74] and EgoSchema [52]. 5.2. Evaluation Benchmarks We evaluate our model on six diverse video questionanswering [56], [52], VNBench [87], NExT-QA [74], EgoSchema LongVideoBench [72], and Video-MME [16]. benchmarks:"
        },
        {
            "title": "Test",
            "content": "1https://openai.com/policies/row-terms-of-use 5 BIMBA-LLaVA. We implement this variant by applying BIMBA on the image-pretrained MLLM LLaVANeXT [45], which uses CLIP [60] vision encoder and Vicuna-7B [11] LLM. BIMBA-LLaMA. We implement this variant by applying BIMBA on the image-pretrained MLLM LLaMA3.2 [53], which uses Meta-CLIP [75] vision encoder and LLaMA-3.2-LLM-8B LLM. 5.4. Baselines In addition to comparing with prior methods that were based on different MLLMs and trained on different data, we implemented the following baselines for fair comparison. Vanilla. We implement this baseline by removing the spatiotemporal token selector from our model, resulting in no token compression. All tokens generated by the image encoder are passed directly to the LLM. Pooling. This baseline applies spatiotemporal pooling as the compression method, using the same compression ratio as our main model. Self-Attention. We implement this baseline by replacing the selective-scan layer of our spatiotemporal token selector with self-attention layers. Perceiver. This baseline leverages the Perceiver [30] mechanism to compress spatiotemporal tokens and uses the same compression ratio as our model. 5.5. Implementation Details We use simple strategy to train both our models and baselines: starting from an image-pretrained frame encoder, we fine-tune the MLLM on the video instructiontuning dataset. We freeze the image encoder and train the spatiotemporal token selector and the LLM by applying LoRA [24] to the LLM. By default, the BIMBALLaVA model takes 64 video frames divided into 64 24 24 = 36, 864 tokens and compresses this sequence to 16 12 12 = 2, 304 tokens which are passed to the LLM. By default, BIMBA-LLaMA processes 64 frames divided into 64 40 40 = 102, 400 tokens and compresses this sequence to 16 20 20 = 6, 400 tokens for LLM. 6. Results and Analysis First, we analyze various aspects of our model, including comparison with different compression methods (Section 6.1), architectural design (Section 6.2), and questionconditioned token selection (Section 6.3). Lastly, we compare with state-of-the-art video MLLMs in Section 6.4. 6.1. Comparison of Compression Methods We compare BIMBA with other compression techniques and baseline that uses all tokens without compression. 74 72 68 66 64 62 60 r A 58 55 52 49 46 a c BIMBA Self-Attention Perceiver Pooling Vanilla BIMBA Self-Attention Perceiver Pooling Vanilla 5K 10K 15K 20K 25K 30K 35K 5K 10K 15K 20K 25K 30K 35K (a) NExT-QA performance of models based on LLaVA. (b) EgoSchema performance of models based on LLaVA. Tokens Tokens r A 75 71 69 67 r A 61 55 52 49 46 BIMBA Self-Attention Perceiver Pooling Vanilla BIMBA Self-Attention Perceiver Pooling Vanilla 20K 40K 60K 80K 100K 20K 40K 60K 80K 100K Tokens Tokens (c) NExT-QA performance of models based on LLaMA. (d) EgoSchema performance of models based on the LLaMA. Figure 3. Accuracy achieved by BIMBA and baseline models on NeXT-QA (left) and EgoSchema (right) as function of the number of input tokens for models based on LLaVA (top row) and LLaMA (bottom row). BIMBA achieves the highest accuracy for all sequence lengths, and the difference with other baselines increases as we increase the number of input tokens. Self-attention cannot be applied to long sequences as it causes GPU out-of-memory issues once the number of tokens becomes too large. Analysis of accuracy. We begin by analyzing the accuracy of the different models as function of the number of input spatiotemporal tokens. Specifically, we vary the number of frames sampled from the video to produce input token sequences of varying lengths. We experiment with 1, 4, 8, 16, 32, and 64 frames, resulting in 576, 2,304, 9,216, 18,432, and 36,864 input tokens for models based on LLaVA, and 1,600, 6,400, 12,800, 25,600, 51,200, and 102,400 input tokens for models based on LLaMA. While the Vanilla method generates number of output tokens equal to the length of the input sequence, we apply the compression methods in this comparison using compression of 1 2 2 in spatiotemporal (T ) dimensions up to 16 frames and 222 for inputs of 32 frames and 422 for sequences of 64 frames. Figure 3 shows accuracy for LLaVA-based models (top row) and LLaMA-based models (bottom row) on NeXTQA (left) and EgoSchema (right). First, our results show that the Vanilla method performs significantly worse than our BIMBA, underscoring the importance of spatiotemporal token compression for video question-answering. Second, we observe that as the number of input tokens increases, the performance of the pooling method saturates. For pooling models, performance even declines beyond 16 frames, indicating that the pooling compression strategy struggles to capture long-range spatiotemporal dependencies when many frames are given as input. Third, while self-attention performs similarly to BIMBA for short input sequences (up to 16 frames or 9,216 tokens for BIMBA-LLaVA and 8 frames or 12,800 tokens for BIMBA-LLaMA), it runs out of GPU memory beyond that. In comparison, our model consistently outperforms the widely used compression mechanism Perceiver [30] and other methods, achieving consistently higher accuracy for all token sequence lengths with monotonically increasing performance as more frames are added to the input. It yields the highest performance at 36,864 tokens for BIMBA-LLaVA and 102,400 tokens for BIMBA-LLaMA. This demonstrates the effectiveness of our token compression mechanism for long-range video understanding. Analysis of computational cost. In this section, we compare the computational cost of our model against baselines in terms of GPU memory usage (Figure 4 (left)) and runtime (Figure 4 (right)). We report results for LLaVA-based models, while the supplementary material provides an analysis of LLaMA-based models. As shown in Figure 4 (left), both the Vanilla method 6 60 40 ) ( m BIMBA Self-Attention Perceiver Pooling Vanilla 5K 10K 15K 20K 25K 30K 35K Tokens ) ( t 4 2 1 0 BIMBA Self-Attention Perceiver Pooling Vanilla 5K 10K 15K 20K 25K 30K 35K Tokens (a) Memory Usage of Models based on LLaVA. (b) Runtime of models based on LLaVA. Figure 4. Computation cost of BIMBA and baseline models in terms of memory usage (left) and runtime (right). All models are based on LLaVA. Models based on self-attention or that do not perform compression (Vanilla) run quickly out of memory as the number of input tokens is increased. The runtime of BIMBA grows gracefully as function of the input sequence length, unlike for the case of Vanilla."
        },
        {
            "title": "Learnable\nAverage\nAverage\nAverage\nAverage",
            "content": "67.67 68.61 69.85 71.68 73.57 68.91 70.56 71.16 73.23 75.57 Table 1. Ablations showing the effects of our design choices on NExT-QA. Average pooling initialization of the queries, layer normalization before token selection, bidirectional selective scan, and query interleaving elevate significantly the accuracy of BIMBA. and self-attention exhibit sharp increase in memory usage as the input length increases, leading to out-of-memory (OOM) errors for sequences longer than 16 frames (9,216 tokens). Pooling consumes the least GPU memory but, as previously observed, results in the lowest accuracy. In contrast, our method maintains low memory usage across all input lengths while achieving the highest accuracy. Figure 4 (right) presents runtime comparisons. We can observe that the runtime of the Vanilla method grows dramatically with the input length. Self-attention also incurs higher runtime cost, and we can only experiment with sequences up to 16 frames or 9,216 tokens. Although pooling and Perceiver maintain low memory and runtime costs similar to our method, their accuracy is significantly lower. Finally, our method has runtime cost nearly as low as pooling and can efficiently process sequences up to 64 frames or 36,864 tokens. 6.2. Ablation on Architecture Design In this section, we analyze several key architectural design choices that contribute to the effectiveness of our model. We present the results on the NExT-QA dataset in Table 1. First, we observe that initializing visual queries using 3D pooling layer, as described in Section 4.2, outperforms random initialization, yielding accuracy improvements of +0.94% and +1.65% for BIMBA-LLaVA and BIMBALLaMA, respectively. Second, incorporating layer normalization before the spatiotemporal token selector enhances performance (see row 2 vs. row 3). Third, using bidirec-"
        },
        {
            "title": "Question\nConditioning",
            "content": "BIMBA-LLaVA BIMBA-LLaMA"
        },
        {
            "title": "NextQA EgoSchema NextQA EgoSchema",
            "content": "73.57 74.77 58.40 60.51 75.57 76.61 62.20 64.33 Table 2. Question conditioning improves the accuracy of BIMBA on both NExT-QA and EgoSchema. tional selective scan improves accuracy over the standard selective scan by +1.83% and +2.07%, validating our hypothesis that the standard selective scan, originally developed for NLP, is suboptimal for vision tasks. Finally, interleaving queries among spatiotemporal tokens yields better performance than the standard approach (+1.89% and +2.34%) since this design allows the model to capture longrange temporal dependencies in video data more effectively. 6.3. Question-Conditioned Token Selection This section analyzes the impact of conditioning our spatiotemporal token selector with information extracted from the textual question. As shown in Table 2, adding the question as input to the token selector improves the accuracy by 1.04% on NExT-QA and by 2.13% on EgoSchema for the BIMBA-LLaMA model. We also observe similar improvements for adding question-conditioning to BIMBALLaVA This enhancement demonstrates that including question tokens enables our selective-scan mechanism to focus on content specifically related to the question, allowing it to choose more relevant spatiotemporal tokens from the input video. We do not include this question-conditioned token selection technique in our main BIMBA model because we expect BIMBA to answer multiple questions about video efficiently based on one sequence of selected tokens without recomputing them for each question. 6.4. Comparison with the State of the Art In this section, we compare our model against state-ofthe-art video MLLMs on six different video benchmarks, summarizing the results in Table 3. Since prior models 7 LLM Frames PerceptionTest NExT-QA EgoSchema VNBench LongVideoBench Video-MME 1 - 60 min 8 sec - 60 min 180 sec 180 sec 23 sec 44 sec (Based on Proprietary LLMs) Model Duration LLoVi [81] VideoAgent [68] VideoTree [70] Vicuna-7B LLaVA-NeXT (Video) [45] Vicuna-7B PLLaVA [76] BIMBA-LLaVA Vicuna-7B LLaMA-3.2 (Video) [53] LLaMA3.2-8B PLLaVA (LLaMA-3.2) [53] LLaMA3.2-8B BIMBA-LLaMA LLaMA3.2-8B GPT3.5 GPT4 GPT - - - - - - 66.30 71.30 75.60 52.00 54.10 61.10 - - - (Open-source MLLMs Trained on Our 370K Video Instruction Data) - - - - - - (Open-source MLLMs Trained on Larger Scale Instruction Video Data) 41.66 43.36 52.31 55.21 52.33 60. 46.13 48.55 52.61 53.16 53.13 56.50 67.66 67.56 72.35 73.72 72.77 76.88 64 64 64 64 64 64 LLaMA-VID [42] Video-LLaVA [43] LLaVA-NeXT-Video [84] PLLaVA [76] VideoChat2 [40] LongVA [83] Video-LLaMA2 [10] LLaVA-OneVision [35] Kangaroo [48] Video-XL [63] LongVU [62] Qwen2-VL [67] LLaVA-Video [85] BIMBA-LLaVA Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Qwen2-7B Qwen2-7B Qwen2-7B LLaMA2-8B Qwen2-7B Qwen2-7B Qwen2-7B Qwen2-7B Qwen2-7B 1fps 8 32 16 16 128 32 32 64 2048 1fps 2fps 64 44.60 44.30 48.80 - 47.30 - 51.40 57.10 - - - - 67.90 68.51 - 62.60 - 68.17 - 68.30 - 79.40 - - - - 83.20 83.73 38.50 38.40 43.90 45.16 54.40 - 51.70 60.10 62.70 - 67.60 - 57.30 71.14 10.80 - 20.10 - 12.40 41.50 24.90 51.80 - 61.60 - - 70.77 77.88 - - - - - - - - - - - 43.50 40.20 36.00 - - 56.5 - 49.50 - 55.60 58.20 59.46 - - - 42.21 42.13 45.66 47.31 46.56 50.11 - 40.40 46.50 44.25 47.90 54.30 47.90 58.20 56.00 55.50 60.60 63.30 63.30 64.67 Table 3. We compare BIMBA with state-of-the-art video MLLMs across six diverse video question-answering benchmarks. BIMBALLaVA achieves the highest performance on all datasets when using the Qwen2-7B LLM backbone (third section). Since different MLLMs leverage varying LLM backbones and training data, we also conduct fair comparison by evaluating our model against four baselines trained using the same 370K instruction-tuning dataset and using Vicuna-7B and LLaMA3.2-8B LLM decoders (second section). In this evaluation setting, we observe that our model consistently outperforms all baselines across all benchmarks. rely on varying backbones and training datasets, to ensure fair comparison, we first fine-tune four competitive models, as well as the two default variants of our BIMBA model on our 370K instruction-tuning dataset. These include PLLaVA [76], LLaVA-NeXT (Video) [45], LLaMA-3.2 [53] (Video) and PLLaVA (LLaMA-3.2), variant of PLLaVA using the stronger LLaMA-3.2 [53] image MLLM. The results in the second section of the table show that our model achieves better results than all these baselines. In particular, we note that when using the same Vicuna-7B [11] model, BIMBA-LLaVA outperforms both PLLaVA and LLaVA-NeXT (Video) on all benchmarks, achieving an average accuracy of 55.23% compared to 49.42% for LLaVA-NeXT and 50.40% for PLLaVA. Likewise, our BIMBA-LLaMA variant surpasses LLaMA3.2 (Video) by 3.09% on average and PLLaVA (LLaMA3.2) by 4.21%. These results highlight the effectiveness of our selective-scan-based token compression mechanism, which can be seamlessly integrated into existing MLLMs to enhance performance across wide range of video understanding tasks. Furthermore, since recent MLLMs [35, 48, 62, 63, 67, 85] have been trained with significantly larger instructiontuning datasets and stronger LLM backbones, we also trained variant of BIMBA-LLaVA using approximately 1.6 million video instruction samples and the Qwen27B LLM backbone. The results in the third section of Table 3 show that this BIMBA-LLaVA variant achieves state-of-the-art performance on multiple challenging video question-answering benchmarks, including NExT-QA [74], EgoSchema [52], VNBench [87], LongVideoBench [72], and Video-MME [16]. On the EgoSchema benchmark, our model surpasses the previous best method, LongVU [62], by 3.54%, demonstrating its superior ability to comprehend egocentric videos and handle questions that require long context understanding. Similarly, on VNBench, which focuses on needle-in-the-haystack questions, our approach outperforms LLaVA-Video [85] by 7.11%, highlighting its strong capability to extract key information from very long videos. Additionally, BIMBA-LLaVA (Qwen2-7B) substantially outperforms models based on proprietary LMMs. Specifically, it surpasses VideoTree [70] (GPT-4) by 7.7% on NExT-QA and 10.04% on EgoSchema. Finally, on benchmarks requiring long video comprehension, such as LongVideoBench [72] and Video-MME [16], our model sets new state-of-the-art, further demonstrating its effectiveness in processing and understanding hour-long videos. 7. Conclusion We introduced BIMBA, an efficient multimodal large language model for video question answering. By using spatiotemporal token selector based on selective scan, 8 BIMBA dramatically compresses sequences of tokens extracted from long-form video while preserving salient information, achieving significant memory and runtime savings compared to existing approaches. We introduce simple but effective design contributions, including an interleaved token arrangement to mitigate positional bias and bidirectional selective-scan mechanism for better modeling the spatiotemporal structure of the video. Experimental results show that our model achieves state-of-the-art performance on multiple datasets, marking promising advance for efficient long-form video understanding with LLMs. In the future, we are interested in exploring the adaptation of our model to other tasks, such as video summarization, text-tovideo search, and hierarchical video modeling. 9 BIMBA: Selective-Scan Compression for Long-Range Video Question Answering"
        },
        {
            "title": "Supplementary Material",
            "content": "Our supplementary materials contain additional implementation details (Section S1), additional quantitative results (Section S2), and qualitative results (Section S3). S1. Additional Implementation Details BIMBA-LLaVA is based on the image-pretrained MLLM LLaVA-NeXT [45], which utilizes CLIP [60] as the vision encoder and Vicuna-7B [11] as the LLM. It processes 64 video frames at resolution of 336 336, dividing each frame into 1414 patches, yielding 642424 spatiotemporal tokens. These tokens are compressed to 16 12 12 before being fed into the LLM. In this variant, the vision encoder remains frozen, while the multimodal projector (a linear layer), spatiotemporal token selector, and LLM are trained using LoRA [24]. BIMBA-LLaVA (Qwen2-7B) is built on LLaVAVideo [85], leveraging SigLIP [80] as the vision encoder and Qwen2-7B [12] as the LLM. It processes 128 video frames at resolution of 336 336, dividing each frame into 14 14 patches, producing 128 24 24 spatiotemporal tokens. These are compressed to 128 12 12 before being passed to the LLM. Similar to the previous variant, the vision encoder remains frozen, while the multimodal projector, spatiotemporal token selector, and LLM are trained using LoRA. BIMBA-LLaMA is based on the image-pretrained MLLM LLaMA-3.2 [53], incorporating Meta-CLIP [75] as the vision encoder and LLaMA-3.2-LLM-8B as the LLM. It processes 64 video frames at higher resolution of 560 560, dividing each frame into 14 14 patches, resulting in 64 40 40 spatiotemporal tokens. These are compressed to 16 20 20 before being passed to the LLM. Unlike the other variants, both the vision encoder and multimodal projector remain frozen, with only the spatiotemporal token selector and LLM trained using LoRA. Training Details. We employ standard cross-entropy loss for autoregressive text generation and train the model for 1 epoch with batch size of 128 and learning rate of 2e5. The AdamW [49] optimizer is used, along with cosine learning rate scheduler and warm-up ratio of 0.03. S2. Additional Quantitative Results S2.1. Performance as Function of Video Length In this section, we evaluate the performance of our model on videos of varying lengths from the NextQA [74] dataset, with results presented in Figure S1. Figure S1 (left) shows the relative performance improvement over the PLLaVA [76] baseline for different video durations. We observe that as video duration increases, the relative performance improvement over the baseline becomes more pronounced. This demonstrates the effectiveness of our proposed Mamba-based token compression technique compared to pooling-based methods, particularly for long-range videos. Similarly, the Figure S1 (right) illustrates the relative performance improvement of BIMBA-LLaVA over the LLaMA-3.2 (video) baseline for varying video durations. Here, too, we observe that the relative performance gap widens as video duration increases, showcasing the advantages of our model over the vanilla LLaMA-3.2 (video) baseline, which does not use any compression mechanism. S2.2. Computation Cost of BIMBA-LLaMA In this section, we compare the computational cost of our model with other baselines in terms of GPU memory usage (Figure S2, left) and runtime (Figure S2, right). Our analysis shows that self-attention incurs quadratic costs for both memory and runtime, resulting in out-of-memory errors for inputs longer than 8 frames (12,800 tokens). In contrast, all other methods maintain low memory and runtime costs. Despite having computational efficiency similar to that of the other baselines, our method achieves superior performance, as demonstrated in the previous section. S3. Qualitative Results Our qualitative results include open-ended video question answering (Section S3.1), multiple choice video question answering (Section S3.2), importance of question conditioning (Section S3.3), and significance of bidirectional Mamba and interleaved queries (Section S3.4). S3.1. Open-Ended Video Question Answering In Figure S3, we provide examples of our models performance in open-ended video question answering. The results showcase the models ability to handle diverse video understanding tasks, including generating detailed descriptions, recognizing objects and interactions, identifying finegrained activities, and inferring high-level goals. These examples illustrate the models effectiveness in generalpurpose video understanding. S3.2. Multiple Choice Video Question Answering We show qualitative examples of video question answering of our model and other baselines on NextQA (Figure S4) and EgoSchema (Figure S5) datasets. Both BIMBA-LLaVA (a) LLaVA Backbone on NeXT-QA (b) LLaMA Backbone on NeXT-QA m r y u % 20 10 0"
        },
        {
            "title": "100\nSeconds",
            "content": "150 m r y u % 20 10 5 0"
        },
        {
            "title": "100\nSeconds",
            "content": "150 Figure S1. Relative performance improvement of (left) BIMBA-LLaVA over PLLaVA baseline and (right) BIMBA-LLaMA over LLaMA3.2 (video) baseline for different video durations on NextQA dataset. Our model achieves larger gains as the video length increases. 60 40 20 ) ( m BIMBA Self-Attention Perceiver Pooling Vanilla 20K 40K 60K 80K 100K"
        },
        {
            "title": "Tokens",
            "content": ") ( t 3 2 0 BIMBA Self-Attention Perceiver Pooling Vanilla 20K 40K 60K 80K 100K"
        },
        {
            "title": "Tokens",
            "content": "(a) Memory Usage of LLaMA-3.2 Backbone. (b) Runtime of LLaMA-3.2 Backbone. Figure S2. Computation cost of BIMBA-LLaMA and baseline models in terms of memory usage (left) and runtime (right). Self-attention runs out of memory for longer sequences. All other baselines, including our model, maintain low memory and runtime. and BIMBA-LLaMA generate the correct answers while other baselines fail, demonstrating the effectiveness of our model for this task. S3.3. Importance of Question Conditioning In Figure S6, we showcase example predictions from our model with and without question-conditioned token selection on the NextQA (Figure S6 (a)) and EgoSchema (Figure S6 (b)) datasets. In both cases, incorporating question tokens into our spatiotemporal token selector enables the model to produce the correct answer. This exhibits the ability of our token selector to leverage question tokens effectively, selecting relevant visual tokens to enhance questionanswering performance. S3.4. Bidirectional Mamba and Interleaved Queries In this section, we visualize the effect of bidirectional Mamba and interleaved queries in Figure S7. We calculate response for each frame as follows: first, we take the hidden states of each token after the spatiotemporal token selector and compute dot product with the query tokens. Then, we apply max pooling to the dot product values of tokens within each frame to obtain response for that frame. This response value reflects the weight of each frame in the compressed query representations. Figure S7 (a) shows that using bidirectional scans and interleaved queries enables our model to capture critical information across the entire video and generate the correct answer. In contrast, (b) with bidirectional Mamba and standard queries, the model focuses mainly on the beginning 11 and end of the video, and (c) with unidirectional Mamba and standard queries, the model focuses only on the latter part of the video. Both designs are suboptimal, as they miss critical information and produce incorrect answers. (a) Example 1 of open-ended video question answering. Figure S3. Qualitative Results on Open-Ended Video Question Answering. Our model demonstrates the ability to answer wide range of questions about videos, including detailed descriptions, high-level goals, and fine-grained activities. (b) Example 2 of open-ended video question answering. 13 Figure S4. Qualitative Results on NextQA. Our model generates the correct answer while both PLLaVA and LLaMA-3.2 (video) baselines fail. Figure S5. Qualitative Results on EgoSchema. Our model generates the correct answer while both PLLaVA and LLaMA-3.2 (video) baselines fail. 15 Figure S6. Qualitative Results on Question Conditioned Token Selection on (a) NextQA and (b) EgoSchema datasets. Incorporating question tokens into our spatiotemporal token selector leads to the correct answer in both examples. Using the information from the questions allows our spatiotemporal selection module to focus on the most relevant video parts for answering the question. 16 Figure S7. Visualization of Bidirectional Mamba and Interleave Queries. Utilizing bidirectional Mamba and interleaved queries leads to the correct answer, while the unidirectional Mamba and standard queries fail."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1 [3] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 1, 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [5] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1 [6] Soumyabrata Chaudhuri and Saumik Bhattacharya. Simba: Mamba augmented u-shiftgcn for skeletal action recognition in videos. arXiv preprint arXiv:2404.07645, 2024. [7] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. 1 [8] Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. Rsmamba: Remote sensing image classification with state space model. IEEE Geoscience and Remote Sensing Letters, 2024. 2 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [10] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 2, 8 [11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 1, 5, 8, 10 [12] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 10 Mastering free-form text-image composition and comprearXiv preprint hension in vision-language large model. arXiv:2401.16420, 2024. [14] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [16] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5, 8 [17] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, and Xingyu Liu et al. Ego4d: Around the world in 3,000 hours of egocentric video. IEEE Conf. Comput. Vis. Pattern Recog., pages 1899519012, 2022. 5 [18] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2, 3, 4 [19] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33, 2020. 1, [20] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear statespace layers. Advances in Neural Information Processing Systems, 34, 2021. [21] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. 2, 3 [22] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 2022. [23] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:2298222994, 2022. 2 [24] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 5, 10 [25] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. arXiv preprint arXiv:2403.19046, 2024. [13] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: [26] Weiquan Huang, Yifei Shen, and Yifan Yang. Clip-mamba: Clip pretrained mamba models with ood and hessian evaluation. arXiv preprint arXiv:2404.19394, 2024. 2 18 [27] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip In European classification with state-space video models. Conference on Computer Vision, pages 87104. Springer, 2022. 2 [28] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1874918758, 2023. 2 [29] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1819818208, 2024. 5 [30] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 1, 5, [31] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv:2401.04088, 2024. 1 [32] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. ArXiv abs/2311.08046, 2024. 2 [33] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 2 [34] Byung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. Meteor: Mamba-based traversal of rationale for large language and vision models. arXiv preprint arXiv:2405.15574, 2024. 2 [35] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, [36] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next: Tackling multi-image, video, and 3d in large multimodal models, 2024. 1 [37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [38] Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. Intentqa: Context-aware video intent reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1196311974, 2023. 5 [39] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1, 2 [40] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. ArXiv abs/2311.17005, 2023. 1, [41] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In European Conference on Computer Vision, pages 237255. Springer, 2025. 2, 5 [42] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. ArXiv abs/2311.17043, 2023. 2, 8 [43] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 1, 8 [44] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. 1 [45] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5, 8, 10 [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [47] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 1 [48] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input, 2024. 1, 8 [49] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. 10 [50] Hui Lu, Albert Ali Salah, and Ronald Poppe. Videomambapro: leap forward for mamba in video understanding. arXiv preprint arXiv:2406.19006, 2024. 2 [51] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1 [52] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2, 5, 8 [53] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https://ai.meta.com/blog/llama3-2-connect-2024-vision-edge-mobile-devices, 2024. 1, 3, 5, 8, 10 [54] OpenAI."
        },
        {
            "title": "Introducing",
            "content": "chatgpt. https://openai.com/blog/chatgpt, 2022. 1 [55] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 1 19 [56] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 2, 5 [57] Xiaohuan Pei, Tao Huang, and Chang Xu. Efficientvmamba: Atrous selective scan for light weight visual mamba. arXiv preprint arXiv:2403.09977, 2024. [58] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adri`a Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. 5 [59] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. 2 [60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5, 10 [61] Ruchit Rawal, Khalid Saifullah, Miquel Farre, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. 1 [62] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 8 [63] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale arXiv preprint arXiv:2409.14485, video understanding. 2024. 8 [64] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tianbo Ye, Yang Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding. ArXiv abs/2307.16449, 2023. [65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. 1 [66] Jiawei Wang, Liping Yuan, and Yuchen Zhang. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 1 [67] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 8 [68] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer, 2024. 8 [69] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. [70] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. arXiv preprint arXiv:2405.19209, 2024. 8 [71] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 5 [72] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 2, 5, 8 [73] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 2 [74] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 5, 8, 10 [75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. 5, 10 [76] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning, 2024. 1, 8, [77] Yijun Yang, Zhaohu Xing, and Lei Zhu. Vivim: video vision mamba for medical video object segmentation. arXiv preprint arXiv:2401.14168, 2024. 2 [78] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and Feiyan Huang. mplugowl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 1 [79] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2020. 5 20 [80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 10 [81] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. [82] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1 [83] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 8 [84] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 2, 8 [85] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 8, 10 [86] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. 2 [87] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing Liu. Needle in video haystack: scalable synthetic framework for benchmarking video mllms. arXiv e-prints, pages arXiv2406, 2024. 2, 5, 8 [88] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. [89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2023. 1 [90] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024."
        }
    ],
    "affiliations": [
        "Meta AI",
        "UNC Chapel Hill"
    ]
}