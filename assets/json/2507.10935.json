{
    "paper_title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization",
    "authors": [
        "Shaowen Tong",
        "Zimin Xia",
        "Alexandre Alahi",
        "Xuming He",
        "Yujiao Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill."
        },
        {
            "title": "Start",
            "content": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization Shaowen Tong1 Zimin Xia2 Alexandre Alahi2 Xuming He1 Yujiao Shi1* 1ShanghaiTech University, China 2 Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland {tongshw2024,hexm,shiyj2}@shanghaitech.edu.cn, {zimin.xia,alexandre.alahi}@epfl.ch 5 2 0 2 5 ] . [ 1 5 3 9 0 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Cross-view localization, the task of estimating cameras 3-degrees-of-freedom (3-DoF) pose by aligning groundlevel images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, Geometry guided weakly supervised self Distillation framework that uses teacher-student learning with Field-of-View (FoV)- based masking to enhance local feature learning for robust In GeoDistill, the teacher model cross-view localization. localizes panoramic image, while the student model predicts locations from limited FoV counterpart created by FoV-based masking. By aligning the students predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill. 1. Introduction Visual localization estimates cameras pose by matching its image to known environment. Cross-view localization, specifically, determines the 3-degrees-of-freedom (3-DoF) camera pose, i.e., planar position and yaw orientation, by aligning ground-level query image with satellite image that covers its surroundings. Leveraging the widespread *Corresponding author. Figure 1. Our GeoDistill encourages the student model to extract discriminative local features from FoV-based masked inputs, resulting in more accurate localization (top right) and reduced uncertainty (bottom right). In contrast, the teacher model (middle), pre-trained on panoramas, lacks explicit enforcement for learning local features, leading to more uncertainty and wrong localization. availability of satellite images, this approach has attracted significant attention for large-scale outdoor applications, including autonomous navigation and augmented reality. State-of-the-art cross-view localization methods primarily follow fully supervised paradigm [19, 30, 3537], relying on precisely annotated ground camera poses for training. However, obtaining such annotations at scale is costly, often requiring fleets of mobile mapping vehicles equipped with expensive sensor kits to traverse the environment. Furthese methods often suffer significant perforthermore, mance degradation in the cross-area setting [19, 30, 3537], where test images originate from regions different from the training areas. This limitation significantly hinders the scalability of cross-view localization deployment. To address this challenge, we propose leveraging weakly supervised learning. While obtaining precise ground truth data is costly, collecting noisy ground truth remains accessible. For example, images with coarse localization can be obtained using mobile phones GPS. Using this coarse location, we can retrieve satellite images covering the local surroundings, allowing us to pair these images with the ground-level query image. Recently, few studies [31, 38] have explored this direction for 2-DoF location estimation with different formulations. [38] assumes access to additional test-area data for fine-tuning, but in practice, collecting data from all test regions is infeasible. G2SWeakly [31] matches ground images with corresponding positive and negative satellite images, leveraging image-level deep metric learning objectives to achieve the goal of relative pose estimation between the ground and its positive satellite image. Although promising, image-level metric learning cannot provide strong supervision for accurate local features. These discriminative local features are essential to achieve high accuracy in estimating relative poses. This work aims to learn discriminative local features that generalize well in different areas using only ground-satellite image pairs. To do so, we propose field-of-view (FoV)- based masking strategy for the query images to encourage the model to focus on different local features (See. Fig. 1 and Fig. 4c for some examples). Unlike patchor pixelbased masking, which might remove critical information (such as ground structures) and retain irrelevant parts (like the sky), FoV-based masking ensures that the query image always includes useful portion of the scene, enabling reasonable predictions. straightforward approach to leverage this FoV-based masking might be to use it as data augmentation technique. However, this naive approach makes it harder for the model to learn generalizable features, as removing information from query images further complicates the already challenging cross-view localization task. To tackle this challenge, we propose GeoDistill, teacher-student knowledge self-distillation framework that explicitly encourages the model to make similar predictions for both panoramic images and masked images depicting the same scene. Since panoramas typically provide more accurate location predictions than images with limited FoV, the teacher model takes the full panoramas as input, while the student model uses the FoV-based masked counterparts. The student learns to mimic the teachers predictions, despite receiving less information. This forces the student to mine discriminative local features, such as lane markings, without relying on the overall global scene structure. As training progresses, the student model gradually learns to emphasize key local features and can outperform the teacher. As shown in Fig. 1, the teacher model, which uses panorama as input, makes incorrect predictions (top) or outputs high uncertainty along the road (bottom). In contrast, our student model, benefiting from enhanced local feature extraction, makes accurate predictions (top) and reduces uncertainty (bottom). Therefore, we update the teacher models weights by incorporating the students weights through moving average, progressively refining the teacher as better learning target. Concretely, our contributions are summarized as follows: We introduce GeoDistill, weakly supervised selfdistillation paradigm that enhances local discriminative feature learning for robust cross-view localization. We demonstrate that this learning paradigm applies to different localization frameworks and improves their performance by over 10% without architectural modifications. We explore FoV-based masking strategies to enhance localization-critical feature learning. We demonstrate that while FoV-based masking as naive data augmentation impairs performance, it significantly improves the performance of localization frameworks when applied within our proposed teacher-student self-distillation pipeline. Existing weakly supervised approaches lack the ability to estimate the relative orientation between ground-satellite images. We design an orientation estimation network that predicts the relative orientation between ground-satellite images without precise planar position ground truth. 2. Related Work Cross-view localization is typically formulated as either large-scale image retrieval [3, 20, 21, 24, 28, 40, 44] or, more recently, fine-grained pose estimation. The latter, popularized by the VIGOR benchmark [43], aims to determine the precise 3-DoF pose between ground image and its corresponding aerial view (e.g., satellite imagery [8, 19, 27, 2932, 3538] or OpenStreetMap data [25, 26]). The core challenge lies in the substantial visual disparity between viewpoints. To bridge this viewpoint gap, one dominant strategy involves explicit geometric projections. Some approaches transform satellite imagery into ground-view perspective using polar transformations [27, 29]. Complementary methods project ground images into birds-eyeview (BEV) representation [8, 30, 32, 35]. While these projection-based techniques have shown promise, they risk information loss during the transformation process. contrasting line of work bypasses explicit geometry, instead employing end-to-end neural architectures that directly regress pose parameters from the original image pairs [19, 36, 37]. This direct approach preserves all visual information but requires the model to implicitly learn more complex cross-view spatial relationships. While architecturally distinct, these approaches are universally designed for fully supervised setting, demanding access to datasets with precise and costly ground-truth(GT) pose annotations. More recently, weakly supervised paradigms have marked significant advance [31, 38]. These methods leverage noisy GPS data as weak supervision signal, substantially reducing the dependency on precisely annotated GT. This innovation greatly enhances the scalability and deployability of cross-view localization in real-world scenarios, where obtaining accurate pose annotations can be prohibitively expensive or impractical. However, these pioneering weakly supervised methods have their own limitations. [38] simplifies the problem to 2-DoF translation by assuming known camera orientation and requires indomain data for adaptation. Meanwhile, while [31] successfully predicts orientation, its applicability is confined to ground image captured by standard pinhole cameras. student model. to more compact Knowledge distillation(KD) [1, 2, 4, 11, 34] aims to transfer knowledge from more comprehensive teacher Selfmodel distillation(SD) [15, 16, 22, 42] stands as compelling paradigm within KD frameworks. Unlike traditional KD, SD represents specialized branch pioneered by BornAgain Networks [9], wherein student network learns from teacher network with identical architecture, often initialized with the same weights. This approach elegantly circumvents the architectural constraints of conventional KD while preserving its knowledge transfer benefits. SD effectively harnesses networks inherent capacity by establishing teacher-student dynamic through differentiated training procedures [17, 41] or strategically varied input data [7, 39]. The fundamental insight of SD lies in its iterative refinement mechanismusing predictions from previously trained model as target values for subsequent retraining cycles [22], thereby enabling the model to progressively distill and refine its own knowledge representation. This self-referential learning process has demonstrated remarkable efficacy in enhancing model performance without requiring additional architectural complexity or external knowledge sources. 3. Methodology This section first formalizes the cross-view localization task and our weakly supervised learning setting. Then, it introduces the details of our proposed GeoDistill method. 3.1. Task Definition Cross-view localization estimates the 3-DoF pose, i.e., the yaw θ and 2D translation = (u, v), between groundlevel image Ig and geo-referenced satellite image Is. Current methods [19, 30, 31, 36, 37] train neural network to generate heat map for localization, with the highestconfidence location serving as the predicted position, ˆy = argmaxu,v H(u, v). (1) The neural network is typically trained in supervised manner, relying on ground truth location and yaw for learning. However, acquiring precise ground truth location data is costly, whereas ground-level images with noisy GPS measurements are readily available1. Thus, weakly super1Obtaining accurate location data requires expensive mapping vehicles, while coarse location and yaw orientation can be easily obtained using built-in phone GPS and compasses. vised learning presents more practical alternative. Following [31, 38], the coarse location is used to identify satellite image that covers the ground cameras location. Our objective is then to use the ground-aerial image pairs to train deep model for cross-view localization. As shown in Fig. 2, we decompose the task into two sequential steps: First, our Rotation Estimator (Sec. 3.2) predicts the yaw angle ˆθ, and uses it to rotate Ig by horizontally shifting the panorama. The transformed image (cid:101)Ig is then fed into Location Estimator that predicts heat maps for localization ˆH = ( (cid:101)Ig, Is). Notably, our proposed Geometry-Guided Self-Distillation (Sec. 3.3) is generic weakly supervised learning paradigm compatible with various location estimators. 3.2. Orientation Estimation The motivation of our orientation estimation network is to align prominent structural cues between ground and aerial views shared, such as road layouts, which are typically the most dominant visual feature in outdoor scenes. However, primary challenge arises from the inherent perspective distortion of panoramic images, which projects real-world straight roads into curves. To overcome this geometric mismatch, we employ spherical transform [35] to project Ig into Birds-Eye-View (BEV) representation, g. This projection rectifies the road geometry, making it consistent with the top-down satellite image Is and thus enabling direct comparison for orientation alignment. Both the Is and are fed into unshared feature extractor backbone, denoted as Es() and Eg(), respectively: Fs = Es(Is), Fg = Eg(Ib g), (2) where Fs, Fg RHW C. To facilitate cross-view feature interaction, we concatenate the extracted feature maps channel-wise to obtain fused feature representation F: = Concat(Fs, Fg), where RHW 2C. (3) The fused feature map is processed by Multilayer Perceptron (MLP) to predict orientation ˆθ. To simplify the problem, we formulate this as classification problem over discrete classes, one per degree in predefined range. In the classification setup, the network outputs probability distribution, and training uses the Cross-Entropy loss LCE between this distribution and the smoothed labels. 3.3. Geometry-Guided Self-Distillation To achieve robust cross-view localization, model must learn to identify discriminative local features. We propose novel self-distillation method to directly cultivate this capability. At its core, the method enforces prediction consistency between complete panorama and its partial views. Figure 2. Overview of the proposed GeoDistill for 3-DoF ground-to-satellite relative pose estimation. Given ground image, we first estimate its orientation with respect to the satellite image (Stage 1). For location estimation (Stage 2), we apply the proposed geometryguided teacher-student self-distillation (GeoDistill) to backbone framework, which can be any cross-view localization networks. All components in this pipeline are involved during training, while the green arrows indicate the workflow during inference. Since the full and partial views depict the same geographical location, they must map to the identical satellite coordinate. This geometry-guided consistency requirement serves as powerful supervision signal, compelling the model to discover salient local cues rather than depending on the brittle context of full panorama. The teacher model is pre-trained Location Estimator ft(; θt), where θt denotes its weights. It can be any recent cross-view localization method that generates heat maps for localization [19, 30, 31, 36, 37]. The student model fs(; θs) has the same architecture as the teacher and is initialized with the teachers weights, i.e., θs = θt at the start of training. key design in GeoDistill is that the teacher and student will receive different inputs, forcing them to extract distinct features for localization. FoV-based geometric consistency. Our geometric guidance enforces prediction consistency between full view and partial view of the same scene, which we achieve via an teacher-student architecture. The teacher model ft receives the transformed full 360 panorama (cid:101)Ig and the satellite image Is. This is identical to standard cross-view localization methods [19, 30, 31, 36, 37], where models typically construct complete feature map of the scene [31] or learn global image descriptors [37] for localization. To encourage the student fs to explore local features in the image, we do not feed the full panorama to it. Instead, we apply mask with random FoV to the panorama (cid:101)Ig, (cid:101)Ig = ( (cid:101)Ig), (x) = Mmask, (4) where Mmask {0, 1}HW . The resulting masked image (cid:101)Ig simulates an image with limited FoV. Fig. 4c illustrates two examples of the FoV-based masking. During training, the teacher networks weights, θt, are frozen, serving as provider of stable and reliable learning targets. Both teacher and student networks process their respective inputs, along with the satellite image Is. to generate heat maps: Ht = ft( (cid:101)Ig, Is; θt), Hs = fs( (cid:101)Ig , Is; θs). (5) This design creates geometrically-grounded learning task where the student must learn to produce localization output consistent with the teachers, but from incomplete information. This forces the student to move beyond reliance on the global scene structure and instead mine for robust, discriminative local features for accurate localization. Uncertainty preservation. Although the teacher model provides learning target, its predictions inevitably contain noise. The students predictions are also noisy, exacerbated by its less informative input. Directly aligning these noisy distributions is difficult. While one solution might be to use hard, one-hot target, this would discard the valuable dark knowledge encoded in the teachers heatmap. This knowledge, encoded in the distributions shape and relative activation strengths, implicitly teaches the student about structural similarities and model uncertainty. We adopt more nuanced strategy: sharpening both heatmaps before calculating the distillation loss. This serves dual purpose: it reduces the distributions entropy, compelling the student to focus on the teachers highestconfidence predictions, while simultaneously mitigating the impact of noisy, low-confidence signals. Specifically, we apply softmax with temperature τ < 1 to both outputs: Pt = Softmax(Ht/τ ), Ps = Softmax(Hs/τ ). (6) Bidirectional knowledge flow. The student learns from the teacher by minimizing the self-distillation loss LSD, defined as the Cross-Entropy between their sharpened output distributions Ps and Pt: (cid:34) LSD = EX (cid:35) Pt(i) log Ps(i) . (7) (cid:88) While the students weights θs are updated via gradient descent, we facilitate bidirectional knowledge flow [4, 12] by updating the teachers weights θt as an Exponential Moving Average (EMA) of the students: θt αθt + (1 α)θs. (8) This EMA mechanism allows the teacher to progressively absorb the robust features learned by the student. For inference, we use this continuously refined teacher model to achieve enhanced performance. 4. Experiments In this section, we conduct experiments to answer the following questions: (1) How necessary and effective is each component of the proposed GeoDistill approach? (2) How does GeoDistill compare to other (Sec. 4.4). weakly supervised methods? Can it also enhance fully supervised methods? (Sec.4.2). (3) What is the performance gap between GeoDistill and fully supervised state-of-the-art methods? (Sec. 4.3). First, we outline the dataset, evaluation metrics, base models, and implementation details. 4.1. Datasets and Evaluation Metrics We evaluate GeoDistill on two different datasets: VIGOR contains 105,214 pairs of geo-referenced ground panoramas and corresponding aerial images from four US cities, with each aerial image covering 70m 70m area. Following the official protocol, we only use positive pairswhere the ground cameras location is within the central quarter of the aerial imagefor all training and evaluation. Panoramas are North-aligned in VIGOR [43], we augment the panoramas by applying random orientation noise within 45 range to generate orientation label. We adopt the standard Same-Area and Cross-Area splits for evaluation. For hyperparameter tuning, validation set is created by holding out 20% of the training data, consistent with prior works[30, 31, 35, 37]. KITTI [10] provides limited-FoV ground images captured by pin-hole camera from Germany, coupled with aerial views from [29]. Following the standard setup [27], we use the same-area and cross-area splits, where ground camera locations are within central 40m 40m aerial patch and an orientation prior with 10 noise is given. Evaluation metrics. Performance evaluation is conducted using standard metrics: mean and median errors, computed separately for localization (in meters) and orientation (in degrees), across all test samples, providing comprehensive assessment of accuracy. Base models. We validate the effectiveness and broad applicability of the proposed GeoDistill on two distinct finegrained cross-view localization methods: G2SWeakly [31] and CCVPE [37]. Our primary experiments build upon G2SWeakly, state-of-the-art weakly supervised approach that projects panoramas into BEV images to mitigate visual discrepancies. This ensures our entire pipeline remains strictly weakly supervised. While the original G2SWeakly uses VGG backbone, we also implement variant using DINOv2 to leverage its powerful, generalizable feature representations. Furthermore, to demonstrate the versatility of GeoDistill, we apply it to CCVPE [37], fully supervised method that relies on descriptor-based matching and trains its network using vanilla panoramas. Implementation details. For location estimation, we use the code released by the authors of CCVPE [37] and G2SWeakly [31] for model implementations. Following the two models default settings, we use batch size of 8, and learning rate of 0.0001 with Adam optimizer [18] for both models. The temperature τ is set to 0.06 for both teacher and student. The EMA ritio α is 0.9. For G2SWeakly [31]- DINO variant, we employ pre-trained DINOv2-b14 as the feature extractor. During training, the weights of the DINOv2 are kept frozen, and we append DPT [23] module to fine tune DINO feature. Our orientation estimation framework employs EfficientNet-B0 [33] with pretrained weights on Imagenet [5] as both the ground and aerial feature extractors, with non-shared weight. The satellite image and BEV transformed from the ground image both have size of 512 512. All experiments were conducted using single NVIDIA 4090 GPU. 4.2. Effectiveness of Proposed Distillation Paradigm To validate the effectiveness and generalizability of GeoDistill, we conduct comprehensive evaluation. First, we assess the proposed self-distillation paradigm by incorporating it into two distinct base models: the weakly supervised G2SWeakly [31] and the fully supervised CCVPE [37]. Subsequently, we combine the orientation and location estimators to evaluate the full 3-DoF pose estimation performance, with detailed comparison against fully supervised methods presented in Sec. 4.3. Experiments are conducted on the VIGOR and KITTI datasets. Notably, key advantage of our method is its minimal supervision requirement: it operates solely on orientationaligned ground-satellite image pairs, without the need for GT translation. This holds even when GeoDistill is applied to CCVPE, model that conventionally requires complete GT poses for its training. Results on VIGOR. Tab. 1(top half) highlights the significant performance gained by GeoDistill on the VIGOR dataset. For the CCVPE [37], which suffers from notable performance disparity between evaluation settings, Figure 3. Qualitative comparison of probability maps before (left) and after (right) applying proposed self-distillation learning paradigm on VIGOR [43] Cross-Area test set. The first row presents input panoramic scenes, while the second row shows the predicted localization heat maps. Red indicates localization probability, with darker shades representing higher probabilities. Dataset Method VIGOR KITTI CCVPE [37] + GeoDistill G2SWeakly [31](VGG) + GeoDistill(VGG) G2SWeakly [31](DINO) + GeoDistill(DINO) CCVPE [37] + GeoDistill G2SWeakly [31](VGG) + GeoDistill(VGG) G2SWeakly [31](DINO) + GeoDistill(DINO) Cross-Area Same-Area Mean(m) 4.97 4.05 (18.5%) 5.20 4.49 (13.6%) 3.58 2.68 (25.1%) 8.94 6.99(21.8%) 12.54 12.16 (3.0%) 12.61 11.85 (6.0%) Median(m) 1.68 1.57 (6.5%) 1.44 1.22 (15.3%) 1.45 1.20 (17.2%) 3.33 3.14 (5.7%) 10.56 10.22 (3.2%) 11.64 11.17 (4.0%) Mean(m) 3.60 3.21 (10.8%) 4.81 4.26 (11.4%) 3.61 3.08 (14.7%) 1.28 1.25 (0.2%) 11.11 10.97 (1.3%) 11.68 11.52 (1.4%) Median(m) 1.36 1.31 (3.7%) 1.61 1.37 (14.9%) 1.59 1.39 (12.6%) 0.71 0.71 (0%) 9.74 9.62 (1.2%) 10.96 10.91 (0%) Table 1. Localization performance improvement over different baselines on VIGOR [43] and KITTI test set. Our proposed self distillation learning paradigm consistently improves base models performance without access to ground truth location labels. our distillation proves highly effective. It yields the most pronounced gains in the challenging cross-area scenario, effectively narrowing the performance gap and enhancing the models generalization. When combined with the G2SWeakly [31], it consistently yields substantial improvements. Notably, the performance lift is markedly greater for the stronger DINO-based variant. This suggests our methods efficacy scales with the base models quality, as superior feature extractor like DINO provides richer foundation from which to distill nuanced knowledge, leading to more pronounced gains. Results on KITTI. We further test on the KITTI dataset, which presents new challenge with its limited FOV captured by pinhole images, unlike VIGORs 360 panoramas. Tab. 1(bottom half) shows that results on KITTI follow similar trend. After distillation, CCVPE delivers substantial gains in the cross-area split. In the same-area setting, however, performance is unchanged, as the baselines already low localization error creates performance ceiling. When applied to G2SWeakly, our paradigm improves both VGG and DINO variants, though the gains are more modest than on VIGOR. We attribute this to the baselines weaker initial performance on KITTI, which provides noisier supervisory signal for distillation. Overall, the consistent performance gains across different models and datasets validate our self-distillation paradigm as an effective, plug-and-play solution for selfimprovement without architectural modifications. Qualitative results. Fig. 3 qualitatively compares the location probability maps predicted by the G2SWeakly [31] as base model on the VIGOR dataset, both with and without our distillation paradigm. As highlighted by purple rectangles, the vanilla G2SWeakly [31] struggles when localization should depend on local features. It often assigns high probability to incorrect locations while attributing low probability to the GT location, indicating poor cross-view feature matching. In contrast, our distillation-enhanced model effectively leverages these discriminative local features to produce accurate and confident location predictions. This demonstrates our methods ability to significantly improve feature representation for cross-view localization. 4.3. Comparison with Fully Supervised Methods We assess the generalization capabilities about 3-DoF pose estimation of GeoDistill by conducting rigorous evaluation on the VIGOR cross-area setting. We compare our method against several fully supervised approaches. Among them, HC-Net [35] represents the current stateof-the-art performance. We evaluate performance under varying levels of orientation prior noise, including 0 and 45 for ground images. Notably, we implement GeoDistill based on G2SWeakly [31] to ensure entire pipeline remains strictly weakly supervised, requiring no GT location labels. Tab. 2 shows GeoDistills effectiveness. With VGG backbone, GeoDistill already achieves the best median localization error among all methods, surpassing the fully supervised SOTA. Although its mean error is marginally higher in this setup, its superior median performance highlights its robustness. This advantage becomes definitive when using DINO backbone. Our weakly supervised model then surpasses all fully supervised methods across both mean and median metrics. Additionally, GeoDistill surpasses all competing approaches in orientation estimation, including those that are fully supervised, demonstrating the effectiveness of our orientation estimation network."
        },
        {
            "title": "Method",
            "content": "CVR [43]* SliceMatch [19]* Boosting [30]* CCVPE [37]* DenseFlow [32]* HC-Net [35]* Ours(VGG) Ours(DINO) CCVPE [37]* HC-Net [35]* Ours(VGG) Ours(DINO) 0 45 Orientation Localization Mean Median Mean Median 9.45 5.53 5.16 4.97 5.01 3.35 4.49 2.68 5.16 3.46 4.99 4.20 8.33 2.55 1.40 1.68 2.42 1.59 1.22 1.20 1.78 1.60 1.33 2. 26.77 15.29 1.35 3.00 1.35 2.72 1.35 2.72 - - - - - - - - - - - - - - - - Table 2. Performance comparison with fully supervised approaches on 2-DoF and 3-DoF pose estimation on VIGOR [43] Cross-Area test set. Here, GeoDistill employs G2SWeakly as base model, ensuring the model is trained strictly with weak supervision. * indicates fully supervised methods. Best in bold. Backbone Mask CNN ViT Maximum Act. Random Patch FoV Baseline Random Patch FoV Baseline Mean(m) 5.14 5.21 4.49 5.20 3.10 2.68 3.58 Median(m) 1.42 1.44 1.22 1.44 1.33 1.20 1.45 Table 3. Performance comparison with different masking strategies on VIGOR [43] Cross-Area test set. 4.4. Model Analysis (a) Random patch masking (b) Maximum activation masking To validate the core components of GeoDistill, we conduct series of ablation studies. We build upon the state-of-theart weakly supervised method, G2SWeakly [31], as our base model. Unless stated otherwise, all experiments utilize its original implementation with VGG backbone. Our analysis investigates the contributions of our key components: the FoV-based masking, the uncertainty preservation strategy, and the teacher-student parameter update strategy. Why FoV-based masking? To learn discriminative local features, our GeoDistill proposes FoV-based masking. By aligning predictions from an input with limited information (student) with those from an input with full information (teacher), we encourage the student model to focus on learning discriminative local features. Specifically, our ap- (c) FoV-based Masking Figure 4. Different masking strategies. proach mimics query image from limited-FoV camera. By preserving coherent scene geometry, this strategy compels the model to learn more robust and discriminative local features, leading to superior performance. In contrast, alternative strategies such as random patch-based masking, often used in masked autoencoders [13], and activationG2SWeakly [31] CCVPE [37] Data Aug? Yes No Yes No Mean(m) 5.64 5.20 5.37 4.97 Median(m) 1.64 1.44 2.16 1.68 Table 4. Localization performance comparison on two baselines with or without the FoV-based masking as data augmentation on VIGOR [43] Cross-Area test set. Uncertainty Single-mode W/o sharpen W/ sharpen Baseline Mean(m) 4.96 5.23 4.49 5.20 Median(m) 1.36 1.44 1.22 1.44 Table 5. Localization performance comparison with different uncertainty preservation strategies on VIGOR [43] Cross-Area test set. Single-mode preserves the highest confidence prediction. based masking [6, 14], risk destroying these crucial scene structures and losing important local features, as illustrated in Fig.4. This fundamental drawback leads to suboptimal performance gains. To validate this, we performed comprehensive comparison. Given that masked autoencoders are typically ViT-based, we benchmarked random masking against our method on both CNN (VGG) and ViT (DINOv2) backbones. Tab.3 shows that our FoV-based masking is consistently superior in both settings, highlighting its backbone-agnostic advantage. FoV-based masking as data augmentation? In our GeoDistill framework, the proposed FoV-based masking serves as core component for generating powerful learning signal. By creating discrepancy between the student (masked input) and the teacher (full input), it establishes challenging but valuable self-distillation task. This process forces the student to learn robust features from partial views by distilling knowledge from the teachers complete perspective, even amidst extreme viewpoint. One might consider applying FoV-based masking as conventional data augmentation strategy. However, this approach proves detrimental. As demonstrated in Tab.4, simply using FoVbased masking as augmentation consistently impairs performance in both fully and weakly supervised settings. This finding is consistent with results reported in [37], which also found that overly challenging augmentations can degrade model performance. This shows the necessity of our teacher-student pipeline, which successfully transforms this difficult masking task into an effective learning mechanism. Effectiveness of uncertainty preservation strategy. key advantage of our teacher-student framework lies in its ability to distill dark knowledge the valuable spatial uncertainty contained in the teachers heatmap. We validate Figure 5. Mean localization error of the student model when trained with different FoVs on VIGOR [43] cross-Area test set. this via an ablation study against two extremes, as shown in Table 5. On one hand, single-mode distillation [38] discards this dark knowledge by enforcing deterministic, singlepoint target, which leads suboptimal. On the other hand, baseline without sharpening (w/o sharpen) fails to converge because the raw dark knowledge is too diffuse and noisy to serve as stable learning signal. Our approach strikes crucial balance. By sharpening the teachers probability distribution, it refines the dark knowledgefiltering out noise while preserving the essential uncertainty. This transforms the teachers output into potent and stable supervision signal, which is essential for robust localization. FoV size selection. The choice of FoV for masking is critical to the success of GeoDistill. An excessively narrow FoV provides the student with insufficient contextual information, making the learning task intractable. Conversely, an overly large FoV makes the students input too similar to the teachers full view, diminishing the discrepancy between them and resulting in weak, ineffective supervision signal. Fig. 5 confirms our hypothesis. Mean localization error increases significantly when the FoV is either too small (lower than 90) or too large (over 240). Motivated by these findings, we adopt dynamic FoV for training, randomly sampling from the 180 to 240 range for each instance. This design maintains balance between task difficulty and solvability in distillation, and the incorporation of randomness contributes to improved generalization and robustness. 5. Conclusion We present GeoDistill, weakly supervised self-distillation framework that improves cross-view localization by learning salient local features. Through teacher-student learning with FoV-based masking, our method enhances localization accuracy and reduces uncertainty, showing significant gains on multiple datasets, particularly those lacking precise annotations. We also introduce novel orientation network that predicts relative orientation without location supervision, overcoming key limitation of previous weakly supervised approaches. GeoDistill provides scalable and effective solution for both weakly and fully supervised methods, demonstrating its high potential for large-scale, real-world localization tasks. 6. Acknowledgment The authors are grateful for the valuable comments and suggestions by the reviewers and AC. This work was supported by NSFC (62406194), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (KLIP-HuMaCo). part of the experiments of this work were supported by the core facility Platform of Computer Science and Communication, SIST, ShanghaiTech University."
        },
        {
            "title": "References",
            "content": "[1] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning, 2023. 3 [2] Cristian Buciluˇa, Rich Caruana, and Alexandru NiculescuMizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535541, 2006. 3 [3] Sudong Cai, Yulan Guo, Salman Khan, Jiwei Hu, and Gongjian Wen. Ground-to-aerial image geo-localization with hard exemplar reweighting triplet loss. In The IEEE International Conference on Computer Vision (ICCV), 2019. 2 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. 2021. 3, 5 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 5 [6] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. 8 [7] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Maskclip: Masked self-distillation advances contrastive languageimage pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1099511005, 2023. [8] Florian Fervers, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, and Rainer Stiefelhagen. Uncertainty-aware vision-based metric cross-view geolocalization. arXiv preprint arXiv:2211.12145, 2022. 2 [9] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International conference on machine learning, pages 16071616. PMLR, 2018. 3 [10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 5 [11] Jianping Gou, Baosheng Yu, Stephen Maybank, and InternaDacheng Tao. Knowledge distillation: survey. tional Journal of Computer Vision, 129(6):17891819, 2021. [12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 5 [13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. 2021. 7 [14] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. 2020. 8 [15] Zeyi Huang, Yang Zou, B. V. K. Vijaya Kumar, and Dong Huang. Comprehensive attention self-distillation for weaklysupervised object detection. In Advances in Neural Information Processing Systems, pages 1679716807. Curran Associates, Inc., 2020. 3 [16] Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, and Il-Chul Moon. Refine myself by teaching myself: Feature refinement via self-knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1066410673, 2021. 3 [17] Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum Hwang. Self-knowledge distillation with progressive refinement of targets. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 65676576, 2021. 3 [18] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. Computer Science, 2014. 5 [19] Ted Lentsch, Zimin Xia, Holger Caesar, and Julian FP Kooij. Slicematch: Geometry-guided aggregation for cross-view pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1722517234, 2023. 1, 2, 3, 4, 7 [20] Liu Liu and Hongdong Li. Lending orientation to neural networks for cross-view geo-localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [21] Li Mi, Chang Xu, Javiera Castillo-Navarro, Syrielle Montariol, Wen Yang, Antoine Bosselut, and Devis Tuia. Congeo: Robust cross-view geo-localization across ground view variations. In European Conference on Computer Vision, pages 214230. Springer, 2024. 2 [22] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in hilbert space. 2020. 3 [23] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. [24] Krishna Regmi and Mubarak Shah. Bridging the domain gap for ground-to-aerial image matching. In The IEEE International Conference on Computer Vision (ICCV), 2019. 2 [25] Paul-Edouard Sarlin, Daniel DeTone, Tsun-Yi Yang, Armen Avetisyan, Julian Straub, Tomasz Malisiewicz, Samuel Rota Bul`o, Richard Newcombe, Peter Kontschieder, and Vasileios Balntas. Orienternet: Visual localization in 2d public maps the AAAI conference on artificial intelligence, pages 5565 5572. Issue: 01. 3 [40] Hongji Yang, Xiufan Lu, and Yingying Zhu. Cross-view geo-localization with layer-to-layer transformer. Advances in Neural Information Processing Systems, 34:2900929020, 2021. 2 [41] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 37133722. 3 [42] Zhilu Zhang and Mert Sabuncu. Self-distillation as instancespecific label smoothing. In Advances in Neural Information Processing Systems, pages 21842195. Curran Associates, Inc., 2020. 3 [43] Sijie Zhu, Taojiannan Yang, and Chen Chen. Vigor: Crossview image geo-localization beyond one-to-one retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36403649, 2021. 2, 5, 6, 7, 8, 11 [44] Sijie Zhu, Mubarak Shah, and Chen Chen. Transgeo: Transformer is all you need for cross-view image geo-localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11621171, 2022. with neural matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2163221642, 2023. 2 [26] Paul-Edouard Sarlin, Eduard Trulls, Marc Pollefeys, Jan Hosang, and Simon Lynen. SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding. In NeurIPS, 2023. 2 [27] Yujiao Shi and Hongdong Li. Beyond cross-view image retrieval: Highly accurate vehicle localization using satelIn Proceedings of the IEEE/CVF Conference lite image. on Computer Vision and Pattern Recognition, pages 17010 17020, 2022. 2, 5 [28] Yujiao Shi, Liu Liu, Xin Yu, and Hongdong Li. Spatialaware feature aggregation for image based cross-view geolocalization. In Advances in Neural Information Processing Systems, pages 1009010100, 2019. 2 [29] Yujiao Shi, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li. Accurate 3-dof camera geo-localization via ground-to-satellite image matching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 2, 5 [30] Yujiao Shi, Fei Wu, Akhil Perincherry, Ankit Vora, and Hongdong Li. Boosting 3-dof ground-to-satellite camera localization accuracy via geometry-guided cross-view transformer. arXiv preprint arXiv:2307.08015, 2023. 1, 2, 3, 4, 5, 7 [31] Yujiao Shi, Hongdong Li, Akhil Perincherry, and Ankit Vora. Weakly-supervised camera localization by groundto-satellite image registration. In European Conference on Computer Vision, pages 3957. Springer, 2024. 2, 3, 4, 5, 6, 7, [32] Zhenbo Song, Jianfeng Lu, Yujiao Shi, et al. Learning dense flow field for highly-accurate cross-view camera localization. Advances in Neural Information Processing Systems, 36, 2024. 2, 7 [33] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. 2019. 5 [34] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence: review IEEE transactions on pattern analysis and new outlooks. and machine intelligence, 44(6):30483068, 2021. 3 [35] Xiaolong Wang, Runsen Xu, Zhuofan Cui, Zeyu Wan, and Yu Zhang. Fine-grained cross-view geo-localization using correlation-aware homography estimator. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 3, 5, 7 [36] Zimin Xia, Olaf Booij, Marco Manfredi, and Julian FP Kooij. Visual cross-view metric localization with dense uncertainty estimates. In European Conference on Computer Vision, pages 90106. Springer, 2022. 2, 3, 4 [37] Zimin Xia, Olaf Booij, and Julian FP Kooij. ConarXiv preprint volutional cross-view pose estimation. arXiv:2303.05915, 2023. 1, 2, 3, 4, 5, 6, 7, 8 [38] Zimin Xia, Yujiao Shi, Hongdong Li, and Julian FP Kooij. Adapting fine-grained cross-view localization to areas without fine ground truth. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 2, 3, 8 [39] Ting-Bing Xu and Cheng-Lin Liu. Data-distortion guided self-distillation for deep neural networks. In Proceedings of Teacher Param Update Fixed Prev. Student EMA Baseline Cross-Area Same-Area Mean() Median() Mean() Median() 4.65 5.02 4.49 5.20 4.46 4.49 4.26 4.81 1.28 1.37 1.22 1.44 1.43 1.44 1.37 1.61 Table 6. Localization performance comparison with different update strategies for the teacher network in the VIGOR dataset [43]. Prev Student means using the student from the last epoch as the teacher for the current epoch. Fixed means the teacher network does not update during training. EMA refers to the exponentially moving average teacher adopted in our method. A. Teacher-student Parameter Update Strategy We investigate different strategies for updating the teacher models parameters, including keeping the teacher models parameters fixed, denoted as Fixed, and using the student models parameters from the last epoch as the teacher models parameters for the current epoch, denoted as Prev Student. As shown in Tab. 6, all these different teacher parameters update strategy improves the performance over the baseline model, demonstrating the effectiveness of our key idea: using different FoVs to create discrepancy between teacher and student models, and this discrepancy works effectively as learning signal to encourage the model focusing on discriminative local features that are useful for cross-view matching. Compared to Fixed and our EMA parameters update strategy, Prev. Student suffers from abrupt parameters shifts, which causes significant location prediction inconsistency (before and after teacher parameters update) for some examples, resulting in inconsistent supervision which negatively affects the magnitude of the performance improvement. In contrast, our EMA update strategy combines the merits of Fixed and Prev. Student. It inherits the stability of fixed teacher model while also adaptively integrating the students refined knowledge, resulting in the most considerable performance improvement. B. Different Training Objectives for Self-"
        },
        {
            "title": "Distillation",
            "content": "To evaluate the impact of different training objectives, we performed an ablation study comparing Cross-Entropy (CE) and Kullback-Leibler Divergence (KLD) as loss functions for our student network. Table 7 shows that CE and KLD achieve similar localization accuracy. Loss KLD CE (ours) Cross-Area Same-Area Mean() Median() Mean() Median() 4.50 4.49 1.22 1.22 4. 4.26 1.37 1.37 Table 7. Localization performance comparison with different training objective in VIGOR dataset. Table 8. Localization performance comparison on VIGOR Same Area test set. Best in bold. The second-best is underlined. Here, * indicates fully supervised methods. Noise Method CVR[39]* SliceMatch[18]* Boosting[26]* CCVPE[33]* HC-Net[31]* GeoDistll(ours) CCVPE[33]* HC-Net[31]* GeoDistll(ours) 0 45 Localization Orientation Mean Median Mean Median 8.82 5.18 4.12 3.60 2.65 4.26 3.50 2.70 4.71 - - - 10.59 1.92 - 10.56 2.12 2.90 7.68 2.58 1.34 1.36 1.17 1.37 1.39 1.18 1.48 - - - 5.43 1.04 - 5.96 1.04 1.11 C. Comparison with Fully Supervised Methods in VIGOR Same Area Test Set Here, we supply the comparison of GeoDistill using G2SWeakly as backbone with state-of-the-art fully supervised methods in the Same-Area setting of the VIGOR dataset in Tab. 8. As anticipated, fully supervised methods generally perform better in the same-area setting than our weakly supervised GeoDistill framework. This is because fully supervised methods are trained with precisely annotated ground truth data within the same geographic area used for testing, allowing them to effectively learn areaspecific features and optimize for performance within the training distribution. In contrast, GeoDistill, trained with weakly supervised noisy GPS data and designed for crossarea generalization, is not explicitly optimized for samearea performance. For the cross-area evaluation, as highlighted in the main paper, GeoDistill achieves the second-best performance among the compared fully supervised approaches, highlighting its excellent generalization ability compared to fully supervised approaches. D. Evaluating GeoDistill with Unlabeled Target Domain Data For completeness, we evaluated GeoDistill under the unIntrigulabeled target domain data assumption of [34]. ingly, retraining CCVPE [33] with GeoDistill yielded similar performance using either source (4.05m mean error) or target domain data (3.95m) without GT, aligning with [34]s weakly supervised distillation (3.85m) for domain adapting. However, target domain data availability is often impractical, limiting real-world applicability. Moreover, while [34] uses reliable teacher predictions as pseudolabels for retraining, generalization to truly unseen regions remains concern. Like fully supervised methods, targetdomain fine-tuning approaches risk performance degradation when encountering new out-of-distribution data. Conversely, GeoDistills key advantage is achieving significant generalization gains by retraining solely on source domain data. This enables robust generalization to arbitrary unseen cities, offering more scalable and practical solution. GeoDistills ability to match target-domain adaptation performance without requiring target data, while ensuring superior generalization, underscores its practical utility and generalization prowess."
        }
    ],
    "affiliations": [
        "Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland",
        "ShanghaiTech University, China"
    ]
}