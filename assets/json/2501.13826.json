{
    "paper_title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos",
    "authors": [
        "Kairui Hu",
        "Penghao Wu",
        "Fanyi Pu",
        "Wang Xiao",
        "Yuanhan Zhang",
        "Xiang Yue",
        "Bo Li",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans acquire knowledge through three cognitive stages: perceiving information, comprehending knowledge, and adapting knowledge to solve novel problems. Videos serve as an effective medium for this learning process, facilitating a progression through these cognitive stages. However, existing video benchmarks fail to systematically evaluate the knowledge acquisition capabilities in Large Multimodal Models (LMMs). To address this gap, we introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU features a curated collection of 300 expert-level videos and 900 human-annotated questions across six disciplines, evaluating knowledge acquisition through stage-aligned question-answer pairs: Perception, Comprehension, and Adaptation. A proposed knowledge gain metric, {\\Delta}knowledge, quantifies improvement in performance after video viewing. Evaluation of LMMs reveals a steep decline in performance as cognitive demands increase and highlights a significant gap between human and model knowledge acquisition, underscoring the need for methods to enhance LMMs' capability to learn and adapt from videos."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 6 2 8 3 1 . 1 0 5 2 : r Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos Kairui Hu1 Penghao Wu1 Fanyi Pu1 Wang Xiao1 Yuanhan Zhang1 Xiang Yue2 Bo Li1 Ziwei Liu1* 1S-Lab, Nanyang Technological University 2Carnegie Mellon University https://videommmu.github.io/ Figure 1. An illustration of Video-MMMU: Evaluating the knowledge acquisition capability from videos through three cognitive stages: 1) Perception: if models can identify key information related to knowledge; 2) Comprehension: if models can interpret the underlying concepts; 3) Adaptation: if models can adapt the knowledge from videos to novel scenarios."
        },
        {
            "title": "Abstract",
            "content": "Humans acquire knowledge through three cognitive stages: perceiving information, comprehending knowledge, and adapting knowledge to solve novel problems. Videos serve as an effective medium for this learning process, facilitating progression through these cognitive stages. However, existing video benchmarks fail to systematically evaluate the knowledge acquisition capabilities in Large Multimodal 1 Models (LMMs). To address this gap, we introduce VideoMMMU, multi-modal, multi-disciplinary benchmark designed to assess LMMs ability to acquire and utilize knowledge from videos. Video-MMMU features curated collection of 300 expert-level videos and 900 human-annotated questions across six disciplines, evaluating knowledge acquisition through stage-aligned question-answer pairs: Perception, Comprehension, and Adaptation. proposed knowledge gain metric, knowledge, quantifies improvement in performance after video viewing. Evaluation of LMMs reveals steep decline in performance as cognitive demands increase and highlights significant gap between human and model knowledge acquisition, underscoring the need for methods to enhance LMMs capability to learn and adapt from videos. 1. Introduction Humans acquire knowledge through three fundamental cognitive stages outlined in Blooms taxonomy [8]: 1) perceiving information, 2) comprehending knowledge, and 3) adapting knowledge to solve novel problems. Video serves as an ideal medium for this learning process, enabling natural progression from information intake to practical application, making video-based learning valuable tool for knowledge acquisition [11, 30, 43]. Consider learning neural network forward propagation through video lectures  (Fig. 1)  : learners first recognize fundamental concepts like activation functions, then demonstrate understanding through exercises, and ultimately apply this knowledge to solve novel exam problems. This progression naturally aligns with Blooms cognitive stages, providing systematic framework for assessing knowledge acquisition from videos. For Large Multimodal Models (LMMs) to operate effectively in the wild like humans, learning from videos is an essential capability for continuous knowledge acquisition. However, existing video benchmarks lack systematic evaluation of this critical ability. To bridge this gap, we introduce Video-MMMU, massive multi-modal, multidisciplinary video benchmark that evaluates the knowledge acquisition capability from educational videos through three main features: 1) Knowledge-intensive Video Collection: Our dataset comprises 300 expert-level videos spanning 6 professional disciplines: Art, Business, Science, Medicine, Humanities, and Engineering, with 30 subjects distributed among them. 2) Knowledge Acquisition-based Question Design: Each video includes three question-answer pairs aligned with the three knowledge acquisition stages: Perception (identifying key information related to the knowledge), Comprehension (understanding the underlying concepts), and Adaptation (applying knowledge to new scenarios). 3) Quantitative Knowledge Acquisition Assessment: We propose knowledge acquisition metric, denoted as knowledge, to measure performance gains on practice exam questions after learning from videos. This metric enables us to quantitatively evaluate how effectively large multimodal models (LMMs) can assimilate and utilize the information presented in the videos to solve real-world, novel problems. We evaluate both open-source and proprietary LMMs on Video-MMMU, revealing several key findings: 1) Progressive Performance Decline: Model performance decreases as cognitive demands increase. While models perform relatively better on perception tasks, their accuracy drops notably on comprehension tasks and declines further on adaptation tasks. 2) Knowledge Acquisition from videos is Challenging: The knowledge acquisition metric knowledge reveals significant gap between human and model performance. While humans achieve substantial improvement (knowledge = 33.1%) after watching the videos, even the top performing models show smaller knowledge gains (GPT-4o [27]: knowledge = 15.6%, Claude-3.5-Sonnet [1]: knowledge = 11.4%). This limitation underscores challenge in current LMMs. While humans naturally acquire knowledge through video-based learning, having developed this capability through classroom learning and educational experiences throughout life, LMMs struggle to effectively learn from videos. These findings emphasize the need for further research to enhance how LMMs acquire and utilize video-based information, bringing them closer to humanlevel learning processes. 2. Related Work 2.1. VideoQA Benchmarks Existing video benchmarks focus primarily on visual understanding tasks, including action understanding [14, 22, 28, 38, 39, 44], temporal reasoning [3, 18, 20, 31, 34, 37, 42], and video captioning [4, 35, 40, 41, 53]. Several benchmarks enhance scene interpretation by incorporating external knowledge, including KnowIT-VQA [10] and WorldQA [50]. Recent benchmarks like Video-MME [9], MMBenchVideo [7], and MLVU [52] have expanded the scope to assess multi-tasking and multi-domain video understanding. While these benchmarks recognize videos as visual scenes for interpretation, Video-MMMU uniquely recognizes video as an educational medium, emphasizing knowledge-driven question-answering on videos. 2.2. Knowledge-driven Benchmarks As AI systems progress toward Expert AGI [24], knowledgedriven benchmarks have emerged to evaluate models professional expertise. Early benchmarks such as AGIEval [51] and ARC [2] focus on standardized exams and science questions, respectively. MMLU [13] expands evaluation across STEM disciplines, while MMLU-Pro [36] introduces more challenging reasoning-focused questions. Multi-modal benchmarks extend this evaluation scope further. ScienceQA [21] assesses multi-modal reasoning on elementary to highschool science questions. MMMU [45] advances to collegelevel questions requiring subject-specific knowledge and deliberate reasoning. MMMU-Pro [46] enhances MMMU questions for more robust evaluation. While these benchmarks evaluate models pre-trained knowledge and reasoning abilities on text and images, Video-MMMU uniquely focuses on assessing how effectively models can acquire and apply knowledge from videos. 2 Figure 2. Sampled Video-MMMU examples across 6 academic disciplines and 3 tracks. The examples are organized in two rows based on distinct video types: (1) Concept-Introduction videos (top row) focus on teaching factual knowledge, fundamental concepts, and theories through explanatory content, while (2) Problem-Solving videos (bottom row) demonstrate step-by-step solutions to an example question. 3. Video-MMMU Dataset We introduce Video-MMMU (Massive Multi-discipline Multimodal Understanding), video benchmark designed to evaluate knowledge acquisition from educational videos across 30 subjects in 6 professional disciplines: Art, Business, Medicine, Science, Humanities, and Engineering. The video distribution across disciplines is shown in Fig. 3a. 3.1. Video Collection The dataset consists of 300 college-level educational videos, systematically curated through rigorous three-phase process: 1) Topic Selection: Domain experts conduct comprehensive analysis of college curricula across 30 subjects, establishing diverse pool of 450 foundational assessment topics. 2) Video Curation: Leveraging GPT-4o [27], we generated 10 search queries per topic. These search queries are processed through the YouTube Data API to create an initial candidate video pool. 3) Quality Assurance: We implemented three-tier review protocol: First, annotators cross-check to filter out videos with poor audio-visual quality or irrelevant content. Second, we employ GPT-4o [27] to assess the technical depth of the videos by analyzing 10 sampled frames from each video. We prioritize in-depth lectures, tutorials, and detailed problem-solving demonstrations while excluding beginner-level introductions and superficial overviews. Finally, domain experts verify alignment with college curriculum standards and confirm appropriate domain knowledge depth. The Video-MMMU dataset comprises two distinct categories: 1) Concept-introduction Videos: These videos provide comprehensive explanations of factual knowledge, including fundamental concepts and theories. 2) Problemsolving Videos: These videos demonstrate step-by-step problem solutions, particularly in STEM disciplines where 3 systematic reasoning and detailed calculations are required. 3.2. QA Annotation 3.2.1. QA Taxonomy We annotate questions across three cognitive stages: Perception, Comprehension, and Adaptation, each assessing progressively deeper levels of knowledge acquisition. Perception Questions assess the ability to perceive information from videos through: 1) Optical Character Recognition (OCR): These questions require identifying and extracting key details from visual content, including formulas, data points, charts, and handwritten notes. An example is shown in Fig. 2 (Business), where the question requires extracting multiple economic variables from handwritten notes. 2) Automatic Speech Recognition (ASR): These questions assess the ability to accurately transcribe spoken content into text, as illustrated in Fig. 2 (Art). Comprehension Questions evaluate the ability to understand knowledge presented in videos through: 1) Concept Comprehension (CC): These questions assess understanding of concepts introduced in the videos. We primarily use multiple-answer multiple-choice (MAMC) format, where each question presents 4-10 statements about video content, with multiple correct statements possible. As shown in Fig. 2 (Humanities), one must identify all correct statements about the video content to demonstrate comprehensive understanding. 2) Problem-solving Strategy Comprehension (PSC): For videos demonstrating step-by-step solutions to example questions, an intuitive way to assess the understanding of the solution is to test the same question with different input values. As illustrated in Fig. 2 (Science), when video demonstrates trajectory time calculation with 25-degree angle, the question changes this to 30 degrees. This approach verifies comprehension of the underlying solution strategy rather than the memorization of answers. The cognitive difficulty lies between perception and adaptation, requiring new calculations while following the same reasoning process in the video. Adaptation Questions assess the ability to adapt video knowledge to new scenarios: 1) Case Study Analysis (CSA): These questions evaluate the application of concepts to novel real-world scenarios. As shown in Fig. 2 (Medicine), while the video explains various pelvic pathologies, the question requires analysis of new patients pelvic radiograph to identify specific abnormalities. This tests the models ability to adapt theoretical knowledge from videos to practical clinical diagnosis. 2) Problem-solving Strategy Adaptation (PSA): These questions evaluate how learners adapt learned solution methods to new problems. For instance, in Fig. 2 (Engineering), the video demonstrates the calculation of Fourier series for one type of waveform, while the question presents different waveform pattern. To solve this new problem, one needs to identify key similarities and differences between the video example and the new problem, then adjust the solution method accordingly. The distribution of these question types is illustrated in Fig. 3b. 3.2.2. Annotation and Quality Control Annotation Process: Our annotation follows multi-stage process to ensure quality: 1) Initial Annotation: Annotators thoroughly review each video and annotate three questions aligned with our cognitive tracks, following the QA taxonomy shown in Fig.3b. To enhance assessment rigor, we annotate 10 options for each multiple-choice question (MCQ). 2) Quality Assurance: Firstly, annotators cross-check each others questions for consistency and clarity. Secondly, QA pairs are processed by OpenAI o1[26] to refine the language and verify the correctness of ground-truth answers. Thirdly, domain experts review each question for technical accuracy and alignment with the intended cognitive stages. For Adaptation questions, experts verify that the question tests the same knowledge presented in the video but in novel scenario, ensuring they utilize the same concepts, formulas, or similar problem-solving strategies. Finally, we employ Gemini 1.5 Pro [32] to analyze each video-question pair and determine whether audio might be helpful to solve the question, as shown in Fig. 3c. This analysis will benefit more future Large Multimodal Models (LMMs) with audio processing capabilities. Question Sources: For the Perception and Comprehension tracks, questions are manually created by our annotators. For the Adaptation track, which requires practical problems from exams and case studies, our approach varies by discipline. In Science, Engineering, Medicine, and Business, we source questions from MMMU [45] and MMMU-pro [46], which provide validated college exam questions well suited for testing knowledge adaptation. For Art and Humanities, where adaptation requires more context-dependent assessment, we manually create case study questions to ensure alignment with video concepts. 3.3. Comparison with Existing Benchmarks Video-MMMU distinguishes itself through its emphasis on how models can learn and apply knowledge from professional educational videos. Our videos feature comprehensive lectures, tutorials, and step-by-step problem-solving demonstrations, delivering dense information through multiple visual formats, including charts, diagrams, and handwritten explanations. With an average duration of 506.2 seconds, the videos provide extensive coverage of domain-specific knowledge across various disciplines. As shown in Table 1, our questions are substantially longer than existing benchmarks, averaging 75.7 words per question, reflecting the complexity of knowledge-driven evaluation. We systematically evaluate knowledge acquisition from videos through three cognitive stages. The Adaptation track advances video-based learning evaluation beyond basic content understanding to assess 4 Figure 3. Taxonomy of QA types and video disciplines. Benchmarks Video Domain Question Video Knowledge Length Duration driven Video-MME [9] MMBench-Video [7] Video-Bench [25] TempCompass [20] MVBench [17] AutoEval-Video [5] Open Open Open Open Open Open Video-MMMU Professional 35.7 10.9 21.3 49.2 27.3 11. 75.7 1017.9 165.4 56.0 11.4 16.0 14.6 506.2 Table 1. Comparison of Video-MMMU and other widely adopted video benchmarks. Inputs. We provide videos and questions as inputs for the Perception and Comprehension tracks. For the Adaptation track, we append the questions image to the end of each video. We add prompt to indicate that the image for the Adaptation track question appears in the final frame. Evaluations. We evaluate model outputs using an automated, rule-based pipeline. The system employs regular expressions to extract key elements such as option letters and numerical values. Responses lacking valid answers are marked as incorrect. We use the micro-averaged accuracy as our evaluation metric. The evaluation is conducted using LMMs-Eval [47]. how effectively models can apply the acquired knowledge to novel problems. 4.2. Main Results 4.2.1. Performance by Track 4. Experiments 4.1. Settings Baselines. We evaluate open-source LMMs including LLaVA-OneVision [15], LLaVA-Video [49], LongVA [48], VILA-1.5 [19], Qwen2-VL [33], InternVL2 [6], Llama3.2 [23], MAmmoTH-VL [12], Aria [16]; and proprietary models GPT-4o [27], Gemini 1.5 Pro [32], Gemini 1.5 Flash [32], Claude-3.5-Sonnet [1]. The numbers of sampled frames are 32 for LLaVA-OneVision, 64 for LLaVA-Video, 64 for LongVA, 32 for VILA-1.5, 32 for InternVL2, 10 for Llama-3.2, 32 for MAmmoTH-VL, 64 for Aria, 20 for Claude-3.5-Sonnet, 50 for GPT-4o. Human Experts. To assess the performance of Human Experts, we recruited senior undergraduate students and instructed them to complete the following tests: The students first attempted the Adaptation question without viewing the videos. Subsequently, they watched each assigned video and answered the corresponding Perception, Comprehension, and Adaptation questions. While students could refer to course materials and notes, they were not allowed to search for answers on the Internet. Human vs. Model Performance: Human experts outperform models across all tracks, with Claude achieving the highest model scores but still showing gap to humans. Both humans and models exhibit declining performance from perception through comprehension to adaptation, indicating that deeper cognitive stages require more advanced capabilities. Perception Track: Many models achieve an accuracy over 50%, suggesting perception is more fundamental capability among the three stages. Comprehension Track: Comprehending college-level knowledge from videos requires pre-trained knowledge as foundation. Compared to the Perception score, most open-source models show 10 20% decline in Comprehension score, while proprietary models show less performance decline and generally achieve higher comprehension scores, demonstrating their superior capabilities in comprehending knowledge-intensive videos. Adaptation Track: Adaptation emerges as the most challenging stage, with most models scoring below 50%. Even top-performing models like Claude-3.5-Sonnet exhibit substantial performance decline in Adaptation. This indicates natural gap between theoretical understanding and practical application. While models might understand the knowledge from videos at surface level, they currently lack 5 Model Overall Results by Track Results by Discipline Perception Comprehension Adaptation Art. Biz. Sci. Med. Hum. Eng. Random Choice Human Expert Proprietary LMMs Gemini 1.5 Flash [32] Gemini 1.5 Pro [32] GPT-4o [27] Claude-3.5-Sonnet [1] Open-source LMMs 14.00 74.44 49.78 53.89 61.22 65. VILA1.5-8B [19] 20.89 LongVA-7B [48] 23.98 Llama-3.2-11B [23] 30.00 LLaVA-OneVision-7B [15] 33.89 VILA1.5-40B [19] 34.00 LLaVA-Video-7B [49] 36.11 InternVL2-8B [6] 37.44 MAmmoTH-VL-8B [12] 41.78 LLaVA-OneVision-72B [15] 48.33 LLaVA-Video-72B [49] 49.67 Aria [16] 50.78 12.00 84.33 57.33 59.00 66.00 72.00 20.33 24.00 35.67 40.00 38.67 41.67 47.33 51.67 59.67 59.67 65.67 14.00 78.67 49.00 53.33 62.00 69. 17.33 24.33 32.33 31.00 30.67 33.33 33.33 40.00 42.33 46.00 46.67 16.00 60.33 11.11 80.95 12.88 78.79 12.12 74.24 22.48 70. 10.48 84.76 13.57 69.91 43.00 49.33 55.67 55.67 25.00 23.67 22.00 30.67 32.67 33.33 31.67 33.67 43.00 43.33 40.00 63.49 57.14 69.52 66.67 34.92 41.27 39.68 49.21 57.14 65.08 55.56 47.62 61.91 69.84 71. 53.03 59.09 66.88 75.00 14.39 20.46 28.79 29.55 27.27 34.09 34.09 37.88 46.21 44.70 47.73 43.18 49.10 51.55 56.06 19.70 21.97 21.21 34.85 23.49 32.58 30.30 36.36 40.15 41.67 44.70 49.61 57.42 64.76 58.14 19.38 24.03 35.66 31.78 37.99 42.64 34.11 36.43 54.26 58.92 58. 59.05 58.10 69.52 75.24 21.91 23.81 33.33 46.67 41.91 45.71 41.91 49.52 60.00 57.14 62.86 45.72 50.31 57.13 66.08 21.53 23.01 28.91 29.20 32.45 27.43 38.05 43.95 43.95 45.13 43.66 Table 2. Video-MMMU Evaluation Results across three cognitive tracks (Perception, Comprehension, Adaptation) and six disciplines (Art, Business, Science, Medicine, Humanities, Engineering). the advanced capability to effectively acquire and apply what they learned from the video to solve practical problems. 4.2.2. Performance by Discipline Model performance varies across disciplines. Models demonstrate superior performance in Art and Humanities disciplines, where videos primarily focus on conceptual presentation. In comparison, they achieve lower accuracies in Science, Engineering, Business, and Medicine, which demand quantitative reasoning and interpretation of detailed technical visuals such as diagrams and handwritten notes. This performance differential suggests models are generally more adept at processing factual knowledge but underperform in domains requiring complex computation, deliberate reasoning, and visual analysis. 4.3. Impact of Audio Transcript Audio conveys information in knowledge-intensive videos. To study the impact of audio transcripts, we use OpenAI Whisper [29] to generate audio transcripts and append them to the input prompt. We conduct evaluation on the topperforming open-source model Aria [16] and proprietary model Claude-3.5-Sonnet [1]. Figure 4. Performance comparison across tracks before and after adding audio transcripts. As shown in Fig. 4, audio transcripts yield overall performance improvements across different evaluation tracks. In the Comprehension track, the enhancement is most pronounced, reflecting audios contribution to video content understanding. Similarly, the Perception track demonstrates 6 (a) Comparison of knowledge (performance improvement in the Adaptation track after watching the video compared to before). (b) Comparison of Wrong-to-Right Rate (the percentage of Adaptation track questions that were initially answered incorrectly without the video but correctly after watching the video) and Right-to-Wrong Rate (vice versa). Figure 5. Key findings in the experiment of knowledge. performance gains, suggesting that audio enhances information extraction from videos. The Adaptation track, however, shows decrease in performance. This decline indicates that while audio enriches basic understanding, it might complicate the adaptation of knowledge to novel scenarios. These contrasting effects reveal trade-off: audio transcripts enhance immediate comprehension but potentially constrain models ability to adapt knowledge to new scenarios. 5. Knowledge Acquisition in Adaptation Track 5.1. Settings We introduce knowledge acquisition metric knowledge to measure how much knowledge models gain from videos through their performance improvement on practical exam questions in the Adaptation track. We define knowledge as: knowledge = Accpost Accpre 100% Accpre 100% where Accpre and Accpost represent the accuracy before and after watching the video, respectively. This normalized metric accounts for different baseline difficulty levels. For example, improving from 90% to 95% (knowledge = 50%) indicates more substantial video-based learning than improving from 0% to 5% (knowledge = 5%). We evaluate knowledge across top-performing open-source and proprietary models. 5.2. Findings Human-Model Knowledge Acquisition Gap: Fig. 5a reveals substantial disparity between human and model learning capabilities. Humans demonstrate knowledge of 33.1% after viewing the videos, while the best-performing model GPT-4o achieves only 15.6%. Some models even exhibit negative knowledge, suggesting their performance declines after video exposure. This gap highlights fundamental challenge in current models. Humans naturally acquire information through video-based learning, having developed this capability through classroom education and video content throughout their lives. While many models can process video information, they struggle to effectively learn new knowledge from the video and apply it in practice. Video Impact on Model Responses: While low knowledge scores might suggest limited net knowledge gain, models responses change substantially after watching the videos. As shown in Fig. 5b, we analyze these changes through two metrics: Wrong-to-Right Rate (the percentage of questions initially answered incorrectly but correctly after watching videos) and Right-to-Wrong Rate (the percentage of questions correctly answered before but incorrectly after watching videos). We define the Wrong-to-Right Rate as: Wrong-to-Right Rate = NWrong-to-Right NWrong-before 100% , where NWrong-to-Right refers to the number of questions that were answered incorrectly before watching the video but correctly after watching the video, and NWrong-before is the total number of questions that were answered incorrectly before watching the video. Similarly, we define the Right-to-Wrong Rate as: Right-to-Wrong Rate = NRight-to-Wrong NRight-before 100% , where NRight-to-Wrong refers to the number of questions that were answered correctly before watching the video but incorrectly after watching the video, and NRight-before is the total number of questions that were answered correctly before watching the video. Interestingly, models achieve moderate Wrong-to-Right Rates (e.g., Gemini-1.5-Pro: 29.5%), demonstrating certain ability to acquire knowledge from videos. However, their high Right-to-Wrong Rates (e.g., LongVA: 55.0%) significantly offset these gains, indicating that they struggle to maintain their initial correct answers while processing new video information. In contrast, human experts demon7 Figure 6. Case of Method Adaptation Error. The model can recall the correct knowledge from the video but fails to adapt the method to new scenario. More error cases are analyzed in the Appendix. knowledge while preserving their prior knowledge. These findings highlight gap between human and model capabilities in video-based learning, particularly in maintaining existing knowledge while processing new information from videos. 5.3. Error Analysis We analyzed the Claude-3.5-Sonnet errors in the Adaptation track by examining 100 randomly sampled error cases. Human annotators analyzed these cases to identify the root causes of mispredictions. The distribution of these errors is shown in Fig. 7, with more error cases provided in the Appendix. Method Selection Error (8%): The models initial thinking direction is incorrect. For example, the model fails to adopt the correct solution strategy demonstrated in the video. Method Adaptation Error (64%): These represent cases where the model can correctly recall and understand the methods demonstrated in the video but fails to adapt the Figure 7. Distribution of the 100 human-annotated errors in Claude3.5-Sonnet. strate effective knowledge acquisition with higher Wrongto-Right Rate (40.4%) and lower Right-to-Wrong Rate (10.7%). This indicates humans ability to integrate new 8 method to new scenarios properly. For example, Fig. 6 shows how models can struggle with subtle scenario differences between the video example and the Adaptation question. While the model recalls the core DFS principles from simple tree example in the video, it fails to adapt these principles flexibly when working with more complex graph containing cycles. This type of error reveals its limitations in video-based learning when applying the learned methods across different contexts. Question Misreading Error (15%): These errors stem from misinterpreting the question requirements, such as misreading numerical values or conditions. Such errors are unrelated to the models ability to apply knowledge from videos. Other Errors: These include Refuse to Answer (4%), where models express uncertainty and decline to provide an answer; Annotation error (4%), where the annotation is inaccurate; and Answer Extraction error (5%), where we failed to extract the answer from the longer output. Our experiment on knowledge provides insights for future research in knowledge acquisition from videos: 1) Models showcase certain ability to acquire knowledge from videos, as indicated by their modest Wrong-to-Right Rates. However, the high Right-to-Wrong Rates often negate these gains, suggesting that models struggle to retain their initial correct reasoning when processing new information from video. 2) The Question Misreading and Method Selection Errors highlight the fundamental limitations in processing knowledgeintensive videos. Accurate question interpretation and thorough understanding of video knowledge are crucial for successful knowledge application. 3) The significant proportion of Method Adaptation errors reveals gap between comprehension and adaptation capabilities, suggesting that applying the knowledge from videos to solve novel, practical scenario remains challenging for the current models. 6. Conclusion Video-MMMU systematically evaluates how large multimodal models (LMMs) acquire knowledge from videos through three cognitive stages: Perception, Comprehension, and Adaptation. Through our proposed knowledge metric, we reveal gap between human and model performance, particularly in adapting acquired knowledge to novel, practical scenarios. Our insights from Video-MMMU underscore the critical need for future research to enhance LMMs ability to learn and apply knowledge from video content effectively."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude Team. Introducing Claude 3.5 Sonnet. https://www.anthropic.com/claude/sonnet, 2024. 2, 5, 6, 1 [2] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, and Michael Witbrock. systematic classification of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 6070, Melbourne, Australia, 2018. Association for Computational Linguistics. 2 [3] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. Temporalbench: Towards fine-grained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 2 [4] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher D. Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 2 [5] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. arXiv preprint arXiv:2311.14906, 2023. [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5, 6, 2 [7] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 2, 5 [8] Mary Forehand. Blooms taxonomy. Emerging perspectives on learning, teaching, and technology, 41(4):4756, 2010. 2 [9] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5 [10] Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. Knowit vqa: Answering knowledge-based questions about videos. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. 2 [11] Michail Giannakos. Exploring the video-based learning research: review of the literature. British Journal of Educational Technology, 44(6):E191E195, 2013. 2 [12] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. 2024. 5, 6, 2 [13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 2 [14] Muhammad Uzair khattak, Muhammad Ferjad Naeem, Jameel Hassan, Naseer Muzzamal, Federcio Tombari, Fahad Shahbaz Khan, and Salman Khan. How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms. arXiv:2405.03690, 2024. 9 [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5, 6, 2 [16] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. 5, 6, 2 [17] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, 2024. 5 [18] Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models. arXiv preprint arXiv:2311.17404, 2024. 2 [19] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023. 5, 6, 2 [20] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024. 2, 5 [21] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [22] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. In Advances in Neural Information Processing Systems, pages 4621246244. Curran Associates, Inc., 2023. 2 [23] Meta. Llama 3.2: Revolutionizing Edge AI and Vision with Open, Customizable Models. https://ai.meta.com/ blog/llama-3-2-connect-2024-vision-edgemobile-devices/, 2024. 5, 6 [24] Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Position: Levels of AGI for operationalIn Proceedings of the izing progress on the path to AGI. 41st International Conference on Machine Learning, pages 3630836321. PMLR, 2024. 2 [25] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. 5 [26] OpenAI. Introducing openai o1. https://openai.com/ o1/, 2024. 4 [27] OpenAI. Hello gpt4-o. https://openai.com/index/ hello-gpt-4o/, 2024. 2, 3, 5, 6, 1 [28] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. 2 [29] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022. 6 [30] Marija Sablic, Ana Mirosavljevic, and Alma Škugor. Videobased learning (vbl)past, present and future: An overview of the research published from 2008 to 2019. Technology, Knowledge and Learning, 26(4):10611077, 2021. 2 [31] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 2 [32] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 4, 5, 6, 1, [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5 [34] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2 [35] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In The IEEE International Conference on Computer Vision (ICCV), 2019. 2 [36] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. 2 [37] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interarXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 2 [38] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97779786, 2021. 2 [39] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. In European Con10 Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, Mexico City, Mexico, 2024. Association for Computational Linguistics. 2 [52] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2 [53] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, pages 7590 7598, 2018. 2 ference on Computer Vision, pages 3957. Springer, 2025. 2 [40] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia, 2017. [41] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [42] Kexin Yi*, Chuang Gan*, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2020. 2 [43] Ahmed Mohamed Fahmy Yousef, Mohamed Amine Chatti, and Ulrik Schroeder. The state of video-based learning: International Journal on review and future perspectives. Advances in Life Sciences, 6(3):122135, 2014. 2 [44] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134, 2019. 2 [45] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 2, 4 [46] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmupro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 2, [47] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 5 [48] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 5, 6, 2 [49] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 5, 6, 2 [50] Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. Worldqa: Multimodal world knowledge in videos through long-chain reasoning. arXiv preprint arXiv:2405.03272, 2024. 2 [51] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and 11 Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Subjects by Discipline Discipline Art Business Science Medicine Subjects Art History, Art Theory, Design, Music Accounting, Economics, Finance, Manage, Marketing Biology, Chemistry, Geography, Math, Physics Basic Medical Science, Clinical Medicine, Diagnostics and Laboratory Medicine, Pharmacy, Public Health Humanities History, Literature, Psychology, Sociology Engineering Agriculture, Architecture and Engineering, Computer Science, Electronics, Energy and Power, Materials, Mechanical Engineering Table 3. Subjects categorized under six disciplines. 8. Additional Knowledge Acquisition Experiment Results We present the results of the knowledge experiment in Table 4. This table includes detailed breakdown of the number of questions that transitioned from Wrong-to-Right and Rightto-Wrong, along with the corresponding rates. The knowledge metric reveals gap between human experts and models, particularly in their ability to learn new information from videos. This skill, which humans exhibit naturally through video-based learning, arises from our longstanding reliance on videos as medium for acquiring knowledge. Humans have developed proficiency in extracting, retaining, and applying information from visual content, making video learning an essential component of natural knowledge acquisition. For models to perform effectively in real-world environments, the ability to learn and adapt from videos is crucial. This capability would allow models to continuously evolve and refine their understanding, thereby enhancing their utility in dynamic and complex scenarios. However, the result suggests that current models are not yet capable of effectively acquiring new knowledge from video and applying it in practice. This suggests that future research needs to focus on improving how models acquire knowledge from videos - Figure 8. Prompt for Adaptation track. specifically, their ability to understand, remember, and apply information from video content. These improvements will be crucial for future LMMs to work effectively in the wild. 9. Prompt for Adaptation Track In the adaptation track, we append the questions image to the end of each video. We introduce the prompt as shown in Fig. 8. 10. Prompt for Determining the Helpfulness of"
        },
        {
            "title": "Audio",
            "content": "For all samples in Video-MMMU, we employ Gemini 1.5 Pro [32] to analyze each video-question pair and determine if audio might be helpful to solve the question, as shown in Fig. 3c. This analysis will benefit more future Large Multimodal Models (LMMs) with audio processing capabilities. We introduce the prompt as shown in Fig. 9. 11. Annotation Pipeline We illustrate our pipeline for video collection and QA annotation in Fig. 10. 12. More Error Analysis This section presents comprehensive analysis of error cases across all three tracks. We begin by examining errors made by Claude-3.5-Sonnet [1] in the Adaptation track. Specifically, Fig. 11 illustrates Method Selection Errors, while Fig. 12 demonstrates Question Misreading Errors. We also analyze error cases by GPT-4o [27] in the Adaptation track. Fig. 13 and Fig. 14 present Method Adaptation Error and Question Misreading Error, respectively."
        },
        {
            "title": "Model",
            "content": "knowledge (%) Wrong-to-Right Right-to-Wrong No. of Questions Rate (%) No. of Questions Rate (%) Human Expert GPT-4o [27] Claude-3.5-Sonnet [1] VILA-1.5-40B [19] Gemini-1.5-Pro [32] LLaVA-Video-72B [49] LLaVA-OneVision-72B [15] VILA-1.5-8B [19] Aria [16] MAmmoTH-VL-8B [12] Gemini-1.5-Flash [32] LLaVA-Video-7B [49] LLaVA-OneVision-7B [15] LongVA [48] InternVL2-8B [6] 33.1 15.6 11.4 9.4 8.7 7.1 6.6 5.9 3.2 1.5 -3.3 -5.3 -5.6 -7.0 -8. 72 44 42 57 49 40 37 48 47 48 39 35 36 29 46 40.4 28.0 28.8 25.2 29.5 22.0 20.6 20.2 25.4 23.9 23.5 18.5 18.2 13.6 24.3 13 19 30 34 33 29 28 35 42 45 42 47 43 47 61 10.7 13.3 19.5 45.9 24.6 24.6 23.3 56.5 36.5 45.5 31.3 42.3 42.2 54.0 55.0 Table 4. Additional Knowledge Acquisition Experiment Results with Delta (%) values. Furthermore, we investigate error cases in both the Perception and Comprehension tracks. For the Perception track, we present two representative error cases in Fig. 15 and Fig. 16. Similarly, for the Comprehension track, we analyze two error cases shown in FigFig. 17 and Fig. 18. Each case study includes detailed analysis of the observed errors. 13. Wrong-to-Right Case Analysis For the Adaptation track, we also analyze the Wrong-toRight examples where models successfully learned from video content to correctly solve Adaptation track questions. For Claude-3.5-Sonnet [1], we present three such examples in Fig. 19, Fig. 20, and Fig. 21. Additionally, we present Wrong-to-Right example of GPT-4o [27] in Fig. 22. Each case study provides detailed analysis of how the model successfully adapted its knowledge. 2 Figure 9. Prompt for determining the helpfulness of audio. 3 Figure 10. An illustration of the dataset curation pipeline. 4 Figure 11. sample error case in the Adaptation track: Method Selection Error by Claude-3.5-Sonnet. 5 Figure 12. sample error case in the Adaptation track: Question Misreading Error by Claude-3.5-Sonnet. 6 Figure 13. sample error case in the Adaptation track: Method Adaptation Error by GPT-4o. 7 Figure 14. sample error case in the Adaptation track: Question Misreading Error by GPT-4o. 8 Figure 15. sample error case in the Perception track. 9 Figure 16. sample error case in the Perception track. 10 Figure 17. sample error case in the Comprehension track. 11 Figure 18. sample error case in the Comprehension track. 12 Figure 19. Wrong-to-Right example of Claude-3.5-Sonnet in the Adaptation track. 13 Figure 20. Wrong-to-Right example of Claude-3.5-Sonnet in the Adaptation track. 14 Figure 21. Wrong-to-Right example of Claude-3.5-Sonnet in the Adaptation track. 15 Figure 22. Wrong-to-Right example of GPT-4o in the Adaptation track."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "S-Lab, Nanyang Technological University"
    ]
}