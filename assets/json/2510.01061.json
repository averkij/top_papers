{
    "paper_title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction",
    "authors": [
        "Mark Boss",
        "Andreas Engelhardt",
        "Simon Donné",
        "Varun Jampani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 6 0 1 0 . 0 1 5 2 : r RESWD: RESTIRD, NOT SHAKEN. COMBINING RESERVOIR SAMPLING AND SLICED WASSERSTEIN DISTANCE FOR VARIANCE REDUCTION Mark Boss1 Andreas Engelhardt1,2 Simon Donné1 Varun Jampani1 1Stability AI 2University of Tübingen Reference Image Watercolor Hand Painted Fantasy City Image Generation + ReSWD Color Matching <SOPNode> <Slope> 0.902 0.821 0.892 </Slope> <Offset> -0.006 0.152 0. </Offset> <Power> 1.587 1.302 1.061 </Power> </SOPNode> <SatNode> <Saturation>0.268</Saturation> </SatNode> Distribution Matching Diffusion Guidance Color Correction Figure 1: Applications. SWD can be used in various applications and with ReSWD we enhance their performances. Here, we show diffusion guidance, color corrections and general distribution matching."
        },
        {
            "title": "ABSTRACT",
            "content": "Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high-dimensional distributions. The Sliced Wasserstein Distance (SWD) offers scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page with code: https://ReservoirSWD.github.io"
        },
        {
            "title": "INTRODUCTION",
            "content": "Distribution matching is central problem in computer vision and graphics: from classical tasks such as histogram matching to more nuanced applications such as color grading (Pitie et al., 2005; Rabin et al., 2012; Bonneel et al., 2015; He et al., 2024), texture alignment (Elnekave & Weiss, Work done during internship at Stability AI. 2022; Heitz et al., 2021), and the guidance of generative models Lobashev et al. (2025). Among the available metrics, the Wasserstein distance has emerged as particularly powerful due to its ability to capture shifts between distributions. However, the Wasserstein distance suffers from the curse of dimensionality and has practical convergence rate scaled with the d-dimensional distribution O(cid:0)n 1 (cid:1), with data points. This limits its direct use in many iterative optimization settings. The Sliced Wasserstein Distance (SWD) (Pitie et al., 2005; Bonneel et al., 2015; Heitz et al., 2021) offers practical alternative by projecting distributions in random directions and averaging their 1-D Wasserstein costs. This reduces the computational burden to sequence of 1-D sorting problems but introduces new challenge: the expectation over projection directions is typically approximated via Monte Carlo (MC) sampling, which suffers from high variance. Increasing the directions would result in the true expected value, but the complexity of SWD scales with the number of random directions as O(cid:0)M log n(cid:1). Hence, the curse of dimensionality is introduced again indirectly with the number of projections, which should increase with the dimensionality of the data. In optimization scenarios, where such SWD metrics are used as loss functions to optimize neural networks, this variance directly results in noisy gradients and slower convergence. variety of variance reduction techniques have been explored for MC estimators, but most remain underutilized in distribution matching objectives. In this work, we draw inspiration from recent advances in rendering, particularly the ReSTIR resampling framework (Bitterli et al., 2020), and adapt Weighted Reservoir Sampling (WRS) (Efraimidis & Spirakis, 2006; Chao, 1982) to the SWD setting. The resulting estimator, which we term Reservoir SWD (ReSWD), continuously reuses and reweighs the most informative projection directions throughout the optimization. Intuitively, ReSWD preferentially retains those directions where the two distributions are most dissimilar, thus concentrating computational effort on projections that produce stronger and more stable gradients. This simple but effective modification significantly reduces stochastic variance while preserving unbiasedness due to the weighting of the WRS, leading to faster and more robust optimization. We demonstrate the advantages of ReSWD on two real-world distribution matching problems of color correction and color diffusion guidance, as shown in Fig. 1, as well as synthetic general distribution matching problems, showing clear improvements over the standard SWD and existing variance reduction baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Distribution Matching and Optimal Transport. Distribution matching is the general task of aligning two distributions. particularly effective way of achieving this is with Optimal Transport (OT) by minimizing the cost of moving the probability mass, which is often performed using Wasserstein distances (Villani et al., 2008; Peyré & Cuturi, 2019). Entropic regularization accelerates OT through Sinkhorn iterations (Cuturi, 2013), leading to scalable solvers and Sinkhorn divergences that interpolate between OT and kernel Maximum Mean Discrepancies (MMD) (Feydy et al., 2019). OT has been widely applied to textures, color transfer, and barycenters (Rabin et al., 2012; Bonneel et al., 2015; Pitie et al., 2005). To address high-dimensional costs, Sliced Wasserstein (SW) replaces couplings with averages of 1-D transports. Early works introduced SW barycenters and Radon/Projected variants that are fast and differentiable (Bonneel et al., 2015; Rabin et al., 2012). Extensions include Max-SW, which selects the most contributing directions instead of averaging over them (Deshpande et al., 2019), and several other techniques to improve efficiency and optimization performance (Kolouri et al., 2019; Nguyen et al., 2021; 2023; Nguyen & Ho, 2023; Nguyen et al., 2024b). SW has also been studied through geometry and flows: gradient flows as generative dynamics with long-time analysis (Cozzi & Santambrogio, 2024; Vauthier et al., 2025), intrinsic geometry of SW space (Park & Slepˇcev, 2025), extensions to CartanHadamard manifolds (Bonet et al., 2025), and stereographic spherical SW for curved domains (Tran et al., 2024). Recent work further reduces variance via quasi-Monte Carlo and control variates (Nguyen et al., 2024a; Nguyen & Ho, 2024). In practice, SW is widely used as loss in graphics, vision, and generative modeling e.g., for textures, patch statistics, and color transfer due to stable gradients and favorable sample complexity (Heitz et al., 2021; Elnekave & Weiss, 2022; Pitie et al., 2005; Wu et al., 2024). Our work ties into the work selecting the most contributing directions similar to Max-SW but achieves this by building reservoir of high contributing directions during the training. This improves efficiency, as we keep more directions in optimization. It also ties in with the work on variance reduction while remaining unbiased, as our technique is inspired by variance reduction techniques in real-time path tracing (Bitterli et al., 2020). Color Transfer and Correction. Classical color transfer methods framed recoloring as histogram or distribution matching across images, often using statistical metrics or optimal transport (Pitie et al., 2005; Rabin et al., 2012; Bonneel et al., 2015). These techniques established the foundation for perceptual color differences in imaging. More recently, multiscale OT and Wasserstein-based metrics have been proposed as perceptually faithful color difference measures (He et al., 2024). With deep learning, style transfer methods enabled more expressive color and appearance manipulation. CNN-based approaches introduced artistic and photorealistic stylization (Gatys et al., 2016; Luan et al., 2017; Huang & Belongie, 2017; Li et al., 2017; 2018; Yoo et al., 2019; An et al., 2020; Chiu & Gurari, 2022; Hong et al., 2021), with adaptive instance normalization and feature transforms widely adopted to control color and tone. These methods evolved from hand-crafted global mappings to neural architectures capable of spatially aware and semantically coherent color control. With the advance of text-guided diffusion models (Rombach et al., 2022b;a), additional mechanisms to control the style such as LoRA (Hu et al., 2021), IP-Adapter (Zhang et al., 2023b), ControlNets (Zhang et al., 2023a) are introduced. recent work by Lobashev et al. (Lobashev et al., 2025) also demonstrated that traditional constraints based on OT can be used during generation to enforce color distribution. Our work can be used to enhance traditional color matching works with improved efficiency and to enable more novel color-guided diffusion tasks."
        },
        {
            "title": "3 METHOD",
            "content": "In our setting, we only ever compare empirical discrete distributions, that is, two sets of samples = {x1, . . . , xNX } Rd and = {y1, . . . , yNY } Rd, drawn from the underlying distributions and Y. 3.1 PRELIMINARIES Wasserstein Distance. The Wasserstein p-distance measures differences between two empirical distributions. In the 1D case relevant to our setting, given two sets of samples and , the Wasserstein distance can be computed simply by sorting both sets and comparing the corresponding order statistics: Wp(X, ) = xi yip , (1) (cid:33)1/p (cid:32) 1 (cid:88) i= This efficient formulation has complexity O(n log n) due to sorting, in contrast to the cubic complexity of the general d-dimensional case (Rabin et al., 2012; Bonneel et al., 2015; Pitie et al., 2005). Sliced Wasserstein Distance (SWD). The Sliced Wasserstein Distance proposes to stochastically approximate the true Wasserstein distance in multi-dimensional distributions. It is defined as (Pitie et al., 2005; Heitz et al., 2021; Elnekave & Weiss, 2022): Sp(X, ) = EθU (Sd1) (cid:104) Wp(πθX, πθY ) (cid:105) , (2) where πθ indicates the projection onto the unit-normal vector θ (uniformly sampled from Sd1, the d-dimensional unit sphere Rd). In practice, the expectation is often approximated via Monte Carlo (MC) integration: Sp(X, ) wi (cid:88) i=1 Wp(πθiX, πθiY ). (3) New candidate Directions M1 M3 Reservoir Projection K1 K2 K4 Distribution 1 Distribution 2 Weight Update Reservoir K1 M3 K3 Calculate Loss Figure 2: Overview. During the optimization we keep reservoir of highly influential directions. We use the Sliced Wasserstein Distance (SWD) as proxy metric for the reservoir update and the final optimization loss. Note that only directions in the reservoir influence the optimization to remain unbiased. with random uniformly sampled directions θi and wi being the sampling weights, i.e. wi = 1 with uniform direction sampling. The resulting complexity is therefore O(Ln log n). The SWD is an unbiased estimate of the true Wasserstein distance (Pitie et al., 2005; Heitz et al., 2021). With modern frameworks such as PyTorch or Tensorflow, the entire metric can be trivially implemented fully differentiable, which enables usage in optimization settings. However, due to the MC integration, SWD can lead to noisy gradients, which we aim to solve with ReSWD. 3.2 RESERVOIR SLICED WASSERSTEIN DISTANCE (RESWD) Although SWD is highly efficient in the calculation at high dimensions, it still suffers from high variance due to the MC integration from random directions. Especially, when SWD is used as loss during an optimization, this leads to noisy gradients. In computer graphics, variance is often also an issue, especially in real-time path tracing where MC integration is also usually employed. Inspired by ReSTIRs resampled-importance-sampling (Bitterli et al., 2020), we incorporate the Weighted Reservoir Sampling (WRS) mechanism into SWD. This allows the reuse of information between optimization steps, and hence faster convergence overall with minimal performance penalties. Weighted Reservoir Sampling is family of algorithms that draw fixed subset from weighted stream of candidates, i.e. directions, in single pass (Chao, 1982; Efraimidis & Spirakis, 2006). Suppose that we wish to maintain reservoir of candidates while processing candidate pool of + items, where denotes the number of newly drawn candidates. Each candidate θj is associated with non-negative weight wj, and the goal is to select exactly survivors such that the marginal inclusion probability of each element is proportional to wj. This is achieved by assigning to every candidate random key (Efraimidis & Spirakis, 2006) 1/wj kj = (4) retaining the elements with the smallest keys. The resulting selection is unbiased: the expected inclusion probability of each element is proportional to its weight, while the reservoir size remains fixed (Efraimidis & Spirakis, 2006). In our context, WRS allows efficient reallocation of computational effort toward projections with higher contribution to the optimization loss, while preserving Monte Carlo unbiasedness. uj U(0, 1), , In Fig. 2, we present an overview of our proposed method. Let be pool of +M 1-D projection directions θ Sd1. For every θ we compute the 1-D p-power Wasserstein cost D(θ) = Wp (cid:0)πθµ, πθν(cid:1) (5) Then each distance calculation performs the following steps. Step 0: Time-decay reweighting. As SWD is most often used in optimization scenarios, we introduce τ as the time decay constant to place more emphasis on newer random directions to account for the shifting optimization field. Before drawing new candidates, we age the stored reservoir weights: wi wi exp(cid:2)(t ti)/τ (cid:3), ki ki exp(cid:2)(t ti)/τ (cid:3), (6) where ti is the step when θi entered the reservoir. This exponential decay (enabled when τ > 0) steadily forgets stale projections so the sampler adapts to non-stationary optimization trajectories. Step 1: Reservoir construction. At optimization step we maintain persistent reservoir Rt1 of directions from the previous optimization step 1 and draw new directions Nt. The candidate set is Pt = Rt1 Nt. Step 2: Weighted reservoir sampling. Following Efraimidis & Spirakis (Efraimidis & Spirakis, 2006) we assign each θ Pt key k(θ) = u1/D(θ), U(0, 1) and keep the smallest keys. D(θ) , i.e. proportionally to its contribution to This selects each θ with probability q(θ) = the loss. θPt D(θ) (cid:80) Step 3: Self-normalized weights. For the survivors {θi}K i=1 we calculate the loss with weights as: (cid:98)Sp(µ, ν) = (cid:88) i= 1/q(θi) 1/q(θj) (cid:123)(cid:122) (cid:125) wi (cid:80) (cid:124) D(θi), (7) This loss can then be used in optimizations with detached gradient calculation for importance weights wi. This remains an unbiased MC estimate of Eθ[Wp] while focusing computation on projections which highlight larger differences. Step 4: ESS-based reservoir reset. We monitor the effective sample size (Bitterli et al., 2020) ESS = ((cid:80) and flush the reservoir whenever ESS < αK (α = 0.5 in all experiments), preventing weight collapse. wi)2/ (cid:80) 2 Handling unequal sample counts. If Nµ = Nν we repeat the shorter projection so that both 1D arrays have length = max(Nµ, Nν) before sorting. Repetitions are picked uniformly with replacement, following Elnekave & Weiss (2022). Complexity. The dominant cost is sorting scalars for each of (K + ) directions: O(cid:0)(K + ) log n(cid:1). Key generation, ESS evaluation, and gradient accumulation are O(K +M + d). Algorithmic summary. The overview of the algorithm is shown in Algorithm 1. m=1, reservoir Rt1, hyper-params K, M, p, α, τ Step 0: Time-decay n=1, {ym}Nν Algorithm 1 ReSWD estimator (per optimisation step) Require: batches {xn}Nµ 1: for (θi, wi, ki, ti) Rt1 do wi wi exp(cid:2)(tti)/τ (cid:3) 2: ki ki exp(cid:2)(tti)/τ (cid:3) 3: 4: end for 5: Nt DRAWDIRECTIONS(M, d) 6: Pt Rt1 Nt 7: for θ Pt do 8: 9: 10: end for 11: Rt directions with smallest k(θ) 12: compute q(θ), weights wi, and (cid:99)Wp via Eq. (7) 13: if ESS < αK then return (cid:99)Wp, 14: else return (cid:99)Wp, Rt 15: end if D(θ) Wp(πθµ, πθν) k(θ) u1/D(θ), (0, 1) Step 1: Reservoir Construction Step 2: Reservoir Sampling Step 3: Self normalizing weights Step 4: ESS-Reset 3.3 APPLICATIONS We showcase our SWD modifications on two real-world applications. Color Correction. Often in movie production two shots have to be matched in terms of their color appearance. We tackle this approach by matching source image using Color Decision List (CDL) (Pines & Reisner, 2009) with reference image. The source image is first passed through differentiable CDL implementation, applying the slope s, offset o, power and λ saturation adjustments according to = S(cid:0)(s + o)p; λ(cid:1). The saturation is defined as S(x; λ) = L(x) + λ(cid:0)x L(x)(cid:1) and L(x) = 0.2126xr + 0.7152xg + 0.0722xb, where r, g, define the color channels. The source and reference are then converted to the CIELAB space. We found that this space improves the color accuracy, which is consistent with recent paper proposing perceptual color metric (He et al., 2024). As we perform the matching on pixel level, we find that lower resolutions result in faster matching speeds with similar performance. Thus, we leverage resolution of 128 in the maximum dimension. Within 150 steps, the CDL parameters are optimized and can be applied to the full-res image or even video clip. Diffusion Guidance. Lobashev et al. (2025) proposed to incorporate SWD guidance in diffusion models to shift the final generation towards certain color distribution. The main concept is to optimize small offset on top of the current latents with SWD steps in each diffusion step. Here, we use the predicted x0 in each step and decode it using the VAE decoder. This prediction is then matched with the reference image. We apply this approach to the more recent flow matching model SD3.5 (Stability AI, 2025). This required several modifications. Lobashev et al. (2025) calculated the full gradient from the input, through the VAE and the U-net. With the increased complexity of larger transformer models, we opted to employ gradient stop after the backbone similar to SDS (Poole et al., 2022). Hence, we only backpropagate through the VAE decoder. Similarly to our color-correction approach, we also opted to perform the matching in the CIELAB space. In addition, we replace the simple gradient descent of Lobashev et al. (2025) with an Adam optimizer. With these changes, we use learning rate of 3e3 and total of 6 steps for 95% of the total denoising steps. We do not reset the reservoir after each denoising step, but twice during the generation, as we only perform 6 SWD steps, which would not suffice to build reservoir. This general method can then be applied to medium, large, and large-turbo SD3.5 models using the recommended CFG and step counts."
        },
        {
            "title": "4 RESULTS",
            "content": "We perform synthetic as well as real-world tests with ReSWD and study the influence of our main hyperparameter, the number of fresh candidates in each optimization step. General Distribution Matching. For general test of our proposed method we create 1000 = 3 distribution pairs (normal, uniform, bimodal normal) and align them based on 1024 samples in 300 steps. All methods leverage 64 projections. In Table 1, our method clearly outperforms previous SWD techniques with slight additional performance cost. To evaluate the optimization behavior of our method, we calculate the mean W1 true Wasserstein score for each dimension in each step. This allows us to plot the convergence behavior of our method in Fig. 3. Here, it is evident that pure SWD and our ReSTIR-based modification have similar trajectory, but building the reservoir initially results in slightly slower trajectory, which produces better results after roughly 140 steps. This also allows us to investigate the correlation between our proposed loss and the true Wasserstein distance. In Fig. 4, our method achieves high correlation with the true loss, indicating the unbiased nature of our method. Color Matching. To evaluate the color matching based on our differentiable color correction pipeline, we created dataset of 10 scenes with two different illumination settings each. For each setting, we also take photo using calibration color chart. We match the illumination pairs and compute the following metrics from the color checker data extracted from the additional image pairs: Color data PSNR and RMSE (between adjusted and ground truth color checker images), transform error, i.e. deviation from an identity transform between target and adjusted colors as RMSE as well as an adapted CTQM metric (Panetta et al., 2016) describing the overall color transfer quality. We select Reinhard et al. (2001), Nguyen et al. (2014), and Larchenko et al. (2025) as our baseline comparisons, as well as our proposed method but without the addition of ReSWD. Table 3 shows that our approach achieves the lowest transform error at competitive runtime. In Fig. 5, we present the visual comparison between the methods. It is also evident that our method outperforms the existing baseline in qualitative results as well. Additionally, it is worth pointing out that our results using parametric model also offer real-world advantages because of better pipeline integration and further manual edits being possible. Table 1: Comparison on 1D distribution matching. Mean-W1 over 1000 distribution matches for various methods alongside the respective running time. Here, we can see that ours provides the best performance with comparatively low run-time cost. Method Mean-W1 [103] Time per step [ms] SWD Nguyen & Ho (2024) (LCV) Nguyen & Ho (2024) (UCV) Nguyen et al. (2024a) (QMC) ReSWD 0.733 0.735 0.726 0.670 0.622 1.03 1.81 2.19 1.51 1. Table 2: Influence of fresh candidates. With fixed budget of 64 projections, we analyze the influence of the fresh candidates. # Candidates Mean-W1 [103] Time per step [ms] 2 4 8 16 32 48 56 0.721 0.673 0.622 0.746 1.192 2.122 3.811 1.99 1.96 1.92 1.91 1.98 1.93 1.85 a D 1 e 101 102 103 1 0. 0.8 ) 1 , ( ρ t r 100 200 300 0 100 300 Step SWD ReSWD Step SWD ReSWD Figure 3: True Wasserstein metric over steps. The effect of reservoir warmup is clear when comparing ReSWD with the true Wasserstein distance during optimization. Initially, ReSWD performs slightly worse due to lower projections in the loss, but can outperform SWD in the end. Figure 4: Pearson correlation with the true Wasserstein Distance. Our method achieves high correlation with the true Wasserstein loss, while improving upon pure SWD (See Fig. 3). This indicates the unbiased nature of our proposed method. Table 3: Comparison on color matching. Here, we compare ReSWD with baseline SWD method and prior color matching works by Reinhard et al. (2001), Nguyen et al. (2014) and Larchenko et al. (2025). We report the errors between the color charts of adjusted and ground truth images as PSNR, transform and CDL error and the image quality as CTQM. Additionally, we compare the runtimes of the methods. Method Color PSNR Transform err. (RMSE) CDL err. (RMSE) CTQM Time per match [s] Reinhard et al. (2001) Nguyen et al. (2014) Larchenko et al. (2025) Ours (SWD) Ours (ReSWD) 21.94 18.76 14.80 24.30 24.64 0.31 1.27 0.47 0.34 0. 0.14 0.33 0.20 0.11 0.10 5.12 5.16 5.05 5.15 5.17 1 3 24 5 Diffusion Guidance. We select Lobashev et al. (2025) as our main comparison. Furthermore, we also implemented our modifications to the matching in the SDXL model to compare our alterations with the baseline, which is based on SDXL. This delineates the influence of the improved SD3.5 model from our matching and alterations. For evaluations we follow Lobashev et al. and select 1000 prompts from the ContraStyles dataset* and 1000 images from Unsplash Lite. We provide the W2 Wasserstein distance, CLIP-IQA (Wang et al., 2023), and CLIP-T (Radford et al., 2021) to present the color matching performance, highlight the quality of the generation, and prompt adherence, respectively. In Table 4 our modifications clearly outperform the prior state-of-the-art technique of Labashev et al. Even with the same base model (SDXL), our method is faster and provides better results. The *https://huggingface.co/datasets/tomg-group-umd/ContraStyles https://unsplash.com/data u e T ) 1 0 0 2 ( . e h R ) 4 1 0 2 ( . e u ) 5 2 0 2 ( . e e a S Figure 5: Color Matching. Notice the consistent images our method produces with accurate shot matching. ReSWD consistently produces good results which match the final color distribution and without introducing any artifacts. We highlighted challenging areas which our method handled well. Note that the reference implementation or Larchenko et al. (2025) and Nguyen et al. (2014) do not support high resolution images. Table 4: Comparison on diffusion guidance. Here, we compare ReSWD with Lobashev et al. (2025) on color guidance in image generation. As our main proposed method uses the more recent SD3.5, we also compare with our modifications on SDXL. Even on the same base model our modifications improve upon the base technique. Method Lobashev et al. (2025) ReSWD + SDXL CLIP-IQA CLIP-T Mean-W2 [102] Time per generation [s] 1.941 1.213 14.861 14.870 0.671 0. 124 34 ReSWD + SD3.5-medium ReSWD + SD3.5-turbo ReSWD + SD3.5-large 0.786 0.800 0.793 14.783 14.882 14.817 0.815 0.675 0.55 32 4 main speed difference comes from our gradient stopping implementation which does not have any influence on the guidance. With the upgraded base model, ReSWD clearly outperforms the previous method. Here, it is interesting that the distilled Turbo variants provides better quality, which we attribute to the straighter trajectories. e e ) 5 2 0 2 ( . e s L S c e ) 5 2 0 2 ( . e s L e Figure 6: Diffusion Guidance. Visualization of our color guidance with SD3.5-Large. Notice, how closely the colors match the reference and the detailed generation quality of our method. Also we noticed that Lobashev et al. (2025) sometimes degenerates (Third example top, First example bottom) whereas ReSWD handles these challenging guidance signal well. We show example generations of this dataset in Fig. 6 along with the reference and the prompts. As evident, the improved generation capacity of SD3.5 along with our necessary changes resulted in drastically better generation which are faithful to the color distribution defined by the input image. Ablation. In Table 2, we assess the influence of the number of new candidates in our general distribution matching task. Too few new candidates starve the reservoir of new directions, resulting in static directions during optimization. Too many new candidates reduce the reservoir size, which drives the optimization solely, resulting in too few directions supporting the optimization. With 8 candidates, we achieved the best overall results in all applications given 64 total projections. Limitations. Our current technique is limited to simple matrix projections, and an extension to learned convolution kernels similar to Elnekave & Weiss (2022) did not provide satisfying results, as the search space for kernels is too large to randomly obtain drastically better kernels. This degraded the results similar to limiting the projection directions and reducing the redrawing of new directions to every few optimization steps."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Our novel combination of real-time rendering-influenced variance reduction techniques in SWD optimization offers more efficient and unbiased solution compared to other recent variance reduction techniques. This results in reduced overhead while maintaining optimal performance. Our technique has demonstrated superior performance in various real-world and synthetic applications, achieving state-of-the-art results."
        },
        {
            "title": "REFERENCES",
            "content": "Jie An, Haoyi Xiong, Jun Huan, and Jiebo Luo. Ultrafast photorealistic style transfer via neural architecture search. In AAAI, 2020. Benedikt Bitterli, Chris Wyman, Matt Pharr, Peter Shirley, Aaron Lefohn, and Wojciech Jarosz. Spatiotemporal Reservoir Resampling for Real-Time Ray Tracing with Dynamic Direct Lighting. ACM Transactions on Graphics (SIGGRAPH), 2020. Clément Bonet, Lucas Drumetz, and Nicolas Courty. Sliced-wasserstein distances and flows on cartanhadamard manifolds. Journal of Machine Learning Research, 26:176, 2025. Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 2015. Min-Te Chao. general purpose unequal probability sampling plan. Biometrika, 1982. Tai-Yin Chiu and Danna Gurari. Photowct2: Compact autoencoder for photorealistic style transfer resulting from blockwise training and skip connections of high-frequency residuals. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2022. Giacomo Cozzi and Filippo Santambrogio. Long-time asymptotics of the sliced-wasserstein flow. SIAM Journal on Imaging Sciences, 2024. Also available as arXiv:2405.06313. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013. Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander Schwing. Max-sliced wasserstein distance and its use for gans. In CVPR, 2019. Pavlos S. Efraimidis and Paul G. Spirakis. Weighted random sampling with reservoir. Information Processing Letters, 2006. Ariel Elnekave and Yair Weiss. Generating natural images with direct patch distributions matching. In ECCV, 2022. Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun-ichi Amari, Alain Trouvé, and Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, 2019. Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016. Jiaqi He, Zhihua Wang, Leon Wang, Tsein-I Liu, Yuming Fang, Qilin Sun, and Kede Ma. Multiscale sliced Wasserstein distances as perceptual color difference measures. In ECCV, 2024. Eric Heitz, Kenneth Vanhoey, Thomas Chambon, and Laurent Belcour. sliced wasserstein loss for neural texture synthesis. In CVPR, 2021. Kibeom Hong, Seogkyu Jeon, Huan Yang, Jianlong Fu, and Hyeran Byun. Domain-aware universal style transfer. In CVPR, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Wang. Lora: Low-rank adaptation of large language models. In ICLR, 2021. Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In CVPR, 2017. Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. NeurIPS, 2019. Maria Larchenko, Alexander Lobashev, Dmitry Guskov, and Vladimir Vladimirovich Palyulin. Color Transfer with Modulated Flows, 2025. URL http://arxiv.org/abs/2503.19062. Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. NeurIPS, 2017. Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz. closed-form solution to photorealistic image stylization. In ECCV, 2018. Alexander Lobashev, Maria Larchenko, and Dmitry Guskov. Color conditional generation with sliced wasserstein guidance. arXiv, 2025. Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In CVPR, 2017. Khai Nguyen and Nhat Ho. Energy-based sliced wasserstein distance. NeurIPS, 36, 2023. Khai Nguyen and Nhat Ho. Sliced Wasserstein Estimation with Control Variates. In ICLR, 2024. Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applications to generative modeling. In ICLR, 2021. Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Nguyen, and Nhat Ho. Hierarchical sliced wasserstein distance. In ICLR, 2023. Khai Nguyen, Nicola Bariletto, and Nhat Ho. Quasi-monte carlo for 3d sliced wasserstein. ICLR, 2024a. Khai Nguyen, Shujian Zhang, Tam Le, and Nhat Ho. Sliced wasserstein with random-path projecting directions. ICML, 2024b. R. M. H. Nguyen, S. J. Kim, and M. S. Brown. Illuminant Aware Gamut-Based Color Transfer. Computer Graphics Forum, 33(7):319328, October 2014. ISSN 0167-7055, 1467-8659. doi: 10.1111/cgf.12500. Karen Panetta, Long Bao, and Sos Agaian. Novel multi-color transfer algorithms and quality measure. IEEE Transactions on Consumer Electronics, 62(3):292300, August 2016. ISSN 1558-4127. doi: 10.1109/TCE. 2016.7613196. Sangmin Park and Dejan Slepˇcev. Geometry and analytic properties of the sliced wasserstein space. Journal of Functional Analysis, 2025. Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355607, 2019. Joshua Pines and David Reisner. ASC Color Decision List (ASC CDL) transfer functions and interchange syntax, version 1.2. Technical report, American Society of Cinematographers Technology Committee, DI Subcommittee, 2009. Francois Pitie, Anil Kokaram, and Rozenn Dahyot. N-dimensional probability density function transfer and its application to color transfer. In ICCV, 2005. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. Julien Rabin, Gabriel Peyré, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision, 2012. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Erik Reinhard, Michael Ashikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer Graphics and Applications, 21(5):3441, 2001. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022a. Robin Rombach, Andreas Blattmann, and Björn Ommer. Text-to-image diffusion models in the wild. In ACM Transactions on Graphics (SIGGRAPH), 2022b. Stability AI. Stable diffusion 3.5. https://github.com/Stability-AI/stablediffusion, 2025. Version 3.5, accessed June 2025. Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu, Rocio Diaz Martin, and Soheil Kolouri. Stereographic spherical sliced wasserstein distances. ICML, 2024. Christophe Vauthier, Anna Korba, and Quentin Mérigot. Properties of wasserstein gradient flows for the slicedwasserstein distance. arXiv, 2025. Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2008. Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. Rundi Wu, Ruoshi Liu, Carl Vondrick, and Changxi Zheng. Sin3dm: Learning diffusion model from single 3d textured shape. In ICLR, 2024. Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. Photorealistic style transfer via wavelet transforms. In CVPR, 2019. Lvmin Zhang, Adit Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, 2023a. Mingdeng Zhang, Han Chen, Yuxin Zeng, Ming Cheng, Lingjie Yang, Zexiang Xu, and Jiatao Gu. IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. In ICCV, 2023b."
        },
        {
            "title": "SUPPLEMENTS",
            "content": "In the supplements, we present more results from our applications."
        },
        {
            "title": "A DIFFUSION GUIDANCE",
            "content": "In Fig. 7 we present further results on diffusion guidance. Lobashev et al. (2025) proposed method and implementation often collapse, blur, or contain artifacts when the SWD guide enforces unlikely colors given the prompt. Our updated pipeline does not produce these issues."
        },
        {
            "title": "B COLOR MATCHING",
            "content": "We show additional results in Fig. 8 with our baselines. Notice that our results achieve consistent color matching performance and remain artifact-free. Our method also supports running at an arbitrary resolution. e e ) 5 2 0 2 ( . e s L S c e ) 5 2 0 2 ( . e s L S Figure 7: Diffusion Guidance. Further results on diffusion guidance with Lobashev et al. (2025). Notice their approach often becomes blurry, collapses or contains artifacts. u e T ) 1 0 0 2 ( . t a e ) 4 1 0 2 ( . e u ) 5 2 0 2 ( . e e a S Figure 8: Color Matching. More results on the color matching task. Notice the subtle changes and artifact free, high resolution color matching our method enables."
        }
    ],
    "affiliations": [
        "Stability AI",
        "University of Tübingen"
    ]
}