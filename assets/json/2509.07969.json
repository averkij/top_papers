{
    "paper_title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
    "authors": [
        "Xin Lai",
        "Junyi Li",
        "Wei Li",
        "Tao Liu",
        "Tianjian Li",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems."
        },
        {
            "title": "Start",
            "content": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search Xin Lai1, Junyi Li1,2, Wei Li1, Tao Liu1 Tianjian Li1, Hengshuang Zhao2 1ByteDance, 2The University of Hong Kong Equal contribution Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, system that executes deep, multiturn reasoningspanning tens of stepsand achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3style behaviors comprises three key components. First, we construct the Visual Probe Dataset, collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. Date: September 10, 2025 Correspondence: Xin Lai at laixin.1@bytedance.com, Hengshuang Zhao at hszhao@cs.hku.hk Project Page: https://github.com/Mini-o3/Mini-o3 5 2 0 2 9 ] . [ 1 9 6 9 7 0 . 9 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Recently, the capability to invoke image-centric tools has been incorporated into wide range of VisionLanguage Models (VLMs). This thinking-with-image capability enables flexible visual operations and fine-grained reasoning, substantially advancing visual understanding. However, while existing open-source VLMs exhibit solid performance on relatively simple visual search benchmarks (e.g., V* Bench [41], HR-Bench [37]), they remain weak on challenging tasks that require trial-anderror exploration. As shown in Fig.1, DeepEyes[49] achieves only 35.1% accuracy on VisualProbe-Hard. We further observe that this underperformance on difficult problems stems from monotonous reasoning patterns 1 Avg@32 Accuracy on VisualProbe-Hard 58 56 54 52 50 48 46 44 42 40 38 36 34 32 30 28 26 Mini-o3 (Ours) Mini-o3 w/o over-turn mask DeepEyes 4 32 16 Upper Limit of Turns During Testing 24 8 Figure 1 Left: Visual search accuracy continues to grow as the upper limit on the number of turns increases for Mini-o3. Right: Distribution of the correct trajectories under different numbers of interaction turns during testing. Mini-o3 demonstrates deeper thinking paths and stronger performance. Despite small upper limit (i.e., 6 turns) during training, it shows the test-time turns scaling property: accuracy continues to grow as the maximum number of turns increases from 4 to 32. and limited interaction turns. For instance, in HR-Bench-4K, DeepEyes uses image tools for an average of merely one turn per example. Unlike OpenAI o3 [28], these models fail to produce diverse reasoning strategies (e.g., depth-first search, trial-and-error exploration, self-reflection) and deep thinking trajectories spanning tens of tool-interaction rounds. Motivated by these observations, we present Mini-o3 and provide complete recipe to reproduce the thinkingwith-image capability with behaviors similar to OpenAI o3. As illustrated in Fig.2, Mini-o3 generates complex reasoning patterns and deep interaction trajectories, delivering unprecedented performance on challenging visual search tasks. Moreover, Fig.1 (left) demonstrates Mini-o3s ability to scale the number of interaction turns at test time: accuracy consistently improves as the upper bound on interaction turns increases from 4 to 32 during inference, despite training with budget of only 6 turns. By scaling both the depth of interaction and the diversity of reasoning patterns, Mini-o3 expands the solvable frontier of difficult problems, as shown in Fig. 1 (right). Our training recipe comprises three components. First, we construct the Visual Probe Dataset, which contains thousands of high-resolution images paired with challenging visual search questions and answers. In contrast to prior benchmarks (e.g., V* Bench, HR-Bench), where targets are often easy to localize, our problems are explicitly designed to require trial-and-error exploration. Notably, the inclusion of such challenging training samples is essential to elicit diverse reasoning patterns and deep interaction trajectories under reinforcement learning. Second, we develop an effective pipeline to iteratively synthesize diverse multi-turn trajectories for coldstart supervised finetuning. Concretely, we begin by crafting small set of representative demonstrations, each comprising the input image and question, along with per-turn observations, thoughts, and actions. These demonstrations cover varied reasoning strategies, including depth-first search, self-reflection, and goal maintenance. We then prompt an existing VLM to mimic these behaviors in few-shot manner and to produce the thought and action for each turn on new queries, iterating until the model completes the task or reaches the interaction budget. Only trajectories that culminate in correct answer are retained. Importantly, the base VLM used for data synthesis need not possess native thinking-with-image ability; in-context mimicking 2 Figure 2 multi-turn trajectory generated by Mini-o3. It shows complicated reasoning patterns (e.g., trial-and-error exploration) and deep thinking paths (i.e., 11 turns) in visual search tasks. More illustrations are given in Appendix. 3 Figure 3 The overview of our framework for multi-turn agentic image tool use. During each turn, the model generates the thought and action iteratively based on the previous observation (or the input question and image). The observation at each turn is obtained based on the parameters indicated by the corresponding action. suffices. Third, to enable scaling the number of interaction turns at inference time for harder problems, we avoid penalizing the over-turn trajectories (those that exceed the upper limit of interaction turns) and introduce an over-turn masking technique in reinforcement learning. Specifically, we mask advantages for trajectories that hit the upper limit of interaction turns or the context length. Consequently, over-turn trajectories are ignored during policy updates, and their losses do not contribute gradients. This simple yet effective strategy encourages the emergence of more complex reasoning patterns without overfitting to short trajectories, thereby supporting test-time scaling of interaction depth. It also alleviates the need for large training-time turn budget: in our experiments, we cap training at only 6 turns, significantly improving efficiency. For example, reducing the training budget from 16 to 6 turns shortens 10-day training run to about 3 days, with negligible impact on test accuracy."
        },
        {
            "title": "2.1 Vision-Language Models",
            "content": "The emergence of Vision-Language Models (VLMs) has marked major milestone in artificial intelligence by enabling the joint understanding of visual and textual modalities. Early seminal works, including BLIP-2 [18], Flamingo [2], and the LLaVA series [9, 15, 22], established foundational paradigm that couples strong pre-trained vision encoders (e.g., ViT [6]) with large language models (LLMs). These systems typically introduce projector to align visual features with the linguistic embedding space, thereby endowing LLMs with visual grounding. Building on this paradigm, more recent multimodal modelssuch as Gemini [34], GPT-4o [13], and Qwen2.5-VL [4], among others [3, 5, 16, 21, 27]have achieved state-of-the-art performance on wide range of visual understanding tasks, notably visual question answering. Their gains are largely driven 4 by scaling model capacity and training on diverse, high-quality imagetext corpora. In parallel, advances in reinforcement learning have enhanced the reasoning capabilities of VLMs by encouraging structured, step-by-step problem solving via Chain-of-Thought prompting [38]. Recent approaches [11, 24, 26, 30, 45, 50] primarily target improved textual reasoning for challenging tasks, including counting, logical inference, and mathematical problem solving."
        },
        {
            "title": "2.2 Tool-Integrated Agents with Reinforcement Learning",
            "content": "Progress in reinforcement learning (RL) including algorithms such as REINFORCE [39], PPO [29], RLOO [14], ReMax [20], GRPO [31], REINFORCE++[10], Dr.GRPO[23], and GSPO [48] has substantially reshaped training paradigms for both LLMs and VLMs. Systems like DeepSeek-R1 [8] and Kimi-K1.5 [36] further demonstrated the efficacy of simple, verifiable reward signals in RL for improving reasoning quality. More recently, tool-augmented agentssuch as OpenAIs o3 and o4 [28], Kimi-Researcher [1], Kimi-K2 [35], and others [7, 19, 25, 33, 42]have shown strong agentic abilities in long-horizon, multi-turn tasks by leveraging broad toolkit (e.g., web browsing, code execution, retrieval). Complementary lines of work, including DeepEyes [49], Chain-of-Focus [46], and Pixel Reasoner [32], as well as related methods [12, 40, 43, 51], aim to equip VLMs with iterative zoom-in and region-of-interest selection, enabling active perception over images. While these directions collectively point to promising path for next-generation visual understanding particularly on challenging, compositional problems current models often exhibit limited interaction depth and overly rigid reasoning patterns, constraining their effectiveness in complex settings. Our work advances this line by presenting an effective training recipe for multimodal agent that supports multi-turn image tool use, thereby improving adaptability and reasoning diversity in visually grounded tasks."
        },
        {
            "title": "3.1 Overview",
            "content": "Overall Agentic Pipeline We illustrate the overall agentic pipeline in Fig. 3. Given user query and an input image, the policy model iteratively produces thought Ti and an action Ai. The action interacts with the environment by invoking image tools, which yields new observation Oi. This observation is appended to the interaction history and fed back to the policy model. The thoughtactionobservation loop terminates when the model returns final answer or when predefined limits on context length or interaction turns are reached. The components are detailed below. Thought Ti: The internal reasoning process used by the policy model to select the next action, conditioned on the interaction history and the current observation. We encourage diverse reasoning patterns within thoughts to facilitate trial-and-error exploration for challenging problems. Action Ai: The action space comprises two options: (1) grounding and (2) emitting final answer. For grounding, we parameterize the action with: bbox_2d: The normalized bounding box in [0, 1]2 specifying the zoom-in region. source: The image on which the grounding operates, chosen from original_image or observation_i. This design allows the model to act on any prior observation in the trajectory. Observation Oi: The observation produced by executing Ai in the environment. Concretely, it is the image patch cropped either from the original image or from historical observation. Two-phase Training Our training procedure consists of two phases. Supervised Fine-Tuning (SFT): We first fine-tune the model on thousands of multi-turn trajectories involving image tool use (i.e., cold-start data). The objective is to teach the model to generate valid trajectories with diverse and robust reasoning patterns. Reinforcement Learning with Verifiable Rewards (RLVR): We then apply GRPO [31] to optimize the policy with verifiable, semantics-aware rewards. Because many ground-truth answers in our RL data require semantic rather than exact string matching, we employ an external LLM as judge to compute reward signals. To maintain training efficiency and stability, we impose upper bounds of 6 interaction turns and 32K context length. 5 Figure 4 Illustration of the VisualProbe dataset. The VisualProbe dataset features 1) small targets; 2) disturbance objects; 3) high-resolution images. As result, it is super challenging and requires iterative exploration and trial-anderror. Figure 5 The pipeline of cold-start data collection."
        },
        {
            "title": "3.2 Training Data Collection",
            "content": "Visual Probe Dataset Hard instances are essential for encouraging reflective, trial-and-error reasoning during reinforcement learning. To this end, we construct challenging visual search dataset, the Visual Probe Dataset (VisualProbe). It comprises 4, 000 visual questionanswer pairs for training and 500 pairs for testing, spanning three difficulty levels: easy, medium, and hard. Compared with prior visual search benchmarks (e.g., V* Bench), VisualProbe is characterized by: (1) small targets, (2) numerous distractor objects, and (3) high-resolution images, as illustrated in Fig. 4. These properties make the tasks substantially more demanding and naturally require iterative exploration and trial-and-error. Diverse Cold-start Data We initially attempted to train the model with reinforcement learning alone, without cold-start supervised fine-tuning (SFT). However, the model tended to produce concise responses and trajectories with few turns. We attribute this behavior to the base models lack of exposure to long-horizon agentic trajectories during pretraining and instruction tuning (here, Qwen2.5-VL-7B-Instruct). To handle complex exploratory tasks, we thus employ cold-start SFT to activate multi-turn tool-use capabilities. The cold-start data collection pipeline is shown in Fig. 5. To generate high-quality, diverse multi-turn 6 69.4 53. 32.4 29.5 ) % ( t r 60 40 20 0 Mini-o3 w/o over-turn masking Mini-o3 12. 1.1 1.9 0 0 0.1 [1, 4) [4, 8) [8, 16) [16, 24) [24, 32) Figure 6 Distribution of interaction-turn percentages across five turn ranges during testing on VisualProbe-Hard. The percentages are calculated only on the correct responses. trajectories, we prompt an existing VLM with in-context learning ability using small set of manually crafted exemplars. The VLM is instructed to imitate the exemplars by iteratively producing thought and an action at each turn. The loop terminates upon emitting final answer or reaching pre-defined turn limit. We retain only trajectories whose final answers are correct. Following this procedure, we collect approximately 6, 000 cold-start trajectories from 6 exemplars."
        },
        {
            "title": "3.3 Reinforcement Learning",
            "content": "Lower Down Max Pixels The base models context length is constrained to 32K tokens. With the default image budget of roughly 12M pixels, the allowable number of interaction turns becomes severely limited by context, which hampers trial-and-error exploration on difficult tasks. To increase the feasible turn count per episode, we reduce the maximum pixels per image to 2M (or lower if necessary). This simple adjustment allows more turns to fit within the same context budget, improving solve rates on long-horizon problems. i=1 In the vanilla GRPO setting, each question is passed to the policy model to generate Over-turn Masking group of outputs {oi}G . Rewards are then computed based on the correctness of the responses. Notably, when response hits the maximum number of turns or exceeds the context length limit, the reward is set to 0, as no valid answer can be produced in such cases. Subsequently, we compute advantages by normalizing the rewards and update the policy using the GRPO optimization objective over mini-batches. In our implementation, we do not include KL or entropy regularization. Formally, the optimization objective is given by: JGRP O(θ) = [qD,{oi}G i=1πθold (q)] 1 (cid:18) min (cid:18) πθ(oiq) πθold Ai = ri mean({r1, r2, ..., rG}) (oiq) Ai, clip i=1 std({r1, r2, ..., rG}) . (cid:18) πθ(oiq) πθold (oiq) , 1 ϵ, 1 + ϵ (cid:19) (cid:19)(cid:19) Ai (1) (2) However, we observe that over-turn responses those that hit the maximum number of turns or exceed the context length are assigned zero reward, which translates into negative advantages after normalization. In effect, such responses are penalized and discouraged throughout training. This design has two drawbacks. First, the correctness of over-turn responses is inherently unknown; blunt penalization thus injects label noise into the return signal and can destabilize training. Second, for efficiency, the turn limit during training must remain modest (typically fewer than 10 turns). As consequence, over-turn responses occur frequently exceeding 20% at the beginning of training. In this regime, naïve penalization biases the model to answer prematurely, substantially suppressing the number of interaction turns (see Fig. 6). This makes highly challenging tasks intractable and severely constrains the potential of test-time scaling. 7 Figure 7 Illustration of the over-turn masking technique. The incomplete responses refer to those that exceed the maximum limit of interaction turns or context length. To prevent the model from collapsing into an answer earlier strategy, we propose an over-turn masking technique whose objective is to avoid penalizing over-turn responses. The overall procedure is illustrated in Fig. 7. Concretely, in addition to the rewards and advantages defined as in vanilla GRPO, we introduce completion mask that indicates whether response terminates successfully. We then compute masked advantages = Mi Ai, so that over-turn trajectories (with Mi = 0) do not contribute negative learning signals. The modified objective, building on (1), is summarized below, with the changes highlighted in red in the formula. overturn GRP (θ) = [qD,{oi}G i=1πθold (q)] 1 PG Mi i= (cid:18) min (cid:18) πθ(oiq) πθold (oiq) AiMi, clip (cid:18) πθ(oiq) πθold (oiq) , 1 ϵ, 1 + ϵ (cid:19) (cid:19)(cid:19) AiMi Mi = 1{oi <= Ccontext} 1{turn(oi) <= Cturn}. (3) (4) Here, oi and turn(oi) denote the token length and the number of turns in response oi, respectively. Moreover, because some responses are incomplete, we normalize the objective by the number of completed generations, PG Mi, rather than by the total number of generations G. With this technique, we mask out the loss for over-turn responses, thereby removing any implicit penalty. Notably, although we adopt relatively small upper bound on the number of turns during training, test-time trajectories can extend to dozens of rounds, with accuracy improving monotonically. The proposed over-turn masking is thus essential for realizing the benefits of test-time scaling in the number of interaction turns, as illustrated in Fig. 7. 3."
        },
        {
            "title": "Inference",
            "content": "Generation with Temperature During inference, we observe that greedy decoding tends to produce repeated words or sentences, likely because the effective context grows with the number of turns. To mitigate this issue, simple yet effective method is to set the temperature to 1.0, which introduces sufficient randomness to reduce repetition without substantially degrading coherence."
        },
        {
            "title": "4 Experiment",
            "content": "8 Table 1 Performance comparisons among existing models and ours on visual search tasks. The sizes of all listed models are 7B. For VisualProbe and V* Bench, we report Avg@32 to reduce variance caused by randomness. We report Avg@8 and Avg@1 for HR-Bench and MME-Realworld, respectively. Model VisualProbe hard medium easy GPT-4o [13] LLaVA-OneVision [15] Qwen2.5-VL-Instruct [4] SEAL [41] DyFo [17] Chain-of-Focus [46] Pixel Reasoner [32] DeepEyes [49] Mini-o3 (Ours) 11.2 13.4 23.9 - - - 28.8 35.1 48. 15.4 12.5 26.0 - - - 29.6 29.8 50.4 47.5 36.2 39.1 - - - 58.4 60.1 67.0 V* 65.2 70.9 75.5 75.4 81.2 88.0 86.3 83.3 88.2 HR-Bench 8K 4K MME-Realworld 62.0 61.2 68.2 - - - 74.0 73.2 77.5 58.3 54.0 62.7 - - - 66.9 69.5 73.3 45.2 57.4 57.3 - - - 64.4 64.0 65.5 The models only report the metric of Avg@1 and the model weights are not available. Re-evaluated using its official model and evaluation code to yield the metric of Avg@32."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "Supervised Finetuning During SFT, we use Qwen2.5-VL-7B-Instruct as the base model. Given the contextlength constraints in multi-turn agentic interactions, we set the maximum pixel budget to 2M unless otherwise specified. We train on approximately 6, 000 cold-start samples for 3 epochs. The learning rate is set to 1 105, and the global batch size is 32. Reinforcement Learning For reinforcement learning, we follow DAPO [44] and adopt clip-higher, dynamic sampling, and token-level policy loss to ensure stable training. We set the group size to 16. By default, the upper and lower clip ratios are 0.30 and 0.20, respectively. The global batch size is 256, with mini-batch size of 32. We use constant learning rate of 1 106. Neither KL regularization nor entropy regularization is applied. To maintain training efficiency, we cap the maximum number of turns at 6 and set the maximum context length to 32K tokens. We also implement asynchronous rollouts to accelerate training. Dataset For training, we use the VisualProbe training split. In addition, to preserve performance on simpler visual search cases, we randomly sample 8, 000 examples from DeepEyes-Datasets-47k [49]. The test suites include VisualProbe-test, V* Bench, HR-Bench, and MME-Realworld [47]. Evaluation Metric We find that single-run evaluation exhibits high variance and does not reliably reflect robustness due to sampling stochasticity. To mitigate this, we report the Avg@K metric: each problem is evaluated times with temperature set to 1.0, and accuracy is computed by averaging across the responses."
        },
        {
            "title": "4.2 Main Result",
            "content": "The performance comparison between existing models and Mini-o3 on visual search tasks is presented in Table 1. To ensure robust and convincing evaluation, we assess all models on VisualProbe, V* Bench, and HR-Bench. Across all datasets, Mini-o3 achieves state-of-the-art performance, substantially outperforming other open-source baselines. We attribute these gains to Mini-o3s ability to sustain more complicated and deeper reasoning trajectories."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "In this section, we present an extensive ablation study to quantify the contribution of each component in our method. The overall results are summarized in Table 2. Unless otherwise specified, all experiments are conducted on the VisualProbe test set with the maximum pixel budget set to 1M. 9 Table 2 Ablation study for main components of the method. Max pixels are set to 1M. Upper limit on the number of turns is set to 6 during training. Evaluations are made on VisualProbe test set. ID hard RL data 1 2 4 (cid:33) (cid:33) (cid:33) cold-start (cid:33) (cid:33) (cid:33) over-turn (cid:33) (cid:33) (cid:33) Hard Medium Easy Avg. Turns (correct) 35.8 25.4 32. 44.4 46.4 18.7 45.7 47.9 66.7 57.3 61.1 67.4 4.8 1.0 3. 5.5 Table 3 Ablation study on the values of max pixels. Evaluations are made on VisualProbe test set. Also, we calculate the average number of interaction turns among overall and correct trajectories. Max Pixels 0.5M 1M 2M 12M Hard 36.4 44.4 48.0 36.1 Medium 44.8 47.9 50.4 40.7 Easy 64.8 67.4 67.0 62. Avg. Turns (All) 8.0 6.3 6.5 1.0 Avg. Turns (Correct) 6.7 5.5 5.6 1.0 Hard RL Data We compare experiments 1 and 4 in Table 2. Removing the hard RL data leads to performance decrease of approximately 8.6 points on VisualProbe-Hard, indicating that challenging RL samples are crucial for encouraging complex reasoning trajectories. Cold-start SFT To assess the necessity of cold-start SFT, we contrast experiments 2 and 4 in Table 2. The results show that cold-start SFT is essential for multi-turn tool use: performance collapses without it. We hypothesize that the base model lacks exposure to multi-turn agentic trajectories during pre-training or instruction tuning, and cold-start SFT serves as pivotal initialization. Over-turn Masking comparison between experiments 3 and 4 in Table 2 demonstrates that over-turn masking benefits reinforcement learning, particularly in multi-turn settings. It offers two main advantages. First, it stabilizes training by avoiding incorrect penalization of truncated responses whose correctness is inherently uncertain. Second, it enables test-time turn scaling and unlocks strong performance on highly challenging tasks that require substantially more turns than the training-time upper bound. This trend is further corroborated in Fig. 6. Max Pixels Table 3 evaluates different maximum pixel budgets. We observe that both overly large and overly small settings are suboptimal. An excessively large budget induces premature early stopping, reducing the number of interaction turns and limiting iterative refinement. Conversely, small budget increases perceptual hallucinations. We also report the average number of interaction turns in the same table, which highlights trade-off between perceptual accuracy and interaction depth. Optimal overall performance is achieved by appropriately tuning the max-pixel budget. Upper Limit on Turns During Training To quantify the effect of larger interaction-turn budget during training, we track the accuracy on VisualProbe-Hard over the course of training and compare budgets of 6 and 12 turns in Fig. 8. lower budget leads to faster initial convergence, but the performance plateaus after approximately 150 steps. In contrast, higher turn budget attains superior performance ceiling, albeit with slower convergence."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we investigate multi-turn image-based tool use for Vision-Language Models (VLMs). To address challenging visual search problems that demand iterative exploration and trial-and-error, we introduce Mini-o3, model capable of producing diverse reasoning patterns and deep chains of thought. Its trajectories scale to tens of turns, during which accuracy continues to improve, yielding substantial gains over prior models on 10 Upper limit = 12 Upper limit = 6 r A t l 0.5 0.45 0.4 0.35 0.3 0.25 20 40 60 100 120 140 160 180 200 80 Training Steps Figure 8 Accuracy on VisualProbe-Hard during the training progress. The upper limit of the number of turns is set to 6 and 12, respectively. multiple visual search benchmarks. To enable these capabilities, we develop three-pronged approach. First, we construct VisualProbe, challenging visual search dataset comprising both training and evaluation tasks. Second, we devise simple yet effective pipeline for collecting cold-start data by leveraging the in-context learning ability of an existing VLM. Third, we enhance vanilla GRPO with an over-turn masking strategy that prevents undue penalties on responses that exceed the training budget on turns. This modification facilitates test-time turn scaling and enables the solution of particularly difficult problems. We believe this recipe offers practical guidance for reinforcement learning and the development of multimodal models with multi-turn interactions."
        },
        {
            "title": "References",
            "content": "[1] Moonshot AI. End-to-end rl training for emerging agentic capabilities, 2025. URL https://moonshotai.github. io/Kimi-Researcher/. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [3] Anthropic. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet/. Technical Report, 2024. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [7] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontiers of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. In European Conference on Computer Vision, pages 390406. Springer, 2024. [10] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [11] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [12] Xinyu Huang, Yuhao Dong, Weiwei Tian, Bo Li, Rui Feng, and Ziwei Liu. High-resolution visual reasoning via multi-turn grounding-based reinforcement learning. arXiv preprint arXiv:2507.05920, 2025. [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [14] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! In DeepRLStructPred@ICLR, 2019. URL https://api.semanticscholar.org/CorpusID:198489118. [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [16] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [17] Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo: training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding, 2025. URL https://arxiv.org/abs/2504.14920. [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 12 [19] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. [20] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, RUoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient method for aligning large language models. 2023. [21] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [23] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. In Conference on Language Modeling (COLM), 2025. [24] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [25] Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. [26] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mmeureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [27] Meta. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. https://ai.meta.com/ blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. Technical Report, 2024. [28] OpenAI. Introducing o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o3-and-o4-mini/. [29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [30] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR, 2024. [31] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [32] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. [33] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. [34] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [35] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi 13 Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. [36] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. [37] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 79077915, 2025. [38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [39] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229256, 1992. [40] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. [41] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. [42] Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning, 2025. URL https://arxiv.org/abs/2509.02479. [43] Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. Visionthink: Smart and efficient vision language model via reinforcement learning. arXiv preprint arXiv:2507.13348, 2025. [44] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [45] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [46] Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025. [47] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. 14 [48] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https: //arxiv.org/abs/2507.18071. [49] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [50] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [51] Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large language models with active perception via grpo. arXiv preprint arXiv:2505.21457, 2025."
        },
        {
            "title": "Appendix",
            "content": "A More illustrations of multi-turn trajectories Figure 9 An example of visual search in urban intersection generated by Mini-o3. The scene shows busy city street with multiple signs and traffic elements. Our model conducts multi-turn reasoningprogressive zoom-in, hypothesis revision, and backtrackingto identify the direction the arrow is pointing. 16 Figure 10 An example of visual search in container yard generated by Mini-o3. The scene depicts stacked shipping containers with dense text markings. Our model performs multi-turn reasoningtargeted zoom-ins, cross-checking across observations, and corrective backtrackingto locate the string 67.200\" and read the number directly beneath it. It outputs the correct value, 22G1\", demonstrating precise text localization and robust step-by-step verification in cluttered setting. 17 Figure 11 An example of visual search in lakeside village generated by Mini-o3. Our model performs multi-turn reasoningcoarse-to-fine zooming, refocusing, and verification across observationsto localize circular road sign above triangular warning sign. Mini-o3 ultimately recognizes the digits 30\" on the sign after 18 reasoning turns."
        }
    ],
    "affiliations": [
        "ByteDance",
        "The University of Hong Kong"
    ]
}