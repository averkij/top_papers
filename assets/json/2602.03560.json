{
    "paper_title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
    "authors": [
        "Yizhao Gao",
        "Jianyu Wei",
        "Qihao Zhang",
        "Yu Cheng",
        "Shimao Chen",
        "Zhengju Tang",
        "Zihan Jiang",
        "Yifan Song",
        "Hailin Zhang",
        "Liang Zhao",
        "Bo Yang",
        "Gang Wang",
        "Shijie Cao",
        "Fuli Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . [ 1 0 6 5 3 0 . 2 0 6 2 : r HySparse: Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing"
        },
        {
            "title": "Shijie Cao",
            "content": "Fuli Luo LLM-Core Xiaomi"
        },
        {
            "title": "Abstract",
            "content": "This work introduces Hybrid Sparse Attention (HySparse), new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layers token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10."
        },
        {
            "title": "1 Introduction",
            "content": "The demand for long-context capabilities has become cornerstone of modern Large Language Models (LLMs), driven by emerging paradigms such as test-time scaling (Guo et al., 2025; Jaech et al., 2024) and agentic workflows (Anthropic, 2024; Kimi Team et al., 2025a). Yet, the selfattention mechanism in standard Transformers scales quadratically with sequence length, causing computational latency and cost to grow prohibitively as context length increases. Sparse attention offers straightforward and effective solution to mitigate this quadratic bottleneck (Child et al., 2019). Sparse attention computes attention over selected subset of important tokens rather than all tokens in the sequence. Existing methods can be broadly categorized into training-free and trainable approaches. Training-free methods rely on fixed patterns or heuristic to select important tokens (Jiang et al., 2024; Tang et al., 2024; Xiao et al., 2023, 2024; Yang et al., 2025b; Zhang et al., 2023). Trainable sparse attention learns which tokens to attend during training, either via low-cost self-distillation (Gao et al., 2024; Liu et al., 2025a) or by being directly integrated into pre-training (Lu et al., 2025; Yuan et al., 2025). Nevertheless, sparse attention architectures still suffer from two fundamental limitations: (1) Proxy-based Sparse Token Selection. Sparse attention fundamentally depends on selecCorresponding author. 1 tion mechanism to identify important tokens prior to attention computation. Existing methods typically rely on lightweight proxies, such as predefined patterns, heuristics, approximate estimates, or additional selection modules (Gao et al., 2024, 2025; Jiang et al., 2024; Liu et al., 2025a; Lu et al., 2025; Yuan et al., 2025). However, these proxies are inherently approximate and may fail to capture true token importance, particularly in long and evolving contexts. As result, sparse token selection is often bounded by the fidelity of the proxy, potentially limiting the expressive power of sparse attention. While learnable sparse attention alleviates selection errors by learning token selection during training, it does not fundamentally eliminate the proxy-based bottleneck and introduces additional selection modules that increase training complexity. (2) Computation Reduction without Memory Relief. Modern sparse attention methods increasingly adopt dynamic sparsity to preserve model fidelity. Unlike static patterns (e.g., fixed strides or block structures), which can reduce KV cache storage but often incur noticeable performance degradation, dynamic approaches typically retain the full KV cache. This is because complete KV cache eviction is irreversible and destructive, as token importance may shift as generation progresses and context evolves. While dynamic sparse attention can effectively reduce computation, it provides no relief for memory consumption. Maintaining full-sized KV cache therefore remains dominant bottleneck for serving throughput and maximum batch size, limiting the practical benefits of sparse attention in long-context settings. To address these challenges, we introduce Hybrid Sparse Attention (HySparse). The key idea is to interleave every full attention layer with multiple sparse attention layers, where the sparse layers strategically derive important token selection and KV caches from the preceding full layer. This design is motivated by two empirical observations in recent literature: token saliency is stable across consecutive layers (2.3), and cross-layer KV cache sharing reduces memory without hurting performance (2.4). In HySparse, full attention can precisely identify important token selection and already produces KV caches, which sparse layers can directly reuse. By reusing the important token indices from full attention, sparse selection becomes oracle-guided. This eliminates the need for auxiliary proxy modules and ensures stable end-to-end training. By reusing the KV caches from full attention, sparse attention adds no per-layer KV overhead, effectively alleviating the memory pressure associated with dynamic sparse attention. Meanwhile, inspired by hybrid sliding window attention (SWA) architectures (Agarwal et al., 2025; Gemma Team et al., 2025; Xiaomi Team et al., 2026), HySparse augments each sparse attention layer with an additional SWA branch that maintains small, local KV cache to enhance short-range modeling capacity. We evaluate HySparse on both 7B dense and 80B Mixture-of-Experts (MoE) model settings. For the 7B dense model, we adopt full-to-sparse layer ratio of 1:3, while for the 80B MoE model, more aggressive 1:11 ratio is used. In both cases, the final layer employs full attention to preserve global aggregation. Across tasks and context lengths, HySparse consistently outperforms both full attention and hybrid SWA baselines, without incurring any additional KV cache cost relative to the hybrid SWA baseline. Remarkably, in the HySparse 80B MoE model with 49 total layers, only 5 layers use full attention, meaning nearly 10 KV cache reduction, while the models still delivers substantial performance gains. Compared with Hybrid SWA, HySparse can significantly reduce the number of full attention layers, effectively pushing the hybrid ratio to its limit. In summary, these results indicate that HySparse provides simple and effective architectural solution to the core limitations of sparse attention, achieving strong long-context modeling capability with clear efficiency and memory advantages."
        },
        {
            "title": "2.1 Training-free vs. Trainable Sparse Attention",
            "content": "Sparse attention methods can be divided into training-free and trainable approaches. Trainingfree methods rely on fixed patterns or heuristics to identify important tokens. They are applied as drop-in modification at inference, enabling fast sparsity decisions with minimal computational cost (Tang et al., 2024; Xiao et al., 2023, 2024; Zhang et al., 2023). However, applying sparsity only at inference creates traininginference mismatch, which may lead to error accumulation in long decoding or multi-step reasoning (He and Lab, 2025; Hu et al., 2026; Liu et al., 2025b). Trainable sparse attention methods, in contrast, learn token importance during training through lightweight selection modules. By integrating sparsity into the training process, they improve alignment between training and inference, selecting more informative tokens with higher recall and overall accuracy (Gao et al., 2024, 2025; Liu et al., 2025a; Lu et al., 2025; MiniCPM Team, 2025; Yuan et al., 2025; Zhao et al., 2025). However, training these selection modules are nontrivial. One approach uses auxiliary losses, such as self-distillation, to align the gating or indexer module with the original dense attention (Gao et al., 2024, 2025; Liu et al., 2025a). These methods are simple but suboptimal. Alternatively, NSA (Yuan et al., 2025) performs end-toend sparse pretraining by injecting the compressed attention (selection module) output into the main attention. This design allows the selection module to receive learning signals only indirectly through the final attention output, without direct supervision on its token selection decisions."
        },
        {
            "title": "2.2 Hybrid Attention Architecture",
            "content": "To reduce quadratic compute and KV cache costs, hybrid attention has emerged as promising solution for scaling context length. For instance, MiniMax-01 (Li et al., 2025a) integrates both linear attention and softmax attention mechanisms in structured pattern. Similarly, Qwen3Next (Qwen Team, 2025) and Kimi Linear (Kimi Team et al., 2025b) incorporate Gated DeltaNet (Yang et al., 2024b) or its variants. The Nemotron family (Basant et al., 2025; Blakeman et al., 2025) and Jamba (Lieber et al., 2024) integrate Mamba modules (Dao and Gu, 2024; Gu and Dao, 2023) with standard self-attention modules. Models such as GPT-OSS (Agarwal et al., 2025), Gemma3 (Gemma Team et al., 2025), and MiMo-V2-Flash (Xiaomi Team et al., 2026) employ heterogeneous interleaving of sliding window attention and global full attention layers. The sliding window size can be as small as 128 tokens with negligible KV cache overhead. Yet, the hybrid model with dynamic sparse attention has not been fully explored."
        },
        {
            "title": "2.3 Cross-Layer Salient Token Stability",
            "content": "Several concurrent works have observed that salient tokens (sparse tokens with higher attention scores) tend to remain relatively stable across consecutive layers in standard transformer models (Deshmukh et al., 2025; Hao et al., 2025; Yang et al., 2024a, 2025a; Zarch et al., 2025). These methods exploit this property to accelerate inference as training-free manner. Specifically, they identify important tokens using full attention layer and reuse the salient token indices in subsequent layers to perform sparse attention computation. Inspired by these works, we elevate this empirical observation to hybrid attention architecture for pretraining, in which full attention layers identify important tokens that are subsequently reused by following sparse attention layers."
        },
        {
            "title": "2.4 Cross-layer KV Cache Sharing",
            "content": "Cross-layer KV cache sharing is memory optimization technique in which Key and Value tensors computed in one layer are reused by subsequent layers, instead of being recomputed and stored independently for every layer. This design substantially reduces the KV cache memory footprint, while empirical studies show that it incurs little to no degradation in model accuracy. YOCO (Sun et al., 2024), CLA (Brandon et al., 2024), the Apple Foundation Model (Li et al., 2025b), and Gemma 3n (Gemma Team et al., 2025) integrate cross-layer KV cache sharing mechanisms directly into their model architectures. SwiftKV (Qiao et al., 2025) adapts standard pretrained models to support cross-layer KV cache sharing over subset of layers via distillation. MiniCache (Liu et al., 2024) also observes that KV cache exhibits high similarity between adjacent layers in the middle-to-deep regions of LLMs, and proposes cross-layer compression methods to exploit this redundancy."
        },
        {
            "title": "3.1 HySparse Overview",
            "content": "Figure 1 HySparse Architecture Diagram. Each full attention layers is interleaved with multiple sparse attention layers. Sparse attention directly reuses the KV cache and important token indices from the preceding full attention layer. As shown in Figure 1, HySparse architecture replaces the standard Transformer backbone with repeated hybrid blocks that are composed of one full attention layer followed by ğ‘ consecutive sparse attention layers. At its core, both the sparse important token indices and the KV caches used in these sparse layers are directly derived from the preceding full attention layer within the same block. The full attention layer computes standard scaled dot product self-attention but additionally outputs block-wise attention importance scores ğ‘†, from which we derive TopK block indices. These indices are then reused by the next ğ‘ sparse layers. To reduce KV cache memory and bandwidth 4 consumption, HySparse further incorporates cross-layer KV cache sharing. The sparse attention layers reuse the KV cache produced by the preceding full attention layer within hybrid block for the block sparse attention branch. The sliding window attention (SWA) branch, in contrast, maintains its own lightweight KV cache to enhance short-range modeling capacity. Finally, sigmoid gates are applied to the output of the two branches (Qiu et al., 2025) before summed as the final attention output."
        },
        {
            "title": "3.2 Full Attention Layers",
            "content": "The full attention layers follow the standard softmax self-attention formulation used in Transformers. To identify salient tokens for subsequent sparse attention, the attention scores must be exposed for selection. However, materializing the full attention matrix is prohibitively expensive in terms of both memory and bandwidth. Consequently, modern Transformers rely on FlashAttention (Dao, 2023; Dao et al., 2022; Shah et al., 2024), which avoids explicitly storing attention scores by computing softmax in tiled manner with online normalization. To mitigate this issue, instead of outputting the full attention score matrix, HySparse only materializes block-level (tile-level) maximum attention scores for TopK selection. With the standard self-attention formulation: ğ’’ğ‘¡, ğ’Œğ‘¡, ğ’—ğ‘¡ = Wğ‘/ğ‘˜/ğ‘£ ğ’™ğ‘¡. (cid:16) ğ‘¡(cid:213) exp ğ‘¡ ğ‘–=1 ğ‘—=1 exp (cid:17) ğ’’ ğ‘¡ ğ’Œğ‘– ğ‘‘ (cid:16) ğ’’ ğ‘¡ ğ’Œ ğ‘— ğ‘‘ (cid:17) ğ’—ğ‘–, ğ’ğ‘¡ = (1) (2) where ğ‘‘ is the head dimension size. Let ğµ be the block size of our attention score output, and there will be ğ‘¡/ğµ number of blocks. We define the column token index set at block index ğ‘– as Bğ‘– = {(ğ‘– 1) ğµ + 1, . . . , min(ğ‘–ğµ, ğ‘)}. Then the block-level max attention score ğ‘…ğ‘¡ğ‘¡/ğµ will be: exp Sğ‘– ğ‘¡ = max ğ‘– Bğ‘– ' ğ‘¡ ğ‘—=1 exp (cid:16) (cid:17) ğ’’ ğ‘¡ ğ’Œ ğ‘– ğ‘‘ (cid:16) ğ’’ ğ‘¡ ğ’Œ ğ‘— ğ‘‘ (cid:17) ï¬ ï¬ (3) We find that the Sğ‘¡ can be easily obtained by slightly modifying the FlashAttention kernel, following the approach similar to (Gao et al., 2024, 2025). Specifically, FlashAttention already computes the row-wise maximum of the attention logits during its online softmax procedure, and this intermediate result can be reused to derive block-wise attention scores by storing and appropriately rescaling it. Algorithm 1 summarizes the modified FlashAttention procedure, and we assume the sparse attention block size ğµ is the same as ğµğ‘ for simplicity. In addition to the standard attention output, the kernel emits block-level attention scores that can be directly used for salient block selection in later sparse attention layers with negligible overhead. With the block-wise attention scores ğ‘†, we apply TopK operator to select key-block indices that are reused by the subsequent sparse attention layers. Noted that attending to ğ‘˜ tokens in sparse attention corresponding to selecting ğ‘˜/ğµ TopK blocks of tokens, where ğµ is the block size. In HySparse, the default ğ‘˜ and ğµ is 1024 and 64, respectively. Under Grouped-Query Attention (GQA) (Ainslie et al., 2023), we further aggregate ğ‘† within each query group (via group-wise maximum) so that all heads in the same group share identical sparse indices, improving sparse attention kernel efficiency and reducing indexing overhead. 5 Algorithm 1 FLASHATTENTION with Block Attention Score Output (assuming ğµ = ğµğ‘ for simplicity) Require: Queries Rğ‘¡ğ‘‘, Keys Rğ‘¡ğ‘‘, Values Rğ‘¡ğ‘‘, softmax scale ğœ Ensure: Attention output Rğ‘¡ğ‘‘, Block attention scores Rğ‘¡ğ‘¡/ğµ 1: ğ‘‡ğ‘Ÿ ğ‘¡/ğµğ‘€, 2: Initialize: Oğ‘–, Sğ‘–, ğ’ğ‘–, â„“ğ‘– 3: for ğ‘– = 0, . . . , ğ‘‡ğ‘Ÿ 1 do 4: ğ‘‡ğ‘ ğ‘¡/ğµğ‘ Load Qğ‘– Rğµğ‘€ ğ‘‘ from HBM to SRAM for ğ‘— = 0, . . . , ğ‘‡ğ‘ 1 do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: ğœ Load ğ‘—, ğ‘— Rğµğ‘ ğ‘‘ from HBM to SRAM Ağ‘– ğ‘— Qğ‘–K ğ‘— ğ’ğ‘– ğ‘— rowmax(Ağ‘– ğ‘—), Store ğ’ğ‘– ğ‘— to Sğ‘– ğ‘— ğ’ğ‘– max(ğ’ğ‘–, ğ’ğ‘– ğ‘—) Update Oğ‘–, â„“ğ‘– as in FLASHATTENTION end for Oğ‘– Oğ‘–/â„“ğ‘–, Write Oğ‘– to HBM for ğ‘— = 0, . . . , ğ‘‡ğ‘ 1 do Sğ‘– ğ‘— (Sğ‘– ğ‘— ğ’ğ‘–) / â„“ğ‘–, Write Sğ‘– ğ‘— to HBM end for 15: 16: end for 17: return O, S"
        },
        {
            "title": "3.3 Sparse Attention Layers",
            "content": "Each sparse layer contains two attention branches that operate on the same query but use different KV sources. Block Sparse Attention branch attends only to key-value blocks indexed by I, where both and the KV cache are derived from the preceding full attention layer. SWA branch attends to local sliding window of size ğ‘¤ with its own small KV cache, improving locality and expressivity. ğ‘¤ is set to be 128 in HySparse implementation. The two branch outputs are then fused via lightweight sigmoid gate. The detailed processes can be model as below. First, we compute the standard SWA branch output using: ğ‘¡, ğ’Œ ğ’’ ğ‘¡, ğ’— ğ‘¡ = Wğ‘/ğ‘˜/ğ‘£ ğ’™ğ‘¡ (cid:17) (cid:16) ğ‘¡(cid:213) exp ğ‘¡ ğ‘–=ğ‘¡ğ‘¤+1 ğ‘—=ğ‘¡ğ‘¤+1 exp ğ‘¡ ğ’Œ ğ’’ ğ‘– ğ‘‘ (cid:16) (cid:17) ğ’— ğ‘– ğ‘¡ ğ’Œ ğ’’ ğ‘— ğ‘‘ ğ’ ğ‘¡ = (4) (5) Then, when computing the sparse attention branch, we concatenate the selected key and value blocks from the shared K, from the full attention layers using block indices I. The sparse attention and SWA branch uses the same query ğ’’ ğ‘¡, and the output can then be written as: (cid:16) K, = concat {K/V[ ( ğ‘—1) ğµ+1: ğ‘—ğµ] } ğ‘— ğ‘˜(cid:213) exp ğ‘˜ ğ‘–=1 ğ‘—=1 exp (cid:16) (cid:17) ğ’’ ğ‘¡ ğ’Œğ‘– ğ‘‘ (cid:16) ğ’’ ğ‘¡ ğ’Œ ğ‘— ğ‘‘ (cid:17) ğ’—ğ‘– ğ’ğ‘¡ = (cid:17) (6) (7) Finally, we apply sigmoid gate on both branch outputs and sum them to obtain the final attention layer output (Qiu et al., 2025). (cid:0) (cid:1) ğ‘”ğ‘¡, ğ‘” Wğ‘”/ğ‘” ğ’™ğ‘¡ ğ‘¡ = ğœ ğ’ğ‘¡ = ğ‘”ğ‘¡ ğ’ğ‘¡ + ğ‘” ğ‘¡ ğ’ ğ‘¡ (8) (9) Through our experiments, we find that maintaining an independent KV cache for the SWA branch is essential for preserving model expressivity. One possible explanation is that SWA primarily serves as local information pathway and requires different representations to capture shortrange coherence, whereas the KV shared from the preceding full attention layer is optimized for global retrieval and may lack sufficient local features. The two-branch gated fusion yields dynamic mixture of global and local information while remaining efficient in both computation and memory usage."
        },
        {
            "title": "4.1 Experiments Setup",
            "content": "Configuration 7B Dense 80B MoE Layers Attention Heads (Q / KV) Head Dimensions Hidden Size Hybrid Ratio (Full : Sparse) Sliding Window Size Sparse Attn Block Size Sparse Attn TopK Tokens MoE Expert (Activated/Total) 36 32/8 128 4096 1 : 3 128 64 49 64/4 128 2048 1 : 11 128 64 1024 8 : 512 Table 1 Model Architecture Configurations. Model Configuration In the following experiments, we use standard 7B dense Transformer with 36 layers and an 80B-A3B MoE model with 49 layers. We adopt Grouped-Query Attention (GQA), using 32 query heads with 8 KV heads for the 7B dense model, and 64 query heads with 4 KV heads for the 80B MoE model. We evaluate three architectures: (1) Full-Attn: all layers use standard full attention. (2) Hybrid SWA: hybrid sliding window attention models, using full-toSWA layer ratio of 1:3 for the 7B model and 1:11 for the 80B MoE model. (3) HySparse: hybrid models with the same hybrid ratios as Hybrid SWA, but augmenting SWA with the proposed sparse attention (Top-1024 tokens with block size 64). For all hybrid models, the final layer uses full attention. For sparse attention and sliding window attention, we incorporate per-head learnable sink biases, following the approach in gpt-oss (Agarwal et al., 2025). For Full-Attn in the MoE setting, we additionally employ gated attention (Qiu et al., 2025) to stabilize training. Detailed model configurations are listed in Table 1. Training Hyper-parameters For the 7B models, we first train on 1T tokens with sequence length of 8,192 using the AdamW optimizer (ğ›½1 = 0.9, ğ›½2 = 0.95, ğœ– = 1010), weight decay 0.1, and gradient clipping with maximum norm of 1.0. Training uses BF16 precision and WSD schedule with maximum learning rate of 8.3104. To extend the models for long-context evaluation, we further train on 200B tokens with sequence length of 32,768 and learning rate of 3.0 105. 7 The RoPE base frequency is adjusted to 640,000 at this stage. For the 80B MoE model, training is performed on 500B tokens with sequence length of 32,768 using the WSD schedule with maximum learning rate of 1 103, and the RoPE base frequency is also set to 640,000. Evaluation Benchmark We evaluate HySparse based on series of benchmarks, encompassing various capabilities: (1) General language understanding and reasoning, including BBH (Suzgun et al., 2023), MMLU (Hendrycks et al., 2021a), MMLU-Redux (Gema et al., 2024), MMLUPro (Wang et al., 2024), DROP (Dua et al., 2019), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), TriviaQA (Joshi et al., 2017), (2) Mathematics reasoning, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b) (3) Coding, including HumanEval (Chen, 2021), MBPP (Austin et al., 2021), (4) Chinese understanding, including C-Eval (Huang et al., 2023), CMMLU (Li et al., 2023). (5) Long context, Ruler (Hsieh et al., 2024)."
        },
        {
            "title": "4.2 Performance of HySparse on General Benchmarks",
            "content": "Benchmark # Shots 7B Dense (Hybrid 1:3) 80B MoE (Hybrid 1:11) Full-Attn Hybrid SWA HySparse Full-Attn Hybrid SWA HySparse General BBH MMLU MMLU-Redux MMLU-Pro DROP ARC-Challenge HellaSwag WinoGrande TriviaQA Mathematics GSM8K MATH Code HumanEval MBPP Chinese C-Eval CMMLU 3-shot 5-shot 5-shot 5-shot 3-shot 25-shot 10-shot 5-shot 5-shot 8-shot 4-shot 0-shot 3-shot 5-shot 5-shot 52.2 56.9 59.6 26.8 53.1 70.2 77.5 73.7 50.1 33.3 9.2 25.0 51.0 50.6 52. 54.0 57.5 60.8 26.5 43.8 74.9 77.8 74.9 50.0 35.6 9.2 22.0 52.8 50.6 52.9 53.5 58.8 61.6 29.0 52.4 75.0 78.1 74.3 51.1 37.9 10. 23.5 51.6 52.2 54.5 56.1 61.8 65.6 33.8 56.7 78.4 78.2 71.2 54.7 53.8 28.6 35.4 55.3 64.6 66. 48.2 54.9 57.4 27.2 47.8 63.9 77.1 69.0 52.2 45.3 25.8 31.7 51.9 58.8 58.4 56.3 62.2 66.2 32.6 56.5 77.6 79.0 72.1 55.5 54.1 30. 38.4 59.3 65.0 67.0 Table 2 Comparison of HySparse across 7B dense models (trained on 1T tokens) and 80B MoE models (trained on 500B tokens) with Full-Attn and Hybrid SWA. Best results in each row are highlighted in bold. 7B Dense Performance Table 2 compares 7B models under three attention variants. Overall, HySparse achieves strong performance across general benchmarks, mathematics, and Chinese understanding, while Hybrid SWA offers competitive and computationally efficient baseline that is particularly strong on BBH and MBPP+. Specifically, HySparse surpasses the Full-Attn baseline on broad set of knowledge and reasoning benchmarks, including MMLU (58.8 vs. 56.9), MMLURedux (61.6 vs. 59.6), and MMLU-Pro (29.0 vs. 26.8), suggesting that sparse token selection can preserve (and even enhance) global reasoning and factual recall despite reduced attention computation. HySparse also yields consistent gains on multi-step reasoning tasks such as GSM8K and MATH. On classic multiple-choice commonsense and reading comprehension benchmarks, HySparse is either best or comparable, with slight improvements on ARC-Challenge, HellaSwag, 8 and TriviaQA. For Chinese benchmarks, HySparse provides clear gains on both C-Eval (52.2 vs. 50.6) and CMMLU (54.5 vs. 52.5). Scaling to 80B MoE We further evaluate HySparse in an 80B-A3B MoE setting with more aggressive full-to-sparse layer ratio of 1:11. Despite having only five full attention layers, HySparse outperforms both Full-Attn and Hybrid SWA across nearly all benchmarks; only MMLU-Pro, DROP, and ARC-C are slightly lower than Full-Attn. Notably, the performance gains are often larger than those observed in the 7B dense setting. In this regime, Hybrid SWA exhibits noticeable accuracy degradation compared to Full-Attn on several benchmarks, including BBH, DROP, the MMLU series, GSM8K, and Chinese understanding. This suggests that relying solely on local window attention becomes insufficient as the hybrid ratio becomes more aggressive. By introducing the sparse attention branch, HySparse mitigates this gap by recovering access to globally relevant tokens selected from the full attention, and in many cases even surpasses the Full-Attn baseline, while requiring 10 less KV cache. These results highlight key advantage of HySparse: the number of full attention layers can be substantially reduced without sacrificing modeling capability, which is also the design rationale of HySparse."
        },
        {
            "title": "4.3 Long-context Benchmarks",
            "content": "Size Ctx Type S1 S2 S3 MK1 MK2 MK3 MQ MV VT CWE FWE Total 7B 80B 16k 32k 16k 32k Full-Attn Hybrid SWA 100.0 100.0 HySparse 100.0 100.0 100.0 99.8 98.6 97.6 100.0 100.0 100.0 99.4 Full-Attn Hybrid SWA 100.0 100.0 100.0 100.0 HySparse 100.0 100.0 100.0 99.6 99.8 Full-Attn Hybrid SWA HySparse 100.0 95.2 100.0 99.8 94.8 99.8 100.0 Full-Attn Hybrid SWA 100.0 HySparse 99.2 98.2 100.0 100. 92.6 70.8 99.0 81.2 58.8 99.4 98.0 96.2 98.2 99.6 93.2 98.2 99.0 89.6 96.8 100.0 99.8 99. 99.4 98.8 99.6 96.4 96.6 96.6 75.8 53.4 76.2 99.2 99.4 94.4 94.7 99.1 97.4 97.2 89.8 96.7 96.4 98.3 90.5 87.2 98.5 93.4 91.2 94.7 88.8 93.0 99.2 86.4 69.4 100.0 99. 97.3 94.9 95.4 66.7 57.8 83.2 90.3 91.3 92.2 99.4 90.2 99.0 86.0 74.1 74.5 77.0 61.0 73.1 98.4 89.5 85.7 89.6 79.5 56.6 37.1 23.7 60. 16.6 10.4 38.8 74.5 13.3 40.2 40.7 8.4 20.8 97.4 98.5 95.5 95.1 88.5 95.1 80.4 69.2 86. 66.7 54.3 82.1 93.0 91.6 94.1 88.2 84.2 89.3 93.6 72.7 90.6 82.1 69.5 87.4 Table 3 RULER benchmark performance for 7B dense and 80B MoE models. Note that all 7B models were first trained on 1T tokens with sequence length of 8K, and then further trained on 200B tokens with sequence length of 32K. All 80B MoE models were trained on 500B tokens with sequence length of 32K. Table 3 shows that HySparse consistently preserves strong long-context accuracy. For the 7B dense model, HySparse improves the overall score over both baselines at 16k and 32k, reaching totals of 94.1 and 89.3 (vs. 93.0 and 88.2 for Full-Attn and 91.6 and 84.2 for Hybrid SWA). The gains are most apparent on the harder multi-key/value and reasoning-heavy subsets, e.g., HySparse substantially boosts CWE compared to baselines (60.8 at 16k and 38.8 at 32k), indicating better robustness as context grows. For the 80B MoE model, Hybrid SWA degrades sharply (72.7 at 16k and 69.5 at 32k) under an aggressive hybrid ratio, whereas HySparse remains competitive with Full-Attn at 16k (90.6 vs. 93.6) and notably surpasses it at 32k (87.4 vs. 82.1), driven by large recoveries on difficult components such as MK3 (98.4 vs. 77.0) and stronger VT/MQ/MV stability. In general, HySparse provides on-par long-context capabilities compared with Full-Attn across settings, while significantly reducing computation and KV cache size."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "In this section, we present detailed ablation studies on key architecture design choices, focusing on: (1) whether to include an intra-layer SWA branch within each sparse layer, and (2) how KV cache sharing is applied across the sparse attention and SWA branches. All ablation studies are conducted on the 7B dense models. The results are summarized in Table 4 and Figure 2. DROP GSM8K MMLU MMLU-Pro BBH Group Baselines Method Full-Attn Hybrid SWA Oracle Token Selection (w/o KV cache sharing) HySparse (w/o intra-layer-SWA) HySparse (w/ intra-layer-SWA) 52.6 43. 46.4 52.2 KV Cache Sharing (w/ intra-layer SWA) HySparse (sharing for SA & SWA) 47.9 HySparse (sharing only for SA) 51.9 32.6 35.7 29.7 37.7 30. 36.7 56.8 57.6 57.1 56.1 52. 58.4 26.8 26.6 25.0 26.5 23. 29.0 52.1 54.3 48.2 52.4 47. 53.9 Table 4 Ablation of Different Architecture Design Choices on 7B experiments. Figure 2 HySparse Accuracy vs. Training Iterations. Intra-layer Hybridization with SWA This study investigates whether sparse attention Study 1: layers still benefit from an additional SWA branch when the sparse indices are provided by oracle token selection (with KV cache sharing disabled). natural hypothesis is that the SWA branch might be redundant: if recent or local-context tokens are important, oracle selection from the preceding full attention layer should already include them among the selected blocks. However, our results indicate that removing the SWA branch leads to clear accuracy drop. Specifically, we compare HySparse (w/o intra-layer SWA) against HySparse (w/ intra-layer SWA) under the same oracle-selected sparse indices, with both branches sharing the same QKV projection layers. As shown in Table 4, adding the intra-layer SWA branch yields consistent improvements across most benchmarks: DROP increases from 46.4 to 52.2 (+5.8), GSM8K from 29.7 to 37.7 (+8.0), MMLU-Pro from 25.0 to 26.5 (+1.5), and BBH from 48.2 to 52.4 (+4.2). These gains suggest that even with high-quality sparse selection, dedicated sliding window pathway remains important for modeling short-range coherence and local computation patterns that are not reliably captured by sparse global retrieval alone. Additionally, the SWA branch may help stabilize optimization by providing consistent local pathway, particularly during early training stages. Study 2: Cross-layer KV Cache Sharing Configuration This study examines how KV cache sharing should be applied when each sparse layer contains both sparse attention (SA) branch for global retrieval and sliding window attention (SWA) branch for local modeling. natural design choice is to maximize memory reuse by sharing the same KV cache for both branches, i.e., reusing the KV cache produced by the preceding full attention layer for both SA and SWA. However, this coupling can be overly restrictive because the two branches serve distinct roles: SA primarily requires globally informative keys and values aligned with block-level retrieval, whereas SWA benefits from dedicated local representation that emphasizes short-range coherence and local computation patterns. In our KV cache sharing experiments, we compare HySparse (sharing for both SA & SWA) against HySparse (sharing only for SA). As shown in Table 4, sharing KV caches for both SA and SWA substantially degrades accuracy. In contrast, sharing KV only for the SA branch while maintaining an independent KV cache for SWA recovers and improves performance across all evaluated tasks: DROP increases from 47.9 to 51.9 (+4.0), GSM8K from 30.2 to 36.7 (+6.5), MMLU from 52.8 to 58.4 (+5.6), MMLU-Pro from 23.2 to 29.0 (+5.8), and BBH from 47.2 to 53.9 (+6.7). These results suggest that SA can safely reuse the cross-layer KV cache from full attention to save GPU memory, whereas SWA should maintain its own KV cache to preserve strong local information. Forcing SWA to reuse the KV cache from the preceding full attention layer likely deprives it of the short-range, local features it requires and entangles it with globally optimized representations, thereby weakening the local pathway and reducing overall accuracy."
        },
        {
            "title": "5 Discussion & Future Works",
            "content": "Can We Ultimately Avoid Full Attention? Our findings connect to broader trend in efficient attention, including hybrid attention and sparse attention methods. recurring theme is that it remains challenging to completely eliminate ğ‘‚(ğ‘›2)-style full attention components in practice: hybrid models retain explicit full attention layers, while sparse attention methods such as SeerAttention (Gao et al., 2024) and DSA (Liu et al., 2025a) typically rely on gating or indexing mechanisms that still operate in ğ‘‚(ğ‘›2), albeit in compressed form. In this context, what matters most is the ratio of expensive global computation to cheaper local or sparse computation, as well as GPU memory usage. Our results suggest that HySparse, with oracle token selection and cross-layer KV sharing, provides promising approach to reduce this ratio while preserving long-context modeling capabilities. Potential of HySparse for Eï¬€icient KV Cache Offloading HySparse also points to straightforward systems-level strategy for long-context serving: offload the full attention KV cache to external memory and pre-fetch it before computation, while keeping only the persistent selected/sparse KV on the GPU for subsequent sparse attention layers. Previous work such as OmniKV (Hao et al., 2025) has explored similar approaches in post-training setting. This technique has the potential to significantly reduce the KV cache footprint on GPU, enabling larger batch sizes and improving overall inference efficiency."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced Hybrid Sparse Attention (HySparse), simple yet effective hybrid attention architecture that interleaves each full attention layer with multiple sparse-attention layers. By strategically deriving important token selections and KV caches from preceding full attention layers, HySparse eliminates the need for proxy-based token selection and enables sparse layers to operate without additional memory overhead. Importantly, HySparse allows for substantial reduction in the number of full attention layers in hybrid models without compromising modeling capabilities. In future work, we plan to scale HySparse to even larger model sizes and train on more tokens to fully exploit its potential for efficient and accurate long-context modeling."
        },
        {
            "title": "References",
            "content": "S. Agarwal, L. Ahmad, J. Ai, S. Altman, A. Applebaum, E. Arbus, R. K. Arora, Y. Bai, B. Baker, H. Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. J. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. LebrÃ³n, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Anthropic. Building effective AI agents. https://www.anthropic.com/research/building-effecti ve-agents, Dec. 2024. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. A. Basant, A. Khairnar, A. Paithankar, A. Khattar, A. Renduchintala, A. Malte, A. Bercovich, A. Hazare, A. Rico, A. Ficek, et al. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model. arXiv preprint arXiv:2508.14444, 2025. A. Blakeman, A. Basant, A. Khattar, A. Renduchintala, A. Bercovich, A. Ficek, A. Bjorlin, A. Taghibakhshi, A. S. Deshmukh, A. S. Mahabaleshwarkar, et al. Nemotron-h: family of accurate and efficient hybrid mamba-transformer models. arXiv preprint arXiv:2504.03624, 2025. W. Brandon, M. Mishra, A. Nrusimha, R. Panda, and J. Ragan-Kelley. Reducing transformer keyvalue cache size with cross-layer attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. M. Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. 2023. URL https://arxiv.org/abs/2307.08691. T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. T. Dao, D. Fu, S. Ermon, A. Rudra, and C. RÃ©. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344 16359, 2022. D. Deshmukh, S. Goyal, N. Kwatra, and R. Ramjee. Kascade: practical sparse attention method for long-context llm inference. arXiv preprint arXiv:2512.16391, 2025. 13 D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246. Y. Gao, Z. Zeng, D. Du, S. Cao, P. Zhou, J. Qi, J. Lai, H. K.-H. So, T. Cao, F. Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Y. Gao, S. Guo, S. Cao, Y. Xia, Y. Cheng, L. Wang, L. Ma, Y. Sun, T. Ye, L. Dong, et al. Seerattentionr: Sparse attention adaptation for long reasoning. arXiv preprint arXiv:2506.08889, 2025. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, et al. Are we done with mmlu? ArXiv preprint, abs/2406.04127, 2024. URL https://arxiv.org/abs/2406.04127. Gemma Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. RamÃ©, M. RiviÃ¨re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Incentivizing reasoning capability in llms via reinforcement learning. arXiv Deepseek-r1: preprint arXiv:2501.12948, 2025. J. Hao, Y. Zhu, T. Wang, J. Yu, X. Xin, B. Zheng, Z. Ren, and S. Guo. Omnikv: Dynamic context selection for efficient long-context llms. In The Thirteenth International Conference on Learning Representations, 2025. H. He and T. M. Lab. Defeating nondeterminism in llm inference. Machines Lab: https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. Connectionism, 2025. doi: Thinking 1 0 . 6 4 4 3 4 / . 2 0 2 5 0 9 1 0. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv preprint, abs/2103.03874, 2021b. URL https://arxiv.org/abs/2103.03874. C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg. arXiv preprint Ruler: Whats the real context size of your long-context language models? arXiv:2404.06654, 2024. J. Hu, F. Li, M. Xu, F. Meng, S. Zhao, T. Hu, T. Peng, A. Liu, W. Huang, C. Liu, et al. Lil: Less is less when applying post-training sparse-attention algorithms in long-decode stage. arXiv preprint arXiv:2601.03043, 2026. 14 Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/c6ec1844bec96d6 d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. H. Jiang, Y. Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H. Abdi, D. Li, C.-Y. Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Kimi Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. Kimi Team, Y. Zhang, Z. Lin, X. Yao, J. Hu, F. Meng, C. Liu, X. Men, S. Yang, Z. Li, et al. Kimi linear: An expressive, efficient attention architecture. arXiv preprint arXiv:2510.26692, 2025b. A. Li, B. Gong, B. Yang, B. Shan, C. Liu, C. Zhu, C. Zhang, C. Guo, D. Chen, D. Li, et al. Minimax01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025a. E. Li, A. B. L. Larsen, C. Zhang, X. Zhou, J. Qin, D. A. Yap, N. Raghavan, X. Chang, M. Bowler, E. Yildiz, et al. Apple intelligence foundation language models: Tech report 2025. arXiv preprint arXiv:2507.13575, 2025b. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. ArXiv preprint, abs/2306.09212, 2023. URL https://arxiv.org/abs/2306.09212. O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. A. Liu, J. Liu, Z. Pan, Y. He, G. Haffari, and B. Zhuang. Minicache: Kv cache compression in depth dimension for large language models. Advances in Neural Information Processing Systems, 37: 139997140031, 2024. A. Liu, A. Mei, B. Lin, B. Xue, B. Wang, B. Xu, B. Wu, B. Zhang, C. Lin, C. Dong, et al. Deepseekv3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. R. Liu, Y. Sun, M. Zhang, H. Bai, X. Yu, T. Yu, C. Yuan, and L. Hou. Quantization hurts reasoning? an empirical study on quantized reasoning models. arXiv preprint arXiv:2504.04823, 2025b. 15 E. Lu, Z. Jiang, J. Liu, Y. Du, T. Jiang, C. Hong, S. Liu, W. He, E. Yuan, Y. Wang, Z. Huang, H. Yuan, S. Xu, X. Xu, G. Lai, Y. Chen, H. Zheng, J. Yan, J. Su, Y. Wu, Y. Zhang, Z. Yang, X. Zhou, M. Zhang, and J. Qiu. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. MiniCPM Team. Minicpm4: Ultra-efficient llms on end devices. 2025. A. Qiao, Z. Yao, S. Rajbhandari, and Y. He. Swiftkv: Fast prefill-optimized inference with In Proceedings of the 2025 Conference on knowledge-preserving model transformation. Empirical Methods in Natural Language Processing, pages 2574525764, 2025. Z. Qiu, Z. Wang, B. Zheng, Z. Huang, K. Wen, S. Yang, R. Men, L. Yu, F. Huang, S. Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sinkfree. arXiv preprint arXiv:2505.06708, 2025. Qwen Team. Qwen3-next: Towards ultimate training & inference efficiency. https://qwen.ai/blog ?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list, Sept. 2025. Accessed: 2026-01-22. K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6399. J. Shah, G. Bikshandi, Y. Zhang, V. Thakkar, P. Ramani, and T. Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Y. Sun, L. Dong, Y. Zhu, S. Huang, W. Wang, S. Ma, Q. Zhang, J. Wang, and F. Wei. You only cache once: Decoder-decoder architectures for language models. Advances in Neural Information Processing Systems, 37:73397361, 2024. M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and J. Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.824. J. Tang, Y. Zhao, K. Zhu, G. Xiao, B. Kasikci, and S. Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ad236edc564f3e3156e1b2feafb99a24-Abs tract-Datasets_and_Benchmarks_Track.html. 16 G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. G. Xiao, J. Tang, J. Zuo, J. Guo, S. Yang, H. Tang, Y. Fu, and S. Han. Duoattention: Efficient longcontext llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. B. Xiaomi Team, Xiao, B. Xia, B. Yang, B. Gao, B. Shen, C. Zhang, C. He, C. Lou, F. Luo, G. Wang, et al. Mimo-v2-flash technical report. arXiv preprint arXiv:2601.02780, 2026. L. Yang, Z. Zhang, Z. Chen, Z. Li, and Z. Jia. Tidaldecode: Fast and accurate llm decoding with position persistent sparse attention. arXiv preprint arXiv:2410.05076, 2024a. L. Yang, Z. Zhang, A. Jain, S. Cao, B. Yuan, Y. Chen, Z. Jia, and R. Netravali. Less is more: arXiv preprint Training-free sparse attention with global locality for efficient reasoning. arXiv:2508.07101, 2025a. S. Yang, J. Kautz, and A. Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024b. S. Yang, J. Guo, H. Tang, Q. Hu, G. Xiao, J. Tang, Y. Lin, Z. Liu, Y. Lu, and S. Han. Lserve: Efficient long-sequence llm serving with unified sparse attention. arXiv preprint arXiv:2502.14866, 2025b. J. Yuan, H. Gao, D. Dai, J. Luo, L. Zhao, Z. Zhang, Z. Xie, Y. Wei, L. Wang, Z. Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. H. E. Zarch, L. Gao, C. Jiang, and M. Annavarm. Delta: Dynamic layer-aware token attention for efficient long-context reasoning. arXiv preprint arXiv:2510.09883, 2025. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can machine really finish your sentence? In A. Korhonen, D. Traum, and L. MÃ rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472. Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Re, C. Barrett, Z. Wang, and B. Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=RkRrPp7GKO. W. Zhao, Z. Zhou, Z. Su, C. Xiao, Y. Li, Y. Li, Y. Zhang, W. Zhao, Z. Li, Y. Huang, et al. Infllmv2: Dense-sparse switchable attention for seamless short-to-long adaptation. arXiv preprint arXiv:2509.24663, 2025."
        }
    ],
    "affiliations": [
        "Xiaomi"
    ]
}