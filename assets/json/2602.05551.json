{
    "paper_title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
    "authors": [
        "Yue Ma",
        "Zhikai Wang",
        "Tianhao Ren",
        "Mingzhe Zheng",
        "Hongyu Liu",
        "Jiayi Guo",
        "Mark Fong",
        "Yuxuan Xue",
        "Zixiang Zhao",
        "Konrad Schindler",
        "Qifeng Chen",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 1 5 5 5 0 . 2 0 6 2 : r FastVMT : Eliminating Redundancy in Video Motion Transfer Yue Ma2, Zhikai Wang1, Tianhao Ren1, Mingzhe Zheng2, Hongyu Liu2, Jiayi Guo3, Mark Fong, Yuxuan Xue4, Zixiang Zhao5, Konrad Schindler5, Qifeng Chen2, Linfeng Zhang1(cid:66) 1 EPIC Lab, SJTU 2 HKUST 3 THU 4 Meta 5 ETH Zurich Project: https://fastvmt.github.io/ Figure 1. Efficient motion transfer with FastVMT: By eliminating redundant attention computations and reusing previously computed gradients, we achieve faster motion transfer for singleas well as multi-object motion, camera ego-motion, and complex articulations."
        },
        {
            "title": "Abstract",
            "content": "Video motion transfer aims to synthesize videos by generating visual content according to text prompt while transferring the motion pattern observed in reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit 1 gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves 3.43 speedup without degrading the visual fidelity or the temporal consistency of the generated videos. 1. Introduction With the rapid advancement of AI-generated content (AIGC), wide range of downstream visual synthesis applications have been significantly empowered [3, 4, 7, 21, 22, 2527, 32, 3741, 62], among which motion transfer has emerged as particularly important and intuitive paradigm. Motion transfer aims to generate novel video by transferring the dynamics of reference video sequence to target sequence, while preserving the targets appearance and semantics. For instance, the reference video might show an action sequence performed by an actor, which shall be transferred to target subject while preserving their identity; or the reference might prescribe particular camera path through the scene, which one would like to replicate for the target scene (see Fig. 1). In other words, motion transfer offers an intuitive interface for controllable motion synthesis, with applications ranging from movie productions and game development to digital advertising and content creation on social media platforms. Recent advances in video motion transfer increasingly leverage large, foundational generative video models [28]. These models typically employ the DiT architecture within denoising diffusion loop1. They are not only capable of synthesizing high-quality videos from noise, but can also be conditioned with text or image prompts to control the video style and content. variety of motion transfer approaches have emerged that leverage these powerful visual priors, in either training-based and training-free fashion. Training-based methods (e.g., MotionDirector [85], MOFT [81], DeT [45]) extract the motion patterns of specific reference video by fine-tuning the parameters of the diffusion backbone. For example, MotionDirector [85] and DreamMotion [14] adopt dual-path versions of low-rank adaptation [10] to disentangle the representations of motion and appearance in the diffusion DiT. Although they are capable of generating videos whose motion follows the reference, they suffer from practical limitations: overfitting to every new reference video is time-consuming (e.g., up to 2 hours on an A100 GPU) and therefore unsuitable for open-domain and real-time settings. To achieve efficient and generally applicable motion transfer, attention has shifted to training-free frameworks [35, 63, 69]. They obviate the need for per-video fine-tuning and thus Equal contribution. (cid:66) Corresponding author. 1In this paper, the term diffusion includes flow-based interpolants [18, 20]. enable significantly faster synthesis (e.g., 10 minutes on an A100 GPU). The training-free approach also exploits the gradual, iterative denoising process of contemporary video foundation models: The reference video is first inverted into the embedding space of the DiT to extract features that encode the motion. Then the output video is synthesized by denoising diffusion, guided by both text prompt and the gradient between the motion embeddings of the source and target video. Our work is motivated by the observation that, in existing implementations of this pipeline, both the extraction of motion embedding from DiT backbone and the computation of motion gradients introduce considerable redundancy. Rather elementary properties of videos, and of the associated generative process, suggest that the computational cost of training-free motion transfer can be reduced considerably. (i) Motion redundancy: To extract the motion embeddings from latents [68] or attention maps [35] in the inversion stage, it is not necessary to calculate pairwise similarities between all tokens of consecutive frames. Frame-to-frame motion has limited magnitude and is locally smooth, hence motion features can be computed more efficiently, see Fig. 2(a). (ii) Gradient redundancy: In the denoising stage, there is no need to recalculate all gradients at each timestep. We find that motion transfer is case of stable gradient optimization. Motivated by the idea of deterministic sampling to upgrade DDPM [9] to DDIM [46], we examine the gradient updates in consecutive optimization steps and observe that they tend to be similar, see Fig. 2(b). Consequently, gradients can be reused over multiple iterations. Based on these observations, FastVMT makes two contributions to achieve efficient motion transfer. (1) Instead of extracting motion embeddings token by token, as in DiTFlow [36], we design sliding-window strategy that operates on downsampled attention maps and an associated corresponding window loss, to perform more reliable and more efficient local search for motion correspondence. (2) We address gradient redundancy with step-skipping gradient computation. Gradients are recalculated only at selected iteration steps, between those steps, the most recent values are reused so as to reduce the total number of gradient calculations and amortize them better. These two tricks enable high-fidelity video generation with camera trajectories and/or object motions according to the source video, see Fig. 1. Extensive experiments and user studies confirm that FastVMT achieves state-of-the-art performance both qualitatively and quantitatively, with up to 14.91 lower latency. Furthermore, FastVMT delivers 3.43 speedup with minimal performance degradation, preserving near-lossless quality across various evaluation metrics when compared to the original training-free video motion transfer pipeline. 2 Figure 2. Motivation of our method. Training-free video motion transfer can benefit from redundancies, both at the level of the DiT architecture and of the iterative diffusion process. (a) Motion redundancy: Video motion is small and locally consistent, so motion token in one frame will only ever match tokens in the next frame within local neighborhood. (b) Gradient redundancy: Gradient updates in consecutive optimization steps are mostly similar (visualized here with PCA). There is no need to recompute them at every single step. 2. Related work 3. Method Text-to-video generation. Text-to-video generation aims to synthesize realistic videos by precisely matching both the visual content and motion dynamics described in the input prompt. Previous works [1, 6, 8, 24, 54, 5659, 66, 73 76, 79] introduce temporal modules in UNet architectures to generate coherent videos. To generate complex video motion, the advancement of Diffusion Transformer-based methods for text-to-video generation exhibits superior performance in both spatial quality and temporal consistency. These models [2, 16, 23, 29, 52, 65, 67, 71, 72, 77, 78, 80] demonstrate the power of scaling transformers to produce highly realistic video clips from detailed prompts, unlocking potential for diverse downstream video generation tasks. Video motion transfer. Motion transfer focuses on generating novel videos while transferring motion from reference videos, differing from video-to-video translation [19, 31, 84] by decoupling spatial appearance and temporal motion. Early approaches rely on explicit control signals such as poses [25, 84], depths [44, 64], and bounding boxes [60]. Training-based methods [14, 30, 31, 43, 48, 49, 82, 83, 85] employ spatial-temporal decoupled attention mechanisms by dual-path LoRA architecture. Recent works [43, 61] improving motion-appearance disentanglement, though they remain time-consuming and non-reusable. Training-free methods [5, 11, 12, 17, 35, 47, 50, 63, 70] extract motion embeddings during inference, with DiTFlow [35] proposing attention motion flow optimization. However, existing methods suffer from computational redundancy in both the architectural and diffusion process perspectives. In contrast, we first analyze the redundancy in training-free motion transfer and design the sliding-window motion extraction and step-skipping optimization to improve efficiency. Given an input video = [I 1, ..., n], and the prompt describing the target video content, we aim to design an efficient training-free framework to generate novel video = [J 1, ..., n] following the input prompt P, while preserving the same camera pose changes and object motion. To achieve this, we propose FastVMT, an efficient framework using DiT-based video generative model [52] to transfer motion efficiently. The pipeline of our method is shown in Fig. 4. We first analyze the existing redundancy in previous works and introduce our motivation in Sec. 3.1. The slidingwindow motion extraction strategy is present in Sec. 3.2. To improve the motion consistency, we design the corresponding window loss in Sec. 3.3. Finally, in Sec. 3.4, we propose the step-skipping gradient optimization to ensure gradient efficiency. 3.1. Motivation We summarize the two observed redundancies of state-ofthe-art approaches in the training-free video motion transfer task and propose the modules to address them. Motion redundancy. In the inversion stage, existing training-free video motion transfer approaches [35, 63, 69] utilize the global token similarity to obtain the reference motion flow. Specifically, for every optimization step, each token requires calculating the similarity with all tokens in the next attention map. However, we note that every motion token will only correspond with token in nearby regions in the next attention map. As shown in Fig. 2(a), the corresponding token in the dogs nose would only appear around nearby regions rather than on the road. Therefore, such property about temporal consistency makes it unreasonable to extract the motion flow by calculating token-by-token similarity globally. To address this, we introduce the sliding3 tative query at the tile center and compute its attention with all keys in the target frame. For any pair of frames (i, j) in the video, the representative cross-frame attention map Arep ij is computed as:"
        },
        {
            "title": "Arep",
            "content": "ij = softmax (cid:32) Q(i) rep(K(j))T Dh (cid:33) τ RNtilesS (1) tw where Ntiles = is the number of tiles, = th is the spatial token length, and τ is the temperature parameter. From this representative attention map, we estimate the window center for each tile as: c(ij) uv = (cid:88) s="
        },
        {
            "title": "Arep",
            "content": "ij [s] pos(s) (2) where pos(s) denotes the spatial position of token s. This estimated center guides the subsequent window-constrained Attention Motion Flow (AMF) computation, enabling efficient motion extraction while maintaining spatial precision. Sliding-window motion extraction. To enhance the computational efficiency and precision of AMF extraction, we propose novel sliding window strategy that mitigates the redundant computations inherent in prior methods. Our approach leverages the observation that long-range query-key interactions in self-attention layers yield diminished motion information, and the most relevant keys for an object are typically confined to local spatial window due to finite motion speeds. We extract AMF from query and key K, both of shape (N, H, W, D), where and denote the height and width of the latent representation, and is the number of frames. Here, = {q1, . . . , qN }, with qi, {1, . . . , } representing the query tensor for specific frame, and follows similar notation. Unlike prior methods that compute AMF across all q-k pairs while attending to the entire spatial dimension, our approach employs sliding window to constrain computations both temporally and spatially: Twindow(qi) = {qj : [i, min(i + sf , )]}, Swindow(kh,w) = {kh,w : (h, w) h,w} (3) where sf represents the temporal span and h,w denotes spatial window of size centered at position (h, w). To determine the optimal window center, we partition each frame into spatial blocks and select representative queries. The window center for each block is computed as: rep (K(j))T (cid:17) Q(i) c(ij) block = Pblock + argmax(h,w) (4) (cid:16) h,w where Pblock is the block center position and the argmax operation yields the displacement vector from representative query-key interactions. 4 Figure 3. Illustration of step-skipping gradient optimization. We observe that skipping some steps in the gradient optimization step does not degrade the motion transfer performance. window motion extraction strategy. Only the regional tokens are calculated for efficient motion extraction. Meanwhile, such design enables correcting the mismatch during the motion extraction, as shown in Fig. 5, ensuring the motion consistency of generated results. Gradient redundancy. During the optimization process of training-free motion transfer methods, significant computational bottleneck emerges from the repetitive gradient calculations performed at each inner optimization step. Specifically, for every denoising timestep, the optimization loop performs gradient computation across all inner optimization steps to update the latent representation. However, we observe that the gradient updates exhibit high similarity across consecutive optimization steps within the same denoising timestep. As shown in Fig. 2(b), the PCA analysis reveals that gradient patterns remain relatively stable across adjacent optimization steps. Therefore, such stable gradient optimization makes it unnecessary to compute gradients at every optimization step. To address this, we introduce the step-skipping gradient optimization strategy. Only specific optimization steps require gradient computation, while intermediate steps reuse cached gradients for efficient optimization (in Fig. 3). 3.2. Efficient attention window Attention acquisition. We leverage the inherent attention mechanism within video Diffusion Transformers (DiTs) to extract fine-grained motion patterns, based on the premise that correlated content across video frames is naturally captured by the self-attention layers query-key interactions. Given an input video = [I 1, ..., n], and the prompt of target video content, we utilize the 3D VAE encoder [52] to obtain its latent representation zref = E(xref ). To obtain clean motion signal, this latent is passed through specific DiT block at low denoising step, typically = 0. For our tile-based approach, we first partition the spatial dimensions into tiles of size (th, tw). For each tile, we select represenFigure 4. Overview of our method. Left: Given reference video, we first leverage the sliding window to extract motion embedding from attention during the inversion stage. At the denoising stage, we calculate the total loss and leverage the step-skipping gradient optimization to guide the video generation. Right: The Step-skipping gradient optimization is proposed to improve gradient redundancy. Additionally, we introduce the corresponding-window loss to boost the motion consistency of generated videos. Our approach significantly enhances efficiency. Temporally, it reduces the time complexity from O(F 2) to O(F ), where is the number of frames, enabling scalable video generation. Spatially, by constraining computations to local window containing the most relevant keys, we eliminate redundant calculations, thereby achieving precise AMF extraction with minimal quality loss. 3.3. Corresponding-window Loss Motivated by the observation that motion information is predominantly captured by closely adjacent query-key pairs, we design weighted AMF loss and corresponding-window loss to enhance motion transfer accuracy with temporal stability. The weighted AMF loss aligns the motion patterns between reference and generated videos by computing the L2 distance between their respective displacement matrices, which is formulated as: LAMF = 1 (cid:88) (i,j)F wji ref ij gen ij 2 2 (5) where represents all frame pairs within the temporal span sf , and the weights are defined as: (cid:40) wd = 1.0 α d1 sf 1 0 if sf otherwise (6) where α is set as 0.2 to provide linear decay, and = i represents the frame distance. 5 To enhance temporal consistency, we introduce corresponding-window loss that penalizes inconsistencies in key representations across adjacent frames within the sliding windows: Lwindow = 1 1 (cid:88) i=0 1 (cid:88) p=1 1 Ni 1 Ni1 (cid:88) t=1 (cid:13) (p) (cid:13) (cid:13) ijt+ (p) ijt (cid:13) 2 (cid:13) (cid:13) 2 , (7) ij denotes the mean key representation within the ij for tile when anchoring at frame where (p) sliding window (p) and comparing with target frame j. The total loss combines both components with appropriate weighting: Ltotal = λAMF LAMF + λwindow Lwindow, (8) where λAMF is set to 5 to emphasize motion alignment, and λtrack is set to 1 to balance the corresponding-window loss. This dual-component loss ensures both accurate motion transfer and temporal stability, effectively addressing motion consistency challenges in video generation. 3.4. Step-Skipping Gradient Optimization Despite the computational efficiency introduced by our sliding window strategy, optimizing the latent representation remains computationally intensive due to the high cost of back propagation through multiple DiT blocks. Through empirical analysis, we observe high degree of similarity in the gradients of the latent representation across consecutive tile size =(30, 52) and tile stride =(15, 26) in VAE space; this yields per-frame token grid of = 8 by = 8 for the DiT. During motion transfer, as Pondaven et al. [34], we enable our sliding-window based AMF guidance at the first 20% outer denoising steps; each guided step runs 10-step latent-only inner optimization with AdamW and linear learning-rate decay 0.003 0.002. At each guided diffusion step t, we form reference latent by adding step-consistent noise to cached clean latents and perform forward pass with null text to extract queries/keys from the 15th DiT block. Qualitative comparison. We compare our approach with the state-of-the-art video motion transfer methods visually: MOFT [63], MotionInversion [55], MotionClone [17], SMM [69], MotionDirector [85], DiTFlow [34], and DeT [45]. For fair comparison, we adapt the Wan-2.1 as the same backbone. Our experimental results demonstrate that FastVMT achieves superior performance and greater versatility across wide range of motion transfer scenarios. As illustrated in Fig. 9, these works [35, 45, 63, 69] have the challenge of handling complicated interaction motion. In contrast, our method enables generating videos with aligned movement patterns, preserving the spatial relationships between moving subjects. Quantitative comparison. We compare our method with the state-of-the-art video motion transfer on on 50 highquality videos selected from the DAVIS dataset [33]. For fair comparison, we employ Wan-2.1 as the same backbone. Previous works are constrained by the limited video length, with evaluations conducted using only 32 frames at resolution of 830 480. In this context, we classify the stateof-the-art (SOTA) methods into two categories: trainingfree and tuning-based, based on whether they leverage spatial/temporal LoRA for optimizing complex motion patterns. (a) Time: We record the total time required for completing the motion transfer process, including any inference-time optimization. Leveraging proposed sliding-window motion extraction and step-skipping gradient optimization, FastVMT is the fastest method. Its runtime is faster than training-free methods, while delivering better performance. (b) Motion Fidelity: As in Yatim et al. [69], we use motion fidelity to assess the similarity of tracklets between reference and generated videos. (c) Temporal Consistency: We measure frame-to-frame coherence by calculating the average feature similarity of consecutive video frames using CLIP [42]. (d) Text Similarity: CLIP is used to extract features from the target video, and the average cosine similarity between the input prompt and video frames is computed. (f) User Study: To account for the limitations of automatic metrics in capturing real-world preferences, we conducted user study with 20 volunteers. They ranked methods based on motion preservation, appearance diversity, text alignment, and overall quality, using 1 (best) to 8 (worst) scale. Our 6 Figure 5. Illustration of attention motion flow extraction with sliding window. Without the sliding window, attention tokens are prone to incorrect correspondences (middle). Incorporating sliding window improves alignment, leading to better motion consistency (right). optimization steps. Leveraging this insight, we propose an interval-based gradient reuse strategy that selectively computes gradients while maintaining optimization effectiveness. Our step-skipping optimization operates with fixed interval during the inner optimization loop. For total of optimization steps, gradient computation occurs only when the current step satisfies the condition mod = 0, or when using the full AMF mode. The algorithm can be formalized as: (cid:40) xLtotal(xj) xj gcached if mod = 0 or mode = AMF otherwise Lj = (9) where gcached represents the gradient from the most recent computation step. This strategy reduces gradient computations from to approximately J/ per guidance step, achieving theoretical speedup of /J/ in the optimization phase. The cached gradient gcached is updated after each actual gradient computation: gcached = gj when mod = (10) This approach significantly reduces computational overhead while maintaining motion transfer quality, as the gradient similarity across consecutive steps ensures that cached gradients remain effective for optimization guidance. 4. Experiments 4.1. Implementation details In our experiment, we employ the open-sourced video generation model WAN-2.1 [51] as the base text-to-video generation model. The denoising steps are employed for 50 for all experiments. Unless stated, the output resolution is 480 832 with = 81 frames (internally rounded to 4k+1). Latents are initialized as Gaussian noise of shape (cid:1). Latent tiling is enabled with (cid:0)1, 16, 1 4 + 1, 8 , 8 Figure 6. Gallery of our method. Given reference video, our FastVMT is capable of generating high-quality video clips that faithfully preserve diverse motion patterns. method outperforms others in both automated metrics and user preferences. In addition, we collect 40 real-world videos and 40 highquality generated videos by advanced text-to-video generative models [16, 53]. For each video, we generate 5 different prompts. Four metrics in VBench [13] are employed for more accurate evaluation (in Tab. 1). (1) Subject Consistency: We assess whether the identity of the subject is preserved across frames. (2) Motion Smoothness: The metric evaluates inter-frame continuity using learned motion priors. (3) Aesthetic Quality uses LAION-trained aesthetic predictor to score visual appeal. (4) Background Consistency: We evaluate the coherence of the background. Our proposed method significantly outperforms all baseline approaches across every video quality metric, thereby showcasing the state-of-the-art performance in novel video. 4.2. Ablation study Effectiveness of sliding-window based motion extraction. As shown in Tab. 2 and Fig. 10, removing the sliding window mechanism results in performance degradation across multiple metrics and increased computational overhead and Figure 7. Illustration of token correspondence performance across different attention layers of DiT. We extract correspondence maps from multiple attention layers. The middle layers exhibit stronger matching quality. inference time. In Fig. 8, we present the visual results without sliding windows. It is clear to observe light reduction in motion fidelity and temporal consistency, confirming that our approach effectively balances computational efficiency with motion transfer quality. Additionally, we also show the visual comparison of attention motion extraction in various attention layers in DiT (see Fig. 7). The motion extraction is more accurate in the middle layer of DiT. 7 Table 1. Comparison with state-of-the-art video motion transfer methods. Red and Blue denote the best and second best results, respectively. Method Quantitative Metrics Text Sim. Motion Fid. Temp. Cons. Time (s) Vbench Metrics Sub. Cons. Back. Cons. Aes. Qual. Motion Smooth. MotionInversion [15] MotionDirector [85] DeT [45] MOFT [63] MotionClone [17] SMM [69] DiTFlow [35] Ours 0.2388 0.2336 0.2187 0.2297 0.2304 0.2374 0.2091 0.2422 0.6515 0.4524 0. 0.6511 0.7315 0.7353 0.4062 0.7471 Training-Based Methods 0.9605 0.9531 0.9818 632.41 806.64 2745.60 Training-Free Methods 0.9797 0.9722 0.9366 0.9822 0. 595.81 397.05 809.70 626.83 184.18 0.9339 0.9173 0.9787 0.9593 0.9601 0.8907 0.9557 0.9809 0.9372 0.9379 0.9654 0.9413 0.9545 0.9352 0.9678 0.9684 0.4062 0.3443 0. 0.4581 0.4615 0.5770 0.5310 0.5778 0.9532 0.9633 0.9598 0.9716 0.9616 0.9702 0.9801 0.9891 5. Conclusion In this work, we introduced FastVMT, training-free video motion transfer framework that explicitly addresses motion redundancy in diffusion transformer architectures and gradient redundancy along the diffusion trajectory. To eliminate the motion redundancy, we propose the sliding-window strategy associated with corresponding window loss to achieve more reliable and more efficient local search for motion correspondence. To migrate gradient redundancy, We introduce step-skipping gradient computation to ensure computational efficiency. By incorporating the proposed strategies, our method achieves 3.43 average speedup without compromising either visual fidelity or temporal consistency. We believe this line of work opens new opportunities for building more efficient and practical generative video motion transfer."
        },
        {
            "title": "Reproducibility Statement",
            "content": "All quantitative tables, qualitative images, and video results in this work are reproducible and correspond to raw model outputs without manual editing or post-hoc alteration, except for minimal format conversion and compression. After the review process, we will release partial public repository to support reproduction, including inference scripts, example data, and example videos. The datasets, configurations, and procedures used for training and evaluation are documented in Section 4.1. We will also provide fixed configuration files and random seeds so that independent runs can reproduce the visual results within expected stochastic variation."
        },
        {
            "title": "Ethics Statement",
            "content": "Our work studies motion-transfer video editing. The proposed dataset contains videos of people, vehicles, and landscape camera motions. To mitigate representational bias in demonstrations, we curated and display examples spanning different races, genders, and styles in the main text and appendix. All illustrative videos shown in this paper are Figure 8. Qualitative ablation of the proposed modules. The reference video is shown at the top-left. The prompt is white cat is running on the ground. Table 2. Quantitative ablation. Red and Blue denote best and second best. Method Text Sim. Motion Fid. Temp. Cons. Time(s) w/o Sliding Wind. w/o Cor. Loss w/o Step Skip. Ours 0.2352 0.2345 0.2317 0.2422 0.6912 0.5942 0.7044 0.7471 0.9654 0.9762 0. 0.9865 227 183 302 184 Effectiveness of corresponding-window loss. Tab. 2 and Fig. 10 reveal that excluding the corresponding-window loss leads to substantial degradation in motion fidelity, highlighting its essential role in maintaining accurate motion transfer. As shown in Fig. 8, equipping with this loss function effectively constrains temporal inconsistencies to ensure robust motion alignment, while introducing minimal computational overhead (less than 1% increase in processing time), thus preserving both accuracy and efficiency. Effectiveness of step skipping gradient upgrading. The step-skipping strategy significantly reduces computational time while preserving video generation quality. As demonstrated in Tab. 2 and Fig. 10, this optimization achieves substantial time savings with negligible impact on motion fidelity and temporal consistency, validating the effectiveness of gradient reuse in our framework. Figure 9. Qualitative comparison with baselines. We perform the visual comparison with various baselines using various kinds of motions. Our method obtains better performance in various motions."
        },
        {
            "title": "Acknowledgment",
            "content": "This research was supported by the Shanghai Science and Technology Program (Grant No. 25ZR1402278). Part of the compute is supported by the SwissAI Compute Grant a144 and a154."
        },
        {
            "title": "References",
            "content": "[1] Siran Chen, Yue Ma, Yu Qiao, and Yali Wang. M-bev: Masked bev perception for robust autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11831191, 2024. 3 [2] Yiyang Chen, Xuanhua He, Xiujun Ma, and Yue Ma. Contextflow: Training-free video object editing via adaptive context enrichment. arXiv preprint arXiv:2509.17818, 2025. 3 [3] Chengyu Fang, Chunming He, Fengyang Xiao, Yulun Zhang, Longxiang Tang, Yuelin Zhang, Kai Li, and Xiu Li. Realworld image dehazing with coherence-based pseudo labeling and cooperative unfolding network. Advances in Neural Information Processing Systems, 37:9785997883, 2024. 2 [4] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 29692977, 2025. 2 [5] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv preprint arXiv:2501.15891, 2025. 3 [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [7] Chunming He, Chengyu Fang, Yulun Zhang, Tian Ye, Kai Li, Longxiang Tang, Zhenhua Guo, Xiu Li, and Sina Farsiu. Reti-diff: Illumination degradation image restoration with retinex-based latent diffusion model, 2024. Figure 10. Quantitative ablation comparison on VBench metrics. We select seven metrics to evaluate the effectiveness of the proposed strategy. sourced from publicly available web content; we respect the original licenses and terms of service and use the content solely for research purposes. We will not publicly release the dataset prior to completing the insertion of AI-generated watermarks and an ethics/content-safety audit. We explicitly prohibit harmful or deceptive uses of our methods and data, including deepfake attacks and other malicious generative behaviors. When any portion of our code is made public, we will enforce visible and/or machine-detectable watermarking during inference to help deter misuse. Any future releases will be accompanied by usage terms that forbid impersonation, harassment, or other malicious applications, and we will remove or restrict content that raises privacy, legal, or safety concerns. 9 [8] Yin-Yin He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. In arXiv preprint arXiv:2211.13221, 2022. 3 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [10] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [11] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. 3 [12] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. 3 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2180721818, 2023. 7 [14] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similarity score distillation for zero-shot video editing. arXiv preprint arXiv:2403.12002, 2024. 2, 3 [15] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 92129221, 2024. 8 [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 7 [17] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 3, 6, 8 [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [19] Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, et al. Human motionformer: Transferring human motions with vision transformers. arXiv preprint arXiv:2302.11306, 2023. [20] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [21] Xvyuan Liu, Xiangfei Qiu, Hanyin Cheng, Xingjian Wu, Chenjuan Guo, Bin Yang, and Jilin Hu. Astgi: Adaptive spatio-temporal graph interactions for irregular multivariate time series forecasting. In ICLR, 2026. 2 [22] Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, and Bin Yang. Rethinking irregular time series forecasting: simple yet effective baseline. In AAAI, 2026. 2 [23] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: review on background, technology, limitations, and opportunities of large vision models. ArXiv, abs/2402.17177, 2024. 3 [24] Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, and Yue Ma. Follow-your-shape: Shape-aware image editing via trajectory-guided region control. arXiv preprint arXiv:2508.08134, 2025. 3 [25] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 2, [26] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, HeungYeung Shum, Wei Liu, et al. Follow-your-click: Open-domain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024. [27] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2 [28] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video generation: survey. arXiv preprint arXiv:2507.16869, 2025. 2 [29] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. 3 [30] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6018 6026, 2025. 3 [31] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. [32] Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, et al. Follow-your-emoji-faster: Towards efficient, fine-controllable, and expressive freestyle portrait animation. arXiv preprint arXiv:2509.16630, 2025. 2 [33] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, and Van Gool et al. benchmark dataset and evaluation method10 ology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 6 [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2 [34] Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, and Fabio Pizzati. Video motion transfer with diffusion transformers. arXiv preprint arXiv:2412.07776, 2024. 6 [35] Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, and Fabio Pizzati. Video motion transfer with diffusion transformers. In CVPR, 2025. 2, 3, 6, [36] Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, and Fabio Pizzati. Video motion transfer with diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2291122921, 2025. 2 [37] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, and Bin Yang. TFB: Towards comprehensive and fair benchmarking of time series forecasting methods. In Proc. VLDB Endow., pages 23632377, 2024. 2 [38] Xiangfei Qiu, Zhe Li, Wanghui Qiu, Shiyan Hu, Lekui Zhou, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Aoying Zhou, Zhenli Sheng, Jilin Hu, Christian S. Jensen, and Bin Yang. Tab: Unified benchmarking of time series anomaly detection methods. In Proc. VLDB Endow., pages 27752789, 2025. [39] Xiangfei Qiu, Xingjian Wu, Hanyin Cheng, Xvyuan Liu, Chenjuan Guo, Jilin Hu, and Bin Yang. Dbloss: Decomposition-based loss function for time series forecasting. In NeurIPS, 2025. [40] Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, and Bin Yang. DUET: Dual clustering enhanced multivariate time series forecasting. In SIGKDD, pages 11851196, 2025. [41] Xiangfei Qiu, Yuhan Zhu, Zhengyu Li, Hanyin Cheng, Xingjian Wu, Chenjuan Guo, Bin Yang, and Jilin Hu. Dag: dual causal network for time series forecasting with exogenous variables. arXiv preprint arXiv:2509.14933, 2025. 2 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [43] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customizea-video: One-shot motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 332349. Springer, 2024. [44] Runway. Gen-1: Structure and content-guided video synthesis with diffusion models. https://runwayml.com/ research/gen-1, 2023. 3 [45] Qingyu Shi, Jianzong Wu, Jinbin Bai, Jiangning Zhang, Lu Qi, Xiangtai Li, and Yunhai Tong. Decouple and track: Benchmarking and improving video diffusion transformers for motion transfer. arXiv preprint arXiv:2503.17350, 2025. 2, 6, 8 [47] Yiren Song, Xiaokang Liu, and Mike Zheng Shou. Diffsim: Taming diffusion models for evaluating visual similarity. arXiv preprint arXiv:2412.14580, 2024. 3 [48] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. 3 [49] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: for multiHarnessing diffusion transformers domain procedural sequence generation. arXiv preprint arXiv:2502.01572, 2025. 3 [50] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. 3 [51] Team Wan, Ang Wang, and et.al. Wan: Open and advanced large-scale video generative models, 2025. 6 [52] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4 [53] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningying Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Rui Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wen-Chao Zhou, Wente Wang, Wen Shen, Wenyuan Yu, Xianzhong Shi, Xiaomin Huang, Xin Xu, Yan Kou, Yan-Mei Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhengbin Han, Zhigang Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. ArXiv, abs/2503.20314, 2025. 7 [54] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [55] Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Yingcong Chen. Motion inversion for video customization. arXiv preprint arXiv:2403.20193, 2024. 6 11 [56] Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, and Huchuan Lu. Stableidentity: Inserting anybody into anywhere at first sight. IEEE Transactions on Multimedia, 2025. [57] Qinghe Wang, Baolu Li, Xiaomin Li, Bing Cao, Liqian Ma, Huchuan Lu, and Xu Jia. Characterfactory: Sampling consistent characters with gans for diffusion models. IEEE Transactions on Image Processing, 2025. [58] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. [59] Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, and Xu Jia. Multishotmaster: controllable multi-shot video generation framework. arXiv preprint arXiv:2512.03041, 2025. 3 [60] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Proceedings, 2024. 3 [61] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. NeurIPS, 2024. 3 [62] Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, and Chenjuan Guo. K2VAE: koopman-kalman enhanced variational autoencoder for probabilistic time series forecasting. In ICML, 2025. [63] Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 2, 3, 6, 8 [64] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textual and structural guidance. IEEE Transactions on Visualization and Computer Graphics, 2024. 3 [65] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. 3 [66] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. Dgl: Dynamic global-local prompt tuning for text-video retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 65406548, 2024. 3 [67] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. 3 [68] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84668476, 2024. [69] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84668476, 2024. 2, 3, 6, 8 [70] Hidir Yesiltepe, Tuna Han Salih Meral, Connor Dunlop, and Pinar Yanardag. Motionshop: Zero-shot motion transfer in video diffusion models with mixture of score guidance. arXiv preprint arXiv:2412.05355, 2024. 3 [71] Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, and Keze Wang. MAT-agent: Adaptive multi-agent training optimization. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3 [72] Jusheng Zhang, Kaitong Cai, Yijia Fan, Jian Wang, and Keze Wang. Cf-vlm:counterfactual vision-language fine-tuning, 2025. 3 [73] Jusheng Zhang, Kaitong Cai, Xiaoyang Guo, Sidi Liu, Qinhan Lv, Ruiqi Chen, Jing Yang, Yijia Fan, Xiaofei Sun, Jian Wang, Ziliang Chen, Liang Lin, and Keze Wang. Mm-cot:a benchmark for probing visual chain-of-thought reasoning in multimodal models, 2025. 3 [74] Jusheng Zhang, Kaitong Cai, Jing Yang, and Keze Wang. Learning dynamics of vlm finetuning, 2025. [75] Jusheng Zhang, Yijia Fan, Kaitong Cai, Zimeng Huang, Xiaofei Sun, Jian Wang, Chengpei Tang, and Keze Wang. Drdiff: Dynamic routing diffusion with hierarchical attention for breaking the efficiency-quality trade-off, 2025. [76] Jusheng Zhang, Yijia Fan, Kaitong Cai, and Keze Wang. Kolmogorov-arnold fourier networks, 2025. 3 [77] Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, Wenhao Chai, Jian Wang, and Keze Wang. GAMagent: Game-theoretic and uncertainty-aware collaboration for complex visual reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3 [78] Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, and Keze Wang. Tri-MARF: tri-modal multi-agent responsive framework for comprehensive 3d object annotation. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025. [79] Jusheng Zhang, Xiaoyang Guo, Kaitong Cai, Qinhan Lv, Yijia Fan, Wenhao Chai, Jian Wang, and Keze Wang. Hybridtokenvlm: Hybrid token compression for vision-language models, 2025. 3 [80] Jusheng Zhang, Zimeng Huang, Yijia Fan, Ningyuan Liu, Mingyan Li, Zhuojie Yang, Jiawei Yao, Jian Wang, and Keze Wang. KABB: Knowledge-aware bayesian bandits for dynamic expert coordination in multi-agent systems. In Fortysecond International Conference on Machine Learning, 2025. 3 [81] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Mo12 tioncrafter: One-shot motion customization of diffusion models. arXiv preprint arXiv:2312.05288, 2023. 2 [82] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. [83] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 3 [84] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. arXiv preprint arXiv:2305.17098, 2023. 3 [85] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465, 2023. 2, 3, 6,"
        }
    ],
    "affiliations": [
        "EPIC Lab, SJTU",
        "ETH Zurich",
        "HKUST",
        "Meta",
        "THU"
    ]
}