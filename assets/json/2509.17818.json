{
    "paper_title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
    "authors": [
        "Yiyang Chen",
        "Xuanhua He",
        "Xiujun Ma",
        "Yue Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 8 1 8 7 1 . 9 0 5 2 : r ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment Yiyang Chen1, Xuanhua He2,, Xiujun Ma1,, Yue Ma2,, 1 State Key Laboratory of General Artifical Intelligence, Peking University, Beijing, China 2 The Hong Kong University of Science and Technology Project Page: https://yychen233.github.io/ContextFlow-page Figure 1. Showcase of ContextFlow. Our ContextFlow achieves versatile and high-fidelity video object editing without any training. Our method demonstrates superior ability in range of object-related challenging tasks, including object insertion (1st row), swapping (2nd row), and deletion (3rd row). The core design of our approach is Adaptive Context Enrichment, which allows for seamless integration of new elements with realistic interactions and meticulous preservation of the original scenes."
        },
        {
            "title": "Abstract",
            "content": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude hard feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, novel training-free framework for DiT-based video object editing. In detail, we first employ high-order Rectified Flow solver to establish robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), mechanism that addresses contextual conflicts. Instead of replacing feaCorresponding Authors. tures, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose systematic, data-driven analysis to identify task-specific vital layers. Based on novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-ofthe-art training-based approaches, delivering temporally coherent, high-fidelity results. 1. Introduction Video object editing aims to achieve range of objectrelated challenging editing tasks, including object insertion, swapping, and deletion. Unlike original video editing, video object editing requires the model to meticulously preserve the unmodified background while seamlessly integrating the edited object into the videos original motion and context. This is task that demands high spatial and temporal consistency. Currently, there are two primary technical paths in the research community: training-based and training-free methods. (1) Training-based methods aim to build powerful and feed-forward models. Recent examples include video propagation-based models like I2V Edit [38], GenProp [22] and ReVideo [36], as well as other architectures such as VideoAnyDoor [54], GetIn [25], VACE [4] and UNIC [65], which achieve impressive results on respective benchmarks. However, these training-based methods are limited by the prohibitive computational costs and the demand for expensive large-scale datasets. (2) Compared with the trainingbased methods, the training-free method offers more flexible and cost-effective alternative. Early works, like AnyV2V [18], leverage the vast knowledge embedded in pre-trained foundation models, obviating the need for any task-specific fine-tuning. This paradigm typically relies on foundational workflow: first, inverting the source video to noise latent using DDIM Inversion [49], and then guiding the new generation via Plug-and-Play (PnP) feature injection [10]. In this process, key internal features from reconstruction of the source video are injected into the generation process of the edited video to enforce structural consistency. However, this established workflow faces limitations. It often struggles with fidelity, leading to artifacts, inconsistent object identity, and difficulty in preserving the original background. These issues are further amplified by the recent architectural shift from U-Nets to Diffusion Transformers (DiTs), as traditional guidance mechanisms are illsuited for this new class of models. We provide detailed analysis in Section 3.1. To address these critical limitations, we propose ContextFlow, novel training-free framework that significantly advances the editing process for DiT-based models. Instead of relying on lossy inversion and crude feature replacement, ContextFlow establishes high-fidelity, highly reversible foundation for editing based on RF-Solver [58]. At its core is novel Adaptive Context Enrichment mechanism, which empowers the model to dynamically fuse information from the original video and the desired edit on per-token basis. This dynamic approach addresses the conflict between content preservation and synthesis. To apply this guidance with efficiency and precision, we propose systematic, data-driven Vital Layer Analysis to identify the most crucial intervention points within the DiT architecture. Our contributions can be summarized as follows: We propose ContextFlow, novel training-free framework that is the first to apply Rectified Flow inversion to video object editing. This establishes high-fidelity Figure 2. Motivation for ContextFlow. We highlight two core failures of prior methods: DDIM inversion causes poor reconstruction results, while hard replacement leads to misaligned attentions that only focus on the original background. ContextFlow systematically solves both. and nearly reversible foundation that significantly reduces editing artifacts. We design an adaptive context enrichment mechanism that addresses the contextual conflict of feature replacement. By concatenating Key-Value pairs, our method provides soft guidance that effectively balances the tradeoff between edit fidelity and content preservation. We introduce systematic and data-driven Vital Layer Analysis to identify the most crucial blocks for context injection in DiTs. This replaces the heuristic-based layer selection of U-Net frameworks and enables targeted, efficient guidance. Extensive experiments on diverse editing tasks, including object insertion, deletion, and swapping, demonstrate that ContextFlow significantly outperforms existing trainingfree approaches and even surpasses several state-of-theart training-based methods. 2. Related Work 2.1. Diffusion-based Video Editing The landscape of video editing has been reshaped by diffusion models. Current research in this area is largely bifurcated into tuning-based and training-free methods. Tuningbased methods adapt pre-trained models for specific videos or subjects, exemplified by per-video optimization [62] and subject-driven personalization [8, 12, 13, 21, 27, 35, 46, 61, 63, 64, 66, 68, 69]. Other approaches focus on decoupling content and motion control during training to offer more flexible and precise editing capabilities [3, 2629, 31, 33, 36, 38, 39, 67]. While powerful, these methods often incur significant computational costs. Training-free methods aim for zero-shot video editing without model training. Early efforts focused on preserving video structure by manipulating cross-frame attention maps [7, 9, 32, 34, 42, 57]. significant recent trend, however, has shifted towards propagating modified first frame by leveraging the strong generative priors of pretrained Image-to-Video (I2V) models. Seminal works in this direction, such as AnyV2V [18], have demonstrated impressive consistency and quality. This paradigm has been further generalized in frameworks like 2 Figure 3. Overview of the ContextFlow. Our method begins with high-fidelity video inversion using RF-Solver to obtain shared noise latent zT . dual-path sampling process then decouples reconstruction and editing. The editing path is guided by our core mechanism, Adaptive Context Enrichment, where Key-Value pairs from the reconstruction path are concatenated into the self-attention blocks of the editing path. This guidance is precisely targeted to vital layers, identified via our Guidance Responsiveness analysis, and is only active during the first half of the denoising process to balance fidelity and consistency. UniEdit [1] and GenProp [22]. In parallel, other works [2, 5, 8, 15, 16, 24, 30, 56] have focused on customizing the diffusion process itself to enhance motion control and appearance preservation. object-related video editing tasks, constructing trainingfree framework based on the latest DiT model to simultaneously support multiple functions such as object insertion, object swapping, and object deletion. 2.2. Reference-Guided Object Editing 3. Method critical limitation of text-only guidance is its inability to specify the unique visual identity of particular real-world object. This has spurred the development of referenceguided editing, which uses an image to define the targets appearance. common principle is establishing featurelevel correspondence to guide attention and ensure faithful appearance propagation. Techniques like Zero-to-Hero [51] establish robust feature-level connections between the reference image and video frames, guiding attention mechanisms to ensure faithful appearance transfer. This principle of using correspondence to guide propagation is shared by variety of recent works that aim for consistent referencebased editing, including VideoSwap [11], AnyV2V [18], and I2VEdit [38]. more formidable challenge is to seamlessly insert new object from reference image into an existing video. Training-free methods such as InVi [47], GetInVideo [70], and iDiT-HOI [48] address demands that include not only identity preservation and temporal coherence but also plausible interaction with the scene. These editing-focused methods differ from reference-guided generation frameworks like VideoAnydoor [54] and MotionCtrl [60], which synthesize entirely new videos from scratch based on reference object and motion prompts, rather than modifying pre-existing footage. Our approach aims to unify Given reference video, we aim to edit objects in the video, including the object insertion, swapping, and deletion. In the following section, we first analyze the core challenges in training free video object editing and provide our motivation in Sec. 3.1. Then the overview of the proposed ContextFlow is presented in Sec. 3.2. We introduce high-fidelity inversion, adaptive context enrichment and Vital layer analysis in Sec. 3.3, Sec. 3.4 and Sec. 3.5. 3.1. Core Challenges and Motivations 3.1.1. The Challenge of Video Inversion for Editing For training-free editing, critical first step is to invert real video back to its corresponding noise latent z1. This process should ideally be perfectly reversible, creating an unambiguous anchor that encodes all spatiotemporal information of the source video. This is typically modeled as solving an Ordinary Differential Equation (ODE) that describes the path from noise to data. However, in practice, this ODE is solved numerically. Standard techniques like DDIM Inversion, which rely on first-order solvers similar to the Euler method, discretize the generation path from z1 to zv into steps: zti1 = zti + (ti1 ti)vθ (zti, t) (1) 3 where ti goes from 1 to 0. Naively reversing this process for inversion introduces significant discretization errors, which accumulate over timesteps. This results in noisy latent that cannot faithfully reconstruct the original video, as shown in Figure 2, severely compromising the quality and consistency of any subsequent edits. 3.1.2. Contextual Conflict in Guidance The conventional PnP mechanism, which involves hard replacement of features, is often too crude. This rigid intervention can create conflict between the source videos structure and the edited object, leading to visual artifacts and inconsistent object identity. For example, queries from the editing path, seeking to form new concept, are forced to attend to keys from the original video. This mismatch confuses the attention mechanism, leading to suppressed edits or artifacts. As visualized in Figure 2, hard replacement creates semantic conflict, causing the editrelated queries to erroneously attend to keys and values from the original, irrelevant video context. This issue is compounded by the move to homogeneous Diffusion Transformers, whose lack of distinct semantic layersunlike hierarchical U-Netsmakes it unclear where and how to effectively inject guidance. In response to these three core challenges, we propose ContextFlow, novel framework designed specifically for high-fidelity, DiT-based video object editing. 3.2. Overview of ContextFlow Our proposed framework, ContextFlow, addresses this by designing controlled generation process within pretrained I2V Diffusion Transformer, all without any weight modification. As illustrated in Figure 3, our approach is built upon three foundational parts. First, to solve the inversion fidelity problem, we establish near-lossless and highly reversible foundation using Rectified Flow, creating clean canvas for editing. Next, to address the contextual conflict inherent in feature replacement, we introduce an Adaptive Context Enrichment mechanism. Instead of crude hard replacement, this method enriches the context by concatenating the Key-Value (KV) pairs from both the source videos reconstruction and the editing path, empowering the DiTs self-attention to dynamically balance preservation and synthesis. Finally, to answer the critical question of where to apply this guidance, our data-driven Vital Layer Analysis systematically identifies the most crucial layers for intervention. This avoids the drawbacks of naive all-layer injection and replaces unreliable heuristics, ensuring both precision and efficiency. Together, these components enable robust, high-fidelity video editing in training-free manner. 3.3. High-Fidelity Inversion via Rectified Flow Our editing workflow begins with the source video Vsrc and target prompt stgt. Using an off-the-shelf image editor (e.g., AnyDoor [6]), we first modify the initial frame Vsrc[0] to create the edited frame Iedit. The primary challenge is then to propagate the static edit in Iedit throughout the video, guided by stgt, while maintaining fidelity to Vsrc. As established in our preliminaries, the success of this editing hinges on the quality of the initial noise latent. Standard inversion methods like DDIM are lossy, making it difficult to disentangle editing artifacts from inversion errors. To eliminate this ambiguity, higher-order numerical solver is essential. Therefore, we introduce RF-Solver [58], training-free, second-order sampler that provides the highly reversible and high-fidelity mapping required for robust generative foundation. RF-Solver achieves its precision by utilizing secondorder Taylor expansion to more accurately estimate the ODE path during inversion. 1 2 (ti+1ti)2v(1) zti+1 = zti+(ti+1ti)vθ(zti, ti)+ θ (zti , ti) (2) where v(1) is the numerically estimated time derivative of the velocity field. We apply this to the VAE-encoded latents of the source video, conditioned on its original first frame Vsrc[0] and null-text prompt ssrc = ϕ, to obtain unique noise anchor z1: θ z1 = RF-Solverinversion(Vsrc, Vsrc[0], ssrc) (3) The resulting anchor z1 provides faithful representation of the original videos spatiotemporal information, establishing robust foundation for our editing mechanism 3.4. Adaptive Context Enrichment for DiT-based"
        },
        {
            "title": "Guidance",
            "content": "With reliable noise anchor z1 established, our core challenge becomes propagating the edit from the triplet (z1, Iedit, stgt) while preserving the original videos structure. naive, single-path generation, denoising from z1 using only the new conditions (Iedit, stgt) is insufficient as it lacks continuous guidance from the source video, leading to content drift. While prior methods use feature injection from the original source video, their reliance on hard replacement creates fundamental problem in DiTs. 3.4.1. The Contextual Conflicts of Hard Replacement We identify the core limitation of naive feature injection as contextual conflict. This occurs when edit-specific queries (Qedit), which seek to form new semantic concept (e.g., Pikachu is floating on the sea.), are forced to attend to context (K res, res) from the original video that only contains information about the original scene (e.g., the sea surface). As illustrated in Figure 4, this semantic mismatch confuses the attention mechanism, leading to suppressed edits or artifacts. This necessitates more intelligent fusion strategy. 4 Figure 4. Resolving Contextual Conflict. Hard replacement misdirects attention for edited queries, suppressing object synthesis. Our Adaptive Context Enrichment resolves this by offering dual context: the Editing Path for synthesizing the new object, and the Reconstruction Path for preserving background structure. Attention in unedited regions remains correct, confirming our method is non-invasive. 3.4.2. Adaptive Fusion via Context Enrichment Our solution is fundamentally different: adaptive context enrichment. Instead of replacing the context, we enrich it, empowering the pre-trained attention module to perform dynamic, content-aware fusion. To implement this, we access both editing and reconstruction contexts simultaneously via synchronized dual-path process. Both paths originate from the same noise anchor z1. The first, our Reconstruction Path, is conditioned on the original video inputs (Vsrc[0], sϕ). It focuses on preserving content fidelity by denoising z1 back to the source video, providing the essential source context (keys res t,l ). In parallel, the Editing Path handles the creative task. Conditioned on the edit inputs (Iedit, stgt), it synthesizes the desired changes from the same z1, providing the editing queries Qedit and its own internal context (K edit With both sets of contexts available, we perform the enrichment within the Editing Paths self-attention. We augment the key and value by concatenating them with their counterparts from the Reconstruction Path: t,l and values res , edit t,l ). t,l t,l Kaug = Concat([K edit t,l Vaug = Concat([V edit t,l , res , res t,l t,l ]) ]) (4) (5) The self-attention is then computed using this expanded context: Self-Attnenriched = softmax (cid:32) Qedit t,l (Kaug)T (cid:33) Vaug (6) Figure 5. Task-Dependent Guidance Responsiveness (min-max normalized data in the figure). higher Guidance Responsiveness indicates greater influence. There are three primary zones across all layers, which distribute in the shallow area (layers 1-10), midlayer area (layers 15-21) and deep area (layers 26-32) respectively. Moreover, the numerical ranking of Guidance Responsiveness for these three regions varies depending on the specific task. 3.5. Targeted Fusion via Vital Layer Analysis Having established how to guide the generation, we now address the critical question of where. Injecting guidance uniformly across all layers of DiT is not only computationally wasteful but also conceptually flawed. Applying our semantic-level guidance uniformly across all layers risks disrupting the DiTs functional hierarchy of layers, potentially weakening the edits results. Therefore, targeted intervention is required. Prior works on U-Net architectures have relied on empirical heuristics for layer selection. However, such heuristics are not reliably transferable to the different and more homogeneous structure of DiTs. To formalize this, we propose data-driven method to identify the most influential layers. We define Guidance Responsiveness Metric, GRl, that quantifies layers responsiveness to our Contextual Enrichment mechanism. For each layer l, we calculate this by performing onestep denoising on set of videos with the editing conditions and computing two feature maps for each layer: xno-CE (i.e., layer ls self-attention output without Contextual Enrichment) and xCE (i.e., layer ls self-attention output with Contextual Enrichment applied only at layer l). The Guidance Responsiveness is measured by the dissimilarity: GRl = 1 mean (cid:0)cosine similarity (cid:0)xno-CE (cid:1)(cid:1) , xCE (7) This design leverages the inherent optimization behavior of self-attention. The enriched key-value space allows each query to attend to its most relevant informationbe it from the source context for background preservation, or the edit context for new content synthesis. This transforms guidance from rigid command into dynamically weighted fusion process, enabling robust and high-fidelity fusion. high GRl score signifies that the layer is highly sensitive to the guidance and thus influential in the editing process. Applying this analysis across range of editing tasks reveals that layer responsiveness is not uniform, but instead exhibits highly structured and task-dependent pattern, as shown in Figure 5. There are three primary zones of high responsiveness across the models depth: an early 5 Task Method Identity Alignment CLIP-I DINO-I CLIP-Score Overall cons. Video Quality Smoothness Dynamic Aesthetic Reconstruction Quality PSNR SSIM e p e e AnyV2V VACE AnyV2V-DiT I2VEdit Ours AnyV2V VACE AnyV2V-DiT I2VEdit Ours AnyV2V VACE AnyV2V-DiT I2VEdit Ours 0.5943 0.5683 0.6376 0.6710 0.6504 0.6046 0.6080 0.6617 0.6683 0.6644 0.4053 0.3967 0.4479 0.4595 0.4566 0.5641 0.5917 0.5983 0.6003 0.6004 0.2776 0.2569 0.3060 0.3124 0.3107 0.3210 0.3226 0.3362 0.3282 0.3391 0.2891 0.2794 0.2863 0.2790 0.2854 0.1887 0.1386 0.2579 0.2600 0.2691 0.2384 0.2412 0.2597 0.2595 0.2648 0.2170 0.1948 0.2136 0.2081 0. 0.9804 0.9921 0.9917 0.9827 0.9918 0.9848 0.9926 0.9907 0.9819 0.9924 0.9781 0.9889 0.9886 0.9816 0.9900 0.3077 0.3077 0.3846 0.3077 0.4231 0.0769 0.1538 0.1538 0.0769 0.0769 0.1500 0.3000 0.3000 0.3000 0. 0.5287 0.5724 0.6145 0.5846 0.6227 0.5739 0.6144 0.6076 0.5995 0.6176 0.5378 0.5645 0.5413 0.5169 0.5405 20.57 18.86 26.06 26.23 26.26 21.88 29.63 20.18 26.19 22.66 22.07 31.57 21.14 25.44 22. 0.7055 0.9033 0.8478 0.8360 0.8575 0.6867 0.9238 0.6854 0.7966 0.7518 0.6530 0.9064 0.6514 0.7758 0.7030 Table 1. Quantitative comparison on object insertion, swap, and deletion tasks. Our method has achieved impressive performance across numerous metrics for each task, demonstrating its comprehensiveness. block (layers 1-10), middle block (layers 15-21), and deep block (layers 26-32). Crucially, the dominant zone of activity varies systematically with the task. For object insertion, peak responsiveness is consistently located in the early-layer block. In contrast, object swapping elicits the strongest response in the deep-layer block. Object deletion presents unique dual-peak pattern, showing high responsiveness in both the middle and deep blocks. These empirical evidence strongly suggests structural pattern within the DiTs layers. The observed patterns are consistent with the broader understanding of Transformer architectures, where early layers typically handle spatial and structural information, while deeper layers manage more abstract semantic concepts [44, 53]. For instance, the reliance of insertion on early layers aligns with need to establish spatial layout, while deletions dependence on deep layers corresponds to high-level semantic operation. By selecting only the top-k layers with the highest importance for each task, we ensure our intervention is potent, targeted, and computationally efficient. This principled selection strategy completes our framework, delivering robust and precise video editing solution. 4. Experiments 4.1. Experimental Setup Implementation Details. Our framework is built upon Wan2.1-I2V-14B-480P [55], publicly available image-tovideo Diffusion Transformer with 40 layers. We adhere to training-free paradigm, requiring no optimization or finetuning of the pre-trained diffusion model. For inversion, we utilize RF-Solver [58] with 50 steps to map the source video into noise latent, and the subsequent editing also uses 50 sampling steps. We set the timestep threshold τ to 0.5 (i.e., the mechanism is adopted for the first 50% of timesteps) and guidance scale of 3.0. Evaluation Dataset and Baselines. We evaluate our method on the Unic-Benchmark [65]. For first-frame editing, we use AnyDoor [6] for insertion, InsertAnything [50] for swapping, and MagicQuill [23] for deletion. We compare against baselines including training-free AnyV2V [18], our adapted AnyV2V-DiT, and training-based VACE [4] and I2VEdit [38]. For evaluation, we measure task-specific fidelity (CLIP-score, DINO-score), background preservation (PSNR, SSIM), and overall video quality using metrics from VBench [14] (e.g., consistency, smoothness, dynamic and aesthetic quality). 4.2. Comparison with baselines We conduct comprehensive quantitative evaluation against state-of-the-art methods on object insertion, swapping, and deletion tasks. The results, summarized in Table 1 and Figure 6, demonstrate the performance of ContextFlow. For creative edits like object insertion and swapping, ContextFlow outperforms most baselines in critical Identity and Alignment metrics. Its leading Aesthetic and Dynamic scores also reflect high visual fidelity and temporal coherence. In contrast, while I2VEdit achieves higher Identity scores, it requires per-video training and produces stiff, copy-paste-like results(Figure 6). This effect is not what we desire, even though it has achieved higher value in alignment metrics. Other methods show more significant flaws: VACE fails on out-of-distribution insertion and exhibits poor identity matching in swaps, while AnyV2V yields blurry visuals and unstable objects. 6 Figure 6. Qualitative comparisons of our method against open-source video editing baselines. The guiding text prompt is shown below the original videos. Our method demonstrates satisfactory results. Meanwhile, other methods either struggle to achieve the intended goals of insertion, replacement, or deletion, fail to assign proper motion patterns to the modified objects, or generate strange artifacts or lowquality videos. Zoom in for better visualization. In the object deletion task, ContextFlow achieves top scores in Smoothness and Dynamics while maintaining high reconstruction quality. In contrast, VACE incorrectly replaces objects with hallucinated content, while AnyV2V leaves shadow artifacts and shows poor reconstruction quality (Figure 6). The AnyV2V-DiT baseline shows that directly applying traditional U-Net guidance mechanisms to the DiT architecture causes severe visual artifacts, including geometric distortions, inconsistent temporal dynamics, and spatial deformations. This result provides empirical evidence that the architectural transition to Transformers requires fundamentally new guidance mechanisms. 4.3. Ablation Studies To rigorously analyze the contributions of our proposed components, we conduct series of ablation studies on the object insertion task. Our analysis is structured to answer three fundamental questions regarding our ContextFlow framework: 1) Is our guidance mechanism effective and how should it be implemented? 2) How much guidance is optimal and where should it be injected? 3) During which phase of the denoising process should guidance be active? 4.3.1. Core Mechanism Validation: Guidance Strategy We first validate the necessity and design of our adaptive Context Enrichment (CE) mechanism. We compare our method against two critical variants: one that omits Contextual Enrichment entirely, relying solely on the inverted noise and edited first frame, and another that substitutes our K/V concatenation with conventional hard replacement strategy. As presented in Table 2, ablating the CE module (w/o Contextual Enrichment) leads to discernible decline across all metrics. While high-fidelity inversion provides strong foundation, explicit guidance during the denoising path is crucial for context-aware editing. More revealingly, the K/V Replacement strategy significantly impairs Identity Preservation scores. We attribute this to the destructive nature of replacement, which discards valuable contextual information from the editing paths context. In contrast, our concatenation approach is additive; it enriches the context, empowering the self-attention module to dynamically balance between source and target contexts. Method Ours CLIP-I DINO-I CLIP-Score Overall cons. Aesthetic 0.6504 0.4566 0. 0.2691 0.6227 Ablation on Guidance Strategy w/o CE. K/V Replace 0.6447 0.6349 0.4529 0. 0.3086 0.3018 0.2634 0.2544 0.6186 0.6200 Table 2. Ablation on the core guidance strategy. Our method demonstrates clear superiority over both the absence of guidance and destructive replacement approach, highlighting the critical importance of our carefully designed strategic integration. 7 Method Ours CLIP-I DINO-I CLIP-Score Overall cons. Aesthetic 0.6504 0.4566 0.3107 0. 0.6227 Ablation on Layer Selection Strategy Injection on All layers Inj. on 4 least-responsive Inj. selection follow AnyV2V 0.5715 0.6138 0.6458 0.4067 0.4367 0.4553 0.2685 0.2970 0. 0.1863 0.2365 0.2634 0.5863 0.6059 0.6210 Table 4. Ablation on the layer selection strategy. Our principled selection consistently outperforms alternative approaches, clearly underscoring the notable effectiveness of our proposed Guidance Responsiveness metric and its strong correlation with performance. trade-off between structural preservation (favoring higher τ ) and edit flexibility (favoring lower τ ). As evaluated in Table 5, low τ = 0.2 offers insufficient guidance, while high τ = 1.0, which applies guidance throughout the entire process, slightly compromises aesthetic quality by restricting the model during the final, high-fidelity refinement stages. We identify τ = 0.5 as the optimal equilibrium. Method CLIP-I DINO-I CLIP-Score Overall cons. Aesthetic τ = 0.2 Ours(τ = 0.5) τ = 1.0 0.6465 0.6504 0.6482 0.4547 0.4566 0.4568 0.3098 0.3107 0.3115 0.2600 0.2691 0.2713 0.6200 0.6227 0. Table 5. Ablation on the injection timestep τ . Experimental results show that value of τ = 0.5 strikes the best balance between high edit fidelity and strong structural coherence. 5. Conclusion We present ContextFlow, training-free framework specifically designed for addressing the key challenges of video object editing, including preserving temporal continuity and maintaining object-background consistency. Our core contribution, Adaptive Context Enrichment, strategically injects controlled structural guidance that is extracted and refined from dedicated parallel reconstruction path. This injection process relies on custom-built Key-Value concatenation mechanism, which ensures smooth information integration. By precisely targeting vital layers that are identified through in-depth Guidance Responsiveness analysis, our method effectively balances edit fidelity with background stability. Experiments conducted across diverse test scenarios confirm the effectiveness of our approach in creating high-quality, temporally consistent video object edits, ultimately empowering users with intuitive fine-grained creative control over the editing process. 4.3.2. Targeted Guidance Analysis: How Much and Where? Having established the efficacy of our mechanism, we now investigate the specifics of its application: determining the optimal quantity and location for K/V injection. How Much Guidance? We first analyze the impact of k, the number of top-ranked layers selected for injection. As detailed in Table 3, the results reveal distinct unimodal performance curve. Insufficient guidance (k < 4) fails to provide robust structural anchor, leading to weaker identity preservation. Conversely, excessive guidance (k > 4) over-constrains the model, stifling its generative capacity and causing the desired edit to diminish, as evidenced by the sharp performance degradation for = 32 and = 40. An optimal balance is achieved at = 4, which corresponds to the top 10% of layers in the 40-layer DiT. Method CLIP-I DINO-I CLIP-Score Overall cons. Aesthetic = 0 = 1 = 2 Ours(k = 4) = 8 = 16 = 32 = 40 0.6447 0.6467 0.6452 0.6504 0.6456 0.6330 0.6100 0. 0.4529 0.4537 0.4530 0.4566 0.4541 0.4435 0.4303 0.4067 0.3086 0.3087 0.3077 0.3107 0.3089 0.3058 0.2959 0.2685 0.2634 0.2645 0.2623 0.2691 0.2620 0.2566 0.2458 0.1863 0.6186 0.6196 0.6188 0.6227 0.6240 0.6157 0.5998 0.5863 Table 3. Ablation on the number of injected layers. As shown in our experiments, performance peaks at k=4, which serves as an optimal trade-off between providing sufficient guidance strength without overly constraining and ensuring proper generative freedom. Where to Inject? With the optimal quantity established as = 4, we now validate our principled method for selecting which four layers to target. In Table 4, we benchmark our Guidance Responsiveness-based selection against several alternatives. Injecting into all 40 layers proves detrimental, corroborating our earlier finding that over-constraining the model is counterproductive. Conversely, injecting into the four least-responsive layers provides insufficient structural cues, failing to anchor the edit effectively. Finally, U-Net-based heuristic adapted from AnyV2V (layers 3-10) performs commendably but is still surpassed by our method on key identity metrics. This demonstrates that our data-driven Guidance Responsiveness metric is not merely theoretical construct but practical and superior tool for identifying the most impactful layers for precise video editing. 4.3.3. Guidance Window Analysis: When to Inject? Finally, we analyze the injection timestep threshold τ , which governs the temporal duration of the Contextual Enrichment mechanism. This parameter critically mediates the 8 ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary document provides additional details, experiments, and visualizations to complement our main paper. The contents are organized as follows: Implementation and Evaluation Details. Expanding on the main text, we provide comprehensive overview of our experimental setup, computational cost, evaluation protocol specifics, and deeper dive into our Guidance Responsiveness analysis. Additional Qualitative Comparisons. We present more visual comparisons against some state-of-the-art video editing techniques methods, specifically Unic [65] and Pika [40], to further highlight the superiority of ContextFlow. Visual Analysis for Ablation Studies. We provide visual evidence to support the quantitative results of our ablation studies presented in the main paper. Limitations and Future Work. We discuss the current limitations of our method and suggest potential directions for future research. 6. Implementation and Evaluation Details 6.1. Experimental Setup and Computational Cost This section expands on the implementation details mentioned in the main paper. Our framework is implemented in Python using PyTorch. All experiments were conducted on server with two NVIDIA A800 (80GB) GPUs. We build our training-free framework upon the publicly available Wan2.1-I2V-14B-480P [55] model. The dual-path generation process, which is central to our Adaptive Context Enrichment mechanism, requires approximately 120GB of VRAM in total ( 60GB per GPU). For typical 81-frame video at 480p resolution, the dual-path generation process takes approximately 25 minutes. As our method propagates an edit from the first frame, we utilize off-the-shelf image editing tools to generate the initial edited frame Iedit. For the experiments presented, we use AnyDoor [6] for object insertion, InsertAnything [50] for object swapping, and MagicQuill [23] for object deletion. 6.2. Further Details on Guidance Responsiveness"
        },
        {
            "title": "Analysis",
            "content": "Expanding on the Guidance Responsiveness analysis presented in Figure 5 of the main paper, we offer details on its robustness. The GR curves were derived by averaging results from diverse set of 10 videos for each task, including in-domain videos from the Unic-Benchmark [65] and outof-domain videos from sources like the DAVIS dataset [41]. 6.3. Evaluation Protocol Details We provide further clarification on the metrics used for quantitative evaluation. Identity Preservation (CLIP-I / DINO-I). To measure how well the identity of the reference object is preserved, we compute frame-wise similarity. For CLIP-I, we use the pre-trained clip-vit-large-patch14 [43] model from OpenAI. For DINO-I, we use the dinov2-giant [37] model from Facebook. The process is as follows: (1) we extract the image feature of the reference object; (2) for each frame in the generated video, we extract its image feature; (3) we compute the cosine similarity between the reference feature and each frames feature; (4) the final score is the average similarity across all frames. Background Preservation (PSNR / SSIM). These metrics are computed exclusively on the unedited regions of the video to measure background stability. The unedited area is defined by mask. For object swapping and deletion tasks on the Unic-Benchmark, we use the ground-truth masks provided with the dataset. For the object insertion task, where no prior object exists, we generate mask for the newly inserted object using Grounding-SAM [45], and the background is defined as the area outside this mask. The final PSNR and SSIM scores are averaged over all frames. VBench Metrics. For the readers convenience, we briefly summarize the calculation methods for the VBench metrics used in our paper, drawing from the descriptions in the official VBench suite [14]. Overall Consistency. This metric evaluates the overall video-text consistency, reflecting both semantic and style alignment. It is computed using ViCLIP [59] as an aiding metric, where the model assesses the video against text prompts that contain different semantics and styles. Motion Smoothness. To evaluate temporal quality beyond simple appearance consistency, this metric assesses whether the motion in the generated video is smooth and follows the physical laws of the real world. It utilizes the motion priors from video frame interpolation model [20] to quantify the smoothness of the generated motions. Dynamic Degree. Complementing smoothness, this metric measures the degree of dynamics (i.e., the presence of 1 context. Our choice of = 4 strikes an optimal balance, yielding dynamic yet stable object. 8.3. Ablation on Layer Selection  (Table 4)  . Figure 20 and Figure 21 validates our data-driven layer selection strategy. Injecting information into all layers will lead to an excessive amount of information from the reconstruction path being introduced into the editing path, making it difficult to maintain the edited objects and causing the generated video to unconsciously lean towards the original video. Injecting guidance into the four least-responsive layers yields poor results, similar to providing weak guidance. Using U-Net-based heuristic (from AnyV2V [18]) in the DiT model, which in sense amounts to randomly selecting certain layers within DiT for injection, still leads to suboptimal results. Due to the inability of the injected layers to solve substantive problems, it often leads to stiff interaction between new objects and the background, and even unconscious emergence of unreasonable phenomena (such as the chimpanzee in Figure 21 strangely growing tail). This visually confirms that our Guidance Responsiveness metric correctly identifies the most impactful layers for intervention. 8.4. Ablation on Guidance Window  (Table 5)  . Figure 22 and Figure 23 illustrates the importance of the timestep threshold τ . value of τ = 0.2 often results in insufficient fusion between the edited object and its background, leading to abnormal deformations of the edited object or unnatural modifications to the surrounding backIn contrast, τ = 0.5 effectively addresses these ground. issues, achieving both excellent fusion quality and robust preservation of the reference identity. Notably, the performance with τ = 1.0 is comparable to that of τ = 0.5. This observation suggests that through the fusion of the editing path with the reconstruction path via the Adaptive Context Enrichment mechanism during the first half of the diffusion process, sufficient essential information has already been acquired. Consequently, during the latter half of the diffusion process, there is minimal need to further extract useful information from the reconstruction path. Considering the trade-off between computational efficiency and editing performance, τ = 0.5 emerges as the optimal choice. large motions) to ensure that completely static videos are not unfairly favored in temporal quality scores. It uses the RAFT [52] model to estimate the degree of dynamics in synthesized videos. Aesthetic Quality. This metric evaluates the frame-wise artistic and beauty value perceived by humans. It employs the LAION [19] aesthetic predictor to assess aspects such as the layout, the richness and harmony of colors, photorealism, naturalness, and the overall artistic quality of the video frames. 7. Additional Qualitative Comparisons To further situate ContextFlows performance, we provide qualitative comparisons against more strong, contemporary methods, for example Unic [65] and commercial trainingbased tools like Pika [40] and Kling [17]. As shown in Figure 7 - Figure 14 , ContextFlow consistently demonstrates superior performance in both identity preservation and motion consistency. ContextFlow accurately maintains the intricate details and colors of the reference image, while ensuring it interacts realistically with the clouds. Both Unic and Pika also allow users to input reference images to define objects for insertion or swapping. However, they cannot enable users to freely and customarily edit the first frame, limitation that causes them to struggle to maintain such high fidelity to the reference image. 8. Visual Analysis for Ablation Studies To complement the quantitative ablation studies in the main paper, this section provides corresponding visual analysis. These visualizations offer intuitive insights into how each component of ContextFlow contributes to the final result. 8.1. Visualization of Ablation on Guidance Strategy  (Table 2)  . Figure 15 - Figure 17 visualizes the impact of our core guidance mechanism. Without any adaptive Context Enrichment (CE), the inserted object struggles to maintain consistent identity and motion. Using K/V Replacement instead of our concatenation approach leads to severe artifacts and identity degradation, validating our hypothesis that hard replacement causes destructive contextual conflict. Our full method successfully fuses the object while preserving the background. 8.2. Ablation on Amount of Guidance  (Table 3)  . Figure 18 and Figure 19 shows the effect of varying k, the number of guided layers. With insufficient guidance (k = 1), the object appears unstable, which causes poor video quality. With excessive guidance (k = 40, all layers), the object becomes overly constrained by the background 2 Figure 7. Visualization of Qualitative Comparison. Target prompt: blue car is driving on the road. Our method accurately simulates the movement state of the blue car traveling on the road, whereas the other methods fail to achieve the effect of the car moving correctly in the proper position on the road. Figure 8. Visualization of Qualitative Comparison. Target prompt: colossal Statue of Liberty stands atop white base with classical columns, surrounded by dense forest. Our method achieves the highest fidelity to the reference image. 3 Figure 9. Visualization of Qualitative Comparison. Target prompt: red poppy flower surrounded by purple flowers. dragonfly is stopping at the red poppy flower. Our method has achieved results that most closely to match the reference image as well as the target prompt (i.e., dragonfly is stopping at the red poppy flower). Figure 10. Visualization of Qualitative Comparison. Target prompt: cute blue fluffy monster sits on top of toy jeep against brick wall. Our method achieves the highest fidelity to the reference image, and it has also achieves excellent preservation of the shape of the brick walls in the original video. 4 Figure 11. Visualization of Qualitative Comparison. Target prompt: man rides motorcycle down lonely road, with little flying dragon. The flying dragon is near the man through the video. It has the same speed with the man. Our method achieves the highest fidelity to the reference image, while the movements of the little dragon are also the most natural. Figure 12. Visualization of Qualitative Comparison. Target prompt: colossal Eiffel Tower stands atop white base with classical columns, surrounded by dense forest. Our method achieves the highest fidelity to the reference image. 5 Figure 13. Visualization of Qualitative Comparison. Target prompt: hand-held white plastic watering can is used to water vibrant collection of pink and purple flowers, including geraniums and possibly petunias, in white planter. The target to be deleted in this video is the railing in the lower left corner. Our method has high fidelity for the unedited areas, and its effect is better than that of Kling [17]. Figure 14. Visualization of Qualitative Comparison. Target prompt: Two young women, dressed in cozy winter attire, stand together in tranquil snowy winter landscape, engaging in the warmth of sharing hot drink. The target to be deleted in this video is the girl on the far left. Our method performs better. 6 Figure 15. Visualization of Ablation on Guidance Strategy. Target prompt: man rides motorcycle down lonely road. large, imposing Statue of Liberty stands in the background. Neither Method w/o CE. nor Method K/V replacement can effectively integrate the Statue of Liberty into the background correctly and naturally, resulting in the inserted object either disappearing or failing to blend well into the background environment. Please zoom in for closer check. Figure 16. Visualization of Ablation on Guidance Strategy. Target prompt: cute, black and red dragon with large, glowing eyes stands on the plane wing, playfully flipping its wings. Both Method w/o CE. and Method K/V replacement struggle to maintain the little dragons shape in conformity with the reference image as it was in the first frame; instead, strange deformations occur. Please zoom in for closer check. 7 Figure 17. Visualization of Ablation on Guidance Strategy. Target prompt: Pikachu is floating on the sea. Method w/o CE. fails to properly integrate Pikachu with the background, thus failing to achieve the effect required by the promptfloating on the sea surface. In contrast, Pikachu in Method K/V replacement has extremely stiff movements and lacks the natural sense of dynamics associated with floating. Please zoom in for closer check. 8 Figure 18. Visualization of Ablation on Amount of Guidance. Target prompt: man rides motorcycle down lonely road, with little flying dragon. The flying dragon is near the man through the video. It has the same speed with the man. When 2 or 8 16, the generated video exhibits strange colored bubbles and abnormal deformation of the little dragons ears, leading to poor editing quality (enlarged via magnifying glass in the figure). When 32 the little dragon disappears. Please zoom in for closer check. 9 Figure 19. Visualization of Ablation on Amount of Guidance. Target prompt: red poppy flower surrounded by purple flowers. large gorilla is gently trying to touch the red poppy flower. When 2, the generated video will have discordant black stripes (zoom in and see the red box area in the figure); when 8, the movement posture of the gorilla will appear strange; when 32, the gorilla will become translucent or disappear in subsequent frames. 10 Figure 20. Visualization of Ablation on Layer Selection. Target prompt: Pikachu is floating on the sea. Injection on all layers and Injection on 4 least responsive layers will cause Pikachu to disappear quickly in subsequent frames or fail to display actions correctly according to the target prompt. Injection selection follow AnyV2V [18] will produce an extremely stiff and weird floating effect that is highly unnatural, along with an odd smile not required by the prompt. Please zoom in for closer check. 11 Figure 21. Visualization of Ablation on Layer Selection. Target prompt: red poppy flower surrounded by purple flowers. large gorilla is gently trying to touch the red poppy flower. Injection on all layers and Injection on 4 least responsive layers will cause the inserted object to disappear or become translucent. Injection selection follow AnyV2V [18] will once again result in the strange form of the gorilla: an oddly appearing tail (within the red circle). Please zoom in for closer check. Figure 22. Visualization of Ablation on Guidance Window. Target prompt: An octopus is swimming in the sea. magnifying effect is applied to zoom in on the octopus in the image. τ = 0.2 causes the octopus to take on an abnormal shape with odd black patches, making it unable to blend well with the environment. In contrast, under τ = 0.5 and τ = 1.0, the octopus has more natural shape and blends appropriately with the environment. Please zoom in for closer check. Figure 23. Visualization of Ablation on Guidance Window. Target prompt: man in dark grey blazer and white shirt is seated on city fountains edge, eating from wooden bowl with fork. cute blue fluffy monster is on the ground. τ = 0.2 will cause an abnormal text-like pattern to appear in the red box at the bottom right corner of the picture, while such pattern will not appear under τ = 0.5 and τ = 1.0. Please zoom in for closer check. 13 9. Limitations and Future Work Despite its strong performance, ContextFlow has several limitations that open avenues for future research. Dependency on First-Frame Edit. The quality of our video edits is inherently tied to the quality of the initial edited frame, Iedit . Artifacts or inaccuracies in the firstframe edit, produced by third-party image editors, are likely to be propagated and potentially amplified throughout the video. Automating or integrating the first-frame editing process into single, cohesive framework could address this dependency. Challenges with Extreme Motion and Occlusion. While ContextFlow is robust in many scenarios, it may face challenges with videos containing extremely rapid object or camera motion, or complex, prolonged occlusions of the edited object. In such cases, maintaining perfect temporal consistency and object integrity can be difficult for any I2V model, and our guidance mechanism may struggle to provide stable enough signal. Computational Overhead. As training-free method relying on dual-path sampling process, ContextFlow is computationally intensive. As detailed in A.1, the inference time and high VRAM requirement may be barrier for users with less powerful hardware. Future Work. Based on these limitations, we identify several promising directions for future work. First, developing more robust framework that can handle severe motion and occlusions remains key challenge. exploring model compression or distillation Second, techniques could significantly reduce the computational cost and improve inference speed. integrating advanced motion control mechanisms could allow for more fine-grained manipulation of the edited objects trajectory and dynamics, moving beyond simple propagation. Finally,"
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: unified tuningfree framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185, 2024. 3 [2] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise, 2025. 3 [3] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. 2 [4] Kun Chen, Wen-He Zhang, Chang-Jiang Jiang, Zhi-Xin Chen, Fang-Lue Chen, Xiao-Li Zhang, and Zhen-Jie Wang. Vace: trainable image-to-video editing framework for all-in-one video editing. arXiv preprint arXiv:2402.16272, 2024. 2, 6 [5] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. 3 [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF conage customization. ference on computer vision and pattern recognition, pages 65936602, 2024. 4, 6, [7] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: DifIn Proceedings of fusion transformer for image editing. the AAAI Conference on Artificial Intelligence, pages 2969 2977, 2025. 2 [8] Kunyu Feng, Yue Ma, Xinhua Zhang, Boshi Liu, Yikuang Yuluo, Yinhan Zhang, Runtao Liu, Hongyu Liu, Zhiyuan Qin, Shanhui Mo, et al. Follow-your-instruction: comprehensive mllm agent for world data synthesis. arXiv preprint arXiv:2508.05580, 2025. 2, 3 [9] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing, 2023. 2 [10] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. Advances in Neural Information Processing Systems, 35:1471514728, 2022. 2 [11] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2 [13] Xuanhua He, Quande Liu, Zixuan Ye, Weicai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, and Kun Gai. Fulldit2: Efficient in-context conditioning for video diffusion transformers. arXiv preprint arXiv:2506.04213, 2025. [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 1 [15] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 14 [16] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion In Proceedings of the IEEE/CVF Conference on Models. Computer Vision and Pattern Recognition, 2024. 3 [17] Kling AI. Official Website, 2024. 2, 6 [18] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks, 2024. 2, 3, 6, 11, 12 [19] LAION-AI. aesthetic-predictor. GitHub repository, 2022. 2 [20] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [21] Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, and Qifeng Chen. Avatarartist: Open-domain 4d avatarization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1075810769, 2025. 2 [22] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, and Jiaya Jia. Generative video propagation. arXiv preprint arXiv:2412.19761, 2024. 2, 3 [23] Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Wen Wang, Zhiheng Liu, Qifeng Chen, and Yujun Shen. Magicquill: An intelligent interactive image editing system. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1307213082, 2025. 6, 1 [24] Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, and Yue Ma. Follow-your-shape: Shape-aware image editarXiv preprint ing via trajectory-guided region control. arXiv:2508.08134, 2025. 3 [25] Hao Luo, Yuanpeng Tu, Xi Chen, et al. GetInVideo: Reference-guided video object insertion. Assumed arXiv preprint, 2025. 2 [26] Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, and Harry Yang. Model reveals what to cache: Profiling-based feature reuse for video diffusion models. arXiv preprint arXiv:2504.03140, 2025. 2 [27] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, and Yu Qiao. Visual knowledge graph for human action reasoning in videos. In Proceedings of the 30th ACM International Conference on Multimedia, pages 41324141, 2022. 2 [28] Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Magicstick: Controllable video editing via control handle transformations. arXiv preprint arXiv:2312.03047, 2023. [29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 2 [30] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 3 [31] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. 2 [32] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. 2 [33] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 60186026, 2025. [34] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [35] Eyal Molad, Elia Gat, Yoni Elhassan, Idan Dror, and Daniel Cohen-Or. Dreamix: Video editing with pre-trained textto-image diffusion model. ACM Transactions on Graphics (TOG), 42(4):111, 2023. 2 [36] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505, 2024. 2 [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 1 [38] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2VEdit: First-Frame-Guided Video Editing via Imageto-Video Diffusion Models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2, 3, 6 [39] Yatian Pang, Bin Zhu, Bin Lin, Mingzhe Zheng, Francis EH Tay, Ser-Nam Lim, Harry Yang, and Li Yuan. Dreamdance: Animating human images by enriching 3d geometry cues from 2d poses. arXiv preprint arXiv:2412.00397, 2024. 2 [40] Pika Labs. Pika 2.0. Pika Labs Website, 2025. 1, 2 [41] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation, 2018. 1 [42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fusing Attentions for Zero-Shot Text-Based Video Editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 15 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and et al. Learning Transferable Visual Models from Natural Language Supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [44] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In Advances in Neural Information Processing Systems, pages 12116 12128, 2021. 6 [45] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 1 [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 2 [47] Nirat Saini, Navaneeth Bodla, Ashish Shrivastava, Avinash Ravichandran, Xiao Zhang, Abhinav Shrivastava, and Bharat Singh. Invi: Object insertion in videos using off-the-shelf diffusion models, 2024. [48] Zhelun Shen, Chenming Wu, Junsheng Zhou, Chen Zhao, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Wei He, and Jingdong Wang. idit-hoi: Inpainting-based hand object interaction reenactment via video diffusion transformer, 2025. 3 [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 2 [50] Wensong Song, Hong Jiang, Zongxing Yang, Ruijie Quan, and Yi Yang. Insert anything: Image insertion via in-context editing in dit. arXiv preprint arXiv:2504.15009, 2025. 6, 1 [51] Tongtong Su, Chengyu Wang, Jun Huang, and Dongming initialization empowering Lu. reference-based video appearance editing, 2025. 3 Zero-to-hero: Zero-shot [52] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 2 [53] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT redisIn Proceedings of the covers the classical NLP pipeline. 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy, 2019. Association for Computational Linguistics. 6 [54] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. arXiv preprint arXiv:2501.01427, 2025. 2, 3 [55] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 6, 1 16 [56] Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, and Jinfeng Xu. Consistent video editing as flowdriven image-to-video generation, 2025. [57] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. 2 [58] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 2, 4, 6 [59] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: largescale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 1 [60] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: Unified and Flexible Motion Controller for In ACM SIGGRAPH 2024 Conference Video Generation. Papers, 2024. 3 [61] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subject-driven video customizaarXiv preprint tion with precise motion control. arXiv:2410.13830, 2024. 2 [62] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 2 [63] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024. [64] Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270, 2025. 2 [65] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 2, 6, 1 [66] Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, and Zeyu Wang. Magiccolor: Multi-instance sketch colorization. arXiv preprint arXiv:2503.16948, 2025. 2 [67] Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, et al. Videogen-of-thought: Step-by-step generating multi-shot video with minimal manual intervention. arXiv preprint arXiv:2412.02259, 2024. 2 [68] Chenyang Zhu, Kai Li, Yue Ma, Longxiang Tang, Chengyu Fang, Chubin Chen, Qifeng Chen, and Xiu Li. Instantswap: Fast customized concept swapping across sharp shape differences. arXiv preprint arXiv:2412.01197, 2024. 2 [69] Chenyang Zhu, Kai Li, Yue Ma, Chunming He, and Xiu Li. Multibooth: Towards generating all your concepts in an imIn Proceedings of the AAAI Conference on age from text. Artificial Intelligence, pages 1092310931, 2025. [70] Shaobin Zhuang, Zhipeng Huang, Binxin Yang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Chong Sun, Zheng-Jun Zha, Chen Li, and Yali Wang. Get in video: Add anything you want to the video, 2025."
        }
    ],
    "affiliations": [
        "State Key Laboratory of General Artificial Intelligence, Peking University, Beijing, China",
        "The Hong Kong University of Science and Technology"
    ]
}