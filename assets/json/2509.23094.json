{
    "paper_title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
    "authors": [
        "Yuchu Jiang",
        "Yue Cai",
        "Xiangzhong Luo",
        "Jiale Fu",
        "Jiarui Wang",
        "Chonghan Liu",
        "Xu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache."
        },
        {
            "title": "Start",
            "content": "D2CACHE: ACCELERATING DIFFUSION-BASED LLMS VIA DUAL ADAPTIVE CACHING Yuchu Jiang1,2 Yue Cai1,2 Xiangzhong Luo1,2 Jiale Fu1,2 Chonghan Liu3 Xu Yang1,2 1Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education 3Qiyuan Tech kamichanw@seu.edu.cn, xiangzhong.luo@seu.edu.cn 2Southeast University Jiarui Wang1,2 5 2 0 2 7 ] . [ 1 4 9 0 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce Dual aDaptive Cache (d2Cache), which is training-free approximate KV cache framework for accelerating dLLM inference. d2Cache features twostage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d2Cache naturally offers more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (i.e., LLaDA and Dream) demonstrate that d2Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have recently achieved remarkable success in generating continuous data like images (Yang et al., 2023), but text generationa fundamentally discrete taskhas long been dominated by autoregressive models (ARMs) (Touvron et al., 2023; Achiam et al., 2023; Guo et al., 2025). Building on the foundations of ARMs, recent studies have successfully extended diffusion processes to discrete language modeling and further scaled up these models (Nie et al., 2025; Ye et al., 2025; Li et al., 2025). These diffusion-based large language models (dLLMs) offer several key advantages over ARMs, such as mitigating the reversal curse (Berglund et al., 2023) and capturing high-level global semantic patterns (Nagarajan et al., 2025). Despite their potential, recent dLLMs still face substantial efficiency challenges (Wu et al., 2025). Due to bidirectional attention, dLLMs cannot benefit from the standard key-value (KV) cache as ARMs do. As shown in Figure 1 (a), ARMs leverage causal attention to sequentially generate new tokens and append each new token to the end of the sequence. This autoregressive process naturally enables the reuse of earlier KV states when generating the next token (Li et al., 2024). In contrast, as shown in Figure 1 (b), dLLMs feature an iterative decoding process over fixed-length sequence, where masked tokens are progressively replaced with decoded tokens. However, under bidirectional attention, updating even single masked token changes the context seen by all other tokens (Ye et al., 2025; Nie et al., 2025). As result, the KV states of the entire sequence must be recomputed at each decoding step, making dLLMs inherently incompatible with the standard KV cache. To address the above efficiency challenges, recent studies (Ma et al., 2025; Wu et al., 2025; Liu et al., 2025; Hu et al., 2025) have explored approximate KV cache to accelerate dLLM inference. These studies build on the following key observation: for subset of tokens, their KV states often exhibit Corresponding Author 1 Figure 1: (a) In ARMs, causal attention requires each token to interact only with its preceding tokens. (b) In dLLMs, bidirectional attention requires each token to attend to both its preceding and subsequent tokens, such that any modification in the subsequent tokens necessitates recomputation of the entire sequence. (c) The proposed d2Cache adaptively selects small subset of tokens in dLLMs and updates their KV states through two-stage fine-grained process. The KV states of the remaining tokens can be approximately cached for reuse in subsequent decoding step. high similarity across consecutive decoding steps. This enables to approximately reuse these KV states, which can avoid redundant computations and reduce the overall inference cost. In practice, these studies typically divide the sequence (including prompt tokens, masked tokens, and decoded tokens) into static segment, where their KV states can be approximately reused, and dynamic segment, where their KV states need to be frequently updated within fixed window of decoding steps. However, these studies are coarse-grained and apply the same strategy to all tokens within both static and dynamic segments. As result, they either suffer from limited flexibility or require complicated tuning. Moreover, since coarse-grained designs cannot capture the fine-grained tokenlevel dynamics of KV states, they inevitably reuse KV states that should be updated, or update KV states that can be safely reused, thus limiting the achievable acceleration gains. To address these limitations, we seek to develop an effective fine-grained approximate KV cache strategy, which can adaptively select tokens and update their KV states at each decoding step rather than within fixed decoding window (Ma et al., 2025; Wu et al., 2025; Liu et al., 2025; Hu et al., 2025). To this end, we first perform fine-grained analysis to investigate the KV state dynamics in dLLMs. Our results show that, for masked tokens, their KV states evolve through three phases: (1) gradual-change phase during the early decoding steps, (2) rapid-change phase in the few steps immediately preceding their decoding, and (3) stable phase after being decoded. Notably, we find that it is sufficient to update the KV states of masked tokens only during the rapid-change phase. Nonetheless, unlike masked tokens, prompt and decoded tokens exhibit substantially smaller KV state dynamics across consecutive decoding steps. This makes the above phase-based caching strategy less effective and necessitates another caching alternative for prompt and decoded tokens. Inspired by prior KV cache research in ARMs (Feng et al., 2024; Cai et al., 2024), which reveals that attention is unevenly distributed and concentrated on small subset of tokensthus allowing to prune the KV states of less important oneswe investigate whether dLLMs exhibit the same attention behavior. Our results confirm that attention in dLLMs is likewise concentrated on small subset of tokens, especially prompt and decoded tokens. Therefore, similar to KV cache pruning, we can adaptively update the KV states of tokens that receive consistently higher attention, whereas the KV states of the remaining tokens can be safely cached for reuse in subsequent decoding step. Motivated by the above observations, we propose Dual aDaptive Cache (d2Cache), training-free approximate KV cache framework for accelerating dLLM inference, as shown in Figure 1 (c). Specifically, d2Cache features two-stage fine-grained selection strategy that identifies tokens and adaptively updates their KV states at each decoding step, while the KV states of the remaining tokens can be cached and reused. In the meantime, d2Cache also naturally delivers more reliable decoding option, which seamlessly enables quasi left-to-right generation and thus mitigates premature overconfidence in the tokens at the end of the sequence. Extensive experiments on representative dLLMs (i.e., LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025)) demonstrate that d2Cache not 2 only achieves substantial inference speedups, but also yields consistent improvements in generation quality. Finally, we summarize our main contributions as follows: We present fine-grained analysis on the KV state dynamics in dLLMs, which explicitly reveals three-phase decoding pattern and uneven attention distribution. Building on the above findings, we propose training-free approximate KV cache framework, namely d2Cache, to accelerate dLLM inference. d2Cache features two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while the KV states of the remaining tokens can be cached for reuse in subsequent decoding step. Extensive experiments demonstrate that d2Cache can achieve substantial inference speedups while consistently improving generation quality across various dLLMs and datasets."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion-based large language models. Building on the success of diffusion models in continuous domains, such as image and video generation (Yang et al., 2023; Ho et al., 2022), recent studies have extended diffusion models to discrete language tasks (Sahoo et al., 2024; Shi et al., 2024; Nie et al., 2024; Arriola et al., 2025). Unlike autoregressive models (ARMs) that generate tokens sequentially (Touvron et al., 2023; Achiam et al., 2023; Guo et al., 2025), dLLMs feature an iterative denoising process over masked sequences, which can enable bidirectional context modeling and inherently support parallel decoding (Li et al., 2025). More recently, large-scale dLLMs, such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), have demonstrated competitive performance on reasoning and instruction-following tasks, establishing themselves as promising alternative to ARMs. Despite their promising performance, their reliance on bidirectional attention necessitates substantial inference overheads, which significantly hinder their practical deployments. Approximate KV cache for dLLMs. Due to bidirectional attention, dLLMs cannot directly benefit from the standard KV cache (Li et al., 2025) as ARMs do. To address this limitation, recent studies have observed that the KV states in dLLMs remain highly similar across consecutive decoding steps. Building on this observation, several approximate KV caching techniques have recently emerged (Liu et al., 2025; Ma et al., 2025; Wu et al., 2025; Hu et al., 2025). Among them, dLLM-Cache (Liu et al., 2025) partitions the input sequence into two segmentsprompt and responseand updates their KV states at different frequencies. dKV-Cache (Ma et al., 2025) introduces one-step delayed KV caching scheme, in which decoded tokens are stored not at the current decoding step but at the subsequent decoding step. Fast-dLLM (Wu et al., 2025) features block-wise semi-autoregressive decoding and caches all KV states except those in the current decoding block. However, due to the coarse-grained nature, these methods inevitably reuse KV states that should be actively updated or update KV states that can be safely reused, which thus suffer from inferior acceleration gains. comprehensive comparison between our d2Cache and two concurrent similar works (i.e., dLLM-Cache and Fast-dLLM) is provided in Section of the Appendix."
        },
        {
            "title": "3.1 GENERATION PROCESS OF DLLMS",
            "content": "dLLMs feature an iterative denoising paradigm to generate text over discrete decoding steps (Nie et al., 2025), where fully masked initial sequence is progressively transformed into fully unmasked final output. Formally, let denote the token vocabulary, which includes special token [MASK]. The inference process begins with an initial sequence y0 of length L, which is constructed by concatenating prompt segment with response segment r0 of masked tokens. We denote the set of indices corresponding to these masked tokens as M0 = {p, + 1, . . . , + 1}. At each decoding step [0, . . . , 1], the corresponding sequence yt is first fed into the given dLLM model as input, which produces probability distribution p(xi yt) over the vocabulary for t. Based on this distribution, the most confident token predictions ˆXt and each masked position xi 3 (a) (b) Figure 2: (a) PCA of 77th masked tokens trajectory on LLaMA-8B-Instruct with GSM8K (L=328, n=256, and =256). (b) Sequential distances between token pairs decoded in adjacent steps. their associated confidence scores St can be derived as follows: ˆXt = {ˆxi ˆxi = arg max = F(cid:0)p(xi where F() is function that measures the token-level prediction confidence score. = yt), Mt}, yt)(cid:1) , Mt}, St = {si = ˆxi si xV p(xi (1) Furthermore, the decoding process employs scheduling function to generate set of indices It, which specifies the masked positions in yt to be replaced with their predicted tokens: It = G(ˆxt, st, yt), where yi t+1 = (cid:26)ˆxi yi if It, otherwise. (2) In practice, the scheduling is typically performed either by randomly sampling subset of Mt or by choosing those masked positions with the highest confidence scores (Nie et al., 2025). Subsequently, the masked index set for the next decoding step is updated as Mt+1 = Mt It. After iterations, when the condition MT = holds, the whole generation process is stopped and we get the final sequence yT with no remaining masked tokens (Nie et al., 2025)."
        },
        {
            "title": "3.2 KV STATE DYNAMICS AND DECODING ORDER IN DLLMS",
            "content": "Recent studies on approximate KV cache in dLLMs have shown that the KV states of certain tokens exhibit high similarity across adjacent decoding steps (Wu et al., 2025; Liu et al., 2025). Leveraging this redundancy, they first partition the entire sequence into static segment and dynamic segment, after which they cache the KV states of tokens in the static segment for reuse. Despite its efficacy, this segment-level partitioning scheme is coarse-grained and totally ignores the fine-grained tokenlevel dynamics. To bridge this gap, we begin with masked tokens and perform experiments on LLaDA-8B-Instruct with GSM8K to explore how their KV states evolve during generation. KV state dynamics in dLLMs. To analyze the dynamics of KV states for masked tokens, we employ principal component analysis (PCA) to project their layer-averaged key states into two dimensions and visualize their trajectories across decoding steps. As shown in Figure 2 (a), the KV states of masked tokens evolve through three phases: (1) gradual-change phase during the early decoding steps (i.e., steps 0-64), (2) rapid-change phase in the few steps immediately preceding their decoding (i.e., steps 64-98), and (3) stable phase after being decoded (i.e., steps 98-255). We find that it is sufficient to update the KV states of masked tokens only during the rapid-change phase, whereas the KV states of masked tokens from the other two phases can be safely cached for reuse. More importantly, this does not degrade the final generation quality, as shown in Figure 5. Decoding order in dLLMs. Building on the above findings, natural question arises: how can we determine whether masked token is about to be decoded before its actual decodingessentially chicken-and-egg problem? To shed light on this, we leverage LLaDA-8B-Instruct and randomly 4 (a) (b) Figure 3: Attention rollout analysis over sequence, where the example and setting are the same as in Figure 2. (a) Attention rollout visualization at step 126, showing the sum of rollout values over all key positions (top) and the pairwise rollout values across different positions (bottom). (b) The total absolute differences in rollout values between each two adjacent decoding steps. sample 64 examples from GSM8K, in which we analyze the sequential distance between token pairs decoded in adjacent steps. As shown in Figure 2 (b), LLaDA-8B-Instruct tends to decode the next masked token from positions close to the most recently decoded token, with 90% of tokens falling within distance of 10. This reveals an interesting decoding pattern: dLLMs tend to decode masked tokens located near previously decoded tokens. Therefore, we can estimate whether masked token is about to be decoded according to the density of decoded tokens in its local context."
        },
        {
            "title": "3.3 ATTENTION DISTRIBUTIONS IN DLLMS",
            "content": "Prior research on ARMs has observed that attention is not uniformly distributed but instead concentrated on small subset of salient tokens (Xiao et al., 2023). This observation has served as the foundation for various optimization techniques, which apply differentiated strategies to tokens based on their importance (Feng et al., 2024; Cai et al., 2024). This naturally raises the following question: can the above observation from ARMs generalize to dLLMs? To answer this question, we conduct experiments on LLaDA-8B-Instruct with GSM8K to analyze the attention distribution. Attention salience among tokens. Inspired by prior attention studies on ARMs, we employ attention rollout (Abnar & Zuidema, 2020) to visualize how attention propagates across tokens. The attention rollout algorithm aggregates cumulative attention by recursively multiplying the attention matrices across layers, yielding global attribution map that highlights how information propagates from input tokens to the final output. More details about the attention rollout algorithm are provided in Section 4.2. As shown in Figure 3 (a, bottom), queries consistently attend to small subset of key positions in prompt and decoded tokens, revealing that these tokens dominate the attention distribution compared to other tokens. As shown in Figure 3 (a, top), masked tokens receive negligible attention, which is substantially lower than that allocated to both prompt and decoded tokens. Similarity of attention allocations in adjacent steps. Building on the above findings, we further calculate the sum of absolute differences in rollout values across all pairs of decoding steps. As shown in Figure 3 (b), the attention allocations across adjacent decoding steps are highly similar. This suggests that the attention allocation of the current decoding step can be used to approximate that of the next decoding step. In light of this, analogous to KV cache optimization techniques in ARMs, KV state updates can thus be restricted to tokens that receive higher attention."
        },
        {
            "title": "Key Observations and Takeaways",
            "content": "1. The KV states of masked tokens evolve through three phases: gradual change rapid change stable. Notably, it is sufficient to update the KV states of masked tokens only 5 during the rapid-change phase, whereas the KV states of masked tokens can be safely cached for reuse during the gradual-change and stable phases. Figure 2 (a) 2. The decoding order of masked tokens correlates with the number of decoded tokens in the local context. Therefore, we can estimate whether the masked token is in the rapid-change phase according to the density of decoded tokens in the local context. Figure 2 (b) 3. The attention distribution in dLLMs is highly uneven: prompt and decoded tokens receive most attention from other tokens, while masked tokens only receive negligible attention. Moreover, attention allocations across consecutive decoding steps are highly similar. The attention allocation of the current step can thus be used to select tokens for KV state updates in the next step, i.e., prioritizing tokens that receive greater attention. Figure"
        },
        {
            "title": "4 D2CACHE: DUAL ADAPTIVE CACHE",
            "content": "Motivated by the observations in Section 3, we present Dual aDaptive Cache (d2Cache), trainingfree approximate KV cache framework for accelerating dLLM inference. Unlike ARMs, which can naturally reuse previous KV states (Li et al., 2024), dLLMs cannot exploit this mechanism due to their non-autoregressive decoding nature (Wu et al., 2025), as shown in Figure 1. To bridge this gap, d2Cache seeks to adaptively identify tokens whose KV states should be actively updated at each decoding step, while caching the remaining tokens for reuse in subsequent decoding step. Overview of d2Cache. As seen in (Nie et al., 2025), tokens in dLLMs can be grouped into three categories: prompt tokens, masked tokens, and decoded tokens. Based on this categorization, we introduce two-stage fine-grained token selection strategy. ❶ Certainty prior-guided selection from masked tokens. After each forward pass, d2Cache assigns each masked token certainty prior, defined as the product of its prediction confidence and the density of known tokens (i.e., prompt or decoded tokens) in its local context. d2Cache then adaptively selects subset of masked tokens with higher certainty prior. In light of this, d2Cache naturally delivers an alternative decoding scheme: masked tokens can be decoded according to their certainty prior rather than prediction confidence. This certainty prior-guided decoding has proven more reliable than the default confidence-based decoding (see Table 2). ❷ Attention-aware selection from remaining tokens. Furthermore, for the remaining tokens (especially prompt and decoded tokens), d2Cache adaptively selects subset of tokens with higher attention activations, which can be identified using attention rollout Abnar & Zuidema (2020). Finally, for the tokens selected in these two stages, d2Cache updates their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse in subsequent decoding step. An intuitive example of this two-stage token selection is provided in Figure 1 (c)."
        },
        {
            "title": "4.1 STAGE 1: CERTAINTY PRIOR-GUIDED SELECTION",
            "content": "As shown in Figure 2 (b), the decoding order in dLLMs is highly localized: 90% of subsequent tokens are decoded within distance of 10 from the most recently decoded token. Building on this finding, we introduce certainty prior, which quantifies (1) the prediction confidence and (2) the certainty density of neighboring tokens that are known (i.e., prompt or decoded tokens). For each masked token, we define its certainty prior as the product of its prediction confidence and the density of known tokens in its local context. In practice, the certainty prior can capture structural certainty, where higher value indicates that the masked token is more likely to be decoded sooner. Formally, at each decoding step [0, . . . , 1], the sequence yt is fed into the given dLLM to generate predictions ˆXt for the masked tokens xt, together with their corresponding confidence 1. With the above in mind, natural definition of certainty density is the proportion of scores St known tokens (i.e., prompt or decoded tokens) within fixed local window. However, this definition ignores the effect of relative distance among tokens: intuitively, known token that is closer to masked token xi should impose stronger constraints on xi than another known token that is farther away. To capture this intuition, we introduce the following position-aware certainty density: D(i) = (cid:88)L1 j=0 ϕ (i j) I{j /M }, s.t. ϕ (i j) = exp (cid:18) j2 2σ2 (cid:19) , (3) 1For the simplicity of notation, we omit the subscript for the current step in the remainder of this paper. 6 where denotes the position of the masked token xi and denotes the position of each known token in the sequence. In practice, the Gaussian function ϕ() assigns larger weights to known tokens that are closer to xi and smoothly diminishes the impact of distant ones, making D() distance-aware aggregation of certainty from all known tokens. The effect of weighting is further controlled by the hyperparameter σ, which denotes the standard deviation of the Gaussian function ϕ(). larger σ broadens the positional scope considered by D(i), thereby causing the certainty density of different xi to converge. Finally, we incorporate D() into to measure the certainty prior and select the masked tokens with the top-k calibrated scores, with their indices forming the candidate set . = arg topk iM D(i) si. (4) This formulation ensures that token selection considers both prediction performance and certainty density, which thus can provide principled foundation for more reliable token selection. Certainty prior-guided decoding. The above certainty prior delivers novel decoding alternative: masked tokens can be decoded according to their certainty prior rather than their prediction confidence. We demonstrate that the certainty prior-guided decoding can achieve more reliable decoding performance than the default confidence-based decoding, as shown in Table 2. The intuition here is that the certainty prior-guided decoding can preserve quasi left-to-right decoding order, since masked tokens located closer to known tokens exhibit higher structural and predictive certainty. This quasi left-to-right decoding order effectively mitigates the issue of premature overconfidence in sequence termination during the early decoding steps (Huang et al., 2025)."
        },
        {
            "title": "4.2 STAGE 2: ATTENTION-AWARE SELECTION",
            "content": "In Section 4.1, we present certainty prior-guided selection, which explores masked tokens whose KV states should be updated at each decoding step. In this section, we extend the selection process to the remaining tokens. Notably, we observe that attention rollout (Abnar & Zuidema, 2020)a widely used attention analysis technique in ARMscan effectively generalize to dLLMs, particularly for analyzing prompt and decoded tokens, making it well suited for our subsequent token selection. As described in (Abnar & Zuidema, 2020), the attention rollout algorithm aggregates cumulative attention by recursively multiplying the attention matrices across layers, yielding global distribution map that reveals how information propagates from input tokens to the final output. Formally, let denote the indices of the remaining tokens. At the decoding step + 1, the input of the given dLLM is no longer the full sequence yt+1, but instead subset of it: t+1 = {yi t+1 }. (5) To further derive , at each decoding step t, we first collect the attention scores A(l) RHy from each layer {1, . . . , }, where and denote the number of attention heads and layers. We then average the resulting attention scores across all heads to obtain A(l) and expand A(l) into full-sized attention matrix E(l) RLL as follows: E(l) i,: = (cid:40) A(l) ei i,: if U, otherwise, (6) where ei is the one-hot vector with value of 1 at position i. Following (Abnar & Zuidema, 2020), we further define the per-layer transition matrix (l) by combining the expanded attention matrix E(l) with the residual connection (i.e., an identity matrix I) and applying row-wise normalization: (l) = normalizerow-sum-to-1(E(l) + I). (7) The cumulative attention rollout matrix is then iteratively computed, starting with (0) = I: (l) = (l) (l1). (8) The final rollout matrix (N ) captures the end-to-end influence between all token pairs. To quantify the overall contribution of each token, we further derive an influence score cj for each token by summing the columns of (N ) as follows: i=1 Finally, we sort tokens according to their influence scores cj and directly select the indices of the smallest set whose cumulative probability exceeds the predefined threshold p, thus forming . (cid:88)L cj = (N ) ij . (9) 7 Table 1: Comprehensive evaluation results on LLaDA-Inst (Nie et al., 2025) and Dream-Inst (Ye et al., 2025). Bold numbers indicate the best results and green texts denote the speedup ratios. Dataset Method LLaDA-Inst Dream-Inst Throughput Latency(s) Score Throughput Latency(s) Score GSM8K 4-shot Gen. Len. = MBPP 3-shot Gen. Len. = 512 HumanEval 0-shot Gen. Len. = 512 Math-500 4-shot Gen. Len. = 256 AVG Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache 2.77 (1.0) 8.29 (3.0) 9.64 (3.5) 8.56 (3.1) 2.48 (1.0) 6.97 (2.8) 6.80 (2.7) 8.67 (3.5) 4.99 (1.0) 8.67 (1.7) 7.90 (1.6) 14.00 (2.8) 3.08 (1.0) 6.71 (2.2) 10.61 (3.4) 12.02 (3.9) 3.33 (1.0) 7.66 (2.3) 8.74 (2.6) 10.81 (3.2) 110.26 30.34 26.15 22.41 199.90 71.79 73.27 43.86 105.76 57.48 63.12 35.44 82.51 37.84 23.79 20. 124.61 49.36 46.58 30.48 77.6 76.8 77.0 79.2 14.4 12.8 13.8 12.4 45.1 44.5 43.9 48.2 38.4 38.2 38.0 37.9 43.9 43.1 43.2 44. 2.62 (1.0) 7.50 (2.9) 10.12 (3.9) 12.25 (4.7) 2.73 (1.0) 7.07 (2.6) 7.29 (2.7) 12.47 (4.6) 4.39 (1.0) 5.35 (1.2) 7.89 (1.8) 14.06 (3.2) 3.51 (1.0) 7.19 (2.0) 10.72 (3.1) 13.80 (3.9) 3.31 (1.0) 6.78 (2.0) 9.01 (2.7) 13.15 (4.0) 85.94 33.75 24.88 21. 182.78 71.13 69.47 40.32 114.86 94.33 63.84 36.61 71.05 35.36 23.52 18.80 113.66 58.64 45.43 29.27 76.7 74.6 77.0 78.2 52.0 52.4 52.0 58. 56.7 56.5 56.1 61.6 45.2 44.2 44.4 44.6 57.6 56.9 57.4 60."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Models, datasets, metrics and hardware. Following recent conventions (Wu et al., 2025), we evaluate our d2Cache on the Base and Instruct variants of two representative dLLMs (i.e., LLaDA8B (Nie et al., 2025) and Dream-v0-7B (Ye et al., 2025)), which are denoted as LLaDA-Base/Inst and Dream-Base/Inst, respectively. Following Fast-dLLMWu et al. (2025), we evaluate our d2Cache on four benchmarks, including GSM8K (Cobbe et al., 2021), MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021), and Math-500 (Lightman et al., 2023), to assess performance across diverse reasoning and code generation tasks. The performance is reported in terms of task accuracy, which is evaluated using the lm-eval-harness framework (Gao et al., 2024). For fair comparisons, we report both inference throughput and latency, where throughput denotes the average number of tokens generated per second and latency denotes the average inference time per sample. All experiments are performed on NVIDIA 3090 24GB GPUs. Baselines. We consider three baselines, including Vanilla and two representative approximate KV cache methods (i.e., dLLM-Cache Liu et al. (2025) and Fast-dLLM Wu et al. (2025)). For Vanilla, at each decoding step, the masked position with the highest confidence is replaced with its predicted token. For dLLM-Cache and Fast-dLLM, we employ their default configurations as reported in (Liu et al., 2025; Wu et al., 2025). More details are provided in Section of the Appendix. Implementation details. Unless otherwise specified, the standard deviation σ of the Gaussian function in Equation (3) is set to 10.0, the number of masked tokens selected per step is fixed at 32, the cumulative probability threshold is set to 0.1, and decoding is performed under the certainty prior."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "The evaluation results on LLaDA-Inst and Dream-Inst are summarized in Table 1. Additional evaluation results on LLaDA-Base and Dream-Base can be found in Section C.1 of the Appendix (see Table 4). Notably, we observe that d2Cache achieves the best overall performance on average across four benchmarks, which delivers the highest throughput, the lowest latency, and the best score, consistently outperforming Vanilla, dLLM-Cache (Liu et al., 2025), and Fast-dLLM (Wu et al., 2025). Across all models and datasets, our d2Cache obtains an average 3.24.0 speedup over Vanilla. Taking Dream-Inst on GSM8K as an example, our d2Cache improves the inference throughput from 8 Table 2: Comparisons of different decoding schemes under the default NAR setting, where Conf denotes the confidence-based decoding and CP denotes our certainty prior-guided decoding. Method LLaDA-Inst Dream-Inst GSM8K MBPP HumanEval Math-500 AVG GSM8K MBPP HumanEval Math-500 AVG NAR w/ Conf NAR w/ Only CP NAR w/ d2Cache 57.5 79.0 79.2 3.0 14.0 12.4 42.1 44.5 48.2 26.4 39.0 38. 32.7 44.1 44.4 51.6 78.1 78.2 34.2 59.2 58 26.8 54.3 61.6 3.2 43.6 44.6 29.0 58.8 60. 2.62 to 12.25 tokens per second, leading to 4.7 inference speedup. More importantly, these substantial inference speedups are achieved without sacrificing accuracy, as the attainable score on average across four datasets remains comparable to or better than Vanilla. Furthermore, compared to recent representative approximate KV cache works (Wu et al., 2025; Liu et al., 2025), our d2Cache can also deliver better performance in terms of both inference efficiency and accuracy. For example, compared to Fast-dLLM, our d2Cache yields 1.5 inference speedup on Dream-Inst, while maintaining +3.2% accuracy on average across four datasets. These results clearly demonstrate the efficacy of d2Cache, which benefits from its two-stage fine-grained selection strategy."
        },
        {
            "title": "5.3 ABLATIONS AND ANALYSIS",
            "content": "Certainty prior-guided decoding vs. confidence-based decoding. As discussed in Section 4.1, d2Cache naturally delivers an alternative decoding scheme: masked tokens can be decoded according to their certainty prior rather than their prediction confidence. To evaluate its efficacy, we further compare our certainty prior-guided decoding with the standard confidence-based decoding under the default non-autoregressive (NAR) setting. As shown in Table 2, our certainty prior-guided decoding delivers more reliable performance than the confidence-based decoding under the default NAR setting. In addition, when integrated into d2Cache, our certainty prior-guided decoding achieves the best average performance across four datasets, which clearly demonstrates its superiority. Effect of σ on decoding order. We visualize the decoding step for each masked position using LLaDA-Inst on 64 randomly sampled examples from GSM8K. As shown in Figure 4, we compare NAR decoding with our certainty prior-guided decoding, where the hyperparameter σ (see Equation (3)) is set to 10, 40, and 80. We find that NAR decoding exhibits distinctive U-shaped trajectory: tokens at both sequence boundaries are first generated, which then converge towards the center (Huang et al., 2025). At the first glance, this behavior seems inconsistent with our earlier observation that dLLMs tend to prioritize decoding masked tokens adjacent to known tokens (i.e., prompt or decoding tokens). This discrepancy, however, stems from the supervised fine-tuning (SFT) of LLaDA-Inst, where the excessive number of [EOS] tokens in the training data biases the model towards producing an unnatural number of [EOS] tokens during inference (Nie et al., 2025). In contrast, our certainty prior-guided decoding yields more natural and controllable left-to-right generation order, where smaller σ makes the generation closer to autoregressive decoding. Computational redundancy during the gradual-change phase. As discussed in Section 3.2, the KV states of masked tokens evolve through three phases: gradual-change, rapid-change, and stable. It is thus natural to update the KV states of masked tokens during both the gradual-change and rapidchange phases, while caching them for reuse during the stable phase. However, our analysis shows that it is sufficient to update the KV states of masked tokens only during the rapid-change phase. To shed light on this, we conduct an ablation on Dream-Inst, in which we compare the full-update strategy (updating tokens during both the gradual-change and rapid-change phases) with our default selective-update strategy (updating tokens only during the rapid-change phase). As shown in Figure 5, our default selective-update strategy (i.e., Phase 2 Only) delivers higher inference throughput than the full-update strategy (i.e., Phases 1 & 2), while maintaining comparable or even better score. This finding reveals counterintuitive property of dLLMs: increased computation does not necessarily translate into improved performance. Instead, selectively updating only the most critical tokens can reduce computational redundancy and, in some cases, even yield better performance. 9 Figure 4: Visualization of the decoding order using certainty prior with different σ and NAR decoding. Each dot at (i, t) indicates that the token at position is decoded at step t. Figure 5: Comparisons of different update strategies, including updating tokens only during the rapid-change phase (Phase 2 Only) and updating tokens during both the gradual-change and rapidchange phases (Phases 1 & 2)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose Dual aDaptive Cache (d2Cache), training-free approximate KV cache framework for accelerating dLLM inference. Through fine-grained analysis of KV state dynamics, we uncover two key insights behind dLLMs: (1) the KV states of masked tokens exhibit substantial changes only in the few steps immediately preceding their decoding, indicating that their KV states can be reused beyond this phase; and (2) attention distributions are highly skewed towards small subset of prompt and decoded tokens, indicating that the KV states of low-attention tokens can be reused. Building on these insights, d2Cache introduces two-stage fine-grained selection strategy that adaptively identifies tokens and updates their KV states at each decoding step, whereas the KV states of the remaining tokens can be safely cached for reuse in subsequent decoding step, thus substantially reducing redundant computations and improving inference efficiency. Extensive experiments on representative dLLMs (i.e., LLaDA and Dream) demonstrate that d2Cache achieves substantial inference speedups, while also yielding consistent improvements in generation quality."
        },
        {
            "title": "REFERENCES",
            "content": "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928, 2020. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on is fail to learn is a. arXiv preprint arXiv:2309.12288, 2023. 10 Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:8633 8646, 2022. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025. Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, and Tong Xiao. Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models. arXiv preprint arXiv:2508.13021, 2025. Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, and Lei Chen. survey on large language model acceleration based on kv cache management. arXiv preprint arXiv:2412.19442, 2024. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. 11 Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Vaishnavh Nagarajan, Chen Henry Wu, Charles Ding, and Aditi Raghunathan. Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction. arXiv preprint arXiv:2504.15266, 2025. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM computing surveys, 56(4):139, 2023. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025."
        },
        {
            "title": "A RELATIONSHIPS WITH CONCURRENT WORKS",
            "content": "Figure 6: Illustration of existing approximate KV cache works. (a) In Fast-dLLM, the tokens of the current block and all subsequent blocks are recomputed. Once block has been fully decoded, the KV cache at all positions is refreshed. (b) In dLLM-Cache, the prompt and response update their corresponding segment cache at intervals of Kp and Kr steps, respectively. During steps when the response is not updated, subset of response tokens is still updated in each layer. We note two concurrent works on approximate KV cache for dLLMs, including dLLM-Cache (Liu et al., 2025) and Fast-dLLM (Wu et al., 2025). While both share the same motivations of accelerating dLLM inference through approximate KV cache, our d2Cache is fundamentally different. First and foremost, as shown in Figure 6, dLLM-Cache and Fast-dLLM both operate at the coarsegrained segment level, which partition the input sequence into multiple segments and apply different KV state updates to each segment. For instance, dLLM-Cache divides the input sequence into two segmentsprompt and responseand updates their KV states at different frequencies. Similarly, Fast-dLLM relies on block-wise semi-autoregressive decoding, which divides the input sequence into multiple blocks (or segments) and sequentially generates these blocks from left to right with tailored KV state updates to each block. Nonetheless, due to the coarse-grained nature, dLLM-Cache and Fast-dLLM inevitably reuse KV states that should be updated or update KV states that can be reused, thus limiting the achievable inference gains. In contrast, our d2Cache operates at the fine-grained token level, which adaptively identifies tokens whose KV states should be updated at each decoding step, while caching the KV states of the remaining tokens for reuse in subsequent decoding step. Thanks to the fine-grained token selection, our d2Cache achieves significant inference speedups while maintaining strong generation quality across different tasks, compared to both dLLM-Cache and Fast-dLLM."
        },
        {
            "title": "B BASELINE HYPERPARAMETERS",
            "content": "In this section, we provide more details about the hyperparameter configurations for the baseline methods (i.e., Fast-dLLM (Wu et al., 2025) and dLLM-Cache (Liu et al., 2025)) across different models and datasets. For Fast-dLLM, we closely follow common practices in prior work and set the block size to 32 for all models (Wu et al., 2025). For dLLM-Cache, we consider its key hyperparameters Kp and Kr, where Kp denotes the prompt refresh interval and Kr denotes the response refresh interval. To ensure fair comparisons, we employ the default configurations as reported in (Liu et al., 2025), which are also summarized in Table 3. Furthermore, following (Nie et al., 2025), for the Instruct variants (i.e., LLaDA-Inst and Dream-Inst), all methods except d2Cache adopt block-wise semi-autoregressive (semi-AR) decoding with block size of 32, whereas the Base variants (i.e., LLaDA-Base and Dream-Base) are evaluated in fully non-autoregressive (NAR) manner. 13 Table 3: Configurations of dLLM-Cache. Kp and Kr are the refresh interval of prompt and response."
        },
        {
            "title": "Model",
            "content": "GSM8K"
        },
        {
            "title": "HumanEval",
            "content": "Math-"
        },
        {
            "title": "MBPP",
            "content": "LLaDA-8B-Base LLaDA-8B-Instruct Dream-v0-7B-Base Dream-v0-7B-Instruct LLaDA-8B-Base LLaDA-8B-Instruct Dream-v0-7B-Base Dream-v0-7B-Instruct LLaDA-8B-Base LLaDA-8B-Instruct Dream-v0-7B-Base Dream-v0-7B-Instruct LLaDA-8B-Base LLaDA-8B-Instruct Dream-v0-7B-Base Dream-v0-7B-Instruct Kp Kr 5 25 7 50 8 100 2 25 50 25 5 50 50 100 50 25 100 25 10 5 5 1 1 8 1 4 1 4 5"
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "C.1 EXPERIMENTAL RESULTS ON THE BASE VARIANTS In addition to the Instruct variants of LLaDA-8B (Nie et al., 2025) and Dream-v0-7B (Ye et al., 2025), we also conduct experiments on their Base variants, which are denoted as LLaDA-Base and Dream-Base, respectively. As shown in Table 4, our d2Cache consistently outperforms other approximate KV cache methods in terms of both average inference efficiency and accuracy across four datasets. Furthermore, we also note that the performance of Fast-dLLM (Wu et al., 2025) is substantially lower than that of the Vanilla baseline, particularly on Dream-Base with the MBPP dataset, where it exhibits decline of about 20 points. This degradation aligns with prior findings that Base models are ill-suited for block-wise semi-autoregressive decoding (Nie et al., 2025). This further highlights the superiority of d2Cache over Fast-dLLM, as the latter heavily relies on blockwise semi-autoregressive decoding, which significantly restricts its applicability. C.2 MORE VISUALIZATION RESULTS ON ATTENTION ROLLOUT In this section, we present additional examples of attention rollout corresponding to the sample used in Figure 3. As shown in Figure 7, the attention pattern also aligns with our findings in Section 3.3. C.3 MORE VISUALIZATION RESULTS ON KV STATE DYNAMICS To further substantiate our findings in Section 3.2, we visualize additional KV state dynamics. In Figure 8, which visualizes the trajectories of the key states and value states of the same masked token during decoding, both are closely aligned in both trajectory shape and magnitude, and both exhibit the same gradualrapidstable dynamic pattern. This result suggests that, for both key states and value states, it is sufficient to update them only during the rapid-change phase, where these KV states can be safely cached for reuse during the other two phases. We hypothesize that this rapid change arises because tokens are particularly sensitive to changes in their local context. Specifically, at step t, if masked token xi is located near another masked token that is decoded, then at step + 1 the embedding of xj xj changes from [MASK] to the embedding of concrete token. This provides xi with additional contextual information; the smaller the distance j, the more tightly constrained the context becomes, thereby substantially altering the models representation of xi t. These observations motivate the introduction of distance-aware decay into the certainty density, as defined in Equation (3). 14 Table 4: Comprehensive evaluation results on LLaDA-Base (Nie et al., 2025) and Dream-Base (Ye et al., 2025). Bold numbers indicate the best results and green texts denote the speedup ratios. Dataset Method LLaDA-Base Dream-Base Throughput Latency(s) Score Throughput Latency(s) Score GSM8K 4-shot Gen. Len. = 256 MBPP 3-shot Gen. Len. = 512 HumanEval 0-shot Gen. Len. = 512 Math-500 4-shot Gen. Len. = AVG Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache Vanilla + dLLM-Cache + Fast-dLLM d2Cache 2.31 (1.0) 7.72 (3.3) 7.62 (3.3) 11.25 (4.9) 2.52 (1.0) 6.52 (2.6) 5.11 (2.0) 8.62 (3.4) 5.02 (1.0) 9.04 (1.8) 5.78 (1.2) 14.36 (2.9) 3.14 (1.0) 9.83 (3.1) 8.20 (2.6) 10.80 (3.4) 3.25 (1.0) 8.28 (2.5) 6.68 (2.1) 11.26 (3.5) 112.39 33.30 33.17 22. 195.59 77.20 98.77 43.41 100.54 55.60 87.65 35.60 80.44 25.94 30.76 20.13 122.24 48.01 62.59 30.43 70.4 69.3 66.7 72.1 39.2 38.6 39.0 38. 32.3 31.7 32.9 33.5 32.2 29.6 29.0 30.4 43.5 42.3 41.9 43.5 2.67 (1.0) 9.28 (3.5) 8.36 (3.1) 12.37 (4.6) 2.81 (1.0) 7.73 (2.8) 5.30 (1.9) 12.67 (4.5) 5.45 (1.0) 5.47 (1.0) 5.72 (1.0) 14.36 (2.6) 3.55 (1.0) 9.70 (2.7) 8.74 (2.5) 13.86 (3.9) 3.62 (1.0) 8.06 (2.2) 7.03 (1.9) 13.32 (3.7) 96.29 27.88 30.14 21.74 177.14 64.75 95.30 40.10 92.11 91.72 88.02 37.18 71.54 26.08 28.83 18. 109.27 52.61 60.57 29.41 71.7 64.7 69.5 73.5 51.4 49.8 31.2 53.6 51.2 51.8 53.7 61.0 39.0 35.2 38.0 39.6 53.3 50.4 48.1 56. C.4 ABLATION ON HYPERPARAMETERS To determine the optimal hyperparameters, we conduct systematic experiments on Dream-Inst using the HumanEval dataset. As shown in Figure 9 (a), the number of masked tokens selected per step has substantial influence on performance. While larger values of generally improve performance, increasing beyond 32 yields diminishing returns and may even result in slight degradation. This suggests that = 32 provides the most robust performance across different values of p. In parallel, the cumulative probability threshold controls the proportion of probability mass retained among the remaining tokens at each step and primarily affects throughput. The performance for = 0.10 and = 0.20 is nearly indistinguishable, indicating that the marginal benefit of increasing becomes negligible beyond 0.1. Considering the trade-off between efficiency and accuracy, we therefore set = 0.1 and = 32 throughout our experiments unless explicitly specified. The standard deviation σ of the Gaussian function in Equation (3) determines the range considered during certainty prior selection. smaller σ biases the selection of masked tokens for updating and decoding towards those closer to the prompt or already decoded tokens, whereas larger σ incorporates broader local context. While the influence of σ on decoding order has been discussed in Section 5.3, we further investigate its impact on performance using Dream-Inst. As shown in Figure 9 (b), when σ = 1, Dream-Inst tends to decode in an approximately autoregressive manner, leading to degraded performance. Conversely, when σ = 80, resembling non-autoregressive decoding, Dream-Inst prematurely generates the [EOS] token, which also degrades performance. These observations are consistent with the properties of LLaDA (Nie et al., 2025). We find that σ = 10 yields the best performance, enabling flexible, quasi left-to-right decoding order. 15 (a) (b) (c) (d) Figure 7: Visualization of attention rollout on LLaDA-Inst (Nie et al., 2025) with GSM8K, which is generated using the same sample and configuration as in Figure 3. 16 (a) Key state trajectory for the 91st masked token. (b) Value state trajectory for the 91st masked token. (c) Key state trajectory for the 186th masked token. (d) Value state trajectory for the 186th masked token. Figure 8: Visualization of PCA-projected trajectories of LLaDA-Inst on GSM8K, which are generated using the same sample and configuration as in Figure 2 (a). Figure 9: Ablation studies on hyperparameters (a) and k, and (b) σ. All experiments are conducted on Dream-Inst with the HumanEval dataset."
        }
    ],
    "affiliations": [
        "Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education",
        "Qiyuan Tech",
        "Southeast University"
    ]
}