{
    "paper_title": "GUI-360$^\\circ$: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
    "authors": [
        "Jian Mu",
        "Chaoyun Zhang",
        "Chiming Ni",
        "Lu Wang",
        "Bo Qiao",
        "Kartik Mathur",
        "Qianhui Wu",
        "Yuhang Xie",
        "Xiaojun Ma",
        "Mengyu Zhou",
        "Si Qin",
        "Liqun Li",
        "Yu Kang",
        "Minghua Ma",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction. GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360$^\\circ$ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs. The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 2 7 0 3 4 0 . 1 1 5 2 : r Preprint. GUI-360: COMPREHENSIVE DATASET AND BENCHMARK FOR COMPUTER-USING AGENTS Jian Mu1, Chaoyun Zhang2, Chiming Ni3, Lu Wang2, Bo Qiao2, Kartik Mathur2, Qianhui Wu2, Yuhang Xie4, Xiaojun Ma2, Mengyu Zhou2, Si Qin2, Liqun Li2, Yu Kang2, Minghua Ma2, Qingwei Lin2, Saravan Rajmohan2, Dongmei Zhang2 1Nanjing University 2Microsoft 3ZJU-UIUC 4Peking University"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce GUI-360, large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multimodal trajectories, and the absence of unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction. GUI-360 addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and hybrid GUI+API action space that reflects modern agent designs. Benchmarking stateof-the-art visionlanguage models on GUI-360 reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360 and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs. The full dataset has been made public on https://huggingface.co/ datasets/vyokky/GUI-360."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in visionlanguage and large language models have sparked rapid progress toward intelligent agents that automate tasks inside digital environments Zhang et al. (2024a). Such agents interpret natural-language requests, perceive screen content via pixels and/or accessibility (a11y) metadata, plan sequences of operations, and then either navigate the GUI or invoke APIs to complete tasks on users behalf Zhang et al.. They can dramatically reduce user effort for routine productivity tasks and enable novel humancomputer workflows. However, realizing this potential requires two tightly coupled capabilities: reliable screen understanding (element grounding or screen parsing) Cheng et al. (2024); Lu et al. (2024); Zheng et al. (2025b) and robust action planning (stepwise action prediction and execution) Zhang et al. (2024b). Both capabilities in turn depend critically on large, diverse, and high-quality datasets grounded in realistic execution contexts Wang et al. (2024c). We focus on concrete, under-served class of agents called computer-using agents (CUAs) OpenAI (2025a): agents whose primary operating domain is the desktop computer environment. Desktop Corresponding author. 1 Preprint. Figure 1: An illustration of GUI Grounding, Screen Parsing and Action Prediction tasks included in our GUI-360. CUAs differ from web Zheng et al. (2024) or mobile Wang et al. (2024b) agents in several important ways. Desktop applications present very high-resolution mixed-content screens, heterogeneous widgets and document formats, arbitrary window layouts (multi-window and multi-monitor settings), and frequently lack standardized accessibility metadata Zhang et al. (2025). Tasks on desktop systems are also often longer-horizon and more compositionally structured (e.g., find table in document, transform cells in Excel, then copy results into PowerPoint). These characteristics make desktop CUAs substantially more challenging to train and evaluate than their web or mobile counterparts Zhang et al. (2024a). Despite growing interest, progress on desktop CUAs is hampered by three persistent gaps. First, there is scarcity of real-world task collections: existing datasets are often handcrafted or LLMsynthesized Sun et al. (2024), which limits their ability to capture the frequency and diversity of authentic user intents. Second, automated pipelines for data collection and annotation are largely missing Nayak et al. (2025). Manual execution and labeling of desktop interactions is expensive, error-prone, and difficult to scale, making it impractical to generate multi-modal execution trajectories at scale. Third, no unified, large-scale benchmark exists that supports the breadth of tasks needed for comprehensive evaluation Nayak et al. (2025); Li et al. (2025). Prior datasets typically focus on single aspectsuch as element detection, single modality, or narrow application subset, rather than jointly enabling GUI grounding, screen parsing, and action prediction with execution traces and failure cases. To fill these gaps, we introduce GUI-360, comprehensive dataset and benchmark suite for desktop computer-using agents. GUI-360 is built around three design goals: realism, scalability, and task breadth. Concretely, the dataset and benchmark provide the following properties: 1. Real-world, high-frequency queries. Task intents are harvested from authentic sources (search logs, community forums, and in-app help content) to reflect common user needs and highfrequency workflows. 2. Automated collection and annotation pipeline. We present an LLM-augmented, largely automated pipeline for template construction, task instantiation, batched execution, and LLM-driven quality filtering, minimizing human intervention while preserving execution realism. 3. Comprehensive multi-modal annotations. Each example contains full-resolution screenshots, accessibility metadata, natural-language goals, intermediate agent thoughts, and stepwise action trajectories (including successful and failed runs). These assets jointly support three canonical tasks: (a) GUI Grounding: given step-level plan, predict the screen coordinates or UI element to interact with; (b) Screen Parsing: given screenshot, enumerate the set of interactable UI elements and their properties; and (c) Action Prediction: given the current state and user intent, predict the next action (click/type/select/API call). We show an illustration of included three tasks in Figure 1. 4. Large scale and practical coverage. GUI-360 contains over 1.2M executed action steps spanning thousands of trajectories across widely used Windows office applications (Word, Excel, 2 Preprint. Table 1: Comparison of GUI datasets across dimensions. checkmark () indicates support, while cross () indicates not supported. Dataset ScreenSpot ScreenSpot-Pro DeskVision UI-Vision GUI-360 Query Source Humandesigned Humandesigned Online Humandesigned In-App/Online/ Search Task Samples Data Collection Action Reasoning A11y Info. Fail Case Grounding Parsing Action Pred. GUI API 1,200+ Human N/A N/A 1,581 Human N/A N/A N/A N/A N/A 54,855 Auto. N/A N/A 8,227 Human 1,225, Auto. PowerPoint), and is designed so that the template set and pipeline can be extended to other desktop applications. 5. Hybrid GUI+API action space. To reflect modern CUA architectures Zhang et al. (2025); Zhang et al., our action space mixes direct GUI operations with higher-level API calls where available, enabling evaluation of both perception-driven and API-assisted strategies. Table 1 compares GUI-360 with existing GUI datasets across key dimensions. Unlike prior efforts that focus narrowly on grounding or small-scale scripted tasks, GUI-360 provides full coverage of grounding, parsing, and action prediction with large-scale, automatically collected corpus. Notably, it is the first dataset to include accessibility information, reasoning supervision, and both GUIand API-level actions, making it uniquely comprehensive benchmark for CUA research. To assess GUI-360 utility, we benchmark state-of-the-art visionlanguage models, asking two questions: (i) how well do existing models generalize to realistic desktop CUAs without adaptation, and (ii) how much can fine-tuning on GUI-360 bridge the gap? Our results reveal consistent patterns: off-the-shelf models struggle with grounding in heterogeneous layouts and often fail in stepwise action prediction, leading to cascading errors. Training on GUI-360 yields significant gains. These findings underscore both the limitations of current models and the value of GUI-360 as scalable, challenging benchmark for driving progress in CUAs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "GUI and Computer-Using Agents LLMs have enabled agents Zhang et al. (2024a) that automate tasks across web Zheng et al. (2024; 2025a), mobile Wang et al. (2024b;a; 2025), and desktop platforms Zhang et al. (2024b; 2025); Qin et al. (2025). Desktop environments are particularly challenging for the CUAs due to high-resolution displays and complex layouts. Such agent typically rely on accessibility metadata or visual screen understanding Gur et al. (2023); Xie et al. (2023). When accessibility information is missing, they resort to screen parsing and visual grounding, as in SeeClick Cheng et al. (2024), OmniParser Lu et al. (2024), and GUI-Actor Wu et al. (2025). UFO Zhang et al. (2024b; 2025) pioneered hybrid approaches that combine accessibility with screen parsing for more reliable control detection, while recent efforts further integrate APIs for efficiency Zhang et al.. Ultimately, progress depends on large-scale data to support robust screen understanding and tool use. GUI-360 aims to fill this gap by providing comprehensive dataset and benchmark for GUI grounding, action prediction, and screen parsing. Data and Benchmarks for CUA key obstacle in building effective CUAS lies in obtaining high-quality training data and reliable benchmarks. While such resources are increasingly available for web Deng et al. (2023); Zhou et al. (2023) and mobile domains Rawles et al. (2023; 2024), the desktop setting remains comparatively underexplored despite its greater complexity. Several efforts have emerged in this space. UI-Vision Nayak et al. (2025) provides human-annotated desktop benchmark with bounding boxes, UI labels, and action trajectories. DeskVision Xu et al. (2025) 3 Preprint. Figure 2: The data collection pipeline for GUI-360. introduces cross-OS dataset for desktop region captioning. OfficeBench Wang et al. (2024d) offers live testing environment for office applications through handcrafted cases and oracle evaluation. However, these datasets and benchmarks either require costly human annotation of trajectories and GUI states, or cover only narrow subsets of tasks, limiting their scalability and diversity. Our work, GUI-360, addresses this gap by introducing an automated data collection pipeline that generates large-scale resources for GUI grounding, screen parsing, and action prediction. This design makes GUI-360 the most comprehensive and scalable datasetbenchmark suite to date for training and evaluating GUI agents."
        },
        {
            "title": "3 THE COLLECTION OF GUI-360◦",
            "content": "The construction of GUI-360 follows three-stage pipeline designed to maximize scalability while minimizing human effort, as shown in Figure 2. 1. Query Acquisition. We begin by collecting real-world user queries from reliable sources and augmenting them with synthetic variants to ensure coverage of diverse and realistic task intents. 2. Automatic Trajectory Collection. We design specialized CUA named TrajAgent to execute tasks in an automatic, consistent and high-quality manner. The TrajAgent generates and collect detailed trajectories that jointly support GUI grounding, screen parsing, and action prediction, enabling multi-task supervision from single execution. 3. Evaluation and Post-processing. Finally, we apply automated evaluators and systematic postprocessing to verify correctness, filter noise, and enhance overall data quality. Together, these stages yield scalable and comprehensive pipeline for constructing GUI-360. By integrating real-world task diversity, automated trajectory collection, and rigorous quality control, our approach provides unified dataset and benchmark that supports multiple core capabilities of GUI agents. 3.1 QUERY ACQUISITION High-quality and actionable user queries are the foundation of reliable dataset, as they determine both task realism and execution fidelity Xu et al. (2025). To capture queries that faithfully reflect real-world user needs, we design dedicated four-stage acquisition pipeline (Figure 1): (i) Prototypical Query Sourcing. We gather prototypical, high-frequency queries about software usage from diverse and trustworthy real-world sources, ensuring coverage of practical user intents. (ii) Environment Template Construction. Based on the collected queries, we design template software environments that can realistically support their execution. These templates encode the minimal yet sufficient context needed for task instantiation. (iii) Task Instantiation. Each query is grounded into suitable environment by matching it to the best-fitting template. We then rephrase the query into concrete, executable form tailored to the selected environment. (iv) Quality Filtering. Finally, we apply post-processing to discard low-quality or ambiguous queries, retaining only those 4 Preprint. Table 2: Raw query statistics across applications and sources. Source Word Excel PowerPoint In-App Online Search 274 1,914 25,715 159 3,393 25,000 Total 27, 28,552 316 1,701 19,681 21,698 that are actionable and fully grounded in executable environments. This four-stage pipeline ensures that the resulting queries are both realistic and executable, providing strong basis for collecting high-quality trajectories in later stages of the GUI-360 pipeline. Prototypical Query Sourcing. Most existing CUA datasets rely heavily on human-crafted instructions or LLM-generated queries, which often fail to reflect real-world usage patterns or capture high-frequency tasks. To better ground our dataset in authentic user behavior, we source prototypical task descriptions from three complementary channels: 1. In-App Help Content (In-APP): We mine built-in help documentation and tutorials of software applications, which provide standardized, well-structured task descriptions covering core functionalities. 2. Online Websites (Online): We crawl forums, Q&A platforms, and community-driven websites to obtain diverse and authentic task descriptions contributed by real users. 3. Search Queries (Search): We extract queries from search engines that mention or relate to the target software. These queries capture pressing, high-frequency user needs and common practical challenges. We show the queries distribution from different source in Table 2. By combining these sources, we ensure that the collected queries balance authenticity (from search and community data) with completeness (from in-app documentation), resulting in broad and realistic coverage of user intents. Environment Template Construction. Each user query must be grounded in suitable environment that provides the necessary context for agent execution. For example, task such as make the first line of text bold in Word requires document containing editable text; without such setup, the query is not actionable. Naively creating bespoke environment for every query would be prohibitively expensive and redundant, since many queries share common contextual requirements. To address this, we introduce an environment template construction process that systematically amortizes environment setup across queries. Specifically, we leverage GPT to analyze each query and extract its underlying requirements (e.g., the presence of text, table, or an image). Queries with similar requirements are clustered and abstracted into environment template descriptions, which specify the minimal context needed for execution. We then manually instantiate curated set of high-frequency templates from these descriptions. In practice, we design 30 templates for Word, 30 for Excel, and 6 for PowerPoint. Despite this relatively small set of 66 templates, the coverage is substantial: single template can accommodate diverse content and scenarios, enabling the collection to support approximately 95% of prototypical queries. This template-driven strategy dramatically reduces human effort by avoiding per-query environment creation, and ensure consistent and reproducible environments, enabling robust execution and trajectory collection at scale. Task Instantiation. Prototypical queries collected from real-world sources are often vague and underspecified (e.g., How to make text bold?). To be executable by an agent, each query must be grounded in specific environment and reformulated into an actionable instruction (e.g., Make the phrase Hello World bold in the Word document). We refer to this process as task instantiation. To systematically achieve this, we design an automated two-stage pipeline: 1. Template Matching. For each query, we retrieve candidate environment templates. Each template is defined by set of contextual constraints (e.g., Word document containing text, or an 5 Preprint. Excel sheet with numerical entries). Using an LLM, we provide the query together with textual descriptions and screenshots of all available templates. The LLM identifies the most suitable template by reasoning over query requirements and environment affordances. Queries that fail to match any template are discarded. 2. Query Concretization. Once template is selected, the LLM rephrases the vague query into fully instantiated one, grounded in the chosen environment. For example, the generic request make text bold is concretized as make the text Hello World bold in the current document. This ensures the query is unambiguous, executable, and directly tied to well-defined environment. This pipeline is fully automated and enforces that all instantiated tasks are (i) actionable, by guaranteeing an associated environment exists, (ii) concrete, by eliminating ambiguity in the query, and (iii) scalable, as the process can be applied to thousands of queries with minimal human intervention. In practice, this approach yields large collection of executable tasks while maintaining high fidelity to real-world user intent. Quality Filtering. Although task instantiation produces large set of grounded queries, not all of them are suitable for reliable agent training. Many instantiated tasks may suffer from contextual mismatches, external dependencies, or inherent ambiguities. To ensure robustness, we design task filtering pipeline that employs an LLM as an automatic quality gate. Concretely, the LLM-based judge evaluates each candidate task against set of well-defined constraints. Tasks are discarded if they: 1. Non-Executable (NONEXEC): Inputs that do not describe concrete action are removed. This includes subjective statements, vague preferences, or general inquiries that cannot be directly executed. For instance, instructions containing words such as custom, you want, or undefined operations like edit text without specifying the target element fall under this category. 2. Cross-Application Dependency (CROSSAPP): Tasks that require interaction with applications beyond {app} are excluded. This includes operations that involve opening or manipulating content in external software (e.g., Excel, Edge, File Explorer, or system settings). Representative examples include merging files across applications, printing documents (which requires printer integration), or exporting data to third-party tool. 3. Version Management (VERCTRL): Tasks that involve checking, updating, downgrading, or modifying the version of {app} are discarded. Since version management depends heavily on system environment and external factors, these tasks are not considered executable within the scope of our benchmark. 4. Template Dependency (TPLMISS): Tasks that rely on specific document or workspace templates that are absent from the provided context are excluded. For example, instructions that assume the existence of predefined table, chart, or object not available in the given file are categorized as TPLMISS. Note that application-wide settings (e.g., enabling dark mode) do not fall under this restriction, as they do not depend on document templates. 5. Irrelevant or Invalid (INVALID): Remaining cases that do not fit into the above categories, or are otherwise infeasible due to irrelevance, ambiguity, or context mismatch, are marked as invalid. For example, task unrelated to {app} or lacking sufficient contextual information for execution would be discarded under this category. Table 3 summarizes the outcome of our LLM-based task filtering across Word, Excel, and PowerPoint. Overall, we retain 59,553 out of 79,075 candidate tasks (75.3%) as NORMAL (self-contained, concretely specified, and template-feasible); the remaining 24.7% are filtered for various reasons. Summary. The complete query acquisition pipeline transforms raw, vague user queries into curated collection of high-quality, executable tasks. Starting from diverse sources of authentic user requests, we progressively ground them into concrete environments through template construction, instantiate them into actionable forms, and finally enforce strict quality filtering. This systematic process yields dataset that is both scalable and faithful to real-world user intent, while maintaining the rigor necessary for robust agent training. 6 Preprint. Table 3: Distribution of task categories after filtering, shown as percentages of total candidate tasks for each application. Category Word Excel PowerPoint CROSSAPP 15.61% 13.27% TPLMISS 4.18% 2.93% NONEXEC 1.68% 1.56% VERCTRL 0.13% 0.17% INVALID 6.98% 2.70% NORMAL 77.02% 73.76% 12.13% 7.51% 3.65% 0.27% 1.46% 74.97% Figure 3: The overall architecture of the TrajAgent. 3.2 AUTOMATIC TRAJECTORY COLLECTION Once queries have been instantiated and grounded, the next step is to generate corresponding action plans, execution trajectories, and outcomes. Traditionally, this stage relies on human operators to perform tasks and log interactions, which is costly and difficult to scale. To overcome this limitation, we develop specialized execution framework, TrajAgent, that automatically completes queries in batch (2) and records detailed execution data trajectories. The workflow consists of three main phases: 1. Environment Preparation. For each task, TrajAgent initializes the corresponding environment template and ensures all preconditions for execution are satisfied. 2. Task Execution and Data Logging. The agent performs the query step-by-step, generating complete action trajectory while recording the full GUI state at each step. Additional relevant data, such as intermediate screenshots, control properties, and execution metadata, are also captured. 3. Environment Reset. After execution, TrajAgent closes or resets the environment to clean state, preparing it for the next task in the batch. This fully automated process ensures that every task is executed reliably, trajectories are captured consistently, and human intervention is entirely eliminated. By integrating environment preparation, execution, and logging into single pipeline, TrajAgent enables large-scale, high-fidelity trajectory collection suitable for training and benchmarking CUAs. 3.2.1 TRAJAGENT DESIGN The core requirement for large-scale trajectory collection is twofold: the executor must (i) complete instantiated queries with high success rate to maximize data efficiency, and (ii) record every element required for dataset construction (screen snapshots, accessibility metadata, action logs, 7 Preprint. Table 4: Success rate of the two-stage execution strategy across applications. Word Excel PowerPoint Total Round 1 (GPT-4o) Round 2 (GPT-4.1) 16.65% 9.27% 16.03% 18.20% Overall 30.50% 25.78% 8.00% 14.91% 21.71% 11.63% 16.38% 26.09% pre/post states, etc) with strict fidelity. To meet these requirements we design TrajAgent, an orchestrated, multi-agent execution framework that reliably completes concrete tasks and produces high-fidelity execution traces suitable for downstream training and evaluation. Architecture overview. TrajAgent follows an orchestration pattern Zhang et al. (2024b) composed of MasterAgent (MAgent), pool of ExecutionAgents (EAgent), with set of auxiliary services (Perception, Action Executor, and Recorder), as shown in Figure 3. The MAgent receives an concrete task and decomposes it into sequence of manageable subtasks (planning). Each subtask is dispatched to an available EAgent for execution. EAgents operate as lightweight workers that (a) perceive the current GUI state, (b) select or synthesize the next low-level action (click/type/select/API call), (c) execute the action via UI automation or API invocation, and (d) return observations and status back to the MAgent. The Recorder persistently logs the full GUI state and metadata before and after each action, ensuring temporally-aligned dataset. Perception. Within each EAgent, the Perception service captures full-resolution screenshot at every decision step and queries the Windows accessibility API (UI Automation) Haverty (2005) to extract list of actionable controls (name, type and exact bounding box). The accessibility-derived control metadata is rendered on the screenshot as Set-of-Mark (SoM) Yang et al. (2023). The EAgent uses both the raw screenshot and the SoM/controls for decision-making: screenshots provide visual context while accessibility metadata supplies precise semantic and locational information, reducing visual reasoning overhead. Action Executor. The Action Executor utilizes app-specific MCP servers Hou et al. (2025) to provide an extensible set of tools for the EAgent. In addition to conventional GUI actions (e.g., mouse clicks, keyboard input), our design incorporates app-level API actions, reflecting modern CUA practices. These API actions improve task efficiency and serve as reliable fallbacks when GUI interactions are prone to failure. At each step, the EAgent selects the most appropriate action based on its current observation, internal reasoning, and task plan, following the classical ReAct paradigm Yao et al. (2023). This iterative perceptionreasoningaction loop continues until the task is fully completed, ensuring both robustness and fidelity in trajectory collection. Recorder. The Recorder is responsible for collecting all multi-modal information at each execution step to construct the dataset. This includes screenshots, accessibility metadata, and agent outputs. Importantly, single execution of the agent produces data for all downstream tasks, significantly improving data efficiency. Table 5 summarizes the input and output for each task type. Accessibility-derived data ensures precise element locations and properties, while screenshots provide full visual context. Two-Stage Execution. Leveraging the components described above, queries are executed in batches within Windows Sandbox, with the EAgent automatically collecting all required data. To reduce dependency on the capabilities of single model and improve coverage, we adopt twostage execution strategy. In the first stage, GPT-4o serves as the base model to complete the queries. Any queries that fail in this stage are then re-executed using stronger model GPT-4.1. As shown in Table 4, the two-stage execution strategy substantially improves success rates compared to relying on single model. In the first round, GPT-4o captures non-trivial portion of tasks, particularly in Word. In the second round, GPT-4.1 recovers many of the failures from GPT-4o, with notable gains in Excel and PowerPoint. Together, this staged approach boosts overall completion to 26.09%, showing that cascading models of complementary strengths increases both success rate and dataset versatility, while avoiding over-reliance on single model. 8 Preprint. Table 5: Task input-output specifications for dataset collection. Task Input Output GUI Grounding Application Agents current step screenshot, the at thought Screen Parsing Application screenshot Action Prediction User query, Application Accessibility screenshot, information (optional) Operation coordinates of the target element, obtained via accessibility APIs of all List on actionable screen with name and bounding box, \"Open Menu\", e.g., \"bbox\": [12,34,56,78]} {\"name\": controls Action call, with optional metadata such as agents thought and plan 3.3 EVALUATION AND POST-PROCESSING To ensure the high quality and usability of the automatically collected trajectories, we perform three-stage post-processing procedure: (i) Trajectory Validation. Each trajectory is automatically evaluated to retain only successful and executable task executions, ensuring that downstream training and evaluation are based on realistic completions. (ii) Data Sanitization. Low-quality steps, incomplete records, or any data that fail to meet predefined quality criteria are removed. This step eliminates noise and increases the overall reliability of the dataset. (iii) Data Structuring. The cleaned trajectories are reformatted and normalized into the required structure for the 3 downstream tasks. This includes standardizing screenshot metadata, accessibility information, action calls, and annotations to create consistent, machine-readable dataset. Together, these stages guarantee that the final dataset is both high-quality and fully compatible with diverse CUA training and benchmarking pipelines. Trajectory Validation. To ensure the reliability of collected data, each trajectory undergoes automatic validation. We design an evaluation agent, EvaAgent, which leverages GPT-4.1 in an LLMas-a-judge paradigm Gu et al. (2024). EvaAgent inspects the trajectory step by step, including the screenshot, accessibility information, executed actions, intermediate thoughts, and the final application state. Following prior work Wang et al. (2024c), it employs chain-of-thought style reasoning process to decompose the query into several fine-grained evaluation criteria. trajectory is marked as successful only if all criteria are satisfied, thereby enforcing stricter notion of task completion. To assess its reliability, we conducted small-scale study on 100 randomly sampled trajectories. EvaAgents judgments achieved 86% agreement with human annotators, demonstrating that it provides sufficiently accurate and scalable validation. Compared to hard-coded scripts or brittle oracle rules, this approach offers greater flexibility across diverse applications and enables rapid filtering of high-quality, successful trajectories at scale. Data Sanitization. After validation, we perform final cleaning step to ensure completeness and consistency of the collected trajectories. This involves removing any step that lacks an executed action, screenshot, or essential metadata required for downstream tasks. Such sanitization further improves the overall data quality and ensures that only fully executable, well-documented steps are retained for model training and evaluation. Data Structuring. Finally, we transform the sanitized data into standardized JSON format tailored for model consumption, following the inputoutput specifications summarized in Table 5. For the Action Prediction task, we provide two input modalities: visual-only and visual+a11y: Visual-only: The model receives raw screenshots as input. Interaction-related arguments (e.g., click positions) are represented as the absolute coordinates of the center of the corresponding bounding box. 9 Preprint. Table 6: Training and test dataset statistics across domains (Word, Excel, PowerPoint). GUI-360-Train Word Excel PowerPoint Total Total Trajectories Total Steps Average Steps per Trajectory Steps for Grounding Tasks Steps for Screen Parsing Steps for Action Prediction Total Elements Total Images Average Elements per Image GUI Action Rate (%) API Action Rate (%) 5,633 41,742 7.41 30,695 41,742 41,742 3,270,104 83,484 78.34 76.1 23.9 4,348 29,363 6.75 22,319 29,363 29,363 12,211,852 58,726 415.89 80.7 19.3 3,769 34,263 9.09 26,473 34,263 34,263 2,186,738 68,526 63.82 87.4 12.6 13,750 105,368 7.66 79,487 105,368 105,368 17,668,694 210,736 167.69 81.0 19.0 Total Trajectories Total Steps Average Steps per Trajectory Steps for Grounding Tasks Steps for Screen Parsing Steps for Action Prediction Total Elements Total Images Average Elements per Image GUI Action Rate (%) API Action Rate (%) GUI-360-Bench Word 1,409 10,597 7.52 7,784 10,597 10,597 839,273 21,194 79.20 76.2 23.8 Excel PowerPoint Total 1,087 7,175 6.60 5,444 7,175 7,175 2,940,016 14,350 409.76 80.2 19. 943 8,512 9.03 6,552 8,512 8,512 545,328 17,024 64.07 87.5 12.5 3,439 26,284 7.64 19,780 26,284 26,284 4,324,617 52,568 164.53 81.0 19.0 Visual+a11y: The model additionally receives the list of actionable elements from the accessibility API, which are also annotated on the screenshot using the Set-of-Mark (SoM) representation. Interaction arguments are expressed as element ID and name, chosen from the provided element list. This reduces the need for explicit coordinate prediction and lowers the visual grounding overhead. For evaluation, we partition the trajectories into training (80%) and benchmark (GUI-360-Bench, 20%) splits. All three tasksGUI Grounding, Screen Parsing, and Action Predictionshare the same data partition to maintain consistency across evaluations. 3.4 GUI-360 STATISTICS Following the pipeline described above, we construct comprehensive dataset, GUI-360, for training across three core GUI tasks, and companion benchmark, GUI-360-Bench, for systematic evaluation. Scale. Table 6 summarizes the statistics of GUI-360-Train (80%) and GUI-360-Bench (20%). In total, GUI-360 contains 13,750 trajectories with over 105k steps, averaging 7.66 steps per trajectory. The dataset also provides 210k screenshots paired with 17.7M annotated UI elements, yielding rich multimodal supervision at both the visual and accessibility levels. For evaluation, GUI360-Bench adds further 3,439 trajectories and 26k steps, maintaining similar distribution of average step length and GUI/API action rates. In addition, we include 62,170 trajectories comprising 1,093,525 steps for failure cases, which can serve as valuable signals for reinforcement learning, similar to the approach in Wang et al. (2024c). These failure cases capture challenging or error-prone situations that models often struggle with, providing rich supervision for improving robustness and reliability. 10 Preprint. This unprecedented scale, both in interaction traces and annotated elements, makes GUI-360 one of the largest and most comprehensive resources for GUI learningsufficiently large to train highcapacity models and to rigorously benchmark their generalization in realistic desktop environments. Diversity. Beyond scale, GUI-360 emphasizes breadth and functional diversity. Using GPT-4o, we classified user queries into finegrained categories, as shown in Figure 4. The corpus spans Word (41.0%), Excel (31.6%), and PowerPoint (27.4%), each with rich internal coverage. Word tasks range from text formatting to layout and review, Excel covers data entry, formulas, and visualization, while PowerPoint emphasizes content editing, design, and transitions. This balance ensures exposure to both frequent operations and rarer, long-tail behaviors. As result, models trained on GUI360 are encouraged to generalize across routine workflows while remaining robust to less common but practically important tasks, making it comprehensive and challenging benchmark for desktop CUA research."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "Figure 4: Dataset Composition. For each app, tasks are divided into six categories according to their core operational intent. Our experimental evaluation proceeds in two stages. First, we perform an out-of-the-box evaluation of several state-of-the-art vision language and agent models on GUI-360Bench to measure their zero-/few-shot capabilities and to diagnose common failure modes. Second, we investigate how targeted training on GUI360 (supervised fine-tuning and policy optimization) improves model performance and robustness. All experiments use the same data partitioning: the training split described in Section 3.3 (80%) and the held-out benchmark GUI-360-Bench (20%). 4.1 GUI GROUNDING We begin with the GUI grounding task: given natural-language task description and the current GUI state, the model must predict the screen location for the next interaction (represented as 2D coordinate). Predicted coordinates are evaluated against the accessibility-derived bounding box of the target element. Baselines. We evaluate mix of general-purpose and domain-specialized models. The generalpurpose VLM/LLM baselines are GPT-4o Hurst et al. (2024), GPT-4.1 OpenAI (2025b), o3 OpenAI (2025d), and GPT-5 OpenAI (2025c). We also include several open-source and grounding-focused models: Qwen-VL-2.5 (7B) Bai et al. (2025), UGround-7B Gou et al. (2024), Aguvis-7B Xu et al. (2024), UI-TARS-1.5 (7B) Qin et al. (2025), and GUI-Actor (7B) Wu et al. (2025). Finally, we report results for supervised fine-tuned variants (Qwen-2.5 7B-SFT, UI-TARS-1.5 7B-SFT) that are trained on the GUI-360 training split. Performance Metrics. The primary evaluation metric for GUI grounding is accuracy, defined as the proportion of predictions where the predicted coordinate ˆci lies within the bounding box of the corresponding ground-truth target element bi. Formally, Acc = 1 (cid:88) i=1 {ˆci bi}, where is the total number of test cases and {} is the indicator function. 11 Preprint. Table 7: Performance of different models on the GUI grounding task across applications. Model Word Excel PowerPoint Overall 15.22% 5.08% GPT-4o 17.30% 7.48% GPT-4.1 36.62% 19.44% o3 34.52% 20.31% GPT-5 38.09% 26.76% Qwen-2.5-VL-7B 57.44% 43.09% UGround-7B 53.14% 37.69% Aguvis-7B 63.50% 58.61% UI-TARS-1.5 7B GUI-Actor 7B 54.84% 45.84% Qwen-2.5-VL-7B-SFT 84.11% 79.20% 84.73% 79.84% UI-TARS-1.5 7B-SFT 5.41% 7.01% 31.06% 17.36% 41.55% 59.53% 59.57% 64.21% 62.68% 82.84% 81.98% 9.38% 11.41% 29.96% 25.34% 35.78% 53.85% 50.50% 62.27% 54.50% 82.30% 82.49% We report accuracy separately for each application domain (Word, Excel, and PowerPoint) as well as an overall aggregated score across all benchmark examples. To highlight the effect of task-specific adaptation, we evaluate two settings: (i) the zero-shot performance of each baseline model directly on GUI-360-Bench, and (ii) the performance after supervised fine-tuning (SFT) on the GUI-360 training set. Performance Comparison. Table 7 reports the evaluation results of different models on GUI360-bench for the GUI grounding task. We observe that general-purpose GPT models (e.g., GPT-4o and GPT-4.1) achieve only modest performance, with overall accuracy below 12%. More advanced general models such as GPT-o3 and GPT-5 show improvements (2030%), yet still struggle with precise GUI grounding. Domain-specific pretraining brings substantial gains: models like UGround-7B and GUI-Actor 7B surpass 50%, demonstrating the effectiveness of grounding-oriented pretraining. Finally, supervised fine-tuning (SFT) on GUI-360 yields the largest performance leap, with Qwen2.5 7B-SFT and UI-TARS-1.5 7B-SFT achieving over 82% accuracy across applications. This progression clearly highlights the value of GUI-360 for both training and benchmarking: it not only reveals the limitations of general-purpose models on GUI tasks but also provides high-quality training data that enables fine-tuned models to achieve state-of-the-art performance. 4.2 SCREEN PARSING The screen parsing task requires model to take clean screenshot as input and output the complete set of interactable UI elements on the screen. Each predicted element consists of semantic name (e.g., Start Menu) and bounding box. This task is challenging due to heterogeneous widgets, dense layouts, occlusions, and mixed content (text, icons, images). Baselines. We evaluate both general-purpose VLMs and specialized screen-parsing models. General models include GPT-4o Hurst et al. (2024), GPT-4.1 OpenAI (2025b), GPT-o3 OpenAI (2025d), GPT-5 OpenAI (2025c), and Qwen-VL-2.5 (7B) Bai et al. (2025). Specialized baselines include OmniParser and OmniParser-v2 Lu et al. (2024). Evaluation metrics. We measure parsing quality along three complementary axes: (i) element detection accuracy (precision / recall / F1), (ii) localization quality (mean IoU on matched pairs), and (iii) semantic name accuracy (average text embedding similarity on matched pairs). All metrics are computed per image and then averaged across the benchmark (macro-average). Let be the ground-truth set of elements for an image and the predicted set. We obtain one-toone matched set by performing greedy bipartite matching sorted by descending IoU, and keeping only pairs with IoU > 0.5. For predicted box and ground-truth box we use the standard intersection-over-union: IoU(p, g) = area(p g) area(p g) . 12 Preprint. For each image define: Precisioni = Mi Pi , Recalli = Mi Gi , F1i = 2 Precisioni Recalli Precisioni + Recalli , where denotes set cardinality. The reported precision, recall, and F1 are the macro-averages across images: Precision = 1 (cid:88) i=1 Precisioni, Recall = 1 (cid:88) i=1 Recalli, F1 = 1 (cid:88) i=1 F1i. To quantify localization quality, we compute the mean IoU for each image over the matched pairs Mi. If an image has no matched pairs (Mi = 0), we define its IoU as 0. The overall mean IoU is then the macro-average across all images: IoU = 1 (cid:88) (cid:40) 1 Mi (cid:80) (p,g)Mi IoU(p, g), Mi > 0 i=1 0, Mi = . Similarly, for semantic-name accuracy, we embed predicted and ground-truth names using sentence encoder ϕ() (e.g., sentence-transformer) and compute the cosine similarity for each matched pair. If an image has no matches, we assign similarity of 0 for that image. The macro-average over all images is: Sim = 1 (cid:88) i= (cid:40) 1 Mi 0, (cid:80) (p,g)Mi ϕ(namep),ϕ(nameg) ϕ(namep) ϕ(nameg) , Mi > 0 Mi = . Together, these metrics separate whether elements are detected (precision/recall/F1), how precisely they are localized (mean IoU), and how well their semantic roles are recovered (mean embedding similarity). Performance Comparison. Table 8 presents detailed comparison of general-purpose VLMs and specialized screen parsing models across Word, Excel, and PowerPoint. Overall, general-purpose models such as GPT-4o, GPT-4.1, GPT-o3, and GPT-5 struggle with both element detection and localization, achieving low F1 scores (0.0190.128) and moderate mean IoU values (0.2290.578). Notably, GPT-o3 exhibits the highest overall F1 among the general models (0.128) and maintains relatively strong localization (IoU 0.578), but its recall remains limited, indicating many missed elements. GPT-4.1 and GPT-5 show uneven performance across applications: GPT-5 performs best on Excel (F1 0.126) but poorly on PowerPoint (F1 0.059), suggesting sensitivity to layout complexity and domain-specific content. In contrast, specialized parsers significantly outperform general-purpose models in all metrics. OmniParser and OmniParser-v2 achieve overall F1 scores above 0.40 and mean IoU above 0.73, with strong text similarity (0.5650.568), demonstrating robust detection, accurate localization, and reliable semantic recovery. The incremental improvement from OmniParser to OmniParser-v2 is modest but consistent, reflecting refinement in handling dense layouts and occlusions. Application-wise, Word and PowerPoint benefit most from these specialized models due to dense interactive regions, while Excel remains challenging because of its compact grid structure, though performance still exceeds general-purpose VLMs. These results reveal two key insights: (i) general-purpose VLMs are limited in screen parsing due to the need for fine-grained spatial reasoning and UI semantics, and (ii) task-specific training, as in OmniParser, provides substantial gains in both element coverage and semantic correctness, highlighting the necessity of specialized architectures for accurate and reliable screen understanding. 4.3 ACTION PREDICTION The action prediction task bridges the gap between users natural language command and the executable action calls required by the agent. This represents the ultimate goal of contextual user 13 Preprint. Table 8: Comparison of different models across domains (Precision, Recall, F1, Text Similarity, Avg IOU Accuracy). Model Domain Precision Recall F1 Text Sim. Avg IOU GPT-4o GPT-4.1 o3 GPTQwen2.5-VL-7B OmniParser OmniParser v2 Word Excel PowerPoint Overall Word Excel PowerPoint Overall Word Excel PowerPoint Overall Word Excel PowerPoint Overall Word Excel PowerPoint Overall Word Excel PowerPoint Overall Word Excel PowerPoint Overall 0.040 0.020 0.037 0.034 0.101 0.102 0.091 0. 0.173 0.178 0.129 0.160 0.106 0.172 0.065 0.111 0.384 0.041 0.047 0.181 0.392 0.431 0.417 0.411 0.396 0.431 0.418 0.413 0.017 0.002 0.021 0. 0.065 0.026 0.073 0.057 0.128 0.099 0.109 0.114 0.079 0.109 0.056 0.080 0.014 0.002 0.011 0.010 0.520 0.217 0.588 0.459 0.525 0.217 0.590 0. 0.024 0.004 0.026 0.019 0.077 0.039 0.080 0.067 0.144 0.118 0.115 0.128 0.088 0.126 0.059 0.089 0.023 0.003 0.014 0.015 0.440 0.270 0.479 0. 0.444 0.270 0.481 0.408 0.170 0.085 0.171 0.147 0.307 0.278 0.330 0.306 0.481 0.335 0.526 0.456 0.315 0.274 0.314 0.304 0.137 0.082 0.111 0. 0.619 0.450 0.594 0.565 0.625 0.450 0.596 0.568 0.252 0.133 0.282 0.229 0.518 0.514 0.480 0.505 0.631 0.523 0.559 0.578 0.615 0.574 0.508 0. 0.358 0.094 0.128 0.211 0.730 0.748 0.718 0.731 0.738 0.748 0.721 0.735 automation (CUA): translating abstract intent into precise, structured interactions with the GUI. As discussed in Section 3.3, we consider two evaluation settings: visual-only (where the agent has access solely to screenshots) and visual+a11y (where accessibility metadata is also provided). The latter setting is designed to test how much accessibility information improves grounding and execution. Baselines. We evaluate set of general-purpose VLMs, including GPT-4o Hurst et al. (2024), GPT-4.1 OpenAI (2025b), GPT-o3 OpenAI (2025d), GPT-5 OpenAI (2025c), and Qwen-VL-2.5 (7B) Bai et al. (2025). In addition, we fine-tune Qwen-VL-2.5 (7B) via supervised fine-tuning (SFT) and reinforcement learning (RL) on GUI-360 to examine post-training improvements. Performance Metrics. The evaluation of action prediction is more nuanced than grounding, since each action step is composed of function, set of arguments, and status flag (continue or finish). We therefore report three component accuracies and one aggregated metric: Function accuracy (Accfunc): the proportion of predictions where the predicted function ˆfi exactly matches the ground-truth function fi. { ˆfi = fi}. Accfunc = 1 (cid:88) i=1 14 Preprint. Table 9: Model performance comparison with screen Visual-only (left) and with screen Visual+A11y (right). Values are reported as percentages. Visual-only Visual+A11y Model Word Excel PowerPoint Total Word Excel PowerPoint Total 3.61 GPT-4o 3.60 GPT-4.1 16.85 GPT-o3 9.05 GPT-5 Qwen-2.5 7B 15.70 Qwen-2.5 7B-SFT 49.10 1.96 1.88 13.06 6.21 12.75 45.12 3.35 2.55 24.42 10.26 25.09 56.53 3.12 2.82 17.92 8.59 17.52 50.08 61.36 35.46 34.00 31.68 15.64 31.68 29.15 33.13 28.76 26.39 3.56 7. 48.11 50.98 48.84 48.23 22.51 34.99 36.71 39.19 36.72 34.86 14.18 25.78 Table 10: Comparison of Qwen and SFT models with/without A11Y across domains. Each cell shows Qwen / SFT, with bold indicating the better value. w/o A11Y w/ A11Y Metric Excel Word PPT Overall Excel Word PPT Overall Function Match Args Match Status Match Args Mismatch Err. Coord. OOB 61.67 / 81.07 12.84 / 45.43 95.96 / 94.13 87.17 / 54.56 74.94 / 63.97 62.97 / 81.18 15.80 / 49.38 96.37 / 93.85 84.21 / 50.62 66.63 / 61.39 88.50 / 91.02 25.16 / 56.72 97.44 / 96.17 74.84 / 43.28 96.15 / 82. 69.82 / 83.93 17.61 / 50.34 96.56 / 94.59 82.39 / 49.66 76.68 / 67.47 50.61 / 80.64 3.60 / 7.47 85.74 / 95.27 96.40 / 92.53 74.91 / 89.77 66.62 / 83.19 15.74 / 31.82 98.23 / 95.77 84.26 / 68.18 83.58 / 76.72 90.82 / 91.53 22.60 / 35.16 99.00 / 96.34 77.40 / 64.84 98.78 / 90.54 68.95 / 84.83 14.25 / 25.91 94.93 / 95.79 85.74 / 74.10 84.71 / 84.73 Argument accuracy (Accargs): evaluated conditionally on the predicted function. If the function is spatial action such as click, correctness requires that the predicted coordinate (ˆxi, ˆyi) falls inside the ground-truth bounding box bi. For symbolic arguments (e.g., menu item name, keystroke, value,), correctness requires exact match between predicted and ground-truth arguments. Formally, Accargs = 1 (cid:88) i=1 {ˆai ai fi}, where the equivalence relation depends on the function type fi. Status accuracy (Accstatus): whether the predicted status flag ˆsi matches the ground-truth status si. Step success rate (Accstep): step is considered correct only if all three components (function, arguments, status) are correct simultaneously. Accstep = 1 N (cid:88) i=1 { ˆfi = fi ˆai ai ˆsi = si}. Performance Comparison. Table 9 reports the results of action prediction under the visual-only and visual+a11y settings. When relying on screenshots alone, all models perform poorly, with accuracy below 20% in most cases. This highlights the intrinsic difficulty of inferring precise action arguments purely from pixel-level cues, even for state-of-the-art proprietary VLMs such as GPT4.1 and GPT-5. By contrast, providing accessibility information dramatically boosts performance. For example, GPT-4o improves from 3.12% to 36.71%, and GPT-4.1 nearly triples its performance from 2.82% to 39.19%. This demonstrates that structured element annotations effectively reduce the burden of visual grounding, enabling models to focus on action semantics. Furthermore, supervised fine-tuning (SFT) on GUI-360 delivers substantial gains. Qwen-2.5 7B improves from 17.52% to 50.08% after SFT in the visual-only setting, nearly threefold improvement, showing the strong training signal provided by GUI-360. However, when a11y information is introduced, the benefits of SFT diminish, suggesting that a11y annotations already encode much of the structural alignment that SFT otherwise learns. Overall, these results highlight both the challenge and opportunity presented by GUI-360: action prediction is extremely difficult without explicit structural information, but with accessibility-enhanced input and dataset-driven post-training, models can achieve substantial improvements. 15 Preprint. Result Analysis. To obtain fine-grained understanding of action prediction, we decompose evaluation into three aspects: (i) function match, which tests whether the predicted function type is correct; (ii) argument match, which checks whether the predicted arguments (e.g., coordinates or values) align with ground truth; and (iii) status match, which verifies whether the model correctly predicts task continuation or completion. In addition, we track two error sources: Args Mismatch Error, capturing the proportion of incorrect arguments, and Coord. OOB, which reflects out-ofbounds coordinate predictions in the visual-only setting or incorrect element selections when a11y information is available. From Table 10, we observe that supervised fine-tuning (SFT) brings large gains in function match and especially in argument match, reducing errors by more than half across domains. The dominant source of failure remains Args Mismatch Error, which suggests that grounding actions to the correct interface element is the most challenging aspect. Notably, Coord. OOB contributes significantly to these mismatches, highlighting the models difficulty in spatial grounding from raw screenshots. Comparing the two evaluation settings, we find that introducing a11y metadata substantially reduces Coord. OOB errors, showing that structured semantic cues provide more reliable grounding than relying on visual information alone. However, even with a11y support, argument prediction accuracy lags far behind function prediction, indicating that grounding argumentseither through spatial reasoning or element identificationremains the key bottleneck for reliable GUI action prediction. Overall, these findings highlight that while models can correctly identify the intended operation, achieving precise grounding is still an open challenge, and incorporating structured UI metadata such as a11y is promising direction."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduced GUI-360, large-scale dataset and benchmark suite for advancing research on desktop computer-using agents. GUI-360 fills three critical gaps in the field: the lack of realistic task collections, the absence of scalable data collection pipelines, and the shortage of unified benchmarks spanning GUI grounding, screen parsing, and action prediction. Through an automated, LLM-augmented pipeline, we curated over 1.2M steps across thousands of trajectories in widely used Windows applications, paired with rich multimodal annotations including screenshots, accessibility metadata, reasoning traces, and both successful and failed executions. Our empirical evaluation highlights both the difficulty of the domain and the promise of GUI-360. State-of-the-art visionlanguage models exhibit significant limitations when applied out-of-the-box, particularly in grounding and action prediction. Yet, fine-tuning and reinforcement learning on GUI360 deliver consistent improvements, underscoring the datasets utility as training and evaluation resource. Importantly, results remain far from human-level reliability, establishing GUI-360 as challenging but necessary foundation for future progress. We release GUI-360, the accompanying benchmark GUI-360-Bench, and the full data collection framework to the research community. We hope these resources catalyze systematic advances in screen understanding, multimodal reasoning, and robust action planning, ultimately bringing practical and reliable computer-using agents closer to reality."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. 16 Preprint. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. Rob Haverty. New accessibility model for microsoft windows and cross platform development. ACM SIGACCESS Accessibility and Computing, (82):1117, 2005. Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, Tamer Ozsu, Aishwarya Agrawal, David Vazquez, et al. Uivision: desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent. OpenAI. Introducing gpt-4.1 in the api. April 2025b. URL https://openai.com/index/ gpt-4-1/. Accessed: 2025-09-18. OpenAI. Gpt-5 is here. https://openai.com/gpt-5/, 2025c. Accessed: 2025-09-18. OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025d. Accessed: 2025-09-18. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. Preprint. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024a. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024b. Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang, Si Qin, et al. Large action models: From inception to implementation. arXiv preprint arXiv:2412.10047, 2024c. Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025. Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, and Jingbo Shang. Officebench: Benchmarking language agents across multiple applications for office automation. arXiv preprint arXiv:2407.19056, 2024d. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. Yibin Xu, Liang Yang, Hao Chen, Hua Wang, Zhi Chen, and Yaohua Tang. Deskvision: Large scale desktop region captioning for advanced gui agents. arXiv preprint arXiv:2503.11170, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023. Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. Api agents vs. gui agents: Divergence and convergence. In ICML 2025 Workshop on Computer Use Agents. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024a. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024b. Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 18 Preprint. Boyuan Zheng, Michael Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, et al. Skillweaver: Web agents can self-improve by discovering and honing skills. arXiv preprint arXiv:2504.07079, 2025a. Jiani Zheng, Lu Wang, Fangkai Yang, Chaoyun Zhang, Lingrui Mei, Wenjie Yin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Vem: Environment-free exploration for training gui agent with value environment model. arXiv preprint arXiv:2502.18906, 2025b. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A ACTION SET",
            "content": "To support diverse interaction across desktop applications, we design unified action set that combines general-purpose GUI operations with application-specific APIs using MCP servers. The action set is deliberately lightweight yet expressive, enabling agents to cover the full spectrum of common productivity tasks while remaining tractable for model training. The GUI actions (click, type, drag, wheel mouse input) form the foundation of interaction, as they are applicable to any graphical user interface. These actions abstract low-level mouse and keyboard events into structured calls, supporting variants such as absolute and normalized coordinates, modifier keys, and multi-step operations. On top of this universal layer, we extend the action set with fine-grained APIs for Word, Excel, and PowerPoint. These APIs expose high-level document, spreadsheet, and presentation semantics, such as inserting tables, modifying cell values, reordering columns, adjusting font properties, or setting slide backgrounds. By combining GUI-agnostic operations with domain-specific APIs, the action set achieves both generality and efficiency: agents can rely on GUI actions for arbitrary interfaces while exploiting APIs for structured tasks where precise semantics matter. Table 11 summarizes the full action set. This unified design allows agents trained on GUI-360 to operate seamlessly across heterogeneous applications, balancing robustness with expressivity. GUI-360 SCHEMA Each execution step in GUI-360 is stored as structured JSON object following unified schema. This schema ensures consistency across tasks and provides rich multimodal supervision for grounding, parsing, and action prediction. Table 12 summarizes the key fields. Discussion. The schema integrates three complementary perspectives: (i) Visual context through multi-view screenshots, (ii) Structural context via accessibility metadata and hierarchical UI trees, and (iii) Cognitive traces through observations, reasoning, actions, and evaluations. This rich structure allows GUI-360 to jointly support grounding, parsing, and action prediction, while enabling both supervised training and fine-grained evaluation. By standardizing every execution step, the schema provides scalable foundation for reproducibility and extensibility across applications."
        },
        {
            "title": "C BASELINE DETAILS",
            "content": "We summarize the baselines evaluated on GUI-360 across the three core tasks: GUI grounding, screen parsing, and action prediction. These baselines include both general-purpose visionlanguage models and domain-specific approaches designed for GUI reasoning. Below we briefly introduce each group of models. C.1 GUI GROUNDING GPT-4o Hurst et al. (2024): proprietary multimodal VLM used off-the-shelf for grounding. GPT-4.1 OpenAI (2025b): proprietary VLM emphasizing instruction-following and tool use. 19 Preprint. Table 11: Supported actions across Word, Excel, and PowerPoint, grouped by shared GUI actions and application-specific APIs. Action click type drag wheel mouse input insert table select text select table select paragraph save as set font table2markdown insert excel table select table range set cell value auto fill reorder columns Description Click at given position (absolute or normalized), supporting left/right/middle/x button, single/double click, and optional modifier key. Type text or hotkeys at position, with options for clearing text or focusing on control. Supports special keys like {VK CONTROL}c. Drag from start to an end position with configurable mouse button, duration, and optional key hold (e.g., shift, control). Scroll at given position with positive (up) or negative (down) wheel distance. Insert table with specified number of rows and columns. Select exact text in the document. Select table by its index number. Select paragraphs by start and end indices, with option to restrict to non-empty ones. Save the document with specified directory, file name, and extension (default: PDF). Change font family and/or size. Extract the contents of worksheet table into Markdown format. Insert table (list of lists) into sheet at specified starting cell. Select range of cells by coordinates in sheet. Set the value (or formula) of specific cell. Autofill values in specified cell range. Reorder columns in sheet according to given list of column names. Type GUI GUI GUI GUI Word API Word API Word API Word API Word API Word API Excel API Excel API Excel API Excel API Excel API Excel API set background color save as Change the slide background color using hex RGB value, for selected or all slides. Save the presentation with specified directory, file name, and extension (default: PowerPointX). Optionally save slides as images. PowerPoint API PowerPoint API o3 OpenAI (2025d): OpenAI reasoning model family; evaluated zero-shot for grounding. GPT-5 OpenAI (2025c): latest OpenAI flagship; strong general reasoning baseline. Qwen2.5-VL-7B Bai et al. (2025): open-source 7B multimodal baseline. UGround-7B Gou et al. (2024): GUI visual grounding model (Qwen2-VL backbone) trained with large-scale GUI data. Aguvis-7B Xu et al. (2024): vision-centric GUI agent with unified cross-platform action space. UI-TARS-1.5 (7B) Qin et al. (2025): multimodal agent optimized for GUI reasoning and interactive tasks. GUI-Actor (7B) Wu et al. (2025): coordinate-free grounding model with an attention-based action head. SFT variants: Qwen2.5-VL-7B fine-tuned on GUI-360 for task-adapted grounding. C.2 SCREEN PARSING GPT-4o / GPT-4.1 / o3 / GPT-5 Hurst et al. (2024); OpenAI (2025b;d;c): general-purpose VLMs used off-the-shelf for detection/localization. Qwen2.5-VL-7B Bai et al. (2025): open-source multimodal baseline. OmniParser / OmniParser-v2 Lu et al. (2024): screen-parsing tools that produce element sets (names + bounding boxes) from raw screenshots. 20 Preprint. Table 12: Execution Step Schema for GUI-360. Each entry records metadata, screenshots, accessibility data, reasoning traces, and actions. Field Description execution id Unique identifier for the execution instance (e.g., word 1 1). app domain request template Application domain (e.g., Word, Excel, PowerPoint). Natural-language task description provided to the agent. Environment template file used for instantiation. step id / total steps Current step index and total number of steps in the trajectory. evaluation Automatic assessment of the step, including reasoning, evidence, sub-scores, and final completeness label. step.screenshots Multiple synchronized screenshots: clean view, full desktop, annotated version, and selected-controls view. step.ui tree Hierarchical UI structure with element IDs, names, control types, bounding boxes, and children. step.control infos Metadata from accessibility APIs and merged control sources, providing bounding boxes, labels, and semantic text. step.observation Agents textual observation of the current state. step.thought step.action Agents intermediate reasoning for the next action. Executed action, including function type (e.g., click), arguments, target coordinates, and status flag. status Overall status of the step (CONTINUE or FINISH). C.3 ACTION PREDICTION GPT-4o / GPT-4.1 / o3 / GPT-5 Hurst et al. (2024); OpenAI (2025b;d;c): proprietary VLMs evaluated in both visual-only and visual+a11y settings. Qwen2.5-VL-7B Bai et al. (2025): open-source baseline for structured action generation. Qwen2.5-VL-7B-SFT: supervised fine-tuning on GUI-360 for step-wise function/argument/status prediction. Summary. Together, these baselines span spectrum from general-purpose VLMs to specialized GUI-focused agents. This diversity allows us to systematically evaluate the unique challenges posed by GUI-360 across grounding, parsing, and action prediction, and to measure how far current models remain from robust, human-level computer-using agents."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "Data Collection. To construct GUI-360, we deployed cluster of 15 Windows 11 virtual machines, each provisioned with 4 CPU cores. These VMs executed tasks in parallel, enabling efficient large-scale trajectory collection. Task execution followed two-phase strategy: in Phase 1, GPT-4o was used as the agent; in Phase 2, all failed tasks were re-executed with GPT-4.1 for recovery (see Section 3). Both models were queried with the temperature fixed at 0.0 to ensure deterministic outputs and reproducibility. Model Access. For evaluation on GUI-360-Bench, we used multiple OpenAI models, including GPT-4o, GPT-4.1, o3, and GPT-5, all accessed via the Azure OpenAI Service. Unless otherwise 21 Preprint. stated, the decoding temperature was set to 0.0 across all experiments to minimize variance and ensure consistent evaluation. Fine-tuning and Training. All supervised fine-tuning (SFT) and reinforcement learning (RL) experiments were conducted on compute cluster equipped with NVIDIA A100 GPUs (40GB memory per GPU). Specifically, each run was distributed across 4 A100 GPUs using mixed-precision training (FP16) for efficiency. We adopted standard optimization settings following prior work on multimodal fine-tuning, with learning rates tuned over {1e-5, 5e-6, 1e-6}. Checkpointing and gradient accumulation were applied to ensure stable training for long trajectories."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work presents GUI-360, large-scale dataset and benchmark for GUI agents on Windows applications. We carefully considered ethical implications throughout the data collection, processing, and release pipeline. First, no human subjects were directly involved in the data collection process, and thus no personally identifiable information (PII) or sensitive user data is included. All queries and trajectories were generated and executed within controlled sandbox environments to ensure both privacy and security. Second, we adhered to software license terms and platform usage guidelines, ensuring that the collection process does not violate proprietary restrictions or legal compliance requirements. Third, we performed multiple post-processing stages, including trajectory validation, data sanitization, and structuring, to filter out incomplete, low-quality, or potentially misleading samples, thereby reducing the risk of harmful insights or erroneous model behaviors. We acknowledge the potential downstream misuse of GUI automation technologies, such as unauthorized system manipulation or exploitation of accessibility features. To mitigate this, we restrict dataset release to non-sensitive application contexts (Word, Excel, PowerPoint) and exclude scenarios that could pose privacy, security, or safety risks. The dataset is intended solely for academic research on improving robustness, generalization, and evaluation of GUI agents. All models and baselines are evaluated under responsible-use guidelines, and we encourage future researchers to follow the same principles."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We place strong emphasis on reproducibility. To this end, we will release all data collection code, execution framework, and templates used to instantiate user queries. The full dataset GUI-360, along with the benchmark split GUI-360-Bench, will also be publicly available. Detailed descriptions of the pipeline are provided in Section 3, with task definitions summarized in Table 5, filtering statistics in Table 3, and execution details in Appendix D. Additional implementation details, hyperparameters, and evaluation protocols are included in the supplementary material. Together, these resources ensure that both our dataset creation and experimental results can be fully reproduced, verified, and extended by the research community."
        },
        {
            "title": "LLM USAGE STATEMENT",
            "content": "In preparing this work, we used large language models (LLMs) strictly as an assistive tool for text polishing and minor language refinement. All research ideas, technical designs, analyses, and conclusions were conceived and carried out entirely by the authors."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Nanjing University",
        "Peking University",
        "ZJU-UIUC"
    ]
}