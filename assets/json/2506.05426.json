{
    "paper_title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
    "authors": [
        "Wenhao Wu",
        "Fuhong Liu",
        "Haoru Li",
        "Zican Hu",
        "Daoyi Dong",
        "Chunlin Chen",
        "Zhi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose \\textbf{T2MIR} (\\textbf{T}oken- and \\textbf{T}ask-wise \\textbf{M}oE for \\textbf{I}n-context \\textbf{R}L), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 2 4 5 0 . 6 0 5 2 : r Mixture-of-Experts Meets In-Context Reinforcement Learning Wenhao Wu1 Fuhong Liu1 Haoru Li1 Zican Hu1 Daoyi Dong2 Chunlin Chen1 Zhi Wang1 1Nanjing University 2Australian Artificial Intelligence Institute, University of Technology Sydney {wenhaowu, chika, zicanhu}@smail.nju.edu.cn haoruli.research@gmail.com daoyi.dong@uts.edu.au {clchen, zhiwang}@nju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "In-context reinforcement learning (ICRL) has emerged as promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Tokenand Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and task-wise MoE that routes diverse tasks to specialized experts for managing broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) is emerging as powerful mechanism for training autonomous agents to solve complex tasks in interactive environments [37, 38], unleashing its potential across frontier challenges including preference optimization [4], training diffusion models [24], and reasoning such as OpenAI-o1 [67] and DeepSeek-R1 [42]. Recent studies have been actively exploring how to harness the in-context learning capabilities of the transformer architecture to achieve substantial improvements in RLs adaptability to downstream tasks through prompt conditioning without any model updates, i.e., in-context RL (ICRL) [25]. Current research in offline settings encompasses two primary branches, algorithm distillation (AD) [26] and decision-pretrained transformer (DPT) [5], owing to their simplicity and generality. They share common structure that uses across-episodic transitions as few-shot prompts to transformer policy, and following-up studies continue to increase the inPreprint. Under review. context capacity by hierarchical structure [17], noise distillation [16], model-based planning [27], etc [18, 43, 28, 44]. Despite these efforts, two notable challenges remain in fully harnessing in-context learning within decision domains, as RL is notably more dynamic and complex than supervised learning. The first is the intrinsic multi-modality of datasets. In language or vision communities, the inputs to transformers are atomic words or pixels with consistent semantics in the representation space [34]. In ICRL, the prompt inputs are typically transition samples containing three modalities of state, action, and reward with large semantic discrepancies. States are usually continuous in nature in RL, actions like joint torques tend to be more high-frequency and less smooth, and rewards are simple scalars that can be sparse over long sequences [29]. The second is task diversity and heterogeneity. Compared to supervised learning, RL models are more prone to overfit the training set and struggle with generalization across diverse tasks [45]. The learning efficiency can be hindered by intrinsic gradient conflicts in challenging scenarios where tasks vary significantly [35]. 1 The above limitations raise key question: Can we design scalable ICRL framework to tackle the multi-modality and diversified task distribution within single transformer, advancing RLs in-context learning capacities one step closer to achievements in language and vision communities? In the era of large language models (LLMs), the mixture-of-experts (MoE) architecture [30] has shown remarkable scaling properties and high transferability while managing computational costs, such as in Gemini [48], Llama-MoE [36], and DeepSeekMoE [47]. The architectural advancement also extends to various domains such as computer vision [6], image generation [7], and RL [19, 46], highlighting the significant potential and promise of MoE models. Intuitively, MoE architectures can serve as an encouraging remedy to tackle the two aforementioned bottlenecks in ICRL. First, MoE enables different experts to process tokens with distinct semantics more effectively [49]. This is natural match for processing the multiple modalities within the state-action-reward sequence. Second, MoE can alleviate gradient conflicts by dynamically allocating gradients to specialized experts for each input through the sparse routing mechanism [66]. This property is promising for handling broad task distribution with significant diversity and heterogeneous complexity. Inspired by this, we propose an innovative framework T2MIR (Tokenand Task-wise MoE for In-context RL), which for the first time harnesses architectural advances of MoE to develop more scalable and competent ICRL algorithms. We substitute the feedforward layer in transformer blocks with two parallel ones: token-wise MoE and task-wise MoE. First, the token-wise MoE layer is responsible for automatically capturing distinct semantic features of input tokens within the multimodal state-action-reward sequence. We include load-balancing loss and an importance loss to avoid tokens from all modalities collapsing onto minority experts. Second, the task-wise MoE layer is designed to assign diverse tasks to specialized experts with sparse routing, effectively managing broad task distribution while alleviating gradient conflicts. To facilitate task-wise routing, we introduce contrastive learning method to maximize the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. Finally, the outputs of the two parallel MoE components are concatenated and fed into the next layer. Figure 1: t-SNE visualization of expert assignments on Cheetah-Vel where tasks differ in target velocities. Left: token-wise MoE enables different experts to process tokens with distinct semantics. Right: task-wise MoE effectively manages broad task distribution, where the difference between expert assignments is positively related to the difference between tasks. In summary, our main contributions are threefold: We unleash RLs in-context learning capacities with simple and scalable architectural enhancement. To our knowledge, we are the first to bring the potential and promise of MoE to ICRL. We design token-wise MoE to facilitate processing the distinct semantics within multi-modal inputs, and task-wise MoE to tackle diversified task distribution with reduced gradient conflicts. 1For example, given two navigation tasks where the goals are in adverse directions, the single RL model ought to execute completely opposite decisions under the same states for the two tasks. 2 We build our method upon AD and DPT, and conduct extensive experiments on various benchmarks to show superiority over competitive baselines and visualize deep insights into performance gains."
        },
        {
            "title": "2 Related Work",
            "content": "In-Context RL. Tackling task generalization is long-standing challenge in RL. Early methods address this via the lens of meta-RL, including the memory-based RL2 [64], the optimizationbased MAML [20], and the context-based VariBAD [65] and UNICORN [3]. With the emergence of transformers that show remarkable in-context learning abilities [8], the community has been exploring its potential to enhance RLs generalization via prompt conditioning without model updates [25]. Many ICRL methods have emerged, each differing in how to train and organize the context. In online settings, AMAGO [23, 9] trains long-sequence transformers over entire rollouts with actor-critic learning, and [10] leverages the S4 (structured state space sequence) model to handle long-range sequences for ICRL tasks. Classical studies in offline settings include AD [26] and DPT [5]. AD trains causal transformer to predict actions given preceding histories as context, and DPT predicts the optimal action given query state and prompt of interactions. Follow-up studies enhance incontext learning from various algorithmic aspects [16, 18, 43, 28, 44], such as IDT with hierarchical structure [17] and DICP with model-based planning [27]. Complementary to these algorithmic studies, we introduce MoE to advance ICRLs development from an architectural perspective. Mixture-of-Experts. The concept of MoE is first proposed in [57], which employs different expert networks plus gating network that decides which experts should be used for each training case [58]. Then, it is applied to tasks of language modeling and machine translation [30], gradually showing prominent performance in scaling up transformer model size while managing computational costs [56, 33]. Up to now, MoE architectures have become standard component for advanced LLMs, such as Gemini [48], Llama-MoE [36], and DeepSeekMoE [47]. Recent efforts have explored extending MoE advancements to RL domains. [19] incorporates soft MoE modules into value-based RL agents, and performance improvements scale with the number of experts used. [46] uses an MoE backbone with CNN encoder to process visual inputs and improves the policy learning ability to handle complex robotic environments. [66] strengthens decision transformers with MoE to reduce task load on parameter subsets, with three-stage training mechanism to stabilize MoE training. Accompanied by these encouraging efforts, we aim to harness the potential of MoE for ICRL."
        },
        {
            "title": "3 Preliminaries",
            "content": "3.1 In-Context Reinforcement Learning We consider multi-task offline RL setting, where tasks follow distribution Mn = S,A,Tn, Rn, γ (M ). Tasks share the same state space and action space A, while differing in the reward , an function or transition dynamics . An offline dataset Dn = {(sn j=1 is collected by arbitrary behavior policies for each task out of total of training tasks. The agent can only access n=1 to train an in-context policy as π (cid:0)ansn; τ the offline datasets {Dn}N pro is trajectory prompt that encodes task-relevant information. At test time, the trained policy is evaluated on unseen tasks sampled from (M ) by directly interacting with the environment. The prompt is initially empty and gradually constructed from history interactions. The objective is to learn an in-context policy to maximize the expected episodic return over test tasks as J(π) = EM (M )[JM (π)]. )}J , (cid:1), where τ , rn pro 3.2 Mixture-of-Experts standard sparse MoE layer consists of experts {E1, ..., EK} and router G. The router predicts an assignment distribution over the experts given the input x. Following the common practice [30, 46], we only activate the top-k experts to process the inputs. In general, the number of activated experts is fixed and much smaller than the total number of experts, thus scaling up model parameters and significantly reducing computational cost. Formally, the output of the MoE layer can be written as w(i; x) = softmax (topk(G(x))) [i], = (cid:88)K i= w(i; x)Ei(x), (1) where topk() selects top experts based on the router output G(x). softmax() normalizes top-k values into weight distribution w(). Probabilities of non-selected experts are set to 0. 3 Figure 2: The overview of T2MIR. (a) We substitute the feedforward layer in causal transformer blocks with two parallel MoE layers and concatenate their outputs to feed into the next layer. (b) Token-wise MoE: it automatically captures distinct semantic features within the multi-modal (s, a, r) inputs, and uses Lbalance as regularization loss to avoid tokens from all modalities collapsing onto minority experts. (c) Task-wise MoE: it assigns diverse tasks to specialized experts, and includes contrastive learning loss Lcontrastive to enhance task-wise routing via more precise capture of taskrelevant information, where τi is the query and τi /τi are positive/negative keys."
        },
        {
            "title": "4 Method",
            "content": "In this section, we present T2MIR (Tokenand Task-wise MoE for In-context RL), an innovative framework that harnesses architectural advancements of MoE to tackle ICRL challenges of the multi-modality and diversified task distribution. Figure 2 illustrates the method overview, followed by detailed implementations. Algorithm pseudocodes are presented in Appendix A. 4.1 Token-wise MoE By interacting with the outer environment, we gather the data represented as sequence of stateaction-reward transitions in the form of τ = (s0, a0, r0, ..., sT , aT , rT ), which often serves as the prompt input to ICRL models. The state is usually continuous in nature, which can contain physical quantities of the agent (e.g., position, velocity, and acceleration) or an image in visual RL [46]. The action tends to be more high-frequency and less smooth, such as joint torques or discrete commands. The reward is simple scalar that is often sparse over long horizons. In language or vision communities, the inputs to transformers are atomic words or pixels with consistent semantics in the representation space [34]. In contrast, ICRL encounters new challenge of processing the prompt data that encompasses three modalities with significant semantic discrepancies. Inspired by the surprising discoveries that the MoE structures can effectively capture distinct semantic features of input tokens [56, 49], we introduce token-wise MoE layer to tackle the multiple modalities within the state-action-reward sequence. As shown in Figure 2-(b), each element in the multi-modal sequence corresponds to token. The MoE consists of K1 experts Etok and router Gtok, and the router learns to assign each token to specific experts with noisy top-k gating. Let denote the hidden state of given token after self-attention calculation. The token router Gtok computes distribution of logits {Gtok(ih)}K1 i=1 for expert selection. The top-k experts will be selected to process this token, and their outputs will be weighted to produce the final output ytok as wtok(i; h) = softmax (topk(Gtok(ih))) [i], ytok = (cid:88)K1 i=1 wtok(i; h) Etok(ih). (2) The router tends to converge on producing large weights for the same few experts, as the favored experts are trained more rapidly and thus are selected even more by the router. In order to avoid 4 tokens from all modalities collapsing onto minority experts, we incorporate regularization term to balance expert utilization that consists of an importance loss and load-balancing loss [30] as Lbalance = wimp CV (Imp(h))2 + wload CV (Load(h))2, where CV () denotes the coefficient of variation, and wimp and wload are hand-tuned scaling factors. The first item encourages all experts to have equal importance as defined by Imp(h) = (cid:80) Gtok(h). The second item encourages experts to receive roughly equal numbers of training examples, where Load() is smooth estimator of the number of examples assigned to each expert for batch of inputs. more detailed description of the balance loss can be found in Appendix B. (3) 4.2 Task-wise MoE In typical ICRL settings, single policy model is trained across multiple tasks. The learning efficiency can be impeded by intrinsic gradient conflicts in challenging scenarios with significant task variation. For instance, given two navigation tasks where the goals are in adverse directions, the policy ought to make contrary decisions under the same states. Given the same state-action trajectories, the two tasks can produce exactly opposite policy gradients, leading to sum of zero gradient during training. The MoE structure was originally proposed to use different parts of model, called experts, to specialize in different tasks or different aspects of task [57, 58]. Drawing from this natural inspiration, we introduce task-wise MoE layer to handle broad task distribution with significant diversity and heterogeneous complexity, leveraging modular expert learning to alleviate gradient conflicts. As shown in Figure 2-(c), the MoE consists of K2 experts Etask and router Gtask, and the router learns to assign sequence of tokens to specialized experts at the task level. Given trajectory τ = (s0, a0, r0, ..., sT , aT , rT ) from some task , we use to denote the average hidden state of all three-modality tokens after self-attention calculation. The task router Gtask computes distribution of logits {Gtask(ih)}K2 i=1 for expert selection. The top-k experts will be selected to process these tokens, and their outputs will be weighted to produce the final output ytask as (cid:88)K2 i=1 wtask(i; h) = softmax (cid:0)topk(Gtask(ih))(cid:1) [i], wtask(i; h) Etask(ih). ytask = (4) Task-wise Routing via Contrastive Learning. The task-wise router can be viewed as an encoder for extracting task representations from input tokens, based on the intuition that similar tasks are preferred to the same expert and tasks with notable differences are allocated to disparate experts. An ideally discriminative task representation should accurately capture task-relevant information from offline datasets, while remaining invariant to behavior policies or other unrelated factors. To achieve this goal, we propose to maximize the mutual information between the task and its router representation, enhancing task-wise routing that minimally preserves task-irrelevant information. Formally, the mutual information I() aims to quantify the uncertainty reduction of one random variable when the other is observed, measuring their mutual dependency. Mathematically, we formalize the router Gtask as probabilistic encoder Gtask(h), where denotes the task representation. The distribution is determined by its task , where (M ). The objective for the router is: max I(z; ) = Ez,M (cid:20) log p(M z) p(M ) (cid:21) . (5) Drawing inspiration from noise contrastive estimation (InfoNCE) [59] in the contrastive learning literature [60, 61], we derive lower bound of Eq. (5) using the following theorem. Theorem 1. Let denote set of tasks following the task distribution (M ), and = . is given task. Let = (τ ), Gtask(h), and e(h, z) = p(zh) p(z) , where τ is trajectory from task and is average hidden state of all tokens after the self-attention calculation function (). Let denote the average hidden state generated by any task M, then we have I(z; ) log + M,z,h (cid:20) log (cid:80) e(h, z) e(h, z) (cid:21) . (6) Appendix presents the complete proof. Since direct evaluation of p(z) or p(zh) is intractable, we employ NCE and importance sampling techniques that compare the target value with randomly sampled negative ones. Consequently, we approximate using the exponential of score function, similarity metric between the latent codes of two examples. Given batch of trajectories (τ1, ..., τm), we denote router representation zi from τi as the query q, and representations of other trajectories as the keys = {z1, ..., zm}zi. Points from the same task with the query, Mj, are set as positive key {k+} and those from different tasks are set as negative {k} = K{k+}. We adopt bilinear products [60] as the score function, with similarities between the query and keys computed as qW k, where is learnable parameter matrix. Then, we formulate sampling-based version of the tractable lower bound as the InfoNCE loss for the contrastive router as Lcontrastive = log exp(qW k+) exp(qW k+)+exp(qW k) = log (cid:80) exp(z (cid:80) iMj exp(z iMj zi )+(cid:80) /Mj zi ) exp(z (7) zi) It optimizes an -way classification loss to classify the positive pair among all pairs, equivalent to maximizing lower bound on mutual information in Eq. (6), with this bound tightening as gets larger. Following MoCo [62], we maintain query router Gq task, and use momentum update with coefficient β [0, 1) to ensure the consistency of the key representations as task and key router Gk Gk task βGk task + (1 β)Gq task, (8) where only query parameters Gq task are updated through backpropagation. 4.3 Scalable Implementations We develop scalable implementations of T2MIR using two mainstream ICRL backbones that offer promising simplicity and generality: AD [26] and DPT [5], yielding two T2MIR variants as follows. T2MIR-AD. It trains an MoE-augmented causal transformer policy πθ to autoregressively predict actions given preceding learning histories hist as the prompt: hist := (s0, a0, r0, ..., st1, at1, rt1, st, at, rt) = (st, at, rt). During training, we minimize negative log-likelihood loss over individual tasks as L(θ) = (cid:88)N n=1 (cid:88)T 1 t=0 log πθ (cid:0)a = an sn ; τ pro (cid:1) , where τ pro := hisn t1, (9) (10) which can be derived as cross-entropy loss for discrete actions, and mean-square error (MSE) loss when the action space is continuous. Akin to AMAGO [23], we adopt Flash Attention [63] to enable long context lengths on single GPU. For the context, we use the same position embedding for st, at, and rt to maintain semantics of temporal consistency. T2MIR-DPT. It trains transformer policy to predict an optimal action given query state and prompt of interaction transitions. In practice, we randomly sample trajectory τpro from the dataset as the task prompt for each query-label pair (st, ), and train the policy to minimize the loss as (cid:88)N (cid:88)T log πθ (cid:0)a = an sn ; τ pro (cid:1) , where τ pro Dn, L(θ) = (11) n=1 t=0 Different from AD in Eq. (10), DPT requires labeling optimal actions for query states to construct the offline dataset (cid:80) ), inheriting principles from pure imitation learning. , an t(sn (cid:80)"
        },
        {
            "title": "5 Experiments",
            "content": "We comprehensively evaluate our methods on popular benchmarking domains using various qualities of offline datasets. In general, we aim to answer the following research questions: Can T2MIR demonstrate consistent superiority on in-context learning capacity when tested in unseen tasks, compared to various strong baselines? (Sec. 5.1) How do the token-wise and the task-wise MoE layers affect T2MIRs performance, respectively (Sec. 5.2)? We also gain deep insights into each component via visualizations in Sec. 5.4. How robust is T2MIR across diverse settings? We construct offline datasets with varying qualities to extensively evaluate T2MIR and baselines. Also, we investigate T2MIRs sensitivity to various hyperparameters, such as the expert configuration and the loss ratio. (Sec. 5.3 and Appendix G) 6 Environments. We evaluate T2MIR on four benchmarks that are widely used in multi-task settings: i) the discrete environment DarkRoom, which is grid-world environment with multiple goals; ii) the 2D navigation environment Point-Robot, which aims to navigate to target position in 2D plane; iii) the multi-task MuJoCo locomotion control environment, containing Cheetah-Vel and Walker-Param; iv) the Meta-World manipulation platform, including Reach and Push. We construct three datasets with different qualities: Mixed, Medium-Expert, and Medium. Appendix presents more details about environments and dataset construction. Baselines. We compare T2MIR to five competitive baselines, including four ICRL methods AD [26], DPT [5], IDT [17], DICP [27], and context-based offline meta-RL approach UNICORN [3]. More details about baselines are given in Appendix E. As DICP has three implementations, we use the best results among them in our experiments. For all experiments, we conduct evaluations across five random seeds. The mean of the obtained return is plotted as the bold line, with 95% bootstrapped confidence intervals of the mean results indicated by the lighter shaded regions. Appendix gives implementation details of T2MIR-AD and T2MIR-DPT. Appendix presents comprehensive hyperparameter analysis, including expert selection in token-wise and task-wise MoE, InfoNCE loss ratio, and the positioning of MoE layers. 5.1 Main Results We compare our method against various baselines under an aligned evaluation setting, where prompts or contexts are generated from online environment interactions to infer task beliefs during testing. Figure 3 and Table 1 present test return curves and numerical results of the best performance of T2MIR and baselines using Mixed datasets. In these diverse environments with varying reward functions or transition dynamics, both T2MIR-AD and T2MIR-DPT achieve significantly superior performance regarding the in-context learning speed and final asymptotic results compared to the strong baselines. In most cases, our two implementations take the top two rankings for the best and second-best performance, showcasing comparably strong generalization abilities to unseen tasks. It highlights the effectiveness of introducing MoE architectures to the two mainstream ICRL backbones. Notably, DICP exhibits faster learning speed and comparable asymptotic performance compared to our T2MIR in DarkRoom. In this environment with small state-action space, DICP naturally enables more efficient policy search via look-ahead model-based planning. Then, DICP struggles when facing more complicated environments like Reach and Push, as it is more difficult to find high-return trajectories when planning in large space. In contrast, the superiority of T2MIR is more pronounced in harder problems, underscoring its potential to tackle challenging ICRL tasks. Figure 3: Test return curves of two T2MIR implementations against baselines using Mixed datasets. 7 Figure 4: Ablation results of both T2MIR-AD and T2MIR-DPT architectures using Mixed datasets. 5.2 Ablation Study We compare T2MIR to three ablations: 1) w/o Task-MoE, it only retains the token-wise MoE layer and regularizes the router with balance loss in Eq. (3); 2) w/o Token-MoE, it only retains the task-wise MoE layer and enhances the router with contrastive loss in Eq. (7); and 3) w/o all, it degrades to vanilla AD and DPT. Figure 4 presents ablation results of T2MIR-AD and T2MIR-DPT. First, both T2MIR architectures show decreased performance when any MoE is removed, with the worst occurring when both MoE layers are excluded. It demonstrates that the two kinds of MoE designs are essential for T2MIRs capability and they complement each other. Second, with the AD backbone, ablating the token MoE incurs more significant performance drop than ablating the task MoE, indicating that the token MoE plays more vital role for T2MIR-AD. AD takes long training histories as context, which can contain redundant trajectories for identifying the underlying task, akin to the memory-based meta-RL approach RL2 [64]. Hence, using the token MoE to handle multimodality within the long-horizon context may yield greater benefits. Third, the situation is reversed with the DPT backbone, as the task MoE is more essential for T2MIR-DPT. DPT takes small number of transitions as the task prompt, placing greater emphasis on accurately identifying task-relevant information from limited data. Numerical results of ablations can be found in Appendix H. Table 1: Test return of T2MIR against baselines using Mixed datasets, i.e., numerical results of the best performance from Figure 3. Best result in bold and second best underline. Environment T2MIR-AD T2MIR-DPT AD DPT DICP IDT UNICORN 90.90.0 DarkRoom 5.20.3 Point-Robot 68.92.1 Cheetah-Vel Walker-Param 435.77.2 823.97.1 Reach 665.813.9 Push 72.513.1 57.55.6 78.31.4 90.90.0 5.60.4 5.80.3 5.30.2 5.50.5 6.60.6 -5.00.5 -43.20.8 119.230.4 56.11.0 74.93.2 82.83.0 80.622.8 395.27.1 425.99.7 403.715.4 429.45.8 372.824.6 492.829.3 790.420.2 754.915.0 775.31.1 808.53.6 748.80.1 764.69.3 615.46.0 560.064.2 620.64.6 547.467.2 604.76.3 633.28.6 49.22.8 8 5.3 Robustness Study Robustness to Dataset Qualities. We evaluate T2MIR on Medium-Expert and Medium datasets. As shown in Figure 5, both T2MIR-AD and T2MIR-DPT exhibit superior performance over baselines, validating their robustness when learning from varying qualities of datasets. Notably, most baselines suffer significant performance drop when trained on Medium datasets, while T2MIR maintains satisfactory performance despite the lower data quality. It highlights T2MIRs appealing applicability in real-world scenarios where agents often learn from suboptimal data. Figure 5: Test return curves of T2MIR against baselines using Medium-Expert and Medium datasets. Figure 6: Analysis results of the number of selected experts against the total in tokenand task-wise MoE on T2MIR-AD. For example, 2/6 denotes selecting the top 2 from 6 experts. Robustness of Expert Configurations. We vary expert configurations in both MoE layers. For token-wise MoE that manages tokens of three modalities, we always activate one-third of the experts and vary the total number of experts. Results in Figure 6-(a) show that moderate configuration (2/6) yields the best performance. For task-wise MoE that manages task assignments, we always activate two experts while varying the total number of experts. Results in Figure 6-(b) show that the performance slightly increases with more experts in total, and tends to saturate soon. 5.4 Visualization Insights into MoE Structure Modality Clustering in Token-wise MoE. In Figure 1 (left), the spatial proximity indicates similarity in expert assignments. It exhibits clear clustering pattern that tokens from different modalities are routed to distinct experts, highlighting the successful utilization of the MoE structure to process tokens with distinct semantics. Task Clustering in Task-wise MoE. In Figure 1 (right), points of similar color come from similar tasks. It forms clear pattern in the router representation space, where similar tasks cluster closely and distinct tasks are widely separated. This confirms the effective use of the MoE structure to manage broad task distribution. Appendix gives more visualization on the two MoE layers. Figure 7: Cosine similarity of gradients between PointRobot tasks in four quadrants (I-IV), comparing T2MIR-AD (MoE) with AD (MLP). Gradient Conflict Mitigation. In Figure 7, AD (lower triangle) shows significant gradient conflict between opposing tasks (e.g., vs. III, deep blue with large negative cosine value) and fails to discriminate tasks (e.g., III vs. IV, deep red with large positive cosine value). In contrast, T2MIR-AD (upper triangle) maintains nearly orthogonal gradients across diverse tasks, especially for opposing ones (e.g., vs. III or II vs. IV, cosine value near 0). This comparison verifies T2MIRs advantage in both gradient conflict mitigation and task discrimination."
        },
        {
            "title": "6 Conclusions, Limitations, and Future Work",
            "content": "In the paper, we introduce architectural advances of MoE to address input multi-modality and task diversity for ICRL. We propose scalable framework, where token-wise MoE processes distinct semantic inputs and task-wise MoE handles broad task distribution with contrastive routing. Improvements in generalization capacities highlight the potential impact of our method, and deep insights via visualization validate that our MoE design effectively addresses the associated challenges. Though, our method is evaluated on widely adopted benchmarks in ICRL, with relatively lightweight datasets compared to popular large models. An urgent improvement is to evaluate on more complex environments such as XLand-MiniGrid [11, 25] with huge datasets, unlocking the scaling properties of MoE in ICRL domains. Another step is to deploy our method to vision-language-action (VLA) tasks [68, 69] that naturally involve more complex input multi-modality and task diversity."
        },
        {
            "title": "References",
            "content": "[1] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, volume 34, pages 1508415097, 2021. [2] Yunkai Gao, Rui Zhang, Jiaming Guo, Fan Wu, Qi Yi, Shaohui Peng, Siming Lan, Ruizhi Chen, Zidong Du, Xing Hu, et al. Context shift reduction for offline meta-reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 8002480043, 2023. [3] Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Yang YU, Junqiao Zhao, and Pheng-Ann Heng. Towards an information theoretic framework of context-based offline meta-reinforcement learning. In Advances in Neural Information Processing Systems, volume 37, pages 75642 75667, 2024. [4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741, 2023. [5] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 4305743083, 2023. [6] Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, et al. M3ViT: Mixture-ofexperts vision transformer for efficient multi-task learning with model-accelerator co-design. In Advances in Neural Information Processing Systems, volume 35, pages 2844128457, 2022. [7] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. RAPHAEL: Text-to-image generation via large mixture of diffusion paths. In Advances in Neural Information Processing Systems, volume 36, pages 4169341706, 2023. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901, 2020. [9] Jake Grigsby, Justin Sasek, Samyak Parajuli, Daniel Adebi, Amy Zhang, and Yuke Zhu. AMAGO-2: Breaking the multi-task barrier in meta-reinforcement learning with transformers. In Advances in Neural Information Processing Systems, volume 37, pages 8747387508, 2024. [10] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 4701647031, 2023. [11] Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, Artem Agarkov, Viacheslav Sinii, and Sergey Kolesnikov. XLand-MiniGrid: Scalable meta-reinforcement learning environments in JAX. In Advances in Neural Information Processing Systems, volume 37, pages 4380943835, 2024. [12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In Proceedings of International Conference on Machine Learning, pages 18611870, 2018. [13] Haoqi Yuan and Zongqing Lu. Robust task representations for offline meta-reinforcement learning via contrastive learning. In Proceedings of International Conference on Machine Learning, pages 2574725759, 2022. [14] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In Proceedings of International Conference on Machine Learning, volume 162, pages 2463124645, 2022. [15] Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. In Proceedings of International Conference on Machine Learning, volume 202, pages 21362 21374, 2023. 11 [16] Ilya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viacheslav Sinii, and Sergey Kolesnikov. Emergence of in-context reinforcement learning from noise distillation. In Proceedings of International Conference on Machine Learning, pages 6283262846, 2024. [17] Sili Huang, Jifeng Hu, Hechang Chen, Lichao Sun, and Bo Yang. In-context decision transformer: Reinforcement learning via hierarchical chain-of-thought. In Proceedings of International Conference on Machine Learning, 2024. [18] Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, and Sergey Kolesnikov. In-context reinforcement learning for variable action spaces. In Proceedings of International Conference on Machine Learning, pages 4577345793, 2024. [19] Johan Samir Obando Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Nicolaus Foerster, Gintare Karolina Dziugaite, Doina Precup, and Pablo Samuel Castro. Mixtures of experts unlock parameter scaling for deep RL. In Proceedings of International Conference on Machine Learning, pages 3852038540, 2024. [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of International Conference on Machine Learning, pages 11261135, 2017. [21] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline metareinforcement learning with advantage weighting. In Proceedings of International Conference on Machine Learning, pages 77807791, 2021. [22] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In Proceedings of International Conference on Machine Learning, pages 53315340, 2019. [23] Jake Grigsby, Linxi Fan, and Yuke Zhu. AMAGO: Scalable in-context reinforcement learning for adaptive agents. In Proceedings of International Conference on Learning Representations, 2024. [24] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In Proceedings of International Conference on Learning Representations, 2024. [25] Alexander Nikulin, Ilya Zisman, Alexey Zemtsov, Viacheslav Sinii, Vladislav Kurenkov, and Sergey Kolesnikov. XLand-100B: large-scale multi-task dataset for in-context reinforcement learning. In Proceedings of International Conference on Learning Representations, 2025. [26] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation. In Proceedings of International Conference on Learning Representations, 2023. [27] Jaehyeon Son, Soochan Lee, and Gunhee Kim. Distilling reinforcement learning algorithms for in-context model-based planning. In Proceedings of International Conference on Learning Representations, 2025. [28] Zhenwen Dai, Federico Tomasi, and Sina Ghiassian. In-context exploration-exploitation for reinforcement learning. In Proceedings of International Conference on Learning Representations, 2024. [29] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. In Proceedings of Is conditional generative modeling all you need for decision-making? International Conference on Learning Representations, 2023. [30] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In Proceedings of International Conference on Learning Representations, 2017. 12 [31] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. In Proceedings of International Conference on Learning Representations, 2024. [32] Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong Zhang. Transformers learn temporal difference methods for in-context reinforcement learning. In Proceedings of International Conference on Learning Representations, 2025. [33] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. In Proceedings of International Conference on Learning Representations, 2024. [34] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and Nikos Komodakis. What to hide from your students: Attention-guided masked image modeling. In Proceedings of European Conference on Computer Vision, pages 300318, 2022. [35] Siao Liu, Zhaoyu Chen, Yang Liu, Yuzheng Wang, Dingkang Yang, et al. Improving generalization in visual reinforcement learning via conflict-aware gradient agreement augmentation. In Proceedings of IEEE/CVF International Conference on Computer Vision, pages 2343623446, 2023. [36] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. LLaMA-MoE: Building mixture-of-experts from llama with continual pre-training. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 1591315923, 2024. [37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, et al. Humanlevel control through deep reinforcement learning. Nature, 518(7540):529533, 2015. [38] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):4753, 2022. [39] Zhendong Chu, Renqin Cai, and Hongning Wang. Meta-reinforcement learning via exploratory task clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11633 11641, 2024. [40] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUS). arXiv preprint arXiv:1606.08415, 2016. [41] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [42] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al. DeepSeekR1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [43] Juncheng Dong, Moyang Guo, Ethan Fang, Zhuoran Yang, and Vahid Tarokh. In-context reinforcement learning without optimal action labels. In ICML 2024 Workshop on In-Context Learning, 2024. [44] Denis Tarasov, Alexander Nikulin, Ilya Zisman, Albina Klepach, Andrei Polubarov, Lyubaykin Nikita, Alexander Derevyagin, Igor Kiselev, and Vladislav Kurenkov. Yes, Q-learning helps offline in-context RL. In ICLR 2025 Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models, 2025. [45] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:201264, 2023. [46] Suning Huang, Zheyu Zhang, Tianhai Liang, Yihan Xu, Zhehao Kou, Chenhao Lu, Guowei Xu, Zhengrong Xue, and Huazhe Xu. MENTOR: Mixture-of-experts network with task-oriented perturbation for visual reinforcement learning. arXiv preprint arXiv:2410.14972, 2024. 13 [47] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, et al. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [48] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [49] Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. CLIP-MoE: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024. [50] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. [51] Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 3745, 2012. [52] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: physics engine for model-based In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and control. Systems, pages 50265033, 2012. [53] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [54] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: benchmark and evaluation for multi-task and meta reinforcement learning. In Proceedings of Conference on Robot Learning, pages 10941100, 2020. [55] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):18, 2021. [56] Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. StableMoE: Stable routing strategy for mixture of experts. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 70857095, 2022. [57] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. [58] Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6(2):181214, 1994. [59] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [60] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Proceedings of International Conference on Machine Learning, pages 56395650, 2020. [61] Zican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, and Zhi Wang. Attention-guided contrastive role representations for multi-agent reinforcement learning. In Proceedings of International Conference on Learning Representations, 2024. [62] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97299738, 2020. [63] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, volume 35, pages 1634416359, 2022. [64] Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. 14 [65] Luisa Zintgraf, Sebastian Schulze, Cong Lu, Leo Feng, Maximilian Igl, Kyriacos Shiarlis, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: Variational Bayes-adaptive deep RL via meta-learning. The Journal of Machine Learning Research, 22(1):1319813236, 2021. [66] Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, and Dacheng Tao. Mastering massive multi-task reinforcement learning via mixture-of-expert decision transformer. In ICLR Workshop on Modularity for Collaborative, Decentralized, and Continual Deep Learning, 2025. [67] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, et al. OpenAI o1 system card. arXiv preprint arXiv:2412.16720, 2024. [68] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, et al. OpenVLA: An open-source vision-language-action model. In Proceedings of Conference on Robot Learning, 2024. [69] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Algorithm Pseudocodes Regularization Loss for Balancing Expert Utilization Contrastive Learning for Task-wise MoE Router The Details of Environments and Dataset Construction D.1 The Details of Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 The Details of Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Baseline Methods Network Architecture and Hyperparameters of T2MIR Analysis of Hyperparameters and T2MIR Architecture G.1 Hyperparameter Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Architecture Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Numerical Results for Studies of Ablation and Robustness More Visualization Insights Limitations 19 20 21 21 22 23 24 24 25 26"
        },
        {
            "title": "A Algorithm Pseudocodes",
            "content": "Based on the implementations in Sec. 4, this section gives the brief procedures of T2MIR. Algorithm 1 and Algorithm 3 show the pipline of training T2MIR-AD and T2MIR-DPT, respectively. We train all components including token-wise MoE, task-wise MoE and their routers together with the main causal transformer network end to end. Then, Algorithm 2 and Algorithm 4 show the evaluation phase, where the agent can improve its performance on test tasks by interacting with the environments without any parameter updates. Algorithm 1: Model Training of T2MIR-AD Input: Training tasks Ttrain and corresponding offline datasets Dtrain Causal transformer πθ; Batch size B; 1 for each iteration do 2 Initialize query batch = {}, key batch = {} for = 1, 2, ..., do Trajectory number L; Contrastive loss weight wcon"
        },
        {
            "title": "Prompt length T",
            "content": "Sample task Mi Ttrain and obtain the corresponding dataset Di from Dtrain Sample trajectories from Di Sort trajectories by return and concatenate to τi = {s0, a0, r0, . . . , sT , aT , rT } Sample another trajectories from Di to obtain τ to and respectively Add τi and τ as positive key b=1 and key batch = {τ }B end Get query batch = {τ }B Autoregressively predict the actions with πθ and compute loss in Eq. 10 using Compute balance loss Lbalance in Eq. 3 Compute contrastive loss Lcontrastive with key batch in Eq. 7 Update πθ by minimize loss = L(θ) + Lbalance + wcon Lcontrastive Update momentum router in task-wise MoE in Eq. 8 b=1 Algorithm 2: Model Evaluation of T2MIR-AD Input: Test tasks Ttest; Causal transformer πθ; History buffer R; History episode length Evaluation episodes 15 16 end 3 5 6 7 8 9 11 12 13 14 3 5 6 7 8 9 11 12 13 1 for each task Mi Ttest do 2 Initialize history buffer = {} for = 1, , do Reset env. and get feedback s0 for each timestep do Predict action at = πθ(st, R) Step env. and get feedback st+1 and rt Add st, at, rt to buffer end Sort by episode return if > then Drop the first episode in end end 14 15 end 17 3 9 10 11 13 14 15 16 17 19 20 21 3 4 6 7 8 9 10 Algorithm 3: Model Training of T2MIR-DPT Input: Training tasks Ttrain and corresponding offline datasets Dtrain Causal transformer πθ; Batch size B; Prompt length ; Contrastive loss weight wcon Expert policy π 1 for each task Mi Ttrain do 2 Obtain dataset Di from Dtrain Get optimal action for states in Di using expert policy π Obtain query dataset Dq 4 5 end 6 Obtain query datasets Dq 7 for each iteration do 8 train Initialize query batch = {}, key batch = {} for = 1, 2, ..., do Sample task Mi Ttrain Obtain the corresponding dataset Di from Dtrain and query dataset Dq Sample state-action pairs (s, a) and (s, a) from Dq Sample trajectory from Di and obtain τi = {s0, a0, r0, . . . , sT , aT , rT } Sample another trajectory from Di to obtain τ (cid:1) and (cid:0)(s, a), τ Add (cid:0)(s, a), τi (cid:1) to and respectively as positive key from Dq train end Get query batch = (cid:8)(cid:0)(s, a), τ (cid:1)(cid:9)B Autoregressively predict the optimal action with πθ and compute loss in Eq. 11 using Compute balance loss Lbalance in Eq. 3 Compute contrastive loss Lcontrastive with key batch in Eq. 7 Update πθ by minimize loss = L(θ) + Lbalance + wcon Lcontrastive Update momentum router in task-wise MoE in Eq. 8 b=1 and key batch = (cid:8)(cid:0)(s, a), τ (cid:1)(cid:9)B b= 22 23 end Algorithm 4: Model Evaluation of T2MIR-DPT Causal transformer πθ; Input: Test tasks Ttest; History buffer R; Episode history buffer τ Evaluation episodes 1 for each task Mi Ttest do 2 Initialize episode history buffer τ = {} for = 1, , do Initialize history buffer = {} Reset env. and get feedback s0 for each timestep do Predict action at = πθ(st, τ ) Step env. and get feedback st+1 and rt Add st, at, rt to buffer end τ end 12 13 end"
        },
        {
            "title": "B Regularization Loss for Balancing Expert Utilization",
            "content": "In this section, we provide detailed information about the regularization loss mentioned in Sec. 4.1. To prevent the gating network from converging to state where it consistently assigns large weights to the same few experts, we adopt both importance loss and load-balancing loss as proposed by Shazeer et al. [30]. The importance loss aims to encourage all experts to have equal significance within the model, and the importance of an expert relative to batch of training examples is defined as the sum of the gate values assigned to that expert: Imp(h) = (cid:88) xX Gtok(xh), (12) where Gtok(xh) represents the gate value for expert given input x. An additional loss term Limportance is then added to the overall loss function, which is calculated as the square of the coefficient of variation of the set of importance values multiplied by hand-tuned scaling factor wimp: Limportance = wimp CV (Imp(h))2. (13) Despite ensuring equal importance among experts, discrepancies may still arise in the number of input tokens each expert receives due to the top-k activation mechanism. One expert might be assigned large weights for only few tokens while another expert could receive small weights across many tokens but fail to activate. To address this issue, load-balancing loss is introduced to ensure that each expert handles approximately the same number of input tokens. To achieve this, smooth estimator Load(h) of the number of examples assigned to each expert. This estimator allows gradients to propagate through backpropagation, by utilizing the noise term in the gating function, which we omit in Eq. 2. Given trainable weight of noise matrix Wnoise, Eq. 2 is rewrite as wtok(i; h) = softmax(H(ih))[i], H(ih) = topk(cid:0)Gtok(ih) + StandardNormal() Softplus(h Wnoise)i (cid:1). (14) Let (i; h) denote the probability that wtok(i; h) is non-zero, given new random noise choice for element i, while keeping the already sampled noises for other elements constant. Note that, wtok(i; h) is non-zero if and only if H(ih) is greater than the kth-greatest element of H(h) excluding itself. Thus, we have (i; h) = r(cid:0)Gtok(ih)+StandardNormal()Softplus(hWnoise)i > kth_excluding(H(h), k, i)(cid:1), (15) where kth_excluding(H(h), k, i) means the kth-greatest element of H(h), excluding i-th element. Based on this, we can compute: (i; h) = Φ (cid:18) Gtok(ih) kth_excluding(H(ih), k, i) Softplus((h Wnoise)i) (cid:19) , (16) where Φ is the cumulative distribution function (CDF) of the standard normal distribution. Then, the estimated load for expert is given by: Load(i; h) = (cid:88) (i; h). (17) Finally, the load-balancing loss is defined as the square of the coefficient of variation of the load vector, scaled by hand-tuned parameter wload: Lload = wload CV (Load(h))2. (18) Combining both losses yields the final regularization loss showed in Eq. 3 used in our model which helps maintain balanced utilization of experts during training, preventing any single expert from dominating the computation. 19 Contrastive Learning for Task-wise MoE Router In this section, we give the proof of Theorem 1 based on lemma as follows. Lemma 1. Give task from the task distribution (M ), let = (τ ) as the average of hidden state after self-attention calculation of task , Gtask(zh). Then, we have p(M z) p(M ) = Eh (cid:20) p(zh) p(z) (cid:21) . Proof. p(M z) p(M ) = = (cid:90) p(zM ) p(z) p(zh)p(hM ) p(z) (cid:21) (cid:20) p(zh) p(z) . = Eh (19) (20) dh The proof is completed. Theorem 1. Let denote set of tasks following the task distribution (M ), and = . is given task. Let = (τ ), Gtask(h), and e(h, z) = p(zh) p(z) , where τ is trajectory from task and is average hidden state of all tokens after the self-attention calculation function (). Let denote the average hidden state generated by any task M, then we have I(z; ) log + M,z,h (cid:20) log (cid:80) e(h, z) e(h, z) (cid:21) . (21) Proof. Using Lemma 1 and Jensens inequality, we have M,z,h (cid:20) log (cid:80) (cid:21) e(h, z) e(h, z) = M,z,h log p(zh) p(z) + (cid:80) p(zh) p(z) MM p(zh) p(z) = M,z,h log 1 + p(z) p(zh) p(zh) p(z) (cid:88) MM M,z,h = M,z,h (cid:20) (cid:18) log 1 + (cid:20) (cid:18) log 1 + p(z) p(zh) p(z) p(zh) (N 1)EM MM (cid:21)(cid:19)(cid:21) (cid:20) p(zh) p(z) (cid:19)(cid:21) (N 1) 1 1 + p(z) p(zh) (N 1) 1 p(z) p(zh) (cid:21)(cid:21) (cid:20) p(zh) p(z) (cid:21) log = M,z,h log M,z,h log EM,z (cid:20) log Eh (cid:20) log = EM,z p(M z) p(M ) = I(z; ) log N. log (22) Thus, we complete the proof."
        },
        {
            "title": "D The Details of Environments and Dataset Construction",
            "content": "In this section, we show details of the evaluation environments over variety of benchmarks, as well as the collection of offline datasets on these environments. D.1 The Details of Environments We evaluate T2MIR and all the baselines on classical benchmarks including discrete environment commonly used in ICRL [5], the 2D navigation, the multi-task MuJoCo control [52] and the MetaWorld [54]. More specifically, we evaluate all tested methods on the following environments as DarkRoom: discrete environment where the agent navigates in 10 10 grid to find the goal. The observation is the 2D coordinate of the agent, and the actions are left, right, up, down, and stay. The agent is started from [0, 0] to find the goal which is uniformly sampled from the grid. The reward is sparse and only = 1 when the agent is at the goal, and = 0 otherwise. It provides 100 tasks, from which we randomly sample 80 tasks as training tasks and hold out the remaining 20 for evaluation. The maximal episode step is set to 100. Point-Robot: problem of point agent navigating to given goal position in the 2D space. The observation is the 2D coordinate of the robot. The action space is [0.1, 0.1]2 with each dimension corresponding to the moving distance in the horizontal and vertical directions. The reward function is defined as the negative distance between the point agent and the goal location. Tasks differ in goal positions that are uniformly distributed in unit square, resulting in the variation of the reward functions. We randomly sample 45 goals as training tasks, and another 5 goals for evaluation. The start position is sampled uniformly from [0.1, 0.1]2 for each learning episode and the maximal episode step is set to 20. Cheetah-Vel: multi-task MuJoCo continuous control environment in which the reward function differs across tasks. It requires planar cheetah robot to run at particular velocity in the positive x-direction. The observation space is R20, which comprises the position and velocity of the cheetah, the angle and angular velocity of each joint. The action space is [1, 1]6 with each dimension corresponding to the torque of each joint. The reward function is negatively correlated with the absolute value between the current velocity of the agent and the goal, plus the control cost. The goal velocities are uniformly sampled from the distribution [0.075, 3.0]. We randomly sample 45 goals as training tasks, and another 5 goals for evaluation. The maximal episode step is set to 200. Walker-Param: multi-task MuJoCo benchmark where tasks differ in state transition dynamics. It need to control two-legged walker robot to run as fast as possible with varying environment dynamics. The observation space is R17 and the action space is [1, 1]6. The reward function is proportional to the running velocity in the positive x-direction, which remains consistent for different tasks. The physical parameters of body mass, inertia, damping, and friction are randomized across tasks. We randomly sample 45 physical parameters as training tasks, and another 5 for evaluation. The maximal episode step is set to 200. Reach, Push: two typical environments from the robotic manipulation benchmark Meta-World. Reach and Push control robotic arm to reach goal location in 3D space and to push the puck to goal, respectively. The observation space is R39, which contains current state, previous state, and the goal. We only use the current state vector, thus the observation space is modified to R18 without goal information. The action space is [1, 1]4. Tasks differ in goal positions which are uniformly sampled, resulting in the variation of reward functions. The initial position of object is fixed across all tasks. We randomly sample 45 goals as training tasks, and another 5 goals for evaluation. The maximal episode step is set to 100. Furthermore, we give the details of the heterogeneous version Cheetah-Vel we used in Appendix I. Cheetah-Vel-3_Cluster: heterogeneous version of Cheetah-Vel, where the goal velocities are different. We sample the goal velocities from three Gaussian distributions: (0.5, 0.152), (1.5, 0.152), (2.5, 0.152). From each Gaussian distribution, we sample 14 tasks for training and 2 tasks for evaluation, resulting in 42 training tasks and 6 test tasks. 21 Table 2: Hyperparameters of SAC used to collect multi-task datasets."
        },
        {
            "title": "Environments",
            "content": "Point-Robot Cheetah-Vel Cheetah-Vel3_Cluster"
        },
        {
            "title": "Training Warmup",
            "content": "steps 2000 250000 400000 steps 100 2000 Walker-Param"
        },
        {
            "title": "Discount Entropy",
            "content": "factor ratio 20 10000 10000 10000 3e-4 3e3e-4 3e-4 0.005 0.005 0.005 0.005 0.99 0. 0.99 0.99 0.2 0.2 0.2 0.2 Table 3: Hyperparameters of PPO used to collect multi-task datasets."
        },
        {
            "title": "Environments",
            "content": "total_timesteps n_steps learning_rate batch_size n_epochs γ"
        },
        {
            "title": "Reach\nPush",
            "content": "400000 1000000 2048 2048 3e-4 3e-4 64 64 10 10 0.99 0. D.2 The Details of Datasets For discrete environment DarkRoom, we use the expert policy to collect datasets by progressively reducing the noise, as in [16]. For Point-Robot and MuJoCo environments, we employ the soft actor-critic (SAC)[12] algorithm to train policy independently for each task, and the detailed hyperparameters are shown in Table 2. For Meta-World environments, we use the implementation of Stable Baselines 3[55], and the detailed hyperparameters are shown in Table 3. During training, we periodically save the policy checkpoints and use them to generate various qualities of offline datasets as Mixed: using all policy checkpoints to generate datasets. We use each checkpoint to generate same number of episodes, e.g., using one checkpoint to generate one episode. Medium-Expert: using the policy checkpoints whose performance is below 80% of the final achieved level to generate datasets. The max performance in Medium-Expert datasets is about 80% of that in Mixed datasets. Medium: using the policy checkpoints whose performance is below 50% of the final achieved level to generate datasets. The max performance in Medium datasets is about 50% of that in Mixed datasets. For query state-action pairs used in DPT and T2MIR-DPT, we use the expert policies that achieve 100%, 80% and 50% performance to provide actions for states in Mixed, Medium-Expert and Medium datasets, respectively. As we find the actions provided by expert policies contain too many values outside the boundary of the action space in Meta-World environments, we just use the state-action pairs in offline datasets as query datasets for Reach and Push."
        },
        {
            "title": "E Baseline Methods",
            "content": "This section gives the details of the five representative baselines, including four ICRL approaches and one context-based offline meta-RL method. These baselines are thoughtfully selected as they are representative in distinctive categories. Furthermore, since our proposed T2MIR method belongs to the ICRL category, we incorporate more methods from this class as baselines for comprehensive comparison. The detailed descriptions of these baselines are as follows: AD [26], is the first method to achieve ICRL through sequential modeling of offline historical data using an imitation loss. By employing causal transformer with sufficiently large across-episodic contexts to imitate gradient-based RL algorithms, AD learns improved policies for new tasks entirely in context without requiring external parameters updating. 22 DPT [5], is an ICRL method that models contextual trajectories via supervised learning to predict optimal actions. After pretraining on offline datasets, DPT provides contextual learning capabilities for new tasks, enabling online exploration and offline decision-making given query states and contextual prompts. IDT [17], is an ICRL method that simulates high-level trial-and-error processes. It introduces an innovative architecture comprising three modules: Making Decisions, Decisions to Go, and Reviewing Decisions. These modules generate high-level decisions in an autoregressive manner to guide low-level action selection. Built upon transformer models, IDT can address complex decision-making in long-horizon tasks while reducing computational costs. DICP [27], combines model-based reinforcement learning with previous ICRL algorithms like AD and IDT. By jointly learning environment dynamics and policy improvements in context, DICP employs transformer architecture to perform model-based planning and decision-making without updating model parameters Furthermore, DICP generates actions by simulating potential future states and predicting long-term returns. UNICORN [3], is context-based offline meta-RL algorithm grounded in information theory. It proposes unified information-theoretic framework, which focuses on optimizing different approximation bounds of the mutual information objective between task variables and their latent representations. Leveraging the information bottleneck principle, it derives novel general and unified task representation learning objective, enabling the acquisition of robust task representations. To ensure fair comparison, all baselines are adjusted to an aligned setting, where test datasets are not available for policy evaluation. We also standardize the size and quality of the offline datasets for all baselines. In our experimental setup, AD faces difficulties in distilling effective policy improvement operators due to the scarcity of available offline historical data. Thus, we use an enhanced version of AD that incorporates trajectory-ranking mechanism based on cumulative rewards, inspired by AT [15]. Network Architecture and Hyperparameters of T2MIR This section gives details of the architecture and hyperparameters of T2MIR implementations as follows. Router Network. We implement the router network using multi-layer perceptron (MLP) architecture without bias. Specifically, the router contains two linear layers with Tanh as the activation function between them. The first linear maps hidden state to n_experts-dim vector along with the Tanh activation, and the second linear maps the n_experts-dim vector to another n_experts-dim vector as the output of router. Architecture of T2MIR. We implement the T2MIR-AD based on causal transformer akin to AMAGO [23] with Flash Attention [63]. For T2MIR-DPT, we build our T2MIR framework upon the transformer architecture in DPT [5], flatten the trajectories into state-action-reward sequence as input. Specifically, we employ separate embeddings for states, action, and rewards, adding learnable positional embedding based on timesteps. Each block employs multi-head self-attention module followed by feedforward network or MoE layer with GELU activation [40]. For action prediction, we employ linear to map the output of transformer blocks to an action vector with Tanh activation. Table 4 and Table 5 show the detailed hyperparameters used for T2MIR-AD and T2MIR-DPT using Mixed datasets, respectively. Compute. We train our models on one Nvidia RTX4080 GPU with the Intel Core i9-10900X CPU and 256G RAM. The training process takes about 0.5-3 hours, depending on the complexity of the environments. Compared with T2MIR-free backbones, T2MIR takes lightly higher computational cost during the training process, but it requires less computational resources during task inference. 23 Table 4: Hyperparameters in training process of T2MIR-AD using Mixed datasets. Hyperparameters DarkRoom Point-Robot Cheetah-Vel Walker-Param Reach"
        },
        {
            "title": "Push",
            "content": "training steps learning rate Prompt length token experts K1 task experts K2 InfoNCE weight MoE layer position Momentum ratio β 300000 3e-4 400 6 12 0.01 top 0.995 100000 3e-4 80 6 8 0.01 top 0.995 100000 3e-4 800 6 8 0.01 top 0.995 100000 3e-4 800 6 8 0.01 top 0.995 100000 5e-5 400 6 8 0.01 top 0.995 3e-4 400 6 8 0.01 top 0.995 Table 5: Hyperparameters in training process of T2MIR-DPT using Mixed datasets. Hyperparameters DarkRoom Point-Robot Cheetah-Vel Walker-Param Reach"
        },
        {
            "title": "Push",
            "content": "training steps learning rate Prompt length token experts K1 task experts K2 InfoNCE weight MoE layer position Momentum ratio β 300000 3e-4 100 6 8 0.001 top 0.995 100000 3e-4 20 6 8 0.001 top 0.995 100000 3e-4 200 6 8 0.01 top 0.995 100000 3e-4 200 6 8 0.01 top 0.995 100000 5e-5 100 6 8 0.01 top 0.995 3e-4 100 6 8 0.001 top 0.995 Analysis of Hyperparameters and T2MIR Architecture G.1 Hyperparameter Analysis Analysis of Expert Configurations on T2MIR-DPT. We also investigate different expert configurations on T2MIR-DPT, as shown in Figure 8. Table 6 and Table 7 give the numerical results of T2MIR with different expert configurations in tokenand task-wise MoE, respectively. For token-wise MoE, as same as T2MIR-AD, the results show that moderate configuration (2/6) outperforms both smaller (1/3) and larger (4/12) settings. For task-wise MoE, the performance improves with the increase number of total experts, and tends to be stable. Figure 8: Analysis results of the number of selected experts against total experts in tokenand task-wise MoE on T2MIR-DPT. For example, 2/6 denotes selecting the top 2 from 6 experts. Weight of InforNCE Loss. We also investigate the influence of the loss weight of InfoNCE loss in task-wise MoE, the results are shown in Figure 9 and Table 8. The performance of T2MIR is not sensitive to small weights. But when the InforNCE loss weight is set to 0.1, it gains an obvious decrease. We suppose this is because the final loss of Lcontrastive in Eq. 7 has same magnitude with imitation loss L(θ) in Eq. 10 and Eq. 11, and it disturbs the overall learning process. 24 Table 6: Numerical results of T2MIR with varying expert numbers in token-wise MoE. Best in bold. T2MIR-AD 1/3 2/ 4/12 T2MIR-DPT 1/3 2/6 4/12 Point-Robot 6.60.7 Cheetah-Vel 86.12. 5.60.2 -5.20.3 -68.92.1 80.10.5 Cheetah-Vel 53.31.3 Point-Robot 6.10.5 5.30.3 -5.00.5 -43.20.8 50.71.9 Table 7: Numerical results of T2MIR with varying expert numbers in task-wise MoE. Best in bold. T2MIR-AD 2/4 2/6 2/8 T2MIR-DPT 2/4 2/ 2/8 5.20.3 5.30.3 Point-Robot Cheetah-Vel 91.727.9 76.31.7 5.20.2 -5.20.3 -68.92.1 Cheetah-Vel 51.57.4 45.92.1 Point-Robot 5.20.4 -5.00.5 -43.20. Figure 9: Test return curves of T2MIR with different contrastive loss weights. Table 8: Numerical results of T2MIR with different loss weights of InfoNCE loss, i.e., numerical results of best performance from Figure 9. Best result in bold. T2MIR-AD 0.1 0.01 0. T2MIR-DPT 0.1 0.01 0.001 Point-Robot Cheetah-Vel 148.45.0 6.91. -5.20.1 -68.92.1 74.11.0 Cheetah-Vel 49.41.6 Point-Robot 5.20.3 6.10.4 5.40.1 -43.20.8 54.91.3 -5.00. G.2 Architecture Analysis Position of MoE Layer. Further more, we explore how the placement of the MoE layer affects performance. We substitute the feedforward network in one transformer block with MoE layer and study three settings of different positions: bottom, middle and top, where the MoE layer is used in the first, 2 -th and last transformer block, respectively. Figure 10 and Table 9 present the results, where substituting the feedforward network in the last transformer block with MoE layer (top) gets more stable performance. Figure 10: Test return curves of T2MIR with different positions of MoE layer. 25 Table 9: Numerical results of T2MIR with different positions of MoE layer, i.e., numerical results of best performance from Figure 10. Best result in bold. T2MIR-AD bottom middle top T2MIR-DPT bottom middle top Point-Robot 6.00.8 5.80.3 Cheetah-Vel 5.70.4 -57.91.6 70.25.4 68.92.1 Cheetah-Vel 52.94.0 43.82.2 Point-Robot 5.50.3 -5.20. -5.00.5 -43.20."
        },
        {
            "title": "H Numerical Results for Studies of Ablation and Robustness",
            "content": "The numerical results for ablation and the robustness to dataset quality are presented in Table 10 and Table 11, respectively. Table 10: Numerical results of ablation study on both T2MIR-AD and T2MIR-DPT using Mixed datasets, i.e., the best performance from Figure 4. Best result in bold. T2MIR-AD T2MIR w/o Task-MoE w/o Token-MoE AD -68.92.1 Cheetah-Vel Walker-Param 435.77.2 665.813.9 Push 76.13.7 418.63.9 656.817.8 101.129.7 410.85.1 634.04.0 119.230.4 395.27.1 604.76.3 T2MIR-DPT T2MIR w/o Task-MoE w/o Token-MoE DPT -43.20.8 Cheetah-Vel Walker-Param 492.829.3 633.28.6 Push 49.53.4 446.16.5 627.24.0 47.51.6 475.332.5 628.07.2 56.11.0 425.99.7 615.46. Table 11: Numerical results of T2MIR against baselines using Medium-Expert (top) and Medium (bottom) datasets, i.e., numerical results of best performance from Figure 5. Best result in bold and second best underline. Medium-Expert T2MIR-AD T2MIR-DPT AD DPT DICP IDT UNICORN Cheetah-Vel Push 77.81.5 654.424.3 -56.06.8 130.324.2 67.30.6 73.41.6 83.114.4 80.622.8 513.751.1 586.39.5 573.68.5 485.529.2 632.97.1 595.43. Medium T2MIR-AD T2MIR-DPT AD DPT DICP IDT UNICORN Cheetah-Vel Push 76.611.4 600.56.8 -63.34.0 135.819.2 67.93.2 75.62.5 105.34.1 105.93.7 577.716.8 514.212.7 416.920.9 554.39.8 402.4162.9 530.06."
        },
        {
            "title": "I More Visualization Insights",
            "content": "t-SNE Visualization on Push. We also visualize the multimodal property and task clustering in T2MIR-AD on Push, as shown in Figure 12. The visualization results consistently exhibit the ability of token-wise MoE to process token from different modalities with different experts, and task-wise MoE to distribute trajectories from different tasks to different experts. Heterogeneous Setting. For further study of the task clustering ability of T2MIR, we construct heterogeFigure 11: return curves of neous version Cheetah-Vel inspired by MILET [39]. We T2MIR against baselines on Cheetahsplit the velocities of tasks into three intervals, constructVel-3_Clustering using Mixed datasets. ing Cheetah-Vel-3_Cluster environment. Figure 11 presents the test return curves of T2MIR and baselines using Mixed datasets. We demonstrate the probability of task assignments to some experts, as shown in Figure 13, the velocities from three different distributions are divided by dashed lines. The expert-1 dominates at lower speeds, the expert-2 dominates at higher speeds, and it exhibits mixture of different experts for medium Test 26 Figure 12: t-SNE visualization of expert assignments on Push. Left: token-wise MoE enables different experts to process three-modality tokens. Right: task-wise MoE effectively manages task distribution, where trajectories from same task are prone to be distributed to same experts. Figure 13: Probability of task assignments to some experts. The goal velocities are sampled from three distributions and divided by dashed lines. speeds, with expert-1 being dominant. Moreover, there are switched among different experts at the boundaries between different velocity distributions. The results indicate that distinct experts dominate different tasks sampled from the three distributions, which further validates our motivation to employ token-wise MoE."
        },
        {
            "title": "J Limitations",
            "content": "We discuss the limitations and computational efficiency of our method in this section. Constrained by limited computing resources, our method is trained on lightweight datasets, e.g., the Mixed datasets of Push contain 500 episodes per task and 45 training tasks, which have totally 2.25 million transitions. Although the size of datasets are small, we find its enough to gain high performance. But the size of datasets limits the capacity of MoE in extending the size of model parameters. Future work may evaluate their study on more complex environments and larger datasets such as XLandMiniGrid [11, 25]. The efficiency of contrastive loss in task-wise MoE when facing massive number of tasks is not thoroughly explored in this work. It is interesting to explore whether the contrastive loss is effective for managing task assignments in scenarios involving large number of tasks. Integrating the T2MIR framework incurs slightly higher computational cost during the training process, but it requires less computational resources during task inference."
        }
    ],
    "affiliations": [
        "Australian Artificial Intelligence Institute, University of Technology Sydney",
        "Nanjing University"
    ]
}