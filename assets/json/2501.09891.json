{
    "paper_title": "Evolving Deeper LLM Thinking",
    "authors": [
        "Kuang-Huei Lee",
        "Ian Fischer",
        "Yueh-Hua Wu",
        "Dave Marwood",
        "Shumeet Baluja",
        "Dale Schuurmans",
        "Xinyun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver."
        },
        {
            "title": "Start",
            "content": "2025-1-"
        },
        {
            "title": "Evolving Deeper LLM Thinking",
            "content": "Kuang-Huei Leea,b,1, Ian Fischera,1, Yueh-Hua Wuc,2, Dave Marwood1, Shumeet Baluja1, Dale Schuurmans1,3 and Xinyun Chen1 aFirst author contribution, bSenior author contribution, cWork done as student researcher at Google DeepMind, 1Google DeepMind, 2UC San Diego, 3University of Alberta We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of formal solver. 1. Introduction How can large language model (LLM) be guided to think deeper about complex problem and leverage inference time compute to improve its problem solving ability? Prior research has investigated various strategies for leveraging inference time compute, such as chain-of-thought [41, 21], self-consistency [39], sequential revision based on feedback [36, 30, 8, 19, 1], and search guided by auxiliary verifiers or evaluators [43]. When solution evaluator is available, search strategies have an advantage of being able to reliably improve problem solving ability with increased compute. For example, methods such as Bestof-N [4, 24, 25] and tree search [37] naturally exploit additional compute to explore larger set of solution candidates, thereby increasing the probability of finding successful solution. To better exploit inference time compute, we propose an evolutionary search strategy for LLMs that combines free-flowing stochastic exploration with large-scale iterative refinement. We refer to this approach as Mind Evolution. As illustrated in Figure 1, Mind Evolution is genetic search strategy that evolves diverse population of candidate solutions, leveraging an LLM to generate, recombine and refine solution candidates based on feedback from an evaluator. The overall process is analogous to combining divergent thinking (free-flowing parallel idea exploration) with convergent thinking (idea evaluation and selection), considered as hallmarks of intelligent problem solving behavior [14]. Unlike Best-of-N, which searches broadly by generating independent candidates for evaluation, Mind Evolution searches both broadly and deeply, exploring diverse set of candidates and refining the most promising alternatives. Unlike sequential reasoning approaches, such as self-refinement or tree search [37, 25], which require evaluation of individual reasoning steps, Mind Evolution performs global refinement of complete solutions, and therefore only requires global solution evaluator rather than stepwise process reward. Also, typical of evolutionary methods, Mind Evolution can be easily parallelized. There has been prior work on combining evolutionary search with LLMs, primarily in the literature on evolutionary program generation [34, 17, 29, 23, 6]. However, this prior work focuses on searching through formal program spaces, using guidance from execution feedback or code explanation. By contrast, Mind Evolution is not restricted to searching in formal space. This allows Mind Evolution to be applied to problems that are not formalized, or remain difficult to formalize, as long as programmatic solution evaluator is available. In particular, we focus on natural language planning tasks where candidate solutions can still be automatically parsed, evaluated and critiqued using an implementable oracle evaluator. This approach exploits the observation that it is often easier to evaluate the quality of candidate solution than it is to generate good solutions for given problem [11]. In the domain of natural language planning, we consider the TravelPlanner [42] and Natural Plan [47] benchmarks, where constraint satisfaction problems are expressed in natural language without any explicit formalization of the underlying objectives, constraints or variables. These problems require set of interconnected decisions that satisfy set of global and local constraints. For example, in TravelPlanner, travel plan should be produced that respects various accommodation and dinning constraints, while also considering budget limitations and other preferences, all expressed solely in natural language. To date, LLMs have yet to achieve good performance on these tasks 2025 Google DeepMind. All rights reserved 5 2 0 2 J 7 1 ] . [ 1 1 9 8 9 0 . 1 0 5 2 : r Evolving Deeper LLM Thinking Figure 1 Mind Evolution is genetic-based evolutionary search strategy that operates in natural language space. The figure illustrates how Mind Evolution evolves population of solution candidates toward higher quality candidates for travel planning task. The candidate population is improved through an iterative process, where an LLM is used to recombine and refine candidates in each iteration. without the aid of formal solvers [16]. For example, Gemini 1.5 Flash and o1-preview only achieve success rate of 5.6% and 11.7% on TravelPlanner respectively, while for the Meeting Planning domain in Natural Plan, they respectively only achieve 20.8% and 44.2%. Even exploiting Best-of-N over 800 independently generated responses, Gemini 1.5 Flash still only achieves 55.6% success on TravelPlanner and 69.4% on Meeting Planning. In this paper, we show that exploration and refinement with evolutionary search can notably improve problem solving ability. In particular, when controlling for inference time compute, Mind Evolution allows Gemini 1.5 Flash to achieve 95.6% success rate on TravelPlanner and 85.0% on Meeting Planning. We further experiment with two-stage approach, where any unsolved problem instances are subsequently tackled by Mind Evolution with Gemini 1.5 Pro, which leads to 100% success on TravelPlanner and 98.4% on Meeting Planning. All of the experiments in this paper only use off-the-shelf LLMs without any finetuning. To our knowledge, the only prior work that achieves comparable performance on the TravelPlanner benchmark is [16], which leverages an auxiliary formal solver and requires the LLM to first translate given problem instance into an equivalent formalization. In general, it takes significant effort and expertise to correctly formalize problem expressed in natural language; prompting an LLM to correctly perform such translation requires at least as much domain expertise. Mind Evolution removes this constraint by directly optimizing solutions in the space of natural language. Finally, we introduce new benchmark problem, StegPoet, that involves encoding hidden message in generated essay, story or poem. This form of stenography [33] is difficult to formalize and solve, yet hidden message detector can still be implemented to programmatically guide the search. Our motivation is to demonstrate the applicability of search beyond natural language domains that can be easily formalized. We find that Mind Evolution allows Gemini 1.5 Pro to achieve success rate of 87% in this task. 2. Related Work Pairing LLMs with Evolutionary Search In addition to the program generation studies discussed in Section 1, several recent works have explored combining LLMs and evolution for numerical optimization [26, 3] and combinatorial optimization [28, 44]. The problem spaces we tackle in this work, such as natural language planning, can also be viewed as combinatorial optimization problems optimizing plans subject to constraints specified in natural language. In contrast to these previous studies, we focus on evolving solutions in natural language spaces instead of formal spaces. This removes the requirement of task formalization, which requires significant effort and expert knowledge for each task instance. Other works have also applied evolutionary search to prompt optimization, with the goal of improving performance on target tasks [45, 10, 15]. Among these, EvoAgent [45] also evaluated their approach on the TravelPlanner benchmark. In contrast to our work, which performs evolutionary search directly on 2 Evolving Deeper LLM Thinking plans, EvoAgent evolves new LLM agents to form multi-agent system for problem solving. Their best success rate on the TravelPlanner validation set was 7.2% with GPT-4, while our approach achieved over 95% with Gemini 1.5 Flash. then enter the population. Population fitness generally increases over successive generations, as parents with greater fitness are more likely to be selected for recombination. Pairing LLMs with Evaluators In this work, we evaluate solutions with program-based evaluators during the evolutionary search. The idea of integrating execution-based evaluators in the inference loop has been widely adopted in the literature of code generation, where the execution environment provides feedback for the LLM to fix bugs in the generated code [7, 22, 27, 46, 8, 17, 29, 23, 6, 36]. Other prior work has also considered using learned verifiers, reward models, or self-evaluation for response refinement [20, 30], search [37, 4, 9, 43, 35], and improving model learning [40, 25, 32, 1]. These approaches can often be applied to wider domains and free-form solutions, but learned feedback models or self-evaluators can be noisy and are not perfectly reliable. We leave consideration of such approximate feedback mechanisms for future work. 3. Method Mind Evolution employs genetic search strategy, combined with an LLM and tailored set of prompts, to orchestrate an efficient search for solutions to natural language planning tasks. Before describing Mind Evolution in detail, we first provide brief overview of language-based genetic algorithms. Island Model To sustain diversity in an evolving population it is also helpful to introduce an island model [38, 5], where distinct sub-populations (islands) are created and evolved independently between migration and island reset events that occur at specified frequencies. For migration operation, the solutions on one island are stochastically chosen based on fitness to migrate to an adjacent island. For an Island Reset operation, the populations on islands with low overall fitness are replaced by strong solutions from the global population, which also has selection effect. The island model has been adopted in recent successful efforts, such as FunSearch [34]. Language-based Genetic Representation The individual candidates in language-based genetic algorithm are represented by natural language. This allows the strong language understanding and generation capabilities of an LLM to be leveraged to implement powerful recombination (crossover and mutation) and island reset operations through prompting. 3.2. Mind Evolution Figure 1 illustrates the design of Mind Evolution, with its hyperparameters listed in Table 1. The core components of Mind Evolution are: 3.1. Language-based Genetic Algorithm Overview 1. the specific choices for the selection and migration Genetic algorithms [18, 12, 31] are meta-heuristic inspired by natural selection. In genetic algorithm, population of candidate solutions is evolved toward populations that contain greater proportion of higher quality individuals with respect to target optimization objective. Such an objective is also often referred to as the fitness function. Each individual candidate has genetic representation that can be mutated and recombined with others. Evolutionary search usually begins with population of independently generated candidate solutions. In each generation, the fitness of every individual is evaluated with respect to the target objective. Candidates are then stochastically selected for reproduction based on their fitness (selection). In reproduction, the genetic representations of selected parents are combined (crossover) and potentially altered (mutation) to produce new child solutions. Such process creates the next generation of children, which operations; 2. the set of prompts that implement the initialization, recombination (crossover and mutation), and island reset operations with an LLM; 3. the fitness function that evaluates the quality of given solution and optionally provides feedback on issues detected. The overall evolution process is repeated until valid solution is found, or until ùëÅgens generations have been completed, after which the best scoring candidate is returned. Fitness Evaluation As discussed in Section 1, we implement fitness function for each problem domain, where candidate solutions are parsed and evaluated programmatically. In principle, any function that can evaluate solution quality can be used, including LLM evaluation. The evaluation function plays three key 3 Evolving Deeper LLM Thinking ùëÅ Parameter Default Value Description 10 4 5 4 3 2 5 15 5 1/6 5 5 ùëÅgens ùëÅ island ùëÅconvs ùëÅseq reset interval ùëÅreset ùëÅtop candidate ùëÅparent ùëÉùëüno parents ùëÅemigrate ùëÅretries The maximum number of generations to search for solution. How many independent populations to evolve. How many conversations per island. How many turns per conversation. How frequently to reset islands in generations. How many islands to reset. Lowest mean score islands are chosen. How many starting parents to transfer to islands when reset. How many candidate parents to consider when resetting islands with the LLM. Maximum number of parents conversation can have. Probability of conversation having no parents. How many plans to emigrate to the next island after each island. How many times to try to generate plan before giving up at each turn. ùëÅ Table 1 Definition of hyperparameters in Mind Evolution. Unless otherwise specified, the experiments in work use the default values. The product of the first four hyperparameters gives the maximum number of candidate solutions generated (800 in the default setting). roles in Mind Evolution: (1) scoring solutions by measuring the optimization objective, if any; (2) verifying whether the solution satisfies given constraints; and (3) providing corresponding textual feedback. For example, the evaluation function for the Meeting Planning task scores proposed plan and provides textual feedback based on how many constraints are violated (e.g. meetings conflict with existing schedules), how many valid meeting events are included in the schedule, and whether the plan follows the required format (see Appendix A.2 for more details). We have found that using textual feedback is important empirically, as shown in our ablation study in Section 4.4. Note that for many classical search problems (e.g., NP-complete problems), verifying solutions can be much easier than solving the problem [11]. Similarly, we observe that it is possible to write an evaluation function for the natural language planning tasks we consider. The ability to check the correctness of candidate solution does not obviously lead to the ability to generate valid solution in the tasks we consider. That is, implementing an evaluation function is not equivalent to solving the task. Population Initialization Given target problem, we independently sample ùëÅconvs initial solutions by prompting an LLM with description of the problem, any information needed for solving the problem, and relevant instructions. If ùëÅseq > 1, each of these initial solutions is then evaluated and refined sequentially through ùëÅseq 1 additional turns of the Refinement through Critical Conversation process explained below. In total, this initialization procedure generates ùëÅconvs ùëÅseq candidate solutions, which forms the initial population on the first island for the first generation. Figure 2 Illustrating the Refinement through Critical Conversation (RCC) process, where an initial solution is first proposed, then evaluated and subjected to feedback from critic, after which an author proposed refined solution and the process iterates. Refinement through Critical Conversation (RCC) Given candidate solution (or set of candidate solutions for the process of recombination) we leverage an LLM to generate an improved solution by organizing critical conversation between critic character and an author character, as illustrated in Figure 2. Separating these two roles is intended to improve the critical thinking ability of an LLM. Each conversational turn is structured as prompt-driven process, where solutions are refined based on critical feedback, similar to Reflexion [36]. In particular, the critic first analyzes the candidate solution(s) provided as input, interprets the textual evaluation feedback, and suggest ways to correct any issues presented in the feedback. The author then proposes single refined solution based on the input candidate(s), the subsequent evaluation(s), and the critics analyses. The specific prompts used to drive these conversations are given in Appendix A.1. An ablation study in Section 4.4 shows that the critics analysis step provides substantial performance improvements. 4 Evolving Deeper LLM Thinking Selection To produce the next generation of an island, we follow Boltzmann tournament selection [13] where 0 to ùëÅparent parents are stochastically sampled from the population according probability distribution that is derived from softmax transformation of their fitness scores. In this way, higher-performing solutions are more likely to be selected for reproduction, while other candidates can still be occasionally selected for diversity. Crossover and Mutation We implement the crossover and mutation operations as single recombination step, where an LLM is instructed to improve given set of parents using the RCC process described above (Figure 2). In particular, for recombination we sample 1 to ùëÅparent parents and alter Step (b) in Figure 2 to first incorporate the evaluation results of the parents, then apply the critic to all parents and propose the revised solution as an initial solution for the next generation. Then, if ùëÅseq > 1, we continue to follow Steps (c)(d)(e) to sequentially generate ùëÅseq 1 child solutions by refining each previous child using the RCC process. For each generation on each island, ùëÅconvs ùëÅseq child solutions are added to the island population, with duplicate solutions removed. For selection, we follow Boltzmann tournament instead of explicitly retiring candidate solutions, except when performing an Island Reset below. Migration between Islands Between migration events, each island population is evolved independently. During migration, the top ùëÅemigrate solutions are cloned from the current Island ùëñ to the next Island ùëñ + 1 after completing the generation on the current island (we update the populations on the islands sequentially from 1 to ùëÅisland). Migration is performed cyclically between the islands, so emigrants from Island ùëÅisland arrive at Island 1. We have found that this form of cyclic migration accelerates the overall evolution process. Island reset happens every ùëÅreset interval Island Reset generations. During an Island Reset event, the top performers are first selected from the global population, the populations on ùëÅreset islands with the lowest average scores are retired, and the selected top performers are cloned onto the reset islands. To select top performers, we explore two approaches: (1) directly select the top ùëÅtop candidates according to fitness; and (2) first select the top ùëÅcandidate candidates according to fitness, then prompt the LLM to select ùëÅtop good candidates from this pool that are substantially different from each other. The ablation study in Section 4.4 show that the latter strategy, using an LLM for Island Reset, achieves better performance. 4. Experiments Tasks We evaluate Mind Evolution on three benchmark natural language planning domains: two tasks from Natural Plan [47], including Trip Planning (Section 4.2) and Meeting Planning (Section 4.3), and the TravelPlanner [42] benchmark (Section 4.1). (We omit the Calendar Scheduling task from Natural Plan, since these problems can be solved by enumeration.) Implementation details for each task is provided in Appendix A, including the prompts (Appendix A.1) and evaluation functions used (Appendix A.2). Models We use Gemini 1.5 Flash (gemini-1.5-flash001) as the default LLM in our experiments below. The hyperparameters used when applying Mind Evolution to Flash are specified in Table 1. In addition to evaluating Mind Evolution with the Flash model, we also investigate two-stage approach, where Gemini 1.5 Pro model (gemini-1.5-pro-exp-0827) is used to tackle problems that are not solved within the ùëÅgens generation limit. Such two-stage approach provides better cost-efficiency than using the Pro model on every problem instance. When applying Mind Evolution to the Pro model we alter the hyperparameters from those specified in Table 1 to: ùëÅconvs = 8, ùëÅseq = 3, ùëÅparent = 10, ùëÉùëüno parents = 1/5. Baselines For each task, we compare Mind Evolution to three baseline search strategies that use the same solution evaluator and task-specific prompts: 1. 1-Pass, where solution is proposed using single forward pass of the LLM. 2. Best-of-N [4], where up to 800 candidate solutions are independently generated until successful solution is found (the same upper bound as Mind Evolution). 3. Sequential-Revision+, where 10 candidate solutions are proposed independently, then revised separately for 80 turns using the RCC process (Figure 2). Note that 10 independent threads of 80-turn refinements are used instead of single 800-turn refinement, because we rarely observe improvements after 80 turns. This baseline is similar to running 10 trials of multi-turn Reflexion [36]. Additionally, for reference, we also include an additional 1-Pass baseline that uses OpenAI o1-preview. 5 Evolving Deeper LLM Thinking Metrics We measure Success Rate as the percentage of problem instances that are solved completely within benchmark domain, separating the validation and test sets. (Note that the Success rate is referred to as Solve Rate in Natural Plan [47] and Final Pass Rate in TravelPlanner [42].) To assess the cost of inference compute we report the number of LLM calls, the number of input and output tokens, and the total API cost of calling the LLM. (These costs are given in US Dollars, using prices from October 2024 when the experiments were conducted. The base rates are listed in Appendix D.) Note that assessing computational cost is particularly important when evaluating search strategies like Mind Evolution, since search is more expensive than generating single solution. These statistics can help researchers and developers understand the cost-benefit trade-offs when using search to enhance LLM problem solving ability. 4.1. TravelPlanner TravelPlanner [42] is natural language planning benchmark that simulates the problem of organizing trip plan for user who expresses preferences and constraints. We focus on the sole-planning mode (see [42] for details), where each problem instance consists of list of options regarding accommodation, restaurants, attractions and transportation, plus additional constraints that specify user preferences for budget, cuisine, etc. plan is evaluated based on whether it satisfies the user preferences and commonsense constraints. Table 2 gives detailed results that compare the overall Success Rate and computational cost of Mind Evolution versus the baseline strategies. In terms of Success Rate, Mind Evolution clearly outperforms the baseline strategies, achieving over 95%. By comparison, Sequential-Revision+ provides reasonable baseline, achieving almost 83%, while Best-of-N struggles, achieving only 55.6%. Overall, these results demonstrate clear advantage of an evolutionary strategy that combines broad search, through stochastic exploration, with deep search that leverages an LLM for solution refinement. Considering the two-stage approach, where Mind Evolution uses Gemini 1.5 Pro for any unsolved problems, we find that nearly the entire dataset can be solved, achieving 100% success rate on validation and 99.9% on test problems respectively. The only work we are aware of that comes close to this success rate is [16], which uses GPT-4 for auto-formalization then leverages formal solver to achieve 98.9% and 97.0% on validation and test respectively. Mind EvoFigure 3 Success rate on the validation set of the TravelPlanner benchmark, organized by problem instance difficulty and the number of travel days. lution achieves comparable results without requiring formal solver. Finally, we note that the TravelPlanner dataset is organized into three levels of difficulty (Easy, Medium, Hard) and three trip durations (3 days, 5 days, 7 days), rendering 9 different problem classes. Figure 3 presents breakdown of the success rates achieved across these different categories, showing that the success rates of 1-Pass and Best-of-N decline when planning for more travel days, but the trend is less clear for Mind Evolution and Sequential-Revision+, both of which iteratively refine proposed solutions. 4.2. Natural Plan Trip Planning The Trip Planning task [47] involves finding an itinerary that consists of sequence of cities to visit and number of days in each that satisfies flight connectivity and scheduling constraints see Table 3 for problem instance. We split the benchmark into 320 validation and 1,280 test instances (described in more detail in Appendix B). The results in Table 2 again show that Mind Evolution strongly outperforms the baselines on this task, achieving 96.2% on the validation and 94.1% on the test instances. Table 2 also shows qualitative comparison between the results produced by Mind Evolution and the baseline strategies. Note that Best-of-N performs better in this scenario (77.2%), even beating Sequential-Revision+ (74.4%). We find that for the two-stage approach, Mind Evolution achieves 100% on the validation set and 99.6% on the test set. These findings again highlight the benefit of evolutionary search versus simple sampling and sequential refinement. Finally, we note that the difficulty of this task varies with the number of cities to visit, ranging from 3 to 10. Figure 4 shows breakdown of the Success Rate in terms of number of cities, where the relative advantage of Mind Evolution appears to increase as the number of cities grows. 6 Evolving Deeper LLM Thinking TravelPlanner [42]"
        },
        {
            "title": "Success Rate LLM Calls",
            "content": "Input Tokens Output Tokens API Cost (Oct 2024) 1-Pass (o1-preview 1-Pass) val val Best-of-N val Sequential-Revision+ val Mind Evolution val val Mind Evolution test test (+pro) (+pro) Natural Plan [47] Trip Planning 10/180 = 5.6% 21/180 = 11.7% 100/180 = 55.6% 149/180 = 82.8% 172/180 = 95.6% 180/180 = 100% 952/1000 = 95.2% 999/1000 = 99.9% 1-Pass (o1-preview 1-Pass) val val Best-of-N val Sequential-Revision+ val Mind Evolution val val 66/320 = 20.6% 116/320 = 36.2% 247/320 = 77.2% 238/320 = 74.4% 308/320 = 96.2% 320/320 = 100% Mind Evolution test 1204/1280 = 94.1% test 1275/1280 = 99.6% (+pro) (+pro) Natural Plan [47] Meeting Planning 1-Pass (o1-preview 1-Pass) val val Best-of-N val Sequential-Revision+ val Mind Evolution val val Mind Evolution test test (+pro) (+pro) 104/500 = 20.8% 221/500 = 44.2% 347/500 = 69.4% 310/500 = 62.0% 425/500 = 85.0% 492/500 = 98.4% 419/500 = 83.8% 491/500 = 98.2% 1 1 472 280 174 (257) 167 (67) 1 1 274 391 168 (111) 196 (211) 1 1 444 484 406 (890) 394 (828) 0.009M 0.008M 4.44M 35.53M 3.10M (3.25M) 3.02M (3.05M) 0.002M 0.002M 0.61M 41.57M 1.48M (1.51M) 1.78M (1.86M) 0.007M 0.006M 3.99M 32.16M 5.35M (13.36M) 5.24M (12.25M) 0.001M 0.008M 0.47M 0.29M 0.18M (0.19M) 0.18M (0.18M) 0.001M 0.008M 0.18M 0.38M 0.19M (0.19M) 0.22M (0.24M) 0.001M 0.006M 0.31M 0.40M 0.41M (0.91M) 0.40M (0.83M) US$0.001 US$0.601 US$0.47 US$2.75 US$0.29 (US$0.54) US$0.28 (US$0.33) <US$0.001 US$0.53 US$0.10 US$3.23 US$0.17 (US$0.22) US$0.20 (US$0.37) US$0.001 US$0.47 US$0.39 US$2.53 US$0.52 (US$2.55) US$0.51 (US$2.34) Table 2 Experimental results on benchmark natural language planning tasks. (+pro) denotes the two-stage results, where we use Gemini 1.5 Pro to solve the problems that were not solved in experiments using Gemini 1.5 Flash. Number of LLM calls, token counts, and API cost are averaged across the validation or test problem set, and they are calculated only on the remaining problems for the (+pro) experiments. Here, we also show OpenAI o1-preview results as reference. Figure 4 Success rate on the validation set of the Trip Planning benchmark per number of cities to visit. 4.3. Natural Plan Meeting Planning For the Meeting Planning task sequence of meetings should be scheduled to maximize the number of meetings between individuals subject to availability, location and travel time constraints [47]. This task differs from TravelPlanner and Trip Planning in that not every meeting can be scheduled for every problem instance, implying that it is not possible to know whether an optimal solution has been reached. Therefore, to obtain the results shown in Table 2, we allow the searches to proceed until the upper bounds on iteration counts have been reached. For this task, we split the set of instances into 500 validation and 500 test instances (see Appendix for details). The results shown in Table 2 continue to demonstrate significant performance for Mind Evolution over baseline strategies, achieving an 85.0% Success Rate on the validation set and 83.8% on the test set. Notably, the two-stage approach using Gemini 1.5 Pro achieves success rates to 98.4% and 98.2% on validation and test respectively. Finally, Figure 5 shows the breakdown of success rates by the number of people to schedule meetings with. In this case, we find that Mind Evolution sustains significant advantage in success rate as the number of people increases. Evolving Deeper LLM Thinking Q: You plan to visit 5 European cities for 16 days in total. You only take direct flights to commute between cities. You want to spend 5 days in Madrid. From day 3 to day 7, there is annual show you want to attend in Madrid. You plan to stay in Zurich for 3 days. You would like to visit Frankfurt for 3 days. You would like to visit Santorini for 6 days. You are going to attend wedding in Santorini between day 7 and day 12. You want to spend 3 days in Riga. Here are the cities that have direct flights: Zurich and Riga, Frankfurt and Riga, Santorini and Zurich, Madrid and Zurich, Frankfurt and Zurich, Madrid and Santorini, Frankfurt and Madrid. Find trip plan of visiting the cities for 16 days by taking direct flights to commute between them. Method 1-Pass Best-of-N Answer Madrid (Day 1-7) Santorini (Day 7-12) Zurich (Day 12-14) Frankfurt (Day 14-16) Riga (Day 16-19) √ò 7 days for Madrid instead of 5; 4 days for Riga instead of 3; 19 days in total instead of 16. Madrid (Day 1-7) Santorini (Day 7-12) Zurich (Day 12-14) Frankfurt (Day 14-16) Riga (Day 16-16) √ò 7 days for Madrid instead of 5; 1 day for Riga instead of 3. Sequential Revisions+ Zurich (Day 1-3) Frankfurt (Day 3-5) Riga (Day 5-7) Santorini (Day 7-12) Madrid (Day 12-16) √ò omitted the show in Madrid (Day 3-7); no direct flight from Riga to Santorini. Mind Evolution (ours) Frankfurt (Day 1-3) Madrid (Day 3-7) Santorini (Day 7-12) Zurich (Day 12-14) Riga (Day 14-16) Table 3 An example problem instance from the Trip Planning task in Natural Plan, with the predicted plans from Mind Evolution and the baselines. 1-Pass and Best-of-N both make mistakes on number of days to stay, but satisfy the requirements of being in Madrid and Santorini on specific days. The Sequential-Revision+ plan omits the annual show in Madrid and plans non-existent flight, but is correct in the number of days. In contrast, the Mind Evolution plan satisfies all specified requirements. Figure 5 Success rate on the validation set of the Meeting Planning benchmark per number of people to meet with. 4.4. Analysis and Ablation Studies To understand how Mind Evolutions performance scales, and how the different components affect its behavior, we provide additional measurements and ablations to gain additional insight. Scaling Regarding scaling, Figure 6 reports the Success Rate achieved by Mind Evolution across the planning tasks as function of the number of generations. These results clearly show steady improvement for Mind Evolution as the number of generations is increased. To compare the scaling of Mind Evolution to that of the baseline search methods, we also plot the Success Rate and average task evaluation scores as function of the number of candidate solutions generated by the each strategy (Figures 79). The task evaluation scores are calculated by penalizing unsatisfied constraints and suboptimality of the objective value, hence the maximum score that can be achieved in any problem instance is zero (see Appendix A.2 for details). In Appendix D, we provide another perspective on the cost-benefit trade-offs in terms of the specific API costs incurred. Figures 79 show the results for the TravelPlanner, Trip Planning and Meeting Planning tasks respectively. In each case, we see that the overall success rates and average task evaluation scores improve monotonically with an increasing number of proposed solutions across all search methods. These plots also show that Mind Evolution is consistently more effective than the baseline strategies with respect to the number of candidate solutions needed to achieve specified level of success rate (or average task performance). We note that Best-of-N appears to be significantly underperforming on TravelPlanner. We hypothesize that this occurs because this task involves implicit commonsense constraints (e.g., trip plan should return to the origin city, restaurant cannot be visited twice, etc.), which are not given in the problem instance but instead learned from evaluation feedback, which Best-of-N does not leverage. 8 Evolving Deeper LLM Thinking Figure 6 Success rate on the validation set for each natural language planning benchmark at each generation of Mind Evolution. Figure 9 Meeting Planning success rates and evaluation scores as the number of candidate solutions is increased. Critic S/Q Prompts Textual Feedback Reset with LLM Success Rate 46.1% 71.1% 76.1% 91.1% 95.6% Table 4 An ablation study of Mind Evolution components on the TravelPlanner validation set. Each column in the table shows an experiment where indicates whether component is used. If Critic is disabled, we skip the critic step in Figure 2 and go straight to the author step. S/Q Prompts stands for Strategy/Question prompts, which are additional taskspecific instructions in the critical thinking prompts (see Appendix A.1 for details). If Textual Feedback is disabled, we do not include evaluation feedback in the prompts. If Reset with LLM is disabled, we directly select global elites by their evaluation scores in island reset events, rather than use an LLM to choose, as described in Section 3.2. island = 4, ùëÅconvs = 5) island = 1, ùëÅconvs = 20) w/ island model (ùëÅ w/o island model (ùëÅ ùëÅconvs = 10, ùëÅgens = 5 ùëÅconvs = 5, ùëÅgens = 10 (default) ùëÅconvs = 4, ùëÅgens = 13 Succ. Rate 87.5% 77.4% 82.5% 87.5% 85.0% Table 5 Hyperparameter studies on the Trip Planning problem instances with 10 cities. The first two rows show the difference between enabling and disabling the island model. The bottom three rows illustrate trade-off between the number of candidates per generation versus the number of generations. (Note that the bottom row (ùëÅconvs = 4, ùëÅgens = 13) produces slightly more than 800 solutions (832). observed on the harder problem instances from the other benchmark tasks.) In Table 5, the top two rows compare the effect of including or excluding the island model from the evolutionary search, controlling for the same number (800) of candidate solutions. These results show that the island model significantly 9 Figure 7 TravelPlanner success rates and evaluation scores as the number of candidate solutions is increased. Figure 8 Trip Planning success rates and evaluation scores as the number of candidate solutions is increased. Ablations We also conducted set of ablations to study the contribution of the different components of Mind Evolution. Table 4 shows that using the critic step in the RCC process (Figure 2 in Section 3.2) and textual feedback from the evaluation functions are the most critical to performance, although the other components also make meaningful contributions to performance. To assess hyperparameter sensitivity, we investigated the Trip Planning task in greater detail, choosing the harder setting with 10 cities to better reveal differences in performance. (Similar results are also Evolving Deeper LLM Thinking"
        },
        {
            "title": "Task",
            "content": "Results (Cipher and Poem) Message To Encode (M): 10,20,30,40, 50,60,70,80, 90,100,10,20 Style: Poem Inspiration: Shel Silverstein Words Between (B): 4 Topic: \"The Fun of Walking\" 10 : rooster; 20 : flowers; 30 : bright; 40 : flames; 50 : cherry; 60 : crimson; 70 : sunset; 80 : ruby; 90 : scarlet; 100 : burning; like to walk, like to stride, With ROOSTER crows and FLOWERS by my side. like to walk, like to roam, Past BRIGHT green fields and FLAMES at home. like to walk, like to stroll, To see the CHERRY trees and CRIMSON hole. like to walk, like to ramble, To watch the SUNSET and hold RUBYs gamble. like to walk, like to wander, To watch SCARLET leaf, and fire, BURNING, under. like to walk, like to stride, With ROOSTER crows and FLOWERS by my side. Figure 10 StegPoet example. Example of the encoding of StegPoet problem instance (left) and correct solution (right) that includes the number-to-word cipher and poem in the style of childrens poetry author. Note that ùëÄ = 12 in this instance. We added capitalization to the code words to highlight them. 1-Pass Success Rate Set 0/101 = 0.0% val 1/101 = 1.0% Best-of-N val 20/101 = 19.8% Sequential-Revision+ val 47/101 = 46.5% Mind Evolution val 88/101 = 87.1% val Mind Evolution test 106/245 = 43.3% test 194/245 = 79.2% (+pro) (+pro) Input Tokens Output Tokens API Cost (Oct 2024) <$0.001 < 0.001M $0.19 0.25M $3.20 0.24M $0.33 0.20M $0.65 0.22M 0.22M 3.63M 0.24M 3.84M 0.002M 1.56M 41.69M 3.56M 3.74M $0.34 $0.72 Table 6 Experimental results on StegPoet. Price and token counts are averages per problem. All results use Gemini 1.5 Flash, except (+pro), which solves the problems that were not solved in the Flash runs, using Gemini 1.5 Pro. Figure 11 Histogram of Success Rate for each difficulty level. 1-Pass returns valid responses, but fails to solve any of the problems, so it is not visible in the histogram. improves the performance of Mind Evolution. The bottom three rows compare the effect of increasing the number of candidate solutions per generation versus having more generations while controlling for similar number of candidates considered overall. In this case, it appears that deeper evolutionary search indeed has benefits, although it is also important to continue exploring broadly in each generation. 5. Challenging New Task: StegPoet We introduce challenging new task, StegPoet, where hidden message should be stenographically encoded [33] into piece of creative writing. Even though the problem is difficult to formalize, it remains amenable to programmatic verification, which makes it addressable by the methods considered in this paper. In this task, hidden message (ùëÄ) expressed by sequence of numbers should be encoded in piece of creative text about particular topic, expressed in the form of an essay, story or poem. The goal is to both provide number-to-word substitution cipher and generated text that uses the cipher to encode the message. Figure 10 gives an example. We impose an additional constraint that there must be, on average, ùêµ words between successive cipher words in the generated text, which ensures that simply listing the cipher words as the text portion does not qualify as solution when ùêµ > 0. The difficulty of this problem varies along four axes: 1. Difficulty increases with the length of the hidden message, ùëÄ. We set 10 ùëÄ 30. 2. The repetition of the numbers in ùëÄ. The more repetition, the more stringent the constraints. 3. The closeness of the repeated numbers to each other. Each form of writing dictates how much repetition of the same word and proximity of occurrence is acceptable. The LLM must balance adherence to the form with the need to correctly encode the message. 4. Empirically, as ùêµ (the mean distance between cipher words) grows, the problem becomes more difficult. 10 Evolving Deeper LLM Thinking In our tests, 3 ùêµ 7. We divide the problem instances into validation split of 101 instances and test split of 245 instances. See Appendix for additional details about the StegPoet evaluation. Detailed performance results for Mind Evolution and the baseline strategies are given in Table 6, while Figure 11 shows performance per difficulty level. Here the two-stage Mind Evolution (+pro) achieves 87.1% on validation and 79.2% on test. Best-of-N only manages to solve 1% of the validation tasks. 6. Conclusion We have presented Mind Evolution, an evolutionary search approach for solving challenging natural language planning problems, by scaling inference-time compute for stochastic exploration and iterative refinement. An evaluation on the TravelPlanner and Natural Plan natural language planning benchmarks, as well as new benchmark StegPoet introduced in this paper, demonstrates that Mind Evolution significantly outperforms Best-of-N and sequential revision. To our knowledge, this is the first approach that is able to achieve such level of success on these tasks without explicitly leveraging formal solver. Limitations The main limitation of the current work is the focus on natural language planning problems where proposed solutions can be programmatically evaluated and critiqued. In future work, we aim to extend beyond this limitation by developing LLM-based evaluators that would enable broader applications."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors thank Sergio Guadarrama and Doina Precup for supporting this work. We also thank Sirui Xie, John Canny, and the Google DeepMind FunSearch team for valuable discussion."
        },
        {
            "title": "References",
            "content": "[1] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022. formation Theory, 67(6):32873294, 2021. doi: 10.1109/TIT.2020.2996543. [3] S. Brahmachary, S. M. Joshi, A. Panda, K. Koneripalli, A. K. Sagotra, H. Patel, A. Sharma, A. D. Jagtap, and K. Kalyanaraman. Large language model-based evolutionary optimizer: Reasoning with elitism. arXiv preprint arXiv:2403.02054, 2024. [4] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R√©, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [5] E. Cant√∫-Paz et al. survey of parallel genetic algorithms. Calculateurs paralleles, reseaux et systems repartis, 10(2):141171, 1998. [6] A. Chen, D. M. Dohan, and D. R. So. EvoPrompting: Language models for code-level neural arIn Proceedings of the 37th chitecture search. International Conference on Neural Information Processing Systems, pages 77877817, 2023. [7] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. CodeT: Code genIn The Eleventh eration with generated tests. International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=ktrw68Cmu9c. [8] X. Chen, M. Lin, N. Sch√§rli, and D. Zhou. to selfTeaching large language models debug. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= KuPixIqPiq. [9] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rockt√§schel. Promptbreeder: Self-referential self-improvement via prompt arXiv preprint arXiv:2309.16797, evolution. 2023. [11] M. R. Garey and D. S. Johnson. Computers and Intractability: Guide to the Theory of NP Completeness. W. H. Freeman & Co., 1979. [2] B. Berger, M. S. Waterman, and Y. W. Yu. Levenshtein distance, sequence comparison and biological database search. IEEE Transactions on In- [12] D. E. Golberg. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison Wesley, 1989. 11 Evolving Deeper LLM Thinking [13] D. E. Goldberg. note on Boltzmann tournament selection for genetic algorithms and population-oriented simulated annealing. Complex Systems, 4:445460, 1990. [25] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [14] J. P. Guilford. The Nature of Human Intelligence. 1967. [15] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023. [16] Y. Hao, Y. Chen, Y. Zhang, and C. Fan. Large language models can plan your travels rigorously with formal verification tools. arXiv preprint arXiv:2404.11891, 2024. [17] E. Hemberg, S. Moskal, and U.-M. OReilly. Evolving code with large language model. Genetic Programming and Evolvable Machines, 25(2):21, 2024. [18] J. H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI, 1975. second edition, 1992. [19] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. arXiv preprint arxiv:2303.17491, 2023. [20] J. H. Kirchner, Y. Chen, H. Edwards, J. Leike, N. McAleese, and Y. Burda. Prover-verifier games improve legibility of LLM outputs. arXiv preprint arXiv:2407.13692, 2024. [21] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zeroshot reasoners. Advances in Neural Information Processing Systems, 35:2219922213, 2022. [22] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. [23] J. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, and K. O. Stanley. Evolution through large models. In Handbook of Evolutionary Machine Learning, pages 331366. Springer, 2023. [26] F. Liu, X. Lin, Z. Wang, S. Yao, X. Tong, M. Yuan, and Q. Zhang. Large language model for multi-objective evolutionary optimization. arXiv preprint arXiv:2310.12541, 2023. [27] J. Liu, Y. Zhu, K. Xiao, Q. FU, X. Han, Y. Wei, and D. Ye. RLTF: Reinforcement learning from unit test feedback. Transactions on Machine Learning Research, 2023. ISSN 2835URL https://openreview.net/ 8856. forum?id=hjYmsV6nXZ. [28] S. Liu, C. Chen, X. Qu, K. Tang, and Y.-S. Ong. Large language models as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation (CEC), pages 18. IEEE, 2024. [29] V. Liventsev, A. Grishina, A. H√§rm√§, and L. Moonen. Fully autonomous programming with large language models. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 11461155, 2023. [30] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. [31] M. Mitchell. An Introduction to Genetic Algorithms. MIT press, 1998. [32] J. S. Park, J. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122, 2023. [33] N. Provos and P. Honeyman. Hide and seek: An introduction to steganography. IEEE security & privacy, 1(3):3244, 2003. [34] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. [24] Z. Liang, Y. Liu, T. Niu, X. Zhang, Y. Zhou, and S. Yavuz. Improving LLM reasoning through scaling inference computation with collaborative verification. arXiv preprint arXiv:2410.05318, 2024. [35] A. Setlur, C. Nagpal, A. Fisch, X. Geng, J. Eisenstein, R. Agarwal, A. Agarwal, J. Berant, and A. Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. arXiv preprint arXiv:2410.08146, 2024. 12 Evolving Deeper LLM Thinking [36] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement Information Advances in Neural learning. Processing Systems, 36, 2024. [37] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [38] R. Tanese. Distributed genetic algorithms for function optimization. University of Michigan, 1989. [39] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reaIn The Eleventh soning in language models. International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=1PL1NIMMrw. [40] Z. Wang, Y. Li, Y. Wu, L. Luo, L. Hou, H. Yu, and J. Shang. Multi-step problem solving through verifier: An empirical analysis on modelarXiv preprint induced process supervision. arXiv:2402.02658, 2024. [41] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chainof-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [42] J. Xie, K. Zhang, J. Chen, T. Zhu, R. Lou, Y. Tian, Y. Xiao, and Y. Su. Travelplanner: benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024. [43] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1180911822, 2023. [44] H. Ye, J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim, J. Park, and G. Song. ReEvo: Large language models as hyper-heuristics with reflective evolution. arXiv preprint arXiv:2402.01145, 2024. [45] S. Yuan, K. Song, J. Chen, X. Tan, D. Li, and D. Yang. EvoAgent: Towards automatic multiagent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228, 2024. [46] K. Zhang, D. Wang, J. Xia, W. Y. Wang, and L. Li. ALGO: Synthesizing algorithmic programs with LLM-generated oracle verifiers. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 54769 54784, 2023. [47] H. S. Zheng, S. Mishra, H. Zhang, X. Chen, M. Chen, A. Nova, L. Hou, H.-T. Cheng, Q. V. Le, E. H. Chi, et al. NATURAL PLAN: Benchmarking LLMs on natural language planning. arXiv preprint arXiv:2406.04520, 2024. 13 Evolving Deeper LLM Thinking A. Implementation Details Here we describe the implementation details of Mind Evolution. The code will be made available. A.1. Prompt Design We first use Meeting Planning as an example to illustrate the structure of the prompts used. The prompts, as well as the model responses when parent solutions are given, are shown in Figures 12-16. The prompts begin with general instructions and general problem definition, few-shot examples, then task description. The few-shot examples help the LLM understand the problem and generate solutions closer to the desired formats. For TravelPlanner, we take two 3-day example plans from the training set and use them across all tasks (3-7 days). For Trip Planning, we take two example plans from the few-shot examples provided by the benchmark and use them across all tasks. For Meeting Planning, we use the 5-shot examples provided by the benchmark for each task. After the task description, we include parent solutions with corresponding evaluation feedback, followed by critical thinking instructions (in Figures 14 15). These instructions lead the LLM to improve the parent solutions, following the Refinement through Critical Conversation (RCC) process described in Section 3.2. The critical thinking instructions include problem-specific Strategy/Question prompts based on findings in each validation set (ablated in Section 4.4). In the model responses, one can see that the LLM follows the critical thinking instructions in playing the critic role to analyze the parent solutions, and playing the author role to propose new solution. We also give an example of the prompt and model response for TravelPlanner, which has the same structure, in Figures 1722. A.2. Evaluation Functions In this work, solutions are evaluated programmatically with function. As described in Section 3.2, an evaluation function has three main roles: (1) scoring solutions by measuring the optimization objective, if any; (2) verifying whether the solution satisfies given constraints; and (3) providing corresponding textual feedback. Specifically, we score natural language plans by penalizing the constraints that are not satisfied, the objectives that are not maximized, and for not following the required solution format. Thus the maximum score for all tasks is zero. We also provide textual feedback that describes how the constraints are not satisfied and how the objectives are not maximized. TravelPlanner Our evaluation function for TravelPlanner is modified from the TravelPlanner evaluation code [42]. The evaluation code expects travel plans in JSON format. We modify the original evaluation code to make it output cumulative score that reflects all the constraints that are not satisfied, instead of simply answering whether or not plan satisfies all the constraints. We also make it provide textual feedback for the violated constraints. In the TravelPlanner validation set, the constraints are provided in both user query text and structured JSON format. However, in the test set, the constraints are only described in user query text. To make it easier for the evaluation function to consider the constraints, we extract them from user query into JSON using Gemini 1.5 Flash. For example, to extract the requested cuisines, we prompt Gemini with Look at the following text and tell me if there are any cuisine requirements on the upcoming trip... multiple times, and formulate the final answer via majority voting. To verify the reliability of this approach, we tested on the validation set and found complete agreement between the JSON extracted from user query and the provided JSON. In addition, we upload our test solutions to the TravelPlanner evaluation server, and found that the results agree with the official evaluation. Trip Planning Similar to TravelPlanner, the Trip Planning evaluation function expects plans in JSON format. Since Trip Planning user queries are programmatically generated, we can parse the constraints specified in user queries. These constraints include number of days to stay in city, specific days to be in city (e.g., for events), and whether there are flights between cities. Our evaluation function scores plan by the constraints that are not satisfied and whether it conforms with the desired JSON format, while also providing corresponding textual feedback. Meeting Planning The Meeting Planning evaluation function also expects plans in JSON. Constraints are also provided in structured JSON format. Unlike TravelPlanner and Trip Planning, Meeting Planning has an optimization objective the number of friends to meet with. We modify the original evaluation evaluation function to score proposed plan by how many people that are not going to be met with; whether it conflicts with the schedules of other people; whether it includes meetings with the same person more than once; whether any part of the plan conflict with other parts; whether it follows the desired format as instructed. In Figures 2324 we present the evaluation function that implements the simple logic described above as an example. 14 Evolving Deeper LLM Thinking Figure 12 Example Meeting Planning prompt and model response with parent solutions given (Part 1) 15 Evolving Deeper LLM Thinking Figure 13 Example Meeting Planning prompt and model response with parent solutions given (Part 2) 16 Evolving Deeper LLM Thinking Figure 14 Example Meeting Planning prompt and model response with parent solutions given (Part 3) 17 Evolving Deeper LLM Thinking Figure 15 Example Meeting Planning prompt and model response with parent solutions given (Part 4) 18 Evolving Deeper LLM Thinking Figure 16 Example Meeting Planning prompt and model response with parent solutions given (Part 5) 19 Evolving Deeper LLM Thinking Figure 17 Example TravelPlanner prompt and model response with parent solutions given (Part 1) 20 Evolving Deeper LLM Thinking Figure 18 Example TravelPlanner prompt and model response with parent solutions given (Part 2) 21 Evolving Deeper LLM Thinking Figure 19 Example TravelPlanner prompt and model response with parent solutions given (Part 3) 22 Evolving Deeper LLM Thinking Figure 20 Example TravelPlanner prompt and model response with parent solutions given (Part 4) 23 Evolving Deeper LLM Thinking Figure 21 Example TravelPlanner prompt and model response with parent solutions given (Part 5) 24 Evolving Deeper LLM Thinking Figure 22 Example TravelPlanner prompt and model response with parent solutions given (Part 6) 25 Evolving Deeper LLM Thinking import e e from i import Any , Sequence f t _ n _ l ( plan : t [ ] , , r _ a n : , t _ e : e _ e e : t [ , Any ] , t e _ r : t [ , Any ] ) : \" \" \" l e meeting plan . Args : plan : s planned p , v PM. , Marina t t You meet James 75 minutes from 3:45PM 5:00PM . ] such as [ You r R i i t 9:00AM. , 7 minutes and i t 9:07AM. , You t i 3:45 You a _ a n : Your t _ e : e _ e e : i i r d such as { Stephanie : { a n : t l t time , such as 10:30AM a n , i l time , s D r , r _ e : 1 0 : 3 0AM , amount time t meeting , end_time : 1 : 3 0PM , meeting_time : 120}} t e _ r : t e between a n , such as { Marina t t : { s D r : 20} , s D r : { Marina t t : 19}} \" \" \" met_with = {} r = 0 . 0 d k = [ ] _ a n = r _ a n _ e = e e . e e . p e ( t _ e , e s t e ( plan , s ) \"% s i plan : : t . r i ( \" You r \" ) : n u i e . r i ( \" You v \" ) : t t = p . i ( \" v u _ e = _ e + e e . e t ( \" ) [ 1 ] . i ( \" \" ) [ 0 ] . i ( ) minutes=d a _ r [ _ a n ] [ t t ] ) _ a n = t t e s . r i ( \" You t \" ) : raw_end_time = p . i ( \" t i end_time = None : \" ) [ 1 ] . i ( \" . \" ) [ 0 ] . i ( ) end_time = e e . e e . p e ( raw_end_time , \"% e a E r : r = 2 d k . append ( \" \" { p } \" examples . \" ) end_time <= _ e : n i because time format doesn l t n _ e _ = end_time . f e (\"% r = 2 d k . append ( \" \" { p } \" i l because but p i s a a ends { _ e _ } and you cannot go backwards time . \" ) _ e = end_time f e . r i ( \" You meet \" ) : Figure 23 The Meeting Planning evaluation function (part 1). 26 Evolving Deeper LLM Thinking person = p . i ( \" meet person met_with : \" ) [ 1 ] . i ( \" r \" ) [ 0 ] . i ( ) r = 2 d k . append ( \" \" { p } \" more than once . \" ) n i because you would be meeting with { person } met_with [ person ] = 1 new_time = _ e + e e . e t ( minutes=f n _ e e [ person ] [ \" meeting_time \" ] ) = e _ e e [ person ] [ \" a n \" ] r _ e = e _ e e [ person ] [ \" r _ e \" ] end_time = e _ e e [ person ] [ \" end_time \" ] r _ e _ = r _ e . f e (\"% _ e _ = end_time . f e (\"% u _ a n == and _ e >= r _ e and new_time <= end_time : r += 1 _ e = new_time e : o = 2 d k . append ( \" \" { p } \" n i because doesn match s d o { person } , who l be { } from { r _ e _ } { _ e _ } . \" ) e : s l r ( \" Unknown plan format \" ) c E p n : r = 10 d k . append ( \" \" { p } \" . \" ) n i because format doesn l t examples l _ e = ( e _ e e . keys ( ) ) not_met_with = \" , \" . n ( t ( _ e ( met_with . keys ( ) ) ) ) u c , d k Figure 24 The Meeting Planning evaluation function (part 2). Evolving Deeper LLM Thinking B. Data Splits TravelPlanner TravelPlanner has 45 training tasks, 180 validation tasks, and 1,000 test tasks in the original benchmark. Natural Plan Trip Planning The Trip Planning benchmakr has 1,600 example tasks. There are eight different difficulty levels, ranging from 3 to 10 cities. Each difficulty level has 200 examples. We split the dataset into validation and test sets by putting the first 40 examples from each difficulty level into validation, and the last 160 examples into test, giving 320 examples in validation (which we used for prompt development) and 1,280 for test. In Figure 4, we show the performance at each difficulty level. Natural Plan Meeting Planning The Meeting Planning benchmark has 1,000 example tasks. There are ten different difficulty levels, ranging from meeting one to ten different friends. Each difficulty level has 100 examples. We split the dataset into validation and test sets by putting the first 50 examples from each difficulty level into validation, and the last 50 examples into test, giving 500 examples in validation (which we used for prompt development) and 500 for test. In Figure 5, we show the performance at difficulty level. C. GPT Results Table 7 presents the results of Mind Evolution using GPT-4o-mini with the same sets of prompts. Specifically, with 1-pass inference, GPT-4o-mini also struggles at planning tasks, achieving 0% on TravelPlanner, 9.1% success rate on Trip Planning, and 20.2% success rate on Meeting Planning. Again, Mind Evolution significantly improves the performance by over 100% relatively across different benchmarks. TravelPlanner [42] Natural Plan [47] Trip Planning Natural Plan [47] Meeting Planning Success Rate 79.4% 48.1% 86.4% Table 7 Mind Evolution with GPT-4o-Mini results on validation sets. D. Model Pricing and API Cost Curves Table 8 shows the API pricing of different models used in our evaluation (Tables 2), at the time of writing (October 2024). Figure 25 gives insight into the scaling properties of the various strategies in terms of their API cost, which Model Gemini 1.5 Flash Gemini 1.5 Pro GPT-4o-Mini OpenAI o1-preview Input Token Output Token $0.075/M $1.25/M $0.15 $15.00/M $0.30/M $5.00/M $0.60 $60.00/M Table 8 Pricing at the time of writing (October 2024). These differences serve as proxy for real computational cost differences among models. is also linear combination of the input token counts and the output token counts, weighted by base rate  (Table 8)  . E. Additional Examples In addition to Table 3, we present qualitative examples of TravelPlanner and Meeting Planning in Table 9 and Table 10, respectively. 28 Evolving Deeper LLM Thinking (a) TravelPlanner (b) Trip Planning (c) Meeting Planning Figure 25 API cost per task (Oct 2024) vs. Success Rate on the validation sets with Gemini 1.5 Flash. API cost is also linear combination of input token counts and output token counts. Note that Sequential-Revision+ curves are cut short. As the conversations can run up to 80 turns, it usually consumes lot more tokens than the other methods. 29 Evolving Deeper LLM Thinking Q: Could you construct 3-day journey for two people from Chicago to Albany that takes place from March 22nd to March 24th, 2022? Our budget is $2,300. We require accommodations that allow smoking and should ideally be entire rooms. We will not be self-driving during this trip. On the subject of cuisine, were open to any suggestions you might have. [Transportation Options] Taxi: N/A, Self-Driving: N/A, Flights: F3732604 [...] [Accommodation Options in Albany] 2Br Union square Newly furnished (minimum 30 nights), [...] [...] Method 1-Pass Best-of-N -, dinner: -, breakfast: Desi Villa, Albany, lunch: from Chicago to Albany, accommodation: Answer Spacious room with huge bay [Day 1] city: window & natural light, Albany, attraction: Kaiser, Albany, lunch: -, transportation: Flight Number: F3983351, from Chicago to Albany, Departure Time: 20:20, Arrival Time: 23:04 [Day 2] city: Albany, accommodation: Spacious room with huge bay window & natural light, Albany, attraction: Albany Pine Bush Preserve, Just Albany;New York State Museum, Albany;, breakfast: Parkash, Albany, dinner: Kaiser, Albany, transportation: - [Day 3] city: from Albany to Chicago, accommodation: -, attraction: Hear about Here, Albany;, breakfast: Cafe Coffee Day The Square, Albany, lunch: PiccoLicko, Albany, dinner: Starvin Marvin, Albany, transportation: Flight Number: F3813300, from Albany to Chicago, Departure Time: 16:40, Arrival Time: 17:52 √òThe total cost exceeds the budget of $2300 by $114, [...] [Day 1] city: from Chicago to Albany, accommodation: Mid Town East Side Apartment, Albany, attraction: Kaiser, New York State Museum, Albany, breakfast: Albany, lunch: -, transportation: Taxi, from Chicago to Albany, duration: 12 hours 53 mins, distance: 723 km, cost: 532 [Day 2] city: Albany, accommodation: Mid Town East Side Apartment, Albany, attraction: Albany Institute of History & Art, Albany, breakfast: Echoes Satyaniketan, Albany, lunch: Cafe Coffee Day The Square, Albany, dinner: Urban Punjab, Albany, transportation: - [Day 3] city: from Albany to Chicago, accommodation: -, attraction: Historic Cherry Hill, Albany, breakfast: Just Parkash, Albany, lunch: BK≈õ The Juice Bar, Albany, dinner: -, transportation: Flight Number: F4008387, from Albany to Chicago, Departure Time: 05:05, Arrival Time: 06:11 √òTaxi from Chicago to Albany on day 1 is not valid option, [...] -, dinner: Sequential Revisions+ [Day 1] city: from Chicago to Albany, accommodation: 2Br Union square Newly furnished, Albany, attraction: -, breakfast: -, dinner: -, lunch: -, transportation: Flight Number: F3983350, from Chicago to Albany, Departure Time: 14:29, Arrival Time: 17:03 [Day 2] city: Albany, accommodation: 2Br Union square Newly furnished, attraction: New York State Museum, Albany;Albany Pine Bush Preserve, Albany, breakfast: Cafe Coffee Day, Albany, lunch: PiccoLicko, Albany, dinner: Kaiser, Albany, transportation: - [Day 3] city: from Albany to Chicago, accommodation: -, attraction: The McPherson Legacy to the City of Albany - Robert Burns Statue, Albany, breakfast: -, lunch: -, dinner: -, transportation: Flight Number: F3732604, from Albany to Chicago, Departure Time: 05:53, Arrival Time: 07:24 √ò2Br Union square Newly furnished requires staying at least 30 nights, [...] [Day 1] city: from Chicago to Albany, accommodation: Mid Town East Side Apartment, Albany, attraction: -, breakfast: -, dinner: -, lunch: -, transportation: Flight Number: F3983350, from Chicago to Albany, Departure Time: 14:29, Arrival Time: 17:03 [Day 2] city: Albany, accommodation: Mid Town East Side Apartment, Albany, attraction: Albany Institute of History & Art, Albany, breakfast: Starvin Marvin, Albany, lunch: Cafe Coffee Day The Square, Albany, dinner: PiccoLicko, Albany, transportation: - [Day 3] city: from Albany to Chicago, accommodation: -, attraction: The McPherson Legacy to the City of Albany - Robert Burns Statue, Albany, breakfast: -, lunch: -, dinner: -, transportation: Flight Number: F4008387, from Albany to Chicago, Departure Time: 05:05, Arrival Time: 06:11 Mind Evolution (ours) Table 9 An example TravelPlanner task and the solutions proposed by Mind Evolution and the baselines method. Evolving Deeper LLM Thinking Q: You are visiting San Francisco for the day and want to meet as many friends as possible. Solve the problem by considering various different schedules and picking the best one to optimize your goals. Travel distances (in minutes): The Castro to Sunset District: 17. The Castro to Presidio: 20. The Castro to Bayview: 19. The Castro to Chinatown: 20. The Castro to Mission District: 7. Sunset District to The Castro: 17. Sunset District to Presidio: 16. Sunset District to Bayview: 22. Sunset District to Chinatown: 30. Sunset District to Mission District: 24. Presidio to The Castro: 21. Presidio to Sunset District: 15. Presidio to Bayview: 31. Presidio to Chinatown: 21. Presidio to Mission District: 26. Bayview to The Castro: 20. Bayview to Sunset District: 23. Bayview to Presidio: 31. Bayview to Chinatown: 18. Bayview to Mission District: 13. Chinatown to The Castro: 22. Chinatown to Sunset District: 29. Chinatown to Presidio: 19. Chinatown to Bayview: 22. Chinatown to Mission District: 18. Mission District to The Castro: 7. Mission District to Sunset District: 24. Mission District to Presidio: 25. Mission District to Bayview: 15. Mission District to Chinatown: 16. CONSTRAINTS: You arrive at The Castro at 9:00AM. Michelle will be at Sunset District from 6:30PM to 8:30PM. Youd like to meet Michelle for minimum of 120 minutes. Amanda will be at Presidio from 9:30PM to 10:00PM. Youd like to meet Amanda for minimum of 30 minutes. Sandra will be at Bayview from 10:00AM to 2:30PM. Youd like to meet Sandra for minimum of 90 minutes. Kevin will be at Chinatown from 6:15PM to 7:15PM. Youd like to meet Kevin for minimum of 45 minutes. Mark will be at Mission District from 12:30PM to 1:45PM. Youd like to meet Mark for minimum of 75 minutes. Method 1-Pass Best-of-N Answer [You start at The Castro at 9:00AM, You travel to Bayview in 19 minutes and arrive at 9:19AM, You meet Sandra for 90 minutes from 10:00AM to 11:30AM, You travel to Mission District in 13 minutes and arrive at 11:43AM, You meet Mark for 75 minutes from 12:30PM to 1:45PM, You travel to Sunset District in 24 minutes and arrive at 2:09PM, You wait until 6:30PM, You meet Michelle for 120 minutes from 6:30PM to 8:30PM, You travel to Chinatown in 30 minutes and arrive at 8:30PM, You wait until 6:15PM, You meet Kevin for 45 minutes from 6:15PM to 7:00PM, You travel to Presidio in 19 minutes and arrive at 7:19PM, You wait until 9:30PM, You meet Amanda for 30 minutes from 9:30PM to 10:00PM] √òMeeting Mark for 75 minutes from 12:30PM to 1:45PM doesnt match the schedule of Mark, who will be at Mission District from 12:30PM to 01:45PM [...] [You start at The Castro at 9:00AM, You travel to Bayview in 19 minutes and arrive at 9:19AM, You wait until 10:00AM, You meet Sandra for 90 minutes from 10:00AM to 11:30AM, You travel to Mission District in 13 minutes and arrive at 11:43AM, You wait until 12:30PM, You meet Mark for 75 minutes from 12:30PM to 1:45PM, You travel to Sunset District in 24 minutes and arrive at 2:09PM, You wait until 6:30PM, You meet Michelle for 120 minutes from 6:30PM to 8:30PM, You travel to Chinatown in 30 minutes and arrive at 8:30PM , You wait until 6:15PM, You meet Kevin for 45 minutes from 6:15PM to 7:00PM, You travel to Presidio in 19 minutes and arrive at 7:19PM, You wait until 9:30PM, You meet Amanda for 30 minutes from 9:30PM to 10:00PM] √òWaiting until 6:15PM is invalid because the previous step ends after 06:15PM. Mind Evolution (ours) Sequential Revisions+ [You start at The Castro at 9:00AM, You travel to Mission District in 7 minutes and arrive at 9:07AM, You wait until 12:30PM, You meet Mark for 30 minutes from 12:30PM to 1:00PM, You travel to Sunset District in 17 minutes and arrive at 1:17PM, You wait until 6:30PM, You meet Michelle for 30 minutes from 6:30PM to 7:00PM, You travel to Presidio in 16 minutes and arrive at 7:16PM, You wait until 9:30PM, You meet Amanda for 30 minutes from 9:30PM to 10:00PM] Not meeting with Kevin and Sandra. [You start at The Castro at 9:00AM, You travel to Bayview in 19 minutes and arrive at 9:19AM, You wait until 10:00AM, You meet Sandra for 90 minutes from 10:00AM to 11:30AM, You travel to Mission District in 13 minutes and arrive at 11:43AM, You wait until 12:30PM, You meet Mark for 75 minutes from 12:30PM to 1:45PM, You travel to Chinatown in 16 minutes and arrive at 2:01PM, You wait until 6:15PM, You meet Kevin for 45 minutes from 6:15PM to 7:00PM, You travel to Presidio in 19 minutes and arrive at 7:19PM, You wait until 9:30PM, You meet Amanda for 30 minutes from 9:30PM to 10:00PM] Not meeting with Michelle, but this is best possible plan. Table 10 An example Meeting Planning task and the solutions proposed by Mind Evolution and the baselines method. 31 Evolving Deeper LLM Thinking 3. If word appears an incorrect number of times (too few or too many) in the text, it is indicated, along with the error. 4. An annotated copy of the text is returned. The annotations indicate where the cipher-keywords were found (they are shown asterisked), and the first error is indicated. 5. If the text encodes the cipher correctly, but also encodes extra words, that is indicated. 6. If everything in ùëÄ is correct, but ùëÄ < ùëÄ, it is indicated as such. For this task, we experimented with many different genre forms (poetry, short-story fiction, essay, monologue, etc.), as well as inspirations from contemporary to classic writers. F. Additional Details for StegPoet The prompt design used for StegPoet is given in Figure 26. StegPoet Evaluation Each proposed solution should contain cipher and text component. The first step is to calculate what is encoded in the text by finding all the cipher strings; this is done via simple capitalization-agnostic character-matches. We refer to the actual encoded string as ùëÄ. If ùëÄ = ùëÄ the problem is solved correctly. The numeric evaluation of proposed solution is computed as follows: 1. Invalid if the text or cipher component cannot be parsed or violates constraints. (a) Words in the cipher cannot be subsets of each other (e.g., origin and original). (b) Words in the cipher cannot be repeated. (c) Words in the cipher should be at least 4 characters long. (d) Words in the cipher should contain only alphabetic characters. 2. What is the first position, ùëñ, in which ùëÄùëñ ùëÄ ùëñ ? This is the integer part of the score. 3. Compute the Levenshtein distance between ùëÄ and ùëÄ. Levenshtein distance is often used in information theory and linguistics to measure the difference between two sequences [2]. This is scaled between (0,1) and added to the integer component above. Additionally, textual feedback, without numeric penalties, is also provided in the revision request made to the LLM. 1. clearly marked list of what ùëÄ was found. 2. list of number mappings missing from the cipher, or unnecessary numbers specified in the cipher. 32 Evolving Deeper LLM Thinking For this task, you are the worlds best poet, linguist and hidden code creator! You strive to write in the style of shel silverstein. would like you to come up with 1:1 mapping from numbers to words for the list of numbers demarcated by <HIDDEN-MESSAGE START> and <HIDDEN-MESSAGE END>. Then, cleverly hide the numbers in an poem about the fun of walking. SPECIFICS: * You can choose the words. * number cannot have more than one word assigned to it. * word cannot be used twice in the mapping. * Create substitution-ENCODING-CIPHER for each number and then put those words into the poem in the order that they appear in the hidden message. * If you use word in the ENCODING-CIPHER in the poem, then it can only be used to encode number. Nowhere else. * The numbers must be encoded in the order that they appear in the hidden message. Otherwise it will be mistaken as part of the message. <HIDDEN-MESSAGE START> 90 70 50 40 40 10 50 80 70 900 50 30 <HIDDEN-MESSAGE END> Here are some guidelines to follow. * The poem must NOT contain any numbers. * Numbers should not be encoded with word or words that directly represent the number. For example, DO NOT represent \"2\" : \"Dual\". * In general, numbers should not be represented by words that are numeric in nature. * Numbers should not be represented by hyphenated words or using any other non alphabetic character. * The words used for ciphers should not be subsets of each other. * Your poem should make sense, be understandable, and be in the style of shel silverstein. * Your poem should not just be list of words that look like jibberish. Try to have at least 4 words between your cipher words. Similarly, \"4\" : \"four\" is not acceptable. They should be encoded with single words. Absolutely crucial: * Important -- make sure that each number in the hidden message is represented in the poem. * DO NOT SKIP ANY numbers. The numbers must be encoded in the SAME order as they are given to you. * PRECISION IS OF THE UTMOST IMPORTANCE. * Note that we did not use common words such as and and the etc. to encode the numbers, as such words may be difficult to use only at the specific times required to encode the hidden-message. * You should not use common words to encode the numbers in the cipher. * The words you use in the cipher should be at least 4 characters long. * Though you may have to use the same word multiple times, try to avoid using the same sentence or phrase multiple times. Please begin and end your poem with <POEM START> and <POEM END>. FORMATTING INSTRUCTIONS: * Please begin your answer by providing the mapping from the numbers in the hidden message to the words you select. * Ive included 3 examples of successful ENCODING-CIPHERs below. * VERY, VERY IMPORTANT: * Note the semicolon that is required after each cipher entry in the ENCODING-CIPHER section. This must be present! You should come up with your own creative ENCODING-CIPHERs! EXAMPLE #1: Can you please hide the message demarcated by <HIDDEN-MESSAGE START> and <HIDDEN-MESSAGE END> into poem about computers. <HIDDEN-MESSAGE START> 77 22 33 40 44 77 50 66 55 5 40 40 3 70 8 <HIDDEN-MESSAGE END> <ENCODING-CIPHER START> \"22\" : \"computers\"; \"33\" : \"become\"; \"44\" : \"vital\"; \"55\" : \"them\"; \"66\" : \"need\"; \"77\" : \"everyday\"; \"40\" : \"more\"; \"50\" : \"need\"; \"70\" : \"certain\"; \"3\" : \"grow\"; \"5\" : \"exist\"; \"8\" : \"future\"; <ENCODING-CIPHER END> <POEM START> Everyday, computers become more vital to our lives. Everyday, we need them to exist more and more. That will grow, for certain, in the future. <POEM END> Figure 26 An example initial prompt for StegPoet. Only 1 of 3 examples is shown."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "UC San Diego",
        "University of Alberta"
    ]
}