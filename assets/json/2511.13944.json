{
    "paper_title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
    "authors": [
        "Noam Glazner",
        "Noam Tsfaty",
        "Sharon Shalev",
        "Avishai Weizman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions."
        },
        {
            "title": "Start",
            "content": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets Noam Glazner* Faculty of Engineering Bar-Ilan University Ramat Gan, Israel Email: noam.glazner@live.biu.ac.il ORCID: 0009-0003-1524-1157 Sharon Shalev Independent Researcher Email: shalev1310@gmail.com ORCID: 0009-0003-5054-230X Noam Tsfaty Intelligence Systems Afeka College of Engineering Tel Aviv, Israel Email: Tsfaty.Noam@s.afeka.ac.il ORCID: 0009-0009-5246-8274 Avishai Weizman* School of Electrical and Computer Engineering Ben-Gurion University of the Negev Beersheba, Israel Email: wavishay@post.bgu.ac.il ORCID: 0009-0004-1182-8601 AbstractWe propose cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions. Index TermsDeep Learning, Object Detection, Frame Selection, Clustering, Video Datasets, Information Leakage. I. INTRODUCTION In deep learning research, database construction and data splitting strategies play crucial role in determining the reliability and generalization capability of trained models. Imagebased models can be trained by randomly splitting images into training, validation, and test sets [1]. However, when datasets are derived from video sources, this approach can lead to information leakage due to high spatial and temporal correlations among consecutive frames, e.g., frames recorded in the same background and containing the same objects but in slightly different positions or rotations [1], [2]. As result, models can achieve inflated performance estimates, particularly in image-based object detection trained on video-derived frames. To address this issue, we propose cluster-based frame selection strategy that groups visually similar frames before splitting into the dataset partitions (train, validation, test), ensuring that correlated images remain together. The proposed method is simple, scalable, and can be integrated into existing dataset preparation pipelines without modification of the training process. This approach can improve the fairness of dataset partitioning and improve the robustness of model evaluation in video-derived frames datasets. II. PROPOSED METHOD An overview of the proposed framework is presented in Figure 1. The process begins by considering collection of *These authors contributed equally to this work. Fig. 1. Illustration of the proposed cluster-based frame selection pipeline. Each video is decomposed into individual frames, from which features are extracted using semantic representations (e.g., CLIP [3]), handcrafted descriptors (e.g., HOG [4]), or lightweight learned features (e.g., XFeat [5]). Dimensionality reduction (e.g., PaCMAP [6]) and clustering (e.g., HDBSCAN [7]) are then applied to group visually similar frames before dataset partitioning. unlabeled videos denoted as = {V1, V2, . . . , VK}, where represents the total number of videos in the dataset. Each video Vk is decomposed into sequence of frames {Ik,1, Ik,2, . . . , Ik,Nk }, where Nk denotes the number of frames extracted from video Vk. A. Feature Extraction For each frame Ik,i, feature vector fk,i Rd is extracted using handcrafted or learned descriptors. Classical descriptors such as the scale-invariant feature transform (SIFT) [8], [9] and the histogram of oriented gradients (HOG) [4] capture texture and shape information by encoding gradient local orientation patterns within localized image regions. The Accelerated Features for Lightweight Image Matching (XFeat) [5] descriptor provides efficient keypoint detection and description through lightweight convolutional architecture, while deep pretrained models such as Contrastive LanguageImage Pretraining (CLIP) [3], Sigmoid Loss for Language Image Pre-training (SigLIP) [10], and DINO-V3 [11] offer semantic image representations that can capture visual concepts. The Vector of Locally Aggregated Descriptors (VLAD) [12] was applied to both SIFT and XFeat to combine their local keypoint descriptors into single compact, fixed-length feature 5 2 0 2 7 ] . [ 1 4 4 9 3 1 . 1 1 5 2 : r vector. VLAD creates compact representation by assigning each local descriptor to its nearest codeword in learned codebook and aggregating the residuals between them into single vector. The feature extraction process is defined as: TABLE CLUSTERING PERFORMANCE OF HDBSCAN ON VALIDATION SETS FROM THE IMAGENET-VID AND UCF101 DATASETS USING DIFFERENT FEATURE EXTRACTIONS AND PACMAP (256-D). fk,i = Φfeat(Ik,i), (1) where Φfeat() denotes the chosen feature extraction function. B. Dimensionality Reduction and Clustering The extracted feature vectors fk,i are projected into lowdimensional embedding space using the pairwise controlled manifold approximation projection (PaCMAP) algorithm [6]: zk,i = PPaCMAP(fk,i) (2) where zk,i Rm represents the embedded frame in the latent space, and PPaCMAP() denotes the projection operator applied by the PaCMAP algorithm. Subsequently, the embedded representations zk,i are clustered using the hierarchy of density-based spatial clustering (HDBSCAN) [7] algorithm. Due to the continuous and non-uniform nature of video data, density-based clustering methods such as HDBSCAN are generally more suitable than centroid-based approaches (e.g., K-Means [13] algorithm), as they can capture irregular cluster shapes and variable frame densities, well-suited for this task. C. Cluster-Based Dataset Partitioning The HDBSCAN clustering algorithm is employed to group similar frames based on their feature representations zk,i. Let Cj denote the j-th cluster containing feature zk,i, which correspond to frame Ik,i. Each cluster Cj represents visually related frames, which are then assigned to the same dataset partition to prevent data leakage between sets. III. EXPERIMENTS AND RESULTS We analyze the results in Table using the validation set of the ImageNet-VID set of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015) [14] and all partitions of the UCF101 [15] dataset. The ImageNet dataset provides annotated images categorized by object synsets, whereas the UCF101 dataset consists of trimmed video clips labeled at the video level. To reduce visual redundancy and ensure that consecutive frames are not nearly identical, we extracted one frame per second from each video in the UCF101 dataset. These structured annotations allow us to detect and mitigate potential data leakage by ensuring that all frames from given video remain within single cluster. In our experiments, the images were resized to 224224 for XFeat, CLIP, DINO, and SigLIP, and to either 128128 or 224224 for HOG, with the 128128 setting performing slightly better while also producing lower-dimensional descriptor. The VLAD vectors were reduced to 1024 dimensions to provide uniform feature representation, and the PaCMAP embeddings zk,i were projected to 256-dimensional space (m = 256) to ensure consistent low-dimensional representation across all feature extraction methods. To evaluate the effectiveness of the proposed frame grouping method, we assess clustering Feature Extraction SIFT + VLAD HOG HOG (128 128) XFeat + VLAD CLIP (ViT-B/32) SigLIP (ViT-B/16) DINO-V3 (ViT-B/16) Dataset ImageNet-VID UCF101 ImageNet-VID UCF101 ImageNet-VID UCF101 ImageNet-VID UCF101 ImageNet-VID UCF101 ImageNet-VID UCF101 ImageNet-VID UCF101 V-measure 0.81 0.57 0.82 0.61 0.87 0.67 0.90 0.72 0.92 0.75 0.93 0.75 0.96 0. AMI 0.80 0.38 0.81 0.48 0.86 0.54 0.89 0.58 0.91 0.66 0.92 0.67 0.96 0.80 quality using the Adjusted Mutual Information (AMI) [16] and V-measure [17] metrics, which quantify how well the resulting clusters correspond to the ground-truth videos. The AMI measures the agreement between predicted clusters and true labels while accounting for chance, whereas the V-measure evaluates the trade-off between cluster homogeneity (the extent to which each cluster is composed of samples from single class) and completeness (the extent to which samples of the same class share cluster). In Table I, all feature extraction methods use an input image size of 224 224 except for HOG (128 128). The results show that representations from deep pretrained models outperform classical and lightweight descriptors on both datasets, demonstrating their superior ability to capture the similarity of information leakage detection. In particular, DINO-V3 achieves the highest V-measure and AMI scores on both ImageNet-VID and UCF101. XFeat, combined with VLAD, also provides noticeable improvement over traditional descriptors, while SIFT + VLAD remains the weakest across both datasets, especially on UCF101, where temporal variability increases the difficulty of clustering. IV. CONCLUSIONS AND FUTURE WORK This work presents simple cluster-based frame selection strategy designed to minimize information leakage in video datasets. By grouping visually similar and temporally correlated frames before dataset partitioning, the proposed method can improve diversity and provide fair evaluation setting, thereby mitigating overfitting in object detection models trained on video datasets. The experimental results demonstrate that DINO-V3 embeddings achieve the best clustering across both ImageNet-VID and UCF101 datasets, highlighting the advantage of representations for identifying information leakage. One limitation of the approach is its reliance on HDBSCAN and its hyperparameter choices, and we plan to explore adaptive clustering strategies that may address this limitation. We also aim to quantify the performance gap in image-based object detection models by training with and without the proposed approach to assess the impact of information leakage across different datasets."
        },
        {
            "title": "REFERENCES",
            "content": "[1] D. Botache, K. Dingel, R. Huhnstock, A. Ehresmann, and B. Sick, Unraveling the complexity of splitting sequential data: Tackling challenges in video and time series analysis, 2023. [2] R. B. D. Figueiredo and H. A. Mendes, Analyzing information leakage on video object detection datasets by splitting images into clusters with high spatiotemporal correlation, IEEE Access, vol. 12, pp. 47 646 47 655, 2024. [3] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in International Conference on Machine Learning, 2021. [4] N. Dalal and B. Triggs, Histograms of oriented gradients for human detection, in 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR05), vol. 1. Ieee, 2005, pp. 886893. [5] G. A. Potje, F. Cadar, A. Araujo, R. Martins, and E. R. Nascimento, Xfeat: Accelerated features for lightweight image matching, in IEEE/CVF Conference on Computer Vision and Pattern Recognition CVPR , June 16-22, 2024. IEEE, 2024, pp. 26822691. [6] Y. Wang, H. Huang, C. Rudin, and Y. Shaposhnik, Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization, Journal of Machine Learning Research, vol. 22, no. 201, pp. 173, 2021. [7] L. McInnes, J. Healy, S. Astels et al., hdbscan: Hierarchical density based clustering. J. Open Source Softw., vol. 2, no. 11, p. 205, 2017. [8] D. G. Lowe, Distinctive image features from scale-invariant keypoints, International journal of computer vision, vol. 60, no. 2, pp. 91110, 2004. [9] D. Lowe, Object recognition from local scale-invariant features, in Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, 1999, pp. 11501157 vol.2. [10] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 11 97511 986. [11] O. Simeoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [12] H. Jegou, M. Douze, C. Schmid, and P. Perez, Aggregating local descriptors into compact image representation, in 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010, pp. 33043311. [13] M. Ahmed, R. Seraj, and S. M. S. Islam, The k-means algorithm: comprehensive survey and performance evaluation, Electronics, vol. 9, no. 8, p. 1295, 2020. [14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., Imagenet large scale visual recognition challenge, International journal of computer vision, vol. 115, no. 3, pp. 211252, 2015. [15] K. Soomro, A. Zamir, and M. Shah, Ucf101: dataset of 101 human actions classes from videos in the wild, ArXiv, vol. abs/1212.0402, 2012. [16] N. X. Vinh, J. Epps, and J. Bailey, Information theoretic measures for clusterings comparison: is correction for chance necessary? in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 10731080. [17] A. Rosenberg and J. Hirschberg, V-measure: conditional entropybased external cluster evaluation measure, in Conference on Empirical Methods in Natural Language Processing, 2007."
        }
    ],
    "affiliations": [
        "Afeka College of Engineering",
        "Bar-Ilan University",
        "Ben-Gurion University of the Negev"
    ]
}