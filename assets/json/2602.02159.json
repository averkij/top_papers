{
    "paper_title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing",
    "authors": [
        "Lingkun Long",
        "Yushi Huang",
        "Shihao Bai",
        "Ruihao Gong",
        "Jun Zhang",
        "Ao Zhou",
        "Jianlei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 9 5 1 2 0 . 2 0 6 2 : r Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing Lingkun Long1, Yushi Huang2,3, Shihao Bai3, Ruihao Gong1,3, Jun Zhang2, Ao Zhou1, Jianlei Yang 1Beihang University 2Hong Kong University of Science and Technology 3SenseTime Research"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present FocusdLLM, novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design past confidence-guided indicator to predict unmasked regions. Built upon this, we propose sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed crosslayer consistency. Experimental results show that our method offers more than 29 lossless speedup under 32K context length. The code is publicly available at: https://github.com/ Longxmas/Focus-dLLM."
        },
        {
            "title": "Introduction",
            "content": "Diffusion large language models (dLLMs) (Bie et al., 2025; Gong et al., 2025; Arriola et al., 2025) have recently emerged as compelling nonautoregressive paradigm for text generation, replacing left-to-right token emission with iterative denoising over fixed-length sequence (Li et al., 2022; Gong et al., 2022; Austin et al., 2021; Lou et al., 2023; He et al., 2023). By updating multiple positions in parallel and leveraging bidirectional attention, dLLMs offer an appealing path toward higher decoding throughput while retaining 1 strong generation quality. Moreover, recent studies have substantially extended the context length of dLLMs (Liu et al., 2025a; He et al., 2025), demonstrating effective long-context extrapolation and scaling to long inputs. Nevertheless, efficient long-context inference remains key obstacle for the dLLM due to its nonautoregressive decoding and bidirectional full attention nature. Prior methods (Wu et al., 2025; Liu et al., 2025b; Ma et al., 2025) to address this challenge fall into two categories: (i) Approximated KV cache and (ii) sparse attention. The former selectively refreshes KV states by exploiting strong redundancy between adjacent steps. However, attention computation is still costly over the full cached context. On the other hand, sparse attention (Tang et al., 2024; Xiao et al., 2024a; Xu et al., 2025; Yuan et al., 2025) offers practical solution, but it often requires token importance estimation using the currently decoded token as query (Zhang et al., 2023; Xiao et al., 2024a). Since the positions to be decoded (unmasked) are not known in advance for dLLMs, recent works (Song et al., 2025; Huang et al., 2025) leverage inaccurate coarsegrained estimation, leading to suboptimal performance and limited efficiency. This paper, therefore, asks: Can we accurately predict the positions of the unmasked tokens and only retain necessary computation to achieve more effective long-context inference acceleration for dLLMs? To tackle this challenge, we first make an indepth analysis to investigate the predictability of the unmasked tokens. In particular, we discover that the confidence scores at the same positions in two consecutive steps exhibit strong positive correlation, and the positions of currently unknown tokens largely overlap with those that had the highestconfidence tokens in the previous step. Thus, unmasked positions for the current steps can be inferred from previous-step confidence. Besides, we also analyze the redundancy of attention patterns and observe that attention sink (Xiao et al., 2023; Ruscio et al., 2025), which contributes significantly to the attention score in LLMs (Bai et al., 2023; Touvron et al., 2023), displays notable cross-layer consistency for dLLMs. This phenomenon suggests sink tokens can be identified at an intermediate depth. Therefore, we can directly reuse them without re-identification in deeper layers. Motivated by the above findings, we propose Focus-dLLM, training-free sparse attention framework with approximated KV cache, to accelerate long-context dLLM inference. To begin with, we introduce past confidence-guided indicator that uses confidence scores from step t1 to predict the unmasked positions at step t, and then window-expands them to preserve semantic coherence. Next, we design sink-aware pruning strategy for diffusion decoding: Using the tokens within the positions predicted before as queries, we select only the most relevant tokens for attention while retaining step-wise attention sinks. Moreover, this approach shares the identified sink tokens across layers to further reduce additional overhead. Leveraging these novel techniques, our framework computes attention over the predicted unmasked queries and the selected necessary key-value pairs. As result, it achieves considerable inference speedups without compromising performance throughout the dynamic decoding process. Our contributions are summarized as follows: We analyze diffusion inference dynamics and reveal strong positive correlation of token confidence across adjacent denoising steps, together with dynamic and structured attention patterns in dLLMs. We propose Focus-dLLM, novel training-free acceleration framework that consists of past confidence-guided indicator for predicting the next unmasked positions with sink-aware dynamic token pruning strategy for efficient sparse attention. Experiments show that Focus-dLLM achieves substantial speedups over baselines while preserving accuracy. For instance, it attains betterthan-vanilla performance and delivers 2.05 speedup over Fast-dLLM for UltraLLaDA at 32K context length. et al., 2025; Chen et al., 2025) have emerged as promising non-autoregressive paradigm that enables parallel token generation via iterative denoising. Prior works explore both continuous-space diffusion for text (Li et al., 2022; Gong et al., 2022) and discrete-token diffusion formulations (Austin et al., 2021; Lou et al., 2023; He et al., 2023). Recent masked diffusion LMs (Nie et al., 2025; Zhu et al., 2025; Ye et al., 2025) have been successfully scaled up, demonstrating competitive performance against autoregressive counterparts at billion-parameter scales. Besides, long-context capability (Liu et al., 2025a; He et al., 2025) for dLLMs has also been explored, which pushes the context window up to 16K tokens. KV cache for dLLMs. Due to bidirectional attention and token states evolving across denoising steps, dLLMs cannot directly reuse standard KV cache, motivating line of caching-based accelerations (Ma et al., 2025). Fast-dLLM (Wu et al., 2025) enables approximate KV reuse with blockwise strategies, while others (Ma et al., 2025; Liu et al., 2025b; Huang et al., 2025) exploit dLLMspecific redundancy to reduce repeated computation. More adaptive schemes (Jiang et al., 2025; Nguyen-Tri et al., 2025) further refine cache update granularity and timing. Nevertheless, accurately identifying which tokens require refresh in the next step remains challenging, and long-context inference still incurs substantial computation overhead under caching mechanisms. Sparse attention for dLLMs. Attention sparsification (Zhang et al., 2025b,a,c), orthogonal to the KV cache mechanism, has also been explored to accelerate dLLM inference. Sparse-dLLM (Song et al., 2025) proposes dynamic cache eviction for diffusion decoding, but it adopts coarse and suboptimal block-level metrics. SparseD (Wang et al., 2025) reuses prior sparse patterns, yet it still relies on dense attention in early steps, restricting speedups. Moreover, these approaches do not account for the dynamic attention-sink behavior (Xiao et al., 2023) observed in dLLMs (Rulli et al., 2025). In contrast, our dynamic KV cache compression scheme adapts to step-varying contextual needs while preserving attention sinks for more efficient and accurate longcontext inference."
        },
        {
            "title": "3 Preliminaries",
            "content": "Diffusion large language models. Diffusion large language models (dLLMs) (Li et al., 2025; You Diffusion LLM inference. Unlike autoregressive models that generate tokens sequentially, dLLMs 2 generate text by iteratively denoising fixedlength sequence. Let denote the vocabulary and [MASK] the special mask token. Given prompt = [p1, . . . , pM ], inference initializes at step 0 length-L sequence by appending = masks: x(T ) = [p1, . . . , pM , [MASK], . . . , [MASK] ], (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) (cid:124) =LM (cid:123)(cid:122) Prompt (1) Let M(t) denote the set of masked positions at denoising step t, where M(0) = {M + 1, . . . , L} at initialization. The decoding process then iterates from = 0 to 1. In step t, given the current sequence x(t), the model fθ produces conditional distribution p(xi x(t)) for each masked position M(t). Then, confidence-driven strategy (Nie et al., 2025; Ye et al., 2025) computes the predicted token ˆx(t) and its corresponding confidence score c(t) for each masked position i: ˆx(t) = arg max vV p(xi = x(t)). p(xi = x(t)), c(t) = max vV (2) Last, this strategy unmasks the highest-confidence positions while remasking the rest. Approximate KV cache in dLLMs. Bidirectional attention makes the KV cache mechanism not applicable for dLLMs. To reduce computation costs, recent studies (Wu et al., 2025; Liu et al., 2025b; Ma et al., 2025) exploit Approximate KV cache, which updates KV states for selected subset of tokens while reusing cached states for the rest. Formally, let (t) be the token indices refreshed at step t. The Key state K(t) (cid:40) (and similarly V(t) ) is K(t) = fK(x(t))i, (cid:101)Ki, (t) / (t) (Compute) (Reuse) Figure 1: Confidence dynamics analysis for LLaDA8B-Instruct (Nie et al., 2025) on GSM8K (Cobbe et al., 2021) (L = 76, = 128, and = 128). (Left) Confidence score correlation between adjacent steps. (Right) Step-wise recall rates of predicting the unmasked tokens at using the remasked tokens with top-4 highest confidence scores at 1. poised to be unmasked first requires locating these tokens in advance. To achieve this, we conduct pivotal study related to their confidence score . As illustrated in Figure 1 (Left), c(t) c(t) and c(t1) correlate strongly in positive manner. Also, the tokens that are to be decoded (unmasked) at present similarly high-confidence level in the preceding step 1. To quantitatively explore this relationship, we select the top-4 remasked tokens (i.e., [MASK] at t) with the highest confidence scores at 1 and evaluate their overlap with the tokens decoded at the subsequent step t. As result, Figure 1 (Right) shows remarkably high average recall (96.1%) across decoding steps. These observations support the following key claim: The substantial overlap in confidence distributions reveals that tokens unmasked at can be reliably located according to the confidence of tokens at the prior step 1. , 4.2 Spatial Consistency of Attention Sinks (3) where (cid:101)Ki denotes the cached state from the previous iteration. fK(x(t))i is the current computed Key state, which is also used to update the cache."
        },
        {
            "title": "4 Motivation",
            "content": "In this section, we investigate the token-confidence consistency and attention patterns tailored for dLLMs. Both of them inspire the core design of our Focus-dLLM. 4.1 Temporal Consistency of Confidence For dLLMs, effectively assessing the redundancy of attention computation w.r.t. tokens that are In this part, we explore the properties and variations of attention patterns for dLLMs. Similar to prior studies (Song et al., 2025; Rulli et al., 2025), as depicted in Figure 2, we also find that: (i) Attention maps exhibit strong locality, concentrating near the diagonal and favoring nearby context. (ii) Attention sinks (bright vertical bands), which strongly influence semantic continuity (Xiao et al., 2023; Gu et al., 2025), emerge and evolve across denoising steps. Due to the dynamics of these sinks, it is necessary to repeatedly identify their location to preserve them in high-performing sparse attention. Despite this, we fortunately discovered structured inter-layer consistency for attention sinks. To be 3 Query set and exclude the remaining Query tokens to compute the attention. Section 5.3: We accelerate inference via sink-aware sparse attention mechanism. Since shallow layers are more sensitive to sparsification (Huang et al., 2025), we treat the initial layers as dense layers with full attention. For subsequent sparse layers, we reuse the locations of attention sinks identified at the last dense layer. Finally, we apply dynamic block-wise pruning to Key/Value states of the prompt to keep the most relevant history, while retaining recognized sinks and all response tokens to preserve semantic coherence. 5.2 Past Confidence-Guided Indicator Motivated by the temporal consistency analysis in Section 4.1, we introduce past confidence-guided indicator, which adopts the confidence derived from step 1 to accurately inform the tokens that are likely to be unmasked at step t. To be specific, among all positions M(t) that remain in the [MASK] state within the current decoding block at t, we rank them by their prior confidence scores c(t1) and select top-k indices as the candidate set Ifocus to predict the future unmasked positions at t: (cid:110) (cid:1)(cid:111) i c(t1) top-k(cid:0){c(t1) Ifocus = }jM(t) , (4) where = ρn(t). n(t) is the number of tokens to be unmasked and ρ is pre-defined prediction expansion factor. By leveraging the candidate set Ifocus, we can precisely determine the relevant history to prune redundant attention computation in the next subsection. In addition, as discussed in Section 4.2, the attention mechanism in dLLMs exhibits clear locality property, meaning that token representations depend strongly on nearby semantic context, while distant tokens typically contribute little. To leverage this property for computation savings, we propose window expansion strategy that disregards the distant tokens and only preserves local windows for currently decoded Query tokens (positions in Ifocus) for attention computation. The position set corresponding to the union of windows is given as: Iactive = (cid:91) iIfocus {l w/2 + w/2} , (5) where is the window size. 4 Figure 2: Attention patterns across decoding steps and layers in LLaDA-8B-Instruct (Nie et al., 2025) (L = 49, = 128, = 128). More visual results can be found in the Appendix. specific, the index of attention sinks across different layers (e.g., Layer 9 vs. Layer 19 in Figure 2) typically matches. Therefore, we believe that: Attention sinks with strong cross-layer consistency enable reliable identification at certain intermediate depth, and the results can be reused for deeper layers to eliminate redundant computation."
        },
        {
            "title": "5 Focus-dLLM",
            "content": "5.1 Framework Overview In this section, we present the inference workFollowing flow of Focus-dLLM (Figure 3). Wu et al. (Wu et al., 2025), we adopt the KV caching mechanism with the semi-autoregressive remasking strategy (Nie et al., 2025) (i.e., nonautoregressive unmasking within each block and autoregressive block-wise inference from left block to right block) for dLLMs. For all blocks, Focus-dLLM performs full cache refresh at each block entry step, which is commonly adopted in prior works (Wu et al., 2025; Song et al., 2025). For the other denoising steps, we use our proposed sparse attention pipeline to systematically prune redundancy: Section 5.2: Inspired by Section 4.1, we use confidence scores from the previous step to predict masked positions that are likely to be decoded, which provide focused queries for redundancy estimation in the latter Key/Value pruning. Additionally, guided by the locality pattern in Section 4.2, we expand these positions to local windows to form an active Figure 3: Overview of Focus-dLLM. We predict unmasked positions at the current step using previous confidence scores. These positions act as queries to retrieve relevant prompt blocks, where attention is computed over the union of these blocks and dynamically identified attention sinks. 5.3 Sink-Aware Sparse Attention tance score of each token is computed as: Performing attention over the entire long-context history remains the primary computational bottleneck for inference. To address this, we propose sink-aware sparse attention strategy that selectively retains only the most critical history for diffusion decoding. Dynamic attention sinks identification. While retaining attention sinks is crucial for preserving generation quality (Xiao et al., 2024b; Rulli et al., 2025), existing sparse approaches for dLLMs (Song et al., 2025) typically overlook this, thereby risking the discard of tokens pivotal for generation quality (Rulli et al., 2025). Motivated by our observation in Section 4.2, we propose to explicitly identify and retain them. Crucially, this strategy shares the identified sink tokens across layers, avoiding redundant re-calculation at every depth. Specifically, we designate the first ldense layers as dense layers that perform full attention. Due to the cross-layer consistency observed in Section 4.2, we utilize the attention distribution at the cut-off layer ldense as reliable probe to identify globally salient tokens for the subsequent sparse layers. Let Iactive denote the active token set obtained from Section 5.2. We define the aggregated query representation over active tokens as QIactive. The impor5 Sj = 1 (cid:88) h=1 Softmaxj (cid:32) Qh Iactive Kh (cid:33) , (6) where denotes the number of attention heads. We then select the top-Nsink tokens with the highest scores to form the dynamic attention sink set, denoted as Isink = TopNsink(Sj). Block-wise token pruning. To accelerate inference while maximizing GPU efficiency, we implement block-wise token pruning to reduce computational overhead. Specifically, we partition the prompt tokens into contiguous blocks and assign each block lightweight representative key, computed as the mean of the Key states within the block, Kb = MeanjBlockb(Kj). At timestep t, we estimate the relevance between the predicted candidate queries and each prompt block by aggregating their attention interactions. Concretely, for each block b, we compute relevance score as Rb = 1 (cid:88) (cid:16) h= Qh Ifocus Kh (cid:17) , (7) where Ifocus denotes the predicted candidate set obtained in Section 5.2. Based on these relevance scores, we select the top = α Ntotal_blocks blocks to form the set of Table 1: Performance comparison on LongBench (Bai et al., 2024). Bold indicates the best performance among acceleration methods, and underlined indicates the second best. Single-Doc. QA Multi-Doc. QA Summarization Few-shot Learning Synthetic Code Method Qasper MF-en HotpotQ 2WikiM Musique GovReport MSum E TriviaQ Ave. Score Lsht PRe Lcc B-P UltraLLaDA (He et al., 2025) Vanilla 19. 25.87 16.27 18.00 12.08 32.83 22.48 80. 91.58 41.00 96.75 68.23 59.50 44.90 Fast-dLLM 18.34 Sparse-dLLM 18.04 19.09 SparseD Focus-dLLM 17.02 29.90 27.26 25.87 29.11 17.03 20.59 15.45 22. 17.11 17.88 18.04 21.49 13.36 13.67 11.92 20.20 30.05 29.95 32.64 26.75 22.89 79.50 23.57 76.50 22.50 79.50 21.45 77.00 91.03 91.93 90.70 90.78 42.00 94.75 67.50 58.10 41.50 97.12 67.50 57.72 41.50 96.79 68.10 59.02 41.00 95.73 66.72 57. 44.74 44.86 44.70 45.14 Dream-7B-Instruct (Ye et al., 2025) Vanilla 35.58 40.49 41. 42.10 23.36 23.51 19.66 71.50 87.34 15.75 32.50 63.79 62. 43.03 Fast-dLLM 37.54 Sparse-dLLM 37.50 37.66 SparseD Focus-dLLM 37.38 43.24 43.23 40.96 41.96 35.56 36.83 41.39 38.96 35.74 34.97 41.61 38.56 17.97 17.05 24.17 18. 21.14 20.60 23.51 21.06 19.57 70.50 20.05 70.00 19.66 72.50 19.26 70.00 88.25 88.38 86.85 88.76 16.75 46.17 62.21 61.11 17.00 46.50 62.80 61.20 15.75 36.83 63.86 61.98 17.25 44.25 60.62 60.50 42.75 42.78 43.59 42.82 relevant blocks, Brelevant = Top -C(Rb). The final attention index set is constructed as the union of dynamically identified attention sinks and tokens within the selected relevant prompt blocks: Ip = Isink (cid:91) bBrelevant { Blockb }. (8) Using this index set, we perform sparse attention by gathering keys and values exclusively from the selected prompt tokens and the response tokens. Specifically, for the active queries QIactive, the effective Key-Value pairs are formed as: Kattn = concat(KIp, Kresp), Vattn = concat(VIp, Vresp). The resulting sparse attention is then computed as: Attn = Softmax (cid:18) QIactiveK attn (cid:19) Vattn. (9) d"
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Experiments Settings Models. We evaluate our method on two representative diffusion LLMs: UltraLLaDA (He et al., 2025) and Dream-7B-Instruct (Ye et al., 2025). Baselines. We compare Focus-dLLM against standard native inference (Vanilla) and three dLLM acceleration frameworks: Fast-dLLM (Wu et al., 2025), SparseD (Wang et al., 2025), and SparsedLLM (Song et al., 2025). 6 Benchmarks. To comprehensively assess longcontext capabilities, we conduct evaluations on LongBench (Bai et al., 2024), widely adopted benchmark specifically designed for multi-task long-context understanding. Implementation details. All experiments were conducted on NVIDIA H200 GPUs using OpenCompass (Contributors, 2023). To ensure fair comparison, all baselines utilize the recommended configurations provided in their official implementations. Specifically, for SparseD, we set skip = 20%, ratio = 30%, and block_size = 128; for Sparse-dLLM, we use retention ratio = 0.5 and kernel size = 3. For the Focus-dLLM setup, we adopt identical hyperparameters for both UltraLLaDA and Dream: we set prediction expansion factor ρ = 4, window size = 8, dense layers ldense = 6, and sparsity ratio α = 0.5. Additionally, the number of sink tokens is set to Nsinks = 0.01 , where denotes the prompt length, and prompt block size = 64. Additional details for datasets, models, and methods are provided in the Appendix A. 6.2 Main Results Accuracy. As presented in Table 1, Focus-dLLM demonstrates robust performance across both evaluated diffusion models. On UltraLLaDA (He et al., 2025), our method achieves the highest average score, outperforming the Vanilla baseline and all Figure 4: Niah (Kamradt, 2023) results on UltraLLaDA (He et al., 2025) under long-context settings with maximum context length of 32K across different layer depths. Figure 5: Efficiency evaluation. Comparison of decoding throughput (tokens/s) on UltraLLaDA (He et al., 2025) (Left) and Dream-7B-Instruct (Ye et al., 2025) (Right) across varying context lengths. Red numbers indicate the speedup ratio of Focus-dLLM relative to the Vanilla baseline. competing acceleration frameworks. On Dream7B-Instruct (Ye et al., 2025), Focus-dLLM again surpasses Spare-dLLM (Song et al., 2025) and FastdLLM (Wu et al., 2025), performing on par with the Vanilla baseline. While its accuracy is marginally lower than SparseD (Wang et al., 2025), FocusdLLM offers compelling advantage in efficiency, achieving up to 19.95 speedup at 32K (with 1K denoting 1024 tokens) context length (as shown in Figure 5). This highlights our methods superior balance between performance and inference speed, establishing it as more practical solution. Niah experiments. Figure 4 reports Niah (Kamradt, 2023) results on UltraLLaDA (He et al., 2025) under long-context settings with maximum context length of 32K. Focus-dLLM achieves overall higher scores than Fast-dLLM (Wu et al., 2025) and Sparse-dLLM (Song et al., 2025) across layers, and attains better accuracy than the vanilla baseline at the deepest layer, demonstrating strong needlein-a-haystack retrieval. Efficiency. We evaluate the scalability of FocusdLLM by measuring throughput across context lengths from 8K to 32K context length, both the generation length and generation steps are fixed at 256. As shown in Figure 5, our method consistently outperforms all baselines, with the speedup ratio over Vanilla notably expanding as context Figure 6: Accuracy vs. throughput for UltraLLaDA (He et al., 2025) on LongBench (Bai et al., 2024) with 16K. growsfrom 9.4 at 8K context length to 29.6 at 32K context length. This trend can be attributed to the reduction of redundant attention computation, which tends to incur more significant overhead as sequences lengthen. Consequently, Focus-dLLM maintains superior efficiency and surpasses existing frameworks like Fast-dLLM by up to 2.05 at 32K context length. Accuracy vs. efficiency. Figure 6 compares decoding throughput and LongBench (Bai et al., 2024) accuracy across different methods and configurations, with throughput measured at 16K context length. Focus-dLLM consistently forms stronger Pareto frontier than prior approaches, achieving higher throughput with comparable or better accuracy. Additional experimental details and configu7 rations are provided in the Appendix B."
        },
        {
            "title": "7 Ablation Study",
            "content": "Table 2: Ablation study of Focus-dLLM on UltraLLaDA (He et al., 2025). PCGI denotes Past ConfidenceGuided Indicator, and SA Sparse Attn represents sinkaware sparse attention. Method Avg. Score Throughput Fast-dLLM 44.74 11. 44.23 -0.51 + PCGI + SA Sparse Attn 44.84 +0.10 11.37 +0.34 17.68 +6.65 Focus-dLLM 45.14 +0.40 17.71 +6.68 Effectiveness of each component. We evaluate the impact of the proposed components on LongBench average score and 16K context decoding throughput. Table 2 presents the ablation results building on the Fast-dLLM (Wu et al., 2025) baseline. PCGI filters active queries via our past confidence-guided indicator while attending to the full context KV. SA Sparse Attn prunes context, while passing the entire block as active tokens and identifies redundancy for pruning using these tokens. Applying PCGI alone slightly degrades accuracy, while SA Sparse Attn improves accuracy by filtering irrelevant tokens in long contexts and significantly increasing throughput. Combining both components, Focus-dLLM achieves further accuracy gains and the highest throughput, demonstrating that accurate query selection enables more precise and effective sparse attention. Table 3: Effect of attention sinks on LongBench accuracy for Dream-7B-Instruct (Ye et al., 2025). Incorporating attention sinks consistently improves performance across tasks. Subset w/o Attn Sinks w/ Attn Sinks hotpotqa 2wikimqa trec Avg. Score 37.17 37.68 69.50 41.47 38.96+1.79 38.56+0.88 70.00+0.50 42.82+1.35 Table 3 evaluates the effectiveness of attention sinks on Dream-7B-Instruct (Ye et al., 2025). Incorporating attention sinks leads to clear improvement on LongBench (Bai et al., 2024). These results suggest that effectively retaining attention sinks contributes to the preservation of key contextual information. Figure 7: Ablations on hyperparameters of Focus-dLLM on LongBench (Bai et al., 2024). Ablations on hyperparameters. Figure 7 analyzes the impact of key hyperparameters in Focus-dLLM. Increasing the sparsity ratio α generally improves accuracy, indicating that retaining more relevant context benefits long-context reasoning, while the drop observed for Dream (Ye et al., 2025) at α=0.7 suggests that excessive retention may introduce irrelevant context and dilute useful signals. For the prediction expansion factor ρ, small values (e.g., 1) lead to poor accuracy due to insufficient recall of future decoded token positions, whereas larger values provide more reliable coverage and steadily improve performance. Varying the number of dense layers ldense results in non-monotonic behavior, implying that attention sinks are not fully stabilized in shallow layers and that sparsification sensitivity differs across depths. similar trend is observed for the window size w: overly small windows miss necessary local context, moderate windows improve accuracy, and excessively large windows degrade performance by introducing unrelated tokens."
        },
        {
            "title": "8 Conclusion",
            "content": "We analyzed diffusion inference dynamics and introduced Focus-dLLM, training-free framework for accelerating long-context dLLM inference. By leveraging past confidence-guided indicator for query prediction and sink-aware pruning strategy to retain critical history, our method effectively eliminates redundant computation. Experiments demonstrate that Focus-dLLM achieves over 29 speedup at 32K context length while maintaining superior performance compared to state-of-the-art baselines."
        },
        {
            "title": "Limitations",
            "content": "While Focus-dLLM demonstrates high efficiency in text tasks, its extension to multimodal reasoning remains direction for future exploration. Additionally, our current hyperparameters are manually configured, which may not achieve optimal performance across all specialized domains. Developing fully adaptive mechanism for dynamic parameter adjustment represents promising avenue to further enhance the frameworks versatility and robustness."
        },
        {
            "title": "References",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025. Block diffusion: Interpolating between autoregressive and diffusion language models. Preprint, arXiv:2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, and 29 others. 2023. Qwen technical report. Preprint, arXiv:2309.16609. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, and 1 others. 2024. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pages 3119 3137. Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, and 12 others. 2025. Llada2.0: Scaling up diffusion language models to 100b. Preprint, arXiv:2512.15745. Sitong Chen, Shen Nie, Jiacheng Sun, Zijin Feng, Zhenguo Li, Ji-Rong Wen, and Chongxuan Li. 2025. Masked diffusion models as energy minimization. arXiv preprint arXiv:2509.13866. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. 2025. Scaling diffusion language models via adaptation from autoregressive models. Preprint, arXiv:2410.17891. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. 2022. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. 2025. When attention sink emerges in language models: An empirical view. Preprint, arXiv:2410.10781. Guangxin He, Shen Nie, Fengqi Zhu, Yuankang Zhao, Tianyi Bai, Ran Yan, Jie Fu, Chongxuan Li, and Binhang Yuan. 2025. Ultrallada: Scaling the context length to 128k for diffusion large language models. Preprint, arXiv:2510.10481. Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuan-Jing Huang, and Xipeng Qiu. 2023. Diffusionbert: Improving generative masked language models with diffusion models. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 45214534. Jianuo Huang, Yaojie Zhang, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, and Linfeng Zhang. 2025. Mask tokens as prophet: Fine-grained cache Preprint, eviction for efficient dllm inference. arXiv:2510.09309. Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, and Xu Yang. 2025. d2cache: Accelerating diffusion-based llms via dual adaptive caching. Preprint, arXiv:2509.23094. G. Kamradt. 2023. Needle in haystack - pressure https://github.com/gkamradt/ testing llms. LLMTest_NeedleInAHaystack. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. 2025. survey on diffusion language models. arXiv preprint arXiv:2508.10875. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusionlm improves controllable text generation. Advances in neural information processing systems, 35:4328 4343. Xiaoran Liu, Yuerong Song, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. 2025a. Longllada: Unlocking long context capabilities in diffusion llms. Preprint, arXiv:2506.14429. 9 Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. 2025b. dllm-cache: Accelerating diffusion large language models with adaptive caching. Preprint, arXiv:2506.06295. Aaron Lou, Chenlin Meng, and Stefano Ermon. 2023. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025. dkv-cache: The cache for diffusion language models. Preprint, arXiv:2505.15781. Quan Nguyen-Tri, Mukul Ranjan, and Zhiqiang Shen. 2025. Attention is all you need for kv cache in diffusion llms. Preprint, arXiv:2510.14973. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Large language diffusion models. Preprint, arXiv:2502.09992. Maximo Eduardo Rulli, Simone Petruzzi, Edoardo Michielon, Fabrizio Silvestri, Simone Scardapane, and Alessio Devoto. 2025. Attention sinks in diffusion language models. Preprint, arXiv:2510.15731. Valeria Ruscio, Umberto Nanni, and Fabrizio Silvestri. 2025. What are you sinking? geometric approach on attention sink. Preprint, arXiv:2508.02546. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 2024. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685. Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. 2025. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. Preprint, arXiv:2508.02558. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. 2024. Quest: Queryaware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Zeqing Wang, Gongfan Fang, Xinyin Ma, Xingyi Yang, and Xinchao Wang. 2025. Sparsed: Sparse attention for diffusion language models. arXiv preprint arXiv:2509.24014. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. 2025. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. Preprint, arXiv:2505.22618. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. 2024a. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. Advances in Neural Information Processing Systems, 37:119638119661. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024b. Efficient streaming language models with attention sinks. Preprint, arXiv:2309.17453. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. 2025. XAttention: Block sparse attention with antidiagonal scoring. In Forty-second International Conference on Machine Learning. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025. Dream 7b: Diffusion large language models. Preprint, arXiv:2508.15487. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. 2025. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native sparse attention: Hardware-aligned and natively trainable sparse attention. Preprint, arXiv:2502.11089. Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. 2025a. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML). Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. 2025b. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR). Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. 2025c. Spargeattn: Accurate sparse attention accelerating 10 any model inference. In International Conference on Machine Learning (ICML). Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, and 1 others. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. Preprint, arXiv:2505.19223."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Details of Focus-dLLM This section provides additional details regarding the implementation of our Focus-dLLM framework. To maximize computational efficiency, our framework leverages specialized GPU kernels for attention computations. The core sink-aware sparse attention operator, which handles dynamic context pruning, is implemented using Triton (Tillet et al., 2019). This allows for fine-grained control and optimization of memory access patterns for sparse matrix operations. For dense attention computationswhich occur in the initial dense layers (l ldense) and during full-cache refreshes at block entrieswe utilize the highly optimized FlashAttention (Shah et al., 2024) kernel to accelerate inference. Empirical analysis revealed that the final layers of Dream-7B-Instruct (Ye et al., 2025) exhibit high sensitivity to attention sparsification. To mitigate potential performance degradation, we designate the final four transformer layers of this model as dense, ensuring they always perform full attention. This hybrid strategy preserves the integrity of critical generation stages, achieving superior trade-off between accuracy and efficiency. Focus-dLLM strictly adhere to the original decoding strategies of both UltraLLaDA (He et al., 2025) and Dream-7B-Instruct (Ye et al., 2025). The generation process follows the semi-autoregressive remasking paradigm, where transfer scheduler dictates which tokens are unmasked at each step based on their confidence scores, consistent with the methods described in (He et al., 2025) and (Ye et al., 2025). The complete inference procedure of Focus-dLLM is detailed in Algorithm 1. A.2 Baselines In our experiments, we compare Focus-dLLM against the vanilla inference of representative diffusion models and several state-of-the-art acceleration frameworks. Vanilla dLLMs. We use the standard inference implementations of UltraLLaDA (He et al., 2025) and Dream-7B-Instruct (Ye et al., 2025) as our primary baselines. UltraLLaDA is developed by fine-tuning LLaDA (Nie et al., 2025) for long-context capabilities, while Dream is adapted from pre-trained autoregressive model. These representative diffu11 Algorithm 1 Focus-dLLM Inference Procedure Require: Prompt p, Mask token [MASK], Max steps , Sparse ratio α, Expansion factor ρ, Transfer Calculate candidate count Full refresh at block entry Candidate set Window expansion Scheduler S, Dense layers ldense, Window size w. Ensure: Generated sequence x(T ) 1: Initialize x(0) [p, [MASK]1, . . . , [MASK]N ] 2: Initialize K, cache as empty; Confidence scores c(0) 0 3: for = 0 to 1 do 4: Determine dynamic prediction token counts n(t) Number of tokens to unmask at step given ρ n(t) if IsBlockEntry(t) then Iactive {1, . . . , L} use_sparse False else Ifocus Select top-k indices based on c(t) Iactive (cid:83) use_sparse True {i w/2, . . . , + w/2} iIfocus end if Layer-wise Forward Pass for layer = 1 to Llayers do if use_sparse and > ldense then Sparse Attention Mechanism Isink IdentifySinks(Layer ldense) Compute Block Relevance Rb using QIfocus and Kb Determine selection size = α Ntotal_blocks Select relevant prompt blocks Brelevant Top-C(Rb) Ip Isink (cid:83) Kattn Concat(KIp, Kresp) Vattn Concat(VIp, Vresp) (cid:19) { Blockb } bBrelevant (cid:18) QIactive attn Vattn Hl Softmax else Full Attention & Cache Update Hl FullAttn(QIactive, K, V) Update KV Cache for indices in Iactive end if end for Denoising and State Update Update x(t) to x(t+1) and compute new confidence c(t+1) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: end for 36: return x(T ) sion LLMs perform full attention computation over the entire sequence at each denoising step, without any caching or sparsification mechanisms. Fast-dLLM. As strong baseline for approximate KV cache methods, Fast-dLLM (Wu et al., 2025) introduces block-wise approximate KV cache tailored for the bidirectional attention in dLLMs. It reuses cached activations from previously decoded blocks to reduce redundant computation. Sparse-dLLM. This method (Song et al., 2025) accelerates dLLM inference by integrating dynamic cache eviction with sparse attention principles. It leverages the temporal stability of token saliency to identify and retain critical KV entries while dynamically evicting unimportant entries from both the prefix and suffix contexts. SparseD. As pure sparse attention baseline, SparseD (Wang et al., 2025) is tailored for the Table 4: Detailed information of the datasets in the LongBench benchmark. Label Eval. Metric Avg. Len. Gen. Len. Steps Language Sample Num. Qasper F1 MultiFieldQA-en F1 F1 HotpotQA F1 2WikiMQA F1 Musique Rouge-L GovReport Rouge-L QMSum Rouge-L MultiNews Accuracy TREC F1 TriviaQA Rouge-L SAMSum Lsht Accuracy PassageRetrieval Accuracy Edit Sim Lcc Edit Sim RepoBench-P 3,619 4,559 9,151 4,887 11,214 8,734 10,614 2,113 5,177 8,209 6,258 22,333 9,289 1,235 4, 32 64 32 32 32 512 512 512 64 32 128 64 32 64 64 EN 32 EN 64 EN 32 EN 32 EN 32 EN 512 EN 512 EN 512 EN 64 EN 32 EN 128 ZN 64 32 EN 64 Python/C#/Java Python/Java 64 200 150 200 200 200 200 200 200 200 200 200 200 200 500 500 unique attention patterns in dLLMs. Its core strategy involves pre-computing head-specific sparse patterns once and reusing them across subsequent denoising steps. To preserve generation quality, it applies full attention during the critical early steps before switching to the pre-computed sparse patterns for the remainder of the inference process. To ensure fair comparison, all methods uniformly employ the semi-autoregressive remasking strategy, with the block length set to 32 across all experiments. A.3 Generation Settings Table 4 presents the detailed configurations for each task in the LongBench benchmark. To align with the evaluation settings of UltraLLaDA (He et al., 2025), we process all input contexts by truncating them to maximum length of 16K tokens using the \"drop-middle\" strategy. For each specific task, the generation length (Gen. Len.) and the generation steps (Steps) are configured as specified in the table. Details of Accuracy vs. Efficiency"
        },
        {
            "title": "Experiments",
            "content": "This section provides the detailed results underpinning our accuracy vs. efficiency analysis. Table 5 presents comprehensive performance comparison on the LongBench (Bai et al., 2024) for UltraLLaDA (He et al., 2025), including various configurations for both baseline methods and our own. For Sparse-dLLM (Song et al., 2025), we vary the retention ratio r, which determines the percentage of KV cache entries preserved. For SparseD (Wang et al., 2025), configurations differ in the skip ratio (the initial portion of steps using full attention) and the selection ratio r. The configurations for our method, Focus-dLLM, correspond to different settings of the sparsity ratio α , which controls the amount of prompt context retained for attention computation, while all other hyperparameters remain consistent with the setup described in the main text(section 6.1). As the results consistently demonstrate, Focus-dLLM establishes better accuracy-efficiency trade-off, achieving superior overall performance compared to prior acceleration methods."
        },
        {
            "title": "C Attention Patterns of dLLM",
            "content": "To supplement our analysis in Section 4.2, Figure 8 presents broader visualization of attention patterns from LLaDA-8B-Instruct (Nie et al., 2025) across various layers and denoising steps. The heatmaps clearly illustrate the principles of locality (strong diagonals) and the formation of attention sinks (bright vertical bands). Most importantly, the figure provides strong visual evidence for the crosslayer consistency of these sinks. The locations of the prominent vertical bands are remarkably stable across different layers (e.g., Layer 9, 19, and 31) at any given denoising step. This observed stability is the primary motivation behind our method, as it validates our strategy of identifying sink locations at an intermediate depth and reusing them for deeper layers to eliminate redundant computation. 13 Table 5: Detailed performance and throughput comparison on LongBench (Bai et al., 2024) for UltraLLaDA (He et al., 2025). We report results for baselines and various configurations of our method, Focus-dLLM. Single-Doc. QA Multi-Doc. QA Summarization Few-shot Learning Synthetic Code Method Vanilla Fast-dLLM Sparse-dLLM (r=0.3) Sparse-dLLM (r=0.4) Sparse-dLLM (r=0.5) Sparse-dLLM (r=0.6) Sparse-dLLM (r=0.7) Qasper 19.14 18.34 17.10 17.99 18.04 19.06 19.03 SparseD (skip=0.2,r=0.3) 19.09 SparseD (skip=0.2,r=0.2) 18.85 SparseD (skip=0.2,r=0.1) 18.06 SparseD (skip=0.1,r=0.3) 18.89 SparseD (skip=0.1,r=0.2) 19.34 Focus-dLLM (α=0.3) Focus-dLLM (α=0.4) Focus-dLLM (α=0.5) Focus-dLLM (α=0.6) Focus-dLLM (α=0.7) 16.63 16.60 17.02 16.91 17.71 MF-en 25.87 29.90 25.82 27.67 27.26 26.94 27.35 25.87 25.70 25.76 24.72 24.21 29.57 27.83 29.11 28.20 29.36 HotpotQ 2WikiM Musique GovReport MSum C A TriviaQ 16.27 17.03 20.82 18.90 20.59 20.80 21.64 15.45 16.10 15.40 14.45 14. 23.09 23.81 22.47 22.75 23.61 18.00 17.11 18.63 18.55 17.88 18.30 18.04 18.04 17.14 16.64 14.65 13.84 21.14 21.34 21.49 23.21 22.52 12.08 13. 15.35 13.11 13.67 14.31 13.24 11.92 11.64 11.47 10.93 10.80 18.59 18.52 20.20 18.94 19.12 32.83 30.05 27.95 28.77 29.95 30.16 30.63 32.64 32.29 31.44 32.55 31. 26.09 26.85 26.75 26.39 26.97 22.48 80.00 22.89 79.50 22.52 71.00 23.01 74.50 23.57 76.50 23.68 77.00 23.38 78.50 22.50 79.50 22.52 79.50 22.60 79.50 22.93 79.50 23.12 79.50 21.05 69.50 21.03 72.00 21.45 77.00 21.86 76.50 21.58 76.50 91.58 91. 91.93 91.43 91.93 91.43 91.43 90.70 90.70 91.05 91.70 91.53 91.28 90.78 90.78 90.78 90.78 Ave. Score Throughput(16K) LSH PRe Lcc B-P 41.00 96.75 68.23 59.50 42.00 94.75 67.50 58.10 41.50 98.71 67.07 57.18 42.00 98.17 67.80 57.68 41.50 97.12 67.50 57.72 42.50 96.25 67.99 57.44 41.50 95.42 67.94 57.40 41.50 96.79 68.10 59.02 41.50 96.67 68.02 58.77 41.50 96.84 67.98 58.59 41.50 97.12 67.86 59.18 42.00 97.88 67.72 59. 38.00 97.27 66.19 57.07 40.00 97.23 66.55 56.18 41.00 95.73 66.72 57.12 41.50 95.84 66.67 56.75 41.00 96.67 66.85 56.72 44.90 44.74 44.28 44.58 44.86 45.07 45.04 44.70 44.57 44.37 44.31 44.22 44.27 44.52 45.14 45.10 45.34 1.06 11. 12.67 12.34 11.94 11.57 11.19 1.38 1.44 1.51 1.44 1.52 19.48 19.37 19.30 19.10 19.14 Figure 8: Attention patterns in LLaDA-8B-Instruct (Nie et al., 2025) across various layers and denoising steps. The heatmaps demonstrate the emergence of attention sinks (vertical bands) and their strong positional consistency across different layers within the same step."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Hong Kong University of Science and Technology",
        "SenseTime Research"
    ]
}