{
    "paper_title": "SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner",
    "authors": [
        "Yufan Zhou",
        "Ruiyi Zhang",
        "Jiuxiang Gu",
        "Nanxuan Zhao",
        "Jing Shi",
        "Tong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SUGAR, a zero-shot method for subject-driven video customization. Given an input image, SUGAR is capable of generating videos for the subject contained in the image and aligning the generation with arbitrary visual attributes such as style and motion specified by user-input text. Unlike previous methods, which require test-time fine-tuning or fail to generate text-aligned videos, SUGAR achieves superior results without the need for extra cost at test-time. To enable zero-shot capability, we introduce a scalable pipeline to construct synthetic dataset which is specifically designed for subject-driven customization, leading to 2.5 millions of image-video-text triplets. Additionally, we propose several methods to enhance our model, including special attention designs, improved training strategies, and a refined sampling algorithm. Extensive experiments are conducted. Compared to previous methods, SUGAR achieves state-of-the-art results in identity preservation, video dynamics, and video-text alignment for subject-driven video customization, demonstrating the effectiveness of our proposed method."
        },
        {
            "title": "Start",
            "content": "SUGAR: Subject-Driven Video Customization in Zero-Shot Manner Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Nanxuan Zhao, Jing Shi, Tong Sun Adobe Research {yufzhou, ruizhang, jigu, nanxuanz, jingshi, tsun}@adobe.com 4 2 0 2 3 1 ] . [ 1 3 3 5 0 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present SUGAR, zero-shot method for subject-driven video customization. Given an input image, SUGAR is capable of generating videos for the subject contained in the image and aligning the generation with arbitrary visual attributes such as style and motion specified by userinput text. Unlike previous methods, which require testtime fine-tuning or fail to generate text-aligned videos, SUGAR achieves superior results without the need for extra cost at test-time. To enable zero-shot capability, we introduce scalable pipeline to construct synthetic dataset which is specifically designed for subject-driven customization, leading to 2.5 millions of image-video-text triplets. Additionally, we propose several methods to enhance our model, including special attention designs, improved training strategies, and refined sampling algorithm. Extensive experiments are conducted. Compared to previous methods, SUGAR achieves state-of-the-art results in identity preservation, video dynamics, and video-text alignment for subject-driven video customization, demonstrating the effectiveness of our proposed method. 1. Introduction Subject-driven customization [9, 17, 33, 44, 46, 50] seeks to create images or videos for specific identity which only appears in user-provided image or video. The generated images and videos are also expected to align with arbitrary requirements specified in user-input text. Although some efforts have been made in subject-driven video customization, existing approaches struggle to generate videos with good identity preservation and text-alignment. In this work, we propose SUGAR, novel method that performs SUbject-driven, customized video GenerAtion in zeRo-shot manner. Compared to existing methods, our SUGAR can generate videos with better identity preservation and text-alignment. Specifically, SUGAR can extract identity-only information from user-input image, and generate videos for the target subject with style, texture and motion guided by arbitrary user-input text. Some generated examples are shown in Figure 1. Our approach is inspired by some recent works on subject-driven image customization [9, 50, 51]. In this work, we explore two key questions: how to build largescale synthetic dataset specifically designed for subjectdriven video customization to enable zero-shot capability, and how to enhance model performance through improved model design, training strategy, and sampling algorithm. Our major contributions can be summarized as follows: We propose novel method for zero-shot subjectdriven video customization. Given single subject image, our method generates high-quality video for the subject, while adhering to the requirements such as specified style or motion from input text; We propose dataset construction pipeline and construct dataset containing 2.5 million image-videotext triplets. As shown in our experiments, our synthetic dataset is necessary for achieving good text alignment; We propose and test various model designs, training strategies, and sampling algorithms, which significantly improve the performance of our final model; We conducted extensive experiments, and our proposed method outperforms all previous methods across different metrics. Some ablation studies are also conducted to provide readers with deeper understanding of the proposed method; 2. Related Works Recently, researchers have achieved significant progress in text-guided image and video generation. Large-scale pretrained models are capable of generating text-aligned images [11, 22, 30, 32, 35] or videos [3, 4, 6, 13, 15, 19, 23, 28, 34, 37, 41] with high-quality and diverse content. However, despite of their impressive capabilities, these models fail to perform customized generation for novel concepts, such as generating creative images or videos for specific subject from single user-provided testing image. Various approaches have been proposed for the customized generation task in both image and video domains. Textual Inversion [12] proposes to represent the target with 1 Figure 1. Generated examples from the proposed method, where the frames are randomly sampled from the generated video. Our proposed method will generate videos for specific subject contained in user-input image, in zero-shot manner. The generated video will also meet the requirements described by user-input text. text embedding which can be optimized inside the text embedding space of pre-trained text-to-image (T2I) generation model. DreamBooth [33] and CustomDiffusion [20] propose to fine-tune the pre-trained T2I model on the testing image so that fine-grained details of the subject can be captured. In the domain of text-to-video (T2V) generation, some methods [44, 46] also propose to fine-tune pre-trained T2V models or adapters, on user-input subject image or video so that the resulting models can generate customized videos for the target subject. Figure 3. The proposed pipeline for synthetic data generation. contains only the target subject without any background. The identity image is obtained by applying segmentation on s, using pre-trained Grounded SAM [31]. We use to denote user-input text specifying requirements such as video style and subject motion. The target video is represented by x. Some pre-trained encoders are used in our approach, including pre-trained DINOv2 [25] image encoder, CLIP [29] image encoder and T5 text encoder, which are represented by fD(), fC() and fT5() respectively. Our model design is illustrated in Figure 2, we design our method by extending the architecture of CogVideoX [48], which is transformer-based diffusion model defined in the latent space of pre-trained Variational Auto-encoder [18]. The input of our model is the concatenation of DINO embedding, CLIP image embedding, text embedding and noisy latent code. Different projection layers are applied respectively before concatenation. 3.1. Synthetic Dataset Construction Recall that given subject image s, our target is to generate video x, which should contain the desired identity and be aligned with user-input text c. To obtain model that can perform zero-shot video customization, we would like to train the model on large-scale dataset consisting of triplet samples in the form of (x, c, s). Specifically, we expect and to contain the same identity but differ in terms of visual attributes such as style, color, etc., as required by text c. Training with such dataset leads to model that extracts identity-only information from the input image, leading to more flexible and text-aligned generation. Our synthetic dataset is generated with the pipeline presented in Figure 3, with details provided as follows. Data sourcing We start with collection of image-label samples featuring millions of subjects, covering wide range of subjects including objects and animals. For each image, we generate text prompt which simulates user intention, describing target video containing the subject with specified visual attributes such as style, texture, color, or background. Specifically, we have pre-defined set of text templates which is first manually designed then enriched by large language model [40]. Figure 2. Illustration of our model, randomly sampled frames from the generated video are shown in the figure for better illustration. However, aforementioned approaches require extra time and computation cost due to the need of fine-tuning pretrained models, which is inefficient for real-world applications. Thus some researchers investigate test-time tuningfree methods which can generate customized images or videos in zero-shot manner, without the need for test-time fine-tuning [9, 10, 24, 26, 28, 36, 45, 50, 51]. Our proposed SUGAR, inspired by these works, also aims to generate subject-driven customized videos with better quality in zero-shot manner. Different from concurrent work DreamVideo-2 [45], which also focuses on zero-shot subject-driven T2V generation, we consider transformer-based rather than UNetbased diffusion model. Additionally, we introduce novel pipeline for constructing synthetic datasets and explore distinct model design, training strategy and sampling algorithm, setting our work apart from DreamVideo-2. 3. Method Recent works [9, 50, 51] have shown that training generative models on high-quality synthetic dataset leads to impressive results in zero-shot image customization. Inspired by these works, we start with constructing synthetic dataset designed for subject-driven video customization. Then we discuss how to better utilize this synthetic data with improved model design, training strategy and sampling algorithm. Throughout the paper, we use to represent the input subject image and use to denote the identity image, which 3 Customized image generation Given an image-text pair from the previous step, we utilize the existing method to perform subject-driven text-to-image generation, resulting in customized image based on the specified subject and text. Specifically, we choose to follow Toffee [51] for this step because of its superior performance and efficiency. We filter out the generated image when it is not aligned with the text, or the identity deviates too much from the input image. The image-text alignment and identity preservation are evaluated by cosine similarity between features extracted with pre-trained CLIP [29] and DINO [7]. Customized video generation Given an image customization result from the previous step, we perform data processing steps, including padding, inpainting, and resizing to modify its aspect ratio and resolution. The processed image is then fed into pre-tained image-to-video (I2V) model to generate dynamic video. Due to computation and efficiency issues, we use Stable Diffusion XL inpainting model [27] and DynamiCraft [47]. The dataset quality can be further improved by adopting better pre-trained models. At last, we apply video-level filtering, to filter out low-quality videos. For instance, we evaluate subject consistency by computing average DINO [7] similarity between consecutive frames, and similarity between randomly sampled frames, to filter out videos with inconsistent subjects. Inspired by previous work [16], we also filter out static videos using optical flow [39]; In the end, we obtain dataset containing 2.5 millions of image-video-text triplet samples. In later experiments, we will show that this synthetic dataset plays an important role and hugely improves the performance of resulting model. 3.2. Model Training and Testing Different from subject-driven T2I generation [9, 12, 21, 24, 33, 36, 43, 4951] which focus on identity preservation and text-alignment, subject-driven video generation further takes subject motion into consideration. However, we find that most samples from our synthetic dataset are videos of standing still subjects. This is because the pre-trained I2V generation model we used often fails to generate textaligned subject motions. As result, our model directly trained with the constructed dataset also fails to generate text-aligned subject motion. To solve this problem, we propose the followings. Utilizing Real-world Video To improve the subject motion of generated videos, we proposed to include largescale video-text dataset in our training. Large-scale realworld datasets [1, 8] often contain videos with variety of subject motions which we would like our model to learn. Different from our dataset, the real-world video-text dataset does not have the input image. To bridge this gap, we use Grounded SAM [31] and perform object segmentation on randomly sampled frames to obtain the identity image. limitation of real-world dataset is that it is difficult to find frames within the same video which contain the same subject with different styles or textures. As result, identity images and video frames from real-world videos will share the same style, subject texture, and other visual attributes. Training model on such data makes it challenging to generate video in styles that is different from In our experiments, we find that utilizthe input image. ing both real-world and synthetic datasets leads to better performance than training the model on only synthetic or real-world datasets. Training Strategy consequent question is how to train the model with both synthetic and real-world datasets. Throughout the paper, we assume that training samples are sampled from both real-world and synthetic datasets. We use to denote the probability of sampling from the synthetic dataset. As result, = 0 stands for training the model only on real-world dataset, while = 1 meaning training solely on synthetic dataset. In this work, we propose the following training strategies: SUGAR-Mix: simple training strategy where we train the model on mixed dataset with fixed during training. We try different selected from [0.0, 0.5, 1.0] in our experiments; SUGAR-TS: two-stage training strategy, where TS stands for two-stage. The model is first trained on realworld videos, which corresponds to = 0.0. Then the model is trained on mixed dataset with = 0.5; SUGAR-TSF: two-stage training strategy, where TSF stands for two-stage with frozen layers. The is trained on real-world videos and mixed model dataset sequentially. Different from SUGAR-TS, we freeze some layers in SUGAR-TSF during the second stage training. Our assumption is that the layers closer to the input side determine the basic semantic and motion of the generated video. By freezing these layers in second stage, the model retains its motion generation capability learned from real-world videos, while learning improved visual attribute change from synthetic videos. For simplicity, we keep the first half of the model layers frozen in our experiments; Special Attention Design To mitigate the effects of learning limited motions from synthetic video dataset, we propose some special designs for attention operation in the transformer. Our assumption is that not all the frames have to be directly conditioned on the input image embeddings. Although image embeddings are important for maintaining subject identity, they also contain rich structure information that leads to generated videos with little subject mo4 where D, represent embeddings for unconditional case, denotes the text embedding of negative text prompt, ωI and ωT denote the guidance scales for image and text respectively [5, 14]. To further improve the subject motion at inference time, we propose to drop image embeddings at the early stage of sampling. Dropping image embeddings provides more flexibility, leading to generated videos with better textalignment and subject motion. However, dropping image embeddings also leads to identity loss as we will show in the experiments. To solve this problem, we propose to only drop fD(z) and keep using fC(z) which contains coarse information of the input identity. As we will show in experiments, this approach keeps the generated identity closely aligned with the target while achieving improved subject motion. Let 0 be hyper-parameter, for timestep , we revise the estimation (1) and (2) to be ϵθ = ϵθ(xt, D, C, T) + ωI (ϵθ(xt, D, fC(z), fT5(c)) ϵθ(xt, D, C, fT5(c)) + ωT (ϵθ(xt, D, C, fT5(c)) ϵθ(xt, D, C, T)), (3) and ϵθ = ϵθ(xt, D, C, T) + ωT (ϵθ(xt, D, fC(z), fT5(c)) ϵθ(xt, D, fC(z), T) + ωI (ϵθ(xt, D, fC(z), T) ϵθ(xt, D, C, T)). (4) In our experiments, (2) and (4) works slightly better than (1) and (3). 4. Experiment 4.1. Evaluation Metric Following previous benchmarks on subject-driven image customization [33] and text-to-video generation [16], we propose to evaluate our model with the following metrics, which stand for some requirements we care about the most in subject-driven video customization. Identity preservation We use DINO [7] score to assess whether the generated video contains the same subject as input image. Specifically, for given video, we calculate the average cosine similarity between DINO features of the input image and each video frame. Intuitively, higher DINO score reflects better identity preservation. Text-alignment To assess how well the generated video aligns with input text, we calculate the feature similarity between video and text using pre-trained CLIP [29] and ViCLIP [42]. Specifically, we compute the average CLIP similarity between all video frames and the text. Higher CLIP and ViCLIP scores indicate stronger text alignment. Figure 4. Different attention designs of our proposed model. One embedding can attend to another one only when the corresponding position is marked shadow in the above illustration. For instance, in design (b) image embeddings can not attend to the first frame, but the first frame can attend to the image embeddings. tion. Our new attention mechanism is termed as Selective Attention, highlighting that embeddings can only attend to specific selected others. We propose several special designs as illustrated in Figure 4, which presents how different embeddings attend to each other. For the ease of illustration, we simply use length of 1 for all kinds of embeddings in the figure. It is important to note that design (a) is equivalent to T2V model which can not perform customized generation, as its video output is not conditioned on the input image. Meanwhile, design (e) is simple extension of pre-trained T2V generation model, making it baseline design for comparison. In later section, we will show how our proposed attention design can improve the subject motion of generated videos without the loss of identity preservation. Improved Sampling Classifier-free guidance (CFG) [14] has been shown to play an important role in sampling from diffusion models. To enable CFG sampling, we drop DINO, CLIP, and text embeddings with probability of 0.5, 0.2, and 0.2 independently during training. Instead of using the same level of guidance for all conditions, we follow [5] to use different levels of guidance for identity and text conditions, which provides better flexibility in generation. We propose two variants to modify score estimation: ϵθ = ϵθ(xt, D, C, T) (1) + ωI (ϵθ(xt, fD(z), fC(z), fT5(c)) ϵθ(xt, D, C, fT5(c)) + ωT (ϵθ(xt, D, C, fT5(c)) ϵθ(xt, D, C, T)), and ϵθ = ϵθ(xt, D, C, T) + ωT (ϵθ(xt, fD(z), fC(z), fT5(c)) ϵθ(xt, fD(z), fC(z), T) + ωI (ϵθ(xt, fD(z), fC(z), T) ϵθ(xt, D, C, T)), (2) 5 Method VideoBooth DreamVideo Vidu-1.5 Subject-driven T2I + I2V + DynamiCrafter + CogVideoX + Runway Gen-3 + Luma Dream Machine SUGAR (Ours) ωT =7.5, ωI =7.5 ωT =5.0, ωI =7.5 ωT =4.0, ωI =7.5 ωT =3.0, ωI =7.5 ωT =2.5, ωI =7.5 DINO Score () CLIP Score () Dynamic ViCLIP Score () Degree () Consis. () Subject Background Consis. () 0.493 0.376 0.654 0.660 0.642 0.650 0.578 0.742 0.723 0.708 0.684 0. 0.266 0.296 0.303 0.321 0.323 0.326 0.320 0.311 0.318 0.322 0.327 0.329 0.173 0.213 0.237 0.260 0.264 0.265 0.270 0.250 0.262 0.267 0.273 0. 0.512 0.722 0.645 0.420 0.325 0.522 0.634 0.608 0.687 0.737 0.757 0.772 0.938 0.875 0.928 0.970 0.969 0.944 0.915 0.979 0.976 0.972 0.970 0. 0.960 0.937 0.942 0.971 0.967 0.950 0.935 0.983 0.981 0.980 0.977 0.974 Table 1. Quantitative evaluation of different methods. ωT and ωI are guidance scales for text and identity conditions, respectively. We use underline to indicate the best result. We also highlight cells for our method when the results outperform those of all baseline methods. Dynamic Degree Because we aim to generate dynamic video rather than static one, it is essential to evaluate whether video contains large motions. Following [16], we use RAFT [39] to estimate the dynamic degree of the generated video, with higher score indicating better motion dynamics. Consistency We expect the subject and background to remain consistent throughout Following VBench [16], we utilize pre-trained DINO and CLIP encoder to measure subject consistency and background consistency respectively. The detailed computation process can be found in [16], higher score indicates better consistency. the video. 4.2. Main Results We compare our method with baselines which fall into two categories: some approaches generate customized videos with single, end-to-end model, while others involve sequential process. In the latter, we combine subject-driven image customization with image-to-video (I2V) generation methods, allowing us to obtain customized images and videos sequentially. We select VideoBooth [17] and DreamVideo [44] as single-model baselines, as they are well-known methods for video customization. We also include Vidu-1.5 [2, 38] in comparison, which recently started supporting subjectdriven video customization and made the service publicly accessible. concurrent work DreamVideo-2 [45] requires an additional input besides subject image, which is the trajectory of subject bounding box, thus can not be directly compared with ours. For the sequential generation baseline, we choose Toffee [51] for subject-driven image customization because of its strong performance, and utilize several popular or state-of-the-art I2V generation models including DynamiCrafter [47], CogVideoX-5B [48], Luma Dream Machine [23], Runway Gen-31 [34]. Following previous works [33, 44], we use subject images provided by DreamBench [33] for evaluation. Specifically, DreamBench consists of 30 subjects including various animals and objects. We selected one image for each subject as shown in the Appendix. Different from DreamBench which is designed for subject-driven image customization, we would like to include motion requirements in our testing prompts. However, certain motions are only applicable to some subjects. For example, the motion dance is suitable for dog but not for vase. To address this, we categorize the subjects into 3 classes: animals, active objects which may perform human-like motions, and static objects. It is important to note that static objects can also exhibit motions such as sliding and floating. We carefully designed 18 prompts for each category, which are provided in the appendix, resulting in 540 subject-prompt combinations. Our main quantitative results are shown in Table 1. Our SUGAR model is trained with attention design (b), training strategy SUGAR-TSF, and evaluated with different sampling hyper-parameters. We choose = = 1000 for simplicity unless otherwise specified. From the results, we can find that our proposed method outperforms the previous method on different metrics across different settings. We observe that Vidu-1.5 demonstrates impressive identity preservation and video quality. However, it faces challenges when the input text aims to modify subject attributes like style, texture, or color. It either generates video without target attribute changes or video with the desired changes but suffers from identity loss, which compromises its performance. Some qualitative results and discussions are provided in the Appendix. 1We use Gen-3 Alpha Turbo as it is efficiently accessible via API. 6 Figure 5. Comparison of models with different attention designs. Figure 6. Comparison of models using different training strategies. 4.3. Ablation Study Comparison of different attention designs To understand the impact of different attention designs, we train several models with different designs under the same training strategy. Specifically, we choose the simplest training strategy SUGAR-Mix with = 0.5, to eliminate the influence of training strategy and focus solely on the model design. The results are shown in Figure 5, where the models are evaluated with collection of inference settings: we keep ωT = 7.5 fixed and test the models with ωI selected from {2.5, 3.0, 4.0, 5.0, 7.5}. We also test model trained with attention design (a), whose results will not be influenced by different ωI as the generation is not conditioned on the input image because of the attention design. It is not included in the figure because it only obtains DINO score of 0.339. From the results, we can conclude that design (b) outperforms others in general. Even if only single frame is directly conditioned on the input image, we do not observe subject identity loss. Furthermore, under the same level of identity preservation evaluated by the DINO score, design (b) obtains better text alignment as indicated by CLIP and ViCLIP scores, it also generates better motions, as indicated by higher dynamic degree. Comparison of different training strategies To compare different training strategies, we train multiple models using different strategies while keeping the same attention design. In these experiments, we use the simplest attention design, which is design (e) from Figure 4. The results are shown in Figure 6. We notice that SUGAR-Mix with = 0 achieves competitive dynamic degree and much worse text-alignment compared to others. This is because the model is trained on samples processed from real-world videos, which provide rich information on subject motions but lack the style and color changes between the target videos and identity images. On the other hand, SUGAR-Mix with = 0.5 or = 1.0 achieves better text-alignment due to the contribution of synthetic dataset, while the dynamic degree drops significantly. Among all the training strategies, our proposed SUGAR-TSF obtains good text-alignment without the loss of dynamic degree, showing the effectiveness of the proposed strategy. The necessity of video customization dataset In previous ablation study, we have already shown that the synthetic customization dataset is important to obtain good text-alignment. One remaining question is: instead of using the video customization dataset, what if we use the image customization dataset by treating the images as videos with the single frame? This is an important question as image customization dataset requires less construction cost, and many existing text-to-video generation models can be trained on videos and image datasets simultaneously. We conduct an ablation study by training two models, TSF-Video and TSF-Image. Both models are trained with the SUGAR-TSF strategy and attention design (e). We first train model on real-world videos, then fine-tune it on two mixed datasets. TSF-Video is fine-tuned on mixture of real-world videos and our synthetic dataset with = 0.5, TSF-Image is fine-tuned on mixture of real-world videos and image customization dataset. For fair comparison, we directly sample random frames from the video customization dataset as target images for TSF-Image during training. The results are presented in Figure 7, from which we can see that TSF-Video obtains better results in text-alignment. TSF-Video also achieves higher dynamic degree given the same level of subject consistency. We can conclude that video customization dataset provides stronger supervision for training than image customization dataset. 7 Figure 7. Comparison of models trained with video customization dataset and image customization dataset. Figure 9. Comparison of dropping different image embeddings. Figure 8. Generated examples with different levels of guidance. Impact of dual conditions In addition to quantitative results of sampling using different ωI , ωT , we present qualitative comparison in Figure 8, where we set = = 1000 to exclude the effect of dropping image embeddings. From the comparison, we can see that using large guidance for identity conditions leads to video with restricted motion, which can be improved by reducing ωI . However, when ωI is too small, it may result in identity loss. Impact of dropping image embedding We have shown that we can control the level of identity preservation and dynamic degree by using different ωI and ωT . We would like to investigate the impact of dropping image embeddings at inference. Specifically, we set = 900 and test 4 settings including dropping only DINO or CLIP embedding, dropping both embeddings and model trained without dropping. The results are shown in Figure 9, from which we can find that dropping DINO embedding at the early sampling stage leads to slightly better dynamic degree, without the loss of text-alignment and identity preservation. qual8 Figure 10. Generated examples of dropping different embeddings. itative comparison is shown in Figure 10, where we use ωI = ωT = 7.5 to exclude the influence of dual condition sampling. From the comparison we can find that dropping image embedding indeed leads to better subject motion. However, dropping both DINO and CLIP embeddings may lead to identity loss: the robot in the figure has extra legs which do not exist in the input image. 5. Conclusion We propose SUGAR, zero-shot method for subject-driven video customization, without the need of test-time finetuning. Our approach consists of dataset construction pipeline, special attention design, training strategy and improved sampling techniques. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art results."
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 4 [2] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 6 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with laIn Proceedings of the IEEE/CVF tent diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), pages 2256322575, 2023. 1 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 5 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 4, 5 [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers, 2024. 4 [9] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023. 1, 3, 4 [10] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481, 2023. 3 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2022. 1, 4 [13] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2025. [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 5 [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1 [16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models, 2023. 4, 5, 6 [17] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image In Proceedings of the IEEE/CVF Conference prompts. on Computer Vision and Pattern Recognition, pages 6689 6700, 2024. 1, 6 [18] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [19] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 1 [20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. [21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 4 [22] Black Forest Labs. Flux, 2024. 1 [23] LumaLabs. Luma dream machine, 2024. 1, 6, 12 [24] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image arXiv preprint generation without test-time fine-tuning. arXiv:2307.11410, 2023. 3, 4 [25] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael 9 Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 3 [26] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992, 2023. 3 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 4 [28] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1, [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 4, 5 [30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. In Proceedings of the Zero-shot text-to-image generation. 38th International Conference on Machine Learning, pages 88218831. PMLR, 2021. 1 [31] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 3, 4 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 1, 2, 4, 5, 6 [34] RunwayML. Gen-3 alpha, 2024. 1, 6, 12 [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [36] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. arXiv preprint arXiv:2304.03411, 2023. 3, 4 [37] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations. 1 [38] Shengshu Technology. Vidu-1.5, 2024. 6, 12 [39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 4, 6 [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [41] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 1 [42] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2023. 5 [43] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 4 [44] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. 1, 2, [45] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 3, 6 [46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning In of image diffusion models for text-to-video generation. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 1, 2 [47] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 4, 6 [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 6 10 [49] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhancing detail preservation for customized text-to-image genarXiv preprint eration: regularization-free approach. arXiv:2305.13579, 2023. [50] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. CusIn Protomization assistant for text-to-image generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 91829191, 2024. 1, 3 [51] Yufan Zhou, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, and Tong Sun. Toffee: Efficient million-scale dataset construction for subject-driven text-to-image generation, 2024. 1, 3, 4, 6, 12 11 Figure 11. Comparison of models with different attention designs. A. More Results More comparison of different attention designs We have tested different attention designs where the models are trained with Sugar-Mix (p = 0.5), and found that attention design (b) leads to the best results. Will this still hold true if we use our improved training strategy Sugar-TSF? To answer this question, we conduct another experiment whose results are shown in Figure 11. For clearer presentation, we only report the results of design (b) and design (e) here. From the results, we can conclude that design (b) is still slightly better, while the performance gap becomes smaller, due to the contribution of our improved strategy Sugar-TSF. More comparison with baseline methods We present some generated examples in Figure 12, 13, 14, 15, 16, 17. Baseline methods using sequential generation, which combine Toffee [51] with Luma Dream Machine [23] and Runway Gen-3 [34], sometimes fail to maintain the subject identity. This is because the pre-trained I2V models used in these methods are not trained on customization dataset, they may generate creative but undesired outputs. For example, the appearance of cat from Luma Dream Machine in Figure 12 changes; the toy from Luma Dream Machine in Figure 15 has deformation issue. Another issue with sequential generation baseline is error propagation: if the subject-driven image customization fails to generate textaligned image, it will cause the generated video to fail as in Figure 15 baselines fail to generate subject with well: texture change because the image customization fails. Vidu-1.5 [38], although obtains good performance in some cases, fails to change the style or texture of the subject as shown in Figure 13, 14, 15. All the baseline methods face challenges in understanding concepts such as pencil drawing drawn by hand and Figure 12. Generated examples from SUGAR and baselines. Figure 13. Generated examples from SUGAR and baselines. Figure 14. Generated examples from SUGAR and baselines. 2D animated illustration in Figure 13 and 14. They also fail to generate certain subject motions. For instance, in Figure 16 and 17, the robots in their generated videos seem to be sliding rather than walking or dancing as required by the input text. 12 Figure 15. Generated examples from SUGAR and baseline. Figure 18. Images used in quantitative evaluation. Figure 16. Generated examples from SUGAR and baseline. Figure 19. Testing prompts designed for active objects. tively. High-resolution images can be found from the official release of DreamBench at https://github.com/ google/dreambooth. Designed testing prompts We provide our testing prompts for quantitative evaluation in Figure 21, Figure 19 and Figure 20. We use sks as special token, which will be replaced by corresponding noun such as dog at testing. Figure 17. Generated examples from SUGAR and baseline. B. More Details Subject images in quantitative evaluation The subject images used in our quantitative evaluation are presented in Figure 18, where animals, active objects and static objects are indicated by blue, green and red color respec13 Figure 20. Testing prompts designed for static objects. Figure 21. Testing prompts designed for animals."
        }
    ],
    "affiliations": [
        "Adobe Research"
    ]
}