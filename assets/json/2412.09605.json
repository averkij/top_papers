{
    "paper_title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
    "authors": [
        "Yiheng Xu",
        "Dunjie Lu",
        "Zhennan Shen",
        "Junli Wang",
        "Zekun Wang",
        "Yuchen Mao",
        "Caiming Xiong",
        "Tao Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 5 0 6 9 0 . 2 1 4 2 : r AG TTR K: AGENT TRAJECTORY SYNTHESIS"
        },
        {
            "title": "VIA GUIDING REPLAY WITH WEB TUTORIALS",
            "content": "Yiheng Xu Dunjie Lu Zhennan Shen Yuchen Mao Caiming Xiong Tao Yu University of Hong Kong Salesforce Research {yhxu,tyu}@cs.hku.hk cxiong@salesforce.com https://agenttrek.github.io Junli Wang Zekun Wang"
        },
        {
            "title": "ABSTRACT",
            "content": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, scalable data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs visual-language model (VLM) agent to simulate their execution in real digital environment. VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Graphical User Interfaces (GUIs) are fundamental medium for human-computer interaction, enabling users to perform tasks across various digital pl atforms. Automating GUI operations through agentic automation has the potential to significantly enhance productivity by enabling autonomous task completion using human-centric tools. Additionally, this approach can foster the development of advanced AI systems capable of learning from rich digital environments. Recent advancements in large language models (LLMs) have endowed the models with powerful abilities in understanding, reasoning, and decisionmaking, which are essential for the evolution of GUI agents in diverse contexts such as web (Zheng et al., 2024), desktop (Xie et al., 2024), and mobile applications (Zhang et al., 2023). Despite these advancements, the performance of GUI agents remains suboptimal. Contemporary Large Language Models (LLMs) are primarily engineered and trained on datasets optimized for generating informative responses (Ouyang et al., 2022; OpenAI, 2024). Their architecture and training paradigms are Figure 1: Expected Agent Trajectories Equal contribution 1 Figure 2: Overview of the AgentTrek Pipeline: (1) Automatic Tutorial Collection from the Internet: Tutorial-related data is extracted and filtered from internet sources using heuristic methods and FastText model. An LLM processes the filtered textual data, transforming it into structured tutorials. (2) Trajectory data collection via guided replay: VLM agent interacts with the real digital environment guided by tutorials, while high-quality trajectory data, including observations, actions, and reasoning, is collected. Another VLM evaluator acts as judger to further improve the effectiveness of the synthetic dataset. (3) Training and fine-tuning with replay data: The collected trajectory data is used to train and fine-tune GUI agent models, which are evaluated on standard agent benchmarks, demonstrating significant improvements. not inherently designed to make complex, sequential action decisions that require long-term observation and historical context. Consequently, training GUI agents with multi-step trajectory data is crucial to improving their capabilities. High-quality agent trajectories contain several key components: high-level goal, sequence of interleaved observations, natural language reasoning, and grounded actions (as shown in Figure 1). Unfortunately, such data is not readily available on the internet like textual or image data, as it involves complex situational reasoning and multimodal interactivity. Existing approaches typically rely on human annotation to collect these trajectories (Deng et al., 2024; Rawles et al., 2023; Li et al., 2024), process that is both expensive and not scalable. To address this data scarcity, data synthesis has emerged as vital approach in AI system development. Synthesizing agent trajectories presents significant challenges due to the need for interwoven natural language instructions, visual observations, and context-specific actions that must be accurately grounded in the GUI environment. Although there have been some successful applications of LLMs in data synthesis pipelines (Ye et al., 2022; Peng et al., 2023; Qin et al., 2023), these complexities still make GUI trajectory synthesis particularly demanding. In this work, we present AgentTrek, scalable data synthesis pipeline specifically designed for training GUI agents. We begin by automatically gathering and filtering tutorial-like text from the web, which describes GUI tasks and workflows in web environments. These tutorials are then transformed into agent tasks with high-level objectives and detailed step-by-step instructions. Using visual-language model (VLM) agent, we simulate the execution of these tasks, guided by the synthesized tutorials. An evaluator model is also employed to subsequently verify whether the goal was successfully achieved. Through this comprehensive pipeline, we efficiently generated large volume of high-quality web agent trajectories. Our experimental results demonstrate that training GUI agent models with these synthesized web agent trajectories not only improves their performance but also enables them to surpass the capabilities of their initial teacher models, which is the replay model GPT-4 in our case. Compared to traditional human-annotated data pipelines, our method is significantly more cost-effective, emphasizing the scalability and economic viability of the AgentTrek pipeline. We introduce AgentTrek, novel pipeline that leverages web tutorials to synthesize highquality web agent trajectory data at scale, effectively bridging the gap between LLM capabilities and the demanding need for multi-step, context-rich training data for GUI agents. 2 Extensive experiments demonstrate that agents trained with our synthesized data outperform those trained on existing datasets in both grounding and planning capabilities, validating the effectiveness of AgentTrek. Our pipeline significantly reduces the cost and scalability obstacles of human-annotated data collection, providing practical approach for large-scale GUI agent training through data synthesis. Table 1: Comparison of AgentTrek with other trajectory datasets for training. For the calculation of dataset size and average steps, see Appendix A. Datasets RUSS ScreenAgent WebLINX MM-Mind2Web GUIAct Size 80 203 969 1009 2482 AgentTrek (Ours) 10398 Average Steps HTML AxTree Intermediate Reasoning Video Matching Screenshot Website Task Inst. Level 5.4 4.3 18.8 7.3 6.7 12. Yes No Yes Yes No Yes No No No No No Yes No Yes No No No Yes No No No No No Yes No Yes Yes No Yes Yes 22 - 155 137 121 Low High & Low High & Low High High 127 High & Low"
        },
        {
            "title": "2 METHOD",
            "content": "We introduce pipeline to collect and process GUI tutorials from the internet for training visual language models (VLMs) in web automation tasks. The method comprises three main steps: 1. Collecting Tutorials: We extract web interaction tutorials from large datasets using keyword filtering and language models to identify and standardize relevant content. 2. Guided Replay: An agent uses these tutorials to perform tasks in web environment, interacting with real websites while we record its actions and thoughts. 3. Model Training: We train visual agent model that relies on screenshots and standard GUI actions, enhancing its web navigation capabilities with the collected data. This approach enables efficient training of VLMs without extensive manual annotation, offering scalable solution for automating web tasks. 2.1 AUTOMATIC TUTORIALS COLLECTION FROM INTERNET We first extract web interaction tutorials from Redpajama dataset (Computer, 2023). rule-based heuristic filter is applied to create preliminary dataset, subset of which is annotated by an advanced LLM to generate labeled samples for training effective FastText classification model (Joulin et al., 2017), the tutorial classifier. This classifier further enhances the data quality through filtering. Finally, LLMs are employed to tag and paraphrase the raw text of tutorials into standardized format, preparing them for the replay phase in Section 2.2. 2.1.1 PREFILTER FUNCTION Although GUI tutorials are abundant online, they constitute only small fraction of web content, making pre-filter essential for identifying relevant content. Similar patterns often appear in tutorials, such as distinctive keywords like click and type, as well as platform-specific terms like macOS and Windows. We compiled rule-based filter using keyword lists sourced from official websites and forums. Leveraging RedPajama data with over 20 billion URLs, our pre-filter applies Keyword Matching in the first 38k words, evaluates samples based on Length, and filters them by URL Format for relevance. Validated using 180 positive and 105 negative ground-truth samples, the prefilter achieved 92.69% recall rate on positive samples, ensuring both diversity and quantity. After filtering, the dataset size is reduced from 20.8 billion to 68.8 million entries (Figure 4). 3 Figure 3: Overview of the tutorial filtering and classification pipeline. Starting with Redpajama, the data is prefiltered, annotated by an advanced LLM, and used to train tutorial classifier. The classifier further filters the raw text, which is then paraphrased into structured tutorials with task descriptions, prerequisites, and step-by-step instructions. 2.1.2 LLM LABELER While initial rule-based filtering narrows the context, the proportion of true positive tutorial content remains low. To improve the quality and relevance of selected tutorials, we leverage an advanced LLM, GPT-4O MINI, for automated labeling, due to its strong ability to comprehend and analyze complex, information-dense text. Prior to full implementation, we tested GPT-4O MINI on manually annotated ground-truth validation set, where it achieved an F1 score nearly 90%. In cases where human and LLM annotations conflicted, the LLM demonstrated the ability to identify tutorial-related content in lengthy texts that humans might overlook. This, along with the validation set result, suggests that GPT-4O MINI may surpass human performance in webpage labeling, enabling efficient generation of large labeled dataset for training in the following section. 2.1.3 FASTTEXT FILTER Following the automated labeling process, we employed FastText, an n-gram-based deep learning model, as our classifier. FastText classifies tutorial text segments as tutorial or non-tutorial, with binary output and confidence score to enhance the accuracy of tutorial selection. To train the model, we combined LLM-labeled data with human-labeled samples, creating dataset of approximately 90,000 examples. The train-test split is 95:5, with the model demonstrating strong classification performance. Using this classifier, we further curated the initial filtered dataset, collecting approximately 18.8 million tutorial-like web text samples. Prefilter LLM FastText 0.69 0.885 0.895 Metric Precision Recall F1 0.60 0.89 0. 0.61 0.885 0.895 Table 2: Performance of Filters. 2.1.4 TAG & PARAPHRASE After filtering the raw tutorial content using the FastText model, we then tag and paraphrase the content for further processing, including extracting meta-information and formatting the tutorials according to standardized template. To handle the length and noise of the raw tutorial data, we employed GPT-4O MINI, streamlining the tagging and paraphrasing while ensuring the output aligned with the comprehensive template and gold-standard examples. The key components of the template include specifying the Platform and Target (e.g., macOS, Windows, browser, or app), providing concise Task Description, listing Prerequisites needed Figure 4: The data flow during the early stages of our pipeline. before starting, outlining Step-by-Step Instructions for completing the task, and detailing the Expected Outcome. The cost for tagging and paraphrasing 1,000 entries is approximately 0.89 dollars."
        },
        {
            "title": "2.2 TRAJECTORY DATA COLLECTION VIA GUIDED REPLAY",
            "content": "Figure 5: Overview of Guided Replay data collection and evaluation pipeline. VLM agent is provided with the filtered and formatted tutorials, then observes and interacts with the real environment during execution, while all the actions and intermediate thoughts are recorded as data trajectory. The final result is evaluated by an advanced VLM to ensure the correctness. 2.2.1 TRAJECTORY DATA DEFINITION The trajectory data generated by our pipeline is designed to enhance an agents web navigation capabilities by integrating high-level planning, low-level instructions, and grounded operations. Each data instance includes the following components: Task Information. Detailed task metadata, including platform, task description, prerequisites, instructions, and expected outcomes, which support both planning and execution. Post-processed Textual Trajectory. Refined after replay, highlighting key elements for model finetuning. This includes Task Metadata, summarizing the task to encourage adaptive decision-making, Observations to provide visual context, Intermediate Reasoning offering insights into the agents decision-making process, and Action Sequence to capture detailed element information for web interactions. Screenshots and Video Recordings. Visual records of the entire process for comprehensive documentation. Reproducible Native Trace. Captured via Playwright, including DOM snapshots, HTML, network flow, and action sequences, allowing full reconstruction and detailed analysis of agent-environment interactions."
        },
        {
            "title": "2.2.2 GUIDED REPLAY WITH TUTORIALS",
            "content": "Although we have collected and processed high-quality tutorials, significant gap remains in acquiring the grounding data crucial for training more effective agent model. To address this, we leverage BrowserGym (Drouin et al., 2024) to enable the model to replay tasks under the guidance of the generated tutorials. Figure 6: Guided replay example. This example demonstrates an agents execution of finding the return policy for mens football apparel, showcasing its actions alongside the corresponding inner thoughts. BrowserGym is versatile environment for web task automation in the Chromium browser, enabling Visual Language Model (VLM) agents to execute web-based operations (Drouin et al., 2024). Agents are provided with tagged and paraphrased tutorials and target web url, allowing them to navigate directly to the task scene. Step-by-step instructions guide the agent through the task, with the expected outcome determining task completion. The agents initial observations include the webpages viewport screenshot and accessibility tree (AXTree), but the HTML file is excluded due to its size and irrelevance to visual agents. Actions are executed using Playwright (Microsoft, 2023) functions such as click, select option, and clear, while Playwright also records detailed traces, including target elements, coordinates, screenshots, and DOM snapshots at the same time, along with agents internal thoughts between actions. Token consumption is about 8,027 per step and 86,114 per task. With GPT-4O-08-06, replaying 1,000 tasks costs approximately 215 dollars. Cost detail see 2.2.3 EVALUATION OF TRAJECTORY Although large amount of guided replay data has been recorded, it is crucial to extract the effective segments that can truly contribute to enhancing the agents performance. Recent work by (Pan et al., 2024) highlights the potential of Visual Language Models (VLMs) in evaluating trajectory data using recorded images and interaction processes as input. VLMs are highly scalable, capable of processing large datasets concurrently at low cost, and provide transparent evaluations. Therefore, we implemented VLM Evaluator to further improve our data quality. VLM Evaluator Design. To ensure trajectory data quality, we define effectiveness based on two criteria: adherence to task instructions and successful completion of core components. We employ GPT-4O as the backbone of our VLM evaluator, using structured prompt to assess recorded trajectories. The evaluator receives the task description d, the agents action history a, and inner thoughts for each step. The sequential format is: {task description; inner thought 1; action 1; inner thought 2; action 2; ...}, as illustrated in Figure 5. The VLM provides trajectory-level assessment and performs stepwise analysis, offering justifications for any ineffective trajectory and identifying the earliest point of failure. Validation on Human-Annotated Set. Although the capabilities of Vision Language Models (VLMs) are well-recognized, validation is essential. To assess the automatic evaluators perfor6 Table 3: Evaluator Accuracy Comparison Table 4: Cost Breakdown Trajectory Evaluator Replayed Web Tutorials GPT-4o WebArena Results GPT-4V Cap. + GPT-4 Cap. + Mixtral Acc. 84.0% 80.6% 82.1% 74.4% Phase Cost/1k ($) Model T&P Replay Eval Total 0.89 215.36 3.10 219.35 gpt-4o-mini gpt-4o gpt-4o mance, we manually reviewed 1,081 trajectories and created validation set of 558 samples with human-annotated justifications. As shown in the Table 3, despite handling different input formats across various task scenarios, the evaluator achieved high performance metrics. And according to the observation shown in Appendix D, evaluator often applys stricter standards than human evaluators. This demonstrates its robustness in accurately identifying effective trajectories. 2.3 TRAIN AND FINE-TUNE THE MODEL WITH TRAJECTORY DATA We chose purely visual agent model that relies exclusively on screenshot-based observations, rather than incorporating accessibility trees or textual representations, for several reasons. First, GUIs are inherently visual, and mapping instructions to visual elements aligns more closely with human cognitive processes. Second, Textual representations, such as HTML or accessibility trees, are often verbose, which leads to heavy overhead for computation. Second, Different websites can have varying structures for their textual representation while image-based representations allow the model to unify its observations across diverse platforms with varying resolutions, improving generalization. 2.3.1 PURE VISION & GUI ACTION FRAMEWORK In this work, we propose to unify observation and action space via pure vision and standard pyautogui commands with pluggable action system. Using pure vision as input eliminates the need for the model to understand different UI source codes across platforms, even when visually similar elements are written differently in HTML. Additionally, HTML input typically costs an average of 4,000 tokens per step. In contrast, recent VLMs with high-resolution multimodal understanding, such as Qwen2-VL, require only 1,200 tokens for 720p image. This significantly lowers the computational cost while maintaining sufficient visual information for the task. For action, we hoose the widely used standard pyautogui action space with pluggable action system. Most web agent leverage playwright action sapce. But playwright actions incline to interact with html selector element instead of visual ui element. Therefore, we use pyautogui commands to unify basic GUI operations on web. Since we collect the data from website by playwright, we need to map the playwright actions to pyautogui actions as shown in the Figure 9. In addition, we utilize pluggable action system to cover specific playwright action like select option. 2.3.2 MODEL ARCHITECTURE AND TRAINING Unlike agents that rely on structured UI representations like accessibility trees, vision-based grounding requires models to map intents directly to visual observations. For this, we chose Qwen2VL (Wang et al., 2024), which uses NaViT as an image encoder with dynamic resolution support Dehghani et al. (2023). By removing absolute position embeddings and incorporating 2D-RoPE Su et al. (2024), Qwen2-VL can process images of any resolution, efficiently converting them into variable visual tokens. This makes Qwen2-VL ideal for GUI agents, as it can encode high-resolution images with fewer token costs, making it well-suited for our tasks. Our training process, starting with VLM capable of high-resolution image understanding, consists of one tunnig stage. We use data from AgentTrek Data to enhance VLM capabilities in grounding and planning."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "AgentTrek autonomously gathers thousands of trajectories enriched with comprehensive multimodal data, including screenshots, accessibility trees, internal reasoning, and detailed actions with precise coordinates. This serves as an excellent resource for fine-tuning LLMs into proficient web agents and training VLMs into dependable visual Web agents. To showcase the utility of AgentTrek data, we can independently train text-based agent and vision-only agent to complete the assigned tasks."
        },
        {
            "title": "3.1 EXPERIMENTAL SETUP",
            "content": "Agent Training. For the text-based web agent, we utilize 6,000 agent trajectories, included accessibility tree as observations and playwright actions as the agents action space, from the AgentTrek dataset to fine-tune the Qwen2 series LLMs of various sizes, including 7B, 32B, and 72B. For the vision-only Web agent, we fine-tune Qwen2-VL Wang et al. (2024) using 10,000 selected agent trajectories from the dataset. Evaluation For Text-based Web Agent. To demostrate the capability of the text-based agent, we select WebArena Zhou et al. (2023) as evaluation benchmark. WebArena, based on real websites, creates multiple virtual environments and uses various evaluation methods to assess the task completion rate, making it more suitable for real-world task completion evaluation. Evaluation For Vision-based Web Agent. To validate the effectiveness of our dataset, it is essential to demonstrate its impact on improving both the grounding and planning capabilities of the model. Therefore, we will evaluate the models performance on two benchmarks that assess these abilities. (1) ScreenSpot (Cheng et al., 2024), which is GUI visual grounding benchmark comprising 1.2K single-step instructions and target element bounding boxes. It spans mobile, desktop, and web environments, categorizing elements into text and icons. Given that our data originates exclusively from the web, we focus solely on web-based performance. (2) Multimodal-Mind2Web (Deng et al., 2024), the multimodal extension of the web agent benchmark Mind2Web (Deng et al., 2024). It consists of three categories, namely, cross-task, cross-website and cross-domain, ranked by their degree of deviation from the training data. 3.2 MAIN RESULTS WebArena. Through the experimental results from table 5, we can obtain that: (1) AgentTreks textual trajectories significantly boost performance on WebArena, surpassing open-source baselines and GPT-4o. (2) Considering that WebArena is an OOD web agent benchmark featuring self-hosted websites, strong results on WebArena confirm that AgentTreks data generalizes well to unseen domains. Table 5: Comparison of task success rate on WebArena Model WebArena CodeLlama-7B-Instruct (Ou et al., 2024) LLaMa3-chat-8B (Ou et al., 2024) Qwen2.5-7B-Instruct LLama3-chat-70B (Ou et al., 2024) GPT-4o(Zhou et al., 2023) GPT-4(Ou et al., 2024) Synatra-CodeLlama-7B (Ou et al., 2024) AutoWebGLM (OOD SFT) (Lai et al., 2024) AutoWebGLM (In-domain RFT) (Lai et al., 2024) Qwen2.5-7B-Instruct w/ AgentTrek Qwen2.5-32B-Instruct w/ AgentTrek 0.00 3.32 3.80 7.02 13.10 14.41 6.28 8.50 18.20 10.46 16. ScreenSpot. Fine-tuning with the AgentTrek dataset significantly improved Qwen2-VLs grounding ability for both text and icon-based tasks, more than doubling its baseline performance and surpass8 ing several models on the ScreenSpot benchmark. This demonstrates the strong impact of AgentTrek in enhancing the models grounding capabilities for web-based GUI tasks. Table 6: Comparison of grounding performance on ScreenSpot Web Grounding Model Text Icon/Widget Average GPT-4 (Cheng et al., 2024) GPT-4o (Cheng et al., 2024) Qwen2-VL-7B SeeClick (Cheng et al., 2024) CogAgent (Cheng et al., 2024) GPT-4 + OmniParser (Lu et al., 2024) Qwen2-VL-7B w/ AgentTrek 9.2 12.2 35.2 55.7 70.4 81.3 81.7 8.8 7.8 25.7 32.5 28.6 51. 51.5 9.0 10.1 30.7 44.7 50.7 67.0 67.4 Mind2Web. The baseline Qwen2-VL-7B model is excluded due to its poor ability to locate target elements, critical requirement for web-based tasks. As result, only the fine-tuned models are included in the table. Fine-tuning with the AgentTrek dataset greatly enhanced Qwen2-VLs performance, particularly in the Operation F1 metric, where it surpassed both GPT-3.5 and GPT-4 in all settings. The combination of AgentTrek and Mind2Web datasets delivers the best results across all metrics and settings. While fine-tuning with Mind2Web alone yields strong performance, the model still benefits from the addition of AgentTrek data. These findings highlight the complementary strengths of the two datasets: AgentTrek provides precise, grounded data, while Mind2Web contributes valuable resources for tackling complex web-based tasks. Additional details on result sources are provided in Appendix J.2. Table 7: Performance comparison across different methods and evaluation settings. H, I, AT, M2W stand for HTML, Image, AgentTrek, Mind2Web Obs Model Method Cross-Task Cross-Website Cross-Domain Ele.Acc Op.F Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR HTML + Image GPT-3.5 GPT-4 GPT-4 GPT-4 Choice Choice Choice SoM Qwen2-VL Vision + AT + M2W Vision + AT + M2W Vision 19.4 40. 46.4 29.6 45.5 54.8 60.8 59.2 63.1 73.4 - 84.9 89.5 88.9 16.8 32. 40.2 20.3 40.9 50.9 55.7 14.9 30.2 38.0 20.1 40.8 52.9 57.6 56.5 61. 67.8 - 82.8 83.9 88.1 14.1 27.0 32.4 13.9 35.1 44.9 51.4 25.2 35. 42.4 27.0 48.6 51.8 56.0 57.9 61.9 69.3 - 84.1 86.8 87.5 24.1 29. 36.8 23.7 42.1 47.7 52."
        },
        {
            "title": "4 ANALYSIS",
            "content": "With our AgentTrek pipeline, we generate large-scale trajectory data that excels in three areas. First, the dataset offers extensive diversity, covering multiple domains and task types, and benefiting from internet-sourced tutorials that enhance task execution. Our experiment showed 230% performance increase when agents followed detailed instructions. Second, the data is gathered from real-world web environments, avoiding the limitations of simulations. Starting with RedPajama, we filtered and processed 23,430 tutorials, producing 10,398 successful trajectories from 127 websites. Third, the data is comprehensive, capturing highand low-level task details, including DOM/HTML structures, AXTree snapshots, video recordings, and screenshots. This rich data improves the agents performance on long-horizon tasks, and with per-trajectory cost of just $0.551, our pipeline offers an efficient, scalable solution for data generation. 4.1 IMPORTANCE OF TUTORIALS Tutorials extracted from the internet play crucial role in guiding the replay process. First, they ensure diversity in the generated trajectories. Tutorials often have distinct task goals, and even when they target the same objective, they may offer different execution methods, enriching the trajectory 9 data. Second, tutorials significantly improve the agents execution. We tested the agent on 400 tasks, replaying them twice: once with tutorials and once using only high-level goals. The results show that step-by-step instructions greatly enhanced performance. Without tutorials, only 63 effective trajectories were generated (15.78% of the total). With tutorials, the agent produced 208 effective trajectories (52%), marking an increase of over 230%, demonstrating the importance of detailed instructions in improving reliability and effectiveness. For further analysis, please see Appendix B"
        },
        {
            "title": "4.2 DATA COMPOSITION",
            "content": "Figure 7: The distribution of website with domains involved in our dataset To summarize the data flow through our pipeline: First, we filter tutorial data from the RedPajama web snapshot. Next, the filtered data is paraphrased for clarity and classification. We then gather up-to-date data from mainstream websites for replay and, finally, collect effective trajectory data from the replays. After filtering RedPajamas vast dataset, we retained over 18.8 million entries. By applying criteria such as recency and popularity, 23,430 tutorials were prepared for replay. With success rate of 44.4%, we generated 10,398 trajectories, covering 127 websites across 11 distinct categories. 4.3 COMPARISON WITH EXISTING WORK AND RESEARCH CHALLENGES AgentTrek generates comprehensive, large-scale trajectory data, excelling in several key areas as shown in Table 1 (Niu et al., 2024; L`u et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024). First, with nearly 5k verified trajectories and an average of 12.1 steps per trajectory, our dataset provides strong foundation for training and evaluating agents on longhorizon web tasks. Second, it is the most complete dataset to date, incorporating DOM/HTML structures, AXTree data, intermediate reasoning steps, full video recordings, and corresponding screenshots for each action. Moreover, despite full automation without human intervention, our dataset maintains diversity across 120 websites and 12 distinct task categories. By leveraging modern large language models (LLMs), we can extract both high-level task objectives and detailed step-by-step instructions, offering flexibility for future use. Finally, our pipeline significantly reduces the cost and scalability challenges of human-annotated data collection. With success rate factored in, the cost per trajectory is just $0.551, making our approach both efficient and scalable for large-scale data generation. Cost detail see C"
        },
        {
            "title": "5 RELATED WORK",
            "content": "LLM-based Agents. LLM-based agents are autonomous systems that leverage large language models (Brown et al., 2020) to interact with real-world websites and os environments. These agents can 10 understand natural language instructions and perform wide range of complex tasks across various domains, such as e-commerce, online assistance, and knowledge navigation (Nakano et al., 2021; Cheng et al., 2024). Recent efforts in this space include models like SeeAct (Zheng et al., 2024) and WebVoyager (He et al., 2024), which aim to generalize agent behavior to real-world websites. While LLM-based agents have shown promise, challenges remain in the need for agent specified data. Our work extends this line of research by introducing cost-effective pipeline to generate comprehensive agent trajectory data, advancing the state-of-the-art in data synthesis for agent-based applications. Agent Data. As agents gain increasing popularity, the demand for efficient and scalable data is becoming both larger and more urgent. However, most existing data primarily serve as supplements to various benchmarks (Zhou et al., 2023; Li et al., 2023; Deng et al., 2024), with few datasets specifically designed for agent trajectory analysis. Furthermore, these datasets are often limited by the need for human annotation, which hampers scalability. In our work, our pipeline managed to automatically generate comprehensive agent trajectory data in cost-efficient manner, paving the way for new direction in data synthesis within the field of agents. Automatic Evaluation for Digital Agents. Recently, there has been growing interest in automating the evaluation of digital agents using Vision-Language Models (VLMs) and Large Language Models (LLMs). These methods leverage models to assess agent performance in real-world tasks. Research in this area spans several dimensions: some works focus on trajectory-level success (Pan et al., 2024), while others evaluate stepwise success based on adherence to instructions (Wornow et al., 2024). Additionally, evaluations are conducted across various task environments, such as web-based platforms and mobile operating systems like Android and iOS (Pan et al., 2024). In our work, we prompt VLM, GPT-4o, as an autonomous evaluator, using agents interacton process as inputs to assess whether the agent has successfully completed tasks at the trajectory level."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce AgentTrek, an efficient pipeline designed to automatically generate comprehensive and cost-effective agent trajectory data. Additionally, we present large and diverse dataset generated using this approach, which we validate by training models and evaluating their performance with promising result.Our research establishes novel and promising direction for the future development of LLM agent, particularly in the automatic and low-cost synthesis of trajectory data. AgentTrek serves as strong standard for agent data generation, setting the stage for future advancements in this field."
        },
        {
            "title": "REFERENCES",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. ArXiv preprint, abs/2401.10935, 2024. URL https://arxiv.org/abs/2401.10935. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Luˇcic, and Neil Houlsby. Patch pack: Navit, vision transformer for any aspect ratio and resolution, 2023. URL https://arxiv.org/ abs/2307.06304. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Hadj Laradji, Manuel Del Verme, Tom Marty, Leo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks? ArXiv preprint, abs/2403.07718, 2024. URL https://arxiv.org/abs/2403. 07718. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. ArXiv preprint, abs/2401.13919, 2024. URL https://arxiv.org/abs/2401.13919. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Association for Computational Linguistics, 2017. URL https://aclanthology.org/E17-2068. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: large language model-based web navigating agent, 2024. URL https://arxiv.org/abs/2404.03648. Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. zero-shot language agent for computer control with structured reflection. ArXiv preprint, abs/2310.08740, 2023. URL https://arxiv.org/abs/2310.08740. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents, 2024. URL https: //arxiv.org/abs/2406.03679. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. URL https://arxiv.org/abs/2408.00203. Xing Han L`u, Zdenˇek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multiturn dialogue, 2024. Microsoft. Playwright for python documentation. https://playwright.dev/python/, 2023. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browserassisted question-answering with human feedback. ArXiv preprint, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332. Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: vision language model-driven computer control agent. 2024. OpenAI. Gpt-4v(ision) system card, 2024. URL https://openai.com/research/ gpt-4v-system-card. Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale, 2024. URL https://arxiv.org/abs/2409. 15637. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv preprint, abs/2203.02155, 2022. URL https://arxiv.org/abs/2203.02155. 12 Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. ArXiv preprint, abs/2404.06474, 2024. URL https: //arxiv.org/abs/2404.06474. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. ArXiv preprint, abs/2304.03277, 2023. URL https://arxiv.org/abs/2304. 03277. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control, 2023. URL https://arxiv.org/ abs/2307.10088. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents. 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Ke-Yang Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv preprint, abs/2409.12191, 2024. URL https: //arxiv.org/abs/2409.12191. Michael Wornow, Avanika Narayan, Ben Viggiano, Ishan S. Khare, Tathagat Verma, Tibor Thompson, Miguel Angel Fuentes Hernandez, Sudharsan Sundar, Chloe Trujillo, Krrish Chawla, Rongfei Lu, Justin Shen, Divya Nagaraj, Joshua Martinez, Vardhan Agrawal, Althea Hudson, Nigam H. Shah, and Christopher Re. Do multimodal foundation models understand ena benchmark for business process management tasks. ArXiv preprint, terprise workflows? abs/2406.13264, 2024. URL https://arxiv.org/abs/2406.13264. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. ArXiv preprint, abs/2404.07972, 2024. URL https://arxiv.org/abs/2404.07972. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun yue Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. ArXiv preprint, abs/2310.11441, 2023. URL https://arxiv.org/abs/2310.11441. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35, 2022. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. ZeroGen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022. URL https://aclanthology.org/2022.emnlp-main.801. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users, 2023. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. ArXiv preprint, abs/2401.01614, 2024. URL https://arxiv.org/abs/ 2401.01614. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. ArXiv preprint, abs/2307.13854, 2023. URL https://arxiv.org/ abs/2307.13854."
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Calculation of Other Trajectory Datasets Analysis of the Effectiveness of Tutorials Cost Details Evaluator Alignment Action Mapping Experimental Results on Textual Data Details in Collecting Tutorials G.1 Prefilter Function . . G.2 LLM Labeler Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Tagging & Paraphrasing Prompt and Format . . . . . . . . . . . . . . . . . . . . . Examples of Failed Guided Replay Trajectories Scaling up AgentTrek Evaluation Benchmarks J.1 GUI Grounding Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J.2 Offline GUI Agent Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Description of guided replay 15 15 15 16 16 16 18 18 20 21 21 21"
        },
        {
            "title": "A CALCULATION OF OTHER TRAJECTORY DATASETS",
            "content": "RUSS: Cited based on the data provided in the table from WebLINX (L`u et al., 2024). ScreenAgent: Statistics obtained from the dataset available at https://github.com/ niuzaisheng/ScreenAgent/tree/main/data/ScreenAgent/train. WebLINX: Calculated based on the train set information from Table 8 in (L`u et al., 2024) and data on HuggingFace (excluding the say actions), resulting in total of 18,249 nonsay actions with 969 demos. Mind2Web: Statistics derived from https://huggingface.co/datasets/ osunlp/Mind2Web, specifically from the training subset. Webshop (agent-eto): Data statistics sourced from https://huggingface.co/ datasets/agent-eto/eto-sft-trajectory. WonderBread: Calculations based on data presented in (Wornow et al., 2024)."
        },
        {
            "title": "B ANALYSIS OF THE EFFECTIVENESS OF TUTORIALS",
            "content": "Key factors contributing to this improvement include: 1. Direct Access to Target URL: Tutorials provide the target URL, allowing direct access to the initial task state, reducing errors in locating the correct webpage. 2. Assisted Planning with Human Expertise: Tutorials aid in planning by providing steps informed by human experience, which tend to be reliable, thereby reducing the likelihood of errors during task execution and bridging the gap in the agents knowledge for unknown tasks. 3. Navigating Multi-Level Menus: Tutorials offer clear paths to hidden elements, preventing the agent from failing due to incorrect navigation through complex menus."
        },
        {
            "title": "C COST DETAILS",
            "content": "In this part we provide the details of our cost in generating trajectory data with via our pipeline: Phase Tag and Paraphrase Replay Evaluator Cost per 1,000 Entries (USD) 0.886 215.359 3.104 Model Used gpt-4o-mini gpt-4o-2024-08-06 gpt-4o-2024-08-06 Table 8: Cost breakdown for each phase in the process Another two important factors are the ratio of web-related tutorials (0.275) and the Replay Success Rate (39.9%). Using these, we can calculate the cost per verified effective trajectory as follows: Cost per trajectory = Tag and Paraphrase price Web ratio + Replay price + Evaluate price Replay Success Rate The cost per 1,000 verified effective trajectories is 550.75 $."
        },
        {
            "title": "D EVALUATOR ALIGNMENT",
            "content": "In this part, we provide the details of metrics between the human and automatic evaluator. Trajectory Evaluator Accuracy Web Tutorials VLM Evaluator Webarena GPT-4V Captioner + GPT-4 Captioner + Mixtral 84.0% 80.6% 82.1% 74.4% 15 Figure 8: Confusion Matrix of our VLM evaluators performance on the human-annotated validation set, compared with evaluators across different scenarios."
        },
        {
            "title": "E ACTION MAPPING",
            "content": "Table 9: Mapping between Playwright and PyAutoGUI Action Spaces. Category Playwright Action PyAutoGUI Action Basic Actions Advanced Actions page.click() page.type() page.press() page.hover() page.scroll() page.fill() page.dblclick() page.dragAndDrop() page.clear() pyautogui.click() pyautogui.write() pyautogui.press() pyautogui.moveTo() pyautogui.scroll() pyautogui.write() (clearing) pyautogui.doubleClick() pyautogui.dragTo() pyautogui.click() pyautogui.hotkey(ctrl, A) pyautogui.press(delete) Plugin playwright.select option() browser.select()"
        },
        {
            "title": "F EXPERIMENTAL RESULTS ON TEXTUAL DATA",
            "content": "To provide further supports for the effective of our AgentTrek data, we conducted an experiment to evaluate the performance of pure textual agent using the textual version data of our AgentTrek trajectories. This allow us to study the contribution of textual modalities of AgentTrek. We fine-tuned the Qwen2.5-7B-Instruct model using AgentTrek trajectories that included accessibility tree as observations and playwright actions as the agents action space. We then evaluated the model on WebArena, an OOD web agent benchmark featuring self-hosted websites. These websites are entirely out-of-domain (OOD) from the AgentTrek dataset, ensuring that the evaluation reflects the models generalization capability. We fine-tuned Qwen2.5-7B-Instruct on AgentTreks textual data and achieved the following results on WebArena and Miniwob++ as shown in Table 5. We observe that our fine-tuned model achieves the highest performance among open-source web agents and approaches the performance of GPT-4o, demonstrating the effectiveness of AgentTrek data in improving real-world web agent capabilities and generalization across modalities."
        },
        {
            "title": "G DETAILS IN COLLECTING TUTORIALS",
            "content": "G.1 PREFILTER FUNCTION Keyword Density: The web content must contain minimum of 20 common keywords, ensuring sufficient topic coverage. Keyword Diversity: The text must incorporate at least 4 distinct common keywords. 16 System Prompt You are an expert in evaluating the performance of web navigation agent. to complete task. trajectory, your goal is to decide whether the agents execution is successful or not. The agent is designed to help human user navigate website Given the users task goal, the agents *Evaluation Criteria* Whether the agents trajectory is effective and corresponding to the goal the task should be if the agent is stuck in the loop at the early stage of the task, these actions may be some failed task can be considered successful if most trajectory when the task is to change the google account password, it cant Review the agents actions and reasoning processes step by step. if the agent is stuck in the very first login stage, which means the agent sometimes cant stop after finishing task and Determine if the agent has achieved the task goal based on the *Instructions* 1. 2. it fails to log into target website at the beginning, thats failure. 3. trajectory. is effective. 4. continue doing repeated actions. attempt after series of correct actions. regarded as successful if the correct actions are effective and almost reach the goal. 5. which means they dont even get close to the goal before they get stuck in the loop, thats failure. get stuck before third step. 6. be regarded as successful when agent finish at trying to click \"manage your account\". 7. regard as successful agent. 8. agent does most things right and just forget to save the change at last. 9. of them, thats still success. and birthday, but agent only update name, thats fine. 10. successful when it finish writing the review and reach the step to post it, dont have to click the post button. 11. considered successful if the agent reach the step to click print button. 12. nothing because of it, it should also be regarded as successful. if the task is finished at the initial state and the agent do if the task is to post review, the agent can be considered if the original task has 2 subtasks, the agent only complete one e.g. if there are over 8 correct action in the trajectory, it can be final saving action is not must. the task is to update name Since we dont have printer, some printing related task can be for example, the agent begin to the task is successful if the in the trajectory, an action always follows corresponding *IMPORTANT* 1. reasoning, which shows the observation and thought of the agent. 2. Thoughts: Status: <your thoughts and reasoning process> your response should be contain: \"success\" or \"failure\" User Prompt The goal of the task: {task des} trajectory: {trajectory} Figure 9: Prompts to query the VLM Autonomous Evaluator. Essential Keyword Frequency: At least one mandatory keywords must appear multiple times (minimum twice) within the content, demonstrating topic relevance. 17 G.2 LLM LABELER PROMPT To achieve more precise and context-aware labeling, we designed the following prompt to guide the LLM in further assessing whether the given URL and context meet our requirements, as illustrated in Fig 10. System Prompt You are an assistant that classifies content based on specific criteria. serves as tutorial specifically related to graphical user interfaces (GUI), such as for web applications, desktop applications, or operating systems. Your task is to evaluate whether given piece of content It includes task description outlining what needs to be Classification Criteria The content qualifies as GUI-related tutorial if it meets the following conditions: 1. achieved. 2. GUI, such as: - Step 1: - Step 2: Open the application Navigate to the settings menu It provides clear step-by-step instructions for interacting with Given the URL and context, determine if the content is GUI-related tutorial or not. Output 1 if it is GUI-related tutorial and 0 if it is not. Provide only the number as the output. User Prompt - URL: {url} - Context: {context} Figure 10: User Prompt for Classifying GUI Tutorials G.3 TAGGING & PARAPHRASING PROMPT AND FORMAT Here we present the prompt designed to utilize LLM to help do the tagging & paraphrasing of the identified GUI-tutorial related context. 18 User Prompt The following is tutorial from the website. tutorials. first tutorial according to the specified schema: Please extract the first tutorial only and format the It may contain several Text: {context} Schema: { \"platform\": \"Platform category (choose from: macOS, Windows (Default if not specified in the tutorial), Linux, Android, iOS)\", \"target type\": \"Type of platform (choose from: Web browser, PC app, Mobile app, PC operating system, Mobile operating system, where the tutorials steps are performed). Tutorials that involve interacting with the browser software itself, such as opening Chrome settings, should be classified as PC app type.\", \"target object\": \"Specific name of the web browser or (non web browser) applications or operating system where the tutorials steps are performed (e.g., Chrome browser (Default for browser and web tutorial), Microsoft Excel (app name), Windows system settings)\", \"target web URL\": \"The exact URL of the web page where the tutorials actions take place, applicable only if the target object is web browser (e.g., None, https://mail.google.com, https://www.amazon.com, https://github.com). is always not the URL where the tutorials actions are about. example, tutorial from https://abidakon.com/how-to-make-google-slide-vertical/ about changing Google Slides, its target web URL should be https://docs.google.com/presentation.\", Be careful, the URL provided at the beginning For \"task description\": \"Task description text (Provide concise summary in one sentence, including essential details)\", \"prerequisites\": \"Prerequisite text describing necessary conditions before starting the task\", \"instructions\": [ \"Step 1: // Following instructions Instruction text describing the action to be taken\", ] \"instructions steps\": \"Total number of instructions steps\", \"expected result\": \"Text describing the expected result after following the instructions\" } Figure 11: User Prompt for Extracting and Formatting GUI Tutorials"
        },
        {
            "title": "H EXAMPLES OF FAILED GUIDED REPLAY TRAJECTORIES",
            "content": "Figure 12: Replay agent was unable to complete booking before the actual date due to the tutorial expiration. 20 Table 10: Multimodal Mind2Web Step SR across varying amounts of training data from AgnetTrek (AT). Data Amount Cross-Task Cross-Website Cross-Domain 20% 40% 60% 80% 100% 36.1 41.0 41.6 42.6 42. 35.5 35.8 37.2 38.0 37.5 39.5 42.5 42.8 44.3 45."
        },
        {
            "title": "I SCALING UP AGENTTREK",
            "content": "In this section, we further scale up the data amount of AgnetTrek (more than 10K trajectories) to explore the effectiveness of AgentTrek in large-scale size. We trained the model using varying proportions of the dataset (20% to 100%) and assessed its performance on Multimodal-Mind2Web across three splits. The results are presented in Table 10. We observe that the performance improves steadily as more data is used, with the best results achieved when using the full dataset. This underscores the value of scaling up AgentTrek in improving model effectiveness."
        },
        {
            "title": "J EVALUATION BENCHMARKS",
            "content": "In this section, we introduce more details of evaluation benchmarks used in our work. J.1 GUI GROUNDING EVALUATION ScreenSpot. ScreenSpot (Cheng et al., 2024) is benchmark developed specifically for GUI visual grounding tasks, featuring 1.2K single-step instructions along with the coordinates of target elements. The dataset includes diverse grounding instructions tailored for mobile, desktop, and web platforms and categorizes elements into text and icons/widgets. Two distinct assessment scenarios are utilized: (1) Original Instructions, where models directly execute grounding actions as per the provided instructions; and (2) Self-plan, where models are expected to formulate plans in natural language based on the original instructions before carrying out the grounding actions. J.2 OFFLINE GUI AGENT EVALUATION Multimodal-Mind2Web. We evaluated the offline planning capabilities of GUI agents on websites using the Multimodal-Mind2Web benchmark (Zheng et al., 2024), which is an extension of the original Mind2Web benchmark (Deng et al., 2024). Performance was measured using Element Accuracy (Ele.Acc), Operation F1 (Op.F1), and Step Success Rate (Step SR). The GPT-3.5 and GPT-4 results for the HTML and HTML+Image observation are derived from the SeeAct (Zheng et al., 2024) method. For the Choice method, it employs DeBERTa-base cross-encoder to rank the interactive elements on the current HTML page. The top 50 elements are selected as options, and the GPT-3.5/GPT-4 model then chooses one of these elements as the answer. For the SoM method (Yang et al., 2023; Zheng et al., 2024), it renders new webpage image by adding red bounding boxes and labels to every HTML node in the source code, allowing GPT-4 to understand the webpage screenshot and identify the target action object by referring to the labels. For the Image observation, as detailed in Section 2.3.1, we use Qwen2VL (Wang et al., 2024) within pure vision framework. Specifically, Qwen2VL processes only the webpage screenshots and specifies the target action object by generating its coordinates."
        },
        {
            "title": "K DETAILED DESCRIPTION OF GUIDED REPLAY",
            "content": "In this section, we detailedly describe an example of model execution in guided replay. 21 Observation Prior to Execution Before executing any actions in the task execution process, the model observes the current webpage within the BrowserGym environment. Each webpage provides rich data, including its HTML structure, accessibility tree (AXTree), and screenshots. The model uses the AXTree as the primary observation source, with each element in the AXTree uniquely identified by [bid]. This structured observation ensures accurate and consistent interaction: Axtree with Element bid [119] link Magento Admin Panel [120] image Magento Admin Panel [121] navigation [122] menubar , orientation=horizontal [124] link ue604 DASHBOARD [127] link ue60b SALES ... [614] banner [617] heading Dashboard [620] link ue600 admin ... Figure 13: Observation Prior to Execution in Guided Replay the information in tutorial element. The model realize that the target element is menubar, which associates with bid(122) the tutorial may provide detailed textual description of the target Axtree with Element bid Step 1: click the menubar to see the sales Figure 14: Observation Prior to Execution in Guided Replay the action executed When the model performs an action, it does not need to provide fine-grained target element information such as coordinates, only the action type and the target objects bid as follows: Axtree with Element bid click(119) Figure 15: Observation Prior to Execution in Guided Replay We can retrieve the target elements in the webpage through the bid and perform corresponding operations through playwright action."
        }
    ],
    "affiliations": [
        "Salesforce Research",
        "University of Hong Kong"
    ]
}