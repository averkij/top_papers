{
    "paper_title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
    "authors": [
        "Pavan Kumar Anasosalu Vasu",
        "Fartash Faghri",
        "Chun-Liang Li",
        "Cem Koc",
        "Nate True",
        "Albert Antony",
        "Gokul Santhanam",
        "James Gabriel",
        "Peter Grasch",
        "Oncel Tuzel",
        "Hadi Pouransari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\\times$1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85$\\times$ faster TTFT and a vision encoder that is 3.4$\\times$ smaller."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 3 0 3 3 1 . 2 1 4 2 : r FastVLM: Efficient Vision Encoding for Vision Language Models Pavan Kumar Anasosalu Vasu Nate True"
        },
        {
            "title": "Albert Antony",
            "content": "Fartash Faghri Gokul Santhanam Chun-Liang Li James Gabriel Cem Koc Peter Grasch Oncel Tuzel Hadi Pouransari Apple {panasosaluvasu,fartash,chunliang li,cem koc,otuzel,mpouransari}@apple.com Core authors; Project lead"
        },
        {
            "title": "Scaling the input",
            "content": "image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked selfattention layers. At different operational resolutions, the vision encoder of VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLMa model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152 1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85 faster TTFT and vision encoder that is 3.4 In the LLaVA-1.5 setup, FastVLM achieves 3.2 smaller. 1. Introduction (a) Qwen2-0.5B (b) Vicuna-7B Figure 1. FastVLM is more than 3 faster than prior work. Comparison of commonly used vision encoders for VLMs with (a) Qwen2 [79] 0.5B LLM and (b) Vicuna 7B [90] LLM. All the vision encoders are CLIP [63] pretrained. For fair comparison all models are trained using LLaVA-1.5 [49] setup with the vision encoders made trainable for resolution adaptation, see Sec. 4 for more details. Marker size for each model corresponds to number of parameters of the vision encoder. The x-axis is the sum of vision encoder latency and LLM prefilling time. All models are benchmarked on an M1 Macbook Pro. Vision Language Models (VLMs) enable visual understanding alongside textual inputs. VLMs are often built by passing visual tokens from pretrained vision backbone to pretrained Large Language Model (LLM) through projection layer (also known as the connector module). Pre1 vious works [49, 50] have explored various training and fine-tuning strategies for these three components: the vision backbone, the adapter, and the LLM, which is typically decoder-only model. Several studies [25, 56, 60] highlight image resolution as key factor in VLM performance, especially for textand chart-rich data. However, increasing image resolution presents multiple challenges. First, pretrained vision encoders may not support high-resolution images, as this would make pretraining inefficient. To address this, one approach is to continuously pretrain the vision backbone to adapt it for high resolutions [6]. Alternatively, tiling strategies, such as Sphinx [48], S2 [66], and AnyRes [49], divide images into subregions, with each subregion processed independently by the backbone. This approach is particularly suitable for ViT-based backbones, which cannot accept varying input resolutions. further challenge is the runtime computational cost associated with high-resolution inference. Both single highresolution inference and multiple inferences at lower resolution (the tiling strategy) result in significant latency when generating visual tokens. Additionally, high-resolution images naturally produce more tokens, which increases the LLM prefilling time (the LLM forward pass time on all tokens in the context, including visual tokens), thereby further increasing the time-to-first-token (TTFT), which is the sum of the vision encoder latency and the LLM prefilling time. In this work, we study VLM design and training from runtime efficiency perspective motivated by their on-device deployment. We explore the optimization landscape as image resolution increases, aiming to improve accuracylatency trade-off, where latency includes both the vision encoder inference time and the LLM prefilling time. Using extensive experiments with different LLM sizes and resolutions, we establish the Pareto optimal curve for specific vision backbone, showing the best accuracy achievable within given runtime budget (TTFT) based on different choices of resolution and LLM size. We start by exploring the use of hybrid convolutionaltransformer architecture FastViT [76], pretrained with MobileCLIP [77], as vision backbone for the VLM setup (Section 3.1). We demonstrate the potential of this hybrid backbone, which generates visual tokens over 4 faster than ViT model while achieving higher overall VLM accuracy with multi-scale features (Section 3.1.1). However, further architectural optimization is possible when the primary goal is high-resolution VLM (rather than embedding generation as in MobileCLIP-pretrained FastViT). We introduce new hybrid vision encoder, FastViTHD, specifically designed for efficient VLM performance on highresolution images (Section 3.2), and use it as the vision backbone to obtain FastVLM through visual instruction tuning. FastVLM demonstrates significantly improved accuracy-latency trade-off over VLMs based on ViTs, convolutional encoders, and our previously discussed hybrid FastViT for different input image resolutions and LLM sizes (Figure 1a, Figs. 1b and 4). In particular, FastVLM outperforms several prior works while being smaller, faster, and trained with less data  (Table 6)  . Compared to LLaVaOneVision [40] operating at the highest possible resolution 1152), FastVLM obtains comparable performance (1152 with the same 0.5B LLM, but with 85 faster TTFT and 3.4 smaller vision encoder. The following is summary of our contributions: We demonstrate the efficacy of hybrid vision backbones in VLMs compared to ViTs. We also introduce additional architectural interventions, such as multi-scale vision features, to further improve VLM performance while maintaining efficiency. We design and pretrain new hybrid architecture, FastViTHD, optimized for efficient VLM performance with high resolution input for FastVLM. In controlled experimental setup, where only the vision backbone is changed, we show that FastViTHD outperforms its ViTbased and convolution-based counterparts when used in smaller VLMs: achieving 3.2 size than SigLIP-SO400M [87], and 2.3 faster TTFT and 1.7 smaller size than ConvNeXT [25]. We further demonstrate that FastVLM scales effectively as more visual instruction tuning data becomes available. faster TTFT and 3.6 We systematically study the VLM accuracy-latency tradeoff by considering both the vision backbone latency and the LLM prefilling time on actual hardware benchmarks. Our results demonstrate an improved resolution-latencyaccuracy trade-off achieved by FastVLM, measured ondevice rather than estimates. 2. Related Works Large Multimodal Models. With the emergence of large language models [62, 71, 73, 79, 90] and large pretrained vision models, such as CLIP [63], trained on web-scale image-text datasets, several multimodal architectures have been proposed to encode images aligned with large language model (LLM) to enable the interpretation of visual signals. Earlier works like Frozen [74] and Florence [1, 2] used cross-attention mechanism where the image embeddings are fused with text embeddings in intermediate layers of the LLM. More recently, auto-regressive architectures have gained popularity where the image embedding is fed alongside text as input to an LLM. Some prominent works that use this architecture are LLaVA [4951], mPLUG-Owl [8183], InstructBLIP [19], BLIP-3 [78], SPHINX [48], MiniGPT-4 [91], VILA [45], MM1 [60], Qwen-VL [4], InternVL [14, 15] and Cambrian-1 [72]. Recently, Fuyu [5] and EVE [21] introduced simplified architecture that passes raw images directly to the LLM decoder. 2 Chameleon [70] introduced early fusion mixed-modal models where images are tokenized using pretrained codebook. While skipping the image encoder is an intriguing approach, the performance of this new class of models lags behind architectures that use pretrained image encoder. Efficient Image Encoding. CLIP [63] pretrained vision transformers [22] are widely used for encoding images to vision-language model. Some of the common choices include SigLIP [87], EVA-CLIP [69], InternViT [14] and DFN-CLIP [23]. To further boost performance, recent works like [32, 67, 72] use an ensemble of vision encoders pretrained using different objectives. These works are orthogonal to our work as they can benefit from using an efficient vision encoder among the ensemble of vision encoders. Since ViT architecture has been widely adopted in VLMs and the number of visual tokens remains one of the main causes of inefficiency, works like LLaVA-PruMerge [64] and Matryoshka-based token sampling methods [7, 28] have emerged to improve efficiency of image encoding by pruning visual tokens dynamically. Other works like [9, 1719] reduce the number of visual tokens by incorporating perceiver-style resamplers or simply pooling using depth-wise convolutions or average pooling. Rather than using an isotropic architecture like ViT and then designing custom resamplers and projectors, hierarchical architectures can be simpler design choice. Hierarchical backbones like ConvNeXT [53] and FastViT [76] produce fewer tokens as they downsample the input tensor at every stage of compute. Recently, ConvLLaVA [25] was introduced that uses pure-convolutional vision encoder to encode images for VLM. In our work, we introduce an improved convolution-transformer hybrid architecture for VLMs and discuss the pareto-optimal operating points when this architecture is scaled to higher input resolutions. 3. Architecture In this section, we first explore the adoption of the FastViT hybrid vision encoder for vision-language modeling. We then introduce architectural interventions to improve performance on VLM tasks. We present FastViTHD, new hybrid vision encoder designed for efficient high-resolution VLM. We provide comprehensive ablations to demonstrate the optimality of FastViTHD over FastViT and prior works for different LLMs and input resolutions. Figure 2 illustrates the overall architecture of FastVLM and FastViTHD. The training setup for all results in this section follows the same configuration as LLaVA-1.5 [49] with Vicuna-7B [90] as the LLM decoder, unless mentioned otherwise. See Sec. 4 for more details. 3 Image Input #Visual Latency Encoder Res. Tokens Enc.(ms) ViT-L/14 336 ViT-L/14 576 576 127.4 127.4 FastViT FastViT 256 768 64 576 3.0 34. GQA TextVQA POPE DocVQA 62.0 63.5 60.2 62.7 58.2 59.2 51.6 62.3 85.9 86. 82.9 86.5 28.1 28.7 15.8 34.4 Seed BenchI Avg-5 66.1 68. 61.5 67.1 60.1 61.2 54.4 62.6 Table 1. FastViT has higher accuracy than ViT-L/14 at near 4 lower latency. To scale resolution up to 768, FastViT is made trainable during Stage-2 training of LLaVA-1.5 setup. To have fair comparison, we also report the performance of ViT-L/14 finetuned during Stage-2 training of LLaVA-1.5. All latencies are reported in milliseconds. See Sec. 4 for details. 3.1. FastViT as VLM Image Encoder VLMs such as LLaVA have three main components: an image encoder, vision-language projector, and large language model (LLM). Both the performance and runtime efficiency of VLM highly depend on its vision backbone. Encoding images at high resolution is essential for achieving strong performance across various VLM benchmarks, especially for text-rich tasks. Therefore, vision encoder with scalable resolution is particularly beneficial for VLMs. We identify hybrid vision encoders (convolutional layers followed by transformer blocks) as an ideal candidate for VLMs, as their convolutional component enables native resolution scaling, and their transformer blocks further refine high-quality visual tokens for consumption by the LLM. We use CLIP-pretrained hybrid vision encoder in our experiments. Specifically, we use the MCi2 image encoder from MobileCLIP [77], which has 35.7M parameters, is pretrained on DataCompDR [24, 77], and is based on the FastViT [76] architecture. For simplicity, we refer to this encoder as FastViT throughout the rest of the paper. As shown in Tab. 1, using FastViT at its CLIP-pretrained resolution (256 256) alone does not yield strong VLM. The main advantage of hybrid encoder like FastViT lies in its favorable image resolution scaling characteristics, meaning it generates 5.2 fewer tokens than the ViT architecture with patch size of 14. The considerable token reduction gives significant advantage to VLM, as it greatly reduces the prefilling time and time-to-first-token of the transformer decoders with quadratic-complexity. When the input resolution of FastViT is scaled to 768 768, it produces the same number of visual tokens as ViT-L/14 with an input resolution of 336 336 but achieves better performance on VLM benchmarks. This performance gap is even more pronounced on text-rich benchmarks like TextVQA and DocVQA, despite both architectures producing the same number of visual tokens. Moreover, even if it results in same number of tokens with higher resolution, it takes much less image encoding time, thanks to the efficient convolution layers. Figure 2. Overview of the FastVLM architecture. FastVLM consists of our novel vision encoder, FastViTHD, trained using the same setup as LLaVa. The FastViTHD architecture is designed and trained for low latency at high resolution, utilizing novel multi-scale pooling, additional self-attention layers, and downsampling to generate 4 fewer tokens than FastViT, and 16 fewer tokens than ViT-L/14 at resolution 336."
        },
        {
            "title": "Seed\nBenchI",
            "content": "Avg-5 FastViT 62.7 FastViT AvgPool 63.0 FastViT DWConv 63.0 - 62.3 62.2 62.5 86.5 86.2 86.8 34.4 35.1 34. 67.1 66.9 67.4 62.6 62.7 62.9 Table 2. Pushing FastViT VLM performance using multi-scale features and pooling strategies. These modifications slightly improve FastViT. Training setup is LLaVA-1.5 with Vicuna 7B. 3.1.1. Multi-Scale Features Typical convolutional and hybrid architectures split up the computations into 4 distinct stages with downsampling operation between them. While the VLM relies on features from the penultimate layer, features in earlier stages of the network extract information at different granularity. Aggregating information from multiple scales can complement high-level features from the penultimate layer. This design is commonly used in object detection models like [47]. The architecture for multiple scale feature extraction is shown in Fig. 2. We ablate between 2 designs to pool features from different stages, i.e. AvgPooling and 2D Depthwise convolutions. From Tab. 2, we find that using depthwise convolutions results in better performance. Along with multi-scale features, we also experimented with different connector designs for FastViT (more details provided in supplementary materials). Collectively, these model interventions benefit hierarchical backbones like ConvNeXt and FastViT. 3.2. FastViTHD: High Resolution Encoder for VLM While FastViT with the introduced model interventions performs well as an image encoder that is 8.7 smaller than ViT-L/14, previous studies [14, 42] have demonstrated that increasing the scale of the image encoder improves its generalization capabilities. Common practice in hybrid architectures is to scale the number of self-attention layers Figure 3. Novel scaling strategy of FastViTHD lowers latency at various image resolutions. FastViT-Naive, naive scaling of the FastViT architecture, and our proposed FastViTHD have the same number of parameters. ConvNeXt-L is provided for reference. All models are benchmarked on M1 Macbook Pro and trained with LLaVA-1.5 setup and Vicuna 7B. Note that the y-axis is in log scale. along with the width of each layer in 4-stage architecture like [16, 85], but this approach has its drawbacks. From Fig. 3, simply scaling-up the number of self-attention layers in stage 3 and 4 by following [16, 85], in the existing FastViT architecture is not optimal. In fact, it is even slower than ConvNeXT-L. More details on the naively scaled version of FastViT are provided in supplementary materials. To reduce the impact of the added self-attention layers, we introduce an extra stage preceded by downsampling layer, see Fig. 2. In this approach, the self-attention layers only handle tensors that have been downsampled by factor of 32 on each side, compared to factor of 16 in typical and more recent hybrid models like ViTamin [11]. The selfattention layers with the widest MLP layers process input tensors downsampled by factor of 64 on each side. Our design reduces image encoding latency and generates 4 Image Encoder ViT-L/14 [24] ViTamin-L [11] ConvNeXt-L FastViTHD Encoder Size(M) 304 333 200 125 Input Res. 224 224 320 224 Latency Enc.(ms) 47.2 38.1 34.4 6.8 Zero-Shot Avg Perf. Retrieval ImageNet Avg Perf. on 38 tasks 79.2 80.8 76.8 78.3 60.8 60.3 64.8 67. 66.3 66.7 63.9 66.3 Table 3. FastViTHD achieves competitive results on CLIP benchmarks at significantly lower latency. We follow the same setup described in [11] to report average retrieval performance and setup described in [24] to report average performance on 38 tasks. All models are benchmarked on M1 Macbook Pro. Image Encoder Input Latency #Visual Tokens Res. Enc.(ms) FastViTHD 256 10. 16 320 C.N-L C.N-XXL 256 FastViTHD 512 FastViTHD 768 512 C.N-L C.N-XXL 512 FastViTHD 1024 34.4 89.9 33.5 122. 71.9 397.1 235.6 100 64 64 144 256 256 256 GQA TextVQA POPE DocVQA 60. 61.9 62.7 63.0 62.4 61.8 62.3 63.1 53.1 55.5 56.3 59.3 62. 61.0 65.1 64.4 82.3 85.3 85.3 86.4 87.7 86.3 87.7 88.1 17. 21.3 21.6 25.7 32.9 30.8 36.2 35.6 Seed BenchI Avg-5 63. 64.6 65.6 67.1 68.2 66.8 68.4 68.5 55.5 57.7 58.3 60.4 62. 61.3 63.9 63.9 Table 4. FastViTHD achieves higher accuracy than ConvNeXT while having lower latency at higher resolution. The models are grouped based on the total number of visual tokens produced for the LLM to process. C.N stands for ConvNeXT. Training setup is LLaVA-1.5 with Vicuna 7B. fewer tokens for the compute-intensive LLM decoder, 4 thereby decreasing the time-to-first-token (TTFT). The architecture schematic is shown in Fig. 2, and we call this model FastViTHD. The model architecture consists of 5 stages, as shown in Fig. 2, with the first three stages utilizing RepMixer [76] blocks and the last two stages employing multi-headed selfattention [22] blocks. The model depth at each stage is [2, 12, 24, 4, 2], and the embedding dimensions for each stage are [96, 192, 384, 768, 1536]. The MLP expansion ratio for the ConvFFN layers is set to 4.0. The model has 125.1M parameters, which is 3.5 larger than the largest FastViT variant from MobileCLIP, but is still smaller than popular ViT alternatives. smaller and 6.9 We follow the CLIP pretraining setup of [77] using the DataCompDR-1B dataset to pretrain FastViTHD before employing it for FastVLM training. Table 3 shows that FastViTHD, despite being 2.4 faster than ViT-L/14, achieves comparable average performance across 38 multi-modal zero-shot tasks [24]. In comparison to ViTamin [11], hybrid transformer architecture built for VLMs, FastViTHD delivers superior average retrieval performance while being 2.7 faster. In Tab. 4, we compare FastViTHD with other CLIP-pretrained hierarchical backbones, i.e. ConvNeXT-L and ConvNeXTXXL, for VLM tasks after LLaVa-1.5 training. FastViTHD performs as well as ConvNeXT-XXL while being 6.8 smaller and 3.3 smaller and 5.6 faster. Figure 4. FastViTHD improves the Pareto-Optimal curve for accuracy versus time to first token compared with FastViT. Comparison of FastViT and FastViTHD backbones paired with Qwen2 [79] family (chat variant) LLMs of varying sizes and different image resolutions (annotated for each point). The Paretooptimal curve is highlighted for the two vision backbones. Training setup is LLaVA-1.5. Note that the x-axis is in log scale. Figure 5. Vision latency dominates at high resolution. Breakdown of FastVLMs time to first token for varying image resolutions. Vision encoder is FastViTHD and LLM is Qwen2-1.5B. 3.2.1. Vision Encoder - Language Decoder Interplay The accuracy-latency trade-off in VLM is influenced by several factors. On one hand, the overall performance of the VLM depends on (1) the input image resolution, (2) the quantity and quality of visual tokens, and (3) the capability of the LLM. On the other hand, the total latency (time to first token generation) of VLM is determined by (1) the latency of the vision encoder and (2) the prefilling time of the LLM. The latter is affected by both the number of tokens produced by the vision encoder and the size of the LLM. Due to the complex optimization landscape of VLMs, claims regarding the optimality of vision encoder must be verified across various pairs of (Resolution, LLM). Here, 5 the input image and set the input resolution of the image encoder to the tile size. The tiled inference (AnyRes) was introduced in prior works [48, 50] to enable ViT models to process high resolution images. Since FastViTHD is designed to run inference efficiently on high input resolutions, we analyze the optimal operating point for various resolutions using the two strategies. From Fig. 6, we see that simply setting the input resolution of the model to the desired resolution results in VLMs with the best accuracy-latency tradeoff. Only at extremely high image resolutions like 1536 1536 do we see the benefits of dynamic resolution, as model inference at this resolution is mostly affected by memory bandwidth available on-device. If dynamic resolution is desired, using setting with fewer tiles exhibits better accuracy-latency tradeoff. With advancements in hardware and improvements in memory size and bandwidth, we expect that FastVLM can be efficiently scaled to even higher resolutions without the need for tiling strategies. 3.2.3. Comparison with Token Pruning & Downsampling We further compare the performance of FastViTHD operating at different resolutions to popular token pruning methods in literature. From Tab. 5, we find that VLMs achieve better accuracy to latency trade-off using hierarchical backbone as opposed to using token pruning methods on isotropic architectures like ViT. By simply training the VLMs at lower input resolution, FastViTHD achieves visual token counts as low as 16, while improving over recent token pruning methods. Interestingly, even the most effective token pruning methods, such as those proposed by [7, 28, 29, 80], perform worse than FastViTHD trained at lower input resolution of 256 256. 4. Experiments In this section, we present our training setup and results. Training Setup. For all the ablations presented in Sec. 3, we follow the 2-stage setup described in LLaVA-1.5 [49] with Vicuna-7B [90] as the LLM decoder, unless mentioned otherwise. During the first stage, only the projector is trained using LLaVA-558K alignment dataset for one epoch, with batch size of 256 and learning rate of 103. At this stage, the input image resolution matches the backbone pretraining resolution (e.g., 256 for FastViT and 224 for FastViTHD). In the second stage, we use LLaVA-665K supervised finetuning dataset, training the models for one epoch and tuning all the modules, i.e., vision encoder, projector and the LLM. At this stage, the input image resolution is set to the target resolution. In Sec. 4, we present results with different LLM decoders, primarily with Qwen2-0.5B/1.5B/7B model family [79] (chat variant) and Vicuna-7B model [90]. We report results in two training setups, the first one is the 2-Stage setup introduced in LLaVA-1.5. We additionally scale the Figure 6. Dynamic input resolution (AnyRes) is only optimal at the highest resolution when using fewer tiles (22). The vision encoder is FastViTHD. The tile grid size is specified in parenthesis. Training setup is LLaVA-1.5 with Vicuna 7B. Note that the x-axis is in log scale. we empirically demonstrate the optimality of FastViTHD over FastViT. For each vision encoder, we consider three LLMs, Qwen2 [79]-0.5B/1.5B/7B, along with range of input image resolutions. For each (Resolution, LLM) pair, we conduct LLaVA-1.5 [49] pretraining and visual instruction tuning, and evaluate the resulting model over range of tasks. The results are presented in Fig. 4. First, we observe that for vision encoder, the Paretooptimal curve (highlighted in Fig. 4), which represents the maximum achievable performance for given runtime budget (TTFT), consists of varying sizes of LLMs. Specifically, pairing high resolution with small LLM is suboptimal as small LLM cannot effectively utilize that many tokens, and TTFT will be dominated by the latency of the vision encoder (see Fig. 5). Second, the Pareto-optimal curve for FastViTHD in Fig. 4 is significantly better than that of FastViT. For given runtime budget, considering all possible (Resolution, LLM) pairs, we achieve significantly better performance (an improvement of over 2.5 points on the Average-5 metric) with FastViTHD. Similarly, FastViTHD can reach target VLM performance up to 3 faster. It is important to note that in previous sections, we demonstrated that FastViTbased VLM already represents significant improvement over ViT-based VLMs, and yet FastViTHD provides substantial gains over FastViT. 3.2.2. Static vs. Dynamic Input Resolution There are two approaches to input resolution scaling, the first approach is to change the input resolution of the model to the desired resolution. The second approach is to tile"
        },
        {
            "title": "Model",
            "content": "Input #Visual Res. Tokens"
        },
        {
            "title": "GQA SQA",
            "content": "TextVQA"
        },
        {
            "title": "POPE",
            "content": "VQA Seed v2 Bench ViT-L/14 M3 [7] ViT-L/14 MQT [28] FastViTHD 336 336 256 ViT-L/14 PruMerge [64] 336 ViT-L/14 PruMerge+ [64] 336 ViT-L/14 M3 [7] 336 336 FastV [13] 336 SparseVLM [89] 336 VisionZip [80] 336 VisionZip [80] DynamicLLaVAI [29] 336 336 DynamicLLaVAIT [29] FastViTHD 512 ViT-L/14 M3 [7] ViT-L/14 MQT [28] FastV [13] SparseVLM [89] VisionZip [80] VisionZip [80] FastViTHD 336 336 336 336 336 336 768 ViT-L/14 MQT [28] FastViTHD 336 1024 9 16 16 40 40 36 64 64 64 64 115 115 64 144 144 192 192 192 192 144 256 256 - 83.4 - 58.0 57.6 67.5 80.8 71.1 - 60.6 69.2 53.1 82.3 74.7 - - - 85.5 68.5 56.0 76.3 72.0 68.3 57.1 84.0 76. - - 60.3 - 46.1 51.1 47.8 48.0 55.0 52.7 62.2 51.8 75.1 68.2 55.1 69.0 55.5 77.0 62.9 57.0 68.8 56.0 80.9 74.2 61.4 69.1 57.0 85.0 78.0 61.3 68.6 56.5 85.9 77.9 63.0 68.9 59.3 86.4 78.0 - - 87.0 - 61.3 61.4 67.6 83.9 76.4 - 52.7 67.3 52.5 64.8 67.1 57.6 69.1 56.1 83.6 75.6 59.3 68.9 57.3 85.3 76.8 60.1 68.2 57.8 84.9 77.4 62.4 67.6 62.9 87.7 78.9 55.4 - 58.8 - - 58.0 51.9 51.1 52.2 53.4 - - 61. 59.7 - 57.1 55.8 56.4 57.1 62.5 61.6 67.5 84.4 76.8 - 63.1 67.4 64.4 88.1 79.2 - - Table 5. FastViTHD more effectively reduces tokens compared with token pruning methods. The models are grouped based on total number of visual tokens. - indicates that performance was not reported in the respective paper. All models presented in this table are trained using LLaVA-1.5 setup with Vicuna 7B. - indicates further finetuning as reported in [80]. - indicates vision only sparsification and IT indicates vision-language sparsification, as reported in [29]. dataset used in Stage 2 from 665k samples to 1.1 million samples, which is subset of the instruction tuning dataset used in InternVL [14] (more details in Sec. D). For the second training setup, we follow the current trend in literature [38, 60] of training the VLMs in 3 stages, i.e. Stage 1 for training the connector, Stage 1.5 for resolution scaling and Stage 2 for visual instruction tuning. In Stage 1.5, we use the densely captioned CC3M [65] and CC12M [10] datasets [38]. For the final stage we use 1.1 million instruction tuning dataset. In this setup, the input image resolution is set to the backbone pretraining resolution for Stage 1 and adjusted to the target resolution for the following two stages. In both setups, the vision encoder and LLM are frozen only in stage 1, while all modules are finetuned in the remaining stages. All FastVLM models reported in the paper are trained on single node with 8 NVIDIA H100-80GB GPUs. Stage 1 training of VLM is quick, taking roughly 30 minutes to train with Qwen2-7B decoder. Stage 1.5 and Stage 2 training runs are dependent on input resolution. For an input resolution of 1024 1024, Stage 1.5 takes 77 hours and Stage 2 takes 8 hours. The reported wall clock times correspond to the following datasets used in these stages: 15 million samples in Stage 1.5 and 1.1 million samples in Stage 2. Evaluation. We evaluate the models on the main7 stream benchmarks of GQA [30], ScienceQA [55], TextVQA [68], POPE [43], LLaVA-in-the-wild [50], VQAv2 [26], MMVet [84], MMMU [86], DocVQA [59] and SeedBench [37]. For GQA, ScienceQA, TextVQA, POPE and LLaVA-in-the-wild benchmarks, we use the official evaluation from LLaVA [50]. For the remaining evaluations we use lmms-eval [88] library v0.2.2. We use the default settings for all the evaluations and lmms-eval defaults to 0613 version of GPT for evaluations that rely on GPT as judge. For ablations presented in Sec. 3, we report GQA, TextVQA, POPE, DocVQA and SeedBench. GQA and SeedBench are general knowledge benchmarks, DocVQA and TextVQA represent text-rich evaluations and POPE is hallucination benchmark. Together these benchmarks provide diversity and are quick to evaluate for ablations. Most importantly, they exhibit lower variance to different initializations and under probabilistic decoding setting. We report the variance for all the evals for different initialization in Sec. D.3. The standard deviation across the 5 selected metrics is less than 0.5. We call the average of these 5 benchmarks Avg-5, and use it as reliable signal for our analysis. Our empirical estimate of the standard deviation for Avg-5 is 0.1. Benchmarking. We benchmark all the models on MacBook Pro with the M1 Max chip and 32GB RAM. The image encoder is converted to Core ML package file using coremltools v7.2 and benchmarked on the neural engine using XCode 15.4 (15F31d). The LLM is benchmarked on the MacBook Pro GPU using MLX [27]. The model is first converted using mlx lm.convert tool, which converts the models on huggingface to the MLX format and casts the tensors to FP16. The prefilling latency is estimated using mlx lm.cache prompt tool [27]. Time-To-First-Token (TTFT) is estimated by adding the image encoding latency at specific resolution to the LLM prefilling latency for the associated visual tokens. 4.1. Comparison with state-of-the-art In Tab. 6, we compare FastVLM with recently published methods. The training setup can vary widely between works. For each, we report the LLM decoder and the sizes of the instruction tuning and pretraining datasets used to train the respective VLMs, to facilitate fair comparison. Hierarchical Backbones. When we compare FastVLM (R18) with ConvLLaVA [25] (R16), with the same LLM and similar training data size, our model obtains +8.4% better performance on TextVQA and +12.5% better performance on DocVQA while being 22% faster. The gap widens at higher resolution, where FastVLM (R26 and R27) achieves superior performance on wide range of benchmarks while being 2 faster than ConvLLaVA (R24), with the same LLM decoder. Row Ann. R1 R2 R3 Method Vision Encoder LLM Data (M) Input #Visual Vis. Enc. TTFT (PT+IT) Res. Tokens Size(M) (ms) 0.5B Model Comparison GQA SQA Text VQA POPE LLaVA MMVQA Doc BenchW Vet v2 VQA MMMU Seed BenchI nanoLLaVA LLaVAOV [40] FastVLM (Ours) FastVLM (Ours) ViT-SO400M Qw.1.5 Qw.2 ViT-SO400M Qw.2 FastViTHD Qw.2 FastViTHD - 729 384 4.5+3.2 1152 7290 256 15+1.1 1024 256 15+11.9 1024 430 430 125 125 535 14124 166 166 54.8 59.0 46.7 84. - 67.2 - - 61.6 61.4 57.4 87.4 62.9 80.8 61.6 87.0 R5 MobileVLMv2 [18] FastVLM (Ours) R6 FastVLM (Ours) R8 R9 R10 R11 R12 R13 R14 DeepSeekVL [54] MM1 [60] FastVLM (Ours) FastVLM (Ours) InstructBLIP [19] FastVLM (Ours) FastVLM (Ours) R15 MobileVLMv2 [18] R16 R17 R18 R19 R20 R21 ConvLLaVA [25] FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) Qwen-VL [4] R22 R23 Qwen-VL-Chat [4] ConvLLaVA [25] R24 FastVLM (Ours) R25 FastVLM (Ours) R26 FastVLM (Ours) R27 FastVLM (Ours) R28 LLaVA-1.5 [49] R29 R30 MobileVLMv2 [18] ShareGPT4V [12] R31 ViTamin [11] R32 ConvLLaVA [25] R33 VILA [45] R34 R35 LLaVA-FlexAttn [41] R36 R37 R38 R39 MM1 [60] LLaVA-NeXT FastVLM (Ours) FastVLM (Ours) ViT-L/14 FastViTHD FastViTHD ViT-SO400M ViT-H FastViTHD FastViTHD ViT-g/14 FastViTHD FastViTHD ViT-L/14 ConvNeXT-L FastViTHD FastViTHD FastViTHD FastViTHD FastViTHD ViT-G/14 ViT-G/14 ConvNeXT-L FastViTHD FastViTHD FastViTHD FastViTHD ViT-L/14 ViT-L/14 ViT-L/14 ViTamin-L ConvNeXT-L ViT-L/14 ViT-L/14 ViT-H ViT-L/14 FastViTHD FastViTHD ML. Qw.2 Qw. DS. - Qw.2 Qw.2 Vic. Vic. Vic. Vic. Vic. Vic. Vic. Vic. Qw.2 Qw.2 Qw. Qw. Vic. Vic. Vic. Vic. Qw.2 Vic. Vic. Vic. Vic. Vic. L-2 Vic. - L-3 Qw.2 Qw.2 1-2B Model Comparison 336 1.2+3.6 15+1.1 768 15+11.9 768 - 384 3000+1.5 1344 15+1.1 1024 15+11.9 1024 144 144 144 576 720 256 256 304 125 430 632 125 125 458 152 152 - - 233 233 7B Model Comparison 129+1.2 224 256 0.5+0.6 256 15+1.1 336 1.2+3.6 768 4.9+0.6 768 0.5+0.6 768 0.5+1.1 768 15+1.1 15+1.1 768 15+11.9 1400+50 448 1400+50 448 4.9+0.6 1024 0.5+0.6 1024 0.5+1.1 1024 15+1.1 1024 15+1.1 1024 336 0.5+0.6 336 1.2+3.6 336 1.2+0.7 0.5+0.6 384 4.9+0.6 1536 336 50+1 0.5+0.6 1008 3000+1.5 1344 672 15+6.5 1024 15+11.9 1024 - 32 16 16 144 144 144 144 144 144 144 256 256 256 256 256 256 576 576 576 576 576 576 576 720 2880 256 256 1012 125 125 304 200 125 125 125 125 125 1844 1844 200 125 125 125 125 304 304 304 333 200 304 304 632 304 125 125 59.3 66.7 52.1 84.3 63.9 75.8 64.4 87.2 63.8 89.8 66.2 87. - - - - 87.6 62.3 68.2 87.4 64.2 74.8 66.0 88.0 63.9 90.5 68.4 87.6 49.2 60.5 50.1 60.6 69.2 53.1 82.3 62.1 75.7 57.2 83.9 - - - 62.6 74.8 62.3 85.3 59.1 87.3 62.4 67.6 62.9 87.7 63.2 73.5 67.5 86.3 65.0 78.7 69.4 87.5 65.6 85.9 69.5 87.2 64.5 96.2 72.8 87.8 - - 59.3 67.1 63.8 57.5 68.2 61.5 - - 62.5 87.7 63.1 67.4 64.4 88.1 63.3 74.1 67.4 87.1 65.2 80.3 70.6 87.2 65.8 84.9 72.1 87. 302 150 150 460 496 387 387 387 446 446 - - 1157 577 577 577 641 1297 62.0 70.4 58.2 85.9 1297 64.6 74.8 66.8 86.1 1297 63.3 68.4 60.4 85.7 1308 61.6 67.6 59.8 85.5 2740 65.8 87.3 - 1297 62.3 68.2 64.4 85.5 48.9 85.9 - 72.6 72.8 86.6 62.2 - - - - 20347 65.2 72.8 64.6 - 641 641 66.0 87.4 73.1 87.3 64.5 96.0 74.8 87.2 VLMs with Multiple Vision Encoders and 8B LLM R40 MiniGemini-HD R41 Cambrian-1 [72] ConvNeXT-L ViT-L/14 ViT-SO400M ConvNeXt-XXL DINOv2-ViT-L/14 ViT-L/14 L-3 1.5+1. L-3 2.5+7 2880 576 1536 672 384 1024 518 336 200 304 430 846 304 21832 64.5 75.1 70.2 5085 64.6 80.4 71.7 - - - - 56.0 63.6 - 65.2 67. - 67.5 66.9 72.9 60.9 60.4 64.0 - - 63.8 63.9 67.0 73.0 73.9 - - - 64.8 66.5 71.5 75.8 59.6 - 72.6 66.1 - 69.7 - 81.5 80.1 72.4 73.5 - - - 70.8 - 70.0 29.1 - 31.8 77.0 61.0 31.0 78.7 66.2 - - - 35.4 79.4 61.3 39.7 80.2 65.7 - - 34.8 39.4 68.4 - 37.6 79.9 67.7 41.0 80.8 72. - 26.2 - 27.5 74.7 17.4 31.5 77.3 29.8 - - - 44.8 44.8 - 31.5 78.9 32.9 33.0 79.1 57.3 42.2 81.3 65.5 41.3 81.3 66.9 45.4 81.9 72.0 79.5 65.1 - 78.2 62.6 - 44.4 48.5 - 31.7 79.2 35.6 32.4 79.3 62.8 40.1 81.6 72.4 44.1 81.7 73.3 - - 31.1 76.6 28.1 - - - - 59.0 - - 37.6 80.6 33.6 78.9 45.9 34.9 79.9 29.4 78.7 42.1 82.8 76.8 78.2 - 47.6 82.3 78.7 49.9 82.3 78.9 - 30.4 31.4 30.9 31.7 - 34.9 37. 32.2 33.2 33.1 38.4 30.6 36.2 37.6 - 36.3 34.9 36.9 37.0 43.6 48.1 - - 35.1 35.1 37.3 36.7 46.2 35.3 - - - 35.8 - - 37.0 41.7 42.8 46.4 - 65.5 65.6 68. - 71.7 72.4 66.7 65.6 71.4 73.0 - 63.7 68.8 - 68.8 68.2 69.9 73.7 75.3 74.7 - - 69.3 68.5 69.9 73.5 75.1 66.1 - 69.7 - 70.2 62.8 - 69.9 72.7 75.9 75. - - - - 74.6 37. 73.2 77.8 42.7 74.7 Table 6. VLM evaluations and comparison with recent methods. The models are grouped based on total number of visual tokens. - indicates that performance was not reported in the respective paper. For the dataset column, - indicates that the dataset size for pretraining (PT) or instruction tuning (IT) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of PT. TTFT means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in format favorable to MLX [27] Vic. refers to Vicuna [90], Qw.2 refers to Qwen2 [79] and Qw. refers to Qwen [3]. L-2 refers to LLaMA-2. L-3 refers to LLaMA-3. ML. refers to MobileLLaMA [17, 18]. DS. refers to DeepSeek LLM [20]. For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision [40] and MM1 [60] use dynamic input resolution. - performance numbers reported from [72]. For MiniGeminiHD [44], the dataset sizes is inferred from the pretrain and instruction tuning JSON files. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up. Dataset Scaling. When designing new architecture, it is crucial to consider how effectively the model scales with training data. We demonstrate the performance of FastVLM when scaling the pretraining and instruction tuning datasets. By increasing the instruction tuning dataset from 0.6M to 1.1M samples (R18), FastVLM outperforms prior works like MobileVLMv2 [18] (R15), which was trained on larger instruction tuning and pretraining datasets. Additionally, when scaling the pretraining dataset by incorporating an intermediate pretraining stage for resolution scaling with 15M samples, FastVLM (R19) matches or surpasses MM1 [60] (R36) across wide range of benchmarks, including MMMU, MMVet, SQA, POPE, and SeedBench. Remarkably, FastVLM achieves this performance fewer visual tokens. With an input while generating 5 resolution of 1024 1024 and larger instruction tuning dataset of size 11.9M, FastVLM (R39) outperforms MM1 (R36) and LLaVA-NeXT (R37) across various benchmarks. Even on text-rich evaluations, like TextVQA and DocVQA, which are sensitive to input resolution and number of visual tokens, our model achieves better performance with less visual tokens than MM1 and LLaVA2.8 NeXT respectively. We provide details of the dataset splits in Sec. D. and 11.3 Multiple Vision Encoders. Recently, MiniGemini [44] and Cambrian-1 [72] introduced models that rely on multiple vision encoders. In Tab. 6, we compare FastVLM (R38), which uses single vision encoder with methods that use multiple encoders and trained on similarly scaled visual instruction tuning dataset. In Cambrian-1 [72] (R41), vision encoding contributes 3.2 more than LLM prefilling to the total time-to-first-token of approximately 5 seconds (detailed breakdown is provided in Tab. 9). FastVLM (R38) outperforms Cambrian-1 (R41) when trained on similar visual instruction tuning dataset, while being 7.9 faster. By scaling the instruction tuning dataset to 11.9M, FastVLM (R39) achieves superior performance fewer visual tokens, even over Cambrian-1 (R41) with 2.3 on text-rich evaluations (see Tab. 10) that are sensitive to the number of visual tokens. Effect of Decoder. VLM performance also depends on the quality of LLM, as demonstrated in prior studies, like [39]. By switching from Vicuna-7B (R19, R27) to Qwen2 [71, 79] models (R20, R28), we see good improvement in performance across all the benchmarks. The improvements are significant on MMVet, LLaVA-in-thewild and MMMU benchmarks. With Qwen2-0.5B as the LLM decoder, FastVLM (R3) matches the performance of LLaVA-OneVision [40] (R2) on key benchmarks such as SeedBench, MMMU, and MMVet, while being 85 faster fewer instruction tuning samples. This and trained on 2.9 result underscores the quality of our vision encoder, as both models use the same LLM decoder, while FastViTHD is 3.4 smaller compared to SigLIP-SO400M [87]. 5. Conclusion In this work, we introduced FastVLM, which leverages FastViTHD image encoders designed for enhanced resolution scaling while maintaining efficiency. By strategically trading off the costly self-attention in ViTs with purpose-built hybrid architecture, FastViTHD processes high-resolution images efficiently, and outputs substantially reduced number of visual tokens. The design of FastVLM enables competitive performance with prior works across wide range of VLM benchmarks, while improving efficiency in both time-to-first-token and the number of parameters in the vision backbone. Rigorous benchmarking on an M1 MacBook Pro demonstrates that FastVLM achieves state-of-the-art resolutionlatency-accuracy trade-off compared to existing works."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. 2 [2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 2 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 8, 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 202k. 2, 8 [5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. 2 9 [6] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. [7] Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae arXiv preprint Lee. Matryoshka multimodal models. arXiv:2405.17430, 2024. 3, 6, 7 [8] Jie Cao and Jing Xiao. an augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, 2022. 2 [9] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35583568, 2021. 7, [11] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision In Proceedings of the models in the vision-language era. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4, 5, 8, 1, 2 [12] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 8, 2 [13] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, 2024. 7 [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2, 3, 4, 7 [15] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2 [16] Chenglin Yang et al. MOAT: Alternating mobile convolution and attention brings strong vision models. In ICLR, 2023. 4 [17] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 3, 8, 2 [18] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Faster and Lin, Bo Zhang, et al. Mobilevlm v2: stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. 8, 9, 2 [19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 2, 3, 8 [20] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. 8, 2 [21] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. 2 [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 3, [23] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 3 [24] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 3, 5 [25] Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Zheng. Convllava: Hierarchical backbones as visual encoder for large multimodal models, 2024. 2, 3, 7, 8 [26] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 7 [27] Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. MLX: Efficient and flexible machine learning on apple silicon, 2023. 7, 8, 3 [28] Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang. Matryoshka query transformer for large vision-language models, 2024. 3, 6, 7 [29] Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, and Shaohui Lin. Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification, 2024. 6, 7 [30] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 7 [31] Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. 2 [32] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In International Conference on Machine Learning (ICML), 2024. 3 [33] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. 2 [34] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. 2 [35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, arXiv preprint and Ross Girshick. arXiv:2304.02643, 2023. Segment anything. [36] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 2017. 2 [37] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 7 [38] Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. Llava-next: What else influences visual instruction tuning beyond data?, 2024. 7, 1, 2 [39] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 9 [40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 8, [41] Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan. Flexattention for efficient high-resolution vision-language modIn European Conference on Computer Vision, pages els. 286302. Springer, 2025. 8 [42] Kevin Y. Li, Sachin Goyal, Joao D. Semedo, and J. Zico Kolter. Inference optimal vlms need only one visual token but larger models, 2024. 4 [43] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 7 [44] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2023. 8, 9 [45] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740755, 2014. 2 [47] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 4 [48] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 2, 6 [49] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 1, 2, 3, 6, 8 [50] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 6, 7 [51] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2 [52] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: On the hidden mystery of ocr in large multimodal models, 2024. [53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [54] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding, 2024. 8, 2 [55] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. 7, 2 [56] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-ofresolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. 2 [57] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering In Findabout charts with visual and logical reasoning. 11 ings of the Association for Computational Linguistics: ACL 2022, 2022. 2, [58] Minesh Mathew, Viraj Bagal, Rub`en Perez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. Infographicvqa, 2021. 3 [59] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 7, 2 [60] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. Mm1: Methods, analysis & insights from multimodal llm pretraining, 2024. 2, 7, 8, 9, 3 [61] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 2 Gpt-4 technical arXiv preprint [62] OpenAI. report. arXiv:2303.08774, 2023. 2 [63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2, 3 [64] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 3, 7 [65] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 7, 2 [66] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? In European Conference on Computer Vision (ECCV), 2024. 2 [67] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv:2408.15998, 2024. [68] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 7, 2 [69] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 3 12 [70] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. 3 [71] Qwen Team. Qwen2.5: party of foundation models, 2024. 2, 9 [72] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 2, 3, 8, 9 [73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 2 [74] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Conference on Neural Information Processing Systems (NeurIPS), 2021. 2 [75] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Mobileone: An improved one millisecond mobile backbone. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1 [76] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: fast hybrid vision transformer using structural reparameterization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 3, 5, 1 [77] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced In Proceedings of the IEEE/CVF Conference on training. Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 5, [78] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, and Ran Xu. xgenmm (BLIP-3): family of open large multimodal models. CoRR, abs/2408.08872, 2024. 2 [79] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1, 2, 5, 6, 8, 9, 3 understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [80] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models, 2024. 6, 7 [81] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplugowl3: Towards long image-sequence understanding in multimodal large language models, 2024. [82] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. [83] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023. 2 [84] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 7 [85] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer baselines for vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 4 [86] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 7 [87] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. International Conference on Computer Vision (ICCV), 2023. 2, 3, 9 [88] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. [89] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 7 [90] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 1, 2, 3, 6, 8 [91] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language 13 FastVLM: Efficient Vision Encoding for Vision Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Training Setup B. Architecture Details For experiments presented in Tab. 1, Tab. 2, Tab. 4, Tab. 5, we perform 2-stage training with the hyperparameters listed in Tab. 7. The model is trained for single epoch in all the stages. Note, in Tab. 5, we do not re-train other token pruning works, we simply report the performance of the respective methods as they adhere to the 2-stage training setup described in Tab. 7, which was originally introduced in LLaVA-1.5 [49]. To showcase our models performance in the presence of additional dataset, we scale both pretraining and instruction tuning datasets in Sec. 4. For results presented in R13, R17, R18, R25, R26 in Tab. 6, we still perform 2-stage training described in Tab. 7, for R18 and R26, we use instruction tuning dataset of size 1.1 million samples in Stage-2. For results presented in R3, R4, R6, R7, R10, R11, R14, R19, R20, R21, R27, R28, R38 and R39, we scale-up both instruction tuning dataset and pretraining dataset. We also introduce and additional stage of pretraining with the scaledup dataset as described in Tab. 8. Details of 1.1 million, 6.5 million and 11.9 million instruction tuning dataset is presented in Sec. D. The patch embedding layers shown in Fig. 2, consists of 7 depthwise convolutions with [75] style train-time over7 parameterization, followed by 1 1 pointwise convolution. 7 depthwise convolution is set to 2 in orThe stride for 7 der to downsample the input tensor. In [77], squeeze-excite layers were incorporated into this block; however, we found them to negatively impact inference latency, especially for high image resolutions, so we opted not to include them in our model. We use the same ConvFFN layer defined in [76], i.e. 7 7 depthwise convolutions preceding typical FFN layer. The stem downsamples the input tensor by factor of 4 on each side, and each patch embedding layer downsamples the input tensor by factor 2. Although recent architectures like ViTamin [11] recommend an overall downsampling factor of only 16, FastViTHD incorporates an additional patch embedding layer compared to FastViT, resulting in an overall downsampling factor of 64 for the input tensor. In each stage, we increase the number of channels by factor of 2 as done in FastViT and other convolutional and hybrid transformer architectures. This results in Stage-5 with the widest MLP layers in the architecture, performing self-attention on an input tensor which is downsampled by factor of 64. Stage-1 Stage-"
        },
        {
            "title": "Data",
            "content": "LLaVA-1.5 558K LLaVA-1.5 665k Learning Rate Batch size LR. schedule LR. warmup ratio Optimizer"
        },
        {
            "title": "Trainable\nmodules",
            "content": "1e-3 256 cosine decay 0.03 AdamW"
        },
        {
            "title": "Projector",
            "content": "2e-5 128 cosine decay 0.03 AdamW"
        },
        {
            "title": "Full\nModel",
            "content": "B.1. Naive Scaling In order to scale the model size of FastViT, we simply increased the embedding dimensions per stage to [128, 256, 512, 1024], and set the number of layers per stage to [2, 12, 16, 6]. Patch embedding layers in each stage use squeeze-excite layers and the MLP expansion ratio is set to 3.0, following the design in [77]. Table 7. 2-Stage training setup used in ablations for Sec. 3. C. Additional Results Stage-1 Stage-1. Stage-2 Data LLaVA-1.5 558K Recap-CC3M + Recap-CC12M [38] 1.1M / 6.5M / 11.9M Learning Rate Batch size LR. schedule LR. warmup ratio Optimizer Trainable modules 1e-3 256 cosine decay 0.03 AdamW Projector 2e-5 128 cosine decay 0.03 AdamW Full Model 2e-5 128 cosine decay 0.03 AdamW Full Model Table 8. 3-Stage training setup used for results with scaled-up data in Tab. 6. We present the performance of FastVLM on text-rich benchmarks under various training settings in Tab. 10. FastVLM surpasses MM1 and Cambrian-1 across wide range of benchmarks by scaling up pretraining and instruction tuning datasets. This result highlights the quality of visual tokens produced by FastViTHD, as FastVLM is able to achieve these improvements with 2.8 less visual tokens than MM1 and with vision encoder that is 5.1 smaller. D. Datasets D.1. Pretraining Datasets For Stage-1 training, we only use LLaVA-1.5 558K [49] dataset. For Stage-1.5 training, we use densely captioned 1 Row Ann. Method Vision Encoder LLM Input Res. #Visual Vis. Enc. Size(M) Tokens 0.5B Model Comparison nanoLLaVA LLaVAOV [40] FastVLM (Ours) ViT-SO400M ViT-SO400M FastViTHD Qw.1.5 Qw.2 Qw.2 384 1152 1024 729 7290 256 R1 R2 R3 R4 R6 R7 R8 MobileVLMv2 [18] FastVLM (Ours) DeepSeekVL [54] MM1 [60] FastVLM (Ours) R9 R11 InstructBLIP [19] FastVLM (Ours) R12 MobileVLMv2 [18] R13 R14 ConvLLaVA [25] FastVLM (Ours) FastVLM (Ours) ConvLLaVA [25] LLaVA-1.5 [49] R20 R26 R27 MobileVLMv2 [18] ShareGPT4V [12] R28 R29 ViTamin [11] ConvLLaVA [25] R30 VILA [45] R31 MM1 [60] R33 LLaVA-NeXT R34 FastVLM (Ours) R21 FastVLM (Ours) R36 R35 MiniGemini-HD Cambrian-1 [72] 430 430 125 304 125 430 632 125 1012 125 304 200 125 200 304 304 304 333 200 304 632 304 125 125 Vision Enc. Latency(ms) LLM Prefilling(ms) 272.1 2721.4 116.3 127.4 54. 272.1 - 116.3 149.5 6.8 127.4 164.3 54.8 54.8 696.1 127.4 127.4 127.4 137.6 1569.7 127.4 - 637.0 116.3 116.3 1569.7 552.6 272.1 2290.4 1171.5 127.4 263.3 11402.4 50. 458 97.1 - - 116.1 152.1 143.4 332.1 332.1 332.1 391.2 461.1 1170.0 1170.0 1170.0 1170.0 1170.0 1169.5 - 19709.7 461.1 524.5 19709. 1223.6 ViT-L/14 FastViTHD ViT-SO400M ViT-H FastViTHD ViT-g/14 FastViTHD ViT-L/14 ConvNeXT-L FastViTHD FastViTHD ConvNeXT-L ViT-L/14 ViTamin-L ConvNeXT-L ViT-L/14 ViT-H ViT-L/14 FastViTHD FastViTHD 1-2B Model Comparison ML. Qw.2 DS. - Qw.2 336 384 1344 1024 144 144 576 720 256 7B Model Comparison Vic. Vic. Vic. Vic. Vic. Qw. Vic. Vic. Vic. Vic. L-2 - L-3 Vic. Qw.2 224 256 336 768 768 768 1024 336 336 336 384 1536 336 1344 672 1024 32 16 144 144 144 144 256 576 576 576 576 576 576 720 2880 256 256 VLMs with Multiple Vision Encoders and 8B LLM ConvNeXT-L ViT-L/14 ViT-SO400M ConvNeXt-XXL DINOv2-ViT-L/14 ViT-L/14 LL-3 1536 672 384 1024 518 336 2880 576 200 304 430 846 304 304 Table 9. Breakdown of prefilling latencies for recent methods. The models are grouped based on total number of visual tokens. For models that were difficult to export or unavailable, we mark them as - in the table. Vic. refers to Vicuna [90], Qw.2 refers to Qwen2 [79] and Qw. refers to Qwen [3]. L-2 refers to LLaMA-2. L-3 refers to LLaMA-3. ML. refers to MobileLLaMA [17, 18]. DS. refers to DeepSeek LLM [20]. For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision [40] and MM1 [60] use dynamic input resolution. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up. versions of CC3M [65] and CC12M [10] introduced in [38]. The total size of this dataset is 15 million image-text pairs. We generated 300 generic questions, such as What is in this photo?. For each (image, dense-caption) pair, we randomly selected generic question to form triplet of (question, image, dense-caption). With 0.5 probability, we placed the images special token <image> either before or after the question. From recent works like [38, 60, 72] and our results in Tab. 6, scaling dataset in Scale-1.5 is beneficial to improve the performance of VLM across wide range of evaluations. Even though FastViTHD is smaller than ViT-L/14 and ViT-H used in [38, 60] respectively, we see similar scaling trends. D.2. Visual Instruction Tuning Datasets We use 3 different version of instruction tuning datasets. [49]. The smallest scale is LLaVA-1.5 665K dataset We further scale up this dataset by including training splits of the following datasets; AI2D [33], ScienceQA [55], ChartQA [57], COCO [46], DocVQA [59], DVQA [31], GeoQA+ [8], OCRVQA [61], SegmentAnything [35], SynthDoG-EN [34], TextVQA [68] and Visual Genome [36]. The conversational data for the listed datasets is sourced from [14]. The total number of samples in this dataset is 1.1 million and is referred to as 1.1M in all the tables. We further scale-up instruction tuning dataset using image-based conversational data from Cambrian-7M [72], which amounts to 5.4 million samples. Filtered Cambrian-7M [72] is merged with 1.1M dataset to obtain 6.5M instruction tuning dataset. We then append all available single-image instruction tuning data open-sourced by LLaVA-OneVision [40] to 6.5M to obtain 11.9M instruction tuning dataset. From Tab. 6, we see further improvements in VLM benchmarks when in2 Row Ann. R1 R"
        },
        {
            "title": "Method",
            "content": "MM1 [60] LLaVA-NeXT"
        },
        {
            "title": "Vision\nEncoder",
            "content": "ViT-H ViT-L/"
        },
        {
            "title": "LLM",
            "content": "- L-3 Data (M) Input #Visual Vis. Enc. TTFT (PT+IT) Res. Tokens Size(M) (ms) 3000+1.5 1344 672 - 720 2880 632 304 -"
        },
        {
            "title": "ChartQA OCRBench TextVQA DocVQA InfoVQA",
            "content": "72.6 69.5 62.6 49.0 72.8 64.6 76.8 72.6 45.5 - Cambrian-1 [72] R4 R5 R6 R7 R8 FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) FastVLM (Ours) R9 R10 FastVLM (Ours) R11 FastVLM (Ours) R12 FastVLM (Ours) R13 FastVLM (Ours) R14 FastVLM (Ours) ViT-L/14 ViT-SO400M ConvNeXt-XXL DINOv2-ViT-L/14 L2.5+7 336 384"
        },
        {
            "title": "FastViTHD\nFastViTHD\nFastViTHD\nFastViTHD\nFastViTHD\nFastViTHD",
            "content": "768 0.5+0.6 Vic. 768 0.5+1.1 Vic. 768 Vic. 15+1.1 Qw.2 15+1.1 768 Qw.2 15+11.9 768 0.5+0.6 1024 Vic. 0.5+1.1 1024 Vic. Vic. 15+1.1 1024 Qw.2 15+1.1 1024 Qw.2 15+6.5 1024 Qw.2 15+11.9 1024 576 144 144 144 144 144 256 256 256 256 256 256 304 430 846 125 125 125 125 125 125 125 125 125 125 125 5085 73.3 62.4 71. 77.8 - 387 387 387 446 446 577 577 577 641 641 641 17.1 59.1 65.4 69.3 74.2 19.2 61.0 66.9 71.0 76.6 77. 30.0 38.4 45.3 45.9 59.0 29.3 38.3 47.1 49.7 52.9 63.3 62.9 67.5 69.4 69.5 72.8 64.4 67.4 70.6 72.1 73.1 74.8 32.9 57.3 65.5 66.9 72.0 35.6 62.8 72.4 73.3 78.7 78. 28.7 29.7 32.0 34.3 44.3 28.9 32.0 34.7 37.5 44.2 49.7 Table 10. Comparison with recent methods on text-rich benchmarks. The models are grouped based on total number of visual tokens. - indicates that performance was not reported in the respective paper. For the dataset column, - indicates that the dataset size for pretraining (PT) or instruction tuning (IT) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of PT. TTFT means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in format favorable to MLX [27] Vic. refers to Vicuna [90], Qw.2 refers to Qwen2 [79]. L-3 refers to LLaMA-3. * - For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like MM1 [60] use dynamic input resolution. - performance numbers reported from [72]. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up. struction tuning dataset is scaled, following trends exhibited by image encoders much bigger than FastViTHD. D.3. Evaluations In addition to evaluations listed in Sec. 4, we report performance of FastVLM on ChartQA [57], OCRBench [52] and InfoVQA [58] to compare FastVLM against recent methods on text-rich benchmarks. In Tab. 11, report performance of FastViT model (with architectural interventions) from multiple training runs and compute the standard deviation of metrics reported in Tab. 6. As described in Sec. 4, for ablations we are interested in benchmarks that are quick to evaluate and exhibit lower variance to different initializations. From Tab. 11, GQA, TextVQA, POPE, DocVQA and SeedBench fit the criteria. While VQAv2 also exhibits lower variance it is substantially larger and takes long time to evaluate. The standard deviation across the selected metrics is below 0.5, so we use the average of these metrics as reliable indicator for our analysis in Sec. 3. 3 GQA SQA TextVQA POPE LLaVA BenchW MMVet VQAv2 DocVQA 62.69 64.25 62.68 64.95 62.69 65.64 60.71 60.61 60. Std. 0.0047 0.57 0.041 85.8 86.1 85.3 0.33 59.4 60.1 61.4 0. 29.6 31.6 31.1 0.85 77.27 77.39 77.31 0.049 27.57 28.37 28.26 0. Seed BenchI 53.31 53.55 53.46 0.099 Table 11. VLM benchmarks across three independent runs with frozen FastViT image encoder. Training setup is LLaVA-1.5 with Vicuna 7B as LLM. Standard deviation across runs is listed in the bottom row."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}