{
    "paper_title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
    "authors": [
        "Man Tsung Yeung",
        "Penghui Qi",
        "Min Lin",
        "Xinyi Wan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 8 8 2 5 0 . 1 1 4 2 : r Man Tsung Yeung 1 2 Penghui Qi 1 2 Min Lin 1 Xinyi Wan ABSTRACT Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in 5% to 51% improvement in throughput compared to naıve approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism."
        },
        {
            "title": "INTRODUCTION",
            "content": "As the scale of transformer models (Vaswani et al., 2017; Brown et al., 2020) continues to grow, model parallelism has garnered significant attention within the deep learning community. Several model parallel techniques have been proposed to address the challenges associated with training large models, including Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020; Zhao et al., 2023), Tensor Parallelism (TP) (Shoeybi et al., 2019), and Pipeline Parallelism (PP) (Huang et al., 2019; Harlap et al., 2018; Narayanan et al., 2021; Qi et al., 2023; 2024). Each of these methods has its own advantages and limitations. For instance, ZeRO is effective in reducing memory by eliminating redundant parameter storage, but suffers from high communication overhead when gathering partitioned parameters and gradients for scenarios with limited network bandwidth or requiring frequent parameter updates. TP can efficiently handle large model parameters by splitting them across devices, but often faces low arithmetic intensity and requires significant inter-device communication. Among these techniques, PP shows distinct advantages due to its low communication cost and high arithmetic intensity, making it particularly attractive for training large-scale models. Work was done during an internship at Sea AI Lab. 1Sea AI Lab 2National University of Singapore. Correspondence to: Xinyi Wan <wanxy@sea.com>. Preprint. However, PP faces two significant challenges: pipeline bubbles and high memory consumption. Pipeline bubbles occur when there are idle periods in the pipeline stages, leading to suboptimal utilization of computational resources. Various strategies have been proposed to mitigate pipeline bubbles, such as token-level PP (Li et al., 2021) and interleaved 1F1B (Narayanan et al., 2021). An exceptional advancement is zero-bubble pipeline parallelism (Qi et al., 2023; 2024), which achieves almost zero bubble in many scenarios through splitting backward pass into activation gradient computation and weight gradient computation. In most PP schedules, the activations of several microbatches are stored to reduce pipeline bubbles, making memory critical bottleneck to scale large models. Previous work has explored activation recomputation (Chen et al., 2016; Korthikanti et al., 2023), memory transferring (Kim et al., 2023) and memory-efficient V-Shape scheduling (Qi et al., 2024) to mitigate this issue. Despite various effort, the memory bottleneck still poses the largest limitation for PP. In this paper, we focus on an imbalance issue in PP caused by vocabulary-related layers, which is often overlooked in practice but can significantly degrade the performance in both throughput and memory. Typically, transformer layers are uniformly distributed across pipeline stages, while the first stage contains an additional input layer and the last stage contains an additional output layer. Such imbalanced setup greatly hurts the performance in both computation and memory. Firstly, pipeline bubbles are introduced in Balancing Pipeline Parallelism with Vocabulary Parallelism Figure 1. Repeating pattern in an imbalanced pipeline. Bubbles are incurred due to an extra output layer in the last pipeline stage. other pipeline stages due to their less workload, as shown in Figure 1. Additionally, the additional input layer in the first stage exacerbates the memory bottleneck for most PP schedules like 1F1B (Harlap et al., 2018). Finally, as the vocabulary size grows larger (Tao et al., 2024), this imbalance becomes more pronounced, as shown in Figure 2. For instance, in the case of Gemma2 9B with vocabulary size of 256k (Team et al., 2024), both the computation and parameter memory of the output layer are approximately 5 times those of the transformer layers, highlighting the severity of the issue. To address this imbalance issue, we propose novel Vocabulary Parallelism approach to balance the computation and memory in PP. By partitioning the vocabulary layers across all pipeline devices, we introduce several methods to group the computation and communication barriers together with generalizable scheduling approach in PP, with only small constant memory overhead. Extensive experiments demonstrate our approach significantly outperforms naıve layer redistribution and other existing methods, resulting in up to 51% improvement in throughput. Figure 2. Ratio of compute and memory of vocabulary layers compared to transformer layers in Gemma2-9B."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Balancing Activation Memory line of research addresses another aspect of imbalance in PP, the activation memory with the 1F1B schedule. For instance, BPipe (Kim et al., 2023) transfers activations between devices, trading communication for reduced peak memory. Another approach uses V-Shape scheduling to create pipeline schedFigure 3. Transformer Layer Redistribution for 7B GPT-like model with vocabulary size 128k. In this case, each stage has 2 transformer layers, while output layer is equivalent to 2.4x of transformer layer on compute and 2.6x on parameter memory. ule with balanced and efficient memory usage. These methods are orthogonal to our work, and combining them can achieve fully balanced pipeline parallelism in both computation and memory (activations and parameters). Balancing Vocabulary Layers Some existing training systems try to mitigate the imbalance caused by vocabulary layers by redistributing transformer layers across different stages. DeepSpeed (Smith et al., 2022) uses greedy method to automatically re-balance the workload between stages at the layer level. Similar strategies are employed in the training of Skywork-MoE (Wei et al., 2024). However, simply redistributing transformer layers faces several disadvantages. Firstly, even after redistribution, compute imbalance can still persist since only subset of pipeline stages receive additional layers. An example is shown in Figure 3. This is particularly evident when the number of layers on each stage is small. Secondly, different layer types have varying compute-to-memory ratios, meaning that the re-balancing can only be based on either compute or memory but not both. In practice, the re-balancing is typically performed based on compute, leaving the memory imbalance still significant, particularly for input vocabulary layers that require minimal compute but substantial memory. Lastly, the effectiveness and planning of redistribution heavily depend on both the model settings and pipeline parallel settings. This makes it less flexible and challenging to adopt in various scenarios. Its also worth noting that some models pretrained from Balancing Pipeline Parallelism with Vocabulary Parallelism scratch like Llama 3 (Dubey et al., 2024) reduce one transformer layer each from the first and the last stages, respectively. This method requires changes to architecture of models, which is out of the scope of this paper. Also, it has limitations if the training starts from checkpoint where the number of transformer layers of model is fixed. Another method to mitigate the imbalance problem in pipeline parallelism (PP) is the interlaced pipeline, reported by the automatic parallelism framework nnScaler (Lin et al., 2024). This approach distributes the input and output vocabulary layers across different pipeline devices using tensor parallel (TP) style (Narayanan et al., 2021). By alternating between TP for vocabulary layers and PP for transformer layers, it aims to balance compute and memory overhead. However, TP requires frequent synchronization between devices, leading to two major drawbacks. First, the peak activation memory for 1F1B increases to 1.5 times of its original value (see Appendix B.1), which may make the critical memory bottleneck even worse. Second, the synchronized all-reduce during the output vocabulary layer introduces additional pipeline bubbles for each microbatch. Our ablation study in Appendix B.2 shows these all-reduce along slows down the end to end training by approximately 11% on 32 GPUs. These significant overheads in both activation memory and pipeline bubbles render the interlaced pipeline impractical in real-world scenarios."
        },
        {
            "title": "PIPELINE PARALLELISM",
            "content": "To completely address the imbalance issue in PP, we propose Vocabulary Parallelism under the following design principles: We partition the vocabulary layers across the vocabulary dimension, and distribute it evenly to all pipeline devices. To be native to pipeline parallelism, the computation of vocabulary layers should be represented as passes similar to forward/backward passes of transformer layers. Integrating vocabulary passes into pipeline schedules should not drastically affect the memory and efficiency of the original pipeline schedule. Intuitively, after partitioning the vocabulary layers to all pipeline devices, computations on each device can be scheduled independently by inserting them cleverly into the existing pipeline passes, as long as the dependencies are still satisfied. However, it is worth noting that partitioning the softmax computation creates several all-reduce operations. These communication barriers create cross-device dependencies. Figure 4. Computation graph of the output layer after partitioning across the vocabulary dimension. There are three all-reduce communications across all devices. Driven by this intuition, in Section 4, we discuss how to partition the computation in vocabulary layers to multiple devices and group them into pipeline passes. We observe that the communication barriers (e.g. the all-reduces in softmax) within the vocabulary layers increases the activation memory consumption of the pipeline schedule. As an improvement, we propose two novel algorithms to reduce the number of communication barriers, which reduces the activation memory overhead to minimum. In Section 5, we discuss how to integrate these vocabulary passes into existing pipeline schedules. Inspired by the framework presented in Qi et al. (2024), we insert the vocabulary passes into the building blocks of existing schedules and simply repeat building blocks to construct pipeline schedules. This relieves us from the hassle of deciding the ordering of vocabulary passes of every microbatch and is naturally generalizable to other schedules."
        },
        {
            "title": "4 VOCABULARY PASSES CONSTRUCTION",
            "content": "In this section, we introduce how to split the vocabulary layers into several computation passes after partitioning them across all pipeline devices, and how to optimize the number of communication barriers. Balancing Pipeline Parallelism with Vocabulary Parallelism 4.1 Naıve Approach follows: In the input layer, each device can perform forward and backward computations independently. We provide details on the input layer in Appendix C, and focus on the output layer for the remainder of this paper. Figure 4 shows the computation graph of the output layer after partitioning the layer across devices. We denote the microbatch size as b, sequence length as s, hidden dimension as and vocabulary size as . The partitioned output layer can be grouped into three computation passes F1, F2 and B, separated by three all-reduce communications involving the maximum of logits, the sum of logits and the input gradients respectively. We can overlap these all-reduce communications with transformer layer computation by placing them in separate stream, as shown in Figure 5. Figure 5. Overlapping all-reduce communication with transformer layer computation. Figure 6 shows the computation and communication dependencies for single microbatch. Notably, each of these all-reduce communications will introduce communication barrier across all pipeline devices, which complicates the pipeline scheduling. As shown later in Section 5.2, the number of communication barriers also increases the activation memory consumption of the pipeline schedule. Therefore, we aim to reduce the number of communication barriers by reordering the operations in the output layer. Figure 6. Scheduling dependencies in the naıve output layer implementation. 4.2 Definitions We detail the computation in the output layer. Given the output of the last transformer layer and the embedding weights , we first compute : = XW (1) Then, the (safe) softmax of each sequence is computed as softmax(Yij) = eYij mi sumi (2) where mi = maxk Yik is the maximum of the logits and sumi = (cid:80) eYikmi is the sum of logit exponents. Assuming the cross entropy loss is used, in the backward phase, we have = (softmax(Y ) = (softmax(Y ) G) G)T (3) (4) where is the ground truth matrix with Gigi = 1 and Gij = 0 otherwise, where gi is label for token i. 4.3 Forward Phase Optimization Inspired by online softmax (Milakov & Gimelshein, 2018; Dao et al., 2022), we observe that the all-reduce communication for mi and sumi can be done after computing the softmax. Instead of using the global maximum and sum, each device instead computes softmax(Yi) using the local maximum and sum from its vocabulary partition. We then have softmax(Yij) = softmax(Yij) imi sum em sumi (5) where and sum sum, respectively. are the locally computed maximum and Using equation 5, we have Algorithm 1 that reduces the 3 communication barriers to 2, which are denoted as C1 and C2 respectively. Algorithm 1 Output layer with 2 communication barriers function forward and backward(W ) Receive Broadcast XW maxV /p (cid:80)V /p (cid:11) sum k=1 Yik k=1 eYikm eYij sum AllReduce em sum imi AllReduce sum softmax(Yij) (cid:10) (cid:7) mi sum sumi (cid:6) (cid:11) softmax(Yi) softmax(Yi) G)W G)T (softmax(Y ) (softmax(Y ) Reduce (cid:10) end function sum sumi (cid:8) C0 C1 C2 (cid:9) (cid:4) (cid:5) (cid:8) (cid:9) Balancing Pipeline Parallelism with Vocabulary Parallelism The elementwise operations in C1 only involves tensors of size [bs] as opposed to size [bs, /p], which greatly reduces the computation pressure when overlapped with transformer layer computation. 4.4 Backward Phase Optimization We also observe that all three all-reduce communications can be done after computing the matrix multiplications for the input gradients. Note that Figure 7. Computation order in the output layer for single microbatch, corresponding to the naıve implementation, Algorithm 1 and Algorithm 2 respectively. beneficial to reduce the number of communication barriers. As shown in section 5.2, reduction in the number of communication barriers will help to save activation memory."
        },
        {
            "title": "5 PIPELINE SCHEDULING",
            "content": "In this section, we show systematic method about how to make minimal changes to typical pipeline schedules to include the output layer passes. We apply our method on two different schedules, 1F1B (Harlap et al., 2018) and V-Half (Qi et al., 2024). Despite its popularity, an inherent problem of the 1F1B schedule is an imbalanced activation memory across pipeline devices. In contrast, V-Half balances out the activation memory by V-Shape device placement, reducing the activation memory requirement to half of that of 1F1B. By integrating Vocabulary Parallelism into V-Half, we aim to achieve completely memory-balanced pipeline. (cid:16) C0 (cid:17) (cid:8) = softmax(Y )W imi sum em sumi GW (6) We can compute softmax(Y )W and GW beforehand, and all-reduce after we obtain mi and sumi. Since the matrix multiplications in equation 6 are already computed, computing within the communication barrier only involves lightweight operations. This allows us to complete both phases in the output layer with only single communication barrier C1, as shown in Algorithm 2. Algorithm 2 Output layer with 1 communication barrier function forward and backward(W ) Receive Broadcast XW maxV /p (cid:80)V /p (cid:19) sum k=1 Yik k=1 eYikm softmax(Yij) eYij sum (cid:18) (cid:11) mi sum sumi (cid:10) (cid:7) softmax(Y )W GW AllReduce em sum imi AllReduce sum sum Reduce sumi softmax(Y ) softmax(Y ) (softmax(Y ) G)T end function (cid:6) C1 5.1 Scheduling Dependencies sum sumi (cid:9) (cid:4) (cid:5) In Algorithms 1 and 2, we perform output layer computation with 2 and 1 communication barriers, respectively. For each microbatch, we have to integrate the output layer passes, and , into the pipeline schedules. The pipeline schedule has to adhere to the following constraints: Note that the weight gradient step can be arbitrarily delayed since no other operations depend on it, similar to the idea in zero-bubble strategy (Qi et al., 2023). We compare the two algorithms with the naıve implementation in Figure 7. By placing the operations in the communication barrier in separate stream, they will be able to overlap with transformer layer computation. Compared to Algorithm 1, Algorithm 2 introduces bit more computation overhead (shown in Section 6.5). However, it is still All passes must be scheduled after the forward pass of the last transformer layer completes. All passes must be scheduled after all passes complete. For Algorithm 1 only, the backward pass of the last transformer layer must be scheduled after all passes complete. In contrast, the passes can be arbitrarily delayed in Algorithm 2. For example, Figure 8 shows the scheduling dependencies for single microbatch in Algorithms 1 and 2 respectively. Balancing Pipeline Parallelism with Vocabulary Parallelism Figure 8. Scheduling Dependencies in Algorithms 1 and 2. 5.2 Methodology To elegantly integrate these and passes into typical pipeline schedules while adhering to the constraints, we follow Qi et al.s framework (2024) to construct pipeline schedules. In this framework, each pipeline schedule can be structured by its building block, which is defined by the scheduling pattern of each microbatch. By uniformly repeating building block, we can construct pipeline schedule, with peak activation memory calculated by dividing its lifespan by its interval. The lifespan is the time between forward and its corresponding backward, while the interval is the workload of single microbatch on each device, as illustrated in Figure 9. This approach greatly simplifies dependency management for each microbatch and facilitates memory consumption analysis. Considering the building block of the schedule, by inserting 2 or 1 intervals (for Algorithms 1 and 2 respectively) between the forward and backward pass of the last transformer layer, we can create space to schedule the output layer computation. Within the repeating interval in the building block, we can schedule output layer passes (S and ) arbitrarily in each pipeline device. We show an example based on 1F1B in Figure 9, where one-forward-one-backward-one-output pattern is strictly followed. The final 1F1B schedules are presented in Figure 10, which is produced by uniformly repeating the building blocks. Additionally, the building block for V-Half can be found in Appendix D. For the peak activation memory, as we insert at most 2 intervals to the lifespan, the peak activation memory only increases by at most 2 microbatches, which is small constant overhead. This is remarkable improvement compared to synchronous pipeline schedules, which multiplies the activation memory requirement by 1.5 (see Appendix B.1). Furthermore, the memory savings from balancing the vocabulary parameters outweighs the increase in activation memory. Figure 9. Modified building blocks for the 1F1B schedule corresponding to Algorithm 1 and Algorithm 2. The output layer passes are inserted. of communication barriers, which motivates our optimization of communication barriers in Section 4."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "We construct our experiments to verify: a) Our schedules with Vocabulary Parallelism can bring acceleration; b) Our methods can achieve balanced memory usage when combined with memory-balanced schedules like V-Half (Qi et al., 2024); c) The partitioning of vocabulary layers has reasonable scaling factor compared to linear scaling. 6.1 Implementation We implement the pipeline scheduler and the partitioned vocabulary layers based on the open-source Megatron-LM project (Narayanan et al., 2021). In practice, scheduling under the assumption that backward takes twice time as forward might introduce unnecessary bubbles, especially when these values differ significantly. As result, we profile the run time of the passes and schedule the and passes accordingly. We handle the communication groups in separate streams, allowing the communication barrier to overlap with the transformer layer passes. We map the CUDA streams to separate GPU work queues to achieve this overlapping1. However, this would affect communication-computation overlap performance of tensor parallelism (Narayanan et al., 2021) as it relies on single work queues. To mitigate this problem, we set all model parallel communication groups to use high-priority streams. Additionally, both AllReduce and Reduce mentioned on Algorithm 1 and 2 are implemented as NCCL (NVIDIA) AllReduce to avoid imbalanced comNotably, as shown in Figure 9, the activation memory increased in terms of microbatches is equivalent to the number 1See https://docs.nvidia.com/deploy/mps/index.html#cudadevice-max-connections. Balancing Pipeline Parallelism with Vocabulary Parallelism Figure 10. Full 1F1B schedules with Vocabulary Parallelism, corresponding to (a) Algorithm 1 and (b) Algorithm 2 respectively. Algorithm 1 requires activation memory for + 2 microbatches while Algorithm 2 only requires + 1, where is the number of devices. munication volume across devices. We also pad the vocabulary size to be multiple of 2p to improve memory alignment in the vocabulary layers, where is the number of devices. In particular, we observe an approximate 8% increase in performance if our method is applied to 24 devices with padded size 256032 (a multiple of 48), compared to the original value 256008. In all our experiments, we untied the input and output embedding weights. However, note that our method eases embedding weight tying since the input and output layers have identical device placement. 6.2 Setup We compare the following methods implemented on the 1F1B schedule (Harlap et al., 2018). Baseline: The naıve implementation in MegatronLM. It distributes the transformer layers equally to all pipeline stages, while assigning the input and output layers to the first and last pipeline devices. This leads to highly imbalanced compute and memory. Redis: Redistributes the transformer layers across pipeline stages to balance out the computation as much as possible. We follow the derivation by Narayanan et al. (2021) to estimate the number of floating point operations in each pipeline stage, and minimize the length of the longest stage. Vocab-1: Implements Vocabulary Parallelism with only forward phase optimization, as described in Algorithm 1. Vocab-2: On top of Vocab-1, applies backward phase optimization, as described in Algorithm 2. Interlaced: Our implementation of the fully synchronous interlaced pipeline proposed by Lin et al. (2024). We experiment the implementations by pretraining GPT-like Table 1. Settings used in experiments on 1F1B schedule. PIPELINES (GPUS) 8 16 32 MODEL SIZE LAYERS ATTENTION HEADS HIDDEN SIZE SEQUENCE LENGTH MICROBATCH SIZE NUMBER OF MICROBATCHES VOCABULARY SIZE 64 40 5120 32 24 3072 4B 10B 21B 48 32 4096 2048 / 4096 1 128 32K / 64K / 128K / 256K models of varying model and vocabulary sizes with up to 32 NVIDIA A100 SXM 80G GPUs distributed across 4 nodes, interconnected by RoCE RDMA network. The running time of each iteration is recorded after several warm-up iterations. We compare the 5 methods under each fixed setting of model and vocabulary size, shown in Table 1. 6.3 Comparison of Methods We present comparisons of the throughput measured in FLOPs utilization (MFU) and peak memory in Figures 11 and 12, respectively. As shown in the figures, the layer redistribution approach suffers from significant performance degradation of 8% to 33% for large vocabulary sizes, since the output layer alone already has higher computation cost than that in the other pipeline devices. Its performance is also highly dependent on the model configuration, or more specifically, the ratio of compute between the vocabulary layer and transformer layers. For example, there is 9.7% drop in MFU when increasing the vocabulary size from 64k to 128k for the 10B model with sequence length 2048, but that is not observed with sequence length 4096. In contrast, the Vocab and Interlaced approaches have consistent MFU when scaling up the vocabulary sizes. Vocabulary Parallelism outperforms the interlaced pipeline under multi-node setup, due to its overlapped communication. For the 21B model, Vocabulary Parallelism outperforms the Balancing Pipeline Parallelism with Vocabulary Parallelism Figure 11. Throughput of different methods on 1F1B. Interlaced OOMs when training with 32 GPUs and sequence length 4096. Figure 12. Peak memory of different methods on 1F1B interlaced pipeline by 6.7% to 8.2% in MFU. For peak memory usage, the naıve implementation and layer redistribution approaches have an imbalanced parameter memory, leading to high peak memory for large vocabulary sizes. Although the Vocabulary Parallelism methods require extra activation memory, it is effectively negligible when we scale up the pipeline parallel size. However, the interlaced pipeline requires 1.5 times activation memory compared to 1F1B. This resulted in out-of-memory when training the 21B model with sequence length 4096. 6.4 Memory-Balanced Schedule We show that our method can achieve balanced memory usage by applying Vocab-1 on the V-Half schedule (Qi et al., 2024), memory-balanced schedule. The implementation is based on the open-sourced V-Half implementation by Qi et al. (2024). To support division into virtual pipeline chunks, we adopt different configurations in the experiments, as shown in Table 2. We compare the naıve V-Half schedule implementation and that incorporated with Vocab-1. The throughput and peak memory for each pipeline device are shown in Figures 13 and 14 respectively. The naıve implementation resulted in out-of-memory in cases with 32 GPUs and 256k vocabuTable 2. Settings used in experiments on V-Half schedule. PIPELINES (GPUS) 16 32 MODEL SIZE LAYERS ATTENTION HEADS HIDDEN SIZE SEQUENCE LENGTH MICROBATCH SIZE NUMBER OF MICROBATCHES VOCABULARY SIZE 64 48 6144 32 32 4096 7B 16B 30B 48 40 5120 2048 / 4096 1 128 32K / 64K / 128K / 256K lary size. Similar to the previous experiments, the baseline suffers from huge performance drop when we increase the vocabulary size, while Vocab-1 maintains steady MFU, consistently outperforming the baseline by 7.2% to 143%. Besides, the baseline has significant memory imbalance across pipeline devices with up to 45GB difference, while Vocab-1 achieves balanced memory usage across pipeline devices. Although the first pipeline device still holds slightly more parameters due to positional and token type embedding, the extra memory required is small constant. In our experiments, this is less than 2.5GB. Balancing Pipeline Parallelism with Vocabulary Parallelism Figure 13. Throughput of different methods on V-Half. Baseline OOMs when training with 32 GPUs and vocabulary size 256k. Figure 14. Peak memory of different methods on V-Half. The shaded area denotes the range of maximum allocated memory for all devices. 6.5 Scaling Analysis of Vocabulary Layers We analyze the scalability of vocabulary layers in our Vocabulary Parallelism implementation. Using vocabulary size of 256k, we measure the average throughput of all and passes across all devices in our implementation. We compare this with the ideal scenario where the vocabulary layers linearly scale (i.e. times of the original throughput when distributed to devices). The output layers for Vocab-1 and Vocab-2 are considered separately. The time used for the communications is not included since it overlaps with other computation. The results are shown in Table 3. Table 3. The scaling factor of vocabulary layer computation relative to linear scaling on sequence lengths 2048 and 4096. SEQ LAYER 8GPU 16GPU 32GPU 2048 4096 OUTPUT-VOCAB-1 OUTPUT-VOCAB-2 INPUT 91.29% 84.22% 80.59% 86.72% 79.84% 75.93% 39.99% 28.85% 15.18% OUTPUT-VOCAB-1 OUTPUT-VOCAB-2 INPUT 93.21% 88.02% 85.24% 88.36% 83.42% 79.66% 27.69% 15.52% 8.35% Parallelizing the vocabulary layers comes with some computation overhead, which can be attributed to two causes. Firstly, partitioning the vocabulary layers will reduce the model FLOPs utilization (MFU) of GPU kernels as the operations are smaller and hence less parallelized. Secondly, this brings extra computation, especially for the input layer where all devices have to construct the output tensor, whose size is independent of the size of the vocabulary partition. However, its worth noting that both input and output still only take small portion of the computation of the entire model after being partitioned."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we identified the problem that when training LLMs with pipeline parallelism, vocabulary layers causes non-negligible imbalance for both compute and memory. Existing methods either fails to achieve balance or introduce significant performance overhead to the original pipeline schedule. To address this issue, we proposed Vocabulary Parallelism, method that partitions vocabulary layers evenly to pipeline devices and integrates them into existing pipeline schedules. Our method achieves compute and memory balance for the vocabulary layers. As result, experiments shows that it improves the throughput by up to 51% while also reduces peak memory consumption Balancing Pipeline Parallelism with Vocabulary Parallelism compared to existing methods."
        },
        {
            "title": "REFERENCES",
            "content": "Although our implementation of the vocabulary layers are pure python-based, we find that similar optimizations to Algorithm 2 opens an opportunity of fusing the forward and backward pass in CUDA kernels to avoid writes/reads of the softmax results, which can be huge in long-context large-vocabulary settings, to main memory, similar to the rationale of FlashAttention (Dao et al., 2022). Also, while our work focuses on the imbalanced vocabulary layers for pure text-based LLMs, we believe the embedding layers for multimodal LLMs suffer from the same problem and can be further explored. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Harlap, A., Narayanan, D., Phanishayee, A., Seshadri, V., Devanur, N., Ganger, G., and Gibbons, P. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Kim, T., Kim, H., Yu, G.-I., and Chun, B.-G. Bpipe: Memory-balanced pipeline parallelism for training large language models. In International Conference on Machine Learning, pp. 1663916653. PMLR, 2023. Korthikanti, V. A., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023. Li, Z., Zhuang, S., Guo, S., Zhuo, D., Zhang, H., Song, D., and Stoica, I. Terapipe: Token-level pipeline parallelism In Internafor training large-scale language models. tional Conference on Machine Learning, pp. 65436552. PMLR, 2021. Lin, Z., Miao, Y., Zhang, Q., Yang, F., Zhu, Y., Li, C., Maleki, S., Cao, X., Shang, N., Yang, Y., Xu, W., Yang, M., Zhang, L., and Zhou, L. nnScaler: Constraint-Guided parallelization plan generation for deep learning training. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 347363, Santa Clara, CA, July 2024. USENIX Association. ISBN 9781-939133-40-3. Balancing Pipeline Parallelism with Vocabulary Parallelism Wei, T., Zhu, B., Zhao, L., Cheng, C., Li, B., Lu, W., Cheng, P., Zhang, J., Zhang, X., Zeng, L., et al. Skywork-moe: deep dive into training techniques for mixture-of-experts arXiv preprint arXiv:2406.06563, language models. 2024. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Milakov, M. and Gimelshein, N. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115, 2021. NVIDIA. GitHub - NVIDIA/nccl: Optimized primitives for collective multi-GPU communication https://github.com/NVIDIA/ github.com. nccl. [Accessed 31-10-2024]. Qi, P., Wan, X., Huang, G., and Lin, M. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023. Qi, P., Wan, X., Amar, N., and Lin, M. Pipeline pararXiv preprint allelism with controllable memory. arXiv:2405.15362, 2024."
        },
        {
            "title": "VOCABULARY LAYERS",
            "content": "Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. Tao, C., Liu, Q., Dou, L., Muennighoff, N., Wan, Z., Luo, P., Lin, M., and Wong, N. Scaling laws with vocabulary: Larger models deserve larger vocabularies. arXiv preprint arXiv:2407.13623, 2024. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Following the calculations of Narayanan et al. (2021) and neglecting insignificant terms, we present the computational and memory expenses in relation to single transformer layer in Table 4. In this table we denote the microbatch size as b, sequence length as s, hidden dimension as and vocabulary size as . It is worth noting that the activation memory is excluded from this analysis, as it typically has transient nature for vocabulary layers. Table 4. Compute and memory cost of vocabulary and transformer layers LAYER TYPE COMPUTE FLOPS TRANSFORMER INPUT OUTPUT bsh(72h + 12s) 3bsh 6bshV PARAM MEMORY 24h2 2hV 2hV"
        },
        {
            "title": "PIPELINE",
            "content": "B.1 Memory Analysis One concern of interlaced pipeline is that the peak activation memory of 1F1B is raised to 1.5 times of its original value. This increase in memory consumption can be analyzed using the framework introduced in Qi et al. (2024). As shown in Figure 15, the interlaced schedule enlarges the lifespan of 1F1Bs building block from 3p to approximately 4.5p where is the number of devices, resulting in 1.5x peak activation memory consumption. Balancing Pipeline Parallelism with Vocabulary Parallelism (a) Building block of 1F1B (b) Building block of interlaced pipeline parallel. The red vertical lines indicate synchronization introduced by TP of vocabulary layers. Figure 15. Comparison between building blocks of 1F1B and Interlaced PP. We schedule the input layer passes as follows: During the warm-up forward passes, we insert the input layer forward pass one microbatch before the first transformer layer forward pass. This allows time for gathering the input layer outputs. In the stable phase, we piggyback the input layer forward pass with the output layer passes, scheduled as least one repeating interval beforehand. Similarly, the input layer backward passes are piggybacked at least one repeating interval afterwards, allowing enough time to broadcast the output gradient to all devices. During the cool-down backward passes, we insert the input layer backward pass one microbatch after the last transformer layer backward pass. This schedule ensures that each device is holding the input layer outputs for at most two microbatches at any instant, reducing the memory pressure. B.2 Overhead of Tensor Parallel Communication V-Half PIPELINE SCHEDULING In practice, tensor parallel is only used for intra-node parallelism due to its high communication volume. The interlaced pipeline has introduced tensor parallel style parallelization for the vocabulary layers, which creates additional pipeline bubbles for each microbatch during the tensor parallel communication. To quantify the size of the bubbles, we conduct an ablation study by training 21.5B model using 32 GPUs. We remove the synchronous all-reduce communications in the vocabulary layers, and measure the speedup in end-to-end iteration time. Note that the all-reduce communications that are overlapped with the computation are still kept. By removing the synchronous all-reduce communications, the end-to-end iteration time improved by 10.95%. This shows that the synchronous all-reduce communications contributed to approximately 11% of the idle time when training using an interlaced pipeline. We conclude that the interlaced pipeline is undesirable for multi-node training."
        },
        {
            "title": "INPUT LAYER",
            "content": "While the output layers involve complex dependencies and communications, input layer computation can be completed independently before and after the transformer layer passes. The only required communications are an all-reduce communication after the forward pass, and broadcast communication before the backward pass. These communications can be overlapped with the transformer layer computation, and can be scheduled well-ahead or after. Following the scheduling methodology in section 5.2, we show the building block for the V-Half schedule in Figure 16. Figure 16. Modified building block for the V-Half schedule."
        },
        {
            "title": "E DETAILED EXPERIMENT DATA",
            "content": "For Sections 6.3 and 6.4, we present the detailed experimental data in Tables 5 and 6 respectively. The following metrics are computed: MFU: The FLOPs utilization of the training system. (2021)s derivation to We follow Narayanan et al. compute the FLOPs of the model. Peak Memory: The maximum peak memory across all devices. Balancing Pipeline Parallelism with Vocabulary Parallelism Table 5. Comparison of Methods on 1F1B. SETUP METHOD MFU (%) PEAK MEMORY (GB) 32K 64K 128K 256K 32K 64K 128K 256K 8GPU, SEQ LENGTH 2048 8GPU, SEQ LENGTH 4096 16GPU, SEQ LENGTH 2048 16GPU, SEQ LENGTH 4096 32GPU, SEQ LENGTH 32GPU, SEQ LENGTH 4096 BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED BASELINE REDIS VOCAB-1 VOCAB-2 INTERLACED 46.16 46.01 50.42 50.23 51.18 47.05 46.93 50.98 50.93 51.41 45.66 45.56 49.02 48.90 48.94 47.56 47.41 50.93 50.97 49.52 42.81 43.48 45.85 45.54 42. 43.68 44.01 46.41 46.23 - 40.48 46.37 50.28 50.18 50.94 41.87 46.78 50.98 50.75 51.82 40.09 42.82 50.62 50.49 48.97 41.21 43.07 50.97 50.80 49.53 37.28 37.29 45.92 45.86 42. 38.11 38.12 46.44 46.35 - 33.11 44.22 49.93 49.82 50.97 35.00 47.44 50.83 50.56 51.32 32.44 38.65 50.54 50.46 49.19 33.88 43.15 50.71 50.68 49.77 28.97 36.32 45.90 45.86 42. 30.05 37.87 46.68 46.55 - 25.23 38.91 50.12 49.69 50.92 26.75 43.01 50.66 50.40 51.38 24.21 36.98 50.66 50.46 49.52 25.33 40.15 51.22 50.90 49.84 20.86 29.16 46.11 46.16 43. 21.63 31.03 46.83 46.84 - 14.86 14.86 15.63 14.83 17.20 21.39 21.39 24.04 22.44 27.20 24.03 24.03 24.37 23.57 29.23 36.99 36.99 39.46 37.89 49.16 33.45 33.45 33.38 32.72 42. 54.97 54.97 57.41 56.09 - 16.32 16.32 16.02 15.23 17.57 22.85 22.85 24.47 22.89 27.64 25.98 25.98 24.63 23.83 29.47 38.94 38.94 39.73 38.18 49.44 35.89 35.89 33.55 32.88 43. 57.41 57.41 57.56 56.26 - 19.25 19.25 16.84 16.04 18.43 25.78 25.78 25.41 23.80 28.60 29.92 29.92 25.14 24.35 29.97 42.85 42.85 40.31 38.77 50.05 41.17 41.17 33.86 33.20 43. 62.29 62.29 57.88 56.61 - 25.64 25.64 18.59 17.78 20.17 31.64 31.64 27.34 25.73 30.53 38.71 38.71 26.26 25.47 31.10 50.90 50.90 41.53 39.92 51.28 52.16 52.16 34.51 33.84 44. 73.05 73.05 58.58 57.31 - Table 6. Comparison of Methods on V-Half. SETUP 16GPU, SEQ LENGTH 2048 16GPU, SEQ LENGTH 4096 24GPU, SEQ LENGTH 24GPU, SEQ LENGTH 4096 32GPU, SEQ LENGTH 2048 32GPU, SEQ LENGTH 4096 METHOD BASELINE VOCAB-1 BASELINE VOCABBASELINE VOCAB-1 BASELINE VOCAB-1 BASELINE VOCAB-1 BASELINE VOCAB-1 MFU (%) PEAK MEMORY (GB) 32K 64K 128K 256K 32K 64K 128K 256K 46.41 52.82 50.01 58.69 51.07 56.70 54.53 60. 52.80 57.70 56.06 60.10 38.52 53.11 41.17 58.56 43.13 56.50 45.96 60. 45.56 57.62 48.17 60.14 28.75 53.41 31.36 58.44 32.38 55.72 34.99 59. 35.69 57.69 37.85 60.72 19.99 52.89 21.90 57.59 22.54 54.86 24.31 58. - 57.80 - 59.82 15.57 13.20 21.22 20.14 23.94 21.08 33.60 32. 34.11 30.85 48.84 47.99 19.77 13.46 25.61 20.41 29.12 21.29 38.97 32. 40.28 31.04 55.19 48.19 28.55 13.98 34.56 20.96 39.98 21.72 49.90 33. 53.22 31.42 68.12 48.59 46.77 15.02 53.11 22.06 61.71 22.57 72.60 34. - 32.18 - 49."
        }
    ],
    "affiliations": []
}