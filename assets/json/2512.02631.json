{
    "paper_title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "authors": [
        "Zhengcheng Wang",
        "Zichuan Lin",
        "Yijun Yang",
        "Haobo Fu",
        "Deheng Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 1 3 6 2 0 . 2 1 5 2 : r SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization Zhengcheng Wang Zichuan Lin Yijun Yang Haobo Fu Deheng Ye Tencent AI Lab (cid:135) github.com/WzcTHU/SeeNav-Agent huggingface.co/wangzc9865/SeeNav-Agent {jensencwang,zichuanlin}@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agents understanding of current spatial states. Subsequently, novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL3B model reaches navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability. *Equal contribution. Preliminary work. Under review. In Vision-and-Language Navigation (VLN) [1, 5, 23], an agent is required to continuously interact with the environment based on the natural language instructions it receives, ultimately reaching the target location specified in the instructions. In each interaction step, the environment provides the agent with visual information of the current scene. The agent needs to analyze the current environmental image in combination with the received instruction and output an action plan for navigation within the environment. However, as Fig. 1 shows, VLN Agents based on Large VisionLanguage Models (LVLMs) often encounter errors at the levels of perception, reasoning and planning, which limit their navigation performance [22]. Visual hallucination is typical kind of perception error, where the agent may incorrectly claim to see an object that is not actually in its field of view, or fail to recognize an object that is present. Reasoning errors mainly include incorrect understandings of spatial relationships and improper reflections on environmental feedback. For example, when the target object appears on the left side of the input view, the agent may mistakenly believe that it is on the right side and decide to move right. As for planning errors, these mainly include generating invalid or infeasible actions, such as moving towards obstacles. Above categories of errors reveal significant limitations in the navigation capabilities of VLN Agents. To solve these limitations, some of the existing works attempt to add additional markers to the input images, such as obstacle masks or potential path indicators, to introduce more explicit visual cues and improve the perception ability of [8, 18]. Such technique is also known as vithe agent sual prompt (VP). However, although many works have employed various types of VP methods, there is still lack of research on how to effectively utilize multi-view image inputs through VP. In addition, existing VP studies are relatively fragmented at the methodological level, with lack of systematic analysis and experiments on how differnavigation process, thereby further enhancing the agents reasoning and planning abilities. Extensive comparative and ablation experiments on the EmbodiedBench Navigation benchmark demonstrate the effectiveness of the proposed SeeNav-Agent framwork. With SeeNav-Agent, GPT4.1+VP achieves success rate surpassing the current state-of-the-art (SOTA) closesource model by 20 pp in zero-shot manner, while Qwen2.5-VL-3B-Instruct+VP+SRGPO exceeds the previous SOTA close-source model by 5.6 pp. Furthermore, the proposed SRGPO exhibits superior convergence speed, training stability, and generalization ability. 2. Related Work Vision-and-Language Navigation (VLN). VLN is type of multimodal embodied task, where an agent, following human instructions (e.g., Navigate to the pot in the room), observes environmental images, assesses the current state, makes action decisions, and executes actions step by step in the environment to reach target. Traditional VLN pipelines treat image perception, map node segmentation, reasoning and action planning as independent modules, and the agent usually moves in discrete environment [3, 10, 13, 24]. Recent research based-on LVLMs has provided unified solution for VLN tasks. The LVLMsbased VLN agents use powerful multimodal models as the backbone, and fuse perception, reasoning, and planning in an end-to-end manner for continuous navigation [12, 26]. These works typically leverage closed-source models directly for navigation [8] or fine-tune open-source models through SFT/RFT before application [7, 15]. Visual Prompt (VP) for VLN Agents. The meaning of visual prompt (VP) is to add visual marker information such as bounding boxes, color blocks, and numerical marks to the input image, allowing LVLMs to better handle certain visual tasks [14, 17, 21]. Since the pre-training dataset of LVLMs contains large amount of VQA corpora, LVLMs are inherently more adept at VQA-style tasks. Through the VP method, planning-style tasks like VLN can be converted into VQA via zero-shot setting, allowing for better utilization of the LVLMs inherent capabilities. PIVOT [14] projects the sampled candidate actions on the input image of the agent as arrows with numbers, then enables the LVLM to select set of optimal actions through VQA, and finally iteratively optimizes the action distribution to provide the optimal navigation action. VLMnav [8] enhances the success rate of the VLN Agent by annotating walkable area masks and farthest reachable arrows on images. Similarly, it also enables the agent to select the optimal action from series of action projections for movement through prompts. However, the aforementioned works lack systemic research on how to effectively utilize multi-view inputs through VP and how to coordinate different VP modules to enhance perFigure 1. The key motivations. Left: Analysis for different navigation error types from LVLM-based VLN agents. Right: Examples of different error types. ent types of VP can complement and enhance each other. Besides, Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are also classic techniques used to enhance VLN agents [7, 12, 15]. However, traditional RFT approaches only utilize the sparse outcome reward, while process-based RFT methods often calculate action advantage with respect to the identical anchor state [6]. When the anchor state is hard to define, the computation becomes inefficient and the scalability is limited. How to overcome the limitations of the anchor state and develop an efficient and stable process-based RFT algorithm for navigation tasks remains challenging problem. To address above challenges, novel framework named SeeNav-Agent is introduced in this work. First, to reduce the hallucinations of the VLN agent and enhance its ability of spatial understanding, we introduce dual-view visual prompt method. By introducing VP, the probability of perception and reasoning errors can be reduced. In addition, by using an action projection mechanism within VP, the navigation task can be transformed from planning problem into Visual Question Answering (VQA) problem, thereby reducing the difficulty of the navigation task. Subsequently, we further enhance the agents reasoning and planning abilities through SFT and RFT. During RFT, verifiable process reward for each step of the navigation task is designed. We further propose the Step Reward Group Policy Optimization (SRGPO) algorithm, which utilizes the process reward signals by randomly grouping navigation steps and computing relative advantages. The main contribution of this work is summarized as: We propose dual-view visual prompt technique for navigation task, which significantly reduces the visual hallucinations and improves the spatial understanding capability of the VLN Agent with zero-shot setting. We design novel RFT algorithm, namely SRGPO, which efficiently integrates the step-level rewards of the formance, which will be discussed in this work. Reinforcement Fine-Tuning (RFT) for VLN Agents. RFT is commonly used post-training method for VLN Agents, which allows the agent to continuously interact with the environment, further enhancing its reasoning and planning performance. Proximal Policy Optimization (PPO) [25] and Group Relative Policy Optimization (GRPO) [7, 11, 15] are widely used RFT frameworks for LVLM Agents. However, the classic GRPO architecture can only handle sparse outcome rewards. For long-horizon navigation which often requires dozens of interaction steps to complete the task, it is quite challenging to perform optimization solely using the outcome signal that indicates whether the trajectory is successful. Existing work has verified that the introduction of process rewards during RFT can effectively enhance performance [4, 19, 20]. To incorporate step-level rewards into GRPO, GiGPO [6] adopts the idea of grouping identical steps. It rolls out multiple trajectories from the same initial state and groups identical states within these trajectories to calculate step-level advantages. However, in continuous navigation tasks, the definition of identical states is relatively strict. To ensure sufficient number of steps within each group, it is necessary to roll out as many trajectories as possible, resulting in large computational load. In this work, robust and verifiable process reward for navigation tasks is designed for obtaining steplevel reward signals. Meanwhile, we improve the GRPO framework and propose SRGPO, an RFT algorithm based on random step grouping, which efficiently addresses the issue in GiGPO that requires grouping of identical states. 3. Methodology 3.1. Framework and Problem Formulation In this section, we introduce novel LVLM-based embodied navigation framework named SeeNav-Agent. As shown in Fig. 2, SeeNav-Agent includes specifically designed dual-view visual prompt technique for the input to reduce perception hallucinations and to enhance the spatial understanding capability of the agent. During RFT, to overcome the limitations of previous RFT algorithms like GRPO and GiGPO, we propose novel algorithm named SRGPO that can efficiently utilize both episode-level and step-level navigation rewards in the advantage estimation stage. Before introducing the specific methods, we first define the relevant concepts of the LVLM-based VLN. In VLN, an embodied agent in 3D interactive environment needs to receive natural language instructions, and make series of movement decisions by observing and analyzing the images of the surrounding environment and the instruction information. The input state space of the VLN agent at time step for is defined as Ot = {I, S, Vt, Ht}, where is the human instruction; represents the action strategy given by humans, which is embedded in the input context; Vt denotes the environmental image observed by the agent at step and Ht is the action interaction history before step t. In this work, we use an action interaction history with time window as Ht = (atTH , ftTH , atTH +1, ftTH +1, . . . , at1, ft1), where at refers to the action at step t, and ft denotes the feedback from the environment for this action. Both at and ft are presented in text form. TH represents the time window for action history. The output of the agent in each step is (ct, at), where ct represents the analysis and thinking process about the current state, and at is the action decision chosen from the action space A. The LVLM-based VLN agent is parameterized as policy network πθ(atOt) with learnable parameters θ. 3.2. Dual-View Visual Prompt for VLN Agent Most existing VLN works tend to use only single perspective as the input for the agent, such as front view (FV) or birds-eye view (BEV). However, due to the poor ability of LVLM itself to understand spatial information like depth of field, the spatial relationships of objects in the FV are not as clear as those in the BEV for LVLM. But the problem with relying solely on the BEV is that the shapes of objects in it are often different from common representations, so an LVLM may fail to recognize target object in the BEV. Therefore, we consider enabling the agent to see both FV and BEV simultaneously. This dual-view (DV) input can enrich the visual information, but it also increases the difficulty for the agent to process and understand the images. If utilized inappropriately, the performance of the agent may not improve but instead decline [22]. Consequently, we have additionally designed series of visual prompt modules as follows to help the agent better understand the visual information from the DV input, and an example of VP modeuls is shown in Fig. 3. Bounding box. For navigation tasks such as moving toward target object, incorrect judgment of the targets existence is one of the most common visual hallucinations. The agent may claim to see an object that does not actually exist in the view, or ignore target that is clearly within its field of view, leading to errors in decision-making. Therefore, we enhance the existence information of the target by drawing bounding boxes (BB) around it in both FV and BEV, and also add relevant prompts to reduce hallucinations in judging the target existence. Navigation line. Inspired by the guidance trajectories in the minimaps of open-world games, we add navigation line (NL) arrows from the agent to the target. Since FV presents the first-person perspective of the agent, the starting point of the NL is set at the midpoint of the bottom edge. In BEV, however, the starting point of the NL is the agents center point. NL can enhance the agents understanding of the relFigure 2. Overview of SeeNav-Agent. Different from previous VLN works tend to use single view image as input and use method like GRPO or GiGPO for RFT, our SeeNav-Agent designs dual-view input with visual prompt to enhance the visual module in zero-shot manner, and proposes SRGPO to introduce process reward signals efficiently during the RFT stage by randomly grouping steps. Action projection. The main actions in navigation include movement in various directions and perspective rotation, which can be projected onto images. Through action projection (AP), the agent can observe the potential consequences to certain extent of executing specific action. Besides, it can also convert planning problem into VQA of selecting the optimal action on the image, reducing the difficulty of navigation. We draw movement actions in the BEV and draw rotation actions in the FV with arrows and action IDs, thus realizing the projection operation of the action space. View alignment. In BEV, the agent can face to various directions due to the randomness of the BEV orientation, which may lead the agent to mistakenly understand the leftright and front-back relationships between itself and objects (such errors may still occur even with the AM). Therefore, we rotate the BEV according to the agents orientation, ensuring that the agent always faces forward in the BEV, which means its viewing direction is consistent with FV. 3.3. Step Reward Group Policy Optimization While VP can mitigate perception error, further addressing reasoning and planning errors still requires post-training with RFT. Classic GRPO [16] learns from the outcome reward, however, for agentic problems like VLN that require long-sequence interactions with the environment, GRPO suffers from sparse reward signals, and how to assign credit to each step becomes challenging problem. Existing work Figure 3. Example of Dual-View Visual Prompt. The left part is the BEV, and the right part is the FV. The numbers 0 to 7 represent different action IDs. Yellow part of the agent marker indicates the left side, purple part indicates the right side, and the green arrow points to the front of the agent. ative position and distance between itself and the target. Agent marker. In navigation, enabling the agent to understand its position in space helps it plan the next action. Therefore, we use the agent marker (AM) represented by circle and an arrow in the BEV to indicate the position and orientation of the agent. Additionally, since the agent often makes mistakes in understanding left-right relationships, we mark the left and right sides of the circle with different colors, which enhances its comprehension of the relative positional relationship between itself and the target. like GiGPO [6] proposes step-level optimization method based on the identical anchor state. However, the definition of identical states is relatively strict, and for problems where the anchor state is hard to define, the computational burden will significantly increase. To solve this challenge, we propose SRGPO with verifiable process rewards, which leverages the unique properties of navigation tasks. The core insight of SRGPO is that if we can define process reward that is almost independent of the state, flexible grouping of steps and advantage estimation can be achieved. SRGPO overcomes the drawbacks of GRPO, which fails to consider process rewards, and the strict prerequisite of GiGPO, which requires grouping identical states, thereby achieving efficient utilization of process reward signals during the navigation process. Verifiable process reward for navigation. For navigation, whether the final result is successful or not is naturally clear outcome reward. However, for each step in the navigation process, the quality of each action can also be judged clearly. For example, if the current action can bring the agent closer to the target, or pull the target from outside the field of view back into the FV, it can be considered good action. We denote the agent position at time step as pt, and the target object as g. We denote the visible object set from FV at time step as Ft. We define the verifiable process reward (VPR) Rs for navigation as follows: Rs = Rs Rs t,base = t,valid, t,base λvalid Rs 1, 1, 0, otherwise, if dist(pt, g) < dist(pt1, g), else if 1{gFt} > 1{gFt1}, Rs t,valid = 1{at /A}. (1) (2) (3) level group can be organized as: GE = {(τ 1, R(τ 1)), (τ 2, R(τ 2)), . . . , (τ , R(τ ))}, (4) and the episode-level normalized advantage for each trajectory can be calculated by: AE(τ i) = R(τ i) mean({R(τ j)}N std({R(τ j)}N j=1) j=1) (5) Step-level advantages. The episode-level reward signals are relatively sparse, while RFT tends to benefit from denser reward signals. Therefore, we use verifiable process reward signals to calculate step-level advantages. Similar to GiGPO, calculating step-level advantages requires grouping steps first. However, unlike GiGPO, since the verifiable process rewards designed in Eq. (1) do not depend on specific states, we do not need to define the anchor states which describe state identity as in GiGPO. The core idea is that, through the definition of verifiable process rewards, we discard the prerequisite that to compare the quality of two actions, it must be done under the same state. Therefore, when calculating step-level advantages, we can change the concept of groups in the episode-level advantage calculation and instead randomly sample steps from the entire batch to form step groups. Assuming one batch contains tasks, we have trajectories, and the step-level groups with size NS can be defined as: GS = (cid:110) (c(i) , a(i) {1, . . . , B} , Rs(i) (cid:111) NS ) {1, . . . , Ti}, (6) where dist(pt, g) is the distance between the agent and the target at step t, and Ft means the target is in the FV at step t. Rs t,valid describes the penalty for invalid actions. The design of the process reward relies solely on the distance changes between the agent and the target for two consecutive steps, as well as the changes in the visibility of the target in the FV. The key insight of this reward design is that it is independent of the specific environmental state. Therefore, the quality of actions can be compared under any two different states, which also provides prerequisite for the subsequent design of the SRGPO algorithm. Episode-level advantages. We first introduce the traditional episode-level advantage calculation adopted in the vanilla GRPO. We roll out trajectories {τ i}N i=1 using the agent policy πθold for one task, where each trajectory τ = {(Oi,0, ai,1), (Oi,1, ai,2), . . . , (Oi,Ti1, ai,Ti)}. The outcome reward for each trajectory is defined as R(τ i). For navigation tasks, when the agent completes the task, the reward is 1; otherwise, the reward is 0. Then, the episodewhere and are randomly sampled from their valid range, and Ti represents the episode length of the i-th trajectory. With the step-level group, the step-level advantage for specific step within the group can be calculated by: AS(c(i) Rs(i) , a(i) ) = mean({Rs(j) (c(j) std({Rs(j) (c(j) , a(j) , a(j) , Rs(j) , Rs(j) ) GS}) ) GS}) (7) SRGPO training objective. The two levels of advantages are finally combined into unified advantage expression that can take into account both the step level and the trajectory level credit assignment: Ai,t = AE(τ i) + ω AS(c(i) , a(i) ), (8) where ω is balance coefficient. With the bi-level advantage, the policy optimization objective designed for SRGPO is shown in Eq. (9). op(O),{τ i}N i=1πθold JSRGPO(θ) = (cid:34) Ti(cid:88) (cid:88)"
        },
        {
            "title": "1\nN\nt=1\n− βDKL(πθ(·|o)||πref(·|o)),",
            "content": "i=1 min (ρi,t(θ)Ai,t, clip(ρi,t(θ), 1 ϵ)Ai,t) (cid:35) where ρi,t(θ) = πθ(a(i) πθold (a(i) ratio and β is the KL penalty coefficient. o(i)) o(i)) t is the importance sampling (9) 4. Experiments 4.1. Experimental Setup Benchmark. We evaluate the proposed SeeNav-Agent on the EmbodiedBench Navigation benchmark [22], which is an embodied navigation environment based on AI2-THOR [9]. This benchmark contains 60 unique in-room navigation tasks defined with unique initial robot position, target object information and language instruction. The robot should navigate to the target object according to the visual observations and textual feedback provided by the environment. The success of the task is determined by whether the distance between the robot and the target object is less than predefined value. The action space includes eight lowlevel actions with corresponding IDs: Move 0-forward / 1-backward / 2-right / 3-left by meters; Rotate to the 4-right / 5-left by θ degrees; Tilt the camera 6-upward / 7downward by ϕ degrees. The environment provides textual feedback on the execution result of each action, such as successful execution or execution failure caused by encountering an obstacle. This benchmark also provides series of test results on the success rates of existing opensource/Proprietary LVLMs in navigation tasks. See supplementary for more environmental implementation details. Baselines and metrics. To verify the effectiveness of the proposed SeeNav-Agent framework, we use three kinds of baselines: 1) close-source LVLMs, 2) open-source LVLMs and 3) post-training methods including SFT, GRPO and GiGPO. During post-training, we choose Qwen2.5-VL-3BInstruct [2] as our backbone model. We use the task success rate as the main metric to evaluate all methods. Implementation details. The penalty coefficients λvalid is set to 0.1 and the balance coefficient ω in Eq. (8) is 0.5. The rollout group size for GRPO, GiGPO and SRGPO is 4 and the step-level group size NS is 16. For in-distribution (i.d.) training setting, we conduct randomization on the 60 base scenes used by EmbodiedBench-Navigation, including randomly placing the movable objects, randomly setting the agents initial position and orientation, and randomly selecting navigation goals. For the out-of-distribution (o.o.d.) training setting, we select 60 new scenes from the AI2THOR scene library that are not used in EmbodiedBench for training, and apply the same randomization as described above. Throughout the entire RFT process, the VP is consistently embedded within the agent. The action history time window is set to TH = 5. The maximum number of interaction rounds with the environment is set to 20 and the maximum training epoch for RFT is 150 for i.d. and 100 for the o.o.d. setting. When performing SFT on the Qwen2.5VL-3B-Ins model, expert trajectories are generated by GPT4.1 on i.d. scenes. When constructing the dual-view input, we concatenate the BEV and FV, and feed them into the model as single image. 4.2. Main Results The main results are shown in Tab. 1. Compared with the close-source models, our VP technique surpasses the best Claude-3.5-Sonnet by 21.7 pp with GPT4.1 as the backbone. Compared with the open-source models, our method achieves 55.6 pp improvement on the base model Qwen2.5-VL-3B-Ins and surpasses the best InternVL3-78B by 5.6 pp. Moreover, our SRGPO significantly outperforms the post-training baselines including SFT, GRPO and GiGPO. These results fully demonstrate the effectiveness of the proposed SeeNav-Agent framework. 4.3. Ablation Study for Visual Prompt We conducted ablation experiments on each VP modules and the results are shown in Tab. 2. The results show that by introducing VP, 21.7 pp improvement in navigation success rate can be achieved with zero-shot manner. From the ablation results, it can be suggested that simply introducing dual-view (DV) input and view alignment (VA) cannot improve the success rate, instead, it may even lead to decline in the agents performance. The reason is that it is difficult for the agent to accurately understand and align the spatial information brought by the two different views. Furthermore, the results also indicate that each module in the VP recipe works collectively. It is necessary to include all VP modules simultaneously to achieve the optimal navigation success rate. Among these modules, bounding-boxes (BB), action projection (AP), and view alignment (VA) are relatively more important, which means removing any one of them will lead to significant drop in the success rate of the navigation task. Fig. 5 shows the differences in the thinking and acting processes of GPT4.1 before and after introducing VP modules when the agent loses sight of the target in FV. In this scenario, the agent is instructed to move to the Safe. The reasoning process of the vanilla GPT4.1 model exhibits hallucination, mistakenly believing that the Safe is still within its field of view. Consequently, it makes the decision to move forward, only to encounter an obstacle and fail in executing the action. In contrast, after applying VP, the agent can correctly detect that the target is out of sight. FurtherTable 1. Main Results. RFT cases are trained with i.d. settings Method Succ. rate Close-Source LVLMs Claude-3.7-Sonnet Claude-3.5-Sonnet GPT-4o GPT-4o-mini GPT4.1 Gemini-1.5-Pro Gemini-2.0-flash Qwen-VL-Max Open-Source LVLMs Llama-3.2-90B-Vision-Ins Llama-3.2-11B-Vision-Ins InternVL3-78B InternVL3-8B Qwen2.5-VL-72B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-3B-Ins Ovis2-34B gemma-3-27b-it 0.500 0.667 0.550 0.317 0.650 0.233 0.633 0.500 0.233 0.483 0.667 0.383 0.467 0.200 0.167 0.633 0.533 Post-Train baselines (backbone: Qwen2.5-VL-3B-Ins) VP+SFT VP+GRPO VP+GiGPO (vanilla) VP+GiGPO (w. VPR) 0.367 0.4080.008 0.2940.028 0.5720. SeeNav-Agent (ours) (backbone: Qwen2.5-VL-3B-Ins) 0.7230.008 (0.556) VP+SRGPO SeeNav-Agent (ours) (backbone: GPT4.1) 0.867 (0.217) GPT4.1+VP Table 2. Ablation Study for Visual Prompt. All combinations are based on GPT4.1 and comb.(A) is the FV only baseline. Comb. DV BB AP AM NL VA Succ. rate (A) (B) (C) (D) (E) (F) (G) (H) 0.650 0.450 0.650 0.583 0.683 0.800 0.800 0. Figure 4. Training Process of Qwen2.5-VL-3B-Instruct. The solid line and the shaded region represent the mean and standard deviation obtained from multiple training runs. 4.4. Effectiveness of SRGPO The training curves of the navigation success rate on i.d. training scenes for GRPO, GiGPO and SRGPO are shown in Fig. 4. We conduct three runs for each method. For GiGPO, we test both the step reward from the original paper VP+GiGPO(vanilla) and the verifiable step reward proposed in this work VP+GiGPO(w. VPR). As can be seen from the figure, VP+SRGPO significantly outperforms VP+GRPO and VP+GiGPO(vanilla) in both convergence speed and final training success rate. Besides, it also demonstrates the best training stability, as evidenced by the very small standard deviation of its training curves. However, when GiGPO adopts the verifiable step reward, it can converge to success rate close to that of SRGPO, but its convergence speed is still much slower. The reason is that althrough GiGPO is capable of grouping steps, its grouping efficiency is lower than that of SRGPO due to the constraint of identical states. In other words, the number of steps in each group is relatively small. After the introduction of the verifiable step reward, GiGPO becomes similar to SRGPO with small number of steps per group. Although VP+GiGPO(w. VPR) can achieve similar success rate to VP+SRGPO during training, VP+SRGPO demonstrates much faster convergence and better stability. Furthermore, to compare the navigation performance of agents after RFT, we evaluate the success rate on the testing scenarios from EmbodiedBench-Navigation using models saved at the last epoch. As shown in Tab. 1, the testing navigation success rate of VP+SRGPO is 14.5 pp higher than that of VP+GiGPO(w. VPR), and 31.7 pp higher than that of VP+GRPO, indicating that the agent trained with SRGPO possesses superior generalization performance. more, with the information from NL, the agent identifies that the target should be on its right side. Finally, among the projected actions, the agent selects action 4 for view rotation and brings the target back into its field of view. 4.5. Discussion of Hyperparameter for SRGPO The step-level group size NS is an important hyperparameter in SRGPO. If adopting small NS, the estimation of Figure 5. Case Comparison of vanilla-GPT 4.1 and GPT4.1 with VP. The bottom part shows the image and text feedback from the environment after taking actions. This case clearly shows that, with the dual-view VP technique, the visual hallucination can be reduced and the spatial understanding capability of the VLN agent can be enhanced. step-level advantage may become less accurate and less robust, which in turn negatively affects the generalization ability of the VLN agent. We tried reducing NS from 16 to 8 and ran SRGPO for three times. The resulting agents attain mean navigation success rate of 0.411 with standard deviation of 0.103 on the test set, which is notably worse than the performance achieved with NS = 16 as reported in Tab. 1. This phenomenon also explains why in Fig. 4 GiGPO with VPR can converge to level comparable to SRGPO in the training scenarios, but its testing success rate still lags far behind SRGPO. Notably, increasing NS does not require any additional rollout computation cost in SRGPO, which demonstrates more scalable advantage compared to GiGPO. 4.6. Out-Of-Domain Generalization Capability To further evaluate the generalization ability of the model trained with SRGPO, we switched the training scenes to o.o.d. setting. During the training process, the navigation success rates of models trained with different algorithms on the test set change as illustrated in Fig. 6. From the figure, it can be seen that the testing curve of SRGPO is significantly higher than those of the other three algorithms, indicating that even when trained in o.o.d. scenes, the model trained by SRGPO demonstrates stronger navigation capabilities than the other models. This also suggests that the training paradigm of SRGPO enables the model to acquire more general navigation knowledge, thereby validating the superior generalization ability of the proposed method. 5. Conclusion In this work, we propose SeeNav-Agent, novel LVLMbased embodied navigation framework that includes zeroFigure 6. Success Rate on Testing Scenes. All RFT algorithms are trained on o.o.d. scenes and tested on base scenes of the EmbodiedBench-Navigation. shot dual-view visual prompt technique for the input side and an efficient RFT algorithm named SRGPO for posttraining. With the dual-view VP modules, the visual hallucinations can be significantly reduced and the spatial understanding capability of the VLN Agent can be improved. Through systematic analysis and ablation experiments on different VP modules, we identified the optimal VP recipe, which enables the best alignment and utilization of information from dual-view inputs. Based on the defined verifiable process reward for navigation that is independent of the state, the proposed SRGPO algorithm can randomly group steps for advantage estimation and thereby efficiently utilize the step reward signals. Compared with GRPO and GiGPO, SRGPO not only addresses their inherent limitations, but also demonstrates significant advantages in training stability, convergence efficiency, and generalization ability. On EmbodiedBench-Navigation, GPT4.1+VP achieves success rate surpassing the previous SOTA close-source model by 20 pp, while VP+SRGPO exceeds the previous SOTA model by 5.6 pp, demonstrating the superiority of the SeeNav-Agent framework. In the future, we will evaluate the proposed method on more diverse and complex navigation benchmarks. Moreover, although this work focuses on the navigation task, we may also consider extending the proposed SRGPO to other tasks where state-independent process reward can be welldefined. In addition, designing additional perception modules to generate VP more efficiently is another promising direction for future research."
        },
        {
            "title": "References",
            "content": "[1] Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Multimodal map pre-training for language-guided navigation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [3] Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, and Kai Xu. Cognav: Cognitive process modeling for object goal navigation with llms. arXiv preprint arXiv:2412.10439, 2024. 2 [4] Alex James Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. Dense reward for free in reinforcement learning from human feedback. In International Conference on Machine Learning, 2024. 3 [5] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. In NeurIPS, 2021. 1 [6] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. 2, 3, [7] Chen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, and Si Liu. Octonav: ToarXiv preprint wards generalist embodied navigation. arXiv:2506.09839, 2025. 2, 3 [8] Dylan Goetting, Himanshu Gaurav Singh, and Antonio Loquercio. End-to-end navigation with vision-language models: Transforming spatial reasoning into question-answering. In Proceedings of the International Conference on Neurosymbolic Systems, pages 2235, 2025. 1, 2 [9] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 6 [10] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-Across-Room: Multilingual visionand-language navigation with dense spatiotemporal grounding. In Conference on Empirical Methods for Natural Language Processing (EMNLP), 2020. 2 [11] LinFeng Li, Jian Zhao, Yuan Xie, Xin Tan, and Xuelong Li. Compassnav: Steering from path imitation to decision understanding in navigation. arXiv preprint arXiv:2510.10154, 2025. 3 [12] Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025. [13] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving visionand-language navigation with image-text pairs from the web. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 2 [14] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Iterative visual promptWahid, Zhuo Xu, et al. Pivot: ing elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. 2 [15] Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, and Hengshuang Zhao. Vln-r1: Vision-language navigation via reinforcement fine-tuning. arXiv:2506.17221, 2025. 2, 3 [16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4 [17] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712, 2023. 2 [18] Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, and Chen Feng. Vlm see, robot do: Human demo video to robot action plan via vision language model. arXiv preprint arXiv:2410.08792, 2024. 1 [19] Hanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, and Wenjie Li. Spa-rl: Reinforcing llm agents via stepwise progress attribution. arXiv preprint arXiv:2505.20732, 2025. [20] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 3 [21] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 2 [22] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. 1, 3, 6 [23] Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, and Jiwen Lu. Sg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation. arXiv preprint arXiv:2410.08189, 2024. 1 [24] Hang Yin, Xiuwei Xu, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Unigoal: Towards universal zero-shot goaloriented navigation. arXiv preprint arXiv:2503.10630, 2025. [25] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3 [26] Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Empowering vision-language models with global-to-ego memory arXiv preprint for long-horizon embodied navigation. arXiv:2502.14254, 2025. 2 Mem2ego: et al. SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Experimental Details A.1. Parameters for EmbodiedBench-Navigation"
        },
        {
            "title": "Environment",
            "content": "The parameters for the EmbodiedBench-Navigation environment are set as ffollows, which are aligned with the original parameters used in the original Embodied-Bench paper [22]. These parameters are used in both training and testing environments. Visible Distance: 10m The maximum visible distance for objects perceived by the agent. Table 3. An Example of the Interaction History. Image Width: 500px The pixel width of the singleThe action history: <action info> Step 0, action id 0, Move forward by 0.25. <env feedback> Last action MoveAhead executed successfully. <action info> Step 1, action id 0, Move forward by 0.25. <env feedback> Last action MoveAhead executed successfully. <action info> Step 2, action id 0, Move forward by 0.25. <env feedback> Last action MoveAhead executed successfully. <action info> Step 3, action id 2, Move rightward by 0.25. <env feedback> Last action MoveRight is invalid. Cube.668 is blocking Agent 0 from moving by (0.0000, 0.0000, 0.2500). <action info> Step 4, action id 3, Move leftward by 0.25. <env feedback> Last action MoveLeft is invalid. Cube.690 is blocking Agent 0 from moving by (0.0000, 0.0000, -0.2500). view image. Image Height: 500px The pixel height of the singleview image. Field of View: 100 The field of view width visible from the agents first-person perspective. Move Magnitude: 0.25m The distance the agent moves one step forward, backward, left, or right. Rotate Degree: 90 The number of degrees the agent turns its view left or right each time. Tilt Degree: 90 The number of degrees the agent turns its view up or down each time. Min Distance: 2.5m The minimum distance between the agents starting position and the target. Max Distance: 3m The maximum distance between the agents starting position and the target. Success Threshold: 1m The distance threshold for determining whether the agent has successfully reached the vicinity of the target. A.2. Example of Language Prompts The language prompts used for the SeeNav-Agent with visual prompts in this work is shown in Tab. 4. In the prompt, <image>is placeholder for the input image, {user instruction} is the navigation command given by human, like Navigate to the Bread in the room and be as close as possible to it, and {history} represents the interaction history between the agent and the environment, which is list of natural language. An example of the interaction history is shown in Tab. 3. The maximum time window for the interaction history is set to 5 in this work, which means that the {history} part is limited to recording interaction information for the past five steps at most. When VP or certain modules of VP are not used in the experiments, we remove the prompts related to the corresponding modules from the original prompt. Table 4. Language Prompt Template for SeeNav-Agent with Visual Prompts. <image> ## You are robot operating in home. You can do various tasks and output sequence of actions to accomplish given task with images of your status. Your input is concatenation of your top-down view and the first-person view image, with the top-down image on the left and the first-person view on the right. The colored circle in the top-down view represents your current position, with the YELLOW side indicating your LEFT and the PURPLE side indicating your RIGHT. The GREEN arrow shows your current camera orientation. In the first person view and the overhead view, the red bounding box in both views highlights the object you need to navigate to. The candidate actions are shown in the overhead view and the first person view images, and each action is represented by blue arrow, with the corresponding action ID at the end of the arrow. Now the human instruction is: {user instruction}. ## The available action id (0 to 7) and action names are: action id 0: Move forward by 0.25, action id 1: Move backward by 0.25, action id 2: Move rightward by 0.25, action id 3: Move leftward by 0.25, action id 4: Rotate to the right by 90 degrees, action id 5: Rotate to the left by 90 degrees, action id 6: Tilt the camera upward by 30 degrees, action id 7: Tilt the camera downward by 30 degrees. Among these actions, actions 0-3 will only be annotated in the top-down view, while actions 4-7 will only be annotated in the first-person view. {history} Now your task is to determine which arrows action will help you reach the navigation target with red bounding box (if you encounter any obstacles, prioritize bypassing the obstacles currently blocking your way), and choose the optimal action id. *** Strategy *** To achieve the task, 1. Reason about the current visual state and your final goal, and 2. Reflect on the effect of previous actions. 3. Summarize how you learn from the Strategy. Aim for one action per step. At last, output the action id from the available actions to execute. 1. The action arrows on the images indicate the forward direction of the agent after performing the corresponding actions (actions 0-3), or the direction of the agents view rotation (actions 4-7). You can use this information to help decide which action to take. 2. Strategy2: When determining the relative left-right position between the target and the agent, do not simply look at whether the target is on the left or right side of the top-down view. Instead, you need to take the agents orientation into account (for example, when the agent is facing downward in the image, objects that appear to be on the left side in the image are actually on the agents right side). 3. Strategy3: The red bounding box marks your navigation target. Please pay special attention to whether there is corresponding red bounding box and red nevigation arrow in your FIRST-PERSON VIEW. Avoid mistakenly judging an existing box as absent, or assuming non-existent box is present. 4. Strategy4: There is red navigation arrow in the top-down view and the first-person view that point from the agent to the target, you can use this arrow to assist the current navigation task. If the red navigation arrow is not visible in the first-person view, it means that the target is not visible in the first-person view. 5. Strategy5: When you choose an action, please pay attention to the relationship between the action arrows in the top-down view and the red navigation arrow. Your movement direction should help shorten the red navigation arrow and align the green arrow with the red arrow. 6. Strategy6: If an invalid action has occurred in the action history, please do not perform this action again unless you have already performed rotation operation. 7. Strategy7: Based on the information in the images, determine which arrows action will best help you reach the navigation target with red bounding box (if you encounter any obstacles, prioritize bypassing the obstacles currently blocking your way), and choose the optimal action. 8. Strategy8: You should use rotation (action 4-5) or camera tilt (action 6-7) sparingly, only when you lose track of the target object and its NOT IN YOUR VIEW. If so, plan nothing but ONE ROTATION OR TILT at step until that object appears in your view. After the target object appears, start navigation and avoid using rotation until you lose sight of the target again. You are supposed to output in JSON. The output json format should be {visual state description: str, reasoning and reflection: str, language plan: str, executable plan: List[{action id: int, action name: str}]} The fields in above JSON follows the purpose below: 1. visual state description is for description of current state of the first-person view and the top-down view image, 2. reasoning and reflection is for summarizing the history of interactions and any available environmental feedback. Additionally, provide reasoning as to why the last action or plan failed and did not finish the task, and providing reasoning process of why you choose the current action, 3. language plan is for describing the best action you choose from all the action arrows, which is started by the step number and the action name, 4. executable plan is the best action you choose that having an action ID and name. 5. keep your plan efficient and concise."
        }
    ],
    "affiliations": [
        "Tencent AI Lab"
    ]
}