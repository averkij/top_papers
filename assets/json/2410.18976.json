{
    "paper_title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark",
    "authors": [
        "Sara Ghaboura",
        "Ahmed Heakl",
        "Omkar Thawakar",
        "Ali Alharthi",
        "Ines Riahi",
        "Abduljalil Saif",
        "Jorma Laaksonen",
        "Fahad S. Khan",
        "Salman Khan",
        "Rao M. Anwer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the best open-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark and evaluation scripts are open-sourced."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 6 7 9 8 1 . 0 1 4 2 : r CAMEL-Bench: Comprehensive Arabic LMM Benchmark Sara Ghaboura1* Ahmed Heakl1* Omkar Thawakar1 Ali Alharthi1 Ines Riahi2 Abduljalil Saif2 Jorma Laaksonen2 Fahad Shahbaz Khan1,3 Salman Khan1,4 Rao Muhammad Anwer1,2 1Mohamed bin Zayed University of AI, 2Aalto University, 3Link√∂ping University, 4Australian National University https://mbzuai-oryx.github.io/Camel-Bench/"
        },
        {
            "title": "Abstract",
            "content": "Recent years have witnessed significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly Englishcentric. In this work, we develop comprehensive LMM evaluation benchmark for the Arabic language to represent large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closedsource, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the best open-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.1 1. Introduction Figure 1. The proposed CAMEL-Bench covers eight diverse and challenging domains: multimodal understanding and reasoning, OCR and documents, charts and diagrams, videos, cultural-specific content, medical images, agricultural images, and remote sensing understanding in Arabic. CAMEL-Bench covers 38 sub-domains with over 29K questions carefully curated by native Arabic speakers to rigorously evaluate essential skills desired in Arabic LMMs. Large multimodal models (LMMs) have recently achieved significant advancements across broad spectrum of tasks, including visual reasoning, perception, and multimodal understanding. Closed-source models such as GPT-4V and open-source LMMs, such as LLaVA [28] have demonstrated effectiveness in tasks like image captioning [43], visual ques1* Equal Contributions tion answering (VQA) [24, 25], and complex visual reasoning [12]. These recent developments have led to the introduction of different benchmarks to evaluate the performance of open and closed-source LMMs. Despite these advances, most existing LMM benchmarks are English-centric, limiting their applicability to other languages [44]. With over 400 million speakers, Arabic is the 5th most widely spoken languages globally. In the context of large Figure 2. CAMEL-Bench examples spanning eight diverse domains, encompassing wide range of visual data types and tasks. language models (LLMs), there exist various attempts in developing Arabic LLMs [19, 48] which has also led to the introduction of Arabic LLM benchmarks [21]. In the context of LMMs, few recent works explore Arabic-centric evaluations in certain areas such as, scientific exams [13], cultural aspects [4, 46], Arabic question answers and documents [1, 34]. However, there is still lack of comprehensive and diverse Arabic LMM evaluation benchmark (see Tab. 1) to rigorously evaluate and study LMMs for Arabic. To address the aforementioned issue, we introduce the first comprehensive Arabic LMM evaluation benchmark, named CAMEL-Bench. CAMEL-Bench is designed to encompass wide range of tasks and focus on the Arabicspeaking population. It spans eight diverse domains and 38 sub-domains (see Fig. 1). The eight domains are: Multimodal understanding and reasoning, OCR and document understanding, chart and diagram understanding, video understanding, cultural-specific understanding, medical image understanding, Agricultural image understanding, and remote sensing understanding. Further, the 38 sub-domains (see Fig. 1) covered by our CAMEL-Bench are: visual understanding and reasoning, object hallucination evaluation, math and logic reasoning, scientific reasoning, VQA, infographics VQA, complex visual perception, real-world spatial understanding, multi-image understanding, objectlevel perception, newsletter, powerpoint slides, scene text, handwriting, lines, books, documents, charts, diagrams, tables, general video scenes, cultural-specific occasions, countries and landmarks in videos, countries and landmarks in images, food, celebrities, cultural VQA, basic medical science, clinical medicine, public health, pharmacy, diagnosis, medical understanding, plant types, fruit and veggies idenDomain/Characteristics Exams-V* CVQA* Henna KHATT CAMEL-Bench Multimodal Und. & Reasoning OCR & Docs Und. Charts & Diagrams Und. Video Und. Medical Image Und. Agricultural Image Und. Remote-Sensing Und. Cultural-Specific Und. Open Source Question Numbers 823 200 1.1K 5K (ours) 29K Table 1. Comparison of our CAMEL-Bench with existing Arabic LMM benchmarks: Exams-V [13], CVQA [46], Henna[4], and KHATT [34]. Here * denotes that only Arabic part of benchmark is counted. tification, plant illness, and geospatial imagery subdomains (land, transportation and construction). Our CAMEL-Bench comprises 29,036 questions (see Fig. 2) and follows an extensive manual verification process by native-speakers to ensure the resulting benchmark is of high-quality. We conduct extensive experiments using open and closed-source LMMs. Our results reveal the need for substantial improvement in handling of Arabic multimodal data, shedding light on the areas requiring further Arabic LMM improvements. 2. CAMEL-Bench 2.1. Data Collection Our dataset encompasses eight diverse domains to ensure versatile multi-task Arabic LMM benchmark for different real-world scenarios. Each domain is further sub-divided into different sub-domains, each focusing on distinct aspect. During the data collection process, we either utilize Domains Multimodal Understanding and Reasoning OCR and Document Understanding Chart and Diagram Understanding Video Understanding Cultural Specific Understanding Medical Imaging Understanding Agricultural Image Understanding Remote Sensing Understanding Total Source CountBench, MMT-Bench-MI, POPE MathVista ScienceQA-IMG, Exams-V GQA, VizWiz, VQAv2 AI-Generated (GPT-4o), Pinterest BLINK Sub-Domains Visual Understanding/ Reasoning MME, MMBench, MMT-Bench-MI, SEED, MMMU Object Hallucination Evaluation Math and Logic Reasoning Scientific Reasoning Visual Question Answering InforGrahpics VQA Complex Visual Perception Real-world Spatial Understanding RealWorldQA Multi-image Understanding Object-level Perception Scanned Documents (OCR) Scanned Documents (VQA) Scene Text (OCR) Books (OCR) PowerPoint Slides (OCR) PowerPoint Slides (VQA) Handwriting (OCR) Newsletters (OCR) Lines (OCR) Charts Diagrams Understanding Tables Countries/ Landmarks Cultural-Specific Occasions General Video Scenes Celebrities Food Countries/ Landmarks Basic Medical Science Clinical Medicine Public Health Pharmacy Diagnosis Medical Understanding Agriculture Image Understanding AgroGPT GeoChat Remote Sensing Understanding MMT-Bench-MI, MuirBench COCO, ImageNet, Mocheg, Snli-Ve ArabicDatasetOCR MTVQA EvArEST Historical Arabic Handwritten Text Recognition Dataset ISI-PPT-Dataset ISI-PPT-Dataset KHATT Line PATD PATS-01 ChartQA MMMU (diagrams), ICON-QA, AI-Generated, Pinterest, BCE-Arabic BCE-Arabic, Excel Pexel Pexel Video-MME arab-celeb-dataset arabic-food-101, Pexel Pexel MMMU, MMMU Pro MMMU, MMMU Pro MMMU, MMMU Pro MMMU, MMMU Pro MMMU, MMMU Pro MMT-MI-Bench Number of Questions 3,971 997 531 1,624 3,840 120 1,422 624 1,062 60 480 703 1,217 40 2,354 711 1,400 506 520 745 1,994 81 87 24 654 444 347 494 89 83 87 82 87 78 769 709 29,036 Table 2. Different data sources used for 38 sub-domains corresponding to eight domains, with around 29k questions in total. The different data sources include: MME [15], MMBench [30], MMT-Bench-MI [56], SEED [23], MMMU [58], MMMU-Pro [60], CountBench [39], POPE [26], MathVista [33], Exams-V (Arabic portion) [13], ScienceQA-IMG [32], GQA [20], VizWiz [10], VQAv2 [17], BLINK [16], MuirBench [50], COCO [27], Imagenet [14], Mocheg [55], Snli-Ve [54], Pinterest [42], RealWorldQA [53], PATS-01 [3], KHATT [34], PATD [40], Historical Arabic Handwritten Text Recognition Dataset [37], ISI-PPT-Dataset [52], EvArEST [18], MTVQA [49], ChartQA [35], IconQA [31], BEC-Arabic [47], Claude-3.5 [5], arab-celeb-dataset [36], arabic-food-101 [6], Countries and landmarks [41, 51, 57], Pexel [41], AgroGPT [7], GeoChat [22]. These data sources are carefully translated and verified to ensure quality and relevance. available Arabic multimodal data samples or employ samples from existing English-centric LMM benchmarks. These English samples are then translated to Arabic via GPT-4o and verified. Alternatively, we manually collect and generate the Arabic samples for remaining sub-domains from internet. Tab. 2 presents the details of different data sources used for data collection for the 38 sub-domains corresponding to eight domains, with around 29k questions in total. 2.2. Question-Answers Pairs Generation We note that major part of our original Arabic data is not derived from ready-made VQA datasets. Some sub-domains, such as celebrities and food, consist of image-only data, while others, like Pexels countries and landmarks, contain image-caption pairs. To create rich and diverse VQA corpus, we first ensure that each image is accompanied by detailed contextual information. This context is sourced from combination of Wikipedia (e.g., for food-related data), manual curation (e.g., for countries and landmarks in videos), and AI-generated content based on manually provided context (e.g., for diagrams and infographics). Next, we generate multiple-choice questions (MCQs) for each sample using the GPT-4o model. The prompt is meticulously crafted to adhere to key criteria: each sample generates three multiple-choice questions (MCQs), with four distinct, non-synonymous options per question, only one of which is correct. The questions contain no embedded hints, ensuring that answers are derived exclusively from the image, without requiring prior knowledge. Additionally, the image must provide enough information to fully support the correct answer, eliminating the need for guesswork. In total, this process produces corpus of 4.4K generated questions with 17.7K answers, enabling comprehensive set of questions for evaluation. Figure 3. The CAMEL-Bench Filtering and Verification Pipeline consists of two paths: Original Arabic and translated Arabic. For original Arabic (top row), 20% random sample undergoes manual verification; if errors are below 40%, the data passes; otherwise, the entire sub-category is reviewed. For Translated Arabic (bottom row), We employ Qwen7B model [8] to assess semantic similarity between the original and translated question-answer pairs on fuzzy-basis evaluation. Pairs passing the evaluation proceed, while those that fail undergo manual review. Based on this, data may require Manual Handling for manual re-translation, Refine & Verify for refinement through the model, or Non-Translated Review where the data is re-sent for translation due to the absence of an Arabic version. 2.3. Data Filtering and Verification 3. CAMEL-Bench Benchmark Evaluation The data collection and question-answer pair generation process lead to over 41k questions in total which then undergoes to filtering and verification process. The CAMEL-Bench filtering and verification process (see Fig. 3) is carefully conducted based on whether the QA text is originally Arabic or translated into Arabic from English language. For all sub-domains derived from original Arabic context, we take 20% randomly sampled subset for manual verification. In case if the error remains less below 40% threshold, the sub-category is accepted into CAMEL-Bench. Alternatively, the entire sub-category undergoes manual review. In case of the translated Arabic data from English, the original English context is also incorporated into the filtering and verification process. Here, Qwen7B [8] is used to compare the semantic similarity between the English and the English-translated data at the QA-pair level using fuzzy evaluation. To ensure the model understands semantic similarity in Arabic, we provided 5 few-shots prompting. Subsequently, QA-pairs rejected by Qwen7B [8] are manually reviewed, resulting in one of three outcomes. Manual Handling implying that data requires full re-translation. Refine and Verify referring that the translation can be refined using the model. Non-Translated Review implying that the non-translated data is re-sent to the model for translation. Consequently, we obtain 29,036 high-quality questions. Evaluation Metrics: Our evaluation framework is designed with three specialized metrics, each carefully aligned to different types of datasets and tasks. For MCQ datasets like MMT [56] and MMMU [58], we utilize exact match accuracy to ensure precise evaluation. For optical character recognition (OCR) datasets, such as PATS [3] and Evarest [18], where accurate text extraction is critical, we adopt edit distance [45] as the key metric. For more flexible datasets like VQAv2 [17], MathVista [33], and GeoChat [22], where multiple synonymous answers can be considered correct. we implement fuzzy evaluation method for all such datasets. This approach uses GPT-4o to compare the predicted answer with the ground truth, while accounting for the context of the question. By incorporating these diverse metrics, our evaluation provides robust and comprehensive assessment that adapts to the unique demands and response formats of each dataset. Tab. 3 presents comparative evaluation of five different models on range of multimodal (MM) understanding tasks, each assessing the capabilities of the models in distinct domains. The models include GPT-4o, GPT-4o-mini, Gemini1.5-Pro, Gemini-1.5-Flash, and Qwen2-VL-2B, evaluated on key tasks such as multimodal reasoning, OCR & document understanding, chart & diagram interpretation, video analysis, and several domain-specific tasks like cultural understanding, medical imaging, agricultural (agro) understanding, and remote sensing. GPT-4o excels across tasks, leading in MM reasoning (57.90), chart/diagram understanding (73.57), Method GPT-4o GPT-4o-mini Gemini-1.5-Pro Gemini-1.5-Flash Pangea-7B Qwen2-VL-2B InternVL2-8B LLaVa-NeXt-7B MM Understanding OCR & Document Charts & Diagram Understanding Understanding & Reasoning Video Cultural Specific Medical Understanding Understanding Remote Sensing Imaging Specific Understanding Agro 57.90 48.82 46.67 45.58 40.09 40.59 30.41 26. 59.11 42.89 36.59 33.59 26.47 25.68 15.91 19.12 73.57 64.98 47.06 48.25 38.87 27.83 30.27 27.56 74.27 68.11 42.94 53.31 49.01 38.90 51.42 44.90 80.86 65.92 56.24 46.54 20.34 34.27 20.88 28.30 49.90 47.37 33.77 42.86 31.99 29.12 29.48 22.54 80.75 79.58 72.12 76.06 74.51 52.02 44.47 42. 22.85 16.93 17.07 14.95 6.67 12.56 5.36 8.33 Table 3. Performance comparison of different closed-and open-source LMMs on CAMEL-Bench. We present per-domain results of seven LMMs: GPT-4o [38], GPT-4o-mini [38], Gemini-1.5-Pro [2], Gemini-1.5-Flash [2], Pangea-7B [59], Qwen2-VL [9], InternVL28B [11], and LLaVaNeXt-7B [29]. GPT-4o excels in most domains, while GPT-4o-mini offers an impressive balance of performance and model size. All models struggle with remote sensing, medical imaging, OCR & document understanding, and general multimodal understanding and reasoning domains. Open-source models like InternVL2-8B and LLaVaNeXt-7B show decline in performance across domains, with their best results in video understanding. Figure 4. Qualitative example highlighting different scenarios where different closed-weight models struggle on CAMEL-Bench. The correct response is shown in green, and the incorrect one in the red box. Figure 5. Qualitative example highlighting different scenarios where different open-weight models struggle on CAMEL-Bench. The correct response is shown in green, and the incorrect one in the red box. video analysis (74.27), cultural (80.86) and agro-specific understanding (80.75). Models perform well on MCQs and binary-option tasks due to guessing probability and context. Infographics, designed for easy interpretation, also see high accuracy across all models. In contrast, Arabic OCR tasks, particularly in datasets like Khatt, historical documents prove exceptionally challenging. This difficulty stems from the complex nature of Arabic script, which uses ligatures and diacritics (small markings that alter pronunciation and meaning). Remote sensing understanding also remains difficult, with scores like 22.85 (GPT-4o) and 16.93 (Qwen2-VL-2B), highlighting the complexities of interpreting satellite imagery. Among the open-source models evaluated on our Arabic multimodal benchmark, Pangea-7B stands out by outperforming InternVL2-8B and LLaVaNeXt-7B in key areas. It achieves higher scores in multimodal understanding and reasoning (40.09), OCR and document understanding (26.47), and charts and diagram understanding (38.87). This suggests that Pangea-7Bs multilingual and culturally diverse training data enhance its ability to handle complex tasks across different languages and cultures. However, similar to other open-source models, Pangea-7B struggles in remote sensing understanding, scoring 6.67, highlighting challenges with specialized tasks. Overall, Pangea-7Bs performance underscores the benefits of incorporating diverse linguistic and cultural data in training multilingual multimodal LLMs while indicating areas for improvement. The Fig. 4 and Fig. 5 highlight critical challenge in Arabic multimodal understanding, where all models fail to accurately interpret the linguistic context in the provided CAMEL-Bench samples. This underscores the complexity of Arabic linguistics, especially in multimodal tasks, and the need for more robust language models that can effectively integrate both visual and textual information in Arabic contexts. 4. Conclusion, Limitations and Societal Impact We present comprehensive and diverse benchmark, named CAMEL-Bench, for Arabic LMM evaluation. To the best of our knowledge, CAMEL-Bench is the first comprehensive Arabic LMM benchmark comprising eight diverse domains and 38 sub-domains with around 29k questions that are filtered from larger pool of 41k samples with the quality verified by native speakers. We conduct extensive evaluations of openand closed-source LMMs, highlighting the need for substantial improvements in different areas for future Arabic LMM development. Although our CAMEL-Bench strives to significantly contribute towards developing sophisticated Arabic LMMs, we note that it mainly covers modern standard Arabic and does not fully explore other Arabic dialects. As the data samples are either based on existing datasets or new data that is crawled from the internet, it is possible that CAMEL-Bench exhibits biases already existing in the benchmarks. Nevertheless, we believe CAMEL-Bench is step towards the inclusion of Arabic language and Arabicspeaking populations in accessing the benefits of LMMs."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely thank Farah Husain Salem Abdullah Alharth for her valuable contributions to the manual data verification process."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, and Adam Jatowt. Arabicaqa: comprehensive In Proceedings of dataset for arabic question answering. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 20492059, 2024. 2 [2] Google AI. Gemini: family of highly capable multimodal models, 2023. 5 [3] Husni Al-Muhtaseb. Arabic text recognition of printed manuscripts. Efficient recognition of off-line printed Arabic text using Hidden Markov Models, Bigram Statistical Language Model, and post-processing. PhD thesis, University of Bradford, 2010. 3, 4 [4] Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, and Muhammad Abdul-Mageed. Peacock: family of arabic multimodal large language models and benchmarks. arXiv preprint arXiv:2403.01031, 2024. 2 [5] Anthropic. Claude, 2024. AI assistant. 3 [6] Arar Tawil. Arabic food 101. https://www.kaggle.com/ datasets/araraltawil/arabic-food-101, 2023. 3 [7] Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, and Rao M. Anwer. Agrogpt: Efficient agricultural vision-language model with expert tuning. arXiv, 2024. [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 4 [9] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 5 [10] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: In Proceednearly real-time answers to visual questions. ings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. 3 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 5 [12] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In ICML, 2021. 1 [13] Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024. 2, 3 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255. Ieee, 2009. 3 [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3 [16] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 3 [17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 69046913, 2017. 3, 4 [18] Heba Hassan, Ahmed El-Mahdy, and Mohamed Hussein. Arabic scene text recognition in the deep learning era: Analysis on novel dataset. IEEE Access, 2021. 3, 4 [19] Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. Acegpt, localizing large language models in arabic, 2023. [20] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 67006709, 2019. 3 [21] Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, et al. Arabicmmlu: Assessing massive multitask language understanding in arabic. arXiv preprint arXiv:2402.12840, 2024. 2 [22] Kartik Kuckreja, Muhammad S. Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad S. Khan. Geochat: Grounded large vision-language model for remote sensing. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 4 [23] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In Proceedings of the International Conference on Machine Learning (ICML), pages 1288812900. PMLR, 2022. 1 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning (ICML), pages 1973019742. PMLR, 2023. 1 [26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 3 [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740755. Springer, 2014. 3 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1 [29] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. [31] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021. 3 [32] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 3 [33] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 3, 4 [34] Sabri Mahmoud, Irfan Ahmad, Wasfi Al-Khatib, Mohammad Alshayeb, Mohammad Tanvir Parvez, Volker M√§rgner, and Gernot Fink. Khatt: An open arabic offline handwritten text database. Pattern Recognition, 47(3):10961112, 2014. 2, 3 [35] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 3 [36] Mohammad-Alfaifi. Github - mohammad-alfaifi/arab-celebdataset. https://github.com/mohammad-alfaifi/arabceleb-dataset, n.d. Accessed: 2024-10-15. [37] Rayyan Najam and Safiullah Faizullah. Historical arabic handwritten text recognition dataset, 2024. 3 [38] OpenAI. Gpt-4o model. https://openai.com, 2024. Accessed: 2024-10-14. 5 [39] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Con- [53] xAI. xai. grok-1.5 vision preview. https://x.ai/blog/ grok-1.5v, 2024. 3 [54] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706, 2019. 3 [55] Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multimodal fact-checking and explanation generation: challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27332743, 2023. 3 [56] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, jiayi lei, Quanfeng Lu, Peng Gao, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. MMT-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 3, 4 [57] YouTube. https://www.youtube.com/, 2024. Accessed: 2024-10-01. [58] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 4 [59] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. arXiv preprint arXiv:2410.16153, 2024. 5 [60] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmupro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 3 ference on Computer Vision (ICCV), pages 31703180, 2023. 3 [40] PATD. Printed arabic text database for recognition systems. http://www.inf.u-szeged.hu/patd/. 3 [41] Pexel. Pexel: The best free stock photos, royalty-free images and videos shared by creators. https://www.pexels.com/. [42] Pinterest. Pinterest platform. https://www.pinterest. com/. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 87488763. PMLR, 2021. 1 [44] Hanoona Rasheed, Muhammad Maaz, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, and Fahad S. Khan. Palo: large multilingual multimodal language model. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. 1 [45] Eric Sven Ristad and Peter Yianilos. Learning string-edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522532, 1998. 4 [46] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. arXiv preprint arXiv:2406.05967, 2024. 2 [47] Rana SM Saad, Randa Elanwar, NS Abdel Kader, Samia Mashali, and Margrit Betke. Bce-arabic-v1 dataset: Towards interpreting arabic document images for people with visual impairments. In Proceedings of the 9th ACM International Conference on Pervasive Technologies Related to Assistive Environments, pages 18, 2016. [48] Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, et al. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. arXiv preprint arXiv:2308.16149, 2023. 2 [49] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. Mtvqa: Benchmarking multilingual text-centric visual question answering, 2024. 3 [50] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 3 [51] Wikipedia. Wikipedia the free encyclopedia. https://www. wikipedia.org/. 3 [52] Yue Wu and Prem Natarajan. Self-organized text detection with minimal post-processing via border learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017. A. Appendix B. More on Dataset Curation The dataset utilized in this work was carefully curated with rigorous focus on data quality, relevance, and diversity. Our curation process involved selecting multimodal data from various domains, including images, text, videos, and specialized fields such as medical imaging, agriculture, and remote sensing. To ensure the integrity and accuracy of the dataset, we employed multiple stages of data verification. This process involved cross-validation, thorough verification procedures for Arabic content, and the integration of standardized data sources where applicable. C. Dataset Overview and Task Splits This section provides comprehensive breakdown of the datasets used across eight distinct categories, illustrating the diversity and depth of our evaluation framework. Each category is further divided into sub-domains, ensuring that the multimodal models are rigorously tested on wide range of tasks and datasets. This structure guarantees comprehensive coverage and introduces varied challenges to thoroughly assess model performance. Refer to Tab. 2 for detailed breakdown of the data categories with their statistics. C.1. Multimodal Understanding and Reasoning This category encompasses various sub-domains such as visual understanding, object hallucination evaluation, and complex visual perception. Key datasets include MME, MMBench, ScienceQA-IMG, and VQA2. These datasets test the models ability to handle intricate reasoning tasks across both visual and textual inputs, with total of 3,971 questions under the visual understanding sub-domain, and significant representation from other tasks like scientific reasoning (1,624 questions) and object-level perception (60 questions). C.2. OCR and Document Understanding Document understanding covers scanned documents, scene understanding, text extraction, and more. This category emphasizes precise OCR and textual recognition from images and scanned materials. Datasets like ArabicDatasetOCR and ISI-PPT-Dataset challenge the model to process diverse range of document types. substantial number of questions come from Handwritten Text datasets (1,400 questions) and PPT OCR (2,354 questions), ensuring the model is evaluated across both structured and unstructured document types. C.3. Chart and Diagram Understanding In chart and diagram interpretation, models are tested on understanding visual representations of data, such as charts, diagrams, and tables. This includes datasets like ChartQA, MMMU, and BCE-Arabic. The evaluation focuses on tasks such as understanding diagrammatic reasoning and tabular data with 1,994 questions from diagram datasets and 745 questions involving charts, providing robust examination of the models ability to interpret visual data efficiently. C.4. Video Understanding This category assesses the models ability to process and comprehend video data, focusing on tasks like recognizing countries, landmarks, and occasions. Video-MME is prominent dataset, contributing 654 questions to the evaluation. The inclusion of diverse sub-domains, such as recognizing cultural aspects through video, highlights the importance of temporal and visual information synthesis in multimodal reasoning. C.5. Cultural Specific Understanding The cultural understanding domain tests the models capacity to handle tasks specific to certain cultures, including food, landmarks, and celebrities. Datasets like arabic-food-101 and Pexel challenge the model to recognize culturally significant items, with 444 questions focused on celebrities and 494 on countries/landmarks. These tasks highlight the models ability to adapt and generalize across different cultural contexts. C.6. Medical Imaging Covering range of sub-domains in the medical field, this category includes tasks related to basic medical science, clinical medicine, and public health, using datasets like MMMU and MMT-MI-Bench. These datasets assess the models potential in specialized medical contexts, with over 1,200 questions spanning across diagnosis, medical understanding, and pharmacy, ensuring rigorous evaluation of the models performance in handling critical medical information. C.7. Agricultural Image Understanding The agricultural domain is represented through datasets like AgroGPT, with 769 questions focused on agricultural understanding tasks. These tasks test the models capacity to process and interpret images related to agricultural settings, reinforcing the models ability to work with real-world scenarios in agriculture and environment-based challenges. C.8. Remote Sensing Understanding This category evaluates the models ability to handle remote sensing data, specifically focusing on geographical data interpretation through datasets like GeoData VQA and GeoChat. With 709 questions in this domain, the model is tested on its spatial reasoning and understanding of complex remote-sensing imagery, crucial for applications in fields like environmental monitoring and geography. In total, the dataset includes 29,036 questions across all categories, providing comprehensive and diverse benchmark for evaluating the multimodal models performance across wide spectrum of tasks. This balanced distribution ensures that the model is tested thoroughly, with each domain offering unique challenges and insights into the models strengths and areas for improvement. D. CAMEL-Bench Data Samples Fig. 2 showcases CAMEL-Benchs versatility across eight distinct domains, covering tasks like Multimodal Reasoning, OCR & Document Understanding, Chart & Diagram Interpretation, Video Scene Analysis, and more specialized areas like Remote Sensing, Agricultural Image Analysis, Medical Image Interpretation, and Cultural-Specific Knowledge. Each domain presents unique challenges, from logical reasoning and handwritten text recognition to medical diagnostics and cultural symbol identification. This variety emphasizes CAMEL-Benchs strength in supporting the development of AI systems capable of addressing real-world applications in healthcare, agriculture, geospatial analysis, and cross-cultural contexts."
        }
    ],
    "affiliations": [
        "Aalto University",
        "Australian National University",
        "Link√∂ping University",
        "Mohamed bin Zayed University of AI"
    ]
}