{
    "paper_title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "authors": [
        "Wei Fu",
        "Jiaxuan Gao",
        "Xujie Shen",
        "Chen Zhu",
        "Zhiyu Mei",
        "Chuyi He",
        "Shusheng Xu",
        "Guo Wei",
        "Jun Mei",
        "Jiashu Wang",
        "Tongkai Yang",
        "Binhang Yuan",
        "Yi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \\emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \\textbf{up to 2.57$\\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 8 9 2 4 2 . 5 0 5 2 : r AREAL: Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning Wei Fu12, Jiaxuan Gao12, Xujie Shen2, Chen Zhu2, Zhiyu Mei12, Chuyi He2, Shusheng Xu12, Guo Wei2, Jun Mei2, Jiashu Wang23, Tongkai Yang2, Binhang Yuan3, Yi Wu12 1 IIIS, Tsinghua University, 2 Ant Research, 3 HKUST fuwth17@gmail.com, jxwuyi@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AREAL, fully asynchronous RL system that completely decouples generation from training. Rollout workers in AREAL continuously generate new outputs without waiting, while training workers update the model whenever batch of data is collected. AREAL also incorporates collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AREAL balances the workload of rollout and training workers to control data staleness, and adopts staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AREAL achieves up to 2.57 training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AREAL is available at https://github.com/inclusionAI/AReaL/."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has been new scaling paradigm for enhancing the capabilities of large language models (LLMs) by enabling thinking abilities [53]. Given prompt, RL allows an LLM to generate thinking tokens before outputting final answer, enabling test-time scaling [30, 48]. These thinking LLMs are named Large Reasoning Models (LRMs) and have been shown to have particularly strong capabilities on challenging reasoning problems, such as math [10, 5, 21], coding [3, 15, 16], logic puzzles [23, 35], and agentic tasks [24, 59]. Effective RL training often requires massive parallelization to derive large batch of rollouts for sufficient exploration, which is the key to obtaining the optimal model performance. For example, popular RL algorithms, such as PPO [43] and GRPO[44], often require an effective training batch of thousands of outputs [62, 63, 55]. Moreover, an LRM can generate tens of thousands of thinking tokens for each input prompt [7], further posing an urgent need for an efficient training system to run RL training on large scale. Preprint. Under review. However, developing an efficient large-scale RL system is challenging. An RL system would need to frequently switch between LLM generation and training, which can introduce significant system overhead without careful optimizations. For LRMs, the output length of the training model varies significantly for different prompts throughout the RL process, which results in an ever-changing workload for both generation and training. This characteristic often triggers idle time in highperformance hardware, leading to waste of computation. Furthermore, classical large-scale RL algorithms like PPO or GRPO typically require on-policy training data, i.e., samples generated by the latest model, to ensure the best model performance, which poses additional system challenges. Consequently, most existing large-scale RL systems are designed in fully synchronous manner [28, 12, 46, 45] by strictly alternating between LLM generation and training, which ensures that the LLM is always trained on the latest outputs for the best practical performance. In such synchronous design, the generation step must wait until the finish of the longest output within batch. Due to the varying output lengths for LRM, synchronous RL system suffers from severe training inefficiency. Very recently, there have also been attempts to explore parallel generation and training [31, 25, 50]. These works use outputs generated from previous model version to update the current model. For the best performances, the model version used for rollout generation is limited to only one or two steps older. However, all these systems still follow batched generation setting, where all the samples within training batch are from the same model version. Accordingly, the issue of system inefficiency during the generation phase still remains unaddressed. To fundamentally resolve the issues in system design, we develop AREAL, fully Asynchronous RL training system for LRMs that completely decouples generation from training without hurting the final performance. AREAL runs LLM generation in streaming manner, where each rollout worker continuously generates new outputs without waiting, leading to high GPU utilization. Meanwhile, the trainer workers in AREAL run parallel model updates whenever training batch is obtained from the rollout workers. Once the model is updated, we synchronize the model weights in each rollout worker. In such an asynchronous design, each training batch of AREAL may contain samples generated by different model versions. Therefore, AREAL incorporates modified version of the PPO algorithm, which can leverage samples generated from models of up to 8-step older without any performance drop. AREAL also conducts data filtering process to ensure the staleness of each training sample is well controlled. In addition, AREAL also introduces several system-level optimizations, including interruptible rollout workers, dynamic batching for variable-length outputs, and parallel reward service, which further improve the overall training throughput. We evaluate AREAL on challenging mathematical reasoning and code generation tasks using models up to 32B parameters. Compared to state-of-the-art synchronous systems, AREAL achieves up to 2.57 higher training throughput and linear scaling efficiency up to 512 GPUs. Crucially, this acceleration even comes with improved solution accuracy on these tasks, illustrating that AREAL delivers significant efficiency gains without sacrificing (and indeed enhancing) model performance."
        },
        {
            "title": "2 Related Work",
            "content": "RL for LLMs Reinforcement learning (RL) has emerged as the predominant paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs) [32, 33]. Existing RL approaches typically focus on tasks with well-defined reward functions, including mathematical reasoning [10], coding [15, 16], scientific problem solving [40, 37], and tool use [59]. During training, models learn to reason by progressively extending the length of chain-of-thought trajectories [53, 7]. Recent open-source initiatives have demonstrated significant success in improving model capabilities through smaller distilled models [25, 26]. Our work builds upon this research direction, distinguishing itself from preference-based RLHF [34] and zero-shot reasoning approaches [62, 63, 13] that attempt to acquire reasoning skills from pre-trained models without task-specific fine-tuning. The decoupled asynchronous RL architecture [22, 9, 27], combined with corresponding algorithmic innovations [8, 17], has achieved remarkable success in game applications [2, 52]. Although similar asynchronous approaches have been explored for LLM training, they typically focus on short-context settings [31, 1, 41] (e.g., RLHF) or one/two-step generation-training overlap [25, 49]. Our work extends these studies and provides more flexible trade-off between staleness and training speed, as we will show in Section 5. In contrast to concurrent work [66] that maximizes system-level efficiency, we adopt an algorithm-system co-design approach that provides both an expressive system and practical algorithm implementation. Our interruptible generation technique is conceptually similar to partial rollout [18] in synchronous RL systems. Instead of setting fixed length budget, 2 Figure 1: Execution timeline of synchronous (left) and an one-step overlap (right) RL system showing underutilized inference devices. AREAL dynamically interrupts generation while maintaining consistent training batch sizes through buffering, thus preserving the stability of the PPO. Compared with prior methods [41, 31], our algorithmic innovation in the asynchronous setting can endure higher data staleness and remains compatible with interruptible generation. LLM Training and Inference Our work focuses on dense transformer models [51]. The RL training primarily consists of generation (inference) and training phases. Generation involves autoregressive decoding, which requires requires efficient KV cache management [65, 19] and optimized decoding kernels [60]. Training requires careful orchestration of data, tensor, and pipeline parallelism strategies [39, 47, 64]. While conventional synchronous systems execute generation and training sequentially on the same hardware resources, they require different optimal parallelization strategies. Recent work has proposed context switching [20, 18] or weight resharding [46, 28] techniques to address this mismatch. AREAL advances synchronous RL systems by decoupling generation and training, completely eliminating resharding overhead from the critical training path."
        },
        {
            "title": "3 Background",
            "content": "3.1 Preliminaries about RL Training RL Formulation and PPO We formulate our problem within the Markov Decision Process (MDP) framework [38], defined by the tuple S, A, r, P, γ, H. Here, represents the state space, the action space, the transition model, : the reward function, γ the discount factor, and the horizon. The LRM implements parameterized policy πθ : where each action at corresponds to text token from the vocabulary. The state st consists of question s1 = followed by previously generated response tokens (a1, .., at1), with deterministic transitions st+1 = concat(st, at). Given question distribution D, we optimize the objective: J(θ) = EqD,atπθ (q,a<t) (cid:34) (cid:88) (cid:35) γt1r(st, at) . t=1 (1) Following common practice [7, 26], we use rule-based reward function that only provides non-zero feedback on the final action, indicating answer correctness, and set γ = 1. We optimize this objective using Proximal Policy Optimization (PPO) [43]: JPPO(θ) = EqD,atπold(q,a<t) (cid:34) (cid:88) t= (cid:16) ut(θ) ˆA(st, at), clip (ut(θ), 1 ϵ, 1 + ϵ) ˆA(st, at)) min (cid:35) (cid:17) , (2) πold(atst) denotes the importance ratio and ˆA(st, at) represents the estimated where ut(θ) = πθ(atst) advantage [42]. Following standard practices in RL [43, 34], we divide the global batch into minibatches for sequential parameter updates.1 Distributed Systems for LRM Training Our work focuses on enhancing reasoning capabilities for LRMs after Supervised Fine-Tuning (SFT), distinct from approaches that incentivize reasoning in pre-trained base models [7]. LRMs after SFT produce long reasoning sequences (e.g., 32K tokens) and usually require large global batch sizes (e.g., 128 prompts with 16 responses each) for stable RL training [7, 26, 25, 62, 63]. In synchronous RL systems, two phases are iteratively executed: generation (rollout) and training. The generation phase uses the latest model parameters to produce multiple reasoning traces for each query in the training batch. The training phase then updates the model parameters based on the generated trajectories. These phases iteratively on the same GPUs. 1This differs from gradient accumulation, which performs single update across minibatches. 3 Figure 2: The AREAL architecture featuring asynchronous generation and training components. 3.2 Motivation for Asynchronous RL system We identify two essential limitations in synchronous RL systems: Inference devices are underutilized. As shown in Figure 1 (left), generation must wait for the longest sequence to complete before training can begin. This leads to non-uniform decoding length across GPUs, which underutilizes GPU compute resources. Scalability is poor in synchronous RL systems. synchronous systems distribute generation across all devices, reducing the per-GPU decoding batch size. This pushes the decoding process into memory-IO-bound regime [4, 29] where additional devices fail to improve throughput."
        },
        {
            "title": "4 System Architecture",
            "content": "The limitations identified in Section 3.2 motivate our design of system that fully decouples generation and training across separate GPU clusters. This system should be hardware-efficient, scalable, and equipped with the flexibility for an customized RL workflow. We implement these principles in AREAL, an asynchronous RL system specifically designed for efficient large-scale LRM training. 4.1 System Overview Figure 2 presents the architecture and data flow of AREAL. The system comprises 4 core components: Interruptible Rollout Worker handles two types of requests: (1) The generate request generates responses given prompts. (2) The update_weights request interrupts all ongoing generations and loads parameters of new versions. Upon the interruption, the rollout workers discard KV caches computed by old weights, and re-compute them using the new weights. Afterwards, the rollout workers continue to decode the unfinished sequences until the next interruption or termination. We emphasize that such interruptions and in-flight weight updates would result in trajectories composed of segments produced by different model versions. This introduces novel algorithmic challenge, which will be addressed in Section 5. Reward Service evaluates the accuracy of the responses generated by the model. For example, in the coding task, this service extracts the code and executes unit tests to verify its accuracy. Figure 3: Illustration of generation management in AREAL. Vertical lines shows the ready time for the next step training. Blue crosses show the interrupted requests when new parameters arrive. 4 Trainer Workers continuously sample from the replay buffer, accumulating data until reaching the configured training batch size. They then perform PPO updates and store the resulting parameters in distributed storage. To ensure data freshness, data from the replay buffer is used only once. Rollout Controller serves as critical bridge between the rollout workers, reward service, and the model workers. During the training process, it reads data from the dataset and invokes the rollout workers generate request. The received response is then sent to the reward service to obtain the reward. The trajectory, along with the reward, is stored in the replay buffer, waiting to be trained by the model worker. After the model worker updates the parameters, the controller calls the rollout workers update_weight. We illustrate the generation and training management in Figure 3. This asynchronous pipeline ensures continuous full utilization of both generation and training resources. 4.2 Algorithmic Challenges While the asynchronous system design offers significant acceleration through improved device utilization, it introduces several technical challenges that require algorithmic considerations. Data Staleness Due to the asynchronous nature of AREAL, each training batch contains data from multiple prior policy versions. Prior works on asynchronous RL training systems have demonstrated that such staleness can degrade learning performance in both RLHF [31] and game environments [2]. Data staleness would lead to distribution gap between the training data and the latest model. In asynchronous RL training for LRMs, this issue could be even more severe for long trajectories due to extended decoding time. Inconsistent Policy Versions As discussed in Sec. 4.1, the generated trajectories may involve segments produced by different policy versions. This inconsistency fundamentally violates the formulation of standard PPO in Eq. 2 that assumes all actions being generated by single policy πold. In the following section, we detail our technical innovations for overcoming these challenges while preserving the efficiency advantages of an asynchronous system."
        },
        {
            "title": "5 Addressing the Algorithmic Challenges in AREAL",
            "content": "5.1 Staleness-Aware Training To avoid negative impact of the data stalenss issue, we constrain the version discrepancy between the policy version of generated trajectories and the training policy πθ. We introduce hyperparameter η representing the maximum permitted staleness. Given the latest parameter version i, total generated trajectories Nr, and training batch size B, we enforce: Nr/B + η. (3) When η = 0, the system degenerates to the synchronous RL setting where generation exactly matches training batches. When η = 1, the system recovers to the previous one-step overlap methods [31, 25]. During training, we prioritize older trajectories to ensure staleness remains below η. In our implementation, the rollout controller tracks both Nr and server parameter versions i. It rejects requests that would violate the staleness constraint. While this approach guarantees bounded staleness, overly conservative η values can unnecessarily throttle generation throughputparticularly for long-context generations where the completion time of batch varies significantly. This motivates our adoptation of decoupled PPO objective that can make efficient use of slightly staled data. 5.2 Decoupled PPO Objective We apply decoupled PPO objective [11] that disentangles the behavior policy and the proximal policy. The behavior policy πbehav represents the policy used for sampling trajectories and the proxy policy πprox is proximal policy serving as recent target to regularize the update of πθ. By applying importance sampling on the sampled trajectories, we could derive decoupled PPO objective suitable for asynchronous RL training, 5 J(θ) = EqD,atπbehav (cid:88) t=1 min( πθ πbehav Importance Ratio ˆAt, (cid:122) πprox πbehav Importance Ratio (cid:125)(cid:124) clip( (cid:123) πθ πprox Trust Region Center , 1 ϵ, 1 + ϵ) ˆAt)) = EqD,atπbehav (cid:34) (cid:88) t=1 πprox πbehav (cid:16) min uprox (θ) ˆAt, clip (uprox (cid:17) (θ), 1 ϵ, 1 + ϵ) ˆAt) (cid:35) , (4) (5) (θ) = πθ(atst) πprox(atst) is the importance ratio w.r.t. where uprox the proximal policy. We omit the state-action terms for conciseness. The main difference between the asynchronous PPO objective Equation (5) and the standard one Equation (2) lies in the proximal policy πprox for regularizing the model update. In asynchronous PPO training, using the behavior policy as the proximal policy will pull the latest policy πθ towards the old-version and low-quality policies, thus slowing down model improvements. By employing recent policy as the proximal policy, model updates would happen within the trust region around the high-quality proximal policy πprox, thus stabilizing training. The decoupled PPO objective in Equation (5) provides natural benefit: it relaxes the requirement that all data within one training batch should be generated with singe policy. This property is crucial for maintaining algorithmic correctness when combining interruptible generation with policy updates. We claim that the inconsistent policy versions across trajectory maintains equivalence to single behavior policy πbehav. (See Appendix for the proof.) Proposition 1. For any sequence (q, a1, . . . , aH ) generated by policies (πθ, . . . , πθ+k) where πθ+i produces tokens (ati, . . . , ati+1), where 1 = t0 < < tk+1 = H, there exists behavior policy πbehav such that the interrupted generation is equivalent to sampling entirely from πbehav. Practical Remark While Hilton et al. [11] maintains an exponential moving average of parameters for πprox, this approach is prohibitively expensive for LRMs. Consequently, we simply use the parameters before each model update step as πprox. Equation 5 is implemented by recomputing token probabilities upon the arrival of the global batch in each training step."
        },
        {
            "title": "Implementation",
            "content": "We implement AREAL using Python and PyTorch [36] built upon the ReaLHF [28] framework. Our system combines SGLang [65] v0.4.6 for generation serving with Megatron-Core [47] v0.11.0 as the training backend, managed by SLURM [61] for resource scheduling. To maximize throughput for both generation and training phases, we implement several key system-level optimizations that address critical bottlenecks in the pipeline. AREAL decouples GPU computation from CPU operations, including rule-based reward computation (such as string matching for math problems or unit test execution for code) and TCP-based data transfer. By executing these operations in separate threads and pipelining the workflow, we overlap reward computation and data transfer with subsequent generation requests. We use asyncio coroutines to concurrently runs multiple requests in the rollout worker to avoid mutual blocking waits. To handle the training with variable-length sequences, we employ padding-free sequence packing strategy coupled with dynamic allocation algorithm. The algorithm balances token distribution across micro-batches under fixed memory constraints (see Algorithm 1). This approach maximizes GPU memory utilization while minimizing the number of required forward-backward passes."
        },
        {
            "title": "7 Experiments",
            "content": "Our evaluation comprises three components: (1) comprehensive comparisons against state-of-theart open-source frameworks across model sizes, (2) strong-scaling analysis with varying compute resources, and (3) ablation studies validating our design choices. 7.1 Experiment Setup We evaluate AREAL on challenging math and coding tasks. We employ the distilled Qwen2 model series [56, 57] from DeepSeek-R1 [7] as base models (i.e., R1-Distilled-Qwen), spanning from 1.5B 6 to 32B parameters. For each task-model combination, we train for fixed number of PPO updates and evaluate the final checkpoint. Our evaluation on mathematical tasks follow the Qwen evaluation protocol [58, 14], while coding models are assessed on LiveCodeBench (8/1/24-2/1/25) [15] using the official protocol. Unless otherwise specified, we set the maximum staleness η = 4 and adopt the training configurations used in Section 7.2, with additional hyperparameters detailed in Appendix A. We conduct experiments on an H800 GPU cluster comprising 64 nodes, each equipped with 8 GPUs. The cluster features NVLink for intra-node connectivity and RoCE with 3.2Tbps bandwidth for inter-node communication. To ensure rapid convergence, we allocate minimum of 16 nodes as baseline pod configuration for complete experiments. We scale the number of nodes proportionally with model size, ultimately utilizing 48 nodes for training our largest 32B parameter model. This scaling strategy enables us to run experiments of varying sizes in parallel while maintaining efficient resource utilization. For AREAL, we maintain fixed ratio between inference and training devices, allocating threequarters of the devices for inference. This configuration was selected against an equal 50-50 partition based on our early experiments, where the 75-25 partition demonstrated higher training throughput. While we adopt this ratio as heuristic configuration, we emphasize that the optimal partition may vary across different settings and could potentially benefit from dynamic adjustment during training, as discussed in Section 8. 7.2 End-to-End Comparison We establish two state-of-the-art baselines using synchronous RL systems: DeepScaleR [26] for mathematical reasoning with 1.5B model, and DeepCoder [25] for code generation with 14B model, both trained using verl [46]. For larger 7B and 32B models where comparable baselines are unavailable, we performed controlled experiments by training from scratch using synchronous variant of AREAL. After training, we evaluate on the challenging AIME24 benchmark for math models and the LiveCodeBench [15] benchmark for coding models. They are considered to the golden benchmarks and provide the most truthful reflection of reasoning capability improvement after RL, which was also used by models released by large companies [7, 32, 33, 54, 6]. Evaluation results on more benchmarks are presented in Appendix B. Our main results are shown in Table 1. Since the code of obtaining previous SOTA models can be out-of-date, we measure the throughput and estimate the training hours using the latest verl code for fair comparison. AREAL consistently matches or exceeds baseline performance while achieving significant speedups. In particular, our system demonstrates up to 2.57 improvement in training throughput compared to synchronous approaches without significant performance degradation. Table 1: End-to-End Performance Comparison. We evaluate on the AIME24 benchmark for math and LiveCodeBench (8/1/24-2/1/25) for coding. We limit the maximum generation length to 32K tokens and sample 32 responses per question, reporting the average pass@1 accuracy. * represents the best known reproducible results obtained via RL , as cited from DeepScaler [26] and DeepCoder [25] respectively. AReaL achieve the comparable performance with 2x less training hours. Model AIME24 # Nodes PPO Steps Training Hours 1.5B basemodel w/ VeRL w/ Sync.AReaL w/ AReaL (ours) 7B basemodel w/ VeRL w/ Sync.AReaL w/ AReaL (ours) 29.3 43.1* 42.0 42.2 54.3 - 63.0 63.1 - 16 16 16 - 24 24 24 - 1000 1000 1000 - 1000 1000 - 33.6 41.0 14.8 - 52.1 57.7 25.4 Model LiveCodeBench # Nodes PPO Steps Training Hours 14B basemodel w/ VeRL w/ Sync.AReaL w/ AReaL (ours) 32B basemodel w/ VeRL w/ Sync.AReaL w/ AReaL (ours) 53.4 57.9* 56.7 58.1 57.4 - 61.2 61.0 - 320 320 - 240 240 240 - 44.4 48.8 21.9 - 46.4 51.1 31.1 - 32 32 32 - 48 48 48 Table 2: Evaluation scores when varying data staleness, comparing performance with and without the decoupled objective. Numbers within 1 of the oracle score are underlined. Max.Stale. 0 (Oracle) 1 2 4 8 16 AIME24 AIME25 AMC MATH 500 W/o With W/o With W/o With W/o With 42.0 32.9 84.4 89. 41.8 40.0 23.3 35.7 35.8 34.0 42.1 41.8 42.2 41.0 38.7 36.9 30.7 32.1 23.1 27.8 26.2 26.9 31.9 32.5 32.0 31.1 32.5 29.9 83.3 82.3 58.5 81.2 78.4 79.4 85.2 84.3 85.1 82.9 83.2 81. 89.9 89.6 66.9 87.8 87.4 87.1 89.8 89.6 89.5 89.2 89.1 88.1 7.3 Scalability We compare the scalability of AREAL with verl [46], the state-of-the-art synchronous RL system, across different model sizes and context lengths. AREALs throughput surpasses the baseline in most settings, and could achieve at most 2.57 speedup. We select the minimum number of GPUs when verl does not encounter the OOM issue for 7B models and 32k context length, then we proportionally adjust the number of GPUs according to the model size. We measure the effective throughput for training, defined as the rate of consuming generated tokens during PPO updates, after proper warmup steps. Figure 4 presents the results for context lengths of 16k and 32k. Here, context length refers to the sum of prompt length and generated length, with the maximum prompt length capped at 1k. Across all settings, AREAL demonstrates an approximate linear scaling trend with increased device count, while the synchronous system typically fails to scale effectively. We note that for smaller context lengths, the advantage of AREAL can be smaller because the generation throughput cannot match the pace of training throughput. Although many sequences are generated, they are not effectively consumed by the training process. Additionally, AREAL is more robust with longer generation lengths due to asynchronous and interruptible generation. The generation of long responses can be fully hidden in the critical path, so extending generation length does not drastically affect the effective training throughput of AREAL. 7.4 Algorithm Ablations We conduct ablation studies to validate our algorithmic innovations in Section 5 by training 1.5B LRM on math tasks. Specifically, we vary the maximum allowed staleness η and compare configurations with and without the decoupled PPO objective. Figures 5a and 5b show the learning curves after 1000 PPO updates. Table 2 presents the corresponding final evaluation performances across multiple mathematical reasoning benchmarks. Figure 4: The strong scaling trend. Dotted lines indicate ideal linear scaling. verl consistently encounters OOM with 32k context length and the 32B model so the data points are missing. 8 (a) Learning curves with naive PPO. (b) Learning curves with eq. (5). (c) Effective training throughput. Figure 5: Ablation studies of the decoupled PPO objective and staleness control. Both algorithmic choices are essential. With moderate staleness value and the decoupled objective, training progress can be accelerated by 2x while maintaining final evaluation performance. Figure 5a demonstrates that naive PPO fails to match the performance of the synchronous RL oracle (i.e., the performance when η = 0). Even slight staleness can significantly degrade final performance due to the improper clipping center and policy changes during interruptible generation. Furthermore, increasing data staleness consistently degrades learning performance, aligning with observations from prior work in other domains [2, 31]. However, as shown by comparing Figure 5b and Figure 5a, the decoupled PPO objective substantially improves training stability when handling stale data, consistent with findings from [11] in game domains. Notably, even with the decoupled objective, unbounded staleness (maximum staleness ) still results in inferior performance compared to the zero-staleness oracle. When properly constrained, moderate staleness (e.g., η 4) has minimal impact on final performance while significantly accelerating training through the asynchronous pipeline, as demonstrated in Figure 5c and Table 2. Note that the one-step asynchronous method from deepcoder [25] is similar to the situation of η = 1. These results validate our approach of combining controlled staleness with the decoupled PPO objective for efficient asynchronous RL training. Figure 6: Ablation study of interruptible generation. 7.5 System Ablations Interruptible Generation We ablate interruptible generation and present the resulting generation throughput in Figure 6. Without interruptible generation, the controller must wait for the longest response. In particular, interruptible generation leads to 12% and 17% throughput increase for 1.5B and 7B models respectively on 4 nodes, which validates our architectural design choice. Dynamic Microbatch Allocation We investigate the effectiveness of dynamic batching by comparing PPO training throughput against standard micro-batching strategy. The standard micro-batching strategy can result in multiple long sequences being assigned to the same micro-batch, thus usually requiring sufficiently large number of micro-batches to prevent out-of-memory errors. In our experimental setup, we configured 32 micro-batches for the standard setting and established token budget of 32,768 per micro-batch for the dynamic batching approach. As demonstrated in Figure 7, dynamic batching yields an average of 30% throughput improvements across various model sizes."
        },
        {
            "title": "8 Conclusion",
            "content": "Figure 7: Ablation study of dynamic micro-batch allocation. This paper introduces AREAL, fully asynchronous system designed for efficient large-scale reinforcement learning (RL) training. The AREAL architecture provides both the flexibility and 9 expressiveness required for implementing asynchronous algorithms. Building upon this foundation, we contribute several algorithmic innovations, including staleness-aware training and decoupled PPO objective, which enable efficient and stable PPO training in asynchronous environments. Our experimental results demonstrate AREALs superior hardware efficiency, sample efficiency, and scalability compared to existing synchronous RL systems. This work provides starting point for reliably scaling RL training. We hope that it can enable future advances in large-scale AI systems that push the boundaries of machine intelligence further."
        },
        {
            "title": "References",
            "content": "[1] B. R. Bartoldson, S. Venkatraman, J. Diffenderfer, M. Jain, T. Ben-Nun, S. Lee, M. Kim, J. Obando-Ceron, Y. Bengio, and B. Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable LLM post-training. CoRR, abs/2503.18929, 2025. doi: 10.48550/ARXIV.2503.18929. URL https://doi.org/10.48550/arXiv.2503.18929. [2] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/abs/1912.06680. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. [4] Z. Chen, A. May, R. Svirschevski, Y. Huang, M. Ryabinin, Z. Jia, and B. Chen. Sequoia: Scalable and robust speculative decoding. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 129531129563. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ ea1f5f0878d43ff4fb8bf64ef4a2326c-Paper-Conference.pdf. [5] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. [6] G. Deepmind, Mar 2025. URL https://blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/. [7] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. [8] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning 10 Research, pages 14061415. PMLR, 2018. URL http://proceedings.mlr.press/v80/ espeholt18a.html. [9] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, and M. Michalski. SEED RL: scalable and efficient deep-rl with accelerated central inference. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgvXlrKwH. [10] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. [11] J. Hilton, K. Cobbe, and J. Schulman. Batch size-invariance for policy optimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 6ceb6c2150bbf46fd75528a6cd6be793-Abstract-Conference.html. [12] J. Hu, X. Wu, W. Wang, Xianyu, D. Zhang, and Y. Cao. Openrlhf: An easy-to-use, scalable and high-performance RLHF framework. CoRR, abs/2405.11143, 2024. doi: 10.48550/ARXIV. 2405.11143. URL https://doi.org/10.48550/arXiv.2405.11143. [13] J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H. Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https://doi.org/10.48550/arXiv.2503. 24290. [14] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang, A. Yang, R. Men, F. Huang, X. Ren, X. Ren, J. Zhou, and J. Lin. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL https://doi.org/ 10.48550/arXiv.2409.12186. [15] N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=chfJJYC3iL. [16] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [17] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in distributed reinforcement learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=r1lyTjAqYX. [18] KimiTeam, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, C. Tang, C. Wang, D. Zhang, E. Yuan, E. Lu, F. Tang, F. Sung, G. Wei, G. Lai, H. Guo, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Yao, H. Zhao, H. Lu, H. Li, H. Yu, H. Gao, H. Zheng, H. Yuan, J. Chen, J. Guo, J. Su, J. Wang, J. Zhao, J. Zhang, J. Liu, J. Yan, J. Wu, L. Shi, L. Ye, L. Yu, M. Dong, N. Zhang, N. Ma, Q. Pan, Q. Gong, S. Liu, S. Ma, S. Wei, S. Cao, S. Huang, T. Jiang, W. Gao, W. Xiong, W. He, W. Huang, W. Wu, W. He, X. Wei, X. Jia, X. Wu, X. Xu, X. Zu, X. Zhou, X. Pan, Y. Charles, Y. Li, Y. Hu, Y. Liu, Y. Chen, Y. Wang, Y. Liu, Y. Qin, Y. Liu, Y. Yang, Y. Bao, Y. Du, Y. Wu, Y. Wang, Z. Zhou, Z. Wang, Z. Li, Z. Zhu, Z. Zhang, Z. Wang, Z. Yang, Z. Huang, Z. Huang, Z. Xu, and Z. Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599. [19] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica. In Efficient memory management for large language model serving with pagedattention. J. Flinn, M. I. Seltzer, P. Druschel, A. Kaufmann, and J. Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https: //doi.org/10.1145/3600006.3613165. [20] K. Lei, Y. Jin, M. Zhai, K. Huang, H. Ye, and J. Zhai. PUZZLE: efficiently aligning large language models through light-weight context switch. In S. Bagchi and Y. Zhang, editors, Proceedings of the 2024 USENIX Annual Technical Conference, USENIX ATC 2024, Santa Clara, CA, USA, July 10-12, 2024, pages 127140. USENIX Association, 2024. URL https: //www.usenix.org/conference/atc24/presentation/lei. [21] J. LI, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. C. Huang, K. Rasul, L. Yu, A. Jiang, Z. Shen, Z. Qin, B. Dong, L. Zhou, Y. Fleureau, G. Lample, and S. Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [22] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. Gonzalez, M. I. Jordan, and I. Stoica. Rllib: Abstractions for distributed reinforcement learning. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 30593068. PMLR, 2018. URL http://proceedings.mlr.press/v80/liang18b.html. [23] B. Y. Lin, R. L. Bras, K. Richardson, A. Sabharwal, R. Poovendran, P. Clark, and Y. Choi. Zebralogic: On the scaling limits of llms for logical reasoning, 2025. URL https://arxiv. org/abs/2502.01100. [24] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=zAdUB0aCTQ. [25] M. Luo, S. Tan, R. Huang, A. Patel, A. Ariyak, Q. Wu, X. Shi, R. Xin, C. Cai, M. Weber, et al. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. [26] M. Luo, S. Tan, J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, L. E. Li, R. A. Popa, and I. Stoica. by DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. J. Luo, Surpassing o1-preview with https://pretty-radio-b75.notion.site/ 1.5b model Deepscaler: scaling rl. [27] Z. Mei, W. Fu, J. Gao, G. Wang, H. Zhang, and Y. Wu. SRL: scaling distributed reinforcement learning to over ten thousand cores. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=lajn1iROCu. [28] Z. Mei, W. Fu, K. Li, G. Wang, H. Zhang, and Y. Wu. Realhf: Optimized RLHF training for large language models through parameter reallocation. CoRR, abs/2406.14088, 2024. doi: 10.48550/ARXIV.2406.14088. URL https://doi.org/10.48550/arXiv.2406.14088. [29] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang, R. Y. Y. Wong, A. Zhu, L. Yang, X. Shi, C. Shi, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, page 932949, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703867. doi: 10.1145/3620666.3651335. URL https://doi.org/10.1145/3620666.3651335. [30] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. J. Candès, and T. Hashimoto. s1: Simple test-time scaling. CoRR, abs/2501.19393, 2025. doi: 10.48550/ARXIV.2501.19393. URL https://doi.org/10.48550/arXiv.2501.19393. [31] M. Noukhovitch, S. Huang, S. Xhonneux, A. Hosseini, R. Agarwal, and A. C. Courville. Asynchronous RLHF: faster and more efficient off-policy RL for language models. CoRR, abs/2410.18252, 2024. doi: 10.48550/ARXIV.2410.18252. URL https://doi.org/10. 48550/arXiv.2410.18252. 12 [32] OpenAI,"
        },
        {
            "title": "Sep",
            "content": "2024. learning-to-reason-with-llms/. [33] OpenAI, Apr introducing-o3-and-o4-mini/. 2025."
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/"
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ [34] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html. [35] J. Pan, J. Zhang, X. Wang, L. Yuan, H. Peng, and A. Suhr. Tinyzero. https://github.com/JiayiPan/TinyZero, 2025. Accessed: 2025-01-24. [36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imIn H. M. Wallach, H. Larochelle, perative style, high-performance deep learning library. A. Beygelzimer, F. dAlché-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 80248035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html. [37] L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, S. Shi, M. Choi, A. Agrawal, A. Chopra, A. Khoja, R. Kim, J. Hausenloy, O. Zhang, M. Mazeika, D. Anderson, T. Nguyen, M. Mahmood, F. Feng, S. Y. Feng, H. Zhao, M. Yu, V. Gangal, C. Zou, Z. Wang, J. P. Wang, P. Kumar, O. Pokutnyi, R. Gerbicz, S. Popov, J. Levin, M. Kazakov, J. Schmitt, G. Galgon, A. Sanchez, Y. Lee, W. Yeadon, S. Sauers, M. Roth, C. Agu, S. Riis, F. Giska, S. Utpala, Z. Giboney, G. M. Goshu, J. of Arc Xavier, S. Crowson, M. M. Naiya, N. Burns, L. Finke, Z. Cheng, H. Park, F. Fournier-Facio, J. Wydallis, M. Nandor, A. Singh, T. Gehrunger, J. Cai, B. McCarty, D. Duclosel, J. Nam, J. Zampese, R. G. Hoerr, A. Bacho, G. A. Loume, A. Galal, H. Cao, A. C. Garretson, D. Sileo, Q. Ren, D. Cojoc, P. Arkhipov, U. Qazi, L. Li, S. Motwani, C. S. de Witt, E. Taylor, J. Veith, E. Singer, T. D. Hartman, P. Rissone, J. Jin, J. W. L. Shi, C. G. Willcocks, J. Robinson, A. Mikov, A. Prabhu, L. Tang, X. Alapont, J. L. Uro, K. Zhou, E. de Oliveira Santos, A. P. Maksimov, E. Vendrow, K. Zenitani, J. Guillod, Y. Li, J. Vendrow, V. Kuchkin, and N. ZeAn. Humanitys last exam. CoRR, abs/2501.14249, 2025. doi: 10.48550/ARXIV.2501.14249. URL https://doi.org/10.48550/arXiv.2501.14249. [38] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics. Wiley, 1994. ISBN 978-0-47161977-2. doi: 10.1002/ 9780470316887. URL https://doi.org/10.1002/9780470316887. [39] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In C. Cuicchi, I. Qualters, and W. T. Kramer, editors, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https://doi.org/10.1109/ SC41405.2020.00024. [40] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. URL https://doi.org/10.48550/arXiv.2311.12022. [41] N. L. Roux, M. G. Bellemare, J. Lebensold, A. Bergeron, J. Greaves, A. Fréchette, C. Pelletier, E. Thibodeau-Laufer, S. Tóth, and S. Work. Tapered off-policy REINFORCE: stable and efficient reinforcement learning for llms. CoRR, abs/2503.14286, 2025. doi: 10.48550/ARXIV. 2503.14286. URL https://doi.org/10.48550/arXiv.2503.14286. [42] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous In Y. Bengio and Y. LeCun, editors, 4th control using generalized advantage estimation. International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, 13 May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506. 02438. [43] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347. [44] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402. 03300. [45] G. Shen, Z. Wang, O. Delalleau, J. Zeng, Y. Dong, D. Egert, S. Sun, J. J. Zhang, S. Jain, A. Taghibakhshi, M. S. Ausin, A. Aithal, and O. Kuchaiev. Nemo-aligner: Scalable toolkit for efficient model alignment. CoRR, abs/2405.01481, 2024. doi: 10.48550/ARXIV.2405.01481. URL https://doi.org/10.48550/arXiv.2405.01481. [46] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pages 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi. org/10.1145/3689031.3696075. [47] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR, abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053. [48] C. V. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=4FWAwZtd2n. [49] P. I. Team, S. Jaghouar, J. Mattern, J. M. Ong, J. Straube, M. Basra, A. Pazdera, K. Thaman, M. D. Ferrante, F. Gabriel, F. Obeid, K. Erdem, M. Keiblinger, and J. Hagemann. Intellect-2: reasoning model trained through globally decentralized reinforcement learning, 2025. URL https://arxiv.org/abs/2505.07291. [50] P. I. Team, S. Jaghouar, J. Mattern, J. M. Ong, J. Straube, M. Basra, A. Pazdera, K. Thaman, M. D. Ferrante, F. Gabriel, F. Obeid, K. Erdem, M. Keiblinger, and J. Hagemann. Intellect-2: reasoning model trained through globally decentralized reinforcement learning, 2025. URL https://arxiv.org/abs/2505.07291. [51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. URL https://proceedings.neurips. cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. [52] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, Ç. Gülçehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul, T. P. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):350354, 2019. doi: 10.1038/S41586-019-1724-Z. URL https://doi.org/10.1038/s41586-019-1724-z. [53] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. [54] XAI, Feb 2025. URL https://x.ai/news/grok-3. 14 [55] H. Xin, D. Guo, Z. Shao, Z. Ren, Q. Zhu, B. Liu, C. Ruan, W. Li, and X. Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. CoRR, abs/2405.14333, 2024. doi: 10.48550/ARXIV.2405.14333. URL https://doi.org/10.48550/arXiv.2405. 14333. [56] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, X. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. doi: 10.48550/ARXIV.2407.10671. URL https: //doi.org/10.48550/arXiv.2407.10671. [57] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. [58] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, K. Lu, M. Xue, R. Lin, T. Liu, X. Ren, and Z. Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024. doi: 10.48550/ARXIV.2409. 12122. URL https://doi.org/10.48550/arXiv.2409.12122. [59] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. CoRR, abs/2406.12045, 2024. doi: 10.48550/ARXIV.2406. 12045. URL https://doi.org/10.48550/arXiv.2406.12045. [60] Z. Ye, L. Chen, R. Lai, W. Lin, Y. Zhang, S. Wang, T. Chen, B. Kasikci, V. Grover, A. Krishnamurthy, and L. Ceze. Flashinfer: Efficient and customizable attention engine for LLM inference serving. CoRR, abs/2501.01005, 2025. doi: 10.48550/ARXIV.2501.01005. URL https://doi.org/10.48550/arXiv.2501.01005. [61] A. B. Yoo, M. A. Jette, and M. Grondona. SLURM: simple linux utility for resource management. In D. G. Feitelson, L. Rudolph, and U. Schwiegelshohn, editors, Job Scheduling Strategies for Parallel Processing, 9th International Workshop, JSSPP 2003, Seattle, WA, USA, June 24, 2003, Revised Papers, volume 2862 of Lecture Notes in Computer Science, pages 4460. Springer, 2003. doi: 10.1007/10968987_3. URL https://doi.org/10.1007/10968987_3. [62] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, H. Lin, Z. Lin, B. Ma, G. Sheng, Y. Tong, C. Zhang, M. Zhang, W. Zhang, H. Zhu, J. Zhu, J. Chen, J. Chen, C. Wang, H. Yu, W. Dai, Y. Song, X. Wei, H. Zhou, J. Liu, W. Ma, Y. Zhang, L. Yan, M. Qiao, Y. Wu, and M. Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476. [63] Y. Yue, Y. Yuan, Q. Yu, X. Zuo, R. Zhu, W. Xu, J. Chen, C. Wang, T. Fan, Z. Du, X. Wei, X. Yu, G. Liu, J. Liu, L. Liu, H. Lin, Z. Lin, B. Ma, C. Zhang, M. Zhang, W. Zhang, H. Zhu, R. Zhang, X. Liu, M. Wang, Y. Wu, and L. Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.org/abs/2504.05118. [64] Y. Zhao, A. Gu, R. Varma, L. Luo, C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Mathews, and S. Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860, 2023. doi: 10.14778/3611540.3611569. URL https://www.vldb.org/pvldb/vol16/p3848-huang.pdf. [65] L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. H. Yu, S. Cao, C. Kozyrakis, I. Stoica, J. E. Gonzalez, C. W. Barrett, and Y. Sheng. Sglang: Efficient execution of structured language model programs. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/ 2024/hash/724be4472168f31ba1c9ac630f15dec8-Abstract-Conference.html. 15 [66] Y. Zhong, Z. Zhang, X. Song, H. Hu, C. Jin, B. Wu, N. Chen, Y. Chen, Y. Zhou, C. Wan, H. Zhou, Y. Jiang, Y. Zhu, and D. Jiang. Streamrl: Scalable, heterogeneous, and elastic rl for llms with disaggregated stream generation, 2025. URL https://arxiv.org/abs/2504.15930."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 PPO Details We disable the critic model and the reference model in PPO. The advantage estimation parameter λ in GAE and the RL discount factor γ are fixed at 1. The reward is 5 at the final token if the answer is correct and -5 otherwise. We additionally adopt advantage normalization across the global batch to stabilize the training. Other learning related hyperparameters and configurations can be found in Table 3. Table 3: Training configurations and hyperparameters. Training Configuration Batch size (number of prompts) Random seed 512 PPO Parameters PPO Minibatches Clipping ϵ Advantage normalization Discount factor γ GAE λ Optimizer Parameters Optimizer Learning rate Weight decay β1 β2 Adam ϵ Gradient norm clipping Learning rate scheduler Warmup steps proportion Precision Parameters Parameter dtype KV cache dtype Gradient dtype Optimizer state dtype Generation Parameters Answers per prompt Temperature Top-p Top-k Max prompt length Min generation length Max generation length 4 0.2 True 1.0 1.0 Adam 2.0 105 0.05 0.9 0.95 1 105 1.0 constant 0.001 fp16 fp16 fp32 fp32 16 1.0 1.0 -1 1024 0 A.2 Dataset Details For the math task, we use the open-source data from DeepScaleR [26], For code training, we used the dataset released by DeepCoder [25]. All compared methods use the same dataset. 16 A.3 Dynamic Batching Algorithm 1 Dynamic Batching Require: Sequence lengths = {s1, s2, . . . , sn}, maximum micro-batch capacity C, minimum number of micro-batches kmin if batches < kmin or no existing batch can fit then Create new micro-batch containing sequence batches.append({s}) Ensure: Balanced partition of sequences into micro-batches with total length 1: Sort in descending order 2: batches 3: for all do 4: 5: 6: 7: 8: 9: 10: 11: end for 12: return batches Find all batches that can accommodate Select the micro-batch with fewest sequences end if else The dynamic batching algorithm is shown in Algorithm 1. A.4 Baselines In our experiments, we use the lastest version (main branch of verl repository, May 7, 2025) of verl [46] to evaluate the training throughput in Figure 4 and the training hours in Table 1. For most of the results, we use SGLang [65] v0.4.6 as generation backend and pytorch FSDP [64] as training backend. In few cases where SGLang raises errors (experiments with 32B models or 64 nodes), we use vLLM [19] v0.8.4 as substitution."
        },
        {
            "title": "B Additional Results",
            "content": "We evaluate the models trained with AReaL on more math benchmarks, and list the results in Table 4. Table 4: Results on math benchmarks. Model AIME24 AIME25 AMC23 MATH 500 1.5B basemodel w/ Sync. AReaL w/ AReaL 7B basemodel w/ Sync. AReaL w/ AReaL 29.3 42.0 42.2 54.3 63.0 63.1 24.4 32.9 32.0 41.7 50.0 47.3 71.0 84.4 85.1 89.5 93.2 93. 84.3 89.2 89.5 92.8 94.2 94.3 Proof of Proposition 1 Proposition 1. For any sequence (q, a1, . . . , aH ) generated by policies (πθ, . . . , πθ+k) where πθ+i produces tokens (ati, . . . , ati+1), where 1 = t0 < < tk+1 = H, there exists behavior policy πbehav such that the interrupted generation is equivalent to sampling entirely from πbehav. Proof. For question q, let St(q) denote states encountered at step by the sequence of policies. Since Sti(q) Stj (q) = for = j, we can construct: πbehav(s) = (cid:26)πθ+j(s) arbitrary if tj tj+1 and St(q) otherwise"
        },
        {
            "title": "D Limitations and Future Work",
            "content": "Our work presents several limitations that suggest directions for future research. First, the ratio between inference and training devices could be further optimized for specific training setups. Additionally, this ratio might benefit from dynamic adjustment during training, particularly as context lengths typically increase when fine-tuning pre-trained base models. While we focused our evaluation on single-step mathematical and coding tasks, the AREAL architecture is not inherently limited to these domains. We leave the exploration of multi-turn interactions and agentic scenarios to future work."
        }
    ],
    "affiliations": [
        "Ant Research",
        "HKUST",
        "IIIS, Tsinghua University"
    ]
}