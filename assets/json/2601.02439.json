{
    "paper_title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "authors": [
        "Hao Bai",
        "Alexey Taymanov",
        "Tong Zhang",
        "Aviral Kumar",
        "Spencer Whitehead"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents."
        },
        {
            "title": "Start",
            "content": "January 7, 2026 WebGym: Scaling Training Environments for"
        },
        {
            "title": "Visual Web Agents with Realistic Tasks",
            "content": "Hao Bai1,2 Alexey Taymanov1 Tong Zhang2 Aviral Kumar3 Spencer Whitehead1 1 Microsoft 2 UIUC CMU Work partly done during internship at Microsoft. 6 2 0 2 5 ] . [ 1 9 3 4 2 0 . 1 0 6 2 : r Figure 1: Example rollouts from visual web agents trained on different training environments. Tasks in prior large-scale training setups were relatively simple, e.g., Test-Time-Interaction (TTI; Shen et al. (2025), Row 1), resulting in failures of trained agents on many held-out tasks (task shown: From ArXiv, access the website of the university that maintains and manages ArXiv. How many undergraduate students are currently at the university?, where the agent fails to access the Cornell website). We build WebGym (Row 2, bottom), significantly larger training environment supporting harder and more diverse tasks, totaling nearly 300k tasks (3 the size of TTI), and train an agent via online reinforcement learning to acquire more generalizable skills (task shown: Find the product code for the Austral Oak TrueScale laminate benchtop design on laminex.com.au., where the base model (untrained) agent gets stuck repeating its behavior (repeat action). In contrast, the agent trained with WebGym (RL w/ WebGym) successfully solves the task). We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with simple reinforcement learning (RL) recipe, which trains on the agents own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves 4-5x rollout speedup compared to naıve implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents. WebGym: Scaling Training Environments for Visual Web Agents"
        },
        {
            "title": "1 Introduction",
            "content": "Vision-language models (VLMs) have recently demonstrated remarkable performance on agentic tasks on the web. These tasks often resemble those that human user would aim to perform on the web, such as Compare the prices and specs between AirPod 3 and AirPod 2 on Apple.com. To solve such tasks, an agent will perform series of actions, such as navigating to the relevant website, searching for both products, and finally, deciding on and submitting an answer to complete the task. Some web agents rely purely on textual information and try to solve tasks by simply utilizing accessibility trees Zhou et al. (2023) as the observation for the agent, while we focus on visual web agents, where agents observe screenshots, much like humans. This choice is based on the premise that web interaction semantics are defined by the rendered interface users see, and that reasoning over visual affordances enables more robust generalization across diverse and dynamically generated web pages (Koh et al., 2024; Zhou et al., 2024; Shen et al., 2025). There is growing body of work on evaluating visual web agents by using benchmarks that involve either artificial or real-world websites. Benchmarks with artificial websites typically provide source code for hosting simulated environments that resemble real websites, such as REAL Bench (Garg et al., 2025) and VisualWebArena (Koh et al., 2024). In contrast, realistic benchmarks such as GAIA-Web (Mialon et al., 2023), BrowseComp (Wei et al., 2025), and InSTA-v3 (Trabucco et al., 2025) use tasks based on live websites. These live-website benchmarks tend to be more challenging, as real websites are inherently non-stationary (Bai et al., 2024; 2025a). For example, each time an agent visits the homepage of an e-commerce site, the set of displayed products may differ, and the same sequence of past actions could yield very different outcomes. Recent work has demonstrated that vision-language models can be adapted into web agents via supervised fine-tuning and task-specific training pipelines (Hong et al., 2025). However, in contrast to the rapid progress of online reinforcement learning (RL) in text-only domains such as software engineering and mathematical reasoning (Pan et al., 2024a; Setlur et al., 2025), scaling RL for visual web agents remains substantially more challenging. Unlike text-based settings, where rollouts are fast and rewards are easily verifiable, visual web tasks require reasoning over fine-grained rendered interfaces and often lack unambiguous success signals (Huang et al., 2025a; Sarch et al., 2025; Xue et al., 2025), making data collection slow and inefficient. These challenges have inhibited scaling up post-training methods for visual web agents, particularly for diverse and long-horizon tasks, beyond current short-horizon settings (Zhou et al., 2024). Thus, to train advanced visual web agents, we first need strong task set that includes problems which are challenging for existing models, and that span diverse domains and goals to encourage generalizable policy learning. good task set should be able to support scaling performance by scaling these dimensions. These tasks must also be paired with clear evaluation protocols that provide meaningful learning signals. Such task set would need to be substantially larger and more varied than existing ones (Zhou et al., 2024). In addition, utilizing large task set requires high training speed, with rollout efficiency being the main bottleneck. Rollouts in visual web environments typically take far longer than the learning updates themselves, largely due to the high computational overhead of simulating web browsers in current RL pipelines. To advance scaling of training recipes for visual web agents, we introduce WebGym, training environment featuring the largest open-source task set and the fastest rollout system to date (to our knowledge) for training web agents. Specifically, WebGym provides (1) task set with nearly 300,000 tasks spanning wide range of difficulties, domains, and evaluation criteria, enabling agents to generalize across diverse and challenging scenarios, and (2) an efficient asynchronous rollout system capable of collecting 1,800 trajectories with an average of 13.2 steps in just 30 minutes using 128 CPUs and 24 NVIDIA H100 GPUs. This achieves WebGym: Scaling Training Environments for Visual Web Agents 3 4-5 speedup compared to traditional synchronous rollout systems. We study three dimensions of scaling the task set in WebGym: task set breadth, depth and size, and observe significant improvements over each dimension with the task set introduced in WebGym. With these improvements, we demonstrate that even simple REINFORCE-like training algorithm can improve the performance of Qwen3-VL-Instruct-8B, strong vision-language model to state-of-the-art performance on its scale on web agent benchmarks, across wide spectrum of tasks, ranging from easy to long-horizon in strongly out-of-domain settings."
        },
        {
            "title": "2 Related Work",
            "content": "Visual web agents for realistic tasks. This work focuses on visual web agents designed to solve real-world tasks using vision-language models (VLMs). Such agents emulate how humans interact with browsers on websites: they take website screenshots and metadata as input, and generate interactive actions supported by the browser (e.g., clicking or typing) to make progress toward given goal. This process repeats until the agent outputs an action indicating task completion, resulting in multi-step trajectories. Prior to this work, both general-purpose VLMs and web-specific visual agents have shown promising capabilities on this problem of training visual web agents. General-purpose VLMs can perform reasonable grounding and reasoning on web tasks when combined with prompt engineering or light post-training on web-related data to improve general agent behavior, as demonstrated by models such as Gemma3 (Gemma-Team et al., 2025), GLM-4.1v (Hong et al., 2025), and Qwen3-VL (Bai et al., 2025b). In contrast, web-specific visual agents are models that are heavily post-trained on web/web-adjacent tasks, like UI-TARS (Qin et al., 2025). Evaluation benchmarks and training environments for realistic web agents. Recent work has made substantial progress in developing robust evaluation benchmarks for realistic web-browsing agents, while training environments remain relatively scarce. Existing benchmarks typically feature small, curated task sets with evaluation hints to assess specific reasoning or interaction abilities. Representative visual web benchmarks include WebVoyager (He et al., 2024), Mind2Web-Live (Pan et al., 2024b), and Mind2Web-2 (Gou et al., 2025), alongside broader but non-visual ones such as AgentSynth (Xie et al., 2025), InSTA (Trabucco et al., 2025), BrowseComp (Wei et al., 2025), TravelPlanner (Xie et al., 2024), DeepShop (Lyu et al., 2025), and GAIA (Mialon et al., 2023). training environment, in contrast, must provide not only large-scale task sets but also efficient rollout and training systems. Efforts like WebRL (Qi et al., 2024) and the Proposer-AgentEvaluator (PAE) framework (Zhou et al., 2024) have taken steps in this direction, but their task sets remain limited in diversity and hardness1, and their rollout systems are not optimized for speed. WebGym addresses these issues by expanding task diversity via rubric-derived fact groups and controlled decomposition into atomic and compositional variants (while limiting additional synthesis for sources like PAE-WebVoyager and AgentSynth to avoid near-duplicate synthetic goals), and by using fully asynchronous rollout system that accelerates data collection and enables scalable reinforcement learning for web agents. Scaling online reinforcement learning for training agents. Prior work has improved the performance of LLMs and VLMs by scaling the size and diversity of supervised fine-tuning (SFT) datasets, such as Super Natural-Instruct (Wang et al., 2022) and OpenThoughts (Guha et al., 2025). More recently, studies have shown that increasing the number of training trajectories through online RL can further boost agent capabilities, even with algorithms like REINFORCE (Williams, 1992), across software engineering (Pan et al., 2024a), math reasoning (Setlur et al., 2025), deep research (Zheng et al., 2025; Jin et al., 2025; He et al., 2025), and general reasoning. WebGym builds on these insights by constructing large, diverse task set spanning multiple 1In this paper, difficulty or difficulty level specifically means the difficulty levels we propose as an attribute of the task set we propose, while we use the word hardness for indicating whether given task is difficult or not. WebGym: Scaling Training Environments for Visual Web Agents 4 Source Task Set InSTA-v3 (Trabucco et al., 2025) PAE-WebVoyager (Zhou et al., 2024) AgentSynth-Web (Xie et al., 2025) BrowseComp (Wei et al., 2025) TravelPlanner (Xie et al., 2024) Mind2Web-Live (Pan et al., 2024b) Online Mind2Web (Xue et al., 2025) DeepShop (Lyu et al., 2025) Mind2Web-2 (Gou et al., 2025) GAIA-Web (Mialon et al., 2023) Difficulties # Websites # Tasks 146,348 13 328 7 1 76 139 1 44 1 146,441 128,499 2,086 1,266 1,225 542 300 150 130 87 WebGym (Ours) 127,645 292,092 Table 1: Task sourcing for WebGym. We aggregate web agent task sets from 10 widely-used benchmarks and environments to seed our procedural construction. We report original statistics (#difficulties, #websites, #tasks) for each seed set used to construct the WebGym task set. Some task sets are marked with only 1 website because the original task sets do not provide task-specific website. 1: Let be the task set with tasks, each having task description, difficulty, domain, website, and evaluator rubric fields. 2: Gather all tasks from datasets included in WebGym. 3: for each task in do 4: 5: Infer website and domain from task description. Generate evaluator rubric as fact groups using LLM, 6: 7: 8: 9: where each group contains one or more facts. Set difficulty as total number of facts. if at least 2 groups exist and at least one group has 3 or more facts then for each proper subset of groups containing at least one large group do Create decomposed task from subset using LLM, set its rubric and difficulty, then add to C. end for 10: 11: 12: end for end if Algorithm 1: Task Set Construction Procedure of WebGym. domains and difficulty levels, and training agents via on-policy RL from collected rollouts."
        },
        {
            "title": "3 Tasks in WebGym and Evaluation Protocol",
            "content": "Training with online RL requires large number of model-generated rollouts guided by reliable reward signals on family of tasks (environments in traditional multi-task RL nomenclature) that we curate. To learn generalizable policies with such procedure, the task set must (1) span diverse domains and websites, (2) cover range of difficulties from atomic to compositional tasks, and (3) include verifiable evaluators that translate outcomes into meaningful learning signals. Existing web agent benchmarks, while effective for evaluation, are typically too small and static for large-scale training. Existing training tasks are either non-diverse (Zhou et al., 2024) and lack wide range of task difficulties (Trabucco et al., 2025), or do not come paired with good task-level evaluation criteria, rubrics, or guidance for enabling robust policy evaluation (Trabucco et al., 2025; Zhou et al., 2024; Xie et al., 2025; Wei et al., 2025; Mialon et al., 2023). We describe how we procedurally construct task set that satisfies these desiderata, with assistance from LLMs. We start from seed collection of high-quality tasks drawn from existing benchmarks and training environments (3.1). We systematically annotate these tasks with evaluation criteria (evaluation references) and then decompose them into subtasks (3.2) to achieve both breadth (coverage across many websites and domains) and depth (multi-criterion compositional tasks) (3.4). The construction is fully done with the GPT-4o model and all prompts are provided in to ensure reproducibility and enable scaling up further. We then detail how we split the task set into train and test subsets (3.3) and present our evaluation protocol (3.5)."
        },
        {
            "title": "3.1 Obtaining a Seed Set of Tasks",
            "content": "To obtain set of seed tasks, we collect 10 of the most recent and highly utilized task sets in the open-source community, as listed in Table 1. These tasks come from both existing benchmarks and existing training sets. Importantly, we do not designate any entire benchmark as held out, because many benchmarks share websites, domains, or task patterns with one another, making train-test separation at the level of existing benchmarks unreliable. Instead, WebGym constructs its own train-test split at the task and website level to ensure strict separation without discarding large amounts of useful data. Specifically, our tasks come from a) InSTA-v3 (Trabucco et al., 2025), which provides wide coverage of websites and domains; b) PAE-WebVoyager (Zhou et al., 2024) and AgentSynth (Xie et al., 2025), which provide synthetic tasks focused WebGym: Scaling Training Environments for Visual Web Agents 5 Figure 2: Task decomposition system. WebGym decomposes tasks by generating valid combinations of fact groups from the original tasks rubric. Decomposition requires 2 groups with at least one large group (3 facts). Each valid combination (excluding the full set) produces new task with lower difficulty while maintaining consistency with the original objectives. on narrow domains; c) BrowseComp (Wei et al., 2025) and GAIA-Web (Mialon et al., 2023), which include difficult tasks with concrete, verifiable answers; d) TravelPlanner (Xie et al., 2024) and DeepShop (Lyu et al., 2025), which focus on domain-specific browsing problems; and e) Mind2Web-Live (Pan et al., 2024b), Online Mind2Web (Xue et al., 2025), and Mind2Web-2 (Gou et al., 2025), which contain human-verified high-quality tasks that are indeed solvable. We next describe how we construct our full task set."
        },
        {
            "title": "3.2 Task Set Construction Procedure",
            "content": "Recent research has demonstrated strong improvements when constructing curriculum on tasks with similar goals but different hardness (Shen et al., 2025; Setlur et al., 2025). In parallel, other efforts show that developing evaluation rubrics can significantly improve evaluation accuracy (Pathak et al., 2025; Huang et al., 2025b; Gunjal et al., 2025). We combine these ideas by generating evaluation rubrics and using them to construct decomposed tasks. We elaborate our task construction algorithm in Algorithm 1 and an example in Figure 2. All constructions are generated using GPT-4o. We begin by sourcing and merging all tasks specified in 3.1. For benchmarks that do not specify concrete website for task, we prompt GPT-4o to infer an appropriate website. Next, we use GPT-4o to generate an evaluation rubric structured as fact groups, where each group contains one or more evaluation facts. We define the difficulty or difficulty level of task as the total number of facts across all groups in its evaluation rubric. The prompt we use to generate the evaluation rubric is shown in E.5. With these fact groups, we can procedurally generate decomposed tasks through combinations. Task decomposition occurs only when two conditions are met: (1) the original task has at least 2 fact groups, and (2) at least one group is large (i.e., it contains 3 or more facts). When these conditions are satisfied, we generate new tasks by selecting proper subsets of the fact groups, where each subset must contain at least one large group to ensure meaningful task complexity. For example, as illustrated in Figure 2, the seed task Examine all the items displayed on the webpage, compare their prices and amounts and add the item with lowest unit price and one to shopping cart contains 2 fact groups, with one fact group containing 3 criterion while the other fact group containing only 1. In this case, only the first fact group will participate in the decomposition, and thus our algorithm generates only 1 new task with the only large group. Take harder task for example, such as: Find an eligible upcoming piano concert in the US or Canada by Chopin Competition prize winner, providing the pianists name, competition year/prize, YouTube link to their final performance, and the concerts date, city, venue, and event page link. We decompose this into three fact groups: G1 (concert eligibility) with 2 facts (timing and location constraints), G2 (pianist details) with 3 facts (name, competition prize, YouTube link), and G3 (concert details) with 4 facts (date, city, venue, event link), yielding total WebGym: Scaling Training Environments for Visual Web Agents 6 Figure 3: Statistics of the WebGym training task set. Left: website distribution as function of the sorted index (from more tasks to less) of website. Right: difficulty distribution of the train and test task sets. The transparent bars over the original bar mean decomposed tasks, and the slashed bars means test set.] difficulty of 9. Since the decomposition conditions are met (3 groups 2, and G2 and G3 are large with 3 facts each), we generate valid subset combinations: {G2} (d=3), {G3} (d=4), {G1,G2} (d=5), {G1,G3} (d=6), and {G2,G3} (d=7). The combination {G1} alone is invalid since G1 has only 2 facts (no large group), and {G1,G2,G3} is excluded as it equals the original task. Example rubrics can be found in D.1. We show the prompt we use to generate the decomposed tasks in E.6. As result, tasks generated by WebGym (1) avoid being trivial by restricting to sufficiently large groups, (2) are strictly easier than the original task, yielding denser reward signals, and (3) are guaranteed to be well-defined whenever the original task is well-defined. In contrast, PAE (Zhou et al., 2024) generates synthetic tasks that may be ill-posed or impossible to solve. While WebGym is related to the approach in AgentSynth (Xie et al., 2025), it does not require executing agent rollouts to generate new tasks making it cheaper."
        },
        {
            "title": "3.3 Train-Test Split",
            "content": "Having expanded the source tasks into substantially larger task set, we next describe the train-test split. As were primarily interested in imbuing agents with broadly generalizable skills and capabilities, we construct an out-of-distribution (OOD) task set with 1,167 tasks where each OOD task originates from distinct website. We remove all tasks in the training set associated with the same websites as those in the test set. This guarantees that all OOD tasks come from entirely unseen websites relative to the training set."
        },
        {
            "title": "3.4 Statistics: Size, Breadth and Depth",
            "content": "After the train-test split, the WebGym training set contains 292,092 tasks spanning 127,645 websites. The top 20 websites account for about half of all tasks (Figure 3, left), most of which are from the PAE-WebVoyager synthetic task set, while there are significant number of websites with few tasks that build the diversity up, which is majorly composed of other source task sets. This reflects an intentional balance: many long-tail websites ensure cross-domain diversity, while frequent websites provide sufficient repetition for effective RL training. The task difficulty distribution (Figure 3, right) shows that around 80% of the tasks are Easy tasks (difficulty 1-3), which is intentional to encourage learning generalizable basic skills while we focus on more specific objectives at higher difficulty levels. To verify topical coverage beyond high-frequency sites, we compute domain diversity using the Mind2Web-2 taxonomy (6 domains, 24 subdomains) with GPT-4o. Figure 4 (left) illustrates broad domain coverage, with rollouts from different domains to promote diverse training samples. The trajectory length analysis (Figure 4, right) reveals clear correlation between WebGym: Scaling Training Environments for Visual Web Agents 7 Figure 4: Analysis of the WebGym task set. Left: distribution of tasks across domains according to the Mind2Web-2 taxonomy (Gou et al., 2025). Right: Distribution of trajectory lengths by task difficulty for answered trajectories over multiple iterations. For trajectories working on mediumand hard-difficulty tasks that exceeds 30 tasks during evaluation, we filter in only trajectories under 30 tasks to make the comparison fair. The violin plots show that the probability density of trajectory lengths, with the KDE mode (black line, KDE mode is the peak of the smoothed probability density curve, where the violin is widest) indicating the most likely length and mean (red) indicating the average. task difficulty and the number of steps required: easy tasks (difficulty 1-3) require 7.8 steps, medium tasks (difficulty 4-6) require 9.9 steps, and hard tasks (difficulty 7+) demand 11.9 steps on average. The KDE mode (which measures most likely trajectory length) also increases with difficulty, from 3.7 to 4.9 to 5.5 steps. This trend confirms that the tasks classified to be harder in WebGym do actually involve more complex multi-step action and navigation before the agent can provide an answer."
        },
        {
            "title": "3.5 Evaluator and Reward Function Design",
            "content": "Evaluating long-horizon visual agents is challenging because the produced trajectories often make partial progress, end up at visually similar but incorrect pages, and generate answers that may appear correct unless verified against grounded evidence. Unlike math reasoning or coding, reference answers of any sort are rare to obtain for most task instructions because only small minority of benchmarks provide them (for example BrowseComp and GAIA-Web (Wei et al., 2025; Mialon et al., 2023)) and the notion Figure 5: Agreement between automated evaluators and human judgment. Rubric-based evaluation (with explicit criteria) consistently improves agreement over task-only evaluation, yielding higher accuracy and precision across LLMbased evaluators. Among the evaluators, GPT-4o shows the largest shift after adding the rubric: precision increases the most while recall drops, indicating that the rubric makes GPT-4o apply stricter, more conservative pass criteria. of reference answer does not exist in many cases (e.g., tasks that do not involve question answering). This difference reflects fundamental challenge in designing open-ended web tasks with binary answer. Thus, evaluation cannot rely on direct matching to ground-truth outputs and must instead interpret the trajectory in context. To this end, we mainly focus on LLM-judged evaluation, while reference answers, when available in the source benchmark, overrides the judge. WebGym: Scaling Training Environments for Visual Web Agents 8 For faithful evaluation, recent research has adopted criterion proposal approach that generates task-specific criteria to guide evaluation (Kimi-Team et al., 2025; Chen et al., 2025). However, we posit that tasks usually come with structures, so we propose to generate rubric with structures such as fact groups. As discussed in Section 3.2, in WebGym, each task is paired with rubric of several fact groups, where each group contains one or more criteria that guides evaluation reducing false positives from ambiguity about task success. With the task-specific rubric, trajectory receives reward only when all criteria are satisfied, producing binary signal consistent across domains and difficulty levels. Since not all screenshots are informative, the judge focuses on evidence-bearing pages, obtained via high-recall keypoint selection process inspired by Online Mind2Web (Xue et al., 2025) that retains relevant observations while filtering out distractors such as detours and ads. In practice, we use GPT-4o for both keypoint selection (prompt shown in E.2 and per-criterion evaluation (prompt shown in E.3). The evaluator proposes keypoint screenshots from the task description, iterates through trajectory steps, and issues binary judgments for each criterion using the selected keypoints and final answer. We validate the evaluator against human annotations on 80 collected trajectories sampled uniformly across difficulty levels (10 trajectories per level from difficulties 1-6, and around 5 trajectories per level from difficulties 7-10), where nearly half of the trajectories in each difficulty are marked as correct by the WebGym evaluator to balance the set, and find that rubric-guided evaluation improves agreement over task-only judging, which increases accuracy and precision for all evaluator models being tested (GPT-4o, Qwen3-VL8B-Instruct, and Gemma3-27B-it). We observe minor regressions in the recall metric for the subset of stronger evaluator models (Qwen3-VL-8B-Instruct and especially GPT-4o), where some rubrics tend to be overly strict (Figure 5). Note that this regression is almost impossible to eliminate when using an LLM-based rubric proposer: depending on the prompt used to instruct the model, it may generate stricter rubrics, occasionally resulting in over-strict evaluations. Examples of evaluations with and without criteria are shown in D.2. This trade-off is ultimately beneficial for RL training: we exchange slight reduction in sample efficiency for more precise but conservative learning signals, but at the same time, this helps prevent error accumulation in long-horizon settings."
        },
        {
            "title": "4 Rollout System in WebGym",
            "content": "The primary bottleneck in scaling up RL training for web agents lies in the slow speed of rollout generation, particularly in multi-step RL settings. This challenge becomes more severe when simulation resources, such as CPUs, are limited. To make WebGym practical for large-scale agent training, we address this issue by replacing synchronized, batch-style rollout collection with an asynchronous system that keeps both CPUs and GPUs fully utilized throughout multi-step rollout generation. We show in Figure 6 how tasks are queued into process pool, how stateful browser sessions are isolated for each rollout, and how new tasks immediately begin inference once resources become available, eliminating the need for global synchronization barriers. We now describe this system and benchmark its performance and throughput. To our knowledge, the WebGym provides the the first, open-source rollout system optimized for asynchronous multi-turn simulations for web agent tasks. In contrast to work on RL infrastructure for LLMs, WebGym focuses on an orthogonal dimension: speeding up rollout generation up by introducing highly efficient simulation server for web browsing so that the rollout pipeline does not synchronize between any step or episode. This server can be easily integrated with any RL framework that focuses on system-level speedup like optimizing the weights save/load overhead and algorithmic speedup like introducing episode/step/tokenWebGym: Scaling Training Environments for Visual Web Agents 9 Figure 6: Asynchrony eliminates burst-idle behavior in web rollouts. Left: WebGym implements an asynchronous rollout system that (1) shortens single-trajectory collection duration by isolating rollout processes and (2) allows new rollout processes to join early by replacing batched inference with process pool; the example shown is toy case with 3 available environment buckets but 4 tasks waiting to roll out. Right: CPU utility trace over sampled rollout period, comparing the synchronized rollout framework to WebGym: synchronized barriers induce spiky all-busy then all-idle utilization, whereas WebGym streams work as soon as observations are ready, smoothing CPU load and reducing idle gaps while CPU workers await policy actions. level off-policy RL. Examples of these RL frameworks include veRL (Sheng et al., 2025), veRL-agent (Feng et al., 2025), PipelineRL (Piche et al., 2025), SkyRL (Cao et al., 2025), and AReaL (Fu et al., 2025). We will use the simplest implementation of the RL loop in our experiments, where rollout and training happen synchronously. We instead focus on optimizing rollout generation as discussed above. Since we only optimize rollout generation, in this section, our benchmarking numbers are based on inference-only experiments, and no training is involved."
        },
        {
            "title": "4.1 Strategy and System Architecture",
            "content": "Naıve implementations of rollout systems are synchronous in nature, like what was implemented in Shen et al. (2025); Bai et al. (2024); Qi et al. (2024), where fixed group of browser sessions is forced to advance at the same pace to perform some number of rollouts in parallel. At each step, every session loads its webpage on the CPUs, the system sends all screenshots from the group together to the GPU for one combined policy call, and then waits until the slowest session completes before any session proceeds; common variant waits at the end of each full trajectory instead of at every step. These step and episode barriers are problematic in web settings because step times and horizons vary widely across tasks and even across data-collection policies. Faster sessions sit idle while slower ones catch up, and hardware activity arrives in synchronized bursts that saturate either CPUs or GPUs at once, and then drop to idle. We illustrate this burst-idle behavior in Figure 6 and then quantify its impact on end-to-end data collection time and scaling behavior in Figure 7. To mitigate the inefficiency from this burst-idle behavior, our framework adopts server/client architecture (Ali et al., 2020), where the server hosts simulation environments and the client organizes the rollout loop. The server simulates the browser environments following master/worker paradigm (Aida et al., 2003), with master node routing API requests to CPU worker nodes that host the actual simulators and execute actions. The master node configuration determines how much parallel work the system can support, making it easy to scale horizontally by adding workers or vertically by increasing per-node resources. The server is entirely CPU-based and requires no GPUs. The client hosts agent instances on the GPU and send requests to the server. To manage the requests sent to the server, we adapt an operation-specific local queue that enables spreading CPU and GPU workload evenly under tight resource settings, with more details in B.1. The client WebGym: Scaling Training Environments for Visual Web Agents 10 Figure 7: Benchmarking speed and throughput of the WebGym asynchronous rollout framework. Left: the WebGym framework boosts the rollout speed up significantly with 4x-5x speedup. This figure shows time cost and average CPU utility percentage when reaching all 256 environments running w.r.t. different amounts of CPUs, while using the same amount of GPU resources for running inference on the VLM-based agent. The limit of browsers per CPU can scale to 2 without crashing. Right: the WebGym framework boosts the rollout speed up linearly w.r.t. GPU nodes. This figure shows WebGym throughput w.r.t. different numbers of GPU nodes when enough CPUs are provided on each machine (thus speed is bounded by GPU). is GPU-heavy and relies little on CPU budgets."
        },
        {
            "title": "4.2 Benchmarking our Rollout System",
            "content": "We demonstrate the strong capability of the framework under different GPU/CPU resource constraints, comparing WebGym to synchronized multi-step rollout strategy, as shown in Figure 7 (left). We fix the number of GPUs (to 3 nodes of 8 NVIDIA H100 GPUs) and environments for all experiments to collect 1800 trajectories with an average (instead of max) of 13.2 steps thus in total 23,760 steps, then modify only the number of CPUs on the asynchronous rollout server. For all runs, we use CPU clusters where each worker node provides 64 AMD EPYC 7763 CPUs. We observe 4-5 speedup with our rollout system compared to the synchronous framework shown in Figure 6 (left), with greater gains under tighter CPU constraints (Figure 7, left). Using only 64 CPUs, our system collects 1.8k rollouts in 48.6 minutes, whereas the synchronous system requires 264 minutes. As the number of CPUs increases, the rollout time decreases until around 128 CPUs, beyond which inference throughput plateaus as the three H100 GPU nodes become the primary bottleneck. In this GPU-limited inference regime, adding more CPUs provides minimal benefit because the GPU inference service becomes saturated, causing requests to queue. This effect appears in Figure 7 (left) as the flattened throughput curve and in Figure 6 (right) as intervals of reduced CPU utilization while workers await GPU responses. When CPU resources are scarce, the asynchronous design streams observations to the GPUs as soon as they are available, preventing GPU starvation and maintaining high GPU utilization while smoothing CPU load. This contrast between CPU-limited and GPU-limited regimes explains the utilization patterns and measured time-to-trajectory observed in Figure 7. We also benchmark the scalability of WebGym by increasing the number of concurrent trajectory collections. In this experiment, we provision sufficient CPU resources (768 CPUs) and vary the number of H100 GPU nodes used by the system. Figure 7 (right) shows the system exhibits strong scaling that is sub-linear, but very close to the desired linear speed-up curve. This slowdown primarily stems from the synchronization overhead from coordinating rollouts across many nodes. WebGym: Scaling Training Environments for Visual Web Agents"
        },
        {
            "title": "5 Training Web Agents with WebGym",
            "content": "We train web agents using the tasks and system provided by WebGym. Our goal is to study the design choices behind simple, visual web agent by training on WebGym. Concretely, we center our empirical results around two central questions. First, we study how WebGym can transform existing VLMs to efficiently solve complex web tasks via RL training (5.2). Second, we investigate how the agents generalization capability scales when trained with the large and diverse task set provided by WebGym, which eventually leads to performance improvement (5.3). We show that via simple REINFORCE-style RL training procedure, we can train agents that improve performance from the initial 26.2% to 42.9% on the test split of WebGym. The hyper-parameters of the best run are shown in F."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "To study the two questions outlined above, we first establish the experimental setup we operate in. Action space. We parameterize the action space by extending the definition of Shen et al. (2025) with navigation action, so that the action space now includes clicking, typing, scrolling, returning to the previous page, and navigating to website specified by the agent. The WebGym rollout system supports both the Set-of-Marks mode (Yang et al., 2023), where interactive elements are labeled in the screenshots by the simulator, and coordinate-based screenshot-only mode. We operate directly in the coordinate-based mode, which is supported by the base model (Qwen3-VL-8B) we use to train via RL. Policy update. We run REINFORCE (Williams, 1992) with binary terminal rewards, without any baseline or negative gradient. This update is equivalent to online filtered behavior cloning (BC) (Bai et al., 2024) or thresholded version of reward-weighted regression (Peters and Schaal, 2007), which retains only successful trajectories and maximizes their log-likelihood. The training objective is given by: (cid:34) arg max θ ET tasks o0:τ, a0:τ1πθ (T ) (cid:34)(cid:32)τ1 t=0 log πθ(at ot, ) 1[R(o0:τ, ) = 1] , (1) (cid:33) (cid:35)(cid:35) where θ denotes the parameters of the (multi-step) agent policy πθ, and tasks is task instance sampled from the training task distribution. rollout proceeds in discrete interaction steps indexed by t. At each step t, the agent samples an action at πθ( ot, ) conditioned on the full observation history ot (and the task), and the environment returns the next observation. The rollout terminates at step τ only when one of the following two conditions are met: (i) when the agent emits an ANSWER action, or (ii) when the step index reaches the training horizon htrain, the maximum allowed number of interaction steps during training; thus τ htrain. The realized trajectory therefore contains actions a0:τ1 and observations o0:τ, where o0 is the initial observation and oτ is the terminal observation after the last action. If termination occurs under the first condition (i.e., when an ANSWER action is produced), the environment does not advance and we set oτ = oτ1 (i.e., the final observation is identical to the last pre-answer observation). The binary terminal reward R(o0:τ, ) {0, 1} indicates whether the overall trajectory succeeds on task . The objective increases the log-likelihood of all actions along successful trajectories while assigning zero weight to unsuccessful ones, avoiding negative gradients from failures. In principle, we expect that incorporating baseline (negative gradient) would only improve performance further, but chose to avoid it for now for faster experimentation due to improved training stability with only positive gradient. Base policy. We use the Qwen3-VL model family for its strong grounding and reasoning capabilities (Bai et al., 2025b; QwenLM-Team, 2025). We experiment on the 8B-sized model with both Instruct and Thinking WebGym: Scaling Training Environments for Visual Web Agents 12 variants. Task sampling approach. We ablate on random task sampling and ratio-over-difficulty sampling over the WebGym task set, and present results for both approaches in 5.3. We also maintain dynamic blocklist of websites that prevent access, avoiding those sites for subsequent training periods (B.2). Controlling horizon during training. To keep rollout collection efficient, we cap the maximum number of interaction steps per episode. Although these trajectories may still be successful (i.e., they eventually solve the task), they can be suboptimal in terms of step count, taking many unnecessary interactions compared to shorter solution. Without step cap, rollouts may include many such long-but-successful trajectories, which are expensive to collect and dilute training with avoidable steps. By truncating the horizon, we reduce wasted environment interactions during data collection and bias training toward shorter, more efficient successes, improving step efficiency. This is also preferable to simply tuning the learning objective to emphasize trajectories shorter than threshold while still collecting long trajectories, because the horizon cap prevents spending those extra steps in the first place rather than only downweighting them after collection. We study the effect of the training horizon in 5.3. Probing task hardness with proprietary VLMs. To understand the hardness of the OOD test set in WebGym, we also evaluate GPT-4o and GPT-5 (Thinking) as two strong proprietary models on the OOD test set. Results show that simply using the same prompt as Qwen3-VL (E.1) with Set-of-Marks (SoM (Yang et al., 2023), used here because these models cannot accurately output coordinates) leads to non-trivial but far below 100% performance, where GPT-4o achieves 27.1% on the full test set, and GPT-5 achieves 29.8% on randomly sampled subset with 300 tasks from the OOD test set (subsampled due to the budget costs associated with GPT-5). Note that GPT-4o achieves 25.6% on this task subset, so the variance caused by subsampling is not significant. As we discuss next, our trained agent significantly outperforms both of these compared approaches."
        },
        {
            "title": "5.2 Design Choices for Agent Training",
            "content": "In this subsection, we describe the key design choices we had to utilize to train agents via RL on WebGym. Inspired by prior research on RL for LLMs (Setlur et al., 2025; Xiong et al., 2025), we note that biasing the sampled tasks towards hard tasks improves the performance and sample efficiency, as long as easy tasks are also used either via co-training or via prior stage of curriculum. Thus, we start from difficulty sampling that slightly biases towards harder tasks (concretely, easy : medium : hard = 2 : 5 : 3). All experiments in this section follow this task sampling strategy, while we study different sampling strategies further in 5.3. Memory prompt for long-horizon tasks. Our base model (Qwen3-VL-8B) follows simple prompting template for computer-use agents, which includes high-level action description (e.g., click on the search button) paired with low-level tool call (e.g., Click [209, 441]) (Bai et al., 2025b; QwenLM-Team, 2025). The exact prompt is shown in E.1. From preliminary experiments, we observe that this template yields strong initial performance, because the Qwen-3 series of models are trained on data collected from web environments with this prompt (Bai et al., 2025b; QwenLM-Team, 2025), but this prompt provides limited improvement if we use it to rollout trajectories with RL on WebGym, as shown in Figure 9 (left). This limitation arises from the models inability to retain and reuse information gathered in earlier steps. For example, in the task shown in Figure 2, where the goal is to compare two products, the agent must recall details about the first product when evaluating the second. In the current design, this requires including the entire conversation history and all prior screenshots as input, which is inefficient and impractical. WebGym: Scaling Training Environments for Visual Web Agents 13 (L) (R) Figure 8: Ablations on base models, prompting, and action filtering. (left) test-set success rate curves of different models (Qwen3-VLInstruct-8B, Qwen3-VL-Thinking-8B, GPT-4o, and GPT-5-Thinking), and of Qwen3-VL-Instruct-8B under different constraints (either removing the memory prompt or removing the repetition penalty during RL). (right) test-set same-screenshot (repetitive inefficient action) rate and trajectory length curves before and after applying the action repetition penalty (filtering) on the Instruct model. The hyper-parameters of the best run (Instruct-8B) are shown in F. The orange curve is an average of two runs, while the translucent orange area around the orange line denotes the variance of these two runs. We observe that even if the starting variance between different runs can be large, the curves converges after 24k training trajectories are collected. All plots are produced with an EMA (Exponential Moving Average) smoothing of 0.50. Figure 9: Ablations on base models, prompting, and action filtering. (left) test-set success rate curves of different models (Qwen3-VLInstruct-8B, Qwen3-VL-Thinking-8B, GPT-4o, and GPT-5-Thinking), and of Qwen3-VL-Instruct-8B under different constraints (either removing the memory prompt or removing the repetition penalty during RL). (right) test-set same-screenshot (repetitive inefficient action) rate and trajectory length curves before and after applying the action repetition penalty (filtering) on the Instruct model. The orange curve is an average of two runs, while the translucent orange area around the orange line denotes the variance of these two runs. We observe that even if the starting variance between different runs can be large, the curves converges after 24k training trajectories are collected. All plots are produced with an EMA (Exponential Moving Average) smoothing of 0.50. more formal treatment of this challenge from partially observable Markov decision process (POMDP) perspective is provided in C. To solve this limitation, we propose that the model should maintain memory by responding with the updated memory in each step, inspired by GLM-4.1v (Hong et al., 2025). The prompt with memory is also demonstrated in E.1. Utilizing the memory prompt significantly boosts performance after RL (see Figure 9, left, Instruct-8B versus w/o Memory Prompt). Removing repeated actions with reward penalty. We frequently observe cases where the base model is stuck at the same screenshot and attempts to repeat the same action multiple times. In order to measure how often this behavior happens, we calculate the fraction of times the screenshot at one step is the same as the next one (% Same Screenshot), and the trajectory length (Avg. Steps per Trajectory) in Figure 9 (middle). Preliminary experiments show that standard RL substantially reduces this behavior of repeating ineffective actions, but does not fully eliminate it(Figure 9, middle). Thus, we design penalty for these repeated actions when doing RL: we explicitly filter out step if the resulting next step presents an identical screenshot, even if the current step is part of successful trajectory. After applying this penalty, our RL runs become substantially more sample efficient as shown in Figure 9, left (Instruct 8B vs w/o Repetition Penalty). Use of thinking for every step. In addition to the Instruct variant of Qwen3-VL-8B, we also experiment with the Thinking variant of this model with exactly the same prompt as the Instruct model. Results as shown in Figure 9 (left) reveal that the Thinking model attains higher initial performance compared to the Instruct model, suggesting that extended reasoning at each step can be beneficial. However, the Thinking model produces significantly longer responses: before RL training: for instance, it produces 2139 characters per response compared to 1088 of the Instruct model on the held-out test tasks. Thus training the Thinking model WebGym: Scaling Training Environments for Visual Web Agents 14 Base Model Prompt RL on WebGym Performance (%) Proprietary Models GPT-4o GPT-5 Memory Memory Open-source Models Qwen3-VL-Instruct-8B Official* Qwen3-VL-Instruct-8B Memory Qwen3-VL-Thinking-8B Memory Qwen3-VL-Instruct-8B Memory (Ours) 27.1 29.8 27.4 26.2 28.2 42.9 Table 2: WebGym empowers the small Qwen3-VL-Instruct-8B model to learn with RL to achieve state-of-the-art performance on the holdout test task set, arriving at strikingly 42.9% success rate, significantly surpassing all agents empowered by proprietary models and small-yet-effective open-source agentic models. *The Official prompt refers to the web agent prompt specified in QwenLM-Team (2025). incurs larger latency cost and requires more compute. Notably, as training progresses with WebGym, the performance gap between the Instruct and Thinking models decreases considerably, and soon the Instruct model surpasses the Thinking model. Given the favorable trade-off between performance and efficiency, we adopt the Instruct model for subsequent experiments, though we believe that developing compute-efficient RL recipe to effectively train Thinking model is an interesting direction for future work."
        },
        {
            "title": "5.3 Main Results: Scaling Agent Performance with WebGym",
            "content": "Next, we present our main results scaling agent performance with WebGym. In particular, we are interested in understanding what concretely about the design of WebGym scales performance the most, and how much improvement we can obtain by running RL on WebGym. In the end, were able to scale the performance up to 42.9%, significantly surpassing existing agents empowered by proprietary models like GPT-4o and GPT-5-Thinking and strong open-source models like Qwen3-VL-Instruct-8B and Qwen3-VL-Thinking-8B. The comparison of these results are demonstrated in Table 2. In this section, we will understand how WebGym enables these results by specifically studying the effects of: (i) breadth (domain diversity), (ii) depth (difficulty composition), and (iii) the train-time interaction horizon, during RL training on WebGym. Our starting point is the best checkpoint from 5.2, which biases towards harder tasks (easy : medium : hard = 2 : 5 : 3). 1) Scaling task set breadth (domain coverage) for better generalization. To study the effect of having wide variety of domains in WebGym, we randomly remove half of the subdomains from the training set (exclude domains) while keeping the same training recipe otherwise as all the comparisons. This results in the subsampled training task set that only contains 53% of original tasks. Across all evaluation slices, removing domains (exclude domains) compared to using all tasks from WebGym (biased to hard) consistently slows down improvements and lowers the final success rate (Figure 10ad), indicating that domain breadth matters even when the difficulty mixing ratio is unchanged. This gap is not due to oversampling and overfitting on fewer tasks: note that the number of collected training trajectories (36k) is far smaller than the number of training tasks remaining ( 150k), so most training happens on distinct tasks. The degradation reflects reduced domain diversity, which limits generalization to unseen websites and subdomains. To study 2) and 3) below, we compare four task compositions: a) training only on easy tasks (only easy, 1 : 0 : 0), b) only on medium tasks (only medium, 0 : 1 : 0), c) task mixture biased towards harder tasks (biased to hard, 2 : 5 : 3, as used in 5.2), and d) sampling tasks uniformly at random from the full training set (uniform sampling, approximating ratio of 25 : 5 : 1 given the task set). 2) Scaling candidate task set size by sampling more easy tasks avoids overfitting to narrow hard tasks WebGym: Scaling Training Environments for Visual Web Agents 15 Figure 10: Exploring scaling dimensions of WebGym and difficulty-aware training. Test-set success rate curves under different variations to training: removing domains (exclude domains), tuning difficulty ratios (uniform sampling, biased to hard, only easy, only medium), and shortening the train-time step budget (shorten horizon). Results are reported for (a) Overall (all tasks), (b) Easy (13), (c) Medium (46), and (d) Hard (7+). The orange curve averages two runs; the translucent orange band shows their variation. All plots are produced with an EMA (Exponential Moving Average) smoothing of 0.50. with the filtered BC loss. In our previous experiments, we sample with difficulty mixture of 2 : 5 : 3, which we note is heavily biased towards harder tasks compared to uniform sampling, because easy tasks actually comprise around 80% of all tasks. This can make the effective task set size (i.e., expected number of tasks sampled at any given training iteration that are distinct from all current and previous iterations) very small because large portion of the training tasks are repeated high-difficulty tasks. As we roll out more and more trajectories, we believe that this imbalance leads to overfitting because the test performance curve begins to drop right around 36k-42k training trajectories on the x-axis, as shown in Figure 10 (biased to hard). This problem can, in principle, be resolved with better loss function or, perhaps more simply, be resolved by scaling the size of candidate task set. We choose to scale up the size of the candidate task set by adjusting the task sampling ratio so that the sampled tasks include more easy tasks that are distinct from each other. In fact, we found that training exclusively on easy tasks performs surprisingly well and is more stable than over-emphasizing medium and hard tasks. In particular, the only easy setting consistently outperforms only medium on overall success as well as on easy and medium test tasks (Figure 10ac), and it does not exhibit the late-stage plateauing or mild performance regression observed when harder tasks are upweighted (e.g., the biased to hard setting after 36k trajectories; Figure 10ac). This behavior contrasts with typical findings in LLM reasoning, but is consistent with the structure of WebGym, where the very large set of easy tasks spans many websites and domains and thus supports generalization to unseen websites. On the other hand, very hard tasks in math reasoning can be built for all sub-areas for mathematics. While repeated exposure to narrower set of domains that provide harder tasks remains important (as discussed below), the size afforded by easy tasks plays crucial role in mitigating overfitting and enabling generalization to WebGym: Scaling Training Environments for Visual Web Agents 16 new websites. In contrast, aggressively upweighting medium and hard tasks increases repeated exposure to smaller subset of domains and interaction patterns, leading to overfitting that manifests as performance plateaus and slight degradation in our results. 3) Scaling candidate task set depth (wider range of difficulty levels) by including tasks from all difficulty levels to promote more efficient learning. Despite the strong performance of the only easy setting, we believe that higher-difficulty tasks still play an important role. This intuition is also supported by our empirical observations: the performance gap between only easy and biased to hard is substantially smaller on the mediumand hard-difficulty test subsets than on the easy-difficulty subset. Motivated by this, we increase the depth of the task set by incorporating tasks spanning wider range of difficulty levels. The simplest way to achieve this is through uniform sampling from the full training set. We find that uniform sampling yields the highest overall performance among the four difficulty-mixing strategies we evaluate (Figure 10a), and it achieves significantly higher success rate on the medium-difficulty test set compared to training on easy tasks alone (Figure 10c). Because the only easy setting already draws from large and diverse pool of tasks, the additional gains from uniform sampling are best attributed to the inclusion of some mediumand hard-difficulty tasks. Injecting these harder tasks provides training signals that enable the agent to develop capabilities beyond easy interactions, particularly on medium-difficulty test tasks. Finally, since uniform sampling includes only small fraction of hard tasks, we do not observe significant difference between these two settings on the hard-difficulty test set. 4) Horizon: controlling train-time interaction budget. Additionally, we study one important train-time hyperparameter for algorithmic exploration: the train-time interaction budget. To control this budget and shape the distribution of training trajectories, we reduce the per-episode step limit from (15, 30, 45) to (10, 20, 30) (shortened horizon). Empirically, shortening the horizon improves both sample efficiency and final performance across all difficulty slices. In particular, the success rate increases from peak of 38.2% at the previous best checkpoint (uniform sampling with step budget of (15, 30, 45)) to peak of 42.9%. We hypothesize that tighter step budget acts as regularizer on the policys interaction process. By limiting opportunities for late-stage recovery, it discourages trajectories that succeed only after extended sequences of low-yield exploration, and instead concentrates training signal on earlier, higher-impact decisions. This increases the density of informative stateaction transitions per episode, improving credit assignment and reducing variance in policy updates. Notably, performance on hard tasks also improves substantially (Figure 10d), consistent with the model learning more robust and reusable interaction primitives (e.g., efficient navigation and decisive goal completion) that transfer beyond the capped training horizon and support longer-horizon problem instances at evaluation time. All main results from this section are summarized in Table 3. Our final agent is trained with the memory prompt and penalty for repetitive actions, with task horizon of (10, 20, 30) over (easy, medium, hard) tasks, and with uniform sampling across all task difficulties, surpassing the best proprietary model evaluated (GPT5-Thinking) by 13.1%, as shown in Table 2 and Table 3. We provide raw results to promote fair comparison, and encourage readers to include raw results in future work to compare with these scores."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "We introduced WebGym, the largest open-source training environment for visual web agents. Our procedural task construction approach generates structured evaluation rubrics and decomposes seed tasks into valid criterion subsets, enabling both domain breadth and hardness depth while ensuring decomposed tasks WebGym: Scaling Training Environments for Visual Web Agents 17 remain semantically coherent. To support large-scale training, we developed an asynchronous rollout system that eliminates synchronization barriers, substantially accelerating data collection for on-policy RL. Our experiments reveal several empirical insights: explicit memory mechanisms are essential for longhorizon tasks requiring cross-step information retention; penalizing repeated ineffective actions improves sample efficiency; domain diversity directly translates to out-of-distribution generalization; uniform difficulty sampling outperforms both easy-only and hard-biased curricula; and shorter training horizons improve both efficiency and final performance. With these design choices, simple REINFORCE algorithm enables strong generalization to entirely unseen websites, outperforming proprietary models. Future work. Despite the availability of meaningful task-specific rubrics in our task set, LLM-generated rubrics can sometimes be overly strict, which slightly reduces sample efficiency during training. An important direction for future work is to explore more advanced designs for rubric-based evaluation. One promising approach is to treat LLM-generated rubrics as soft guidance rather than strict criteria, for example by training an evaluator that jointly conditions on both the rubric and the task instruction. Another promising direction is to investigate reinforcement learning algorithms that utilize WebGym more efficiently, particularly methods based on dynamic curricula (Shen et al., 2025), automatic curriculum learning (Bai et al., 2024), multi-agent training, and cross-site navigation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Rui Yang, Rui Pan, Yifei He from UIUC, Yifei Zhou from UC Berkeley, Anikait Singh, Salman Abdullah from Stanford for their early discussions of this research, members of the CMU AIRe lab for their feedback on human evaluation and presentations, and Eduardo Salinas, Gustavo De Rosa, Ahmed Awadallah from Microsoft Research (AI Frontiers Lab) for their continued support throughout this work."
        },
        {
            "title": "References",
            "content": "Kento Aida, Wataru Natsume, and Yoshiaki Futakata. Distributed computing with hierarchical master-worker paradigm for parallel branch and bound algorithm. In CCGrid 2003. 3rd IEEE/ACM International Symposium on Cluster Computing and the Grid, 2003. Proceedings., pages 156163. IEEE, 2003. Ali, Alauldeen, and Ruaa. What is client-server system: architecture, issues and challenge of client-server system. HBRP Publication, 2(1):16, 2020. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37: 1246112495, 2024. Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, and Aviral Kumar. Digi-q: Learning q-value functions for training device-control agents. arXiv preprint arXiv:2502.15760, 2025a. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025b. URL https://arxiv.org/abs/2511.21631. WebGym: Scaling Training Environments for Visual Web Agents 18 Shiyi Cao, Dacheng Li, Fangzhou Zhao, Shuo Yuan, Sumanth R. Hegde, Connor Chen, Charlie Ruan, Tyler Griggs, Shu Liu, Eric Tang, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-agent: Efficient rl training for multi-turn llm agent, 2025. URL https://arxiv.org/abs/2511.16108. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/abs/2505.24298. Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, et al. Real: Benchmarking autonomous agents on deterministic simulations of real websites. arXiv preprint arXiv:2504.11543, 2025. Gemma-Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jimenez Gutierrez, Yiheng Shu, et al. Mind2web 2: Evaluating agentic search with agent-as-a-judge. arXiv:2506.21506, 2025. arXiv preprint Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, et al. Pasa: An llm agent for comprehensive academic paper search. arXiv preprint arXiv:2501.10120, 2025. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025a. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025b. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. WebGym: Scaling Training Environments for Visual Web Agents 19 Kimi-Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop: benchmark for deep research shopping agents. arXiv preprint arXiv:2506.02839, 2025. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024a. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024b. Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Arnav Ramamoorthy, Pratyush Ghosh, Aaryan Raj Jindal, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, et al. Rubric is all you need: Improving llm-based code evaluation with question-specific rubrics. In Proceedings of the 2025 ACM Conference on International Computing Education Research V. 1, pages 181195, 2025. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Zoubin Ghahramani, editor, Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, volume 227 of ACM International Conference Proceeding Series, pages 745750. ACM, 2007. doi: 10.1145/1273496.1273590. URL https://doi.org/10.1145/1273496.1273590. Alexandre Piche, Ehsan Kamalloo, Rafael Pardinas, Xiaoyin Chen, and Dzmitry Bahdanau. Pipelinerl: Faster on-policy reinforcement learning for long sequence generation, 2025. URL https://arxiv.org/abs/2509.19128. WebGym: Scaling Training Environments for Visual Web Agents 20 Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. QwenLM-Team. Qwen3-vl: Vision-language models by qwenlm. https://github.com/QwenLM/Qwen3-VL, https: //github.com/QwenLM/Qwen3-VL/issues/1651, 2025. GitHub repository. Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025. Amrith Setlur, Matthew YR Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms. arXiv preprint arXiv:2506.09026, 2025. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, et al. Thinking vs. doing: Agents that reason by scaling test-time interaction. arXiv preprint arXiv:2506.07976, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, and Ruslan Salakhutdinov. Towards internet-scale training for agents. arXiv preprint arXiv:2502.06776, 2025. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024. Jingxu Xie, Dylan Xu, Xuandong Zhao, and Dawn Song. Agentsynth: Scalable task generation for generalist computer-use agents. arXiv preprint arXiv:2506.14205, 2025. Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, and Tong Zhang. Reinforce-ada: An adaptive sampling framework under non-linear rl objectives, 2025. URL https://arxiv.org/abs/ 2510.04996. Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, and Yu Su. An illusion of progress? assessing the current state of web agents. arXiv preprint arXiv:2504.01382, 2025. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. WebGym: Scaling Training Environments for Visual Web Agents 21 Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and Erran Li. Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet agents. arXiv preprint arXiv:2412.13194, 2024. WebGym: Scaling Training Environments for Visual Web Agents"
        },
        {
            "title": "A Raw Experimental Results",
            "content": "We demonstrate the raw values of all experiments presented in 5 in Table 3 below. Method Instruct-8B Thinking-8B Htrain 15 : 30 : 45 15 : 30 : 45 W/o Memory Prompt W/o Repetition Penalty 15 : 30 : 45 15 : 30 : 45 Uniform Sampling Only Easy Only Medium Exclude Domains Shorten Horizon GPT-4o-SoM GPT-5-SoM (Think) 15 : 30 : 45 15 : 30 : 45 15 : 30 : 45 15 : 30 : 45 10 : 20 : Sampling Domain Memory Rep. Penalty Zero-shot (%) RL Peak (%) 2 : 5 : 3 2 : 5 : 3 2 : 5 : 3 2 : 5 : 3 25 : 5 : 1 1 : 0 : 0 0 : 1 : 0 2 : 5 : 3 25 : 5 : Complete Complete Complete Complete Complete Complete Complete Half Complete 26.2 28.2 27.4 26. 26.2 26.2 26.2 26.2 26.2 27.1 29.8 34.50.4 31.5 33.0 38.2 36.9 35. 31.0 42.9 Table 3: Raw ablations and performance results of our trained agents on the OOD test set. Htrain: training horizon (easy:medium:hard steps); Sampling: difficulty ratio (easy:medium:hard); Domain: subdomain coverage; Memory: memory prompt; Rep. Penalty: repetition penalty. For Instruct-8B (Biased to Hard), we report the peak value of the averaged curve from two runs, with indicating the half-range."
        },
        {
            "title": "B Training Environment Designs",
            "content": "B.1 Rollout System: Operation-specific Local Request Queue To avoid flooding the server with requests, the most naive approach is to implement global queue (first-infirst-out rule) to avoid flooding the server with requests. However, below we show the ineffectiveness of this implementation because (1) it causes extensive warmup time and (2) it creates operation bottlenecks. To improve it, we demonstrate that each operation type should have its own queue. This is to say, requests with different types of operations should not have any priority over each other. Figure 11: WebGym implements an operation-specific queue system that balances the CPU and GPU machine usages. Here we illustrate the computational model of this pool system in the left subplot, and the comparison of the two queue designs on the right. We represent the centralized CPU queue with Queue, and the cascading queue system for with the initial letter of the request type, specifically, Navigating to webpage (N), taking Screenshot (S), and Executing Action (E) that takes medium amount of time. green box means the CPU machine is under optimal load. The computational model of rolling out trajectory in multi-step RL setting is shown in Figure 11 (Left). When simulation is required for an RL environment, there will be sequence of CPU operations bounded by WebGym: Scaling Training Environments for Visual Web Agents 23 Figure 12: Left: Number of trajectories successfully finished and crashed out under high CPU load when = 10min with 64 CPUs, 256 environments, and 3 GPU nodes. Top line: global queue. Bottom line: local queue. Right: The block rate increases as training goes on. CPU resources. On the other hand, high-speed RL inference usually put high pressure on GPU resources as well. For the web agents rollout collection, the computational model can be simplified to five stages: navigating to website with the simulator (N), taking screenshot with the simulator (S), proposing an action with the agent, executing an action with the simulator, and sending the trajectory to an evaluator to obtain reward (E), with their speed defined on the right side of each box. Assume we send burst batch of 180 requests to the rollout system at = 0s. As shown in Figure 11 (middle), with global queue, at = 1s, although 60 Navigation requests are finished, the Screenshot requests arrive strictly after the rest of the Navigation requests, so they have to wait for all Navigation requests to return. This makes GPU starve at = 2s. Similarly, after = 5s, only after all Screenshot requests are returned can the Execution requests be processed, and after all Execution requests can be processed can the next-step Screenshot requests be processed, causing the GPU to starve again and again during the multi-step loop. To improve this, WebGym implements local queue that does not specify any priority among operations. At = 1s, Screenshot requests and Navigation requests will be processed with the same priority. This results in 30 Navigation requests and 15 Screenshot requests being returned at = 2s (as they share CPU resources). When propagating this pattern through the pipeline, the GPU resources will not starve at any moment. We experiment with the performance boost of this local queue under extreme CPU condition (64 CPUs needs to run 256 environments). Results are shown in Figure 12 (left). We observe that the crash percent decreases significantly if we employ the local queue, showcasing the effectiveness of this design. B.2 Rollout Sampler: Anti-blocking As we observe that some websites block frequent requests from the same server as shown in Figure 12 (right), we maintain list of websites that blocks the agent with an LLM. The blocking is detected by GPT-4o with the prompt specified in E.4. Modeling the Markov Decision Process On Long-Horizon Web Navigation Tasks partial observable Markov decision process (POMDP) extends Markov decision process (MDP) to settings where the agent does not have direct access to the underlying state but instead receives incomplete observations. Formally, POMDP is defined as tuple (S, A, P, R, O, Ω, γ), where is the set of latent states, WebGym: Scaling Training Environments for Visual Web Agents 24 is the action space, P(s s, a) defines the state transition dynamics, R(s, a) is the reward function, is the observation space, Ω(o s) specifies the observation model that maps hidden states to observations, and γ [0, 1) is the discount factor. Because the agent does not directly observe st, it maintains belief state bt, probability distribution over S, which it updates using past actions and observations. As result, the optimal policy maps belief states to actions, π(bt) at, instead of mapping fully observed states to actions as in standard MDPs. Recent work models the web agent tasks as POMDP, because historical observations must be included to successfully infer the next action. Formally, for web agent task, if we represent the observation (including the screenshot and metadata of that webpage) at step as ot, recent work represents the true state st with st = otn:t (concatenation of the observations of last steps) (Bai et al., 2025b; Shen et al., 2025). This is called windowed history, designed to fit into the context limit of the vision-language models. Formally, this modeling assumes the approximation Pr(cid:0)ot+1o1:t, a1:t1, c(cid:1) Pr(cid:0)ot+1otn:t, atn:t1, c(cid:1), (2) where is the vector that represents observation, and is the vector that represents task (Zhou et al., 2024; Shen et al., 2025; Bai et al., 2025b; QwenLM-Team, 2025). In words, these works treat observations of the last several steps as sufficient statistics of history for predicting future observations. In practice, the window size is usually set to 3. For long-horizon tasks, this modeling over-simplifies the problem because the policy will frequently return to the same state. If the policy is not aware of its full history, it can produce repetitive sequences of actions because it is not aware of what it already finished earlier. Previously, there have been clever designs in token-level RL (modeling token as step), e.g. RL from human feedback (RLHF), that mitigate this problem by directly represent st with the representation of the token from the auto-regressive vision-language model, which automatically encodes information from all previous steps, naturally enforcing st = o1:t. Is there way we can bring this into sequence-level RL (modeling sequence as step)? Practically, the problem with inputting full history observations o1:t to the VLM policy is that the context window of even the most advanced VLMs is not large enough, partially because image is too dense representation for web agents tasks. Recent work like GLM-4.1v (Hong et al., 2025) addresses this limitation by introducing memory as part of the tokens that the VLM should output in each step. Formally, the memory at step t, denoted by mt, is result of the observation without memory ot, the memory from the previous step mt1, and the task instruction c, denoted as mt = VLMθ( ot, mt1, c), and the observation at step can now be denoted as ot = (mt, ot, c). This design leads to continuous compression of the memory based on the progress of the task, which resembles RLHF and Long-Short-Term Memory (LSTM). We practice this idea for our agent context management, as shown in Section E.1."
        },
        {
            "title": "D Qualitative Examples",
            "content": "D.1 Example Rubrics We show some example rubrics generated in Table 4. We can observe general correlation between the instruction length of the task and the number of criteria in its rubrics. WebGym: Scaling Training Environments for Visual Web Agents 25 Table 4: Examples of WebGym Tasks with Evaluation Rubrics Task Evaluation Rubric Find out Marathon event. the date of the next Baden Group 1: Find date of next Baden Marathon event Facts: Find the recommended daily feeding amount for an adult British Shorthair cat (over 12 months) on royalcanin.pl, assuming target weight of 5kg and moderate activity level. - date of the next Baden Marathon event Group 1: Identify the target cat specifications Facts: - adult British Shorthair cat - age over 12 months - target weight of 5kg - moderate activity level Group 2: Find recommended daily feeding amount Facts: Find the highest-rated mover in Honolulu to shift vehicle and large appliances and who has virtual discussion options available in Yelp. - recommended daily feeding amount for the specified cat on royalcanin.pl Group 1: Identify highest-rated mover in Honolulu Facts: - mover is located in Honolulu - mover is highest-rated Group 2: Services provided by the mover Facts: - mover offers vehicle shifting services - mover offers large appliances shifting services Group 3: Communication options available Facts: - mover has virtual discussion options available Group 4: Source of information Facts: - information is found on Yelp D.2 Comparing Rubric-based Evaluation with Task-based Evaluation We find many cases where rubric-based evaluation is able to produce significantly more comprehensive evaluation that checks details of each criteria of the task, which leads to many wins over using only the task description. One example is shown in Figure 13. The human evaluation for this example is NOT SUCCESS. WebGym: Scaling Training Environments for Visual Web Agents 26 Figure 13: Trajectory for the comparison between vanilla evaluation and rubric-based evaluation. Task description-based evaluation. Verify criterion a: The response mentions the food groups eliminated during the Original Whole30 program: added sugar, alcohol, grains, legumes, dairy, and baked goods, pasta, cereal, chips, and fries. The screenshots provide evidence of these eliminations as listed on the Whole30 website. Additionally, the reasons mentioned in the response for eliminating these groups, such as identifying food sensitivities and improving relationship with food, are also corroborated by the screenshots, particularly those discussing the philosophy of Whole30. Verify criterion b: The task asked for the food groups strictly eliminated during the Original Whole30 program and the reasons for their elimination according to the programs philosophy. The agents response matches the task requirements accurately, listing the correct food groups and providing the reasons as asked, based on the information available in the screenshots. Verdict: SUCCESS Rubric-based evaluation. There are 2 groups for this rubric in total. WebGym: Scaling Training Environments for Visual Web Agents 27 Group 1: Identify food groups eliminated during the Original Whole30 program Description: Identify food groups eliminated during the Original Whole30 program Facts: Identify the food groups strictly eliminated during the Original Whole30 program Verify criterion a: The response claims that the food groups strictly eliminated during the Original Whole30 program include added sugar, alcohol, grains, legumes, dairy, and items such as baked goods, pasta, cereal, chips, and fries. The screenshots clearly show list of eliminated food groups that includes added sugar (real or artificial) alongside the other listed categories. Therefore, the responses identification of eliminated food groups is supported by the screenshots. Verify criterion b: This group requires identifying the food groups strictly eliminated during the Original Whole30 program. The response provides concrete list of eliminated groups (including added sugar), which directly satisfies the identification requirement. Verdict: SUCCESS Group 2: Reasons for eliminating each food group Description: Reasons for eliminating each food group Facts: Reasons for eliminating each food group according to the Original Whole30 programs philosophy Verify criterion a: While the screenshots list the eliminated food groups, they do not explicitly provide the programs philosophy-based reasons for eliminating each group. The response offers general motivations (e.g., identifying sensitivities or resetting habits), but these statements are not directly verifiable from the screenshots as provided. Verify criterion b: This group specifically asks for reasons for eliminating each food group according to the Original Whole30 philosophy. The response does not provide distinct, food-groupspecific reasons (it stays general rather than explaining each eliminated group), and the reasons it does provide are not grounded in the screenshots. Therefore, the response does not meet the requirement of explaining why each food group is eliminated. Verdict: NOT SUCCESS Final verdict: NOT SUCCESS"
        },
        {
            "title": "E Prompts",
            "content": "E.1 Agent Action Prompt The prompt we use to rollout trajectories is the agent action prompts shown below. Both the official prompt as provided in QwenLM-Team (2025) and the memory prompt that we use across 5 are included below. Prompt: Agent Action Generation ================================================================================ MESSAGE 1: SYSTEM ================================================================================ You are helpful assistant. WebGym: Scaling Training Environments for Visual Web Agents 28 # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> { name: computer use, description: Use mouse and keyboard to interact with computer. The screens resolution is 1000x1000. * You do not have access to download files or play videos. * Focus on web browsing and navigation tasks only., parameters: { type: object, properties: { action: { type: string, description: The action to perform: * left click: Click the left mouse button at the specified coordinates. * type: Type text at the specified coordinates. The system will automatically click at the coordinates, type the text, and press Enter. * scroll: Scroll the page in the specified direction (up or down). * wait: Wait for the specified number of seconds for changes to occur. * go back: Go back to the previous page in browser history. * navigate: Navigate directly to specific website URL. The URL must start with https://. CRITICAL: If you see reCAPTCHA or any CAPTCHA challenge on the screen, DO NOT attempt to solve it. Instead, immediately use the navigate action to go to different relevant website to complete your task. * answer: Provide the final answer to complete the task., enum: [left click, type, scroll, wait, go back, navigate, answer] }, coordinate: { type: array, description: [x, y] coordinates (01000 range). Required for left click and type actions. For type action, specify WHERE to type (e.g., coordinates of input field)., items: { type: integer, minimum: 0, maximum: 1000 }, minItems: 2, maxItems: 2 }, text: { type: string, description: Text to type or answer. Required for type and answer actions. Note: For type action, the system will automatically click at the coordinates, type the text, and press Enter no need to click separately before typing. }, direction: { type: string, enum: [up, down], description: Scroll direction. Required for scroll action. }, time: { type: number, WebGym: Scaling Training Environments for Visual Web Agents 29 description: Seconds to wait. Required for wait action. }, url: { type: string, description: URL to navigate to. Required for navigate action. Must start with https://. } }, required: [action] } } </tools> For each function call, return JSON object with function name and arguments within <tool call></tool call> XML tags: <tool call> {name: <functionname>, arguments: <argsjsonobject>} </tool call> # Response format {{if w/ Memory Prompt}} Response format for every step: 1) Memory: facts you would like to memorize for future actions in json format. Include the current step. 2) Progress: Decompose the task into subtasks and what has been finished so far with json format. Include progress of the current step. 3) Intention: clearly state which subtask youre working on at this step with the json key. 4) Action: short sentence describing what to do in the UI to accomplish the next subtask. 5) single <tool call>...</tool call> block containing only the JSON: {name: <functionname>, arguments: <argsjsonobject>}. Rules: Output exactly in the order: Memory, Progress, Intention, Action, <tool call>. You MUST use json format for the Memory and Progress parts. Example Task: Search and compare the prices and locations of product 1 and product 2 on Amazon. Example of Memory json format: {Price of product 1: 10.00, Location of product 1: 10.00, Price of produce 2: 12.00}. Example of Progress json format: {Go to Amazon.com: finished, Search for price of product 1: finished, Search for location of product 1: finished, Search for price of product 2: finished, Search for location of product 2: not finished, Compare product 1 and product 2: not finished}. Example of Intention json key format: Search for location of product 2. You CAN NOT modify previous Memory. Only append to it. You CAN modify Progress from previous conversation to further decompose the task and guide your next action. For example, if the previous assistant message specifies Progress: {Go to Amazon.com: finished, Search for product 1: finished, Search for product 2: not finished, Compare product 1 and 2: not finished}, You should further decompose Search for product 1 and Search for product 2 into Search for price of product 1 and Search for location of product 1, and Search for price of product 2 and Search for location of product 2. Do not output anything else outside those five parts. {{/if w/ Memory Prompt}} {{if w/o Memory Prompt}} Response format for every step: 1) Action: short sentence describing what to do in the UI. WebGym: Scaling Training Environments for Visual Web Agents 30 2) single <tool call>...</tool call> block containing only the JSON: {name: <functionname>, arguments: <argsjsonobject>}. Rules: Output exactly in the order: Action, <tool call>. Action describes the highlevel intention of the tool call within single sentence. Do not output anything else outside those two parts. {{/if w/o Memory Prompt}} ================================================================================ MESSAGE 2: USER ================================================================================ [IMAGE: Screenshot of current webpage] Please generate the next action according to the UI screenshot and task. Task: Find the details of the public transport journey from Glasgow Queen Street to Edinburgh Waverley, departing on May 22, 2025, at 09:00 am. Initial website: https://www.nationalrail.co.uk Generate the next action to complete the task. ================================================================================ MESSAGE 3: ASSISTANT {{Example response, note that when using the w/ memory prompt, the previous assistant message will contain memory response}} ================================================================================ Action: Click on the search input field to enter the departure station. <tool call> {name: computer use, arguments: {action: left click, coordinate: [245, 312]}} </tool call> ================================================================================ MESSAGE 4: USER ================================================================================ [IMAGE: Screenshot after clicking] ================================================================================ MESSAGE 5: ASSISTANT {{Example response, note that when using the w/ memory prompt, the previous assistant message will contain memory response}} ================================================================================ Action: Type the departure station name into the search field. <tool call> {name: computer use, arguments: {action: type, coordinate: [245, 312], text: Glasgow Queen Street}} </tool call> WebGym: Scaling Training Environments for Visual Web Agents 31 E.2 Evaluator Prompt: Keypoint Detection Prompt: Evaluator/Evaluation Criteria Generation You are an expert evaluator determining whether an image contains relevant information for completing task. **Instructions**: Answer YES if the image shows ANY taskrelated content: actions taken, progress, search results, tool usage, error messages, or blocking screens. Answer NO only if completely irrelevant: generic homepage, unrelated webpage, or blank screens with no context. When in doubt, answer YES. **Response format**: 1. **Reasoning**: [One sentence explanation] 2. **Decision**: [YES or NO] User Prompt: **Task**: {task} **Key Points for Task Completion**: {eval rubric} The snapshot of the web page is shown in the image. Does this image contain relevant information for the task? (Answer YES unless its completely irrelevant) Where: {task} = task.task name (the task description) {eval rubric} = List of all criteria from task.evaluator reference Example filled in: **Task**: Find the price of iPhone 15 Pro on Amazon **Key Points for Task Completion**: The agent should provide the exact price of the iPhone 15 Pro The price must be visible in the screenshot The product must be from Amazon E.3 Evaluator Prompt: Per-criteria Evaluation The evaluation system uses two-criterion approach, each checked separately: Criterion is checked per-fact (each rubric/fact from task decomposition is evaluated separately). Criterion is checked once per task (verifies the agents final answer against screenshots). E.3.1 Criterion A: Fact Verification Prompt: Evaluator/Criterion - Fact Verification ================================================================================ MESSAGE 1: SYSTEM ================================================================================ You are an expert in evaluating the performance of web navigation agent. The agent is designed to help human user WebGym: Scaling Training Environments for Visual Web Agents 32 navigate website to complete task. Your goal is to verify whether SPECIFIC FACT can be confirmed by the provided screenshots. As an evaluator, you will be presented with the following components: 1. Task Instruction: The original task description (provided for CONTEXT ONLY) 2. Fact Group: group of related facts decomposed from the task instruction (provided for CONTEXT ONLY) 3. Fact to Check: specific fact that you need to verify (THIS IS YOUR PRIMARY FOCUS) 4. Trajectory: complete list of observations and actions that were taken by the agent 5. Result Screenshots: Visual representation of the screen showing the result or intermediate state CRITICAL: Your judgment should ONLY focus on whether the FACT TO CHECK can be verified by the screenshots. You are NOT checking the agents response only whether the screenshots contain evidence for the fact. Guidelines for evaluation: Your primary responsibility is to assess whether the screenshots contain evidence that verifies the FACT TO CHECK. The fact to check may involve more than one subfact. ALL subfacts must be verifiable from the screenshots. If the fact requires specific information (e.g., concert is in the US or Canada), the screenshots must show this information. If the evaluation criteria asks to find specific item, the screenshots must show that exact item (not similar one). IMPORTANT Handling OR conditions: When the fact or task contains OR (e.g., best books on cooking OR gardening OR home decor), satisfying ANY ONE of the alternatives is sufficient for SUCCESS. Example: If the task is find best books on cooking OR gardening OR home decor and the screenshots show best cooking books, this is SUCCESS the agent does NOT need to find all three. OR indicates alternatives/options, not requirement to verify all items. Response format (you should STRICTLY follow the format): 1. Analysis: [Describe what evidence you see in the screenshots related to the fact to check] 2. Verdict: [SUCCESS if the fact is verified by screenshots, NOT SUCCESS otherwise] ================================================================================ MESSAGE 2: USER ================================================================================ ===Your Turn=== Task Instruction (for context only): [task instruction] Fact Group (for context only): [fact group] Fact to Check (PRIMARY FOCUS verify this against the screenshots): [fact to check] Completion history: [trajectory] Relevant screenshots: attached. Evaluation: (MUST end with line 2. Verdict: [SUCCESS or NOT SUCCESS]) WebGym: Scaling Training Environments for Visual Web Agents 33 E.3.2 Criterion B: Response Verification / Anti-Hallucination Prompt: Evaluator/Criterion - Anti-Hallucination Check ================================================================================ MESSAGE 1: SYSTEM ================================================================================ You are an expert in detecting hallucinations in web navigation agent responses. Your goal is to verify whether the agents FINAL RESPONSE is supported by the provided screenshots. As an evaluator, you will be presented with the following components: 1. Task Instruction: The task the agent was trying to complete 2. Final Response: The agents answer/response to the task 3. Result Screenshots: Visual representation of the screens the agent visited CRITICAL: Your job is to check if the agents response contains information that is NOT shown in the screenshots. Agents frequently hallucinate or make up answers that are not verified by what they actually saw. Guidelines for evaluation: Check whether EVERY claim in the agents response can be verified by the screenshots. If the response mentions specific facts (names, numbers, dates, locations, etc.), these MUST be visible in the screenshots. If the response contains information not shown in ANY screenshot, this is hallucination mark as NOT SUCCESS. If the response is vague or says couldnt find the information, check if this matches what the screenshots show. YOU SHOULD EXPECT THAT THERE IS HIGH CHANCE THAT THE AGENT WILL MAKE UP AN ANSWER NOT VERIFIED BY THE SCREENSHOTS. Response format (you should STRICTLY follow the format): 1. Claims in response: [List the specific claims/facts in the agents response] 2. Screenshot verification: [For each claim, state whether it appears in the screenshots] 3. Verdict: [SUCCESS if ALL claims are verified by screenshots, NOT SUCCESS if ANY claim is not verified] ================================================================================ MESSAGE 2: USER ================================================================================ ===Your Turn=== Task Instruction: [task instruction] Agents Final Response: [response] Screenshots from the agents session: attached. Evaluation: (MUST end with line 3. Verdict: [SUCCESS or NOT SUCCESS]) E.4 Evaluator Prompt: Blocking Detection Additionaly, we introduce blocking detection step, which is used to identify if failures were due to anti-bot measures vs. agent errors. Those websites that frequently blocks the agent will be excluded from being sampled. WebGym: Scaling Training Environments for Visual Web Agents 34 Prompt: Evaluator/Blocking Detection ================================================================================ MESSAGE 1: SYSTEM ================================================================================ You are an expert in analyzing web navigation trajectories to detect if website has EXPLICITLY and UNRECOVERABLY BLOCKED an automated agent using antibot measures. CRITICAL DISTINCTION: Blocked = Website explicitly prevents access with UNRECOVERABLE antibot measures that the agent CANNOT escape from by navigating NOT Blocked = Agent makes navigation mistakes, clicks wrong elements, or encounters temporary obstacles that could be navigated away from EXPLICIT SIGNS OF UNRECOVERABLE BLOCKING (answer YES): CAPTCHA challenges that persist and cannot be bypassed (reCAPTCHA, hCaptcha, image selection puzzles) Cloudflare protection screens that remain stuck (Checking your browser, Just moment, DDoS protection) Explicit Access Denied or 403 Forbidden error pages that appear consistently Please verify you are human prompts that block all access Bot detection pages that explicitly state automated access detected and prevent further navigation Security challenge pages that completely prevent any website access DO NOT COUNT AS BLOCKING (answer NO): Agent clicked wrong buttons or links (navigation mistakes) Agent cannot find information on normally accessible page (task difficulty) Page loaded slowly but eventually loaded (performance issue, not blocking) Agent got lost or confused navigating the site (agent error) Page not found or 404 errors (broken links, not blocking) Empty search results (legitimate website response, not blocking) Temporary errors that the agent could escape from by going back or navigating elsewhere CRUCIAL: Only answer YES if you see PERSISTENT, UNRECOVERABLE antibot measures that prevent ALL navigation on the website. Response format (you MUST strictly follow): 1. Analysis: [Describe what you see in the screenshots any EXPLICIT signs of antibot blocking measures?] 2. Blocked: [YES or NO] ================================================================================ MESSAGE 2: USER ================================================================================ ===Your Turn=== Task: [task] Trajectory: [trajectory] Screenshots: attached. Did the website block the agent? (MUST end with line 2. Blocked: [YES or NO]) WebGym: Scaling Training Environments for Visual Web Agents 35 E.5 Task Set Generation Prompt: Evaluation Criteria Proposal Prompt: Task/Evaluation Criteria Generation ======== Guidelines ======== Youre helpful assistant that generates factbased evaluation rubrics for web navigation tasks. Your task is to: 1. Break down the task into hierarchical fact groups, where each group contains specific facts to verify 2. Each fact represents ONE verifiable piece of information that can be checked in trajectory 3. The overall task difficulty equals the TOTAL NUMBER of all facts across all groups Key principles: FIRST analyze the task to identify logical groupings of related information For each group, list ALL specific facts that need to be verified Facts should be AS DETAILED AS POSSIBLE break down complex requirements into individual checkable facts Each fact has difficulty of 1 (facts are atomic units) The OVERALL DIFFICULTY is the TOTAL COUNT of all facts across all groups trajectory is marked correct if and only if ALL facts are verified Do not make up facts not present in the original task ======== CRITICAL: Do Not Decompose Mathematical Problems ======== When task involves mathematical calculations, physics problems, or computational work, DO NOT break down the calculation steps. Treat the entire computation as SINGLE fact. BAD Example: Task: What is the result of 2ˆ3 + 2ˆ2 + 2ˆ1 + 2ˆ0? { fact groups: [{id: 1, facts: [calculate 2ˆ3, calculate 2ˆ2, ...]}], difficulty: 5 } Problem: Breaking down calculation into individual steps. CORRECT: { fact groups: [{id: 1, facts: [result of the equation 2ˆ3 + 2ˆ2 + 2ˆ1 + 2ˆ0]}], difficulty: 1 } ======== CRITICAL: Do Not Decompose HowTo Instructions ======== When task asks How to do something, the task is asking for INSTRUCTIONS from the website, not to verify each step was performed. BAD Example: Task: How can set up an Apple Watch for the first time? { fact groups: [ {id: 1, facts: [ensure Apple Watch is charged, check compatibility]}, {id: 2, facts: [turn on Apple Watch, bring close to iPhone, ...]} ], WebGym: Scaling Training Environments for Visual Web Agents 36 difficulty: 11 } Problem: Breaking down procedure into action steps. CORRECT: { fact groups: [{id: 1, facts: [instructions for setting up Apple Watch for the first time]}], difficulty: 1 } ======== CRITICAL: AND vs OR Logic ======== ALL facts must be verified facts use AND logic by default. If the task contains OR statements (e.g., do or Y), represent as SINGLE fact with OR capitalized. BAD Example: Task: What should user do if they were trying to contact the original owners or access the previous content. { fact groups: [ {id: 1, facts: [method to contact the original owners]}, {id: 2, facts: [method to access the previous website content]} ] } Problem: Treating OR as AND (creating separate groups). CORRECT: { fact groups: [ {id: 1, facts: [method to contact the original owners OR access the previous website content]} ], difficulty: 1 } ======== CRITICAL: Do Not Enumerate Unspecified Items ======== When task asks to identify or find items WITHOUT specifying what they are, DO NOT make up specific facts. Similarly, do not enumerate details or information into specific types. BAD Example: Task: What are the contact details for Hamble Harbour Office? { fact groups: [{id: 1, facts: [telephone number, email address, physical address]}], difficulty: 3 } Problem: Enumerating specific types when task just says contact details. CORRECT: { fact groups: [{id: 1, facts: [contact details for Hamble Harbour Office]}], difficulty: 1 } WebGym: Scaling Training Environments for Visual Web Agents 37 ======== CRITICAL: Such as Are Illustrative, Not Facts ======== Phrases like such as, for example, e.g. provide examples to clarify, NOT additional facts. BAD Example: Task: Find vegetarian lunch dish, such as chickpea salad or lentil soup, with 5star rating. { fact groups: [{id: 1, facts: [recipe is vegetarian, example dishes include chickpea salad or lentil soup]}] } Problem: Creating fact for examples. CORRECT: { fact groups: [{id: 1, facts: [recipe is for vegetarian dish, dish is suitable for lunch, recipe has 5star rating]}], difficulty: 3 } ======== Example ======== Task: Find the full list of exhibitions at The Ringling, including their titles, dates, and descriptions. Expected response: { fact groups: [ { id: 1, description: find complete list of exhibitions at The Ringling, facts: [ full list of exhibitions, current exhibitions included, upcoming exhibitions included, exhibitions are at The Ringling ] }, { id: 2, description: get details for each exhibition, facts: [ title for each exhibition, specific dates for each exhibition (if available), brief description for each exhibition ] } ], difficulty: 7 } ======== Output Format ======== Respond with valid JSON: fact groups: Array of objects with id, description, facts (array of strings) difficulty: Integer = TOTAL COUNT of all facts across all groups WebGym: Scaling Training Environments for Visual Web Agents 38 ======== Your Turn ======== Task: {task} >>>>>>>>>>> E.6 Task Set Generation Prompt: Task Proposal from Fact Groups Prompt: Task/Task Generation from Selected Fact Groups ======== Task Generation ======== You are helpful assistant that generates simplified task descriptions based on selected fact groups from the original task. Original Task: {original task} Selected Fact Groups: {selected groups} Your task is to: 1. Create new task description that incorporates ONLY the requirements explicitly listed in the selected fact groups above 2. The new task should read naturally and be selfcontained 3. Do NOT include requirements from fact groups that were not selected 4. Maintain the original context and domain of the task 5. **CRITICAL**: Do NOT add any information that is not explicitly present in the selected fact groups above, even if that information appears in the original task Important Guidelines: If fact group mentions concert but does NOT mention upcoming or in the US or Canada, do NOT include those constraints in the generated task ONLY use constraints and requirements that are explicitly listed as facts in the selected groups Do not infer or add information from the original task that is not in the selected fact groups Output Format: Respond with ONLY the new task description as plain text (no JSON, no extra formatting). Example: Original Task: Find the full list of current and upcoming exhibitions at The Ringling, including their titles, specific dates (if available), and brief description for each. Selected Fact Groups: Group 1 (find complete list of exhibitions at The Ringling): full list of exhibitions, current exhibitions included, upcoming exhibitions included, exhibitions are at The Ringling New Task: Find the full list of current and upcoming exhibitions at The Ringling. Your Turn: >>>>>>>>>>> Hyper-parameters Our system consists of four main components: environment management, data collection (rollout), model training, and the agent model. The hyperparameters for each component are carefully tuned to balance WebGym: Scaling Training Environments for Visual Web Agents 39 performance, scalability, and sample efficiency. The final hyperparameters can be found in Table 5. Table 5: Hyperparameters for All Experiments. Bolded option suggests an experimented optimality. Component Hyperparameter Simulation CPU Server size HTTP pool size (screenshot/execute) Environment (2 workers) HTTP pool size (metadata/navigate) HTTP pool size (allocate/release) Max vLLM sessions Train tasks rollout size Task Sampling Max steps (easy,medium,hard) Interaction mode Temperature Top-p Top-k Learning rate Effective batch size Times each sample is trained Max gradient norm Weight decay Warmup steps LR scheduler type Samples to train per iteration Recency bias power Replay buffer capacity Cutoff length Gradient checkpointing bf16 precision Model type Variant Max new tokens Rollout Training Model Value 256 256 4 80 512, 1024 uniform/ratio (15,30,45), (10,20,30) coordinates-only, set-of-marks 1.0 0. 2 1e-5, 5e-6, 1e-6 3*4*8*2 2 1.0 0, 0.01 30, 100 constant with warmup, cosine 1200, 1800 1, 2 last 4, last 8 4096, 8192, 16384 False True Qwen3-VL-8B Instruct, Thinking 1024, 2048,"
        }
    ],
    "affiliations": [
        "CMU",
        "Microsoft",
        "UIUC"
    ]
}