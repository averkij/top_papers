{
    "paper_title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
    "authors": [
        "Haocheng Xi",
        "Shuo Yang",
        "Yilong Zhao",
        "Muyang Li",
        "Han Cai",
        "Xingyang Li",
        "Yujun Lin",
        "Zhuoyang Zhang",
        "Jintao Zhang",
        "Xiuyu Li",
        "Zhiying Xu",
        "Jun Wu",
        "Chenfeng Xu",
        "Ion Stoica",
        "Song Han",
        "Kurt Keutzer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality."
        },
        {
            "title": "Start",
            "content": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Haocheng Xi * 1 Shuo Yang * 1 Yilong Zhao 1 Muyang Li 2 Han Cai 3 Xingyang Li 2 Yujun Lin 3 Zhuoyang Zhang 2 Jintao Zhang 1 Xiuyu Li 1 Zhiying Xu 4 Jun Wu 4 Chenfeng Xu 5 Ion Stoica 1 Song Han 3 2 Kurt Keutzer 1 6 2 0 2 3 ] . [ 1 8 5 9 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "rapid progress Despite in auto-regressive video diffusion, we identify an emerging systemalgorithm bottleneck that limits both deployability and capability: KV-cache memory. In auto-regressive video generation models, the KV-cache grows with history and rapidly dominates GPU memory (often 30 GB), blocking deployment on widely available hardware. More importantly, memory-bounded KV budgets force small working memory, which directly degrades long-horizon consistency in identity, layout, and motion, etc. To bridge this gap, we present Quant VideoGen (QVG), training-free KV-cache quantization framework for auto-regressive video diffusion model. QVG exploits videos redundancy through Semantic-Aware Smoothing to produce low-magnitude, quantization-friendly residuals. QVG further propose Progressive Residual Quantization, coarse-to-fine multi-stage scheme that reduces quantization error while enabling smooth qualitymemory trade-off. Across LongCat-Video, HY-WorldPlay, and Self-Forcing, QVG establishes new Pareto quality-memory frontier, reducing KV memory by up to 7.0 with < 4% end-to-end latency overhead and significantly better quality over baselines. spatiotemporal 1. Introduction Video diffusion models have progressed at remarkable pace. Powered by bidirectional attention backbones (e.g., HunyuanVideo (Wu et al., 2025), Wan2.1/Wan2.2 (Wan 1University of California, Berkeley 2Massachusetts Institute of Technology 3NVIDIA 4Amazon 5The University of Texas at Austin. Kurt Keutzer <keutzer@berkeley.edu>, Chenfeng Xu <xuchenfeng@utexas.edu>. Correspondence to: Preprint. February 4, 2026. 1 et al., 2025)), todays systems can synthesize short clips with compelling photorealism and coherent motion. Yet this quality leap has not translated into long-horizon capability: from 2024 to 2026, mainstream bidirectional-attention video diffusion models have largely remained confined to 5-10 second generations in practical settings, leaving persistent gap in deployment scenarios that demand minute-level, even hour-level continuity and interaction. central reason is the unfavorable generation scaling. Bidirectional video diffusion models perform inference in way that: at each denoising step, tokens attend to both past and future frames. From systems perspective, this induces late-commit execution model: early frames cannot be safely output until the full window finishes denoising. Auto-regressive video diffusion thus marks paradigm shift. By enforcing temporal causality, approaches such as CausVid (Yin et al., 2025) and Self-Forcing (Huang et al., 2025) turn the computation graph amortized, output frames depend only on retained history, so they can be incrementally committed and streamed as soon as they are produced. This shift has already opened up application regimes that are difficult to support with bidirectional attention, including live-streaming video generation (Feng et al., 2025), interactive content control (e.g, Matrix Game (He et al., 2025)), and long-horizon spatial exploration or 3D-consistent synthesis (e.g., world model pipelines (Xiao et al., 2025)). However, auto-regressive video models introduce systemalgorithm coupled problem: KV-cache memory. Specifically, auto-regressive inference accumulates large KV-cache that must remain resident to avoid KV-cache recomputation. In practice, KV-cache quickly dominates GPU memory and becomes the binding resource well before raw compute saturates (Team et al., 2025). For instance, generating 5second 480p video by LongCat-Video (Team et al., 2025) requires approximately 38K tokens, corresponding to roughly 34 GB of KV-cache, which already exceeds the memory capacity of single RTX 5090 GPU. As generation horizons lengthen, this constraint rapidly becomes hardware-limiting: even frontier world-model systems still limit generation to Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 1. QVG makes long video generation extremely memory-efficient and maintains high video quality. On LongCat-Video and HY-WorldPlay, QVG reduces the memory footprint by up to 7 and achieves PSNR of 28.7, much better than the baseline. around 60 seconds in practice*. Consequently, the memory bottleneck often determines whether these models can be broadly deployed. More importantly, KV-cache is not merely an efficiency bottleneck, it is also capability bottleneck. We observe strong correlation between context length and long-horizon consistency: retaining longer history in KV-cache substantially improves the preservation of identity, scene layout, and motion semantics over extended generation (Hong et al., 2025). In this paper, we tackle the memory bottleneck by quantizing the KV-cache in auto-regressive video models. Although KV-cache quantization is mature in LLM serving with extensive work (Liu et al., 2024; Hooper et al., 2024; Kang et al., 2024; Ashkboos et al., 2024), porting these techniques naively to video diffusion leads to severe quality degradation. The gap stems from fundamentally different activation statistics: video models exhibit substantially more heterogeneous numeric distributions across both token and channel dimensions ( 3.2), rendering the LLM-oriented assumptions brittle. To bridge this gap, we propose Quant VideoGen (QVG), training-free KV-cache quantization framework that achieves Pareto-frontier trade-off between generation quality and KV-cache memory footprint. The key observation behind QVG is that KV-cache of video models exhibits strong spatio-temporal redundancy (Xi et al., 2025; Yang et al., 2025), where tokens that are spatially or temporally adjacent tend to be numerically similar in latent space. Based on this observation, we propose Semantic-Aware Smoothing, which groups tokens based on their similarity in latent space before quantization, to mitigate the heterogeneity in the numeric distribution. We apply the k-means algorithm on KV-cache along the sequence length axis to form token groups. By subtracting the average value of each group (i.e., the centroid), the resulting residual tensors *https://labs.google/projectgenie have much smaller magnitude and are more homogeneous, making it quantization-friendly numeric distribution. To further reduce the quantization error, we propose Progressive Residual Quantization, scheme that compresses the residual tensors in multiple stages to further improve the performance. Inspired by streaming video codecs (Ma et al., 2017), which progressively encode multi-scale representations, QVG progressively groups residuals to capture information from coarse to fine granularity. This design enables flexible trade-off between quality and compression rate by varying the number of stages. We evaluate QVG on autoregressive video generation models, including LongCat-Video (Team et al., 2025), HYWorldPlay (Sun et al., 2025; HunyuanWorld, 2025), and Self-Forcing (Huang et al., 2025), primarily on H100 GPUs. Across models and benchmarks, QVG consistently outperforms state-of-the-art KV-cache quantization baselines, delivering higher visual quality at lower memory cost. Concretely, QVG reduces KV-cache memory by up to 7.0 while incurring minimal end-to-end latency overhead (< 4%), making it practical for real-world deployment. Notably, QVG makes it possible to run HY-WorldPlay-8B on single RTX 4090 for the first time, achieving PSNR > 29 relative to the BF16 reference, which was previously infeasible due to memory constraints. On the same hardware (e.g., H100), QVG further enables substantially longer effective KV-cache lengths for self-force (Huang et al., 2025), which translates into improved visual quality, even surpassing baseline BF16 under the original cache budget. 2. Related Work Auto-regressive video generation models and memory. Recent video generation models are progressively shifting from bidirectional, clip-level denoising (Wan et al., 2025; Wu et al., 2025) toward chunk-level auto-regressive generation (Yin et al., 2025; Huang et al., 2025), driven by the demand for long-horizon synthesis (Bruce et al., 2024; 2 Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 2. (a) Adopting full KV-cache can resolve the drifting problem but is very likely to be bottlenecked by memory. QVG can successfully generation high-quality long-videos. (b) Video diffusion models exhibit substantial spatiotemporal redundancy: tokens that are spatially or temporally adjacent have high cosine similarity, making compression feasible. HunyuanWorld, 2025) and real-time interaction (Feng et al., 2025). In auto-regressive video diffusion, frame chunks are generated sequentially with causal attention and KV caching, enabling substantially lower latency than offline diffusion. Beyond pushing the faster video generation, another major line of work pursues long-length generation. Training-free length extension methods reschedule noise or adjust temporal frequency to extrapolate pretrained models beyond their training horizon (Qiu et al., 2023; Lu et al.; Lu & Yang, 2025; Zhao et al., 2025). Complementarily, diffusioncausal hybrids (Chen et al., 2025; Song et al., 2025) improve variable-horizon conditioning and stabilize long rollouts, and are beginning to appear in streaming systems, powering applications ranging from world models and interactive agents to entertainment creation (HunyuanWorld, 2025; Sun et al., 2025; Polyak et al., 2024; Shin et al., 2025). Some works extend long-horizon generation through explicit memory mechanisms and chunked rollouts (Henschel et al., 2025; Kodaira et al., 2025; Xiao et al., 2025; Zhang et al., 2025b; 2026). However, we emphasize that longhorizon generation is not only an algorithmic memory challenge, but also system one: limited on-device hardware memory directly constrains how much algorithmic memory, i.e. KV-cache, can be retained. Consequently, consistent long-horizon generation is fundamentally limited by the amount of history memory we can maintain within limited hardware memory. Quantization-based KV-cache compression. Compressing KV-cache via quantization has been widely studied in LLMs, with diverse designs aimed at reducing memory footprint while preserving generation quality. KIVI (Liu et al., 2024) and KVQuant (Hooper et al., 2024) demonstrate that keys and values exhibit different statistics and propose tuning-free low-bit KV-cache quantization scheme that explicitly handles heavy-tailed outliers. Beyond explicit outlier handling, rotation-based approaches such as QuaRot (Ashkboos et al., 2024) and RotateKV (Su et al., 2025) apply structured transformations to smooth activation distributions before quantization. Several works also explore token-heterogeneous strategies, e.g., prioritizing subset of tokens to preserve quality under aggressive compression (Duanmu et al., 2024; He et al., 2024; Su & Yuan, 2025). Vector-quantization-based methods compress KV-cache by representing tokens with learned codebooks (Hooper et al., 2024; Zhang et al., 2025a; Li et al., 2025). While effective for LLMs, these methods do not explicitly exploit videospecific spatiotemporal redundancy ( 3.3) and are not tailored to the distinct numeric characteristics that make video KV-cache quantization particularly challenging ( 3.2). 3. Motivation 3.1. KV-Cache Bottlenecks Auto-Regressive Video Generation Video KV-cache is extremely memory-intensive. In autoregressive video diffusion models, the KV-cache grows linearly with the number of latent tokens and quickly dominates GPU memory usage for long-horizon, high-resolution generation. For model with layers and hidden dimension d, storing the KV-cache for video with spatial size and temporal length requires MemKV = 2 (HW ) ByteBF16. For example, in LongCat-Video (Team et al., 2025), 5second 480p context corresponds to roughly 38K latent tokens, resulting in KV-cache memory footprint of about 34 GB, compared to only 27 GB for model parameters. Thus, KV-cache capacity is the primary bottleneck for longhorizon video generation. Short-context results in inferior generation quality. Ex3 Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 3. (a-c) Semantic-Aware Smoothing effectively smoothing the KV-cache distribution to make it more regular and quantizationfriendly. We (1) group similar tokens together based on their semantic similarity and (2) subtract the centroid for each group to smooth the distribution. (d) The magnitude is significantly reduced and more concentrated around 0, making it much easier to be quantized. isting auto-regressive video systems commonly enforce fixed-length context window in their default inference configurations. For example, Wan distilled Self-Forcing (Huang et al., 2025) uses rolling KV-cache with default window of 21 latent frames, and HY-WorldPlay (HunyuanWorld, 2025; Sun et al., 2025) retains compact memory of only 20 frames. This truncation is primarily driven by GPU memory concerns, where full-context KV-cache becomes quickly infeasible for long-horizon generation. However, bounding the context effectively shrinks the models working memory, which can exacerbate long-horizon drift and limit revisitability and temporal consistency, as visualized in Figure 2(a). Therefore, we hope to address this memory bottleneck by quantizing the KV-cache to lower bit-precision. 3.2. Video KV-Cache Quantization is Challenging Quantization maps floating-point values into low-bit values to reduce the memory footprint. In this paper, we consider symmetric per-group integer quantization with bit-width b. The quantize and dequantize process is formulated as XINT, SX = Q(XBF16), ˆX = SX XINT (1) where XINT = (cid:25) (cid:22) XBF16 SX , SX = max(XBF16) 2b1 For any XBF16, the quantization error satisfies ˆx = SX RoundErr (cid:19) (cid:18) SX SX , (2) (3) where RoundErr(u) = u. Assuming each elements are independent, the expected error obeys E[x ˆx] SX , (4) so large scaling factors (e.g., caused by outliers) lead to large quantization error. 4 Crucially, this effect is exacerbated in auto-regressive video generation, where the KV-cache exhibits highly dynamic numeric ranges across both tokens and channels. Empirically, on Wan distilled Self-Forcing and LongCat-Video, we observe that max 1e2 and max 1e3. Beyond the exceptionally large numerical range, we also observe irregularity across the channel dimension at the token level: channels that are outliers for some tokens may not be outliers for others, as shown in Figure 3(a-b). This behavior is intrinsic to video models: tokens correspond to diverse spatial regions and motion patterns whose relevance evolves over time, leading attention projections to produce strongly non-uniform activation scales across space and time. 3.3. Video KV-Cache is Highly Redundant Spatiotemporal redundancy in video tokens. Video content exhibits strong redundancy across both time and space, and this redundancy is reflected in the latent tokens. As shown in Figure 2(b), for fixed spatial location (i.e., the same patch index), tokens from adjacent frames often remain highly similar because large portions of scene are static or evolve smoothly. Shown in Figure 2(c), spatially nearby patches also map to highly similar latent tokens: when two nearby patches have high pixel-level similarity, their corresponding latent tokens typically exhibit high cosine similarity as well. Progressive encoding of videos. Videos exhibit an inherent progressive structure that naturally supports residual-based representations. Temporal coherence allows most frames to be predicted at coarse level, capturing global layout and dominant motion, while finer details are introduced incrementally through residuals. As result, video content can be encoded progressively from coarse scene structure and color composition to fine-grained textures and high-frequency details. We quantitatively showcase this in Table 1. Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 4. Overview of QVG framework. (a) Original tensors distribution is irregular and hard to quantize. (b) Semantic-Aware Smoothing groups similar tokens and subtracts centroids for each group to make the residual quantization friendly. (c) Progressive Residual Quantization further lowers quantization error by iteratively applying Semantic-Aware Smoothing algorithm. (d) The final residual tensor becomes much easier to quantize and has much lower quantization error. 4. Methodology Based on these insights, we introduce QVG as reliable KVcache quantization technique for video generation. We first introduce Semantic-Aware Smoothing in 4.1 to smooth the KV-cache distribution and make it more quantizationfriendly, as visualized in Figure 3. We then introduce Progressive Residual Quantization in 4.2 to further improve the generation quality. Besides, we also discuss several algorithm-system co-design optimizations in 4.3. In Figure 4 we provide an overview of our proposed method. 4.1. Semantic-Aware Smoothing As discussed in 3.3, video tokens exhibit strong spatiotemporal redundancy. Semantic-Aware Smoothing exploits this redundancy to form semantically similar groups, reducing their magnitude and enabling accurate low-bit quantization. Semantic-based grouping. QVG processes the KV-cache in chunk-by-chunk manner, where each chunk consists of tokens from few frames. Consider chunk containing tokens (e.g., = HW Tc for height, width, and Tc latent frames). Let RN denote the corresponding KV-cache, with being the head dimension. We partition the tokens into groups using the k-means algorithm based on their hidden representations. This produces set of disjoint groups = {G1, G2, . . . , GC}, where each group Gi contains tokens with similar hidden-representations. As visualized in Figure 3(a), tokens within the same group exhibit significantly more homogeneous value distributions. We represent the mean value of each group (also known as centroid) as Ci Rd. Residual computation via centroid subtraction. Then for each group Gi, to make the distribution smoother, we subtract the centroid and obtain the residual: Ri = XGi Ci, Ri RGid, (5) where XGi refers to the matrix formed by tokens Gi. As discussed in 3.2, quantization error is proportional to the maximum value in the group. Due to the k-means clustering, these large values are expected to be shared across the group and are captured by Ci. Therefore, as shown in Figure 3, by subtracting the centroid the maximum value in each group becomes much smaller, which results in lower quantization error. As visualized in Figure 6, we successfully reduce the quantization error of Key Cache by 6.9, and reduce the quantization error of Value Cache by 2.6 on all precision choices. This proves the effectiveness of our method. Summarization and visualization. Smoothing process can be represented as follows: Semantic-Aware R, C, π = SA-Smoothing(X, C), (6) where RN is the residual tensor, RCd is the centroids, and π {1, . . . , C}N is the assignment vector that denotes the centroid assignment of each token. In Figure 3, we provide visualizations to illustrate the effectiveness of Semantic-Aware Smoothing. We first directly visualize the tensor in 2D plot in Figure 3 (a)-(c), and use gray lines to separate different groups for better visualization. As indicated in Figure 3 (b), the distribution becomes more regular after the semantic-based grouping. And as Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Table 1. Quality and Compression results of QVG and baselines. QVG-Pro achieves 4.97 5.20 compression ratio, while achieving much better fidelity scores than all baselines. QVG further pushes the compression ratio to 6.94 7.05 and still maintains nearlossless video quality scores. Method Compression Ratio (BF16) PSNR SSIM LPIPS Background Consistency Image Quality Subject Consistency Aesthetic Quality LongCat-Video-13B INT2 KV Cache, 480p RTN KIVI Quarot QVG-Pro QVG 6.40 6.40 6.40 4.97 6.94 20.872 20.317 21.573 30.376 28.716 0.719 0.719 0.759 0.935 0.909 LongCat-Video-13B INT4 KV Cache, 480p RTN KIVI Quarot QVG-Pro QVG 3.55 3.55 3.55 3.05 3.72 32.984 32.158 33.744 37.095 37.141 0.940 0.946 0.960 0.977 0.978 HY-WorldPlay-8B INT2 KV Cache, 480p RTN KIVI Quarot QVG-Pro QVG 6.40 6.40 6.40 5.20 7.05 24.199 24.272 25.207 31.562 29.174 0.696 0.701 0.738 0.923 0.882 HY-WorldPlay-8B INT4 KV Cache, 480p RTN KIVI Quarot QVG-Pro QVG 3.55 3.55 3.55 3.15 3.75 33.634 33.768 33.997 35.109 34.454 0.948 0.950 0.951 0.960 0.954 0.203 0.208 0.171 0.048 0. 0.045 0.040 0.033 0.024 0.024 0.229 0.230 0.205 0.069 0.094 0.056 0.055 0.053 0.048 0.051 96.22 84.84 84.84 86.12 96.20 95.06 96. 96.13 96.16 95.48 96.67 95.94 97.92 96.16 96.95 97.34 98.00 97.98 97.92 97.98 97.95 97.97 97.93 97.96 72. 59.60 38.10 50.70 71.74 71.47 72.72 72.14 72.67 72.47 72.66 72.34 74.33 71.86 71.40 72.26 74.15 73.87 74. 74.26 74.30 74.33 74.30 74.23 95.51 70.63 75.25 80.61 94.92 94.11 95.51 95.53 95.26 95.48 95.44 94.34 97. 96.08 95.89 96.64 97.96 97.90 97.90 97.87 97.95 97.90 97.88 97.90 64.83 43.38 41.58 49.49 63.88 62.22 64. 64.57 64.53 64.92 64.93 64.88 69.85 69.15 68.19 69.38 69.45 69.80 69.85 70.13 69.92 69.76 69.45 69.66 indicated in Figure 3 (c) and (d), the magnitude becomes much smaller after applying the Semantic-Aware Smoothing algorithm, confirming that Semantic-Aware Smoothing markedly compresses the dynamic range and is well-suited for low-bit quantization. 4.2. Progressive Residual Quantization Motivated by the progressive structure of videos ( 3.3), we then design Progressive Residual Quantization to further push the quantization error down in coarse-to-fine manner. Given the output of Semantic-Aware Smoothing, Progressive Residual Quantization iteratively re-quantize the residual tensor to capture finer-grained details. Progressive residual refinement. Formally, let R(0) = denote the initial input, and let be the total number of stages, each using the same number of groups C. At each stage t, we apply semantic-aware smoothing on the residual tensor to get new one: R(t), C(t), π(t) = SA-Smoothing(R(t1), C). quantize the final output into low-bit representation: XINT, SX = Q(R(T )) and store it into global memory. We also store all C(t) and π(t) with 1 , while all residuals R(t) are treated as intermediate results and discarded. Each stage focuses on re-quantizing the remaining residuals, enabling Progressive Residual Quantization to progressively model information from coarse semantic structures to finegrained variations, thereby leading to lower quantization error. The whole pipeline is visualized in Figure 4. The dequantization process. We first describe reconstruction for Semantic-Aware Smoothing, which naturally extends to the Progressive Residual Quantization setting. Given the stored centroids and assignment vector π, each token is reconstructed by adding back its assigned centroid to the corresponding residual. Specifically, let denote the residual tensor, the reconstruction of input token at index 1 is given by adding back the assigned centroid to the corresponding residual: After stages, we obtain the final residual tensor R(T ). We ˆXGi = Ri + Cπi. (7) 6 Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 5. (ab) Imaging Quality over long-horizon generation on Self-Forcing Model. Both QVG and QVG-Pro preserve near-lossless quality, while prior baselines degrade drastically. (c) The first stage of Progressive Residual Quantization yields the most significant reduction in MSE. Subsequent stages further reduce the error, but with diminishing returns. 5. Experiments 5.1. Setup Models. We evaluate QVG on open-sourced autoregressive video generation models including LongCatVideo-13B (Team et al., 2025), HY-WorldPlay-8B (HunyuanWorld, 2025), and Self-Forcing-Wan-1.3B (Huang et al., 2025) to generate videos with 480p resolution. LongCat-Video-13B conducts video continuation task based on fixed length context window of 73 frames and repeatedly generates the next 20 frames. HY-WorldPlay-8B and Self-Forcing-Wan-1.3B condition on the entire history of previously generated frames and generate new video segments in chunk-wise manner, with chunk sizes of 12 and 16 frames, respectively. Metrics. We evaluate the fidelity compared with the BF16 KV-cache baseline using PSNR, LPIPS, and SSIM. We evaluate the perceptual quality of the generated videos using VBench (Huang et al., 2023), and report the Background Consistency, Image Quality, Subject Consistency, and Aesthetic Quality. We report the KV-cache compression ratio to measure the memory footprint reduction. We also report the incurred overhead in the end-to-end generation pipeline. For the similarity experiments on LongCat-Video-13B, we report the number of the first generated chunk as the content starts to diverge while maintaining the same quality. All other metrics are reported under the long-generation setting. Datasets. We use the prompt suite provided by the MovieGen benchmark (Polyak et al., 2024). Specifically, we follow Self-Forcings official prompt settings. Baselines. We compare QVG with Round-to-Nearest Quantization (RTN), KIVI (Liu et al., 2024), and QuaRot (Ashkboos et al., 2024) as our baselines. For QuaRot, we only imhttps://github.com/guandeh17/SelfForcing/blob/main/prompts/MovieGenVideoBench_extended.txt Figure 6. Semantic-Aware Smoothing effectively reduces the quantization error by 6.9 and 2.6 for keys and values, respectively. Keys has higher MSE reduction since values cache are more irregular than keys cache. For Progressive Residual Quantization, reconstruction proceeds by iteratively applying this operation from stage to stage 1. Starting from the quantized output XINT and SX , we first dequantize then progressively restore ˆX (T 1), . . . , ˆX (0), where ˆX (0) corresponds to the final reconstructed tensor. This process is exactly the replay of the progressive quantization method. 4.3. Efficient Algorithm-System Co-design Fast k-means with streaming centroid caching. While k-means clustering is essential to semantic-aware smoothing, its iterative procedure and the k-means++ initialization can introduce non-trivial latency in streaming inference. We propose centroid caching approach to accelerate by initializing the centroid of new video chunk using the assignment strategy of the previous chunk. This strategy reduces the k-means overhead by 3. Dequantization kernel. We implement fused kernel that dequantizes the tensor and adds back the assigned centroids for all stages in Progressive Residual Quantization. The intermediate result is stored in registers to avoid repeatedly reading it from global memory. Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Figure 7. (a) Memory usage decomposition of QVG. (b-c) Trade-off curve of quantization block size for KV Cache. plement its KV cache quantization part and do not quantize the weights and activations. We use block size 16 settings for fair comparison. Implementation. We implement QVG with customized CUDA and Triton kernels and benchmark on NVIDIA H100 GPUs (CUDA 12.8). We use streaming chunk-wise compression to quantize KV-cache once per chunk and avoid re-compression drift, and adopt pre-RoPE key caching for more quantization-friendly key distributions (Hooper et al., 2024); we adopt FP8 E4M3 per-group scaling factors to reduce overhead. We evaluate INT2/INT4 under two configurations: QVG using S=1 and B=64, and QVG-Pro using S=4 stages and group size B=16. We set number of centroids K=256 to store assignment vectors in uint8. 5.2. Quality Evaluation In this section, we report the results of QVG on LongCatVideo-13B, HY-WorldPlay-8B, and Self-Forcing-Wan. As shown in Table 1, we report the results on LongCatVideo-13B and HY-WorldPlay-8B. QVG-Pro consistently achieves the best fidelity scores, while QVG delivers the largest compression ratios with only marginal quality degradation. On all VBench metrics, both QVG and QVG-Pro achieves near-lossless performance, while all baselines exhibit huge degradation, especially under the INT2 quantization setting. Notably, our method achieves 28.716 PSNR under 6.94 compression ratio for LongCat-Video-13B, and 29.174 PSNR under 7.05 compression ratio for HYWorldPlay-8B. These results demonstrate that our method can generate substantially higher-quality long videos with improved memory efficiency. We report the performance of the Self-Forcing model in Figure 5(a). Specifically, we measure the Image Quality score every 50 frames along long video sequences to evaluate whether QVG can mitigate long-horizon drift. While the BF16 KV-cache baseline also exhibits moderate quality degradation, both Quant VideoGen and Quant VideoGenPro maintain near-lossless quality even when extending to 700 frames. In contrast, all other baselines experience sharp degradation after approximately 100 frames. These 8 results demonstrate the effectiveness of our method in resisting long-horizon drift. 5.3. Efficiency Evaluation Memory usage breakdown. We analyze the memory footprint of QVG in detail by decomposing it into four components: quantized values, assignment vector, centroids, and scaling factors. As shown in Figure 7(a), under both INT2 and INT4 precision, quantized values account for the majority of memory usage ( 65%). Notably, QVG allocates larger proportion to quantized values compared to QVG-Pro, which results in higher compression ratio. End-to-end latency. We evaluate the end-to-end latency to quantify the overhead introduced by quantization and dequantization in our method. On LongCat, QVG increases the overall generation time by 2.1%. On HY-World, the end-to-end overhead is 1.5%, while on Self-Forcing it is 4.3%. These results indicate that QVG introduces only modest latency overhead and does not slow down the overall generation pipeline. 5.4. Sensitivity Test Number of quantization stages. We study the impact of the number of stages in Progressive Residual Quantization on the reduction ratio of MSE. As shown in Figure 5(b-c) the first stage provides the dominant reduction in error, resulting in 5.83 MSE reduction compared with naive quantization method. Although subsequent stages decrease MSE by at least 1.10, their contributions gradually decreases as the stage count grows. Quantization group size. We test the impact of quantization block size on the performance of QVG, ranging from 16 to 64. We vary the number of kmeans stages from 1 to 4 to get trade-off curve. larger block size leads to higher compression ratio, but also lower quality. As shown in Figure 7(b-c), block size of 64 achieves the best trade-off, while block size of 16 guarantees the best quality. Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization 6. Conclusion In this paper, we propose Quant VideoGen (QVG), training-free KV-cache quantization framework that leverages video-specific spatiotemporal redundancy to mitigate the memory bottleneck in auto-regressive video generation. We propose Semantic-Aware Smoothing that groups semantically similar tokens and subtracts group centroids to produce quantization-friendly residuals. We then propose Progressive Residual Quantization to further reduce quantization error in coarse-to-fine manner. Across multiple video models and benchmarks, QVG achieves up to 7.04 KV-cache compression with 4% latency overhead while preserving near-lossless visual quality. These results demonstrate that QVG enables practical, memory-efficient long-video and world generation."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213100240, 2024. 2, 3, 7 Bruce, J., Dennis, M., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., Aytar, Y., Bechtle, S., Behbahani, F., Chan, S., Heess, N., Gonzalez, L., Osindero, S., Ozair, S., Reed, S., Zhang, J., Zolna, K., Clune, J., de Freitas, N., Singh, S., and Rocktäschel, T. Genie: Generative interactive environments, 2024. URL https://arxiv.org/abs/2402.15391. 2 Chen, B., Martí Monsó, D., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Nexttoken prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:24081 24125, 2025. 3 Duanmu, H., Yuan, Z., Li, X., Duan, J., Zhang, X., and Lin, D. Skvq: Sliding-window key and value cache quantization for large language models. arXiv preprint arXiv:2405.06219, 2024. 3 tion, 2025. URL https://arxiv.org/abs/2511. 07399. 1, 3 He, X., Peng, C., Liu, Z., Wang, B., Zhang, Y., Cui, Q., Kang, F., Jiang, B., An, M., Ren, Y., Xu, B., Guo, H.- X., Gong, K., Wu, S., Li, W., Song, X., Liu, Y., Li, Y., and Zhou, Y. Matrix-game 2.0: An open-source realtime and streaming interactive world model, 2025. URL https://arxiv.org/abs/2508.13009. He, Y., Zhang, L., Wu, W., Liu, J., Zhou, H., and Zhuang, B. Zipcache: Accurate and efficient kv cache quantization with salient token identification. Advances in Neural Information Processing Systems, 37:6828768307, 2024. 3 Henschel, R., Khachatryan, L., Poghosyan, H., Hayrapetyan, D., Tadevosyan, V., Wang, Z., Navasardyan, S., and Shi, H. Streamingt2v: Consistent, dynamic, and extendable long video generation from text, 2025. URL https: //arxiv.org/abs/2403.14773. 3 Hong, Y., Mei, Y., Ge, C., Xu, Y., Zhou, Y., Bi, S., HoldGeoffroy, Y., Roberts, M., Fisher, M., Shechtman, E., Sunkavalli, K., Liu, F., Li, Z., and Tan, H. Relic: Interactive video world model with long-horizon memory, 2025. URL https://arxiv.org/abs/2512.04040. 2 Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami, A. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2024. 2, 3, 8 Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 1, 2, 4, 7 Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., and Liu, Z. Vbench: Comprehensive benchmark suite for video generative models, 2023. URL https://arxiv.org/abs/2311. 17982. HunyuanWorld, T. Hy-world 1.5: systematic framework for interactive world modeling with real-time latency and geometric consistency. arXiv preprint, 2025. 2, 3, 4, 7 Feng, T., Li, Z., Yang, S., Xi, H., Li, M., Li, X., Zhang, L., Yang, K., Peng, K., Han, S., Agrawala, M., Keutzer, K., Kodaira, A., and Xu, C. Streamdiffusionv2: streaming system for dynamic and interactive video generaKang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna, T., and Zhao, T. Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. 2 9 Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization Kodaira, A., Hou, T., Hou, J., Tomizuka, M., and Zhao, Y. Streamdit: Real-time streaming text-to-video generation, 2025. URL https://arxiv.org/abs/2507. 03745. Li, J., Zhang, Y., Hassan, M. Y., Chafekar, T., Cai, T., Ren, Z., Guo, P., Karimzadeh, F., Reed, C., Wang, C., and Gan, C. Commvq: Commutative vector quantization for kv cache compression, 2025. URL https://arxiv. org/abs/2506.18879. 3 Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. 2, 3, 7 Lu, Y. and Yang, Y. Freelong++: Training-free long video generation via multi-band spectralfusion. arXiv preprint arXiv:2507.00162, 2025. URL https:// arxiv.org/abs/2507.00162. 3 Lu, Y., Liang, Y., Zhu, L., and Yang, Y. Freelong: Trainingfree long video generation with spectralblend temporal attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 3 Ma, T., Ma, M., Lee, Y. H., and Hu, F. Bitstreamoriented protection for the h.264/scalable video coding (svc). Wirel. Pers. Commun., 97(4):51155135, December 2017. doi: 10.1007/ ISSN 0929-6212. s11277-017-4771-5. URL https://doi.org/10. 1007/s11277-017-4771-5. 2 Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 3, Qiu, H., Xia, M., Zhang, Y., He, Y., Wang, X., Shan, Y., and Liu, Z. Freenoise: Tuning-free longer video diffusion via noise rescheduling, 2023. 3 Shin, J., Li, Z., Zhang, R., Zhu, J.-Y., Park, J., Shechtman, E., and Huang, X. Motionstream: Real-time video generation with interactive motion controls, 2025. URL https://arxiv.org/abs/2511.01266. 3 Song, K., Chen, B., Simchowitz, M., Du, Y., Tedrake, R., and Sitzmann, V. History-guided video diffusion, 2025. URL https://arxiv.org/abs/2502.06764. 3 quantization for llms via outlier-aware adaptive rotations. arXiv preprint arXiv:2501.16383, 2025. 3 Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world model. arXiv preprint, 2025. 2, 3, 4 Team, M. L., Cai, X., Huang, Q., Kang, Z., Li, H., Liang, S., Ma, L., Ren, S., Wei, X., Xie, R., et al. Longcat-video technical report. arXiv preprint arXiv:2510.22200, 2025. 1, 2, 3, Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2 Wu, B., Zou, C., Li, C., Huang, D., Yang, F., Tan, H., Peng, J., Wu, J., Xiong, J., Jiang, J., et al. Hunyuanvideo 1.5 technical report. arXiv preprint arXiv:2511.18870, 2025. 1, 2 Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., Chen, J., Stoica, I., Keutzer, K., and Han, S. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. URL https://arxiv.org/abs/2502.01776. 2 Xiao, Z., Lan, Y., Zhou, Y., Ouyang, W., Yang, S., Zeng, Y., and Pan, X. Worldmem: Long-term consistent world simulation with memory, 2025. URL https://arxiv. org/abs/2504.12369. 1, 3 Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., Chen, J., Han, S., Keutzer, K., and Stoica, I. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation, 2025. URL https://arxiv.org/abs/ 2505.18875. 2 Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models, 2025. URL https://arxiv.org/abs/2412.07772. 1, Zhang, H., Ji, X., Chen, Y., Fu, F., Miao, X., Nie, X., Chen, W., and Cui, B. Pqcache: Product quantization-based kvcache for long context llm inference, 2025a. URL https://arxiv.org/abs/2407.12820. 3 Su, Z. and Yuan, K. Kvsink: Understanding and enhancing the preservation of attention sinks in kv cache quantization for llms. arXiv preprint arXiv:2508.04257, 2025. 3 Zhang, L., Cai, S., Li, M., Wetzstein, G., and Agrawala, M. Frame context packing and drift prevention in nextframe-prediction video diffusion models, 2025b. URL https://arxiv.org/abs/2504.12626. 3 Su, Z., Chen, Z., Shen, W., Wei, H., Li, L., Yu, H., and Yuan, K. Rotatekv: Accurate and robust 2-bit kv cache Zhang, L., Cai, S., Li, M., Zeng, C., Lu, B., Rao, A., Han, S., Wetzstein, G., and Agrawala, M. Pretraining frame Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization preservation in autoregressive video memory compression, 2026. URL https://arxiv.org/abs/2512. 23851. 3 Zhao, M., He, G., Chen, Y., Zhu, H., Li, C., and Zhu, J. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025."
        }
    ],
    "affiliations": [
        "MIT",
        "MIT-IBM Watson AI Lab",
        "Stanford University",
        "Tsinghua University",
        "UC Berkeley"
    ]
}