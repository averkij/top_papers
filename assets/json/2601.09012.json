{
    "paper_title": "TranslateGemma Technical Report",
    "authors": [
        "Mara Finkelstein",
        "Isaac Caswell",
        "Tobias Domhan",
        "Jan-Thorsten Peter",
        "Juraj Juraska",
        "Parker Riley",
        "Daniel Deutsch",
        "Cole Dilanni",
        "Colin Cherry",
        "Eleftheria Briakou",
        "Elizabeth Nielsen",
        "Jiaming Luo",
        "Kat Black",
        "Ryan Mullins",
        "Sweta Agrawal",
        "Wenda Xu",
        "Erin Kats",
        "Stephane Jaskiewicz",
        "Markus Freitag",
        "David Vilar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 2 1 0 9 0 . 1 0 6 2 : r 2026-01-"
        },
        {
            "title": "TranslateGemma Technical Report",
            "content": "Google Translate Research Team We present TranslateGemma, suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ two-stage fine-tuning process. First, supervised fine-tuning is performed using rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation. 1. Introduction In an increasingly interconnected world, machine translation (MT) plays pivotal role in bridging language barriers, facilitating global communication, and democratizing access to information. The development of large language models (LLMs) has significantly advanced the state-ofthe-art in MT. However, progress is greatly benefitted by the availability of strong, open models that allow for transparency, reproducibility, and community-driven innovation. To this end, we present TranslateGemma, an open variant of the Gemma 3 foundation model (Gemma Team, 2025), specifically enhanced for machine translation. While Gemma 3 is already potent multilingual LLM, TranslateGemma has been further refined to deliver superior translation quality. This improvement is achieved through two-stage process: Supervised Finetuning (SFT) on diverse corpus of parallel data (Section 3) and Reinforcement Learning (RL) from human and model-based feedback (Section 4). Our SFT approach leverages blend of humantranslated and synthetically-generated parallel texts, carefully curated to improve translation quality without compromising the models general capabilities. The RL phase employs combination of reward models designed to optimize translation quality. We demonstrate the efficacy of TranslateGemma on the WMT25 and WMT24++ datasets, showing substantial gains across 55 language pairs. Furthermore, TranslateGemma retains the inherent multimodal capabilities of the original Gemma 3 model. Our experiments on the Vistra corpus (Salesky et al., 2024) indicate that the enhancements in text translation also positively impact image translation performance, showcasing the models versatility. We believe the release of TranslateGemma will provide valuable resource for researchers and practitioners in the field of machine translation. 2. Training data We use two types of data for the training of the models, most of it shared between the SFT and RL phases. 2.1. Synthetic Gemini-Generated Translation"
        },
        {
            "title": "Data",
            "content": "Our goal is to generate high-quality synthetic data for each language, as this has been shown Corresponding author(s): vilar@google.com. See Contributions section for full author list. 2026 Google. All rights reserved TranslateGemma Technical Report to greatly improve translation quality (Finkelstein et al., 2024). As the source of monolingual data we use the MADLAD-400 corpus (Kudugunta et al., 2023). This data comes from the SMOL (Caswell et al., 2025) and GATITOS (Jones et al., 2023) datasets. SMOL covers 123 languages and GATITOS covers 170. We aim to produce up to 10K synthetic examples per language pair. In order to select the source sentences that potentially benefit more from the synthetic data generation, we first bucket the original segments by length. We then sample each bucket to obtain 1 million source segments for each language pair we wish to generate synthetic data for. We then run preliminary filtering step across these source segments where we take 2 samples from Gemini 2.5 Flash, once using greedy decoding and once sampled with temperature of 1.0 and compare their scores according to MetricX 24-QE (Juraska et al., 2024). We select the sources where the sample achieves the largest improvement over the greedy decoding. The intuition behind this source filtering approach is that we wish to select sources that will benefit the most from 128-sample QE decoding, so we use 2 samples as low-cost approximation. After this selection process, for each of the sources for each language pair we generate 128 samples from Gemini 2.5 Flash and then apply MetricX 24-QE filter to select the best-performing examples. We generate translations of two distinct lengths this way: individual sentences and text blobs of up to 512 tokens. This way we aim to support both translations of individual segments as well as longer texts. For generating these translations we used the same prompt as we used for further training (see Section 5.2). In order to avoid formatting issues or erroneous translations, we apply an additional formatting filtering step, again based on Gemini 2.5 Flash. This methodology was applied for all language pairs that are covered by WMT24++ (Deutsch et al., 2025) plus an additional set of 30 language pairs that are specified in Appendix B. 2.3. Language distribution The final proportion of languages for the SFT and RL phases can be found in Figure 1. For RL we used the same translation data as for SFT, except for GATITOS and SMOL that were used in SFT only. We provide the full list of languages that were included in training in Appendix C. 2.4. Generic Instruction-Following Data Our SFT mixture also includes 30% generic instruction-following data from the original Gemma 3 mixture. The purpose of including this data is to prevent the model from overfitting to the translation task and to maintain generic instruction-following capabilities. 3. Supervised Fine-Tuning For supervised fine-tuning (SFT), we begin with the released Gemma 3 27B, 12B and 4B checkpoints. We use parallel data including both human-generated texts as well as synthetic data generated by Gemini (Gemini Team, 2025), as described in Section 2. In addition we use generic instruction-following data. We use the Kauldron SFT tooling1 to fine-tune the Gemma 3 checkpoints. For fine-tuning we use the AdaFactor optimizer (Shazeer and Stern, 2018) with learning rate of 0.0001 and batch size of 64, running for 200k steps. We update all model parameters, but freeze the embedding parameters, as preliminary experiments indicated this helped with translation performance for languages and scripts not covered in the SFT data mix. 2.2. Human-Generated Translation Data 4. Reinforcement Learning To increase the diversity and script coverage of the data we also include data for additional lowerresource languages. For these languages, we opt to use human-generated parallel data instead. We performed reinforcement learning on top of the SFT checkpoint, using an ensemble of metrics 1https://kauldron.readthedocs.io/en/latest/ 2 TranslateGemma Technical Report (a) SFT data mixture. (b) RL data mixture. Figure 1 Language distribution in the TranslateGemma data mixtures measured as model tokens. as reward models, to further boost translation quality. We used the following metrics as reward models during RL: MetricX-24-XXL-QE (Juraska et al., 2024), learned, regression-based translation metric producing floating point score between 0 (best) and 25 (worst), matching the standard Multidimensional Quality Metrics (MQM) score range (Freitag et al., 2021). MetricX scores were linearly rescaled, using 5.0 score, when computing rewards, so that higher scores indicate better quality. Although MetricX can take source, reference, and hypothesis as input, we used it as QE metric by passing in an empty reference. Gemma-AutoMQM-QE, finetuned AutoMQM model (Fernandes et al., 2023). This model was initialized from the Gemma 327B-IT checkpoint (Gemma Team, 2025), and was trained on MQM ratings data from WMT 2020 - WMT 2023 (Freitag et al., 2021; Lommel et al., 2014). Default MQM weights (Freitag et al., 2021) were used in computing (token-level) rewards from AutoMQM outputs. As with MetricX, it ignores the reference translation. ChrF (Popović, 2015), lexical overlap-based translation metric. This was the only reward model for which the (synthetic) references were used. ChrF scores were scaled by factor of two to be on approximately the same scale as the other rewards. Naturalness Autorater developed in-house, using the base RL policy model as prompted LLM-as-a-Judge. As with AutoMQM, this Autorater elicited span-level annotations. This Autorater was instructed to penalize spans in the machine-translated text which did not sound like they were produced by native speaker (conditioned on the naturalness errors in the output not stemming from an unnatural source input). Generalist reward model covering many tasks, including reasoning, instruction following, and multilingual abilities, adapted from the general Gemma 3 post-training setup (Gemma Team, 2025). We used RL algorithms extended to support token-level advantages, which were added to the advantages computed from sequence-level rewards. This allowed us to use fine-grained, span-level reward signals from AutoMQM and the Naturalness Autorater directly, for improved credit assignment and training efficiency in the spirit of Ramos et al. (2025). See Figure 2 for an illustration of how MetricX and AutoMQM rewards were (additively) combined during advantage computation. The combined advantages were then batch-normalized. 5. Automatic Evaluation 5.1. Text translation We evaluate TranslateGemma using MetricX 24 (Juraska et al., 2024) and Comet22 (Rei et al., 3 TranslateGemma Technical Report Figure 2 Illustration of how sequence-level and token-level rewards are additively combined during advantage computation in RL. Note that advantage is computed from sequence-level rewards as reward-to-go, meaning that rewards are broadcast uniformly to every token. Table 1 Automatic evaluation results using MetricX and Comet22 (C22) on WMT24++."
        },
        {
            "title": "Size System",
            "content": "MetricX C22 27B Gemma 3 12B Gemma"
        },
        {
            "title": "4.04\nTranslateGemma 3.09\n4.86\nTranslateGemma 3.60\nGemma 3\n6.97\nTranslateGemma 5.32",
            "content": "83.1 84.4 81.6 83.5 77.2 80.1 4B 2022). The TranslateGemma models consistently show improved translation quality compared to the baseline Gemma 3 models across all evaluated sizes and metrics, as detailed in Table 1. For the 27B parameter model, the TranslateGemma version attains an average MetricX score of 3.09, substantial reduction from the baseline Gemma 3s score of 4.04. This represents relative decrease of approximately 23.5%, signaling marked increase in translation fidelity. Similar trends are observed for the other model sizes. The 12B TranslateGemma model achieves MetricX of 3.60, down from 4.86 for the baseline (a 25.9% reduction), while the 4B TranslateGemma model scores 5.32, compared to 6.97 for the baseline (a 23.6% reduction). Comet22 confirms the trend of improvements for the TranslateGemma model. In addition, this shows that improvements carry over to metrics not explicitly optimized for in the RL phase. For instance, the 12B TranslateGemma model shows score of 83.5, up from 81.6. The 4B TranslateGemma model exhibits even larger increases, with Comet22 rising from 77.2 to 80.1. The effect of model scale on performance is also apparent. As expected, larger models tend to produce better translations within both the baseline and TranslateGemma series. However, the enhancements brought by the TranslateGemma finetuning are such that smaller TranslateGemma models can achieve performance levels comparable to or even exceeding those of larger baseline models. Notably, the 12B TranslateGemma model surpasses the performance of the larger 27B baseline Gemma 3 model. Similarly, the 4B TranslateGemma model achieves comparable results to the 12B baseline Gemma 3 model. This efficiency gain allows for high-quality translation with reduced computational resources."
        },
        {
            "title": "A more granular analysis of MetricX scores for",
            "content": "4 TranslateGemma Technical Report each of the 55 language pairs, presented in Appendix A, reveals that the improvements of TranslateGemma are consistent across all 55 language pairs evaluated. Some example improvements for specific languages are Table 2 Automatic evaluation results using MetricX and Comet22 (C22) for image translation performance, on the Vistra corpus. The scores are the average of translating from English into German, Spanish, Russian and Chinese. English to German: 1.63 down to 1.19, English to Spanish: 2.54 down to 1.88, English to Hebrew: 3.90 down to 2.72, English to Swahili: 5.92 down to 4.45, English to Lithuanian: 6.01 down to 4.39, English to Estonian: 6.40 down to 4.61 and English to Icelandic: 8.31 down to 5.69. These examples highlight the models improved ability to handle diverse range of languages, both for high-resource languages (e.g. German, English) as well as low-resource ones (e.g. Icelandic, Swahili). We also hypothesize that the 27B model, with its higher capacity, will have benefited more from the vast amount of languages seen during the SFT phase (detailed in Appendix C), although we do not have direct experimental confirmation of this. 5.2. Prompting the Model The model has been trained using the prompt shown in Figure 3, which is also the prompt we used in our evaluations. We recommend using the same prompt for producing new translations. Tools for automatically wrapping the text with it are provided in the model repository. Size System MetricX C22 27B Gemma 12B Gemma 3 2.03 Gem. Translate 1.58 2.33 Gem. Translate 2.08 Gemma 3 2.60 Gem. Translate 2.58 76.1 77.7 74.9 72.8 69.1 70.7 4B prompt asking it to translate the text in it.2 In particular, we did not include any other information about the text, like its location in the image or previous OCR pass. The results, presented in Table 2, show that TranslateGemma retains the image processing capabilities of the base Gemma 3 models. The improvements in translation quality attained by TranslateGemma carry over for this task, with the exception of the 12B model measured in Comet22. We see MetricX score improvements of nearly 0.5 points in the case of the 27B model, or 0.25 for the 12B model. The smaller 4B model obtains only small improvements when compared to the baseline, probably due to its limited capacity. 5.3. Image Translation 6. Human Evaluation We used the Vistra benchmark (Salesky et al., 2024) to assess whether the models retained their ability to translate text within images after our additional training steps. Note that no multimodal training data was used in the SFT or RL steps reported in this work. In order to simplify the evaluation protocol, we selected only images that, according to the reference, contained single instance of text. This resulted in set of 264 images. An example is shown in Figure 4. The input to the model was just the image together with We conduct an additional human evaluation on limited set of language directions to measure TranslateGemmas translation performance. We do so using MQM (Freitag et al., 2021; Lommel et al., 2014), human evaluation framework where professional translators highlight error spans in translations, with document context, assigning severity and category to each, with score being automatically derived by counting 2The model release also includes an interface for image translation, similar to the one for text translation. 5 TranslateGemma Technical Report You are professional {source_lang} ({src_lang_code}) to {target_lang} ({tgt_lang_code}) translator. Your goal is to accurately convey the meaning and nuances of the original {source_lang} text while adhering to {target_lang} grammar, vocabulary, and cultural sensitivities. Produce only the {target_lang} translation, without any additional explanations or commentary. Please translate the following {source_lang} text into {target_lang}:nnn{text} Figure 3 Preferred prompt when using the model. source_lang refers to the source language name, e.g. English, src_lang_code to the source language code, e.g. en-US, target_lang to the target language, e.g. German, and tgt_lang_code to the target language code, i.e. de-DE. different language families and writing systems. The source data is all taken from the WMT25 translation task, using the literary, news, and social domains. For all language pairs, we evaluated TranslateGemma 12B and 27B, as well as Gemma 3 27B. To avoid issues with rater fatigue, each document in the dataset was truncated at paragraph boundaries to have no more than 12 source sentences, skipping documents with more than 12 sentences in the first paragraph. However, for the literary domain, where each document is an entire book chapter, documents were split into chunks of 1 or more paragraphs up to the 12-sentence limit, with each chunk being human-evaluated in isolation. Following Riley et al. (2024), we used pseudo-SxS rater assignment, where all system outputs for particular source document were evaluated by the same rater. The results can be found in Table 3. For most language pairs, the human evaluation confirms the trend we see on the automatic metrics, with TranslateGemma clearly outperforming Gemma 3. There are two exceptions: when the target language is German, where both models are on par, and JapaneseEnglish where TranslateGemma actually suffers regression. Looking into the error categorization, we found that this is due to mistranslation of named entities, while other error categories did improve. The improvements for TranslateGemma are especially relevant for low-resource language pairs. E.g. for English to Marathi we obtain an improvement of 1.6 points, or 1.0 for English to Swahili or Figure 4 Example of the pictures included in the Vistra benchmark. the errors with weighting scheme. We collected the annotations using the open-source Anthea tool.3 We evaluate the models in 10 language pairs, from 3 distinct source languages: English to German English to Chinese (Simplified) English to Italian English to Serbian (Cyrillic) English to Korean English to Swahili (Kenyan) English to Marathi Czech to Ukrainian Czech to German Japanese to English We selected this set to have mix of highand low-resource languages, in addition to having 3https://github.com/google-research/ google-research/tree/master/anthea TranslateGemma Technical Report Table 3 MQM results of the human evaluation for TranslateGemma and Gemma 3. Lower scores are better. TranslateGemma Gemma 3 Language Pair EnglishItalian EnglishGerman EnglishMarathi EnglishKorean EnglishSwahili CzechUkrainian EnglishChinese EnglishSerbian CzechGerman JapaneseEnglish 27B 1.8 2.3 3.1 3.1 4.2 5.3 6.3 8.7 10.3 13.4 12B 2.0 3.2 4.6 4.6 5.2 8.5 8.4 15.8 11.4 15.7 27B 2.5 2.2 4.7 3.8 5.2 6.3 7.4 10.4 10.2 11.6 Czech to Ukrainian. The human evaluation also confirms the performance difference between the 27B and 12B TranslateGemma models already demonstrated by the automatic metrics. That said, the 12B model still stays competitive with the bigger Gemma 3 model, especially for highresource languages. 7. Conclusions In this work, we introduced TranslateGemma, series of open models based on Gemma 3, specifically enhanced for machine translation. Through combination of supervised fine-tuning on diverse, high-quality parallel datablending human and synthetic sourcesand novel reinforcement learning approach utilizing an ensemble of reward models, we have improved translation performance across wide spectrum of languages and model sizes (4B, 12B, and 27B parameters). Our automatic evaluations on the WMT24++ dataset, encompassing 55 language pairs, show consistent gains for TranslateGemma models over the baseline Gemma 3 models both in MetricX and Comet22. We observed strong performance increase across various language types, including high-resource languages like German and Spanish, and lower-resource languages such as Icelandic or Swahili. key finding is the enhanced efficiency of the TranslateGemma models, where smaller fine-tuned models often match or exceed the performance of larger baseline models, offering better trade-off between quality and computational cost. Furthermore, we have shown that TranslateGemma models retain the multimodal capabilities of the original Gemma 3. Experiments on the Vistra benchmark indicate that the improvements in text translation extend to the image translation task, particularly for the 12B and 27B models, without any specific multimodal fine-tuning. The release of the TranslateGemma models contributes valuable set of open-source tools for the machine translation community, fostering further research and application development. We believe these models will serve as strong foundation for variety of translation-related tasks and encourage their adoption and exploration."
        },
        {
            "title": "References",
            "content": "Isaac Caswell, Elizabeth Nielsen, Jiaming Luo, Colin Cherry, Geza Kovacs, Hadar Shemtov, Partha Talukdar, Dinesh Tewari, Baba Mamadi Diane, Djibrila Diane, Solo Farabado Cissé, Koulako Moussa Doumbouya, Edoardo Ferrante, Alessandro Guasoni, Christopher Homan, Mamadou K. Keita, Sudhamoy Deb7 TranslateGemma Technical Report Barma, Ali Kuzhuget, David Anugraha, Muhammad Ravi Shulthan Habibi, Sina Ahmadi, Anthony Munthali, Jonathan Mingfei Liu, and SMOL: ProfessionJonathan Eng. 2025. ally translated parallel data for 115 underrepresented languages. In Proceedings of the Tenth Conference on Machine Translation, pages 11031123, Suzhou, China. Association for Computational Linguistics. Daniel Deutsch, Eleftheria Briakou, Isaac Rayburn Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. 2025. WMT24++: Expanding the language coverage of WMT24 to 55 languages & dialects. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1225712284, Vienna, Austria. Association for Computational Linguistics. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation, pages 10661083, Singapore. Association for Computational Linguistics. Mara Finkelstein, David Vilar, and Markus Freitag. 2024. Introducing the NewsPaLM MBR and QE dataset: LLM-generated high-quality parallel data outperforms traditional web-crawled data. In Proceedings of the Ninth Conference on Machine Translation, pages 13551372, Miami, Florida, USA. Association for Computational Linguistics. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460 1474. Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, et al. 2025. Gemma 3 technical report. Alexander Jones, Isaac Caswell, Orhan Firat, and Ishank Saxena. 2023. GATITOS: Using new multilingual lexicon for low-resource machine translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 371405, Singapore. Association for Computational Linguistics. Juraj Juraska, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. 2024. MetricX-24: The Google submission to the WMT 2024 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 492504, Miami, Florida, USA. Association for Computational Linguistics. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: multilingual and document-level large audited dataset. Advances in Neural Information Processing Systems, 36:6728467296. Arle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014. Multidimensional quality metrics (mqm): framework for declaring and describing translation quality metrics. Tradumàtica: tecnologies de la traducció, (12):455463. Maja Popović. 2015. chrF: character n-gram Fscore for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Gemini Team, Gheorghe Comanici, Eric Bieber, Ice Pasupat, Noveen Mike Schaekermann, Miguel Moura Ramos, Tomás Almeida, Daniel Vareta, Filipe Azevedo, Sweta Agrawal, Patrick 8 TranslateGemma Technical Report Fernandes, and André F. T. Martins. 2025. Finegrained reward optimization for machine translation using error severity mappings. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: UnbabelIST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Parker Riley, Daniel Deutsch, George Foster, Viresh Ratnakar, Ali Dabirmoghaddam, and Markus Freitag. 2024. Finding replicable human evaluations via stable ranking probability. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 49084919, Mexico City, Mexico. Association for Computational Linguistics. Elizabeth Salesky, Philipp Koehn, and Matt Post. 2024. Benchmarking visually-situated translation of text in natural images. In Proceedings of the Ninth Conference on Machine Translation, pages 11671182, Miami, Florida, USA. Association for Computational Linguistics. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 45964604. PMLR."
        },
        {
            "title": "Contributions",
            "content": "Core Contributors Mara Finkelstein Isaac Caswell Tobias Domhan Jan-Thorsten Peter Juraj Juraska Parker Riley Daniel Deutsch Lead David Vilar Markus Freitag Contributors"
        },
        {
            "title": "Cole Dilanni\nColin Cherry\nEleftheria Briakou\nElizabeth Nielsen\nJiaming Luo\nKat Black\nRyan Mullins\nSweta Agrawal\nWenda Xu",
            "content": "Support"
        },
        {
            "title": "Erin Kats\nStephane Jaskiewicz",
            "content": "9 TranslateGemma Technical Report A. Automatic metrics per language Table 4 Comparison of performance of the TranslateGemma (GT) models with baseline Gemma models (G3) for each language pair in the WMT24++ set using MetricX. enar_EG enar_SA enbg_BG enbn_IN enca_ES encs_CZ enda_DK ende_DE enel_GR enes_MX enet_EE enfa_IR enfi_FI enfil_PH enfr_CA enfr_FR engu_IN enhe_IL enhi_IN enhr_HR enhu_HU enid_ID enis_IS enit_IT enja_JP enkn_IN enko_KR enlt_LT enlv_LV enml_IN enmr_IN ennl_NL enno_NO enpa_IN enpl_PL enpt_BR enpt_PT enro_RO enru_RU ensk_SK ensl_SI ensr_RS ensv_SE ensw_KE ensw_TZ enta_IN ente_IN enth_TH entr_TR enuk_UA enur_PK envi_VN enzh_CN enzh_TW enzu_ZA TranslateGemma Gemma 3 27B 12B 4B 27B 12B 4B 2.54 2.42 2.80 1.88 3.18 3.48 2.11 1.19 2.57 1.88 4.61 1.99 3.19 2.98 2.21 2.19 4.69 2.72 3.52 2.05 4.24 2.07 5.69 1.88 3.53 4.18 2.81 4.39 5.69 3.64 3.17 1.67 2.09 3.67 4.14 2.13 2.55 2.86 2.18 3.81 3.55 2.78 2.00 4.45 4.30 2.87 3.76 2.33 4.18 2.98 3.12 1.97 1.86 2.04 6.99 27B 2.78 2.66 3.25 2.12 3.58 4.03 2.45 1.36 3.34 2.06 6.15 2.28 3.77 3.17 2.37 2.44 4.93 3.12 3.69 2.31 5.00 2.17 7.93 2.17 3.82 4.78 2.97 5.41 7.22 4.30 3.47 2.01 2.38 4.44 4.58 2.36 2.68 3.25 2.48 4.54 4.24 2.68 2.31 5.36 5.25 2.98 3.97 2.66 4.64 3.29 3.59 2.20 2.07 2.21 10.73 3.57 3.43 4.30 2.92 5.20 5.41 3.25 1.93 4.66 2.51 11.03 3.34 5.68 4.20 2.92 2.97 6.32 4.99 4.33 3.17 7.84 2.63 15.54 2.64 4.44 7.11 3.93 9.58 12.12 7.33 4.30 2.84 3.26 5.53 5.64 2.93 3.09 4.18 3.25 6.70 7.12 5.66 3.14 10.65 10.30 3.90 4.83 3.49 6.17 4.16 5.67 2.87 2.66 2.77 18.29 3.32 3.19 3.90 2.56 4.07 4.62 3.00 1.63 3.73 2.54 6.40 2.98 4.19 3.62 2.78 2.78 5.27 3.90 4.11 2.62 5.51 2.72 8.31 2.60 4.11 5.09 3.43 6.01 7.55 4.77 4.11 2.48 2.94 4.40 5.17 2.90 3.39 3.99 3.01 5.04 4.56 3.75 2.73 5.92 5.73 3.53 4.41 2.96 5.32 3.79 3.86 2.56 2.47 2.63 9. 3.70 3.64 4.47 2.87 4.89 5.32 3.36 1.93 4.34 2.75 8.89 3.41 5.11 4.03 2.97 3.01 5.79 4.41 4.36 3.08 6.79 2.84 12.16 2.84 4.30 6.82 3.79 7.71 9.90 6.84 4.60 2.82 3.17 5.99 5.64 3.15 3.78 4.39 3.39 5.86 5.73 3.76 3.06 7.90 7.85 3.83 4.74 3.19 6.02 4.28 4.86 2.88 2.61 2.80 14.80 4.60 4.49 5.81 3.86 6.97 7.47 4.40 2.72 6.31 3.35 14.78 4.77 7.54 5.22 3.76 3.90 7.67 6.70 5.03 4.26 10.75 3.27 19.22 3.60 5.09 10.48 4.72 13.39 15.75 11.89 5.64 3.87 4.23 11.20 7.07 3.77 4.13 5.70 4.54 9.17 9.39 6.94 4.14 14.05 13.89 5.04 5.76 4.14 8.03 5.40 7.80 3.62 3.27 3.62 21.52 12B 4B 27B 12B 4B TranslateGemma Gemma 3 10 TranslateGemma Technical Report B. Additional synthetic data languages For the following languages we created synthetic data, in addition to the languages covered by WMT24++: English-Armenian, English-Hawaiian, English-Western Frisian, English-Corsican, English-Hmong, English-Maltese, English-Tajik, English-Samoan, English-Macedonian, English-Mongolian, EnglishGalician, English-Albanian, English-Uzbek, English-Uyghur, English-Belarusian, English-Sinhala, English-Basque, English-Haitian Creole, English-Bosnian, English-Kyrgyz, English-Kazakh, EnglishKhmer, English-Scottish Gaelic, English-Lao, English-Irish, English-Luxembourgish, English-Burmese, English-Sundanese, English-Javanese, English-Malay. C. Full list of languages for SFT Table 5 shows the languages paired with English in both directions, Table 6 the languages paired with English as source language and Table 7 the languages pairs not involving English. Together these three tables give the full language coverage of the SFT data used for TranslateGemma. 11 TranslateGemma Technical Report Table 5 Languages paired with English in both directions. Abkhaz (ab) Afrikaans (af) Assamese (as) Aymara (ay) Balinese (ban) Banjar (bjn) Batak Simalungun (bts) Bhojpuri (bho) Breton (br) Cantonese (yue) Chhattisgarhi (hne) Chuukese (chk) Dari (fa-AF) Dogri (doi) Dzongkha (dz) Egyptian Arabic (arz) Fon (fon) Ga (gaa) Guarani (gn) Hindi (hi) Igbo (ig) Isoko (iso) Jingpo (kac) Kanuri (kr) Kashmiri (ks) Kikuyu (ki) Kokborok (trp) Krio (kri) Lahnda Punjabi (Pakistan) (pa-Arab) Ligurian (lij) Lombard (lmo) Magahi (mag) Malay (Jawi Script) (ms-Arab) Mapudungun (arn) Meadow Mari (chm) Mizo (lus) Mundari (Devanagari script) (unr-Deva) Nepalbhasa (Newari) (new) North Levantine Arabic (apc) Occitan (oc) Papiamento (pap) Rohingya (Latin script) (rhg-Latn) Sambalpuri (spv) Saraiki (skr) Shan (shn) Sicilian (scn) South Ndebele (nr) Surjapuri (sjp) Sylheti (syl) Tetum (tet) Tiv (tiv) Tswana (tn) Turkish (tr) Venda (ve) Waray (Philippines) (war) Yakut (sah) Acehnese (ace) Ahirani (ahr) Assyrian Neo-Aramaic (aii) Badaga (bfq) Baluchi (bal) Baoul00e9 (bci) Batak Toba (bbc) Bikol (bik) Buginese (bug) Chakma (Latin script) (ccp-Latn) Chichewa (ny) Chuvash (cv) Dhivehi (dv) Dombe (dov) East Circassian (kbd) Ewe (ee) French (fr) Garo (Latin script) (grt-Latn) Hakha Chin (cnh) Ho (Warang Chiti script) (hoc-Wara) Ilocano (ilo) Italian (it) Kiche (quc) Kapampangan (pam) Kedah Malay (meo) Kiluba (Luba-Katanga) (lu) Komi (kv) Kumaoni (kfy) Latgalian (ltg) Limbu (Limbu script) (lif-Limb) Luganda (lg) Maithili (mai) Mam (mam) Marshallese (mh) Meiteilon (Manipuri) (mni-Mtei) Modern Standard Arabic (ar) NKo (bm-Nkoo) Nepali (ne) North Ndebele (nd) Oromo (om) Polish (pl) Romani (rom) Sango (sg) Sepedi (nso) Sherpa (Tibetan script) (xsr-Tibt) Silesian (szl) Spanish (es) Susu (sus) Tahitian (ty) Thai (th) Tok Pisin (tpi) Tulu (tcy) Tuvan (tyv) Venetian (vec) West Circassian (ady) Yoruba (yo) Acholi (ach) Alur (alz) Avar (av) Bagheli (bfy) Bambara (bm) Bashkir (ba) Bemba (Zambia) (bem) Bodo (India) (brx) Bundeli (bns) Chamorro (ch) Chinese (zh-CN) Crimean Tatar (Cyrillic script) (crh) Dhundari (dhd) Dutch (nl) Eastern Huasteca Nahuatl (nhe) Faroese (fo) Friulian (fur) German (de) Hausa (ha) Hunsrik (hrx) Indonesian (id) Jamaican Patois (jam) Kalaallisut (kl) Karakalpak (kaa) Khasi (kha) Kinyarwanda (rw) Kongo (kg) Kurdish (Sorani) (ckb) Lepcha (lep) Limburgish (li) Luo (luo) Makassar (mak) Mandeali (mjl) Marwadi (mwr) Mewari (mtr) Moor00e9 (mos) Navajo (nv) Nigerian Pidgin (pcm) Northern Sami (se) Ossetian (os) Qeqchi (kek) Rundi (rn) Sanskrit (sa) Sesotho (st) Shina (scl) Sindhi (Devanagari script) (sd-Deva) Sudanese Arabic (Deprecated BCP) (apd) Swahili (sw) Tamazight (Latin Script) (ber-Latn) Tibetan (bo) Tonga (Tonga Islands) (to) Tumbuka (tum) Twi (ak) Vietnamese (vi) Wolof (wo) Yucatec Maya (yua) Afar (aa) Amharic (am) Awadhi (awa) Bagri (bgq) Banjar (Arabic script) (bjn-Arab) Batak Karo (btx) Betawi (bew) Braj (bra) Buryat (bua) Chechen (ce) Chittagonian (ctg) Crimean Tatar (Latin script) (crh-Latn) Dinka (din) Dyula (dyu) Efik (efi) Fijian (fj) Fulani (ff) Goan Konkani (gom) Hiligaynon (hil) Iban (iba) Inuktut (Syllabics) (iu) Japanese (ja) Kangri (xnr) Kashmiri (Devanagari script) (ks-Deva) Kiga (cgg) Kituba (DRC) (ktu) Korean (ko) Kurukh (kru) Libyan Arabic (ayl) Lingala (ln) Madurese (mad) Malagasy (mg) Manx (gv) Mauritian Creole (mfe) Minang (min) Morrocan Arabic (ar-MA) Ndau (ndc-ZW) Nimadi (noe) Nuer (nus) Pangasinan (pag) Quechua (qu) Russian (ru) Santali (Latin Script) (sat-Latn) Seychellois Creole (crs) Shona (sn) Somali (so) Surgujia (sgj) Swati (ss) Tamazight (Tifinagh Script) (ber) Tigrinya (ti) Tsonga (ts) Tunisian Arabic (aeb) Udmurt (udm) Wagdi (wbr) Xhosa (xh) Zapotec (zap) Table 6 Languages from English Albanian (sq) Belarusian (be) Chinese (Taiwan) (zh-TW) Estonian (et) Greek (el) Hmong (hmn) Javanese (jv) Lao (lo) Malay (ms) Norwegian (no) Romanian (ro) Serbian (Latin) (sr-Latn) Swahili (Kenya) (sw-KE) Telugu (te) Arabic (Egypt) (ar-EG) Bosnian (bs) Corsican (co) Filipino (fil) Gujarati (gu) Hungarian (hu) Kannada (kn) Latvian (lv) Malayalam (ml) Persian (fa) Samoan (sm) Sinhala (si) Swahili (Tanzania) (sw-TZ) Tshiluba (Luba-Lulua) (lua) Armenian (hy) Bulgarian (bg) Croatian (hr) Finnish (fi) Haitian Creole (ht) Icelandic (is) Kazakh (kk) Lithuanian (lt) Maltese (mt) Portuguese (Brazil) (pt-BR) Santali (Ol Chiki script) (sat) Slovak (sk) Swedish (sv) Ukrainian (uk) Bangla (bn) Burmese (my) Czech (cs) French (Canada) (fr-CA) Hawaiian (haw) Inuktut (Latin) (iu-Latn) Khmer (km) Luxembourgish (lb) Marathi (mr) Portuguese (Portugal) (pt-PT) Scottish Gaelic (gd) Slovenian (sl) Tajik (tg) Urdu (ur) Basque (eu) Catalan (ca) Danish (da) Galician (gl) Hebrew (he) Irish (ga) Kyrgyz (ky) Macedonian (mk) Mongolian (mn) Punjabi (pa) Serbian (Cyrillic) (sr-Cyrl) Sundanese (su) Tamil (ta) Uyghur (ug) Table 7 Non-English language pairs Amharic (am)Arabic (ar) Cantonese (yue)Mandarin Chinese (zh) Czech (cs)German (de) Amharic (am)Mandarin Chinese (zh) Cantonese (yue)Taiwanese Mandarin (zh-Hant) Czech (cs)Ukrainian (uk) Arabic (ar)Swahili (sw) Chinese (zh-CN)Japanese (ja) Mandarin Chinese (zh)Swahili (sw)"
        }
    ],
    "affiliations": [
        "Google"
    ]
}