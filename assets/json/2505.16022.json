{
    "paper_title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning",
    "authors": [
        "Wei Liu",
        "Siya Qi",
        "Xinyu Wang",
        "Chen Qian",
        "Yali Du",
        "Yulan He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training."
        },
        {
            "title": "Start",
            "content": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning Wei Liu Kings College London Siya Qi Xinyu Wang Chen Qian Yali Du Yulan He(cid:66) Shanghai Jiao Tong University The Alan Turing Institute {wei.4.liu, yali.du, yulan.he}@kcl.ac.uk 5 2 0 2 1 2 ] . [ 1 2 2 0 6 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances, such as DeepSeek R1-Zero, highlight the effectiveness of incentive training, reinforcement learning paradigm that computes rewards solely based on the final answer part of language models output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding, where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7%. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training1."
        },
        {
            "title": "Introduction",
            "content": "Recent progress in Large Language Model (LLM) reasoning has been accelerated by incentive training (Guo et al., 2025; Xie et al., 2025; Yu et al., 2025; Zeng et al., 2025; Liu et al., 2025a; Yuan et al., 2025b; Hu et al., 2025), new Reinforcement Learning (RL) paradigm which optimizes models by computing rewards only on the final answer part in model response, and incentivize models to generate intermediate tokens like reasoning steps spontaneously. Notably, methods such as DeepSeek R1-Zero (Guo et al., 2025) have shown that in domains like mathematics and coding, using simple rule-based verifier to calculate reward and 1Code available at https://github.com/thinkwee/NOVER 1 Figure 1: Comparison of NOVER with other posttraining paradigms. perform RL-only incentive training can achieve impressive performance, known as Reinforcement Learning with Verifiable Reward (RLVR). It has been demonstrated that such incentivized reasoning ability can generalize to areas like multi-modality (Shen et al., 2025; Zhang et al., 2025; Yang et al., 2025; Feng et al., 2025b) or language agent (Xia and Luo, 2025; Jin et al., 2025; Song et al., 2025; Wang et al., 2025; Feng et al., 2025a). However, the success of incentive training with RLVR hinges on external verifiers that can judge the correctness of model outputs. For many highly contextualized and hard-to-grade tasks requiring similar reasoning or cognitive capability, ranging from social behaviour analysis to creative writing, such verifiers are infeasible or hard to construct (Weng, 2025). Recent works have explored training general-purpose large verifier models across diverse domains (Ma et al., 2025; Seed et al., 2025; Su et al., 2025). However, building accurate verifier models involves complex pipeline, which is significantly more costly than traditional reward Figure 2: Examples of Qwen2.5-7B-NOVER on range of text-to-text tasks, demonstrating its ability to handle open-ended questions such as Discuss, Why, or Create. These tasks often require free-form answers that are difficult to verify with clear-cut labels, posing challenges for incentive training. models for preference alignment (Liu et al., 2025b). Moreover, once trained, these verifier models demand substantial computational resources to be deployed during the subsequent RL training. To address these limitations of RLVR, we propose NOVER, NO-VERifier Reinforcement Learning, novel framework for incentive training without an external verifier. As shown in Figure 1, compared with traditional Supervised Fine-Tuning (SFT), NOVER can perform incentive training similar to RLVR; compared to Reinforcement Learning from Human Feedback (RLHF), NOVER does not need trained reward model for verification; and compared with RLVR, NOVER extends the incentive training to areas where reliable verifier is hard to acquire. NOVER requires only standard SFT data and utilizes the model itself to build reward proxy for lightweight RL training. By calculating perplexity-based reward based on the models reasoning process, it enables incentivedriven reinforcement learning across wide range of text-to-text tasks, as shown in Figure 2. Our main contributions are as follows: 1. We introduce NOVER, verifier-free incentive training framework that generalizes RLVR to arbitrary text-to-text tasks with minimal SFT data. 2. Our experiments and analysis demonstrate NOVERs superior performance. It is stable to train compared with model-as-verifier methods and mitigates reward hacking, shapes reasoning patterns, and adapts to diverse tasks. 3. NOVERs flexibility enables imaginative approaches such as inverse incentive training. This paradigm teaches model how to fish rather than simply give it fish, surpassing standard incentive training on tasks that need creativity."
        },
        {
            "title": "2 Related Work",
            "content": "Language Model Reasoning Early research designed prompting techniques and workflows based on human cognitive priors to enhance the reasoning capabilities of LLMs. Chain-of-Thought (CoT) prompting (Wei et al., 2022) enabled step-by-step reasoning, later extended by Zero-Shot CoT (Kojima et al., 2022) and Self-Consistency (Wang et al., 2023b). More recent methods, such as Tree of Thoughts (Yao et al., 2023), Least-to-Most (Zhou et al., 2023), Plan-and-Solve (Wang et al., 2023a), Sketch-Navigation (Liu et al., 2024), and MultiPerspective Self-Reflection (Yan et al., 2024), introduced structured exploration over reasoning trajectories. Beyond prompting, verification-based approaches were proposed (Cobbe et al., 2021) to solve math reasoning. Neuro-symbolic methods (Pan et al., 2023) fuse LLMs with symbolic solvers. Previous works also utilize Process Reward Models (Lightman et al., 2024) combined with Monte Carlo Tree Search (Kocsis and Szepesvári, 2006) for step-level exploration to replicate OpenAIs o1 (Jaech et al., 2024). Incentive Training In contrast to the above methods, DeepSeek-R1 (Guo et al., 2025) proposed 2 simpler paradigm called incentive training. Building on this idea, recent works such as Logic-RL (Xie et al., 2025), DAPO (Yu et al., 2025), SimpleRL (Zeng et al., 2025), OpenReasoner (Hu et al., 2025), Dr.GRPO (Liu et al., 2025a), and VAPO (Yuan et al., 2025b) investigated best practices on aspects such as exploration-exploitation, emergence of \"aha\" moments, and task difficulty variance. Several recent efforts attempted to extend incentive training beyond math and coding (He et al., 2025; Lu et al., 2025; Gurung and Lapata, 2025; Su et al., 2025; Ma et al., 2025). However, these approaches often rely on domain-specific rules for verification or involve training large verifier models, which can be computationally expensive."
        },
        {
            "title": "3 Background",
            "content": "Rule-based Reward Given training prompt with template (Guo et al., 2025) asking to generate intermediate tokens (e.g., reasoning steps) followed by final answer a, incentive training aims to use RL to optimize model πθ with outcome reward Rrule, which is solely computed by rulebased verifier : {0, 1} on a: Rrule = v(a) (1) Such sparse but accurate outcome reward encourages the model to autonomously generate intermediate tokens that could lead to better a. In reasoning tasks, for example, may be reasoning behaviours like task decomposition or self-reflection. To ensure the verifier can easily parse outputs, DeepSeek-R1 introduces tag format reward Rf , which requires to be enclosed in <think> tags and in <answer> tags: Rf = fformat(t, a) (2) where fformat : {0, 1} is the regular expression based format check function. Then the combined reward is: R(p, t, a) = wrule Rrule + wf Rf (3) GRPO After the calculation of reward, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is then used to optimize πθ. For each prompt p, the model rolls out group of completions = {(t1, a1), ..., (tG, aG)}. The groupnormalized advantage is: Ai = R(p, ti, ai) µC σC (4) 3 where µC and σC are the mean and standard deviation of rewards in the group. Then the policy is updated with clipped objective: JGRPO(θ) = E{p,ti,ai} (cid:32) 1 G (cid:88) i=1 rclip βDKL(πθ πref) rclip = min (ri, clip(ri, 1ϵ, 1+ϵ)) Ai ri = πθ(ti, ai p) πθold (ti, ai p) (cid:33) (5) (6) (7) where ϵ is the clip ratio and β is the KL penalty weight. is the policy ratio (Schulman et al., 2017). This framework enables LLMs to develop explicit, high-quality reasoning processes without dense supervision for intermediate steps."
        },
        {
            "title": "4 Method",
            "content": "Figure 3: The overall process of NOVER. It can be seen in Equation 1 that the incentive training described above requires verifier to compute Rrule. NOVER eliminates the need for verifier by introducing reasoning perplexity as reward proxy, which can be calculated on any SFT data. 4.1 Reasoning Perplexity as Proxy The core idea of NOVER is that the perplexity of the ground truth answer, conditioned on the models reasoning trajectory, can serve as natural proxy for reward. Given prompt p, the ground truth answer g, and model response consisting of reasoning tokens and answer tokens a, we compute the reasoning perplexity Pr using proxy model πp as follows: (cid:32) Pr(p, t, g) = exp (cid:80)g i=1 log πp(gi p, t, g<i) (t) (cid:33) (8) where denotes the number of tokens in the ground truth. (t) is simple normalization factor based on the length of the reasoning tokens, which alleviates the length bias of perplexity: (t) = max (1, 1 + log(t)) (9) lower reasoning perplexity Pr indicates higher probability of generating the correct answer based on the reasoning tokens. As the calculation of Pr is done in teacher-forcing way without any decoding, computation of Pr requires only 5% of total training time (see details in Appendix A. Then Pr can be utilized to calculate the reward defined in 4.3. 4.2 Policy-Proxy Synchronization Unlike recent work that employs an extra frozen model as the proxy (Gurung and Lapata, 2025), we instead use the policy model πθ itself as the proxy πp. This choice reflects their shared goal: minimizing the perplexity of correct answers given high-quality reasoning. Moreover, an extra frozen proxy can diverge from the evolving policy, leading to inconsistency (see 6.3). Importantly, use πθ as πp does not mean the model acts as both athlete and referee, as πp itself does not judge but leverages the objective ground truth for fair evaluation. In practice, we initialize both πθ and πp from the same pretrained checkpoint and periodically sync πθ to πp every Tsync steps via exponential smoothing, following TR-DPO (Gorbatovski et al., 2025). πp α πp + (1 α) πθ (10) where α [0, 1] denotes the synchronization rate. Such synchronization enables the proxy model to gradually adapt to improvements in the policy while ensuring stable calculation of perplexity. Since we employ LoRA (Hu et al., 2022) for efficient training, the πθ and πp share the same base model and can be switched seamlessly by replacing the LoRA adapter, which comprises only about 0.4% of the full model parameters. 4.3 Verifier-Free Reward Given prompt and its corresponding group of completions (ti, ai), we compute two rewards based on the reasoning perplexity Pr. Reasoning Reward Reasoning perplexity Pr can serve directly as reward, as it reflects the quality of the reasoning trajectory. VR-CLI (Gurung and Lapata, 2025) and reasoning advantage SFT (Foster et al., 2024) suggest calculating relative perplexity gain to stabilize training, but we found that GRPOs group normalization already achieves similar calculation (see Appendix E), so there is no need for such redundant operation. In practice, we observe that relative perplexity improvements still vary widely across different samples, causing high variance in clip ratios and unstable training. To mitigate this, we propose to discretize reasoning perplexity into reasoning rewards Rr. For each completion (pi, ti, ai) with r, we compute Rr based on its rank among nvalid valid completions. Let = {P 1 , 2 }, sorted as (1) (nvalid) , reasoning rewards are assigned by quantile rank: , ..., nvalid Rr(pi, ti, ai) = (cid:40) nvalidrank(P )+1 nvalid 0, , if rank(P otherwise ) (11) In practice, we use either = 1 or = nvalid. The former assigns reward of 1.0 only to the best (lowest-perplexity) completion, which is suitable for tasks with objectively correct answers. The latter distributes rewards to all completions based on their normalized rank and is better suited for subjective or open-ended tasks. Efficiency Reward Recent work (Sui et al., 2025) suggests that longer reasoning is not always better, and effective reasoning length is what matters. We introduce an Efficiency Reward Re, encouraging the model to generate shorter but better reasoning trajectories. completion should be rewarded if it achieves lower reasoning perplexity with fewer reasoning tokens compared to other completions. The Re can be calculated as: Re(pi, ti, ai) = (cid:80)n j=1,j=i 1 (cid:2)P ti < tj(cid:3) < nvalid 1 (12) It is notable that although Re shares similar target with Rr, where lower perplexity is better, we can not use Re to replace Rr since in the early training stage, most completions could not develop efficient reasoning to acquire the Re. These two rewards are then combined with the tag format reward Rf in Equation 2 to form the final reward for GRPO training. The combined reward function is then: Rtotal = wf Rf +I(Rf = 1)(wrRr +weRe) (13) where wf , wr, we are weights that control the relative importance of each reward component. All these rewards are in the range [0, 1]. Notably, they are conditionally dependent rather than simply additive: only when the tag format is correct (Rf = 1) are the other two rewards calculated and added; otherwise, they are set to 0. We provide detailed analysis of how this design effectively prevents the 4 \"curse of proxy\" in 6.3. Although our experiments are conducted using GRPO for efficiency, NOVER can be combined with various RL algorithms, such as PPO (Schulman et al., 2017)."
        },
        {
            "title": "5 Experimental Setup",
            "content": "Dataset To minimize data contamination from pretraining, we prioritized the use of the most recent datasets available. We focused on general reasoning datasets that require multi-step inference and span multiple domains, not limited to STEM. In addition, we included text generation datasets that demand reasoning and planning abilities, such as creative writing, social intelligence, and multilingual translation. To evaluate the effectiveness of the NOVER-incentivized model, we select datasets spanning four broad domains: (1) General Reasoning: This category includes challenging tasks that go beyond standard STEM benchmarks, requiring complex reasoning, including three datasets, Natural Reasoning (Yuan et al., 2025a), General Thought (General Reasoning, 2025), and WebInstruct (TIGER-Lab, 2025), which require models to produce factually grounded answers, often involving multi-step reasoning and justification. (2) Creative Writing: To assess long-form narrative generation and planning ability, we use SS-GEN (Feng et al., 2025c), benchmark focused on coherent, socially themed storytelling. (3) Social Intelligence: We evaluate models on emotionally and socially grounded reasoning using EmoBench (Sabour et al., 2024) and ToMBench (Chen et al., 2024), which test emotion recognition, social action prediction, and theory-of-mind reasoning. (4) Multilingual Ability: We employ the OPUS book corpus (Tiedemann, 2012), which includes translation tasks across 16 languages and 64 source-target language pairs, to evaluate cross-lingual reasoning and generalization. We construct training and test sets for all datasets and subcategories via uniform sampling. All datasets were cleaned and filtered to ensure high-quality data. Specifically: Natural Reasoning We excluded samples lacking reference answer. We also filtered out samples where the reference answer was merely number, single word, or single sentence, as such cases are often trivial for rule-based verifiers and do not reflect the openended reasoning tasks we aim to study. General Thoughts and WebInstruct These datasets underwent the same preprocessing as Natural Reasoning. Whats more, these two datasets contain multiple-choice questions, and we converted them into free-form QA format, ensuring that the reference answers included both the correct choice and its content. Due to inconsistent option formatting in the original data (such as 1., A), A, (a).), we designed seven regex-based patterns to clean and standardize the multiple-choice items. EmoBench We selected two subsets, emotional understanding and emotional application. For emotional understanding, the prompt was adapted to specify dual-option response format, reflecting the original structure in which each question is paired with two multiple-choice items (with four candidates each), targeting emotion recognition and causality, respectively. FANToM We chose two subsets aligned with free-form reasoning, which are full_fact (asking for an entity) and full_belief_gen (asking for opinion in the perspective of somebody). Other Datasets No preprocessing was applied to the remaining datasets. For datasets with existing train/test splits, we retained them. For others, we created splits by evenly sampling across subcategories (e.g., academic subjects) to ensure distributional consistency. For very large datasets, we sample 2,000 examples for training, 1,000 for validation, and 1,000 for testing. Dataset #Original #Filtered #Train #Validation #Test NR GT WI SGN EB TB OPUS FT 1,150,000 431,000 232,000 5,087 400 2,860 1,250,632 1,863 192,178 78,483 231,833 5,087 400 2,860 1,250,632 1, 2,000 2,000 2,000 4,070 272 1,432 2,000 1,308 1,000 1,000 1,000 509 571 1,000 1,000 1,000 1,000 508 128 857 1,000 555 Table 1: Dataset statistics after filtering and splits into training, validation, and test sets. Baselines We evaluate 3B and 7B versions of Qwen 2.5 (Yang et al., 2024) against several baselines. Prior work suggests that instructionfollowing and basic CoT reasoning can emerge during multi-stage pretraining (Wei et al., 2022; Zeng et al., 2025). Thus, we include both vanilla and CoT responses to isolate the effect of NOVER incentive tuning from capabilities already acquired in pretraining. We also consider an SFT baseline, where Method Qwen2.5-3B Base odel + CoT + SF + OV ER Qwen2.5-7B Base odel + CoT + SF + OV ER General Reasoning Writing Social Intelligence Multilingual NR GT WI SGN EB TB OPUS 21.80 % 43.10 % 18.40 % 18.70 % 32.03 % 46.79 % 24.40 % 48.90 % 24.20 % 14.76 % 28.12 % 51.23 % 27.00 % 36.20 % 27.30 % 20.08 % 36.72 % 48.66 % 28.60 % 60.30 % 28.10 % 41.64 % 38.28 % 57.88 % 31.80 % 48.50 % 20.70 % 24.21 % 28.91 % 44.22 % 31.20 % 57.60 % 29.20 % 33.46 % 38.28 % 50.99 % 27.50 % 45.20 % 33.50 % 37.85 % 47.66 % 57.06 % 38.20 % 61.80 % 36.60 % 50.79 % 49.22 % 67.79 % Avg. 28.22 % 27.57 % 30.47 % 39.36 % 31.09 % 34.62 % 38.87 % 47.31 % 35.58 % 46.43 % 39.62 % 16.70 % 1.40 % 17.30 % 20.70 % 19.30 % 1.60 % 23.30 % 26.80 % 19.90 % 23.50 % 8.20 % Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct R1-Distill-Qwen-7B 27.10 % 50.00 % 31.50 % 21.25 % 40.62 % 58.69 % 29.90 % 56.20 % 35.60 % 67.72 % 46.88 % 65.23 % 41.00 % 60.20 % 38.00 % 40.16 % 35.16 % 54.61 % Table 2: Overall performance. Each cell is shaded based on its relative improvement. Values in bold indicate the best-performing variant, excluding other post-trained models. NR: Natural Reasoning, GT: General Thoughts-430k, WI: WebInstruct, SGN: SS-GEN, EB: EmoBench, TB: TomBench, OP: OPUS-BOOK-TRANSLATION. the same pretrained checkpoints are fine-tuned on each datasets training split (identical to that used by NOVER). This comparison highlights whether NOVER enhances generalization and abstraction in contrast to SFT (Chu et al., 2025). For reference, we report results from three strong post-trained models: Qwen2.5-3B/7B-Instruct, and R1-DistillQwen-7B, 7B variant distilled from DeepSeekR1 671B (Guo et al., 2025). These models, trained with massive instruction-following and reasoning data, serve as high-performance baselines. Notation Definition αLoRA pdropout dtype β wf wr we τ ηmax α Tsync Lgen ϵ ϵhigh Tmax LoRA rank LoRA scaling factor LoRA dropout ratio Training precision Group Size Batch size per GPU KL coefficient Format reward weights Reasoning reward weights Efficiency reward weights Rollout temperature Maximum learning rate Synchronization coefficient Synchronization interval Max completion length clip range Upper clip range Max training steps Value 16 32 0.1 bfloat16 8 8 0.1 1 1 1 0.6 1e5 0.9 100 [512, 2048] 0.1 0.2 5000 Table 3: Hyperparameters used for NOVER training. Training and Evaluation Our training framework is built on Huggingface TRL (von Werra et al., 2020), using LoRA adapters (Hu et al., 2022) for efficient fine-tuning. All hyperparameters for training NOVER are listed in Table 3. For general reasoning tasks and the SS-GEN creative writing task, we set the maximum completion length to 2048, while for other tasks, it is limited to 512. Training is run for up to 5,000 steps, although we typically apply early stopping when the validation reward, especially the tag format reward, no longer improves. For 3B models, training generally requires more steps than for 7B models. Training time is mainly determined by rollout efficiency, which depends on the maximum generation length and varies by task. For example, training Qwen2.5-7B for 1,000 steps with max length of 1,024 tokens on two H100 GPUs takes about 2 hours. For evaluation, we report accuracy on all datasets. For choice questions, we extend the extractor from Team et al. (2025) to identify both the option letter and corresponding text, then match against ground truth. For openended QA, we use Gemini-2.0-flash (Anil et al., 2023; Gemini et al., 2024) in an LLM-as-a-Judge setup. All results are averaged over three runs."
        },
        {
            "title": "6 Results and Analysis",
            "content": "We present comprehensive and structured experiments organized into three main parts. (1) To examine when NOVER performs well, 6.1 analyzes its performance across different task types, and 6.2 investigates how data distribution influence reasoning ability. (2) To understand why NOVER is effective, 6.3 examines how NOVER addresses reward hacking for stable learning; 6.4 compares its verifier-free design with Model as Verifier approaches; and 6.5 studies how incentivized reason6 Figure 4: Accuracy of NOVER on three general reasoning tasks by the distribution of different aspects. ing patterns evolve during training. (3) To demonstrates NOVERs flexibility, in 6.6 we explored inverse incentive training for creative writing. 6.1 Overall Performance Table 2 summarizes the performance of NOVER and all baselines. We observe that, for both 3B and 7B model sizes, NOVER consistently outperforms all baselines across all types of tasks. Remarkably, NOVER enables the 3B model to achieve nearly 60% accuracy on General Thoughts, approaching the performance of the much larger R1-DistillQwen-7B model distilled from 671B teacher. In Appendix C, we give detailed example on how reasoning perplexity discriminates between good and bad responses, thus helping the model to reinforce learning. The improvement is particularly pronounced on datasets that challenge areas where less data is pretrained, such as EmoBench (for social intelligence) and OPUS (for multilingual). In these areas, direct CoT prompting may reduce accuracy. Our manual inspection of CoT outputs reveals that the base model is capable of generating well-structured and fluent CoT. However, these CoT-generated rationales can be easily hallucinated (Huang et al., 2025; Ye et al., 2024; Li et al., 2024). In contrast, NOVER effectively corrects such hallucinated reasoning processes, as illustrated by detailed example in Appendix B. Notably, SFT sometimes even underperforms the base model, as it encourages the model to directly map questions to answers without an explicit reasoning process. In contrast, the CoT, NOVER, and even the base model can generate intermediate reasoning tokens that aid in answering. 6.2 When and Where for Effective Reasoning Question format We analyze NOVERs performance distribution on the general reasoning area, as shown in Figure 4. For question format, the accuracy on multiple-choice questions remains consistently higher than that on open-ended QA. This is primarily because the presence of candidate options in the question effectively reduces the search space during RL optimization, thereby lowering the task difficulty. We further extract and analyze the key action words from the questions. The model achieves higher accuracy on questions with clear solution directions, such as find, determine, and classify, since the corresponding reasoning chains are more likely to be sampled and reinforced during training. In contrast, for questions with more flexible requirements and less prior constraint on the reasoning process, such as analyze, the models performance is relatively weaker. Through the lens of length We also analyze the effect of length across four components: question, reference, generated answer, and reasoning process. For the first three, we observe that shorter inputs or outputs generally lead to higher accuracy. Interestingly, for the length of the model-generated reasoning process, accuracy remains relatively stable within the range of 60 to 240 tokens. This suggests that NOVER effectively incentivizes the model to adaptively generate reasoning of appropriate length according to the difficulty of each question, property that is closely related to the design of the efficiency reward. False Premise Task Recent work on RLVR suggests that RL may not exceed the capabilities of pre7 Method Base + CoT + SF 3B 7B 12.43% 14.59% 14.23% 19.28% 26.49% 29.73% + OV ER 18.74% 23.42% Table 4: Experiments on FANToM (Kim et al., 2023), theory of mind task with false premise problems. trained models (Yue et al., 2025) in domains with verifiable and structured answers. While NOVER focuses on free-form domains, we conduct false premise task that similarly reveals the limits of RL. We adopt FANToM (Kim et al., 2023), theory-ofmind dataset where models answer questions from specific characters perspective in multi-party dialogues. Some questions rest on false premises (e.g., asking for character As opinion on movie when was absent during the discussion on the movie in the conversation), making them unanswerable. Experiments show that SFT can memorize such refusal patterns from training data, whereas NOVER depends on the pretrained model to generate candidate responses and selectively reinforce the better ones. Lacking exposure to refusal behavior during pretraining, the model struggles to reject false-premise questions, resulting in weaker performance than SFT (see Table 4 and Appendix D). These findings suggest that future work should consider integrating multiple post-training strategies. 6.3 Curse of Proxy 2016), where models exploit flaws in the proxy rather than genuinely improving performance. To analyze this, we perform ablations on NOVER and record completion lengths to track the models training status(Figure 5). (1) Training with only Rf and Rr reveals two failure modes: reasoning explosion (red), which is overlong, garbled and formatbroken outputs, and reasoning collapse (green), where models stop thinking and generate minimum reasoning tokens, such as keyword pointing out the discipline of the problem. Both stem from proxy misalignment with the evolving policy, which leads to reward hacking. (2) Adding proxy-policy synchronization mitigates this lot (blue), though eventual explosion still occasionally occurs. (3) Introducing the efficiency reward Re and conditioned reward combination enables partial self-recovery (orange), as invalid completions receive zero reward, encouraging resampling. Yet, recovery is repeated from time to time, which leads to inefficient training. (4) Full NOVER integration yields stable training (purple): the model stays sober, optimizing reasoning length only when it improves outcomes. Synchronization further reduces proxy bias, supporting robust optimization. We have verified that NOVER can be trained stably for over 200,000 steps without encountering reasoning explosion or collapse, while consistently maintaining high group diversity. 6.4 Verifier, Free, or Not Group Baselines Model as Verifier Method Base + CoT + SF + GV + LJ + LJS 3B 7B 18.40% 20.70% 24.20% 29.20% 27.30% 33.50% 18.30% 30.00% 21.40% 3.80% 21.60% Verifier-Free + OV ER 28.10% 36.60% Table 5: Experiments on WebInstruct. We compare LLM-as-a-Judge (LJ) and the officially released verifier model for WebInstruct, the general verifier GV . Figure 5: The curse of proxy: inaccurate proxy reward may lead to reward hacking. NOVER uses reasoning perplexity as proxy for the ideal reward, common practice in RL (Ng and Russell, 2000; Christiano et al., 2017). However, this introduces the curse of proxy: imprecise rewards can lead to reward hacking (Amodei et al., To evaluate NOVER on reasoning tasks with difficult verification, we compared it against alternative verifier designs: an LLM-as-a-judge (LJ) and fine-tuned verifier model. Experiments were conducted on WebInstruct, which includes an official general verifier model (GV ) (Ma et al., 2025). For LJ, we used Gemini-2.0-flash with two prompt variants: lenient judge prompt, and stricter 8 version (LJs) (aligned with our evaluation setup). As shown in Table 5, model-based verifiers were highly unstable. With LJ, lenient prompts encouraged reward hacking, where πθ generated vague but superficially valid outputs to elicit positive rewards (e.g., giving rough ideas instead of precise answers). In contrast, strict prompts yielded sparse rewards and unstable training, thus, the 3B model failed to train. The dedicated verifier also proved unreliable, often misled by the policy model. For example, the policy might only sketch initial steps and prompt the verifier to complete the calculation, causing the verifier to abandon judgment and instead solve the task, then assign an undeserved positive reward. 6.5 Incentivized Reasoning Patterns Figure 6: Change of reasoning patterns over steps. We further investigated how reasoning patterns evolved during the training of NOVER. We extracted the reasoning tokens generated by the model at different training steps, including the outputs produced using CoT prompting before training (as 0 step), on fixed test set. These reasoning traces were then classified using Gemini-2.0-flash into one of several predefined reasoning patterns. Following the design in (Zeng et al., 2025), we categorize reasoning patterns in the NOVER incentivized reasoning process into six main kinds. Here is the definition of each kind: 1. Direct The direct recall of factual information, definitions, or established concepts without further analysis or transformation. This pattern involves stating information from memory as-is. 2. Decomposition The systematic organization of problem into manageable components, establishing clear steps, intermediate goals, or 9 methodical frameworks. This pattern involves creating structured approach to solving complex problems. 3. Enumeration The listing of multiple possibilities, options, alternatives, or cases without immediately selecting or committing to any specific one. This pattern involves comprehensively covering various aspects or potential scenarios. 4. Reflection The process of revisiting, questioning, or reassessing previously stated ideas, assumptions, or conclusions. This pattern involves reflecting on ones own reasoning and making adjustments based on further consideration. 5. Assumption The introduction of hypothetical conditions or premises that serve as foundation for further reasoning. This pattern involves making conditional statements to explore potential scenarios or outcomes. 6. Causation The establishment of cause-effect relationships between events, actions, or conditions. This pattern involves explaining how one factor leads to or influences another. As shown in Figure 6, before training, the model primarily used task decomposition typical of CoT prompting. As training progressed with the influence of the efficiency reward, the model learned to skip redundant reasoning steps when it could directly provide intermediate conclusions, resulting in notable increase in the proportion of the direct reasoning pattern. At the same time, other reasoning types began to appear and gradually stabilized, reflecting the development of reasoning that is both effective and efficient. 6. Inverse Incentive Training We further explore NOVERs flexibility in an interesting setting using the SS-GEN creative writing dataset (Feng et al., 2025c), where both ground truth stories and structural rubrics are provided. Unlike typical setups, this experiment adopts new format, where models generate story within <story> tags, followed by self-assessment in <assessment> tags. Figure 7 illustrates how we construct the inverse incentive training pipeline. Crucially, training data omits ground-truth stories and provides only rubric-based narrative criteria. We adapt NOVER by treating these rubrics as the 1. Clear Structure clear structure refers to that there is straightforward title establishing the intervention goal, an introduction explaining the story theme and setting, main body depicting the specific situation and appropriate responses, and conclusion summarizing the core information presented. 2. Describe more than Direct The number of descriptive sentences should exceed twice the number of coaching sentences in Social Story. descriptive sentence accurately describes the interaction, an event, or an explanation of the rationale that underlies what and why people think, say, or do, including perspective and affirmative sentences. Coaching sentences gently and safely guide the behavior. 3. Appropriate Perspective A): The Social Story should never use the second-person perspective to prevent being overly direct. B): When describing negative behaviors, the Social Story should never employ the firstperson perspective to safeguard the dignity and esteem of the audience. 4. Positive Tone The Social Story should always describe situations or guide behaviors in positive manner, especially when depicting typical or expected behaviors in specific situations. 5. Accurate Expression The story should use language that is as clear and unambiguous as possible because ASD children typically interpret things literally rather than inferring implicit meanings that require social insight. 6. Appropriate Vocabulary The Social Story should choose the most comfortable and accurate vocabulary for the audience. Firstly, use positive verbs while also being mindful of the varying implications of verbs. Avoid using terms that are likely to evoke strong emotional feelings such as \"shouldnt\", \"must\", \"supposed to\" and so on. We compare stories from the standard NOVER and the rubric-trained variant. Results consistently favor the latter across all criteria, highlighting NOVERs flexibility for creative applications beyond classical reasoning tasks. 10 Figure 7: Comparison between standard NOVER training (up) and NOVER_RUBRIC training (down). guidelines for self-assessment, while the story becomes the intermediate process to incentivize, denoted NOVER_RUBRIC. This approach inverts the standard paradigm, implementing process as outcome strategy. What we want is the intermediate tokens (story) instead of the final outputs (assessment). NOVERs design thus effectively steers generation toward rubric satisfaction, without needing ideal story exemplars during training. Empirical results show that Qwen2.5-7Bs accuFigure 8: Human judgement on different rubrics for SS-GEN story generation. racy improves from 50.79% (standard NOVER) to 64.37% after rubric-based training. To further validate the gains, we sampled 100 examples and asked human annotators to express preferences for each story concerning each rubric. The details of each rubric are provided to human annotators, including:"
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we present NOVER, verifier-free framework for incentive training that pushes the boundaries of incentive learning to encompass any text-to-text task. NOVER demonstrates strong performance, and we discuss both the advantages and limitations of such reinforcement learning paradigm, highlighting promising future directions for achieving robust and effective post-training. The flexibility of NOVER also enables novel learning paradigms such as inverse incentive training."
        },
        {
            "title": "Limitations",
            "content": "(Zeng et al., 2025; Liu et al., 2025a) show that incentive training requires the base model to possess certain fine-tuned capabilities, such as partial CoT reasoning and instruction-following, to effectively incentivize reasoning. This observation is consistent with our findings on Qwen models. Moreover, general reasoning tasks in freeform format demand stronger base model capabilities than structured-answer tasks like math or coding. Currently, Qwen is suitable choice in open-source models that meet the above requirements, so we conduct experiments on Qwen instead of other open-source models. Major open-source model teams have also recognized this and strengthened multi-stage pretraining by incorporating large amounts of CoT and instruction data, which gives us confidence that NOVER will be able to incentivize more models in the future. Due to computational constraints, our experiments were limited to 3B and 7B models, without scaling up to hundred-billion-parameter models. Nevertheless, we believe NOVER provides new direction for scaling in the post-training era. Instead of scaling the policy model or the reward model independently, NOVER suggests coordinated scaling, since the policy model itself serves as proxy and effectively takes on the role of the reward model."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the UK Engineering and Physical Sciences Research Council through Turing AI Fellowship (grant no. EP/V020579/1, EP/V020579/2) and the Prosperity Partnership scheme (grant no. UKRI566). Wei is supported by PhD studentship provided by Kings College London. The authors acknowledge the use of Kings Computational Research, Engineering and Technology Environment (CREATE) at Kings College London."
        },
        {
            "title": "References",
            "content": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, and Dan Mané. 2016. Concrete Problems in AI Safety. CoRR, abs/1606.06565. John Schulman, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, and 33 others. 2023. Gemini: Family of Highly Capable Multimodal Models. CoRR, abs/2312.11805. Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang. 2024. ToMBench: Benchmarking Theory of Mind in Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1595915983. Association for Computational Linguistics. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 42994307. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training. CoRR, abs/2501.17161. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. CoRR, abs/2110.14168. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025a. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025b. Video-R1: Reinforcing Video Reasoning in MLLMs. CoRR, abs/2503.21776. Yi Feng, Mingyang Song, Jiaqi Wang, Zhuang Chen, Guanqun Bi, Minlie Huang, Liping Jing, and Jian 11 Yu. 2025c. SS-GEN: Social Story Generation Framework with Large Language Models. In AAAI25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 13001308. AAAI Press. Tom Foster, Eshaan Ahmed, James Cook, Shaked Lifshitz, Tim Rocktäschel, and Jakob N. Foerster. 2024. On Reward Functions For Self-improving Chainof-thought Reasoning Without Supervised Datasets In Language Gamification - (Abridged Version). NeurIPS 2024 Workshop. Gemini, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. 2025. General Reasoning. 430K: Open https://huggingface.co/ datasets/GeneralReasoning/ GeneralThought-430K. Reasoning GeneralThoughtDataset. Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. 2025. Learn Your Reference Model for Real Good In The Thirteenth International ConAlignment. ference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, and 80 others. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR, abs/2501.12948. Aashish Gurung and Mirella Lapata. 2025. Learning to Reason for Long-form Story Generation. arXiv preprint arXiv:2503.22828. Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, and Osamu Yoshie. 2025. R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning. CoRR, abs/2502.19735. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. 2025. OpenReasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model. CoRR, abs/2503.24290. 12 Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Trans. Inf. Syst., 43(2):42:142:55. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, and 80 others. 2024. OpenAI o1 System Card. CoRR, abs/2412.16720. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. SearchR1: Training LLMs to Reason and Leverage Search CoRR, Engines with Reinforcement Learning. abs/2503.09516. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023. FANToM: Benchmark for Stress-testing Machine Theory of Mind in Interactions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1439714413. Association for Computational Linguistics. Levente Kocsis and Csaba Szepesvári. 2006. Bandit Based Monte-carlo Planning. In Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings, volume 4212 of Lecture Notes in Computer Science, pages 282293. Springer. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-shot Reasoners. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, and Muhao Chen. 2024. Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 76757688. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. In The Twelfth 2024. Lets Verify Step by Step. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Wei Liu, Chenxi Wang, YiFei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, and Chen Qian. 2024. Autonomous agents for collaborative task under information asymmetry. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025a. Understanding R1-Zero-like Training: Critical Perspective. CoRR, abs/2503.20783. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. 2025b. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495. Yilun Lu, Cheng Zhang, Jiaming Song, Linxi Fan, and William Wang. 2025. ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs. arXiv preprint arXiv:2504.01698. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. 2025. GeneralReasoner: Advancing LLM Reasoning Across https://github.com/ All Domains. TIGER-AI-Lab/General-Reasoner/ blob/main/General_Reasoner.pdf. Andrew Ng and Stuart Russell. 2000. Algorithms for Inverse Reinforcement Learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 663670. Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3806 3824. Association for Computational Linguistics. Siya Qi, Yulan He, and Zheng Yuan. 2024. Can we catch the elephant? the evolvement of hallucination evaluation on natural language generation: survey. arXiv e-prints, pages arXiv2404. Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna Sunaryo, Tatia Lee, Rada Mihalcea, and Minlie Huang. 2024. EmoBench: Evaluating the Emotional Intelligence of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 59866004, Bangkok, Thailand. Association for Computational Linguistics. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347. ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, and 1 others. 2025. Seedthinking-v1. 5: Advancing superb reasoning modarXiv preprint els with reinforcement learning. arXiv:2504.13914. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. CoRR, abs/2402.03300. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, and 1 others. 2025. Vlm-r1: stable and generalizable r1style large vision-language model. arXiv preprint arXiv:2504.07615. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. CoRR, abs/2503.05592. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. 2025. Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains. CoRR, abs/2503.23829. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. 2025. Stop Overthinking: Survey on Efficient Reasoning for Large Language Models. CoRR, abs/2503.16419. M.-A-P. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, and 77 others. 2025. SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines. CoRR, abs/2502.14739. Jörg Tiedemann. 2012. Parallel Data, Tools and InIn Proceedings of the Eighth terfaces in OPUS. International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012, pages 22142218. European Language Resources Association (ELRA). TIGER-Lab. 2025. WebInstruct-verified Dataset. https://huggingface.co/datasets/ TIGER-Lab/WebInstruct-verified. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. TRL: Transformer Reinforcement Learning. https://github.com/ huggingface/trl. 13 Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve Prompting: Improving Zeroshot Chain-of-thought Reasoning by Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 26092634. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, and 1 others. 2025. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Lilian Weng. 2025. Why we think. lilianweng.github.io. Xiaobo Xia and Run Luo. 2025. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-RL: Unleashing LLM Reasoning with Rule-based Reinforcement Learning. CoRR, abs/2502.14768. Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, and Yulan He. 2024. Mirror: Multiple-perspective Selfreflection Method for Knowledge-rich Reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 70867103. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 Technical Report. CoRR, abs/2412.15115. Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. 2025. R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-modal Formalization. CoRR, abs/2503.10615. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2024. Cognitive Mirage: Review of Hallucinations in Large Language Models. In Proceedings of the First International OpenKG Workshop: Large Knowledge-Enhanced Models colocacted with The International Joint Conference on Artificial Intelligence (IJCAI 2024), Jeju Island, South Korea, August 3, 2024, volume 3818 of CEUR Workshop Proceedings, pages 1436. CEUR-WS.org. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: An Open-source LLM Reinforcement Learning System at Scale. CoRR, abs/2503.14476. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, and 1 others. 2025a. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, and 1 others. 2025b. VAPO: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? arXiv preprint arXiv:2504.13837. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025. SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild. CoRR, abs/2503.18892. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025. R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization. CoRR, abs/2503.12937. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-most Prompting Enables Complex Reasoning in Large Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net."
        },
        {
            "title": "A Profiling",
            "content": "Figure 9: Profiling of the training time in NOVER. Figure 9 shows the average training time per sample using two H100 GPUs with completion length of 2,048 tokens. Most of the time is spent on the rollout stage, where vLLM (Kwon et al., 2023) is used to sample multiple completions from the policy model efficiently. In contrast, computing the reasoning and efficiency rewards accounts for only about 5% of the total training time."
        },
        {
            "title": "B Reasoning Hallucination Case",
            "content": "Figure 10 presents an example from the OPUS translation dataset where both the base model and NOVER perform reasoning to generate translation. As shown, the base model is able to produce seemingly correct reasoning steps, sometimes nearly identical to those of NOVER. However, despite generating the right steps, the base model fails to reach the correct conclusion. For instance, its reasoning chain ends with self-check that asserts the translation is perfect and faithfully conveys the meaning of the original sentence, including the description of the womans appearance. Yet, in the earlier steps, the model translates only the subject, verb, and object, omitting any reference to the womans appearance. This kind of confident but incorrect self-verification, known as one kind of reasoning hallucination (Huang et al., 2025; Qi et al., 2024), can cause the model to overlook essential content, resulting in worse performance than directly producing the translation. In contrast, NOVER follows similar reasoning path but incorporates critical refinements. For example, it explicitly notes the need to translate not only the core grammatical elements but also any additional de16 scriptive components. As result, in the initial step of identifying main elements, NOVER successfully detects phrases related to the womans appearance, such as perfect figure and elegant dress, and accurately translates and integrates them in later steps. Moreover, NOVER provides more rigorous rubrics, such as emphasizing in the rearrangement step that Spanish word order may differ from English."
        },
        {
            "title": "C Reasoning Perplexity Example",
            "content": "Figure 11 shows two completions generated by NOVER in response to medical question from the natural reasoning dataset, each associated with different level of reasoning perplexity. As illustrated, Completion 1, which has lower reasoning perplexity, is clearly superior to Completion 2 in terms of physiological mechanisms, terminological accuracy, and the discussion on the role of dietary factors, and overall scientific reasoning. In contrast, Completion 2 is relatively coarse and contains misleading or missing descriptions of relevant terms and mechanisms. Although reasoning perplexity does not directly compare the answer with reference, it estimates the likelihood of the reference based on the inferred reasoning path, thereby providing concise and effective measure of answer quality, as evidenced by this example."
        },
        {
            "title": "D False Premise Example",
            "content": "Figure 12 illustrates an example from the FANToM dataset, shedding light on why NOVER may underperform compared to SFT in the false premise task setting. As shown, the example involves multi-party dialogue and poses question requiring higher-order theory of mind reasoningfor instance, inferring how person would think about certain topic from person As perspective. The trap, however, is that may not have participated in the conversation and thus is unaware of the topic, making the question unanswerable. The corresponding reference typically reflects this, such as is unaware of. . . . SFT can quickly memorize and reproduce such response patterns (e.g., somebody is unaware of. . . ), whereas NOVER relies on the base model to first generate correct answer, which is then reinforced. However, the base model may struggle to recognize the false premise and might not even consider the question potentially unanswerable. Consequently, as shown in the figure, NOVER attempts to answer the question but remains constrained by the flawed assumption that Case Study in OPUS Translation Task Question: Translate the following text from English to Spanish: With her perfect figure and elegant dress she was, indeed, strange apparition upon lonely moorland path. Reference: Dada la perfección elegancia de su vestido, resultaba, desde luego, una extraña aparición en la solitaria senda del páramo. Base Model CoT: To translate the given text from English to Spanish, will follow these steps: 1. Identify the main components of the sentence: subject, verb, and object. 2. Translate each component individually. 3. Rearrange the components to form grammatically correct sentence in Spanish. 4. Ensure that the translated sentence maintains the original meaning and tone. Step 1: Identify the main components of the sentence. Subject: \"She\" ...... Step 2: Translate each component individually. Subject: \"Ella\" (She) ...... Step 3: Rearrange the components to form grammatically correct sentence in Spanish. \"Ella era, en efecto, una extraña aparición en un sendero solitario de tierra mojada.\" Step 4: Ensure that the translated sentence maintains the original meaning and tone. The translated sentence maintains the original meaning and tone, as it describes the womans appearance and the setting in which she appears. Base Model Translation: Ella era, en efecto, una extraña aparición en un sendero solitario de tierra mojada. NOVER thinking process: <think> To translate the given text from English to Spanish, will follow these steps: 1. Identify the main components of the sentence: subject, verb, object, and any additional descriptive elements. 2. Translate each component into Spanish while maintaining the original meaning and context. 3. Rearrange the translated components to fit the Spanish sentence structure, which may differ from English. 4. Review the final translation for accuracy and fluency. Heres the step-by-step translation process: 1. Main components: - Subject: \"she\" ...... - Additional descriptive elements: \"perfect figure\", \"elegant dress\", \"indeed\", \"upon lonely moorland path\" 2. Translations: - \"she\" -> \"ella\" ...... - \"perfect figure\" -> \"figura perfecta\" - \"elegant dress\" -> \"traje elegante\" - \"indeed\" -> \"realmente\" - \"upon lonely moorland path\" -> \"en una solitaria senda de moorland\" 3. Rearranged Spanish sentence structure: - \"Ella era, realmente, una extraña aparición en una solitaria senda de moorland con su figura perfecta traje elegante.\" 4. Reviewing the final translation: - The translation maintains the original meaning and context, and the sentence structure is appropriate for Spanish. have translated the given text from English to Spanish by identifying the main components, translating each component, rearranging them to fit the Spanish sentence structure, and reviewing the final translation for accuracy and fluency. </think> NOVER translation: <answer> Ella era, realmente, una extraña aparición en una solitaria senda de moorland con su figura perfecta traje elegante. </answer> Figure 10: case in the OPUS translation dataset, where the base model generates hallucinated CoT that leads to wrong translation, while NOVER follows similar steps but corrects the wrong parts in the thinking process. 17 Two Completions of NOVER on Natural Reasoning Dataset Question: What is the primary mechanism by which the body eliminates excess cholesterol, and how do dietary factors such as fiber intake and fatty food consumption affect this process? Provide detailed explanation of the gastrohepatic cycle and the role of bile secretion in cholesterol elimination. Reference: The primary mechanism by which the body eliminates excess cholesterol is through the gastrohepatic cycle, where excess cholesterol is secreted by the bile ducts back into the gastrointestinal tract. Dietary factors such as fiber intake can help prevent the digestion of cholesterol, while fatty food consumption can increase cholesterol production. NOVER completion 1, with reasoning perplexity = 1.406 (the good one) <think> The primary mechanism by which the body eliminates excess cholesterol involves process called the gastrohepatic cycle. This cycle consists of several steps: 1. Cholesterol in the blood is taken up by the small intestine and converted into bile acids, which are then secreted into the intestine. 2. Bile acids are reabsorbed by the small intestine and returned to the liver via the portal vein. 3. In the liver, the bile acids are converted back into cholesterol and stored or used for other purposes. 4. When the body has an excess of cholesterol, the liver produces more bile acids, which are excreted in the feces. ...... In summary, the gastrohepatic cycle is the primary mechanism by which the body eliminates excess cholesterol, and dietary factors such as fiber intake and fatty food consumption can affect this process in various ways. </think> <answer> The primary mechanism by which the body eliminates excess cholesterol involves process called the gastrohepatic cycle. This cycle consists of several steps: Cholesterol in the blood is taken up by the small intestine and converted into bile acids, which are then secreted into the intestine. Bile acids are reabsorbed by the small intestine and returned to the liver via the portal vein. In the liver, the bile acids are converted back into cholesterol and stored or used for other purposes. When the body has an excess of cholesterol, the liver produces more bile acids, which are excreted in the feces. Dietary factors such as fiber intake and fatty food consumption can affect this process in various ways. Dietary fiber, particularly soluble fiber, can bind to bile acids in the intestine and prevent their reabsorption. This leads to an increased excretion of bile acids in the feces, which in turn increases the amount of cholesterol that is eliminated from the body. Fatty foods, particularly those high in saturated and trans fats, can increase the amount of cholesterol produced by the liver. This can lead to an increase in the amount of cholesterol that needs to be eliminated from the body. Certain fatty acids, such as omega-3 fatty acids, can also have beneficial effect on cholesterol metabolism. Omega-3 fatty acids can reduce the amount of cholesterol produced by the liver and increase the amount of cholesterol that is eliminated from the body. </answer> NOVER completion 2, with reasoning perplexity = 1.516 (the bad one) <think> The primary mechanism by which the body eliminates excess cholesterol is through the excretion of cholesterol-rich bile acids in the feces. This process involves the breakdown and conversion of cholesterol into bile acids in the liver, followed by the release of these bile acids into the small intestine during digestion. Bile acids are then excreted from the body in the feces. Dietary factors such as fiber intake and fatty food consumption can affect this process in several ways. Fiber-rich foods can increase the viscosity of the stool, which slows down the passage of bile acids through the digestive tract. This allows more time for the bile acids to be absorbed back into the bloodstream and reused, reducing the amount of cholesterol that is excreted in the feces. ...... Bile secretion plays critical role in cholesterol elimination by providing the necessary environment for the breakdown and conversion of cholesterol into bile acids in the liver. Bile acids are then released into the small intestine during digestion, where they interact with fatty acids and triglycerides to form micelles. These micelles help to emulsify the fatty acids and make them more available for absorption by the small intestine. </think> <answer> The primary mechanism by which the body eliminates excess cholesterol is through the excretion of cholesterol-rich bile acids in the feces. Dietary factors such as fiber intake and fatty food consumption can affect this process by increasing or decreasing the amount of cholesterol that is excreted in the feces. The gastrohepatic cycle is the process by which bile is produced in the liver, stored in the gallbladder, and released into the small intestine during digestion. Bile secretion plays critical role in cholesterol elimination by providing the necessary environment for the breakdown and conversion of cholesterol into bile acids in the liver. </answer> Figure 11: We show two completions from NOVER on the Natural Reasoning dataset, showing how reasoning perplexity discriminates good responses from bad ones. 18 is aware of the topic, and reasons accordingly. This highlights why NOVER may perform worse than SFT on counterintuitive tasks. Addressing such cases may require hybrid approach: first using SFT to expose the model to appropriate response patterns and challenge its assumptions, followed by reinforcement learning to strengthen accurate reasoning."
        },
        {
            "title": "E Proof of Equivalence of Relative\nPerplexity Gain and Absolute\nReasoning Perplexity after Group\nNormalization",
            "content": "Let us denote the reasoning perplexity of i-th completion in group of size as abs , where this group of completions share the same ground truth and the same prompt p, with different reasoning tokens ti, abs = Pr(p, ti, g), (14) and define the relative perplexity gain (Gurung and Lapata, 2025; Foster et al., 2024) as rel , rel = Pr(p, ti, g) Pbase(p, g) Pbase(p, g) . (15) Where Pbase is the base perplexity of the proxy models prediction on ground truth tokens without reasoning tokens t, where the group mean and standard deviation are µ() = σ() = 1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 () 1 (cid:88) (cid:16) j=1 (20) µ() () (cid:17) Substituting the affine relation into the mean, we have µrel = = 1 1 (cid:88) j=1 (cid:88) j=1 rel (cid:0)αP abs + β(cid:1) (21) = α (cid:88) abs + β 1 j=1 + β = αµabs Similarly, the standard deviation transforms as σrel = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) (cid:16) j=1 αP abs + β µrel (cid:17)2 = ασabs = ασabs (since α > 0) (22) Therefore, the relative advantage Arel can be (cid:32) Pbase(p, g) = exp (cid:80)g i=1 log πp(gi p, g<i) (cid:33) (16) expressed as Arel = Since within one group, all completions share the same and g, hence all completions share the same Pbase(p, a). It can be regarded as positive scalar constant shared by all completions in the group, and we can rewrite rel as an affine transformation of abs : = αP abs rel + β, (17) = = = + β(cid:1) µrel rel σrel + β (cid:0)αµabs αP abs ασabs µabs ) ασabs µabs abs σabs α(P abs (23) where α = 1 Pbase(p, a) > 0, β = 1. (18) The group-normalized advantage for either form of perplexity is computed by standard normalization within one group: = Aabs This demonstrates that the group-normalized advantage values computed from absolute reasoning perplexity Pr and from relative perplexity gain normalized by Pbase are exactly identical. Consequently, both formulations yield the same advantage signals for policy optimization under the GRPO training. A() = µ() () σ() , (19) 19 False Premise Example in FANToM Dataset Question Here is dialogue: Eliana: Hello, guys. find it fascinating how various cultures influence food and cuisine. For instance, in my culture, Brazilian cuisine is culmination of Europe, Africa, and Native American influences. Kade: How so, Eliana? Could you give an example? Eliana: Absolutely, Kade! Take feijoada for example, its Brazilian dish thats similar to bean stew, filled with beef and pork. Its traditionally served with rice, collard greens, and farofa, cassava flour mixture. Feijoada was often made by slaves, who would use whatever meats were available, hence the mixture. Addison: Thats very interesting, Eliana. Food does tell powerful story about the people, doesnt it? hail from Scotland, and over there, we have dish called haggis. Its combination of sheeps heart, liver, and lungs minced with onion, oatmeal, suet, and spices. know it sounds bit... exotic, but its actually quite delicious. Kade: Wow, Addison! Ive never heard of anything like that before. must add that to my food bucket list. Although, as Canadian, guess the most iconic food must be poutine: Wow, Addison! Ive never heard of anything like that before. must add that to my food bucket list. Although, as Canadian, guess the most iconic food must be poutine: French fries and cheese curds topped with brown gravy. Its simple dish yet so full of flavor. ...... Alberto: Id be happy to share, Addison! Well, for starters, we have Dia de los Muertos, or Day of the Dead... Alberto: Dia de los Muertos is very important festival in Mexico. Its time when we honor and celebrate our deceased loved ones, remembering them with music, food, and colorful altars. Kade: Sounds like profound tradition, Alberto. In Canada, we have the National Indigenous Peoples Day. Its day to celebrate and appreciate the cultural diversity of the First Nations, Inuit, and Métis peoples. Addison: love the sound of both those celebrations. In Scotland, we have numerous festivals, but one of the biggest is Hogmanay, our New Years Eve celebration. It involves lots of food, drink, and merrymaking, as well as the singing of the traditional folk song Auld Lang Syne. Alberto: That sounds like so much fun, Addison! think its so powerful how we keep our traditions alive and pass them on to future generations. Kade: Absolutely, Alberto. Its these traditions, these festivals, the food and the language that makes our cultures unique and gives us our identities. Addison: Well said, Kade. Ive learned so much about Mexican and Canadian culture today. Its great when we can all sit and learn from each other. Alberto: agree, Addison. And this exchange isnt just limited between us, but it should be encouraged everywhere. It fosters understanding and respect for diversity. Kade: Definitely, Alberto. So, guess we meet at the next food tasting session, where we have taste of Brazil, Scotland, Canada, and Mexico! What dishes from Scotland and Canada does Eliana think that Alberto believes were discussed by Addison and Kade respectively? Reference: Alberto is unaware about which dishes from Scotland and Canada were discussed by Addison and Kade respectively as he was not involved in the conversation when this was discussed. Thus, Eliana does not have any basis to form an opinion on what Alberto believes about the mentioned dishes. NOVER Response: <think> Addison and Kade were the ones who discussed the dishes from Scotland and Canada. From the dialogue, it is evident that Addison mentioned the dish \"haggis\", which is combination of sheeps heart, liver, and lungs minced with onion, oatmeal, suet, and spices. The other dish mentioned is the \"poutine\" which is dish consisting of French fries and cheese curds topped with brown gravy. Alberto, on the other hand, mentioned the dish \"mole sauce\" which is mixture of spices, chocolate, chilies, and other ingredients that was born out of blend of indigenous and Spanish influences. Kade, during the conversation, mentioned the \"powwow\", which is time when people gather to celebrate their deceased loved ones with music, food, and colorful altars, and is traditional Mexican festival. So, Alberto believed that Addison and Kade discussed the dishes from Scotland and Canada, which are haggis and poutine respectively. </think> <answer> Haggis is dish from Scotland. Kade mentioned the dish \"powwow\" during the conversation, which is traditional Mexican festival. So, Alberto believed that Addison and Kade discussed the dishes from Scotland and Canada, which are haggis and poutine respectively. </answer> Figure 12: false premise example in FANToM (Kim et al., 2023) dataset, where the question can not be answered."
        }
    ],
    "affiliations": [
        "Kings College London",
        "Shanghai Jiao Tong University",
        "The Alan Turing Institute"
    ]
}