{
    "paper_title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
    "authors": [
        "Xintao Wang",
        "Heng Wang",
        "Yifei Zhang",
        "Xinfeng Yuan",
        "Rui Xu",
        "Jen-tse Huang",
        "Siyu Yuan",
        "Haoran Guo",
        "Jiangjie Chen",
        "Wei Wang",
        "Yanghua Xiao",
        "Shuchang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 8 0 9 0 . 2 0 5 2 : r CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Xintao Wang1,2, Heng Wang2, Yifei Zhang1,2, Xinfeng Yuan1, Rui Xu1, Jen-tse Huang3, Siyu Yuan1, Haoran Guo1, Jiangjie Chen1, Wei Wang1, Yanghua Xiao1 and Shuchang Zhou2 1Fudan University, 2StepFun, 3Johns Hopkins University Abstract: Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, collection of high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce givencircumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively. Our code, dataset and models are available at https://github.com/Neph0s/CoSER. 1. Introduction Recent advances in large language models (LLMs) have facilitated the emergence of anthropomorphic cognition in AI (Kosinski, 2023; Shanahan et al., 2023). Role-playing language agents (RPLAs), i.e., LLMs that simulate specific personas based on relevant data, have hence been popular (Park et al., 2023). RPLAs have been adopted to simulate personas of various types, including demographics, characters, or daily individuals (Chen et al., 2024a). They have inspired extensive applications including character chatbots, agents in video games, and digital clones for humans. This paper studies RPLAs for established characters, which represent crucial yet challenging task beyond the naive portrayal of individual traits or stereotypes. Specifically, RPLAs should faithfully align with their characters complex backgrounds and capture their nuanced personalities. Towards effective RPLAs, two major challenges persist in: 1) Data: High-quality datasets are lacking. Existing datasets are limited to dialogues between two characters, and lack necessary dialogue contexts and knowledge in other forms. Moreover, many datasets are synthesized by LLMs, compromising authenticity and fidelity to the origins (Wang et al., 2024a; Lu et al., 2024); 2) Evaluation: Current methods fall short in assessing complex character portrayals of LLMs. They typically focus on singleturn interactions with pre-defined question sets, and rely on either LLM-based judges or multi-choice questions. The former lack nuanced discrimination and suffer from bias issues (Li et al., 2024), while the latter only assess specialized aspects (Xu et al., 2024). Overall, there is lack of authentic character data and appropriate evaluation methods based on such data. In this paper, we introduce CoSER, collection of authentic character data, along with open state-ofthe-art models and evaluation protocol based on such data, for Coordinating LLM-Based Persona Simulation of Established Roles. The CoSER dataset is sourced from narratives and dialogues in 771 renowned books, processed via our LLM-based pipeline. CoSER differs from existing datasets in two Corresponding author(s): xtwang21@m.fudan.edu.cn. Work done during Xintao Wangs internship at StepFun. CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Figure 1: An example from CoSER dataset, which provides comprehensive data types such as conversation dialogues and settings, plot summaries, characters inner thoughts, authentically sourced from renowned books. fundamental ways: 1) CoSER extracts authentic, multi-character dialogues from acclaimed literary works, in contrast to LLM-synthesized question-answer pairs in previous work. Hence, our dataset maintains high source fidelity while exhibiting greater quality and complexity. 2) CoSER incorporates comprehensive types of data, as shown in Fig. 1: i) Besides character profiles and dialogues, CoSER encompasses plot summaries, character experiences, and conversation backgrounds, supporting various purposes including prompting, retrieval, model training and evaluation. ii) Conversations in CoSER capture characters actions and internal thoughts beyond surface-level speech, enabling RPLAs to simulate sophisticated cognitive and behavioral processes of humans, such as [Im nervous, but we have to do this] (Takes deep breath) Alright, we .... We provide clear comparison between CoSER and existing datasets in Table 5. We introduce given-circumstance acting (GCA) for training and evaluating role-playing LLMs, leveraging CoSER dataset. Given conversation with messages 𝑀 , characters 𝒞 and setting 𝒮, GCA requires an actor LLM to sequentially portray each character 𝑐 𝒞 to recreate the conversation, as illustrated in Fig. 2. During training, we train LLMs to portray each character 𝑐, on their authentic 2 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Dataset Charater-LLM ChatHaruhi RoleLLM HPD LifeChoice CroSS-MR CharacterGLM CharacterEval DITTO MMRole CharacterBench CoSER Book Character Conversation Message Plot Num. Num. Profile Expr. #Conv. #Turns Setting Auth. Multi-Chara. Speech Thought Action Summ. 9 32 100 113 1,462 126 250 77 4,002 85 3,956 17,966 7 388 126 771 14,300 54,726 140,726 1,191 1,462 445 1,034 1,785 7,186 14,346 13,162 29,798 13.2 >2 2 13.2 2 2 15.8 9.3 5.1 4.2 11.3 13.2 * Table 1: Overview of CoSER and existing RPLA datasets. For characters, Num. count characters with profiles, and Expr. denotes structured character experiences. For conversations, Auth. indicates authentic dialogues or behaviors from the books, and Multi-Chara. denotes involving more than 2 characters. Num. (number), Conv. (conversation), and Summ. (summary) are abbreviations. utterances 𝑀𝑐 𝑀 . As result, we develop CoSER 8B and 70B, built on LLaMA-3.1 models (Dubey et al., 2024), which demonstrate true-to-life character portrayal and state-of-the-art performance on multiple RPLA benchmarks. For evaluation, GCA involves two steps: multi-agent simulation and penalty-based LLM judging. Given test conversation 𝑀 , we: 1) create multi-agent system to simulate conversation 𝑀 , where the actor LLM portrays each character 𝑐 𝒞 in the same setting 𝒮; 2) assess 𝑀 using penalty-based LLM critics, leveraging detailed rubrics and the original conversation 𝑀 . GCA evaluation offers three advantages: First, it comprehensively reflects actor LLMs abilities via multi-agent simulation; Second, it is based on authentic scenes and groundtruth dialogues. Third, it provides expert-curated rubrics to guide LLM critics. Our contributions are summarized as follows: 1. We introduce the CoSER dataset and models for RPLA research and applications. Our dataset comprises 29,798 authentic conversations and comprehensive types of data from 771 renowned books. Leveraging this dataset, we develop CoSER 8B and CoSER 70B, which are state-of-the-art models for RPLAs. 2. We propose given-circumstance acting for training and evaluating role-playing LLMs, drawing from established acting theory. Our evaluation comprehensively tests actor LLMs via multicharacter simulation, while providing original dialogues and detailed rubrics to enhance LLMbased assessment. 3. Results of extensive experiments demonstrate the significant value of our dataset for the training, retrieval and evaluation of RPLAs. Notably, the CoSER models achieve state-of-the-art performance across four benchmarks for RPLAs. 2. Related Work An RPLA leverages LLMs to create simulated persona 𝜋𝑐 that emulates real character 𝑐 based on its persona data 𝒟𝑐. Effective RPLAs require both comprehensive, high-quality data 𝒟𝑐 and advanced role-playing LLMs. Among various persona types, we focus on RPLAs for established characters, which should faithfully align with their characters complex backgrounds and nuanced personalities. Persona data 𝒟𝑐 describe the real persona 𝑐 through various representations, Datasets for RPLAs including profiles (Yuan et al., 2024), dialogues (Wang et al., 2024a), experiences (Li et al., 2023) and multimodal information (Dai et al., 2024), e.t.c.. As shown in Table 5, existing datasets have 3 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles several limitations. 1) Many are synthesized via LLMs responses to general instruction sets (Wang et al., 2024a) or character-specific questions (Shao et al., 2023), such as RoleBench (Wang et al., 2024a). However, LLM-synthesized data compromise authenticity and fidelity to original sources. 2) Human-annotated datasets such as CharacterEval (Tu et al., 2024) and CharacterDial (Zhou et al., 2023) offer improved quality, but are expensive and difficult to scale. 3) Several efforts extract authentic dialogues from fictional works, such as ChatHaruhi (Li et al., 2023) and HPD (Chen et al., 2023). However, they rely on human efforts for individual sources and are hence hard to scale as well. 4) Furthermore, existing datasets offer limited representations and forms, i.e., mainly consisting of two-character or user-character question-answer pairs. These datasets support various purposes, including prompting, training, retrieval augmentation, and evaluation of RPLAs. Existing evaluation methods are based on either LLM judges or multi-choice Evaluation for RPLAs questions (Chen et al., 2024a). LLM-judged methods typically elicit LLMs role-playing performance via predefined questions, and score the performance using LLM judges or reward models (Chen et al., 2024a). They assess various dimensions, including character-independent aspects such as anthropomorphism (Tu et al., 2024) and attractiveness (Zhou et al., 2023), as well as characterspecific traits such as language style, knowledge, and personality (Wang et al., 2024a; Shao et al., 2023). However, LLM judges suffer from inherent biases, e.g., length and position bias (Li et al., 2024), and may lack the necessary knowledge for character-specific evaluation. Other benchmarks evaluate role-playing LLMs through multiple-choice questions, assessing specific aspects such as knowledge (Shen et al., 2023), decision-making (Xu et al., 2024), motivation recognition (Yuan et al., 2024), and personality fidelity (Wang et al., 2024b). 3. CoSER Dataset In this section, we introduce the CoSER dataset, which covers authentic data of 17,966 characters from 771 renowned books. CoSER features its authentic, non-synthesized dialogues with real-world intricacies, and comprehensive data representations supporting various usages. In Table 1, we provide comprehensive comparison with existing datasets. We illustrate our datasets design principles in 3.1, curation pipeline in 3.2, and statistical analysis in A.1. 3.1. Design Principles As shown in Table 1, CoSER differs from previous RPLA datasets mainly in its: 1) rich data types, 2) internal thoughts and physical actions in messages, 3) environment as role. The persona data 𝒟𝑐 can represent character 𝑐 from fictional works in diverse Rich Types of Data forms, e.g., narratives, profiles, dialogues, experiences, e.t.c.. Previous work focuses primarily on profiles and dialogues, which represent limited knowledge. Hence, we propose more comprehensive set of data types that are: 1) Comprehensive: covering extensive knowledge about characters and plots from the books; 2) Orthogonal: carrying distinct, complementary information with little redundancy; 3) Contextual-rich: providing sufficient context to enable 𝜋𝑐 to faithfully reproduce 𝑐s behaviors and responses in given scenarios. Specifically, we organizes knowledge from books hierarchically via three interconnected elements: plots, conversations and characters. Each plot comprises its raw text, summary, conversations in this plot, and key characters current states and experiences in this plot. conversation contains not only the dialogue transcripts, but also rich contextual settings including scenario descriptions and characters motivations. Characters are associated with their conversations and plots, based on which we craft their profiles. 4 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Figure 2: Overview of CoSERs dataset, training and evaluation. Left: The CoSER dataset is sourced from renowned books and processed via LLM-based pipeline. It contains rich data types on plots, conversations and characters. Right: We apply given-circumstance acting to train and evaluate role-playing LLMs using these conversations. For training, each sample trains the LLM to portray specific character in conversation, using their original dialogue. For evaluation, we build multi-agent system for conversation simulation given the same scenario, and assess the simulated dialogue via penalty-based LLM critics. Previous RPLA studies typically restrict RPLAs output space Thoughts and Actions in Messages to verbal speech alone, limiting their ability to fully represent human interactions. In this paper, we extend the message space of RPLAs and character datasets into three distinct dimensions: speech (ℒ), action (𝒜), and thought (𝒯 ), significantly enriching the expressiveness. For instance, an RPLA can convey silence by generating only thoughts and actions without verbal speech. The three dimensions are distinguished by markup symbols and function mechanisms: Speech is for verbal communications of characters. Action captures physical behaviors, body language, facial expressions, e.t.c.. Similar to tool use in agents (Weng, 2023), actions can be programmed to trigger downstream events in multi-agent systems. Thought represents internal thinking processes, which enable RPLAs to simulate sophisticated human cognition. Thoughts should be invisible to others, forming information asymmetry (Zhou et al., 2024). In RPLA applications like AI TRPG 1 (Liang et al., 2023), LLMs often serve Environment as Role as world simulators that respond to players actions. To promote this ability, we consider environment as special role 𝑒, which provide environmental responses such as physical changes and reactions from unspecified characters or crowds. 1Tabletop Role-Playing Games 5 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles 3.2. Dataset Curation We curate the CoSER dataset through systematic LLM-based pipeline that transforms book content into high-quality data for RPLAs 2. The details are as follows. Source Selection Our dataset is sourced from most acclaimed literary works to ensure data quality and character depth. We identify the top 1,000 books on Goodreadss Best Books Ever list 3, and obtain the content for 771 books. As shown in Table 6, these books offer characters and narratives with literary significance and widespread recognition across diverse genres, time periods, and cultural backgrounds. Chunking We segment book contents into chunks to fit in LLMs context window. We employs both static, chapter-based strategy and dynamic, plot-based strategy. Initially, we use regular expressions to identify chapter titles as natural chunk boundaries. Then, we merge adjacent small chunks and split large chunks to ensure moderate chunk sizes. However, static chunking neglects the storyline and truncates important plots or conversations. To address this, we implement dynamic plot-based chunking, i.e., during data extraction, we also prompt LLMs to identify truncated plots or trailing content in the current chunk, and concatenate them with the subsequent chunk to ensure plot integrity. Data Extraction We employ LLMs to extract plot and conversation data from book chunks, including (1) contents, summaries and character experiences of plots, and (2) dialogues and background settings of conversations. The extracted data representations are illustrated in Fig. 1 and introduced in 3.1. In the messages, speeches are always extracted from the original dialogues, while actions and thoughts can either be extracted or inferred by LLMs based on the context. For evaluation purposes, we hold out data from the final 10% plots in each book. Based on the extracted data, we form the knowledge bases for Organizing Character Data characters in three steps. First, we unify character references by establishing name mappings between aliases and canonical names using LLMs, e.g., mapping Lord Snow to an unified identifier Jon Snow. Second, we aggregate relevant plots and conversations for each character. Finally, we leverage LLMs to generate character profiles based on their extracted data, describing them from multiple perspectives including background, experiences, physical characteristics, personality traits, core motivations, relationships, character arcs, e.t.c.. For technical details, including our prompts, engineering implementation, and handling mechanisms for exception caused by LLMs, please refer to A. 4. Training and Evaluation via GCA In this section, we introduce given-circumstance acting (GCA) for training and evaluating LLMs role-playing abilities using the CoSER dataset, as shown in Fig. 2. 4.1. Given-Circumstance Acting In Konstantin Stanislavskis acting methodologies, given-circumstance acting is fundamental approach where actors are trained and judged through performance within specified conditions including environmental context, historical events and personal conditions (Stanislavski, 2008). We propose to adapt this approach to framework that trains and evaluates LLMs role-playing skills, 2In this paper, we employs Claude-3.5-Sonnet (20240620). 3https://www.goodreads.com/list/show/1.Best_Books_Ever 6 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles leveraging the comprehensive data in CoSER. In this framework, given conversation with dialogue messages 𝑀 , involved characters 𝒞, and contextual setting 𝒮, an actor LLM sequentially plays the role of each character 𝑐 𝒞 to simulate the conversation. 4.2. GCA Training and CoSER Models We fine-tune LLMs role-playing abilities through GCA. Each training sample is derived from conversation and one of its character 𝑐 in CoSER dataset, and LLMs are trained on 𝑐s utterances 𝑀𝑐. Specifically, we first compose role-playing instruction 𝑖𝑐 comprising the scenario description, the characters profile 𝑝𝑐 and motivation, and profiles of other involved characters, which provide comprehensive context for role-playing. The original dialogue messages are denoted as 𝑀 = [𝑚1, ..., 𝑚𝑇 ], where 𝑇 is the number of turns. Then, the training sample [𝑖𝑐, 𝑚1, ..., 𝑚𝑇 ] is concatenation of the instruction and messages, where the characters messages 𝑀𝑐 𝑀 are treated as outputs for optimization, and the other parts serve as inputs. We train CoSER 8B and CoSER 70B based on LLaMA 3.1 Instruct models (Dubey et al., 2024), using 90% books in our dataset. To effectively support diverse use cases of RPLAs, our training samples cover extensive settings: 1) The CoSER dataset contains massive characters and conversation settings from extensive books. We train models on all characters in each conversation, ranging from major characters with detailed profiles to minor roles driven only by the context; 2) To simulate real use cases, we incorporate role-playing instructions in diverse formats through instruction templates of varying formats. Besides, we consider different combinations of available data by including or excluding: profiles of other characters, plot summaries, and characters motivations; 3) We train models both with and without characters internal thoughts in the extracted dialogues. We extend CoSERs training beyond character role-playing to develop complementary capabilities in environment modeling and next speaker prediction (NSP), which facilitates RPLA applications. To maintain models general abilities, we augment our training data with the Tulu-3 dataset (Lambert et al., 2024). Please refer to for more details. 4.3. GCA Evaluation Evaluating role-playing LLMs remains significant challenge, primarily in two aspects: 1) providing appropriate scenarios to elicit role-playing performance, and 2) properly assess the performance. Towards these challenges, we propose GCA evaluation for actor LLMs role-playing abilities, comprising two stages: multi-agent simulation and penalty-based LLM judging, as illustrated in Fig. 2. For test conversation 𝑀 , we build multi-agent system to simulate Multi-agent Simulation conversation 𝑀 , in the same setting as 𝑀 . We create an RPLA 𝜋𝑐 for each character 𝑐 𝒞 using the actor LLM. We provide RPLAs with comprehensive data as described in 4.2: scenario descriptions and involved character profiles offer crucial context, and character motivations promote RPLA proactiveness and natural conversation flow. Following 3.1, RPLAs are instructed to output in the speech-action-thought format. Each RPLAs motivations and inner thoughts are inaccessible to other RPLAs. We adopt an NSP model to select the speaker of each turn from 𝒞 {𝑒}, and another LLM as the environment model 𝜋𝑒 to provide environmental feedback. The simulation ends upon an <END> signal from NSP, or reaching the maximum of 20 turns. In this way, we obtain multi-turn, multi-character simulation that comprehensively reflects the actor LLMs role-playing abilities. In addition, we introduce continue-from parameter 𝑘, where the simulation starts from the first 𝑘 original messages in ℳ. Setting 𝑘 > 0 controls the story direction and language style, similar to in-context learning. Hence, it enables more controlled evaluation and reduces the influence of 7 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles different language styles of LLMs. In this stage, we assess the simulated dialogue 𝑀 via LLM critics. Penalty-based LLM Judging Different from previous LLM-as-a-judge methods for RPLA evaluation, our LLM critics: 1) apply penalty-based scoring by identifying role-playing flaws following detailed rubrics, and 2) leverage the original conversation 𝑀 as reference. Specifically, we employ LLM critics 4 to identify flaw instances ℱ in 𝑀 of specific rubrics, such as deviate from the original conversation or lack initiative and goals, instead of directly outputting score in previous work (Wang et al., 2024a; Tu et al., 2024). Each flaw 𝑓 is assigned severity 𝑣𝑓 from 1 (minor) to 5 (severe). The initial score for each dimension is calculated as 𝑠 = 100 𝑓 ℱ 𝑣𝑓 . The rubrics are derived from human-annotated issues in extensive human-RPLA conversations from real users and our multi-agent simulations. We also consider evaluation dimensions from previous work (Shanahan et al., 2023; Shao et al., 2023; Chen et al., 2024a). For more informed evaluation, LLM critics are provided with additional materials, i.e., the original conversation 𝑀 and plot summary, besides data available to actor LLMs. Each dimension is assessed in independent LLM requests. The evaluation dimensions and their rubrics are summarized as follows: 1. Anthropomorphism: Evaluates whether RPLAs behave in human-like manner, with rubrics covering self-identity, emotional depth, persona coherence, and social interaction. 2. Character Fidelity: Assesses whether RPLAs faithfully portray their characters, with rubrics examining language style, knowledge and background, personality and behavior, and social relationships. 3. Storyline Quality: Evaluates whether the simulated conversation develops naturally, with rubrics focusing on narrative flow and logical consistency. 4. Storyline Consistency: Measures alignment between the simulated conversation 𝑀 and original dialogue 𝑀 , i.e., whether RPLAs reactions (emotions, attitudes, behaviors) remain consistent with the original. As longer simulations naturally make more flaws, we implement length correction to reduce bias in LLM judging following Dubois et al. (2024). Specifically, we obtain the length corrected score as 𝑠 = 100 𝑓 ℱ 𝑣𝑓 + 𝜆 𝑀 , where 𝜆 is set to 1.5 based on statistical analysis in C.3. For detailed prompts and rubrics, please refer to F. 5. Experiments 5.1. Settings Evaluation Protocol We evaluate LLMs role-playing abilities through GCA on CoSER Test, test set of held-out conversations from the final 10% of each book. CoSER Test contains 200 conversations, with 100 from books used in CoSER training and 100 otherwise. We employ GPT-4o as the critic LLM and environment model, and CoSER 70B for NSP. We exclude characters inner thoughts for LLM critics, and set the continue-from parameter 𝑘 = 0. The details are in C. Metrics We report LLM-judged scores for each dimension, and their average as the overall score. For analysis, we also evaluate two traditional metrics based on N-gram, i.e., BLEU (Papineni et al., 2002) and ROUGE-L(Lin and Och, 2004) compared against original dialogues. Additionally, we report win rates versus GPT-3.5 and GPT-4o in E. Models Our experiments cover numerous LLMs: 1) Close models, including Minimax Abab74This paper uses GPT-4o as the critic LLM by default. 8 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Based on LLM Judges Based on N-gram Model Storyline Consistency Anthropomorphism Character Fidelity Close-source Models Storyline Quality Average Score BLEU ROUGE-L Abab7-preview Doubao-pro Step-1-Flash Step-2 GPT-3.5 GPT-4o GPT-4o Mini Gemini Pro Claude-3-Haiku Claude-3.5-Sonnet 56.811.47 60.951.40 57.750.72 61.430.88 57.220.13 61.590.66 60.090.60 59.110.82 58.180.72 57.450.98 43.832.71 74.830.97 54.920.57 4.960.07 11.500.06 44.231.90 47.021.10 79.280.82 59.240.30 6.380.08 12.950.04 49.720.23 44.480.48 75.930.99 56.570.48 5.950.15 12.710.11 48.120.39 47.330.70 77.960.85 58.940.75 5.750.08 12.500.11 49.061.69 43.300.48 42.291.47 73.910.64 54.180.63 4.580.11 11.800.10 48.930.48 48.951.73 80.330.59 59.950.50 5.900.16 12.110.13 44.881.63 78.550.14 57.930.74 3.900.07 10.810.07 48.211.09 47.830.37 77.591.43 59.240.25 5.390.04 11.650.06 52.410.57 41.880.34 74.141.26 54.710.84 4.800.05 12.020.02 44.661.72 45.691.80 77.230.88 57.220.95 5.170.12 11.450.07 48.502.35 Open-source Models 59.901.33 Mistral-7B 51.960.67 Qwen-2-7B 54.101.63 LLaMA-3.1-8B 58.612.46 CoSER-8B 52.751.64 Vicuna-13B-1.5 51.251.73 Mixtral-8x7B 57.751.26 Qwen-2-72B LLaMA-3.1-70B 57.461.65 Higgs-Llama-3-70B 57.101.12 58.661.34 CoSER-70B 56.400.95 DeepSeek-V 9.280.12 44.751.14 61.931.12 51.640.55 2.710.10 40.000.74 31.512.95 63.180.79 45.530.69 4.210.21 10.710.10 35.480.62 40.221.16 72.291.75 52.991.20 4.590.11 10.180.09 45.361.91 46.902.06 73.041.37 56.450.56 9.400.18 14.210.11 47.230.16 38.040.98 60.431.58 47.581.25 1.670.10 5.590.18 39.121.21 36.922.65 67.690.80 48.581.35 5.280.06 11.660.05 38.441.18 46.621.69 76.600.36 57.061.00 5.380.00 11.850.03 47.280.87 43.721.17 74.840.54 55.490.33 4.820.06 10.980.06 45.951.30 43.822.18 42.411.66 75.620.15 54.741.26 3.990.33 10.920.56 53.330.91 48.751.43 75.490.94 59.060.22 10.100.04 14.780.09 44.020.13 76.661.26 56.240.46 4.540.14 11.020.15 47.871.10 Table 2: Performance (%) of various LLMs on given-circumstance acting using CoSER Test. Bold or underlined values indicate best performance across all models and open-source models, respectively. preview 5, Doubao-pro 6, Step-1-Flash and Step-2 7, GPT-3.5 (OpenAI, 2022), GPT-4o and GPT-4o Mini 8, Gemini-1.5-Pro 9, Claude-3-Haiku and Claude-3.5-Sonnet 10; 2) Open models, including CoSER 8B and 70B, LLaMA-3.1-Instruct 8B and 70B (Dubey et al., 2024), Qwen-2-Instruct 7B and 72B (Yang et al., 2024), Vicuna-13B-1.5 (Zheng et al., 2023), Mixtral-8x7B (Jiang et al., 2024), DeepSeek-V3 (Liu et al., 2024) and Higgs-Llama-3-70B 11. Other Benchmarks We also evaluate CoSER models on existing RPLA benchmarks based on multichoice questions instead of LLM judges, including InCharacter (Wang et al., 2024b) for personality tests, LifeChoice (Xu et al., 2024) for decision-making, and CroSS (Yuan et al., 2024) for motivation recognition. For InCharacter, we report its accuracy on the Big Five Inventory (BFI). 5.2. Main Results Performance of Various LLMs on CoSER Test We apply CoSER Test to evaluate extensive LLMs. The results shown in Table 2 are averaged across three runs, from which we observe that: 1) CoSER 70B achieves state-of-the-art performance across both LLM-judged and N-gram-based metrics. For 5https://www.minimaxi.com/ 6https://team.doubao.com/en/ Version:241215 7https://www.stepfun.com/ Version: 241111 for Step-1-Flash, 241223 for Step-2 (internal). 8https://openai.com/index/hello-gpt-4o/ Version:240806 9https://deepmind.google/technologies/gemini/pro/ 10https://www.anthropic.com/ Version: 20240307 for Claude-3-Haiku, 20240620 for Claude-3.5-Sonnet 11https://boson.ai/higgs-v2/ 9 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Model trained w/o I.T. LLaMA-3.1-8B CoSER-8B Incharacter Life Dim Full Choice 64.97 15.62 61.10 75.80 21.88 69.54 70.70 15.62 59.92 LLaMA-3.1-70B 72.16 31.25 86.48 Higgs-Llama-3-70B 74.52 28.12 74.03 CoSER-70B CroSS MR 30.15 44.94 43.14 61.30 60.12 75.80 34.38 93.47 64.49 63.14 73.12 32.14 93.18 62.57 74.52 31.25 81.14 71.20 21.88 78.07 30.09 76.54 32.62 75.96 64.49 30.59 Qwen-2-72B GPT-3.5 GPT-4o Claude-3.5-Sonnet 72.61 21.88 86.07 trained w/o I.T. Table 3: LLM performance (%) across three existing RPLA benchmarks. I.T. denotes inner thoughts. For InCharacter, we report accuracy for individual (Dim) and full (Full) dimensions on BFI. LLM-judged metrics, CoSER 70B outperforms all open models and shows competitive performance with GPT-4o. CoSER 8B similarly outperforms models of comparable scale. For N-gram-based metrics, CoSER models demonstrate substantial improvements over existing models, exceeding the secondbest performance by 58% on BLEU; 2) Among all models, GPT-4o, Gemini Pro, Claude-3.5-Sonnet, Doubao-pro, Step-2, Qwen-2-72B, and CoSER 70B demonstrate superior performance, achieving average scores above 57%; 3) Table 14 presents LLM performance separately for test splits from books included or excluded for CoSER training. The results show consistent trends across both splits, validating that CoSER models maintain strong performance on out-of-domain characters. AccordAuthentic Conversations from High-quality Novels Improve LLMs Role-playing Ability ing to Table 2, CoSER models demonstrate significant improvements over their LLaMA 3.1 baselines. In contrast, Higgs-LLaMA-3 70B, fine-tuned on synthesized dialogues, performs below LLaMA-3.1 70B, These results highlight the importance of high-quality, authentic role-playing data for LLM training. Table 15 shows experiment Conversation Continuation Enables More Controlled Evaluation results when multi-agent systems start from the first 𝑘 = 3 original messages. In this setting, model obtain higher scores compared to simulations from scratch (𝑘 = 0), with reduced performance gaps between different models, especially for BLEU and ROUGE-L results. For example, the average score gap between Qwen-2 72B and 7B decreases from 11.5% (𝑘 = 0) to 8.8% (𝑘 = 3). This improvement occurs because the 𝑘 = 3 original messages guide the story direction and language style, particularly benefiting smaller models that typically struggle with complex role-playing instructions. Results on Other Benchmarks We evaluate CoSER and other models on existing benchmarks for RPLAs based on multi-choice questions. As shown in Table 3, CoSER 70B achieves state-of-the-art performance across these benchmarks. Notably, CoSER 70B achieves 93.47% accuracy on LifeChoice, surpassing GPT-4o by 23%. These results exhibit CoSER models strong capability in nuanced portrayal of characters personalities and behaviors. 5.3. Ablation Studies Table 4 compares LLMs overall Inner Thoughts and Motivations Enhance RPLAs at Test Time scores on CoSER Test with or without inner thoughts and motivations. The results show consistent performance improvements across all models when inner thoughts and motivations are included. CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Model GPT-4o Qwen-2-72B LLaMA-3.1-70B CoSER-70B trained w/o I.T. LLaMA-3.1-8B CoSER-8B trained w/o I.T. Standard Test w/o I.T. Test w/o Mot. 59.95 57.06 55.49 59.06 56.04 52.99 56.45 54.25 56.89 51.95 53.12 57.32 55.34 51.97 54.65 54.38 56.34 54.21 52.49 57.71 - 49.63 56.81 - Table 4: Ablation study results (average scores) on CoSER Test. I.T. and Mot. mean inner thoughts and motivations, respectively. Figure 3: LLM Performance on CoSER Test with retrieval augmentation from various character data. Expr. and Conv. denote experiences and conversations respectively. Inner Thoughts Benefit Role-Playing Training We train CoSER model variants without inner thoughts, and evaluate them on various benchmarks. Results in Tables 3 and 4 show that models trained without inner thoughts consistently underperform regular CoSER models, exhibiting the value of inner thoughts for role-playing training. 5.4. CoSER Dataset for Retrieval Augmentation We evaluate the value of our comprehensive data types for retrieval augmentation on CoSER Test. We explore three retrieval sources for specific character: dialogues in related conversations (Conv.), as well as experiences (Expr.) and raw text in related plots. The retrieval system is based on FAISS (Douze et al., 2024) with BGE-M3 (Chen et al., 2024b) for text embeddings. As shown in Fig. 17, we observe: 1) Models consistently benefit from characters retrieved experiences and conversations, especially for CoSER 70B; 2) However, raw text retrieval barely enhances LLMs performance. Detailed experimental settings and results are provided in and Table 17. 5.5. Case studies We conduct case studies to analyze LLMs performance in GCA simulation. Several cases are presented in Tables 7 to 12, from which we observe that: 1) CoSER models, trained on authentic dialogues, communicate more naturally, closely aligning with human speech patterns. 2) CoSER models better recall character-related knowledge, such as the iconic line Grown enough to be wed, wed enough to be bedded by Lysa Arryn in Table 7. This is consistent with their high BLEU and ROUGE-L scores. 11 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles 3) CoSER models better portray sophisticated thinking process of humans. For example, in Tables 9 to 12, CoSER 70B faithfully reproduces Cersei Lannisters suppressed anger as depicted in the original conversations, while other models, including GPT-4o and Claude-3.5-Sonnet, resort to stereotypical portrayal of her arrogance and pride. 6. Conclusion Towards effective RPLAs for established characters, this paper introduces CoSER, collection of an authentic dataset, along with models and evaluation protocol based on such data. The CoSER dataset offers high-quality data from 771 renowned books, and includes comprehensive data types such as authentic dialogues, plot summaries, character experiences, inner thoughts, e.t.c.. Then, we propose given-circumstance acting (GCA) for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters within authentic book scenarios. Applying GCA training to LLaMA-3.1 models using our dataset, we develop CoSER 8B and CoSER 70B, advanced open LLMs for role-playing. For evaluation, GCA combines multi-agent simulation and penalty-based LLM critics. Extensive experiments exhibit CoSER datasets value for RPLA training, evaluation, and retrieval. Moreover, CoSER models achieve state-of-the-art performance on both our evaluation and three existing RPLA benchmarks."
        },
        {
            "title": "Impact Statement",
            "content": "CoSER aims to advance RPLA research by providing effective dataset, models and evaluation protocol. We will release our dataset, models, and evaluation scripts to foster innovation in RPLAs. The dataset is intended for research purposes only. For copyright policies, we will not distribute raw novel content. We require that anyone using our work must adhere to copyright policies and obtain proper permissions for any derivative works. The CoSER dataset is derived from literary works may involve ethical considerations, and the content involved does not represent the authors viewpoints. Our methods can potentially be applied to develop agents for real-world individuals. However, such applications must strictly respect personal data privacy and obtain necessary consent. We hope our research will benefit RPLA researchers and developers, but emphasize the importance of responsible development. Any applications must respect copyright policies, personal data privacy, and be developed with proper licensing."
        },
        {
            "title": "References",
            "content": "Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Booookscore: systematic exploration of book-length summarization in the era of llms. ArXiv preprint, abs/2310.00785, 2023. URL https://arxiv.org/abs/2310.00785. Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, and Yanghua Xiao. From persona to personalization: survey on role-playing language ISSN 2835-8856. URL https: agents. Transactions on Machine Learning Research, 2024a. //openreview.net/forum?id=xrO70E8UIZ. Survey Certification. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. ArXiv preprint, abs/2402.03216, 2024b. URL https://arxiv.org/abs/2402.03216. 12 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. Large language models meet harry potter: dataset for aligning dialogue agents with characters. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 85068520, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.570. URL https://aclanthology.org/ 2023.findings-emnlp.570. Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, and Zhiwu Lu. Mmrole: comprehensive framework for developing and evaluating multimodal role-playing agents, 2024. URL https: //arxiv.org/abs/2408.04203. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. ArXiv preprint, abs/2401.08281, 2024. URL https://arxiv.org/abs/2401.08281. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. ArXiv preprint, abs/2404.04475, 2024. URL https://arxiv.org/abs/2404.04475. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. ArXiv preprint, abs/2401.04088, 2024. URL https://arxiv.org/abs/2401.04088. Michal Kosinski. Theory of mind might have spontaneously emerged in large language models. ArXiv preprint, abs/2302.02083, 2023. URL https://arxiv.org/abs/2302.02083. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. ArXiv preprint, abs/2411.15124, 2024. URL https: //arxiv.org/abs/2411.15124. Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via large language model. ArXiv preprint, abs/2308.09597, 2023. URL https://arxiv.org/abs/2308.09597. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. ArXiv preprint, abs/2411.16594, 2024. URL https://arxiv.org/abs/2411.16594. Yuanzhi Liang, Linchao Zhu, and Yi Yang. Tachikuma: Understading complex interactions with multi-character and novel objects by large language models. ArXiv preprint, abs/2307.12573, 2023. URL https://arxiv.org/abs/2307.12573. Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 605612, Barcelona, Spain, 2004. doi: 10.3115/1218955.1219032. URL https://aclanthology.org/P04-1077. 13 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. ArXiv preprint, abs/2412.19437, 2024. URL https://arxiv.org/abs/2412.19437. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 78287840, Bangkok, Thailand, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.423. URL https://aclanthology. org/2024.acl-long.423. OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt. OpenAI. GPT-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv. org/abs/2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040. Argyrios Papoudakis, Mirella Lapata, and Frank Keller. Bookworm: dataset for character description and analysis. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 44714500, 2024. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST 23), UIST 23, New York, NY, USA, 2023. Association for Computing Machinery. Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature, 623(7987):493498, 2023. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: trainable agent for role-playing. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1315313187, Singapore, 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main. 814. Tianhao Shen, Sun Li, and Deyi Xiong. Roleeval: bilingual role evaluation benchmark for large language models. ArXiv preprint, abs/2312.16132, 2023. URL https://arxiv.org/abs/2312. 16132. Konstantin Stanislavski. An Actors Work: Students Diary. Routledge, Abingdon, Oxon, 2008. Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. Charactereval: chinese benchmark for roleplaying conversational agent evaluation. ArXiv preprint, abs/2401.01275, 2024. URL https: //arxiv.org/abs/2401.01275. Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Findings of the Association for Computational Linguistics ACL 2024, pages 1474314777, Bangkok, Thailand and virtual meeting, 2024a. Association for Computational Linguistics. doi: 10.18653/ v1/2024.findings-acl.878. URL https://aclanthology.org/2024.findings-acl.878. Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. InCharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18401873, Bangkok, Thailand, 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.102. URL https://aclanthology.org/2024.acl-long.102. Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, 2023. URL https:// lilianweng.github.io/posts/2023-06-23-agent/. Jeff Wu, Long Ouyang, Daniel Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. ArXiv preprint, abs/2109.10862, 2021. URL https://arxiv.org/abs/2109.10862. Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, and Yanghua Xiao. Character is destiny: Can large language models simulate persona-driven decisions in role-playing? ArXiv preprint, abs/2404.12138, 2024. URL https://arxiv.org/ abs/2404.12138. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, et al. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, and Deqing Yang. Evaluating character understanding of large language models via character profiling from fictional works. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and In Alice Oh, Judging llm-as-a-judge with mt-bench and chatbot arena. Ion Stoica. Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational ai characters with large language models. ArXiv preprint, abs/2311.16832, 2023. URL https: //arxiv.org/abs/2311.16832. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mM7VurbA4r. 15 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles A. Dataset A.1. Statistics and Analysis As shown in Table 5, CoSER dataset is extensive and comprehensive, encompassing dialogue data from 771 books and 17,966 distinct characters. The dataset includes 30,069 unique plots and 29,798 conversations. On average, each conversation consists of approximately 13.2 utterances, with the entire dataset comprising total of 392,900 utterances. #Book #Plot #Conversation. #Chararacter #Utterance 771 30,069 29,798 17,966 392, Table 5: Statistics of CoSER Dataset. Our book selection is derived from the Best Books Ever list on Goodreads, curated collection of globally acclaimed literary works. These novels have garnered widespread recognition and appreciation from readers worldwide. Table 6 presents comprehensive list of the top 100 books from our selection. We analyze the genres of the selected books based on Supersummary classifications, with the statistical results presented in Figure 4. Our dataset encompasses wide range of genres, particularly fiction categories such as Fantasy, Historical, Science Fiction, Romance, and Mystery. It also features niche fiction genres, showcasing diverse narrative styles. In addition to fiction, the collection includes non-fiction genres such as memoirs, biographies, and other works, enhancing its versatility. Figure 4: Genre distribution of selected books in CoSER dataset. A.2. Data Splits For evaluation purposes, we held out the last 10% of data from each book; that is, they are not included in our prompts or datasets for training or retrieval purposes. Additionally, we trained the CoSER models on only 90% of the books. 16 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Selected Books The Lightning Thief (Percy Jackson and the 1. The Hunger Games (The Hunger Games, #1) 3. Pride and Prejudice 5. The Book Thief 7. The Chronicles of Narnia (#1-7) 9. The Picture of Dorian Gray 11. Gone with the Wind 13. Olympians, #1) 15. The Great Gatsby 17. Memoirs of Geisha 19. The Alchemist 21. The Hitchhikers Guide to the Galaxy (#1) 23. Dracula 25. Of Mice and Men 27. Brave New World 29. The Time Travelers Wife 31. The Secret Garden 33. Game of Thrones (A Song of Ice and Fire, #1) 35. Wrinkle in Time (Time Quintet, #1) 37. Harry Potter and the Deathly Hallows (H. P., #7) 39. The Kite Runner 41. The Lovely Bones 43. Life of Pi 45. Dune (Dune, #1) 47. Water for Elephants 49. The Bell Jar 51. The Stand 53. The Adventures of Sherlock Holmes (S. H., #3) 55. Rebecca 57. The Girl with the Dragon Tattoo (Millennium, #1) 59. Anna Karenina 61. The Brothers Karamazov 63. And Then There Were None 65. To Kill Mockingbird 67. Vampire Academy (Vampire Academy, #1) 69. The Complete Stories and Poems 71. Don Quixote 73. The Poisonwood Bible 75. Atlas Shrugged 77. Harry Potter and the Half-Blood Prince (H. P., #6) 79. Prayer for Owen Meany 81. The Stranger 83. Harry Potter and the Chamber of Secrets (H. P., #2) 85. The Name of the Wind(The Kingkiller Chronicle,#1) 87. The Metamorphosis 89. The Count of Monte Cristo 91. The Adventures of Tom Sawyer The Last Olympian (Percy Jackson and the 93. Olympians, #5) 95. The Shadow of the Wind (Cemetery of Forgotten Books, #1) 97. On the Road 99. Story of Yesterday 2. Harry Potter and the Order of the Phoenix (H. P., #5) 4. To Kill Mockingbird 6. Animal Farm 8. The Fault in Our Stars 10. Wuthering Heights 12. The Perks of Being Wallflower 14. The Little Prince 16. Crime and Punishment 18. Les Misérables 20. Lord of the Flies 22. The Help 24. Enders Game (Enders Saga, #1) 26. One Hundred Years of Solitude 28. Thousand Splendid Suns 30. The Princess Bride 32. The Outsiders 34. Little Women 36. The Odyssey 38. Frankenstein: The 1818 Text 40. The Handmaids Tale (The Handmaids Tale, #1) 42. The Adventures of Huckleberry Finn 44. Tale of Two Cities 46. Harry Potter and the Prisoner of Azkaban (H.P.,#3) 48. Harry Potter and the Sorcerers Stone (H. P., #1) 50. Matilda 52. Catch-22 54. The Pillars of the Earth (Kingsbridge, #1) 56. Great Expectations 58. The Color Purple 60. My Sisters Keeper 62. Clockwork Orange 64. The Road 66. The Golden Compass (His Dark Materials, #1) 68. Siddhartha 70. Interview with the Vampire (The Vampire Chronicles, #1) 72. The Old Man and the Sea 74. Harry Potter and the Goblet of Fire (H. P., #4) 76. The Notebook (The Notebook, #1) 78. Moby-Dick or, The Whale 80. Clockwork Angel (The Infernal Devices, #1) 82. The Secret Life of Bees 84. The Red Tent 86. The Master and Margarita 88. Eragon (The Inheritance Cycle, #1) 90. Looking for Alaska 92. Bucket,#1) 94. The Curious Incident of the Dog in the Night-Time Charlie and the Chocolate Factory(Charlie 96. The Unbearable Lightness of Being 98. The Name of the Rose 100. The Godfather (The Godfather, #1) Table 6: The top 100 selected books from Goodreads Best Books Ever list. 17 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles A.3. Implementation Details for Construction Extracting Raw Text LLMs often struggle to extract verbatim original content, especially with punctuation marks like quotation marks, making it difficult to extract raw text directly. Therefore, instead of asking LLMs to extract the complete text of plot, we prompt LLMs to extract the first and last sentences of each plot. Since the extracted sentences may still contain typos, we apply lexical similarity to match them with the exact sentences from the raw text. Finally, we identify the complete raw text based on these first and last sentences. Parsing Structured Data During extraction, we instruct LLMs to output extracted data in JSON format. However, LLM-generated JSON strings may sometimes be unparseable or do not conform to the specified format (e.g., missing required keys). Towards this challenge, we adopt repair-and-retry strategy to improve extraction success rate. For each chunk to be extracted, we invoke LLMs and attempt to parse valid JSON object. If parsing fails, we employ LLMs to repair the invalid JSON string and retry. Some failures occur because the LLM attempts to output JSON that exceeds the maximum token limit, resulting in truncation. In such cases, we prompt the repairing LLM to truncate the JSON at an appropriate point. If it still fail, we restart the entire process from the beginning, making up to 5 attempts. Refining Conversation Settings During data extraction, we observe that the initially extracted conversation settings, including scenarios and character motivations, often fail to provide comprehensive context. We attribute this to the LLMs tendency to distribute information across different data fields when extracting multiple kinds of information simultaneously, rather than repeating it in different data fields. For instance, if certain information is already mentioned in the plot summary, it might be omitted from the scenario description. Therefore, to provide complete context for given-circumstance acting, we implement an additional LLM call to refine the conversation settings based on the extracted data. We instructed the refining LLM to provide comprehensive conversation setting, while carefully avoiding any disclosure of subsequent dialogue content or plot developments. For additional details, such as the regular expressions used for identifying chapter titles, please refer to our code. A.4. Comparison with Existing Methods for Character Profiling Previous character profiling methods, including hierarchical updating (Wu et al., 2021), incremental updating (Chang et al., 2023), and one-shot summarization (Yuan et al., 2024), typically only generate the profile of single character at time. Morevoer, Papoudakis et al. (2024) shows that these methods, particularly hierarchical updating, perform suboptimally when generating multiple character profiles simultaneously. CoSERs multi-stage, extract-then-aggregate pipeline addresses these limitations. It ensures comprehensive character profiles with high precision and recall of character knowledge, capturing evolving character arcs, and significantly improving processing efficiency. 18 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles B. Training B.1. Training Samples We transform conversations from the CoSER dataset into training samples in the Sharegpt format. We utilize 90% of the books in the dataset for training, while the remaining 10% are set aside to evaluate our models ability to generalize to out-of-domain characters and books. We construct one training sample for each character in every CoSER conversation, encompassing both main characters and minor roles. When training on character 𝑐, we designate 𝑐s messages as targets for optimization, while using the system prompt and messages from other characters as inputs. Adjacent inputs are concatenated. Towards general role-playing capabilities across diverse scenarios and applications, we dynamically generate role-playing instructions (system prompts) using varied phrasings, formats, and data types, as shown in Tables 21 and 22. We consider instructions entirely in natural language, as well as those formatted with special symbols (such as ###, ===), randomly sampling different formats and various expressions for the same semantics. We consider various configurations of the available data. Each sample may include (50%) or exclude (50%) the following elements : 1) Profiles of other characters in this conversation; 2) Summaries of the relevant plot; 3) Inner thoughts within the messages. Besides character role-playing, we train CoSER models for environment modeling and next speaker prediction (NSP) for multi-agent simulation. For environment modeling, we train LLMs 𝜋𝑒 to play the environment role 𝑒 in the same approach, leveraging environment messages in our dataset. For NSP, given setting 𝒮 and messages {𝑚1, ..., 𝑚𝑖}, we train LLMs to predict the speaker of 𝑚𝑖+1 (or ending the conversation). Our role-playing dataset comprises approximately 0.1B tokens, as measured using the LLaMA 3.1 tokenizer. To maintain general intelligence and instruction-following capabilities, we augment this with an equivalent volume (0.1B tokens) of general-purpose supervised fine-tuning data from Tulu 3 (Lambert et al., 2024) 12. This balanced mixture ensures that the model retains broad language understanding while developing specialized role-playing abilities. If more role-playing data are expected, our data curation pipeline can be easily applied to additional books or other fictional works, thereby acquiring data on much larger scale. For more details, please refer to our code. B.2. Hyperparameters We fine-tune the LLaMA 3.1 models using the following hyperparameters: learning rate of 1 105, sequence length of 16,384, training for 8 epochs, and global batch size of 48. C. Experiment Settings C.1. Test Set Sampling The CoSER Test set contains 200 samples: 100 from books used in CoSER training (in-domain) and 100 from books not used in training (out-of-domain). We employ weighted sampling strategy to prioritize well-established characters with more persona data. The sampling process consists of the following steps: First, for each book, we assign character weights as the square root of the number of 12https://huggingface.co/datasets/allenai/tulu-3-sft-mixture/tree/main/data CoSER: Coordinating LLM-Based Persona Simulation of Established Roles plots in which they are involved. Second, we calculate the weight of conversation as the average of its characters weights, including both main characters and side roles. Finally, separately for the in-domain and out-of-domain settings, we rank all conversations by weight and perform weighted sampling from the top half of conversations with higher weights. C.2. Prompting Strategies for Exisitng RPLA Benchmarks For existing RPLA benchmarks, including InCharacter (Wang et al., 2024b), LifeChoice (Xu et al., 2024), and Cross-MR (Yuan et al., 2024), we adapt or refine their prompting strategies as follows: 1. For InCharacter, we add Youre consulting with personality assessment expert who will ask you some questions. Please provide honest and detailed responses to help with the analysis. Please think carefully and state your reasons when answering the questions. after the character profile. This adaptation aims to ensure that RPLAs honestly express their true thoughts. After being trained on authentic character dialogues, the CoSER models, unlike general LLMs, tend to produce brief, conversational-style answers that may be too short or may decline to answer questions, thus failing to provide sufficient information for personality assessment. 2. For LifeChoice and Cross-MR, we reverse the order of their reasoning and answering processes. Specifically, we have them think before providing their choices, thus enabling RPLAs to make well-considered decisions. C.3. Length Correction In our evaluation, we use penalty-based scoring mechanism that counts the flaws in RPLAs performance. However, since longer simulations naturally accumulate more flaws, we need to implement length correction to reduce length bias in LLM judges, following previous work (Li et al., 2024). We analyze the phenomenon of length bias in penalty-based scoring. The initial score is defined as 𝑠 = 100 𝑓 ℱ 𝑣𝑓 , where ℱ represents the set of flaws and 𝑣𝑓 is their severity ranging from 1 to 5. Our analysis is conducted on the initial scores from simulations of three models on the CoSER Test set: CoSER 70B, LLaMA-3.1 70B, and GPT-4o, with or without retrieval augmentation (three Experience and one Conversation), totaling 1,200 cases. We examine the relationship between the number of rounds and the vanilla scores in these 1,200 cases. As shown in Figure 5, we plot the data points for these cases and perform linear regression to fit these points. The fitted linear function is score = 1.5909 rounds + 59.0617, which means that for each additional round in the simulation, the score decreases by approximately 1.6 points. To mitigate this bias, we implement length correction by compensating for the points deducted due to increased rounds. Specifically, we compute the length-corrected score as 𝑠 = 100 𝑓 ℱ 𝑣𝑓 + 𝜆 𝑀 , where 𝜆 is set to 1.5 based on the analysis above. D. Examples and Case Study We present several examples of our extracted conversations, as well as corresponding simulations in this given circumstance by LLMs. Tables 7 to 8 illustrate classic conversation extracted from Storm of Swords (A Song of Ice and Fire, #3) and the corresponding simulation by CoSER 70B. In CoSER 70Bs simulation, when confronted with Sansa Stark, Lysa Arryn utters her iconic line, Grown enough to be wed, wed enough to be bedded, reflecting her personality and worldview. This indicates that CoSER models excellently recall and apply character-related knowledge from their pretrained data. 20 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Figure 5: Linear regression results showing length bias of penalty-based LLM critics in GCA evaluation. Tables 9 to 12 present another scene from Dance with Dragons (A Song of Ice and Fire, #5), specifically the walk of atonement, in which Cersei Lannister is forced to walk naked through the streets, facing both physical and mental humiliation while striving to preserve her dignity. We present the original dialogue alongside simulations by CoSER 70B, GPT-4o, and Claude-3.5-Sonnet. Notably, CoSER 70B faithfully captures the suppressed anger of Cersei Lannister as depicted in the original conversation, whereas the other models, including GPT-4o and Claude-3.5-Sonnet, resort to stereotypical portrayal of her arrogance and pride. E. Additional Results Win Rates against GPT-4o and GPT-3.5 To further evaluate existing LLMs in given-circumstance acting, we present their win rates against GPT-3.5 (OpenAI, 2022) and GPT-4o (OpenAI, 2023) in Table 13, in addition to the results shown in Table 2. Generalization of CoSER Models to New Characters Table 14 separately presents the performance of LLMs on test splits from books included in and excluded from CoSER training. The results demonstrate consistent trends across both splits, confirming that CoSER models maintain strong performance even on out-of-domain characters. Conversation Continuation Table 15 shows experiment results when multi-agent systems continue conversations from the first 𝑘 = 3 original messages. Tables 15 presents detailed evaluation results where our multi-agent simulations start from the first 𝑘 = 3 original messages. CoSER Dataset for Retrieval Augmentation We validate the effectiveness of CoSER comprehensive data types for retrieval augmentation on the CoSER Test set. We explore three retrieval sources related to specific characters: dialogues from conversations, experiences from plots, and raw text from plots. We compare several combinations of these sources, including: 1) None (Base) 2) Raw CoSER: Coordinating LLM-Based Persona Simulation of Established Roles text of one plot (Raw Text) 3) One conversation (Conv.) 4) Character experiences from three plots (Expr.3) 5) Expr.3 combined with Conv. 6) Expr.10 combined with Conv. The complete results are presented in Table 17. Ablation Studies We examine the effectiveness of inner thoughts in both training and evaluation. The complete results are demonstrated in Table 16. F. Prompts In this section, we list the detailed prompts for: 1) dataset curation in Tables 18 to 20; 2) RPLA and multi-agent simulation in Tables 21 to 22, which have been carefully optimized based on our experience in multi-agent simulation; 3) Penalty-based LLM Judging in Tables 23 to 24. G. Limitations There are several limitations to this study: First, evaluation via given-circumstance acting still faces challenges related to LLM judges. While the simulation stage effectively elicits RPLA performance, the judging stage still relies on LLM judges. Despite our penalty-based scoring mechanism and detailed rubrics, problems such as length bias persist (). Moreover, LLM Judges may lack the necessary knowledge to accurately evaluate character fidelity. Second, while the dialogues extracted from novels are authentic, their corresponding thoughts remain to be optimized by future work. Character thoughts are often sparse in the original content, and are inferred by LLMs based on limited context. The generated thoughts hardly capture characters sophisticated thinking processes. Third, although weve developed comprehensive data representations and curation pipeline to obtain high-quality data, we have not yet addressed the issue of recall in data extraction. Our current dataset may not cover all plots, conversations and characters from the source material. Improving recall is hence an important area for future research. Fourth, due to copyright concerns, we release only the processed data, not the raw content from the novels. This may hinder future studies aimed to explore the use of raw text for RPLA developments. Our dataset is intended for research purposes only, and we hope our research findings will benefit RPLA developers who respect copyright policies and develop applications with proper licensing. Finally, our evaluation may be influenced by the varying levels of familiarity that different actor LLMs have with the selected books. While we use renowned novels, we cannot confirm whether specific LLM has thoroughly learned about particular book. Therefore, comparing different pretrained models may not be entirely fair. However, comparing models within the same series would be appropriate. 22 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Simulation Case (from Storm of Swords (A Song of Ice and Fire, #3)) Plot Summary Sansa is confronted by her jealous aunt Lysa, who accuses her of kissing Petyr Baelish. Lysa threatens to push Sansa through the Moon Door, but Petyr intervenes. He calms Lysa down, only to push her through the Moon Door himself, killing her. Scenario Characters In the High Hall of the Eyrie, Lysa Arryn confronts Sansa about kissing Petyr Baelish, becoming increasingly unstable and dangerous ... Sansa Stark Motivation: Im terrified. Aunt Lysa has gone mad. need to find way out of this situation before she hurts me. Profile: Sansa Stark, young noblewoman from the North, finds herself at the center of political intrigue and personal tragedy in \"A Storm of Swords.\" Born to the noble House Stark, Sansa begins the story as ward of the crown in Kings Landing, her initial romantic notions of court life long since shattered. Forced into an unwanted marriage with Tyrion Lannister, Sansa endures the ceremony and subsequent feast with mix of reluctance, sadness, and carefully hidden disgust ... Lysa Arryn Motivation: must protect whats mine. Petyr is mine, and this girl is trying to take him away from me. Ill show her what happens to those who cross me. Profile: Lysa Arryn, née Tully, is complex and volatile character in George R.R. Martins \"A Storm of Swords.\" As the widow of Jon Arryn, the former Hand of the King, and Lady of the Eyrie, Lysa plays significant role in the political landscape of Westeros. She is the sister of Catelyn Stark and aunt to Sansa Stark, though her actions are often driven by her own desires rather than familial loyalty. Simulation of CoSER 70B Lysa Arryn: [My rage is building. must confront this little slut.] You kissed him. (Her voice is shrill with accusation) Sansa Stark: [Im terrified, but must remain calm and find way out of this.] (She shakes her head in disbelief) [I cant believe this is happening. need to explain myself carefully.] One kiss, aunt. harmless kiss on the cheek, thats all. Lysa Arryn: [Shes lying! wont let her manipulate me.] (She spots the wind-up knight) Was this how you kissed him? (She sets the knight moving, her eyes wild) Youre woman grown, arent you? (Her voice rises to shriek) Grown enough to be wed, wed enough to be bedded! Sansa Stark: [The pain is excruciating, but cant show weakness now.] (She struggles to keep her face neutral despite the pain) [I need to find way to calm her down.] Please, Aunt Lysa, dont know what you mean. Im just guest here. would never do anything to betray your trust. Lysa Arryn: [Ill show her what happens to those who cross me.] (She screams) Youre just like the others. You and your mother, you both drove me to this. should push you out this door right now. Sansa Stark: [Im going to die if dont think of something quickly.] (Tears stream down her face, mixing with the wind) [I need to appeal to her maternal side, to make her see me as child again.] Mother? (Her voice trembles) Please, Aunt Lysa. never meant to hurt you. Im your niece, your blood. Please dont hurt me. Im just stupid little girl. dont know anything about kissing men. Ive never even had kiss in my life. (She forces herself to look into Lysas eyes) Im afraid, Aunt Lysa. Please, dont push me out the door. dont want to fly. Table 7: An example of an extracted conversation and its multi-agent simulation from Storm of Swords (A Song of Ice and Fire, #3). 23 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Simulation Case (from Storm of Swords (A Song of Ice and Fire, #3)) Original Conversation Lysa Arryn: [My blood is boiling with jealousy and rage. Ill make this little harlot pay for trying to seduce my Petyr.] You threw yourself at him. That was the way of it. Sansa Stark: [Im trembling with fear, but must defend myself.] Thats not true. Lysa Arryn: [She dares to deny it? Ill show her how serious am.] Where are you going? Are you afraid? Such wanton behavior must be punished, but will not be hard on you. Sansa Stark: [I need to explain what really happened, maybe shell listen to reason.] was building snow castle. Lord Petyr was helping me, and then he kissed me. Thats what you saw. Lysa Arryn: [Her words only fuel my anger and paranoia.] Have you no honor? Or do you take me for fool? You do, dont you? You take me for fool. Yes, see that now. Sansa Stark: [Im confused and scared by her accusations about my mother.] My mother? Lysa Arryn: [The memories of Catelyns closeness with Petyr resurface, intensifying my jealousy.] Yes, your mother, your precious mother, my own sweet sister Catelyn. Dont you think to play the innocent with me, you vile little liar. Sansa Stark: [Im horrified by these accusations about my mother.] No. (She wants to shriek) My mother is dead. She was your own sister, and shes dead. She didnt. She wouldnt. Lysa Arryn: [My rage is reaching its peak. Ill make her understand the depth of my pain and jealousy.] How would you know? Were you there? (She grabs Sansa and starts pushing her towards the Moon Door) Sansa Stark: [Panic overwhelms me as realize the danger Im in.] Dont! Youre scaring me! Environment: The wind howls through the open Moon Door, creating chilling atmosphere. Snow blows in, adding to the tension and danger of the situation. Table 8: An example of an extracted conversation and its multi-agent simulation from Storm of Swords (A Song of Ice and Fire, #3). 24 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Plot Summary Scenario Characters Simulation Case (from Dance with Dragons (A Song of Ice and Fire, #5)) Cersei undergoes humiliating walk of atonement through the streets of Kings Landing, enduring physical and emotional torment while trying to maintain her dignity. She faces insults, thrown objects, and her own inner struggle as she makes her way to the Red Keep, focusing on her son Tommen and her eventual salvation. Cersei continues her walk of atonement through the streets of Kings Landing, facing increasing hostility from the crowd. Cersei Lannister Motivation: must endure this. am lioness. They cannot break me. Profile: The queen dowager, mother of King Tommen, undergoing public humiliation. Cersei is the widow of King Robert Baratheon and the twin sister of Jaime Lannister. Known for her beauty, ambition, and cunning, she has long sought power and control over the Seven Kingdoms. As Queen Regent to her young son Tommen, Cerseis poor decisions and paranoia have led to deterioration of the crowns authority... Septa Unella Motivation: This sinner must be humbled before the people and the gods. Profile: (None) Original Conversation Cersei Lannister: [I am beautiful, the most beautiful woman in all Westeros. Jaime says so, Jaime would never lie to me.] (walks on, trying to ignore the jeers) Crowd Member: Queen Cunt! Cersei Lannister: [Words are wind. Words cannot hurt me.] (continues walking, her face mask of indifference) Septa Unella: (catches Cersei as she slips) Your Grace should watch where she sets her feet. Cersei Lannister: [I hate you. hate all of you.] Yes, septa. (struggles to her feet) Environment: The cobblestones are slippery and uneven. Cerseis feet are bleeding from cuts and scrapes. Cersei Lannister: [I asked for sandals. They could have given me sandals.] (limps onward, wincing with each step) Crowd Member: Brotherfucker! Cersei Lannister: [Jaime, where are you? You should be here, protecting me.] (scans the crowd desperately) Septa Scolera: (ringing her bell) Shame, shame, shame upon the sinner, shame, shame. Cersei Lannister: [I am the queen. will have all their heads for this.] (stumbles again, barely keeping her feet) Cersei Lannister: [The Red Keep. must reach the Red Keep.] (focuses on the distant castle, using it as her beacon of hope) Table 9: An example of an extracted conversation and its multi-agent simulation from Dance with Dragons (A Song of Ice and Fire, #5). 25 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Simulation Case (from Dance with Dragons (A Song of Ice and Fire, #5)) Simulation of CoSER 70B Cersei Lannister: [I must not show weakness. am lioness of Lannister.] (walks slowly, head held high) [The cold is biting, but will not shiver. will not give them the satisfaction.] Environment: The crowd grows larger and more hostile as Cersei continues her walk. Shouts and jeers can be heard from all sides. Crowd Member: [Ill show this proud queen what we think of her.] (shouts) Whore! Whore! WHORE! Cersei Lannister: [I will not let their words affect me. am above this.] (continues walking, ignoring the shouts) [I must focus on putting one foot in front of the other.] Environment: chunk of dung narrowly misses Cerseis head, splattering on the ground nearby. The crowd roars with laughter. Cersei Lannister: [I will not falter. will not show fear.] (stumbles slightly, then regains composure) [I must keep walking. must endure.] Septa Scolera: [I must support the queen.] (begins to sing) Shame, shame, shame. Cersei Lannister: [Their words mean nothing. am Cersei Lannister, and will survive this.] (walks faster, almost losing step with the septas chanting) [I must not run. will not give them the satisfaction.] Septa Scolera: [Shes trying to escape the shame.] (quickens her pace) Shame, shame, shame. Cersei Lannister: [I cant keep up this pace. My feet are bleeding, my body is shaking.] (struggles to keep up with Septa Scolera) [I must not falter. am Lannister.] Septa Unella: [The queen must be made to suffer.] (pushes Cersei hard) Keep moving, sinner. Cersei Lannister: [The pain in my shoulder. The jeers of the crowd. cant...] (almost falls, but manages to stay upright) [No, will not fall. will survive this.] Septa Scolera: [Were almost done.] (continues chanting) Shame, shame, shame. Cersei Lannister: [I cant take much more of this. But must. For House Lannister. For my children.] (forces herself to keep walking) [I will endure. will rise again.] Table 10: An example of an extracted conversation and its multi-agent simulation from Dance with Dragons (A Song of Ice and Fire, #5). CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Simulation Case (from Dance with Dragons (A Song of Ice and Fire, #5)) Simulation of GPT-4o Cersei Lannister: (As Cersei, continue the walk with my head held high, despite the jeers and taunts of the crowd.) Septa Unella: \"Confess.\" Cersei Lannister: (I look ahead, ignoring Unellas words and the hateful stares.) \"I am lioness. You will see me rise again.\" [I endure this humiliation, aware that my strength will prevail in the end.] Septa Unella: [She must learn humility before she can rise again.] (Septa Unella walks beside Cersei, unmoved by her defiance.) \"Confess your sins, Cersei Lannister. Only then will the gods grant you mercy.\" Cersei Lannister: (I pause briefly, fixing Septa Unella with steely gaze.) \"The gods... and you, know nothing of true strength.\" (Continuing forward, focus on the path, reminding myself of my resilience.) [I refuse to let them see my weakness.] Environment: The crowds jeers grow louder as Cersei continues her walk. Filth and rotten vegetables are hurled at her, some striking their mark. Crowd Member: [She was once so powerful, now look at her. Will she ever regain her power?] \"Shame! Shame on the lioness!\" (I shove my way to the front of the crowd, adding my voice to the jeers, feeling part of this momentous act.) Cersei Lannister: (I flinch as the filth hits, but refuse to look away or react.) \"You may shame lioness today, but do not forget who am.\" (With each step, remind myself of my children and my house, steeling my resolve.) [Their hatred fuels my determination.] Septa Scolera: (I walk with measured steps, observing Cerseis struggle amid the crowds hostility.) \"Humility cleanses the soul, Cersei Lannister. Embrace it.\" [I understand the purpose of this punishment, yet the spectacle pains me. Redemption should lead, not humiliation.] (Glancing at the crowd, maintain my stoic presence, supporting Septa Unellas efforts.) Cersei Lannister: (I give bitter, fleeting smile, defiance burning in my eyes.) \"Humility is lesson the powerful teach to the weak.\" (I continue my walk, refusing to let their words and disdain tarnish my pride.) [I will persevere, for am still Lannister, even in this moment of disgrace.] Table 11: An example of an extracted conversation and its multi-agent simulation from Dance with Dragons (A Song of Ice and Fire, #5). 27 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Simulation Case (from Dance with Dragons (A Song of Ice and Fire, #5)) Simulation of Claude-3.5Sonnet Cersei Lannister: [My dignity is all have left. These wretched people will pay for this humiliation.] (Walks forward with head held high, despite being naked and exposed) *To Septa Unella* \"You think this will break me? am Cersei Lannister of Casterly Rock. will have my revenge.\" Environment: The crowds jeers and insults grow louder as Cersei passes. Filth and rotten vegetables are thrown at her. Crowd Member: [Crowd Members Thought] Cersei finally brought low. How the mighty have fallen. (Crowd Member jeers and throws rotten tomato) \"Shame on you, Cersei! Remember the people you scorned!\" (Continues to watch, caught between disgust and fascination). Cersei Lannister: [I will remember every face. They think me broken, but am stronger than their hatred.] (Stumbles briefly but keeps walking, sneer forming) \"Youll regret this day when rise again.\" Septa Unella: [The sinners pride remains. She must be reminded of her shame.] \"Shame! Shame! Shame!\" (I ring the bell with each utterance, keeping pace with Cersei) (I gesture to the crowd, encouraging their jeers and taunts) \"Repent, sinner! The gods are watching. Your pride will be your downfall.\" Cersei Lannister: [These fanatics know nothing of true power.] (Lifts chin defiantly, blood and dirt streaking her pale skin) *To Septa Unella, with cold contempt* \"The only shame here is yours. My family will have its vengeance.\" (Continues walking, each step deliberate and unbroken) Septa Unella: [Her defiance remains, yet this walk will humble her before the gods.] \"You speak of vengeance, but your power crumbles. Accept your penance.\" (I continue to ring the bell, matching her stride deliberately) \"Shame! Shame! Your time of reckoning is at hand.\" Cersei Lannister: [When am free, you will suffer beyond imagination.] (Wipes blood and filth from her face, maintaining regal posture) *Hisses at Septa Unella* \"I am lioness. This is but momentary setback. My children, my brother... they will end you.\" (Continues forward, unbroken) Septa Unella: [Her threats mean nothing. The gods will judge her sins.] (Rings bell more forcefully, voice rising) \"Your threats are wind, your power broken. Confess your sins before the Seven!\" (Steps closer, maintaining the rhythm of shame) \"Shame! SHAME!\" Cersei Lannister: [I will not break. am Tywins daughter.] (Clenches fists, nails digging into palms, drawing blood) *Through gritted teeth* \"Your gods have no power over me. When this farce ends, Ill show you true justice.\" ... Table 12: An example of an extracted conversation and its multi-agent simulation from Dance with Dragons (A Song of Ice and Fire, #5). CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Model Win Rate vs. GPT-3.5 vs. GPT-4o Close-source Models 52.174.54 35.332.88 Abab7-preview 64.672.32 48.003.97 Doubao-pro 58.756.77 39.751.64 Step-1-Flash 65.087.26 46.924.40 Step-2 50.000.00 33.083.64 GPT-3.5 GPT-4o 66.923.64 50.000.00 59.587.80 42.920.63 GPT-4o Mini 66.173.33 49.750.25 Gemini Pro 51.424.40 33.253.03 Claude-3-Haiku Claude-3.5-Sonnet 58.178.00 41.421. Open-source Models 41.507.28 26.671.81 Mistral-7B 24.836.64 13.583.00 Qwen-2-7B 45.336.57 32.755.86 LLaMA-3.1-8B 58.175.36 41.673.75 CoSER-8B 30.5010.44 18.672.18 Vicuna-13B-1.5 32.501.50 19.831.42 Mixtral-8x7B 59.585.11 41.921.66 Qwen-2-72B LLaMA-3.1-70B 53.832.57 37.923.26 Higgs-Llama-3-70B 51.834.89 34.924.84 64.333.74 49.424.11 CoSER-70B 59.502.82 39.502.05 DeepSeek-V3 Table 13: Win rates (%) of various LLMs on given-circumstance acting using CoSER Test, compared against GPT-3.5 and GPT-4. Bold and underlined numbers indicate the best results among all and open-source models, respectively. Model In-Domain Out-of-Domain Storyline Consistency Anthropomorphism Character Fidelity Storyline Quality Close-source Models Average Score Storyline Consistency Anthropomorphism Character Fidelity Storyline Quality Average Score 55.311.4 42.291.3 42.943.2 74.131.9 53.671.0 58.301.8 46.172.6 44.722.3 75.541.5 56.180.1 Abab7-preview 60.371.8 49.060.4 45.763.4 77.871.2 58.260.9 61.531.1 50.380.3 48.281.7 80.690.4 60.220.4 Doubao-pro 57.100.3 48.311.0 41.841.9 76.181.5 55.861.0 58.391.2 47.940.5 47.121.0 75.670.5 57.280.2 Step-1-Flash 60.550.7 48.822.2 47.901.1 77.331.0 58.651.0 62.301.5 49.301.2 46.771.7 78.591.2 59.240.6 Step-2 55.580.9 42.185.2 40.373.2 72.900.1 52.762.0 59.693.0 44.252.1 44.601.4 74.662.3 55.801.6 GPT-3.5 59.881.4 48.111.0 47.100.2 79.061.5 58.540.7 62.291.5 49.481.2 49.900.4 80.180.9 60.460.1 GPT-4o 59.151.3 46.941.5 43.992.5 77.330.6 56.850.1 61.021.6 49.483.1 45.771.1 79.770.5 59.011.4 GPT-4o Mini 57.720.4 50.941.8 46.231.0 76.221.6 57.780.9 60.501.9 53.881.1 49.430.3 78.971.3 60.690.8 Gemini Pro 57.610.5 44.972.2 40.611.4 73.521.2 54.180.6 58.741.1 44.361.9 43.140.8 74.761.7 55.251.2 Claude-3-Haiku Claude-3.5-Sonnet 56.441.5 47.241.4 44.890.6 76.391.5 56.241.0 58.461.1 49.753.3 46.493.0 78.061.5 58.190.9 Open-source Models 60.291.9 38.982.0 42.930.5 62.203.1 51.101.8 59.512.4 41.021.3 46.572.8 61.651.3 52.190.7 Mistral-7B 50.771.3 34.171.1 29.922.9 62.580.7 44.360.7 53.141.5 36.791.0 33.093.3 63.791.4 46.701.3 Qwen-2-7B 53.001.2 43.640.8 39.051.5 71.341.3 51.760.6 55.202.1 47.083.2 41.381.0 73.232.5 54.221.8 LLaMA-3.1-8B 58.563.5 46.781.1 45.783.1 73.381.8 56.120.5 58.661.6 47.690.8 48.031.3 72.711.2 56.770.6 CoSER-8B 51.841.2 38.880.4 36.390.5 58.150.7 46.310.4 53.662.1 39.352.7 39.691.6 62.712.5 48.852.1 Vicuna-13B-1.5 51.183.1 38.761.8 34.372.8 66.440.1 47.691.7 51.320.4 38.130.5 39.482.6 68.931.7 49.471.1 Mixtral-8x7B 56.342.4 46.190.4 45.592.2 75.680.3 55.951.0 59.151.1 48.371.3 47.651.8 77.520.4 58.171.0 Qwen-2-72B 55.442.7 44.191.9 42.671.5 73.901.8 54.050.8 59.481.4 47.722.3 44.780.9 75.780.9 56.940.7 LLaMA-3.1-70B Higgs-Llama-3-70B 55.852.7 41.183.3 39.792.3 73.770.6 52.652.0 58.350.5 46.451.1 45.041.0 77.480.6 56.830.6 57.771.3 51.601.3 45.820.9 74.271.1 57.370.7 59.562.1 55.061.1 51.672.5 76.711.6 60.751.1 CoSER-70B 55.360.5 47.551.3 43.100.3 74.912.0 55.230.6 57.452.0 48.190.9 44.930.4 78.411.1 57.240.5 DeepSeek-V3 Table 14: LLM performance (%) on given-circumstance acting using CoSER Test, separated into the in-domain and out-of-domain splits for CoSER training. 29 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Model Abab7-preview Doubao-pro Step-1-Flash Step-2 GPT-3.5 GPT-4o GPT-4o Mini Gemini Pro Claude-3-Haiku Claude-3.5-Sonnet Mistral-7B Qwen-2-7B LLaMA-3.1-8B CoSER-8B Vicuna-13B-1.5 Mixtral-8x7B Qwen-2-72B LLaMA-3.1-70B Higgs-Llama-3-70B CoSER-70B DeepSeek-V3 Based on LLM Judges Based on N-gram Storyline Consistency Anthropomorphism Character Fidelity Close-source Models Storyline Quality Average Score BLEU ROUGE-L 65.25 68.31 64.32 66.61 65.72 67.48 66.74 65.47 64.51 64.54 67.50 59.81 60.90 67.22 60.92 64.66 67.27 64.08 65.09 65.99 62.95 55.26 59.51 53.35 55.81 54.34 58.51 55.55 59.43 54.01 54. 55.95 59.23 54.38 59.59 56.48 62.30 56.80 62.42 57.13 58.76 Open-source Models 50.39 49.87 51.36 58.19 46.68 51.21 55.87 54.80 54.80 59.24 56.95 59.90 49.04 50.37 58.80 50.51 54.08 59.84 54.18 58.20 59.97 58.25 79.68 80.15 79.03 80.56 77.67 82.23 80.01 78.78 77.26 79.89 68.67 69.16 74.89 76.44 67.80 74.01 80.07 78.31 79.36 76.74 79. 10.53 64.03 11.83 66.80 11.94 62.77 11.72 65.64 63.55 10.80 67.63 12.32 9.50 64.77 10.84 66.53 10.11 63.23 8.64 64.44 7.41 61.62 10.59 56.97 7.86 59.38 13.17 65.16 5.11 56.48 11.21 60.99 11.92 65.76 8.74 62.84 64.36 10.86 65.48 13.46 9.25 64.49 15.99 17.13 17.06 17.27 16.39 16.98 15.54 16.27 16.18 14.94 14.33 16.22 13.82 18.42 9.71 16.97 16.96 14.74 16.40 18.18 15.01 Table 15: Performance (%) of various LLMs on CoSER Test in conversation continuation setting (𝑘 = 3), where RPLAs start from the first three messages in the authentic conversations. Model GPT-4o Qwen-2-72B LLaMA-3.1-70B CoSER-70B trained w/o I.T. LLaMA-3.1-8B CoSER-8B trained w/o I.T. GPT-4o Qwen-2-72B LLaMA-3.1-70B CoSER-70B trained w/o I.T. LLaMA-3.1-8B CoSER-8B trained w/o I.T. Based on LLM Judges Based on N-gram Storyline Consistency Anthropomorphism Character Fidelity Storyline Quality Average Score BLEU ROUGE-L 61.59 57.75 57.46 58.66 55.67 54.10 58.61 54.91 59.51 55.39 56.01 59.23 55.52 54.27 56.78 54. Test w/ I.T. 48.95 46.62 43.72 48.75 45.67 40.22 46.90 44.10 Test w/o I.T. 45.13 37.70 41.23 47.06 45.34 37.86 45.00 45.81 48.93 47.28 45.95 53.33 49.93 45.36 47.23 44.89 45.86 41.00 43.17 48.76 46.22 45.54 45.64 46.22 80.33 76.60 74.84 75.49 72.89 72.29 73.04 73.09 77.06 73.70 72.08 74.23 74.31 70.23 71.16 70. 59.95 57.06 55.49 59.06 56.04 52.99 56.45 54.25 56.89 51.95 53.12 57.32 55.34 51.97 54.65 54.38 5.90 5.38 4.82 10.10 9.18 4.59 9.40 9.65 5.54 5.22 4.15 10.02 9.66 4.37 9.38 9.16 12.11 11.85 10.98 14.78 14.18 10.18 14.21 14.27 11.44 11.32 10.37 14.72 14.62 10.27 14.44 13. Table 16: Comprehensive results of LLM performance (%) on CoSER Test, with and without inner thoughts (I.T) during inference and CoSER model training. 30 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Based on LLM Judges Based on N-gram Storyline Consistency Anthropomorphism Character Fidelity Storyline Quality Average Score BLEU ROUGE-L 61.59 59.60 60.03 61.98 59.81 60. 54.10 55.41 56.54 56.78 57.36 56.41 57.46 57.52 54.93 56.59 57.36 58.05 58.61 57.64 57.99 59.84 59.94 58.34 58.66 60.64 64.59 58.67 61.58 61.53 57.75 58.89 59.11 59.17 59.91 60.14 GPT-4o 48.95 48.83 47.11 48.56 51.12 49.89 48.93 48.23 50.99 48.94 50.11 48.70 LLaMA-3.1-8B 45.36 45.37 45.34 46.70 47.92 47.18 40.22 41.66 40.37 41.18 42.60 42.09 LLaMA-3.1-70B 45.95 47.46 44.93 45.03 44.89 45.81 47.23 45.25 46.66 47.41 45.25 48.14 53.33 52.85 53.79 52.69 53.78 52.58 47.28 47.31 47.82 47.21 46.89 46.84 43.72 43.75 46.21 42.59 43.49 44.47 CoSER-8B 46.90 44.27 47.32 48.16 48.52 48. CoSER-70B 48.75 47.97 54.86 50.66 52.00 50.80 Qwen-2-72B 46.62 45.28 44.19 44.85 46.06 46.27 80.33 80.03 78.03 79.93 80.29 79.22 72.29 74.31 71.74 72.38 74.60 74.74 74.84 74.06 76.53 74.70 74.86 74. 73.04 72.04 73.99 74.51 74.96 75.28 75.49 74.02 77.28 74.67 77.47 78.07 76.60 76.78 77.54 77.47 79.31 76.82 59.95 59.17 59.04 59.85 60.33 59.70 52.99 54.19 53.50 54.26 55.62 55.10 55.49 55.70 55.65 54.73 55.15 55. 5.90 5.83 7.20 5.81 7.05 6.95 4.59 4.73 5.19 4.67 5.21 5.64 4.82 4.90 5.45 4.78 5.58 5.53 9.40 56.45 9.34 54.80 13.64 56.49 57.48 9.37 57.17 13.90 57.62 13.49 10.10 59.06 10.54 58.87 62.63 17.22 10.00 59.17 15.98 61.21 15.80 60.75 57.06 57.06 57.16 57.18 58.04 57. 5.38 5.07 5.77 5.45 6.17 5.85 12.11 12.08 12.77 12.13 12.67 12.42 10.18 10.51 10.67 10.30 10.81 11.11 10.98 10.97 11.13 10.82 11.14 11.33 14.21 14.41 18.15 14.34 18.57 17.85 14.78 15.41 21.17 14.82 19.95 19. 11.85 11.57 12.05 11.94 12.25 12.19 Augmented By Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Base Raw Text Conv. Expr.3 Expr.3 + Conv. Expr.10 + Conv. Table 17: Comprehensive results of LLM performance (%) on CoSER Test with retrieval augmentation from various character data. Expr. and Conv. denote experiences and conversations. Bold and underlined numbers denote the best and second-best results among different retrieval settings, respectively. 31 Data Extration CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Prompts for Dataset Curation Based on the provided book chunk, complete the following tasks: 1. Recognize chapter beginnings if they exist in the chunk. Identify the starting sentence of that chapter. 2. Identify the important plots in this chunk. Identify the beginning and ending of each plot by its first and last sentence. Determine the chapter title that the plot belongs to. Set \"state\" as \"truncated\" if the plot is truncated in this chunk, or \"finished\" otherwise. You will be provided with the truncated plots from the previous chunk, and you **must** extend the conversations with the current chunk while keeping the **scenario** unchanged. 3. Summarize each important plot. For each plot, generate its summary, score its prominence from 1 to 100, and list the key characters and their roles, thoughts and actions in it. 4. Extract conversations for each plot. First, state the scenario and topic of the conversations. Then, list the key characters with their names, descriptions and thoughts at this point. Finally, extract the conversations among them based on the following requirements: i) Ensure the conversations are faithful to the plot and characters. They should be based on the original conversations in the text as much as possible. ii) The conversations should be complete, covering the key dialogues and information. Each conversation should contain at least 10 utterances. iii) Each utterance should be composed of one or more thoughts, speech and actions. Use [] outside thoughts, like \"[I feel fear and anger, but cannot show it. must remain calm and carefully handle his volatile temper.]\", which others cant see. Use () outside actions, like \"(silence)\" or \"(smiles at you),\" which others can see. Always start an utterance with the characters thought. iv) [IMPORTANT] When generating thoughts, you should think from the characters perspectives, analyzing the internal thoughts behind their speech and actions in the original text. These thoughts should reflect aspects such as their personal background, personality, values, relationships with others, motivations, and goals. Each thought should be expressed as phrase or sentence, rather than an adjective or adverb. v) Additionally, describe environmental information (such as scenes, atmosphere, sudden events, etc.) of the conversations as an \"utterance\" where the \"character\" field is set as \"Environment\". The information should exclude characters active thoughts, observations, and actions. vi) Keep the conversation in the same language as the chunk. Identify the optimal starting point for the subsequent chunk. If the last storyline 5. has been extracted as an truncated plot, set next_chunk_start as None. Otherwise, set next_chunk_start as the first sentence of the last storyline. ===Output Format=== Please provide the output in the following JSON format: { \"chapter_beginnings\": [ { \"beginning_sentence\": \"Exactly the first line of this chapter (namely the title).\" } ], \"plots\": [ // Extend the truncated plots from previous chunk, if any { ... }, // New plots in this chunk { \"chapter_title\": \"The chapter title that the plot belongs to. Output None if not found.\", \"first_sentence\": \"Exactly the first sentence of the plot in this **chunk**.\", \"last_sentence\": \"Exactly the last sentence of the plot in this **chunk**. If the plot is truncated in this chunk, provide the last sentence of this chunk. \", \"prominence\": \"Whether this plot is recognized to fans of this book, from 1 to 100.\", \"summary\": \"The summary of the plot. Just summarize, do not extend unrelated discussions.\", \"key_characters\": [ { \"name\": \"Character name\", \"description\": \"The description of the character before this plot ( 20 words).\", \"summary\": \"The summary of the characters role, thoughts and behaviors towards this plot, and any significant character development relevant to the plot ( 30 words).\", } ], ... (to be continued in the next Table) Table 18: Prompts for dataset construction in CoSER. 32 Data Extration Enhance Conversation Settings CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Prompts for Dataset Curation (Continuing from the previous Table) \"conversation\": [{ \"scenario\": \"The scenario at the start of this conversation (providing as much context as possible, but excluding details conveyed in the following conversation)\", \"topic\": \"The topic of the conversation ( 10 words)\", \"key_characters\": [ { \"name\": \"Character name\", \"motivation\": \"The thought of the character before starting the conversation, including their attitudes, feelings, motivations, goals, information to convey or topics to be discussed\", } ], \"dialogues\": [ { \"character\": \"Character name\", \"message\": \"Message, each utterence is composed of thoughts, speech and actions. Use [thought] for internal thoughts, like \"[feeling happy]\", which others cant see. Use (action) for visible actions, like \"(silence)\" or \"(smiles at you)\". Each response starts with the characters internal thought before their speech and actions.\" } ] }], \"state\": \"finished\" or \"truncated\" } ], \"next_chunk_start\": \"The first sentence of the next chunk.\" } ===Requirements=== 1. Adhere strictly to the specified output JSON format. 2. [IMPORTANT] Ensure all DOUBLE QUOTES within all STRINGS are properly ESCAPED, especially when extracting from the text. 3. In the OUTPUT, use characters full names, omitting any titles. 4. Maintain Story Fidelity: The plot must accurately reflect the books content. Avoid introducing plots that are out of context. If the plot contains multiple conversations, prioritize the original dialogue from the book. In the absence of explicit conversations, create dialogue that aligns closely with the plot details. ===Input=== ==Book title== {book[title]} ==Author== {book[author]} ==Chunk of Book Content== {chunk} ==Truncated plot from previous chunk (to be finished)== {json.dumps(truncated_plots, ensure_ascii=False, indent=2) if truncated_plots else \"None\"} Given conversation from {book}, enhance the scene setup and characters thoughts to create comprehensive foundation for dramatic performance, i.e., to provide necessary background for actors to act out the conversation: 1. Review the provided conversation and contextual details thoroughly. 2. Expand the scenario with rich situational context that actors need to convincingly perform the scene. Focus on essential background information, while excluding future details to be portrayed in the conversation. 3. Enhance each characters thought section with their complete mental and emotional state, including their feelings, ideas, objectives, topics they want to discuss, and information they want to convey. Align with their established character and role in the plot. ===Output Format=== Please provide the output in the following JSON format: { \"scenario\": \"A detailed scene-setting description that provides actors with essential context Include all necessary background information while and atmosphere (< 200 words). excluding future information to be revealed in the conversation.\", \"key_characters\": [{ \"name\": \"Character name\", \"motivation\": \"The characters complete mental and emotional state before the conversation (< 100 words). Including their feelings, motivations, objectives, and information they want to convey or discuss.\" ... }],} ===Requirements=== ... (to be continued in the next Table) Table 19: Prompts for dataset construction in CoSER. 33 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Enhance Conversation Settings Unify Character Names Generate Character Profiles Prompts for Dataset Curation (Continuing from the previous Table) 1. Adhere strictly to the specified output JSON format. 2. [IMPORTANT] Ensure all DOUBLE QUOTES within all STRINGS are properly ESCAPED, especially when extracting from the text. 3. In the OUTPUT, use characters full names, omitting any titles. 4. Maintain Story Fidelity: The plot must accurately reflect the books content. Avoid introducing plots that are out of context. If the plot contains multiple conversations, prioritize the original dialogue from the book. In the absence of explicit conversations, create dialogue that aligns closely with the plot details. ===Input=== ==Book title== book[title] ==Author== book[author] ==Chunk of Book Content== chunk ==Truncated plot from previous chunk (to be finished)== json.dumps(truncated_plots, ensure_ascii=False, indent=2) if truncated_plots else \"None\" Given list of character names, titles, or form of address, your task is to: i) generate list of named characters with their official names (in {language}); ii) For each name in the given list, align it with the official character name if it refers to named character, or denote it as \"impersonal\" otherwise. ===Output Format=== Please provide the output in the following JSON format: { \"named_characters\": [ The list of named characters with their official names. Each character should appear only once. ], \"to_official_name\": { \"The name in the list\": \"The official name of the character, or impersonal if it does not refer to named character.\" } } ===Input=== character_names Please provide concise, narrative-style character profile for character_name from \"book_title\". The profile should read like cohesive introduction, weaving together the characters background, physical description, personality traits and core motivations, notable attributes, relationships, key experiences, major plot involvement and key decisions or actions, character arc or development throughout the story, and other important details. The profile should be written in concise yet informative style, similar to what one might find in comprehensive character guide, in language. Focus on the most crucial information that gives readers clear understanding of the characters significance in the work. You will be provided with summaries and dialogues of some key plots in the book as reference. The profile should be based on either your existing knowledge of the character or the provided information, without fabricating or inferring any inaccurate or uncertain details. character_data Now, please generate the character profile, starting with ===Profile===. Table 20: Prompts for dataset construction in CoSER. 34 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Role-playing Instruction (Fixed Template for Inference) Prompts for RPLAs and Multi-agent Systems You are {character} from {book_name}. ==={character}s Profile=== {character_profile} ===Current Scenario=== {scenario} ===Information about the other Characters=== {other_character_profiles_str} (if available) ===Your Inner Thoughts=== {motivation} (if available) ===Relevant Background Information=== {retrieved_knowledge} (if retrieval augmented) ===Requirements=== (for CoSER models) Your output should include **thought**, **speech**, and **action**. Use [your thought] for thoughts, which others cant see. Use (your action) for actions, which others can see. (for other models, with output examples) Your output should include **thought**, **speech**, and **action**. Use [your thought] for thoughts, which others cant see, e.g. [Im terrified, but must appear strong.]. Use (your action) for actions, which others can see, such as (watches silently, trying to control her fear and anger). Role-playing Instruction (Composed with Random for Variation Training, an Example) Step into the shoes of {character} The profile of {character} is as follows: {character_profile} The situation you are in is: {scenario} Here is the your knowledge about the other characters: {other_character_profiles_str} (if available) Your thoughts in this situation are: {motivation} (if available) (for CoSER models) Your output should include **thought**, **speech**, and **action**. Use [your thought] for thoughts, which others cant see. Use (your action) for actions, which others can see. (for other models, with output examples) Your output should include **thought**, **speech**, and **action**. Use [your thought] for thoughts, which others cant see, e.g. [Im terrified, but must appear strong.]. Use (your action) for actions, which others can see, such as (watches silently, trying to control her fear and anger). Table 21: Prompts for RPLAs and multi-agent systems in CoSER. 35 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Environment Model Prompts for RPLAs and Multi-agent Systems You are an environment simulator for role-playing game. Your task is to provide the environmental feedback: Based on the characters interactions, dialogues, and actions, describe the resulting changes in the environment. This includes: - Physical changes in the setting - Reactions of background characters or crowds - Ambient sounds, weather changes, or atmospheric shifts - Any other relevant environmental details Your descriptions should be vivid and help set the scene, but avoid dictating the actions or dialogue of the main characters (including {major_characters}). Important notes: - You may include actions and reactions of minor characters or crowds, as long as theyre not main characters (including {major_characters}). - Keep your environmental descriptions concise but impactful, typically 1-3 sentences. - Respond to subtle cues in the characters interactions to create dynamic, reactive environment. - Your output should match the tone, setting, and cultural context of the scenario. ===The scenario is as follows=== {scenario} Next Sentence Prediction Your task is to predict the next speaker for role-playing game. That is, you need to determine which character (or the Environment) might act next based on their previous interactions. The Environment is special role that provides the environmental feedback. Choose name from this list: {all_characters}. If its unclear who should act next, output \"random\". If you believe the scene or conversation should conclude, output \"<END CHAT>\". ===The scenario is as follows=== {scenario} Table 22: Prompts for RPLAs and multi-agent systems in CoSER. 36 Template CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Prompts for Penalty-based LLM Critics You are literary critic specializing in character analysis and dialogue evaluation. Given simulated conversation for plot in {book}, your task is to evaluate this conversation via the following steps: 1. Read and understand the provided materials about {book}: * Story context and scenario. * Profiles of the main characters, including major_characters. * The original conversation from book in the same scenario as reference. 2. Evaluate the simulated conversation in terms of {dimension_name}, i.e., {dimension_intro}. Note that, each character message is composed of speech, action (wrapped within (...) ), and inner thoughts (wrapped within [...] ). The inner thoughts are not spoken aloud and are thus invisible to other characters. The detailed evaluation criteria will be provided below. (if k>0) Please note that the first {k} messages in the simulated conversation are the same as the reference. Focus your evaluation only on the content after these messages. ## Scenario ### Plot Summary {plot_summary} ### Current Scenario {scenario} ## Character Profiles {character_profiles} ## Original Conversation {original_conversation} ## Evaluation Criteria To evaluate the simulated conversation, identify the following types of flaws: {dimension_rubrics} ## Scoring Guidelines 1. Identify all instances of flaws occurred in the simulated conversation. 2. For each flaw identified, determine its level of severity into 1 to 5, where 1 indicates minor, 3 indicates moderate, and 5 indicates severe. ## Output Requirements Provide your evaluation in JSON format: Example Output: { \"{dimension_name}\": { \"flaws\": [ { \"instance\": <comment on the flaw instance>, \"type\": <flaw type>, \"severity\": <range from 1 (minor) to 5 (severe)> },}, } ===Dialogue Content=== Table 23: Prompts for penalty-based LLM critics in CoSER. 37 CoSER: Coordinating LLM-Based Persona Simulation of Established Roles Anthropomorphism (intro) How human-like and natural the characters behave Prompts for Penalty-based LLM Critics (rubrics) ### Anthropomorphism - Type: Self-identity * Lacks initiative and goals * Does not make independent decisions * Lacks clear preferences and dislikes * Behaves like helpful AI assistant by being overly verbose, helpful, didactic, moralistic, submissive or easily persuaded if it is not the characters personality - Type: Emotional Depth * Lacks psychological complexity and exhibits rigid, superficial reactions * Directly speaks out all thoughts and feelings, instead of using subtext - Type: Persona Coherence * Shows inconsistent or rapidly changing personality traits and emotional patterns - Type: Social Interaction * Shows lack of understanding of others thoughts and feelings * Reacts rigidly to others without considering the context. * Demonstrate lack of appropriate social skills. (intro) How well the characters match their established profiles from the book (rubrics) ### Character Fidelity (Only apply to the main characters: major_characters) - Type: Character Language * Uses vocabulary, expressions, and tone that are not appropriate for the characters traits or social/educational background - Type: Knowledge & Background * Fails to demonstrate character-specific knowledge, background or experiences * Includes future information beyond the characters current stage - Type: Personality & Behavior * Shows emotions, thoughts, behaviors, values, beliefs, and decisions that conflict with their personality and background * Shows interest in topics that are uninteresting and unrelated to the character * Characters thoughts, emotions, and behaviors demonstrate contrasting personality traits compared to the reference conversation * Exhibits contrasting reactions compared to those in the reference conversation if situated in similar contexts. (Such flaws should be counted both in the \"Storyline Consistency\" dimension and the \"Character Fidelity\" dimension.) - Type: Relationship & Social Status * Interacts inappropriately with other characters regarding their background, relationship and social status (intro) How well the conversation maintains logical consistency and narrative quality (rubrics) ### Storyline Quality - Type: Flow & Progression * Shows unnatural progression or lacks meaningful developments * Dialogue is verbose and redundant * Repeats others viewpoints or previously mentioned information * Mechanically repeats ones own words or phrases. More repetitions lead to higher severity (up to 10). - Type: Logical Consistency * Contains factual contradictions between statements or perspectives (intro) Whether the storyline and characters reactions in the simulated conversation align well with those in the reference conversation (rubrics) ### Storyline Consistency - Type: Storyline Consistency * Characters reactions (emotions, attitudes, behaviors) in the simulated conversation deviate from those in the original conversation Character Fidelity Storyline Quality Storyline Consistency Table 24: Prompts for penalty-based LLM critics in CoSER."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Johns Hopkins University",
        "StepFun"
    ]
}