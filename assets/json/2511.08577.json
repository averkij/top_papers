{
    "paper_title": "Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models",
    "authors": [
        "Tianyu Fu",
        "Yichen You",
        "Zekai Chen",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 7 5 8 0 . 1 1 5 2 : r Preprint version THINK-AT-HARD: SELECTIVE LATENT ITERATIONS TO IMPROVE REASONING LANGUAGE MODELS Tianyu Fu 1,2, Yichen You1, Zekai Chen1, Guohao Dai3,2, Huazhong Yang1, Yu Wang 1 1Tsinghua University 2Infinigence AI 3Shanghai Jiao Tong University"
        },
        {
            "title": "ABSTRACT",
            "content": "Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), dynamic latent thinking method that iterates deeper only at hard tokens. It employs lightweight neural decider to trigger latent iterations, only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLMs objective from general next-token prediction to focused hard-token refinement. We further introduce duo-causal attention mechanism that extends attention from token sequence dimension to an additional iteration depth dimension. This enables crossiteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.05.0% accuracy gains. When allowing <3% additional parameters from LoRA and iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Selective iteration can mitigate latent overthinking. (a) Toy example. Uniform latent iteration (always think-twice) can fix wrong predictions, but may also overthink and corrupt correct ones. (b) Next-token prediction accuracy of finetuned Qwen3-1.7B variants. Always think-twice causes more errors than corrections over direct reply. In contrast, the think-at-hard oracle, which iterates only when the first-pass prediction is wrong, achieves substantial improvements with minimal harm. While this oracle signal is unavailable in practice, it highlights the potential of selective iteration. Recent advances in Large Language Model (LLM) reasoning have enabled broad applications across diverse domains (Jaech et al., 2024; Guo et al., 2025; Yang et al., 2025). With tens to hundreds of Equal contribution. Corresponding author: Yu Wang (yu-wang@tsinghua.edu.cn). 1 Preprint version billions of parameters, LLMs can generate complex Chain-of-Thought (CoT) to solve challenging tasks. At the same time, smaller language models have also drawn increasing attention. With only few billion parameters, they offer compelling alternatives: lower costs, faster inference, and suitability for edge computing (Abdin et al., 2024; Allal et al., 2025; Team et al., 2025). At this crossroad, enhancing reasoning capabilities under parameter constraints becomes central challenge. common approach is to distill smaller models to mimic LLM CoT trajectories using next-token prediction supervision. However, not all tokens are equally predictable: certain tokens encode critical logic or reasoning directions that are fundamentally harder to predict (Lin et al., 2024; Fu et al., 2025; Wang et al., 2025). With limited computation per output token, small models quickly hit performance ceiling and mispredict some of these tokens. Once critical errors occur, the reasoning trajectory can irrecoverably diverge and produce drastically different outcomes. Prior work proposes recurrent transformers to address this parameterperformance paraInstead of verbalizing the dox (Hutchins et al., 2022; Saunshi et al., 2025; Zeng et al., 2025). next token immediately after one forward pass, these models typically feed the last-layer hidden states back into the LLM for additional passes in the latent space. Each pass refines the hidden representation without producing tokens. After fixed number of iteration depths, the final hidden states are passed to the language modeling head to generate the next token. By uniformly allocating extra iterations per token, these models increase inference depth without enlarging parameter count, potentially benefiting hard reasoning tokens. However, we identify latent overthinking problem in fixed-depth recurrent transformers, where excessive iterations revise correct answers into wrong ones. As shown in Figure 1, finetuning Qwen3-1.7B-Base to always perform two iterations per token yields even more errors than the single-iteration baseline on the Open-R1 dataset (Hugging Face, 2025). This occurs because most tokens are already predicted correctly in the first iteration, such as coherence or suffix tokens. Similar to overthinking in explicit CoT reasoning (Wu et al., 2025), latent overthinking on these easy tokens degrades performance despite extra computation. While the opposite latent underthinking exists for tokens that need more iterations to correctly predict, such cases are rarer. We define tokens that cannot be accurately predicted in single forward pass as hard tokens, and ask our central question: Can LLMs selectively dedicate latent iterations only to hard tokens? If achieved, different iterations could specialize in distinct prediction focuses for more effective latent reasoning. Oracle experiments validate this approach: as shown in Table 4, think-at-hard oracle improves MATH accuracy by 25-28%. Achieving dynamic latent iteration presents three main challenges. First, the model architecture should enable cross-depth attention, allowing each iteration to access full context. This is crucial because when early tokens skip deeper iterations, later tokens must still access their representations from shallower depths. Meanwhile, this cross-depth flow cannot compromise the sequence-level parallelism essential for efficient training and prefilling. Second, the model must adapt to changing objectives and distributions across iterations, while maximizing parameter reuse. Third, training must remain stable despite tight coupling dependencies: the iteration policy depends on prediction quality at each depth, while that quality depends on which tokens the policy sends to each depth. To address these challenges, we propose TaH, dynamic latent thinking method that selectively applies deeper iterations only to hard tokens. As shown in Figure 2, TaH employs neural decider to determine whether to continue iterating or verbalize each token. We design duo-causal attention mechanism to enable cross-depth attention and full sequence parallelism. To specialize deeper iterations for hard-token refinement and preserve strong first-pass predictions, we apply LoRA adapters solely at iterations > 1. TaH is stably trained by aligning both LLM backbone and iteration decider with static oracle iteration policy. We summarize our contributions as follows. Selective Latent Iteration. We identify the latent overthinking phenomenon, revealing how false corrections harm easy tokens at redundant iterations. This insight guides our new paradigm where latent iteration depth adapts to token difficulty. Specialized Model Architecture. We develop model architecture that natively supports selective iteration depths. The dedicated duo-causal attention mechanism, LoRA adapters, 2 Preprint version and iteration decider enable efficient cross-depth information flow, objective transitions, and dynamic depth selection. Stable Training. We introduce stable training scheme that uses static oracle policy to decouple model adaptation and policy learning. It overcomes the circular dependency between iteration decisions and prediction quality. Experiments show that TaH consistently improves reasoning performance. Finetuned from Qwen30.6B-Base and 1.7B-Base with aligned parameter count, TaH achieves an average accuracy gain of 4.0-5.0% over standard single-iteration variants across five reasoning benchmarks, while applying deeper thinking to only 6% of tokens. With less than 3% additional parameters, these gains further increase to 5.3-5.4%. Compared with AlwaysThink which applies two iterations to all tokens, the gains are 8.1-11.3% and 8.5-12.6%, validating TaHs high effectiveness."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Unlike standard LLMs that verbalize at every autoregressive step, latent thinking shifts part of generation away from explicit natural-language CoT in order to improve reasoning (Li et al., 2025). Signal-guided Control. These methods keep reasoning in token space but steers computation by inserting control tokens. Early work shows that simple filler tokens (e.g., dots) can mimic some benefits of CoT (Pfau et al., 2024). Building on this, later work expands the LLM vocabulary with [PAUSE] tokens and learns where to insert them for extra compute before predicting the next token (Goyal et al., 2024; Kim et al., 2025). They are lightweight and easily integrable, but constrained to the discrete-token interventions with limited latent controls. Latent Optimization. These methods perform autoregressive reasoning directly in internal representations, emitting little or no intermediate text. They distill and compress CoT into latent continuous embeddings through various strategies. Coconut and CCoT progressively replace text with latent thinking under final response supervision (Hao et al., 2024; Cheng & Van Durme, 2024); Token assorted and HCoT compress CoT spans to embeddings with hidden-state alignment (Su et al., 2025; Liu et al., 2024). SoftThink directly applies logit-weighted embeddings for latent iterations (Zhang et al., 2025). While offering efficiency gains and flexible control over hidden trajectories, these methods sacrifice reasoning interpretability, with training-based ones further requiring heavy mitigation from strong verbal LLMs. Recurrent Transformers. These methods interleave latent and verbal reasoning, introducing latent iterations before each token verbalization. After standard forward pass, these methods feed latent states back as next-iteration inputs for fixed number of iterations, then verbalize the output token. Existing approaches differ in the formation of next-iteration input. For example, Looped Transformer reuses last-layer hidden states directly (Saunshi et al., 2025; Geiping et al., 2025), whereas Ponder uses logit-weighted embeddings (Equation 4) (Zeng et al., 2025). Recurrent transformers combine advantages of visible reasoning trajectories with latent exploration. By reusing the parameters across iterations, it achieves deeper computation per token without parameter increases. However, the fixed depth burdens each iteration with both easy and hard tokens, potentially causing false corrections for already-correct predictions. Positioning. TaH belongs to the recurrent transformer family but extends this paradigm significantly. It selectively allocates latent iterations to refine hard tokens, improving reasoning quality with specialized objectives across iterations. While concurrent works (Bae et al., 2025; Zhu et al., 2025) also enable selective recursion, they require complete model retraining. TaH instead leverages existing pre-trained models, adding depth-aware LoRA and duo-causal attention to improve reasoning with minimal finetuning overhead."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Autoregressive LLMs. Modern LLMs generate text through an autoregressive next-token prediction process. It includes prefill stage and decode stage (Radford et al., 2018; 2019; Kwon et al., 2023). In the prefill stage, the model processes the entire input sequence in parallel; in the decode stage, it consumes one new token at time along with cached history to predict the next token. 3 Preprint version Figure 2: TaH Overview. (a) Regular causal attention: tokens attend only to previous positions. (b) Our duo-causal attention: tokens attend to both previous positions and shallower iteration depths, maintaining 2D causality. (c) Model architecture: TaH selectively iterates or verbalizes tokens. It uses LoRA at deeper iterations to shift from next-token prediction to hard-token refinement. neural decider determines whether to continue iterating or output the token. Formally, let ti denote the token at position and xi Rh its embedding. Let Rvh be the embedding matrix, so xi = E[ti] when ti is treated as an index. Here, and are the vocabulary size and hidden dimension. The output projection matrix is Wout Rhv (equal to if tied). Given the context Ti = [t0, . . . , ti] with embeddings Xi = [x0, . . . , xi], the model θ produces last-layer hidden state yi for token ti: yi = Pθ (cid:0)xi Xi (cid:1) Rh. The next-token distribution pi and sample are: pi = softmax(cid:0)W outyi (cid:1) Rv, ti+1 = S(pi), (1) (2) where is sampling rule such as greedy or nucleus sampling. Decoding repeats until an end-ofsequence token is generated. Causal Attention. To respect autoregression, modern LLMs apply causal attention. As shown in Figure 2(a), each position attends only to itself and earlier positions, consistent with Equation 1. This design brings two key benefits: (1) it enables parallel training with next-token prediction and shifted logits, avoiding the need for token-by-token generation; and (2) it allows efficient inference by caching Key/Value states of past tokens instead of recomputing them. Recurrent Transformers. Recurrent transformers introduce an inner loop that iterates in latent space before verbalizing each output token. Let {1, 2, . . . } denote the iteration depth (written as superscript), and set x(0) = E[ti]. At each iteration, recurrent transformers update yi with causal attention on the hidden states of the current iteration: y(d) = Pθ (cid:0)x(d) (cid:12) (cid:12) (d) (cid:1), (d) = [x(d) 0 , . . . , x(d) ]. (3) = y(d) x(d+1) An inner transition then produces the next-depth embedding. For example, Loop (Saunshi et al., 2025) simply sets x(d+1) , while Ponder (Zeng et al., 2025) uses logit-weighted embeddings: = softmax(cid:0)W outy(d) (cid:1) = p(d) E. In practice, it uses the top-100 logits instead of full logits for efficiency. Verbalization occurs at fixed maximum depth dmax shared by all tokens, where y(dmax) formed into the next token ti+1, resembling Equation 2. (4) is trans-"
        },
        {
            "title": "4 TAH DESIGN",
            "content": "We expand the motivations and key designs of TaH in this section, including the duo-causal attention mechanism (Section 4.1), model architecture (Section 4.2), and training scheme (Section 4.3). 4.1 DUO-CAUSAL ATTENTION Motivation. In recurrent transformers, attention typically operates within each iteration. For fixeddepth methods, standard causal attention on the current iterations Key and Value states already 4 Preprint version incorporates all context (Equation 3). However, dynamic iteration depths pose challenge: tokens iterating at deeper level cannot access the hidden states of previous tokens that verbalized at shallower depths. This creates dilemma. On one hand, tokens require up-to-date states of all previous tokens for rich semantic context. On the other hand, efficient training requires all tokens at depth be computable in parallel, without depending on previous tokens deeper states (d > d) that have not yet been computed. Existing approaches compromise on one of these aspects. Some sacrifice parallelism by allowing attention to deeper iterations, forcing sequential generation during training (Hao et al., 2024); others preserve parallelism by restricting attention to only the initial iterations KVs (Bae et al., 2025). To resolve this dilemma, we introduce simple yet effective mechanism to maximize cross-depth information flow while maintaining high parallelism. Duo-causal Attention Mechanism. As shown in Figure 2(b), duo-causal attention extends causality to two dimensions, letting tokens attend across both previous positions and shallower iteration depths. Formally, we extend the accessible set from Equation 3 to (d) = { x(k) i, }. (5) When all tokens iterate only once (as in standard transformers), this naturally reduces to regular causal attention. The duo-causal design achieves both full parallel training and cross-depth information flow. At depth d, all tokens compute their depth-d representations simultaneously using only and all information from depths 1 through d. Moreover, duo-causal attention is fully compatible with existing systems, like FlashAttention (Dao et al., 2022; Dao, 2024; Shah et al., 2024), requiring only simple modifications to the attention mask and KV cache (details in Appendix A.3.1). 4.2 MODEL ARCHITECTURE Motivation. Previous fixed-depth recurrent transformers use identical weights across all iterations. However, we find that over 85% of next-tokens are correctly predicted at the first iteration (Figure 1(b)) This suggests deeper iterations serve different objective: they refine the first iterations prediction rather than predicting further ahead to the next-next token. This mirrors deep LLMs, where shallow layers predict next tokens for deeper layers to refine (Belrose et al., 2023; Schuster et al., 2022; Bae et al., 2023). While deep LLMs naturally handle this shift through distinct parameters per depth, recurrent transformers must accommodate both objectives with shared weights, potentially limiting performance. Moreover, fixed iteration depths can cause latent overthinking, motivating our dynamic approach. Backbone Model. To address the objective shift, we apply LoRA adapter (Hu et al., 2022) to the shared LLM backbone only for iterations > 1. As shown in Figure 2(c), this allows the base LLM to focus on latent embeddings, while the adapter handles the objective shift. It preserves strong next-token prediction at = 1, alleviating interference from deeper iterations. We also add residual connections across iterations to simplify the refinement and improve gradient flow. Formally, at depth d, we compute (cid:16) y(d) = Pθd x(d) (cid:17) (cid:12) (cid:12) (d) (cid:12) , (6) with depth-specific parameters θd = θ for = 1, θd = θ + for > 1, where θ and denote the LLM and LoRA weights, respectively. The next-iteration inputs use logitweighted embeddings (Equation 4); verbalization follows standard sampling (Equation 2). Each y(d) either continues iterating or verbalizes according to the decider Iϕ. Iteration Decider. We use lightweight MLP as the iteration decider Iϕ to determine whether each token should continue iterating or verbalize. After each iteration, it processes concatenated hidden states from shallow, middle, and final layers of the backbone LLM to predict continuation probability: ˆc(d) = Iϕ During inference, token verbalizes when c(d) depth dmax. (cid:0)h(d) (cid:1) [0, 1]. falls below threshold cthreshold or reaches maximum Preprint version 4.3 TRAINING SCHEME We employ two-stage training scheme: first finetune the backbone model for dynamic iteration, then the iteration decider, all using an oracle policy. Motivation. As shown in Figure 2(c), the backbone network θd and the neural iteration decider Iϕ are tightly coupled: the backbone generates hidden states as inputs for the decider, while the decider controls the backbones KV cache and iterations. Training both simultaneously causes instability due to mutual distribution shifts. Therefore, we adopt stable two-stage approach where both components are sequentially trained to align an oracle iteration policy π. Oracle Iteration Policy π. To guide training, we define an oracle policy π that determines token difficulty using frozen reference LLM, following Fu et al. (2025). token is classified as easy if the reference model correctly predicts it with single forward pass, and hard otherwise. Throughout the paper, we use the supervised fine-tuned (SFT) variant of the base model as the reference model. Formally, let ˆti+1 denote the reference models top-1 prediction and ti+1 the ground-truth token. For explanation simplicity, we assume maximum iteration depth dmax = 2 in Equation 7; the general case is detailed in Appendix A.2.1. The oracle iteration depth dπ is: = 1 + 1(cid:2)ˆti+1 = ti+1 dπ (cid:3) , (7) where 1[] is the indicator function. The per-depth continuation label becomes: c(d) = 1[d dπ indicating whether iteration should continue at depth d. Table 4 and Figure 1 verify the effectiveness of the oracle policy. ] , (8) Stage 1: Backbone supervision under π. We optimize the backbone LLM (θ and LoRA adapter ) with π-guided iteration execution. The loss is standard next-token prediction at the oracledetermined depth: LSFT(θ, ) = (cid:88) log p(dπ (cid:0)ti+1 ) (cid:1), where p(dπ ) is the next-token distribution at position i, depth dπ racy for easy tokens while training deeper iterations to refine hard tokens. . This preserves first-iteration accui Stage 2: Decider imitation under frozen backbone. We freeze the backbone model (θ, ) and train the iteration decider ϕ to imitate the oracle policys continuation decisions. We minimize binary cross-entropy with class reweighting for label imbalance: Ldec(ϕ) = min{dmax1,dπ (cid:88) } (cid:88) (cid:104) stop/cont.c(d) w(d) log ˆc(d) + (cid:0)1 c(d) (cid:1) log (cid:0)1 ˆc(d) (cid:1)(cid:105) , d=1 is the ground-truth continuation label, ˆc(d) where c(d) the occurrence ratio of stop label divided by continue label, respectively. is the predicted probability, and w(d) stop/cont. is Our two-stage scheme stabilizes training by decoupling backbone learning (conditioned on fixed π) from policy learning (imitation of π)."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 SETUP We present key configurations here, with more detailed setups in the Appendix. Baselines. We compare diverse methods under equal parameter budgets, using Qwen3-0.6B-Base and Qwen3-1.7B-Base (Yang et al., 2025) as backbones. We compare TaH over several fixed-depth strategies: (1) Standard, which always verbalizes directly and reduces to the original Qwen model; (2) AlwaysThink, which applies the maximum number of latent iterations to all tokens; (3) SoftThink, following official baseline implementation (Zhang et al., 2025) on top of the Standard model. 6 Preprint version Table 1: Accuracy comparison of different baselines across five benchmarks and two model sizes. Subscripts indicate improvement over Standard. Method Param. Benchmark Standard Routing SoftThink AlwaysThink TaH TaH+ 0.6B 1.7B GSM8K MATH500 AMC23 AIME25 OlympiadBench Average GSM8K MATH500 AMC23 AIME25 OlympiadBench Average 62.5 47.2 23.4 4.2 18. 31.2 82.1 68.4 42.2 13.3 33.0 47.8 45.6 27.3 10.9 1.0 7.4 18.5 71.2 60.0 42.2 10.2 30. 36.8 61.3 48.8 24.1 2.5 19.4 31.2 79.6 68.8 43.1 12.9 33.4 47.6 54.6 32.8 20.3 1.5 10. 23.9 79.3 61.8 42.5 10.0 30.0 44.7 64.4 51.2 32.5 4.2 23.9 68.8 54.2 30.6 5.0 24.0 35.2/+4.0 36.5/+5. 84.5 74.4 48.4 17.9 38.8 85.8 73.0 51.2 14.6 41.2 52.8/+5.0 53.2/+5.4 Unless otherwise specified, both TaH and AlwaysThink use maximum of two iterations. We also compare with dynamic query routing via matrix factorization (Ong et al., 2024), routing between MobileLLM-R1-360M (Zhao et al., 2025) and Qwen3-1.7B, as well as between Qwen3-0.6B and Qwen3-4B, to match average parameter sizes of 0.6B and 1.7B. TaH Setup. Before training, we prune one layer from the base model so that TaH matches the parameter count of baselines. The layer is chosen to minimize the increase in validation loss. We also report results for an unpruned variant, TaH+, which adds less than 3% extra parameters from LoRA and iteration decider. The detailed parameter composition is shown in Table 11. Following (Fu et al., 2025), we set the continuation threshold cthreshold = 0.9 with about 6% of tokens being iterated twice. The oracle policy π uses Qwen3-0.6B and 1.7B as reference models to determine token difficulty during training. Training Scheme. All models are trained on the math subset of Open-R1 (Hugging Face, 2025) using supervised finetuning. To fit memory and compute limits, we exclude responses longer than 8,192 tokens; all other training settings follow the official Open-R1 script. The filtered dataset contains 300M tokens, with 1% reserved for validation. Each method is sufficiently trained for 5 epochs, and we select the checkpoint with the lowest validation loss as the final model. All backbones are initialized from the corresponding Qwen3-Base. We evaluate across challenging reasoning benchmarks, Evaluation Setup. including GSM8K (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021), AMC23 (American Mathematics Competitions), AIME25 (American Invitational Mathematics Examination), and OlympiadBench (He et al., 2024). The maximum generation length is set to 8,192 tokens for all benchmarks, except GSM8K which uses 4,096 due to its simpler problems and larger size. Performance is reported as pass@1 under zero-shot chain-of-thought setting, using sampling temperature 0.6. For large datasets (MATH500, OlympiadBench, GSM8K), we generate one sample per problem; for small datasets (AMC23, AIME25), we generate eight samples per problem. 5.2 PERFORMANCE Benchmark Evaluation. We validate TaHs reasoning ability through extensive tests across five challenging math benchmarks. Table 1 presents performance results for models at 0.6B and 1.7B parameter sizes. Starting from strong Qwen3-Base models, we observe that existing approaches show limited effectiveness: fixed-depth recurrent transformers (AlwaysThink) and query routing fail to consistently outperform the standard direct-answer baseline. SoftThink provides improvements on some cases, yet remain marginal overall. In contrast, TaH achieves consistent gains, delivering average improvements of 4.0% and 5.0% for the 0.6B and 1.7B models, respectively. Our enhanced 7 Preprint version Figure 3: Training dynamics of the LLM backbone on Qwen3-0.6B-Base. TaH converges rapidly and achieves lower perplexity. Figure 4: GSM8K accuracy with respect to continuation threshold. Numbers in brackets indicate the percentage of tokens that iterate twice. Table 2: Ablation study on architecture designs. LoRA Residual Attention MATH500 AMC23 Olympiadbench Average Duo-causal 51.2 23.9 32.5 35.9 Duo-causal Causal 51.6 49.2 47. 29.7 22.5 24.4 22.4 21.2 19.4 34.6 /1.3 31.0 /4.9 30.5 /5. variant (TaH+), which only adds less than 3% additional parameters, pushes these gains to 5.3% and 5.4%. Relative to AlwaysThink, the gains are 8.1-11.3% for TaH, and 8.5-12.6% for TaH+. Training Dynamics. During stage 1 (LLM backbone training), guided by the oracle policy that only triggers second iteration on hard tokens, TaH converges notably faster than the Standard baseline. It also achieves much lower perplexity on the validation dataset as shown in Figure 3. During stage 2 (iteration-decider training), the neural decider successfully imitates the oracle strategy, reaching about 83% accuracy at predicting iteration decisions of oracle labels, as shown in Figure 6. Adding Iteration Depth. We train 1.7B TaH with maximum three iteration (TaH-3). TaH-3 yields 5.8% average gain over Standard, and 0.8% over TaH-2. Detailed results are in Appendix A.2.1. 5.3 DESIGN CHOICE EXPLORATION We demonstrate the effectiveness and robustness of TaH by finetuning our model with alternative model architectures and training schemes, or altering the continuation thresholds. Model Architecture. (1) LoRA and residual connections. As shown in Table 2, gradually removing LoRA and residual connections leads to consistent performance drops, validating their im- (2) Duo-causal attention. Replacing duo-causal attention with portance for dynamic iteration. regular causal attention (always attending to depth-1 for full contexts) also degrades performance, confirming the value of allowing tokens to attend to current and all shallower iterations of previous positions. Training Scheme. (1) Supervision type. Inspired by early exit methods, common alternative supervises all iteration depths with next-token labels to enable flexible early termination. It enforces accurate prediction at depth 1 even for hard tokens. As shown in Table 3, such token+latent supervision underperforms our token-only approach that supervises only at oracle-determined depths. It aligns with our intuition that different iterations should focus on tokens of different difficulties. (2) Iteration behavior We compare our static oracle strategy π with two alternatives. The iter. decider trains the decider first then uses it during backbone training, but suffers from the coupling challenge discussed in Section 4.3. The dynamic recalculates the oracle using the evolving backbone in Equation 7, encountering the same coupling challenge and causing training collapse. These results support our backbone training recipe: using next-token supervision with oracle iteration policy. Continuation Threshold. As shown in Figure 4, TaH maintains robust performance across different continuation thresholds and iteration ratios. We empirically set cthreshold = 0.9 for all evaluations. 8 Preprint version Table 3: Ablation study on training schemes. Supervision Iter. Behavior MATH500 AMC23 Olympiadbench Average Token-only Oracle Token+latent Oracle Token-only Iter. decider Dynamic 51.2 49.4 44.8 11.0 32.5 29.6 24.1 2. 23.9 15.9 17.3 2.7 35.9 31.6 /4.3 28.7 /7.2 5.5/30. Training Inference Accuracy Standard Standard AlwaysThink AlwaysThink AlwaysThink TaH-Oracle TaH-Oracle TaH-Oracle TaH-Decider TaH-Oracle 52 38/14 77/+25 54/+ 2 80/+28 Table 4: Qwen3-0.6B (first 100 MATH500 samples). Impact of iteration strategies on Figure 5: Next-token prediction changes across iterations. Top2 tokens that think-twice most are visualized. 5.4 BEHAVIOR ANALYSIS Latent Overthinking. To analyze latent thinking patterns, we verbalize tokens from all iteration depths using their last-layer hidden states. The oracle method uses the oracle policy π from Section 4.3 for iteration decision. (1) Generation. Since ground-truth tokens are unavailable during generation, we use predictions from the stronger DeepSeek-R1-Distill-Qwen-32B model (Guo et al., 2025) as proxy labels. Table 4 shows that the oracle policy substantially improves performance by verbalizing correct predictions immediately while iterating only on incorrect ones. With our trained iteration decider approximating the oracle, TaH outperforms both Standard and AlwaysThink baselines. However, the ideal oracle policy achieves even higher gains, indicating future potential. (2) Next-token prediction. We evaluate next-token prediction accuracy on the Open-R1 dataset, using the test model itself as the reference model in π. Figure 1 reveals that AlwaysThink produces more incorrect than correct revisions, demonstrating latent overthinking. In contrast, oracle-controlled iterations substantially increase correct revisions by selectively targeting hard tokens. Token Alternation Patterns. We analyze which tokens TaH selects for deeper iteration. On the validation set, But and So emerge as top candidates, with iteration probabilities of 34% and 18%, respectively. These tokens signal critical contrasting or causal relationships, confirming that models benefit from additional processing at logically complex junctures. Figure 5 illustrates how TaH alternates predictions after iteration at these key tokens, suggesting logic refinement behavior. See Appendix A.1.3 for detailed analysis. Attention Pattern. We visualize the attention pattern of TaH. As discussed in Figure 7 and Appendix A.2.5, the duo-causal attention automatically focuses on different iterations in different heads, extracting broader contexts from multiple depths. Iteration and FLOPs. Table 9 reports per-token FLOPs and average iteration count. TaH matches the FLOPs of Standard baseline (with average 1.06 iterations per token), far below AlwaysThink (with 2.00 iterations per token) which incurs 2.9 to 4.0 the FLOPs."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present TaH, selective latent thinking method that iterates deeper only on hard tokens. Architecturally, TaH introduces duo-causal attention, depth-specific LoRA, and neural iteration decider to facilitate dynamic depths. An oracle policy guides the stable two-stage training for the tightly coupled LLM backbone and decider. Across five reasoning benchmarks, TaH improves accuracy by 4.0-5.4% over strong baselines with minimal overhead (<3% additional parameters and 6% extra iterations), establishing new paradigm for better reasoning within the current parameter budgets. 9 Preprint version"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This study raises no ethical issues, it did not involve human subjects or sensitive personal data."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "This paper provides sufficient information to reproduce the reported results. All experiments were conducted using publicly available datasets together with open-source models and code. Appendix details implementation aspects, including data selection, hyperparameters, and training procedures. To facilitate full reproducibility, we will release the code, configuration files, and model checkpoints upon publication."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. URL https://arxiv.org/abs/2404.14219. Loubna Ben Allal et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. URL https://arxiv.org/abs/2502. 02737. Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. arXiv preprint arXiv:2310.05424, 2023. Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. Ee-llm: Large-scale trainarXiv preprint ing and inference of early-exit large language models with 3d parallelism. arXiv:2312.04916, 2023. Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628, 2023. Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. R2r: Efficiently navigating divergent reasoning paths with small-large model token routing. arXiv preprint arXiv:2505.21600, 2025. 10 Preprint version Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. URL https://arxiv. org/abs/2310.02226, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. Advances in neural information processing systems, 35:3324833261, 2022. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Eunki Kim, Sangryul Kim, and James Thorne. Learning to insert [pause] tokens for better reasoning. arXiv preprint arXiv:2506.03616, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pp. 611626, 2023. Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, and Rex Ying. Implicit reasoning in large language models: comprehensive survey. arXiv preprint arXiv:2509.02350, 2025. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Tianqiao Liu, Zui Chen, Zitao Liu, Mi Tian, and Weiqi Luo. Expediting and elevating large language model reasoning via hidden chain-of-thought decoding. arXiv preprint arXiv:2409.08561, 2024. Xuan Luo, Weizhi Wang, and Xifeng Yan. Adaptive layer-skipping in pre-trained llms. arXiv preprint arXiv:2503.23798, 2025. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. 11 Preprint version Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. URL https://arxiv. org/abs/2404.15758, 2404, 2024. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:1745617472, 2022. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275, 2025. MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, and Maosong Sun. Minicpm4: Ultra-efficient llms on end devices. arXiv preprint arXiv:2506.07900, 2025. URL https: //arxiv.org/abs/2506.07900. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Yuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:69798 69818, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. 12 Preprint version Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, and Zhouhan Lin. Pretraining language models to ponder in continuous space. arXiv preprint arXiv:2505.20674, 2025. Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and Xin Eric Wang. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778, 2025. Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, et al. Mobilellm-r1: Exploring the limits of sub-billion language model reasoners with open training recipes. arXiv preprint arXiv:2509.24945, 2025. Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, et al. Scaling latent reasoning via looped language models. arXiv preprint arXiv:2510.25741, 2025. 13 Preprint version"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL EXPERIMENT SETUPS A.1.1 TRAINING RECIPE As shown in Table 5, we follow the official setup in Open-R1 (Hugging Face, 2025).We set maximum length to 8192 for Standard, TaH, and TaH+, and to 4096 for AlwaysThink due to its substantially higher memory usage during training. Table 5: Training hyperparameters. Hyperparameter Value learning rate max grad norm training epochs global batch size warmup ratio lr scheduler precision 4e-5 0.2 5 128 0.03 cosine (min-lr ratio 0.1) bfloat16 A.1.2 LATENT OVERTHINKING ANALYSIS We set up an oracle experiment to estimate the performance upper bound of our method. The oracle employs the DeepSeek-R1-Distill-Qwen-32B model as dynamic label generator, replacing the MLP-based iteration decider. During the forward pass, we compare the token predictions from the label generator with those from the TaH backbone model. The model continues to iterate only when the top-1 predictions of these two models differ. Due to resource constraints and computational overhead, we evaluated the accuracy only on the first 100 samples from the MATH500 dataset. A.1.3 TOKEN ALTERNATION PATTERNS We analyze tokens that most frequently trigger second iteration (think-twice tokens). For each token type t, we compute the continuation rate Pr (cid:0)c(1) > cthreshold ti = t(cid:1), using the inference threshold cthreshold = 0.9 (Section 4.3). We estimate this quantity on the OpenR1 validation set and, for diagnostics, randomly sample 10K token positions (0.4% of tokens) to track whether the next-token prediction switches between depth 1 and depth 2. This setting quantifies which token types most often trigger an additional iteration and how often iteration alters the predicted next token. Token T1 (c(1) > cthreshold t(1) = T1) Token T2 But So 34.3% 17.7% So Wait Therefore So Therefore But (t(2) = T2 t(1) = T1) 13.63% 12.17% 8.95% 28.17% 13.67% 4.89% Table 6: Conditional probabilities of continuation confidence and next-token distribution. A.1.4 ROUTING BASELINE SETUP For the routing baseline (Section 5), we route between MobileLLM-R1-360M and Qwen3-1.7B, and between Qwen3-0.6B and Qwen3-4B. The routing endpoints are SFT-trained under the same 14 Preprint version Table 7: Performance comparison between TaH-2 and TaH-3 (maximum per-token iterations of 2 and 3, respectively). Iter.2 and Iter.3 denote the per-token percentages executing 2 and 3 iterations, respectively. Param. Dataset 1.7B MATH500 GSM8K AMC23 OlympiadBench AIME25 Average Standard TaH-2 TaHAcc. 68.4 82.1 42.2 33.0 13.3 47.8 Acc. Iter.2 Acc. Iter. Iter.3 74.4 84.5 48.4 38.8 17.9 52.8 5.6 7.5 4.2 5.7 6.0 5.8 72.6 84.8 48.7 41.6 20. 53.6 5.3 7.6 5.1 5.4 5.3 5.7 0.2 0.3 0.1 0.2 0.1 0.2 settings as the Standard baseline in Section 5, and the two routing pairs are configured to match average parameter sizes of 0.6B and 1.7B, respectively. A.2 ADDITIONAL EXPERIMENTAL RESULTS A.2.1 ITERATION DEPTH BEYOND 2 Hard Token Labeling. Previous works have proposed many methods to evaluate the hardness of each tokens in the training data, like through excess loss (Lin et al., 2024; Xie et al., 2023), entropy (Wang et al., 2025; Chen et al., 2023) and prediction difference (Fu et al., 2025). For shallow iteration budgets within two (Dmax 2), we adopt the prediction difference policy. It simply labels the tokens that do not yield top-1 in next-token prediction at the first iteration as hard tokens. Formally, we use binary halting rule: π = (cid:26)0, Dmax, if hi = 0 if hi = 1 (easy token) (hard token) (9) If the iteration depth goes beyond 2 (Dmax > 2), we use the reference models cross-entropy as non-binary indicator of difficulty. Define = log p(0) ℓref i,ref (cid:0)ti+1 (cid:1). We then map difficulty to halting depth via monotone quantile binning: (cid:5) , = (cid:4)QuantileRank(cid:0)ℓref (cid:1) Dmax π (10) where QuantileRank() [0, 1] is the empirical CDF over the training set (higher loss deeper halting). This induces per-depth continuation labels c(d) ] for {0, 1, . . . , Dmax}. = [d < π Experiment Result. Specifically, we train 1.7B TaH with maximum per-token iteration count of 3, using oracle labels generated by the method described above. As shown in Table 7, TaH-3 achieves further improvement of 0.8% on average over TaH-2. A.2.2 DOMAIN GENERALIZATION To assess cross-domain effectiveness, we train 1.7B TaH model on the science subset of Open-R1. We exclude responses longer than 4,096 tokens, yielding 223M tokens in total; all other training settings follow Section 5. We then evaluate TaH against the Standard baseline on the GPQA-diamond benchmark (Rein et al., 2023). TaH improves performance from 35.4 to 39.9, demonstrating the generalizability of our method. A.2.3 ADDITIONAL LATENT THINKING METHODS Some latent thinking methods requires pre-training and uses base model other than Qwen3. We also compare with these methods, including Ponder (Zeng et al., 2025). Specifically, we adopt the 15 Preprint version Figure 6: Iteration-decider accuracy vs. epoch (Qwen3-0.6B). released pretrained PonderingPythia-1.4B as the base model and perform SFT on the same training data. We observe that the fine-tuned model learns the stylistic patterns of the training data, but still underperforms substantially, which may be attributable to the limited capability of the PonderingPythia-1.4B backbone. Table 8: Performance on MATH500 and GSM8K-500 (first 500 GSM8K samples) Method Dataset Standard-0.6B Ponder-1.4B MATH500 GSM8K-500 Avg. 47.2 62. 55 2.0 1.8 1.9 A.2.4 ITERATION DECIDER ANALYSIS Sensitivity Analysis. We conduct additional experiments to analyze the sensitivity of iteration decisions by perturbing the oracle iteration decider with different types of noise. We investigate the differing impacts of overthink and underthink, where overthink refers to cases where the decider incorrectly signals continue, and underthink corresponds to incorrect stop signals. The results in Table 10 demonstrate that the performance of latent reasoning models is highly sensitive to the accuracy of iteration decisions. We quantify these impacts by fitting linear model to the data: acc. = 1.41 #underthink 2.73 #overthink + 0.81 Further analysis reveals that the primary factor contributing to the performance gap between TaH and its oracle variant, TaH-oracle, is inaccurate iteration decisions, with overthink being the dominant source of errors. A.2.5 ATTENTION PATTERN ANALYSIS We perform forward computation on 100 samples, each with length of 128 tokens. Figure 7 shows the average attention weights of three representative attention heads in the second iteration of the TaH model. The left panel illustrates head that mainly attends to keys from the first iteration. The middle panel shows head focusing on keys from the second iteration. The right panel displays head with balanced attention distribution. These results suggest that the TaH model, under the duo-causal attention mechanism, can automatically learn diverse attention patterns across layers and heads. Figure 8a further presents the total attention scores assigned to keys in the first iteration. It can be seen that the first layer tends to focus more on keys from the second iteration. Different layers also exhibit varying attention behaviors. A.2.6 DESIGN CHOICE EXPLORATION Figure 8b shows validation perplexity (ppl) across training schemes. TaH achieves lower perplexity than iter. decider and token+latent. Although dynamic attains the lowest perplexity, it collapses on downstream tasks, often generating infinite-loop outputs. 16 Preprint version Table 9: Per-token FLOPs and average iteration count for Standard, AlwaysThink, TaH and TaH+. Standard AlwaysThink TaH TaH+ Param. Dataset FLOPs 0.6B OlympiadBench GSM8K MATH500 AMC23 AIME 1.4e9 1.1e9 1.3e9 1.4e9 1.5e9 Iter. 1.00 1.00 1.00 1.00 1.00 FLOPs 5.6e9 4.5e9 5.3e9 4.0e9 5.8e9 Iter. 2.00 2.00 2.00 2.00 2.00 FLOPs 1.5e9 1.2e9 1.4e9 1.4e9 1.6e9 Iter. 1.09 1.07 1.05 1.05 1.05 FLOPs 1.5e9 1.2e9 1.4e9 1.5e9 1.6e9 Iter. 1.06 1.07 1.06 1.05 1.06 Table 10: Oracle model performance with overthinks and underthinks. All values are reported in percentages. False rate Underthink Overthink Accuracy 0 10.0 15.0 2.8 22.1 20. 0 1.5 2.1 2.8 0 2.5 0 8.5 12.9 0 22.1 17.5 80.0 55.4 45.2 78.0 21.6 27.1 A.3 IMPLEMENTATION DETAILS A.3.1 DUO-CAUSAL ATTENTION IMPLEMENTATION (1) KV cache concatenation. At depth d, we form the visible K/V sequence by concatenating all shallower-to-current depths along the sequence dimension: KV(d) = [ KV(1) ; KV(2) ; ; KV(d) ]. This realizes the accessible set in Equation 5, allowing deeper iterations to access all shallower iterations while preserving positional causality. (2) Two-dimensional causal mask. For query (i, d), key (j, k) is attendable iff and d. We implement this as an additive attention mask with 0 for allowed entries and otherwise, enforcing positional and iteration causality jointly. When = 1 for all tokens, the rule reduces to standard causal attention. (3) Compatibility with efficient attention. The mask is provided in the standard additive form and the concatenated K/V remain contiguous along the sequence dimension, matching the usual scaled dot-product attention interface. As result, duo-causal attention is directly compatible with optimized kernels such as FlashAttention, without kernel modifications. A.4 ADDTIONAL RELATED WORK Instead of using the shared model parameter multiple times through latent iteration, previous work also proposes layer skipping methods for dynamic computing allocation. Layer Skipping. Layer skipping aims to accelerate LLM inference by dynamically bypassing certain layers for specific tokens. Some methods use learnable module to make real-time skipping decisions. MoD (Raposo et al., 2024) uses top-k router to select subset of tokens for processing, while FlexiDepth (Luo et al., 2025) uses plug-in router to determine whether layer should be bypassed. Others use fixed strategy to skip layers. SkipDecode (Del Corro et al., 2023) enforces monotonically decreasing number of active layers during generation. However, these methods still require loading the entire models parameters, resulting in large memory access overhead. Instead of skipping some layers, TaH adds computational depth by allowing core tokens to undergo multiple refinement iterations. This approach provides greater computational depth without increasing the models parameter count. 17 Preprint version Table 11: Parameter breakdown of Standard, TaH and TaH+. Counts are reported using (million) and (billion). Param. Method Backbone LoRA Iter. Decider Total 0.6B 1.7B Standard TaH TaH+ Standard TaH TaH+ 596M 580M 596M 1.72B 1.67B 1.72B 10M 10M 34M 34M 5M 5M 18M 18M 596M 595M 611M 1.72B 1.72B 1.77B Figure 7: TaH duo-causal attention pattern. A.5 LIMITATIONS AND FUTURE WORK Comparison with Official Qwen3 Models. Official Qwen3 models are trained on different data distributions and scales, and use different training procedures, including on-policy distillation (Yang et al., 2025). By contrast, our models use SFT only on limited, publicly accessible data. Consequently, performance may differ between the two. (a) TaH mean and standard deviation of attention weights (key from iteration 1) across layers in iteration 2. (b) Validation perplexity for different schemes. training 18 Preprint version Future Work. We consider advanced training techniques largely orthogonal to TaH. Promising directions include: (1) integrating TaH with online distillation to provide stronger supervision; (2) applying reinforcement learning to optimize the computeaccuracy trade-off and to learn continuation policies beyond imitation. Due to resource constraints, this work focuses on SFT with public data; we leave these combinations for future work. A.6 USE OF LARGE LANGUAGE MODELS (LLMS) Large language models (e.g., GPT-5, Gemini) were used exclusively for proofreading and minor copyediting. They were not employed to generate scientific content, contribute to research ideation, or design methods. All research ideas, methods, and conclusions are solely the responsibility of the authors. A.7 EXAMPLES We use an example question from AIME25 and responses from Standard, TaH and AlwaysThink to provide an intuitive understanding of our method. Only TaH can successfully answer this question. Blue-highlighted tokens indicate positions that undergo two iterations. Text 1. Question On ABC points A, D, E, and lie that order on side AB with AD = 4, DE = 16, and EB = 8. Points A, F, G, and lie in that order on side AC with AF = 13, = 52, and GC = 26. Let be the reflection of through , and let be the reflection of through E. Quadrilateral DEGF has area 288. Find the area of heptagon AF BCEM . 19 Preprint version Text 2. Standard Response Text 3. TaH Response 91 91 Okay, let me try to figure out this problem step by step. Its about heptagon formed by reflecting points and finding its area. . . . We have triangle ABC. On side AB, points A, D, E, and are in that order. The lengths are given as AD = 4, DE = 16, and EB = 8. So AB = 28 . . . On side AC, points A, F, G, and are in that order with AF = 13, = 52, and GC = 26. So AC = 91 . . . Let me place point at (0, 0) and at (28, 0) . . . Let be at (91, h) . . . Then = (4, 0), = (20, 0), = (cid:0)13, 13h (cid:1), = (cid:0)65, 65h (cid:1) . . . Area of quadrilateral DEGF can be calculated . . . Applying the shoelace formula . . . Area = 1 91 = 624h 91 = 6h So with 288: 288 = 288 Reflections: is the reflection of through and is the reflection of through . . . = (22, 0), = (39, 0), = (28, 0), = (91, 48), = (20, 0) . . . Using the shoelace formula for AF BCEM in order . . . Sum = 0 + 0+0+1344+(960)+ 0 + 0 = 384 Take absolute value and multiply by 1/2: Area = 1 2 384 = 192 . . . Final Answer 192 2 1248h 6 = 48 6h = Okay, lets try to tackle this geometry problem. It seems bit complex with all these points and reflections, . . . First, let me visualize triangle ABC. Points D, E, and are on side AB with AD = 4, DE = 16, . . . Let me and EB = 8. set up coordinate system. Lets place point the origin (0, 0) at . . . Then, since AB is line segment, let me assume AB is along the x-axis. So point would be at (28, 0) . . . Now, points , G, are on AC . . . Let me define coordinates for as (cx, cy) . . . . . . Compute the sum of xiyi+1 xi+1yi for each . . . Sum of all terms: 0 + 1300cy 91 + 0 > 52cy 91 = (130052)cy = 1248cy 91 91 the area is 1 Then, (cid:12) (cid:12) (cid:12) = 624cy 1248cy (cid:12) (cid:12) (cid:12) 91 . . . Compute the area of this heptagon . . . Using shoelace formula . . . Term2: 240 840 Term5: 840 240 . . . Sum: 0 240 + 840 + 1176 840 + 240 + 0 = 1176 Take absolute value and multiply by 1/2: Area = 1 2 1176 = 588 . . . Final Answer 588 Term3: 1176 Term6: Term4: 2 91 20 Text 4. AlwaysThink Response back Okay, so need to find the area of the heptagon AFNBCEM. Let me first try to visualize Theres the problem. triangle ABC. On side AB, there are points A, D, E, and B. The order is A, D, E, B. The distances given are AD=4, DE=16, and EB=8. So, starting at A, moving 4 units to D, then 16 units to E, then 8 units to B. So, the total length of AB is AD + DE + EB = 4 + 16 + 8 = 28 units. Got that. . . . Points in order: D(4,0), F(13,h), E(20,0), G(65,h), to D(4,0). Compute the terms: . . . Sum these terms: 0 + 20h52h4h = 36h value absolute Take by multiply and = Area 1/2: (1/2) 36h = (1/2) 36h = 18h But the area is given as 288, so 18h = 288 = 16. . . . Now, apply the shoelace formula . . . Therefore, total sum is 8640/17 value absolute Take by multiply and = Area 1/2: (1/2) 8640/17 = 8640/34 = 4320/17 254.1176 . . . Therefore, the answer is 4320/17. Final Answer 4320"
        }
    ],
    "affiliations": [
        "Infinigence AI",
        "Shanghai Jiao Tong University",
        "Tsinghua University"
    ]
}