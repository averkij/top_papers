{
    "paper_title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
    "authors": [
        "Haonan Qiu",
        "Ning Yu",
        "Ziqi Huang",
        "Paul Debevec",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CineScale: Free Lunch in High-Resolution Cinematic Visual Generation Haonan Qiu*, Ning Yu(cid:66), Ziqi Huang, Paul Debevec, Ziwei Liu(cid:66) 5 2 0 2 1 ] . [ 1 4 7 7 5 1 . 8 0 5 2 : r AbstractVisual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of highresolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained these methods are still prone to producing lowmodels. However, quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. Index TermsDiffusion Models, Image Generation, Video Generation, High Resolution"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Diffusion models have revolutionized visual generation [1] [6], empowering individuals without any artistic expertise to effortlessly create distinctive and personalized designs, graphics, and short films using specific textual descriptions. Nonetheless, current visual diffusion models are generally trained on data with limited resolution, such as 5122 for SD 1.5 [7], 10242 for SDXL [1], and 320 512 for VideoCrafter2 [4], hampering their ability to generate highfidelity images or videos at higher resolutions. Given the scarcity of high-resolution visual data and the substantially greater model capacity required for modeling such *This work was done during an internship at Netflix Eyeline Studios, (cid:66)corresponding authors H. Qiu, Z. Huang, and Z. Liu are with Nanyang Technological University. Email: {HAONAN002, ZIQI002}@e.ntu.edu.sg, ziwei.liu@ntu.edu.sg H. Qiu, N. Yu, and P. Debevec are with Netflix Eyeline Studios. Email: {ning.yu, debevec}@scanlinevfx.com data, recent efforts have focused on employing tuning-free strategies for high-resolution visual generation to inherit the strong generation capacities of existing pre-trained diffusion models. Despite the advances achieved by existing methods, they are still prone to producing low-quality images or videos, particularly manifesting as repetitive object occurrences and unreasonable object structures. ScaleCrafter [8] puts forward that the primary cause of the object repetition issue is the limited convolutional receptive field and uses dilated convolutional layers to achieve tuning-free higher-resolution sampling. But the generated results of ScaleCrafter still suffer from the problem of local repetition. Inspired by MultiDiffusion [9] fusing the local patches of the whole images, DemoFusion [10] designed mechanism by fusing the local patches and global patches, almost eliminating the local repetition. Essentially, this solution just transfers the extra signal of the object to the background, leading to small object repetition generation. FouriScale [11] reduces those extra signals by removing the high-frequency signals of the latent before the convolution operation. Although FouriScale completely eliminates all types of repetition, the generated results always have weird colors and textures due to its violent editing on the frequency domain. To generate satisfactory visual contents without any unexpected repetition, we propose FreeScale, tuning-free inference paradigm that enables pre-trained image and video diffusion models to generate vivid higher-resolution results. Building on past effective modules [8], [12], we first propose tailored self-cascade upscaling and restrained dilated convolution to gain the basic visual structure and maintain the quality in higher-resolution generation. To further eliminate all kinds of unexpected object repetitions, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components, ensuring both the structures overall rationality and the objects local quality. This fusion is smoothly integrated into the original self-attention layers, thereby bringing only minimal additional time overhead. Finally, we demonstrate the effectiveness of our model on both the text-to-image model and the text-to-video model, pushing the boundaries of image generation even up to an 8k resolution. Benefiting from the exceptional scalability, DiT has become the dominant architecture in the development of recent foundational diffusion models. Nevertheless, FreeScale and the majority of existing works are built upon the UNet 00000000/00$00.00 2021 IEEE JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST architecture. Due to the architectural gap, these methods exhibit limited effectiveness on DiT-based models. Specifically, the major challenge faced by DiT-based models in highresolution generation is the substantial increase in token count, which results in untrained positional embeddings and overly diluted attention, ultimately hindering generation quality. Indeed, these challenges have been thoroughly explored in large language models for long-text generation [13], [14], providing valuable empirical knowledge like NTK-aware interpolation and attention reweighting. Combining those technologies, we extend the original FreeScale framework by tailoring it to the architectural properties of DiT, yielding new variant that supports high-resolution generation on DiT-based models. Although tuning-free strategies already yield promising results, an excessive number of tokens can lead to degraded video quality due to increased positional encoding errors even under the NTK-aware setting. To address this, we collect small number of 2K-resolution videos and apply LoRA [15] fine-tuning under the NTK-aware positional encoding configuration. This lightweight adaptation enables the model to accommodate the new positional range and generate 4k-resolution videos effectively. We refer to the collection of these high-resolution generation techniques across different model architectures, including the earlier FreeScale presented at ICCV 2025 [16], as CineScale. Additionally, in contrast to prior baseline approaches that are limited to high-resolution text-to-image (T2I) and text-to-video (T2V) only, CineScale extends high-resolution capabilities to image-to-video (I2V) and video-to-video (V2V) tasks on top of state-of-the-art open-source video models. Our contributions are summarized as follows: We propose CineScale, novel inference paradigm extended from FreeScale, enabling higher-resolution visual generation in both UNet-based and DiT-based diffusion models. We empirically evaluate our approach on various tasks including text-to-image (T2I) and textto-video (T2V), image-to-video (I2V) and video-tovideo (V2V), demonstrating the effectiveness of our model. Compared to other state-of-the-art tuning-free methods, we unlock the 8k-resolution (64) text-to-image generation for the first time. With only minimal LoRA fine-tuning, we enable 4k-resolution (9) video generation."
        },
        {
            "title": "2 RELATED WORK\nDiffusion Models for Image Generation. The advent of\ndiffusion models has transformed the landscape of im-\nage and video generation by enabling the production of\nexceptionally high-quality outputs [1]–[6], [17]–[22]. Initial\nbreakthroughs like DDPM [23] and Guided Diffusion [24]\ndemonstrated that diffusion processes could yield remark-\nable image quality. To enhance computational efficiency,\nLDM [7] introduced latent space diffusion, which operates\nin a compressed space, significantly lowering the computa-\ntional burden and training demands; this method laid the\ngroundwork for Stable Diffusion. Building on this, SDXL [1]\nfurther advanced high-resolution image synthesis. Inspired",
            "content": "2 by DiT [25], Pixart-alpha [2] adopted transformer-based architecture, achieving both high fidelity and cost-effective image generation. Diffusion Models for Video Generation. For video generation, VDM [26] pioneered the application of diffusion in this domain, followed by LVDM [27], which extended the method to propose hierarchical latent video diffusion framework capable of generating extended video sequences. To bridge text-to-image and text-to-video (T2V) capabilities, Align-Your-Latents [28] and AnimateDiff [29] introduced temporal transformers into existing T2I models. VideoComposer [30] then offered controllable T2V generation approach, allowing precise management of spatial and temporal cues. VideoCrafter [4], [31] and SVD [32] scaled these latent video diffusion models to handle extensive datasets. Lumiere [33] proposed temporal downsampling within space-time U-Net for greater efficiency. Recently, CogVideoX [5] and Pyramid Flow [34], Mochi [35] three highly regarded open-source models, showcase impressive video generation capabilities, demonstrating the superior performance of DiT structure in video generation. Regarding the remarkable scalability of DiT, all of the development of foundational video diffusion models then turns to DiT architectures. Recent powerful DiT-based models, LTX [36], Hunyuan [37], and Wan [38], have been able to generate relatively realistic videos that follow text input, surpassing previous UNet-based models. For the U-Net structure, we chose SDXL [1] as our pre-trained image model, and VideoCrafter2 [4] as our pre-trained video model. For the DiT structure, we chose Wan [38]. Higher-Resolution Visual Generation. High-resolution visual synthesis is classic challenge in the generative field due to the difficulty of collecting plenty of high-resolution data and the requirement of substantial computational resources. Recent methods for higher-resolution generation can mainly be divided into two categories: 1) training/tuning methods with high-resolution data and large models [12], [39][44], or 2) tuning-free methods without any additional data requirement [45][52]. Training with high-resolution data on larger models should be more fundamental solution. However, high-resolution visual data only accounts for small proportion. Meanwhile, targeting for modeling higher-resolution data demands notably increased requirement in model capacity. Based on current data and calculation resources, tuning-free approaches are more achievable for high-resolution generation. One straightforward approach is to generate visual patches of the same resolution as the training video and then stitch them together. Although eliminating the traininginference gap, this method results in disconnected and incoherent patches. MultiDiffusion [9] addresses this problem by fusing patches smoothly during the denoising process. DemoFusion [10] utilizes this mechanism and adds global perception to ensure the rationality of the overall layout. However, this solution easily leads to the generation of small object repetition. ScaleCrafter [8] argues that the object repetition issue is mainly caused by the limited convolutional receptive field and uses dilated convolutional layers to enlarge the convolutional receptive field. Although successful in removing small object repetition, ScaleCrafter suffers from JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 1. Overall framework of CineScale. (a) Tailored Self-Cascade Upscaling. CineScale first upsamples generated image or video from the training resolution, then gradually adds noise to the high-resolution latent, and finally denoises it to achieve detail reconstruction. Part of the clean latent is reintroduced during denoising to stabilize generation and control detail. (b) Scale Fusion. For the UNet structure, we modify the self-attention layer to combine global and local attention, fusing high-frequency details and low-frequency semantics via Gaussian blur for the final output. We also use Restrained Dilated Convolution to adapt the convolution layer of the model to high resolution for reducing repetition. (c) DiT Extention. To support DiT models, we additionally add NTK-RoPE and Attentional Scaling. Building on the tuning-free setup, Minimal LoRA Fine-Tuning is additionally introduced to help the model better adapt to the modified RoPE, leading to improved performance. new problem of local repetition. FouriScale [11] concludes that all types of repetitions are from the non-alignment of frequency domain on different scales. FouriScale removes the high-frequency signals of the latent prior to convolution operation and achieves no repetition at all. But this violent editing operation on the frequency domain leads to strange results with unnatural colors and textures. Another solution is directly removing the text semantics from unexpected areas in the input level [53], [54]. However, it only works for small object repetition and will suffer information leakage through the temporal layers in the video generation. With the additional pose as input, BeyondScene [55] has achieved 8k human image generation. However, its scope is limited to human image generation due to the requirement of additional pose input. FreeScale is the first 8k-resolution text-to-image generation method without these constraints. Excluding super-resolution approaches [56], [57], current high-definition video generation methods still rely on finetuning models with high-resolution data [58]. In this paper, we first propose some tuning-free adjustments for higher resolution video generation, then utilize minimal LoRA tuning to gain better performance."
        },
        {
            "title": "3 FREESCALE\n3.1 Preliminaries",
            "content": "Latent Diffusion Models (LDM) first encodes given image to the latent space via the encoder of the pretrained auto-encoder E: = E(x). Then forward diffusion process is used to gradually add noise to the latent data z0 p(z0) and learn denoising model to reverse this process. The forward process contains timesteps, which gradually add noise to the latent sample z0 to yield zt through parameterization trick: q(ztzt1) = (zt; (cid:112)1 βtzt1, βtI), q(ztz0) = (zt; αtz0, (1 αt)I), (1) where βt is predefined variance schedule, is the timestep, αt = (cid:81)t i=1 αi, and αt = 1 βt. The reverse denoising process obtains less noisy latent zt1 from the noisy input zt at each timestep: pθ (xt1 xt) = (xt1; µθ (zt, t) , Σθ (zt, t)) , (2) where µθ and Σθ are determined through noise prediction network ϵθ (zt, t) with learnable parameters θ. 3.2 Tailored Self-Cascade Upscaling Directly generating higher-resolution results will easily produce several repetitive objects, losing the reasonable visual structure that was originally good. To address this issue, we utilize self-cascade upscaling framework from previous works [10], [12], which progressively increases the resolution of generated results: (cid:0) z2r αKϕ (zr 0) , 1 αKI(cid:1) , (3) where means the noised intermediate latent, is the resolution level (1 represents original resolution, 2 represents the twice height and width), and ϕ is an upsampling operation. Specifically, FreeScale will denoise using the training resolution. The intermediate results will then be gradually up-sampled. In the higher resolution, blurry details from the upsampling will be removed by adding noise (to the level of timestep K) and denoising. In this way, the framework will generate reasonable visual structure in low resolution and maintain the structure when generating higher-resolution results. There are two options for ϕ: directly upsampling in latent (ϕ (z) = UP(z)) or upsampling in RGB space (ϕ (z) = E(UP(D(z))), where and are the encoder and decoder of pre-trained VAE, respectively. Upsampling in RGB space is closer to human expectations but will add some blurs. We empirically observe that these blurs will hurt the video JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 almost fixed. Therefore, we use the original convolution in the last few timesteps. 3.4 Scale Fusion Although tailored self-cascade upscaling and restrained dilated convolution can maintain the rough visual structures and effectively generate 4 resolution images, generating 16 resolution images still leads to artifacts such as local repetition, e.g., additional eyes or noses. This issue arises because dilated convolution weakens the focus on local features. DemoFusion [10] addresses this by using local patches to enhance local focus. However, although the local patch operation mitigates local repetition, it brings small object repetition globally. To combine the advantages of both strategies, we design Scale Fusion, which fuses information from different receptive scales to achieve balanced enhancement of local and global details. Regarding global information extraction, we utilize global self-attention features. The reason is that the selfattention layer enhances the patch information based on similarity, making it easier for the subsequent crossattention layer to aggregate semantics into complete object. This can be formulated as: hglobal out = SelfAttention (hin) = softmax (cid:18) QK (cid:19) V, (6) where = LQ(hin), = LK(hin), = LV (hin). that, After In this formulation, query Q, key K, and value are calculated from hin through the linear layer L, and is scaling coefficient for the self-attention. the indepenlatent representations via dently applied to these local hout, = SelfAttention (hin, n). And then Hlocal = out [hout, 0 , hout, , hout, N] is reconstructed to the original size with the overlapped parts averaged as hlocal out = , where Rlocal denotes the reconstruction proRlocal cess. self-attention layer (cid:0)Hlocal out is (cid:1) (cid:17) (cid:17) + 1 + 1 (cid:16) (Hh) dh (cid:16) (W w) dw Regarding local information extraction, we follow previous works [9], [10], [59] by calculating self-attention locally to enhance the local focus. Specifically, we first apply shifted crop sampling, Slocal(), to obtain series of local latent representations before each self-attention layer, i.e., Hlocal in = Slocal (hin) = [hin, 0 , hin, , hin, N] , hin, Rchw, where = , with dh and dw representing the vertical and horizontal stride, respectively. After that, the self-attention layer is independently applied to these local latent representations via hout, = SelfAttention (hin, n). The resulting outputs Hlocal out = [hout, 0 , hout, , hout, N] are then mapped back to the original positions, with the overlapped parts averaged to form hlocal , where Rlocal denotes the reconstruction process. While hlocal out tends to produce better local results, it can bring unexpected small object repetition globally. These artifacts mainly arise from dispersed high-frequency signals, which will originally be gathered to the right area through global sampling. Therefore, we replace the high-frequency out = Rlocal (cid:0)Hlocal out (cid:1) Fig. 2. Structure gap. UNet-based LDMs and DiT-based LDMs will face different challenges in the higher-resolution generation task. UNetbased LDMs face repetition problems while DiT-based LDMs face blur problems. generation but help to suppress redundant over-frequency information in the image generation. Therefore, we adopt upsampling in RGB space for higher-resolution image generation and latent space upsampling in higher-resolution video generation. Flexible Control for Detail Level. Different from superresolution tasks, FreeScale will endlessly add more details as the resolution grows. This behavior will hurt the generation when all reasonable details are generated. To control the level of newly generated details, we modify pθ (zt1 zt) to pθ (zt1 ˆzt) with: = zr ˆzr where = (cid:0)(cid:0)1 + cos (cid:0) decay factor with scaling factor α. + (1 c) zr , π(cid:1)(cid:1) /2(cid:1)α is scaled cosine (4) Even in the same image, the detail level varies in different areas. To achieve more flexible control, α can be 2D-tensor and varies spatially. In this case, users can assign different values for different semantic areas according to (zr 0) calculated in the previous process already. 3.3 Restrained Dilated Convolution ScaleCrafter [8] observes that the primary cause of the object repetition issue is the limited convolutional receptive field and proposes dilated convolution to solve it. Given hidden feature map h, convolutional kernel k, and the dilation operation Φd() with factor d, the dilated convolution can be represented as: k(h) = Φd(k), (h Φd(k)) (o) = (cid:88) s+dt=p h(p) k(q), (5) where o, p, and are spatial locations used to index the feature or kernel. denotes convolution operation. To avoid catastrophic quality decline, ScaleCrafter [8] only applies dilated convolution to some layers of UNet while still consisting of several up-blocks. However, we find that dilated convolution in the layers of up-blocks will bring many messy textures. Therefore, unlike previous works, we only apply dilated convolution in the layers of downblocks and mid-blocks. In addition, the last few timesteps only render the details of results and the visual structure is JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Fig. 3. Image qualitative comparisons with other baselines. Our method generates both 20482 and 40962 vivid images with better content coherence and local details. signals in the local representations with those from the global level hglobal : out (cid:16) out = hglobal hfusion (cid:124) out (cid:123)(cid:122) high frequency hglobal out (cid:17) (cid:125) (cid:16) (cid:17) , hlocal + out (cid:124) (cid:125) (cid:123)(cid:122) low frequency (7) where is low-pass filter implemented as Gaussian blur, and hglobal acts as high pass of hfusion out hglobal out (cid:16) (cid:17) . out"
        },
        {
            "title": "4 DIT EXTENSION",
            "content": "4.1 Structure Gap DiT-based LDMs (e.g., FLUX [60] and Wan [38]) have showcased impressive visual generation capabilities recently. Compared to traditional UNet-based video diffusion models, DiT (Diffusion Transformer) replaces the convolutional UNet backbone with Transformer architecture. This shift allows DiT to better model long-range dependencies and complex spatiotemporal correlations in video data. Additionally, DiT naturally scales with model size and benefits from modern training practices used in largeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 Fig. 4. Zoomed in details for the 8k image. FreeScale may regenerate the original blurred areas at low resolution based on the prior knowledge that the model has learned. As shown in the bottom row, two originally chaotic and blurry faces are clearly outlined at 8k resolution. TABLE 1 Image quantitative comparisons with other baselines. FreeScale achieves the best or second-best scores for all quality-related metrics with negligible additional time costs. The best results are marked in bold, and the second-best results are marked by underline. Method 20482 40962 FID KID FIDc KIDc IS Time (min) FID KID FIDc KIDc IS Time (min) SDXL-DI [1] ScaleCrafter [8] DemoFusion [10] FouriScale [11] Ours 64.313 67.545 65.864 68.965 44.723 0.008 0.013 0.016 0.016 0. 31.042 60.151 63.001 69.655 36.276 0.004 0.020 0.024 0.026 0.006 10.424 11.399 13.282 11.055 12.747 0.648 0.653 1.441 1.224 0.853 134.075 100.419 72.378 93.079 49.796 0.044 0.033 0.020 0.029 0. 42.383 116.179 94.975 128.862 71.369 0.009 0.053 0.045 0.068 0.029 7.036 8.805 12.450 8.248 12.572 5.456 9.255 11.382 8.446 6.240 scale vision-language models. However, UNet-based LDMs and DiT-based LDMs encounter distinct challenges in highresolution generation. As illustrated in Figure 2, UNetbased LDMs often suffer from repetitive artifacts, whereas DiT-based LDMs are more prone to blurriness. Most existing higher-resolution generation methods are tailored to one type of architecturesuch as DemoFusion [10] and FouriScale [11] for UNet-based LDMs, and I-MAX [61] for DiT-based LDMsreflecting the general understanding that different problems call for different solutions. Similarly, to effectively support DiT-based architectures, FreeScale must be additionally adapted to address the unique characteristics of DiT."
        },
        {
            "title": "4.2 Tuning-Free Adaptation",
            "content": "In the field of large language models (LLM), techniques have been proposed in YaRN [62] to correct token representations for generating longer text. After appropriate adaptation, these techniques can also be effective in diffusion models based on DIT to generate higher resolution visual content. Positional Encoding. Wan [38] utilizes Rotary Positional Embedding (RoPE) [63] as its positional encoding, injecting relative position information into the model by rotating query and key vectors in multi-head attention. However, when the number of tokens during inference far exceeds that during training, the parameters learned by RoPE can no longer encode the new positions effectively, leading to positional confusion and degraded video quality. To address this issue, we replace RoPE with NTK-RoPE [62]. Specifically, λβ JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 TABLE 2 Image quantitative comparisons with baselines in 2048 4096 resolution. FreeScale still achieves the best or second-best scores for all metrics. Method FID KID FIDc KIDc IS SDXL-DI [1] ScaleCrafter [8] DemoFusion [10] FouriScale [11] Ours 97.493 97.235 72.196 95.891 54.704 0.026 0.032 0.019 0.032 0.004 38.273 107.582 91.264 118.306 65.584 0.009 0.050 0.044 0.061 0.025 7.258 8.001 10.622 8.422 11. TABLE 3 Image quantitative comparisons with super-resolution. Compared to super-resolution post-processing setting SDXL+Real-ESRGAN, FreeScale also achieves competitive performance. As reported in most previously published related works, higher-resolution generation methods are hard to beat SR methods completely on quantitative metrics due to the difference in difficulty between the two tasks. Fig. 5. Flexible aspect ratio generation. FreeScale can directly achieve flexible aspect ratio (the resolution must be multiple of 512) without any adaptation. Method FID KID FIDc KIDc IS SDXL+Real-ESRGAN [56] Ours 43.476 49.796 0.000 0.004 73.524 71. 0.024 0.029 12.599 12.572 replaces β in the original RoPE: (cid:20) (cid:19) (cid:19) , sin , , cos cos (cid:18) β0 (cid:18) β0 (cid:18) (cid:19) βd/21 , sin (cid:18) (cid:19)(cid:21) βd/21 (8) Attention Scaling. Due to the softmax operation in the selfattention mechanism, an excessive number of tokens can cause the output distribution to become overly diluted. To address this, we introduce temperature parameter to help restore the perplexity to reasonable level. Unlike large language models, diffusion models include classifierfree guidance (CFG), which tends to reduce perplexity. Therefore, we adopt relatively moderate value of t: softmax (cid:33) (cid:32) qT mkn t(cid:112)D (9) Noise Shifting. Higher resolutions need more noise to destroy their signa [64]. Accordingly, when employing Tailored Self-Cascade Upscaling, we initially adopt small noise shift at lower resolutions, gradually increasing it as the resolution rises. 4.3 Minimal LoRA Fine-Tuning Although NTK-RoPE, as tuning-free method, can mitigate the impact of increased token counts, subtle biases still accumulate as the resolution increases. Therefore, to pursue higher generation quality, we leverage small amount of data to help the model adapt to higher-resolution positional encoding. Compared to fully fine-tuning, we chose LoRA [15] tuning. LoRA is parameter-efficient fine-tuning approach that keeps the original model weights frozen while introducing trainable low-rank decomposition matrices into selected network layers. This design allows the model to adapt to the new RoPE mode without overfitting to the newly provided videos. 5 EXPERIMENTS FOR UNET STRUCTURE Experimental Settings. We conduct experiments based on an open-source T2I diffusion model SDXL [1] and an opensource T2V diffusion model VideoCrafter2 [4]. Considering the computing resources that can be afforded, we evaluate the image generation at resolutions of 20482 and 40962, and video generation at resolutions of 640 1024. All experiments are produced using single A800 GPU. Evaluation Metrics. Since higher-resolution inference methods are intended to maintain the quality of the original resolution outputs, we calculate all metrics between the originally generated low-resolution images/videos and the corresponding high-resolution outputs. To evaluate the quality of generated images, we report Frechet Image Distance (FID) [65], Kernel Image Distance (KID) [66] and IS (Inception Score) [67]. FID and KID need to resize the images to 299 before the comparison and this operation may cause quality loss for high-resolution images. Inspired by previous work [68], we also use cropped local patches to calculate these metrics without resizing, termed FIDc and KIDc. We use Frechet Video Distance (FVD) [69] to evaluate the quality of video generation. In addition, we test dynamic degree and aesthetic quality from the VBench [70] to evaluate the dynamics and aesthetics. 5.1 Higher-Resolution Image Generation We compare FreeScale with other higher-resolution image generation methods: (i) SDXL [1] direct inference (SDXLDI) (ii) ScaleCrafter [8] (iii) DemoFusion [10], and (iv) FouriScale [11]. FreeU [18] is used if compatible. Qualitative comparison results are shown in Figure 3. We observe that direct generation often results in multiple duplicated objects and loss of the original visual structure. ScaleCrafter tends to produce localized repetitions, while DemoFusion generates isolated small objects nearby. FouriScale can drastically alter the style for certain prompts. In contrast, the proposed FreeScale is capable of generating high-quality images without any unexpected repetition. As shown in Figure 4, FreeScale effectively enhances local details without compromising the original visual structure or introducing object repetitions. Different from simple superresolution, FreeScale may regenerate the original blurred areas at low resolution based on the prior knowledge that JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 Fig. 6. Image qualitative comparisons with super-resolution. FreeScale is not inferior to SDXL+Real-ESRGAN in visual quality, and adds more details. In addition, SR methods will faithfully follow the low-resolution input while FreeScale can regenerate the original blurred areas based on the prior knowledge that the model has learned. Fig. 7. Results of flexible control for detail level. better result will be generated by adding the coefficient weight in the area of Griffons and reducing the coefficient weight in the other regions. Fig. 8. Local semantic editing of images. FreeScale makes the hair purple or edits the face to make this person look more Japanese in the higher-resolution (40962). the model has learned. In Figure 4, two originally chaotic and blurry faces are clearly outlined at 8k resolution. The quantitative results also confirm the superiority of FreeScale. As shown in Table 1, SDXL-DI achieves the best FIDc and KIDc. The reason is that SDXL-DI tends to generate multiple duplicated objects and its crop may be closer to the reference images. However, this behavior will sacrifice the visual structure thus SDXL gains the worst FID, KID and IS in the resolution of 40962. Overall, our approach achieves the best or second-best scores for all quality-related metrics with negligible additional time costs. Flexible Aspect Ratio Generation. As shown in Figure 5, FreeScale can directly achieve flexible aspect ratio (the resolution must be multiple of 512) without any adaptation. We also add quantitative experiments for 2048 4096 resolution. As shown in Table 2, FreeScale still achieves the best or second-best scores for all metrics. Comparison with Super-Resolution. Different from traditional super-resolution (SR) tasks. Higher-resolution generation aims to tap the potential of the pre-trained model itself. Therefore, the performance of the higher-resolution generation method is based on the base model rather than another additional SR model. We compare our method with super-resolution post-processing setting: SDXL+RealESRGAN [56]. As shown in Table 3, FreeScale achieves comJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 9 Fig. 9. UNet-based video qualitative comparisons with other baselines. While other baselines fail in video generation, FreeScale effectively generates higher-resolution videos with high fidelity. Best viewed ZOOMED-IN. Fig. 10. Fast generation with SDXL-Turbo. FreeScale can help SDXLTurbo generate results at 20482 resolution with even 2 timesteps. petitive performance in quantitative metrics. As reported in most previously published related works [8], [10], higherresolution generation methods are hard to beat SR methods completely on quantitative metrics due to the difference in difficulty between the two tasks. However, Figure 6 shows that FreeScale is not inferior to SDXL+Real-ESRGAN in visual quality, and adds more details. In addition, SR methods will faithfully follow the low-resolution input while FreeScale can regenerate the original blurred areas based on the prior knowledge that the model has learned (the eyes and logos in Figure 6). Local Control. FreeScale provides flexible control for detail level in generated results. Figure 7 shows demo of changing the detail level of different semantic areas. During the process of tailored self-cascade upscaling, we will get 1 results as intermediates. Although more details will be added or modified in the later higher-resolution stages, the overall structure and main content of the image have been determined in the 1 results. It is easy to calculate semantic masks [71] and assign different α for each region in Equation 4. As shown in Figure 7, we will obtain better result when we add the coefficient weight in the area of Griffons and reduce the coefficient weight in other regions. Fig. 11. Qualitative image comparisons with ablations. Our full method performs the best. The resolution of results is 40962 for better visualizing the difference between the various strategies. In addition, this mechanism can even be extended to local semantic editing. Utilizing the semantic mask from 1 results, we can inject different text semantics into different regions in the layers of cross-attention. As shown in Figure 8, FreeScale successfully edits the hair and face in the higher-resolution results. 5.2 Fast Generation with SDXL-Turbo FreeScale can easily be compatible with other models with similar structures. SDXL-Turbo [72] is distilled version of JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 Fig. 12. Video ablations in Wan T2V without tuning in resolution 960 1664. Although all variants can generate rough results. Our full method performs the best. Best viewed ZOOMED-IN. TABLE 4 Image quantitative comparisons with other ablations. Our final FreeScale achieves better quality-related metric scores in all experiment settings. The best results are marked in bold. Method 20482 40962 FID KID FIDc KIDc IS Time (min) FID KID FIDc KIDc IS Time (min) w/o Scale Fusion Dilated Up-Blocks Latent Space Upsampling Ours 75.717 75.372 72.454 44.723 0.017 0.017 0.015 0. 76.536 76.673 71.793 36.276 0.026 0.025 0.023 0.006 12.743 12.541 12.210 12.747 0.614 0.861 0.840 0.853 68.115 67.447 65.081 49.796 0.012 0.011 0.009 0. 100.065 98.558 88.632 71.369 0.037 0.035 0.029 0.029 12.415 12.543 11.307 12.572 4.566 6.245 6.113 6.240 TABLE 5 Video quantitative comparisons with baselines. FreeScale achieves the best scores for all metrics. Method FVD Dynamic Degree Aesthetic Quality Time (min) VC2-DI [4] ScaleCrafter [8] DemoFusion [10] Ours 611.087 723.756 537.613 484.711 0.191 0.104 0.342 0. 0.580 0.584 0.614 0.621 4.077 4.098 9.302 3.787 TABLE 6 User study for Image Generation. Users are required to pick the best one among our proposed FreeScale with the other baseline methods in terms of image-text alignment, image quality, and visual structure. Method Text Alignment Image Quality Visual Structure SDXL-DI [1] ScaleCrafter [8] DemoFusion [10] FouriScale [11] Ours 0.87% 7.83% 17.39% 2.17% 71.74% 0.00% 5.22% 14.35% 2.61% 77.83% 0.00% 7.83% 18.26% 1.74% 72.17% SDXL [1] and can produce similar quality results with 2 4 timesteps. However, SDXL-Turbo can only generate results at 5122 resolution due to the knowledge loss during distillation. As shown in Figure 10, FreeScale can help SDXL-Turbo generate results at 20482 resolution. 5.3 Higher-Resolution Video Generation We compare FreeScale with other tuning-free higherresolution video generation methods: (i) VideoCrafter2 [4] TABLE 7 User study for Video Generation. Users are required to pick the best one among our proposed FreeScale with the other baseline methods in terms of text alignment, cover quality, and video quality. Method Text Alignment Cover Quality Video Quality VC2-DI ScaleCrafter DemoFusion Ours 5.38% 4.62% 30.00% 60.00% 4.62% 5.38% 26.92% 63.08% 3.85% 0.77% 30.77% 64.62% TABLE 8 Video quantitative comparisons with other ablations. Our final setting achieves the best or second-best scores for all metrics. The best results are marked in bold, and the second-best results are marked by underline. Method FVD Dynamic Degree Aesthetic Quality Time (min) Dilated Up-Blocks RGB Upsampling Ours 523.323 422.245 484. 0.363 0.381 0.383 0.611 0.604 0.621 3.788 3.799 3.787 direct inference (VC2-DI) (ii) ScaleCrafter [8], and (iii) DemoFusion [10]. FouriScale [11] is not evaluated since its bundled FreeU [18] does not work well in video generation. As shown in Figure 9, the behavior of VC2-DI and ScaleCrafter are similar to the corresponding version in image generation, tending to generate duplicated whole objects and local parts, respectively. However, DemoFusion has completely unexpected behavior in the video generation. Its Dilated Sampling mechanism brings strange patterns JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 13. Video comparison with DiT-based models in resolution 960 1664. Although other baselines can produce reasonable results at moderately higher resolutions, they still suffer from varying degrees of blurriness. In contrast, CineScale generates high-quality videos with rich visual details. Best viewed ZOOMED-IN. TABLE 9 Video ablations in Wan T2V without tuning. Considering all aspects, our full method demonstrates the best overall performance. Method Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality w/o Self-Cascade Upscaling w/o Noise Shifting w/o NTK-RoPE w/o Attention Scaling Ours 0.958 0.963 0.965 0.965 0.967 0.968 0.971 0.970 0.973 0.972 0.989 0.990 0.990 0.989 0.990 0.352 0.250 0.234 0.281 0.258 0.639 0.635 0.661 0.619 0. 0.620 0.621 0.683 0.608 0.666 TABLE 10 Video comparison with DiT-based models in resolution 1088 1920. Our CineScale achieves the best or second-best scores for all metrics. The best results are marked in bold, and the second-best results are marked by underline. Method Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality LTX (2B) [36] Wan-DI [38] SeedVR2 (3B) [73] Ours (1.3B) 0.935 0.935 0.966 0.968 0.951 0.975 0.971 0.974 0.989 0.989 0.990 0.990 0.523 0.281 0.289 0.344 0.607 0.641 0.676 0. 0.668 0.598 0.683 0.668 all over the frames and Skip Residual operation makes the whole video blur. In contrast, our FreeScale effectively generates higher-resolution videos with high fidelity. Table 5 exhibits that our method achieves the best FVD, dynamic degree and aesthetic quality. In addition, the time cost saved by skipping certain timesteps near pure noise (transparent blocks in Figure 1) even outweighs the extra time caused by other modules in FreeScale. 5.4 User Study In addition, we conducted user study to evaluate our results on human subjective perception. Users are asked to watch the generated images of all the methods, where each example is displayed in random order to avoid bias, and then pick the best one in three evaluation aspects. total of 23 users were asked to pick the best one according to the image-text alignment, image quality, and visual structure, respectively. As shown in Table 6, our approach gains the most votes for all aspects, outperforming baseline methods by large margin. We also add human study for video generation. Users were asked to pick the best one according to the text alignment, cover quality, and video quality, respectively. As shown in Table 7, our method still gains the most votes for all aspects, outperforming baseline approaches significantly. 5.5 Ablation Study The proposed FreeScale mainly consists of three components: (i) Tailored Self-Cascade Upscaling, (ii) Restrained Dilated Convolution, and (iii) Scale Fusion. To visually demonstrate the effectiveness of these three components, we conducted ablations on the SDXL generating 20482 and 40962 images. First, we show the advantage of upsampling in RGB space. As shown in Figure 11, upsampling in latent space brings certain artifacts in the lions eyes. Then dilating the convolution in up-blocks or removing Scale Fusion will cause some cluttered textures that appear in the generated results due to small repetition problems. Table 4 shows that our final FreeScale achieves better quality-related metric scores in all experimental settings. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12 Fig. 14. Video comparison with DiT-based models in resolution 1920 3328. At the resolution several times higher than those used during training, LTX and Wan-DI tend to fail completely. While UAV, video super-resolution approach, can still produce visually reasonable results, it is unable to recover fine details that are ambiguous or missing in the low-resolution inputs. In contrast, CineScale consistently generates high-quality videos with rich and faithful visual details. Best viewed ZOOMED-IN. Fig. 15. Video ablations in Wan I2V without tuning in resolution 960 1664. Although all variants can generate rough results. Our full method performs the best. Best viewed ZOOMED-IN. TABLE 11 Video comparison with DiT-based models in resolution 1920 3328. Only three metrics in VBench can be measured due to memory limitations. Compared to super-resolution post-processing setting Upscale-A-Video, CineScale also achieves competitive performance. Method Background Consistency Aesthetic Quality Imaging Quality LTX (2B) [36] Wan-DI [38] Upscale-A-Video [57] Ours (1.3B) 0.975 0.978 0.974 0.971 0.299 0.319 0.661 0. 0.302 0.314 0.680 0.679 We also conduct an ablation study for higher-resolution video generation. As discussed in the method part, we adopt latent space upsampling in video generation. Table 8 shows that our final setting achieves the best or second-best scores for all metrics."
        },
        {
            "title": "6 EXPERIMENTS FOR DIT STRUCTURE",
            "content": "We also conduct experiments based on the DiT-based diffusion model by Wan [38]. Minimal LoRA tuning is conducted on around 20000 free-access videos from Pexels. For text-to-video and image-to-video generation, We randomly sample 128 prompts from the VBench [70], [74] and evaluate through its metrics. Additionally, we simply conduct video-to-video quantitative experiment on ReCamMaster [75], camera control model tuned from Wan. Due to the heavy computational cost of high-resolution video generation, we only conduct quantitative comparisons on the T2V task and with SOTA methods. 6.1 Text-to-Video Generation We first do ablations for CineScale without tuning in resolution 960 1664, which is as four time as the training JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 Fig. 16. Video ablations in ReCamMaster V2V without tuning in resolution 960 1664. Without NTK-RoPE, repeated patterns are prone to occur due to errors in positional encoding. Although all variants can generate rough results. Our full method performs the best. TABLE 12 Video ablations in Wan I2V without tuning. Considering all aspects, our full method demonstrates the best overall performance. Method Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality Wan-DI w/o NTK-RoPE w/o Attention Scaling Ours 0.925 0.945 0.938 0.948 0.939 0.952 0.949 0.955 0.983 0.983 0.985 0.985 0.516 0.492 0.398 0.398 0.626 0.639 0.636 0.645 0.679 0.697 0.686 0. resolution. As shown in Fig. 12, Although all variants can generate rough results. Our full method performs the best. Then we compare FreeScale with other higher-resolution video generation methods: (i) Wan [38] direct inference (Wan-DI) (ii) LTX [36] (iii) SeedVR2 [73], and (iv) UpscaleA-Video [57] in resolution 960 1664 and 1920 3328. SeedVR2 is only applied in resolution 960 1664 due to its high memory occupation, and Upscale-A-Video is only applied in resolution 1920 3328 due to the fixed upscaling rate. Fig. 13 shows that although other baselines can produce reasonable results at moderately higher resolutions, they still suffer from varying degrees of blurriness. In contrast, CineScale generates high-quality videos with rich visual details. And in Fig. 14, at the resolution several times higher than those used during training, LTX and Wan-DI tend to fail completely. While UAV, video super-resolution approach, can still produce visually reasonable results, it is unable to recover fine details that are ambiguous or missing in the low-resolution inputs. In contrast, CineScale consistently generates high-quality videos with rich and faithful visual details. To ensure computational efficiency and fair comparison with other methods, all the above T2V experiments were conducted using the 1.3B version of our model. We further applied the optimized 14B version of the model to 4k resolution, achieving ultra-high-definition text-to-video generation  (Fig. 18)  . We observe that at 4k resolution, faces can be generated with remarkable clarity even when they occupy only small portion of the frame, and temporal consistency is also easier to maintain. Fig. 17. Local semantic editing for video generation. CineScale supports efficient editing by allowing users to preview results at low resolution while modifying high-resolution local semantics via prompts. 6.2 Image/Video-to-Video Generation CineScale also supports image-to-video generation. Due inference cost of the 14B model, we to the substantial conducted quantitative ablation studies using the untuned model at moderately high resolutions only. Both qualitative and quantitative results (Fig. 15 and Table 12) validate the effectiveness of our design. We further applied the optimized version of the model to 4k resolution, achieving ultra-highdefinition image-to-video generation  (Fig. 19)  . Beyond the original Wan model, CineScale also works effectively on the V2V model ReCamMaster [75], which features camera control. We conducted simple tests on sevJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 Fig. 18. 4k text-to-video generation. With minimal LoRA fine-tuning, CineScale can achieve 4k (2176 3840) text-to-video generation. We observe that at 4k resolution, faces can be generated with remarkable clarity even when they occupy only small portion of the frame, and temporal consistency is also easier to maintain. Best viewed ZOOMED-IN. Fig. 19. 4k image-to-video generation. With minimal LoRA fine-tuning, CineScale can achieve 4k (2176 3840) image-to-video generation. eral examples provided in the original paper, and Fig. 16 illustrates the effectiveness of our approach. 6.3 Local Semantic Editing for Video Generation The self-cascade upscaling paradigm in CineScale naturally supports an efficient and user-friendly editing workflow. Based on low-resolution previews, users can adjust prompts during high-resolution denoising to perform localized semantic edits. As shown in Figure 17, starting with low-resolution image depicting boy and dog cuddling on the grass, users can refine the scene through prompt editing once the initial composition is satisfactory. For example, the boy can be transformed into an elderly man, replaced with an African girl by altering gender and ethnicity, or modified with accessories like sunglasses. These changes are accurately reflected in the final high-resolution video. 7 CONCLUSION This study first introduces FreeScale, novel inference paradigm designed to enhance high-resolution generation capabilities in pre-trained diffusion models. By leveraging multi-scale fusion and selective frequency extraction, FreeScale effectively addresses common issues in highresolution generation, such as repetitive patterns and quality degradation. Then we extend FreeScale to CineScale to support DiT base video diffusion models, and enlarge the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 task scope from T2V to I2V and V2V. Experimental results demonstrate the superiority of CineScale in both image and video generation, surpassing existing methods in visual quality. Additional local control capabilities provide users with more flexibility. Eventually, our method achieves 4k video generation with only minimal LoRA fine-tuning. While CineScale shows strong capabilities, generating content at ultra-high resolutions still comes with considerable computational cost. Future work will focus on improving inference efficiency through architectural optimizations, denosing accelerations, and model compression techniques to make high-resolution generation more practical and accessible."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-035T), the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221-0012, MOE-T2EP20223-0002), and Netflix Eyeline Studios."
        },
        {
            "title": "REFERENCES",
            "content": "[1] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. uller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, arXiv preprint arXiv:2307.01952, 2023. J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li, Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. J. Wang, H. Yuan, D. Chen, Y. Zhang, X. Wang, and S. Zhang, Modelscope text-to-video technical report, 2023. [2] [3] [4] H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, Videocrafter2: Overcoming data limitations for highquality video diffusion models, 2024. [5] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [6] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 38363847. [7] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 68410 695. [8] Y. He, S. Yang, H. Chen, X. Cun, M. Xia, Y. Zhang, X. Wang, R. He, Q. Chen, and Y. Shan, Scalecrafter: Tuning-free higherresolution visual generation with diffusion models, in The Twelfth International Conference on Learning Representations, 2024. [9] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, Multidiffusion: Fusing diffusion paths for controlled image generation, arXiv preprint arXiv:2302.08113, 2023. [10] R. Du, D. Chang, T. Hospedales, Y.-Z. Song, and Z. Ma, Demofusion: Democratising high-resolution image generation with no $$$, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 61596168. [11] L. Huang, R. Fang, A. Zhang, G. Song, S. Liu, Y. Liu, and H. Li, Fouriscale: frequency perspective on training-free highresolution image synthesis, arXiv preprint arXiv:2403.12963, 2024. [12] L. Guo, Y. He, H. Chen, M. Xia, X. Cun, Y. Wang, S. Huang, Y. Zhang, X. Wang, Q. Chen et al., Make cheap scaling: selfcascade diffusion model for higher-resolution adaptation, arXiv preprint arXiv:2402.10491, 2024. [13] S. Chen, S. Wong, L. Chen, and Y. Tian, Extending context window of large language models via positional interpolation, arXiv preprint arXiv:2306.15595, 2023. [14] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, YaRN: Efficient context window extension of large language models, in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id= wHBfxhZu1u [15] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [16] H. Qiu, S. Zhang, Y. Wei, R. Chu, H. Yuan, X. Wang, Y. Zhang, and Z. Liu, Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion, arXiv preprint arXiv:2412.09626, 2024. [17] H. Yuan, S. Zhang, X. Wang, Y. Wei, T. Feng, Y. Pan, Y. Zhang, Z. Liu, S. Albanie, and D. Ni, Instructvideo: Instructing video diffusion models with human feedback, in CVPR, 2024. [18] C. Si, Z. Huang, Y. Jiang, and Z. Liu, Freeu: Free lunch in diffusion u-net, in CVPR, 2024. [19] Y. Wei, S. Zhang, Z. Qing, H. Yuan, Z. Liu, Y. Liu, Y. Zhang, J. Zhou, and H. Shan, Dreamvideo: Composing your dream videos with customized subject and motion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 65376549. [20] W. Fan, C. Si, J. Song, Z. Yang, Y. He, L. Zhuo, Z. Huang, Z. Dong, J. He, D. Pan et al., Vchitect-2.0: Parallel transformer for scaling up video diffusion models, arXiv preprint arXiv:2501.08453, 2025. [21] Y. Wang, X. Chen, X. Ma, S. Zhou, Z. Huang, Y. Wang, C. Yang, Y. He, J. Yu, P. Yang et al., Lavie: High-quality video generation with cascaded latent diffusion models, arXiv preprint arXiv:2309.15103, 2023. [22] C. Si, W. Fan, Z. Lv, Z. Huang, Y. Qiao, and Z. Liu, Repvideo: Rethinking cross-layer representation for video generation, arXiv 2501.08994, 2025. [23] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in Neural Information Processing Systems, vol. 33, pp. 68406851, 2020. [24] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, Advances in neural information processing systems, vol. 34, pp. 87808794, 2021. [25] W. Peebles and S. Xie, Scalable diffusion models with transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 41954205. [26] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, Video diffusion models, Advances in Neural Information Processing Systems, vol. 35, pp. 86338646, 2022. [27] Y. He, T. Yang, Y. Zhang, Y. Shan, and Q. Chen, Latent video diffusion models for high-fidelity video generation with arbitrary lengths, arXiv preprint arXiv:2211.13221, 2022. [28] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, Align your latents: High-resolution video synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22 56322 575. [29] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai, Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, arXiv preprint arXiv:2307.04725, 2023. [30] X. Wang, H. Yuan, S. Zhang, D. Chen, J. Wang, Y. Zhang, Y. Shen, D. Zhao, and J. Zhou, Videocomposer: Compositional video synthesis with motion controllability, NeurIPS, 2023. [31] H. Chen, M. Xia, Y. He, Y. Zhang, X. Cun, S. Yang, J. Xing, Y. Liu, Q. Chen, X. Wang et al., Videocrafter1: Open diffusion models for high-quality video generation, arXiv preprint arXiv:2310.19512, 2023. [32] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023. [33] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, et al., Lumiere: A. Ephrat, space-time diffusion model for video generation, arXiv preprint arXiv:2401.12945, 2024. J. Hur, Y. Li, T. Michaeli [34] Y. Jin, Z. Sun, N. Li, K. Xu, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, Pyramidal flow matching for efficient video generative modeling, 2024. [35] G. Team, Mochi 1, https://github.com/genmoai/models, 2024. [36] Y. HaCohen, N. Chiprut, B. Brazowski, D. Shalem, D. Moshe, E. Richardson, E. Levin, G. Shiran, N. Zabari, O. Gordon, P. Panet, S. Weissbuch, V. Kulikov, Y. Bitterman, Z. Melumian, and [57] S. Zhou, P. Yang, J. Wang, Y. Luo, and C. C. Loy, Upscale-a-video: Temporal-consistent diffusion model for real-world video superresolution, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 25352545. [58] J. Ren, W. Li, Z. Wang, H. Sun, B. Liu, H. Chen, J. Xu, A. Li, S. Zhang, B. Shao et al., Turbo2k: Towards ultra-efficient and highquality 2k video synthesis, arXiv preprint arXiv:2504.14470, 2025. [59] H. Qiu, M. Xia, Y. Zhang, Y. He, X. Wang, Y. Shan, and Z. Liu, Freenoise: Tuning-free longer video diffusion via noise rescheduling, arXiv preprint arXiv:2310.15169, 2023. [60] B. F. Labs, Flux.1 : An advanced state-of-the-art generative deep learning model, Black Forest Labs, Tech. Rep., 2024. [Online]. Available: https://flux1.io/ [61] R. Du, D. Liu, L. Zhuo, Q. Qi, H. Li, Z. Ma, and P. Gao, I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow, arXiv preprint arXiv:2410.07536, 2024. [62] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, Yarn: Efficient context window extension of large language models, arXiv preprint arXiv:2309.00071, 2023. [63] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, Roformer: Enhanced transformer with rotary position embedding, Neurocomputing, vol. 568, p. 127063, 2024. [64] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. uller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first international conference on machine learning, 2024. [65] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, Advances in neural information processing systems, vol. 30, 2017. [66] M. Bi nkowski, D. J. Sutherland, M. Arbel, and A. Gretton, Demystifying mmd gans, arXiv preprint arXiv:1801.01401, 2018. [67] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, Improved techniques for training gans, Advances in neural information processing systems, vol. 29, 2016. [68] L. Chai, M. Gharbi, E. Shechtman, P. Isola, and R. Zhang, Anyresolution training for high-resolution image synthesis, in European Conference on Computer Vision. Springer, 2022, pp. 170188. [69] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, Towards accurate generative models of video: new metric & challenges, arXiv preprint arXiv:1812.01717, 2018. [70] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., Vbench: Comprehensive benchmark suite for video generative models, arXiv preprint arXiv:2311.17982, 2023. [71] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 40154026. [72] A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, Adversarial diffusion distillation, in European Conference on Computer Vision. Springer, 2024, pp. 87103. [73] J. Wang, S. Lin, Z. Lin, Y. Ren, M. Wei, Z. Yue, S. Zhou, H. Chen, Y. Zhao, C. Yang, X. Xiao, C. C. Loy, and L. Jiang, Seedvr2: Onestep video restoration via diffusion adversarial post-training, 2025. [74] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang, Y. Wang, X. Chen, Y.-C. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, Vbench++: Comprehensive and versatile benchmark suite for video generative models, arXiv preprint arXiv:2411.13503, 2024. [75] J. Bai, M. Xia, X. Fu, X. Wang, L. Mu, J. Cao, Z. Liu, H. Hu, X. Bai, P. Wan et al., Recammaster: Camera-controlled generative rendering from single video, arXiv preprint arXiv:2503.11647, 2025. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST O. Bibi, Ltx-video: Realtime video latent diffusion, arXiv preprint arXiv:2501.00103, 2024. [37] W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang et al., Hunyuanvideo: systematic framework for large video generative models, arXiv preprint arXiv:2412.03603, 2024. [38] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, J. Wang, J. Zhang, J. Zhou, J. Wang, J. Chen, K. Zhu, K. Zhao, K. Yan, L. Huang, M. Feng, N. Zhang, P. Li, P. Wu, R. Chu, R. Feng, S. Zhang, S. Sun, T. Fang, T. Wang, T. Gui, T. Weng, T. Shen, W. Lin, W. Wang, W. Wang, W. Zhou, W. Wang, W. Shen, W. Yu, X. Shi, X. Huang, X. Xu, Y. Kou, Y. Lv, Y. Li, Y. Liu, Y. Wang, Y. Zhang, Y. Huang, Y. Li, Y. Wu, Y. Liu, Y. Pan, Y. Zheng, Y. Hong, Y. Shi, Y. Feng, Z. Jiang, Z. Han, Z.- F. Wu, and Z. Liu, Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [39] J. Teng, W. Zheng, M. Ding, W. Hong, J. Wangni, Z. Yang, and J. Tang, Relay diffusion: Unifying diffusion process across resolutions for image synthesis, arXiv preprint arXiv:2309.03350, 2023. [40] E. Hoogeboom, J. Heek, and T. Salimans, simple diffusion: Endto-end diffusion for high resolution images, in International Conference on Machine Learning. PMLR, 2023, pp. 13 21313 232. [41] J. Ren, W. Li, H. Chen, R. Pei, B. Shao, Y. Guo, L. Peng, F. Song, and L. Zhu, Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks, arXiv preprint arXiv:2407.02158, 2024. [42] S. Liu, W. Yu, Z. Tan, and X. Wang, Linfusion: 1 gpu, 1 minute, 16k image, 2024. [43] Q. Zheng, Y. Guo, J. Deng, J. Han, Y. Li, S. Xu, and H. Xu, Anysize-diffusion: Toward efficient text-driven synthesis for any-size hd images, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 7, 2024, pp. 75717578. [44] J. Cheng, P. Xie, X. Xia, J. Li, J. Wu, Y. Ren, H. Li, X. Xiao, M. Zheng, and L. Fu, Resadapter: Domain consistent resolution adapter for diffusion models, 2024. [45] M. Haji-Ali, G. Balakrishnan, and V. Ordonez, Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 66036612. [46] M. Lin, Z. Lin, W. Zhan, L. Cao, and R. Ji, Cutdiffusion: simple, fast, cheap, and strong diffusion extrapolation method, arXiv preprint arXiv:2404.15141, 2024. [47] Y. Lee, K. Kim, H. Kim, and M. Sung, Syncdiffusion: Coherent montage via synchronized joint diffusions, Advances in Neural Information Processing Systems, vol. 36, pp. 50 64850 660, 2023. [48] Z. Jin, X. Shen, B. Li, and X. Xue, Training-free diffusion model adaptation for variable-sized text-to-image synthesis, Advances in Neural Information Processing Systems, vol. 36, pp. 70 84770 860, 2023. [49] J. Hwang, Y.-H. Park, and J. Jo, Upsample guidance: Scale up diffusion models without training, arXiv preprint arXiv:2404.01709, 2024. [50] B. Cao, J. Ye, Y. Wei, and H. Shan, Ap-ldm: Attentive and progressive latent diffusion model for training-free high-resolution image generation, arXiv preprint arXiv:2410.06055, 2024. [51] S. Zhang, Z. Chen, Z. Zhao, Y. Chen, Y. Tang, and J. Liang, Hidiffusion: Unlocking higher-resolution creativity and efficiency in pretrained diffusion models, in European Conference on Computer Vision. Springer, 2024, pp. 145161. [52] Y. Kim, G. Hwang, J. Zhang, and E. Park, Diffusehigh: Trainingfree progressive high-resolution image synthesis through structure guidance, arXiv preprint arXiv:2406.18459, 2024. [53] Z. Lin, M. Lin, M. Zhao, and R. Ji, Accdiffusion: An accurate method for higher-resolution image generation, arXiv preprint arXiv:2407.10738, 2024. [54] X. Liu, Y. He, L. Guo, X. Li, B. Jin, P. Li, Y. Li, C.-M. Chan, Q. Chen, W. Xue et al., Hiprompt: Tuning-free higher-resolution generation with hierarchical mllm prompts, arXiv preprint arXiv:2409.02919, 2024. [55] G. Kim, H. Kim, H. Seo, D. U. Kang, and S. Y. Chun, Beyondscene: Higher-resolution human-centric scene generation with pretrained diffusion, in European Conference on Computer Vision. Springer, 2024, pp. 126142. [56] X. Wang, L. Xie, C. Dong, and Y. Shan, Real-esrgan: Training real-world blind super-resolution with pure synthetic data, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 19051914. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "8 BIOGRAPHY SECTION",
            "content": "17 Haonan Qiu is currently Ph.D. student at MMLab@NTU, Nanyang Technological University (NTU), advised by Prof. Ziwei Liu. Prior to that, he obtained his bachelors degree from The Chinese University of Hong Kong, Shenzhen (CUHKSZ). His research interests mainly focus on various research topics related to video diffusion models, including longer video generation, higher-resolution generation, efficient generation, motion control, and ID personalization. He is awarded the AISG PhD Fellowship from 2022 to 2025. Ning Yu is lead research scientist at Netflix Eyeline Studios, leading efforts in visual and multimodal generative AI for filmmaking and visual effects. He previously worked with Salesforce, NVIDIA, and Adobe, and earned joint Ph.D. degree from the University of Maryland and Max Planck Institute for Informatics. He is recipient of the CSAW Europe Best Paper Finalist, Twitch (Amazon) Research Fellowship, Qualcomm Innovation Fellowship Finalist x2, and SPIE Best Student Paper Finalist. Ziqi Huang is currently Ph.D. student at MMLab@NTU, Nanyang Technological University (NTU), supervised by Prof. Ziwei Liu. She received her Bachelors degree from NTU in 2022. Her current research interests include visual generation and evaluation. She is awarded Google PhD Fellowship 2023, and is recipient of the 2025 Apple Scholars in AI/ML PhD Fellowship. Paul Debevec is the Chief Research Officer at Netflixs Eyeline Studios and an Adjunct Research Professor at the USC Institute for Creative Technologies. Pauls work in technology for visual effects and virtual production seen in movies from The Matrix to Avatar and Gravity has been recognized with two Academy Awards, the SMPTE Progress Medal, and Lifetime Achievement Emmy Award. Ziwei Liu is currently Nanyang Associate Professor at Nanyang Technological University. His research revolves around machine learning, computer vision and graphics. He is the recipient of PAMI Mark Everingham Prize, MIT Technology Review Innovators under 35 Asia Pacific, ICBS Frontiers of Science Award, CVPR Best Paper Award Candidate, Asian Young Scientist Fellowship and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV, NeurIPS and ICLR, and an Associate Editor of IJCV."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Netflix Eyeline Studios",
        "Scanline VFX"
    ]
}