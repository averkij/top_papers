{
    "paper_title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
    "authors": [
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Archiki Prasad",
        "Jaemin Cho",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 6 7 9 0 . 4 0 5 2 : r Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems Zaid Khan, Elias Stengel-Eskin, Archiki Prasad, Jaemin Cho, Mohit Bansal University of North Carolina at Chapel Hill {zaidkhan, esteng, archiki, jmincho, mbansal}@cs.unc.edu"
        },
        {
            "title": "Abstract",
            "content": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of system have been useful in fields ranging from reinforcement learning (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for mathematical reasoning as problem generators for stress-testing models. However, prior work has been limited to automatically constructing abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced mathematics problems. We operationalize the task of automatically constructing EFAs as program synthesis task, and develop EFAGen, which conditions large language model (LLM) on seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. Through experiments, we demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs, such as finding problem variations that are harder or easier for learner to solve, as well as data generation."
        },
        {
            "title": "Introduction",
            "content": "In many fields, experts abstract specific instances into general procedures that can generate wide range of related cases. For example, physicists distill observations of falling objects into equations of motion capable of predicting trajectories under varying initial conditions (Smith, 2024). This ability is not limited to certain domain experts: in fact, the ability to infer underlying compositional structures from surface forms is core component of human language and intelligence (Chomsky, 1957; Montague et al., 1970; Partee, 2008; Lake et al., 2017). The outcome of this process of abstraction is often data-generating program whose execution is controlled by parameters, such as gridworld generator that produces different world layouts given different configuration files. In fields such as reinforcement learning, notable instances of data generating programs such as Holodeck (Yang et al., 2024) and BabyAI (Chevalier-Boisvert et al., 2018) have become important parts of the research ecosystem for their capability to endlessly generate well-formed randomized task instances. 1Code, models, and data at zaidkhan.me/EFAGen 1 Figure 1: Left: The generative process underlying computational math problems, where the different instances share the same underlying problem-solving logic (function) but differ in parameter values. We introduce executable functional abstractions (EFAs) to model this latent structure. Right: we study the task of inferring EFAs; i.e., recovering the underlying problem-solving function and parameters from math problems expressed in natural language. We introduce Executable Functional Abstraction (EFA), programmatic abstraction that encapsulates the logic of math problem in parameterized form and enables the automated sampling of problems variants. Although similar abstractions have been used in other domains, automatic construction of EFAs for generating fresh, diverse math problems remains largely unexplored. The property enabling the construction of EFAs for mathematics is that many math problems are surface form of more abstract deep structure. For example, consider the problem in Fig. 1 (left), which asks for positive integers and with greatest common divisor (GCD) of 6 and least common multiple (LCM) of 126, seeking the minimum value of + n, which we denote as LcmGcdMinSum(gcd=6, lcm=126). This specific problem is special case of more general problem LcmGcdMinSum(gcd=g, lcm=l) where l, can be any natural numbers. Inferring an EFA requires transforming the LcmGcdMinSum(gcd=6, lcm=126) problem about specific pair of numbers into program that generates valid LcmGcdMinSum problems with varying parameters while implementing general solution procedure that solves any specific instances of the general problem, such as LcmGcdMinSum(gcd=7, lcm=42). In this paper, we explore the automatic creation of EFAs for higher-level math problems. This leads us to our central research question: How can we automatically transform static math problems into their corresponding executable functional abstractions (EFAs)? The task of automatically transforming static math problems into an EFA is nontrivial. Recent work has made progress with grade-school level math problems (Zhang et al., 2024; Mirzadeh et al., 2025) by taking advantage of the simple computational graphs of their solutions. Higher-level problems with more complex computational graphs have thus far required human involvement to lift problems into functional forms (Shah et al., 2024; Srivastava et al., 2024). An automated approach for mathematical problems more complex than grade-school arithmetic has not been developed. Such automatic construction of EFAs requires simultaneously solving multiple subproblems: identifying which numerical values should be parameterized, discovering the constraints between these parameters to maintain problem validity, abstracting the solution procedure to handle all valid parameterizations, and ensuring mathematical correctness across the entire parameter space. For example, in Fig. 1, and are not parameters of the problem despite already being abstract variables, as they are dependent on the values of the gcd and lcm given. Nor can the gcd or lcm values be allowed to vary arbitrarily. Some parameterizations of the gcd and lcm may yield trivial problems (if the gcd is 1 and the lcm is prime), while other parameterizations are simply invalid (such as gcd > lcm or gcd and lcm being relatively prime). We operationalize the task of inferring EFAs as program synthesis task using large language models (LLMs). Our method, EFAGen, conditions an LLM on static seed math problem 2 and its step-by-step solution to generate candidate programs implementing an EFA for the seed math problem. To generate correct EFA, the program synthesizer must identify which numerical values in the static problem should be treated as parameters, determine appropriate sampling distributions for these parameters, and encode the constraints between them to ensure problem validity  (Fig. 1)  . We formalize mathematical properties wellformed EFA must possess as unit tests that can automatically detect violations of these properties. We can then adopt an overgenerate-then-filter approach (Li et al., 2022), first generating large number of candidate programs implementing EFAs for seed problem, and then rejecting EFAs that fail our tests. Finally, we conduct series of experiments probing properties of the EFAs constructed by EFAGen, demonstrating the utility of model-written EFAs and testing whether LLMs can be trained to be more successful writers of EFAs. We first show that EFAs have properties signaling their coherence. EFAs are faithful to the seed problem they were derived from: the verifiable problems sampled from an EFA help model solve the seed problem the EFA was constructed from. Similarly, the verifiable problems produced by an EFA are learnable: when sampling train and test set from the same EFA, model is able to improve on the test set when given step-by-step solutions of the training problems. Because EFAs allow us to sample large number of verified problems, we can also use them to create more instances of problem that model struggles with, or to refresh static dataset by first constructing an EFA from problem that the model already can solve, and then sampling fresh variants using the EFA that the model struggles with, thereby stresstesting models on similar data. We show that EFAGen can be applied to multiple sources of competition-level mathematics problems to automatically construct EFAs. This applicability to multiple kinds of problems allows us to use EFAs as data augmentation for mathematical problem solving on MATH-Hard (Hendrycks et al., 2021) and FnEval (Srivastava et al., 2024), where we show EFA-based augmentation yields consistent improvements. Finally, we show that models can improve at inducing EFAs from math problems by using the execution feedback from automatic tests in EFAGen as rewards in simple reinforced self-training scheme (Zelikman et al., 2022; Singh et al., 2023; Dong et al., 2023). Our contributions in this paper are as follows: We formalize the notion of Executable Functional Abstractions (EFAs) in Sec. 2.2, and develop EFAGen (Sec. 2.3, Fig. 2), an approach that automatically infers EFAs from advanced math problems, providing scalable approach to generate verifiable problem variants with automatic tests for validity and correctness. We show that these tests can be used as reward signal for training LLMs to improve at the task of inferring EFAs from static problems (Sec. 3.1). We show that EFAGen generates faithful (Sec. 3.2) and learnable (Sec. 3.3) EFAs and can automatically infer EFAs from diverse sources of math data (Sec. 3.5), and that EFAs can be used as data augmentation (Sec. 3.4)."
        },
        {
            "title": "2 Executable Functional Abstractions (EFAs)",
            "content": "Our goal is to automatically convert math problems with static numerical values into parameterized abstractions that can generate variants of the original problems. We refer to these parameterized abstractions as Executable Functional Abstractions (EFAs). EFAs enable the systematic generation of new problem instances by varying numerical parameters while preserving the underlying problem-solving logic. We operationalize the task of inferring an EFA for static math problem as program synthesis task where the goal is to write class implementing the EFA. We use LLMs to generate many candidate EFA implementations for static problem and use suite of automatic unit tests to filter the candidates by rejecting mathematically unsound ones. Below, we describe the desired properties of EFAs (Sec. 2.1), how an EFA is represented as Python class (Sec. 2.2), and how we infer EFAs from static math problems using LLMs (Sec. 2.3). 3 Figure 2: Left: Representation of an executable functional abstraction (EFA) as Python class. Right: Overview of EFAGen, method for automatically inferring EFAs from math problem. In EFAGen, we (a) over-generate multiple EFA candidates with an LLM and (b) filter out invalid candidates that fail automated tests. The EFA can generate new problem variants by sampling parameters and executing the solver. Full code is in Appendix A."
        },
        {
            "title": "2.1 Desired Properties of Abstractions",
            "content": "An effective abstraction of math problem must support variation, preserve validity, and enable automated problem-solving. We identify three core properties of an EFA: Structured parameter space: The abstraction should define set of parameters that characterize the problem and specify valid relationships among them. This includes identifying which parameters are independent, how dependent parameters are derived, and what constraints must be satisfied to ensure valid problem instances. Such structure enables systematic variation, ensuring that changes to parameters yield meaningful variants with potentially different solutions. Procedural generation of instances: The abstraction should support random sampling of set of valid parameters (e.g., EFA.sample() in Sec. 2.2) and converting the abstract problem into natural language form (e.g., EFA.render() in Sec. 2.2), to help users generate valid problem instances by sampling parameter values within defined constraints. These constraints are problem-specific and crucial for generating diverse but coherent examples. Executable solution logic: The abstraction should include method (e.g., EFA.solve() in Sec. 2.2) that computes the correct answer for any valid parameter configuration. This solution logic is typically derived from the chain-of-thought (Wei et al., 2022) used for the static version of the problem and can be implemented as an executable program. 2."
        },
        {
            "title": "EFA as a Python Class",
            "content": "As illustrated in Fig. 2 (a), each EFA is implemented as Python class that encapsulates the logic of math problem in parameterized form. The class defines list of parameters along with three key methods: EFA.sample() parameters: Samples valid set of parameters representing problem variants, respecting all constraints specified in the abstraction. EFA.render(parameters) question: Provides natural language problem statement, given specific (sampled) parameter set. This ensures that each generated instance is presented in format suitable for human or model consumption. In most cases, this involves reusing the problem statement of the seed instruction and mutating the numerical values to be consistent with the given parameters. EFA.solve(parameters) answer: Computes the correct answer for given parameter configuration. The solution is expressed as numerical expression derived through deterministic computations over the parameters. The solver does not need to access the natural language problem statement, as the solution is only dependent on the parameterization of the problem, which is structured object. These methods operationalize the abstraction and enable automated generation, rendering, and evaluation of math problems. 4 EFAGen: Inferring EFAs from Math Problems"
        },
        {
            "title": "2.3\nWe introduce EFAGen, a framework for automatically constructing EFAs from static math\nproblems. Given a problem statement and its solution procedure (typically expressed\nas chain-of-thought reasoning), EFAGen uses a large language model (LLM) to generate a\ncandidate EFA implementation that captures the logic and structure of the original problem.\nThis process relies on supervision that is readily available in many math datasets.\nSince generating correct and robust code is challenging for LLMs, EFAGen adopts an\novergenerate-and-filter approach inspired by AlphaCode (Li et al., 2022). As described in\nFig. 2 (a), for each problem, we sample N (e.g., 50) EFA candidates and apply a suite of\nautomated tests to discard invalid abstractions. EFAGen uses the following tests to validate\ncandidate EFAs, as illustrated in Fig. 2 (b):\n• is extractable(response): Verifies that the class contains all required methods.\n• is executable(EFA): Confirms that the class can be instantiated and executed without\nerrors, and methods like EFA.sample() and EFA.solve() can be called without errors.\n• has dof(EFA): Ensures that sampled parameters differ, rejecting EFAs with zero degrees",
            "content": "of freedom that cannot produce new problems. is single valued(EFA): Confirms that identical parameters yield equivalent solutions, rejecting impermissible implementations including multivalued functions or logically incoherent abstractions. matches original(EFA, orig params, orig sol): Validates that the abstraction, when instantiated with the original parameters, produces the original problem and solution. This serves as cycle-consistency or soundness check. Any program that fails these tests cannot logically be valid implementation of an EFA. EFAGen enables generation of EFAs at scale, as shown in Fig. 2 (c), as large numbers of candidate EFAs can be generated and filtered automatically. Over time, these tests can also be used to fine-tune LLMs toward better abstraction generation, such as with reinforced self-training (Singh et al., 2023; Dong et al., 2023) or reinforcement learning with verifiable rewards (Lambert et al., 2024)."
        },
        {
            "title": "3 Experiments & Results",
            "content": "Datasets. Throughout this section, we use the following datasets in our experiments: MATH (Hendrycks et al., 2021). Competition math dataset with test set that consists of 5000 math problems described in text comprising different categories and five levels of difficulty. As we will show in Sec. 3.1, LLMs struggle with task of EFA generation and therefore, we improve their performance by training on the EFA generation task using the MATH train set consisting of 7500 problems (similar distribution as the test set). MATH-Hard. With our focus on challenging math problems, this is subset of MATH test problems of the highest difficulty (level 5) across all categories (1387 problems). Metrics. To evaluate the performance of LLMs we use the following metrics: EFA Success Rate. We measure the ability of LLMs to generate valid, high-quality EFAs (defined in Sec. 2.1) as the frequency (%) of EFAs generated that past all the diagnostic tests outlined in Sec. 2.3. Pass@k Rate (%). Following Chen et al. (2021), we measure the ability of LLMs to solve math problems by sampling 25 generations with temperature sampling and estimating the unbiased pass@k rate, i.e., the likelihood that out of generated solutions any one corresponds to the correct answer."
        },
        {
            "title": "3.1 Self-Improvement: LMs Can Improve at Inferring EFAs With Execution Feedback\nInferring valid EFAs across diverse math problems is challenging, especially as the diffi-\nculty and complexity of topics increases. For instance, as shown in Fig. 3, Llama3.1-8B-\nInstruct (Llama Team, 2024) struggles to generate valid EFAs for Level 5 problems and for\ntopics such as Precalculus in the MATH dataset, where it is only able to infer valid EFAs",
            "content": "5 Figure 3: LLMs can use our tests to self-improve at inferring EFAs. We plot the percentage of constructed EFAs passing all tests across iterations of self-training, grouped by MATH problem difficulty (left) and by problem category (right). Harder difficulty levels and problem categories are harder to infer EFAs for and improve more during training. for 35% of Precalculus questions. In Sec. 2, we introduce number of unit tests (i.e., verifiable rewards) that indicate whether generated EFA is valid. Here, we show that we can train models to improve on inferring valid EFAs by self-training according to these tests. Specifically, we use rejection-finetuning approach (Zelikman et al., 2022; Singh et al., 2023; Dong et al., 2023), in which we sample EFA candidates from model and filter according to our rewards to construct training dataset of correct examples. We begin with the MATH training set (7,500 problems) and sample 10 candidate EFAs per problem. Candidates failing any of the reward checks are discarded. The remaining valid examples form dataset for supervised fine-tuning. This process sampling, filtering, and retraining is repeated over 5 iterations (see Appendix B.2 for details). We report the EFA success rates across iterations in Fig. 3, where we group by difficulty levels (left) and by annotated problem category (right). Success rates steadily improve over training iterations, especially for harder problems. At iteration 0 (before training), we observe that harder problems (e.g., Level 5) are also harder to infer EFA for, with EFA success rates 17% lower for Level 5 than Level 1 problems. Similarly, certain categories like Intermediate Algebra, Counting and Probability are harder to infer EFAs for. These domains generally see the most significant increases from training. Between iteration 1 and iteration 5, the Intermediate Algebras EFA success rate showed the most significant increase, rising from 52.93% to 81.38%, and Geometry improved from 65.71% to 85.71%. Additionally, the pass rate for Level 5 problems increased from 65.95% to 78.73%. These changes indicate substantial improvements in the models ability to infer EFA across these dataset slices. The final model trained for 5 iterations becomes the basis for our EFAGen method."
        },
        {
            "title": "3.2 Faithfulness: EFAs Capture the Reasoning Required to Solve the Seed Problem",
            "content": "To evaluate the faithfulness of EFAs, we ask: can the generated variant problems improve models solve rate on the original seed problem? We select all of problems from MATH-Hard for which Llama3.1-8B-Instructs pass@5 rate < 50% and for which EFAGen can successfully infer an EFA using the gold solution.2 For each problem, we sample additional problem variants (we ensure their parameters differ from the seed problem) until Llama3.1-8BInstruct solves one correctly. We then check if Llama3.1-8B-Instruct can solve the original problem, given the variant and its solution as an in-context example. Results in Table 1 (left) show 23.07% absolute improvement in pass@1 rate, indicating that EFA-generated variants can teach model the problem-solving reasoning needed for the seed problem. 2Based on the intuition that testing for faithfulness requires an EFA (i.e., requires problem that can be solved in principle) but improving requires problem that is not solved 100% of the time. 6 Faithfulness (Sec. 3.2): EFA helps on the original problem Learnability (Sec. 3.3): EFA helps on its variants Initial Pass@1 +Data from EFA 38.73 (+23.07%) Initial Pass@1 +Data from EFA 31.23 (+16.65%) Sample Size Sample Size 1,000 14.58 15.66 Table 1: EFAs faithfully capture the solutions of the problems they were derived from (left), and problem variants constructed by EFAs share learnable structure (right). Left: Giving solutions to problems variants from an EFA as in-context examples nearly doubles the solve rate of an LLM on the seed problem the EFA was derived from. Right: Giving solutions to problem variants from an EFA as in-context examples helps an LLM solve holdout set of variants from the same EFA. See Sec. 3.2 and Sec. 3.3 for details."
        },
        {
            "title": "3.3 Learnability: Performance on Generated Problems Should Increase with Experience",
            "content": "An effective problem abstraction should enable model to solve both the original seed problem and its variants. To evaluate this, we test whether training on EFA-generated problem variants helps model solve additional variants that are drawn from the same EFA but are different from the seed problem. We sample 1,000 EFAs inferred from the MATH-Hard test set and generate one new variant per EFA, forming held-out test set. For each EFA, we also identify one variant that Llama3.18B-Instruct solves correctly. We then test Llama3.1-8B-Instructs performance on the held-out test set, with and without access to that solved variant as an in-context example. As shown in Table 1, access to one correctly-solved variant improves the models pass rate on other variants by 16.65% on average. This demonstrates that reasoning learned from one variant reliably transfers to others within the same abstraction."
        },
        {
            "title": "3.4 Augmentation: EFAs Are Effective at Expanding Static Math Datasets",
            "content": "While high-quality math datasets exist, these are often expensive to construct. EFAGen offers scalable solution by generating diverse, faithful problem variants through EFAs, thereby augmenting existing datasets. To demonstrate this, we fine-tune Llama3.1-8B-Base using EFA-generated data derived from the MATH training set. Concretely, we annotate 7,500 training problems with step-by-step reasoning from teacher model (Llama3.1-8B-Instruct). We ensure that the reasoning is correct by filtering out the reasoning that yields incorrect answers. Then, for each of the 7,500 problems, we construct an EFA and sample one problem variant. We compare two training settings. In the first setting, we use only the teacherlabeled seed data. In the second, we augment the seed data by adding EFA-generated examples in 1:1 ratio. We perform experiments with both 33% (2,500) and 100% (7,500) of the seed data and evaluate performance on three benchmarks: MATH-500 split (Lightman et al., 2023) and the November and December splits of FnEval, each containing perturbed versions of MATH problems. See Appendix B.4 for hyperameter details. As shown in Table 2, EFA-based augmentation leads to consistent improvements across all evaluation metrics: Pass@1, Pass@10 rate, and Majority@25 (Wang et al., 2022). For example, in the 33% seed setting, Pass@1 improves by +1.9 on MATH-500 and by +2.2 on both FnEval splits. In the full 100% seed setting, the gain still holds, underscoring the value of EFAs in enhancing data quality and model performance."
        },
        {
            "title": "3.5 Generality: EFAGen Can Work Across Diverse Math Domains",
            "content": "Importantly, EFAGen generalizes beyond the distribution of questions in the MATH dataset. As detailed in Fig. 4, our approach successfully infers EFAs across various math sources from the NuminaMath dataset (Li et al., 2024) ranging from grade-school problems (GSM8K) to national/international competitions (e.g., AMC, AIME, IMO). This demonstrates the broad applicability of EFAs for structuring and scaling math data across diverse domains. We generally see that easier math domains like GSM8K are easier to infer EFAs for than harder domains like AIME or Olympiad problems; nevertheless, EFAGen can infer some successful EFAs even on the hardest domain. 7 MATH-500 FnEval (November Split) FnEval (December Split) Training Data Pass @ 1 Pass @ 10 Maj @ 25 Pass @ 1 Pass @ 10 Maj @ 25 Pass @ 1 Pass @ 10 Maj @ 25 MATH (33%) +EFA (1:1) MATH (100%) +EFA (1:1) 22.4 24.3 (+1.9%) 24.3 26.1 (+1.8%) 56.4 58.3 (+1.9%) 57.8 60.6 (+2.8%) 36.8 38.8 (+2.0%) 37.0 40.4 (+3.4%) 24.5 26.7 (+2.2%) 26.8 29.3 (+2.5%) 55.3 59.2 (+3.9%) 58.6 60.1 (+1.5%) 39.6 41.8 (+2.2%) 43.1 44.3 (+1.2%) 24.4 26.6 (+2.2%) 26.5 28.8 (+2.3%) 55.4 57.3 (+1.9%) 57.6 59.6 (+2.0%) 39.3 41.2 (+1.9%) 41.5 43.7 (+2.2%) Table 2: EFA-based data augmentation is consistently effective. Comparison with and without synthetic data augmentation using problems drawn from generated EFAs. The table shows performance across MATH-500 and FnEval benchmarks (November and December snapshots). When augmenting, we use 1:1 ratio of examples drawn from training data vs. from an EFA, and report results using 33% of the MATH train set and 100% of the train set. Figure 4: EFAGen can infer EFAs for diverse sources of math problems. Here, we show the results of applying EFAGen to infer EFAs for the NuminaMath (Li et al., 2024) dataset, which contains mix of math problems from diversity of sources ranging from grade school mathematics (GSM8K) to national/international olympiads (olympiads). EFAGen achieves nonzero success rate across all sources of problems."
        },
        {
            "title": "3.6 Adversarial Search: EFAGen Can Find Hard Problem Variants",
            "content": "EFAs can also be used for evaluation or as source of targeted training data by finding hard instances that models struggle with. To demonstrate this, we randomly sample problems from the MATH training that are correctly solved by strong model (GPT4o); we sample = 20 of both Level 1 (easiest) and Level 5 (hardest) problems. For each problem, we construct an EFA using EFAGen and then sample 50 variants from the EFA. We attempt to solve each variant with GPT-4o, and measure for what fraction of problems we are able to find variants among the 50 samples that GPT-4o cannot solve. This is an estimate of the probability that we can use an EFA to sample problems that cannot be solved by the model, even when the seed problem is solvable. The results are shown in Fig. 5 where we see that there is non-zero probability of finding hard variants to given problem, even for easy problems (i.e., Level 1 in MATH) and with strong model like GPT-4o. Figure 5: EFAs can find harder variants of problems. We infer an EFA for sample of Level 1 (easiest) and Level 5 (hardest) seed problems GPT-4o solves correctly, and generate variants of each problem. We plot the percentage of seed problems for which variant that GPT-4o solved incorrectly was found."
        },
        {
            "title": "4 Related Work",
            "content": "Symbolic Approaches to Math Reasoning. distinct line of prior work has focused on assessing the true mathematical reasoning capabilities of LLMs, specifically by measuring the reasoning gap or the drop in math reasoning performance after perturbing questions in existing datasets (Shi et al., 2023; Zhou et al., 2025; Huang et al., 2025; Ye et al., 2025). One prominent approach is to generate different or difficult math questions conditioned on an existing question but test skills by employing frontier models (Zhang et al., 2024; Patel et al., 2025) or human annotators (Srivastava et al., 2024; Shah et al., 2024; Huang et al., 2025). For instance, Srivastava et al. (2024) propose FnEval dataset by manually functionalizing select problems from the MATH dataset (Hendrycks et al., 2021) that can be subsequently used to sample multiple distinct math problems testing similar skills (albeit with different numerical variables). Similarly, Mirzadeh et al. (2025) release the GSM-Symbolic dataset that augments the existing GSM8K dataset (Cobbe et al., 2021) with templates containing placeholders for several numeric and textual variables and can be used to sample distinct math word problems for robust evaluation of LLMs reasoning abilities. In contrast, to this line of work requiring expensive annotations from humans or frontier models (thereby, hindering scalability) and tailored to specific, predefined math datasets; we propose EFAGen that automatically functionalizes any math problem using relatively small language models making it widely-applicable and scalable, i.e., able to sample potentially infinite number of related math problems from any distribution or dataset. Moreover, the aforementioned prior work only focuses on the robust evaluation of LLMs, whereas we extend the concept of abstraction for downstream applications via training, as shown in Sec. 3.4. Data and Environment Generation. Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2023), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2024), adding multiple reasoning strategies (Yue et al., 2024), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by Khan et al. (2025), who define data generation agents that can generate specific data based on particular models weaknesses, covering math and several other domains. Finally, past work that has augmented seed dataset (e.g., Yu et al. (2024); Yue et al. (2024)) has done so by modifying problems in the surface form, whereas our method first infers latent structure and then creates problems by sampling from the structure. In contrast, EFAGen focuses on generating similar examples of existing data by inferring an underlying structure from an example; we show that this has applications to data generation for augmentation but also for stress-testing or measuring the performance gap of models on similar problems."
        },
        {
            "title": "5 Conclusion\nWe introduce Executable Functional Abstraction (EFA), a math abstraction that encapsulates\nthe logic of a math problem in a parameterized form, enabling the automated sampling\nof variant problems. Building on this definition, we propose EFAGen, a framework that\ninfers EFAs via program synthesis using large language models (LLMs) that we train on\neasy-to-compute EFA rewards. Concretely, our approach uses an LLM to over-generate EFA\ncandidates, which are then filtered using a suite of diagnostic tests that verify their validity.\nWe demonstrate that EFAGen can successfully infer EFAs from diverse math problems—and\nthat incorporating execution feedback as a reward in a simple self-training scheme further\nimproves its performance. Moreover, models trained on EFA-based math problems not only\nperform better on the generated variants but also improve accuracy on the original seed\nproblems. Finally, we show that EFAs provide a scalable solution for augmenting diverse\nproblem variants across various math datasets.",
            "content": ""
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-12356, Microsoft Accelerate Foundation Models Research (AFMR) grant program, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018. Noam Chomsky. Syntactic Structures. Mouton, The Hague, 1957. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun Shum, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=m7p5O7zblY. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, et al. Math-perturb: Benchmarking llms math reasoning abilities against hard perturbations. arXiv preprint arXiv:2502.06453, 2025. Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. Dataenvgym: Data generation agents in teacher environments with student feedback. In The Thirteenth International Conference on Learning Representations, 2025. Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training, December 2024. URL http://arxiv.org/abs/2411.15124. arXiv:2411.15124 [cs]. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. 10 Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084, 2024. Llama Team. The Llama 3 Herd of Models, 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Seyed Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. In The Thirteenth International Conference on Learning Representations, 2025. Richard Montague et al. Universal grammar. 1974, pp. 22246, 1970. Barbara Partee. Compositionality in formal semantics: Selected papers. John Wiley & Sons, 2008. Arkil Patel, Siva Reddy, and Dzmitry Bahdanau. How to get your llm to generate challenging problems for evaluation. arXiv preprint arXiv:2502.14678, 2025. Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Nan Rosemary Ke, Michael Curtis Mozer, Yoshua Bengio, Sanjeev Arora, and Anirudh Goyal. AI-assisted generation of difficult math questions. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 3121031227. PMLR, 2023. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. George Smith. Newtons Philosophiae Naturalis Principia Mathematica. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2024 edition, 2024. Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, and Sooraj Thomas. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. 2022. URL http://arxiv.org/abs/2203.11171. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023. 11 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: In Proceedings of the 2020 Conference on State-of-the-art natural language processing. Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.emnlp-demos.6. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1622716237, 2024. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= Tn5B6Udq3E. Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhehao Zhang, Jiaao Chen, and Diyi Yang. Darg: Dynamic evaluation of large language models via adaptive reasoning graph. arXiv preprint arXiv:2406.17271, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, and Beidi Chen. Gsm-infinite: How do your llms behave over infinitely increasing context length and reasoning complexity? arXiv preprint arXiv:2502.05252, 2025."
        },
        {
            "title": "A Qualitative Examples",
            "content": "In this section, we display qualitative examples of EFAs across the MATH training set which were validated by our tests. 12 Box A.1 EFA (Algebra) class MATH_train_5862(BaseModel): coefficient1: float coefficient2: float exponent1: int exponent2: int @classmethod def original(cls) ->'Self': return cls(coefficient1=9, coefficient2=3, exponent1=8, exponent2=3) @classmethod def sample(cls) ->'Self': coefficient1 = random.uniform(1, 10) coefficient2 = random.uniform(1, 10) exponent1 = random.randint(1, 10) exponent2 = random.randint(1, 10) return cls(coefficient1=coefficient1, coefficient2=coefficient2, exponent1=exponent1, exponent2=exponent2) def solve(self) ->str: result = self.coefficient1 / self.coefficient2 * 10 ** (self. exponent1 - self.exponent2) return str(int(result)) def render(self) ->str: return ( f'Simplify $({self.coefficient1} times 10ˆ{{{self.exponent1}}} div ({self.coefficient2} times 10ˆ{{{self.exponent2}}}$' ) Box A.2 EFA (Precalculus) class MATH_train_7423(BaseModel): a: float b: float @classmethod def original(cls) ->'Self': return cls(a=1 / 6, b=1 / 6) @classmethod def sample(cls) ->'Self': = random.random() / 6 = random.random() / 6 return cls(a=a, b=b) def solve(self) ->str: return f'boxed{{left({self.a}, {self.b}right)}}' def render(self) ->str: return ( 'Let $mathbf{{M}} = begin{pmatrix} 2 & 0 1 & -3 end{pmatrix }.$ Find constants $a$ and $b$ so that [mathbf{{M}}ˆ{-1} = mathbf{{M}} + mathbf{{I}}.]' ) 13 Box A.3 EFA (Number Theory) class MATH_train_5095(BaseModel): a1: int b1: int b2: int m: int @classmethod def original(cls) ->'Self': return cls(a1=4, b1=8, b2=3, m=20) @classmethod def sample(cls) ->'Self': a1 = random.randint(1, 100) b1 = random.randint(1, 100) b2 = random.randint(1, 100) while b1 % b2 == 0: b2 = random.randint(1, 100) = random.randint(10, 100) return cls(a1=a1, b1=b1, b2=b2, m=m) def solve(self) ->str: = 2 = 12 + 20 * x_squared = ** 2 remainder = x_squared % self.m return str(remainder) def render(self) ->str: return ( f'If ${{{self.a1}}}xequiv {{{self.b1}}}pmod{{{self.m}}}$ and ${{{ self.b2}}}xequiv {{{self.b2}}}pmod{{{self.m}}}$, then what is the remainder when $xˆ2$ is divided by ${{{self.m}}}$?' ) Box A.4 EFA (Geometry) class MATH_train_2738(BaseModel): original_volume: float original_radius: float original_height: float @classmethod def original(cls) ->'Self': return cls(original_volume=10, original_radius=1, original_height=1) @classmethod def sample(cls) ->'Self': volume = random.uniform(1, 100) radius = random.uniform(1, 10) height = random.uniform(1, 10) return cls(original_volume=volume, original_radius=radius, original_height=height) def solve(self) ->str: new_volume = 12 * self.original_volume return str(new_volume) def render(self) ->str: 14 return ( f'The radius of cylinder is doubled and its height is tripled. If its original volume was {self.original_volume} cubic feet, what is its volume now, in cubic feet?' ) Box A.5 EFA (Counting and Probability) class MATH_train_2221(BaseModel): length: float width: float @classmethod def original(cls) ->'Problem': return cls(length=3, width=2) @classmethod def sample(cls) ->'Problem': length = random.uniform(1, 10) width = random.uniform(1, 10) while length > width: length = random.uniform(1, 10) return cls(length=length, width=width) def solve(self) ->str: area_rectangle = self.length * self.width area_triangle = 0.5 * 2 * 2 probability = area_triangle / area_rectangle return f'{probability}' def render(self) ->str: return ( f'A point $(x,y)$ is randomly picked from inside the rectangle with vertices $(0,0)$, $({self.length},0)$, $({self.length},{self.width })$, and $(0,{self.width})$. What is the probability that $x < y$ ?' )"
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Generating EFAs When generating EFAs, we use the prompt in box B.1. To sample multiple candidates for EFAs, we use beam search with temperature of 0.7 and max generation length of 4096. We extract the resulting EFAs from the LLMs response by looking for markdown code block and extracting all markdown code blocks that have the necessary class structure. Box B.1 Prompt for Inferring EFAs # Instructions for Math Problem Functionalization Your task is to convert mathematical problem and its solution into reusable Python class that can generate similar problems. Follow these steps: 1. Create Python class that inherits from BaseModel with parameters that can vary in the problem. These parameters should capture the core numerical or 15 mathematical values that could be changed while maintaining the same problem structure. 2. Implement the following required methods: - `original()`: class method that returns the original problem's parameters - `sample()`: class method that generates valid random parameters for similar problem - `render()`: An instance method that produces the problem statement as formatted string - `solve()`: An instance method that computes and returns the solution 3. For the `sample()` method: - Generate random parameters that maintain the problem's mathematical validity - Include appropriate constraints and relationships between parameters - Use reasonable ranges for the random values 4. For the `render()` method: - Format the problem statement using f-strings - Include proper mathematical notation using LaTeX syntax where appropriate - Maintain the same structure as the original problem 5. For the `solve()` method: - Implement the solution logic using the instance parameters - Return the final answer in the expected format (string, typically) - Include any necessary helper functions within the method 6. Consider edge cases and validity: - Ensure generated problems are mathematically sound - Handle special cases appropriately - Maintain reasonable complexity in generated problems 7. Do not import any libraries! The following libraries have been imported. Use fully qualified names for all imports: - pydantic.BaseModel is imported as `BaseModel` - random is imported as `random` - math is imported as `math` - numpy is imported as `np` - sympy is imported as `sympy` - typing.Self is imported as `Self` Example usage: ```python problem = MyMathProblem.original() # Get original problem variant = MyMathProblem.sample() # Generate new variant # Get problem statement question = variant.render() answer = variant.solve() # Compute solution ``` The goal is to create class that can both reproduce the original problem and generate mathematically valid variations of the same problem type. # Example 1 ## Problem Statement Evaluate $iˆ5+iˆ{-25}+iˆ{45}$. ## Solution We have $iˆ5 = iˆ4cdot = 1cdot (i) = i$. We also have $iˆ{-25} = 1/iˆ{25} = 1/(iˆ{24}cdot i) = 1/[1cdot (i)] = 1/i = frac1{i}cdotfrac{i}{i} = i/(-1) = -i$ and $iˆ{45} = (iˆ{44})cdot i= 1cdot =i$, and . So, adding these three results gives $iˆ5 + iˆ{-25} + iˆ{45} = i+-i+i = boxed{i}$.nFinal Answer: The final answer is $boxed{ }$. 16 ## Functionalization ```python class Problem(BaseModel): exponent1: int exponent2: int exponent3: int @classmethod def original(cls) -> Self: return cls(exponent1=5, exponent2=-25, exponent3=45) @classmethod def sample(cls) -> Self: exponent1 = random.randint(-100, 100) exponent2 = random.randint(-100, 100) exponent3 = random.randint(-100, 100) return cls(exponent1=exponent1, exponent2=exponent2, exponent3=exponent3) def render(self) -> str: return f\"Evaluate $iˆ{{{self.exponent1}}} + iˆ{{{self.exponent2}}} + iˆ{{{ self.exponent3}}}$.\" def solve(self) -> str: # Compute the values of iˆn mod 4 cycle def compute_i_power(exp: int) -> complex: cycle = [1, 1j, -1, -1j] # 1, i, -1, -i return cycle[exp % 4] # Compute each term term1 = compute_i_power(self.exponent1) term2 = compute_i_power(self.exponent2) term3 = compute_i_power(self.exponent3) # Calculate the sum result = term1 + term2 + term3 # Express as LaTeX result_latex = ( f\"{result:.0f}\" if result.imag == 0 else str(result).replace(\"j\", \"i\") ) return f\"{result_latex}\" ``` # Example 2 ## Problem Statement Altitudes $overline{AX}$ and $overline{BY}$ of acute triangle $ABC$ intersect at $H$. If $angle BAC = 43ˆcirc$ and $angle ABC = 67ˆcirc$, then what is $angle HCA$? ## Solution First, we build diagram: size(150); defaultpen(linewidth(0.8)); pair = (0,0), = (3,0), = (1.2,2), = foot(A,B,C), = foot(B,A,C),H = intersectionpoint(B--Q,A--P); draw(A--B--C--cycle); draw(A--PˆˆB--Q); pair Z; = foot(C,A,B); draw(C--Z); label(\"$A$\",A,N); label(\"$B$\",B,W); label(\"$C$\",C,E); label(\"$X$\",P,S); label(\" $Y$\",Q,E); label(\"$H$\",H+(0,-0.17),SW); label(\"$Z$\",Z,NW); draw(rightanglemark(B,Z,H,3.5)); draw(rightanglemark(C,P,H,3.5)); draw(rightanglemark(H,Q,C,3.5)); Since altitudes $overline{AX}$ and $overline{BY}$ intersect at $H$, point $H$ is the orthocenter of $triangle ABC$. Therefore, the line through $C$ and $H$ is perpendicular to side $overline{AB}$, as shown. Therefore, we have $angle HCA = angle ZCA = 90ˆcirc - 43ˆcirc = boxed{47ˆcirc}$. ## Functionalization ```python class Problem(BaseModel): angle_BAC: int # angle BAC in degrees angle_ABC: int # angle ABC in degrees @classmethod def original(cls) -> Self: return cls(angle_BAC=43, angle_ABC=67) @classmethod def sample(cls) -> Self: # Generate random acute angles that form valid triangle # Sum of angles must be less than 180 angle1 = random.randint(30, 75) # Keep angles acute angle2 = random.randint(30, 75) # Ensure the third angle is also acute if angle1 + angle2 >= 150: angle1 = min(angle1, 60) angle2 = min(angle2, 60) return cls(angle_BAC=angle1, angle_ABC=angle2) def solve(self) -> str: # The angle HCA is complementary to angle BAC # This is because is the orthocenter and CH is perpendicular to AB angle_HCA = 90 - self.angle_BAC return f\"{angle_HCA}\" def render(self) -> str: return ( f\"Altitudes $overline{{AX}}$ and $overline{{BY}}$ of acute triangle $ABC$ \" f\"intersect at $H$. If $angle BAC = {self.angle_BAC}ˆcirc$ and \" f\"$angle ABC = {self.angle_ABC}ˆcirc$, then what is $angle HCA$ ?\" ) ``` # Example 3 ## Problem Statement On true-false test of 100 items, every question that is multiple of 4 is true , and all others are false. If student marks every item that is multiple of 3 false and all others true, how many of the 100 items will be correctly answered? ## Solution The student will answer question correctly if Case 1: both the student and the answer key say it is true. This happens when the answer is NOT multiple of 3 but IS multiple of 4. Case 2. both the student and the answer key say it is false. This happens when the answer IS multiple of 3 but is NOT multiple of 4. 18 Since the LCM of 3 and 4 is 12, the divisibility of numbers (in our case, correctness of answers) will repeat in cycles of 12. In the first 12 integers, $4$ and $8$ satisfy Case and $3,6,$ and $9$ satisfy Case 2, so for every group of 12, the student will get 5 right answers. Since there are 8 full groups of 12 in 100, the student will answer at least $8 cdot 5 = 40$ questions correctly. However, remember that we must also consider the leftover numbers 97, 98, 99, 100 and out of these, $99$ and $100$ satisfy one of the cases. So our final number of correct answers is $40 + 2 = boxed{42}$. ## Functionalization ```python class Problem(BaseModel): total_questions: int # Total number of questions multiple1: int # First multiple (4 in original problem) multiple2: int # Second multiple (3 in original problem) @classmethod def original(cls) -> Self: return cls(total_questions=100, multiple1=4, multiple2=3) @classmethod def sample(cls) -> Self: # Generate reasonable random parameters total = random.randint(50, 200) # Reasonable test length # Choose coprimes or numbers with small LCM for interesting results mult1 = random.randint(2, 6) mult2 = random.randint(2, 6) while mult1 == mult2: # Ensure different numbers mult2 = random.randint(2, 6) return cls(total_questions=total, multiple1=mult1, multiple2=mult2) def solve(self) -> str: def lcm(a: int, b: int) -> int: def gcd(x: int, y: int) -> int: while y: x, = y, % return return abs(a * b) // gcd(a, b) # Find cycle length (LCM) cycle_length = lcm(self.multiple1, self.multiple2) # Count correct answers in one cycle correct_per_cycle = 0 for in range(1, cycle_length + 1): answer_key_true = % self.multiple1 == 0 student_true = % self.multiple2 != 0 if answer_key_true == student_true: correct_per_cycle += 1 # Calculate complete cycles and remainder complete_cycles = self.total_questions // cycle_length remainder = self.total_questions % cycle_length # Calculate total correct answers total_correct = complete_cycles * correct_per_cycle # Add correct answers from remainder for in range(1, remainder + 1): 19 answer_key_true = % self.multiple1 == 0 student_true = % self.multiple2 != 0 if answer_key_true == student_true: total_correct += 1 return str(total_correct) def render(self) -> str: return ( f\"On true-false test of {self.total_questions} items, \" f\"every question that is multiple of {self.multiple1} is true, \" f\"and all others are false. If student marks every item that is \" f\"a multiple of {self.multiple2} false and all others true, how \" f\"many of the {self.total_questions} items will be correctly answered ?\" ) ``` # Your Turn Functionalize the following problem: ## Problem Statement [% problem_statement %] ## Solution [% solution %] ## Functionalization B."
        },
        {
            "title": "EFAGen Training Details",
            "content": "When doing rejection finetuning, we sample 20 candidate EFAs programs from the LLM for each seed problem during the rejection sampling phase. We sample 20 variants from each EFA in order to run the has dof(EFA) and is single valued(EFA) tests. When finetuning on the EFAs that pass all tests, we use the the same prompt box B.1 as the instruction and the extracted code of the EFA as the response. We use Transformers (Wolf et al., 2020) and Llama-Factory (Zheng et al., 2024) libraries for training. We format all data in the Alpaca format (Taori et al., 2023) as instruction-response pairs. We use the Adam optimizer with batch size of 16 and cosine learning rate scheduler with warmup ratio of 0.1 and train for 3 epochs in the FP16 datatype. We apply LoRA to all linear layers with rank of 16 and an alpha of 32, no bias, and dropout of 0.05. We truncate all training examples to maximum length of 4096 tokens with batch size of 32. B.3 Math Inference Settings When doing 0-shot inference with Llama3.1-8B-Instruct, we use the official Llama3.1 prompt in box B.2. When doing few-shot inference with Llama3.1-8B-Instruct, we use modified version of the official prompt, shown in box B.3. When sampling multiple responses, we use beam search with temperature of 0.7 and max generation length of 2048. When sampling single response, we use beam search with temperature of 0.0 and max generation length of 2048. In all cases, we check for equality of answers using the math-verify library. Box B.2 Llama3.1 0-shot MATH Prompt Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide concise solution with minimal explanation. 20 - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] ... Regardless of the approach, always conclude with: Therefore, the final answer is: $boxed{answer}$. hope it is correct. Where [answer] is just the final number or expression that solves the problem. Problem: {{ instruction }} Box B.3 Llama3.1 N-shot MATH Prompt Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] ... Regardless of the approach, always conclude with: Therefore, the final answer is: $boxed{answer}$. hope it is correct. Where [answer] is just the final number or expression that solves the problem. Here are some examples: {% for few_shot_example in few_shot_examples %} Problem: {{ few_shot_example.instruction }} {{ few_shot_example.response }} {% endfor %} Problem: {{ instruction }} B.4 Math Training Details We use the same hyperparameters and chat data format as in Appendix B.2, except we cutoff training data over 2048 tokens. However, we use simpler prompt template, shown in box B.4 to format the teacher responses. When annotating with Llama3.18B-Instruct teacher, we sample 5 responses per math problem with temperature of 0.7. We check for equality of answers using the math-verify library. Box B.4 Minimal instruction-tuning prompt used for augmentation experiments Question: {{ question }} Step-by-step Answer"
        }
    ],
    "affiliations": [
        "University of North Carolina at Chapel Hill"
    ]
}