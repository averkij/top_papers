{
    "paper_title": "Meta-RL Induces Exploration in Language Agents",
    "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 4 8 6 1 . 2 1 5 2 : r META-RL INDUCES EXPLORATION IN LANGUAGE AGENTS Yulun Jiang1,2, Liangze Jiang1,3, Damien Teney3 Michael Moor2, Maria Brbic1, 1EPFL 2ETH Zurich 3Idiap Research Institute Equal Contribution Equal Advising"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn longhorizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LAMER, general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LAMER consists of two key components: (i) cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LAMER significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LAMER also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large language models (LLMs) have shifted from building conversational systems to decision-making agents capable of reasoning about and interacting with their environments (Yao et al., 2023b; Shinn et al., 2023; Wang et al., 2025; Feng et al., 2025). To accomplish the goal, language agents operate in multi-turn, textual observation-action loops, and must adapt quickly using the memory across turns. Central to such adaptation is exploration, which allows agents to test uncertain actions, acquire new knowledge, and avoid premature convergence on suboptimal strategies. However, unlike humans that can explore systematically and make fast adaptation in new environments (Wilson et al., 2014), LLM agents do not robustly engage in exploration without substantial interventions (Krishnamurthy et al., 2024). Recent works have begun to address this limitation by guiding LLMs toward exploratory behaviors at test time. For example, Tajwar et al. (2025) train models offline to distill exploration strategies from trajectories across diverse environments, while Gandhi et al. (2024) induce such strategies from offline search traces. Setlur et al. (2025) train models to learn to explore in-context as better way of spending test-time compute (Snell et al., 2025). However, these works either focus on single-turn non-agentic reasoning problems, or rely on offline data which limits them to imitation rather than active exploration. In this work, we take step toward agents that can actively explore their environment, gather feedback, and leverage this experience for more effective exploitation. Since multi-turn tasks often have sparse success signal after an episode, we consider multi-episode regime (Shinn et al., 2023) where an episode is the unit of exploration and exploitation. Balancing exploration and exploitation can then be naturally formulated as cross-episode reinforcement learning (RL) framework. Training across many similar but different environments under this framework leads to the meta reinforcement learning (Meta-RL) (Duan et al., 2016; Wang et al., 2016; Bauer et al., 2023; Beck et al., 1 Figure 1: Comparison of RL and Meta-RL training on the MineSweeper environment. Left: MetaRL training with LAMER retains higher sample diversity from the base model while achieving better success rates, reaching better trade-off between exploration and exploitation. Right: Distinct trajectories and their empirical probabilities aggregated over multiple sampled trajectories in the MineSweeper environment. Each trajectory corresponds to sequence of clicks (numbered cell) on the board. Sample diversity is quantified by the entropy of the empirical distribution. The Meta-RL trained model produces more diverse and explorative trajectories. 2025), where the agent is forced to discover general strategies that work in unseen and potentially harder environments. Building upon this, we propose LAMER (LLM Agent with Meta-RL), general Meta-RL framework for LLM agent training. LAMER contains two key design principles. First, unlike standard single-episode RL, LAMER is designed around multi-episode structure to train the agent to solve the problem through trial and error. In early episodes, the agent is encouraged to gather diverse experiences and informative feedback from the environment, which are then used to adapt its policy in later episodes. By maximizing long-term rewards across episodes, the agent internalizes learning algorithm that explicitly incentivizes exploration for improved downstream exploitation. Second, at both training and test time, the agent effectively leverages the feedback and reflections from previous episodes to determine its strategy for the next episode. This essentially implements an RL algorithm in context, making the approach naturally suited for LLM agents. Meta-RL produces more diverse samples while simultaneously achieving higher performance, reaching better balance between exploration and exploitation than standard RL (Figure 1). To the best of our knowledge, this is the first time meta-RL framework is used for LLM agent training. We evaluate LAMER on four challenging long-horizon tasks: Sokoban (Racani`ere et al., 2017), MineSweeper (Li et al., 2024), Webshop (Yao et al., 2022) and ALFWorld (Shridhar et al., 2021). Using Qwen3-4B (Yang et al., 2025), we demonstrate that LAMER consistently outperforms prompting and RL baselines across all environments. In addition, we observe that the trained model learns to balance between exploration and exploitation, resulting in improved test-time scaling performance. In particular, LAMER adapts the trained policy at test time, with 11/14/19% absolute performance gains on Sokoban, MineSweeper and Webshop, respectively, over the RL. Furthermore, we show that the LAMER trained model generalizes better to harder and out-of-distribution tasks. In summary, LAMER takes step toward autonomous agents that can actively act to uncover information and improve their decision-making in the new environments."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLM-as-agent. As LLMs become increasingly capable of reasoning about complex scenarios (Wei et al., 2022), there is growing interest in making them decision-making autonomous agents. Earlier works rely on prompting frozen LLMs (Yao et al., 2023b; Shinn et al., 2023; Park et al., 2023; Wang et al., 2024a; AutoGPT). ReAct (Yao et al., 2023b) prompts LLMs with in-context examples to generate both textual actions and reasoning thoughts. Later, Reflexion (Shinn et al., 2023) extends this principle to the multi-episode setting, where the agent verbally reflects on the last episode and maintains their own reflection buffer for the next episodes. More recent research trains LLM agents through designing advanced RL algorithms (Wang et al., 2025; Feng et al., 2025) for multi-turn interactions, or supervised fine-tuning on generated interaction trajectories across diverse tasks (Tajwar et al., 2025). The evaluation of LLM agents also poses challenges because of fully verbal interactions with the environments. Recent benchmarks span wide range of domains, including text-based 2 embodied environments (Shridhar et al., 2021), e-commerce website (Yao et al., 2022), bandits (Nie et al., 2024), classic games (Park et al., 2025; Li et al., 2024) and other tasks (Liu et al., 2024; Nathani et al., 2025). For more comprehensive overview of these efforts, we refer readers to recent surveys (Wang et al., 2024b; Zhang et al., 2025). Meta reinforcement learning (Meta-RL) (Beck et al., 2025) focuses on learning to reinforcement learn in order to rapidly adapt to new environments. Similar to meta-learning (Thrun & Pratt, 1998; Hospedales et al., 2021), it involves an inner-loop that represents an RL algorithm (i.e., an adaptation strategy) by itself, together with an outer-loop that updates the meta-parameters so that the inner loop becomes more effective across many tasks. By training across many tasks, the outer loop forces the agent to learn the exploration strategy necessary to solve the tasks, while the inner loop enables the agent adapt quickly based on the exploration. Depending on how the inner loop is done, there are incontext methods and gradient-based methods. For example, Duan et al. (2016); Wang et al. (2016); Stadie et al. (2018) represent the inner-loop as history-dependent policy parametrized by an RNN, the adaptation is thus done in-context through gathering more information stored in the memory states. On the other hand, Finn et al. (2017) leverages gradient-based approach in which the inner adapts general meta-policy learned by the outer-loop. Our work lies in the former category, where the adaptation occurs entirely in-context at test time, naturally leveraging LLMs in-context learning abilities. Test-time compute. Meta-RL framework in LAMER can be considered as amortizing the testtime compute by training tasks in multi-episode manner rather than single episode. In this way, the learned in-context policy adaptation balances exploration and exploitation for fast adaptation at test time. This is essentially better way of spending test-time compute (Snell et al., 2025; Muennighoff et al., 2025; Wu et al., 2025; Setlur et al., 2025). In our experiments, we match the training compute budget between an RL and Meta-RL baseline, and show that meta-RL encourages superior test-time scaling behavior (through pass@k). Qu et al. (2025) similarly relates meta-RL to test-time compute, but they are limited to single-turn problems of mathematical reasoning, without leveraging the interactive feedback from the environment. Reasoning in LLMs. More broadly, this work relates to reasoning in LLMs, because language agents must use reasoning as part of their decision-making. large bulk of recent work on LLM reasoning has focused on more advanced prompting (Wei et al., 2022; Yao et al., 2023a), posttraining (Cobbe et al., 2021; Luong et al., 2024; Shao et al., 2024; DeepSeek-AI et al., 2025) or bootstrapping (Zelikman et al., 2022) against verifiers or reward models, inducing structured search behavior (Gandhi et al., 2024; Moon et al., 2024), or reflecting on previous answers (Kumar et al., 2024; Xiong et al., 2025; Qu et al., 2024), etc. Most of these works focus on single-turn math (Hendrycks et al., 2021b; Cobbe et al., 2021) and coding (Chen et al., 2021; Hendrycks et al., 2021a) problems, while we target multi-turn agentic environments where environment feedback is available after every action and at the end of the episode."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "We consider the scenario where an LLM agent interacts with the environment to solve multi-turn task. This process can be formulated as Markov decision process = (S, A, P, R, γstep), where and denote the state space and action space, and is the reward function. At each time step = 0, ..., 1, the LLM agent observes state st and selects an action at according to its policy at πθ(st). The environment then provides scalar reward rt and transitions to the next state st+1 according to the transition function ( st, at). trajectory is the sequence of states, actions, and rewards over an episode, i.e., τ = (s0, a0, r0, ..., sT 1, aT 1, rT 1). The objective of reinforcement learning is to maximize the expected discounted return: Eτ πθ (cid:35) γt steprt , (cid:34)T 1 (cid:88) t=0 (1) where γstep [0, 1] is the discount factor. Recent works (Wang et al., 2025; Feng et al., 2025) have shown that RL training has enabled LLM agents to interact with the environment and solve multiturn tasks. However, such agents often learn fixed policy during training and struggle to actively explore and adapt their behavior to the tasks at test time (Nie et al., 2024). 3 Meta-RL. Conversely, by training on distribution of tasks, Meta-RL (Duan et al., 2016; Wang et al., 2016; Bauer et al., 2023; Beck et al., 2025) encourages exploration because it optimizes metaparameters, such that the agent can solve new tasks quickly. In our case, the meta-parameters are the parameters of the LLM. This necessitates the agent to learn general exploration-exploitation strategies suitable for the task distribution trained on. For example, for most navigation tasks in partially observable environments, the optimal strategy is to gather the environment information and locate the target during the first episode, then reach the target as efficiently as possible in the second episode. This explore-then-exploit strategy implemented by the agent is itself an RL algorithm, where the policy learned at the meta-level encodes how to adaptively switch between informationgathering and reward-maximizing behaviors depending on the stage of interaction with new task. For LLM agents operating in multi-turn tasks, the policy can be in context (i.e., without parameter updates at test time), naturally leveraging the in-context capability of LLMs."
        },
        {
            "title": "4 LAMER: A META-RL FRAMEWORK FOR LLM AGENTS",
            "content": "Adopting the Meta-RL principle, we present LAMER, framework for training LLM agents with the ability to actively explore and adaptively learn from the environment. The framework addresses two central challenges: (i) how to balance exploration and exploitation over multiple attempts at task, and (ii) how to efficiently adapt the policy during training and evaluation. To address the first challenge, LAMER introduces cross-episode training scheme that treats each trial as sequence of episodes, enabling the agent to explore in early episodes and exploit this information in later ones. Second, instead of relying on gradient-based updates, LAMER uses self-reflection as an incontext adaptation mechanism, allowing the agent to summarize past experiences and adjust its strategy accordingly. Together, these two components enable scalable training of LLM agents under unified Meta-RL framework, which can be optimized with standard RL algorithms. Cross-episode training framework. sequentially generated by the agent: In the training of LAMER, each trial consists of episodes = (τ (0), τ (1), . . . , τ (N 1)), where τ (n) π(n) θ (), [0, 1], (2) θ where π(n) () is the policy at episode updated from the accumulated history τ (0), ..., τ (n1) through some adaptation strategy. For simplicity, in our analysis we assume all episodes contain the steps of interactions with the environment, i.e., τ (n) = (s(n) 1) for all [0, 1]. The rollout process terminates at if τ (n) is successful (as indicated by the environment feedback). Otherwise, the agent starts new episode τ (n+1) from the same initial state, repeating this procedure until the maximum episode budget is reached. For action a(n) , the discounted return g(n) within the episode τ (n) is: 1, a(n) 1, r(n) 0 , a(n) 0 , r(n) , ..., s(n) 0 g(n) = 1 (cid:88) l=t step r(n) γlt , (3) where γstep [0, 1] is within-the-episode discount factor. To enhance the exploration and maximize the long-term reward, in LAMER framework we define the discounted return G(n) across the episodes of as: G(n) = g(n) (cid:124)(cid:123)(cid:122)(cid:125) within-the-episode + 1 (cid:88) γmn traj g(m) 0 , m=n+1 (cid:124) (cid:123)(cid:122) cross-episode (cid:125) (4) where γtraj [0, 1] is the cross-episode discount factor. Finally, the LLM agent is trained via the following Meta-RL objective: J(θ) = ET πθ (cid:34)N 1 (cid:88) 1 (cid:88) γn traj n=0 t=0 (cid:35) stepr(n) γt = ET πθ (cid:104) (cid:105) . G(0) 0 (5) 4 Here, γtraj is an important factor for the trade-off between exploration and exploitation. Ideally, small γtraj biases the objective towards early episodes and will lead to rapid exploitation to solve the problem. In comparison, larger γtraj emphasizes long-horizon return and therefore encourages more exploration at the early stage. Figure 2: Comparison between the training processes of RL (top) and Meta-RL used in LAMER (bottom). For single task, RL generates group of trajectories independently. In contrast, in LAMER we use Meta-RL and produce the trajectories sequentially and adapt the policy in-context with self-reflection. Trajectory discount factor γtraj is used for cross-episode credit assignment. In-context policy adaptation with self-reflection. In the Meta-RL, policy adaptation is the inner loop of the learning process of an LLM agent. Therefore, flexible and efficient adaptation mechanism plays an important role during training and methods like gradient descent (Finn et al., 2017) might be too expensive, especially for LLMs. In LAMER, we propose self-reflection based strategy(Shinn et al., 2023) to adapt the policy in-context (Brown et al., 2020; Laskin et al., 2023). Specifically, after each episode finishes, we prompt the agent to generate the textual reflection on the previous attempt, providing specific feedback and plan to guide the next episode (see Appendix for the used prompt). The policy is therefore updated through modifying the context, π(n) () = πθ(H(n)) where H(n) denotes the inter-episode memory that contains both the history θ trajectories and reflections. Importantly, the self-reflection step is also explicitly trained in LAMER using the reward obtained in the next episode. Note that the content H(n) can be adjusted according to the predefined memory buffer to reduce the context length and improve the efficiency. By default, we retain both history and reflection in H(n), and provide an ablation study in Section 6.2. Comparison to the RL training. Compared to the RL objective (Eq. 1), Meta-RL extends the credit assignment across multiple episodes to incentivize exploration in the early stages. In practice, given single task, both RL and Meta-RL will sample group of episodes during training to estimate the advantage. The key difference is that the RL rollouts are independent, whereas in Meta-RL each episode is conditioned on the preceding rollouts within the trial. Figure 2 illustrates the conceptual difference between the training processes of RL and Meta-RL. Optimization. The proposed Meta-RL objective in Eq. (5) can be optimized with standard policy gradient methods. Given the per-action cross-episode return G(n) defined above, the gradient can be estimated by θJ(θ) = ET πθ (cid:34) 1 (cid:88) 1 (cid:88) n=0 t=0 θ log πθ(a(n) s(n) , H(n)) A(n) , (6) (cid:35) is the advantage estimation derived from G(n) where A(n) . The framework is compatible with widely used optimizers such PPO (Schulman et al., 2017) and critic-free approaches such as GRPO (Shao et al., 2024) and GiGPO (Feng et al., 2025)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we conduct comprehensive experiments to evaluate LAMER in the Meta-RL framework. Specifically, we present the evaluation on: (i) the overall performance of LAMER across different agent environments; (ii) the generalization ability of LAMER to harder tasks; and(iii) the generalization of LAMER under distribution shifts."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "challenging and diverse evaluate LAMER on four Environments. We environments: Sokoban (Racani`ere et al., 2017), MineSweeper (Li et al., 2024), Webshop (Yao et al., 2022) and ALFWorld (Shridhar et al., 2021). Among them, Sokoban is classic grid-based game on planning where the environment is fully observable. In comparison, the environments of MineSweeper, ALFWorld and Webshop are partially observable, requiring the agent to explore and plan under uncertainty to finish the task. Specifically, MineSweeper is board game about logical deduction on hidden cells. Webshop simulates realistic web-based shopping tasks, and ALFWorld provides text-based embodied environments. We provide the detailed explanation and prompts of the environment in Appendix A. All the experiments are conducted with the text modality, though the proposed method can be naturally applied to multimodal environments. Training details. We use Qwen3-4B (Yang et al., 2025) as our base model for all the experiments. To improve rollout efficiency in agentic loops, we use the non-thinking mode during trajectory generation. Additionally, we validate our method on Llama3.1-8B-Instruct (Grattafiori et al., 2024), see Appendix D.1 for the results. For the Meta-RL setting, we use γtraj = 0.6 as the default trajectory discount factor and explore its influence in the ablation study. We use GiGPO as the default optimization algorithm for all the environments with LAMER. Importantly, for Meta-RL training we sample = 3 episodes and set group size to 8 for each task. To ensure fair comparison, we use group size of 24 in standard RL training, yielding the same number of trajectories used for each gradient update step. All other hyperparameters and configurations are kept identical across RL and Meta-RL for fair comparison and are provided in Appendix C. Our code is publicly available at https://github.com/mlbio-epfl/LaMer. 5.2 PERFORMANCE COMPARISON We compare the performance of the LAMER framework with prompting-based baselines (Zeroshot, ReAct (Yao et al., 2023b; Shinn et al., 2023)), and RL methods (PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024), GRPO (Shao et al., 2024), and GiGPO (Feng et al., 2025)) across three environments: Sokoban, MineSweeper, and Webshop. For each method, we report the success rates under 1, 2, and 3 attempts (i.e., pass@1, pass@2, and pass@3, respectively). The results are summarized in Table 1. Table 1: Performance on Sokoban, MineSweeper and Webshop environments. The results of p@1, p@2 and p@3 denote the success rate (%) under 1, 2, and 3 attempts, respectively. Method Sokoban MineSweeper Webshop p@ p@2 p@3 p@1 p@2 p@3 p@ p@2 p@3 9.8 9.6 9.8 Prompting 6.8 Zero-shot 7.2 ReAct Reflexion 6.4 Training with RL 12.5 PPO 13.5 RLOO 22.9 GRPO GiGPO 41.6 Training with Meta-RL (ours) 42.4 LAMER 15.4 16.6 26.4 43.6 52. 12.9 12.5 12.1 16.8 18.8 27.0 44.1 55.9 4.5 6.3 5.5 29.7 48.8 36.3 52.0 6.6 7.0 7. 34.2 51.2 40.0 54.9 8.6 10.9 9.8 35.5 51.6 40.4 55.1 1.4 3.1 2.7 53.1 67.6 72.9 73.4 2.1 4.5 3. 54.5 68.4 73.0 74.6 2.3 4.5 3.5 54.9 69.1 73.0 75.2 44.1 66.4 74. 67.8 84.4 89.1 6 Meta-RL obtains better performance. Across all three environments, LAMER trained with Meta-RL consistently outperforms both prompting-based baselines and RL-training methods on the final pass@3 success rate. On the Sokoban environment, LAMER achieves 55.9% pass@3 success rate, substantially outperforming the 44.1% from the strongest RL baseline (GiGPO) and 12.9% from prompting-based methods. Similarly, on the MineSweeper environment LAMER reaches 74.4% pass@3 success rate, which is 19% higher than the best RL-trained model. On the Webshop environment, LAMER also performs 14% better than the RL-trained methods. Notably, the performance gains are not limited to pass@3: improvements are also observed on pass@2 for all the environments, and even pass@1 for Sokoban. Together, these results demonstrate that LAMER delivers consistent benefit on the trained agents to solve the long-horizon task in the complex environments. Meta-RL exhibits stronger test-time scaling. LAMER trained with Meta-RL demonstrates remarkable effectiveness in test-time scaling, with larger performance gains across attempts according to the results in Table 1. For example, from pass@1 to pass@3 on Sokoban LAMER achieves 13.5% improvement, significantly larger than both RL-trained and prompting-based baselines (which are less than 5%). Notably, although LAMER starts with slightly lower pass@1 performance than the RL baseline (GiGPO) in the MineSweeper and Webshop environments, it quickly recovers and surpasses all baselines by pass@2 and pass@3. The results indicate that the Meta-RL trained model has successfully learned to actively explore in the earlier episodes and adapt effectively from the mistakes, leading to significant gains in the subsequent attempts. The illustrative trajectories and reflections produced by the trained agents are presented in Appendix E. Meta-RL induces exploration. To further analyze the behavior of the models, we measure the diversity of answer trajectories across environments. For each question, we sample multiple trajectories from the agent and group the identical trajectories that have the same states and actions. These groups are used to form the empirical distribution over distinct trajectories, as shown in Figure 1. We then estimate the entropy the distribution to quantify the trajectory diversity. Figure 3 compares the trajectory diversity of the base model, RL, and Meta-RL agents across environments. We observe that the base model exhibits the highest entropy, indicating it generates wide range of trajectories, though this diversity does not translate into higher success rates (see Table 1). RL-trained agent reduces diversity and converges toward more deterministic behaviors. In contrast, LAMER trained with Meta-RL preserves higher level of diversity than RL baselines, allowing more exploration at test time. Figure 3: Trajectory diversity of base and trained models. Compared to RL, Meta-RL preserves more diverse trajectories from the base model, striking better balance between exploration and exploitation. 5.3 GENERALIZATION TO HARDER TASKS Next we study the generalization ability of the pretrained models on harder tasks. To this end, we take the models trained with the RL and Meta-RL and evaluate them on the harder tasks in the environments of Sokoban and MineSweeper. We increase the difficulty by using more boxes for Sokoban and more mines for MineSweeper in the grid. The results are shown in Figure 4. As expected, the model trained with both RL and Meta-RL underperforms on harder tasks with an increasing number of boxes or mines in the grid. However, Meta-RL consistently outperforms RL on all the difficulty levels. Notably, on the most difficult setting, the model trained from Meta-RL still outperforms the RL-trained model with 10% performance gap on Sokoban, and 5% performance 7 Figure 4: Performance of RL and Meta-RL trained model on the tasks with increased difficulty. For Sokoban, we gradually increase the number of boxes and for MineSweeper, we increase the number of mines in the grid. gap on the MineSweeper. The consistent gap indicates that LAMER trained with Meta-RL not only performs better on the training distribution, but also generalizes better to the harder tasks. 5.4 GENERALIZATION TO UNSEEN TASKS We further study the ability LAMER and alternative methods to generalize out-of-distribution. For this experiment, we use the ALFWorld environment (Shridhar et al., 2021). As text-based embodied environment, ALFWorld contains 6 categories of common household activities: Pick and Place (Pick), Examine in Light (Look), Clean and Place (Clean), Heat and Place (Heat), Cool and Place (Cool), and Pick Two and Place (Pick2). We use the tasks of Pick, Look, Clean and Heat as in-distribution tasks and use Cool and Pick2 as out-of-distribution tasks. We train LAMER and alternative baselines with instances from in-distribution tasks and then evaluate the model on both in-distribution tasks (with held-out test set), and the examples of out-of-distribution tasks. The results are shown in Table 2. As we can see, RL trained model generally performs well on indistribution tasks and outperforms prompting-based methods by achieving more than 20% improvement on Look, Clean and Heat. However, on out-of-distribution tasks, Cool and Pick2, it only obtains 58.1% and 36.0% success rate. LAMER with Meta-RL consistently outperforms RL on both in-distribution and out-of-distribution tasks, with notable performance gap on out-of-distribution tasks. In particular, our LAMER framework achieves 23% performance gains on Cool and around 14% on Pick2. Overall, these results suggest that on ALFWorld, Meta-RL trained model could generalize better to out-of-distribution tasks compared to the RL trained model. Table 2: Evaluation of out-of-distribution generalization on the tasks of ALFWorld. Method i.d o.o.d"
        },
        {
            "title": "Clean Heat Cool",
            "content": "Pick2 Prompting RL Meta-RL 91.9 95.5 97.7 52.9 83.0 100.0 48.4 67.9 90.2 44.8 86.6 89. 42.8 58.1 81.0 21.2 36.0 50."
        },
        {
            "title": "6 ANALYSIS",
            "content": "We further conduct series of ablation studies on the key design factors of LAMER, including (i) the influence of trajectory discounted factor γtraj on the trade-off between exploration and exploitation and (ii) the ablation of inter-episode memory configurations. We additionally discuss (iii) the computation budget of the proposed Meta-RL framework compared to the RL training. 6.1 INFLUENCE OF TRAJECTORY DISCOUNT FACTOR The cross-episodes discount factor γtraj controls how rewards are propagated within trial, thereby mediating the balance between exploration and exploitation in the LAMER framework during training. To understand the effect of the discount factor, we train the agents with LAMER using different values of γtraj on Sokoban, MineSweeper and Webshop environments (Figure 5). We observe that 8 larger value of γtraj does not necessarily lead to better final performance on pass@3, instead, the optimal setting of γtraj varies across different environments. For Sokoban and Webshop environments, intermediate values (e.g., γtraj = 0.6) yield the best results, suggesting that balancing immediate and long-term rewards is more important for these tasks. In contrast, MineSweeper benefits from relatively larger γtraj (e.g., γtraj = 0.9), indicating that extended credit assignment better supports strategic exploration in this environment. Overall, the results show that γtraj provides practical way to control the trade-off between exploration and exploitation across environments. Figure 5: Success rates of models trained with different γtraj. higher value encourages more exploration during training. 6.2 ABLATION ON THE INTER-EPISODE MEMORY In LAMER, the agent policy is adapted in-context through the inter-episode memory H(n), which by default contains both the trajectories and reflections of previous episodes. To assess the influence of memory content to the training, we consider two alternative configurations in H(n): (i) only history trajectories; (ii) only reflections. The performance of the trained agents in each configurations are reported in Table 3. The results show that self-reflection provides clear benefit in LAMER, leading to 21.6% improvement on Sokoban, 11.0% on MineSweeper and 3.5% on Webshop, respectively. Interestingly, the reflection-only configuration also outperforms the default setting in LAMER (which contains both trajectory and reflection) across all environments. We hypothesize that this is because reflection-only memory presents more concise and focused guidance, leading to more effective adaptation of the agents behavior. Table 3: Comparison of LAMER with different inter-episode memory configurations. Content in H(n) Sokoban MineSweeper Webshop Trajectory-only Reflection-only Both 34.8 56.4 55.9 69.5 80.5 74.4 89.3 92.8 89.1 6.3 TRAINING BUDGET We next analyze the training budget of RL and Meta-RL, focusing on both data usage and computational efficiency. To ensure fair comparison, we set the group size for standard RL to be three times larger than that of Meta-RL. This adjustment guarantees that the two methods consume the same number of trajectories for each gradient update. Aside from this scaling, all other experimental configurationssuch as learning rates, batch sizes, and network architecturesare held constant. This design choice highlights that Meta-RL does not require larger data budget compared to RL; in other words, both methods rely on the same total number of trajectories to learn. Nevertheless, LAMER might still introduce additional training time cost compared to the RL baselines. In RL training, all the episodes could be sampled in parallel since they are independent. In contrast, LAMER exhibits less parallelism since episodes within the same trial needs to be generated sequentially. As result, we observe around twice the training time cost for LAMER in our current implementation. This suggests that more sophisticated sampling strategies, such as asynchronous rollout, could further improve the efficiency of LAMER for training LLM agents."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Being able to explore and gather information from the environment is crucial in building autonomous agents that can adapt quickly and robustly. We introduced LAMER, general LLM agent training framework leveraging the principle of meta reinforcement learning. Unlike previous RL methods that maximize single-episode return for immediate payoff, LAMER maximizes discounted crossepisode return, naturally balancing when to explore versus when to exploit to maximize long-term performance. The exploration allowed at training time teaches the agent general explorative strategies that enable rapid in-context adaptation at test time. We show across diverse environments that LAMER significantly outperforms RL methods, is able to generalize to harder environments, and scales better with more episodes at test time. Limitations and future work. Our results raise several promising directions for future work. (i) The generality of our method allows for combining it with other RL algorithms or self-reflection frameworks. We hypothesize that more advanced advantage estimation strategy or stronger reasoning model may enhance the performance. (ii) Our approach requires sampling episodes sequentially for rollouts since episodes are dependent in cross-episode training. This eventually leads to longer training time than RL methods. More efficient training strategies will be explored in future work. (iii) Finally, LAMER trained on easier environments can generalize to harder environments of the same kind or relatively similar domains. This ultimately suggests possibilities in building generalist agents that can adapt to completely novel environments. ACKNOWLEDGEMENTS M.B. gratefully acknowledges the support of the Swiss National Science Foundation (SNSF) starting grant TMSGI2 226252/1, SNSF grant IC00I0 231922, and the Swiss AI Initiative. M.B. is CIFAR Fellow in the Multiscale Human Program."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. AutoGPT. Significant-Gravitas/AutoGPT: An experimental open-source attempt to make gpt4 fully autonomous., 2023. URL https://github.com/Significant-Gravitas/ Auto-GPT/tree/master. Jakob Bauer, Kate Baumli, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, et al. Humantimescale adaptation in an open-ended task space. In International Conference on Machine Learning, 2023. Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. tutorial on meta-reinforcement learning. In Foundations and Trends in Machine Learning, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec 10 Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. In Advances in Neural Information Processing Systems, 2025. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and In Conference on Noah Goodman. Stream of Search (SoS): Learning to search in language. Language Modeling, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge In Advances in Neural Information Processing Systems Datasets and competence with apps. Benchmarks Track, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track, 2021b. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. Akshay Krishnamurthy, Keegan Harris, Dylan Foster, Cyril Zhang, and Aleksandrs Slivkins. Can large language models explore in-context? In Advances in Neural Information Processing Systems, 2024. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. In International Conference on Learning Representations, 2024. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distillation. In International Conference on Learning Representations, 2023. Yinghao Li, Haorui Wang, and Chao Zhang. Assessing logical puzzle solving in large language models: Insights from minesweeper case study. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In International Conference on Learning Representations, 2024. 11 Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Seungyong Moon, Bumsoo Park, and Hyun Oh Song. Guided stream of search: Learning to better search with language models via optimal path guidance. arXiv preprint arXiv:2410.02992, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Conference on Empirical Methods in Natural Language Processing, 2025. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, and Roberta Raileanu. Mlgym: new framework and benchmark for advancing ai research agents. arXiv preprint arXiv:2502.144, 2025. Allen Nie, Yi Su, Bo Chang, Jonathan Lee, Ed Chi, Quoc Le, and Minmin Chen. Evolve: Evaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238, 2024. Dongmin Park, Minkyu Kim, Beongjun Choi, Junhyuck Kim, Keon Lee, Jonghyun Lee, Inkyu Park, Byeong-Uk Lee, Jaeyoung Hwang, Jaewoo Ahn, Ameya S. Mahabaleshwarkar, Bilal Kartal, Pritam Biswas, Yoshi Suhara, Kangwook Lee, and Jaewoong Cho. Orak: foundational benchmark for training and evaluating llm agents on diverse video games. arXiv preprint arXiv::2506.03610, 2025. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement finetuning. In International Conference on Machine Learning, 2025. Sebastien Racani`ere, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri`a Puigdom`enech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, 2017. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Matthew YR Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for LLMs. arXiv preprint arXiv:2506.09026, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, 2023. 12 Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In International Conference on Learning Representations, 2025. Bradly Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. In Advances in Neural Information Processing System, 2018. Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, Zico Kolter, Jeff Schneider, and Russ Salakhutdinov. Training generally curious agent. In International Conference on Machine Learning, 2025. Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn. Springer, 1998. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2024a. Jane Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 2024b. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022. Robert Wilson, Andra Geana, John White, Elliot Ludvig, and Jonathan Cohen. Humans use directed and random exploration to solve the exploreexploit dilemma. Journal of Experimental Psychology: General, 2014. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2025. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, 2023a. 13 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations, 2023b. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, 2022. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for LLMs: survey. arXiv preprint arXiv:2509.02547, 2025."
        },
        {
            "title": "A TASK DESCRIPTION AND DETAILS",
            "content": "Sokoban. We include the classic video game Sokoban as fully-observable environment. The game is 2D square board, with boxes scattered on the board. There are also target positions marked on the board. The player is placed at an initial position, and the goal is to push all the boxes to the target positions. There is no correspondence between each box and the target position. When the player walks into box, it gets pushed in that direction (if theres space). Boxes cant be pushed into walls or other boxes. Once box is pushed into corner or against wall with no way to get behind it, it might become permanently stuck. There is no pull operation in this game. The agent, therefore, has to think several moves ahead to avoid getting boxes stuck in positions where they cant reach their targets. The difficulty of this task is controlled by the board size, the number of boxes, and the wall structure of the board. We train on board size 6 6 with 2 boxes. MineSweeper. We include the classic video game MineSweeper as partially-observable environment. The game is 2D square board, with several mines randomly scattered in the board cells. The goal of the game is to open all the safe cells without revealing the hidden mines. In each step, the agent opens cell, and the first step is always safe. If mine is revealed, the task ends in failure immediately. The state of the opened (safe) cells can either be empty or number from 1 to 8, and the number specifies how many mines are adjacent to the specific cell. The agent needs to use the numbers marked on the opened cells to reason about the position of mines. Success is achieved when all safe cells are revealed. Our implementation is based on simplified version of Li et al. (2024). The difficulty of this task is controlled by the board size and the number of mines. We train on board size 6 6 with 3 hidden mines. Webshop (Yao et al., 2022). We include Webshop as partially-observable text-based environment that simulates online shopping. The agent is given natural language instruction specifying product to purchase with certain attributes. The environment presents simplified e-commerce interface where the agent can search for products, navigate through search results, and examine product pages with details like price, color, size, and customer reviews. The agent must interpret the instruction, search effectively, filter through multiple product options, and select the item that best matches the specified criteria. Success is measured by whether the final purchased item satisfies all the requirements in the original instruction. ALFWorld (Shridhar et al., 2021). We include ALFWorld as partially-observable text-based environment that simulates household tasks in interactive fiction format. The agent receives natural language instructions for common household activities. The environment provides text descriptions of rooms, objects, and possible actions, while the agent must navigate through house, interact with objects, and complete multi-step tasks. Objects may need to be found, picked up, cleaned, heated, or combined with other objects to achieve the goal. The agents view is limited to the current room and nearby objects, requiring exploration and memory of previously visited locations. Success requires understanding the instruction, planning sequence of actions, and executing them correctly while managing partial observability. We train on the training examples of the activities Pick, Look, Clean, Heat. We evaluate in-distribution on the test examples from the same activities, and we evaluate out-of-distribution on the test examples from Cool, Pick2 activities."
        },
        {
            "title": "B EXAMPLE PROMPTS",
            "content": "We provide examples of prompts for each task. There are two types of prompts: (1) the standard version (with name Standard Prompt) used for prompting the agent to play the game; (2) the reflection prompt used for self-reflection on past experience (with name Reflection Prompt) There are variables such as {past experience reflection}, {history actions} in the prompts, among with other task-specific hyperparameters. They are omitted in the prompts for clarity. In practice, they will be replaced with the actual content. Note that the {past experience reflection} will be empty for the first episode. Similar to (Feng et al., 2025), we use <action> </action> block to indicate the final decision of the action, and we use <remark> </remark> to indicate the content of the self-reflection. (see next page for the prompts) 16 B.1 SOKOBAN"
        },
        {
            "title": "Sokoban Standard Prompt",
            "content": "You are an expert agent operating in the Sokoban environment. # Symbols and Their Meaning - Walls (#): These block movement. You cant move through or push anything into walls. - Floor ( ): Open spaces where you can walk and move boxes. - Targets (O): The spots where boxes need to go. - Boxes (X): These are what you need to push onto the targets. - Player (P): Thats you! Youll move around the grid to push boxes. - Box on Target (): box successfully placed on target. - Player on Target (S): You standing on target. # Goal Your goal is to push all the boxes (X) onto the target spots (O). Once all boxes are on the targets, you win! # Rules Your admissible actions are [up, down, left, right]. You can only push one box at time. You cant pull boxes, so plan ahead to avoid getting stuck. You cant walk through or push boxes into walls (#) or other boxes. To avoid traps, do not push boxes into corners or against walls where they cant be moved again. {example} # Observations The initial state of the game is: 0: # # # # # # 1: # # # _ # 2: # _ _ _ # 3: # _ _ # 4: # _ # _ # 5: # # # # # # {past experience reflection} You have already taken the following actions: {history actions} Your current observation is: 0: # # # # # # 1: # # # _ # 2: # _ _ # 3: # _ _ # 4: # _ # _ _ # 5: # # # # # # Now its your turn to make moves (choose the next {num actions per turn} actions). - Your response first be step-by-step reasoning about the current situation observe the positions of boxes and targets, plan path to push box toward target, and avoid traps like corners or walls. - Then choose {num actions per turn} admissible actions and present <action> </action> tags (separated by comma). them within"
        },
        {
            "title": "Sokoban Reflection Prompt",
            "content": "You are an expert agent operating in the Sokoban environment. # Symbols and Their Meaning - Walls (#): These block movement. You cant move through or push anything into walls. - Floor ( ): Open spaces where you can walk and move boxes. - Targets (O): The spots where boxes need to go. - Boxes (X): These are what you need to push onto the targets. - Player (P): Thats you! Youll move around the grid to push boxes. - Box on Target (): box successfully placed on target. - Player on Target (S): You standing on target. # Your Goal Your goal is to push all the boxes (X) onto the target spots (O). Once all boxes are on the targets, you win! # Rules Your admissible actions are [up, down, left, right]. You can only push one box at time. You cant pull boxes, so plan ahead to avoid getting stuck. You cant walk through or push boxes into walls (#) or other boxes. To avoid traps, do not push boxes into corners or against walls where they cant be moved again. # Your Task You will be given the history of past experience. Your job is to **reflect on the past sequence**, identify any **mistakes or inefficiencies**, and then devise **concise, improved plan** starting from the original initial state. # Past Experience The initial state of the game is: 0: # # # # # # 1: # # # _ # 2: # _ _ _ # 3: # _ _ # 4: # _ # _ # 5: # # # # # # You have taken the following actions: {history actions} The final state is: 0: # # # # # # 1: # # # _ # 2: # _ _ # 3: # _ _ # 4: # _ # _ _ # 5: # # # # # # The task is NOT successfully completed. Now its your turn to reflect on the past experience and come up with new plan of action. - Your response should first be step-by-step reasoning about the strategy and path you took to attempt to complete the task. Identify where things went wrong or could be better. - Then devise concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. - Finally, end the response with your reflection and improved plan inside <remark> </remark> tags, to guide the next trial. 18 B.2 MINESWEEPER"
        },
        {
            "title": "MineSweeper Standard Prompt",
            "content": "You are an expert agent operating in the Minesweeper game. You will be given two dimensional {board size} by {board size} board, with {n mines} hidden mines. The rows and columns are indexed from 1 to {board size}. # Cell States - Unopened cells (?): cells that are yet to be revealed and may contain mine. - Blank cells (.): opened and non-mine cells, and they have no neighboring mines - Numbered cells (1-8): opened and non-mine cells, and the number indicates how many mines are in the eight neighboring cells, including those diagonally adjacent. For example, cell with 8 means all its neighboring cells contain mines. - Mine cells (*): opened cells that contain mine. # Your Goal Your goal is to clear the board by revealing all the cells that dont contain mines, without detonating any of the hidden mines scattered throughout the board. Use clues about the number of neighboring mines in each field to reason about the position of mines and non-mine cells. # Reveal Rules Your admissible action is to choose ONE unopened cell (?) to reveal per turn. The outcome depends on the content of that cell: - Blank cell (.): That cell is revealed, and all contiguous blank cells plus their bordering numbered cells are automatically revealed (auto-cascade). - Numbered cell (18): Only that single cell is revealed, showing the count of neighboring mines. - Mine (*): The game ends immediately in loss. # Observation The initial state of the game is: Row 1: . . . . . . Row 2: . . . 1 1 1 Row 3: . . . 1 ? ? Row 4: 1 1 . 1 2 ? Row 5: ? 1 . . 1 1 Row 6: ? 1 . . . . {past experience reflection} You have already chosen the following cells to reveal: (6, 1) Your current observation is: Row 1: . . . . . . Row 2: . . . 1 1 1 Row 3: . . . 1 ? ? Row 4: 1 1 . 1 2 ? Row 5: ? 1 . . 1 1 Row 6: 1 1 . . . . Now its your turn to make move. - Your should first reason step-by-step about the current situation observe the status of the board, inferring the states of unopened cells (?). - Then choose ONE unopened cell (?) to reveal. Put the index of cell in the format of (row, col) within the <action> </action> tag."
        },
        {
            "title": "MineSweeper Reflection Prompt",
            "content": "You are an expert agent operating in the Minesweeper game. You will be given two dimensional {board size} by {board size} board, with {n mines} hidden mines. The rows and columns are indexed from 1 to {board size} # Cell States - Unopened cells (?): cells that are yet to be revealed and may contain mine. - Blank cells (.): opened and non-mine cells, and they have no neighboring mines - Numbered cells (1-8): opened and non-mine cells, and the number indicates how many mines are in the eight neighboring cells, including those diagonally adjacent. For example, cell with 8 means all its neighboring cells contain mines. - Mine cells (*): opened cells that contain mine. # Your Goal Your goal is to clear the board by revealing all the cells that dont contain mines, without detonating any of the hidden mines scattered throughout the board. Use clues about the number of neighboring mines in each field to reason about the position of mines and non-mine cells. # Reveal Rules Your admissible action is to choose ONE unopened cell (?) to reveal per turn. The outcome depends on the content of that cell: - Blank cell (.): That cell is revealed, and all contiguous blank cells plus their bordering numbered cells are automatically revealed (auto-cascade). - Numbered cell (18): Only that single cell is revealed, showing the count of neighboring mines. - Mine (*): The game ends immediately in loss. # Your Task You will be given the history of past experience. Your job now is to **reflect on the past experience**, identify any **mistakes or inefficiencies**, and then devise **concise, improved plan** for your next try starting from the original initial state. # Past Experience The initial state of the game is: Row 1: . . . . . . Row 2: . . . 1 1 1 Row 3: . . . 1 ? ? Row 4: 1 1 . 1 2 ? Row 5: ? 1 . . 1 1 Row 6: ? 1 . . . . You have chosen the following cells to reveal: {history actions} The final state is: Row 1: . . . . . . Row 2: . . . 1 1 1 Row 3: . . . 1 ? ? Row 4: 1 1 . 1 2 ? Row 5: * 1 . . 1 1 Row 6: 1 1 . . . . The task is NOT successfully completed. Now its your turn to reflect on the past experience and come up with new plan of action. - Your response should first be step-by-step reasoning about the strategy and path you took to attempt to complete the task. Identify where things went wrong or could be better. - Then devise concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. - Finally, end the response with your reflection and improved plan inside <remark> </remark> tags, to guide the next trial. 20 B.3 WEBSHOP"
        },
        {
            "title": "Webshop Standard Prompt",
            "content": "You are an expert autonomous agent operating in the Webshop e-commerce environment. Your task is to: Find me slip resistant, non slip mens loafers & slip-ons with rubber outsole, rubber sole with color: 1877blue, and size: 11.5, and price lower than 70.00 dollars. {past experience reflection}{history actions} Your admissible actions of the current situation are: search[your query], click[search]. Now its your turn to take one action for the current step. Your response should first be step-by-step reasoning about the current situation, then think carefully which admissible action best advances the shopping goal. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags."
        },
        {
            "title": "Webshop Reflection Prompt",
            "content": "You are an expert autonomous agent operating in the Webshop e-commerce environment. Your task is to: Find me slip resistant, non slip mens loafers & slip-ons with rubber outsole, rubber sole with color: 1877blue, and size: 11.5, and price lower than 70.00 dollars. You will be given the history of past experience. Your job is to **reflect on the past sequence**, identify any **mistakes or inefficiencies**, and then devise **concise, improved plan** starting from the original initial state. Below are the last few actions and corresponding observations you have: {history actions} The task is NOT successfully completed. Now its your turn to reflect on the past experience and come up with new plan of action. - Your response should first be step-by-step reasoning about the strategy and path you took to attempt to complete the task. Identify where things went wrong or could be better. - Then devise concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. - Finally, end the response with your reflection and improved plan inside <remark> </remark> tags, to guide the next trial. 21 B.4 ALFWORLD ALFWorld Standard Prompt You are an expert agent operating in the ALFRED Embodied Environment. -= Welcome to TextWorld, ALFRED! =- You are in the middle of room. Looking quickly around you, you see bed 1, desk 2, desk 1, drawer 6, drawer 5, drawer 4, drawer 3, drawer 2, drawer 1, garbagecan 1, laundryhamper 1, safe 1, shelf 6, shelf 5, shelf 4, shelf 3, shelf 2, and shelf 1. Your task is to: put mug in desk. {past experience reflection}{history actions} Your admissible actions of the current situation are: go to bed 1, go to desk 1, go to desk 2, go to drawer 1, go to drawer 2, go to drawer 3, go to drawer 4, go to drawer 5, go to drawer 6, go to garbagecan 1, go to laundryhamper 1, go to safe 1, go to shelf 1, go to shelf 2, go to shelf 3, go to shelf 4, go to shelf 5, go to shelf 6, inventory, look. Now its your turn to take an action. - Your response should first by step-by-step reasoning about the current situation. - Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags."
        },
        {
            "title": "ALFWorld Reflection Prompt",
            "content": "You are an expert agent operating in the ALFRED Embodied Environment. -= Welcome to TextWorld, ALFRED! =- You are in the middle of room. Looking quickly around you, you see bed 1, desk 2, desk 1, drawer 6, drawer 5, drawer 4, drawer 3, drawer 2, drawer 1, garbagecan 1, laundryhamper 1, safe 1, shelf 6, shelf 5, shelf 4, shelf 3, shelf 2, and shelf 1. Your task is to: put mug in desk. You will be given the history of past experience. Your job is to **reflect on the past sequence**, identify any **mistakes or inefficiencies**, and then devise **concise, improved plan** starting from the original initial state. Below are the actions you took and the corresponding observations: {history actions} The task is NOT successfully completed. Now its your turn to reflect on the past experience and come up with new plan of action. - Your response should first be step-by-step reasoning about the strategy and path you took to attempt to complete the task. Identify where things went wrong or could be better. - Then devise concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. - Finally, end the response with your reflection and improved plan inside <remark> </remark> tags, to guide the next trial."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "LAMER is compatible with standard policy gradient algorithms. Without specification, we use GiGPO as the default optimization algorithm. During training, the self-reflection step is also explicitly trained using the reward in the subsequent episodes. During training, we match the total number of experiences sampled for each example between RL and Meta-RL to ensure fair comparison. Specifically, for each sample = 3 episodes and set group size to 8 for Meta-RL, and use group size of 24 for standard RL training. Besides that, other hyper-parameters and configuration are kept the same between RL and Meta-RL training. We use Qwen3-4B as the base model and train it with Adam optimizer and learning rate of 1e 6. For Sokoban and MineSweeper, we train the agents with batch size of 16 for 300 epochs. In comparison, we use batch size of 8 and 150 epochs for Webshop and ALFWorld. The environment reward is set to be 10 for successful trajectories and 0 for unsuccessful ones. We use temperature of 1.0 during rollout and 0.7 during evaluation. The maximum number of output tokens is set to 1024. Our code is based on the training framework of verl (Sheng et al., 2025) and verl-agent (Feng et al., 2025)."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "D.1 EXPERIMENTS ON DIFFERENT ARCHITECTURE LAMER is designed as general framework built on meta-RL principles and is model-agnostic. To validate this, we additionally ran experiments on Llama3.1-8B-Instruct (Grattafiori et al., 2024), showing that our method works well on models in different model architecture and model size. Table 4: Performance on Sokoban and MineSweeper environments using Llama3.1-8B-Instruct as base model. The results of p@1, p@2 and p@3 denote the success rate (%) under 1, 2, and 3 attempts, respectively. Method GiGPO LAMER Sokoban MineSweeper p@1 5.9 17.6 p@2 6.3 19.9 p@3 6.3 20.3 p@1 39.1 47.7 p@2 41.4 61. p@3 42.2 65.6 In Table 4, we compare LAMER with the strongest baseline GiGPO (Feng et al., 2025), on Sokoban and MineSweeper. We show The results indicate that on Llama-3.1-8B-Instruct, LaMer still outperforms the RL baselines across the environments, demonstrating the general applicability of our method to different model architecture and size. D.2 COMPARISON TO RL BASELINES WITH INTER-EPISODE MEMORY In our main experiment at Table 1, we follow previous work and evaluate the standard RL methods without access to the inter-episode memory. For comprehensive evaluation, we further evaluate the RL trained agents with access to the inter-episode memory (reflections and previous trajectories). The results of pass@3 are shown in Table 5. We observe that the inter-episode memory enhances the performance of RL trained agents on Sokoban (+3.8%) and MineSweeper (+5.3%), while degrades the performance on Webshop (-1.2%). Nevertheless, LAMER still substantially outperforms RL baselines across all the environments, demonstrating the advantage of the proposed method. Table 5: Performance of RL baselines with access to inter-episode memory (pass@3). Method Sokoban MineSweeper Webshop GiGPO (w/o memory) GiGPO (w/ memory) LAMER 44.1 47.9 55.9 55.1 60.4 74.4 75.2 74.0 89."
        },
        {
            "title": "E EXAMPLES",
            "content": "In Figure 6, we provide an example of trajectories and corresponding reflections produced by the agent when solving the MineSweeper game. Here each trajectory is represented by sequence of clicks (numbered cells) on the board. The mines are not visible to the agent and will lead to failure of the game if clicked. Figure 6: Example of trajectories and reflections produced by LAMER trained agents on the MineSweeper game."
        }
    ],
    "affiliations": [
        "EPFL",
        "ETH Zurich",
        "Idiap Research Institute"
    ]
}