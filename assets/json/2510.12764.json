{
    "paper_title": "AnyUp: Universal Feature Upsampling",
    "authors": [
        "Thomas Wimmer",
        "Prune Truong",
        "Marie-Julie Rakotosaona",
        "Michael Oechsle",
        "Federico Tombari",
        "Bernt Schiele",
        "Jan Eric Lenssen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 6 7 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ANYUP: UNIVERSAL FEATURE UPSAMPLING Thomas Wimmer1,2, Prune Truong3, Marie-Julie Rakotosaona3, Michael Oechsle3 Federico Tombari3,4, Bernt Schiele1, Jan Eric Lenssen1 1Max Planck Institute for Informatics, SIC, 2ETH Zurich, 3Google, 4TU Munich https://wimmerth.github.io/anyup Figure 1: AnyUp is universal feature upsampling model that can upsample any feature from any to any resolution, generalizing to unseen features while achieving state-of-the-art performance."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce AnyUp, method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to wide range of downstream tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "General image feature extractors, such as DINO (Caron et al., 2021; Oquab et al., 2023; Simeoni et al., 2025), CLIP (Radford et al., 2021), SigLIP (Tschannen et al., 2025), or MAE (He et al., 2022), have become fundamental building blocks in modern day computer vision, providing, e.g., semantics or language-alignment to wide range of down-stream applications, such as depth estimation or 3D reconstruction (Yang et al., 2024b; Wang et al., 2025), open-vocabulary semantic segmentation (Engelmann et al., 2024; Wysoczanska et al., 2024), or benchmarking generative models (Asim et al., 2025). An important limitation of such pre-trained models, which are usually transformer-based, is that their output feature map resolution is limited to the number of transformer tokens, preventing the prediction of pixel-level features. Therefore, several recent works, such as FeatUp (Fu et al., 2024), LoftUp (Huang et al., 2025), or JAFAR (Couairon et al., 2025) propose learned feature upsampling methods. While such feature upsampling methods perform well when paired with the vision encoders with which they were trained, they are generally not encoder-agnostic at inference time and need to be retrained to be usable with different feature extractor. This can be costly or, in the case of the latest large vision models (Simeoni et al., 2025), even infeasible with limited computing resources,"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Categorization of feature upsampling methods. AnyUp is the first learnable method that generalizes to any input feature at inference time, while being able to upsample from any to any resolution and being task-agnostic. Bilinear Upsampling Nearest-Neighbor Upsampling Large Input Image Bilateral Filtering (Tomasi & Manduchi, 1998) Guided Filtering (He et al., 2012) Change of ViT Patchification Stride SAPA (Lu et al., 2022) CARAFE (Wang et al., 2019) DySample (Liu et al., 2023) ReSFU (Zhou et al., 2024) Resize Convolution (Odena et al., 2016) LiFT (Suri et al., 2024) FeatUp (Fu et al., 2024) FeatSharp (Ranzinger et al., 2025) LIIF (Chen et al., 2021) LoftUp (Huang et al., 2025) JAFAR (Couairon et al., 2025) AnyUp (Ours) any encoder any resolution any task trainable as, during training of the upsampler, the vision encoder must be queried at least once on higherresolution image for every training sample. In this work, we alleviate this limitation and propose AnyUp, learned feature upsampling method that generalizes to features of any size provided in any resolution while achieving state-of-the-art performance across various downstream tasks. As shown in Tab. 1, AnyUp is the first of its kind. Feature upsamplers extract information from low-resolution feature map and high-resolution RGB guidance image, to infer which pixel in the high-resolution image should receive which feature (Couairon et al., 2025). The key limitation of existing work is that their way of processing the low-resolution feature map is specific to the dimensionality and type of the used feature. In contrast, AnyUp employs feature-agnostic layer that allows to process any feature and can also generalize to novel feature types. In addition to the feature-agnostic layer, we introduce window attention procedure and crop-based training strategy, which further improve feature upsampling quality. In our experiments, AnyUp establishes new state of the art for feature upsampling. Its key advantages include universal applicability it can be trained and applied across all feature types and resolutions and robust generalization to feature types it is was not trained on. Moreover, AnyUp ensures high fidelity by minimizing the distortion of original feature semantics. Through ablations, we analyze different design choices and draw general insights for the feature upsampling task. In summary, our contributions are: We introduce AnyUp, an upsampling model that is feature-agnostic at inference time, i.e., that needs to be trained just once and can be used to upsample features from any source at any resolution and dimensionality. We propose feature-agnostic layer that captures information from features of varying type and dimensionality. We introduce window attention-based upsampling architecture that can be trained effectively using an image part-based loss and retains the input feature space through consistency regularization. We show that AnyUp outperforms prior upsampling methods while being able to generalize to any vision encoder at test time. We make our code and pre-trained weights publicly available at https://github.com/wimmerth/anyup, giving access to light-weight, training-free and easy-to-use feature upsampler."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Visual comparison against other methods. RGB channels correspond to the first three principal components computed over all features. Previous methods result in excessive smoothing or contain other artifacts: See, e.g., in the first row, the smoothed-out cloud features in LoftUp or the feature distribution shift for the mountains in JAFAR, as well as the oversmoothing and haloartifacts of FeatUp and Guided Filter in the third row. AnyUp (Ours) produces sharp output feature maps while preserving the input feature quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Prior works on feature upsampling can be categorized into several groups, based on their capabilities and reliance on data, as summarized in Tab. 1. Training-free upsampling methods can usually be used with any input features, for any task, and can upscale to any resolutions, while being relatively light-weight. This makes them, i.e., bilinear upsampling, the de-facto standard choice in many applications where high-resolution features are required. Methods like Guided Filtering (He et al., 2012) also work surprisingly well on favorable data samples, see Fig. 2, but degrade on, e.g., low-contrast or more complex images, requiring per-sample tuning of hyperparameters and often still causing excessive or insufficient blur (see the last row in Fig. 2). Changing the stride of the patchification in the vision transformer (Dosovitskiy et al., 2021), or simply increasing the resolution of the input image results in computationally heavy inference and risks moving the features out-of-distribution (Ranzinger et al., 2025). Learnable upsampling methods (Suri et al., 2024; Fu et al., 2024; Huang et al., 2025; Couairon et al., 2025) try to improve the quality and robustness over learning-free approaches. The parameterization, however, always results in the loss of feature independence at inference time, i.e., upsampling models are fitted to specific feature encoder (and input dimensionality). In addition, some methods are fixed to specific downstream task or model (Wang et al., 2019; Lu et al., 2022; Liu et al., 2023; Zhou et al., 2024) or cannot be used with arbitrary scaling factors (Odena et al., 2016; Wang et al., 2019; Lu et al., 2022; Liu et al., 2023; Suri et al., 2024; Fu et al., 2024; Ranzinger et al., 2025). The current state-of-the-art approaches, JAFAR (Couairon et al., 2025) and LoftUp (Huang et al., 2025), both concurrent works, formulate feature upsampling as single or stacked high-res to low-res attention, respectively. We adopt this formulation as it offers the advantage of being naturally resolution-agnostic, i.e., features can be upsampled from any to any resolution. However, in our AnyUp, we aim to make this architecture encoder-agnostic, i.e., the upsampling model should be trained once and subsequently be applicable to features from any vision encoder, potentially of different feature dimensionality. More details on selected related work can be found in App. A."
        },
        {
            "title": "3 TASK FORMULATION",
            "content": "Given an input RGB image Ihr RHW 3, the goal of feature upsampling methods is to upsample low-resolution input feature maps Rhwc := e(Ihr) to high-resolution output feature maps := (p, Ihr) RHW c. The necessity of this task is grounded in the use of large pre-trained vision encoders e() as feature extractors and powerful semantic priors in recent years. Almost all such models, be it convolutional neural networks (Fukushima, 1969; LeCun et al., 1989) or vi-"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Method Overview. AnyUp performs window attention-based upsampling (4.2). Input features are processed with feature-agnostic layer (4.1). During training, features computed for randomly sampled image parts are used as reference for the respective part of the upsampled feature map (4.3). sion transformers (Dosovitskiy et al., 2021), perform learned downsampling to latent resolutions < , which is necessary for efficient image processing using these large neural networks. However, when exploiting the features of these powerful models for downstream tasks, it is often necessary to work with high-resolution features, e.g., when performing dense, pixel-wise predictions, or aggregating information from multiple views in 3D (Engelmann et al., 2024; Ye et al., 2024; Asim et al., 2025)."
        },
        {
            "title": "4 LEARNING ENCODER-AGNOSTIC FEATURE UPSAMPLING",
            "content": "We are interested in lightweight and low-parameterized model to prevent memory and compute bottlenecks. For this, we base our model on an attention-based architecture, similar to that in previous works (Couairon et al., 2025; Huang et al., 2025) (see Fig. 3). More specifically, we adopt the architecture of JAFAR (Couairon et al., 2025), which we briefly describe next. The input image and low-resolution feature map are both first passed through convolution blocks with residual connections. Next, positional encodings are applied to the image features. Queries for the final attention layer are computed directly from these pixel features, while information from both the downsampled image and the low-resolution feature map is used to compute the keys. The values in the attention layer are simply the unprocessed patch features from the input feature map. In our method, we aim to fix two of the main limitations of this architecture. First, we replace the initial feature processing layer, which acts only on fixed-dimensional inputs and needs to be learned per vision backbone with our proposed feature-agnostic convolution layer (Sec. 4.1). Second, we simplify the task of the upsampler by restricting the source features to local windows in the attention computation (Sec. 4.2). We further improve the training pipeline for our feature upsampling method by using an image part-based strategy that is explained in Sec. 4.3.1, as well as additional consistency regularization improving the robustness to noise and input feature space preservation (Sec. 4.3.2). 4.1 FEATURE-AGNOSTIC UPSAMPLING BY DESIGN In our feature-agnostic layer, we aim to represent feature maps from any source model with any dimensionality as feature maps with canonical dimensionality by convolving them with learned filter basis. We hypothesize that an attention-based upsampling model, where the outputs are linear combinations of the input features for every pixel, mostly needs to understand overall local structure changes in the input feature map. To capture this structural information while staying agnostic to the input features (and their dimensionality), we design convolutional layer that captures structure independently for all input channels and aggregates this information later. We illustrate the structure of this layer in Fig. 4."
        },
        {
            "title": "Preprint",
            "content": "pi our proposed featureIn each input agnostic layer, channel convolved is with learned kernel basis {ψj Rkk}j=1,...,M , followed by softmax operation on the activations along the basis filter dimension. The contriresulting filter-wise butions then averaged are over all input channels. More the output feature formally, fj of feature-agnostic the be layer convolution computed as can Figure 4: Feature-agnostic layer. Input channels are processed independently and contributions to basis filters are averaged over all channels leading to outputs invariant to input dimensionality. fj ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) i{1,...,N } exp(pi ψj) j{1,...,M } exp(pi ψj) , (cid:80) (1) where is the (varying) number of input channels and the canonical number of output channels. Note that we leave out the spatial structure of the convolution operation to simplify notation in Eq. 1. 4.2 LOCAL WINDOW ATTENTION Analyzing the attention patterns of JAFAR (Couairon et al., 2025), we find that in the global attention mechanism, where pixel query can attend to any feature patch in the input feature map, sometimes vastly unrelated and distant image areas are used as references for upsampling. We argue that we can avoid such patterns and simplify the upsampling problem by restricting the attention computation to local windows around the query point (Ramachandran et al., 2019)., As high-resolution feature is now linear combination of much smaller set of coarse features compared with the global attention, the optimization objective for the upsampler gets easier, as well as efficiency is improved. We refer the reader to App. for visualization of the mentioned attention outliers. 4.3 TRAINING PIPELINE Computing high-resolution features, i.e., with very high-resolution input image, is infeasible for obtaining reference ground-truth that one can compare to at training time. Besides being computationally infeasible at scale, extreme high-resolution inputs also effectively move most models out-of-distribution, as noted by Ranzinger et al. (2024). To circumvent this, prior work proposed different techniques to leverage low-resolution features. FeatUp (Fu et al., 2024) proposes upsampler training as multi-view reconstruction, where equivariance to small image-space perturbations is optimized for, which requires carefully designing these augmentations without shifting the images out-of-distribution for the vision backbone. In LoftUp (Huang et al., 2025), segmentation masks are used as high-resolution guidance signal at the cost of having to query large segmentation model in every training step. In the simplest case, as in JAFAR Couairon et al. (2025), training is only performed at low resolutions, i.e., learning to upsample 16x16 to 32x32 feature map, where both are computed from respectively scaled input images (Suri et al., 2024; Couairon et al., 2025). While Couairon et al. (2025) argue that this lowresolution training is sufficient for training an attention-based upsampling mechanism, we propose an improved training strategy based on local image crops that lets us train more powerful feature upsampler, while being faster and more memory-efficient. 4.3.1 DATA SAMPLING We make use of the fact that we do not necessarily need to supervise the upsampling on the full feature map. Instead, we choose to supervise our method only on smaller local crops. Our training pipeline is illustrated in Fig. 3. More specifically, we take high-resolution image RHW and randomly sample smaller local crop thereof Rhw. We then downsample to the same"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Qualitative Probing Results. Visualization of linear probing results for monocular depth estimation on NYUv2 and semantic segmentation on ADE20k. More visualizations given in App. D. resolution w. Using the computed features = e(I) and ˆq = e(I ), we upsample to highresolution feature map = (I, p) Rhw , where h, are chosen in such way that the cropped part corresponding to matches the resolution of ˆq. Note that while LoftUp (Huang et al., 2025, Sec. 4.2) proposes similar strategy in their second training stage, they compute the loss against an EMA of their method instead of ground-truth features and at much higher resolution, resulting in much more compute-heavy training. On the other hand, our sampling method is more light-weight than JAFARs simple strategy Couairon et al. (2025) described in Sec. 4.3, as we do not need to compute reference features at higher resolution, e.g., 448x448. 4.3.2 OBJECTIVE FUNCTION We follow prior work in feature denoising and feature upsampling (Yang et al., 2024a; Couairon et al., 2025) and minimize the distance between predicted features and target features ˆq: Lcos-mse(q, ˆq) = 1 cos(q, ˆq) + L2(q, ˆq). (2) In addition, we add self-consistency regularization Lself-consistency, which we detail in App. B.1, as well as an input-consistency regularization Linput-consistency, where we simply compute Lcos-mse with the input features and the accordingly downsampled predicted output features q. We do this to further improve the locality of the upsampled features, which is important for sub-object-level tasks like surface normal estimation, as well as maintaining the input feature space."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We train AnyUp on the ImageNet dataset (Krizhevsky et al., 2017) and provide further implementation details in App. B. Comparisons against previous works are provided in Sec. 5.1, where we evaluate probing quality, feature space preservation, and behaviour with different resolution changes. Sec. 5.2 provides more results on the generalization of AnyUp to other features at test time. Sec. 5.3 wraps up the experiments with an ablation study. 5.1 COMPARISON TO PRIOR ART We mainly compare our method to FeatUp (Fu et al., 2024), as well as the concurrent LoftUp (Huang et al., 2025) and JAFAR (Couairon et al., 2025), which both are general, attention-based feature upsampling methods that can upsample from and to any resolution1. However, we note that both models have the significant disadvantage of not being agnostic to the input features at test time and need to be retrained per vision encoder. Our AnyUp does not suffer from this problem and is thus more general method that can be applied out-of-the-box to any new features. If not mentioned otherwise, the DINOv2 ViT-S model (Oquab et al., 2023) is used as feature extractor for training and testing in the experiments, as other works also provide pre-trained weights for this model. 1While both, LoftUp and JAFAR, are considered concurrent according to ICLR guidelines, we choose to compare AnyUp mainly against these methods and FeatUp, as they are the strongest available competitors."
        },
        {
            "title": "Preprint",
            "content": "Qualitative Analysis. We visualize PCA projections of the upsampled features in Fig. 2, which shows visual comparison against prior methods. We observe that AnyUp produces sharp output feature maps while preserving the input feature quality. Prior works, on the other hand, often still result in oversmoothed feature maps or feature distribution shifts as indicated by the red circles. evaluate Semantic Segmentation. We follow the evaluation scheme proposed by prior works (Couairon et al., 2025, Sec. 4.3.1), where linear head, i.e., 1x1 convolution layer, is trained to predict semantic segmentation labels from the high-resolution feature maps. The input images are resized to 448x448 resolution and features are thus of resolution 28x28 or 32x32 depending on the patch size of the vision transformer used for feature extraction. Upsampling is generally performed back to the input image resolution, i.e., equaling 14x or 16x upsampling, if not mentioned otherwise. We performance on the COCO- (Lin et al., 2014), Stuff ADE20k (Zhou et al., 2018) PASCAL and VOC (Everingham et al., Results 2014) datasets. are as mean Intersection-over-Union (mIoU) and accuracy. As observed in Tab. 2, AnyUp provides state-of-the-art upsampling performance for downstream semantic segmentation. Table 2: Semantic Segmentation. Highlights for best, second and third best scores. PASCAL-VOC mIoU () Acc. () mIoU () Acc. () mIoU () Acc. () Bilinear FeatUp LoftUp JAFAR AnyUp 95.38 96.04 96.11 96.22 96.19 81.43 83.37 83.69 84.36 84.00 79.32 81.14 81.32 81.07 81.37 74.12 75.57 75.72 75.48 75.85 40.54 42.19 42.02 42.06 42.43 59.48 61.95 62.15 61.82 62. pixel-wise reported ADE20k COCO Depth and Normal Estimation. We follow Probe3D (Banani et al., 2024) for evaluating the downstream performance on depth and surface normal estimation. Notably, we resize input images, target depth and surface normal maps to 224x224 resolution, if not mentioned otherwise. Results are reported as the root mean square error (RMSE). For normal estimation, we further report the accuracy of predictions for specified angular thresholds. For depth estimation, we additionally report the δ1 score, corresponding to the amount of pixels for which the ratio of prediction to ground-truth is less than 1.25. For more details on the benchmarking, as well as the employed metrics, we refer the reader to Banani et al. (2024, App. A.3). As shown in Tab. 3, AnyUp outperforms all competitors on these tasks, showcasing its strength in preserving the locality of upsampled features. In contrast, LoftUp smoothens the features per-object too much due to its training objective using object segmentation masks and therefore performs suboptimal on tasks like surface normal estimation. Table 3: Surface Normal and Monocular Depth Estimation. RMSE () Surface Normals 11.25 () 22.5 () 30 () RMSE () Bilinear FeatUp LoftUp JAFAR AnyUp 32.70 32.69 33.94 31.54 31.17 0.26 0.25 0.26 0.28 0.29 0.53 0.53 0.51 0.56 0.57 0.66 0.66 0.64 0.68 0.69 Depth (Absolute) δ1 () 0.8081 0.8156 0.8127 0.8052 0.8216 0.4925 0.4816 0.4847 0.4906 0. RMSE () Depth (Relative) δ1 () 0.9112 0.9193 0.9166 0.9180 0.9233 0.3582 0.3413 0.3478 0.3481 0.3378 Upsampling from Any to Any Resolution. We follow the same evaluation procedure as described before but now vary the input feature resolution and target upsampling size, where the latter also corresponds to the size of the guidance image given to the upsampler methods. The semantic segmentation results are obtained on the COCO dataset. As FeatUp (Fu et al., 2024) does not support varying upsampling ratios, we always upsample by factor of 16x and subsequently perform bilinear downsampling to the desired output resolution. Results are shown in Tab. 4. AnyUp outperforms its competitors across most resolution changes while only slightly performing worse than the best competing method when upsampling from 16 112 pixels."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Upsampling from any to any resolution. Linear probing results for Semantic Segmentation (COCO) and depth estimation when varying the feature map and output resolutions. 16 112 Semantic Segmentation 32 224 32 112 16 112 mIoU () Acc. () mIoU () Acc. () mIoU () Acc. () Bilinear FeatUp LoftUp JAFAR AnyUp 56.38 58.88 58.97 59.79 59.63 77.17 79.15 79.37 79.87 79.75 59.42 61.92 61.68 61.91 62.25 79.28 81.10 81.06 81.14 81.41 59.40 61.76 61.20 61.66 62. 79.27 80.99 80.69 80.94 81.26 RMSE (abs) () 0.4927 0.4357 0.4896 0.4871 0.4746 RMSE (rel) () 0.3586 0.3231 0.3533 0.3458 0.3364 Depth Estimation 32 RMSE (abs) () 0.4606 0.4507 0.4591 0.4825 0.4441 RMSE (rel) () 0.3274 0.3145 0.3264 0.3489 0.3079 32 112 RMSE (abs) () 0.4600 0.4513 0.4636 0.4812 0.4455 RMSE (rel) () 0.3273 0.3160 0.3296 0.3498 0.3073 Feature Space Preservation. Ideally, the upsampled features should stay in the same space and distribution as the original, low-resolution features. If that is the case, linear probe trained on the original feature space should directly transfer to the high-resolution features outputted by given upsampler for the same input feature extractor, without any finetuning. We test how much different feature upsampling methods preserve the feature distribution, by using linear probes pre-trained on the original DINOv2 (ViT-S) feature extractor (Oquab et al., 2023)2. In Tab. 5, we evaluate this for two different tasks, depth estimation on the NYUv2 dataset (Silberman et al., 2012) and semantic segmentation on the ADE20k dataset (Zhou et al., 2018). Qualitative results are in App. D. Table 5: Feature Space Preservation. Semantic Segmentation (ADE20k) and Depth Estimation (NYUv2) with linear probes pre-trained on low-resolution DINOv2 features. AnyUp retains the input feature distribution while improving upsampling quality. LoftUp does not retain the input feature distribution, hence its results are heavily degraded. Guided Filtering requires tuning of hyperparameters per sample. Semantic Segmentation mIoU () Acc. () Bilinear Guided Filter FeatUp LoftUp JAFAR AnyUp 39.73 37.54 40.19 4.27 39.06 40.83 73.32 72.25 74.05 46.58 73.75 74.94 Depth Estimation δ1 () RMSE () 0.816 0.813 0.818 0.802 0.815 0.822 0.506 0.518 0.504 0.765 0.503 0. AnyUp preserves the input features the best, while improving the prediction quality at higher resolution through improved upsampling, when compared to learnable and heuristic methods. On the other end of the spectrum, predictions using LoftUp are heavily degraded, which can be explained by the affinity matrix loss employed in its training, aligning intra-image feature similarities instead of directly supervising the predictions with the target features (Huang et al., 2025, Sec. 4.2). 5.2 FEATURE-AGNOSTIC UPSAMPLING instance, Table 6: Generalization to other models. AnyUp matches or surpasses the performance of prior upsampling methods while being trained on fundamentally different feature extractor. When trained with the same feature extractor as used during test time, further small improvements are possible. In Fig. 1, we visualize PCA projections of features, where the same feature upsampling i.e., model model that was trained only on DINOv2 ViT-S features, is applied on features extracted from image encoders ranging from ResNet (He et al., 2016) to DINOv3 (Simeoni et al., 2025). In Tab. 6, we showcase the remarkable generalization capabilities of AnyUp, where model trained on DINOv2 and tested on SigLIP 2 outperforms or comes close to the performance of models directly trained on SigLIP 2. Also, DINOv2-trained AnyUp model generalizes well to DINOv3 features. Finally, as detailed in Fig. 6, AnyUp generalizes well across different DINOv2 architectures. Test Model SigLIP 2LoftUp SigLIP 2LoftUp SigLIP 2LoftUp SigLIP 2JAFAR SigLIP 2JAFAR SigLIP 2JAFAR DINOv3 (ViT-S+) AnyUp AnyUp DINOv3 (ViT-S+) DINOv3 (ViT-S+) DINOv2 (ViT-S) SigLIP 2LoftUp SigLIP 2LoftUp DINOv2 (ViT-S) SigLIP 2JAFAR SigLIP 2JAFAR 51.68 (73.35) 54.45 (75.49) 40.73 (64.87) 58.51 (78.36) 60.32 (79.57) 60.10 (79.40) 0.59 / 0.48 0.57 / 0.46 0.72 / 0.60 0.91 / 0.58 0.90 / 0.57 0.93 / 0. 62.96 (81.82) 62.99 (81.84) AnyUp AnyUp LoftUp AnyUp AnyUp JAFAR 0.51 / 0.37 0.51 / 0.37 RMSE (abs / rel) DINOv2 (ViT-S) Train Model mIoU (Acc) 2We note that pre-trained linear probes and other feature upsampling methods were not released for the more recent DINOv3 (Simeoni et al., 2025) method, hence we stick to DINOv2."
        },
        {
            "title": "Preprint",
            "content": ") ( m 60 50 Semantic Segmentation Test model ViT-B ViT-S ViT-L Depth Estimation 80 70 ) ( . ) ( ) . ( R 0.4 0.3 0. 0.1 0 ) ( ) . ( R 0.3 0. 0.1 0 ViT-S ViT-B ViT-L Train model ViT-S ViT-B ViT-L Train model ViT-S ViT-B ViT-L Train model ViT-S ViT-B ViT-L Train model Figure 6: Generalization to other model sizes. We vary the model used during upsampler training and the model used for linear probing (test model) for AnyUp. We observe: (1) the general trend ViT-L ViT-B ViT-S in linear probing holds no matter the model used in training and (2) there is no significant degradation in upsampling quality when training on smaller, less powerful ViT. 5.3 ABLATION STUDY In Tab. 7, we find that the performance of AnyUp surpasses all its ablations. It is also evidenced that all proposed components lead to notable impact. Note that for the ablation of the data sampling, we replace ours by the simpler training approach of JAFAR (Couairon et al., 2025)). Table 7: Ablations. Effects of removing specific model or training components. mIoU (Acc.) () RMSE (abs / rel) () AnyUp w/o window attn. (4.2) w/o our data sampling (4.3.1) w/o Lself-consistency (4.3.2) w/o any regularization (4.3.2) 62.16 (81.37) 62.12 ( 81.34 ) 62.03 (81.28) 62.09 ( 81.33 ) 61.90 (81.23) 0.4755 / 0.3378 0.4854 / 0.3449 0.4773 / 0.3387 0.4763 / 0.3363 0.4786 / 0.3401 w/o feature path for key computation Remarkably, we observe that when removing the information flow from the input feature map to the key computation in the upsampling model (see Fig. 3), we are still able to obtain results that match the performance of prior feature upsampling methods, see, e.g., RMSE for absolute depth estimation in Tab. 3. This indicates that an upsampling strategy based only on position and color matching between high-resolution image and its downsampled equivalent can already provide relatively strong prior for feature upsampling when trained with our pipeline. 0.4791 / 0.3441 61.97 (81.23)"
        },
        {
            "title": "6 LIMITATIONS",
            "content": "FeatSharp (Ranzinger et al., 2025) combines feature upsampling with explicitly de-noising the input features from positional encoding artifacts (Yang et al., 2024a). We note that this learned per-model denoising step on the input features can be easily prepended to our pipeline but as the learned denoising weights have not been made public at the time of submission, we do not include this analysis in our experiments. Further, through our training strategy on image parts, our model is effectively also trained to mostly ignore such positional encoding artifacts. In addition, our upsampling approach based on single image-to-features-attention is computationally efficient and outperforms previous works while being more general. However, it relies on the simplifying assumption that upsampled features can be computed as linear combinations of low-resolution input features: patch feature almost certainly encodes sub-patch-level spatial information in its high-dimensional channels, see, e.g., works trying to reconstruct RGB images from patch features (Bordes et al., 2022). Such information could likely be extracted and used in feature upsampling when using larger and more complex upsampling model."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced AnyUp, method for feature upsampling from any resolution to any resolution, which generalizes to feature representations that it was not trained on. Key technical novelties include feature-agnostic layer, windowed attention, and training strategy, which work together"
        },
        {
            "title": "Preprint",
            "content": "to achieve state-of-the-art upsampling quality. We make our code and models publicly available at https://github.com/wimmerth/anyup. Acknowledgements This work was partially funded by the Saarbrucken Research Center for Visual Computing, Interaction, and Artificial Intelligence (VIA). Thomas Wimmer is supported by the Max Planck ETH Center for Learning Systems. Jan Eric Lenssen is supported by the German Research Foundation (DFG) - 556415750 (Emmy Noether Programme, project: Spatial Modeling and Reasoning)."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 60346044. IEEE, 6 2025. Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2179521806. IEEE, 6 2024. Florian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised representation knows about. Transactions on Machine Learning Research, 2022. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 10 2021. Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local In Proceedings of the IEEE/CVF conference on computer vision and implicit image function. pattern recognition, pp. 86288638, 2021. Paul Couairon, Loick Chambon, Louis Serrano, Jean-Emmanuel Haugeard, Matthieu Cord, and Nicolas Thome. JAFAR: Jack up any feature at any resolution. arXiv preprint arXiv:2506.11136, 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, and Federico Tombari. Opennerf: Open set 3d neural scene segmentation with pixel-wise features and rendered novel views. In The Twelfth International Conference on Learning Representations, 2024. Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. International Journal of Computer Vision, 111(1):98136, 6 2014. Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. FeatUp: model-agnostic framework for features at any resolution. In The Twelfth International Conference on Learning Representations, 2024. Kunihiko Fukushima. Visual feature extraction by multilayered network of analog threshold elements. IEEE Transactions on Systems Science and Cybernetics, 5(4):322333, 1969. Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE transactions on pattern analysis and machine intelligence, 35(6):13971409, 2012. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016."
        },
        {
            "title": "Preprint",
            "content": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 6 2022. Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang. LoftUp: Learning coordinate-based feature upsampler for vision foundation models. arXiv preprint arXiv:2504.14032, 2025. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 5 2017. Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541551, 1989. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context, pp. 740755. Springer International Publishing, 2014. ISBN 9783319106021. Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao. Learning to upsample by learning to sample. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 60276037, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao. SAPA: Similarity-aware point affiliation for feature upsampling. Advances in Neural Information Processing Systems, 35: 2088920901, 2022. Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 1(10), 10 2016. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in neural information processing systems, 32, 2019. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. AM-RADIO: Agglomerative vision foundation model reduce all domains into one. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1249012500. IEEE, 6 2024. Mike Ranzinger, Greg Heinrich, Pavlo Molchanov, Bryan Catanzaro, and Andrew Tao. FeatSharp: Your vision model features, sharper. In Forty-second International Conference on Machine Learning, 2025. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor Segmentation and Support Inference from RGBD Images, pp. 746760. Springer Berlin Heidelberg, 2012. ISBN 9783642337154."
        },
        {
            "title": "Preprint",
            "content": "Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature transform for dense vit descriptors. In European Conference on Computer Vision, pp. 110128. Springer, 2024. C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271), ICCV-98, pp. 839846. Narosa Publishing House, 1998. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025. Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Content-aware reassembly of features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 30073016, 2019. Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image In Proceedings of the IEEE conference on super-resolution by deep spatial feature transform. computer vision and pattern recognition, pp. 606615, 2018. Monika Wysoczanska, Oriane Simeoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzcinski, and Patrick Perez. Clip-dinoiser: Teaching clip few dino tricks for open-vocabulary semantic segmentation. In European Conference on Computer Vision, pp. 320337. Springer, 2024. Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. In European Conference on Computer Vision, pp. 453469. Springer, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024b. Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 43(6):118, 2024. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302321, 12 2018. Minghao Zhou, Hong Wang, Yefeng Zheng, and Deyu Meng. refreshed similarity-based upsampler for direct high-ratio feature upsampling. arXiv preprint arXiv:2407.02283, 2024."
        },
        {
            "title": "A ADDITIONAL DETAILS ON RELATED WORK",
            "content": "FeatUp (Fu et al., 2024): Fu et al. (2024) propose training the upsampler in multi-view reconstruction setting, where the feature computation and learned upsampling should be equivariant to small image augmentations, using learned feature downsampling operation. FeatUp consists of multiple stacked joint bilateral upsampling (JBU) layers making upsampling only possible for integer factors. LoftUp (Huang et al., 2025): Huang et al. (2025) propose stacked high-res to low-res attention. Their training strategy consists of two stages: First, SAM (Kirillov et al., 2023) is used for object mask generation. Low-resolution features are then blended per-mask using bicubic upsampling. In their second stage, self-distillation with student and teacher models is performed. Instead of using cosine-similarity loss to compare their upsampled predictions with the ground truth, Huang et al. (2025) propose an affinity matrix loss, where the intra-image pixel feature similarity matrices are matched. This loss is computed on high-resolution images. JAFAR (Couairon et al., 2025): Couairon et al. (2025) propose feature upsampling as single highres to low-res attention. Training is performed on low resolutions and small scale changes (up to 4x). While spatial semantic feature modulation (Wang et al., 2018) is used to merge information from the downsampled image and the input feature map in JAFAR, we find that this is not giving noticeable improvements and replace it with simple concatenation of features followed by standard ResNet block. FeatSharp (Ranzinger et al., 2025): FeatSharp builds upon FeatUp (Fu et al., 2024), where tiling of the input image and multiple forward passes of the encoder model are used to get higher-resolution outputs. De-biasing from positional encoding artifacts is learned. The weights for the upsampler are not openly available, hence we do not compare against this method."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "We train our method on the ImageNet dataset (Krizhevsky et al., 2017) for total of 100,000 training steps. We choose batch size of 4 with 4 random local crops per training image. Training takes around 5 hours on single NVIDIA-H100 GPU. We use the AdamW optimizer (Loshchilov & Hutter, 2019) with learning rate of 2e-4 and batch size of 4. We note that the results for semantic segmentation linear probing reported in Sec. 5.1 deviate from the results reported by Couairon et al. (2025). This is caused by fixing bug in the probing training compared to their implementation. We ran all experiments with the official published weights for concurrent works. B.1 SELF-CONSISTENCY REGULARIZATION AND DATA AUGMENTATIONS Figure 7: Randomly sampled augmentations applied on test image (top left)."
        },
        {
            "title": "Preprint",
            "content": "We add self-consistency regularization Lself-consistency, also proposed in prior work (Couairon et al., 2025), where we increase the level and diversity of data augmentations to increase the robustness of our upsampling method. We show batch of randomly sampled augmented images in Fig. 7. The self-consistency regularization is computed as Lself-consistency = dcos-mse (f (p, Ihr), (p, hr)) , (3) where hr) is the noised/augmented version of the high-resolution image. The self-consistency regularization does not rely on ground-truth features ˆq and is thus computed at higher resolution, i.e., 224x224."
        },
        {
            "title": "C COMPULSORY NOTE ON LLM USAGE",
            "content": "No large language models were used in writing or ideation of this paper except as aid for styling Fig. 6."
        },
        {
            "title": "D ADDITIONAL VISUALIZATIONS",
            "content": "Figure 8: Visualization of attention artifacts and removal thereof through local window attention. Unconstrained, global attention leads to upsampled features relying on information from far-away, non-related objects. We provide simple fix to this which also simplifies the upsampling problem for the model by restricting attention only to local windows that are computed relative to the feature map size."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Linear Probing results for semantic segmentation using pre-trained DINOv2 (ViT-S) probe. AnyUp preserves the input feature space while upsampling and sometimes even outperforming the ground-truth, e.g., the tower segmentation in the fifth row. While FeatUps predictions are generally consistent with the low-resolution predictions, it smoothens object boundaries too much and is inable to get sharp segmentations. LoftUp shifts the feature distribution while upsampling, resulting in erroneous predictions with the pre-trained probe. Predictions using JAFAR suffer from reduced details for thin objects, see, e.g., the lights in row 2 or the fence in row 4."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Linear Probing results for depth estimation using pre-trained DINOv2 (ViT-S) probe. AnyUp stands out by preserving sharp edges from the guidance image while preserving the locality of features needed for smooth depth map prediction and preserving full objects, as, e.g., the white board in the fourth row. Note that the feature distribution shift of LoftUp is less pronounced for depth estimation, which can be partly attributed to the scale-shift alignment performed after prediction."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Additional linear probing results for semantic segmentation using probes trained on upsampled DINOv2 (ViT-S) features. AnyUp outperforms prior upsampling methods by also being able to segment fine-details and giving cleaner segmentation outputs."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Additional linear probing results for depth estimation using probes trained on upsampled DINOv2 (ViT-S) features. AnyUp consistently gives sharp object boundaries while matching the ground truth the best."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Google",
        "Max Planck Institute for Informatics",
        "TU Munich"
    ]
}