{
    "paper_title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "authors": [
        "Hai Ci",
        "Xiaokang Liu",
        "Pei Yang",
        "Yiren Song",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/"
        },
        {
            "title": "Start",
            "content": "H2R-Grounder: Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos Hai Ci, Xiaokang Liu, Pei Yang, Yiren Song, Mike Zheng Shou* Show Lab, National University of Singapore {cihai03,mike.zheng.shou}@gmail.com 5 2 0 2 0 1 ] . [ 1 6 0 4 9 0 . 2 1 5 2 : r Figure 1. H2R-Grounder converts human interaction videos into temporally aligned robotic manipulation videos, maintaining motion and background consistency and ensuring physically plausible robot arm structures and interactions. RoboMaster [16] (animation-based) losees motion and background consistency. Kling [29] and Runway Aleph [48] (editing-based) produce geometrically distorted robot arms."
        },
        {
            "title": "Abstract",
            "content": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose video-to-video translation framework that converts ordinary humanobject interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired humanrobot videos for training only set of unpaired robot videos, making the system easy to scale. We introduce transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain clean background and overlaying simple visual cue (a marker and arrow indicating *Corresponding Author 1 the grippers position and orientation), we can condition generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the humans actions. We fine-tune SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to promising direction for scaling up robot learning from unlabeled human videos. Webpage: https://showlab.github.io/H2R-Grounder/ 1. Introduction Collecting large-scale, diverse robot manipulation data remains core challenge in robotics [7, 11, 23, 28]. Recording demonstrations with physical robots is slow, costly, and constrained to lab settings [53], leaving even the largest robot datasets far smaller and less varied than those in NLP. In contrast, human interaction videosfrom casual online clips to egocentric recordingsare abundant and richly depict diverse manipulation behaviors. If robots could learn directly from these human videos, data collection would be vastly accelerated. Prior efforts often rely on specialized hardware [38] to collect paired humanrobot data [4, 27] for learning, which limits scalability. Moreover, the visual embodiment gaphuman arms and hands differ significantly in appearance and motion from robot arms and grippersmakes the learning non-trivial. Recent works [3133] attempt to robotize human videos by rendering robot arm into them to fill the visual gap, enabling imitation learning [31] or representation learning [33] for policy improvement. For instance, Phantom [32] inpaints the human hand in video frames and overlays rendered robot arm in its place based on the estimated hand pose. Masquerade [31] and H2R [33] extend this idea to egocentric views. Although effective, these rendering-based methods often produce physically inconsistent visualsrobots may appear to float or misalign with objectsand require accurate camera calibration and pose estimation, which hinders generalization to in-the-wild videos. See Fig. 2. In this paper, we introduce H2R-Grounder, novel framework that marries the strengths of generative video models with simple, transferable representation of manipulation, H2Rep. Our key insight is to remove the need for any paired humanrobot videos in training by using only unpaired robot videos and an abstract conditioning signal that is common to both human and robot domains. Concretely, we take collection of robot manipulation videos (which may be limited in scene diversity) and algorithmically strip the robot from them: we inpaint the robot arm out of each frame, yielding clean background video of the scene and target objects. Into this background, we overlay minimal pose indicator colored dot and arrow that mark the robot grippers 2D location and orientation. This annotated video serves as the conditioning input. We then fine-tune pre-trained diffusion video generator (Wan2.2 [54]) to reconstruct the original robot video given this conditioned input. Through this process, the model learns to insert robot arm into scene according to the provided pose cues, effectively learning the mapping from gripper end-effector pose sequences to realistic robot imagery. Crucially, the model is learning from actual robot videos, so it observes correct physics, contacts, and occlusions during training but it never sees human in these videos. At test time, we can apply the same procedure to human Figure 2. Issues in prior rendering-based H2R methods. (a) shows the rendered robot arm from Phantom [32], produced using their released code and provided calibrated camera parameters. Without accurate depth, the gripper appears to float above the book. (b) shows an overlaid robotic arm from H2R [33], collected from their public dataset, which suffers from severe floating artifacts and camera misalignment. demonstration video: estimate the humans hand pose, inpaint the person from the frames, and overlay the equivalent pose indicator. This produces transferrable representation H2Rep of the human demonstration, to which our model can now respond by generating robot video. The result is robot manipulation video that follows the humans motion in the scene, with the robot properly interacting with the objects and environment (e.g. grasping and moving objects on table, rather than hovering unnaturally). See Fig. 1. Our approach offers several advantages. It eliminates the need for paired demonstrations, leverages existing robot datasets [7, 11, 28, 53], and produces realistic, temporally consistent results grounded in contact physics. Moreover, our in-context fine-tuning strategy enhances temporal coherence compared to popular video-to-video pipelines such as VACE [25]. Finally, by using minimal 2D pose indicators instead of strict 3D alignment [3133], our method avoids calibration dependencies and generalizes robustly to diverse internet videos. To summarize, our contributions are threefold: 1. novel human-to-robot video translation framework H2R-Grounder, enabling robot video generation from human demonstrations without paired data. 2. simple and transferable intermediate representation H2Rep, unifying human and robot embodiments. 3. An in-context fine-tuning scheme for large diffusion video models, improving realism and temporal consistency for physically grounded generation. 2. Related Work Intermediate Representations for Bridging Humans and Robots. Learning robot control from human videos is long-standing challenge [20, 40, 50]. Due to the large visual embodiment gap between human and robot domains, most works [4, 27, 64] rely on shared intermediate representations as surrogates for joint learning. EgoMimic [27] masks out both human hands and robot arms to minimize appearance differences. Others [1, 64] inpaint manipulators and rely solely on background videos. Further studies leverage affordance maps [2, 38, 41], keypoints [5, 14, 21, 35, 45, 56, 59], flow [18, 49], pretrained models [6, 51], or latent features [39, 58]. While these representations facilitate crossdomain learning, they seldom generate robot videos directly and thus remain limited by information loss or visual misalignment. Our method introduces H2Rep, combining pose sequences and background videos to preserve both motion and scene context. Unlike prior works that only use such representations for feature alignment, we employ them to directly synthesize robot videos from human inputs, closing the visual gap. Translating Human Videos into Robot Videos. Recent works attempt to directly edit human videos into robot-like ones. Phantom [32] overlays rendered robot arms guided by estimated hand poses, while Masquerade [31] extends this to egocentric dataset epic-kitchen [13]. H2R [33] similarly composites simulated robot arms onto inpainted egocentric frames [19]. These rendering-based pipelines exploit large-scale human data but struggle with realismoverlaid arms ignore lighting, depth, and scene geometry, leading to implausible occlusions or contacts. Moreover, they require accurate camerarobot calibration and sensor parameters [31, 32], which are unavailable for in-the-wild videos. MimicDreamer [34, 52] narrows this embodiment gap via generative models, yet still conditions generator on robot renderings, inheriting the same calibration requirement. In contrast, we adpot fully generative approach, synthesizing robot videos conditioned on abstract 2D pose indicators. This design inherently models occlusion and contact learned from real robot data without calibration. HOPMan [4] is related, using off-the-shelf inpainting to remove robot arms and add human hands frame-by-frame [61], producing in-lab humanrobot pairs. However, the reverse processtranslating in-the-wild human videos into robot videosremains infeasible due to the lack of robot video generator. Our work fills this gap by introducing such generator. Cross-Robot Embodiment Transfer. Several studies [10, 30] investigate transferring across robots with similar morphology, benefiting from their comparable kinematics. In contrast, our human-to-robot setting involves third-person videos with full-body humans and robotic manipulators of vastly different structures, making embodiment transfer substantially more challenging. Generative Robot Video Prediction. Robot video prediction models typically generate future frames conditioned on robot actions such as 3D end-effector poses [8, 15, 24, 36, 42, 43, 55, 57, 65]. Our generative model instead conditions on easily obtained 2D pose sequences and background videos, enforcing both pose-consistent motion and scene coherence. The closest baseline, RoboMaster [16], animates robotobject interaction videos from single image given user-defined 2D robot and object trajectories, but it requires manual annotations for object masks and trajectories. We adapt RoboMaster to our H2R setting and show that H2RGrounder achieves superior motionbackground consistency and overall realism. 3. Methodology 3.1. shared abstraction for human and robot videos There exist abundant humanobject interaction (HOI) videos on the web and large collections of robot manipulation videos captured in labs [11, 28, 53]. However, collecting frame-aligned humanrobot pairs at scale is prohibitively costly. We therefore seek shared representation that bridges large-scale HOI videos and robot manipulation videos without requiring paired, frame-aligned supervision. We observe that both domains decompose naturally into: (i) pose trajectory of the manipulator (human hand or robot gripper) that carries action semantics, and (ii) background video that preserves scene layout and the physical state of manipulated objects. If we align human-hand and robotgripper poses, then pose sequence + background becomes common carrier of the key information in both domains. We denote this abstraction by H2Rep. In the following sections, we present: (1) how to extract H2Rep, from robot manipulation videos (Sec. 3.2); (2) how to train an in-context video generation model conditioned on this structured representation to synthesize robot videos (Sec. 3.3); and (3) how to obtain H2Rep from humanobject interaction videos and leverage the video generator to generate frame-aligned robot videos (Sec. 3.4). The overall three-stage pipeline is illustrated in Fig. 3. Notation. Let Vr and Vh be robot video and human video, respectively. Hr and Hh are H2Rep extracted from robot video and human video, respectively. We use for text-prompted video segmentation (Grounded-SAM2 [47]), for video object removal (inpainting), Π for 6-DoF-to-2D pose projection using calibrated cameras, for rendering pose as graphic overlays (red dot for position and blue arrow for orientation), and Blend(A, B; α) = (1 α)A + αB for alpha blending with α = 0.4. We use video VAE encoder/decoder (Enc, Dec), and e() for text embedding. 3.2. Training data construction from robot videos Robot-arm segmentation. Given robot video Vr, we obtain pixel-accurate mask sequence with text prompt: Mr = S(Vr, robotic arm). (1) 3 Figure 3. Paradigm of H2R-Grounder. The overall pipeline consists of three stages: (1) training data collection from robot video datasets, (2) in-context fine-tuning of the video generation model, and (3) transfer from in-the-wild human videos to robot manipulation videos. Gripper pose projection. Let the end-effector (EEF) 6DoF trajectory be TEEF(t) = [p(t), R(t)] and camera intrinsics/extrinsics be (K, Rc, tc). We project to image space: Pr(t) = Π(cid:0)K, Rc, tc; p(t), R(t)(cid:1), (2) and render dot/arrow overlay R(Pr) on each frame. Robot-arm removal (background video). We remove the arm with video inpainting model: VI = I(cid:0)Vr, Mr (cid:1). (3) Empirically, Minimax-Remover [66] preserves background and removes the robot arm more reliably than another popular inpainting model E2FGVI [37], so we adopt it in our pipeline. See Fig. 4. Composing robot video H2Rep. We form the shared representation by blending the rendered pose with the inpainted background: Hr = Blend(cid:0)VI , R(Pr); α(cid:1) , α = 0.4. (4) This yields training pairs Dr = (cid:8)(H(i) i=1, where Hr carries gripper motion and scene evolution, and Vr is the physically grounded target. , V(i) )(cid:9)N 3.3. In-context learning for physically grounded robot video generation We train conditional video generator Gθ (Wan 2.2 backbone [54]) to synthesize Vr conditioned on Hr (and fixed text prompt ctext: robotic arm is interacting with objects.). Following an in-context learning design, both Hr and Vr are encoded by the same VAE and fused by self-attention; only LoRA adapters [22] on the Q/K/V projections are trainable, while all other backbone weights remain frozen: zH = Enc(Hr), zV = Enc(Vr), = (cid:2)zH ; e(ctext)(cid:3). (5) 4 Composing human video H2Rep . H2Rep from the human video also follows the same format as from the robot video: Hh = Blend(cid:0)VI h, R(Ph); α(cid:1) , α = 0.4. (10) H2R translation . We directly condition the trained robot generator Gθ on the human video abstract Hh to generate the robot video from human video: (cid:98)Vr = Gθ(Hh, z1, t, ctext). (11) Because we fine-tune only lightweight LoRA adapters and keep the base generator frozen, Gθ maintains strong OOD generalization so we can apply it to in-the-wild videos. 4. Experiments 4.1. Experimental Setup Training and testing datasets. We use the Droid dataset [28] for training. This dataset contains approximately 76K diverse third-person Franka arm [17] manipulation videos. During training, we randomly sample from the whole dataset while reserving 50 for validation. We report SSIM, and LPIPS [63] to evaluate motion and background consistency as well as high-level visual feature distance between generated and ground-truth videos. To evaluate H2R-Grounder on out-of-distribution (OOD) human videos, we test on two types of data. First, we use the DexYCB humanobject interaction dataset [9], which captures controlled lab-environment videos but exhibits clear domain shifts in both background and action distributions compared with Droid. It includes eight third-person camera views showing interactions between subjects and 20 distinct objects. We use the 100 videos from subject 01 under the camera 932122062010 top-down view as our test set. We do not use the ground-truth human masks or object poses provided by DexYCB; instead, we employ our automatic annotation pipeline described earlier to simulate real-world testing conditions. Since no ground-truth robot videos exist for comparison, we evaluate this set using two complementary metrics: (1) VLM-based evaluation and (2) human studies (gold standard), focusing on four aspectsmotion consistency, background consistency, visual quality, and physical plausibility (robot integrity and contact realism). In addition, we collect internet videos featuring more diverse backgrounds, occlusions, viewpoints, and camera motions for qualitative comparisons with baseline methods. Data preprocessing. All training videos are standardized to resolution of 1280720 and downsampled to 10 fps. We trim each clip to ensure its frame count satisfies mod 4 = 1, which is required by both Minimax-Remover and Wan for frame-aligned generation. During fine-tuning, Figure 4. Comparison of video inpainting methods on the robot arm removal task, evaluated on sample from the Droid [28] dataset. We adopt flow-matching objective. Let z0 = zV , sample z1 (0, I), and linearly interpolate zt = (1 t)z0 + tz1 with target velocity vt = dzt dt = z1 z0. We train the conditional velocity field uθ: = EtU (0,1), (Hr,Vr)Dr, z1N (cid:13) 2 . (cid:13) 2 (6) At inference, robot videos (cid:98)Vr can be generated with the trained generator Gθ from robot video H2Rep Hr: (cid:104)(cid:13) (cid:13)uθ(zt, t, c) vt (cid:105) (cid:98)Vr = Gθ(Hr, z1, t, ctext). (7) Our H2R-Grounder ensures supervision Vr comes from real robot videos with genuine physical interactions, encouraging physically plausible generations. 3.4. Human video robot video Given an arbitrary third-person HOI video Vh, we construct its H2Rep and feed it to the trained generator. Person segmentation and hand pose. For any given HOI video Vh, we first employ Grounded-SAM 2.1 to obtain its mask sequence Mh. Meanwhile, we use ViT-Pose [60] to estimate the human body pose and locate the hand bounding box Bh, followed by HaMeR [44] to accurately estimate the hand pose Phand. We then take the midpoint between the index fingertip and thumb tip as the hand position, and the direction of the thumb as its orientation, forming surrogate pose Ph that effectively represents the hands spatial position and direction. Empirically, we find that Ph aligns well to serve as surrogate for the projected gripper pose in robot manipulation videos. Mh = S(Vh, person), Ph = D(Vh) (estimate surrogate 2D hand pose). (8) Person removal (background video). We use MinimaxRemover to remove person from the video: VI = I(cid:0)Vh, Mh (cid:1). (9) 5 Figure 5. OOD H2R transfer. Top row: results on internet videos. Bottom row: results on DexYCB [9] videos. Figure 6. Ablation on Droid [28] data. Examples obtained by extracting frames at the same timestep. we randomly sample clip of up to 49 frames from each training video. Thanks to Wans strong pretraining, the finetuned generator generalizes well to human videos of different frame rates during inference. 6 Backbone. We fine-tune the Wan 2.2 TI2V-5B model [54] as our primary video generator. Our H2R-Grounder establishes novel paradigm for translating human videos into robot manipulation videos. Under this paradigm, the video generator can be replaced with other conditional video generation frameworks. We study another popular generator VACE [25], which adopts ControlNet [62]-based conditioning mechanism instead of in-context learning. Since VACE depends heavily on accurate textual descriptions, we additionally use Qwen2.5-VL [3] to automatically generate detailed captions for all training and testing videos. We finetune our in-context model for 200 steps with mini-batch size of 4, using 8 NVIDIA H200 GPUs and gradient accumulation factor of 2. For VACE, we train for 2 full epochs on the entire dataset to ensure convergence. 4.2. Comparison with Baselines 4.2.1. Rendering-Based Methods Rendering-based approaches such as Phantom [32] and Masquerade [31] require precise handrobot calibration to compute the transformation between the camera and robot frames, as well as accurate camera intrinsics and vertical field-of-view parameters for physically correct rendering of robot arms. Such parameters are unavailable for our in-thewild human videos, making direct comparison infeasible. Therefore, these methods are excluded from evaluation. 4.2.2. Animation-Based Methods We adapt the recently proposed robot I2V method RoboMaster [16] to the human-to-robot (H2R) translation setting. The original system animates robotobject interaction videos from static image given user-defined robot and object trajectories. To enable comparison under our setup, we construct the required inputs through semi-manual process: (1) The first frame of the humanobject video is inpainted to remove the human, serving as the reference frame; (2) hand pose trajectories are extracted following our H2R-Grounder pipeline and used as surrogates for robot trajectories; (3) the interacted object is manually selected and segmented using SAM 2.1 [46] to obtain its mask; (4) its motion trajectory is tracked by CoTracker3 [26]; (5) the trajectory is manually divided into pre-interaction, interaction, and postinteraction phases; and (6) textual caption describing the robot motion is written. This process allows RoboMaster to generate robotobject interaction animations, albeit with heavy manual preparation. 4.2.3. Commercial Video-Editing Methods Commercial video-editing systems such as Kling [29] and Runway Aleph [48] can replace the subject of video while roughly maintaining temporal coherence and background appearance. We upload an image of Franka robotic arm and prompt Kling to replace the human in each input video with the robot arm. For Aleph, we similarly prompt it to replace the human in the video with Franka robotic arm. This serves as practical baseline representing appearance-level subject replacement rather than true generative translation. 4.2.4. Quantitative Results on DexYCB Tab. 1 and Tab. 2 summarize the results on the DexYCB test set. We evaluate H2R-Grounder, Kling, and RoboMaster through both human studies and VLM-based scoring. Human study. We conduct user study with 22 participants, all holding computer-science backgrounds (bachelors, masters, or PhD). Each participant ranks the outputs from the three methods in terms of motion consistency, background consistency, visual quality, and physical plausibility (measured by structure integrity and contact realism). We report the first-rank ratethe percentage of participants who selected method as best for each aspect. Ties are allowed in the ranking, so the total percentages may not sum to 100%. As shown in Tab. 1, H2R-Grounder achieves the highest first-rank preference across all four evaluation aspects. It is most favored in visual quality (61.4%) and physical plausibility (63.6%), indicating that our generated videos are both visually convincing and physically coherent, with accurate object contacts. The high preference in motion consistency (54.5%) and background consistency (56.8%) further demonstrates that our model produces temporally stable motions while preserving contextual alignment. Kling ranks second, benefiting from its commercial editing pipeline, which yields visually appealing results (40.9%) and stable backgrounds (34.1%). However, it struggles with motion consistency (9.1%) and physical plausibility (9.1%), where the synthesized arms often lose structure or exhibit implausible interactions. Runway Aleph achieves moderate results, particularly in motion consistency (22.7%), but remains less realistic overall. RoboMaster performs the weakest, with preference rates around 23% across most aspects, showing that manually defined trajectories fail to capture natural motion or consistent visual quality. Overall, the human study demonstrates that H2R-Grounder achieves the best balance between motion realism, physical grounding, and visual fidelity, without relying on paired data or calibration. VLM evaluation. We further evaluate using Gemini [12], multimodal visuallanguage model, to rate each generated video on 15 scale across the same four criteria  (Table 2)  . The VLM results align with human preferences: H2R-Grounder attains the highest or comparable scores in motion consistency (3.7), background consistency (4.9), and physical plausibility (4.4), confirming its robust understanding of scene dynamics and contact physics. Kling achieves slightly higher visual quality (4.1 vs. 4.0), likely due to its polished rendering style, but lags behind in realism-related aspects. RoboMaster again performs the worst, limited by its predefined, non-adaptive motion generation. Together, these results highlight that H2R-Grounder delivers the most 7 Table 1. Human preference rate on DexYCB. Users are asked to rank the three generated videos, and our model is most frequently selected as the top choice for all aspects."
        },
        {
            "title": "Physical Plausibility",
            "content": "RoboMaster [16] Runway Aleph [48] Kling [29] Ours 2.3% 22.7% 9.1% 54.5% 2.3% 15.9% 34.1% 56.8% 2.3% 9.1% 40.9% 61.4% 18.2% 6.8% 9.1% 63.6% Table 2. VLM scoring on DexYCB. We prompt Gemini [12] to rate the generated videos across four aspects. Our model outperforms the baselines on most metrics, with slight drop in visual quality compared to Kling [29]."
        },
        {
            "title": "Physical Plausibility",
            "content": "RoboMaster [16] Runway Aleph [48] Kling [29] Ours 2.6 3.7 3.5 3.7 4.5 4.5 4.9 4.9 3.5 3.6 4.1 4.0 2.8 3.9 3.6 4.4 balanced and physically grounded video generation among all baselines. Table 3. Quantitative ablation on the Droid dataset. indicates higher is better; indicates lower is better. SSIM LPIPS HR-Grounder 5B (ours) w/o pose indicator w/o LoRA w/ 14B backbone w/ VACE [25] (1.3B) w/ VACE [25] (14B) 0.82 0.80 0.80 0.79 0.68 0.71 0.22 0.23 0.26 0.23 0.30 0.27 leads to noticeable motion drift: the generated robot arm often deviates from the intended trajectory, confirming that the pose cue is essential for motion control. Without LoRA fine-tuning, the model tends to overfit and does not generate an robot arm. Replacing the in-context video generator with VACE yields lower SSIM and higher LPIPS, showing that ControlNet-based conditioning is less effective for maintaining motionbackground coherence. Scaling to 14B backbone does not yield clear quality improvements but drastically slows inference and limits sequence length (49 17 frames). Considering both accuracy and efficiency, we adopt the 5B model with in-context learning as our final configuration. 4.2.5. Qualitative Results Fig. 5 presents qualitative comparisons of H2R-Grounder against existing baselines on both internet videos and DexYCB sequences. Although our video generator is finetuned only on the DROID indoor dataset, it generalizes well to in-the-wild videos, maintaining consistent backgrounds, accurate motion alignment, and sharp visual quality across different viewpoints. In contrast, Kling and Runway Aleph often produces structurally inconsistent robot arms that deviate from real-world kinematics, while RoboMaster significantly distorts the background and fails to follow the demonstrated motion precisely. As shown in the bottomright example, H2R-Grounder accurately positions the gripper to grasp the banana tip, faithfully following the human hand trajectory. 4.3. Ablation Study Tab. 3 and Fig. 6 analyze the effect of key components in H2R-Grounder. Removing the pose indicator from H2Rep 8 5. Conclusion and Limitation We presented H2R-Grounder, paired-data-free framework that translates human interaction videos into physically grounded robot manipulation videos. Leveraging the unified representation H2Rep, our approach effectively bridges the visual embodiment gap and generates motion-consistent, realistic robot videos without calibration or paired supervision. Limitation. Currently, the framework supports only single-hand to single-arm translation. Extending it to bimanual scenarios is feasible with appropriate dualarm robot data and will be explored in future work. Moreover, as training is conducted solely on datasets featuring the Franka robot arm, H2R-Grounder currently produces only Franka-style outputs. Adapting to other robot embodiments would require fine-tuning or training lightweight LoRA adapters for each robot type."
        },
        {
            "title": "References",
            "content": "[1] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-torobot imitation in the wild. arXiv preprint arXiv:2207.09450, 2022. 3 [2] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as versatile representation for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1377813790, 2023. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [4] Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar, and Shubham Tulsiani. Towards generalizable zero-shot manipulation via translating human interaction plans. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 69046911. IEEE, 2024. 2, 3 [5] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. In European Conference on Computer Vision, pages 306324. Springer, 2024. 3 [6] Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, and Jun Zhu. H-rdt: Human manipulation enhanced bimanual robotic manipulation. arXiv preprint arXiv:2507.23523, 2025. [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. 2 [8] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 3 [9] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 90449053, 2021. 5, 6 [10] Lawrence Yunliang Chen, Chenfeng Xu, Karthik Dharmarajan, Muhammad Zubair Irshad, Richard Cheng, Kurt Keutzer, Masayoshi Tomizuka, Quan Vuong, and Ken Goldberg. Rovi-aug: Robot and viewpoint augmentation for crossembodiment robot learning. arXiv preprint arXiv:2409.03403, 2024. 3 [11] Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martın-Martın, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry 9 Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models, 2025. 2, 3 [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 7, 8 [13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 130:3355, 2022. 3 [14] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, and Franziska Meier. Model-based inverse reinforcement learning from visual demonstrations. In Conference on Robot Learning, pages 19301942. PMLR, 2021. 3 [15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:9156 9172, 2023. [16] Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, and Dahua Lin. Learning video generation for robotic manipulation with collaborative trajectory control. arXiv preprint arXiv:2506.01943, 2025. 1, 3, 7, 8 [17] Franka Robotics GmbH. Franka robotics homepage. https://franka.de/, 2025. Accessed: 2025-11-14. 5 [18] Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, and Dieter Fox. Ifor: Iterative flow minimization for robotic object rearrangement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1478714797, 2022. 3 Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. 3 [20] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination, 2020. 2 [21] Siddhant Haldar and Lerrel Pinto. Point policy: Unifying observations and actions with key points for robot manipulation. arXiv preprint arXiv:2502.20391, 2025. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 4 [23] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 2 [24] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, and Linxi Fan. Dreamgen: Unlocking generalization in robot learning through video world models, 2025. 3 [25] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. 2, 7, 8 [26] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60136022, 2025. 7 [19] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, [27] Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, and Danfei Xu. 10 Egomimic: Scaling imitation learning via egocentric video, 2024. 2, 3 [28] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. 2, 3, 5, 6 [29] Kuaishou. Kling. https://klingai.com/, 2024. Accessed: 2025-11-08. 1, 7, [30] Marion Lepert, Ria Doshi, and Jeannette Bohg. Shadow: Leveraging segmentation masks for cross-embodiment policy transfer. arXiv preprint arXiv:2503.00774, 2025. 3 [31] Marion Lepert, Jiaying Fang, and Jeannette Bohg. Masquerade: Learning from in-the-wild human videos using dataediting, 2025. 2, 3, 7 [32] Marion Lepert, Jiaying Fang, and Jeannette Bohg. Phantom: Training robots without robots using only human videos, 2025. 2, 3, 7 [33] Guangrun Li, Yaoxu Lyu, Zhuoyang Liu, Chengkai Hou, Jieyu Zhang, and Shanghang Zhang. H2r: human-to-robot data augmentation for robot pre-training from videos, 2025. 2, 3 [34] Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, et al. Mimicdreamer: Aligning human and robot demonstrations for scalable vla training. arXiv preprint arXiv:2509.22199, 2025. 3 [35] Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, and Yuke Zhu. Okami: Teaching humanoid robots manipulation skills through single video imitation. arXiv preprint arXiv:2410.11792, 2024. 3 [36] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. [37] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and MingMing Cheng. Towards an end-to-end framework for flowguided video inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1756217571, 2022. 4 [38] Vincent Liu, Ademi Adeniji, Haotian Zhan, Siddhant Haldar, Raunaq Bhirangi, Pieter Abbeel, and Lerrel Pinto. Egozero: Robot learning from smart glasses, 2025. 2, 3 [39] Yangcen Liu, Woo Chul Shin, Yunhai Han, Zhenyang Chen, Harish Ravichandar, and Danfei Xu. Immimic: Cross-domain imitation from human videos via mapping and interpolation, 2025. 3 [40] Priyanka Mandikal and Kristen Grauman. Dexvip: Learning dexterous grasping with human hand pose priors from video. In Conference on Robot Learning, pages 651661. PMLR, 2022. 2 [41] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. arXiv preprint arXiv:2308.10901, 2023. 3 Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, WeiCheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. [43] NVIDIA, :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, and Yuke Zhu. World simulation with video foundation models for physical ai, 2025. 3 [44] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In CVPR, 2024. 5 [45] Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David Yoon, Ryan Hoque, Lars Paulsen, et al. Humanoid policy human policy. arXiv preprint arXiv:2503.13441, 2025. 3 [46] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 7 [42] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, [47] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang 11 [59] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samarth Sinha, and Animesh Garg. Learning by watching: Physical imitation of manipulation skills from human videos. In 2021 IEEE/RSJ international conference on intelligent robots and systems (iros), pages 78277834. IEEE, 2021. 3 [60] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. ViTPose: Simple vision transformer baselines for human pose estimation. In Advances in Neural Information Processing Systems, 2022. 5 [61] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, and Sifei Liu. Affordance diffusion: Synthesizing hand-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2247922489, 2023. 3 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 7 [63] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 [64] Yuxiang Zhou, Yusuf Aytar, and Konstantinos Bousmalis. Manipulator-independent representations for visual imitation. arXiv preprint arXiv:2103.09016, 2021. 2, [65] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive realrobot action simulators. arXiv preprint arXiv:2406.14540, 2024. 3 [66] Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, and Kam-Fai Wong. Minimax-remover: Taming bad noise helps video object removal. arXiv preprint arXiv:2505.24873, 2025. 4 Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 3 [48] Runway. Runway aleph. https://runwayml.com/ research/introducing-runway-aleph, 2025. Accessed: 2025-11-08. 1, 7, 8 [49] Daniel Seita, Yufei Wang, Sarthak Shetty, Edward Yao Li, Zackory Erickson, and David Held. Toolflownet: Robotic manipulation with tools via predicting tool flow from point clouds. In Conference on Robot Learning, pages 10381049. PMLR, 2023. 3 [50] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In Conference on Robot Learning, pages 654665. PMLR, 2023. [51] Mohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, and Abhinav Gupta. Hrp: Human affordances for robotic pretraining. arXiv preprint arXiv:2407.18911, 2024. 3 [52] GigaWorld Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, et al. Gigaworld-0: World models as data engine to empower embodied ai. arXiv preprint arXiv:2511.19861, 2025. 3 [53] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. 2, 3 [54] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced largescale video generative models, 2025. 2, 4, 7 [55] Lirui Wang, Kevin Zhao, Chaoqi Liu, and Xinlei Chen. Learning real-world action-video dynamics with heterogeneous masked autoregression. arXiv preprint arXiv:2502.04296, 2025. 3 [56] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. [57] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:6808268119, 2024. 3 [58] Sicheng Xie, Haidong Cao, Zejia Weng, Zhen Xing, Haoran Chen, Shiwei Shen, Jiaqi Leng, Zuxuan Wu, and Yu-Gang Jiang. Human2robot: Learning robot actions from paired human-robot videos, 2025. 3 12 H2R-Grounder: Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Motivation of H2Rep In this paper, our H2Rep representation overlays the abstract pose sequence onto the background video using an α-blending scheme. Another natural design is to treat pose and background as two separate video streamsone containing only the background, and the other containing only the pose rendered on white or black canvas. This alternative preserves more disentangled information. However, under an in-context generation framework, using dual video streams would effectively double the input tokens, causing both computation and memory to scale quadratically (i.e., 4). To balance efficiency and expressiveness, we adopt the α-blended formulation: the pose is overlaid with controlled transparency so as to minimally affect background content while substantially reducing computational and memory costs. Moreover, this representation remains pixel-aligned with both the human reference and the final generated robot video, which facilitates learning for the video generator. 7. Inference Efficiency Our 5B in-context model runs at about 13 seconds per frame, taking about 648 seconds to generate 49-frame 7041280 video on single H200 GPU, with peak memory consumption of 63 GB."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}