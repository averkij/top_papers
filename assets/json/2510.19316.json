{
    "paper_title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints",
    "authors": [
        "Kailin Jiang",
        "Hongbo Jiang",
        "Ning Jiang",
        "Zhi Gao",
        "Jinhe Bi",
        "Yuchen Ren",
        "Bin Li",
        "Yuntao Du",
        "Lei Liu",
        "Qing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting."
        },
        {
            "title": "Start",
            "content": "KORE: ENHANCING KNOWLEDGE INJECTION FOR LARGE MULTIMODAL MODELS VIA KNOWLEDGEORIENTED AUGMENTATIONS AND CONSTRAINTS Kailin Jiang1,2, Hongbo Jiang3, Ning Jiang4, Zhi Gao5,2, Yuchen Ren7, Bin Li1, Yuntao Du8, Lei Liu1(cid:66), Qing Li2(cid:66) 1University of Science and Technology of China 2State Key Laboratory of General Artificial Intelligence, BIGAI 3Xiamen University 6Ludwig Maximilian University of Munich 5Beijing Institute of Technology 4Northeast Forestry University 7University of Sydney 8Shandong University Jinhe Bi6,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMMs linear layer activations and initializes the adapter by projecting the original weights into the matrixs null space, defining fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5 (7B), LLaVA-v1.5 (13B), and Qwen2.5-VL (7B), show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting. https://kore-lmm.github.io/ 5 2 0 2 2 2 ] . [ 1 6 1 3 9 1 . 0 1 5 2 : r Figure 1: (a) Comparison between KORE and current methods for knowledge injection. (b) Performance of various methods on LLaVA-v1.5 (7B). Red and blue shading correspond to knowledge adaptation and retention evaluations, respectively. (cid:66) Corresponding author."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate remarkable ability to store vast world knowledge within their pre-trained weights and recall it during inference (Petroni et al., 2019; Brown et al., 2020; Roberts et al., 2020; Liu et al., 2024a; Bi et al., 2025c). However, their knowledge remains static and fails to keep pace with the evolving real world, leading to outdated responses and an inability to acquire new information continuously. Therefore, effective knowledge injection methods are crucial, enabling models to inject new knowledge while preserving previous knowledge (e.g., knowledge adaptation and retention in Figure 1 (a)), thus supporting continuous model evolution (Ovadia et al., 2024; Mecklenburg et al., 2024). The most direct method for injecting new knowledge is full fine-tuning, which updates all model weights. However, this strategy incurs prohibitive computational and storage costs. To address this, Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced for resource-friendly adaptation. PEFT techniques, such as adding adapters (Houlsby et al., 2019; Hu et al., 2022; Bi et al., 2025b) or new tokens (Lester et al., 2021; Sabbatella et al., 2024), drastically reduce the number of trainable parameters by freezing the original pre-trained weights. Despite their success, both full fine-tuning and PEFT methods face significant limitations. They often lead to catastrophic forgetting of pre-existing knowledge and struggle to achieve robust generalization. While full fine-tuning can minimize loss on the training data ( F), it frequently overfits (Bi et al., 2025a), failing to effectively extract and manipulate the newly acquired knowledge (e.g., Full-FT repeats training data in Figure 1 (a)). Numerous continual learning techniques, such as rehearsal (Li & Hoiem, 2017a; Hou et al., 2019) and parameter regularization (Kirkpatrick et al., 2017; Li & Hoiem, 2017b), have been proposed to mitigate catastrophic forgetting. However, these methods often fail to balance new knowledge acquisition with prior knowledge retention. For example, regularization approaches like EWC (Kirkpatrick et al., 2017) may impair adaptation to new data, resulting in irrelevant responses and instruction forgetting (e.g., EWC leads to irrelevant answer and instruction forgetting in Figure 1 (a)). Drawing inspiration from data augmentations ability to enhance new knowledge learning (Singhal et al., 2023; Allen-Zhu & Li, 2024) and continual learnings capacity to preserve old knowledge (McCloskey & Cohen, 1989; Ratcliff, 1990), our proposed KORE optimizes the balance between injecting new knowledge and preserving old knowledge, enabling accurate adaptation and powerful retention. Overall, KORE is synergistic method for knowledge-oriented augmentation and constraint. Unlike general augmentation techniques that produce superficial and discrete data variations, KORE automatically augments each piece of knowledge into multi-rounds of dialogue and instruction tasks data. This process constructs profound and structured knowledge, which ensures the generalization and internalization of new knowledge and enables the model to flexibly extract and manipulate learned knowledge during inference. Simultaneously, KORE stores multimodal knowledge in covariance matrix of linear layer activations, assuming effectively captures previous knowledge (Verification in 3.3). We then decompose and extract its null space. Original weights are projected into this null space to initialize adapter for fine-tuning, which ensures tuning direction that minimally interferes with the previous knowledge, thereby achieving knowledge-driven fine-tuning constraint. To validate the effectiveness of our method, we conducted extensive experiments on multiple representative LMMs. The results in Figure 1 (b) demonstrate that KORE exhibits superior performance in both knowledge adaptation and retention compared to standard fine-tuning (e.g., Full-FT, LoRA) and continual learning methods (e.g., EWC, SEFE). Moreover, KORE can augment arbitrary knowledge into structured format and enables customizable knowledge constraints that can be applied based on specific retention needs( 4.2). By balancing adaptation and retention through knowledge-oriented control, KORE achieves superior performance without sacrificing flexibility, highlighting its key role in efficient knowledge injection for broader application."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 KNOWLEDGE INJECTION Injecting new knowledge into LLMs and LMMs is critical challenge with two main paradigms. One approach, Retrieval-Augmented Generation (Song et al., 2016; Fan et al., 2020; Lewis et al., 2020),"
        },
        {
            "title": "Preprint",
            "content": "preserves pre-trained knowledge by leveraging an external knowledge base at inference time, but its efficacy depends on the retrieval systems quality and speed. In contrast, the alternative paradigm directly modifies model parameters, often through efficient methods like full fine-tuning, parameterefficient fine-tuning (Hu et al., 2022; Lauscher et al., 2020). However, these techniques face dual challenge, as they often struggle to effectively inject knowledge while still causing catastrophic forgetting (Ovadia et al., 2024; Mecklenburg et al., 2024). This highlights fundamental trade-off between knowledge adaptation and retention, which remains core problem in knowledge injection."
        },
        {
            "title": "2.2 KNOWLEDGE FORGETTING",
            "content": "Catastrophic forgetting refers to phenomenon of models to lose previously acquired capabilities when adapting to new tasks (McCloskey & Cohen, 1989; Ratcliff, 1990). The continual learning field has proposed various strategies to address this issue, such as knowledge distillation (Li & Hoiem, 2017a; Hou et al., 2019), rehearsal (Li & Hoiem, 2017a; Hou et al., 2019), parameter regularization (Kirkpatrick et al., 2017; Li & Hoiem, 2017b), dynamic architectures (Yan et al., 2021), and complementary projection-based methods (Farajtabar et al., 2020; Chaudhry et al., 2020; Saha et al., 2021). In the era of LLMs and LMMs, world knowledge is acquired through pre-training on massive datasets, yet continual fine-tuning often leads to forgetting of previously learned knowledge. Existing methods struggle to scale effectively to models with enormous parameter sizes. Recent studies, including those on mixture-of-experts architectures (Luo et al., 2024) and orthogonal subspace constraints (Wang et al., 2023), attempt to mitigate catastrophic forgetting, but often fail to balance the retention of world knowledge with the acquisition of new tasks. In contrast, our proposed KORE optimizes the trade-off between injecting new knowledge and preserving old knowledge."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "KORE collaborates with KORE-AUGMENTATION ( 3.1) and KORE-CONSTRAINT ( 3.2) to address the core challenges of knowledge injection, as detailed below. Figure 2: Overview of KORE, synergistic method for knowledge-oriented augmentation and constraint. KORE-AUGMENTATION automatically converts each piece of knowledge into profound and structured knowledge. KORE-CONSTRAINT minimizes interference with previous knowledge by initializing an adapter with null space that stores covariance matrix of previous knowledge. 3.1 KNOWLEDGE-ORIENTED AUGMENTATION Existing knowledge injection methods suffer from poor generalization and struggle to master new knowledge (Ovadia et al., 2024; Jiang et al., 2025; Tang et al., 2025). Inspired by recent work demonstrating that data augmentation effectively enhances generalization (Singhal et al., 2023; AllenZhu & Li, 2024; Wang et al., 2025b; Park et al., 2025), we seek to enhance the models ability to learn new knowledge through data augmentation. However, existing methods are limited to superficial and discrete augmentation, which is insufficient for helping models internalize new knowledge systematically. To address these limitations, we propose KORE-AUGMENTATION, profound and structured augmentation method via automated pipeline, to build structured and comprehensive knowledge for accurate adaptation. We observe that KORE-AUGMENTATION augments the original knowledge into multi-rounds dialogues data (forming the trunk) and instruction tasks data (forming the branches), thereby constructing"
        },
        {
            "title": "Preprint",
            "content": "a comprehensive and higher-level knowledge tree (Left part of Figure 3) that supports superior generalization and internalization of new knowledge. KORE-AUGMENTATION moves beyond enabling models to accurately fit training data for data memorization. Instead, it focuses on helping the model comprehend and reason about the inherent logic and associations within the knowledge itself. This enables the model to think, internalize new knowledge, and effectively extract and manipulate the learned knowledge, thereby achieving real knowledge internalization. Figure 3: Comparison of KORE-AUGMENTATION (left) and general augmentation methods (right). In contrast, general augmentation methods are superficial and discrete. As shown in right part of Figure 3, for text augmentation, techniques such as knowledge-aware (e.g., rephrasing) or knowledgeagnostic (e.g., synonym replacement) only create isolated variations. Likewise, image augmentation, whether knowledge-aware (e.g., semantic-preserving) or knowledge-agnostic (e.g., rotation), operate on surface level. These methods merely generate isolated data points without connection, superficially modifying existing knowledge to broaden exposure. They fail to construct coherent knowledge structure. Consequently, general augmentation methods offer limited support for the generalization and internalization of new knowledge. We experimentally validate this statement in 4.5, with implementation details as follows: Part 1: Constructing Multi-rounds of Dialogue Data. The multi-rounds of dialogue data for each knowledge sample consists of two components: heuristic Q&A (H.Q in Figure 2) and dialogue Q&A. The heuristic Q&A is constructed randomly using manually written templates. For dialogue Q&A, we design rigorous rules and diverse task examples, using GPT-4o to generate up to 10 dialogues from original textual knowledge. Ultimately, this process yields 75,710 dialogue data. Part 2: Constructing Instruction Tasks Data. We use Newss titles or Entitys names as search key words to retrieve the top five images via Google Search. Visual features of both original and collected images are extracted using CLIP (Radford et al., 2021). The two images with the highest cosine similarity are retained. ❶ Visual Recognition: For this task, questions are randomly selected from manually written template, and the answer is defined as Yes. One of the previously retained images serves as the query image, accompanied by the instruction, Answer this question with Yes or No. ❷ Image Caption: For this task, answer is summary generated by GPT-4o based on original textual knowledge. Question is randomly selected from templates, and query image is those remaining from previous steps. And instruction is Answer this question in one paragraph. ❸ VQA: First, we utilize GPT-4o to generate quadruplets (Q, A, S, H) from original textual knowledge, where and form question-answer pair, is the subject in question and is hypernym corresponding to the subject. Subsequently, the subject and hypernym are combined to form search key words for retrieving and downloading images from Google. The instruction is: Answer the question using single word or phrase. This process yields 46,468 VQA samples. Through KORE-AUGMENTATION, We construct KORE-74K using original knowledge of EVOKE, and KORE is training on KORE-74K. See more details about KORE-AUGMENTATION in H. 3.2 KNOWLEDGE-ORIENTED CONSTRAINT Large Multimodal Models effectively leverage their pre-trained knowledge to perform wide range of tasks, and these capabilities are reflected as distinct patterns within their internal activation covariance matrices (Meng et al., 2023; Yang et al., 2024). However, integrating new knowledge or skills into these models presents fundamental challenge. Direct fine-tuning, the conventional approach, often disrupts these carefully established internal structures, leading to the catastrophic forgetting of prior abilities(Rebuffi et al., 2017; Shi et al., 2024). Consequently, the field of continual learning has focused on developing various constraint-based methods to mitigate this performance degradation and preserve foundational knowledge during adaptation (Kirkpatrick et al., 2017; Li & Hoiem, 2017b). Inspired by this, we propose KORE-CONSTRAINT, knowledge-oriented constraint method. It stores previous knowledge in covariance matrix of activations from LMMs linear layers, decomposes this"
        },
        {
            "title": "Preprint",
            "content": "matrix to obtain its null space, and projects the original weights onto this subspace to initialize adapter. This process ensures that the fine-tuning direction minimally interferes with previous knowledge. Following prior work (Meng et al., 2023; Yang et al., 2024), we collect activations from LMMs on set of random samples representing pre-trained knowledge. Let the input activations to linear layer be RdinBL, where is the number of samples, is the sequence length, and din is the input dimension. And its covariance be = XX Rdindin . Given pre-trained weights W0, the fine-tuned weights through LoRA are given by: = W0+BA. To achieve knowledge retention, we want to ensure the output activations derived from pretrained knowledge remain consistent after fine-tuning, formalized by the following condition: = (W0 + BA)C W0C. Simplifying this equation further, we obtain: BAC 0, and to solve this problem, our goal is to have located in the null space matrix (Wang et al., 2021) of C, which is formulated as AC = 0. Following the existing methods for conducting null space projection (Wang et al., 2021), we first apply Singular Value Decomposition (SVD) to = XX : SVD (cid:0)X(X)T (cid:1) = (cid:88) i=1 σiuiuT , (1) where is orthogonal matrix of left singular vectors, respectively, and Λ is diagonal matrix with singular values σ1 σ2 σR > 0 (with = rank(C)). The null space of is spanned by Unull Rdin(dinR), submatrix containing the last (din R) columns of that correspond to zero singular values. As shown in the first step on the right side of Figure 2, Unull satisfies nullC = 0. We approximate the null space with ˆU Rdinr, submatrix containing the left singular vectors from associated with the smallest singular values, where is the predefined LoRAs rank. From this, we define knowledge-oriented constraint projector = ˆU ˆU . As shown in Figure 2, we then initialize the LoRA adapters by factorizing the pre-trained weights projected into this null space. We compute the SVD of the projected weights: SVD (W0P ) = (cid:8)U , Σ, (V )T (cid:9) and initialize the adapter matrices and as: = Σ, = Σ(V )T , (2) Σ denotes the diagonal matrix with entries for singular values. Finally, to ensure the model where is unchanged at the start of fine-tuning, the original weight matrix is adjusted with residual term: 0 = W0 BA. (3) Given the asymmetry between and B, fine-tuning only suffices for strong performance(Zhang et al., 2023; Zhu et al., 2024). Thus, KORE freezes A, which lies in the null space of C. This ensures AC 0, rendering the update term BAC negligible regardless of Bs updates. Proof is in C. 3.3 ANALYSIS OF KNOWLEDGE-ORIENTED CONSTRAINT Figure 4: Performance (higher is better) on (a) MME (Fu et al., 2023) and (b) ScienceQA (Lu et al., 2022) after reconstruction. (c) Covariance matrix visualization for 4 different input activations in the 0-th block. We down-sample the heatmaps into 3232. Similar patterns are marked in red circles."
        },
        {
            "title": "Preprint",
            "content": "KORE-CONSTRAINT relies on the premise that the extracted covariance matrix effectively captures knowledge from previous data. Therefore, we expand CO-SVD (Yang et al., 2024) from pure text scenarios to multimodal scenarios to verify whether covariance matrices can capture multimodal knowledge and activate distinct modes? We apply Plain SVD, ASVD (Yuan et al., 2023) and CO-SVD to fully decompose all layers of LLaVA-v1.5 (7B) pre-trained weights. The weights are reconstructed by removing the components corresponding to the smallest singular values. Our analysis reveals two key findings: ❶ Figure 4 (a) and (b) demonstrate that CO-SVD exhibits superior performance retention compared to Plain SVD, ASVD and suggest that multimodal knowledge can be effectively captured and stored in covariance matrix. ❷ Figure 4 (c) shows that covariance matrices of linear layer inputs share similar outlier patterns for related tasks (e.g., POPE and HallusionBench), but differ from unrelated ones (e.g., MMBench), indicating that distinct tasks exhibit different outlier distributions in the covariance matrix. To build multi-dimensional covariance matrix for KORE, we finally sample 64 examples per category from OneVisions (Li et al., 2025) single-image subset (General, Doc/Chart/Screen, Math/Reasoning, General OCR). See details in D."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we introduce experimental content. See more details and evaluation protocol in B. Setup 1: Knowledge Adaptation Evaluation. we evaluate knowledge adaptation capabilities of pre-trained LMMs (e.g., LLaVA-v1.5 (7B), LLaVA-v1.5 (13B) (Liu et al., 2024b), and Qwen2.5-VL (7B) (Bai et al., 2025)) by fine-tuning them on EVOKE (Jiang et al., 2025), where knowledge is injected as an image-text pair, with evaluation questions derived from the text. Setup 2: Knowledge Retention Evaluation. We evaluate fine-tuned LMMs on 12 benchmarks across 7 capability dimensions. Specifically, evaluation covers the following tasks: ❶ Comprehensive Evaluation (COM): MME (Fu et al., 2023) and MMBench (Liu et al., 2024c); ❷ Optical Character Recognition (OCR): SEEDBench2 Plus (Li et al., 2024) and OCRVQA (Mishra et al., 2019); ❸ Multidisciplinary Reasoning (M-DIS): ScienceQA (Lu et al., 2022) and MMMU (Yue et al., 2024); ❹ Instruction Following (INS): MIA-Bench (Qian et al., 2024); ❺ Multi-Turn MultiImage Dialog Understanding (M-IDU): MMDU (Liu et al., 2025); ❻ Mathematical Reasoning (MAT): MathVista (Lu et al., 2024) and MathVision (Wang et al., 2025a); ❼ Hallucination (HAL): POPE (Li et al., 2023) and HallusionBench (Guan et al., 2024). Setup 3: Baseline Methods. We compare against several baselines: Full-FT, LoRA, Replay, EWC, LwF (Li & Hoiem, 2017b), MoELoRA (Luo et al., 2024), O-LoRA (Wang et al., 2023) and SEFE (Chen et al., 2025). Specifically, Replay is implemented via LoRA, which mixes in fixed quantity (10% of EVOKEs size) of randomly sampled data from the LMMs pre-training corpus. Table 1: Performance of KORE in knowledge adaptation and retention compared with eight baseline methods. Row of LLaVA-v1.5 (7B) shows retention performance of pre-trained model. Bold and underline denote the top and runner-up scores, respectively. Avg score is the mean of the separate averages for adaptation and retention. Results with gray texture are excluded from sorting. Method #Params EVOKE CEM F1 LLaVA-v1.5 (7B) Full-FT LoRA Replay EWC LwF MoELoRA O-LoRA SEFE KORE (r=235) KORE (r=256) 6,759M 340M 340M 340M 340M 340M 340M 340M 340M 369M 18.02 15.23 11.36 15.49 14.58 6.45 6.44 13.38 30.65 31.05 15.17 18. 17.98 19.42 19.99 12.20 12.08 16.88 41.26 41.32 4.1 ANALYSIS OF MAIN RESULTS COM OCR M-DIS INS M-IDU MAT HAL Avg 65. 43.55 48.96 59.72 49.42 53.14 60.79 61.47 42.06 52.41 52.48 45.59 21.55 27.01 37.98 32.88 28.77 38.79 40.91 20. 40.98 39.96 49.22 45.67 43.79 48.64 45.46 43.41 48.27 48.07 40.17 48.68 48.96 66. 25.25 29.66 62.33 29.79 36.19 35.03 34.85 17.73 38.54 60.02 26.37 13.03 13.70 19.31 13.36 13.68 17.85 17.28 13. 16.58 23.18 19.33 18.32 18.02 19.17 18.00 18.22 19.79 19.87 18.20 18.59 18.09 54. 16.09 41.38 51.67 43.50 44.18 49.99 51.12 39.30 51.75 51.50 23.23 24.28 28.68 25.33 25.61 23.98 24.17 22. 37.09 39.11 We present case studies of various methods in and report knowledge adaptation and retention performance of fine-tuned models, drawing the following observations from Table 1:"
        },
        {
            "title": "Preprint",
            "content": "Obs 1: KORE enables accurate adaptation for effectively injecting new knowledge. Specifically, KORE (rank=235) achieves improvements of 12.63 in CEM and 21.27 in F1-Score over the best baseline on EVOKE, even outperforming LoRA by more than twofold. Obs 2: KORE enables powerful retention for effectively preserving old knowledge. Specifically, KORE (rank=235) outperforms LoRA across all knowledge retention tests, achieving top scores on OCR, M-DIS, HAL, and placing second on INS. Despite containing both multi-rounds dialogue and instruction tasks data, KORE-74Ks performance on INS and M-IDU is suboptimal. We attribute this to the number of trainable parameters  (Table 16)  and the source of the covariance matrix  (Table 13)  . For instance, when r=256, KORE shows powerful retention performance, trailing Replay by mere 2.31 on INS and outperforming it by 3.87 on M-IDU. Obs 3: KORE achieves remarkable holistic performance by harmonizing the dual objectives of knowledge injection. Specifically, KORE (rank=235) achieves an 8.41 improvement over the strongest baseline, demonstrating its superior comprehensive performance. These gains arise from KORE ability to optimize the trade-off between injecting and preserving knowledge."
        },
        {
            "title": "4.2 ANALYSIS OF KNOWLEDGE ADAPTATION AND RETENTION’S DETAILED RESULTS",
            "content": "In this section, we present detailed breakdown of performance on knowledge retention for each benchmark, specific knowledge-oriented constraints and fine-grained knowledge adaptation. Figure 5: Comparison between KORE and baseline methods on fine-grained knowledge types. Obs 4: KORE demonstrates superior performance across wide spectrum of fine-grained knowledge. Figure 5 compares 20 fine-grained News and Entity types from EVOKE. KORE consistently outperforms all baselines, demonstrating strong and comprehensive knowledge adaptation. Obs 5: KORE achieves competitive knowledge retention. Specifically, KORE outperforms LoRA (e.g., 6.53 in Avg) and continual learning methods (e.g., EWC, LwF and SEFE), achieving top scores on OCRVQA, MMMU and HallB. Furthermore, by adjusting trainable parameters (rank=256) and covariance matrix source  (Table 13)  , it closely matches or even exceeds Replay. Table 2: Performance comparison between KORE and baseline methods on fine-grained knowledge retention evaluations with LLaVA-v1.5 (7B). MMB: MMBench; SEEDB2P: SEEDBench2 Plus; MathT: MathVista ; MathI: MathVision; HallB: HallusionBench. The score of MME is normalized. Method LLaVA-v1.5 (7B) Full-FT LoRA Replay EWC LwF MoELoRA O-LoRA SEFE KORE (r=235) KORE (r=256) COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMU MIAB MMDU MathT MathI HAL POPE HallB 66. 34.17 44.06 58.96 48.57 50.87 58.26 60.30 36.10 49.84 50.06 64.60 52.92 53.87 60.48 50.26 55.41 63.32 62.63 48. 54.98 54.90 38.78 31.44 30.22 38.34 33.60 32.02 37.42 37.90 22.79 37.73 36.89 52. 11.65 23.80 37.73 32.16 25.52 40.17 43.91 18.07 44.24 43.03 69.83 67.13 66.18 68.77 65.71 66.21 69.04 68.84 65. 68.06 68.51 28.60 24.20 21.40 28.50 25.20 20.60 27.50 27.30 15.30 29.30 29.40 66. 25.25 29.66 62.33 29.79 36.19 35.03 34.85 17.73 38.54 60.02 26.37 13.03 13.70 19.31 13.36 13.68 17.85 17.28 13. 16.58 23.18 25.50 24.70 23.20 25.20 23.30 24.40 27.80 28.20 26.00 25.10 24.70 13. 11.94 12.83 13.13 12.76 12.04 11.78 11.55 10.39 12.09 11.48 86.87 74.22 73.97 85.44 76.22 79.23 80.70 81.46 72. 80.99 80.77 21.76 9.27 8.78 17.90 10.77 9.13 19.29 20.78 5.79 22.51 22.23 Avg 46.74 31.66 33.47 43.00 35.14 35.44 40.51 41.25 29.27 40.00 42.10 Given the diverse prior knowledge of LMMs, we investigate whether KORE can preserve specific knowledge without compromising new knowledge injection or other existing abilities? We construct specific constraints by sampling 256 data per benchmark across four dimensions. Obs 6: Specific constraints enhance knowledge retention and overall performance. Table 3 shows that specific constraints slightly reduce K.A score but substantially improve K.R and overall performance. Figure 6 further shows that specific constraints enhance targeted knowledge retention, notably with 7.17 gain on MME, demonstrating their potential for tailored knowledge retention."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance of knowledge adaptation (K.A) and retention (K.R) under specific knowledge-oriented constraints. Method KORE KOREMME KORE KORE KORE OCRVQA MathT HallB K.A K.R Avg 35.96 38.22 37.09 34.46 34.85 35.20 34.96 43.16 42.21 42.87 42.09 38.81 38.53 39.03 38. Figure 6: Performance comparison of corresponding tasks under specific knowledge-oriented constraints."
        },
        {
            "title": "4.3 ANALYSIS OF VARIOUS LMM SCALES AND ARCHITECTURES",
            "content": "We further evaluate the universality and robustness of KORE on larger and architecturally distinct models, using Replay (the strongest baseline in Table 1) and LoRA as baseline methods. Table 4: Performance comparison between KORE and baseline methods on knowledge adaptation and retention with various LMMs scales and architectures. Methods EVOKE CEM F1 COM OCR M-DIS INS M-IDU MAT HAL Avg Vanilla LoRA Replay KORE Vanilla LoRA Replay KORE 16.26 12.05 22.83 20. 66.86 60.57 65.81 32.89 44.47 59.35 14.56 11.73 14.01 18.51 81.18 52.54 78.54 22. 31.36 56.60 LLaVA-v1.5 (13B) 52.70 51.12 66.04 32.58 47. 45.96 43.72 48.42 51.39 23.26 61.04 65.10 Qwen2.5-VL (7B) 65. 70.32 78.46 64.54 69.17 67.74 22.35 65.26 65. 21.39 70.20 70.51 33.93 17.43 24.62 26.84 61. 23.25 50.72 45.02 19.64 15.82 19.55 20.31 47. 13.52 42.74 43.72 56.77 38.08 54.16 25.21 30. 40.52 41.44 66.96 41.38 67.48 24.21 39. 58.57 42.68 Obs 7: KORE shows enhanced superiority on larger-scale LMM. Table 4 shows that KORE surpasses LoRA (e.g., 16.63 in CEM and 21.64 in F1-Score) on EVOKE, and achieves superior K.R performance across all six dimensions including M-DIS. With an overall improvement of 10.74 over Replay, these results confirm KOREs strong potential for larger LMMs. Obs 8: KORE effectiveness is not architecture-specific. On Qwen2.5-VL (7B), it surpasses LoRA (e.g., 12.63 in CEM and 21.27 in F1-Score) and Replay (e.g., 3.40 in Avg). Smaller improvement stems from Qwen2.5-VLs robust knowledge system, honed via three-stage training, which reduce marginal gains from knowledge injection (e.g., Comparing Table 1 and 4, Qwen2.5VL (7B)s gains are less than LLaVA-v1.5 (7B)s with LoRA on EVOKE). Figure 7: Comparison of different ranks for KORE with LLaVA-v1.5 (7B)."
        },
        {
            "title": "4.4 ANALYSIS OF ABLATION EXPERIMENTS",
            "content": "In this section, we conduct extensive ablation studies (e.g., Rank, W/o Augmentation, W/o Constraint and W/o Frozen Matrix A) to validate the effectiveness of KOREs design. Table 5: Comparison of ablation experiment results of KORE on LLaVA-v1.5 (7B). Setting KORE EVOKE CEM F1 COM OCR M-DIS INS M-IDU MAT HAL Avg 30.65 41.26 52. 10.83 W/o Augmentation 33.93 W/o Constraint W/o Frozen Matrix 31.97 18.31 43.71 41.72 59.96 46.39 50.73 40.98 40.42 32.38 39.56 48. 47.13 46.31 48.37 38.54 32.53 32.70 35.30 16.58 16.00 15.38 16.44 18. 19.71 19.12 19.07 51.75 37.09 49.50 46.47 49.91 26.23 36.46 36.95 Obs 9: Larger rank enhance KOREs performance. Figure 7 shows clear trend: KOREs performance increases with higher rank and more trainable parameters on nearly all evaluations. KORE (rank=64) still surpasses Replay in Avg, only using less than half of parameters of Replay. Obs 10: Ablation studies reveals the effectiveness of KOREs design. Table 5 validates KOREs design, showing that each ablated component contributes positively to its overall performance. W/o Augmentation is particularly detrimental to knowledge adaptation (19.82 in CEM and 22.95 in F1-Score). Meanwhile, W/o Constraint and W/o Frozen Matrix impairs knowledge retention. 4.5 COMPARISON WITH GENERAL AUGMENTATION METHODS This section validates our claim from 3.1 that KORE-AUGMENTATION is superior to general augmentation methods. Obs 11: KORE-AUGMENTATION is superior to general augmentation methods. In Table 6, KORE-AUGMENTATION outperforms general augmentation methods across all metrics, notably achieving an 18.53 improvement in K.A over the strongest baseline. These results strongly demonstrate that KORE-AUGMENTATION is highly effective augmentation method. Table 6: Performance comparison of different augmentation methods. Method K.A K.R Avg KORE-AUGMENTATION 38.82 35.78 36.46 Augmentation for Text Knowledge-Aware Knowledge-Agnostic 20.29 15.60 34.86 35.71 27.38 25.49 Augmentation for Images Knowledge-Aware Knowledge-Agnostic 18.33 18. 34.02 32.09 25.86 25."
        },
        {
            "title": "5 LIMITATIONS & FUTURE DISCUSSION",
            "content": "While KORE demonstrates strong performance in knowledge adaptation and retention, we also recognize its limitations. The augmentation process relies on GPT-4o, which may introduce hallucinations, and is confined to enhancing individual knowledge units. Furthermore, extracting covariance matrices from all linear layers is computationally expensive. Future work will explore more structured augmentation (e.g., knowledge graphs and forest (Ji et al., 2021; Chen et al., 2020) with potential for combination with reinforcement learning), and reduce resource consumption by identifying the most critical layers for covariance computation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose KORE, synergistic method for knowledge-oriented augmentation and constraint that addresses the critical trade-off between injecting new knowledge and preserving existing knowledge. Specifically, KORE automatically converts each piece of knowledge into more profound and structured format, ensuring the model accurately learns and adapts to new knowledge. Simultaneously, it minimizes interference with previous knowledge by initializing an adapter with null space that stores the covariance matrix of previous knowledge, enabling powerful retention. KOREs robust performance is architecture-agnostic (e.g., LLaVA-v1.5 and Qwen2.5-VL) and exhibits enhanced superiority on larger-scale LMMs. Furthermore, its capability for specific knowledge-oriented constraints improves retention performance of specific knowledge, granting KORE high flexibility to address diverse scenarios with specialized preservation needs."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our KORE method offers significant value for real-world applications in knowledge injection and management by effectively injecting new knowledge while preserving old knowledge. However, while the intention behind knowledge injection is positive, it presents risk of misuse, such as the introduction of false, harmful, or biased information to compromise the model. We therefore urge the research community to utilize this technology responsibly and cautiously to ensure its ethical application."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our findings, ❶ detailed implementation specifications and hyperparameters for KORE are provided in and H; ❷ all source code will be released upon the completion of the peer-review process; ❸ all training data and weights will be publicly available on Huggingface after the completion of the peer-review process. We hope these resources will enable other researchers in the field to verify and replicate our results."
        },
        {
            "title": "REFERENCES",
            "content": "Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. In International Conference on Machine Learning, 2024. 2, 3 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. In arXiv Technical Report, 2025. 6 Jinhe Bi, Yifan Wang, Danqi Yan, Aniri, Wenke Huang, Zengjie Jin, Xiaowen Ma, Artur Hecker, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, and Yunpu Ma. Prism: Self-pruning intrinsic selection method for training-free multimodal data selection, 2025a. URL https://arxiv.org/ abs/2502.12119. 2 Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. Llava steering: Visual instruction tuning with 500x fewer parameters through modality linear representation-steering, 2025b. URL https://arxiv.org/abs/2412.12359. 2 Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, et al. Cot-kinetics: theoretical modeling assessing lrm reasoning process. arXiv preprint arXiv:2505.13408, 2025c. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. 2 Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610, 2024. 17 Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual learning in low-rank orthogonal subspaces. Advances in Neural Information Processing Systems, 33:99009911, 2020. 3 Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho-Shing Ip, and Sam Kwong. SEFE: Superficial and essential forgetting eliminator for multimodal continual instruction tuning. In arXiv Technical Report, 2025. 6,"
        },
        {
            "title": "Preprint",
            "content": "Xiaojun Chen, Shengbin Jia, and Yang Xiang. review: Knowledge reasoning over knowledge graph. Expert systems with applications, 141:112948, 2020. 9 Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. Proceedings of the 32nd ACM International Conference on Multimedia (MM 24), 2024. 16 Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuan-Jing Huang, Nan Duan, and Ruofei Zhang. An enhanced knowledge injection model for commonsense generation. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 20142025, 2020. 2 Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 37623773. PMLR, 2628 Aug 2020. 3 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. 5, 6, Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. 6, 17 Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 831839, 2019. 2, 3 Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, pp. 27902799. PMLR, 2019. 2 Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3, 17 Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip Yu. survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494514, 2021. 9 Kailin Jiang, Yuntao Du, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, and Qing Li. When large multimodal models confront evolving knowledge:challenges and pathways. In arXiv Technical Report, 2025. 3, 6, James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114 (13):35213526, 2017. 2, 3, 4, 17 Anne Lauscher, Olga Majewska, Leonardo FR Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glavaˇs. Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. arXiv preprint arXiv:2005.11787, 2020. 3 Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021."
        },
        {
            "title": "Preprint",
            "content": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. 2 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. Transactions on Machine Learning Research (TMLR), 2025. 6 Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 6, 16 Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, 2023. 6, 17 Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):29352947, 2017a. 2, Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):29352947, 2017b. 2, 3, 4, 6, 17 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. 2 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. 6 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pp. 216233. Springer, 2024c. 6, 16 Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. Advances in Neural Information Processing Systems, 37: 86988733, 2025. 6, Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 5, 6, 16 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 6, 17 Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2402.12851, 2024. 3, 6, 18 Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of Psychology of Learning and Motivation, pp. 109165. Academic Press, 1989. 2, 3 Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo O. Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, Tolga Aktas, and Todd Hendry. Injecting new knowledge into large language models via supervised fine-tuning. arXiv preprint arXiv:2404.00213, 2024. 2,"
        },
        {
            "title": "Preprint",
            "content": "Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In ICLR. OpenReview.net, 2023. 4, 5 Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: visual question answering by reading text in images. In Proceedings of the International Conference on Document Analysis and Recognition (ICDAR), pp. 947952, 2019. 6, 16 Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval? comparing knowledge injection in llms. Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024), 2024. 2, 3 Core Francisco Park, Zechen Zhang, and Hidenori Tanaka. New news: System-2 fine-tuning for robust integration of new knowledge. arXiv preprint arXiv:2505.01812, 2025. Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models as knowledge bases? In EMNLP/IJCNLP (1), pp. 24632473. Association for Computational Linguistics, 2019. 2 Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. 6, 16 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, 2021. 4, 28 Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97(2):285, 1990. 2, 3 Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 4 Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? In EMNLP (1), pp. 54185426. Association for Computational Linguistics, 2020. 2 Antonio Sabbatella, Andrea Ponti, Ilaria Giordani, Antonio Candelieri, and Francesco Archetti. Prompt optimization in large language models. Mathematics, 12(6):929, 2024. 2 Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. In International Conference on Learning Representations, 2021. Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. Continual learning of large language models: comprehensive survey. arXiv preprint arXiv:2404.16789, 2024. 4 Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. 2, 3 Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An ensemble of retrieval-and generation-based dialog systems. arXiv preprint arXiv:1610.07149, 2016. 2 Wei Tang, Yixin Cao, Yang Deng, Jiahao Ying, Bo Wang, Yizhe Yang, Yuyue Zhao, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, and Yong Liao. Evowiki: Evaluating llms on evolving knowledge. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025), 2025."
        },
        {
            "title": "Preprint",
            "content": "Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025a. 6, 17 Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training networks in null space of feature covariance for continual learning. In CVPR, pp. 184193. Computer Vision Foundation / IEEE, 2021. 5 Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. Orthogonal subspace learning for language model continual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1065810671, 2023. 3, 6, 17 Yujun Wang, Aniri, Jinhe Bi, Soeren Pirk, and Yunpu Ma. Ascd: Attention-steerable contrastive decoding for reducing hallucination in mllm, 2025b. URL https://arxiv.org/abs/2506.14766. 3 Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023. 17 Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 30143023, 2021. Yibo Yang, Xiaojie Li, Zhongzhu Zhou, Shuaiwen Leon Song, Jianlong Wu, Liqiang Nie, and Bernard Ghanem. CorDA: Context-oriented decomposition adaptation of large language models for task-aware parameter-efficient fine-tuning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 4, 5, 6 Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. ASVD: Activationaware singular value decomposition for compressing large language models. In arXiv Technical Report, 2023. 6, 19 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. 6, 16 Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. 5 Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Saez de Ocariz Borde, Rickard Bruel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, and Justin Solomon. Asymmetry in low-rank adapters of foundation models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024."
        },
        {
            "title": "B MORE DETAILS ABOUT SETUP AND EXPERIMENTAL OPERATION",
            "content": "B.1 KNOWLEDGE ADAPTATION EVALUATION . . . . . . . . . . . . . . . . . . . . . B.2 KNOWLEDGE RETENTION EVALUATION . . . . . . . . . . . . . . . . . . . . . . B.3 EVALUATION PROTOCOL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 BASELINE METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 TRAINING PARAMETERS ABOUT KORE . . . . . . . . . . . . . . . . . . . . . . . B.6 EXPERIMENT RESOURCES ABOUT KORE . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C PROOF OF KORE",
            "content": "16 16 16 16 17 18 18 18 MORE DETAILS ABOUT ANALYSIS OF ABILITY TO CAPTURE KNOWLEDGE 19 D.1 DETAILED EXPERIMENTAL RESULTS FOR CAPTURE KNOWLEDGE . . . . . . . . D.2 COVARIANCE VISUALIZATION RESULTS . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E MORE EXPERIMENTAL RESULTS ABOUT KORE",
            "content": "19 20 21 E.1 MORE MAIN RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 MORE RESULTS ON LMM SCALES AND ARCHITECTURES . . . . . . . . . . . . 21 E.3 MORE RESULTS ON SPECIFIC KNOWLEDGE-ORIENTED CONSTRAIN . . . . . . . E.4 MORE RESULTS ON ABLATION EXPERIMENTS . . . . . . . . . . . . . . . . . . E.5 MORE RESULTS ON COMPARISON WITH GENERAL AUGMENTATION METHODS 22 24 CONVERGENCE COMPARISON OF VARIOUS METHODS VIA LOSS CURVES."
        },
        {
            "title": "G CASE STUDY",
            "content": "H MORE DETAILS ABOUT KORE-AUGMENTATION H.1 MORE CONSTRUCTION PROCESS ABOUT KORE-AUGMENTATION . . . . . . . . H.2 MORE STATISTICAL ANALYSIS ABOUT KORE-AUGMENTATION . . . . . . . . . . H.3 PROMPT DETAILS REGARDING MULTI-ROUNDS OF DIALOGUE . . . . . . . . . H.4 PROMPT DETAILS REGARDING VISUAL RECOGNITION QA . . . . . . . . . . . . H.5 PROMPT DETAILS REGARDING IMAGE CAPTION QA . . . . . . . . . . . . . . . H.6 PROMPT DETAILS REGARDING VQA . . . . . . . . . . . . . . . . . . . . . . . . 26 28 28 30 32 33"
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS IN KORE",
            "content": "In this section, we elaborate on the precise role of large language models within KORE, as detailed below. Usage 1: KORE-74Ks construction. In 3.1 and H, we use GPT-4o to generate multi-rounds dialogue data, summary content of original knowledge, and quadruplets (Q, A, S, H) data, which is in line with current scientific research standards Usage 2:Knowledge Retention Evaluation. In 4, we employ MIA-Bench, MMDU, MathVista, and MathVision, whose evaluation requires large language models as judgesa practice consistent with current research standards. Usage 3: Paper grammar polishing. The initial draft of the paper was written by humans and later refined for grammar using large language models, common practice in contemporary research."
        },
        {
            "title": "B MORE DETAILS ABOUT SETUP AND EXPERIMENTAL OPERATION",
            "content": "B.1 KNOWLEDGE ADAPTATION EVALUATION Our knowledge adaptation evaluation completely follows the settings of EVOKE. Below, we will provide an introduction to EVOKE: EVOKE: This paper introduces EVOKE (Jiang et al., 2025), new benchmark to evaluate how well Large Multimodal Models (LMMs) can learn evolving knowledge without forgetting their original capabilities. It reveals the limitations of current methods in knowledge adaptation and the severity of catastrophic forgetting. The study further shows that knowledge augmentation and continual learning are promising solutions, providing framework for future research. B.2 KNOWLEDGE RETENTION EVALUATION We evaluate fine-tuned LMMs knowledge retention capabilities on 12 benchmarks across 7 capability dimensions. And we follow the settings of VLMEvalKit (Duan et al., 2024) to evaluate these benchmarks, and the following is an introduction 1. MME (Fu et al., 2023) provides holistic evaluation of LMMs perception and cognition across 14 tasks. Its key feature is the use of carefully crafted instruction-answer pairs, which facilitates straightforward assessment without the need for specialized prompt engineering. 2. MMBench (Liu et al., 2024c) is cross-lingual benchmark for comprehensively evaluating LMMs. It features over 3,000 bilingual multiple-choice questions spanning 20 skill dimensions, from visual recognition to abstract reasoning. 3. SEEDBench2 Plus (Li et al., 2024) benchmarks LMMs on interpreting text-rich visuals (e.g., charts, web layouts). It uses 2,300 multiple-choice questions to test reasoning capabilities where integrating textual and visual information is essential. 4. OCRVQA (Mishra et al., 2019) is benchmark for evaluating models ability to answer questions by reading text within images. It focuses on tasks where textual information is essential, requiring tight integration of visual perception and OCR. 5. ScienceQA (Lu et al., 2022) evaluates scientific reasoning through large-scale multimodal benchmark; it features curriculum-based questions with diagrams and provides lectures and explanations for each question to encourage complex reasoning. 6. MMMU (Yue et al., 2024) evaluates LMMs on college-level, multimodal questions requiring expert knowledge. The benchmark includes 11,500 questions from six disciplines, utilizing 30 image formats to test complex, subject-specific reasoning. 7. MIA-Bench (Qian et al., 2024) is targeted benchmark that measures how precisely LMMs can follow complex and multi-layered instructions. It consists of 400 distinct image-prompt combinations engineered to test models ability to comply with detailed and nuanced directives. 8. MMDU (Liu et al., 2025) evaluates LMMs in multi-image, multi-turn conversational scenarios. It specifically assesses models capacity for contextual understanding, temporal reasoning, and maintaining coherence throughout extended interactions."
        },
        {
            "title": "Preprint",
            "content": "9. MathVista (Lu et al., 2024) benchmarks the mathematical reasoning of foundation models in visual contexts. It aggregates 6,141 problems from 31 datasets, requiring detailed visual analysis and compositional logic for solution. 10. MathVision (Wang et al., 2025a) provides challenging dataset of 3,040 visually-presented problems from math competitions. Categorized into 16 mathematical areas and five difficulty tiers, it offers structured evaluation of advanced reasoning in LMMs. 11. HallusionBench (Guan et al., 2024) diagnoses hallucination and illusion in LMMs visual interpretations. It employs 346 images and 1,129 structured questions to quantitatively analyze the causes of inaccurate or inconsistent model responses. 12. POPE (Li et al., 2023) evaluates object hallucination in LMMsthe tendency to describe nonexistent objects. It uses polling-based questioning strategy to reliably measure this tendency. B.3 EVALUATION PROTOCOL To evaluate performance on open-domain question answering tasks, two key metrics are employed: Cover Exact Match (CEM) and F1-Score (F1). The CEM metric determines whether the ground truth answer is fully contained within the models prediction (Xu et al., 2023). It is defined by the equation: CEM = (cid:26)1, 0, yq ˆY otherwise where yq represents the ground truth answer and ˆY is the text generated by the model. The F1-Score, on the other hand, assesses the word-level overlap between the predicted and ground truth answers, providing harmonic mean of Precision and Recall (Chan et al., 2024). Given the ground truth as set of words W(yq) = {y1, . . . , ym} and the models prediction as W( ˆY ) = {ˆy1, . . . , ˆyn}, the number of common words is calculated as U( ˆY , yq) = (cid:80) tW(yq) 1[t W( ˆY )], where 1[] is the indicator function. Based on this, Precision is the fraction of predicted words that are correct, P( ˆY , ) = U( ˆY , yq) W( ˆY ) ; while Recall is the fraction of ground truth words that were successfully predicted, R( ˆY , ) = U( ˆY , yq) W(Y ) . B.4 BASELINE METHODS In this section, we provide brief introduction to the baseline method, as follows: EWC: This seminal continual learning work (Kirkpatrick et al., 2017) introduces Elastic Weight Consolidation (EWC) to mitigate catastrophic forgetting. EWC slows updates to parameters important for prior tasks by imposing quadratic constraint based on the Fisher Information Matrix, elastically preserving old knowledge during new learning. LwF: This work proposes Knowledge Distillation to mitigate catastrophic forgetting (Li & Hoiem, 2017b). The method preserves knowledge by ensuring the new models predictions on new data align with the old models outputs, achieving data-free continual learning through output consistency. LoRA: This highly efficient method, LoRA (Hu et al., 2022), fine-tunes models by training only small, injected low-rank matrices while keeping the original weights frozen. This approach reduces computational costs significantly and helps mitigate catastrophic forgetting. OLoRA: This work proposes an orthogonal subspace-based method for continual learning (Wang et al., 2023). It allocates independent, orthogonal parameter subspaces for each task, constraining updates to prevent interference and mitigate catastrophic forgetting via an elegant geometric solution."
        },
        {
            "title": "Preprint",
            "content": "MoELoRA: The method in (Luo et al., 2024) combines MoE with contrastive learning for PEFT. It specializes experts for different data types and uses contrastive objectives to guide expert collaboration, achieving parameter-efficient fine-tuning that reduces catastrophic forgetting. SEFE: The method in (Chen et al., 2025) tackles multimodal catastrophic forgetting by separately addressing two types: superficial forgetting of style and essential forgetting of knowledge. tailored training strategy preserves essential knowledge during continual instruction learning. B.5 TRAINING PARAMETERS ABOUT KORE We have displayed some training parameter settings, as shown in Table 7. Table 7: Hyperparameter settings for the model training on LLaVA-v1.5 (7B), LLaVA-v1.5 (13B) and Qwen2.5-VL (7B). Rank LLaVA-v1.5 (7B) Deepspeed Zero3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Warmup Ratio LR Schedule Learning Rate cosine decay Vision Select Layer -2 Weight Decay 0 Batch Size 54 Optimizer AdamW Epochs 2 104 0.03 Rank 235 LLaVA-v1.5 (13B) Deepspeed Zero3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Warmup Ratio LR Schedule Learning Rate cosine decay Vision Select Layer -2 Weight Decay Batch Size 32 Optimizer AdamW Epochs 6 2 104 0.03 Qwen2.5-VL (7B) Deepspeed Zero3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Grad Accum Steps Warmup Ratio LR Schedule Learning Rate Image Max Pixels 262144 Optimizer AdamW Epochs 6 Rank 274 8 0. cosine decay 2 104 Batch Size 24 B.6 EXPERIMENT RESOURCES ABOUT KORE All training experiments were conducted using 4 NVIDIA H100 GPUs (each with 96 GiB memory). All evaluation experiments were performed on systems equipped with 4 NVIDIA A100 PCIe GPUs (each with 40 GiB memory)."
        },
        {
            "title": "C PROOF OF KORE",
            "content": "Theorem 1. Under the assumption that W0 is full-rank, the column space of forms subset of the column space of null. Step 1: Let us define the matrix as: Σ(V )T (4) where Σ is the diagonal matrix of singular values and is the right singular vector matrix of Σ is diagonal matrix, it only scales the columns of (V )T without changing W0UnullU their span. Hence, the column space of must be identical to that of (V )T : null. Since = Col(A) = Col((V )T ) Step 2: We use the SVD of W0UnullU null, which gives us: W0UnullU null = Σ(V )T 18 (5) (6)"
        },
        {
            "title": "Preprint",
            "content": "where and are orthogonal matrices, with the columns of representing the right singular vectors. The columns of span the row space of W0UnullU null is projection matrix that projects any matrix onto the subspace spanned by Unull. Therefore, the column space of W0UnullU null. Since Unull is orthogonal, UnullU null is identical to the column space of Col(V ) = Col(W0UnullU null: null) = Col(U null) (7) Step 3: From Step 1, we know that Col(A) = Col((V )T ), and from Step 2, we concluded that Col(V ) = Col(U null). Therefore, combining these two results, we obtain: Col(A) = Col(U null) (8) Thus, the column space of is identical to the column space of Theorem 2. For given layer in large language model, suppose the input activation (l) is derived from pre-trained world knowledge and remains unchanged. Then, under fine-tuning with KORE, the output of the layer is approximately preserved: null, completing the proof. (l) (l) (l) 0 (l), where (l) weight matrix of the same layer after fine-tuning. is the initial weight matrix of the l-th layer before fine-tuning, and (l) 0 denotes the In KORE, we start with the pre-trained weight matrix (l) 0 (l) at layer as: and define the fine-tuned weight matrix (l) = (l) 0 B(l)A(l) + B(l)A(l). The output of the layer is given by: (l)X (l) = (W (l) 0 B(l)A(l) + B(l)A(l))X (l). Using the assumption that A(l)X (l) 0, we simplify the output: (l)X (l) (l) 0 (l). (9) (10) (11) Thus, the output remains approximately unchanged, meaning that the fine-tuning process does not significantly alter the pre-trained knowledge, ensuring that the knowledge preservation property holds. This concludes the proof."
        },
        {
            "title": "D MORE DETAILS ABOUT ANALYSIS OF ABILITY TO CAPTURE KNOWLEDGE",
            "content": "D.1 DETAILED EXPERIMENTAL RESULTS FOR CAPTURE KNOWLEDGE Table 8 presents detailed data and additional results from the experiment illustrated in Figure 4. The results indicate that the number of sampled data points has only limited influence. When the smallest 1536 ranks are discarded, performance with 512 samples is slightly lower than with 256 samples; using 32 samples leads to more noticeable decline compared to 256 samples, yet still significantly outperforms both Plain SVD and ASVD (Yuan et al., 2023). This suggests that even small number of samples is sufficient to capture essential knowledge into the covariance matrix. Furthermore, using test-specific samples allows for better performance after discarding large number of ranks. For instance, when discarding 1536 ranks, CO-SVD (with 256 MME samples) outperforms CO-SVD (with 256 ScienceQA samples) on the MME, while CO-SVD (with 256 ScienceQA samples) surpasses CO-SVD (with 256 MME samples) on ScienceQA. This demonstrates that CO-SVD effectively captures dataset-specific knowledge and preserves structural features in the covariance matrix, enabling knowledge-oriented constraints and resulting in powerful retention."
        },
        {
            "title": "Preprint",
            "content": "Table 8: The detailed numbers and more results of the experiment in Figure 4 Test Data Method Discarded Ranks 128 256 1024 1536 MME ScienceQA Plain SVD ASVD (with 256 MME samples) CO-SVD (with 256 MME samples) CO-SVD (with 32 MME samples) CO-SVD (with 512 MME samples) CO-SVD (with 256 ScienceQA samples) 1492.95 1490.14 1498.17 1508.90 1507.42 1486. 1487.28 1476.02 1511.25 1512.90 1516.68 1492.65 1318.18 1488.48 1514.43 1507.78 1505.33 1478.73 1169.87 1425.41 1486.81 1498.81 1460.32 1419.61 744.03 1239.74 1458.36 1341.82 1449.82 1300.89 Plain SVD ASVD (with 256 ScienceQA samples) CO-SVD (with 256 ScienceQA samples) CO-SVD (with 32 ScienceQA samples) CO-SVD (with 512 ScienceQA samples) CO-SVD (with 256 MME samples) 67.13 67.63 67.19 67.48 67.08 67. 66.85 66.95 67.16 66.77 67.00 67.49 65.59 66.75 67.62 66.97 67.40 67.53 50.41 62.38 67.61 66.61 66.91 65.69 0.73 49.14 66.76 64.58 66.27 62.43 D.2 COVARIANCE VISUALIZATION RESULTS In Figures 8 and 9, we further provide visualizations of the covariance matrices collected from the POPE, HallusionBench, and MMBench tasks. Due to the high and uninformative original dimensionality of 4096 or 11088, we downsampled the covariance matrices to 3232 and visualized their heatmaps. We present activations prior to various linear weights, including mlp.down proj, mlp.gate proj,self attn.v projand self attn.o proj from both layer 2 and layer 30. The results show that heatmaps from POPE and HallusionBenchboth hallucination evaluation tasksshare certain similar patterns (highlighted with red circles) not observed in heatmaps from MMBench. This indicates that the activated covariance matrices exhibit distinct patterns when inputs from different tasks are processed by the LMMs. These visualizations empirically support that covariance matrix patterns can characterize the triggered task. We leverage such patterns to guide the decomposition of pre-trained weights in LMMs, obtaining initialized adapters enriched with more informative knowledge. Figure 8: Covariance matrix visualization for mlp.down proj, mlp.gate proj,self attn.v projand self attn.o proj weights in the 2-th layer on POPE, HallusionBench and MMBench."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Covariance matrix visualization for mlp.down proj, mlp.gate proj,self attn.v projand self attn.o proj weights in the 30-th layer on POPE, HallusionBench and MMBench."
        },
        {
            "title": "E MORE EXPERIMENTAL RESULTS ABOUT KORE",
            "content": "E.1 MORE MAIN RESULTS Regarding the experiment in Figure 5 in 4.2, we have supplemented Table 9 with detailed numerical performance of all methods on fine-grained knowledge types for readers reference. Table 9: Performance comparison between KORE and baseline methods on fine-grained knowledge types with LLaVA-v1.5 (7B). PO: Politics; SP: Sports; BU: Business; HE: Health; CE: Celebrity; FI: Film; AL: Album; WR: Written Work. Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 Full-FT LoRA Replay EWC LwF MoELoRA O-LoRA SEFE 21.35 17. 13.98 17.86 17.05 9.23 9.21 16.66 16.34 19.42 19.43 21.10 21.43 14.86 14.68 18.44 12.92 10.54 7.61 10.45 9.62 3.39 3.67 10.82 10.99 12. 13.16 14.81 13.99 8.72 8.52 12.64 22.49 19.11 15.96 19.83 19.83 6.77 7.01 17.78 20.88 21.50 20.69 23.02 23.66 11.77 12.23 20.92 27.31 20. 16.05 19.00 18.63 12.36 12.55 20.30 20.95 24.03 22.40 24.57 25.82 18.92 18.98 23.23 19.84 17.81 15.38 17.41 19.03 10.53 11.74 17.00 16.47 23. 24.21 23.88 26.20 20.60 20.68 21.55 14.37 12.51 8.48 12.88 11.88 3.40 3.40 9.79 13.88 17.09 16.39 17.58 18.40 9.28 9.22 15.18 13.11 12. 9.40 14.53 12.45 2.95 3.10 10.77 16.93 21.19 18.78 22.07 21.64 10.32 10.51 20.13 12.39 10.57 10.34 12.16 12.39 4.43 4.20 9.09 13.16 15. 15.60 16.91 17.01 8.96 8.28 12.01 12.17 10.72 3.77 10.72 9.28 3.19 3.19 5.51 7.66 8.72 10.79 8.13 11.11 5.22 5.35 7.47 20.34 18. 4.55 15.25 10.17 10.17 8.47 13.56 8.43 12.94 8.23 17.69 17.10 14.07 12.37 13.87 KORE 34.74 42. 23.83 32.31 46.19 50.38 34.69 45. 33.20 45.23 26.17 39.39 27.79 42. 26.93 34.05 16.52 29.54 28.81 43. E.2 MORE RESULTS ON LMM SCALES AND ARCHITECTURES Regarding the experiment in 4.3, we have supplemented the detailed results of knowledge adaptation and retention in Tables 10 and 11, respectively. Obs 1 in E.2: KORE still achieves superior knowledge retention performance on larger-scale LMM and different model architectures. As shown in Table 10, on LLaVA-v1.5 (13B), KORE outperforms Replay on seven benchmarks and achieves comparable overall performance. This result demonstrates KOREs potential for superior performance on larger-scale LMM. On Qwen2.5VL (7B), KORE surpasses LoRA by 20.15 in overall performance, demonstrating its ability to"
        },
        {
            "title": "Preprint",
            "content": "Table 10: Performance comparison between KORE and baseline methods on fine-grained knowledge retention evaluations with LLaVA-v1.5 (13B) and Qwen2.5-VL (7B). Method COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMU MIAB MMDU MathT MathI HAL POPE HallB Vanilla LoRA Replay KORE Vanilla LoRA Replay KORE 65.33 30.00 57. 55.99 82.54 67.88 75.38 36.23 68.38 60.57 65. 62.71 79.81 37.20 81.70 76.98 42.25 36.93 40. 40.32 69.61 59.29 69.16 66.80 59.99 28.22 54. 51.60 71.03 69.79 69.17 68.69 73.90 69.13 70. 71.97 72.10 42.30 85.12 85.55 LLaVA-v1.5 (13B) 31. 18.30 25.90 30.80 66.04 23.26 61.04 65.10 Qwen2.5-VL (7B) 58.60 2.40 45.40 45.40 78.46 21.39 70.20 70. 33.93 17.43 24.62 26.84 61.25 23.25 50.72 45. 27.40 23.90 27.00 27.30 69.70 39.40 63.90 63. 11.88 7.73 12.11 13.32 25.69 13.52 21.58 24. 87.07 71.64 87.09 79.29 86.51 73.73 87.49 75. 26.46 4.52 21.23 18.91 47.42 9.02 47.48 41. Avg 49.51 32.64 45.69 45.35 66.89 38.16 63. 58.31 maintain superior knowledge retention across different model architectures and confirming its universality and robustness. Table 11: Performance comparison between KORE and baseline methods on fine-grained knowledge types with LLaVA-v1.5 (13B) and Qwen2.5-VL (7B). Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 LLaVA-v1.5 (13B) LoRA Replay 20.15 15. 25.10 21.83 12.65 8.16 16.17 14.41 24.79 15.60 28.69 21.76 21.77 15. 29.51 24.74 22.27 18.62 29.09 28.74 11.99 8.77 20.34 18.42 13.72 9. 25.26 21.50 13.18 10.91 18.04 17.16 6.67 5.51 12.18 13.38 10.17 10. 15.87 20.97 KORE 36.77 46.11 25.39 34. 47.16 53.39 37.45 50.95 35.22 48. 28.64 42.67 28.66 44.95 31.02 38. 22.61 35.43 20.34 33.06 Qwen2.5-VL (7B) LoRA Replay 17.76 13.45 14.09 18.40 12.01 7.33 7.18 11.09 17.41 14.03 17.65 17. 22.32 14.58 17.90 22.72 19.03 15.38 17.21 23.72 11.06 9.84 13.93 18. 8.03 7.16 15.91 17.69 21.48 20.45 14.91 28.00 8.70 9.28 10.87 12. 16.95 16.95 11.32 24.89 KORE 26.93 32.51 17. 22.75 31.20 35.11 31.00 39.43 33. 40.49 18.51 30.11 16.11 28.63 26. 33.20 13.33 25.91 25.42 41.24 Obs 2 in E.2: KORE achieves comprehensive performance advantages across diverse knowledge types on both larger-scale LMM and different model architectures. In Table 11, KORE achieves the best knowledge adaptation performance across all news and entity types on both LLaVA-v1.5 (13B) and Qwen2.5-VL (7B), significantly outperforming LoRA and Replay. This demonstrates that KOREs effectiveness in new knowledge injection is not constrained by model scale or architecture, highlighting its powerful universality. E.3 MORE RESULTS ON SPECIFIC KNOWLEDGE-ORIENTED CONSTRAIN For the experiment on specific knowledge-oriented constraints in 4.2, we have provided detailed results and presented them below. Table 12: Performance of specific knowledge-oriented constrains in knowledge adaptation and retention with LLaVA-v1.5 (7B). Methods EVOKE CEM F1 COM OCR M-DIS INS M-IDU MAT HAL Avg KORE 30.65 41. 52.41 KOREMME KORE KORE KORE OCRVQA MathT HallB 29.48 29.95 30.06 29.93 39.44 39.75 40.33 39.98 56.90 52.60 52.40 54. 40.98 39.86 41.47 40.32 36.68 48.68 47.41 48.86 48.57 46.50 38.54 60.10 57.06 60.30 60. 16.58 27.70 27.09 27.69 26.30 18.59 17.92 18.28 19.24 17.42 51.75 37. 52.20 50.15 51.57 52.67 38.81 38.53 39.03 38.52 Obs 1 in E.3: KORE with specific knowledge-oriented constraints achieves superior comprehensive performance. In Table 12, KORE with specific knowledge-oriented constraints (e.g., MME, OCRVQA, MathT, HallB) causes slight decrease in knowledge adaptation efficacy, it yields significant increase in knowledge retention performance on INS and M-IDU, resulting in superior overall performance. Obs 2 in E.3: Specific knowledge-oriented constraints enhance the retention of corresponding knowledge. In Table 13, specific knowledge-oriented constraints enhance the retention of"
        },
        {
            "title": "Preprint",
            "content": "corresponding knowledge without compromising the retention of other knowledge types. This capability underscores KOREs potential for applications requiring customized knowledge preservation. Table 13: Performance of specific knowledge-oriented constrains on fine-grained knowledge retention evaluations with LLaVA-v1.5 (7B). Method KORE KOREMME KORE KORE KORE OCRVQA MathT HallB COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMUT MIAB MMDU MathT MathI 49.84 57.01 50.81 48.87 55.31 54.98 56.79 54.38 55.93 53.44 37. 37.51 36.06 36.41 35.18 44.24 42.22 46.88 44.24 38.18 68.06 66.83 68.22 67.23 67.30 29. 28.00 29.50 29.90 25.70 38.54 60.10 57.06 60.30 60.71 16.58 27.70 27.09 27.69 26.30 25. 24.00 24.30 26.50 23.10 12.09 11.84 12.27 11.97 11.74 HAL POPE HallB 80.99 81.62 80.82 81.04 80. 22.51 22.79 19.47 22.09 24.87 Avg 40.00 43.03 42.24 42.68 41.86 Obs 3 in E.3: Specific knowledge-oriented constraints also achieve excellent adaptation performance across wide spectrum of fine-grained knowledge. In Table 14, KORE with specific knowledge-oriented constraints maintains strong adaptation performance across various News and Entity knowledge types, with negligible performance degradation. Table 14: Performance of specific knowledge-oriented constrains on fine-grained knowledge types with LLaVA-v1.5 (7B). Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 KORE 34.74 42.96 23.83 32. 46.19 50.38 34.69 45.74 33.20 45. 26.17 39.39 27.79 42.61 26.93 34. 16.52 29.54 28.81 43.05 KOREMME KORE KORE KORE OCRVQA MathT HallB 34.05 34.46 33.71 34.23 41.53 41.66 41.72 41.74 23.92 24.29 22.27 24.11 31.46 31.69 30.39 32.09 43.17 43.53 45.95 43.05 47.28 48.34 50.88 46. 34.32 36.35 33.03 35.06 46.12 46.09 43.38 44.92 35.63 33.20 30.77 32.39 45.38 44.35 43.55 43.53 24.48 25.01 26.06 25.21 37.15 37.65 38.82 38. 27.24 27.24 28.15 27.54 40.96 41.17 42.46 41.68 22.61 24.09 25.80 24.66 30.43 31.60 32.97 32.34 15.07 14.78 15.07 14.78 27.72 27.16 27.37 26. 30.51 30.51 30.51 28.81 42.16 42.17 42.11 40.13 E.4 MORE RESULTS ON ABLATION EXPERIMENTS Regarding the experiment in 4.4, we have supplemented the experiments in E.4.1 and E.4.2. E.4.1 RANK ABLATION EXPERIMENTS Table 15: Performance comparison across different ranks in knowledge adaptation and retention with LLaVA-v1.5 (7B). Methods KORE (rank=64) KORE (rank=128) KORE (rank=235) KORE (rank=256) EVOKE CEM F1 24.00 30.72 30.65 31. 33.07 40.55 41.26 41.32 COM OCR M-DIS INS M-IDU MAT HAL Avg 45.35 49.97 52.41 52.48 29.46 36.05 40.98 39.96 45.02 47.07 48.68 48. 44.07 34.87 38.54 60.02 19.62 10.00 16.58 23.18 18.08 17.46 18.59 18.09 44.48 50.30 51.75 51.50 31.81 35.37 37.09 39.11 Obs 1 in E.4.1: Increasing the number of trainable parameters enables KORE to achieve stronger performance. In Table 15, KOREs performance in both knowledge adaptation and knowledge retention exhibits consistent upward trend as the rank and number of trainable parameters increase. This trend is particularly significant on the INS and M-IDU dimensions, which indicates KOREs potential to achieve even stronger performance with larger parameter. Table 16: Performance of comparison across different ranks on fine-grained knowledge retention evaluations with LLaVA-v1.5 (7B). Method COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMUT MIAB MMDU MathT MathI KORE (rank=64) KORE (rank=128) KORE (rank=235) KORE (rank=256) 43.63 47.96 49.84 50.06 47.08 51.98 54.98 54. 33.55 36.32 37.73 36.89 25.36 35.77 44.24 43.03 66.34 67.44 68.06 68.51 23.70 26.70 29.30 29.40 44.07 34.87 38.54 60.02 19.62 10.00 16.58 23. 25.20 23.90 25.10 24.70 10.95 11.02 12.09 11.48 HAL POPE HallB 74.22 79.63 80.99 80.77 14.73 20.97 22.51 22.23 Avg 35.70 37.21 40.00 42.10 Obs 2 in E.4.1: Larger trainable parameter scales enhance KOREs knowledge retention performance. In Table 16, KORE (rank=256) achieves near-comprehensive superiority across 12 benchmarks and surpasses KORE (rank=235) by 2.10 in overall performance. This underscores that larger trainable parameter scale activates stronger knowledge retention in KORE."
        },
        {
            "title": "Preprint",
            "content": "Table 17: Performance comparison across different ranks on fine-grained knowledge types with LLaVA-v1.5 (7B). Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 KORE (rank=64) KORE (rank=128) KORE (rank=235) KORE (rank=256) 28.31 34.70 34.74 35.17 34.84 42.07 42.96 42.98 20.44 24.20 23.83 23.92 27.66 31.56 32.31 31. 36.64 44.50 46.19 45.83 41.11 49.17 50.38 50.35 28.60 36.72 34.69 35.98 38.13 47.68 45.74 47.11 26.72 34.82 33.20 32.79 35.77 44.39 45.23 43. 19.27 26.35 26.17 26.55 31.11 38.89 39.39 39.49 21.24 28.81 27.79 28.46 35.25 43.19 42.61 42.74 18.98 23.86 26.93 27.16 25.33 30.22 34.05 34. 11.01 17.97 16.52 15.65 23.14 28.77 29.54 26.81 22.03 35.59 28.81 27.12 33.44 44.86 43.05 39.92 Obs 3 in E.4.1: Larger trainable parameters improve KOREs knowledge adaptation performance on News and Entity types. In Table 17, KORE (rank=256) achieves robust and consistent performance across broader range of fine-grained knowledge types, demonstrating KOREs potential for superior performance with an increased number of trainable parameters. E.4.2 SETTING ABLATION EXPERIMENTS Table 18: Performance comparison of setting ablation in knowledge retention with LLaVA-v1.5 (7B). Method KORE W/o Augmentation W/o Constraint W/o Frozen Matrix COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMUT MIAB MMDU MathT MathI 49. 58.75 40.55 47.24 54.98 61.17 52.23 54.21 37.73 36.80 31.75 36.01 44. 44.04 33.01 43.10 68.06 68.15 65.81 67.63 29.30 26.10 26.80 29.10 38. 32.53 32.70 35.30 16.58 16.00 15.38 16.44 25.10 28.00 26.50 26.70 12. 11.41 11.74 11.45 HAL POPE HallB 80.99 81.29 79.16 80.84 22.51 17.71 13.77 18. Avg 51.75 40.16 35.78 38.92 Obs 1 in E.4.2: Modifying KOREs design leads to degradation in overall knowledge retention performance. In Table 18, the ablated versions W/o Augmentation, W/o Constraint, and W/o Frozen Matrix exhibit overall performance degradations of 11.59, 15.97, and 12.83 respectively compared to KORE. This significant degradation underscores the high efficacy of KOREs design. Table 19: Performance comparison of setting ablation on fine-grained knowledge types with LLaVAv1.5 (7B). Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 KORE 34.74 14.04 W/o Augmentation 38.45 W/o Constraint W/o Frozen Matrix 36. 42.96 20.22 45.16 43.42 23.83 8.25 25.57 25.11 32.31 14.06 32.56 31.70 46.19 15.96 46.43 46.43 50.38 20.08 50.66 50.44 34.69 14.39 41.33 37. 45.74 23.13 51.22 48.20 33.20 14.57 36.84 36.44 45.23 25.69 45.78 46.44 26.17 7.30 28.97 27.01 39.39 16.21 42.12 39.85 27.79 8.08 29.67 28. 42.61 20.13 44.18 42.88 26.93 8.41 30.45 27.95 34.05 14.15 37.75 34.57 16.52 3.77 20.58 19.13 29.54 6.56 33.59 30.95 28.81 13.56 28.81 28. 43.05 22.27 40.02 39.86 Obs 2 in E.4.2: W/o Constraint yields superior knowledge adaptation performance across wide spectrum of fine-grained knowledge. In Table 19, W/o Constraint achieves superior knowledge adaptation performance on fine-grained News and Entity types. These gains stem from KORE-AUGMENTATIONs ability to perform profound and structured augmentation. E.5 MORE RESULTS ON COMPARISON WITH GENERAL AUGMENTATION METHODS Table 20: Performance comparison of different augmentation methods in knowledge retention with LLaVA-v1.5 (7B). Method COM MME MMB OCR SEEDB2P OCRVQA M-DIS INS M-IDU MAT SQA MMMUT MIAB MMDU MathT MathI HAL POPE HallB Avg KORE-AUGMENTATION 40.55 52.23 31. 33.01 65.81 26.80 32.70 15.38 26. 11.74 79.16 13.77 46.47 Knowledge-Agnostic Knowledge-Aware Knowledge-Agnostic Knowledge-Aware 51.67 50.02 50.43 51.35 55.33 47.68 52.41 51.46 25.99 24.95 11.86 27. 24.77 31.25 14.58 21.91 Augmentation for Text 64.38 65.75 15.20 14.80 Augmentation for Images 64.18 66.29 9.70 14.80 44.37 43.59 43.65 40.84 22.41 20.72 21.60 18. 25.20 24.20 22.60 21.20 11.74 12.07 11.58 17.26 79.04 74.05 73.95 69. 8.40 9.24 8.58 7.68 35.71 34.86 32.09 34.02 Obs 1 in E.5: KORE-AUGMENTATION demonstrates absolute comprehensive performance superiority in knowledge retention evaluations. In Table 20, KORE-AUGMENTATION surpasses the best general augmentation method by margin of 10.76 in overall performance, demonstrating its substantially superior capability for knowledge retention."
        },
        {
            "title": "Preprint",
            "content": "Obs 2 in E.5: KORE-AUGMENTATION demonstrates superior knowledge adaptation performance across wide spectrum of fine-grained knowledge types. In Table 21, KOREAUGMENTATION achieves the best performance on all News and Entity knowledge types, demonstrating its superiority over general augmentation methods for new knowledge injection. Table 21: Performance comparison of different augmentation methods on fine-grained knowledge types with LLaVA-v1.5 (7B). Method Avg PO News SP BU HE Avg CE Entity FI AL WR CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 CEM F1 KORE-AUGMENTATION 38.45 45.16 25. 32.56 46.43 50.66 41.33 Knowledge-Agnostic Knowledge-Aware Knowledge-Agnostic Knowledge-Aware 14.59 20.19 18.38 17.15 20.11 24.99 22.42 23.01 8.52 11.37 10.72 9. 14.84 16.28 14.88 14.93 17.05 24.55 22.97 19.35 21.34 28.48 26.92 24. 17.16 21.96 20.11 18.08 51.22 45.78 36.84 Augmentation for Text 23.34 14.57 24.56 29.00 28.94 19.03 Augmentation for Images 27.28 19.84 26.60 27.05 16.19 25.79 28.97 42.12 29. 44.18 30.45 37.75 20.58 33.59 28. 40.02 9.37 13.37 12.26 11.97 17.99 22.17 19.87 20.84 10.52 13. 12.35 12.86 22.06 26.33 23.27 24.29 6.59 12.95 13.07 13.86 10.74 18. 16.59 19.59 8.12 9.57 10.43 7.25 15.00 12.99 16.13 11.47 13.56 13. 15.25 15.25 21.25 18.90 14.83 21.78 CONVERGENCE COMPARISON OF VARIOUS METHODS VIA LOSS CURVES. Figure 10: The training loss curves on EVOKE of Full-FT, LoRA, EWC, O-LoRA, SEFE and KORE. It should be clarified that Full-FT, LoRA, EWC, O-LoRA, and SEFE are trained using the knowledge injection dataset from EVOKE, whereas KORE is trained using the KORE-74K dataset. The scale of the training data differs between these setups, resulting in varying numbers of iteration steps per epoch. Consequently, KORE exhibits rapid decrease in loss during the first epoch. The purpose of reporting this loss graph is to provide readers with an intuitive understanding of the convergence of various methods. Figure 10 presents the training loss curves of the six methods, providing an intuitive comparison of their convergence behaviors. Although KORE and the baseline methods use different training datasets, the loss curves reveal that O-LoRA and SEFE fail to fit the EVOKEs knowledge injection dataset. While LoRA, EWC, and Full-FT converge to very low loss values and successfully fit the evoke dataset, their performance in Table 1 indicates poor generalization to new knowledge, suggesting overfitting. In contrast, KORE not only converges effectively on the KORE-74K dataset but also demonstrates strong generalization capabilities for novel knowledge."
        },
        {
            "title": "G CASE STUDY",
            "content": "Figure 11: Case Study of News."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Case Study of Entity."
        },
        {
            "title": "Preprint",
            "content": "H MORE DETAILS ABOUT KORE-AUGMENTATION H.1 MORE CONSTRUCTION PROCESS ABOUT KORE-AUGMENTATION Figure 13: Overview of construction pipeline for KORE-74K. The entire data construction process is automated, with only the question templates being manually crafted. In this section, we elaborate on the implementation of KORE-AUGMENTATION. The fully automated construction pipeline and data example are illustrated in Figure 13. The following details each step of the pipeline: Step 1: Constructing Multi-rounds of Dialogue. We design strict rules and diverse task examples, employing GPT-4o to generate multi-turn dialogue data based on the original knowledge. The first turn is heuristic QA pair randomly selected from templates, such as: <Please explain the {type} news that is shown in the image., The image provides the following {type} news summary: {title}.> <Please tell me what the {type} entity in this image is., The {type} entity shown in the picture is {entity name}.> The remaining dialogue data are generated automatically by GPT-4o. For each instance, we first generate up to 10 dialogue questions based on the original knowledge and predefined rules. Then, the corresponding answers are produced using the original knowledge, the generated questions, and the rules as input. The query images are taken directly from the original image set. This process results in complete multi-rounds dialogue dataset, obtaining 9,422 rounds of multi-rounds dialogue data and 75,710 rounds of dialogue. Further templates and prompt designs are provided in H.3. Step 2: Collecting Recognition and Caption Images. We use news titles or entity names as search keywords to retrieve and download the top five images via the Google search engine. CLIP (Radford et al., 2021) is then employed to extract visual features from both the downloaded and original images. We compute cosine similarity between them and retain the two images with the highest similarity scores, excluding any identical matches (similarity = 1). These selected images serve as query images for visual recognition and image captioning tasks. Step 3: Constructing Visual Recognition QA. For this task, templates are first manually created. Questions are randomly selected from these templates, and the answer is defined as Yes. The instruction content is Answer this question with Yes or No., and the query image is randomly chosen from the images obtained in Step 2. template example is provided below: <Is the image depicting news {title}?> <Can you see {entity name} in this picture?>"
        },
        {
            "title": "Preprint",
            "content": "Further templates and prompt designs are provided in H.4. Step 4: Constructing Image Caption QA. We first establish rigorous rules and diverse task examples. Using GPT-4o, we generate summary data based on original knowledge to serve as answers for the image caption task. The instruction content is Answer this question in one paragraph., and the query image corresponds to the remaining images from Step 2. Questions are randomly selected from template, such as: <Could you please describe the {type} news shown in the picture?> <Please provide description for the {type} entity in the image.> Further templates and prompt designs are provided in H.5. Step 5: Constructing VQA. First, strict rules and diverse task examples are established. Using GPT-4o, quadruplets Question, Answer, Subject, Hypernym are generated based on original knowledge, for instance, <Who attempted to assassinate the person in the image during campaign rally in July 2024?, Thomas Matthew Crooks, Donald John Trump, Person>. Subsequently, the subject and hypernym are combined as search keywords to retrieve and download the top 1-ranked image from Google, thereby constructing VQA data. Further prompt designs are provided in H.6. Through the above automated pipeline, we have augmented the EVOKEs knowledge injection dataset to KORE-74K, which can better achieve knowledge adaptation. H.2 MORE STATISTICAL ANALYSIS ABOUT KORE-AUGMENTATION In Table 22, we provide detailed statistical data analysis of KORE-74K. Table 22: Key Statistics of KORE-74K."
        },
        {
            "title": "Total data",
            "content": "- Multi-rounds of dialogue data - Visual recognition data - Image caption data - VQA data"
        },
        {
            "title": "Number",
            "content": "74,734 9,422 (12.6%) 9,422 (12.6%) 9,422 (12.6%) 46,468 (62.2%) 75,710 65,312 44 143 15.5 11."
        },
        {
            "title": "Preprint",
            "content": "H.3 PROMPT DETAILS REGARDING MULTI-ROUNDS OF DIALOGUE Prompts and Templates 1 (Part 1): Multi-rounds of Dialogue Generation Question Prompt: System Prompt: You have received descriptive text that provides you with the knowledge, events, and definitions described in the text. You need to generate questions coherently and cover as much of the descriptive text as possible. You just need to output the problem. The maximum number of generated questions is 10. If the previously generated questions are sufficient to cover the entire descriptive text, the output questions can be less than 10. From the provided descriptive text, create up to 10 coherent questions that comprehensively cover its content. Your output should consist only of the questions. It is acceptable to generate fewer than 10 questions if the material has been fully covered. You are required to formulate set of coherent questions from given descriptive text, covering its contents as completely as possible. The number of questions must not exceed 10, but it is permissible to output fewer if they adequately cover the text. The sole output should be the questions. Generate series of logical questions that cover all the knowledge, events, and definitions in the descriptive text you have received. While the maximum number of questions is 10, you can output smaller number if the text is fully addressed. Please ensure you only output the questions. Your task is to generate questions based on descriptive text, ensuring they are coherent and cover its knowledge, events, and definitions as thoroughly as possible. You should generate maximum of 10 questions and only output the questions themselves. You may provide fewer than 10 if they are sufficient to cover the entire text. User Prompt: News: {news} Please generate questions. Given the news: {news} Please generate questions. Can you generate questions for the following news: {news}. Generate questions for the following news: {news}. Please generate questions based on the following news: {news}."
        },
        {
            "title": "Preprint",
            "content": "Prompts and Templates 1 (Part 2): Multi-rounds of Dialogue Generation Answer Prompt: System Prompt: You have gained knowledge and problem to be solved. You need to answer this question based on the content of your knowledge. Output your answer. You now have the necessary knowledge and specific problem. Based only on this information, provide your answer to the question and output the result. You are equipped with the required information and problem to resolve. Formulate your answer based solely on the content of this knowledge and then output it. Using the knowledge you have been given, solve the problem presented. Your response must be based exclusively on this information. Please output your answer. Now that you have the relevant knowledge and the question, you must provide solution. Ensure your answer is derived strictly from the provided content, then output your response. User Prompt: Given the knowledge: {knowledge} Answer the following question: {question}. Knowledge: {knowledge} Answer the following question: {question}. Answer the following question based on the knowledge: Knowledge:{knowledge} Question: {question}. Here is some knowledge: {knowledge} nNow, answer the following question: {question}. You are given the knowledge:{knowledge} Can you answer the following question:{question}. Prompts and Templates 1 (Part 3): Multi-rounds of Dialogue Heuristic question templates for News: What is the {type} news in the image about? Could you summarize the {type} news story presented in the image? What is the {type} news event being depicted in this picture about? Please explain the {type} news that is shown in the image. Can you tell me what the {type} news in this image is about? Heuristic answer templates for News: The {type} news description in the image is {title}. The {type} news in the image can be described as {title}. According to the image, the {type} news description is {title}. The image provides the following {type} news summary: {title}. The {type} news content shown in the picture is {title}. Heuristic answer templates for Entity: What is the {type} entity in the image? Can you identify the {type} entity shown in the picture? What is the {type} entity depicted in this image? Please tell me what the {type} entity in this image is. What {type} entity is visible in the photo? Heuristic answer templates for Entity: The {type} entity in the image is {entity name}. The {type} entity shown in the picture is {entity name}. The {type} entity depicted in the image is {entity name}. The {type} entity illustrated in the picture is {entity name}. The {type} entity present in the image is {entity name}."
        },
        {
            "title": "Preprint",
            "content": "H.4 PROMPT DETAILS REGARDING VISUAL RECOGNITION QA Prompts and Templates 2: Visual Recognition QA Question templates for News: Is the image depicting news {title}? Answer this question with Yes or No. Does this image illustrate the news titled {title}? Answer this question with Yes or No. Is this picture related to the news with the headline {title}? Answer this question with Yes or No. Is the image about the news report named {title}? Answer this question with Yes or No. Does this photo correspond to the news {title}? Answer this question with Yes or No. Question templates for Entity: Is {entity name} in the image? Answer this question with Yes or No. Does the image show {entity name}? Answer this question with Yes or No. Can you see {entity name} in this picture? Answer this question with Yes or No. Is {entity name} visible in the image? Answer this question with Yes or No. Does this picture contain {entity name}? Answer this question with Yes or No."
        },
        {
            "title": "Preprint",
            "content": "H.5 PROMPT DETAILS REGARDING IMAGE CAPTION QA Prompts and Templates 3 (Part 1): Image Caption QA Question templates for News: Please provide description for the {type} news in the image. Answer this question in one paragraph. Could you please describe the {type} news shown in the picture? Answer this question in one paragraph. Please offer description of the {type} news depicted in the image. Answer this question in one paragraph. Please give description of the {type} news depicted here. Answer this question in one paragraph. Can you tell me about the {type} news featured in the photograph? Answer this question in one paragraph. Answer templates for News: The image depicts {title}. {summary} Question templates for Entity: Please provide description for the {type} entity in the image. Answer this question in one paragraph. Could you please describe the {type} entity shown in the picture? Answer this question in one paragraph. Please offer description of the {type} entity depicted in the image. Answer this question in one paragraph. Please give description of the {type} entity depicted here. Answer this question in one paragraph. Can you tell me about the {type} entity featured in the photograph? Answer this question in one paragraph. Answer templates for Entity: The image depicts {entity name}. {summary} Prompts and Templates 3 (Part 2): Image Caption QA Generation Summary Prompt: System Prompt: You have acquired piece of knowledge, and now you need to condense it into paragraph of no more than 25 words, while trying to maintain the original meaning of the knowledge as much as possible. Your task is to take piece of knowledge youve learned and summarize it. The summary must be paragraph of 25 words or less, while retaining the original meaning. You need to distill the information you have acquired into concise paragraph. Ensure it does not exceed 25 words and preserves the essence of the original knowledge as accurately as possible. Condense concept you have just learned into brief paragraph. You must adhere to 25-word limit, all while making sure the core message remains intact. Take the new information you possess and shorten it into single paragraph. This condensed version must be under 25 words and should accurately reflect the original meaning. User Prompt: Knowledge: {knowledge} Please summarize this knowledge. Given the knowledge: {knowledge} Please summarize this knowledge. Can you summarize this content for the following knowledge: {knowledge}. Summarize questions for the following knowledge: {knowledge}. Please summarize this content based on the following knowledge: {knowledge}."
        },
        {
            "title": "Preprint",
            "content": "H.6 PROMPT DETAILS REGARDING VQA Prompts and Templates 4: VQA Generation Quadruplets Prompt: System Prompt: You have acquired piece of knowledge and are now required to generate up to 5 questions based on it. For each generated item, you must provide the question itself, its answer (which should be word or short phrase), subject object extracted from the question, and that subjects hypernym. When extracting the subject object, you must follow critical rule: the subject must be specific entity that is explicitly mentioned within the question itself, serving as key reference point. Crucially, this extracted subject cannot be the answer to the question. helpful test for identifying the correct subject is to check if its name could be logically replaced by placeholder, such as this company or the entity in the image, while the question remains coherent. If the provided knowledge is fully covered by fewer than 5 questions, you may generate fewer. Your task is to generate up to five question sets from the provided knowledge. Each set must include the question, brief answer (word/phrase), subject object, and its hypernym. When selecting the subject object, you must follow key rule: it must be specific entity explicitly named in the question and cannot be the answer. good test is to see if placeholder like this entity can logically replace it. Fewer than five questions are fine if the knowledge is fully covered. Based on the knowledge youve acquired, create maximum of five questions. For each, provide short answer, identify subject object, and state its hypernym. The subject object must adhere to this critical constraint: it must be specific entity mentioned directly in the question that serves as reference point but is not the answer. To verify your choice, check if substituting generic term like this item would keep the question coherent. You may generate fewer questions if they are sufficient. You are required to produce up to five questions from the given information. For each item, output the question, its short answer, subject object, and that subjects hypernym. The rule for extracting the subject object is that it must be specific, named entity within the questions text and must be different from the answer itself. helpful check is to replace its name with placeholder (e.g., this organization) to see if the question still makes sense. Fewer questions are acceptable if the topic is fully addressed. Formulate as many as five questions based on the knowledge. Each output must consist of the question, concise answer, an extracted subject object, and its hypernym. crucial guideline applies: the subject object must be specific entity named in the question that the query revolves around, but it cannot be the answer. You can confirm the correct subject by checking if placeholder such as the specified object could logically take its place. Generating all five questions is not necessary if the knowledge is completely covered. User Prompt: Knowledge: {knowledge} Please generate questions, answers, subjects, hypernyms. Given the knowledge: {knowledge} Please generate questions, answers, subjects, hypernyms. Can you generate questions, answers, subjects, hypernyms for the following knowledge: {knowledge}. Generate questions, answers, subjects, hypernyms for the following knowledge: {knowledge}. Please generate questions, answers, subjects, hypernyms based on the following knowledge: {knowledge}."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Ludwig Maximilian University of Munich",
        "Northeast Forestry University",
        "Shandong University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "University of Science and Technology of China",
        "University of Sydney",
        "Xiamen University"
    ]
}