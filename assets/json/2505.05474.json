{
    "paper_title": "3D Scene Generation: A Survey",
    "authors": [
        "Beichen Wen",
        "Haozhe Xie",
        "Zhaoxi Chen",
        "Fangzhou Hong",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation."
        },
        {
            "title": "Start",
            "content": "3D Scene Generation: Survey Beichen Wen*, Haozhe Xie*, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu 1 5 2 0 2 8 ] . [ 1 4 7 4 5 0 . 5 0 5 2 : r Abstract3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation. Index Terms3D Scene Generation, Generative Models, AI Generated Content, 3D Vision"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "T HE goal of generating 3D scenes is to create spatially structured, semantically meaningful, and visually realistic 3D environment. As cornerstone of computer vision, it supports wide range of applications, from immersive filmmaking [1], [2] and expansive game worlds [3], [4], [5] to architectural visualization [6], [7]. It also plays crucial role in AR/VR [8], [9], [10], robotics simulation [11], [12], and autonomous driving [13], [14] by providing high-fidelity environments for training and testing. Beyond these applications, 3D scene generation is vital for advancing embodied AI [15], [16], [17] and world models [18], [19], [20], which depend on diverse, high-quality scenes for learning and evaluation. Realistic scene synthesis enhances AI agents ability to navigate, interact, and adapt, driving progress in autonomous systems and virtual simulations. As shown in Figure 1, 3D scene generation has gained significant attention in recent years. Early scene generation methods relied on procedural generation using rule-based algorithms [21] and manually designed assets [22], offering scalability and control in game design [23], urban planning [24], [25], and architecture [26], [27]. However, their reliance on predefined rules and deterministic algorithms limits diversity, requiring extensive human intervention for realistic or varied scenes [28]. Advances in deep generative models (e.g., GANs [29], Diffusion models [30]), enable neural networks to synthesize diverse, realistic spatial This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP202210012, MOE-T2EP202230002), and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). (Corresponding author: Ziwei Liu.) The authors are with S-Lab, Nanyang Technological University, Singapore 637335 (email: beichen002@ntu.edu.sg; haozhe.xie@ntu.edu.sg; zhaoxi001@ntu.edu.sg; fangzhou001@ntu.edu.sg; ziwei.liu@ntu.edu.sg) * denotes equal contribution. 120 100 p # 80 40 20 0 Procedural Generation Neural 3D-based Generation Image-based Generation Video-based Generation 93 40 29 5 7 9 14 Before 2018 2019 2020 2021 2022 2023 2024 2025 Fig. 1. Annual statistics of 3D scene generation papers in computer vision conferences, journals, and preprints. The notable rise in publications and the evolving trends in recent years highlight the need for comprehensive survey. Note that the data for 2025 reflects papers published up until April 30th. structures by learning real-world distributions. Combined with innovations in 3D representations like NeRF [31] and 3D Gaussians [32], neural 3D-based generation methods enhance geometry fidelity, rendering efficiency, and view consistency, making them ideal for photorealistic scene synthesis and immersive virtual environments. Starting from single image, image-based scene generation methods leverage camera pose transformations and image outpainting to iteratively synthesize perpetual views [33], [34] or panoramic local environments [35], [36]. Benefit from the rapid advancement of video diffusion models [37], [38], video generation quality has significantly improved, leading to surge in 3D scene generation research over the past two years. These methods formulate 3D scene generation as form of video generation and enhance view consistency through temporal modeling [39]. The integration of dynamic 3D representations [40], [41] further facilitates the synthesis of immersive and dynamic environments [42], [43]. Introduction ( 1) Preliminaries ( 2) Task Definition and Formulation ( 2.1) 3D Scene Representations( 2.2) Generative Models ( 2.3) Voxel Grid Point Cloud Mesh Neural Fields 3D Gaussians Image Sequence Autoregressive Model VAE GAN Diffusion Model Procedural Generator Rule-based Gen. ( 3.1.1) Procedural Gen. ( 3.1) Optm.-based Gen. ( 3.1.2) Methods: Hierarchical Taxonomy ( 3) Neural 3D-based Gen. ( 3.2) Image-based Gen. ( 3.3) Video-based Gen. ( 3.4) t n n D 3 LLM-based Gen. ( 3.1.3) Scene Parameters ( 3.2.1) Scene Graph ( 3.2.2) Semantic Layout ( 3.2.3) Implicit Layout ( 3.2.4) Holistic Gen. ( 3.3.1) Iterative Gen. ( 3.3.2) Two-stage Gen. ( 3.4.1) One-stage Gen. ( 3.4.2) Indoor Datasets ( 4.1.1) Datasets and Evaluation ( 4) Applications and Tasks ( 5) Challenges and Future Directions ( 6) Datasets ( 4.1) Natural Datasets ( 4.1.2) Urban Datasets ( 4.1.3) Metrics-based Eval. ( 4.2.1) Evaluation ( 4.2) Benchmark-based Eval. ( 4.2.2) 3D Scene Editing ( 5.1) Human Eval. ( 4.2.3) Human-Scene Interaction ( 5.2) Embodied AI ( 5.3) Robotics ( 5.4) Autonomous Driving ( 5.5) Challenges ( 6.1) Future Directions ( 6.2) Fig. 2. The overall structure of our comprehensive survey. Our survey presents three core contributions: 1) summary of key representations and generative models in 3D scene generation, 2) hierarchical taxonomy systematically organizing intertwined papers with indepth analysis, and 3) an exploration of datasets, evaluation metrics, applications, along with an outlook on challenges and future directions. Compared to generating 3D objects and avatars, generating 3D scenes presents significantly greater challenges across several dimensions. 1) Scale: Objects and avatars typically exist within fixed, limited spatial extent, while scenes must accommodate multiple entities across much larger and more variable spatial scale. 2) Structural complexity: Scenes involve complex spatial and semantic relationships among diverse objects, requiring the model to ensure both functional coherence and overall plausibility. 3) Data availability: While large-scale datasets for objectand avatar-level generation are abundant, high-quality, annotated 3D scene datasets remain scarce and expensive to collect. 4) Fine-grained control: Scene generation often demands user control over attributes like object placement, zoning, and style, which remain difficult to incorporate in flexible and interpretable way. Despite rapid progress in 3D scene generation, the field lacks comprehensive survey that systematically categorizes existing approaches, highlights key challenges, and identifies future directions. Prior surveys focus on narrow 2 domains such as procedural generation [44], [45], indoor scenes [46], [47], autonomous driving [48], and text-driven generation [49], [50], offering limited perspectives. Broader surveys on general 3D or 4D content generation [51], [52], [53], [54], [55], [56] often treat scene generation only peripherally, leading to fragmented coverage. Although many existing works explore aspects of scene generation, their broader focus often leads to fragmented understanding that overlooks critical components. Some works focus on specific subdomains, such as diffusion models [55], textdriven scene generation [52], or 4D generation [56], while others neglect key representations like 3D Gaussians [51] and image sequences [53], [54], as well as important paradigms like procedural and video-based generation [51], [53], [54]. Surveys on world models [18], [57], [58] primarily address video prediction in driving scenarios, offering only partial view. These gaps call for comprehensive, up-todate survey that consolidates recent advances and maps out the evolving landscape of 3D scene generation. Contributions. This survey offers structured overview of recent advances in 3D scene generation. We categorize existing methods into four types: procedural, neural 3Dbased, image-based, and video-based generation, highlighting their paradigms and trade-offs. We also review key applications in scene editing, human-scene interaction, embodied AI, robotics, and autonomous driving. Additionally, we examine commonly used scene representations, datasets, and evaluation protocols, and identify current limitations in generative capacity, controllability, and realism. Finally, we outline future directions including higher fidelity, physicsaware and interactive generation, and unified perceptiongeneration models. Scope. This survey primarily focuses on approaches for generating 3D scenes in 3D scene representations. Notably, these generative methods aim to synthesize diverse 3D scenes, whereas 3D reconstruction methods can only generate single scene from given input. For review of reconstruction approaches, readers may refer to [59], [60]. Furthermore, this survey excludes general video generation [38], [61] and general 3D object generation [62], [63], [64] methods, even though they have demonstrated some capability in 3D scene generation. This survey complements existing reviews on 3D generative models [51], [52], [53], [54], [55], as none provide comprehensive overview of 3D scene generation or its relevant insights. Organization. summary of this surveys structure is presented in Figure 2. Section 2 provides the foundational concepts, including task definition and formulation, 3D scene representations, and generative models. Section 3 categorizes existing approaches into four types, detailing each categorys paradigm, strengths, and weaknesses. Section 4 introduces relevant datasets and evaluation metrics. Section 5 reviews various downstream tasks related to 3D scene generation. Finally, Section 6 discusses current challenges, future directions, and concludes the survey."
        },
        {
            "title": "2 PRELIMINARIES\n2.1 Task Definition and Formulation\n3D scene generation maps an input x (e.g., random noise,\ntext, images, or other conditions) to a 3D scene representa-",
            "content": "tion (Sec. 2.2) using generative model (Sec. 2.3). : (1) The generated scene is spatially coherent, implicitly or explicitly defines 3D geometry, and enables multi-view rendering or 3D reconstruction. 2.2 3D Scene Representations Various 3D scene representations have been developed and utilized in computer vision and graphics. In this section, we provide an overview of the key 3D scene representations, discussing their structures, properties, and suitability for 3D scene generation. Voxel Grid. voxel grid is 3D array RHW D, where H, , and represent the height, width, and depth of the grid, respectively. Each voxel stores properties such as occupancy or signed distance values [65], enabling structured volumetric scene representation. Point Cloud. point cloud is an unordered set of 3D points = {pi pi R3}N i=1 that approximates an objects surface. Unlike voxel grids, point clouds are sparse, unstructured, memory-efficient, and are commonly generated from depth sensors, LiDAR, and structure-from-motion [66]. Mesh. polygonal mesh = {MV , ME, MF } defines 3D surface through vertices MV (points in space), edges ME (pairwise connections between vertices), and faces MF (flat polygons, such as triangles or quads). It provides explicit connectivity information, making them ideal for modeling the surfaces of 3D scenes. Neural Fields. Signed Distance Field (SDF) [67] and Neural Radiance Field (NeRF) [31] are continuous implicit functions that can be parameterized by neural networks. SDF maps spatial position R3 to signed distance s(x) R, defining surface as its zero-level set. NeRF maps and view direction R3 to volume density σ(x, r) R+ and color c(x, r) R3. SDF is rendered using sphere tracing [68], while NeRF uses differentiable volume rendering [69], [70]. 3D Gaussians. 3D Gaussians [32] represent 3D scenes using 3D Gaussian primitives = {(µi, Σi, ci, αi)}N i=1, where µi R3 is the center, Σi R33 defines the anisotropic shape, ci R3 is the RGB color, and αi [0, 1] is the opacity. The image can be rendered by rasterizing 3D Gaussians onto 2D plane. Image Sequence. An image sequence, implicitly encoding the scenes 3D structure with images from different viewpoints, e.g., = {Ii RHW 3}N i=1, is crucial 3D scene representation widely used in imageand video-based generation methods, where the 3D structure can be inferred through multi-view reconstruction."
        },
        {
            "title": "2.3 Generative Models",
            "content": "Generative models synthesize data by either learning statistical patterns (e.g., AR models, VAEs [71], GANs [29], diffusion models [30]) or applying predefined rules (e.g., procedural generators). While the former approximates data distributions for novel outputs, the latter constructs structured 3D scenes through deterministic or stochastic logic without learned priors. In this section, we briefly introduce representative generative models in 3D scene generation, highlighting their characteristics and mechanisms. 3 Autoregressive Models (AR models) generate data sequentially, where each element is conditioned on the previously generated elements. common formalization of AR models involves the factorization of the joint probability distribution of data into product of conditional probabilities p(x) = (cid:81)T t=1 p(xtx<t). This decomposition follows directly from the chain rule of probability and ensures that each element xt is generated sequentially, conditioned on all previous elements. The probability p(xtx<t) is modeled by deep generative networks [72], [73], which learn to capture the dependencies between the data. Variational Autoencoders (VAEs) [71] are generative models that encode data into probabilistic latent space and decode it back. Given an input x, the encoder maps it to latent distribution q(zx) parameterized by mean µ and variance σ2, where = µ + σ ϵ, ϵ (0, I). The decoder reconstructs from z. Using the reparameterization trick, VAEs enable backpropagation through stochastic sampling. The loss function combines reconstruction loss (to preserve input features) and KL divergence (to regularize the latent space), which allows VAEs to generate smooth and meaningful data variations. However, since VAEs optimize likelihood, they often spread probability mass beyond the true data manifold, causing blurry and less detailed generated samples [74], [75]. Generative Adversarial Networks (GANs) [29] consist of two networks the Generator and the Discriminator that compete in minimax game. The Generator takes random noise and generates fake data G(z), while the Discriminator tries to distinguish real data from fake data G(z). The objective is to optimize the Generator to create realistic data that the Discriminator cannot distinguish from real data, and to train the Discriminator to classify real and fake data correctly, which can be represented by the objective function min max Expdata(x)[log D(x)] + Ezpz(z)[log(1 D(G(z)))] (2) where pdata(x) is the real data distribution and pz(z) is the random noise distribution. key drawback of GANs is that they can be difficult to train, often suffering from issues like mode collapse and instability [76]. Diffusion Models [30] are generative models that operate by gradually adding noise to data in forward process, transforming it into pure noise, and then learning to reverse this process by denoising to recover the original data. The forward process is modeled as Markov chain, where each step xt is obtained by adding Gaussian noise to the previous step xt1, defined by xt = βtϵt, where ϵt is Gaussian noise and βt controls the noise schedule. The reverse process aims to model p(xt1xt), learning how to reverse the added noise and regenerate the original data. While these models generate high-quality data and are more stable than GANs, they are computationally expensive and slow due to the iterative denoising process [77]. Procedural Generators [44] are algorithmic systems that synthesize 3D scenes through iterative application of parametric rules and mathematical operations. These generators transform an initial state S0 (e.g., geometric primitive or empty scene) into structured output Sn via recursive or iterative processes governed by St+1 = R(St, Θ), where 1 βtxt1 + TABLE 1 General comparison of 3D scene generation categories across key characteristics. Individual methods may vary. 4 Characteristic Procedural Gen. Neural 3D-based Gen. Image-based Gen. Video-based Gen. Realism Diversity View Consistency : Stylized or repetitive textures : Limited variations due to predefined assets : 3D-consistent representations/rendering Semantic Consistency : Procedure ensures Efficiency Controllability Physical Plausibility cross-view coherence : Usually slow; can be faster for lower quality : Limited by predefined rules or constraints : Guaranteed physics engines by : Limited by the quality of 3D datasets : Diversity depends on training data : 3D-consistent representations : 3D priors preserve cross-view coherence : Costly due to complex representations : Rarely support text or image conditions : Constrained by 3D geometry variations : Photorealistic but lacks accurate depth : Rich from real-world images : Usually adopts explicit 3D representation : No global context; lack cross-view coherence : Efficient per frame but lacks reuse : Controlled mainly by text or images : Hard to infer from the static context variations : High-quality temporal coherence : Rich from real-world videos : Implicit geometry estimation, less reliable : Frame-level coherence but possible drift : Costly due to sequential inference : Controlled by diverse conditions : Achieved through temporal modeling represents set of predefined rules (e.g., subdivision, perturbation, or spatial partitioning), and Θ denotes tunable parameters (e.g., seed values, perturbation amplitudes, or recursion depth). The rules define deterministic or constrained stochastic operations, ensuring reproducibility when Θ is fixed."
        },
        {
            "title": "3 METHODS: A HIERARCHICAL TAXONOMY",
            "content": "We classify existing methods into four categories based on their generation paradigms illustrated in Figures 3 to 6: Procedural Generation creates 3D scenes using predefined rules, enforced constraints, or prior knowledge from LLMs, resulting in high-quality outputs that integrate seamlessly with graphics engines. Neural 3D-based Generation employs 3D-aware generative architectures to synthesize scene layouts for object placement or directly generate 3D representations such as voxels, point clouds, meshes, NeRFs, and 3D Gaussians. Image-based Generation uses 2D image generators to synthesize images either in one step or iteratively, sometimes followed by 3D reconstruction for geometric consistency. Video-based Generation uses video generators to create both 3D scenes with spatial movement and 4D scenes that evolve over time, capturing dynamic changes in both space and time."
        },
        {
            "title": "3.1 Procedural Generation",
            "content": "Procedural generation methods automatically generate 3D scenes by following predefined rules or constraints. They are widely used in computer graphics to create diverse environments, including terrains, vegetation, rivers, roads, rooms, buildings, and entire cities. As shown in Table 1, procedural generation methods offer high efficiency and spatial consistency, but often require careful tuning to achieve realism and user control. The paradigms of these methods are illustrated in Figure 3, which can be further categorized into rule-based, optimization-based, and LLM-based generation. Fig. 3. The paradigms of procedural methods for 3D scene generation. (a) Rule-based generation methods follow predefined rules to generate 3D scenes. (b) Optimization-based generation finds an optimized scene under different constraints. (c) LLM-based generation uses large language models (LLMs) for tasks like layout design and object selection, or to generate code that controls other generators. Note that dashed arrows denote optional operations. Optm., Ret., and SG. denote Optimization, Retrieval, and Shape Generation, respectively. Interaction refers to user actions such as click, drag, or selection during the generation process."
        },
        {
            "title": "3.1.1 Rule-based Generation",
            "content": "Rule-based procedural generation encompasses range of approaches that construct 3D scenes through explicit rules and algorithms. These methods directly generate scene geometry, which is then rendered for visualization. Common techniques include fractal-based, grammar-based, simulation-driven, and example-based generation. Fractals [121], [122], [123] are mathematical structures that exhibit self-similarity across scales. Fractal-based methods are widely applied in terrain modeling and texture synthesis, as they efficiently generate visually complex patterns while requiring minimal storage. Techniques such as midpoint displacement [124], [125] and fractional Brownian motion [126] (fBM) generate multi-scale details that resemble natural landscapes. Grammar-based methods consist of an alphabet of symbols, an initial axiom, and set of rewriting rules. Each generated symbol encodes geometric commands for complex shape generation. CityEngine [3] extends L-systems [127] for the generation of road networks and building geometry to create cities. uller et al. [6] build upon shape grammars Summary and comparison of representative works for 3D scene generation. The table compares scene types (e.g., urban) and conditioning modalities (e.g., unconditioned, text, indoor, nature, ID top-down image, constraint, motion, scene graph, semantic volume, SD semantic map, LiDAR, bounding box, camera pose, user action) across various 3D scene representations. Note that Optm., Gen., Rep., and Seq. are short for Optimization, Generative, Representation, and Sequence, respectively. image, TABLE 2 5 Category Method Venue Gen. Model Scene Type Condition 3D Scene Rep. Rule-based Optm.-based LLM-based Scene Parameters Scene Graph Semantic Layout Implicit Layout Holistic Iterative Two-stage One-stage Musgrave et al. [78] CityEngine [3] Cordonnier et al. [79] Infinigen [80] Make it home [28] Wu et al. [27] ProcTHOR [15] Infinigen Indoors [81] LayoutGPT [82] 3D-GPT [83] SceneX [84] DeepSynth [85] ATISS [86] MIME [87] DiffuScene [88] PlanIT [89] GRAINS [90] Graph-to-3D [91] CommonScenes [92] InstructScene [93] GANcraft [94] CC3D [95] InfiniCity [96] SceneDreamer [97] CityDreamer [98] Comp3D [99] BlockFusion [100] GSN [101] GAUDI [102] NeuralField-LDM [103] 3 [104] Director3D [105] ImmerseGAN [106] MVDiffusion [36] PanFusion [107] PERF [108] LayerPano3D [109] PixelSynth [110] GFVS [111] Infinite Nature [33] 3D Cinemagraphy [112] Text2Room [113] Text2NeRF [114] WonderJourney [115] LucidDreamer [116] 4Real [117] DimensionX [42] MagicDrive [39] Vista [118] GenX [119] 4K4DGen [43] GameGen-X [120] SIGGRAPH89 SIGGRAPH01 TOG17 CVPR23 TOG11 CGF18 NeurIPS22 CVPR24 NeurIPS23 3DV25 AAAI25 Procedural Procedural Procedural Procedural Procedural Procedural Procedural Procedural Procedural Procedural Procedural TOG18 NeurIPS21 CVPR23 CVPR24 TOG19 TOG19 ICCV21 NeurIPS23 ICLR24 ICCV21 ICCV23 ICCV23 TPAMI23 CVPR24 3DV24 TOG24 ICCV21 NeurIPS22 CVPR23 CVPR24 NeurIPS24 3DV22 NeurIPS23 CVPR24 TPAMI24 SIGGRAPH25 ICCV21 ICCV21 ICCV21 CVPR23 ICCV23 CVPR24 CVPR24 arXiv NeurIPS24 arXiv24 ICLR24 NeurIPS24 ICLR25 ICLR25 ICLR25 GAN Autoregressive Autoregressive Diffusion Autoregressive VAE VAE Diffusion Diffusion GAN GAN GAN GAN GAN Diffusion Diffusion GAN Diffusion Diffusion VAE&Diffusion Diffusion GAN Diffusion Diffusion Diffusion Diffusion VAE GAN GAN GAN Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion u o e - r d b - m s - V U I I U I I I U N I U U I N N I N U U I N U U U ID X ID T ID ID ID + ID G SD X + SD / / / / / / I I / / / / / + SD + + / / / / Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh Mesh SDF Mesh Mesh NeRF NeRF NeRF NeRF NeRF NeRF SDF NeRF NeRF NeRF Voxel Grid 3D Gaussians Image Seq. Image Seq. Image Seq. NeRF 3D Gaussians Point Cloud Image Seq. Image Seq. Point Cloud Mesh NeRF Point Cloud 3D Gaussians 3D Gaussians 3D Gaussians Image Seq. Image Seq. Image Seq. 3D Gaussians Image Seq. [128] to model highly detailed 3D buildings. Simulation-based procedural generation creates realistic 3D environments by modeling natural and artificial processes. Some methods simulate erosion effects [78], [129], [130] and hydrology [131], [132], [133] to generate terrain with high fidelity. Vegetation simulations model plant growth under resource competition [79], [134], [135] and climate change [136]. In urban contexts, ecosystem-based approach populates cities with vegetation [137], while others simulate city growth and resource distribution to generate settlements that evolve organically over time [138], [139]. Example-based procedural methods are proposed to improve controllability. These techniques take small userprovided example and generate larger scene by expanding its boundary [140], [141] or matching features [142], [143]. Inverse procedural generation attempts to provide highlevel control over the generation process. These methods apply optimization functions to infer parameters from procedural algorithms [26], [144] or learn global distribution for scene arrangement [145]. The aforementioned techniques are often combined to harness their complementary strengths for generating largescale, diverse scenes. For example, Citygen [146] integrates road networks and building generation for cityscapes, while Infinigen [80] combines material, terrain, plant, and creature generators for infinite natural scenes."
        },
        {
            "title": "3.1.2 Optimization-based Generation",
            "content": "Optimization-based generation formulates scene synthesis as an optimization problem that minimizes objectives encoding predefined constraints. These constraints, typically derived from physics rules, functionality, or design principles, are embedded into cost functions and optimized using stochastic or sampling-based methods. Alternatively, statistical approaches learn spatial relationships from data and guide the layout process through probabilistic sampling. Some systems support user-defined constraints and user interactions to enable controllable and semantically meaningful generation. Some approaches formulate physical and spatial constraints as cost functions and apply stochastic optimization methods for scene generation. Physical-level constraints include object interpenetration, stability, and friction [147]. Layout-level constraints, including functional relationships (e.g., co-occurrence, accessibility), interior design guidelines(e.g., symmetry, alignment, co-circularity), and human behavior patterns, have also been considered [28], [148], [149]. High-level constraints such as scene type, size, and layout can be specified by users [15], [27], [150], enabling more controllable and semantically meaningful scene synthesis. Leveraging existing procedural generation pipelines, Infinigen Indoors [81] introduces constraint specification API, allowing users to define custom constraints and achieve highly controllable scene generation. Other methods adopt data-driven models to learn object arrangement patterns from annotated data, transforming scene generation into probabilistic sampling problem. Bayesian networks are commonly used [151], [152], [153] to capture conditional dependencies between objects, while graph-based models [154], [155], [156] model spatial hierarchies or relational structures to improve spatial reasoning and object placement accuracy."
        },
        {
            "title": "3.1.3 LLM-based Generation",
            "content": "Large Language Models [157] (LLMs) and Vision-language models [158] (VLMs) have introduced new paradigm in procedural generation by enabling text-driven scene synthesis, allowing users to specify environments through natural language descriptions, offering greater flexibility and user control over scene design. 6 Several approaches use LLMs to generate scene layouts, such as object parameters [82], [159], [160], [161], [162], [163], [164], [165], [166] and scene graph [167], [168], [169], [170], [171], [172]. Based on these layouts, 3D geometries can be obtained through object retrieval or shape generation. Specifically, LayoutGPT [82] guides LLMs using generation prompts and structural templates to produce object parameters for retrieving assets. CityCraft [161] guides land-use planning with LLMs and retrieves building assets from database to construct detailed urban environments. IDesign [167] and Deng et al. [168] use graph-based object representations to model inter-object semantics more effectively. To support more stylized and versatile scene generation, GraphDreamer [170] and Cube [172] generate scene graphs via LLMs, treating nodes as objects and enabling compositional scene generation through 3D object generation models. The Scene Language [165] introduces language-based scene representation composed of program, words, and embeddings, which can be generated by LLMs and rendered using traditional, neural, or hybrid graphics pipelines. Other methods utilize LLMs as agents to control procedural generation by adjusting parameters of rule-based system or modifying operations within procedural generation software. Liu et al. [173] employ LLMs to fine-tune parameters in rule-based landscape generation, optimizing procedural workflows with learned priors. 3D-GPT [83] and SceneCraft [174] generate Python scripts to control existing procedural frameworks, such as Infinigen [80] and Blender1, allowing direct manipulation of procedural assets. Holodeck [175] generates 3D environment through multiple rounds of conversation with an LLM, including floor and wall texturize, door and window generation, object selection and placement. CityX [24] and SceneX [84] use multiagent system for different stages of generation, producing Python codes for layout, terrain, building, and road generation through Blender rendering. WorldCraft [176] further incorporates object generation and animation modules."
        },
        {
            "title": "3.2 Neural 3D-based Generation",
            "content": "Neural 3D-based methods generate 3D scene representations using generative models trained on datasets with 3D annotations. Recent advancements in NeRF and 3D Gaussians have further enhanced the fidelity and realism. As shown in Table 1, these methods achieve high view and semantic consistency, but their controllability and efficiency remain limited. As shown in Figure 4, the methods are categorized into four types based on the spatial arrangement that controls the layout of generated 3D scenes: scene parameters, scene graph, semantic layout, and implicit layout."
        },
        {
            "title": "3.2.1 Scene Parameters",
            "content": "Scene parameters offer compact way to represent object arrangements, implicitly capturing inter-object relationships without relying on explicit scene graphs. These parameters typically encompass an objects location, size, orientation, class, and shape latent code. As illustrated in Figure 4a, 1. https://www.blender.org/ 7 niture in room from human motion. SUMMON [197] and MIME [87] further improve semantic consistency and physical affordances by generating objects with meshes that align with human-scene contact. Vuong et al. [198] propose multi-conditional diffusion model that integrates text prompts to enhance controllability. To ensure physically plausible layouts free from contact or collisions, INFERACT [199] optimizes scene layout generation while simultaneously simulating human movement in physics-based environment using reinforcement learning."
        },
        {
            "title": "3.2.2 Scene Graph",
            "content": "Scene graphs offer structured, symbolic representation of 3D scenes, with nodes representing objects and edges capturing their spatial relationships. Incorporating scene graphs allows generative models to enforce spatial constraints and preserve relational consistency, facilitating the creation of well-structured 3D environments. Following the paradigm illustrated in Figure 4b, scene graphs, whether generated by models or provided as input, function as layout priors that guide the decoding process to create 3D scene representations by object retrieval or shape generation. Early data-driven approaches [200], [201], [202], [203] represent spatial relationships between objects using scene graphs, which serve as blueprint for 3D scene generation through object retrieval and placement. Subsequent works enhance graph representations and introduce advanced generative models. PlanIT [89] employs deep graph generative model to synthesize scene graphs, followed by an image-based network for object instantiation. GRAINS [90] adopts recursive VAE to learn scene structures as hierarchical graphs, which can be decoded into object bounding boxes. 3D-SLN [204] utilizes scene graphs as structural prior for 3D scene layout generation, ensuring spatial coherence, and further incorporates differentiable rendering to synthesize realistic images. Meta-Sim [205] and MetaSim2 [206] use scene graphs to structure scene generation, optimizing parameters for visual realism and synthesizing diverse 3D scenes using rendering engines. Previous methods enable scene generation from scene graphs but rely on object retrieval or direct synthesis, limiting geometric diversity. To address this, Graph-to-3D [91] introduces graph-based VAE that jointly optimizes layout and shape. SceneHGN [207] represents scenes as hierarchical graphs spanning from high-level layout to fine-grained object geometry, using hierarchical VAE for structured generation. CommonScenes [92] and EchoScene [208] propose scene graph diffusion models with dual-branch design for layout and shape, capturing both global scene-object relationships and local interactions. MMGDreamer [209] introduces mixed-modality graph for meticulous control of object geometry. inter-object Recent methods improve controllability by integrating human input. SEK [210] encodes scene knowledge as scene graph within conditioned diffusion model for sketchdriven scene generation. InstructScene [93] integrates text encoders with graph-based generative models for textdriven scene synthesis. To generalize scene-graph-based generation to broader scenes, Liu et al. [211] map scene graphs onto Birds Eye View (BEV) embedding map, which guides diffusion model for large-scale outdoor Fig. 4. The paradigms of neural 3D-based methods for 3D scene generation. These paradigms use (a) scene parameters, (b) scene graphs, (c) semantic layouts, and (d) implicit layouts as intermediate representations to control the spatial arrangement of generated 3D scenes. These representations, either user-provided or produced by generative models, are then converted into 3D scene representations (e.g., voxel grid, mesh, NeRF, or 3D Gaussians) via retrieval or decoding. Note that dashed arrows denote optional operations. Enc. and Dec. stand for Encoder and Decoder, respectively. Shape Gen. represents Shape Generation. these methods first generate scene parameters as an intermediate representation, which is then used to synthesize the final 3D scene. DeepSynth [85], FastSynth [177], Zhang et al. [178], and Sync2Gen [179] adopt CNN-based architectures that utilize top-down image-based scene representations, sequentially inserting objects by predicting their parameters. Subsequent works explore more advanced models, such as transformers and diffusion models. ATISS [86], SceneFormer [180], COFS [181], and Nie et al. [182] use transformers to autoregressively generate object parameters. RoomDesigner [183] refines this process by decoupling layout and shape generation, ensuring shape compatibility in indoor scenes. CASAGPT [184] leverages cuboids as intermediate object representations to better avoid object collisions. DeBaRA [185] adopts diffusion model for object parameter generation, while PhyScene [186] further integrates physical constraints for physical plausibility and interactivity. To improve controllability in text-driven scene generation, RelScene [187] employs BERT [188] to align spatial relationships with textual descriptions in latent space. DiffuScene [88] leverages latent diffusion models [189] to generate object parameters from text inputs, followed by object retrieval. Ctrl-Room [190] and SceneFactor [191] employ LDMs to generate coarse object layouts from text prompts, with fine-grained appearance obtained via panorama generation and geometric diffusion model, respectively. Epstein et al. [192], SceneWiz3D [193], and DreamScene [194] adopt multi-stage approach, first generating an initial object layout, then refining object geometry using Score Distillation Sampling (SDS) [195], followed by global refinement step to improve compositional consistency. Human movement and interactions often influence the organization of environments, where motion patterns and physical contact inform the arrangement of objects and scene layouts. Pose2Room [196] introduces an end-to-end generative model that predicts the bounding boxes of furscene synthesis. HiScene [212] leverages VLM-guided occlusion reasoning and video diffusion-based amodal completion to generate editable 3D scenes with compositional object identities from single isometric view."
        },
        {
            "title": "3.2.3 Semantic Layout",
            "content": "Semantic layouts serve as an intermediate representation that encodes the structural and semantic organization of 3D scene. It provides high-level guidance for 3D scene generation, ensuring controllability and coherence in the placement of objects and scene elements. As shown in Figure 4c, semantic layouts, whether user-provided or generated, act as precise constraints for generative models, guiding 3D scene generation while enabling optional textural prompts for style control. 2D semantic layout consists of 2D semantic map, sometimes including additional maps such as height maps, viewed from top-down perspective. CC3D [95] generates 3D feature volume conditioned on 2D semantic map, which serves as NeRF for neural rendering. BerfScene [213] incorporates positional encoding and low-pass filtering to make the 3D representation equivariant to the BEV map, enabling controllable and scalable 3D scene generation. Frankenstein [214] encodes scene components into compact triplane [215], generated via diffusion process conditioned on 2D semantic layout. BlockFusion [100] introduces latent triplane extrapolation mechanism for unbounded scene expansion. Incorporating height map with the semantic map enables the direct conversion of 2D layouts into 3D voxel worlds, essential for urban and natural scenes where building structures and terrain elevation provide important priors. InfiniCity [96] utilizes InfinityGAN [216] to generate infinite-scale 2D layouts, which are then used to create watertight semantic voxel world, with textures synthesized through neural rendering. For natural scene generation, SceneDreamer [97] employs neural hash grid to capture generalizable features across various landscapes, modeling spaceand scene-varied hyperspace. To address the diversity of buildings in urban environments, CityDreamer [98] and GaussianCity [217] break down the generation process into distinct background and building components. CityDreamer4D [218] further integrates dynamic traffic systems to generate an expansive 4D city. 3D semantic layout offers enhanced capability to represent more complex 3D layouts compared to 2D, improving controllability, typically by using voxels or 3D bounding boxes. GANcraft [94] uses voxels as the 3D semantic layout, optimizing neural field with pseudo-ground truth and adversarial training. UrbanGIRAFFE [219] and DisCoScene [220] break down the scene into stuff, objects, and sky, and adopt compositional neural fields for scene generation. By incorporating score distillation sampling (SDS) [195], 3D semantic layouts offer better control over text-guided scene generation, improving the alignment of generated scenes with textual descriptions. Comp3D [99], CompoNeRF [221], Set-the-Scene [222], and Layout-your3D [223] generate 3D scenes with compositional NeRFs using pre-defined customizable layouts as object proxies. SceneCraft [224] and Layout2Scene [225] generate indoor scenes by distilling the pretrained diffusion models. Urban Architect [226] integrates geometric and semantic con8 straints with SDS, ensure better view-consistency in urban scene generation. leveraging the scalable hash grid to"
        },
        {
            "title": "3.2.4 Implicit Layout\nImplicit layouts are feature maps that encode the spatial\nstructure of a 3D scene. As shown in Figure 4d, these\nlayouts manifest as latent features of different dimensions.\nEncoders learn to embed 3D scene layout information into\nlatent feature maps, which are then used by the decoder to\ngenerate 3D scenes in the form of NeRF, 3D Gaussians, or\nvoxel grids.",
            "content": "Recent advances in representations like NeRFs and 3D Gaussians have enabled neural networks to directly generate and render high-fidelity RGB images from latent feature maps. Some methods leverage these representations to produce appearance-consistent 3D scenes with photorealistic quality. NeRF-VAE [227] encodes shared information across multiple scenes using VAE. GIRAFFE [228] represents scenes as compositional generative neural fields to disentangle objects from background. GSN [101] and Persistent Nature [229] adopt GAN-based architectures to generate 2D latent grids as implicit scene layouts, which are sampled along camera rays to guide NeRF rendering. GAUDI [102] employs diffusion model to learn scene features and camera poses jointly, decoding them into tri-plane and pose for NeRF-based rendering control. NeuralField-LDM [103] decomposes NeRF scenes into hierarchical latent structure that includes 3D voxel, 2D BEV, and 1D global representations. Hierarchical diffusion models are then trained on this tri-latent space for generation. Director3D [105] uses Gaussian-driven multi-view latent diffusion model to generate pixel-aligned and unbounded 3D Gaussians along generated trajectory, followed by SDS refinement. Prometheus [230] and SplatFlow [231] learn compressed latent space from multi-view images, and decode this latent space into pixel-aligned 3DGS representations. Another branch of work focuses more on generating semantic structure and scene geometry, typically using voxel grids as representations. These methods are not immediately renderable but can be textured through external rendering pipelines. Lee et al. [232] introduce discrete and latent diffusion models to generate and complete 3D scenes consisting of multiple objects, represented as semantic voxel grids. Due to the computational challenges posed by voxel grids, DiffInDScene [233], PDD [234], 3 [104], and LT3SD [235] use hierarchical diffusion pipeline to generate large-scale and fine-grained 3D scenes efficiently. SemCity [236] employs tri-plane representation for 3D semantic scenes, allowing for generation and editing by manipulating the tri-plane space during diffusion. NuiScene [237] encodes the local scene chunks into vector sets, and uses diffusion model to generate neighboring chunks for unbounded outdoor scenes. DynamicCity [238] tackles dynamic scene generation by employing Padded Rollout to unfold Hexplane [239] into 2D feature maps and applying diffusion for denoising, enabling 4D scene generation."
        },
        {
            "title": "3.3 Image-based Generation",
            "content": "The limited availability of annotated 3D datasets constrains the generation of 3D scenes. Image-based generation attempts to bridge the gap between 2D and 3D generation. As 9 the other applies 3D reconstruction (e.g., surface reconstruction [190], [260], [261], NeRF [108], and 3D Gaussian Splatting [109], [262], [263], [264], [265]) as post-processing. In this context, LayerPano3D [109] breaks the generated panorama into depth-based layers, filling in unseen content to help create complex scene hierarchies. Another research direction focuses on generating geometrically consistent street-view panoramas from satellite images. Some methods [266], [267], [268] integrate geometric priors into GAN-based frameworks to learn cross-view mappings. Others [269], [270], [271] estimate 3D structures from satellite images and synthesize textures for rendered street-view panoramas."
        },
        {
            "title": "3.3.2 Iterative Generation",
            "content": "As shown in Figure 5b, iterative generation starts with an initial 2D image, either provided by the user or generated from text prompts. To generate large-scale 3D scenes, these methods progressively extrapolate the scene along predefined trajectory. By expanding and refining content step by step, they continuously optimize the 3D scene representation, enhancing geometric and structural coherence. Given single image, early methods infer 3D scene representations and use them to render novel views. These representations include point clouds [110], [272], [273], [274], multi-plane images [275], [276], depth maps [277], and meshes [278]. Despite enabling fast rendering, these representations limit camera movement due to their finite spatial extent. To enable unrestricted camera movement, Infinite Nature [33], InfiniteNature-Zero [34], Pathdreamer [279], and SGAM [280] follow render-refine-repeat manner, iteratively warping previous views and outpainting missing regions. DiffDreamer [281] improves multi-view consistency by conditioning on multiple past and future frames using diffusion model. Rather than using explicit 3D representations, GFVS [111] and LOTR [282] encode images and camera poses directly, using transformers to generate novel views. Tseng et al. [283], Photoconsistent-NVS [284], and ODIN [285] improve long-term view synthesis consistency with pose-guided diffusion model. CAT3D [286] uses multi-view LDM to generate novel views from input images, followed by 3D reconstruction for interactive rendering. Similarly, Bolt3D [287] generates scene appearance and geometry through multi-view diffusion but directly outputs 3D Gaussians to avoid time-consuming optimization. Text-driven scene generation boosts diversity and controllability by leveraging pretrained text-to-image diffusion models [189], [288]. Without requiring extensive domain-specific training, these methods iteratively shift the camera view, outpaint images based on text prompts. [290], PanoFree [291], OPaPanoGen [289], AOG-Net Ma [292], and Invisible Stitch [293] iteratively outpaint images in perspective view and seamlessly stitch them into panoramic scene. Other approaches leverage depth estimator [294], [295], [296] to merge RGB images into unified 3D scene. SceneScape [297], Text2Room [113], and iControl3D [298] use 3D meshes as an intermediary proxy to fuse diffusion-generated images into coherent 3D scene representation iteratively. WonderJourney [115] adopts point cloud representation and leverages VLMguided re-generation strategy to ensure visual fidelity. Fig. 5. The paradigms of image-based methods for 3D scene generation. (a) Holistic generation creates an entire scene image in one step. (b) Iterative generation progressively extends the scene by extrapolating sequence of images. shown in Table 1, they offer photorealism and diversity with efficient per-frame processing but struggle with depth accuracy, long-range semantic consistency, and view coherence. The methods fall into two categories: holistic and iterative generation, as illustrated in Figure 5. Holistic generation produces complete scene image in single step, while iterative generation gradually expands the scene through extrapolation, generating sequence of images."
        },
        {
            "title": "3.3.1 Holistic Generation\nAs shown in Figure 5a, holistic generation in 3D scene gen-\neration often relies on panoramic images, which provide a\nfull 360° × 180° field of view, ensuring spatial continuity and\nexplicit geometric constraints. This makes them particularly\neffective in mitigating scene inconsistencies that arise in\nperspective views.",
            "content": "Given an RGB image, early methods [240], [241], [242], [243], [244], [245] use GANs for image outpainting to fill masked regions in panoramas. More recent approaches employ advanced generative models (e.g., CoModGAN [246] and VQGAN [247]) for greater diversity and content control. ImmerseGAN [106] leverages CoModGAN for user-controlled generation. OmniDreamer [248] and Dream360 [249] use VQGAN to generate diverse and highresolution panoramas. Leveraging advances in latent diffusion models (LDM) [189], PanoDiffusion [250] enhances scene structure awareness by integrating depth into bimodal diffusion framework. Text-to-image models (e.g., CLIP [251], LDM [189]) enable text-driven panorama generation. Text2Light [35] uses CLIP for text-based generation and hierarchical samplers to extract and piece together panoramic patches based on the input text. Some approaches [252], [253] leverage diffusion models to generate high-resolution planar panoramas. However, they fail to guarantee the continuity at image boundaries, which is essential in creating seamless viewing experience. To address this, MVDiffusion [36], DiffCollage [254], and CubeDiff [255] generate multi-view consistent images and align them into closed-loop panorama for smooth transitions. StitchDiffusion [256], Diffusion360 [257], PanoDiff [258], and PanFusion [107] adopt padding and cropping strategies at boundaries to maintain the continuity. Recent methods extend single-view panorama generation to multi-view for immersive scene exploration, following two main strategies: one directly generates multiview panoramic images with diffusion models [259], while"
        },
        {
            "title": "3.4.1 Two-stage Generation",
            "content": "As shown in Figure 6a, two-stage generation divides the generation into two stages, each targeting multi-view spatial consistency and multi-frame temporal coherence separately. To further improve view consistency, these generated sequences are subsequently used to optimize dynamic 3D scene representation (e.g., 4D Gaussians [40], Deformable Gaussians [41]). VividDream [314] first constructs static 3D scene through iterative image outpainting, then renders multi-view videos covering the entire scene and applies time-reversal [315] to animate them, creating dynamic videos across viewpoints. PaintScene4D [316] first generates video from text description using video diffusion, then refines it through iterative warping and inpainting at each timestamp to maintain multi-view consistency. Similarly, 4Real [117], DimensionX [42], and Free4D [317] first generate coherent reference video and then extend view angles using frame-conditioned video generation."
        },
        {
            "title": "3.4.2 One-stage Generation",
            "content": "As shown in Figure 6b, one-stage generation consolidates generation into single process, implicitly capturing spatiotemporal consistency to produce singleor multi-view videos from any viewpoint and timestep within unified model. Some approaches [318], [319], [320], [321], [322], [323], [324], [325] adopt video diffusion models for iterative view extrapolation, followed by 3DGS optimization to build static scene. To generate dynamic scenes, GenX [119] and CAT4D [326] adopt distinct multiview-temporal strategies to construct multi-view video models capable of generating all views across all timestamps. StarGen [327] and Streetscapes [328] use past frames as guidance for video generation, enhancing long-range scene synthesis through an autoregressive approach. By utilizing the natural multiview 3D prior of panoramic images, 4K4DGen [43] samples perspective images from static panorama, animates them, and aligns them into dynamic panorama. 360DVD [329], Imagine360 [330], Genex [331], and DynamicScaler [332] integrate panoramic constraints into video diffusion models to generate spherical-consistent panoramic videos. In scene generation for video games and autonomous driving, these methods enhance both control and realism by integrating various control signals as conditions. In open-world gaming environments, vast datasets comprising user inputs and rendered videos enable models like DIAMOND [333], GameNGen [334], Oasis [335], GameGenX [120], and WORLDMEM [336] to predict future frames based on user interactions, creating responsive virtual environments as neural game engines. In autonomous driving, models such as DriveDreamer [337], MagicDrive [39], DriveWM [338], and GAIA-1 [339] utilize inputs like text, bounding boxes, Birds Eye View (BEV) maps, and driver actions to control video generation for complex driving scenarios. Recent works further enhance view consistency [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], expand control capabilities [118], [351], [352], [353], [354], enable 3D-level control via occupancy [355], [356], [357], [358], [359], support multimodal output [360], [361], [362], and improve generation speed [363] and sequence length [364], [365], [366], [367]. Fig. 6. The paradigms of video-based methods for 3D scene generation. (a) Two-stage methods employ two generators, at least one being video generator, to synthesize multi-view videos while utilizing dynamic 3D scene representations for consistency and exploration. (b) One-stage methods produce single or multi-view videos within unified process and model, optionally optimizing dynamic 3D scene representations. Note that dashed arrows denote optional operations. Ctrl. stands for Control. Text2NeRF [114] and 3D-SceneDreamer [299] adopt NeRFbased representations to mitigate error accumulation in geometry and appearance, improving adaptability across scenarios. Scene123 [300] further enhances photorealism by using GAN framework, where the discriminator compares outputs from the video generator with those from the scene generator. By introducing 3D Gaussian Splatting [32], LucidDreamer [116], Text2Immersion [301], WonderWorld [302], RealmDreamer [303], BloomScene [304], and WonderTurbo [305] adopt 3D Gaussians as 3D scene representations for higher quality and faster rendering. Leveraging recent advancements in powerful large reconstruction models [306], [307], [308], [309], [310], SynCity [311] enables training-free generation of high-quality 3D scenes by iteratively performing image outpainting, 3D object generation, and stitching. Another research direction conducts iterative view synthesis and image animation simultaneously to build dynamic 3D scene from single image. 3D Cinemagraphy [112] and Make-It-4D [312] use layered depth images (LDIs) to build feature point clouds and animate scenes via motion estimation and 3D scene flow. 3D-MOM [313] first optimizes 3D Gaussians by generating multi-view images from single image, and then optimizes 4D Gaussians [40] by estimating consistent motion across views."
        },
        {
            "title": "3.4 Video-based Generation",
            "content": "[61] Recent advances in video diffusion models [38], have demonstrated significant progress in generating highquality video content. Building on these advancements, video-based 3D scene generation methods produce image sequences, enabling the synthesis of immersive and dynamic environments. As shown in Table 1, they provide high realism and diversity through sequential generation, benefiting from temporal coherence across frames. However, they face challenges in ensuring consistent view alignment. These methods can be divided into two-stage and one-stage categories, with their paradigms illustrated in Figure 6."
        },
        {
            "title": "4.1 Datasets",
            "content": "We summarize the commonly used datasets for 3D scene generation in Table 3, grouping them by scene type into three categories: indoor, natural, and urban."
        },
        {
            "title": "4.1.1 Indoor Datasets",
            "content": "Existing indoor datasets are either captured from real-world scenes using RGB or RGB-D sensors or professionally designed with curated 3D CAD furniture models. Real-world datasets are captured from physical scenes using sensors like depth, DSLR, or panoramic cameras. Early datasets provide RGB-D or panoramic images with semantic labels (e.g., NYUv2 [369], 2D-3D-S [372]), while recent ones like ScanNet [375] and Matterport3D [374] offer 3D reconstructions with dense meshes and instance-level annotations. SUN360 [368] contains 67,583 high-res 360 180 panoramic images in equirectangular format, manually categorized into 80 scene types. NYUv2 [369] provides 1,449 densely annotated RGBD images from 464 indoor scenes, covering per-pixel semantics and instance-level objects. SUN-RGBD [370] offers 10,335 RGB-D images and reconstructed point cloud, with rich annotations including room types, 2D polygons, 3D bounding boxes, camera poses, and room layouts. SceneNN [371] offers 502K RGB-D frames from 100 indoor scenes with reconstructed meshes, textured models, camera poses, and both object-oriented and axis-aligned bounding boxes. 2D-3D-S [372] includes over 70,000 panoramic images from six indoor areas, with aligned depth, surface normals, semantic labels, point clouds, meshes, global XYZ maps, and full camera metadata. Laval Indoor [373] offers 2.2K high-res indoor panoramas (77683884) with HDR lighting from various settings such as homes, offices, and factories. Matterport3D [374] contains 10,800 panoramic images from 194,400 RGB-D views in 90 buildings, with dense camera trajectories, aligned depth maps, and semantic labels. ScanNet [375] offers 2.5M RGB-D frames in 1,513 scans from 707 distinct spaces with camera poses, surface reconstructions, dense 3D semantic labels, and aligned CAD models. Replica [377] provides high-quality 3D reconstructions of 35 rooms across 18 scenes, featuring PBR textures, HDR lighting, and semantic annotations. RealEstate10K [376] contains 10 million frames from 10K YouTube videos, featuring both indoor and outdoor scenes with per-frame camera parameters. 3DSSG [378] provides scene graphs for 478 indoor rooms from 3RScan [398], with 93 object attributes, 40 relationship types, and 534 semantic classes. HM3D [379] offers 1,000 high-res 3D reconstructions of indoor spaces across residential, commercial, and civic buildings. 11 ScanNet++ [380] includes 1,000+ scenes captured with laser scanner, DSLR, and iPhone RGB-D, featuring fine-grained semantics and long-tail categories. DL3DV-10K [381] contains 51.2M frames from 10,510 video sequences across 65 indoor and semi-outdoor locations, featuring varied visual conditions such as reflections and different lighting. Synthetic indoor datasets overcome real-world limitations like limited diversity, occlusion, and costly annotation. Using designed layouts and textured 3D assets, datasets like SUNCG [382] and 3D-FRONT [385] offer large-scale, diverse scenes. Some [383], [384] leverage advanced rendering for photorealistic images with accurate 2D labels. SceneSynth [152] includes 130 indoor scenes (e.g., studies, kitchens, living rooms) with 1,723 unique models from Google 3D Warehouse. SUNCG [382] offers 45,622 manually designed scenes, featuring 404K rooms and 5.7M object instances from 2,644 meshes across 84 categories. Structured3D [383] includes 196.5K images from 3.5K professionally designed houses with detailed 3D annotations (e.g., lines, planes). Hypersim [384] provides 77.4K photorealistic renders with PBR materials and lighting for realistic view synthesis. 3D-FRONT [385] offers 6,813 professionally designed houses and 18,797 diversely furnished rooms, populated with high-quality textured 3D objects from 3D-FUTURE [399]. SG-FRONT [92] augments 3D-FRONT with scene graph annotations."
        },
        {
            "title": "4.1.2 Natural Datasets",
            "content": "Datasets for natural scenes are still limited, mainly due to the difficulties of large-scale collection and annotation in open outdoor environments. However, several notable efforts have been made to advance research in this area. Laval Outdoor [386] provides 205 high-res HDR panoramas of diverse natural and urban scenes. LHQ [387] offers 91,693 curated landscape images from Unsplash and Flickr, designed for high-quality image generation tasks. ACID [33] features 2.1M drone-captured frames from 891 YouTube videos of coastal regions, with 3D camera trajectories obtained via structure-from-motion."
        },
        {
            "title": "4.1.3 Urban Datasets",
            "content": "Urban datasets are built from real-world imagery or synthesized using game engines, providing images and annotations in 2D or 3D. Real-world datasets mainly focus on driving scenes, represented by KITTI [388], Waymo [391], and nuScenes [392], due to the significant attention autonomous driving has received over the past decade. Another major source is Googles street views and aerial views, exemplified by HoliCity [393] and GoogleEarth [98]. These datasets provide rich annotations, such as semantic segmentation and instance segmentation. TABLE 3 Summary and comparison of popular datasets for 3D scene generation. The scene types , , and represent Indoor, Nature, and Urban. The annotations , , and denote scene graph, camera pose, and motion annotations (e.g., optical flow), respectively. S2 / I2 / B2 , and indicate procedural, neural 3D-based, and S3 / I3 / B3 represent 2D/3D semantic maps, instance maps, and bounding boxes. , , image-based, and video-based generation, respectively. MeshR and PCDR are reconstructed mesh and point clouds, respectively. Note that - in #Images, 3D Model, and Annotations indicates datasets do not provide these; - in #Scenes and Area means the information cannot be inferred. 12 Dataset SUN360 [368] NYUv2 [369] Sun-RGBD [370] SceneNN [371] 2D-3D-S [372] Laval Indoor [373] Matterport3D [374] ScanNet [375] RealEstate10K [376] Replica [377] 3DSSG [378] HM3D [379] ScanNet++ [380] DL3DV-10K [381] SceneSynth [152] SUNCG [382] Structured3D [383] Hypersim [384] 3D-FRONT [385] SG-FRONT [92] Laval Outdoor [386] LHQ [387] ACID [33] KITTI [388] Cityscapes [389] SemanticKITTI [390] Waymo [391] nuScenes [392] HoliCity [393] OmniCity [394] KITTI-360 [395] GoogleEarth [98] OSM [98] CARLA [13] Virtual-KITTI-2 [396] CarlaSC [397] CityTopia [218] Year 2012 2012 2015 2016 2017 2017 2017 2017 2018 2019 2020 2021 2023 2012 2017 2020 2021 2021 2023 2017 2021 2021 2012 2016 2019 2020 2020 2020 2023 2023 2024 2024 2017 2020 2022 2025 Type Source #Images #Scenes Area 3D Model Annotations Used by URL U I I I I I I N U U U U U Real Real Real Real Real Real Real Real Real Real Real Real Real Real Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Real Real Real Real Real Real Real Real Real Real Real Real Real Synthetic Synthetic Synthetic Synthetic 67.5K 1.4K 10.3K 502K 70.5K 2.2K 10.8K 2.5M 10M - 363K - 11.1M 51.2M - - 196.5K 77.4K - - 0.2K 91.7K 2.1M 15K 25K - 1M 1.4M 6.3K 108.6K 150K 24K - 42.5K 43.2K 37.5K - 464 - 100 270 - 90 707 - 18 478 1,000 1006 - 130 45,622 3500 461 6,813 6,813 - - - 1 50 1 3 2 1 1 1 1 80 13 1 8 11 - - - 2,260 m2 6,000 m2 MeshR, PCDR - - PCDR MeshR - 0.102 km2 39,980 m2 MeshR, CAD - MeshR - 2,190 m2 - 0.365 km2 - - - 24 km2 - - 0.51 km2 0.51 km2 - - - - - - 76 km2 5 km2 20 km2 - - 25 km2 6,000 km2 - - - 36 km2 - MeshR MeshR MeshR MeshR - CAD CAD CAD CAD CAD CAD - - - LiDAR - LiDAR LiDAR LiDAR CAD - LiDAR Voxel Grid - LiDAR - Voxel Grid Voxel Grid - S2 I2 S2 B3 I3 B3 S2 S2 S2 I3 I2 S3 - I2 S3 I3 I2 S3 I3 S I3 S3 I3 - S3 I3 P - S3 I3 S2 I2 I3 S2 I2 - - - I I N I N N P N N N S I2 B3 I S2 I2 S3 B2 B3 S3 B3 S2 S2 I2 S2 I2 I3 B3 S2 I2 S3 I3 N N N S2 I2 I2 S3 I3 I S2 S2 I2 B2 B3 S3 S2 I2 I3 N KITTI [388], captured in Karlsruhe, includes stereo and optical flow pairs, 39.2 km of visual odometry, and 200K+ 3D object annotations, using Velodyne LiDAR, GPS/IMU, and stereo camera rig with grayscale and color cameras. SemanticKITTI [390] extends KITTI with dense point-wise semantics for full 360LiDAR scans. KITTI-360 [395] extends KITTI with 73.7 km of driving, 150K+ images, 1B 3D points, and dense 2D/3D labels, using setup of two 180fisheye side cameras, front stereo camera, and two LiDARs. Cityscapes [389] provides street-view videos from 50 cities, with 5K pixel-level and 20K coarse annotations for strong and weak supervision. Waymo [391] offers 1M frames from 1,150 20s scenes (6.4 hour total) with 12M 3D and 9.9M 2D boxes, collected in San Francisco, Mountain View, and Phoenix using 5 LiDARs and 5 high-res pinhole cameras. nuScenes [392] provides 1.4M images and 390K LiDAR sweeps from 1,000 20s scenes in Boston and Singapore, using 6 cameras, 1 LiDAR, 5 RADARs, GPS, and IMU, with 3D box tracking for 23 classes. HoliCity [393] aligns 6,300 high-res panoramas (133126656) with CAD models of downtown London for image-CAD fusion. OmniCity [394] provides 100K+ pixel-annotated street, satellite, and panorama images from 25K locations in New York City. GoogleEarth [98] offers 24K New York images from 400 Google Earth2 trajectories with 2D/3D semantic and instance masks plus camera parameters. OSM dataset [98], sourced from Open Street Map3, provides birds eye view semantic maps, height fields, and vector data of roads, buildings, and land use across 80+ cities worldwide. Real-world annotations are costly and viewpointlimited. Synthetic datasets like CARLA [13] and CityTopia [218], built in game engines, provide diverse street and drone views with rich 2D/3D annotations. CARLA [13] is an open-source simulator based on Unreal Engine, offering diverse urban environments, sensor simulations (camera, LiDAR, radar), and customizable driving scenarios with control over weather, lighting, traffic, and pedestrian behaviors, enabling unlimited rendering of RGB images and corresponding 2D/3D annotations. CarlaSC [397] offers 43.2K frames of semantic scenes from 24 sequences across 8 maps, captured by virtual LiDAR sensors in the CARLA simulator under varying traffic conditions. Virtual-KITTI-2 [396] replicates 5 KITTI sequences using Unity, offering photorealistic video under varying conditions with dense annotations for depth, segmentation, optical flow, and object tracking. CityTopia [218] provides 37.5K photorealistic frames with fine-grained 2D/3D annotations from 11 procedural cities in Unreal Engine, featuring varied lighting and aerial/street-view perspectives."
        },
        {
            "title": "4.2 Evaluation",
            "content": "Evaluating 3D scene generation methods is essential for comparing different methods across different domains. Various metrics have been proposed to assess key aspects of generated scenes, including geometric accuracy, structural consistency, visual realism, diversity, and physical plausibility. This section summarizes and discusses commonly used evaluation metrics in 3D scene generation, highlighting their relevance to different generation paradigms and focuses."
        },
        {
            "title": "4.2.1 Metrics-based Evaluation",
            "content": "Fidelity is evaluated by using metrics from image and video generation to assess the visual quality and realism of generated scenes, particularly for renderable outputs like NeRFs, 3D Gaussians, or image sequences. Frechet Inception Distance (FID) [400], Kernel Inception Distance (KID) [401], and Inception Score (IS) [402] are widely used to evaluate the distributional similarity between rendered images and real samples. FID and KID compute statistical distances between feature distributions extracted from pre-trained Inception network, while IS measures both image quality 2. https://earth.google.com/studio 3. https://openstreetmap.org 13 and diversity based on classification confidence. SwAVFID [403], FDD [404], and FIDCLIP [405] explore alternative feature spaces for better correlation with human evaluations. No-reference image quality metrics such as Natural Image Quality Evaluator (NIQE) [406], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [407] are used to estimate perceptual quality directly from the image statistics. CLIP-IQA [408] combines CLIP features with learned IQA models to better align with human perception under textual or semantic conditioning. For assessing photorealism specifically in 3D space, F3D [234] is 3D adaptation of FID, which is based on pre-trained autoencoder with 3D CNN architecture. In addition to perceptual scores, some metrics evaluate distributional alignment between generated and real samples. Minimum Matching Distance (MMD) [409] quantifies the average pairwise distance between closest points across distributions, Coverage (COV) [409] measures how well generated samples cover the target distribution, and 1-Nearest Neighbor Accuracy (1-NNA) [410] estimates mode collapse or overfitting by classifying samples using nearest-neighbor retrieval. Spatial Consistency metrics assess the 3D geometry and multi-view alignment of the generated scenes. For depth error, pseudo ground-truth depth maps can be obtained using state-of-the-art monocular depth estimation models, while the depth map of the scene can be obtained using reliable Structure-from-Motion (SfM) pipelines such as COLMAP [66]. For camera pose error, COLMAP is also used to estimate camera trajectories from the rendered sequences. The distances between these predicted values and groundtruth are computed using distance functions, such as L2 distance, RMSE, and Scale-Invariant Root Mean Square Error (SI-RMSE) [411]. Temporal Coherence is critical metric for evaluating the generated 3D scenes across time, particularly in dynamic scenes or video-based outputs. Flow warping error (FE) [412] measures the temporal stability of video by computing the warping error of optical flow between two frames. Frechet Video Distance (FVD) [413] builds on the principles underlying FID and introduces different feature representation that captures the temporal coherence of video, in addition to the quality of each frame. Focusing on the complex motion patterns in generated videos, Frechet Video Motion Distance (FVMD) [414] designs explicit motion features based on keypoint tracking, measuring the similarity between these features via the Frechet distance for evaluating the motion coherence of the generated videos. Controllability evaluates the ability to respond to user inputs. CLIP Score [415] leverages pre-trained CLIP model to measure the alignment between generated images and conditioning text, reflecting how faithfully the generation follows user-specified prompts. Diversity means the ability to produce varied outputs. Category distribution KL divergence (CKL) [177] compares the object category distribution in the synthesized scenes to that of the training set, with lower divergence indicating better diversity. Scene Classification Accuracy (SCA) [177] uses trained classifier to distinguish between real and generated scenes, measuring how well the distribution of synthetic scenes matches that of real scenes. Plausibility measures how well generated scenes obey physical and semantic constraints. Collision rate measures the proportion of collision objects among all generated objects within scene. Out-of-bounds object area (OBA) assesses the cumulated out-of-bounds object area in scene. like Prolific4 and Amazon Mechanical Turk (AMT)5 facilitate the recruitment of diverse participants and enable efficient scaling of user studies."
        },
        {
            "title": "5 APPLICATIONS AND TASKS",
            "content": "To promote fair, reproducible, and comprehensive evaluation of diverse 3D scene generation methods, recent research has increasingly embraced standardized benchmark suites that integrate multiple metrics, task configurations, and quality dimensions. This trend marks shift from relying only on isolated quantitative metrics to adopting more holistic, task-aligned evaluations that better reflect the complexity of real-world applications. Q-Align [416] adopts large multi-modal models (LMMs) to predict visual quality scores that align with human judgment. It covers three core dimensions: Image Quality Assessment (IQA), Image Aesthetic Assessment (IAA), and Video Quality Assessment (VQA). During inference, the mean opinion scores are collected and re-weighted to obtain the LMM-predicted score. VideoScore [417] enables video quality evaluation by training on large-scale human-feedback dataset. It provides assessment across five aspects: Visual Quality (VQ), Temporal Consistency (TC), Dynamic Degree (DD), Text-to-Video Alignment (TVA), and Factual Consistency (FC). VBench [418] and VBench++ [419] are comprehensive and versatile benchmark suites for video generation. They comprise 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). VBench-2.0 [420] further addresses more complex challenges associated with intrinsic including commonsense reasoning, physicsfaithfulness, based realism, human motion, and creative composition. WorldScore [421] unifies the evaluation of 3D, 4D, and video models on their ability to generate world following instructions. It formulates the evaluation of 3D scene generation to sequence of next-scene generation tasks guided by camera trajectories, jointly measures controllability, quality, and dynamics in various fine-grained features."
        },
        {
            "title": "4.2.3 Human Evaluation",
            "content": "User studies remain an essential component for capturing subjective qualities of 3D scene generation that are difficult to quantify through automated metrics, such as visual appeal, realism, and perceptual coherence. Participants are typically asked to rank or rate generated scenes based on multiple aspects, including photorealism, aesthetics, input alignment (e.g., text or layout), 3D consistency across views, and physical or semantic plausibility. Ideally, participants should include both domain experts (e.g., 3D artists, designers, researchers) and normal users. Their feedback offers complementary perspectives: experts may provide more critical and structured insights, while non-experts better reflect general user impressions. Although human evaluations are resource-intensive and inherently subjective, they provide essential qualitative insights that complement other evaluation methods by capturing human preferences in real-world contexts. Platforms The rapid progress in 3D scene generation has enabled diverse applications across various related domains. This section highlights key areas of 3D scene generation applications, including 3D scene editing, human-scene interaction, embodied AI, robotics, and autonomous driving. 5.1 3D Scene Editing 3D scene editing involves altering scenes appearance and structure, from individual object modifications to complete environment customization. It broadly includes texture editing, which focuses on generating stylized or realistic surface appearances, and layout editing, which involves arranging objects in physically and semantically plausible manner. Texturing and stylization aim to create aesthetic and stylish appearances based on user specifications. While recent advances achieve impressive results on scanned meshes [422], [423], [424] or synthetic indoor datasets [425], [426], [427], they are constrained by incomplete geometry from reconstructions or extensive manual modeling. To address these limitations, recent methods leverage 3D scene generation to synthesize complete and semantically consistent scenes, directly supporting texture generation tasks. Methods such as Ctrl-Room [190], ControlRoom3D [261], RoomTex [428], and DreamSpace [429] employ holistic generation techniques to create panoramic room textures, followed by detailed refinement. Beyond direct generation, 3D scene generation also benefits the evaluation of texturing methods. InstanceTex [430] generates texture across both existing datasets and new scenes generated by EchoScene [208], improving the diversity and robustness of benchmark evaluations. 3D scene layout editing focuses on arranging objects within scene to produce semantically meaningful and physically plausible configurations. Several methods, such as LEGO-Net [431], CabiNet [432], and DeBaRA [185], address the rearrangement of existing scenes. These approaches use object-level attributes, such as class labels, positions, and orientations, to produce more organized and regular arrangements. Some methods support more interactive and dynamic layout editing. For example, SceneExpander [433] and SceneDirector [434] enable real-time editing through intuitive user interactions, such as modifying room shapes or moving objects, and automatically update surrounding objects to maintain spatial coherence. Recent advances in compositional generative NeRF further push the boundary of layout control to enable editing of implicit representation. DisCoScene [220], Neural Assets [435], and Lift3D [436] enable object-level editing by adjusting control signals such as spatial locations or latent features, allowing for flexible and controllable scene manipulation. 4. https://www.prolific.com 5. https://www.mturk.com"
        },
        {
            "title": "5.2 Human-Scene Interaction",
            "content": "Human-Scene Interaction (HSI) focuses on modeling how humans interact with and influence their environment. Realistic character animation and behavior modeling require synthesizing believable interactions between virtual characters and their environments. Recent advances in HSI have made notable progress in generating realistic and physically plausible human motions within 3D environments [437], [438], [439], as well as creating scenes that align with specific motion sequences [87], [197], [198]. To generate human motion conditioned on scene environments, some approaches [437], [440], [441], [442] directly learn from datasets containing scanned indoor scenes and captured human motion [443], [444], [445]. However, these datasets are often limited in scalability and restricted to static scenes, prohibiting the modeling of dynamic humanobject interactions. Some other works [438], [439], [446], [447], [448] employ simulated environments with reinforcement learning to generate physically plausible motion. Yet, due to high setup costs, these simulations often rely on simplified scenes, introducing sim-to-real gap between synthetic training and real-world applications where environments are more complex and diverse. Recent efforts like GenZI [449] have initially addressed this issue by lifting the generated human in 2D images into 3D, enabling zero-shot generalization to novel scenes. Although GenZI still depends on pre-designed synthetic scenes for evaluation, it highlights the potential of combining scene generation with motion generation to scale HSI data more effectively. Integrating high-quality 3D scene generation is essential for advancing scalable and realistic HSI research, particularly by jointly considering human affordances, motion feasibility, and scene semantics."
        },
        {
            "title": "5.3 Embodied AI",
            "content": "In embodied AI, agents interact with environments to develop high-level semantic understanding and goal-directed behaviors. 3D scene generation supports this by providing visually and functionally rich environments that enable tasks like navigation, exploration, and instruction following, with an emphasis on cognitive reasoning over precise physical control. Simulated environments are typically built from reconstructed real-world data [379], [450] or manually designed scenes [451], [452], but both approaches have limitations: real-world datasets suffer from quality and annotation issues, while manual creation is labor-intensive and difficult to scale. In this context, 3D scene generation offers scalable, diverse, and physically plausible alternative for creating simulated environments for embodied AI research. For indoor environments, ProcTHOR [15] uses procedural generation to produce scenes that follow realistic layouts and physical constraints. Holodeck [175] leverages LLM to generate 3D environments that match user-supplied prompts automatically. InfiniteWorld [453] further expands assets with different textures for more diverse and stylish scenes. PhyScene [186] integrates physics and interactivity constraints into conditional diffusion model to synthesize plausibly interactive environments. Architect [454] employs iterative image-based inpainting to populate scenes with 15 large furniture and small objects, enriching scene complexity. Beyond indoor settings, procedural methods have also enabled city-scale simulation. MetaUrban [17], GRUtopia [16], and URBAN-SIM [455] construct diverse, largescale urban environments for embodied agents. EmbodiedCity [456] provides high-quality 3D real environment based on real city, supporting various agents, continuous decision-making, and systematic benchmark tasks for embodied intelligence."
        },
        {
            "title": "5.4 Robotics",
            "content": "In robotics, 3D scene generation enables learning of lowlevel skills like manipulation and control within physically realistic environments. These scenes are typically embedded in simulators, where accurate modeling of dynamics and contact is crucial for training robots to perceive, plan, and act effectively in the real world. Simulated environments have become central tool for developing robotic capabilities across various tasks, including complex manipulation and locomotion. However, recent approaches in robot learning [457], [458], [459], [460], [461], [462] require tremendous human effort to construct these environments and the corresponding demonstrations, restricting the scalability of robot learning even in simulated worlds. RoboGen [463] and RoboVerse [464] automate task, scene, and supervision generation through proposegenerate-learn cycle, where agents propose skills, generate environments with plausible object layouts, and learn with minimal human input. Eurekaverse [465] further scales skill learning by using LLMs to progressively generate diverse and increasingly challenging terrains, forming an adaptive curriculum for parkour training. Beyond explicitly constructing simulated environments, 3D scene generation also serves as world models for predicting future frames that visually represent intended actions, enabling robots to simulate and predict complex manipulation tasks in virtual settings. Robotics-focused video generation models [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476] aim to synthesize videos conditioned on inputs like text or images, specifically to help robots visualize and plan complex manipulation tasks by predicting future action sequences in physically plausible way. Instead of directly generating video frames, some methods [477], [478], [479] leverage NeRFs and dynamic 3D Gaussians to capture the spatial and semantic complexity of real-world environments, enabling more accurate motion estimation and planning."
        },
        {
            "title": "5.5 Autonomous Driving",
            "content": "in au3D scene generation is increasingly important tonomous driving, offering controllable, scalable, and diverse simulations of real-world environments. These capabilities help overcome limitations of real-world datasets and environments. It supports key components of self-driving systems, such as predictive modeling and data generation. Several 3D scene generation methods serve as world models for autonomous driving, enabling future scene prediction, risk anticipation, and the planning of safer, more efficient actions. Some [39], [118], [337], [338], [339], [355], [364], [366] focus on predicting future video frames, while others [480], [481], [482], [483], [484] generate 3D occupancies to model the environment explicitly. With high-fidelity generation, DriveArena [351] and DrivingSphere [359] introduce closed-loop simulators for training and evaluating autonomous driving agents, enabling agents to learn and evolve in closed-loop manner continuously. Autonomous driving demands large, diverse datasets, but real-world collections like nuScenes [392], KITTI [388], and Waymo [391] are costly and rarely capture critical corner cases. Controllable video-based generation methods [341], [343], [344], [345], [353] address this by synthesizing diverse driving scenarios with flexible control over weather, lighting, and traffic conditions, especially for rare and safety-critical events."
        },
        {
            "title": "6 CHALLENGES AND FUTURE DIRECTIONS\n6.1 Challenges",
            "content": "Despite recent advancements, 3D scene generation still has significant potential for improvement. Generative Capacity. Existing generative models exhibit trade-off in jointly satisfying photorealism, 3D consistency, and controllability. Procedural and neural 3D-based approaches excel at generating geometrically coherent scenes with controllable spatial layouts, but often fall short in producing photorealistic textures and lighting. In contrast, imageand video-based generation models achieve high visual realism, yet struggle to maintain 3D consistency, resulting in artifacts such as distorted geometry, unrealistic object interactions, or implausible physical dynamics. As result, current models still find it challenging to synthesize complex, multi-object scenes that are both visually plausible and physically grounded. 3D Representation. The evolution of 3D scene representations has progressed from geometry-centric formats such as voxel grids and point clouds, both of which struggle to capture photorealistic appearance, to NeRFs, which improve visual quality but remain inefficient and lack explicit geometry. Recent advances like 3D Gaussians offer better efficiency but still lack geometric grounding, limiting their applicability to tasks like relighting or physical interaction. Meshand Bezier-triangle-based methods [485], [486], [487] partially address these limitations by introducing explicit surface representations, yet they are largely confined to object-level generation. Scene-level representations that are compact, physically meaningful, and visually realistic remain an open challenge, hindering progress in controllable and generalizable 3D scene generation. Data and Annotations. The progress of 3D scene generation is tightly bound to dataset quality. Synthetic datasets offer precise annotations but suffer from limited content diversity and suboptimal photorealism due to rendering constraints in current game engines. In contrast, real-world scans provide visually realistic imagery but often lack sufficient annotations. While imageand video-based generative methods alleviate annotation needs, they still struggle to capture accurate 3D geometry, often resulting in spatial distortions. Additionally, existing datasets rarely include rich metadata, such as physical affordances, material attributes, or interaction cues, hindering broader applications in robotics, embodied AI, and physical simulation. 16 Evaluation. persistent challenge in 3D scene generation is the lack of unified evaluation protocols. Methods often rely on disparate metrics, hindering consistent comparison. Benchmark-based efforts [420], [421] have partially addressed this by introducing standardized and humanaligned evaluation frameworks. However, current benchmarks are largely conditioned on text or images, with limited support for other inputs such as layouts, actions, or trajectories. Moreover, evaluations still primarily focus on image and video fidelity, offering an insufficient assessment of underlying 3D geometry and physical plausibility. Recent work like Eval3D [488] introduces benchmark that begins to address 3D structural, semantic, and geometric consistency, though it remains limited to object-level generation and lacks scene-level complexity."
        },
        {
            "title": "6.2 Future Directions",
            "content": "Given the substantial progress made and the key challenges outlined above, we believe that future research in 3D scene generation can advance in the following directions. Better Fidelity. High-fidelity 3D scene generation demands coherence in geometry, texture, lighting, and multi-view consistency. While current methods often trade off between geometric accuracy or visual richness, future models should focus on bridging this gap that jointly reason about structure and appearance. Key goals include improved material and lighting modeling, consistent object identity across views, and capturing subtle cues like shadows and occlusions. Achieving scene-level fidelity also means aligning local details with global spatial and semantic coherence, enabling more realistic and useful 3D environments. Physical-aware Generation. Despite impressive visual progress, current methods often overlook the physical plausibility of generated scenes. To ensure that object placements and articulations conform to physical laws, future work should incorporate physics priors, constraints, or simulations into the generation process. Emerging approaches that integrate physics-based feedback, such as differentiable simulators [489], offer promising path toward jointly optimizing structure, semantics, and physical behavior. These capabilities are especially important for embodied AI and robotics, where agents depend on physically consistent environments for effective planning and control. Interactive Scene Generation. Recent advances in 4D scene generation have enabled the creation of dynamic environments with movable objects. However, these scenes remain largely non-interactive, where objects do not respond to user inputs or environmental changes. As result, current generative models produce passive rather than reactive experiences. key future direction is interactive scene generation, where scenes contain interactive objects that can respond meaningfully to physical interactions, user commands, or contextual variations. Achieving this will require models to go beyond geometry and motion, incorporating reasoning about object affordances, causal relationships, and multiagent dynamics. Unified Perception-Generation. promising frontier lies in unifying perception and generation under shared model. Tasks such as segmentation, reconstruction, and scene synthesis benefit from common spatial and semantic priors. Moreover, generation tasks inherently require an understanding of the input modalities. unified architecture could leverage bidirectional capabilities: enhancing generative performance via perceptual grounding and improving scene understanding through generative modeling. Such models could serve as general-purpose backbones for embodied agents, supporting joint reasoning across vision, language, and 3D spatial representations."
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. Mendiburu, 3D movie making: stereoscopic digital cinema from script to screen. Routledge, 2012. [6] [3] [4] [5] [2] N. Anantrasirichai and D. Bull, Artificial intelligence in the creative industries: review, Artificial Intelligence Review, vol. 55, no. 1, pp. 589656, 2022. Y. I. H. Parish and P. uller, Procedural modeling of cities, in SIGGRAPH, 2001. G. N. Yannakakis and J. Togelius, Artificial intelligence and games. Springer, 2018, vol. 2. T. Short and T. Adams, Procedural generation in game design. CRC Press, 2017. P. uller, P. Wonka, S. Haegler, A. Ulmer, and L. V. Gool, Procedural modeling of buildings, ACM TOG, vol. 25, no. 3, pp. 614623, 2006. K. Chang, C. Cheng, J. Luo, S. Murata, M. Nourbakhsh, and Y. Tsuji, Building-GAN: Graph-conditioned architectural volumetric design generation, in ICCV, 2021. S. M. LaValle, Virtual reality. Cambridge university press, 2023. L. Lee, T. Braud, P. Y. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar, C. Bermejo, and P. Hui, All one needs to know about metaverse: complete survey on technological singularity, virtual ecosystem, and research agenda, Foundations and Trends in HumanComputer Interaction, vol. 18, no. 2-3, pp. 100337, 2024. [8] [9] [7] [10] M. M. Soliman, E. Ahmed, A. Darwish, and A. E. Hassanien, Artificial intelligence powered metaverse: analysis, challenges and future perspectives, Artificial Intelligence Review, vol. 57, no. 2, p. 36, 2024. [11] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. MartınMartın, C. Wang, G. Levine, W. Ai, B. J. Martinez, H. Yin, M. Lingelbach, M. Hwang, A. Hiranaka, S. Garlanka, A. Aydin, S. Lee, J. Sun, M. Anvari, M. Sharma, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, Y. Li, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. FeiFei, BEHAVIOR-1K: human-centered, embodied AI benchmark with 1, 000 everyday activities and realistic simulation, arXiv 2403.09227, 2024. [12] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky, π0: vision-language-action flow model for general robot control, arXiv 2410.24164, 2024. [13] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. opez, and V. Koltun, CARLA: an open urban driving simulator, in CoRL, vol. 78, 2017, pp. 116. [14] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, Endto-end autonomous driving: Challenges and frontiers, IEEE TPAMI, vol. 46, no. 12, pp. 10 16410 183, 2024. [15] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador, W. Han, E. Kolve, A. Kembhavi, and R. Mottaghi, ProcTHOR: Large-scale embodied AI using procedural generation, in NeurIPS, 2022. [16] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang, S. Zhao, Y. Chen, S. Yang, P. Cao, W. Yu, Z. Ye, J. Li, J. Long, Z. Wang, H. Wang, Y. Zhao, Z. Tu, Y. Qiao, D. Lin, and J. Pang, GRUtopia: Dream general robots in city at scale, arXiv 2407.10943, 2024. 17 [18] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang, Y. You, Z. Zhang, D. Zhao, L. Xiao, J. Zhao, J. Lu, and G. Huang, Is sora world simulator? comprehensive survey on general world models and beyond, arXiv 2405.03520, 2024. [19] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., Cosmos world foundation model platform for physical AI, arXiv 2501.03575, 2025. [20] D. Liu, J. Zhang, A.-D. Dinh, E. Park, S. Zhang, and C. Xu, Generative physical AI in vision: survey, arXiv 2501.10928, 2025. [21] H. Jiang, D. Yan, X. Zhang, and P. Wonka, Selection expressions for procedural modeling, IEEE TVCG, vol. 26, no. 4, pp. 1775 1788, 2020. [22] W. Zhao, Y. Cao, J. Xu, Y. Dong, and Y. Shan, DI-PCG: diffusionbased efficient inverse procedural content generation for highquality 3D asset creation, arXiv 2412.15200, 2024. [23] M. Hendrikx, S. A. Meijer, J. V. D. Velden, and A. Iosup, Procedural content generation for games: survey, ACM Transactions on Multimedia Computing, Communications, and Applications, vol. 9, no. 1, pp. 1:11:22, 2013. S. Zhang, M. Zhou, Y. Wang, C. Luo, R. Wang, Y. Li, X. Yin, Z. Zhang, and J. Peng, CityX: Controllable procedural content generation for unbounded 3D cities, arXiv 2407.17572, 2024. [24] [25] Y. Yang, J. Wang, E. Vouga, and P. Wonka, Urban pattern: layout design by hierarchical domain splitting, ACM TOG, vol. 32, no. 6, pp. 181:1181:12, 2013. J. O. Talton, Y. Lou, S. Lesser, J. Duke, R. Mech, and V. Koltun, Metropolis procedural modeling, ACM TOG, vol. 30, no. 2, pp. 11:111:14, 2011. [26] [27] W. Wu, L. Fan, L. Liu, and P. Wonka, Miqp-based layout design for building interiors, Computer Graphics Forum, 2018. [29] [28] L. Yu, S. K. Yeung, C. Tang, D. Terzopoulos, T. F. Chan, and S. J. Osher, Make it home: automatic optimization of furniture arrangement, ACM TOG, vol. 30, no. 4, p. 86, 2011. I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. C. Courville, and Y. Bengio, Generative adversarial networks, in NIPS, 2014. J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in NeurIPS, 2020. [30] [31] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, NeRF: Representing scenes as neural radiance fields for view synthesis, in ECCV, 2020. [32] B. Kerbl, G. Kopanas, T. Leimk uhler, and G. Drettakis, 3D Gaussian splatting for real-time radiance field rendering, ACM TOG, vol. 42, no. 4, pp. 139:1139:14, 2023. [33] A. Liu, A. Makadia, R. Tucker, N. Snavely, V. Jampani, and A. Kanazawa, Infinite Nature: Perpetual view generation of natural scenes from single image, in ICCV, 2021. [34] Z. Li, Q. Wang, N. Snavely, and A. Kanazawa, InfiniteNatureZero: Learning perpetual view generation of natural scenes from single images, in ECCV, 2022. [35] Z. Chen, G. Wang, and Z. Liu, Text2Light: Zero-shot text-driven HDR panorama generation, ACM TOG, vol. 41, no. 6, pp. 195:1 195:16, 2022. S. Tang, F. Zhang, J. Chen, P. Wang, and Y. Furukawa, MVDiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion, in NeurIPS, 2023. [36] [37] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach, Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv 2311.15127, 2023. [38] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, and L. Sun, Sora: review on background, technology, limitations, and opportunities of large vision models, arXiv 2402.17177, 2024. [39] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D. Yeung, and Q. Xu, MagicDrive: Street view generation with diverse 3D geometry control, in ICLR, 2024. [40] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, 4D Gaussian splatting for real-time dynamic scene rendering, in CVPR, 2024. [17] W. Wu, H. He, Y. Wang, C. Duan, J. He, Z. Liu, Q. Li, and B. Zhou, MetaUrban: simulation platform for embodied AI in urban spaces, in ICLR, 2025. [41] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, Deformable 3D Gaussians for high-fidelity monocular dynamic scene reconstruction, in CVPR, 2024. [42] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, and Y. Wang, DimensionX: Create any 3D and 4D scenes from single image with controllable video diffusion, arXiv 2411.04928, 2024. [43] R. Li, P. Pan, B. Yang, D. Xu, S. Zhou, X. Zhang, Z. Li, A. Kadambi, Z. Wang, and Z. Fan, 4K4DGen: Panoramic 4D generation at 4K resolution, in ICLR, 2025. [44] R. M. Smelik, T. Tutenel, R. Bidarra, and B. Benes, survey on procedural modelling for virtual worlds, Computer Graphics Forum, vol. 33, no. 6, pp. 3150, 2014. [45] E. Cogo, E. Krupalija, I. Prazina, S. Becirovic, V. Okanovic, S. Rizvic, and R. T. Mulahasanovic, survey of procedural modelling methods for layout generation of virtual scenes, Computer Graphics Forum, vol. 43, no. 1, 2024. S. Zhang, S. Zhang, Y. Liang, and P. Hall, survey of 3D indoor scene synthesis, Journal of Computer Science and Technology, vol. 34, no. 3, pp. 594608, 2019. [46] [47] A. G. Patil, S. G. Patil, M. Li, M. Fisher, M. Savva, and H. Zhang, Advances in data-driven analysis and synthesis of 3D indoor scenes, Computer Graphics Forum, vol. 43, no. 1, 2024. [48] D. V. Ayyildiz, A. J. Alnaser, S. Taj, M. Zakaria, and L. G. Jaimes, survey of learning techniques for virtual scene generation, SAE International Journal of Connected and Automated Vehicles, 2024. [49] H. Wang, X. Xiang, W. Xia, and J. Xue, survey on text-driven 360-degree panorama generation, arXiv 2502.14799, 2025. [50] M. A. Ghorab and A. Lakhfif, Text to 3D, 2D scene generation systems, frameworks and approaches: survey, in Pattern Analysis and Intelligent Systems, 2022. [51] Z. Shi, S. Peng, Y. Xu, Y. Liao, and Y. Shen, Deep generative models on 3D representations: survey, arXiv 2210.15663, 2022. [52] C. Li, C. Zhang, A. Waghwase, L. Lee, F. Rameau, Y. Yang, S. Bae, and C. S. Hong, Generative AI meets 3D: survey on text-to-3D in AIGC era, arXiv 2305.06131, 2023. [53] X. Li, Q. Zhang, D. Kang, W. Cheng, Y. Gao, J. Zhang, Z. Liang, J. Liao, Y. Cao, and Y. Shan, Advances in 3D generation: survey, arXiv 2401.17807, 2024. J. Liu, X. Huang, T. Huang, L. Chen, Y. Hou, S. Tang, Z. Liu, W. Ouyang, W. Zuo, J. Jiang, and X. Liu, comprehensive survey on 3D content generation, arXiv 2402.01166, 2024. [55] Z. Wang, D. Li, and R. Jiang, Diffusion models in 3D vision: [54] survey, arXiv 2410.04738, 2024. [57] [56] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, and Y. Luo, Advances in 4D generation: survey, arXiv 2503.14501, 2025. J. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik, F. Xu, and Y. Li, Understanding world or predicting future? comprehensive survey of world models, arXiv 2411.14499, 2024. [58] T. Feng, W. Wang, and Y. Yang, survey of world models for autonomous driving, arXiv 2501.11260, 2025. [59] X. Han, H. Laga, and M. Bennamoun, Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era, IEEE TPAMI, vol. 43, no. 5, pp. 15781604, 2021. [60] Z. Huang, Y. Wen, Z. Wang, J. Ren, and K. Jia, Surface reconstruction from point clouds: survey and benchmark, IEEE TPAMI, vol. 46, no. 12, pp. 97279748, 2024. [61] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, and Y. Jiang, survey on video diffusion models, ACM Computing Surveys, vol. 57, no. 2, pp. 41:141:42, 2025. [62] U. Singer, S. Sheynin, A. Polyak, O. Ashual, I. Makarov, F. Kokkinos, N. Goyal, A. Vedaldi, D. Parikh, J. Johnson, and Y. Taigman, Text-to-4D dynamic scene generation, in ICML, 2023. [63] D. Xu, H. Liang, N. P. Bhatt, H. Hu, H. Liang, K. N. Plataniotis, and Z. Wang, Comp4D: Llm-guided compositional 4D scene generation, arXiv 2403.16993, 2024. [64] Y. Zheng, X. Li, K. Nagano, S. Liu, O. Hilliges, and S. D. Mello, unified approach for text-and image-guided 4D scene generation, in CVPR, 2024. [67] [66] [65] B. Curless and M. Levoy, volumetric method for building complex models from range images, in SIGGRAPH, 1996. J. L. Sch onberger and J. Frahm, Structure-from-motion revisited, in CVPR, 2016. J. J. Park, P. R. Florence, J. Straub, R. A. Newcombe, and S. Lovegrove, DeepSDF: Learning continuous signed distance functions for shape representation, in CVPR, 2019. J. C. Hart, Sphere tracing: geometric method for the antialiased ray tracing of implicit surfaces, The Visual Computer, vol. 12, no. 10, pp. 527545, 1996. [68] 18 [69] J. T. Kajiya and B. V. Herzen, Ray tracing volume densities, in SIGGRAPH, 1984. [70] N. L. Max, Optical models for direct volume rendering, IEEE TVCG, vol. 1, no. 2, pp. 99108, 1995. [71] D. P. Kingma and M. Welling, Auto-encoding variational bayes, in ICLR, 2014. [72] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in NeurIPS, 2017. [73] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. W. Battaglia, Learning deep generative models of graphs, arXiv 1803.03324, 2018. [74] D. J. Rezende and F. Viola, Taming VAEs, arXiv 1810.00597, [75] [76] 2018. J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi, Dont Blame the ELBO! linear VAE perspective on posterior collapse, in NeurIPS, 2019. I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved training of wasserstein gans, in NIPS, 2017. [77] P. Dhariwal and A. Q. Nichol, Diffusion models beat gans on image synthesis, in NeurIPS, 2021. [78] F. K. Musgrave, C. E. Kolb, and R. S. Mace, The synthesis and rendering of eroded fractal terrains, in SIGGRAPH, 1989. [79] G. Cordonnier, E. Galin, J. Gain, B. Benes, E. Guerin, A. Peytavie, and M. Cani, Authoring landscapes by combining ecosystem and terrain erosion simulation, ACM TOG, vol. 36, no. 4, pp. 134:1134:12, 2017. [80] A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, and J. Deng, Infinite photorealistic worlds using procedural generation, in CVPR, 2023. [81] A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, and J. Deng, Infinigen Indoors: Photorealistic indoor scenes using procedural generation, in CVPR, 2024. [82] W. Feng, W. Zhu, T. Fu, V. Jampani, A. R. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, LayoutGPT: Compositional visual planning and generation with large language models, in NeurIPS, 2023. [83] C. Sun, J. Han, W. Deng, X. Wang, Z. Qin, and S. Gould, 3DGPT: Procedural 3D modeling with large language models, in 3DV, 2025. [84] M. Zhou, J. Hou, C. Luo, Y. Wang, Z. Zhang, and J. Peng, SceneX: procedural controllable large-scale scene generation via largelanguage models, in AAAI, 2025. [85] K. Wang, M. Savva, A. X. Chang, and D. Ritchie, Deep convolutional priors for indoor scene synthesis, ACM TOG, vol. 37, no. 4, p. 70, 2018. [86] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, and S. Fidler, ATISS: autoregressive transformers for indoor scene synthesis, in NeurIPS, 2021. [87] H. Yi, C. P. Huang, S. Tripathi, L. Hering, J. Thies, and M. J. Black, MIME: human-aware 3D scene generation, in CVPR, 2023. J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, and M. Nießner, DiffuScene: Denoising diffusion models for generative indoor scene synthesis, in CVPR, 2024. [88] [89] K. Wang, Y. Lin, B. Weissmann, M. Savva, A. X. Chang, and D. Ritchie, PlanIT: planning and instantiating indoor scenes with relation graph and spatial prior networks, ACM TOG, vol. 38, no. 4, pp. 132:1132:15, 2019. [90] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. R. Zhang, GRAINS: generative recursive autoencoders for indoor scenes, ACM TOG, vol. 38, no. 2, pp. 12:112:16, 2019. [91] H. Dhamo, F. Manhardt, N. Navab, and F. Tombari, Graph-to3D: End-to-end generation and manipulation of 3D scenes using scene graphs, in ICCV, 2021. [92] G. Zhai, E. P. Ornek, S. Wu, Y. Di, F. Tombari, N. Navab, and B. Busam, CommonScenes: Generating commonsense 3D indoor scenes with scene graph diffusion, in NeurIPS, 2023. [93] C. Lin and Y. Mu, InstructScene: Instruction-driven 3D indoor scene synthesis with semantic graph prior, in ICLR, 2024. [94] Z. Hao, A. Mallya, S. J. Belongie, and M. Liu, GANcraft: Unsupervised 3D neural rendering of minecraft worlds, in ICCV, 2021. [95] S. Bahmani, J. J. Park, D. Paschalidou, X. Yan, G. Wetzstein, L. J. Guibas, and A. Tagliasacchi, CC3D: layout-conditioned generation of compositional 3D scenes, in ICCV, 2023. [121] B. Mandelbrot, How long is the coast of britain? statistical selfsimilarity and fractional dimension, Science, vol. 156, no. 3775, pp. 636638, 1967. 19 [96] C. H. Lin, H. Lee, W. Menapace, M. Chai, A. Siarohin, M. Yang, and S. Tulyakov, InfiniCity: Infinite-scale city synthesis, in ICCV, 2023. [97] Z. Chen, G. Wang, and Z. Liu, SceneDreamer: Unbounded 3D scene generation from 2D image collections, IEEE TPAMI, vol. 45, no. 12, pp. 15 56215 576, 2023. [98] H. Xie, Z. Chen, F. Hong, and Z. Liu, CityDreamer: Compositional generative model of unbounded 3D cities, in CVPR, 2024. [99] R. Po and G. Wetzstein, Compositional 3D scene generation using locally conditioned diffusion, in 3DV, 2024. [100] Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, and P. Ji, BlockFusion: Expandable 3D scene generation using latent tri-plane extrapolation, ACM TOG, vol. 43, no. 4, pp. 43:143:17, 2024. [101] T. DeVries, M. A. Bautista, N. Srivastava, G. W. Taylor, and J. M. Susskind, Unconstrained scene generation with locally conditioned radiance fields, in ICCV, 2021. [102] M. A. Bautista, P. Guo, S. Abnar, W. Talbott, A. Toshev, Z. Chen, L. Dinh, S. Zhai, H. Goh, D. Ulbricht, A. Dehghan, and J. M. Susskind, GAUDI: neural architect for immersive 3D scene generation, in NeurIPS, 2022. [103] S. W. Kim, B. Brown, K. Yin, K. Kreis, K. Schwarz, D. Li, R. Rombach, A. Torralba, and S. Fidler, NeuralField-LDM: Scene generation with hierarchical latent diffusion models, in CVPR, 2023. [104] X. Ren, J. Huang, X. Zeng, K. Museth, S. Fidler, and F. Williams, XCube: Large-scale 3D generative modeling using sparse voxel hierarchies, in CVPR, 2024. [105] X. Li, Z. Lai, L. Xu, Y. Qu, L. Cao, S. Zhang, B. Dai, and R. Ji, Director3D: Real-world camera trajectory and 3D scene generation from text, in NeurIPS, 2024. [106] M. R. K. Dastjerdi, Y. Hold-Geoffroy, J. Eisenmann, S. Khodadadeh, and J. Lalonde, Guided co-modulated GAN for 360 field of view extrapolation, in 3DV, 2022. [107] C. Zhang, Q. Wu, C. C. Gambardella, X. Huang, D. Phung, W. Ouyang, and J. Cai, Taming stable diffusion for text to 360 panorama image generation, in CVPR, 2024. [108] G. Wang, P. Wang, Z. Chen, W. Wang, C. C. Loy, and Z. Liu, PERF: panoramic neural radiance field from single panorama, IEEE TPAMI, vol. 46, no. 10, pp. 69056918, 2024. [109] S. Yang, J. Tan, M. Zhang, T. Wu, Y. Li, G. Wetzstein, Z. Liu, and D. Lin, LayerPano3D: Layered 3D panorama for hyperimmersive scene generation, in SIGGRAPH, 2025. [110] C. Rockwell, D. F. Fouhey, and J. Johnson, PixelSynth: Generating 3D-consistent experience from single image, in ICCV, 2021. [111] R. Rombach, P. Esser, and B. Ommer, Geometry-free view synthesis: Transformers and no 3D priors, in ICCV, 2021. [112] X. Li, Z. Cao, H. Sun, J. Zhang, K. Xian, and G. Lin, 3D cinemagraphy from single image, in CVPR, 2023. [113] L. ollein, A. Cao, A. Owens, J. Johnson, and M. Nießner, Text2Room: Extracting textured 3D meshes from 2D text-toimage models, in ICCV, 2023. [114] J. Zhang, X. Li, Z. Wan, C. Y. Wang, and J. Liao, Text2NeRF: Textdriven 3D scene generation with neural radiance fields, IEEE TVCG, 2024. [115] H. Yu, H. Duan, J. Hur, K. Sargent, M. Rubinstein, W. T. Freeman, F. Cole, D. Sun, N. Snavely, J. Wu, and C. Herrmann, WonderJourney: Going from anywhere to everywhere, in CVPR, 2024. [116] J. Chung, S. Lee, H. Nam, J. Lee, and K. M. Lee, LucidDreamer: Domain-free generation of 3D Gaussian splatting scenes, arXiv 2311.13384, 2023. [117] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. A. Jeni, S. Tulyakov, and H. Lee, 4Real: Towards photorealistic 4D scene generation via video diffusion models, in NeurIPS, 2024. [118] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li, Vista: generalizable driving world model with high fidelity and versatile controllability, in NeurIPS, 2024. [119] Y. Zhao, C. Lin, K. Lin, Z. Yan, L. Li, Z. Yang, J. Wang, G. H. Lee, and L. Wang, GenXD: generating any 3D and 4D scenes, ICLR, 2025. [120] H. Che, X. He, Q. Liu, C. Jin, and H. Chen, GameGen-X: Interactive open-world game video generation, in ICLR, 2025. [122] M. B. B, The fractal geometry of nature, New York, 1983. [123] A. Fournier, D. S. Fussell, and L. C. Carpenter, Computer rendering of stochastic models, Commun. ACM, vol. 25, no. 6, pp. 371384, 1982. [124] P. Przemyslaw and H. Mark, fractal model of mountains and rivers, in Graphics Interface, 1993. [125] F. Belhadj and P. Audibert, Modeling landscapes with ridges and rivers: bottom up approach, in GRAPHITE, 2005. [126] M. B. and V. N. J. W, Fractional brownian motions, fractional noises and applications, SIAM review, vol. 10, no. 4, pp. 422437, 1968. [127] L. Aristid, Mathematical models for cellular interactions in development i. filaments with one-sided inputs, Journal of theoretical biology, vol. 18, no. 3, pp. 280299, 1968. [128] G. Stiny and J. Gips, Shape grammars and the generative specification of painting and sculpture, in Information Processing, 1971. [129] A. D. Kelley, M. C. Malin, and G. M. Nielson, Terrain simulation using model of stream erosion, in SIGGRAPH, 1988. [130] G. Cordonnier, G. Jouvet, A. Peytavie, J. Braun, M. Cani, B. Benes, E. Galin, E. Guerin, and J. Gain, Forming terrains by glacial erosion, ACM TOG, vol. 42, no. 4, pp. 61:161:14, 2023. [131] J. Genevaux, E. Galin, E. Guerin, A. Peytavie, and B. Benes, Terrain generation using procedural models based on hydrology, ACM TOG, vol. 32, no. 4, pp. 143:1143:13, 2013. [132] H. Schott, A. Paris, L. Fournier, E. Guerin, and E. Galin, Largescale terrain authoring through interactive erosion simulation, ACM TOG, vol. 42, no. 5, pp. 162:1162:15, 2023. [133] A. Paris, E. Guerin, P. Collon, and E. Galin, Authoring and simulating meandering rivers, ACM TOG, vol. 42, no. 6, pp. 239:1239:14, 2023. [134] O. Deussen, P. Hanrahan, B. Lintermann, R. Mech, M. Pharr, and P. Prusinkiewicz, Realistic modeling and rendering of plant ecosystems, in SIGGRAPH, 1998. [135] M. Makowski, T. Hadrich, J. Scheffczyk, D. L. Michels, S. Pirk, and W. Palubicki, Synthetic silviculture: multi-scale modeling of plant ecosystems, ACM TOG, vol. 38, no. 4, pp. 131:1131:14, 2019. [136] W. Palubicki, M. Makowski, W. Gajda, T. Hadrich, D. L. Michels, and S. Pirk, Ecoclimates: climate-response modeling of vegetation, ACM TOG, vol. 41, no. 4, pp. 155:1155:19, 2022. [137] B. Benes, M. Abdul-Massih, P. Jarvis, D. G. Aliaga, and C. A. Vanegas, Urban ecosystem design, in Symposium on Interactive 3D Graphics and Games, I3D, 2011. [138] C. A. Vanegas, D. G. Aliaga, B. Benes, and P. Waddell, Interactive design of urban spaces using geometrical and behavioral modeling, ACM TOG, vol. 28, no. 5, p. 111, 2009. [139] B. Weber, P. uller, P. Wonka, and M. H. Gross, Interactive geometric simulation of 4D cities, Computer Graphics Forum, vol. 28, no. 2, pp. 481492, 2009. [140] P. Merrell, Example-based model synthesis, in SI3D, 2007. [141] P. Merrell and D. Manocha, Continuous model synthesis, ACM TOG, vol. 27, no. 5, p. 158, 2008. [142] H. Zhou, J. Sun, G. Turk, and J. M. Rehg, Terrain synthesis from digital elevation models, IEEE TVCG, vol. 13, no. 4, pp. 834848, 2007. [143] G. Nishida, I. Garcia-Dorado, and D. G. Aliaga, Example-driven procedural urban roads, Comput. Graph. Forum, vol. 35, no. 6, pp. 517, 2016. [144] C. A. Vanegas, I. Garcia-Dorado, D. G. Aliaga, B. Benes, and P. Waddell, Inverse design of urban procedural models, ACM TOG, vol. 31, no. 6, pp. 168:1168:11, 2012. [145] A. Emilien, U. Vimont, M. Cani, P. Poulin, and B. Benes, Worldbrush: interactive example-based synthesis of procedural virtual worlds, ACM TOG, vol. 34, no. 4, pp. 106:1106:11, 2015. [146] G. Kelly and H. McCabe, Citygen: An interactive system for procedural city generation, in Fifth International Conference on Game Design and Technology, 2007. [147] K. Xu, J. Stewart, and E. Fiume, Constraint-based automatic placement for scene composition, in Graphics Interface, 2002. [148] P. Merrell, E. Schkufza, Z. Li, M. Agrawala, and V. Koltun, Interactive furniture layout using interior design guidelines, ACM TOG, vol. 30, no. 4, p. 87, 2011. [149] P. Kan and H. Kaufmann, Automatic furniture arrangement [173] J. Liu, S. Zhang, C. Zhang, and S. Zhang, Controllable proceduusing greedy cost minimization, in VR, 2018. ral generation of landscapes, in ACM MM, 2024. [150] Y. Zhao, K. Lin, Z. Jia, Q. Gao, G. Thattai, J. Thomason, and G. S. Sukhatme, LUMINOUS: indoor scene generation for embodied AI challenges, arXiv 2111.05527, 2021. [174] Z. Hu, A. Iscen, A. Jain, T. Kipf, Y. Yue, D. A. Ross, C. Schmid, and A. Fathi, SceneCraft: An LLM agent for synthesizing 3D scenes as blender code, in ICML, 2024. 20 [151] P. Merrell, E. Schkufza, and V. Koltun, Computer-generated residential building layouts, ACM TOG, vol. 29, no. 6, p. 181, 2010. [152] M. Fisher, D. Ritchie, M. Savva, T. A. Funkhouser, and P. Hanrahan, Example-based synthesis of 3D object arrangements, ACM TOG, vol. 31, no. 6, pp. 135:1135:11, 2012. [153] L. Yu, S. K. Yeung, and D. Terzopoulos, The Clutterpalette: An interactive tool for detailing indoor scenes, IEEE TVCG, vol. 22, no. 2, pp. 11381148, 2016. [154] S. Qi, Y. Zhu, S. Huang, C. Jiang, and S. Zhu, Human-Centric indoor scene synthesis using stochastic grammar, in CVPR, 2018. [155] S. Zhang, S. Zhang, W. Xie, C. Luo, Y. Yang, and H. Fu, Fast 3D indoor scene synthesis by learning spatial relation priors of objects, IEEE TVCG, vol. 28, no. 9, pp. 30823092, 2022. [156] S. Zhang, Y. Li, Y. He, Y. Yang, and S. Zhang, MageAdd: Realtime interaction simulation for scene synthesis, in ACM MM, 2021. [157] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, in NeurIPS, 2022. [158] OpenAI, GPT-4 technical report, arXiv 2303.08774, 2023. [159] Y. Yang, J. Lu, Z. Zhao, Z. Luo, J. J. Q. Yu, V. Sanchez, and F. Zheng, LLplace: The 3D indoor scene layout generation and editing via large language model, arXiv 2406.03866, 2024. [160] R. Aguina-Kang, M. Gumin, D. H. Han, S. Morris, S. J. Yoo, A. Ganeshan, R. K. Jones, Q. A. Wei, K. Fu, and D. Ritchie, Openuniverse indoor scene generation using LLM program synthesis and uncurated object databases, arXiv 2403.09675, 2024. [161] J. Deng, W. Chai, J. Huang, Z. Zhao, Q. Huang, M. Gao, J. Guo, S. Hao, W. Hu, J. Hwang, X. Li, and G. Wang, CityCraft: real crafter for 3D city generation, arXiv /2406.04983, 2024. [162] F. Sun, W. Liu, S. Gu, D. Lim, G. Bhat, F. Tombari, M. Li, N. Haber, and J. Wu, LayoutVLM: Differentiable optimization of 3D layout via vision-language models, in CVPR, 2025. [163] R. Fu, Z. Wen, Z. Liu, and S. Sridhar, AnyHome: Openvocabulary generation of structured and textured 3D homes, in ECCV, 2024. [164] B. M. Ocal, M. Tatarchenko, S. Karaoglu, and T. Gevers, SceneTeller: Language-to-3D scene generation, in ECCV, 2024. [165] Y. Zhang, Z. Li, M. Zhou, S. Wu, and J. Wu, The Scene Language: Representing scenes with programs, words, and embeddings, in CVPR, 2025. [166] X. Zhou, X. Ran, Y. Xiong, J. He, Z. Lin, Y. Wang, D. Sun, and M. Yang, GALA3D: towards text-to-3D complex scene generation via layout-guided generative Gaussian splatting, in ICML, 2024. [167] A. elen, G. Han, K. Schindler, L. V. Gool, I. Armeni, A. Obukhov, and X. Wang, I-Design: Personalized LLM interior designer, arXiv 2404.02838, 2024. [168] W. Deng, M. Qi, and H. Ma, Global-local tree search in vlms for 3D indoor scene generation, in CVPR, 2025. [169] L. Liu, S. Chen, S. Jia, J. Shi, Z. Jiang, C. Jin, W. Zongkai, J. Hwang, and L. Li, Graph canvas for controllable 3D scene generation, arXiv 2412.00091, 2024. [170] G. Gao, W. Liu, A. Chen, A. Geiger, and B. Sch olkopf, GraphDreamer: Compositional 3D scene synthesis from scene graphs, in CVPR, 2024. [171] X. Li, H. Li, H. Chen, T. Mu, and S. Hu, DIScene: Object decoupling and interaction modeling for complex scene generation, in SIGGRAPH Asia, 2024. [172] K. Bhat, N. Khanna, K. Channa, T. Zhou, Y. Zhu, X. Sun, C. Shang, A. Sudarshan, M. Chu, D. Li, K. Deng, J. Fauconnier, T. Verhulsdonck, M. Agrawala, K. Fatahalian, A. Weiss, C. Reiser, R. K. Chirravuri, R. Kandur, A. Pelaez, A. Garg, M. Palleschi, J. Wang, S. Litz, L. Liu, A. Li, D. Harmon, D. Liu, L. Feng, D. Goupil, L. Kuczynski, J. Yoon, N. Marri, P. Zhuang, Y. Zhang, B. Yin, H. Jiang, M. van Workum, T. Lane, B. Erickson, S. Pathare, K. Price, A. Singh, and D. Baszucki, Cube: roblox view of 3D intelligence, arXiv 2503.15475, 2025. [175] Y. Yang, F. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han, J. Wu, N. Haber, R. Krishna, L. Liu, C. Callison-Burch, M. Yatskar, A. Kembhavi, and C. Clark, Holodeck: Language guided generation of 3D embodied AI environments, in CVPR, 2024. [176] X. Liu, C. Tang, and Y. Tai, WorldCraft: Photo-realistic 3D world creation and customization via LLM agents, arXiv 2502.15601, 2025. [177] D. Ritchie, K. Wang, and Y. Lin, Fast and flexible indoor scene synthesis via deep convolutional generative models, in CVPR, 2019. [178] Z. Zhang, Z. Yang, C. Ma, L. Luo, A. Huth, E. Vouga, and Q. Huang, Deep generative modeling for scene synthesis via hybrid representations, ACM TOG, vol. 39, no. 2, pp. 17:117:21, 2020. [179] H. Yang, Z. Zhang, S. Yan, H. Huang, C. Ma, Y. Zheng, C. Bajaj, and Q. Huang, Scene synthesis via uncertainty-driven attribute synchronization, in ICCV, 2021. [180] X. Wang, C. Yeshwanth, and M. Nießner, SceneFormer: Indoor scene generation with transformers, in 3DV, 2021. [181] W. R. Para, P. Guerrero, N. J. Mitra, and P. Wonka, COFS: controllable furniture layout synthesis, in SIGGRAPH, 2023. [182] Y. Nie, A. Dai, X. Han, and M. Nießner, Learning 3D scene priors with 2D supervision, in CVPR, 2023. [183] Y. Zhao, Z. Zhao, J. Li, S. Dong, and S. Gao, RoomDesigner: Encoding anchor-latents for style-consistent and shape-compatible indoor scene generation, in 3DV, 2024. [184] W. Feng, H. Zhou, J. Liao, L. Cheng, and W. Zhou, CasaGPT: cuboid arrangement and scene assembly for interior design, in CVPR, 2025. [185] L. Maillard, N. Sereyjol-Garros, T. Durand, and M. Ovsjanikov, DeBaRA: Denoising-based 3D room arrangement generation, in NeurIPS, 2024. [186] Y. Yang, B. Jia, P. Zhi, and S. Huang, PhyScene: Physically interactable 3D scene synthesis for embodied AI, in CVPR, 2024. [187] Z. Ye, X. Zheng, Y. Liu, and Y. Peng, RelScene: benchmark and baseline for spatial relations in text-driven 3D scene generation, in ACM MM, 2024. [188] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT: pretraining of deep bidirectional transformers for language understanding, in NAACL-HLT, 2019. [189] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in CVPR, 2022. [190] C. Fang, X. Hu, K. Luo, and P. Tan, Ctrl-Room: Controllable textto-3D room meshes generation with layout constraints, in 3DV, 2025. [191] A. Bokhovkin, Q. Meng, S. Tulsiani, and A. Dai, SceneFactor: Factored latent 3D diffusion for controllable 3D scene generation, arXiv 2412.01801, 2024. [192] D. Epstein, B. Poole, B. Mildenhall, A. A. Efros, and A. Holynski, Disentangled 3D scene generation with layout learning, in ICML, 2024. [193] Q. Zhang, C. Wang, A. Siarohin, P. Zhuang, Y. Xu, C. Yang, D. Lin, B. Zhou, S. Tulyakov, and H. Lee, SceneWiz3D: Towards textguided 3D scene composition, arXiv 2312.08885, 2023. [194] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L. Lee, and P. Y. Zhou, DreamScene: 3D Gaussian-based text-to-3D scene generation via formation pattern sampling, in ECCV, 2024. [195] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, DreamFusion: Text-to-3D using 2D diffusion, in ICLR, 2023. [196] Y. Nie, A. Dai, X. Han, and M. Nießner, Pose2Room: Understanding 3D scenes from human activities, in ECCV, 2022. [197] S. Ye, Y. Wang, J. Li, D. Park, C. K. Liu, H. Xu, and J. Wu, Scene synthesis from human motion, in SIGGRAPH Asia, 2022. [198] V. D. An, M. N. Vu, T. Nguyen, B. Huang, D. Nguyen, T. Vo, and A. Nguyen, Language-driven scene synthesis using multiconditional diffusion model, in NeurIPS, 2023. [199] J. Li, T. Huang, Q. Zhu, and T. Wong, Physics-based scene layout generation from human motion, in SIGGRAPH, 2024. [200] A. X. Chang, M. Savva, and C. D. Manning, Learning spatial knowledge for text to 3D scene generation, in EMNLP, 2014. [201] Z. S. Kermani, Z. Liao, P. Tan, and H. Zhang, Learning 3D scene synthesis from annotated RGB-D images, Computer Graphics Forum, vol. 35, no. 5, pp. 197206, 2016. [226] F. Lu, K. Lin, Y. Xu, H. Li, G. Chen, and C. Jiang, Urban Architect: Steerable 3D urban scene generation with layout prior, arXiv 2404.06780, 2024. 21 [202] Q. Fu, X. Chen, X. Wang, S. Wen, B. Zhou, and H. Fu, Adaptive synthesis of indoor scenes via activity-associated object relation graphs, ACM TOG, vol. 36, no. 6, pp. 201:1201:13, 2017. [203] R. Ma, A. G. Patil, M. Fisher, M. Li, S. Pirk, B. Hua, S. Yeung, X. Tong, L. J. Guibas, and H. Zhang, Language-driven synthesis of 3D scenes from scene databases, ACM TOG, vol. 37, no. 6, p. 212, 2018. [204] A. Luo, Z. Zhang, J. Wu, and J. B. Tenenbaum, End-to-end optimization of scene layout, in CVPR, 2020. [205] A. Kar, A. Prakash, M. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna, A. Torralba, and S. Fidler, Meta-sim: Learning to generate synthetic datasets, in ICCV, 2019. [206] J. Devaranjan, A. Kar, and S. Fidler, Meta-sim2: Unsupervised learning of scene structure for synthetic data generation, in ECCV, 2020. [207] L. Gao, J. Sun, K. Mo, Y. Lai, L. J. Guibas, and J. Yang, SceneHGN: Hierarchical graph networks for 3D indoor scene generation with fine-grained geometry, IEEE TPAMI, vol. 45, no. 7, pp. 89028919, 2023. [208] G. Zhai, E. P. Ornek, D. Z. Chen, R. Liao, Y. Di, N. Navab, F. Tombari, and B. Busam, EchoScene: Indoor scene generation via information echo over scene graph diffusion, in ECCV, 2024. [209] Z. Yang, K. Lu, C. Zhang, J. Qi, H. Jiang, R. Ma, S. Yin, Y. Xu, M. Xing, Z. Xiao, J. Long, X. Liu, and G. Zhai, MMGDreamer: mixed-modality graph for geometry-controllable 3D indoor scene generation, in AAAI, 2025. [210] Z. Wu, M. Feng, Y. Wang, H. Xie, W. Dong, B. Miao, and A. Mian, External knowledge enhanced 3D scene generation from sketch, in ECCV, 2024. [211] Y. Liu, X. Li, Y. Zhang, L. Qi, X. Li, W. Wang, C. Li, X. Li, and M.- H. Yang, Controllable 3D outdoor scene generation via scene graphs, in CVPR, 2025. [212] W. Dong, B. Yang, Z. Yang, Y. Li, T. Hu, H. Bao, Y. Ma, and Z. Cui, HiScene: creating hierarchical 3D scenes with isometric view generation, arXiv 2504.13072, 2025. [213] Q. Zhang, Y. Xu, Y. Shen, B. Dai, B. Zhou, and C. Yang, BerfScene: Bev-conditioned equivariant radiance fields for infinite 3D scene generation, in CVPR, 2024. [214] H. Yan, Y. Li, Z. Wu, S. Chen, W. Sun, T. Shang, W. Liu, T. Chen, X. Dai, C. Ma, H. Li, and P. Ji, Frankenstein: Generating semantic-compositional 3D scenes in one tri-plane, in SIGGRAPH Asia, 2024. [215] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D. Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, Efficient geometry-aware 3D generative adversarial networks, in CVPR, 2022. [216] C. H. Lin, H. Lee, Y. Cheng, S. Tulyakov, and M. Yang, InfinityGAN: Towards infinite-pixel image synthesis, in ICLR, 2022. [217] H. Xie, Z. Chen, F. Hong, and Z. Liu, Generative Gaussian splatting for unbounded 3D city generation, in CVPR, 2025. [218] H. Xie, Z. Chen, F. Hong, and Z. Liu, Compositional generative model of unbounded 4D cities, arXiv 2501.08983, 2025. [219] Y. Yang, Y. Yang, H. Guo, R. Xiong, Y. Wang, and Y. Liao, UrbanGIRAFFE: Representing urban scenes as compositional generative neural feature fields, in ICCV, 2023. [220] Y. Xu, M. Chai, Z. Shi, S. Peng, I. Skorokhodov, A. Siarohin, C. Yang, Y. Shen, H. Lee, B. Zhou, and S. Tulyakov, DisCoScene: Spatially disentangled generative radiance fields for controllable 3D-aware scene synthesis, in CVPR, 2023. [221] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, and L. Wang, CompoNeRF: Text-guided multi-object compositional nerf with editable 3D scene layout, arXiv 2303.13843, 2023. [222] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, and D. CohenOr, Set-the-Scene: Global-local training for generating controllable nerf scenes, in ICCV, 2023. [223] J. Zhou, X. Li, L. Qi, and M. Yang, Layout-your-3D: Controllable and precise 3D generation with 2D blueprint, arXiv 2410.15391, 2024. [227] A. R. Kosiorek, H. Strathmann, D. Zoran, P. Moreno, R. Schneider, S. Mokra, and D. J. Rezende, NeRF-VAE: geometry aware 3D scene generative model, in ICML, 2021. [228] M. Niemeyer and A. Geiger, GIRAFFE: representing scenes as compositional generative neural feature fields, in CVPR, 2021. [229] L. Chai, R. Tucker, Z. Li, P. Isola, and N. Snavely, Persistent Nature: generative model of unbounded 3D worlds, in CVPR, 2023. [230] Y. Yang, J. Shao, X. Li, Y. Shen, A. Geiger, and Y. Liao, Prometheus: 3D-aware latent diffusion models for feed-forward text-to-3D scene generation, arXiv 2412.21117, 2024. [231] H. Go, B. Park, J. Jang, J. Kim, S. Kwon, and C. Kim, SplatFlow: Multi-view rectified flow model for 3D gaussian splatting synthesis, in CVPR, 2025. [232] J. Lee, W. Im, S. Lee, and S. Yoon, Diffusion probabilistic models for scene-scale 3D categorical data, arXiv 2301.00527, 2023. [233] X. Ju, Z. Huang, Y. Li, G. Zhang, Y. Qiao, and H. Li, DiffInDScene: Diffusion-based high-quality 3D indoor scene generation, in CVPR, 2024. [234] Y. Liu, X. Li, X. Li, L. Qi, C. Li, and M. Yang, Pyramid diffusion for fine 3D large scene generation, in ECCV, 2024. [235] Q. Meng, L. Li, M. Nießner, and A. Dai, LT3SD: latent trees for 3D scene diffusion, in CVPR, 2025. [236] J. Lee, S. Lee, C. Jo, W. Im, J. Seon, and S. Yoon, SemCity: Semantic scene generation with triplane diffusion, in CVPR, 2024. [237] H. Lee, Q. Han, and A. X. Chang, NuiScene: Exploring efficient generation of unbounded outdoor scenes, arXiv 2503.16375, 2025. [238] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, and Z. Liu, DynamicCity: Large-scale 4D occupancy generation from dynamic scenes, in ICLR, 2025. [239] A. Cao and J. Johnson, HexPlane: fast representation for dynamic scenes, in CVPR, 2023. [240] N. Akimoto, S. Kasai, M. Hayashi, and Y. Aoki, 360-degree image completion by two-stage conditional gans, in ICIP, 2019. [241] J. S. Sumantri and I. K. Park, 360 panorama synthesis from sparse set of images with unknown field of view, in WACV, 2020. [242] G. Somanath and D. Kurz, HDR environment map estimation for real-time augmented reality, in CVPR, 2021. [243] T. Hara, Y. Mukuta, and T. Harada, Spherical image generation from single image by considering scene symmetry, in AAAI, 2021. [244] T. Hara, Y. Mukuta, and T. Harada, Spherical image generation from few normal-field-of-view images by considering scene symmetry, IEEE TPAMI, vol. 45, no. 5, pp. 63396353, 2023. [245] C. Oh, W. Cho, Y. Chae, D. Park, L. Wang, and K. Yoon, BIPS: bi-modal indoor panorama synthesis via residual depth-aided adversarial learning, in ECCV, 2022. [246] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, and Y. Xu, Large scale image completion via co-modulated generative adversarial networks, in ICLR, 2021. [247] P. Esser, R. Rombach, and B. Ommer, Taming transformers for high-resolution image synthesis, in CVPR, 2021. [248] N. Akimoto, Y. Matsuo, and Y. Aoki, Diverse plausible 360degree image outpainting for efficient 3DCG background creation, in CVPR, 2022. [249] H. Ai, Z. Cao, H. Lu, C. Chen, J. Ma, P. Zhou, T. Kim, P. Hui, and L. Wang, Dream360: Diverse and immersive outdoor virtual scene creation via transformer-based 360 image outpainting, IEEE TVCG, vol. 30, no. 5, pp. 27342744, 2024. [250] T. Wu, C. Zheng, and T. Cham, PanoDiffusion: 360-degree panorama outpainting via diffusion, in ICLR, 2024. [251] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in ICML, 2021. [224] X. Yang, Y. Man, J. Chen, and Y. Wang, SceneCraft: Layout- [252] Y. Lee, K. Kim, H. Kim, and M. Sung, SyncDiffusion: Coherent guided 3D scene generation, in NeurIPS, 2024. montage via synchronized joint diffusions, in NeurIPS, 2023. [225] M. Chen, L. Wang, S. Ao, Y. Zhang, K. Xu, and Y. Guo, Layout2Scene: 3D semantic layout guided scene generation via geometry and appearance diffusion priors, arXiv 2501.02519, 2025. [253] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, MultiDiffusion: Fusing diffusion paths for controlled image generation, in ICML, 2023. [254] Q. Zhang, J. Song, X. Huang, Y. Chen, and M. Liu, DiffCollage: Parallel generation of large content with diffusion models, in CVPR, 2023. [280] Y. Shen, W. Ma, and S. Wang, SGAM: building virtual 3D world through simultaneous generation and mapping, in NeurIPS, 2022. 22 [255] N. Kalischek, M. Oechsle, F. Manhardt, P. Henzler, K. Schindler, and F. Tombari, CubeDiff: Repurposing diffusion-based image models for panorama generation, in ICLR, 2025. [256] H. Wang, X. Xiang, Y. Fan, and J. Xue, Customizing 360-degree panoramas through text-to-image diffusion models, in WACV, 2024. [257] M. Feng, J. Liu, M. Cui, and X. Xie, Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models, arXiv 2311.13141, 2023. [258] J. Wang, Z. Chen, J. Ling, R. Xie, and L. Song, 360-degree panorama generation from few unregistered nfov images, in ACM MM, 2023. [259] W. Ye, C. Ji, Z. Chen, J. Gao, X. Huang, S. Zhang, W. Ouyang, T. He, C. Zhao, and G. Zhang, DiffPano: Scalable and consistent text to panorama generation with spherical epipolar-aware diffusion, in NeurIPS, 2024. [260] G. B. M. Stan, D. Wofk, S. Fox, A. Redden, W. Saxton, J. Yu, E. Aflalo, S. Tseng, F. Nonato, M. uller, and V. Lal, LDM3D: latent diffusion model for 3D, arXiv 2305.10853, 2023. [261] J. Schult, S. S. Tsai, L. ollein, B. Wu, J. Wang, C. Ma, K. Li, X. Wang, F. Wimbauer, Z. He, P. Zhang, B. Leibe, P. Vajda, and J. Hou, ControlRoom3D: Room generation using semantic proxy rooms, in CVPR, 2024. [262] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, DreamScene360: Unconstrained textto-3D scene generation with panoramic Gaussian splatting, in ECCV, 2024. [263] Y. Ma, D. Zhan, and Z. Jin, FastScene: Text-driven fast indoor 3D scene generation via panoramic Gaussian splatting, in IJCAI, 2024. [264] H. Zhou, X. Cheng, W. Yu, Y. Tian, and L. Yuan, HoloDreamer: Holistic 3D panoramic world generation from text descriptions, arXiv 2407.15187, 2024. [265] W. Li, Y. Mi, F. Cai, Z. Yang, W. Zuo, X. Wang, and X. Fan, SceneDreamer360: Text-driven 3D-consistent scene generation with panoramic Gaussian splatting, arXiv 2408.13711, 2024. [266] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys, and R. Qin, Geometry-aware satellite-to-ground image synthesis for urban areas, in CVPR, 2020. [267] Y. Shi, D. Campbell, X. Yu, and H. Li, Geometry-guided streetview panorama synthesis from satellite imagery, IEEE TPAMI, vol. 44, no. 12, pp. 10 00910 022, 2022. [268] S. Wu, H. Tang, X. Jing, H. Zhao, J. Qian, N. Sebe, and Y. Yan, Cross-view panorama image synthesis, IEEE TMM, vol. 25, pp. 35463559, 2023. [269] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys, and M. R. Oswald, Sat2vid: Street-view panoramic video synthesis from single satellite image, in ICCV, 2021. [270] M. Qian, J. Xiong, G. Xia, and N. Xue, Sat2Density: Faithful density learning from satellite-ground image pairs, in ICCV, 2023. [271] N. Xu and R. Qin, Geospecific view generation geometrycontext aware high-resolution ground view inference from satellite views, in ECCV, 2024. [272] S. Niklaus, L. Mai, J. Yang, and F. Liu, 3D ken burns effect from single image, ACM TOG, vol. 38, no. 6, pp. 184:1184:15, 2019. [273] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, SynSin: Endto-end view synthesis from single image, in CVPR, 2020. [274] J. Y. Koh, H. Agrawal, D. Batra, R. Tucker, A. Waters, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, Simple and effective synthesis of indoor 3D scenes, in AAAI, 2023. [275] R. Tucker and N. Snavely, Single-view view synthesis with multiplane images, in CVPR, 2020. [276] T. A. Habtegebrial, V. Jampani, O. Gallo, and D. Stricker, Generative view synthesis: From single-view semantics to novel-view images, in NeurIPS, 2020. [277] M. Shih, S. Su, J. Kopf, and J. Huang, 3D photography using context-aware layered depth inpainting, in CVPR, 2020. [278] R. Hu, N. Ravi, A. C. Berg, and D. Pathak, Worldsheet: Wrapping the world in 3D sheet for view synthesis from single image, in ICCV, 2021. [281] S. Cai, E. R. Chan, S. Peng, M. Shahbazi, A. Obukhov, L. V. Gool, and G. Wetzstein, DiffDreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models, in ICCV, 2023. [282] X. Ren and X. Wang, Look outside the room: Synthesizing consistent long-term 3D scene video from single image, in CVPR, 2022. [283] H. Tseng, Q. Li, C. Kim, S. Alsisan, J. Huang, and J. Kopf, Consistent view synthesis with pose-guided diffusion models, in CVPR, 2023. [284] J. J. Yu, F. Forghani, K. G. Derpanis, and M. A. Brubaker, Longterm photometric consistent novel view synthesis with diffusion models, in ICCV, 2023. [285] M. Wallingford, A. Bhattad, A. Kusupati, V. Ramanujan, M. Deitke, A. Kembhavi, R. Mottaghi, W. Ma, and A. Farhadi, From an Image to Scene: Learning to imagine the world from million 360 videos, in NeurIPS, 2024. [286] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. P. Srinivasan, J. T. Barron, and B. Poole, CAT3D: create anything in 3D with multi-view diffusion models, in NeurIPS, 2024. [287] S. Szymanowicz, J. Y. Zhang, P. P. Srinivasan, R. Gao, A. Brussee, A. Holynski, R. Martin-Brualla, J. T. Barron, and P. Henzler, Bolt3D: Generating 3D scenes in seconds, arXiv 2503.14445, 2025. [288] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in ICCV, 2023. [289] J. Li and M. Bansal, PanoGen: Text-conditioned panoramic environment generation for vision-and-language navigation, in NeurIPS, 2023. [290] Z. Lu, K. Hu, C. Wang, L. Bai, and Z. Wang, Autoregressive omni-aware outpainting for open-vocabulary 360-degree image generation, in AAAI, 2024. [291] A. Liu, Z. Li, Z. Chen, N. Li, Y. Xu, and B. A. Plummer, PanoFree: tuning-free holistic multi-view image generation with cross-view self-guidance, in ECCV, 2024. [292] P. Gao, K. Yao, T. Ye, S. Wang, Y. Yao, and X. Wang, Opa-ma: Text guided mamba for 360-degree image out-painting, arXiv 2407.10923, 2024. [293] P. Engstler, A. Vedaldi, I. Laina, and C. Rupprecht, Invisible Stitch: Generating smooth 3D scenes with depth inpainting, in 3DV, 2025. [294] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer, IEEE TPAMI, vol. 44, no. 3, pp. 16231637, 2022. [295] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. uller, Zoedepth: Zero-shot transfer by combining relative and metric depth, arXiv 2302.12288, 2023. [296] S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, and Y. Aksoy, Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging, in CVPR, 2021. [297] R. Fridman, A. Abecasis, Y. Kasten, and T. Dekel, SceneScape: Text-driven consistent scene generation, in NeurIPS, 2023. [298] X. Li, Y. Wu, J. Cen, J. Peng, K. Wang, K. Xian, Z. Wang, Z. Cao, and G. Lin, iControl3D: An interactive system for controllable 3D scene generation, in ACM MM, 2024. [299] S. Zhang, Y. Zhang, Q. Zheng, R. Ma, W. Hua, H. Bao, W. Xu, and C. Zou, 3D-SceneDreamer: Text-driven 3D-consistent scene generation, in CVPR, 2024. [300] Y. Yang, F. Yin, J. Fan, X. Chen, W. Li, and G. Yu, Scene123: One prompt to 3D scene generation via video-assisted and consistency-enhanced MAE, arXiv 2408.05477, 2024. [301] H. Ouyang, K. Heal, S. Lombardi, and T. Sun, Text2Immersion: arXiv scene with 3D Gaussians, immersive generative 2312.09242, 2023. [302] H. Yu, H. Duan, C. Herrmann, W. T. Freeman, and J. Wu, WonderWorld: Interactive 3D scene generation from single image, in CVPR, 2025. [303] J. Shriram, A. Trevithick, L. Liu, and R. Ramamoorthi, RealmDreamer: text-driven 3D scene generation with inpainting and depth diffusion, in 3DV, 2025. [279] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, Pathdreamer: world model for indoor navigation, in ICCV, 2021. [304] X. Hou, M. Li, D. Yang, J. Chen, Z. Qian, X. Zhao, Y. Jiang, J. Wei, Q. Xu, and L. Zhang, BloomScene: Lightweight structured 3D gaussian splatting for crossmodal scene generation, in AAAI, 2025. [305] C. Ni, X. Wang, Z. Zhu, W. Wang, H. Li, G. Zhao, J. Li, W. Qin, G. Huang, and W. Mei, WonderTurbo: Generating interactive 3D world in 0.72 seconds, arXiv 2504.02261, 2025. [306] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, L. Pan, D. Lin, and Z. Liu, 3DTopia-XL: scaling high-quality 3D asset generation via primitive diffusion, in CVPR, 2024. [307] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang, Structured 3D latents for scalable and versatile 3D generation, in CVPR, 2025. [308] Y. Lan, F. Hong, S. Yang, S. Zhou, X. Meng, B. Dai, X. Pan, and C. C. Loy, LN3Diff: scalable latent neural fields diffusion for speedy 3D generation, in ECCV, 2024. [309] F. Hong, J. Tang, Z. Cao, M. Shi, T. Wu, Z. Chen, T. Wang, L. Pan, D. Lin, and Z. Liu, 3DTopia: large text-to-3D generation model with hybrid diffusion priors, arXiv 2403.02234, 2024. [310] Y. Lan, S. Zhou, Z. Lyu, F. Hong, S. Yang, B. Dai, X. Pan, and C. C. Loy, GaussianAnything: interactive point cloud latent diffusion for 3D generation, in ICLR, 2025. [311] P. Engstler, A. Shtedritski, I. Laina, C. Rupprecht, and A. Vedaldi, SynCity: Training-free generation of 3D worlds, arXiv 2503.16420, 2025. [312] L. Shen, X. Li, H. Sun, J. Peng, K. Xian, Z. Cao, and G. Lin, Makeit-4D: Synthesizing consistent long-term dynamic scene video from single image, in ACM MM, 2023. [313] I.-H. Jin, H. Choo, S.-H. Jeong, H. Park, J. Kim, O. joon Kwon, and K. Kong, Optimizing 4D Gaussians for dynamic scene video from single landscape images, in ICLR, 2025. [314] Y. Lee, Y. Chen, A. Wang, T. Liao, B. Y. Feng, and J. Huang, VividDream: Generating 3D scene with ambient dynamics, ICLR, 2025. [315] H. Feng, Z. Ding, Z. Xia, S. Niklaus, V. F. Abrevaya, M. J. Black, and X. Zhang, Explorative inbetweening of time and space, in ECCV, 2024. [316] V. Gupta, Y. Man, and Y. Wang, PaintScene4D: Consistent 4D scene generation from text prompts, arXiv 2412.04471, 2024. [317] T. Liu, Z. Huang, Z. Chen, G. Wang, S. Hu, l. Shen, H. Sun, Z. Cao, W. Li, and Z. Liu, Free4D: Tuning-free 4D scene generation with spatial-temporal consistency, arXiv 2503.20785, 2025. [318] K. Liu, L. Shao, and S. Lu, Novel view extrapolation with video diffusion priors, arXiv 2411.14208, 2024. [319] W. Yu, J. Xing, L. Yuan, W. Hu, X. Li, Z. Huang, X. Gao, T. Wong, Y. Shan, and Y. Tian, ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis, arXiv 2409.02048, 2024. [320] L. Chen, Z. Zhou, M. Zhao, Y. Wang, G. Zhang, W. Huang, H. Sun, J.-R. Wen, and C. Li, FlexWorld: Progressively expanding 3D scenes for flexiable-view synthesis, arXiv 2503.13265, 2025. [321] H. Feng, Z. Zuo, J.-H. Pan, K.-H. Hui, Y. Shao, Q. Dou, W. Xie, and Z. Liu, WonderVerse: Extendable 3D scene generation with video generative models, arXiv 2503.09160, 2025. [322] S. Zhang, J. Li, X. Fei, H. Liu, and Y. Duan, Scene Splatter: Momentum 3D scene generation from single image with video diffusion model, in CVPR, 2025. [323] J. Hao, P. Wang, H. Wang, X. Zhang, and Z. Guo, GaussVideoDreamer: 3D scene generation with video diffusion and inconsistency-aware gaussian splatting, arXiv 2504.10001, 2025. [324] H. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, and J. Ren, Wonderland: Navigating 3D scenes from single image, in CVPR, 2025. [325] H. Wang, F. Liu, J. Chi, and Y. Duan, VideoScene: Distilling video diffusion model to generate 3D scenes in one step, in CVPR, 2025. [326] R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, and A. Holynski, CAT4D: create anything in 4D with multi-view video diffusion models, in CVPR, 2025. [327] S. Zhai, Z. Ye, J. Liu, W. Xie, J. Hu, Z. Peng, H. Xue, D. Chen, X. Wang, L. Yang, N. Wang, H. Liu, and G. Zhang, StarGen: spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation, in CVPR, 2025. [328] B. Deng, R. Tucker, Z. Li, L. J. Guibas, N. Snavely, and G. Wetzstein, Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion, in SIGGRAPH, 2024. 23 [329] Q. Wang, W. Li, C. Mou, X. Cheng, and J. Zhang, 360DVD: Controllable panorama video generation with 360-degree video diffusion model, in CVPR, 2024. [330] J. Tan, S. Yang, T. Wu, J. He, Y. Guo, Z. Liu, and D. Lin, Imagine360: Immersive 360 video generation from perspective anchor, arXiv 2412.03552, 2024. [331] T. Lu, T. Shu, A. L. Yuille, D. Khashabi, and J. Chen, Generative world explorer, ICLR, 2025. [332] J. Liu, S. Lin, Y. Li, and M. Yang, DynamicScaler: Seamless and scalable video generation for panoramic scenes, in CVPR, 2025. [333] E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. J. Storkey, T. Pearce, and F. Fleuret, Diffusion for world modeling: Visual details matter in atari, in NeurIPS, 2024. [334] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter, Diffusion models are real-time game engines, in ICLR, 2025. [335] Decart, J. Quevedo, Q. McIntyre, S. Campbell, X. Chen, and R. Wachen, Oasis: universe in transformer, 2024. [Online]. Available: https://oasis-model.github.io/ [336] Z. Xiao, Y. Lan, Y. Zhou, W. Ouyang, S. Yang, Y. Zeng, and X. Pan, WORLDMEM: Long-term consistent world simulation with memory, arXiv 2504.12369, 2025. [337] X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu, DriveDreamer: Towards real-world-driven world models for autonomous driving, in ECCV, 2024. [338] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving, in CVPR, 2024. [339] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado, GAIA-1: generative world model for autonomous driving, arXiv 2309.17080, 2023. [340] X. Li, Y. Zhang, and X. Ye, DrivingDiffusion: layout-guided multi-view driving scenarios video generation with latent diffusion model, in ECCV, 2024. [341] B. Xie, Y. Liu, T. Wang, J. Cao, and X. Zhang, Glad: streaming scene generator for autonomous driving, in ICLR, 2025. [342] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, and Q. Xu, MagicDrive3D: Controllable 3D generation for any-view rendering in street scenes, arXiv 2405.14475, 2024. [343] J. Mao, B. Li, B. Ivanovic, Y. Chen, Y. Wang, Y. You, C. Xiao, D. Xu, M. Pavone, and Y. Wang, DreamDrive: Generative 4D scene modeling from street view images, arXiv 2501.00601, 2025. [344] Y. Yan, Z. Xu, H. Lin, H. Jin, H. Guo, Y. Wang, K. Zhan, X. Lang, H. Bao, X. Zhou, and S. Peng, StreetCrafter: Street view synthesis with controllable video diffusion models, in CVPR, 2025. [345] J. Mei, Y. Ma, X. Yang, L. Wen, T. Wei, M. Dou, B. Shi, and Y. Liu, DreamForge: Motion-aware autoregressive video generation for multi-view driving scenes, arXiv 2409.04003, 2024. [346] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang, Panacea: Panoramic and controllable video generation for autonomous driving, in CVPR, 2024. [347] H. Lu, X. Wu, S. Wang, X. Qin, X. Zhang, J. Han, W. Zuo, and J. Tao, Seeing Beyond Views: Multi-view driving scene video generation with holistic attention, arXiv 2412.03520, 2024. [348] D. Liang, D. Zhang, X. Zhou, S. Tu, T. Feng, X. Li, Y. Zhang, M. Du, X. Tan, and X. Bai, Seeing the Future, Perceiving the Future: unified driving world model for future generation and perception, arXiv 2503.13587, 2025. [349] J. Guo, Y. Ding, X. Chen, S. Chen, B. Li, Y. Zou, X. Lyu, F. Tan, X. Qi, Z. Li, and H. Zhao, DiST-4D: Disentangled spatiotemporal diffusion with metric depth for 4D driving scene generation, arXiv 2503.15208, 2025. [350] L. Russell, A. Hu, L. Bertoni, G. Fedoseev, J. Shotton, E. Arani, and G. Corrado, GAIA-2: controllable multi-view generative world model for autonomous driving, arXiv 2503.20523, 2025. [351] X. Yang, L. Wen, Y. Ma, J. Mei, X. Li, T. Wei, W. Lei, D. Fu, P. Cai, M. Dou, B. Shi, L. He, Y. Liu, and Y. Qiao, DriveArena: closedloop generative simulation platform for autonomous driving, arXiv 2408.00415, 2024. [352] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang, DriveDreamer-2: LLM-enhanced world models for diverse driving video generation, in AAAI, 2025. [353] J. Jiang, G. Hong, L. Zhou, E. Ma, H. Hu, X. Zhou, J. Xiang, F. Liu, K. Yu, H. Sun, K. Zhan, P. Jia, and M. Zhang, DiVE: Dit-based video generation with enhanced control, arXiv 2409.01595, 2024. [354] M. Hassan, S. Stapf, A. Rahimi, P. M. B. Rezende, Y. Haghighi, D. Br uggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, M. Cannici, E. Aljalbout, B. Ye, X. Wang, A. Davtyan, M. Salzmann, D. Scaramuzza, M. Pollefeys, P. Favaro, and A. Alahi, GEM: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control, in CVPR, 2025. [355] J. Lu, Z. Huang, Z. Yang, J. Zhang, and L. Zhang, WoVoGen: world volume-aware diffusion for controllable multi-camera driving scene generation, in ECCV, 2024. [356] L. Li, W. Qiu, Y. Cai, X. Yan, Q. Lian, B. Liu, and Y. Chen, SyntheOcc: Synthesize geometric-controlled street view images through 3D semantic mpis, arXiv 2410.00337, 2024. [357] Y. Lu, X. Ren, J. Yang, T. Shen, Z. Wu, J. Gao, Y. Wang, S. Chen, M. Chen, S. Fidler, and J. Huang, InfiniCube: Unbounded and controllable dynamic 3D driving scene generation with worldguided video models, arXiv 2412.03934, 2024. [358] B. Li, J. Guo, H. Liu, Y. Zou, Y. Ding, X. Chen, H. Zhu, F. Tan, C. Zhang, T. Wang, S. Zhou, L. Zhang, X. Qi, H. Zhao, M. Yang, W. Zeng, and X. Jin, UniScene: Unified occupancy-centric driving scene generation, in CVPR, 2025. [359] T. Yan, D. Wu, W. Han, J. Jiang, X. Zhou, K. Zhan, C. Xu, and J. Shen, DrivingSphere: Building high-fidelity 4D world for closed-loop simulation, in CVPR, 2025. [360] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, and H. Wang, BEVWorld: multimodal world model for autonomous driving via unified BEV latent space, arXiv 2407.05679, 2024. [361] Z. Wu, J. Ni, X. Wang, Y. Guo, R. Chen, L. Lu, J. Dai, and Y. Xiong, HoloDrive: Holistic 2D-3D multi-modal street scene generation for autonomous driving, arXiv 2412.01407, 2024. [362] Y. Wu, H. Zhang, T. Lin, L. Huang, S. Luo, R. Wu, C. Qiu, W. Ke, and T. Zhang, Generating multimodal driving scenes via nextscene prediction, in CVPR, 2025. [363] W. Wu, X. Guo, W. Tang, T. Huang, C. Wang, D. Chen, and C. Ding, DriveScape: Towards high-resolution controllable multi-view driving video generation, in CVPR, 2025. [364] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, D. Lin, and K. Yu, Unleashing generalization of end-to-end autonomous driving with controllable long video generation, arXiv 2406.01349, 2024. [365] R. Gao, K. Chen, B. Xiao, L. Hong, Z. Li, and Q. Xu, MagicDriveV2: High-resolution long video generation for autonomous driving with adaptive control, arXiv 2411.13807, 2024. [366] X. Hu, W. Yin, M. Jia, J. Deng, X. Guo, Q. Zhang, X. Long, and P. Tan, DrivingWorld: Constructing world model for autonomous driving via video GPT, arXiv 2412.19505, 2024. [367] J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, and Z. Wu, MaskGWM: generalizable driving world model with video mask reconstruction, in CVPR, 2025. [368] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, Recognizing scene viewpoint using panoramic place representation, in CVPR, 2012. [369] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor segmentation and support inference from RGBD images, in ECCV, 2012. [370] S. Song, S. P. Lichtenberg, and J. Xiao, SUN RGB-D: RGB-D scene understanding benchmark suite, in CVPR, 2015. [371] B. Hua, Q. Pham, D. T. Nguyen, M. Tran, L. Yu, and S. Yeung, SceneNN: scene meshes dataset with annotations, in 3DV, 2016. [372] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, Joint 2D-3DSemantic data for indoor scene understanding, arXiv 1702.01105, 2017. [373] M. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagne, and J. Lalonde, Learning to predict indoor illumination from single image, ACM TOG, vol. 36, no. 6, pp. 176:1 176:14, 2017. [374] A. X. Chang, A. Dai, T. A. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang, Matterport3D: Learning from RGB-D data in indoor environments, in 3DV, 2017. [375] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, ScanNet: Richly-annotated 3D reconstructions of indoor scenes, in CVPR, 2017. [376] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, Stereo magnification: learning view synthesis using multiplane images, ACM TOG, vol. 37, no. 4, p. 65, 2018. 24 B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. De Nardi, M. Goesele, S. Lovegrove, and R. A. Newcombe, The replica dataset: digital replica of indoor spaces, arXiv 1906.05797, 2019. [378] J. Wald, H. Dhamo, N. Navab, and F. Tombari, Learning 3D semantic scene graphs from 3D indoor reconstructions, in CVPR, 2020. [379] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra, HabitatMatterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI, in NeurIPS, 2021. [380] C. Yeshwanth, Y. Liu, M. Nießner, and A. Dai, ScanNet++: high-fidelity dataset of 3D indoor scenes, in ICCV, 2023. [381] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, X. Li, X. Sun, R. Ashok, A. Mukherjee, H. Kang, X. Kong, G. Hua, T. Zhang, B. Benes, and A. Bera, DL3DV-10K: large-scale scene dataset for deep learning-based 3D vision, in CVPR, 2024. [382] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, Semantic scene completion from single depth image, in CVPR, 2017. [383] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou, Structured3D: large photo-realistic dataset for structured 3D modeling, in ECCV, 2020. [384] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in ICCV, 2021. [385] H. Fu, B. Cai, L. Gao, L. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao, and H. Zhang, 3D-FRONT: 3D furnished rooms with layouts and semantics, in ICCV, 2021. [386] Y. Hold-Geoffroy, A. Athawale, and J. Lalonde, Deep sky modeling for single image outdoor lighting estimation, in CVPR, 2019. [387] I. Skorokhodov, G. Sotnikov, and M. Elhoseiny, Aligning latent and image spaces to connect the unconnectable, in ICCV, 2021. [388] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the KITTI vision benchmark suite, in CVPR, 2012. [389] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, The Cityscapes dataset for semantic urban scene understanding, in CVPR, 2016. [390] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, SemanticKITTI: dataset for semantic scene understanding of LiDAR sequences, in ICCV, 2019. [391] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, Scalability in perception for autonomous driving: Waymo open dataset, in CVPR, 2020. [392] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuScenes: multimodal dataset for autonomous driving, in CVPR, 2020. [393] Y. Zhou, J. Huang, X. Dai, L. Luo, Z. Chen, and Y. Ma, Holicity: city-scale data platform for learning holistic 3D structures, arXiv 2008.03286, 2020. [394] W. Li, Y. Lai, L. Xu, Y. Xiangli, J. Yu, C. He, G. Xia, and D. Lin, OmniCity: Omnipotent city understanding with multi-level and multi-view images, in CVPR, 2023. [395] Y. Liao, J. Xie, and A. Geiger, KITTI-360: novel dataset and benchmarks for urban scene understanding in 2D and 3D, IEEE TPAMI, vol. 45, no. 3, pp. 32923310, 2023. [396] Y. Cabon, N. Murray, and M. Humenberger, Virtual KITTI 2, arXiv 2001.10773, 2020. [397] J. Wilson, J. Song, Y. Fu, A. Zhang, A. Capodieci, P. Jayakumar, K. Barton, and M. Ghaffari, MotionSC: Data set and network for real-time semantic mapping in dynamic environments, IEEE Robotics Autom. Lett., vol. 7, no. 3, pp. 84398446, 2022. [398] J. Wald, A. Avetisyan, N. Navab, F. Tombari, and M. Nießner, RIO: 3D object instance re-localization in changing indoor environments, in ICCV, 2019. [377] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Y. Ren, S. Verma, A. Clarkson, M. Yan, [399] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. J. Maybank, and D. Tao, 3D-FUTURE: 3D furniture shape with texture, IJCV, 2021. 25 [400] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, in NIPS, 2017. [425] D. Z. Chen, H. Li, H. Lee, S. Tulyakov, and M. Nießner, SceneTex: High-quality texture synthesis for indoor scenes via diffusion priors, in CVPR, 2024. [401] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, Demystifying MMD gans, in ICLR, 2018. [402] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, Improved techniques for training gans, in NIPS, 2016. [403] S. Morozov, A. Voynov, and A. Babenko, On self-supervised image representations for GAN evaluation, in ICLR, 2021. [404] G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, and G. LoaizaGanem, Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models, in NeurIPS, 2023. [405] T. Kynkaanniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen, The role of imagenet classes in frechet inception distance, in ICLR, 2023. [406] A. Mittal, A. K. Moorthy, and A. C. Bovik, No-reference image quality assessment in the spatial domain, IEEE TIP, vol. 21, no. 12, pp. 46954708, 2012. [407] A. Mittal, R. Soundararajan, and A. C. Bovik, Making completely blind image quality analyzer, IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209212, 2013. [408] J. Wang, K. C. K. Chan, and C. C. Loy, Exploring CLIP for assessing the look and feel of images, in AAAI, 2023. [409] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas, Learning representations and generative models for 3D point clouds, in ICML, 2018. [410] D. Lopez-Paz and M. Oquab, Revisiting classifier two-sample tests, in ICLR, 2017. [411] D. Eigen, C. Puhrsch, and R. Fergus, Depth map prediction from single image using multi-scale deep network, in NIPS, 2014. [412] W. Lai, J. Huang, O. Wang, E. Shechtman, E. Yumer, and M. Yang, Learning blind video temporal consistency, in ECCV, 2018. [413] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, FVD: new metric for video generation, in ICLR, 2019. [414] J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, and R. Liao, Frechet video motion distance: metric for evaluating motion consistency in videos, arXiv 2407.16124, 2024. [415] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, CLIPScore: reference-free evaluation metric for image captioning, in EMNLP, 2021. [416] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun, Q. Yan, X. Min, G. Zhai, and W. Lin, Q-Align: Teaching LMMs for visual scoring via discrete textdefined levels, in ICML, 2024. [417] X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj, K. Wang, Q. D. Do, Y. Ni, B. Lyu, Y. Narsupalli, R. Fan, Z. Lyu, Y. Lin, and W. Chen, VideoScore: Building automatic metrics to simulate fine-grained human feedback for video generation, arXiv 2406.15252, 2024. [418] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, VBench: Comprehensive benchmark suite for video generative models, in CVPR, 2024. [419] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang, Y. Wang, X. Chen, Y. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, VBench++: Comprehensive and versatile benchmark suite for video generative models, arXiv 2411.13503, 2024. [420] D. Zheng, Z. Huang, H. Liu, K. Zou, Y. He, F. Zhang, Y. Zhang, J. He, W.-S. Zheng, Y. Qiao, and Z. Liu, VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness, arXiv 2503.21755, 2025. [421] H. Duan, H.-X. Yu, S. Chen, L. Fei-Fei, and J. Wu, WorldScore: unified evaluation benchmark for world generation, arXiv 2504.00983, 2025. [422] L. ollein, J. Johnson, and M. Nießner, StyleMesh: Style transfer for indoor 3D scene reconstructions, in CVPR, 2022. [423] L. Song, L. Cao, H. Xu, K. Kang, F. Tang, J. Yuan, and Z. Yang, RoomDreamer: Text-driven 3D indoor scene synthesis with coherent geometry and texture, in ACM MM, 2023. [424] Y. Chen, H. Huang, T. Vu, K. Shum, and S. Yeung, StyleCity: Large-scale 3D urban scenes stylization, in ECCV, 2024. [426] Z. Huang, W. Yu, X. Cheng, C. Zhao, Y. Ge, M. Guo, L. Yuan, and Y. Tian, Roompainter: View-integrated diffusion for consistent indoor scene texturing, in CVPR, 2025. [427] I. Hwang, H. Kim, and Y. M. Kim, Text2Scene: Text-driven indoor scene stylization with part-aware details, in CVPR, 2023. [428] Q. Wang, R. Lu, X. Xu, J. Wang, M. Y. Wang, B. Dai, G. Zeng, and D. Xu, RoomTex: Texturing compositional indoor scenes via iterative inpainting, in ECCV, 2024. [429] B. Yang, W. Dong, L. Ma, W. Hu, X. Liu, Z. Cui, and Y. Ma, DreamSpace: Dreaming your room space with text-driven panoramic texture propagation, in VR, 2024. [430] M. Yang, J. Guo, Y. Chen, L. Chen, P. Li, Z. Cheng, X. Zhang, and H. Huang, InstanceTex: Instance-level controllable texture synthesis for 3D scenes via diffusion priors, in SIGGRAPH Asia, 2024. [431] Q. A. Wei, S. Ding, J. J. Park, R. Sajnani, A. Poulenard, S. Sridhar, and L. J. Guibas, LEGO-Net: Learning regular rearrangements of objects in rooms, in CVPR, 2023. [432] A. Murali, A. Mousavian, C. Eppner, A. Fishman, and D. Fox, CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation, in ICRA, 2023. [433] S. Zhang, J. Huang, L. Yue, J. Zhang, J. Liu, Y. Lai, and S. Zhang, SceneExpander: Real-time scene synthesis for interactive floor plan editing, in ACM MM, 2024. [434] S. Zhang, H. Tam, Y. Li, K. Ren, H. Fu, and S. Zhang, SceneDirector: Interactive scene synthesis by simultaneously editing multiple objects in real-time, IEEE TVCG, vol. 30, no. 8, pp. 45584569, 2024. [435] Z. Wu, Y. Rubanova, R. Kabra, D. A. Hudson, I. Gilitschenski, Y. Aytar, S. van Steenkiste, K. R. Allen, and T. Kipf, Neural Assets: 3D-aware multi-object scene synthesis with image diffusion models, in NeurIPS, 2024. [436] L. Li, Q. Lian, L. Wang, N. Ma, and Y. Chen, Lift3D: Synthesize 3D training data by lifting 2D GAN to 3D generative radiance field, in CVPR, 2023. [437] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S. Zhu, Diffusion-based generation, optimization, and planning in 3D scenes, in CVPR, 2023. [438] M. Hassan, Y. Guo, T. Wang, M. J. Black, S. Fidler, and X. B. Peng, Synthesizing physical character-scene interactions, in SIGGRAPH, 2023. [439] L. Pan, Z. Yang, Z. Dou, W. Wang, B. Huang, B. Dai, T. Komura, and J. Wang, Tokenhsi: Unified synthesis of physical humanscene interactions through task tokenization, in CVPR, 2025. [440] J. Wang, Y. Rong, J. Liu, S. Yan, D. Lin, and B. Dai, Towards diverse and natural scene-aware 3D human motion synthesis, in CVPR, 2022. [441] K. Zhao, S. Wang, Y. Zhang, T. Beeler, and S. Tang, Compositional human-scene interaction synthesis with semantic control, in ECCV, 2022. [442] F. Hong, V. Guzov, H. J. Kim, Y. Ye, R. A. Newcombe, Z. Liu, and L. Ma, EgoLM: multi-modal language model of egocentric motions, arXiv 2409.18127, 2024. [443] M. Hassan, V. Choutas, D. Tzionas, and M. J. Black, Resolving 3D human pose ambiguities with 3D scene constraints, in ICCV, 2019. [444] Z. Wang, Y. Chen, T. Liu, Y. Zhu, W. Liang, and S. Huang, HUMANISE: language-conditioned human motion generation in 3D scenes, in NeurIPS, 2022. [445] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. Liu, Z. Liu, J. Engel, R. De Nardi, and R. A. Newcombe, Nymeria: massive collection of multimodal egocentric daily motion in the wild, in ECCV, 2024. [446] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang, Synthesizing diverse human motions in 3D indoor scenes, in ICCV, 2023. [447] L. Pan, J. Wang, B. Huang, J. Zhang, H. Wang, X. Tang, and Y. Wang, Synthesizing physically plausible human motions in 3D scenes, in 3DV, 2024. [448] W. Wang, L. Pan, Z. Dou, J. Mei, Z. Liao, Y. Lou, Y. Wu, L. Yang, J. Wang, and T. Komura, SIMS: Simulating stylized humanscene interactions with retrieval-augmented script generation, arXiv 2411.19921, 2025. [449] L. Li and A. Dai, GenZI: Zero-shot 3D human-scene interaction generation, in CVPR, 2024. [450] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. M. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. X. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and D. Batra, Habitat 2.0: Training home assistants to rearrange their habitat, in NeurIPS, 2021. [451] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, AI2-THOR: an interactive 3D environment for visual AI, arXiv 1712.05474, 2017. [452] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. MartınMartın, C. Wang, G. Levine, M. Lingelbach, J. Sun, M. Anvari, M. Hwang, M. Sharma, A. Aydin, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. FeiFei, BEHAVIOR-1K: benchmark for embodied AI with 1, 000 everyday activities and realistic simulation, in CoRL, 2022. [453] P. Ren, M. Li, Z. Luo, X. Song, Z. Chen, W. Liufu, Y. Yang, H. Zheng, R. Xu, Z. Huang, T. Ding, L. Xie, K. Zhang, C. Fu, Y. Liu, L. Lin, F. Zheng, and X. Liang, InfiniteWorld: unified scalable simulation framework for general visual-language robot interaction, arXiv 2412.05789, 2024. [454] Y. Wang, X. Qiu, J. Liu, Z. Chen, J. Cai, Y. Wang, T. J. Wang, Z. Xian, and C. Gan, Architect: Generating vivid and interactive 3D scenes with hierarchical 2D inpainting, in NeurIPS, 2024. [455] W. Wu, H. He, C. Zhang, J. He, S. Z. Zhao, R. Gong, Q. Li, and B. Zhou, Towards autonomous micromobility through scalable urban simulation, in CVPR, 2025. [456] C. Gao, B. Zhao, W. Zhang, J. Mao, J. Zhang, Z. Zheng, F. Man, J. Fang, Z. Zhou, J. Cui, X. Chen, and Y. Li, EmbodiedCity: benchmark platform for embodied agent in real-world city environment, arXiv 2410.09604, 2024. [457] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, ALFRED: benchmark for interpreting grounded instructions for everyday tasks, in CVPR, 2020. [458] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, CALVIN: benchmark for language-conditioned policy learning for longhorizon robot manipulation tasks, RA-L, vol. 7, no. 3, pp. 7327 7334, 2022. [459] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, LIBERO: benchmarking knowledge transfer for lifelong robot learning, in NeurIPS, 2023. [460] R. Gong, J. Huang, Y. Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S. Zhu, B. Jia, and S. Huang, ARNOLD: benchmark for language-grounded task learning with continuous states in realistic 3D scenes, in ICCV, 2023. [461] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao, Evaluating real-world robot manipulation policies in simulation, in CoRL, 2024. [462] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu, RoboCasa: Large-scale simulation of everyday tasks for generalist robots, arXiv 2406.02523, 2024. [463] Y. Wang, Z. Xian, F. Chen, T. Wang, Y. Wang, K. Fragkiadaki, Z. Erickson, D. Held, and C. Gan, RoboGen: Towards unleashing infinite data for automated robot learning via generative simulation, in ICML, 2024. [464] H. Geng, F. Wang, S. Wei, Y. Li, B. Wang, B. An, C. T. Cheng, H. Lou, P. Li, Y.-J. Wang, Y. Liang, D. Goetting, C. Xu, H. Chen, Y. Qian, Y. Geng, J. Mao, W. Wan, M. Zhang, J. Lyu, S. Zhao, J. Zhang, J. Zhang, C. Zhao, H. Lu, Y. Ding, R. Gong, Y. Wang, Y. Kuang, R. Wu, B. Jia, C. Sferrazza, H. Dong, S. Huang, Y. Wang, J. Malik, and P. Abbeel, RoboVerse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning, arXiv 2504.18904, 2025. [465] W. Liang, S. Wang, H. Wang, O. Bastani, D. Jayaraman, and Y. J. Ma, Eurekaverse: Environment curriculum generation via large language models, arXiv 2411.01775, 2024. [466] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, Learning universal policies via text-guided video generation, in NeurIPS, 2023. [467] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. S. Jaakkola, J. B. Tenenbaum, L. P. Kaelbling, A. Srivastava, and P. Agrawal, Compositional foundation models for hierarchical planning, in NeurIPS, 2023. 26 [468] J. Cen, C. Wu, X. Liu, S. Yin, Y. Pei, J. Yang, Q. Chen, N. Duan, and J. Zhang, Using left and right brains together: Towards vision and language planning, in ICML, 2024. [469] Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, and H. Li, Closed-loop visuomotor control with generative expectation for robotic manipulation, in NeurIPS, 2024. [470] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, Unleashing large-scale video generative pre-training for visual robot manipulation, in ICLR, 2024. [471] C. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang, H. Zhang, and M. Zhu, GR-2: generative videolanguage-action model with web-scale knowledge for robot manipulation, arXiv 2410.06158, 2024. [472] Y. Hong, B. Liu, M. Wu, Y. Zhai, K. Chang, L. Li, K. Lin, C. Lin, J. Wang, Z. Yang, Y. Wu, and L. Wang, SlowFast-VGen: Slow-fast learning for action-driven long video generation, in ICLR, 2025. [473] Y. Hu, Y. Guo, P. Wang, X. Chen, Y. Wang, J. Zhang, K. Sreenath, C. Lu, and J. Chen, Video Prediction Policy: generalist robot policy with predictive visual representations, in ICML, 2025. [474] Z. Ren, Y. Wei, X. Guo, Y. Zhao, B. Kang, J. Feng, and X. Jin, VideoWorld: Exploring knowledge learning from unlabeled videos, arXiv 2501.09781, 2025. [475] H. A. Alhaija, J. Alvarez, M. Bala, T. Cai, T. Cao, L. Cha, J. Chen, M. Chen, F. Ferroni, S. Fidler, D. Fox, Y. Ge, J. Gu, A. Hassani, M. Isaev, P. Jannaty, S. Lan, T. Lasser, H. Ling, M.-Y. Liu, X. Liu, Y. Lu, A. Luo, Q. Ma, H. Mao, F. Ramos, X. Ren, T. Shen, S. Tang, T.-C. Wang, J. Wu, J. Xu, S. Xu, K. Xie, Y. Ye, X. Yang, X. Zeng, and Y. Zeng, Cosmos-Transfer1: Conditional world generation with adaptive multimodal control, arXiv 2503.14492, 2025. [476] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, and C. Gan, TesserAct: learning 4D embodied world models, arXiv 2504.20995, 2025. [477] Y. Ze, G. Yan, Y. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang, GNFactor: Multi-task real robot learning with generalizable neural feature fields, in CoRL, 2023. [478] S. Dasgupta, A. Gupta, S. Tuli, and R. Paul, ActNeRF: Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions, in IROS, 2024. [479] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, ManiGaussian: Dynamic gaussian splatting for multi-task robotic manipulation, in ECCV, 2024. [480] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu, OccWorld: Learning 3D occupancy world model for autonomous driving, in ECCV, 2024. [481] Y. Yang, J. Mei, Y. Ma, S. Du, W. Chen, Y. Qian, Y. Feng, and Y. Liu, Driving in the occupancy world: Vision-centric 4D occupancy forecasting and planning via world models for autonomous driving, in AAAI, 2025. [482] J. Ma, X. Chen, J. Huang, J. Xu, Z. Luo, J. Xu, W. Gu, R. Ai, and H. Wang, Cam4DOcc: Benchmark for camera-only 4D occupancy forecasting in autonomous driving applications, in CVPR, 2024. [483] J. Wei, S. Yuan, P. Li, Q. Hu, Z. Gan, and W. Ding, OccLLaMa: An occupancy-language-action generative world model for autonomous driving, arXiv 2409.03272, 2024. [484] L. Wang, W. Zheng, Y. Ren, H. Jiang, Z. Cui, H. Yu, and J. Lu, OccSora: 4D occupancy generation models as world simulators for autonomous driving, arXiv 2405.20337, 2024. [485] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M. Liu, and Q. Zhang, EdgeRunner: Auto-regressive auto-encoder for artistic mesh generation, in CVPR, 2024. [486] M. Wu, H. Dai, K. Yao, T. Tuytelaars, and J. Yu, BG-Triangle: Bezier gaussian triangle for 3D vectorization and rendering, in CVPR, 2025. [487] M. Guo, B. Wang, K. He, and W. Matusik, TetSphere Splatting: Representing high-quality geometry with lagrangian volumetric meshes, in ICLR, 2025. [488] S. Duggal, Y. Hu, O. Michel, A. Kembhavi, W. T. Freeman, N. A. Smith, R. Krishna, A. Torralba, A. Farhadi, and W.-C. Ma, Eval3D: Interpretable and fine-grained evaluation for 3D generation, in CVPR, 2025. [489] Y. Hu, L. Anderson, T. Li, Q. Sun, N. Carr, J. Ragan-Kelley, and F. Durand, DiffTaichi: Differentiable programming for physical simulation, in ICLR, 2020. 27 Beichen Wen received the B.Eng. degree in computer science and technology from Sun Yatsen University, China, in 2024. He is currently master student at Nanyang Technological University, supervised by Prof. Ziwei Liu. His research interests include computer graphics and 3D vision. Haozhe Xie received his Ph.D. from the Harbin Institute of Technology, in 2021. He is currently research fellow at MMLab@NTU, Nanyang Technological University, Singapore. Previously, he served as senior research scientist at Tencent AI Lab from 2021 to 2023. His research interests include computer vision with focus on 3D generation and reconstruction. He has published several papers in CVPR, ICCV, ECCV, ICLR, and IJCV, and serves as reviewer for these journals and conferences. Zhaoxi Chen received the bachelors degree from Tsinghua University, in 2021. He is currently Ph.D. student at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the AISG PhD Fellowship in 2021. His research interests include inverse rendering and 3D generative models. He has published several papers in CVPR, ICCV, ECCV, ICLR, NeurIPS, TOG, and TPAMI. He also served as reviewer for CVPR, ICCV, NeurIPS, TOG, and IJCV. Fangzhou Hong received Ph.D. degree from MMLab at Nanyang Technological University, supervised by Prof. Ziwei Liu. He received B.Eng. degree in software engineering from Tsinghua University, China, in 2020. His research interests include computer vision and deep learning. Particularly, he is interested in 3D representation learning. Ziwei Liu is currently an associate professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning, and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurlPS, ICLR, ICML, TPAMI, TOG, and Nature Machine Intelligence. He is the recipient of the Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV, NeurlPS, and ICLR, as well as an Associate Editor of IJCV."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore 637335"
    ]
}