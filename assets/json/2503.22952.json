{
    "paper_title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
    "authors": [
        "Yuxuan Wang",
        "Yueqian Wang",
        "Bo Chen",
        "Tong Wu",
        "Dongyan Zhao",
        "Zilong Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 5 9 2 2 . 3 0 5 2 : r OmniMMI: Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts Yuxuan Wang1,2, Yueqian Wang2,3, Bo Chen1,2,4, Tong Wu1,2, Dongyan Zhao2,3, Zilong Zheng1,2 (cid:0) 1 Beijing Institute for General Artificial Intelligence 2 State Key Laboratory of General Artificial Intelligence 3 Wangxuan Institute of Computer Technology, Peking University 4 X-LANCE Lab, Shanghai Jiao Tong University {wangyuxuan1, zlzheng}@bigai.ai https://omnimmi.github.io"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains formidable challenge. In this work, we introduce OmniMMI, comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating. Extensive experimental results reveal that the existing MLLMs fall short in interactive streaming understanding, particularly struggling with proactive tasks and multi-turn queries. Our proposed M4, though lightweight, demonstrates significant improvement in handling proactive tasks and real-time interactions. 1. Introduction The burgeoning field of multi-modal large language models (MLLMs), exemplified by GPT-4o [32] and Gemini Pro [36], marks significant leap towards embodied agentic intelligence by incorporating multi-modal encoders within pre-trained large language models (LLMs), such as video understanding [25, 38, 53], audio comprehension [7], and speech-to-speech dialogue [8, 9, 46, 47], etc. The overarching goal is (i) to transcend the general capabilities of LLMs to process and respond to continuous streams of multi-modal dynamics, encompassing text, vision, and (cid:0)Corresponding author: Zilong Zheng zlzheng@bigai.ai i.e., Omni Large Language Models speech modalities, (OmniLLMs), and (ii) to derive interactive systems that can take the first-person perspective to interact with the real world. However, this rapid development raises crucial question: How can we effectively evaluate the real-world interactive capabilities of OmniLLMs in streaming video contexts? Addressing this question is pivotal to validating their design efficacy and enhancing their performance for comprehensive open-world multi-modal understanding. In response, vast number of benchmarks has been launched with different focuses on long-form video understanding [10, 34, 59], comprehensive video analysis [10], or audio-video understanding [22], etc. However, most of these benchmarks take in the entire video sequence as input, some of which use frame selection techniques with slight information scarification, to produce final answers. These tasks, though challenging, are far from the real-world interactive scenarios where videos are taken in as streaming sequence; refer to Tab. 1 for benchmark comparisons. More recently, OmniBench [22] has been introduced to evaluate models capabilities on visual, acoustic, and textual inputs simultaneously. However, only the last image frame is considered visual input, while the video dynamics across streaming contexts and interactive features are overlooked. To bridge the critical gap, we introduce Multi-modal Interaction Benchmark for OmniLLMs (OmniMMI), which aims at comprehensively evaluating the interactive capabilities of streaming video context in the open world  (Fig. 1)  . We start by formalizing the task of streaming multi-modal understanding ( 3.2). Apart from challenges identified by prior long-form video-audio benchmarks (e.g., temporal dynamics [10, 59] and multi-modal localization [37, 42]), OmniMMI considers two featured obstacles for real-time interactions. Streaming Temporal State Awareness. Streaming video understanding must build an understanding w.r.t. the current and historical temporal state incrementally, without Figure 1. OmniMMI consists of two categories of multi-modal interactive challenges: streaming video understanding (top) and proactive reasoning (bottom). Each query is processed into natural language text and synthetic audio as input. accessing the future context. This contrasts with traditional MLLMs that can leverage the entire multi-modal contexts, posing challenges in our distinguished tasks of action prediction (AP), state grounding (SG) and multiturn dependencies (MD) (3.2.1). Proactive Reasoning and Turn-Taking. Generating responses proactively and appropriately anticipating the turn-taking time spot w.r.t. users intentions and dynamic contexts is crucial feature for general interactive agents. This typically requires models to identify speakers (SI), distinguish between noise or legitimate query (PT), and proactively initiate response (PA) (3.2.2). In light of this, OmniMMI is crafted as the first-ever comprehensive OmniLLM benchmark to address the aforementioned streaming and interactive challenges. As exemplified in Fig. 1, we curate dataset of 1,121 videos sourced from YouTube and open-sourced video-audio data with an average duration of 324 seconds. 2,290 questions are manually annotated and reviewed w.r.t. both visual and auditory information from video inputs, encompassing different topics relevant to the multi-modal contexts  (Fig. 2)  . Notably, to enhance the interactive feature of this benchmark, we designed multi-turn questions (up to 3 turns) where the next response is based on the answer from previous turns; see dynamic state grounding and multi-turn dependency reasoning as examples. Using OmniMMI, we make thorough evaluation of various well-known video large language models (VideoLLMs) and accessible OmniLLMs across all tasks. Surprisingly, most models encounter challenges in multiturn tasks involving streaming video, where they struggle beyond single reasoning step, thus revealing limitations in dynamic settings. In the realm of audio-visual interaction, models that process both audio and visual inputs do not outperform those that handle visual inputs alone, indicating inadequate modality alignment. Furthermore, increasing the model size does not lead to improved performance; instead, models capable of processing longer input lengths demonstrate superior results, highlighting the importance of balancing input length with memory efficiency. Table 1. Comparison with existing Video Benchmark OmniMMI is the first comprehensive benchmark that focuses on streaming and interactive VideoLLMs. * only the last frame of videos are taken as input. is not real-time setting. Benchmark Modality #Videos #Questions MVBench [20] Video-Bench [31] EgoSchema [29] VideoMME [10] LongVideoBench [44] MLVU [59] MultiHop-EgoQA [5] OmniBench [22] StreamingBench [26] OvO-Bench [23] Video Video Video Video, Audio Video Video Video Image*,Audio Video, Audio Video OmniMMI (Ours) Video, Audio 3,641 5,917 5,063 900 3,763 2,593 360 - 900 1,121 4,000 17,036 5,063 2,700 6,678 2,593 1,080 1,142 4,500 2,814 2,290 Length Video(s) #Turn 16.0 56.0 180.0 1017.9 473.0 720.0 180.0 9.2 243.1 428.9 324.3 1 1 1 1 1 1 1 1 1 1 1-3 Multi-Hop Contain-Ego Streaming Proactive Interactive To margin towards real-time interactive reasoning, in 4, we devise novel and robust framework, Multi-modal Multiplexing Modeling (M4), taking inspirations from duplexing modeling of auditory models [27, 30, 46]. We crafted small video-free SFT data, M4-IT, for proactive turn-taking awareness. As such, M4 can be built upon any pre-trained VideoLLMs, enabling an inference-efficient streaming model that can see, listen while generating. 2. Related Work Omni Large Language Models The advancement of OmniLLMs represents notable achievement in MLLMs, aiming for real-time comprehension and processing of diverse modalities. Significant contributions in this domain include GPT-4o and Project Astra, which manage and generate multi-modal inputs and outputs encompassing text, audio, images, and videos. Concurrently, the opensource community has developed models like VITA [11] and Ocean-Omni [21], which integrate distinct models for enhanced non-awakening and interrupting capabilities. Additionally, audio-based conversational models such as LSLM [27] and mini-Omni [46, 47] have emerged, utilizing text-instructed speech generation to facilitate real-time speech interactions while maintaining strong language proficiency. Despite these advancements, existing models often lack proactive reasoning abilities (Sec. 3.2.2) without additional computational overhead. Addressing this gap, we introduce M4, an interactive framework for MLLMs that enables proactive reasoning without necessitating extra forward computation steps or video-specific training. Video Understanding Benchmarks Early general video understanding benchmarks [16, 20, 31, 41, 48, 52] were introduced to evaluate models capabilities in general videolanguage understanding. Other works [43, 45] have focused on temporal grounding for dynamic video content. More recently, several benchmarks for long video understanding [10, 29, 44, 59] have been proposed. The MultiHopEgoQA dataset [5] introduces multi-hop video questionanswering dataset with temporal evidence to assess models multi-hop reasoning abilities over relatively long videos. OmniBench [22] extends visual information to include audio, proposing an audio-focused benchmark complemented by visual information. However, there is lack of comprehensive benchmarks for streaming video understanding and proactive reasoning for interactive VideoLLMs. To address this gap, we propose OmniMMI, aiming to encourage further advancements in this area. Streaming Video Understanding The realm of MLLMs [34, 38, 40, 54] have achieved superior performance in various video-centric tasks by employing progressive training paradigm that unifies different selfor weakly-supervised learning frameworks. Some recent VideoLLMs [15, 33, 42, 58, 60] enables video processing in an online manner and store past video information in memory bank, facilitating long-term video analysis without exceeding computational constraints. VideoLLMonline [3] further addresses the challenge of integrating diverse data modalities by effectively introducing special token after each frame with binary classification task. These advancements underscore the transformative potential of multimodal and interactive video understanding technologies, promising innovative applications across various fields as models continue to improve in their ability to integrate and process diverse data modalities. 3. The OmniMMI Dataset 3.1. Dataset Construction Data Source The existing datasets include Ego4d [13], COIN [35], Shot2Story20K [14], QVHighlight [18], and MLVU [59], which encompass both egocentric and opendomain videos across diverse topics. However, most of these datasets are not specifically aligned with typical interactive scenarios, such as interactions involving camera. To address the issue and minimize data contamination, we integrated the test and validation sets from the aforementioned datasets with newly collected video footage. Specifically, we augmented these datasets by sourcing additional videos Table 2. OmniMMI detailed statistics. Vid.(s): video duration. Que.: question words."
        },
        {
            "title": "Proactive",
            "content": "SG AP MP PT PA SI 300 704 2.35 200 #Videos 200 #Queries Avg. Turns 1.00 Avg. Vid.(s) 350.82 234.95 25.99 Avg. Que. 16.00 200 78 300 200 200 786 2.62 1.00 1.00 374.80 2004.10 149.82 17.49 8.45 26.27 200 200 1.00 549.64 60.91 Figure 3. Distribution of video duration length. description of the annotation procedure and the annotation interface is provided in SM. 3.1.1. Statistics In Tab. 1, we present comparison of our benchmark with existing popular benchmarks in the field of video understanding. Our dataset consists of 1,121 videos and 2,290 queries, with the average video length exceeding five minutes. Notably, segment of our benchmark includes multiple turns, requiring models to correctly answer all associated questions to be labeled as hit. This approach simulates streaming scenario, thereby introducing an additional challenge. Overall, our work is the first to provide comprehensive benchmark specifically designed to evaluate the efficacy of streaming video understanding models. Tab. 2 presents comprehensive statistics of the videos and queries across all six subtasks. We further delineate topics of query prompts into different categories to demonstrate the diversity of questions. The distribution and examples are shown in Fig. 2. As seen, queries related to action/activity predominates in our benchmark, reflecting the dynamic nature of streaming video. 3.2. Benchmark Tasks Streaming Video Formulation We formalize an input streaming video as an infinite video sequence = {v1, , vt, }, where vt corresponds to the temporal time and Vt is the video history up to time t. is the minimum temporal unit that denotes the interval between consecutive frames. At timestamp t, let qn denote the nth natural language user query in the form of text or audio, )} denote the intert action history prior to t, the task is to generate next response w.r.t. the input streaming context, i.e., an ), ), , (qn = (Vt, = {(q1 , an1 , qn , a1 Figure 2. Distribution and examples of different types of query prompts. from YouTube, utilizing 425 keywords pertinent to frequently observed interactive environments. For each keyword, we downloaded maximum of 50 videos, each with duration of less than one hour. Ultimately, we conducted manual review to filter out low-quality videos, resulting in curated set of 78 videos. comprehensive list of these keywords is provided in the supplementary material (SM). Annotation The annotation process requires annotators to analyze both visual and auditory elements of input videos. Initially, annotators review the video chronologically and annotate the time span of relevant actions or states, tailored to the specific task, particularly for streaming video understanding. We provide question-type prompts to guide annotators in focusing on various video aspects, such as actions or object attributes, detailed in SM. For tasks involving proactive alerting and turn-taking, annotators record timestamps for significant events or their conclusions. In speaker identification tasks, annotators mark time spans of human introductions and subsequently label individuals names following activities or special situations. To ensure benchmark quality, annotators must review the video again, considering the noted spans and original dataset information, to refine questions and answers based on their annotations. Quality Review To ensure accuracy, second annotator reviews the initial annotations, focusing on the consistency of the questions, answers, and time spans. Any inconsistencies identified are documented and corrected. We then calculate the inter-annotator agreement to evaluate the consistency and reliability of the annotations. We re-used VIA tool1 for the annotation and reviewing process. detailed 1https://www.robots.ox.ac.uk/vgg/software/via where an is the n-th predicted answer at time t, Of note, at could be an action to be executed in short-term future; see Sec. 3.2.2 for exemplar tasks. 3.2.1. Streaming Video Understanding Dynamic State Grounding (SG): This task aims to ascertain the dynamic states of streaming video at different timestamps. We repeatedly pose the same query at different temporal states, i.e., {qτ = qt; τ [1, t)}. The objective is to determine the correct answer ai τ for each timestamp, where ai τ depends on Vτ µt, with µt being short duration preceding the time τ . Action Planning (AP): Given natural language goal and historical sequence from the streaming video, this task is to identify the correct next action to achieve the goal. Multi-turn Dependency (MD) Reasoning: This task involves answering series of questions where each subsequent question depends on the answer to the previous one. The requirement is that ai forms part of qi+1, i.e., ai = (qi, {a1, a2, . . . , ai1}) and qi+1 = G(qi, ai), where G(, ) generates the next question based on the current answer in predefined question template. The correctness of each answer ai is contingent upon the accuracy of the preceding answers. Metrics We use GPT4-o to assess each step of SG and MD, and the final accuracy is averaged over the correct responses to all questions in the sequence. For AP, the evaluation is computed as the accuracy of selecting response from predefined vocabulary set of action candidates. 3.2.2. Proactive Reasoning Speaker Identification (SI): In streaming videos featuring multiple individuals, proficient model should accurately identify speakers to better comprehend multi-party dialogues. Given an introduction by oneself or others, the question qi pertains to the current situation and requires identifying the name of the corresponding speaker. Proactive Alerting (PA): critical application of streaming video understanding is in surveillance, where the model is expected to notify humans of potentially dangerous situations. The desired response is proactive alerting function at = A(v) to be executed on the consecutive video sequences until appropriate altering information (e.g., informed in Fig. 1) is proactively announced. Proactive Turn-taking (PT): Streaming videos often contain significant noise. competent model should distinguish between queries that require response and those that are merely noise, necessitating silence. We construct series of queries that do not require response to evaluate the models ability to resist responding to noise queries. Metrics We employ GPT4-o to evaluate the accuracy of the SI metric. For PA, the timestamp of the models initial proactive response is recorded and considered successful instance if it occurs within the designated timeframe. For PT, accuracy is determined by calculating the percentage of instances where no response is generated. 4. Multi-modal Multiplexing Modeling We evaluate number of popular open-source MLLMs on OmniMMI in Tab. 3. Surprisingly, the existing MLLMs are far from satisfactory in streaming video understanding. To fill the gap, we develop robust OmniLLM baseline, Multimodal Multiplexing Modeling, which is dubbed as M4. Motivated by recent advances in speech LMs [27, 46], M4 formulates the interactive challenges of real-time multimodal communications with multiplexing modeling [30], technique that enables LMs process multiple inputs simultaneously with single compact representation. Compared with traditional VideoLLMs and OmniLLMs, M4 presents advances in: Proactive Generation. critical aspect of streaming video understanding is the models ability to proactively generate the next response without human intervention. Proactive Interruption. When presented with new query, M4 determines whether it is legitimate or merely noise in single forward step. Efficient Parallel Decoding. With multiplexing inputs, M4 decodes the next token in parallel to the inputs. Proactive Generation Most current methodologies [3, 11, 60] employ special tokens in conjunction with binary classification tasks and threshold settings to enable continuous narration. However, the efficacy of these special tokens is heavily contingent upon the chosen threshold settings, leading to significant obstacles for generalizing across various domains and video-language models. Building on insights from recent works [32, 47], we ask: whether it is possible to enable real-time proactive generation without supplementing time-consuming video-specific training? Solution We answer the question with affirmation. In M4, we derive an attention-based inference method, namely highlight spot, by harnessing the potential within pretrained VideoLLMs. Given streaming video and query qt as in Sec. 3.2, the algorithm goes as follows; refer to SM for formalized pseudo-code: Step I: Streaming KV Cache. For each in-coming frame v, we pre-compute = Wkv and = WV vectors to form KV cache. The attention scores between the query and the frames are calculated w.r.t. the KV cache, i.e., , with their mean and variance as µ, σ. = softmax Step II: Highlight Spot Max-heap. Indices of frames whose attention scores exceed the Gaussian average µ+α σ are stored in max heap, where α is Gaussian factor. Step III: Hit Computation. The peak index from maxheap is extracted. If frame index has higher occurrence frequency than predetermined threshold, it is designated as an alert, triggering response generation. (cid:16) qKT dk (cid:17) Figure 4. Multiplexing Modeling of M4. is the streaming video, qi denotes the input query, ti indicates the generated token, ni denotes noise token which will be discarded from the KVCache. The streaming video KVCache is computed to trigger highlight spot index for the next response generation. Proactive interruption is facilitated through the computation of specific tokens designed for noise and stop signals. The parallel decoding takes mask strategy with dynamic KVCache to process multiple queries in one forward step. Interruption Detection Starting Detection In this process, we calculate the probability of the <bos> token as reference point. Drawing inspiration from Cai et al. [2], we utilize the reciprocal of perplexity as the threshold for identifying this special token. p(xn+k x1, x2, . . . , xn+k1) > β exp (S (p( x1, x2, . . . , xn+k1))) , (1) where β is scaling factor, S() is the entropy function. The threshold for noise detection is dependent on the perplexity of the model. When there is larger perplexity, the threshold is reduced, indicating the query is more like noise that does not need response. Stopping Detection Knowing when to stop is critical feature of an interactive system, which we consider essential for developing duplex system. Similar to noise detection, when presented with new query, we assess whether to halt the generation process by calculating the probability of the <eos> token in single forward pass. This decision is made using the same threshold employed in noise detection. Parallel Decoding We enhance inference speed to achieve real-time interactions through parallel decoding [12, 50, 55, 57]. As illustrated in Fig. 4, when the model is generating new tokens and new input query arises, the model decodes the next token alongside the original token using combination of causal masks, prefix masks, and block masks. Specifically, the causal mask is applied for the language model, the prefix mask pertains to the video context, and the block mask is designed to separate the decoding procedures of different queries in parallel. This method allows for the prediction of the next token while responding to new input query in single forward pass. To enhance the models resilience to noise, once the probability of the next token for the new input query is obtained, we evaluate whether it constitutes noise. If it does, the token is removed from the KVCache, allowing the continuation of the previous generation process. If the query is deemed legitimate, we proceed to decode the new query with the video context, while masking the interrupted sequence being decoded. This approach enables the processing of new queries at any forward step, thereby maintaining low latency. M4-IT Building on the aforementioned framework, we crafted small video-free synthetic instruction finetuning dataset, M4-IT, with the assistance of GPT-4o. M4-IT comprises four components: (i) the original instruction, which is data replay from the instruction data of our base model, in our work, we use the LLaVA-NeXT [54]; (ii) interleaved image-text instruction, which is created by reordering the question and image components of the original instruction; (iii) noise instruction, where GPT-4 is prompted to automatically generate statements that do not require response; and (iv) stop instruction, where GPT-4 is prompted to generate stop phrases for the stop instruction. Detailed descriptions regarding the instruction construction pipeline and prompts are provided in SM. An overall cost of $4.91 was incurred to construct the instruction dataset. 5. Experiments Setup We perform exhaustive evaluations of existing video-language models on OmniMMI. We have meticulously selected three categories of baseline models for our analysis: commercial VideoLLMs, open-source VideoLLMs, and LAVLMs, which encompass spectrum from visual to audio modalities. Within the open-source VideoLLMs, we further explore models that vary in scalability, context length, and real-time design capabilities. 5.1. Main Results Tab. 3 presents the evaluation results. In summary, our analysis yields three key observations. Table 3. Performance comparison of existing VideoLLMs on OmniMMI. The 1st, 2nd, 3rd of SG and MD tasks represent the cumulative accuracy up to and including these stages. The avg. indicates average accuracy across all data points. Models Commercial Video LLMs Gemini-1.5-Pro [36] GPT-4o [32] Open-source Video LLMs VideoChatGPT [28] VideoChat2 [20] Video-LLaVA [25] LLaMA-VID [24] MiniGPT4-Video [1] PLLaVA [49] LLaVA-NeXT-Video [56] ShareGPT4Video [4] LLaMA-VID-13B [24] PLLaVA-13B [49] PLLaVA-34B [49] LLaVA-NeXT-Video-34B [56] LongVA [54] LongVILA [51] LongLLaVA [39] VideoLLM-online [3] VideoLLaMB [42] IXC2.5-OL [] OmniLLMs VideoLLaMA2 [6] VITA [11] MiniOmini2 [47] M4 (ours) M4-a(ours) LLM Num Frames SG AP MD SI PA PT 1st 2nd 3rd avg. 1st 2nd 3rd avg. - - LLaMA-7B Vicuna-7B Vicuna-7B Vicuna-7B Mistral-7B Vicuna-7B Vicuna-7B Llama3-8B Vicuna-13B Vicuna-13B Yi-34B Yi-34B Qwen2-7B Llama3-8B Jamba-9B Llama3-8B Vicuna-7B Qwen2-1.5B Qwen2-7B Mistrl-87B Qwen2-0.5B Qwen2-7B Qwen2-7B 128 50 100 8 8 128 45 16 32 16 128 16 16 32 128 128 1 fps 32 / 1 fps 32 8 16 1 32 / 1 fps 32 / 1 fps 52.33 48.67 19.67 16.95 9.35 5.61 16.33 15. 43.00 39.50 35.00 34.33 16.26 15.57 7.14 7.65 12.00 12.33 38.50 17. 35.33 19.67 32.00 29.67 25.00 37.33 30.33 34.00 33.33 41.33 29.00 30.33 33.33 39.00 36.33 18.00 32.67 40.33 41.00 8.67 17.00 35.67 28.33 4.7 2.37 1.69 2.38 4.75 3.73 2.37 2.03 2.03 3.39 4.07 2.71 4.07 4.41 3.73 4.75 2.71 5.08 12.88 0.00 5.08 6.44 2. 1.87 0.93 0.00 0.00 1.87 0.93 0.93 0.93 0.00 0.00 0.00 0.00 0.00 0.93 0.00 0.00 0.00 0.00 0.00 0.00 0.93 1.87 0.00 3.33 2.33 1.67 2.33 4.00 3.33 3.00 2.00 1.33 2.67 3.67 2.67 3.33 4.33 3.33 4.67 2.33 4.03 10.33 0.00 4.67 5.67 2. 33.50 27.50 28.00 29.00 23.00 30.00 30.50 29.00 30.50 25.00 28.50 32.50 37.50 39.50 29.00 35.00 29.50 30.50 35.00 39.00 14.00 33.5 13.00 18.00 16.33 22.67 21.33 12.67 21.00 17.00 20.33 22.67 25.67 18.67 14.67 33.33 39.00 36.33 18.00 32.67 26.00 23.33 11.33 6.00 35.67 19. 3.11 3.81 5.19 3.80 2.08 3.46 2.08 3.46 3.46 5.54 4.50 2.08 4.07 4.41 3.73 4.75 2.71 4.50 4.15 3.11 1.00 6.44 3.11 0.51 0.51 1.02 0.51 0.51 0.00 0.51 0.00 0.51 2.04 0.00 0.51 0.00 0.93 0.00 0.00 0.00 1.52 0.51 1.52 0.00 1.87 0. 3.00 2.67 3.33 2.67 1.67 1.33 2.00 2.00 3.33 4.33 3.00 1.67 2.33 3.00 3.67 1.33 3.00 4.00 3.00 2.00 1.00 1.67 3.00 3.50 1.00 2.50 7.50 3.00 3.00 1.50 4.50 8.50 6.50 5.00 1.50 3.00 10.00 10.00 0.00 3.00 23.0 5.00 1.50 1.00 9.00 7. 0.50 0.00 ? 25.50 1.50 67.00 62.00 68.5 Challenges in Multi-Turn Tasks for Streaming Video. In the context of multi-turn tasks such as State Grounding and the Multi-Turn Dependency task, models exhibit notable decline in performance when required to handle more than single state or reasoning step. When tasked with managing three states or reasoning steps, the majority of open-source models fail to accurately address all inquiries. These results underscore the limitations of current methodologies in managing dynamic environments and performing multi-turn reasoning, despite their demonstrated efficacy in static video scenarios. Limitations in Audio-Visual Interaction. Although our benchmark is explicitly designed to evaluate visual-audio interaction, current open-source models equipped with both visual and audio inputs do not outperform those with solely visual inputs. This discrepancy highlights deficiency in the alignment of audio and visual features. Moreover, models with specialized speech training [11, 47] perform significantly worse than text input models, emphasizing the critical need for effective alignment and integration of multimodal inputs. Model Size vs. Input Length. Our experiments reveal that increasing model size does not necessarily enhance performance in streaming video tasks. Models with 7B parameters achieve performance levels comparable to those of larger models while maintaining greater efficiency. Conversely, models designed with long context capacities demonstrate improved performance in streaming tasks. Although we faced memory constraints with these models, we posit that balancing input length with memory efficiency is essential for effective understanding of streaming video content. 5.2. Analysis Proactive Alerting We ablate M4 with two backbones, Qwen2 and Llama3.1. Aside from accuracy, the precision and Intersection-over-Union (IoU) for all responses are computed throughout the entire video stream. We evaluate its PA ability under various settings, with results presented in Tab. 4. Further evaluation of the mixed model on the general video understanding task, VideoMME [10], yields scores of 51.74 for M4-Qwen2 and 43.52 for M4-Llama3.1. Findings Our analysis reveals significant performance gap between different LLMs. These results are generally consistent with the performance in Tab. 3. We conclude that leveraging model with strong general ability can enhance proactive capabilities without necessitating the construction of new data, which could potentially compromise the models performance [3]. Moreover, interleaved data appear to improve the models grounding ability, aligning with Table 4. Ablation Study Results for M4 on the Proactive Alerting Task. interleave: the tuning data comprising interleaved text and images. mix: using M4-IT. mean: the attention weight is computed by averaging all tokens from the query."
        },
        {
            "title": "Accuracy",
            "content": "M4-Qwen M4-Qwen-interleave M4-Qwen-mix M4-Qwen-mix-mean M4-Llama M4-Llama-interleave M4-Llama-mix M4-Llama-mix-mean 31.60 32.65 29.58 5.50 8.38 9.17 10.63 0.50 13.90 14.65 10.43 0.00 2.47 1.05 5.26 0. 25.00 26.00 25.50 6.50 10.50 10.00 11.50 0.50 Table 5. Performance on general video understanding task from VideoMME [10]."
        },
        {
            "title": "Short Medium Long General",
            "content": "LongVA LongVA (DataReplay) M4 (Interleave) M4 (Noise) M4 (Stop) M4 61.1 60.9 60.3 60.3 60.3 60.6 48.3 50.7 50.6 51.4 60.8 50.8 45.4 45.0 43.9 45.7 44.0 43.9 52.4 52.2 51.6 52.5 51.7 51.7 existing findings [17, 19]. Further investigation shows that the specific tokens in the query play crucial role in achieving meaningful attention weights for grounding, where the tokens associated with the assistant role have demonstrated superior effectiveness. To effectively demonstrate the efficacy of our M4, we visualize the attention weights in Fig. 5, in which strong correlation is presented between the elements within the relevant frames and the input query, validating the effectiveness of our approach. Figure 5. Attention feature map utilizes query as frames as K. The query consists of the last three tokens of the text query, while the key is represented by the mean-pooled frame. Figure 6. Performance on the Proactive Turn-taking task for noise and normal query over different scaling factor. Proactive Turn-taking To evaluate the efficacy of the instruction data, we applied the proactive turn-taking task to the M4, which was finetuned on M4-IT. Findings The results in Fig. 6 indicate that, after tuning with M4-IT, our method successfully handled all legitimate queries. When subjected to noise input queries, the model demonstrated resilience across broad range of the hyperparameter α. This suggests that our proposed instructional data effectively facilitates learning the format without compromising the models performance on standard queries. Influence on General Task To evaluate the effectiveness of our proposed M4 on general tasks, we further examine the models capability using the VideoMME benchmark for general video understanding. The results are listed in Tab. 5, where each configuration was trained on an identical dataset to ensure fair comparison. Findings Our findings suggest that the introduction of an interleaved image-text instruction format and stop instructions has the most significant impact on the results, primarily due to the heterogeneous nature of the data format. When all these instructional data types are combined, there is compromise in performance; however, it still surpasses the outcomes of the previously mentioned individual instruction types. This mixture achieves an effective balance, transitioning from general MLLM to interactive VideoLLMs without training on any video. 6. Conclusion In this work, we introduce OmniMMI, which evaluates the interactive capabilities of systems processing streaming video in open-world contexts. OmniMMI addresses challenges like streaming temporal state awareness and proactive reasoning with turn-taking. To advance real-time interactive reasoning, we propose novel framework, M4, which enhances proactive turn-taking and efficient streaming capabilities. Our evaluations of previous MLLMs reveal significant limitations in handling multi-turn tasks and modality alignment. As such, we call for future research on efficient designs for open-world interactive OmniLLMs. Acknowledgments The authors thank the reviewers for their insightful suggestions on improving the manuscript. This work presented herein is supported by the National Natural Science Foundation of China (62376031)."
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 7 [2] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. 6 [3] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video In Conference large language model for streaming video. on Computer Vision and Pattern Recognition (CVPR), pages 1840718418, 2024. 3, 5, 7 [4] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 7 [5] Qirui Chen, Shangzhe Di, and Weidi Xie. Grounded multi-hop videoqa in long-form egocentric videos. ArXiv, abs/2408.14469, 2024. 3 [6] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7, [7] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 1 [8] Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for realtime dialogue. arXiv preprint arXiv:2410.00037, 2024. 1 [9] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. 1 [10] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. ArXiv, abs/2405.21075, 2024. 1, 3, 7, 8 [11] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. Vita: Towards open-source interactive omni multimodal llm. ArXiv, abs/2408.05211, 2024. 3, 5, 7, 2 [12] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of LLM inference using lookaIn International Conference on Machine head decoding. Learning (ICML). OpenReview.net, 2024. 6 [13] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1899519012, 2022. 3 [14] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehenarXiv preprint sive understanding of multi-shot videos. arXiv:2312.10300, 2023. 3 [15] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 13504 13514, 2024. [16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: toward spatio-temporal reasoning In Conference on Computer in visual question answering. Vision and Pattern Recognition (CVPR), 2017. 3 [17] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multiimage instruction tuning. ArXiv, abs/2405.01483, 2024. 8 [18] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34: 1184611858, 2021. 3 [19] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. ArXiv, abs/2407.07895, 2024. 8 [20] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 7 [21] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Ocean-omni: To understand the world with omni-modality. arXiv preprint arXiv:2410.08565, 2024. [22] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, et al. Omnibench: Towards the future of universal omnilanguage models. arXiv preprint arXiv:2409.15272, 2024. 1, 3 [23] Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, and Jiaqi Wang. Ovo-bench: How far is your video-llms from real-world online video understanding?, 2025. 3 [24] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: In An image is worth 2 tokens in large language models. European Conference on Computer Vision, pages 323340. Springer, 2025. 7 [25] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 7 [26] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. CoRR, abs/2411.03628, 2024. [27] Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. LanarXiv preprint guage model can listen while speaking. arXiv:2408.02622, 2024. 3, 5 [28] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 7 [29] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [30] Vishvak Murahari, Carlos Jimenez, Runzhe Yang, and Karthik Narasimhan. Datamux: Data multiplexing for neural networks. Advances in Neural Information Processing Systems (NeurIPS), 35:1751517527, 2022. 3, 5 [31] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. ArXiv preprint, 2023. 3 [32] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 5, 7, 2 [33] Rui Qian, Xiao wen Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. ArXiv, abs/2405.16009, 2024. 3 [34] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 1, 3 [35] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video In Conference on Computer Vision and Pattern analysis. Recognition (CVPR), pages 12071216, 2019. [36] Gemini Team. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 7, 2 [37] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2024. 1 Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 3 [39] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 7 [40] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. 3 [41] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. ArXiv, abs/2406.16338, 2024. [42] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurarXiv preprint arXiv:2409.01071, rent memory bridges. 2024. 1, 3, 7 [43] Bo Wu and Shoubin Yu. Star: benchmark for situated reasoning in real-world videos. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [44] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved ArXiv, abs/2407.15754, video-language understanding. 2024. 3 [45] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Conference on Computer Vision and temporal actions. Pattern Recognition (CVPR), 2021. 3 [46] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. 1, 3, 5 [47] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards opensource gpt-4o model with vision, speech and duplex. arXiv preprint arXiv:2410.11190, 2024. 1, 3, 5, 7, [48] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and moIn Association for Computing Machinerys Annual tion. Conference on Multimedia (ACM MM), 2017. 3 [49] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 7 [50] Wang Xu, Shuo Wang, Weilin Zhao, Xu Han, Yukun Yan, Yudi Zhang, Zhe Tao, Zhiyuan Liu, and Wanxiang Che. Enabling real-time conversations with minimal training costs. ArXiv, abs/2409.11727, 2024. 6 [51] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 7 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin [52] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI Conference on Artificial Intelligence (AAAI), 2019. 3 [53] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language In Conference on Computer Vision and Pattern models. Recognition (CVPR), pages 55795588, 2021. 1 [54] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 3, 6, 7 [55] Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, Yuanwei Xu, Weilin Zhao, Maosong Sun, and Zhiyuan Liu. Beyond the turn-based game: Enabling real-time conversations with duplex models. ArXiv, abs/2406.15718, 2024. 6 [56] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 7 [57] Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, and Maosong Sun. Ouroboros: Generating longer drafts phrase by phrase for faster speculative decoding. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [58] Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, and Zheng-Jun Zha. Streaming video In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 14602 14612, 2023. 3 [59] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 1, 3 [60] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia CoRR, Schmid. Streaming dense video captioning. abs/2404.01297, 2024. 3, 5 OmniMMI: Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Audio Adaption Analysis In this section, we further explore the adaptation of our methods to audio speech input. To adapt M4 to receive audio queries, we fine-tuned it on randomly selected subset of the VoiceAssistant dataset [46], which comprises 30,000 audio instructions. To ensure fair comparison, we maintained the same hyperparameters and other settings as those used in the tuning of M4. The results are presented in Table 6. Our findings indicate that tuning the model on purely audio instruction data, without incorporating visual data, does not enhance its proactive turn-taking ability. Consequently, we converted the queries in M4-IT to speech using CosyVoice and mixed them with the VoiceAssistant subset used during the tuning of M4-a. After integrating this audio data, we achieved score of 68.5 on the PT task. Overall, the introduction of audio instruction data still limits the performance of tasks requiring both visual and audio inputs. We believe this limitation arises from the lack of mixed visual and audio data during the training phase. In future work, we aim to enhance the models audio understanding capabilities by incorporating more high-quality multimodal data. B. Highlight Spot Algorithm In this section, we present the pseudo-code of our proposed training-free highlight spot algorithm, as illustrated in Algorithm 1. For any transformer-based model, incoming streaming video frames are stored in the KVCache to avoid redundant computations. Subsequently, we compute the attention weights from the models final layer using the text query as the key and the video as the value. We then identify and save the frame indices whose attention weights exceed threshold, determined by the mean and variance of the previous attention weights. These indices are labeled as consistently salient frames, signifying the frames that need to be highlighted. The consistency threshold is hyperparameter, which is set to 4 in our experiments. Furthermore, we introduce an initial latency step to mitigate the challenges associated with calculating the mean and variance; in practice, this latency step is set to 2. C. Single Question Analysis of Multi-turn Dependency Reasoning In this section, we detail the accuracy of each step in the multi-turn dependency reasoning task. The results are presented in Table 7. Unlike the results presented in Table 3, Algorithm 1 Highlight Spot Require: Video stream V, query q, threshold γ, Gaussian factor α 1: highlight spot.init() 2: for all frame in do 3: 4: 5: 6: 7: 8: KVCache.update(WKv, WV v) attn SelfAttn(v q, KVCache) (µ, σ) std mean(attn) δ µ + α σ cands {t; attn[t] > δ} for all ct in cands do 9: 10: 11: 12: 13: 14: ct highlight spot.get(t) + 1 highlight spot.update(i, ci) end for if highlight spot.heap is not empty then (i, c) highlight spot.peek() if req > γ then send(i) 15: 16: 17: 18: end for end if end if this experiment focuses solely on the accuracy of individual reasoning steps. Generally, we observe decline in accuracy as the number of steps increases. However, in certain instances, accuracy at later step exceeds that of previous one. We attribute this anomaly to potential hallucinations generated by the language models. Overall, there is significant drop in accuracy across successive steps, underscoring the importance of multi-step reasoning in evaluation. This approach helps to mitigate errors introduced by language models, demonstrating the necessity of step-by-step evaluation process. D. Single Question Analysis of Dynamic State"
        },
        {
            "title": "Grounding",
            "content": "In this section, we extend our analysis of the Dynamic State Grounding task by examining the performance on each individual question. The results, as detailed in Table 8, indicate notable decline in performance as the number of states increases. This decline can be attributed to the increased length of the video context and dialogue history, which complicates the process of dynamically grounding the current state to derive the correct answer. Furthermore, our analysis did not reveal significant performance differences across different models at the initial state. However, Table 6. Performance comparison of existing OmniLLM on OmniMMI. The 1st, 2nd, 3rd of SG and MD tasks represent the cumulative accuracy up to and including these stages. The avg. indicates average accuracy across all data points."
        },
        {
            "title": "LLM",
            "content": "Commercial Video LLMs Gemini-1.5-Pro [36] GPT-4o [32] - - OmniLLMs VideoLLaMA2 [6] VITA [11] MiniOmini2 [47] Qwen2-7B Mistrl-87B Qwen2-0.5B"
        },
        {
            "title": "Num\nFrames",
            "content": "128 50 8 16 1 M4 (ours) M4-a(ours) Qwen2-7B Qwen2-7B 32 / 1 fps 32 / 1 fps SG AP MD SI PA PT 1st 2nd 3rd avg. 1st 2nd 3rd avg. 52.33 48.67 19.67 16.95 9.35 5.61 16.33 15.00 43.00 39. 35.00 34.33 16.26 15.57 7.14 7.65 12.00 12.33 38.50 17.00 41.00 8.67 17. 35.67 28.33 12.88 0.00 5.08 6.44 2.37 0.00 0.00 0.93 1.87 0.00 10.33 0.00 4. 5.67 2.00 35.00 39.00 14.00 33.5 13.00 23.33 11.33 6.00 35.67 19.33 4.15 3.11 1. 6.44 3.11 0.51 1.52 0.00 1.87 0.51 3.00 2.00 1.00 1.67 3.00 5.00 1.50 1. 9.00 7.50 25.50 1.50 67.00 62.00 68.5 Table 7. Multi-turn Dependency Reasoning Table 8. Dynamic State Grounding"
        },
        {
            "title": "Models",
            "content": "Step=1 Step=2 Step=3 Overall"
        },
        {
            "title": "Models",
            "content": "State=1 State=2 State=3 Overall"
        },
        {
            "title": "Commercial Video LLMs",
            "content": "Gemini-1.5-Pro GPT-4o Open-source Video LLMs VideoChatGPT VideoChat2 Video-LLaVA LLaMA-VID MiniGPT4-Video PLLaVA LLaVA-NeXT-Video ShareGPT4Video LLaMA-VID-13B PLLaVA-13B PLLaVA-34B LLaVA-NeXT-Video-DPO-34B LongVA LongVILA LongLLaVA VideoLLM-online VideoLLaMB OmniLLMs VideoLLaMA2 VITA MiniOmini2 52.33 48.67 34.24 31.53 36.45 20.56 16.33 15.00 Gemini-1.5-Pro GPT-4o"
        },
        {
            "title": "Commercial Video LLMs",
            "content": "18.00 16.33 22.67 21.33 12.67 21.00 17.00 20.33 22.67 25.67 18.67 14.67 20.67 22.33 26.33 11.67 18.67 23.33 11.33 6.00 19.33 13.49 13.15 13.49 15.22 6.57 13.49 10.03 15.57 14.88 17.80 17.30 14.53 16.27 14.19 18.69 7.27 13. 15.92 12.80 3.11 10.73 11.22 12.24 16.33 13.78 8.67 17.35 10.71 14.80 14.29 16.84 10.20 12.24 13.78 14.29 20.41 10.71 17.86 18.78 8.63 2.03 12. 3.00 2.67 3.33 2.67 1.67 1.33 2.00 2.00 3.33 4.33 3.00 1.67 2.33 3.00 3.67 1.33 3.00 5.00 2.00 1.00 1.67 Open-source Video LLMs VideoChatGPT VideoChat2 Video-LLaVA LLaMA-VID MiniGPT4-Video PLLaVA LLaVA-NeXT-Video ShareGPT4Video LLaMA-VID-13B PLLaVA-13B PLLaVA-34B LLaVA-NeXT-Video-DPO-34B LongVA LongVILA LongLLaVA VideoLLM-online VideoLLaMB Open-source Video LLMs VideoLLaMA2 VITA MiniOmini2 M4 35.00 34.33 35.33 19.67 32.00 29.67 25.00 37.33 30.33 34.00 33.33 41.33 29.00 30. 33.33 39.00 36.33 18.00 32.67 41.00 8.67 17.00 35.67 37.02 33.56 17.97 14.23 16.27 13.56 15.25 13.56 12.20 13.22 14.24 13.90 14.24 11.19 15.59 16.95 11.53 13.56 14. 26.78 8.14 14.92 13.22 38.78 37.24 10.28 6.54 11.21 7.48 14.02 10.29 6.54 10.28 6.54 12.15 10.28 5.61 8.41 14.02 7.48 5.61 10.28 10.28 2.80 10. 6.54 12.00 12.33 3.33 2.33 1.67 2.33 4.00 3.33 3.00 2.00 1.33 2.67 3.67 2.67 3.33 4.33 3.33 4.67 2.33 10.33 0.00 4.67 5. the performance gap widens as the number of states increases, underscoring the importance of models ability to handle longer contexts while maintaining effective grounding capabilities. E. Annotation Details E.1. Raw Video Data Collection To enhance our dataset, we specifically collect data from YouTube, concentrating primarily on videos that are particularly commonly useful in our real-life. We also focus on the videos which are in content involving personal introductions and interpersonal interactions. E.2. Annotation Tool The front-end interface for human annotation is depicted in Figure 7. In this interface, each question or statement is associated with the most relevant time span, which serves either as part of the label or as an aid for subsequent annotation tasks. E.3. Annotation Guidelines To ensure that annotators produce high-quality annotations that align with our specified standards, we provide detailed guidelines, including examples of various question types. You are sophisticated AI designed to simulate human-like conversation by generating noise. This noise consists of naturally flowing statements that mimic the users perspective. Review the users questions and the assistants responses carefully. Using this information, create coherent declarative statements that reflect the users voice. These should resemble everyday human dialogue and do not require response from the assistant. Ensure your output is in the form of declarative sentences and avoid questions. Keep the noise brief and in casual, conversational English. But do NOT need response Figure 7. The Front-End Interface for Human Annotation F.2. Stop Words Category Object State Spatial Relations Dynamic Spatial Relations Action State Scene State Human Object Interaction Human Human Interaction Group Dynamics Emotional State Audio/Speech State Question How many objects are in the scene? How many people are in the room? What is the color of the car? Is the door open or closed? Where is the cat relative to the chair? Is the person walking towards or away from the camera? Where is the ball relative to the player? What is the person doing? What activity is happening in the scene? Is the room well-lit or dim? What is the weather like? Is the street busy or quiet? What is the context of the scene? Is the person holding the book? Are the two people shaking hands? What is the interaction between the two characters? How are the group members interacting? What is the persons emotional state? What does the speaker mentioned? Table 9. Annotation hints for annotators including category and example question. The list of hints is demonstrated in Table 9. We compile set of frequently used stop words to incorporate into our instructional data, thereby serving as the designated stop words: Thats good point, and, Let me stop you there, Just second, dont mean to be rude, but, If could interject, Pardon me, but, Sorry to interrupt, Before you continue, Can we pause for moment?, May add something here?, apologize for cutting in, Could stop you for second?, Id like to add, Could clarify something?, have quick question, This reminds me of, Let me add to that, Can share my thoughts?, Hold on moment, One moment, please, Allow me to explain, Excuse me, Can jump in for moment?, see what you mean, but, think its important to mention G. M4 Implementation Details"
        },
        {
            "title": "Hyperparam",
            "content": "α β γ Model Max Length Learning Rate Warmup Ratio Per Device Batch Size Gradient Accumulation Steps Epoch M4 2 0.2 4 32000 1e-5 0.03 1 4 1 Table 10. Hyperparameters for M4. F. M4-IT Construction Details F.1. Noise data prompt We employ GPT-4o to autonomously generate noise data for the purpose of instruction tuning. The prompt utilized for the generation of noise data is detailed below. In practice, we conduct the training process using four Nvidia A800 GPUs, which requires approximately one hour to fine-tune the model. Table 10 presents detailed account of the hyperparameters employed during both the training and inference procedures."
        }
    ],
    "affiliations": [
        "Beijing Institute for General Artificial Intelligence",
        "State Key Laboratory of General Artificial Intelligence",
        "Wangxuan Institute of Computer Technology, Peking University",
        "X-LANCE Lab, Shanghai Jiao Tong University"
    ]
}