{
    "paper_title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
    "authors": [
        "Yujie Zhao",
        "Hongwei Fan",
        "Di Chen",
        "Shengcong Chen",
        "Liliang Chen",
        "Xiaoqi Li",
        "Guanghui Ren",
        "Hao Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 0 4 9 1 . 2 1 5 2 : r Real2Edit2Real: Generating Robotic Demonstrations via 3D Control Interface Yujie Zhao1,2 Hongwei Fan1,2 Di Chen3 Shengcong Chen3 Liliang Chen3 Xiaoqi Li1,2 Guanghui Ren3 Hao Dong1,2 1CFCS, School of Computer Science, Peking University 2PKU-AgiBot Lab 3AgiBot https://real2edit2real.github.io/ Figure 1. The overview of Real2Edit2Real. Real2Edit2Real generates diverse robotic demonstrations, featuring 10-50 improvement on data efficiency compared with real-world collection across four tasks. sistent depth, which serves as reliable condition for synthesizing new demonstrations. Finally, we propose multiconditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 15 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50. Moreover, experimental results on height and texture editing demonstrate the frameworks flexibility and extensibility, indicating its potential to serve as unified data generation framework."
        },
        {
            "title": "Abstract",
            "content": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, framework that generates new demonstrations by bridging 3D editability with 2D visual data through 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically con- *: Equal contributions. : Corresponding author. 1 1. Introduction Recent advances in robotics have demonstrated remarkable progress in visuomotor policy learning, driven by largescale datasets and powerful model architectures such as the Diffusion Policy [5, 6] and Vision-Language-Action (VLA) models [1, 2, 4, 16, 21]. However, the performance of these methods heavily relies on the availability of diverse and high-quality demonstrations. Spatial generalization, in particular, remains bottleneck for policy robustness [39, 55]. In manipulation tasks where objects are randomly arranged in space, achieving high success rate typically requires large amount of data to cover diverse spatial configurations, and increases substantial data collection costs. To alleviate the burden of repetitive data collection, an efficient strategy is to synthesize new demonstrations from limited existing data. MimicGen-style works [9, 11, 18, 33] proposed segmenting demonstration trajectories according to object interactions, and then transforming and interpolating these object-centric segments to generate new trajectories that adapt to novel object arrangement configurations. Real2Render2Real [57] synthesized demonstrations from human manipulation video through pose tracking and trajectory interpolation. Since these approaches render robotic videos within graphics engine, they inevitably face the visual and physics gaps, which remain significant challenge in robotics. Moreover, they require constructing assets for the manipulated objects, which prevents them from directly augmenting an existing demonstration. DemoGen [55] augmented real-world point-cloud demonstrations through 3D editing and enhanced spatial generalization of 3D Diffusion Policy [58], but it cannot be applied to RGB images and 2D policies, which remain the dominant paradigm in current robot learning and deployment. Consequently, to our best knowledge, there is no existing method to rapidly scale up real-world 2D multi-view manipulation videos with novel trajectories, while preserving both visual realism and correct interactions. introduce this To mitigate Real2Edit2Real, demonstration generation framework that bridges the gap between 3D editability and 2D visual data via 3D control interface, achieving spatially augmented multi-view robotic demonstrations. As shown in Figure 1, Real2Edit2Real does not rely on simulation engines or digital assets, and directly generates multi-view manipulation data from raw RGB demonstrations, featuring novel object placements and corresponding new trajectories, which can be used for VLA training. Our key insight is that depth inherently encodes robot motion and object interactions, making it natural interface between 3D modalities and 2D observations. Specifically, our proposed (1) Metric-scale framework works with three modules: geometry reconstruction of robot manipulation scenarios, where we propose hybrid training paradigm that levergap, we research ages real and simulated data to co-train feed-forward (2) Depth-reliable spatial 3D reconstruction model. editing which combines point-cloud editing with trajectory planning to generate feasible manipulation trajectories while geometrically correcting the robots poses, thereby producing kinematically consistent depth maps that serve as reliable control signals for subsequent video generation. (3) 3D-Controlled video generation for multi-view consistent demonstrations, where we construct video generation model conditioned on depth, together with edges, actions, and ray maps. With Real2Edit2Real, we can edit 2D videos through unified 3D control interface, which facilitates data augmentation for thereby robotic manipulation, enhancing the robustness of downstream policies. To evaluate the quality and efficiency of generated demonstrations, we conduct experiments on four real-world robotic manipulation tasks, covering single-arm to dualarm manipulation. Experimental results indicate that policies trained on data generated from as few as 15 source demonstrations can achieve comparable or higher success rates than those trained on 50 real-world demonstrations, improving data efficiency by up to 1050. Additionally, Real2Edit2Real supports extended editing such as object height and background texture, demonstrating the frameworks flexibility and extensibility, and suggesting its potential as unified robotic data generation framework. 2. Related Work 2.1. Demonstration Generation Due to the difficulty and cost of collecting robotic demonstration data, generating numerous demonstrations from zero or one demonstration has been proposed to rapidly extend robotic data. One line of works [4, 7, 10, 37, 45] use simulation engines to automatically collect demonstrations with pre-defined tasks and motion planning. However, the lack of real-world interaction leads to the Sim2Real gap. Another line of works [9, 11, 18, 25, 33, 5557] focus on generating from one or few collected demonstrations. MimicGen family [9, 11, 18, 25, 33] generates from one demonstration with carefully designed task segments. DemoGen [55] combines MimicGen-style generation with point cloud editing. However, it is incompatible with the widely used setting of multi-view RGB cameras for visuomotor policy training. Real2Render2Real [57] and RoboSplat [56] use 3D Gaussian Splatting (3DGS) [20] with trajectory generation to reduce the gaps in visual fidelity and interaction reality. These works expose two disadvantages. First, the rendering-based techniques that they used still bring the visual domain gap, limiting the Gen2Real performance. Second, the dense image captures that 3DGS requires restrict the scalability of generation. By bridging point-cloud-based demonstration editing and 3D-controlled 2 video generation, Real2Edit2Real jointly achieves scalability, visual quality, and real-world interaction of generated demonstrations in one framework. 2.2. Geometry Reconstruction Reconstructing the detailed environmental geometry is the key to generating controllable and multi-view consistent demonstrations. Early methods such as NeuS [48] and 2DGS [12] use radiance fields and gaussian splatting as 3D representations, which require dense image captures and minute-level post-optimization, both restricting their application scope and efficiency. Recently, feed-forward geometry reconstruction [19, 24, 32, 47, 50, 51, 60] unlocks sparse-view reconstruction in seconds, bringing the potential of recovering geometry from robot camera views [27, 36, 52]. However, directly using the feed-forward model suffers from the domain gap, including wrong camera pose and misaligned metrics between pretrained data and realworld manipulation scenarios. We propose an effective training recipe of VGGT [47] on robotic data to both utilize multi-view consistent depth maps and camera poses from simulated data, and precise geometry metrics from real-world data. The aligned geometry prediction ensures high-quality point cloud, which in turn enables the generation of reliable depth maps for demonstration synthesis. 2.3. Video Generation in Robotics Recent progress in video generation [3, 8, 23, 34] has improved conditional and temporal consistency, empowering several downstream applications in robotics. First, predicting video generation can serve as an enhancement [14, 31, 43, 61] to regular policy learning by jointly predicting future action and observation in one model. Second, actionconditioned video generation [17, 26, 40, 46] performs as the policy evaluator or realistic simulation environment, which receives future action and feeds generated observation back to the pretrained policy model. Third, learning multi-modal conditioned video generation [28, 30, 36, 44, 54] and editing these conditions during inference generalizes the original demonstration across diverse camera poses and textures. However, these works neglect spatial generalization, which is also fundamental for scaling robotic data. In contrast, Real2Edit2Real generates spatially augmented multi-view demonstrations and can be easily combined with first-frame editing to support beyond. 3. Method 3.1. Overview We consider humanoid robot scenario with multi-view videos from the head, left wrist, and right wrist cameras. Then we can formulate our problem as follows: = {Oi}N i=1 = Real2Edit2Real(O, K) (1) 3 , h, r)}N Here, = (I, q, a) consists of multiview videos = {(I t=1, joint angles q, and actions from one source demonstration. represents the robot kinematic model (URDF) and camera parameters, where = (Krobot, Kcam). With our proposed Real2EditReal framework, the source demonstration can be augmented to large set of demonstrations with novel object spatial configurations and trajectories. As shown in Figure 2, our framework consists of three main components: metric-scale geometry reconstruction, depth-reliable spatial editing, and 3D-controlled video generation. In Sec. 3.2, we first introduce hybrid training paradigm that leverages both real and simulated data to enhance the capability of the reconstruction model in robotic environments, which predicts depth maps and camera poses from the source demo as Eq. 2, where t=1 and = {(T = {(Dt )}M r)}N h, Dt h, , Dt , t=1. D, = R(I) (2) In Sec. 3.3, we detail the pipeline of depth-reliable spatial editing, shown in Eq. 3, where we synthesize novel trajectories based on motion planning and point-cloud editing, while correcting the robots poses to obtain physically consistent depth maps that serve as reliable control signals. {(Di, Ti, qi, ai)}N i=1 = E(D, T, K, q, a) (3) In Sec. 3.4, we propose multi-view video generation model G, which produces the complete robotic manipulation video from the first frame controlled by depth, together with Canny edges C(), actions, and ray maps, as Eq. 4. {Ii}N i=1 = {G(Di, Ti, ai, C(Di))}N i=1 (4) 3.2. Metric-scale Geometry Reconstruction Motivated by the fact that 3D data affords greater flexibility for editing than 2D imagery, we initially conduct geometric reconstruction of the source demonstrations. To achieve scale-aware geometry reconstruction and improve its accuracy in humanoid robot scenes, which only include three cameras from the head and both wrists, we propose hybrid training paradigm that combines real and simulated data to fine-tune VGGT [47], enhancing metric-scale depth map and camera pose prediction in humanoid scenarios. Camera Pose. Camera poses obtained via handeye calibration in real-robot settings are susceptible to mechanical tolerances, calibration drift, and kinematic inaccuracies, leading to misalignment. Conversely, simulated data offers perfectly accurate and metrically consistent camera poses, since the virtual sensors are derived directly from the robots standardized URDF model. Therefore, we only use simulated data to supervise the camera loss: Lcamera = (cid:80) ), where we use L1 loss because v{h,l,r} L1( ˆT sim , sim of the precise ground truth. Figure 2. The framework of Real2Edit2Real. (1) Reconstruct metric-scale geometry from multi-view observations. (2)Synthesize novel trajectories with reliable depth rendering. (3) Generate demonstrations controlled by temporal depth signals. Depth Map. Real-world datasets capture metric-scale geometry through depth sensors, but the acquired depth maps are often highly noisy due to sensor limitations, In contrast, reflective or textureless surfaces. simulated data provides noise-free and geometrically the object and scene scale precise depth maps, but may deviate from real-world distributions. To leverage the complementary strengths of both fields of data, we compute the depth loss using mixed method: Ldepth = (cid:80) v{h,l,r}((Lconf(M( ˆDreal )) + Lconf( ˆDsim )). Lconf is the depth loss with uncertainty used in [47], and we use threshold mask to filter invalid noise in real depth maps. ), M(Dreal , Dsim v In addition, the simulation data also supervises the point map loss: Lpointmap = (cid:80) ). The total training loss is shown as Eq.5, where we apply the weight λ of 10 to Lcamera to ensure that its magnitude is comparable to other losses, which stabilizes optimization. v{h,l,r} Lconf( ˆP sim , sim = λLcamera + Ldepth + Lpointmap (5) 3.3. Depth-reliable Spatial Editing Based on point-cloud editing and motion planning, we can synthesize novel object placements and corresponding manipulation trajectories in 3D space. To obtain reliable depth from the edited point clouds, we improve the spatial editing pipeline with techniques such as background inpainting and depth filtering. Crucially, we introduce robot pose correction during spatial editing, ensuring the resulting depth maps are consistent with the robots kinematics. Trajectory Synthesis. Inspired by the previous work [55], we decompose point-cloud demonstration into two types of segments: the motion segment, where the robot moves freely in space, and the skill segment, where the robot interacts with objects. Given transformation that relocates an object, we apply the same transformation to the robots point cloud in the corresponding skill segment. This enAlgorithm 1 Depth-reliable Spatial Editing Pipeline Require: Source point clouds robot, object, bg, joint states Q, action trajectory A, camera poses . Ensure: Novel depth sequence D, joint states Q, action trajectory A, camera poses . function RENDERDEPTH(P, , Q) D1 ProjectPointCloud(P, ) D2 RenderLinkDepth(Q, ) return Merge(D1, D2) end function Random Sample Object Transform R44 list(), list(), list(), list() // Motion Segment start A0, A for in Motion do end TAend, start, end, t) , Qt) , t MotionPlan(A FK(P robot arm robot TP object TtP ee Tt, arm ee D RenderDepth(P Q bg , TtTt, ) , A , TtTt end for // Skill Segment for in Skill do , Qt) IK(TAt) FK(P robot robot arm object T(P ee arm ee D RenderDepth(P Q t ) bg , TTt, ) , A TAt, TTt end for return D, Q, A, sures that the robot-object relation remains consistent with the original demonstration, thereby preserving realistic inFigure 3. The framework of 3D-Controlled Video Generation. We utilize depth as the 3D control interface, in conjunction with edges, actions, and ray maps, to guide the generation of multi-view demonstrations. teractions. The new motion segment is generated through motion planning [42]. Depth Projection. Since the camera is rigidly attached to the robots end-effector, we apply the same transformation to the camera pose to obtain its updated position after editing. This allows us to project the edited point cloud to generate the corresponding depth map. However, the projected depth maps often suffer from holes and noise due to changes in object positions and varying camera distances, which can cause sparse or missing regions. We mitigate these artifacts through background [49] inpainting and depth filtering. Robot Pose Correction. major challenge arises from incorrect robot poses, as the previous editing process treats the entire robot as rigid body. Instead, only the endeffector should be transformed, and the remaining arm must be realigned to preserve kinematic validity. To ensure the reliability of depth maps, we segment out the original robot links with URDF and source joint states, and re-render the arm depth with synthesized actions. This process yields physically plausible robot configurations and produces accurate depth observations without rigid-body artifacts. To summarize the overall procedure, we illustrate in Algorithm 1 an example pipeline consisting of one motion segment followed by one skill segment. 3.4. 3D-Controlled Video Generation After obtaining the edited depth, action, and camera pose sequences from the 3D editing pipeline described in Section 3.3, we then convert them into 2D visual observations required for policy training. We propose 3D-conditioned video generation model that, starting from the first frame, synthesizes novel robot manipulation videos with realistic visual appearance, multi-view consistency, and physically plausible interactions. As shown in Figure 3, we build Transformer-based video generation model followed by [26, 34, 35]. The model works with three key designs: dual-attention mechanism, depth control interface, and smooth object relocation. Dual-attention Mechanism. The dual-attention mechanism consists of intra-view attention and cross-view attention. Intra-view attention performs self-attention over the tokens of each individual view, capturing detailed spatial context within that view. Cross-view attention, on the other hand, computes self-attention across all views simultaneously, enabling the model to utilize the multi-view correspondence. This dual-attention design not only facilitates multi-view consistency in the generated videos by allowing interactions of visual features across different scale contexts, but also significantly reduces computational cost compared to applying global attention at every layer. Depth Control Interface. We use depth as the 3D control interface of video generation. Specifically, we concatenate the depth map with the image latent representations and feed them jointly into the transformer backbone, enabling the model to condition video generation on 3D structural cues. This design ensures the synthesized robotic demonstrations remain consistent with the geometry information in the depth sequence, which encodes robot motion and object interactions. In addition, we incorporate auxiliary conditioning signals, including Canny edges, action maps, and ray maps, which further sharpen object boundaries, improve motion grounding, and enhance multi-view consistency, respectively. Overall, these structured controlling signals provide strong 3D inductive biases, enabling more realistic manipulation behaviors in the generated videos. Smooth Object Relocation. With the conditions control, the model is able to generate the manipulation video from the first frame, but how to relocate the objects in the first # Demo Real 10 Real 20 Real 50 Real 1 Gen 200 Real 2 Gen 200 Real 5 Gen 200 Mug to Basket Pour Water Lift Box Scan Barcode Total Go-1 8 / 20 12 / 20 14 / 20 14 / 20 15 / 20 17 / 20 π0. 8 / 20 14 / 20 13 / 20 15 / 20 15 / 20 18 / 20 Go-1 5 / 20 7 / 20 8 / 20 12 / 20 10 / 20 12 / 20 π0. 1 / 20 2 / 20 8 / 20 10 / 20 10 / 20 12 / 20 Go-1 11 / 20 12 / 20 15 / 20 12 / 20 14 / 20 16 / 20 π0. 13 / 20 15 / 20 17 / 20 10 / 20 17 / 20 18 / 20 Go-1 5 / 20 8 / 20 12 / 20 14 / 20 17 / 20 18 / 20 π0. Go-1 π0.5 4 / 20 5 / 20 11 / 20 11 / 20 14 / 20 17 / 20 36.3% 32.5% 48.8% 45.0% 61.3% 61.3% 65.0% 57.5% 70.0% 70.0% 78.8% 81.3% Table 1. Success rates of Real2Edit2Real on four real-world manipulation tasks and two VLA policies Go-1 and π0.5. Both policies trained on data generated from only 15 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50. frame remains difficult. To make full use of the depthcontrolled video generation model, we convert the object relocation to smooth transformation, where we interpolate both the translation and rotation of objects during spatial editing to synthesize object moving trajectories before manipulation starts. Through this smooth relocation, we convert image editing to video generation and process the object relocation and demonstration generation together by 3D-controlled video generation, achieving unified and efficient generation framework. 4. Experiments To demonstrate the effectiveness of our data generation framework, we present the following experiments. In Section 4.2, we evaluate its impact on real-world policy learnIn Section 4.3, we ing across four manipulation tasks. showcase the flexibility of our proposed framework through height and texture editing. In Section 4.4, we conduct ablations to validate the necessity of each module and our key designs. Finally, in Section 4.5, we provide visualizations of our novel demonstration generation. 4.1. Implementation Details Real2Edit2Real. For Metric-VGGT, we sample 40K frames from the Agibot-DigitalWorld dataset [59] as simulation training data and collect 100K real robot data with depth sensors as real training data. We full-finetune VGGT [47] on 8 H100 GPUs for 150K iterations, 20 hours with the learning rate 2e-4 and backbone learning rate 2e5. For the video generation model, we sample 7K episodes from 64 tasks in the AgiBot-World dataset [4] as training data. We train the video generation model by fine-tuning the backbone of GE-Sim [26](based on Cosmos-Predict2B [59]) on 8 H100 GPUs for 20K iterations, 60 hours with the learning rate 1e-4, batch size 16. With parallelization across 8 H100 GPUs, the average generation time for 20second, 30-FPS episode is 48.6 seconds. Please refer to the supplementary materials for more details. Hardware Setup. We use the Agibot Genie G1 robot with its internal motion API. Three RGB cameras are mounted on its head, left wrist, and right wrist. The workspace is 50cm 40cm rectangular area on the white desktop. Please refer to the supplementary materials for more details. VLA Policy. We conduct experiments on two VLA policies: Go-1 [4] and π0.5 [16]. For Go-1, we only finetune the action expert while keeping the backbone frozen because we use the same embodiment as its pretrained data. The action is the 6D end-effector pose. For π0.5, due to embodiment mismatch, we perform full finetuning. The action is the 7-DoF joint angles. Each training typically consists of 10K iterations; for the smaller training data cases, we proportionally reduce the iteration count to 100 epochs. Both policies are trained on 8 H100 GPUs for 2-4 hours. 4.2. Gen2Real Policy Learning Tasks. As shown in Figure 1, we conduct real-robot experiments on four tasks, covering from single-arm to dual-arm manipulation: Mug to Basket: The robot uses its right arm to grasp the mug and place it stably inside the basket. Pour Water: The robot uses its left arm to pick up the kettle and pour water into the paper cup by aligning the spout with the cup. Lift Box: The robot grasps both sides of the box using its two arms and lifts it. Scan Barcode: The robot grasps snack with its left hand and barcode scanner with its right hand, and scans the barcode by aligning the scanner with it. Settings. For real-world data, we collect demonstrations via teleoperation by placing objects in diverse configurations that uniformly cover the workspace, including variations in both position and orientation. For generation experiments, we randomly sample specified number of source demonstrations from the collected data and apply randomized object relocations around their original poses. This procedure yields 200 synthesized demonstrations, and training is performed solely on these generated samples. During evaluation, objects are uniformly randomly placed across the 6 Figure 4. Experiment setups of (a) height and (b) texture editing. # Demo Tabletop Platform Total Tabletop Real 20 Tabletop Real 1 Gen 40 5/5 4/5 0/5 4/5 50% 80% Table 2. Performance comparison of Go-1 on height generalization. Our method successfully generalizes to the unseen height. workspace to assess spatial generalization. Results. Table 1 shows the manipulation success rate of four tasks with different training data. Real data results show that when the number of demonstrations is fewer than 20, the average task success rate drops below 50%, indicating that VLAs exhibit limited spatial generalization when trained with scarce data. Conversely, policies trained on 200 demonstrations generated from only single source demonstration by Real2Edit2Real achieve comparable spatial generalization to those trained on 50 real-world demonstrations. As the number of source demonstrations increases, the average success rate of policies trained on the same 200 generated demonstrations improves significantly. When trained with data generated from 5 real demonstrations, the policies of Go-1 and π0.5 achieve average success rates of 78.8% and 81.3%, surpassing those trained on 50 real demonstrations by 17.5% and 20%, respectively. This improvement arises because increasing source demonstrations introduces more diverse robotobject interaction patterns and expands the spatial coverage of the generated data. Overall, the experimental results demonstrate that Real2Edit2Real improves data efficiency by 10-50 through data generation, confirming its effectiveness as demonstration generation framework for robotics. 4.3. More Applications Height Editing. With smooth object relocation described in Sec. 3.4, we can also edit the object height as shown in Figure 4. Table 2 shows the performance comparison on height generalization. The policy trained by 20 real demonstrations on the tabletop completely fails on the platform height because of OOD. If we generate 20 demonstrations on the tabletop and 20 demonstrations on the platform through our framework, the policy can achieve 80% success rate. Texture Editing. Since the model described in Sec. 3.4 # Demo White Green Black Blue Red Total 7/10 Real 50 Real 1 Gen 200 7/10 Real 1 Gen 200* 6/ 4/10 4/10 7/10 5/10 5/10 4/10 50% 6/10 4/10 5/10 52% 6/10 7/10 8/10 68% Table 3. Performance comparison of Go-1 under different desktop textures. Real 1 Gen 200* means generating data includes different textures. Our method is robust to the texture variation. Figure 5. Ablation study of geometry reconstruction. The left endeffector is in red, and the right end-effector is in yellow. Figure 6. Ablation study of robot pose correction (RPC). Figure 7. Ablation study of smooth object relocation (SOR). generates video from the first frame, we can easily edit the video through first-frame editing, like changing the background texture. Table 3 illustrates the performance under different desktop textures shown in Figure 4 and indicates that our method can generate demonstrations with different textures to improve policy robustness. 4.4. Ablation Study Geometry Reconstruction. Figure 5 provides point-cloud visualizations of real data, VGGT, and ours. We can see that 7 Figure 8. Visualization of videos generated by Real2Edit2Real on all four real-world tasks. our model predicts the cleanest point cloud and the most accurate camera pose in robot scenarios, while the real-world data suffer from inaccurate camera poses and the VGGT reconstructions contain substantial clutter and noise. Robot Pose Correction. Figure 6 shows the ablation results of robot pose correction. Without robot pose correction, the erroneous depth maps lead to blurry and inconsistent generation results. With robot pose correction, the depth maps become kinematically consistent, allowing the model to generate realistic robot motions in the synthesized videos. Smooth Object Relocation. Figure 7 shows the ablation results of smooth object relocation. Without smooth object relocation, the generated object placements often exhibit In noticeable errors, leading to unusable demonstrations. contrast, smooth object relocation enables the precise placement of objects at the target locations. 4.5. Visualization Figure 8 is the visualization of videos generated by Real2Edit2Real on all four real-world tasks. The generated videos successfully relocate the objects, synthesize the correct manipulation trajectories, and maintain both multi-view consistency and realistic visual appearance. Please refer to the supplementary materials for more visualizations. 5. Conclusion In this work, we introduce Real2Edit2Real, framework that enables scalable demonstration generation by linking 3D editability with 2D visual data. Through metric-scale geometry reconstruction, depth-reliable spatial editing, and 3D-controlled video generation, our approach synthesizes realistic and kinematically consistent multi-view manipulation demonstrations. Experiments across four real-world 8 manipulation tasks show that policies trained on data generated from as few as 15 demonstrations can match or surpass those trained on 50 real-world demonstrations, improving data efficiency by up to 1050. Additional results on height and texture editing further highlight the extensibility of our framework, suggesting its potential to serve as unified engine for scalable data generation. 6. Acknowledgement We would like to thank Zizhao Tong from University of Chinese Academy of Sciences for his fruitful discussion and Haolin Chen from Zhongguancun Academy for his technical support."
        },
        {
            "title": "References",
            "content": "[1] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 2 [2] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 3 [4] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, 2025. 2, 6, 15 [5] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023. 2, [6] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2024. 2 [7] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, et al. Graspvla: grasping foundation model arXiv pre-trained on billion-scale synthetic action data. preprint arXiv:2505.03233, 2025. 2 [8] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 3 [9] Caelan Garrett, Ajay Mandlekar, Bowen Wen, and Dieter Fox. Skillmimicgen: Automated demonstration generation In 8th Annual for efficient skill learning and deployment. Conference on Robot Learning, 2024. 2, 13 [10] Xiaoshen Han, Minghuan Liu, Yilun Chen, Junqiu Yu, Xiaoyang Lyu, Yang Tian, Bolun Wang, Weinan Zhang, and Jiangmiao Pang. Re3sim: Generating high-fidelity simulation data via 3d-photorealistic real-to-sim for robotic manipulation. arXiv preprint arXiv:2502.08645, 2025. 2 [11] Ryan Hoque, Ajay Mandlekar, Caelan Garrett, Ken Goldberg, and Dieter Fox. Intervengen: Interventional data generation for robust and data-efficient robot imitation learning. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 28402846, 2024. 2 [12] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. [13] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. 16 [14] Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, et al. Enerverse: Envisioning embodied future space for robotics manipulation. arXiv preprint arXiv:2501.01895, 2025. 3 [15] Yan Huang, Shoujie Li, Xingting Li, and Wenbo Ding. Umigen: unified framework for egocentric point cloud generation and cross-embodiment robotic imitation learning. arXiv preprint arXiv:2511.09302, 2025. 13 [16] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, 9 Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. vision-language-action model with open-world generalization, 2025. 2, π0.5: [17] Yuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, et al. Enerverse-ac: Envisioning embodied environments with action condition. arXiv preprint arXiv:2505.09723, 2025. 3 [18] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025. 2 [19] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv preprint arXiv:2509.13414, 2025. 3 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 2 [21] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [23] Kuaishou Technology. Kling AI: Next-Generation AI Creative Studio. https://klingai.com/. 3 [24] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. 3 [25] Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, et al. Momagen: Generating demonstrations under soft and hard constraints for multi-step bimanual mobile manipulation. arXiv preprint arXiv:2510.18316, 2025. 2 [26] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025. 3, 5, 6, action model with implicit spatial understanding. preprint arXiv:2507.00416, 2025. 3 arXiv [28] Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang, and Zhizhong Su. Robotransfer: Geometry-consistent video difarXiv preprint fusion for robotic visual policy transfer. arXiv:2505.23171, 2025. 3, 13 [29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 14 [30] Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, and Shuran Song. Geometry-aware 4d arXiv preprint video generation for robot manipulation. arXiv:2507.01099, 2025. 3 [31] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, and Yansong Tang. Manigaussian: Dynamic gausIn Eusian splatting for multi-task robotic manipulation. ropean Conference on Computer Vision, pages 349366. Springer, 2024. [32] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, and Shiwei Li. Matrix3d: Large photogrammetry model all-in-one. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1125011263, 2025. 3 [33] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. In 7th Annual Conference on Robot Learning, 2023. 2, 13 [34] NVIDIA, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, WeiCheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, and Yuke Zhu. World simulation with video foundation models for physical ai, 2025. 3, 5 [27] Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Yuxin Du, Jiting Liu, Encheng Gu, and Bo Zhao. Evo-0: Vision-language- [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 41724182, 2023. 5 [36] Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, and Shanghang Zhang. Wristworld: Generating wrist-views via 4d world models for robotic manipulation. arXiv preprint arXiv:2510.07313, 2025. 3 [37] Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, and Abhisesh Silwal. Splatsim: Zeroshot sim2real transfer of rgb manipulation policies using In 2025 IEEE International Confergaussian splatting. ence on Robotics and Automation (ICRA), pages 65026509. IEEE, 2025. 2 [38] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 14 [39] Vaibhav Saxena, Matthew Bronars, Nadun Ranawaka Arachchige, Kuancheng Wang, Woo Chul Shin, Soroush Nasiriany, Ajay Mandlekar, and Danfei Xu. What matters in learning from large-scale datasets for robot manipulation. In The Thirteenth International Conference on Learning Representations, 2025. 2 [40] Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, and Yong Li. Roboscape: Physics-informed embodied world model. arXiv preprint arXiv:2506.23135, 2025. 3 [41] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick LaarXiv preprint batut, and Piotr Bojanowski. Dinov3. arXiv:2508.10104, 2025. [42] Balakumar Sundaralingam, Siva Kumar Sastry Hari, Adam Fishman, Caelan Garrett, Karl Van Wyk, Valts Blukis, Alexander Millane, Helen Oleynikova, Ankur Handa, Fabio Ramos, et al. Curobo: Parallelized collision-free robot motion generation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 81128119. IEEE, 2023. 5 [43] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In The Thirteenth International Conference on Learning Representations. 3 [44] Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, and Ling Shao. Fidelity-aware data composition for robust robot generalization. arXiv preprint arXiv:2509.24797, 2025. 3, 13 [45] Marcel Torne Villasevil, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, and Pulkit Agrawal. Reconciling reality through simulation: real-to-sim-to-real approach for robust manipulation. Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. 2 [46] Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, and Xingang Wang. Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling. arXiv preprint arXiv:2507.05198, 2025. 3 [47] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: In Proceedings of Visual geometry grounded transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 3, 4, 6, 13 [48] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Advances in Neural Information Processing Systems, 34:2717127183, 2021. [49] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. 5, 13 [50] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 3 [51] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning, 2025. 3 [52] Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, and Jiang Bian. Geometry forcing: Marrying video diffusion and 3d representation for consistent world modeling. arXiv preprint arXiv:2507.07982, 2025. 3 [53] Xiuwei Xu, Angyuan Ma, Hankun Li, Bingyao Yu, Zheng Zhu, Jie Zhou, and Jiwen Lu. R2rgen: Real-to-real 3d data generation for spatially generalized manipulation. arXiv preprint arXiv:2510.08547, 2025. 13 [54] Yuan Xu, Jiabing Yang, Xiaofeng Wang, Yixiang Chen, Zheng Zhu, Bowen Fang, Guan Huang, Xinze Chen, Yun Ye, Qiang Zhang, et al. Egodemogen: Novel egocentric demonstration generation enables viewpoint-robust manipulation. arXiv preprint arXiv:2509.22578, 2025. 3 [55] Zhengrong Xue, Shuying Deng, Zhenyang Chen, Yixuan Wang, Zhecheng Yuan, and Huazhe Xu. DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning. In Proceedings of Robotics: Science and Systems, LosAngeles, CA, USA, 2025. 2, 4, [56] Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, and Jiangmiao Pang. Novel demonstration generation with gaussian splatting enables robust one-shot manipulation. arXiv preprint arXiv:2504.13175, 2025. 2, 13 [57] Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, and Ken Goldberg. Real2render2real: Scaling robot data without dynamics simulation or robot hardware. In 9th Annual Conference on Robot Learning, 2025. 2, 13 [58] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In Proceedings of Robotics: Science and Systems (RSS), 2024. 2 11 [59] Jiyao Zhang, Mingjie Pan, Baifeng Xie, Yinghao Zhao, Wenlong Gao, Guangte Xiang, Jiawei Zhang, Dong Li, Zhijun Li, Sheng Zhang, Hongwei Fan, Chengyue Zhao, Shukai Yang, Maoqing Yao, Chuanzhe Suo, and Hao Dong. Agibot digitalworld. https://agibot-digitalworld.com/, 2025. 6, 15 [60] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance In and camera estimation from uncalibrated sparse views. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2193621947, 2025. 3 [61] Wentao Zhao, Jiaming Chen, Ziyu Meng, Donghui Mao, Ran Song, and Wei Zhang. Vlmpc: Vision-language model predictive control for robotic manipulation. arXiv preprint arXiv:2407.09829, 2024. 12 Real2Edit2Real: Generating Robotic Demonstrations via 3D Control Interface"
        },
        {
            "title": "Supplementary Material",
            "content": "Method MimicGen [33] SkillMimicGen [9] RoboSplat [56] Real2Render2Real [57] DemoGen [55] R2RGen [53] UMIGen [15] RoboTransfer [28] MVAug [44] Real2Edit2Real (ours) Source Generation No Simulation RGB Only VLA Compatible Novel Texture Novel Trajectory Table 4. Comparison with Other One-to-many Demonstration Generation Methods. 7. Contribution Clarification ing its unified and flexible design. To better clarify our contribution, we provide detailed comparison between our method and other one-to-many demonstration generation approaches, as shown in Table 4. Simulation-based methods like MimicGen [33] and SkillMimicGen [9] rely on simulators and require scene and object assets, which not only introduce significant simto-real gap but also make it difficult to perform data augmentation directly on real-world data. Methods such as RoboSplat [56] and Real2Render2Real [57] are built on 3D Gaussian Splatting. Although they do not require simulation engine, they still rely on dense scanning to reconstruct the objects or scenes. This means that they cannot perform data generation using only the RGB observations from the original demonstrations, which significantly limits their scalability. Another line of research, including DemoGen [55], R2RGen [53], and UMIGen [15] , generates new 3D point-cloud demonstrations through pointcloud editing. However, their reliance on depth sensors limits their compatibility with the current mainstream VLA paradigm that uses multi-view RGB inputs, and also prevents them from performing texture-level augmentation. Methods based on video generation, such as RoboTransfer [28] and MVAug [44], can directly augment multi-view 2D demonstrations, but they only enhance visual aspects such as texture, without increasing the diversity of object spatial distributions or robot trajectories. In contrast, our method requires no simulator and directly augments the original RGB observations, significantly improving scalability. It simultaneously generates new textures and trajectories for VLA training, highlight8. Real2Edit2Real Implementation Details In this section, we provide more details of the proposed framework, Real2Edit2Real: In Section 8.1, we provide additional information for the hybrid training paradigm. In Section 8.2, we explain the full pipeline of depthreliable spatial editing in detail. In Section 8.3, we discuss more about 3D-controlled video generation model. 8.1. Metric-scale Geometry Reconstruction Data Visualization. Fig. 9 shows the visualization of the training data. We can see that real-world depth maps are often noisy and contain large invalid regions, whereas synthetic depth is clean and accurate. By training with our proposed hybrid training paradigm, our model learns to reconstruct geometry in metric scale in the real world, effectively compensating for the limitations of depth sensors. Training Details. In Table 5, we provide the details of finetuning VGGT [47] to Metric-VGGT. 8.2. Depth-reliable Spatial Editing Background Depth Completion. As we mentioned in the manuscript, projecting edited point clouds to depth maps may cause missing regions in the background due to the object moving and novel robot motion. To mitigate this artifact, we first inpaint the background, which deletes the foreground objects and robot in the multi-view first frames with an image-edit model [49]. Figure 10 provides the prompt 13 Figure 9. Training Data Visualization of Metric-VGGT. Depth visualization: red is the nearest, blue is the farthest. \"While keeping everything else in the image unchanged, remove the black gripper and the black wire.\" \"While keeping everything else in the image unchanged, remove the white robotic arm.\" \"While keeping everything else in the image unchanged, remove xxx on the table.\" Figure 10. Prompt Used for Background Inpainting. prompt, xxx means the manipulated objects. In the Figure 11. Example of the Inpainted Background. we used for image editing, and Figure 11 shows an example of the inpainted background. Then, we reconstruct the metric geometry of the background with Metric-VGGT. To correct the metric-scale inconsistencies introduced by image editing, we incorporate an additional point cloud alignment procedure, as shown in Algorithm 2. Spatial Editing Pipeline. After getting completed background point cloud, we separate foreground objects through Grounded-SAM [22, 29, 38] and robotic dual arms through forward kinematics. Following, we provide an example to detail the full spatial editing pipeline. Algorithm 3, 4 shows the spatial editing pipeline of the Mug to Basket task. The Object Relocation Segment produces the depth sequence for Smooth Object Relocation described in the manuscript. 14 Config Value VGGT-1B Base Model 100,000 Training Real Data 40,000 Training Sim Data Full Parameter Fine-Tuning Scheme 150,000 Total Training Steps 2e-4 Learning Rate 2e-5 Backbone Learning Rate Cosine Annealing Scheduler LR Scheduler 1e-6 ETA Minimum 1e-2 Weight Decay 3 View Num 16 Global Batch Size Gradient Accumulation Steps 4 Mixed Precision Optimizer Training Image Size bf16 AdamW 518 Table 5. Training Details of Metric-VGGT. Algorithm 2 Background Point-Cloud Alignment Require: Origin first frame point cloud o, unaligned background point cloud edit, table mask table. Ensure: Metric-aligned background point cloud bg. function ESTIMATEPLANE(P) plane RansacPlaneSegment(P) // plane : [a, b, c, d], ax + by + cz + = 0 return plane end function planeo EstimatePlane(P o[M table]) planeedit EstimatePlane(P edit[M table]) scale planeo[3]/planeedit[3] bg scale edit return bg 8.3. 3D-Controlled Video Generation Training Data. For training the 3D-controlled multi-view video generation model, we sample 7K episodes of 64 tasks from the Agibot-World datasets [4]. To get the control conditions of the training data, we used the Metric-VGGT to predict the depth maps and compute the Canny Edges from depth. To ensure the 3D control condition remains consistent across multi-view and temporal, we perform global normalization on the depth sequences of all three views within training chunk, rather than normalizing each depth map individually. Condition Dropout. In the training stage, we finetune the backbone of GE-Sim [26] (based on CosmosPredict-2B [59]) with sampling data from the AgibotWorld Dataset [4]. In multi-condition compositional generation, intensity-based conditions such as depth maps and Canny Algorithm 3 Pipeline of Mug to Basket Require: Source point clouds l, r, mug, basket, background point cloud bg, joint states Q, action trajectory A, camera poses h, l, r, skill-1 start timestep t1, skill-1 end timestep t2, skill-2 start timestep t3, skill-2 end timestep t4. Ensure: Novel depth sequence Dh, Dl, Dr, joint states Q, action trajectory A, camera poses . function RENDERDEPTH(P, , Q) D1 ProjectPointCloud(P, ) D2 RenderLinkDepth(Q, ) return Merge(D1, D2) end function Sample Object Transform Pair Tmug, Tbasket R44 Dh list(), Dl list(), Dr list() list(), list(), list() // Object Relocation Segment for in range(0, 30) do , Tbasket Tbasket mug Tmug Interpolate(Tmug, Tbasket, 30, t) Tmug basket 0 bg 0 0 , Dh Dh RenderDepth(P , Dl Dl RenderDepth(P Dr Dr RenderDepth(P , Q Q0, A A0 0 , (T 0 , Q0) 0 , Q0) 0 , Q0) 0 , 0 ) 0 end TmugAt1, end for // Motion-1 Segment start A0, for in range(0, t1) do , r TtP ree Tt, ree MotionPlan(A FK(P P t , Qt) TmugP mug start, end, t) TbasketP basket Pbg Dh Dh RenderDepth(P , , ) , , Dl Dl RenderDepth(P ) , , TtT Dr Dr RenderDepth(P ) , A Q Q , TtT , (T ) end for // Skill-1 Segment for in range(t1, t2) do IK(TmugAt) FK(P Tmug(P ree , Qt) mug ree )P Dh Dh RenderDepth(P Dl Dl RenderDepth(P Dr Dr RenderDepth(P Q (T t , A TmugAt , , TmugT ) Pbg TmugP mug , , ) , , ) , TmugT , ) end for 15 Algorithm 4 Continued to Pipeline of Mug to Basket Config Value end TbasketAt3, // Motion-2 Segment start TmugAt2, for in range(t2, t3) do , r Tt(P ree )P end, t) start, P mug MotionPlan(A FK(P , Qt) Tt, ree TbasketP basket Dh Dh RenderDepth(P , , ) Dl Dl RenderDepth(P , , ) Dr Dr RenderDepth(P , , TtT ) , A Q Q , TtT , (T ) Pbg end for // Skill-2 Segment for in range(t3, t4) do , Qt) mug IK(TbasketAt) FK(P r Tbasket(P ree ree Dh Dh RenderDepth(P Dl Dl RenderDepth(P Dr Dr RenderDepth(P Q (T t , A TbasketAt , TbaseketT , ) Pbg basket ) t , , ) , , ) , TbasketT , ) GE-Sim-2B Base Model 7000 Episodes 64 Tasks Training Data Full Parameter Fine-Tuning Scheme 20,000 Total Training Steps 1e-4 Learning Rate Constant with Warmup LR Scheduler 1000 LR Warmup Steps 5e-5 Weight Decay Global Batch Size 16 Gradient Accumulation Steps 1 Max Gradient Norm Mixed Precision Optimizer Training Resolution Video Chunk Length Memory Frames 1.0 bf16 AdamW 384512 25 Table 6. Training Details of 3D-controlled Video Generation Model. end for return D, Q, A, edges tend to dominate the visual information, potentially diminishing the influence of other control signals during training [13]. However, these two conditions always introduce noise after spatial editing. To improve robustness against imperfect control signals, we apply random dropout to the depth and Canny edge conditions during training, where they are independently dropped with probability of 0.5, and jointly dropped with probability of 0.1. By randomly masking portions of these inputs, the model is encouraged to rely on complementary visual evidence, rather than depending solely on the intensity conditions, ultimately improving the realism of the generated videos under noisy conditions. Training Details. training the 3D-controlled video generation model. In Table 6, we provide the details of 9. Experiment Details Workspace. Figure 12 shows the workspace of four realrobot manipulation tasks in the manuscript. The workspace is determined by the maximal range in which the robots kinematic configuration can perform the intended tasks. Object Set. Figure 13 shows all the objects we used in the manipulation tasks. Because it is impractical to verify every object in the training set, all objects in the figure are newly purchased to minimize any potential overlap with the Figure 12. Visualization of Manipulation Workspace. dataset. This setup enables more reliable assessment of our frameworks generalization to unseen objects. In addition, the real-world testing laboratory is also absent from the training data used to develop our framework. Data Generation. To generate demonstrations, we first reconstruct the source demonstrations and apply confidence threshold between 30% and 50% to remove spurious points. During spatial editing, we define an augmentation region around the objects original location, typically 40cm40cm square, and augment object rotations within 3060 range. For video generation, we set the diffusion step to 6 and the memory length to 4, for which we uniformly sample from the already generated frames, including both the first and last frames. 16 of 50 real demonstrations. 10.3. Ablation Study of Control Conditions In the manuscript, we introduced our 3D-controlled video generation model, which uses depth as the 3D control interface and incorporates Canny edges computed from the depth map as an auxiliary condition. To investigate the roles of depth and Canny edges in video generation, we conduct qualitative ablation studies by removing each condition individually. Fig. 15, 16, 17, 18 show the results on four tasks, respectively. The results demonstrate that removing either the depth control or the Canny edge constraint leads to issues such as object blurring and incorrect interactions, which substantially degrade the quality of the generated demonstrations. Figure 13. Visualization of Manipulation Objects. R10 R20 R50 R1G200 R2G200 R5G200 11. Additional Visualizations Figure 19, 20, 21, 22 show more visualizations of the four real-world manipulation tasks. 12. Limitation and Discussion Despite the advantages of our proposed Real2Edit2Real framework, which enables scalable multi-view demonstration augmentation, it still has certain limitations. For instance, it does not handle the physical modeling of articulated objects well. This is primarily because the training data for our video generation model contains very few articulated objects. Increasing the number and variety of articulated objects in the training set may help improve our frameworks support for such objects. Nevertheless, we empirically demonstrate the effectiveness and flexibility of our method. Since Real2Edit2Real consists of three modules, which are geometry reconstruction, spatial editing, and video generation, future improvements in any of these modules are expected to further enhance the overall capability of the framework. 0/20 9/ 11/20 13/20 14/20 17/20 Table 7. Performance of Diffusion Policy on the Mug to Basket task. means the number of real demonstrations, and means the number of generated demonstrations. 10. Additional Experiments 10.1. Diffusion Policy on Mug to Basket To further validate the quality of the data generated by Real2Edit2Real, we conduct additional Diffusion Policy [5] experiments on the Mug to Basket task. We use ViT-S encoder initialized with DINO-v3 [41] pre-trained weights and train the Diffusion Policy in full-parameter manner on different training data. Table 7 shows the success rate of diffusion polices trained with real demonstrations and generated demonstrations, which indicates that generating data from only few source demonstrations, like 1-5, can make DP surpass that trained with 50 real demonstrations on this task. 10.2. Generated Data Scaling Analysis In the manuscript, we investigate how increasing the number of source demonstrations affects policy performances. Here, we additionally examine, within our proposed Real2Edit2Real framework, the impact of generating more demonstrations. To this end, we produce varying numbers of demonstrations from single source demonstration and evaluate the resulting policies, using the same training and evaluation protocols as in the manuscript. Experimental results are shown in Figure 14 and Table 8. The results indicate: (1) Both policies exhibit consistently improved success rates when scaling up generated demonstrations. (2) When we generate more than 300 demonstrations from only one demo, the average success rates surpass that 17 # Demo Real 10 Real 20 Real 50 Real 1 Gen 50 Real 1 Gen 100 Real 1 Gen 200 Real 1 Gen 300 Real 1 Gen 400 Mug to Basket Pour Water Lift Box Scan Barcode Total Go-1 8 / 20 12 / 20 14 / 20 8 / 20 12 / 20 14 / 20 15 / 20 15 / 20 π0.5 8 / 20 14 / 20 13 / 9 / 20 11 / 20 15 / 20 16 / 20 19 / 20 Go-1 5 / 20 7 / 20 8 / 20 11 / 20 12 / 20 12 / 20 12 / 20 14 / 20 π0.5 1 / 20 2 / 20 8 / 4 / 20 6 / 20 10 / 20 12 / 20 15 / 20 Go-1 11 / 20 12 / 20 15 / 20 10 / 20 10 / 20 12 / 20 14 / 20 15 / 20 π0.5 13 / 20 15 / 20 17 / 6 / 20 7 / 20 10 / 20 11 / 20 13 / 20 Go-1 5 / 20 8 / 20 12 / 20 11 / 20 12 / 20 14 / 20 18 / 20 18 / 20 π0.5 Goπ0.5 4 / 20 5 / 20 11 / 20 4 / 20 9 / 20 11 / 20 11 / 20 13 / 20 36.3% 32.5% 48.8% 45.0% 61.3% 61.3% 50.0% 28.8% 57.5% 41.3% 65.0% 57.5% 73.8% 62.5% 77.5% 75.0% Table 8. Scaling Analysis of Generated Demonstrations. This compares the performance of polices trained with different numbers of demonstrations generated from only one source demonstration. We can see that increasing the number of generated demonstrations leads to improved success rates for both policies. When we generate more than 300 demonstrations from only one demo, the average success rates even surpass that of 50 real demonstrations. Figure 14. Scaling Analysis of Generated Demonstrations. Bold curves denote the task-averaged performance, while the faint translucent curves visualize the trajectories of individual tasks. Both policies exhibit consistently improved success rates when scaling up generated demonstrations. 18 Figure 15. Ablation Study of Control Conditions on Mug to Basket. 19 Figure 16. Ablation Study of Control Conditions on Pour Water. Figure 17. Ablation Study of Control Conditions on Lift Box. 21 Figure 18. Ablation Study of Control Conditions on Scan Barcode. 22 Figure 19. Visualization of Generated Videos on Mug to Basket. Figure 20. Visualization of Generated Videos on Pour Water. 24 Figure 21. Visualization of Generated Videos on Lift Box. 25 Figure 22. Visualization of Generated Videos on Scan Barcode."
        }
    ],
    "affiliations": [
        "AgiBot",
        "CFCS, School of Computer Science, Peking University",
        "PKU-AgiBot Lab"
    ]
}