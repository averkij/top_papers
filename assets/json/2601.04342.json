{
    "paper_title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "authors": [
        "Mohsen Ghafoorian",
        "Amirhossein Habibian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 ] . [ 1 2 4 3 4 0 . 1 0 6 2 : r ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers Mohsen Ghafoorian, Amirhossein Habibian Qualcomm AI Research* {mghafoor,ahabibia}@qti.qualcomm.com"
        },
        {
            "title": "Abstract",
            "content": "OOM Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-ofthe-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAts hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to 160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides recipe that can be applied to future state-of-the-art bidirectional softmaxbased models. Experiments on VBench and VBench-2.0, as well as human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-airesearch.github.io/rehyat. 1. Introduction The ambition in generative video is shifting from producing short, visually striking clips to creating sustained, coherent sequences with rich dynamics and consistent subject identity. Diffusion-based models have become the method of choice for this goal due to their stability and controllability; however, the choice of backbone is decisive for scaling. While early video diffusion systems adapted U-Net architectures from images, they exhibited limited capacity to model long temporal structure and struggled to scale effectively to higher resolutions and durations. This has motivated transition to Diffusion Transformers (DiTs) [30], which process video as se- *Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 3 . 1 1 . 2 1 . 3 8 : e A R 0 4 . 3 8 : e Figure 1. comparison of our proposed Recurrent Hybrid Attention model with Wan2.1 bidirectional full softmax attention. Top: Compute complexity increase with video duration growth (left: FLOPs, right: phone latency). Bottom: comparing our hybrid model (20 ReHyAt blocks) with original Wan2.1 1.3B, qualitatively and quantitatively. Prompt: cat drinking water. quence of spatiotemporal patches and furnish global context from the first layer. The resulting architectural shift underlies recent state-of-the-art systems (e.g., Wan2.1 [35], CogVideoX [44], HunyuanVideo [22], PyramidalFlow [16], Open-Sora Plan [26]), and has been documented by recent surveys as the prevailing trend in video generation [28, 39]. This progress comes with nontrivial systems cost: the self-attention term scales quadratically with sequence length, O(N 2d) in time and O(N 2) in memory, where is the number of tokens and the hidden dimension [33, 34]. In video, is the product of temporal length and spatial patch count, so even moderate resolutions and durations yield token counts in the tens of thousands. In practice, the attention subroutine consumes the majority of compute in DiT blocks, and memory pressure grows rapidly with longer contexts. Kerneland IO-aware implementations such as FlashAttention [7] reduce constants but do not alter the 2 dependence, leaving training and inference constrained when targeting higher resolutions, extended durations, or multi-shot compositions. As direct consequence, producing videos beyond roughly 10 seconds remains difficult within typical GPU memory and latency budgets, while edge devices such as mobile phones even struggle to generate more than few seconds of videos. Linear attention [20] offers compelling alternative to full softmax attention by reducing complexity from quadratic to linear and enabling constant memory when reformulated as an RNN. This property makes it particularly attractive for generating arbitrarily long videos, where memory growth is critical bottleneck. Beyond efficiency, the recurrent formulation of linear attention allows chunkwise processing, which aligns naturally with sequential video generation. These advantages have motivated recent efforts to explore linear and hybrid attention mechanisms in video diffusion models [4, 11]. However, linear attention introduces significant tradeoff: its kernel-based similarity function lacks the expressiveness of the exponential kernel used in softmax attention. This gap manifests in reduced activation diversity and weaker modeling of fine-grained dependencies [47], often requiring extensive retraining to achieve acceptable quality [4, 11]. Hybrid approaches that combine linear and softmax attention have emerged as potential solution [11], but existing designs remain quadratic in complexity and cannot be reformulated as RNNs, leaving the scalability challenge unresolved. In other words, while these methods improve quality over purely linear attention, they fail to deliver the memory and compute benefits necessary for long-duration video generation. Meanwhile, the most powerful video diffusion models today are trained with bidirectional full softmax attention using massive compute and data resources. Re-training such models with alternative attention mechanisms from scratch is prohibitively expensive and impractical for most research and production settings. This observation motivates different strategy: rather than building efficient models from the ground up, can we distill these high-quality, compute-heavy models into recurrent form that preserves fidelity while dramatically reducing resource requirements? Achieving this would unlock practical scalability for video diffusion, not neglecting the substantial progress made by state-of-the-art architectures. In this paper, we address this challenge by introducing ReHyAt, recurrent hybrid attention mechanism tailored for video diffusion. Our key insight is that preserving softmax attention for small subset of tokensthose most critical for modeling local dependencieswhile applying linear attention globally enables modeling long-range and high fidelity local dependencies while ensuring linear efficiency. We propose temporally chunked hybrid attention design with overlapping chunks to maintain temporal coherence, and show that this formulation can be reformulated into chunk-wise RNN with constant memory complexity. Furthermore, we leverage two-stage training pipelineattention distillation from bidirectional softmax teacher followed by lightweight fine-tuningthat achieves SOTA results within fewer than 200 GPU-hours. We validate our approach by transforming Wan2.1 into its recurrent hybrid counterpart and evaluate on VBench [14], VBench2.0 [51], and human preference study, demonstrating that ReHyAt delivers near state-of-the-art quality with dramatically reduced compute. Fig 1 demonstrates some of the aspects discussed above. Our main contributions are as follows: We propose ReHyAt, novel temporally chunked hybrid attention mechanism that combines local softmax attention with global linear attention. This design preserves high-fidelity modeling of critical dependencies within and across adjacent frames while reducing overall complexity to linear time. We derive chunk-wise recurrent reformulation of ReHyAt, computationally enabling generation of arbitrarily long videos with constant memory usage and efficient inference. Through extensive empirical evaluations and ablation studies, we show that state-of-the-art bidirectional Softmax attention video diffusion model can be transformed into chunk-wise recurrent model, only within few hundred GPU-hours, with negligible impact on the quality. 2. Related Work Efficient Attention. Several approaches aim to reduce the quadratic complexity of self-attention across domains: for vision tasks (e.g., EfficientViT [2], PADRe [23], image generation (e.g., Performer [5], Linformer [38]), SANA [25], LinGen [36], Grafting [3]), and language modeling [29, 37, 43, 46, 48]. These works show the feasibility of sub-quadratic attention but often require heavy retrainIn coning or training from scratch (e.g., SANA [25]). trast, we focus on lightweight distillation and fine-tuning of pre-trained softmax-based models into an efficient hybrid attention design tailored for video diffusion under modest compute budgets. Linear recurrent models such as SSM and RWKV [9, 10, 37, 45, 53] have emerged as alternatives to self-attention for long sequences. However, architectural differences from transformers make distilling DiT weights into these models costly. Our approach preserves the original block structure, enabling effective distillation with minimal training. Finally, as noted in Katharopoulos et al. [20], causal linear attention can be reformulated as an RNN during inferencea property we leverage for efficient long video generation. Video Diffusion Models. Recent large-scale systems such as CogVideoX [44], Open-Sora Plan [26], PyramidalFlow [16], LTX-video [12], and Wan2.1 [35] have significantly advanced video generation quality and scalability, but at substantial compute and memory cost. Mobile/PCoriented designs like Mobile Video Diffusion [42], MoViE [18], SnapGen-V [41], AMD-HummingBird [15], On-device Sora [21], MobileVDiT [40], and NeoDragon [19] aim for lightweight deployment, yet most remain non-DiT-based or still rely on full quadratic attention, limiting scalability for long-duration videos. Video Diffusion Models with Efficient Attention. Prior work has explored accelerating video generation through token merging [1, 8, 17], token downsampling [6, 31], attention tiling [8, 50], and sparsity [24, 49]. Tiling and sparsitybased approaches, in particular, gain efficiency by discarding attention for most tokens. In contrast, our hybrid attention design attends to the full token set, combining linear attention for long-range dependencies with softmax attention for local, high-fidelity interactions. M4V [13] accelerates video DiTs by distilling them into Mamba blocks. Despite our simpler block structure and lightweight training, we outperform M4V in both quality and efficiency. Recently, Attention Surgery [11] proposed temporally uniform hybrid attention method with reasonable quality but retained quadratic complexity. Our approach introduces temporally non-uniform hybrid arrangement, enabling uneven treatment of token dependencies and better inductive bias for video generation. It achieves linear complexity and can be reformulated as memory-efficient RNN, supporting on-device execution and scalable long video generation. Finally, concurrent to our work, SANA-Video [4] introduced video diffusion model incorporating linear attention. In contrast, our method offers hybrid approach combining the computational efficiency of linear attention for long-range dependencies with the accuracy of softmax attention for modeling highly co-dependent adjacent tokens. Furthermore, unlike SANA-Video, our method sets up distillation process from SOTA bidirectional full softmax attention model, making training extremely efficient: we obtain our model in 160 GPU-hourstwo orders of magnitude more efficient than SANA-Video. This work therefore provides low-cost recipe to transform costly Softmax attention SOTA models into efficient RNNs, laying the groundwork for long video generation and on-device execution. 3. Methods: ReHyAt 3.1. Preliminaries: Linear Attention Let RN denote sequence of tokens, each represented by D-dimensional feature vector. At the l-th transformer layer, the block is formulated as: Tl(x) = fl (cid:0)Al(x) + x(cid:1), (1) where fl() applies token-wise transformation, typically lightweight feed-forward network, and Al() represents the self-attention operatorthe component responsible for cross-token interaction. The standard attention mechanism is given by: Al(x) = = softmax (cid:18) qk (cid:19) v, (2) where queries, keys, and values are computed as linear projections: = xwq, = xwk, = xwv, with learnable weights wq, wk, wv RDD . The softmax attention for token can be expressed as: yi = (cid:80)N j=1 sim(qi, kj) vj (cid:80)N j=1 sim(qi, kj) . (3) Applying the kernel trick, the similarity function can be (recovering the origgeneralized from sim(qi, kj) = eqik inal softmax) to sim(qi, kj) = ϕ(qi)ϕ(kj), yielding: yi = ϕ(qi) (cid:80)N ϕ(qi) (cid:80)N j=1 ϕ(kj) j=1 ϕ(kj) . (4) and (cid:80)N j=1 ϕ(kj) Crucially, the terms (cid:80)N j=1 ϕ(kj) do not depend on i, enabling precomputation and caching for linear-time complexity. The mapping ϕ() must be nonnegative; the original work by Katharopoulos et al. [20] proposes ϕ(x) = 1 + elu(x). However, this substitution introduces notable gap in expressiveness compared to the exponential kernel, often requiring substantial retraining or resulting in degraded performance relative to softmax attention. 3.2. Hybrid Attention Formulation Before introducing the formal expression, we note that the hybrid attention mechanism combines contributions from both softmax attention (for local, high-fidelity dependencies) and linear attention (for global, efficient interactions), and normalizes them jointly. For the latent RN D, assume the tokens are flattened from latent tensor of shape (T, H, W, D), where = HW . To overcome the limitations of purely linear attention in video diffusion models, we incorporate hybrid attention mechanism that combines softmax-based and kernelized linear attention formulations. Now consider chunk of Tc temporal slices from the latent, represented as Xt RN D, where = TcHW . Here we have introduced the chunk-indexed reshaped notation RT D, with = N/N representing the number of chunks, to avoid confusion with single token indexing e.g. xi. Following the same notation, we have Qt RN D, and ϕq(Qt) RN . Then for the hybrid attention of tokens in chunk t, we partition the total tokens = {1, 2..., } to attend to, into softmax attention tokens and linear attention tokens . More specifically, the hybrid attention output for token chunk t, ˆyt RN cont RN stitutes of softmax attention and its normalizer aS RN 1 as well as linear attention and its normaland nS RN 1, formulated as ization term aL below: RN D, nL ˆyt = , + aL aS nS + nL exp(Qtk / ct)vj, aS = (cid:88) jT aL = ϕq(Qt) (cid:16) (cid:88) ϕk(kj) (cid:17) , jT exp(Qtk / ct), nS = (cid:88) jT nL = ϕq(Qt) (cid:16) (cid:88) (cid:17) ϕk(kj) , jT (5) (6) (7) (8) (9) where ct is stabilizing constant (typically the maximum exponent), and ϕq() and ϕk() denote the kernel feature maps for the linear component for queries and keys. Here, we propose the following specification for the partitioning of the tokens sets: = {j tN < (t + 1)N }, L = (10) (11)"
        },
        {
            "title": "We observe that",
            "content": "See the top graph in Fig 2. This means that the computation of attention is effectively broken into temporal chunks of Tc slices, where the tokens within each slice more accurately attend to each other with the Softmax attention, and with linear attention to all the other tokens. Overlapping Chunks. the nonoverlapping chunking mechanism defined above, together with the lower fidelity dependency modeling of linear attention, can result into episodic incoherence in motion or appearance between the frames transitioning from one latent chunk to next. To mitigate this, we propose to arrange overlapping chunks for softmax attention, enabling more accurate softmax-attention-based message passing between the chunks. More specifically, for generating attention output for chunk given chunk of Tc slices (i.e. applying this slicing to queries), the keys and values representing the tokens to attend to, are sliced by Tc + To temporal slices instead, where To represents the overlap size. To arrange this, one needs to reformulate and = {j max(tN ToHW, 0) < (t + 1)N } = L as: (12) T Tc To Tc Softmax Attention Causal Linear Attention Non-causal Linear Attention Target Token (q) Source Tokens (k, v) Figure 2. Overview of the temporally chunked hybrid attention arrangement without (top) and with chunk overlap (bottom). The bottom subgraph in Fig 2 illustrates this. Characterization of ϕ. Similar to [11], to enhance the expressiveness of linear attention, we define distinct learnable feature maps ϕq, ϕk : RD RD . Each map first applies lightweight per-head embedding network (implemented as grouped 1 1 convs with non-linear activations) to produce an intermediate representation, which is then split into equal parts. Each part is raised to different polynomial degree 1 to , and concatenated along the feature dimension. Formally, for an input RD, we define: ϕ(x) = [ (ψ1(x))1, (ψ2(x))2, . . . , (ψP (x))P ] RD , where ψi() denotes the i-th learnable embedding slice produced by the shared embedding network. This polynomial expansion allows ϕq(qi)ϕk(kj) to approximate the large dynamic range of the exponential kernel eqik more accurately than fixed ELU-based mappings. 3.3. Recurrent HyAt Linear attention, once causal has the advantage that can be reformulated to RNNs. In this section we show how our hybrid arrangement, unlike [11], can be reformulated as an RNN. For this to be feasible, we first need to make it causal. To achieve this, it is sufficient to reformulate as follows: = {j < max(tN ToHW, 0)} = {j max(tN ToHW, 0) < (t + 1)N } (13) In Fig. 2, this is equivalent to the bottom graph where the specified non-causal linear attention is removed. Now and thanks to the temporal decoupling of , we can define chunk-wise RNN, where the model generates the latents chunk-by-chunk for Tc temporal slices at time. Let st RDD and zt RD1 represent the state variables for the linear attention and its normalizer, t-th chunk. Then we have: where ϕl is (ϕq, ϕk) for the the l-th block, the noise sampling distribution, the distribution of textual prompts, the set of denoising steps, y(l,ϵ,p,i) the output of the bidirectional softmax teacher on block l, for prompt p, sampling noise ϵ, and denoising step i, and ˆy(l,ϵ,p,i) the same trajectory point for the ReHyAt student model. s0 = 0 z0 = 0 yt = aS + ϕq(Qt) st nS + ϕq(Qt) zt (cid:88) st+1 = st + ϕk(kj) zt+1 = zt + jT (cid:88) jT ϕk(kj) (14) (15) (16) (17) (18) (1) Softmax attention within each Three points to note: chunk need not be causal because sampling proceeds chunk-by-chunk, i.e. our method generates the latents for full chunk at once. (2) The model training doesnt have to be done in the RNN form as introduced above. One can train the model in the causal non-recurrent form and then rearrange the trained model to RNN at the sampling time. (3) The computational complexity remains O(N ) with the length of the generated video, while the memory complexity remains constant irrespective of the video duration. 3.4. Two-stage Training Given the enormous compute/data requirements for obtaining SOTA video diffusion models, our proposed method is instead centered around efficiently distilling existing bidirectional full softmax attention, e.g. Wan2.1, into our proposed RNN formulation. To achieve this, we propose twostage process: attention distillation and lightweight finetuning that we expand in the following. Thanks to this specific method design, we obtain recurrent video diffusion model with competitive quality, within less than 200 GPU-hours. 3.4.1. Attention Distillation We first distill bidirectional full softmax teacher model into causal hybrid attention student model. During this stage, each block is trained independently and the only learnable parameters are ϕq and ϕk per block, so as to let ϕ parameters to enable linear attention to approximate the corresponding softmax dependencies. This distillation setup doesnt require any prompt/video pairs for the training; the student model is trained to match the teacher activations for different prompts, noise samples and denoising iterations. The following equation formalizes this: ϕl = ϕl η ϕl (cid:12)y(l,ϵ,p,i) ˆy(l,ϵ,p,i)(cid:12) (cid:12) (cid:12) (cid:33) , (19) (cid:32) EϵN pP iS 3.4.2. Lightweight Fine-tuning After the pretraining distillation stage making the block attentions recurrent hybrid, we have obtained the ϕqs and ϕks per block. However, while the pretraining distillation helps preserve the general structure of the scenes, the details will be far from perfect, specifically on the transition smoothness between chunks, as the blocks are pretrained in isolation. Now fine-tuning the whole DiT model on modest set of prompt/video pairs, for small number of iterations (e.g. 1k) recovers the lost generation quality. This is done by optimizing the normal flow-matching objective [27]. 4. Experimental Setup 4.1. Evaluation of generation quality We evaluate ReHyAt by distilling and fine-tuning Wan2.1 1.3B model [35], widely used efficient SOTA model. For SOTA comparisons, we generate videos at the original Wan resolution and length (81 480 832) using the full set of extended prompts from VBench [14] and VBench-2.0 [51]. In addition to quantitative evaluation, we conduct blinded human preference study to assess visual qualities and prompt alignment. We randomly select 50 prompts from VBench and present participants with paired videos, asking them to choose their preferred video or indicate no significant difference. The order of paired videos randomly change per prompt to avoid any potential biases. In total, we collect 500 paired comparisons. To enable large-scale ablation studies, we train and evaluate our model variants at lower spatial resolution of 320 480 per frame. For all evaluations, we use the model snapshot at the 1000th fine-tuning iteration. 4.2. Assessment of compute complexity FLOPs Analysis. We analyze the number of floating point operations in the proposed ReHyAT method and compare it against flash attention, and other alternatives on the original 5-second Wan video generation setup, as well as analyzing the DiT blocks compute growth as we increase the length of generated videos. For this, we use the DeepSpeed library to measure the complexities. On-mobile Measurements. valuable advantage of the proposed recurrent hybrid attention method is that it computationally enables the generation of longer videos on edge devices such as mobile phones, thanks to lower compute burden, and most importantly, due to significant reduction in peak memory consumption. We port the transformed Models with 2B5B parameters Open-Sora Plan V1.3 [26] CogVideoX 5B [44] CogVideoX1.5 5B [44] Models up to 2B parameters Open-Sora V1.2 [52] LTX-Video [12] SnapGenV [41] Hummingbird 16frame [15] Mobile Video DiT - Mobile [40] Mobile Video DiT - Server [40] CogVideoX 2B [44] PyramidalFlow [16] Neodragon [19] Wan2.1 1.3B [35] Wan2.1 1.3B* [35] Linear/Hybrid Models Efficient VDiT [8] M4V [13] STA [50] VSA [49] SANA-Video [4] Attention Surgery (15R2) [11] Wan2.1 1.3B* + ReHyAt (15Tc=3,To=1) Total Quality Semantic 77.23 81.91 82.01 65.62 77.33 79. 80.14 83.05 82.72 79.76 80.00 81.14 81.35 81.45 83.09 81.55 81.72 81.61 83.31 83.10 76.14 81.91 83.00 82.77 83.71 83.21 83.79 81.35 82.30 83.47 83.73 83.12 84.65 82.48 84.74 83.68 85.23 85.10 - 83.36 85.37 83.60 84.35 85.19 84.57 73.39 70.79 71.84 71.84 74.76 76.86 77.81 69.62 73.36 75.65 75. - 76.10 73.52 79.47 81.35 75.25 80.70 Model VBench-2.0 Total Hum.Fid. Creativity Control. Com.sense Physics 56.0 Wan2.1 1.3B CogVideoX-1.5 5B 53.4 Attn. Surgery 15R2 55.1 ReHyAt 15Tc=3 56.1 ReHyAt 15Tc=5 56.3 80.7 72.1 78.9 81.9 79. 48.7 43.7 47.5 55.1 55.7 34.0 29.6 33.4 30.8 31.9 63.4 63.2 63.1 62.7 64.2 53.8 48.2 52.8 50.0 49.7 Table 2. Quantitative comparison on VBench-2.0 benchmark Prompt Dimension Color Human Action Object Class Overall Consistency Scene Spatial Relationship Subject Consistency Temporal Flickering Temporal Style Total Human Preference % Ours No preference Wan2.1 43.3 21.7 25.0 27.1 40.0 20.0 21.7 24.0 43. 27.6 46.7 41.7 45.0 47.1 60.0 70.0 28.3 54.0 30.0 43.5 10.0 36.7 30.0 25.9 0.0 10.0 50.0 22.0 26.7 29.0 Table 1. Comparisons with SOTA efficient video diffusion models. Wan2.1* is our best reproduction using our evaluation pipeline. Table 3. Results of the method-blinded human visual preference study over 500 paired video comparisons. Rows correspond to subsets filtered by different VBench prompt dimensions. original WAN model with flash attention blocks as well as the transformed ReHyAt modules to Qualcomm AI Runtime (QNN) and profile run-time metrics such as latency, memory read, and memory write on Snapdragon8-Gen4 SoC. For the on-device measurements we report the metrics on 320480 frame resolution with the original 5 seconds WAN video length, as well as longer video durations. 4.3. Training specification Datasets. For fine-tuning low-resolution models, we use 350K subset of the video dataset from Open-Sora Plan [26]. For high-resolution fine-tuning, we use 22K synthetic video samples generated by Wan2.1 14B, with prompts drawn from the same source as used for the low-resolution dataset. Model Hyperparameters. We experiment with converting different numbers of transformer blocks to recurrent hybrid attention: 15, 20, and 25 out of the 30 blocks in Wan2.1 1.3B. For the hybrid blocks, we explore hybridization with various chunk sizes (Tc {1, 2, 3, 5, 7}) as well as different options for overlap size (To {0, 1, 2, 3}). Empirical analysis of the impact of ϕk and ϕq transformation complexity on generation quality shows that lightweight 2-layer MLP with degree-2 polynomial features is sufficient. This configuration adds approximately 2.4M parameters per converted block. Additional details are provided in the appendix. 5. Results 5.1. Generation Quality VBench SOTA. Table 1 compares ReHyAt model distilled from Wan2.1 1.3B against the state-of-the-art efficient video diffusion models up to 5B parameters. We observe that our method performs very competitively, while forming chunk-wise RNN that enables running it on mobile. Note that the compute burden to obtain our model is 160 H100 GPU hours, i.e. less than 1% of SANA-Video (12 days of 64 H100) and less than 0.01% of MovieGen [32]. VBench2.0. Table 2 presents the evaluation and comparison of SOTA methods on VBench-2.0 benchmark. While we observe small drop, ReHyAt still remain competitive to larger models such as CogVideoX1.5 5B. Human Preference Evaluation. Table 3 shows the results for the human visual preference study comparing our 15Tc=3 model against the original Wan2.1 1.3B, from total of 500 paired video comparisons. As can be observed, there is no significant difference between our recurrent hybrid model and the original Wan2.1 in human preference. Figure 3 shows two qualitative samples and how ReHyAt compares to Wan2.1 1.3B. More extensive set of qualitative samples are provided in the supplementary materials. 5.2. Sampling Compute Burden In Fig. 4 we measure how chunk size Tc and chunk overlap size To impact the number of floating point operations, also Figure 3. Qualitative comparison of Wan2.1 1.3B (Top) to ReHyAt 15Tc=3 (bottom) for two sample VBench prompts, cat and dog. and dog drinking water. Attention Block Softmax Flash Attention HedgeHog Linear Attention Uniform Hybrid - R8 ReHyAt - Tc=3 (ours) Number of frames (320480) resolution 81 281 360 464 192 121 141 161 2964 455 625 247 4809 469 818 302 OOM 542 1215 OOM OOM OOM 384 Table 4. On mobile (Snapdragon8-Gen4) latency (ms) vs. number of frames at 320480 resolution with To=3, for various video durations from 5s (81 frames) to 10s (161 frames). As can be observed, our recurrent hybrid method is the only one that can easily extend to more than 10s without out-of-memory errors. Within the feasible extent for flash attention (e.g. on 121 frames), our method is 16 faster than flash attention used in Wan2.1 1.3B. Table 5 shows the memory read/write load that correlates with power consumption and latency. As we observe, due to its more memory-efficient design, our recurrent hybrid attention model is significantly more memory-efficient, e.g. 11more efficient in total memory read/write than flash attention at 121 frames (7.5s duration). Please note that while the total memory/read write is expected to grow linearly with video duration for ReHyAt, the peak-memory usage remains constant. 5.3. Ablations Studies Number of ReHyAt Blocks and Chunk-size Tc. Fig. 5 shows scatter plots comparing the computational cost of different variations of ReHyAt, with various number of converted blocks and chunk-sizes Tc as well as the original Wan2.1 1.3B model, at both 320480 and 480832 resolutions. Table 6 shows the Vbench full set evaluation for various Tcs. As expected, increasing Tc generally improves model quality; however, the increase from 1 to Figure 4. 213052 latent size (5 seconds) Comparison of attention compute (FLOPs) on how it compares to flash attention and uniform hybrid attention [11] with various rates {2, 4, 8}, as measured on 5s videos at 480832 resolution, corresponding to latent size 213052. As can be observed, ReHyAt offers up to 4 operation saving as compared to flash attention used by Wan2.1. On the other hand, our Tc=3, To=1 model variant remains 2 more efficient as compared to the better quality preserving = 2 uniform hybrid attention variation. Fig. 1 top demonstrates how the compute burden grows with increased video duration, comparing the scaling behavior for the Wan2.1 1.3B (flash attention) versus our proposed method (ReHyAt). Here we see that compared to flash attention, our recurrent hybrid attention has significantly better scaling behavior. Table 4 presents the on-mobile, DiT block latencies in miliseconds for various types of attention mechanism, flash attention, HedgeHog linear attention with learnable ϕ, uniform hybrid attention R=7 and our ReHyAt hybrid method Attention Block Softmax Flash Attention HedgeHog Linear Attention Uniform Hybrid - R8 ReHyAt - Tc=3 (ours) Number of frames - Memory Read/Write (GB) 81 101 121 161 5.1 5.7 6.3 1.7 6.0 8.1 10.1 2.8 12.9 7.0 5.2 2.2 16.4 10.1 10.9 3.6 22.7 6.9 6.4 2.7 53.6 11.3 13.2 4.4 R OOM OOM OOM OOM OOM OOM OOM OOM 8.0 7.8 3.0 13.2 35.2 4.8 3.5 5.6 Table 5. Comparison of total memory read/write for Wan2.1 DiT Blocks with various attention mechanisms on Snapdragon8-Gen4 Chunkoverlap To VBench Total Quality Semantic Subj. Cons. 0 1 2 81.56 82.17 82.17 82.19 83.23 83.72 83.84 83.86 74.90 75.96 75.50 75.51 90.90 92.05 92.13 92.24 Table 7. Impact of To on ReHyAt hybrid model quality as measured on VBench. All the models have 25 converted ReHyAt blocks with Tc=3. Causal Block TFLOPs VBench Total Quality Semantic 4.17 4.04 82.27 82.35 83.84 83.97 75.99 75.87 Table 8. Impact of causality on ReHyAt hybrid model quality as measured on VBench on 15Tc=3,To=0 configuration overlap size To values (ranging from 0 to 3) impacts the generation quality. As anticipated, enabling overlap (i.e., going from To = 0 to To = 1) results in notable jump in model quality; however, the total score appears to saturate after that. The mild gradual improvement is still noticeable in the subject consistency dimension. This underlies the importance of overlap mechanism in decreasing temporal incoherencies. Causality. Table 8 shows the compute and quality metrics for two equal hybrid attention formation, with causality being the only difference. We observe that the additional process to remove the non-causal attention dependency does not deteriorate the quality of the model, at least as measured by VBench. On the other hand, the saving in compute by just removing the forward-looking linear attention is not substantial. The major advantage of causal attention lies in enabling RNN reformulation, in turn enabling lower and constant peak memory and thus on-device generation of longer videos. Figure 5. The total DiT FLOPs percentages versus the VBench score of original Wan2.1 1.3B model compared to various hybrid configurations or 320480 (top) and 480832 (bottom) resolutions. Chunk-size Tc Block TFLOPs VBench Total Quality Semantic 1 2 3 5 3.87 4.04 4.30 4.82 80.97 82.08 82.17 82.48 82.37 83.86 83.72 84.12 75.39 74.99 75.96 75. Table 6. Impact of Tc on ReHyAt hybrid model quality. All the models have 25 converted ReHyAt blocks with To=1. yields more significant improvement compared to further increases to 3 and 4. This is perhaps due to the first extension of the softmax from spatial to spatiotemporal. Overlap size To. Table 7 demonstrates how different chunk 6. Conclusion and Future Work In this paper, we introduced ReHyAt, recurrent hybrid attention mechanism for video diffusion transformers that enables scalable, long-duration video generation with constant memory and linear compute requirements. Our lightweight distillation pipeline achieves near stateof-the-art quality with dramatically reduced training cost. While ReHyAt performs strongly overall, small fraction of videosespecially with the most efficient variantsstill show some temporal incoherence, highlighting an area for future improvement."
        },
        {
            "title": "References",
            "content": "[1] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45994603, 2023. [2] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for highresolution dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [3] Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, and Fei-Fei Li. Exploring diffusion transformer designs via grafting. In NeurIPS, 2025. [4] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. In Proceedings of Rethinking attention with performers. the International Conference on Learning Representations, 2021. [6] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synIn Forty-first thesis with hourglass diffusion transformers. International Conference on Machine Learning, 2024. [7] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient In Advances in Neural exact attention with io-awareness. Information Processing Systems, 2022. [8] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Efficient video diffusion transformers with attention tile. arXiv preprint arXiv:2502.06155, 2025. [9] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkvarXiv preprint like architectures for diffusion models. arXiv:2404.04478, 2024. [10] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformermamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. [11] Mohsen Ghafoorian, Denis Korzhenkov, and Amirhossein Habibian. Attention surgery: An efficient recipe to linarXiv preprint earize your video diffusion transformer. arXiv:2509.24899, 2025. [12] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. [13] Jiancheng Huang, Gengwei Zhang, Zequn Jie, Siyu Jiao, Yinlong Qian, Ling Chen, Yunchao Wei, and Lin Ma. M4v: arXiv Multi-modal mamba for text-to-video generation. preprint arXiv:2506.10915, 2025. [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [15] Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, and Emad Barsoum. Amd-hummingbird: Towards an efficient text-to-video model. arXiv preprint arXiv:2503.18559, 2025. [16] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong MU, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In The Thirteenth International Conference on Learning Representations, 2025. [17] Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki Asano, and Amirhossein Habibian. ObjectIn European centric diffusion for efficient video editing. Conference on Computer Vision, pages 91108. Springer, 2024. [18] Adil Karjauv, Noor Fathima, Ioannis Lelekas, Fatih Porikli, Movie: arXiv preprint Amir Ghodrati, and Amirhossein Habibian. Mobile diffusion for video editing. arXiv:2412.06578, 2024. [19] Animesh Karnewar, Denis Korzhenkov, Ioannis Lelekas, Noor Fathima, Adil Karjauv, Vancheeswaran Vaidyanathan Hanwen Xiong, Will Zeng, Rafael Esteves, Tushar Singhal, Fatih Porikli, Mohsen Ghafoorian, and Amirhossein Habibian. Neodragon: Mobile video generation using diffusion transformer. arXiv preprint arXiv:2511.06055, 2025. [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, pages 51565165. PMLR, 2020. [21] Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee. On-device sora: Enabling training-free diffusion-based text-to-video generation for mobile devices. arXiv preprint arXiv:2502.04363, 2025. [22] Tencent AI Lab. Hunyuanvideo: systematic framearXiv preprint work for large video generation model. arXiv:2412.03603, 2025. [23] Pierre-David Letourneau, Manish Kumar Singh, HsinPai Cheng, Shizhong Han, Yunxiao Shi, Dalton Jones, Matthew Harper Langston, Hong Cai, and Fatih Porikli. Padre: unifying polynomial attention drop-in replacement for efficient vision transformer. In The Thirteenth International Conference on Learning Representations, 2025. [24] Qirui Li, Guangcong Zheng, Qi Zhao, Jie Li, Bin Dong, Yiwu Yao, and Xi Li. Compact attention: Exploiting structured spatio-temporal sparsity for fast video generation. arXiv preprint arXiv:2508.12969, 2025. [25] Yifan Li et al. Sana: Efficient attention for diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [26] Bin Lin, Yunyang Ge, Xinhua Cheng, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [28] Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge Ritter. Video diffusion models: survey. Transactions on Machine Learning Research, 2024. [29] Jean Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models. In First Conference on Language Modeling, 2024. [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [31] Elia Peruzzo, Adil Karjauv, Nicu Sebe, Amir Ghodrati, and Amir Habibian. Adaptor: Adaptive token reduction for video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 63656371, 2025. [32] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [33] Markus N. Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. [35] Team Wan et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [36] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear compuIn Proceedings of the Computer Vitational complexity. sion and Pattern Recognition Conference, pages 25782588, 2025. [37] Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37:6243262457, 2024. [38] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. In Proceedings of the International Conference on Learning Representations, 2020. [39] Yimu Wang, Xuye Liu, Wei Pang, Li Ma, Shuai Yuan, Paul Debevec, and Ning Yu. Survey of video diffusion models: Foundations, implementations, and applications. Transactions on Machine Learning Research, 2025. [40] Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, et al. Taming diffusion transformer arXiv preprint for real-time mobile video generation. arXiv:2507.13343, 2025. [41] Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, et al. Snapgen-v: Generating five-second video within five seconds on mobile device. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2479 2490, 2025. [42] Haitam Ben Yahia, Denis Korzhenkov, Ioannis Lelekas, Amir Ghodrati, and Amirhossein Habibian. Mobile video diffusion. In ICCV, 2025. [43] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024. [44] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Zhang Yuxuan, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [45] Yuan Yao, Yicong Hong, Difan Liu, Long Mai, Feng Liu, and Jiebo Luo. Diffusion transformer-to-mamba distillation for high-resolution image generation. arXiv preprint arXiv:2506.18999, 2025. [46] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning Representations, 2024. [47] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. arXiv preprint arXiv:2402.04347, 2024. [48] Michael Zhang, Simran Arora, Rahul Chalamala, Benjamin Frederick Spector, Alan Wu, Krithik Ramesh, Aaryan Singhal, and Christopher Re. LoLCATs: On low-rank linearizing of large language models. In The Thirteenth International Conference on Learning Representations, 2025. [49] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Faster video diffusion with trainable sparse attention. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [50] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. [51] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [52] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [53] Lianghui Zhu, Zilong Huang, Hanshu Yan, Jiashi Feng, Bencheng Liao, Jun Hao Liew, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear atIn Proceedings of the IEEE/CVF Conference on tention. Computer Vision and Pattern Recognition, 2025. ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Appendix 7.1. Training Details and Hyperparameters Unless stated otherwise in the ablation studies, we parameterize ϕ using two-layer MLP with polynomial degree of 2. For each hybrid block, we apply separate transformations for keys and queries, denoted as ϕk and ϕq. Pretraining (Distillation Stage). During pretraining, each block is trained independently while all parameters remain frozen except for ϕk and ϕq. These are optimized using AdamW with batch size of 1 and learning rate of 103, following the value distillation objective described in Equation (19). Teacher activations for distillation are obtained by sampling with 50 denoising steps and guidance scale of 5, using the Euler Ancestral Discrete Scheduler to integrate the reverse diffusion process. Finetuning. In the finetuning stage, we update all parameters of the hybrid DiT, including the ϕ transformations and feed-forward MLP layers. Training uses AdamW with batch size of 16, learning rate of 105, and bf16 mixedprecision. The model is trained for 1,000 iterations. Sampling. For generating videos for VBench evaluation, we employ Wan Enhanced prompts and the following sampling configuration: 50 denoising iterations, classifier guidance scale of 6, and the UniPCMultistep noise scheduler with flow shift of 8. 7.2. Qualitative Samples Figures 824 present uniformly spaced frames from videos generated by the original Wan2.1 1.3B model and several variants of our recurrent hybrid attention models (15Tc=5, 15Tc=3, and 20Tc=3) across 18 prompts at the original resolution of 480832. Full video sequences corresponding to these frames are included in the supplementary materials. 7.3. Detailed VBench Comparison Figure 7 compares selected subset of our hybrid models against Wan2.1 1.3B across all VBench dimensions, evaluated on the full benchmark set at the original resolution (480832). 7.4. Detailed VBench-2.0 Comparison Tables 911 report fine-grained results on the recent VBench-2.0 benchmark at 480832 resolution. We compare two ReHyAt variants (15Tc=3 and 15Tc=5) against Wan2.1 1.3B and attention surgery (15R2). Both hybrid variants perform on par with Wan2.1 1.3B in terms of the overall Total score. Figure 6. Compute complexity growth comparisons w.r.t. video length versus Wan2.1 flash attention and attention surgery, in FLOPs (top) and latency (bottom) 7.5. Compute complexity vs Attention Surgery Figure 6 shows comparison of our recurrent hybrid attention block in terms of scalability with respect to the video length versus attention surgery hybrid and original Wan2.1 flash attention blocks. 7.6. Use of Large Language Models We used Microsoft Copilot (a large language model) exclusively to improve clarity and readability. All technical content, experimental design, and conclusions are entirely our own. Figure 7. Radar plot comparing subset of our hybrid models with the original Wan 1.3B model on the full VBench set and 480832 resolution"
        },
        {
            "title": "Instance\nPreservation",
            "content": "Multi-View Consistency"
        },
        {
            "title": "Complex\nPlot",
            "content": "Wan2.1 1.3B Attention Surgery (15R2) RehHyAt 15Tc=3 RehHyAt 15Tc=5 63.5 62.7 64.7 61.6 25.1 25.1 28.5 28.0 16.4 18.4 14.7 16.7 86.0 84.8 78.4 83.6 9.6 7.1 12.1 10. 97.9 97.1 98.1 94.2 49.1 44.0 22.0 28.6 11.3 13.2 12.7 15.6 Table 9. Full VBench-2.0 results (part 1/3)."
        },
        {
            "title": "Motion Order\nUnderstanding",
            "content": "Wan2.1 1.3B Attention Surgery (15R2) RehHyAt 15Tc=3 RehHyAt 15Tc=5 72.4 66.4 63.7 64.7 80.6 77.0 83.0 83.6 48.4 46.4 46.4 51.0 71.7 70.3 75.0 72.3 40.8 41.4 47.1 44. 69.4 67.3 69.6 67.8 49.1 48.5 63.8 60.4 32.0 33.7 37.0 34.3 Table 10. Full VBench-2.0 results (part 2/3)."
        },
        {
            "title": "Total\nScore",
            "content": "Wan2.1 1.3B Attention Surgery (15R2) RehHyAt 15Tc=3 RehHyAt 15Tc=5 32.1 29.0 25.9 29.0 61.7 70.5 54.6 55.7 48.7 47.5 55.1 55.7 63.4 63.1 62.7 64.2 34.0 33.4 30.8 31. 80.7 79.0 81.9 79.8 53.3 52.8 50.0 49.7 56.0 55.1 56.1 56.3 Table 11. Full VBench-2.0 results (part 3/3). 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 8. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt cat eating food out of bowl Figure 9. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt person playing guitar 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 10. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt cute fluffy panda eating Chinese food in restaurant Figure 11. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt cute happy Corgi playing in park, sunset, with an intense shaking effect 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 12. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt dog running happily Figure 13. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt fat rabbit wearing purple robe walking through fantasy landscape. 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 14. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt person is crying Figure 15. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt cow bending down to drink water from river 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 16. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt bigfoot walking in the snowstorm. Figure 17. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt bear sniffing the air for scents of food 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 18. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt person is using computer Figure 19. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt sheep taking peaceful walk 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 20. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt Cinematic shot of Van Goghs selfie, Van Gogh style Figure 21. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt happy dog wearing yellow turtleneck, studio, portrait, facing camera, dark background 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 3 . 1 1 . 2 5 = 5 3 = 5 1 3 = 0 2 Figure 22. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt this is how do makeup in the morning. Figure 23. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt Vampire makeup face of beautiful girl, red contact lenses. 3 . 1 1 . 2 W 5 = 5 1 3 = 5 1 3 = 0 2 Figure 24. Qualitative videos comparing original Wan2.1 1.3B model to our various hybrid variations for input prompt An astronaut flying in space, featuring steady and smooth perspective"
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}