{
    "paper_title": "Motion Control for Enhanced Complex Action Video Generation",
    "authors": [
        "Qiang Zhou",
        "Shaofeng Zhang",
        "Nianzu Yang",
        "Ye Qian",
        "Hao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/."
        },
        {
            "title": "Start",
            "content": "MVideo: Motion Control for Enhanced Complex Action Video Generation Qiang Zhou1, Shaofeng Zhang2, Nianzu Yang2, Ye Qian1, Hao Li3* 1INF Tech. 2Shanghai Jiao Tong University, 3Fudan University {zhouqiang, qianye.0514}@inftech.ai, {sherrylone, yangnianzu}@sjtu.edu.cn, lihao lh@fudan.edu.cn 4 2 0 2 3 1 ] . [ 1 8 2 3 8 0 . 1 1 4 2 : r tions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/. 1. Introduction Despite significant progress in the field of image [5, 7, 8, 14, 15, 20, 27, 45] and video generation [1, 3, 4, 10, 18, 19, 21, 25, 28, 29, 31, 33, 34, 3638, 42, 47, 49], current models still face substantial challenges when tasked with generating complex action videos. One of the core difficulties lies in the inability of text-based descriptions to fully capture the nuanced details of intricate movements and actions. This limitation significantly hinders the models ability to accurately train and infer complex action sequences in videos. Furthermore, these action videos often span longer durations, increasing the complexity of the generation process by requiring models to maintain temporal coherence across extended periods. In this work, we introduce MVideo, novel framework specifically designed to tackle the inherent challenges of generating complex action videos. Traditional video generation models rely heavily on textual descriptions, which are often inadequate for conveying the dynamic intricacies of complex actions. To overcome this limitation, MVideo leverages mask sequences as an additional conditioning input. Unlike text prompts, mask sequences offer more precise and explicit representation of the desired actions, allowing the model to generate videos that more accurately capture the intended movements. Thanks to advancements in foundational vision models, such as GroundingDINO [17] and SAM2 [23], the mask sequences can be extracted automatically, enhancing MVideos efficiency and robustness in action video generation. Existing video difFigure 1. Complex action videos generated by MVideo, with duration of 12 seconds and spatial resolution of 480 720 pixels."
        },
        {
            "title": "Abstract",
            "content": "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. key limitation lies in the text prompts inability to precisely convey intricate motion details. To address this, we propose novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion condi- *: Corresponding author 1 fusion models can only generate few seconds of video, which is insufficient for creating complete and coherent motion sequence. To address this, MVideo proposes an efficient iterative video generation method that combines image conditions with low-resolution video conditions. This method reduces the computational cost of the model while maintaining temporal consistency, ensuring that even longer videos maintain coherent content and consistent action sequences throughout their duration. MVideo is fine-tuned from the existing video diffusion model CogvideoX [42]. However, through experimentation, we observed that simply adding the mask sequence condition during fine-tuning led to noticeable decline in the models ability to align text prompts with video generation, as reflected in the reduced metrics for overall consistency and imaging quality shown in Table 4. To mitigate this issue, we propose novel consistency loss during training, which distills the text-to-video generation expertise of CogvideoX into MVideo, ensuring that the model not only learns to align mask sequences but also preserves its original textcondition capabilities. Our experiments demonstrate that MVideo effectively aligns text prompts with motion conditions and generalizes to previously unseen mask sequences. This deep alignment capability enables MVideo to generate complex video effects. For instance, given motion condition, MVideo allows modification of foreground objects or background scenes via text prompts and supports editing or combining motion conditions to produce more intricate actions. Additional examples are provided in the case study section 4.3. In summary, our contributions of MVideo are threefold: We introduce MVideo, novel framework that iteratively generates long-duration action videos by integrating additional motion conditions for precise motion control. We show that MVideo generalizes effectively, aligning with unseen motion conditions and enabling complex video generation through motion condition editing and combination. We validate MVideos effectiveness through quantitative and visual comparisons with state-of-the-art video diffusion methods. 2. Related Work diffusion model. Video Video diffusion models (VDMs) [10] have pioneered video generation by extending traditional image diffusion U-Net [24] architectures into 3D U-Net structures, employing joint training with image and video data to address video generation tasks effectively. In contrast to VDMs, Make-A-Video [26] innovates by learning visual-text correlations from paired image-text data and capturing motion dynamics from unsupervised video data, scalable generation. MagicVideo [48] and LVDM [9] extend enabling flexible, VDMs by implementing the Latent Diffusion Model (LDM) for text-to-video (T2V) generation, with LVDM employing hierarchical framework to enhance latent representation modeling. VideoFactory [32] introduces swapped cross-attention mechanism, optimizing temporal and spatial interactions, while ModelScope [29] advances T2V with spatial-temporal convolution and attention in LDM, enriching feature representation and motion comprehension. Show-1 [44] combines pixeland latent-based diffusion models, showcasing the benefits of integrating different paradigms. SVD [2] further validates VDMs adaptability across data scales and complexities. Recently, CogvideoX [42] set new standard for longduration, coherent video generation with significant motion dynamics. Our model, MVideo, builds on CogvideoX [42], finetuning it to enhance action generation capabilities, advancing video diffusion model performance. Motion control. MCDiff [6] is the first approach to use motion as condition for controlled video synthesis. It takes the first frame of video and sequence of motion strokes as input. DragNUWA [43] combines text, image, and trajectory inputs for precise control over video content in semantic, spatial, and temporal dimensions. To address limitations in open-domain trajectory control, it introduces three innovations: Trajectory Sampler (TS) for arbitrary trajectory control, Multiscale Fusion (MF) for granular trajectory control, and an Adaptive Training (AT) strategy to ensure trajectory-consistent video generation. MotionCtrl [35] presents two control modules for camera and object motion. It proposes specific dataset collection methods for these motion types, allowing the trained modules to be integrated into various video diffusion models for enhanced motion control. Direct-a-Video [41] propose strategy for decoupling control of object and camera motion, while MotionBooth [39] animates customized subjects with precise control over both object and camera movements. FreeTraj [22] introduces tuning-free framework to achieve trajectory-controllable video generation, by imposing guidance on both noise construction and attention computation. VMC [12] presents novel one-shot tuning approach, focusing on adapting temporal attention layers within video diffusion models to generate motion-driven videos. Motion Inversion [30] introduces simple yet effective motion embedding that is disentangled from appearance and suitable for motion customization tasks MotionDirector [46] propose dual-path architecture and novel appearancedebiased temporal training objective, to decouple the learning of appearance and motion Previous works use trajectory lines to control target motion, but they struggle with complex actions, such as backflips. In contrast, our approach, MVideo, employs more pre2 cise mask sequences to generate videos of complex actions effectively. 3. Framework In this section, we introduce the MVideo framework in detail, covering the motion conditions, long-duration motion video generation, and training objectives. 3.1. Preliminary Video diffusion models are type of probabilistic generative model that simulates forward and reverse diffusion process. The forward process involves gradually adding noise to video data, transforming it into standard Gaussian distribution. The reverse process, which the model learns, involves denoising and reconstructing the original video data from the noise. The training objective is to learn the reverse diffusion process, typically through loss function that measures the discrepancy between the predicted noise and the true noise added at each step. The commonly used loss function in video diffusion models is the Mean Squared Error (MSE) between the true noise ϵ and the predicted noise ˆϵ. The loss formula is: = Ex,ϵ,c,t[ϵ ˆϵ(xt, c, t)2 2] (1) where is the original video data, xt is the noisy data at step t, is the text prompt, ϵ is the true noise added to x, and ˆϵ(xt, c, t) is the models predicted noise. Moreover, rather than reconstructing the raw video data directly, video VAE [13] encoder is typically used to first transform the raw video into compressed latent representations. This method notably decreases the computational demands on the video diffusion model. 3.2. Motion Control Mask-to-Video. Current video diffusion models often struggle to depict intricate object movements accurately, as text prompts alone are frequently insufficient to specify complex action details. For generating high-complexity action videos, more precise guidance on object actions is essential. In this work, we demonstrate that mask sequences can effectively supplement or replace traditional text prompts. By integrating mask sequences with textual descriptions, our approach enhances the models capability to produce videos with intricate object actions. Mask Extraction. Efficient mask sequence extraction is crucial for the practical application of video diffusion models. Recent advances in foundational vision models provide robust support for this task, facilitating mask extraction across diverse objects. In our method, we employ the GroundingDINO [17] and SAM2 [23] models to automate the generation of mask sequences, as illustrated in Figure 2. Given reference video and description of the target object, we first utilize the GroundingDINO model to identify the bounding box of the object in the initial frame. This bounding box is then input to the SAM2 model, which segments the mask sequence of the target object across the entire video. Aside from basic text prompt, our framework requires only reference video and an object description, enabling the automated extraction of the mask sequence as conditioning input for motion. Mask Fusion. Several approaches can be used to incorporate the mask sequence condition into the diffusion model. Based on our experiments, we found channel concatenation to be particularly effective. This method offers the advantage of not increasing the parameter count or computational complexity of the diffusion transformer, ensuring compatibility with various pretrained video diffusion models. Specifically, we first employ motion encoder to extract motion features from the mask sequences, which are then fused with the noisy latent representations through channel-wise concatenation, as shown in Figure 3. Our experiments show that pretrained video VAE encoder performs effectively as the motion encoder, allowing us to use it without further optimization during training. 3.3. Long Motion Video Generating complex action videos, such as Tai Chi performances, requires longer durations to accurately capture sequence of intricate actions. However, existing pretrained video diffusion models are generally limited to producing clips only few seconds long, which falls short for creating such complex action videos. In this work, we propose an efficient approach for recursively generating high-quality, long-duration videos. Our method involves generating video clips of t1 seconds each (defaulting to 4 seconds) and concatenating them to form the final video. To ensure temporal consistency in appearance and motion across the extended video, we introduce two additional conditioning mechanisms: high-resolution appearance condition and low-resolution motion condition, as illustrated in Figure 3. The appearance condition enhances visual consistency by incorporating high-resolution framesspecifically, the final frame from the preceding clipto align the appearance of the current clip with previous ones. For motion consistency, we introduce motion condition that maintains coherent motion throughout the video by using low-resolution video segments of t2 seconds (also defaulting to 4 seconds) from prior clips as input. To balance efficiency with quality, these motion conditions are set to low resolution of 256 384. Both the appearance and motion conditions are encoded as latent features using video VAE encoder, and these features are concatenated with the noisy latent representations along the 3 Figure 2. Inference Pipeline: Given text prompt and simple description identifying the reference object in the video, MVideo automatically extracts the corresponding mask sequence and iteratively generates long-duration motion videos, using the mask sequence as an additional condition. Figure 3. Training Pipeline: MVideo takes text prompt as input, along with mask sequence, high-resolution image, and lowresolution video as conditions. During training, diffusion loss adapts these new conditions, while consistency loss preserves the models text-alignment capability. sequence dimension, as shown in Figure 3. Experimental results demonstrate that our framework effectively generates long-duration action videos while maintaining strong temporal consistency in both content and motion. 3.4. Training Loss MVideo is finetuned from the pretrained video diffusion model, CogVideoX [42], using custom mask-tovideo training dataset. This finetuning process enhances MVideos mask sequence alignment performance but results in decreased text alignment ability and overall video quality. As shown in Table 4, the evaluation metrics for overall consistency and imaging quality on the VBench [11] test set have declined significantly. To improve MVideos mask sequence alignment while retaining the text alignment capabilities learned by the original CogVideoX model, we introduced consistency loss term in addition to the diffusion loss. This consistency loss is computed as the L2 loss between MVideos predictions and CogVideoXs predictions, serving to maintain alignment between the two models outputs. Our ablation studies confirm that incorporating this consistency loss effectively preserves MVideos text alignment capabilities without compromising mask sequence alignment learning. The final loss function for MVideo training is therefore defined as follows: = Ld + α Lc, (2) where Ld is the diffusion loss, Lc is the consistency loss, and α represents the loss weight, set to 1.0 by default. 4. Experiments 4.1. Experimental setup Implementation details. MVideo is initialized from CogVideoX [42], highly effective open-source video diffusion model. To conserve training resources, we freeze the parameters of CogVideoX-5b and employ LoRA finetuning to incorporate capabilities such as mask sequence conditioning and long video generation. MVideo is trained using 32 GPUs with total batch size of 64 over 10,000 steps. The learning rate is set to 2 104, and the optimizer utilized is Adam. The default resolution for video generation is set to 480 720. For videos longer than 4 sec4 Data Trainset Mask-to-Video text mask sequence Testset VBench [11] Mask-to-Video ComplexMotion number of samples 400, 100 500 200 Model Video overall consistency imaging quality motion smoothness VBench [11] testset OpenSora-v1.2 [47] CogvideoX-2b [42] CogvideoX-5b [42] 4s 720 1280 6s 480 720 6s 480 720 MVideo-5b 12s 480 720 25.63 25.45 26.29 26. 62.76 61.48 61.55 60.53 98.95 98.02 97.44 97.19 Table 1. Statistics of the training and testing data used in MVideo. Table 2. Text alignment performance comparison. onds, the resolution for the low-resolution video condition defaults to 256 384. Training data. The training of MVideo requires triplet text, mask sequence>. data in the format of <video, To prepare these training data, we automatically generated 400,000 samples using the GroundingDINO [17] and SAM2 [23] models, which we refer to as the mask-to-video training set. Specifically, we performed scene cuts on the original HDVILA [40] dataset and then selected video clips longer than 8 seconds from the resulting segments. For each video clip, we initially employed the GroundingDINO model to identify bounding boxes around objects in the first frame. These bounding boxes were then used to initialize the SAM2 model, which subsequently extracted object masks across all frames of the video. Since extracting mask sequences for every object in video is computationally intensive, we focused on the 80 object categories defined in the MSCOCO [16] dataset. Our subsequent experiments indicate that after training MVideo on these mask sequences, the model effectively generalizes to mask sequences of objects beyond these 80 categories during testing. Finally, we utilized the VILA [40] model to extract detailed video captions for these video clips. Testing data. We utilize multiple test sets to systematically evaluate the performance of MVideo, including the VBench [11] test set, the Mask-to-Video test set, and the ComplexMotion test set. The VBench test set does not contain mask sequences and is primarily used to assess MVideos text-to-video generation capabilities across various video durations. The Mask-to-Video test set is collected from the HDVILA dataset and includes mask sequences that do not appear in the training data, allowing us to evaluate MVideos generalization performance in mask alignment. The ComplexMotion test set is specifically curated from the Pexels website to include videos featuring complex motion, enabling an assessment of MVideos generation quality in challenging dynamic scenarios. Metrics. For the quantitative evaluation metrics, we primarily employ the metrics provided by VBench [11] to assess various aspects of the generated videos, including overall consistency and motion smoothness. Additionally, we 5 Model Video Mask-to-Video testset ComplexMotion testset MVideo-5b 12s 480 720 77.90 78. Table 3. Mask sequence alignment performance evaluated using the mask mIoU metric Sm. introduce novel metric called mask mIoU, which specifically evaluates the alignment between the videos generated by MVideo and the corresponding mask conditions. Specifically, for each generated video frame produced by MVideo, we use the bounding box from the ground truth mask to initialize the SAM2 model, which then extracts the corresponding mask ˆm from the generated frame. The mask Intersection over Union (IoU) is calculated between masks and ˆm. Finally, the mask mIoU metric Sm is calculated as the average IoU across all videos and frames in the test set, Sm ="
        },
        {
            "title": "1\nN M",
            "content": "(cid:88) i,j IoU(mi,j, ˆmi,j), (3) where represents the number of generated videos, denotes the number of frames in each video, {0, 1, . . . , 1} is the index of each video, and {0, 1, . . . , 1} is the index of the frames within video. 4.2. Results MVideo is conditioned on both text and mask sequences, and the alignment capabilities for these two conditions are evaluated separately. For text alignment, we use the VBench [11] test set, while for the newly introduced mask sequence alignment, we use the collected mask-to-video test set and the ComplexMotion test set. Text alignment. Table 2 shows that on the VBench test set, MVideo-5b achieves comparable performance in overall consistency, image quality, and motion smoothness to OpenSora-v1.2 and CogVideoX-5b, indicating robust textto-video generation capabilities. Ablation studies suggest that this performance is largely due to the consistency loss applied during training. Mask sequence alignment. The mask-to-video training set for MVideo was created using 80 object names from the MSCOCO dataset. To evaluate generalization, we tested MVideos alignment performance on unseen objects by constructing mask-to-video test set with triplets in the form Figure 4. Visual comparison of videos generated by OpenSora-v1.2 [47], CogVideoX-5b [42], and MVideo-5b. <video, text, mask sequence>. This test set includes mask sequences from objects absent in the training data, such as cheetah, deer, llama, penguin, sea turtle, squirrel, and tiger, with all videos sourced from the public HDVILA dataset. As shown in Table 3, MVideo exhibits strong generalization in mask alignment, achieving high mIoU scores on unseen object mask sequences. Even with novel mask sequences, MVideo consistently aligns masks accurately in generated videos. Notably, on the ComplexMotion test set, it achieves mask mIoU of 78.34, demonstrating effective alignment in scenarios involving complex motion. 4.3. Case Study sual comparisons that more intuitively highlight MVideos superior performance in complex action video generation. Figure 4 clearly shows that, compared to text-to-video models such as OpenSora-v1.2 [47] and CogVideoX-5b [42], MVideo achieves significantly greater complexity, accuracy, and coherence in action representation. Altering background scenes. By integrating mask sequence and text prompt alignment, MVideo enables diverse video generation. For instance, with fixed mask sequence, MVideo can produce varied scenes by simply adjusting the text prompt, as shown in Figure 5. Comparison with State-of-the-Art T2V Models. As previously noted, text prompts alone cannot fully describe complex actions, limiting the VBench [11] evaluation metrics in capturing MVideos advantages for generating intricate movements. To address this, we provide additional viAltering moving objects. Besides modifying video backgrounds via text prompts, MVideo enables altering moving objects as well. For instance, as shown in Figure 6, given mask sequence of running horse, we can generate video of either regular horse running on the moon or robotic 6 Figure 5. MVideo generates videos with identical motion conditions and varied text prompts to change scenes. Each video is 12 seconds long with resolution of 480 720 pixels. Figure 7. Different camera motion videos generated from the same mask sequence. Figure 6. Videos generated with fixed mask sequence, modifying text prompts to alter or introduce new moving subjects. horse performing the same action. Moreover, we can introduce new objects that are not in the original mask sequence. Figure 6 illustrates this capability, showing how the same horse mask sequence can be used to create videos of monkey or an astronaut riding the horse as it runs. Figure 8. More dynamic videos generated by MVideo through mask sequence editing. zoom-out effects. Altering camera motion. Adjusting mask scale and position enables various camera motions in videos. As shown in Figure 7, the same mask sequence produces zoom-in and Editing mask sequence. Editing an existing mask sequence allows for generating more dynamic videos. For instance, as shown in Figure 8, mask sequence of tiger running on flat ground can be modified to depict the tiger 7 Model Video VBench test set overall consistency imaging quality motion smoothness MVideo-2b 4s 480 720 8s 480 720 12s 480 720 16s 480 720 25.55 26.48 25.71 26.14 58.39 58.44 57.76 56. 98.20 97.96 97.83 98.07 Table 5. Ablation on long video generation. MVideo-2b is finetuned from CogVideoX-2b using LoRA and is evaluated on the VBench [11] test set without using any mask condition. All generated videos are produced at frame rate of 8 FPS. to-video dataset reduces MVideos text alignment performance, as indicated by declines in overall consistency and imaging quality metrics. As shown in Table 4, testing on the VBench set revealed drop in overall consistency from 25.45 to 23.72 and imaging quality from 61.48 to 52.67. To address this degradation, we introduced consistency loss that effectively preserves MVideos text alignment and imaging quality. As demonstrated in Table 4, adding the consistency loss during training significantly improves both overall consistency and imaging quality metrics. Long motion video. MVideo iteratively generates long motion videos by generating video clips of 4 seconds each, which are then concatenated to form complete long video. To ensure temporal consistency in the generated long video, MVideo introduces high-resolution image conditions and low-resolution video conditions to enhance both content and temporal coherence. As shown in Table 5, we evaluated the performance of MVideo-2b on the VBench test set by generating videos of varying lengths. The results indicate that MVideo-2b successfully scales the video duration from 4 seconds to 16 seconds while maintaining stable overall evaluation metrics, with no significant drop in performance as the length increases. 5. Conclusion In this work, we introduce MVideo, novel framework for generating complex action videos with improved precision and temporal consistency. Unlike traditional models that rely solely on text prompts, MVideo leverages mask sequences as an additional conditioning input, offering clearer and more accurate depiction of intricate movements. This method allows for better capture of dynamic action sequences, addressing key limitations in current video generation models. Moreover, MVideos iterative generation approach balances computational efficiency with temporal coherence, enabling the creation of longer, narratively cohesive videos without sacrificing consistency. Our experiments demonstrate MVideos strong generalization in mask Figure 9. Videos generated by MVideo through combining multiple mask sequence conditions. Model Consistency loss Video VBench test set overall consistency imaging quality motion smoothness CogvideoX-2b MVideo-2b MVideo-2b 6s 480 8s 480 720 8s 480 720 25.45 23.72 26.48 61.48 52.67 58.44 98. 97.98 97.96 Table 4. Ablation study on consistency loss. MVideo-2b is finetuned from CogVideoX-2b using LoRA and is evaluated on the VBench [11] test set without utilizing any mask condition. running uphill or downhill. This mask-editing capability significantly enhances MVideos versatility. Combine motion conditions. MVideo supports the integration of multiple mask sequence conditions in video generation, enabling the creation of complex and engaging content. For example, as shown in Figure 9, combining the mask sequences of tiger and two dancing individuals allows the generated video to simultaneously capture both distinct motion patterns. 4.4. Ablation study In these ablation experiments, MVideo utilizes CogVideoX2b as the pretrained video diffusion model, with LoRA finetuning on mask-to-video training samples. Training is conducted over 10,000 steps on 16 GPUs with total batch size of 32 and learning rate of 2e-4. Consistency loss. Our experiments show that further finetuning the pretrained video diffusion model on the mask8 alignment and its ability to generate complex action videos by editing or combining mask sequences."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. 1 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 1 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1 [6] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis, 2023. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 1 [9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023. 2 [10] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. 1, 2 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2180721818. IEEE, 2024. 4, 5, 6, [12] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models, 2023. 2 [13] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 3 [14] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024. 1 [15] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam. Rich human feedback for text-to-image generation, 2024. 1 [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects In Computer Vision - ECCV 2014 - 13th Euin context. ropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740755. Springer, 2014. [17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2024. 1, 3, 5 [18] Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole. Vidpanos: Generative panoramic videos from casual panning videos, 2024. 1 [19] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation, 2024. 1 [20] Rishubh Parihar, Sachidanand VS, Sabariswaran Mani, Tejan Karmali, and R. Venkatesh Babu. Precisecontrol: Enhancing text-to-image diffusion models with fine-grained attribute control, 2024. 1 [21] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff 9 Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. [22] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models, 2024. 2 [23] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 1, 3, 5 [24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. 2 [25] Tongkai Shi, Lianyu Hu, Fanhua Shang, Jichao Feng, Peidong Liu, and Wei Feng. Pose-guided fine-grained sign language video generation, 2024. 1 [26] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. 2 [27] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. [28] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. Videotetris: Towards compositional text-to-video generation, 2024. 1 [29] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. 1, 2 [30] Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Yingcong Chen. Motion inversion for video customization, 2024. 2 [31] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicvideo-v2: Multi-stage high-aesthetic video generation, 2024. 1 [32] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Swap attention in spatiotemporal diffusions for text-to-video generation, 2024. 2 [33] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. 1 [34] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models, 2024. 1 [35] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation, 2024. [36] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion, 2023. 1 [37] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control, 2024. [38] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, and Zhiwei Xiong. Artv: Auto-regressive text-to-video generation with diffusion models, 2023. 1 [39] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation, 2024. 2 [40] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions, 2022. 5 [41] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn Special directed camera movement and object motion. Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers 24, page 112. ACM, 2024. [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2024. 1, 2, 4, 5, 6 [43] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, 2023. 2 [44] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023. 2 [45] Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, and Junchi Yan. Continuous-multiple image outpainting in one-step via positional query and diffusion-based approach. arXiv preprint arXiv:2401.15652, 2024. 1 [46] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-tovideo diffusion models, 2023. 2 [47] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang 10 You. Open-sora: Democratizing efficient video production for all, 2024. 1, 5, 6 [48] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models, 2023. 2 [49] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model, 2024."
        }
    ],
    "affiliations": [
        "INF Tech",
        "Shanghai Jiao Tong University",
        "Fudan University"
    ]
}