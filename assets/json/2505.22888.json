{
    "paper_title": "When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy",
    "authors": [
        "Jirui Qi",
        "Shan Chen",
        "Zidi Xiong",
        "Raquel Fernández",
        "Danielle S. Bitterman",
        "Arianna Bisazza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, their ability to think in other languages is less studied. This capability is as important as answer accuracy for real world applications because users may find the reasoning trace useful for oversight only when it is expressed in their own language. We comprehensively evaluate two leading families of LRMs on our XReasoning benchmark and find that even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing a substantial gap in multilingual reasoning. Prompt based interventions that force models to reason in the users language improve readability and oversight but reduce answer accuracy, exposing an important trade off. We further show that targeted post training on just 100 examples mitigates this mismatch, though some accuracy loss remains. Our results highlight the limited multilingual reasoning capabilities of current LRMs and outline directions for future work. Code and data are available at https://github.com/Betswish/mCoT-XReasoning."
        },
        {
            "title": "Start",
            "content": "When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy Jirui Qi1, Shan Chen2,3,4, Zidi Xiong2, Raquel Fernández5, Danielle S. Bitterman2,3,4, Arianna Bisazza1 Co-first authors, Co-senior authors 1University of Groningen, 2Harvard University, 3Mass General Brigham, 4Boston Childrens Hospital, 5University of Amsterdam 5 2 0 2 8 2 ] . [ 1 8 8 8 2 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, the extent to which LRMs can think in other languages is less studied. This is as important as answer accuracy for real-world applications since users may find the thinking trace useful for oversight only if expressed in their languages. In this work, we comprehensively evaluate two leading families of LRMs on our established benchmark XReasoning. Surprisingly, even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing substantial gap in the capability of thinking in non-English languages. Promoting models to reason in the users language via prompt hacking enhances readability and oversight. This could gain user trust, but reduces answer accuracy, exposing an important trade-off. We further demonstrate that targeted post-training, even with just 100 instances, can mitigate this language mismatch, although accuracy is still degraded. Our results reveal the limited multilingual reasoning capabilities of current LRMs and suggest directions for future research."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated impressive reasoning capabilities when prompted to explicitly generate thinking traces step-by-step in natural language (Wei et al., 2023). Recent studies further show that encouraging models to engage in long thinking traces at inference time, or training for this behavior, significantly enhances their reasoning accuracy (Muennighoff et al., 2025; DeepSeek-AI, 2025). This approach has led to the development of new category of models designed to generate more extensive and detailed reasoning processes, referred to as Large Reasoning Models 1All code and datasets released at https://github.com/ Betswish/mCoT-XReasoning. Figure 1: Illustration of the trade-off between answer accuracy and thinking language matching in multilingual reasoning systems. English translations in brackets. (LRMs) (OpenAI et al., 2024; Welleck et al., 2024; Snell et al., 2024; Muennighoff et al., 2025). However, their capabilities are mostly tested on English tasks (DeepSeek-AI, 2025). It is less investigated to what extent LRMs can think in users native language and how this affects their reasoning accuracy, when it comes to multilingual reasoning tasks. Illustrated by Figure 1(a), the matching of the thinking language is as important as the accuracy because it makes the trace more readable and easier for users to verify. Even if the answers are correct, the thinking traces in language users cannot understand may undermine their trust in the model and lead to dissatisfaction with the responses, which becomes especially pronounced as tasks grow more complex in practice. In this work, we comprehensively evaluate six state-of-the-art open-sourced LRMs belonging to two families: Distilled-R1 (DeepSeek-AI, 2025), and Skywork-OR1 (He et al., 2025). Due to the lack of multilingual reasoning datasets, we introduce novel benchmark named XReasoning, where we observe that even the latest 32B LRMs suffer from the issue of language mismatch in their thinking traces. Nevertheless, we reveal that this mismatch can be significantly mitigated with prompt-hacking techniques (c.f., Section 4.1), as shown in Figure 1 (b). However, this comes at the cost of reduced model performance, highlighting trade-off between explainability and accuracy that may limit the practical value and application of these models. Finally, we explore whether post-training can mitigate language mismatches in thinking traces, based on our observation that existing LRMs are more likely to reason in the correct language when prompted to think in English or Chinese, the two most prevalent languages in their training data (DeepSeek-AI, 2025). The results demonstrate that post-training, even with few instances, improves LRMs capability of thinking in the users language but does not completely resolve the trade-off between thinking language matching and accuracy. Overall, we make the following contributions: (i) We reveal the important trade-off between language matching and answer accuracy when the LRMs face multilingual users. (ii) We propose and opensource new benchmark, XReasoning, with challenging math and science questions to evaluate multilingual reasoning capabilities of advanced LRMs. (iii) We show that language mismatch of thinking traces can be alleviated through prompt hacking or post-training, but both lead to drop in answer accuracy. Such persistence of this trade-off for current models highlights the need for future work toward more user-friendly multilingual LRMs."
        },
        {
            "title": "2 Related Work",
            "content": "Multilingual Reasoning Benchmarks Recent research has expanded model reasoning evaluation beyond English. Shi et al. (2023) introduced the Multilingual Grade School Math (MGSM) benchmark, comprising 250 math problems in 11 languages. Similarly to Ahuja et al. (2023), this work illustrates an uneven answer accuracy across languages, typically favoring English with marginally higher-than-random accuracy for low-resource languages like Bengali and Swahili. However, with the rapid shift from LLMs to LRMs, the limitations of existing multilingual benchmarks have become evident (Ghosh et al., 2025). These benchmarks often fail to capture the nuanced accuracy differences across languages; for instance, recent LRMs even achieve 90% accuracy in multiple high-resource languages on MGSM. To fix this limitation, we propose XReasoning, new benchmark covering more challenging math and science questions to better evaluate these LRMs. Improving Answer Accuracy with Cross-lingual Reasoning Various prompting methods leverage English reasoning capabilities in multilingual tasks. The simple translate-test baseline, i.e. translating problems to English before solving them, often enhances accuracy (Shi et al., 2023; Ahuja et al., To minimize reliance on translation, 2023). Huang et al. (2023) introduced XLT, languageindependent prompt template to boost multilingual reasoning accuracy without fine-tuning. On mathematical reasoning tasks, concurrent work (Yong et al., 2025) reveals that models reach higher accuracy when prompted to think or respond in English. Nevertheless, none of the above works focus on whether the model can effectively think using the user-specified language. We argue that this behavior, beyond answer accuracy, should be studied and optimized, especially for user-facing multilingual reasoning systems."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Benchmarks As mentioned in Section 2, existing datasets, like MGSM, are insufficient for evaluating the recent LRMs. For our evaluation, we start from three reasoning datasets of challenging English questions, AIME2024 (Veeraboina, 2024), AIME2025 (Kaggle, 2025), and GPQA (Rein et al., 2023), and use GPT-4O-MINI (OpenAI, 2024) to translate all questions into the other ten languages covered by MGSM,2 following previous work on these languages (Singh et al., 2024; Adelani et al., 2024; Raihan et al., 2024; Azime et al., 2024). The resulting parallel challenging questions, combined with MGSM, form the XReasoning benchmark, consisting of 370 questions, each in 11 languages.3 See Appendix for examples. Models We comprehensively test six advanced LRMs belonging to two model families: Distilled2Concretely, English (EN), Spanish (ES), French (FR), German (DE), Russian (RU), Chinese (ZH), Japanese (JA), Thai (TH), Swahili (SW), Bengali (BN), and Telugu (TE). 3Note that the answers do not require translation since they are all Arabic numbers or option letters. R1 (DeepSeek-AI, 2025) 1.5B, 7B, 14B, and 32B, and Skywork-OR1 (He et al., 2025) 7B and 32B. Model AIME (%) MGSM (%) GPQA (%) Match. Acc. Match. Acc. Match. Acc. Evaluation As focus of this work, we evaluate language matching rate by calculating the ratio of instances for which the LRMs correctly follow the instruction to think in the specified language. The commonly used LANGDETECT toolkit (Shuyo, 2010; Jauhiainen et al., 2019; Gargova et al., 2022; Wyawhare, 2023; Valliyammal et al., 2024; Habib et al., 2024) is adopted to predict the language of the thinking trace between special thinking tokens <think> and </think>."
        },
        {
            "title": "4.1 Language Matching vs. Answer Accuracy",
            "content": "Prompt Hacking Besides the standard prompting with explicitly specified thinking language in the instruction4, we introduce and leverage the prompt hacking technique, widely studied by works in model security and controllability (Schulhoff et al., 2023; Liu and Hu, 2024; Benjamin et al., 2024; Wu et al., 2025), to induce the LRM to generate the thinking traces in the user-expected languages (see Figure 1). Specifically, the hacking prefix By request, will start thinking in {USER_LANG} is translated into the user language5 and concatenated after the <think> token, which indicates the start of the thinking trace. As we will demonstrate, this prefix strongly influences the distribution of subsequent tokens, biasing the model toward generating text in the same language as the prefix. Examples are provided in Appendix D. Overall Results Table 1 presents the overall performance of the six LRMs on XReasoning. All models struggle to follow instructions to think in the user-specified languages when queried with standard prompts, except for the larger models on MGSM (which contain easier questions). Even for Distill-R1-32B, the language matching rate is only 46.3% on AIME and 42.3% on GPQA. In addition, motivating models to generate the thinking traces in the user query language with prompt hacking boosts language matching across the board, from roughly 45-50% to well above 90% at every parameter scale. This leads to 4See Appendix for examples. 5Previous work in retrieval-augmented generation (Zhang et al., 2024; Chirkova et al., 2024; Qi et al., 2025) highlights the importance of translating the instructions into the users language to facilitate the model to respond in their languages. DeepSeek-Distilled-R1 Series 1.5B hack 7B hack 14B hack 32B hack 7B hack 32B hack 49.0 86.6 48.5 90.7 40.9 95.9 46.3 97.9 27.6 84.1 32.9 95.8 4.8 2.7 17.4 8.3 27.2 14.4 25.5 17.0 33.1 80.2 44.4 88.1 42.8 96.6 42.3 96.6 10.5 6.9 25.5 19.2 40.5 29.1 40.3 33.4 Skywork-OR1 Series 31.2 17.2 44.8 29.6 23.7 81.1 32.8 94.7 26.6 23.4 53.6 39.8 54.0 86.1 79.3 93.9 83.2 97.0 94.0 98.8 72.8 92.1 83.1 98.6 28.6 21.8 50.5 48.9 68.1 63.9 70.2 71.7 57.3 51.7 75.4 71.9 Table 1: Average language matching rate (Match.) and answer accuracy (Acc.) over 11 languages. Arrows compare each hack value against its standard counterpart. drop in accuracy, which shrinks as model size increases. Compared to smaller models, larger models remain more accurate when matching the user language in their reasoning, yet measurable accuracy decrease persists on the harder tasks. Taken together, larger model scale mitigates, but does not eliminate, the trade-off between language matching rate and answer accuracy. Language-Specific Results Figure 2 illustrates Distilled-R1-32B performance on AIME questions, showing detailed breakdowns by query and thinking language pairs.6 The heatmaps clearly illustrate the failure of the LRM in generating the thinking trace in the specified language when fed with standard prompts (left-top). Specifically, when prompted to think in French, the model almost always thinks in English7 (see Appendix 4.1). Figure 2 heatmaps also reveal the impact of prompt hacking on language matching rate and answer accuracy. Notably, motivating the model to reason in specific language with the hacked prompts increases the matching rate from an average of 46% to 98%. However, as trade-off, this increment introduces noticeable accuracy decrement from 26% to 17% in average. Interestingly, we observe that reasoning in English, even for non-English queries, consistently results in higher accuracy, especially after prompt hacking. This aligns with concurrent work on im6Colors represent percentages: darker blue indicates higher matching or accuracy; lighter yellow denotes lower values. 7We find the same issue even when prompting the 671B R1 to think in French. Ques. Lang. Think Lang. Specified Language Prediction of Thinking Traces EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN EN EN EN EN EN EN EN EN EN EN FR FR FR FR FR FR FR FR FR FR FR DE DE DE DE DE DE DE DE DE DE DE ZH ZH ZH ZH ZH ZH ZH ZH ZH ZH ZH JA JA JA JA JA JA JA JA JA JA JA RU RU RU RU RU RU RU RU RU RU RU EN: 100.0 EN: 100.0 EN: 100.0 EN: 93.33, ZH: 6.67 EN: 100.0 EN: 100.0 EN: 100.0 EN: 100.0 EN: 100.0 EN: 100.0 EN: 100.0 EN 100.0 EN 96.67, FR: 3.33 EN 100.0 EN 98.09, FR: 1.67, others: 0.24 EN 98.33, FR: 1.67 EN 98.33, FR: 1.67 EN 96.67, FR: 3.33 EN 96.67, FR: 3.33 EN 100.0 EN 70.0, FR: 30.0 EN 98.33, FR: 1.67 EN 70.0, DE: 30.0 EN 83.57, DE: 16.43 DE: 96.43, EN: 1.67, others: 1.19, ZH: 0.71 DE: 84.52, EN: 15.0, others: 0.48 DE: 52.62, EN: 45.71, others: 1.67 EN 84.05, DE: 15.48, others: 0.48 EN 83.57, DE: 16.43 EN 93.33, DE: 6.67 EN 93.09, DE: 6.67, ES: 0.24 DE: 51.43, EN: 48.57 DE: 64.52, EN: 35.48 ZH: 88.1, others: 10.95, EN: 0.95 ZH: 89.05, others: 10.48, EN: 0.48 ZH: 93.81, others: 5.71, EN: 0.48 ZH: 91.43, others: 8.1, EN: 0.48 ZH: 95.0, others: 4.52, EN: 0.48 ZH: 95.95, others: 3.1, EN: 0.95 ZH: 88.81, others: 10.71, EN: 0.48 ZH: 95.0, others: 4.76, EN: 0.24 ZH: 92.14, others: 5.24, EN: 2.62 ZH: 93.09, others: 6.9 ZH: 91.43, others: 8.57 ZH 89.29, others: 5.71, JA: 3.33, EN: 1.67 ZH 77.62, EN: 9.05, others: 8.33, JA: 5.0 EN 55.95, ZH: 40.95, others: 3.1 ZH 87.14, others: 10.48, EN: 2.14, ES: 0.24 ZH 80.24, others: 10.48, JA: 8.81, EN: 0.48 ZH 87.86, others: 9.76, JA: 1.67, EN: 0.71 ZH 85.95, others: 12.86, EN: 1.19 ZH 82.62, others: 8.33, JA: 6.67, EN: 2.38 ZH 87.38, others: 8.1, EN: 4.52 EN 54.29, JA: 25.24, ZH: 11.9, others: 8.57 ZH 80.24, JA: 8.33, EN: 6.67, others: 4.76 RU: 99.76, others: 0.24 RU: 100.0 RU: 96.67, EN: 1.67, ZH: 1.67 RU: 84.76, ZH: 11.9, others: 3.33 RU: 80.0, ZH: 12.62, others: 5.71, EN: 1.67 RU: 99.76, others: 0.24 RU: 100.0 RU: 100.0 RU: 99.76, others: 0.24 RU: 100.0 RU: 98.09, others: 1. Table 2: The distribution of the predicted actual language of the thinking traces when the LRM is prompted to reason in each thinking language. The results are averaged over all questions in the same language. The mismatched thinking languages are highlighted in red. filtered from LIMO (Ye et al., 2025), comprising math problems with step-by-step solutions from Figure 2: Heatmaps of language matching rates (top) and accuracy (bottom) for Distilled-R1-32B on AIME, before (left) and after (right) prompt hacking. proving answer accuracy via cross-lingual reasoning (Yong et al., 2025), supporting the reliability of our experiments and XReasoning benchmark. This finding, we argue, also highlights clear English thinking preference for the advanced LRMs, and the potential of investing more efforts in the multilingual reasoning capabilities of these LRMs. Actual Thinking Languages Corresponding to the language matching heatmaps in Figure 2, we conduct an in-depth analysis of the languages that LRM actually used for its thinking. For each query and thinking language pair, we collect the predicted language distributions of the thinking traces on Distilled-R1-32B, produced with LANGDETECT on all AIME questions. The averaged distributions are shown in Table 2 and 3, where clear mismatch is observed when the LRM is instructed to think in French, Japanese, Thai, and Swahili. Besides, the mismatch thinking languages all fall into either English or Chinese, suggesting the impact of thinking data on the models reasoning capability."
        },
        {
            "title": "4.2 Post-Training with Few Instances",
            "content": "Figure 2 shows strong performance in English and Chinese, but weaker among others. Setup To see whether further training can help, we post-train on Distilled-R1-7B using mini training sets of 100 or 250 instances per low-resource language (Japanese, Thai, Telugu), resulting in six post-trained LRMs. The training data are Ques. Lang. Think Lang. Specified Language Prediction of Thinking Traces EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE EN FR DE ZH JA RU ES BN TH SW TE ES ES ES ES ES ES ES ES ES ES ES BN BN BN BN BN BN BN BN BN BN BN TH TH TH TH TH TH TH TH TH TH TH SW SW SW SW SW SW SW SW SW SW SW TE TE TE TE TE TE TE TE TE TE TE EN 93.33, ES: 6.67 EN 85.0, ES: 15.0 EN 98.33, ES: 1.67 ES: 46.43, EN: 38.1, ZH: 15.0, others: 0.48 ES: 45.0, EN: 40.0, ZH: 14.76, others: 0.24 ES: 78.09, EN: 21.67, others: 0.24 ES: 78.09, EN: 21.67, others: 0.24 EN 85.0, ES: 15.0 EN 75.0, ES: 25.0 ES: 71.43, EN: 28.33, others: 0.24 EN 53.33, ES: 46.67 EN 58.33, BN: 41.67 EN 86.67, BN: 11.67, others: 1.67 EN 71.67, BN: 28.33 BN: 100.0 BN: 91.67, EN: 8.33 BN: 70.0, EN: 30.0 EN 78.33, BN: 21.67 BN: 98.33, EN: 1.67 EN 70.0, BN: 30.0 EN 65.0, BN: 35.0 BN: 55.0, EN: 45.0 EN 43.81, TH: 41.67, ZH: 10.24, others: 4.29 EN 86.19, TH: 11.67, ZH: 1.67, others: 0.48 EN 95.0, TH: 5.0 ZH 86.67, others: 6.43, TH: 5.0, EN: 1.9 ZH 57.14, EN: 28.33, TH: 10.0, others: 4.52 EN 49.05, ZH: 35.0, TH: 13.33, others: 2.62 EN 83.1, ZH: 10.24, TH: 3.33, others: 3.33 EN 75.0, TH: 18.33, ZH: 5.0, others: 1.67 TH: 41.67, ZH: 35.71, EN: 15.95, others: 6.67 EN 55.0, TH: 45.0 EN 48.33, TH: 45.0, ZH: 5.0, others: 1.67 EN 100.0 EN 100.0 EN 98.33, others: 1.67 EN 100.0 EN 99.76, ES: 0.24 EN 100.0 EN 98.33, SW: 1.67 EN 96.67, SW: 3.33 EN 100.0 EN 75.0, SW: 25.0 EN 96.67, SW: 3.33 EN 68.33, TE: 31.67 EN 61.67, TE: 38.33 EN 76.67, TE: 23.33 TE: 88.33, ZH: 10.24, others: 1.43 ZH 55.0, TE: 28.33, others: 7.86, EN: 6.9, JA: 1.9 TE: 95.0, EN: 3.33, RU: 1.67 TE: 61.67, EN: 38.33 EN 65.0, TE: 30.24, BN: 3.81, others: 0.95 EN 76.67, TE: 23.33 TE: 85.0, SW: 8.33, EN: 6.67 TE: 96.67, EN: 3.33 Table 3: Cont. The distribution of the predicted language of the thinking traces, averaged over all questions in the same language, when the LRM is prompted to reason in each thinking language. the teacher model (DeepSeek-R1-671B), and translated by GPT-4O-MINI.8 See Appendix for implementation details and examples. Results Figure 3 illustrates the changes in language matching and answer accuracy before/after post-training, when the LRMs are prompted to think in Japanese, Thai, or Telugu.9 Post-training 8The translated open-sourced https://huggingface.co/collections/shanchen/ xreasoning-681e7625c7a9ec4111a634b6. LIMO is at 9The result is averaged over eleven query languages. See Appendix for full results. Figure 3: Language matching rate and answer accuracy of Distilled-R1-7B with no post-training, post-training on 100 instances, and post-training on 250 instances for 3 languages: Japanese, Thai, and Telugu. on merely 100 in-language instances effectively led the matching rate to sharp increment to nearly 100% for Thai and Telugu and to around 80% for Japanese. However, this gain in matching rate comes at cost in answer accuracy across all datasets. This demonstrates the effectiveness of post-training to improve language matching, but the trade-off with accuracy persists. Increasing the instances does not reliably mitigate the issue. In fact, when increasing from 100 to 250 training instances, the post-trained models suffer from drop in matching rate on Japanese and Thai on AIME and GPQA, and on Thai and Telugu on MGSM; while answer accuracy exhibits only marginal recovery far below the accuracy of the original LRM."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we investigate the overlooked issue of language matching in thinking traces and its trade-off with answer accuracy in multilingual reasoning systems. Using our new XReasoning benchmark, we show that even state-of-the-art LRMs struggle to generate thinking traces in user-specified language. Simple prompt-hacking reduces language mismatch but at substantial cost to accuracy, especially on complex tasks. Further, targeted post-training on 100-250 examples sharply improves language alignment but still markedly reduces accuracy, highlighting this persistent trade-off. We argue that advanced such as reinforcement adaptation strategies, learning (Li, 2017; Ouyang et al., 2022; Shao et al., 2024), will be critical to resolve this tension and develop multilingual reasoning systems suitable for practical, trustworthy applications."
        },
        {
            "title": "Limitations",
            "content": "Our analysis, though informative, is constrained in several important ways. First, the benchmark focuses exclusively on math and science questions translated from AIME 2024/2025, GPQADiamond, and MGSM. Because these tasks yield short, well-structured answers, it remains unclear whether the observed trade-off between thinking language alignment and answer accuracy generalizes to open-ended or domain-specific reasoning, such as everyday common sense or legal discourse. Furthermore, all non-English content (questions, prompt instructions, hacking prefixes, and training instances) was machine translated with GPT-4OMINI. Despite spot checks, subtle semantic drift may persist, introducing noise into both languagematching and answer accuracy scores. second limitation arises from our reliance on an off-the-shelf language identification tool. LANGDETECTs default language profiles are generated from Wikipedia (Shuyo, 2010; Danilák, 2020)10, so brief mathematical expressions or codeswitched traces can be misclassified, slightly inflating or deflating reported language matching rates. In parallel, we measure only the surface-level alignment of the thinking traces with the query language and the correctness of the boxed answer, leaving the deeper analysis of faithful reasoning unanswered (Chen et al., 2025; Chua and Evans, 2025; Arcuschin et al., 2025; Stechly et al., 2025), namely, whether the model actually uses its trace to derive the answer. Finally, our mitigation study is deliberately lightweight: we fine-tune on no more than 250 examples per language, which may not extrapolate to truly low-resource settings or languages with markedly different morphology and orthography. We also assume that presenting traces in users language is desirable and increases trust by enabling human oversight beyond reviewing only the final answer. user-centered evaluation, combining preference elicitation with task performance, will be essential for assessing how multilingual 10https://pypi.org/project/langdetect reasoning systems translate into trust and safe, effective use of LRMs in practice."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors acknowledge financial support from the Google PhD Fellowship (SC), the Woods Foundation (DB, SC), the NIH (NIH R01CA294033 (SC, DB), NIH U54CA274516-01A1 (SC, DB) and the American Cancer Society and American Society for Radiation Oncology, ASTRO-CSDG24-1244514-01-CTPS Grant DOI: ACS.ASTROCSDG-24-1244514-01-CTPS.pc.gr.222210 (DB). The authors have received funding from the Dutch Research Council (NWO): JQ is supported by NWA-ORC project LESSEN (grant nr. NWA.1389.20.183), AB is supported by the above as well as NWO Talent Programme (VI.Vidi.221C.009), RF is supported by the European Research Council (ERC) under European Unions Horizon 2020 programme (No. 819455). We also thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Hábrók high performance computing cluster."
        },
        {
            "title": "References",
            "content": "David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, et al. 2024. Irokobench: new benchmark for african languages in the age of large language models. arXiv preprint arXiv:2406.03368. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 42324267, Singapore. Association for Computational Linguistics. Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Chain-of-thought reasoning in Conmy. 2025. arXiv preprint the wild is not always faithful. arXiv:2503.08679. Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Yonas Chanie, Bontu Fufa Balcha, Negasi Haile Abadi, Henok Biadglign Ademtew, Mulubrhan Abebe Nerea, Debela Desalegn Yadeta, Derartu Dagne Geremew, et al. 2024. Proverbeval: Exploring llm evaluation challenges for low-resource language understanding. arXiv preprint arXiv:2411.05049. Victoria Benjamin, Emily Braca, Israel Carter, Hafsa Kanchwala, Nava Khojasteh, Charly Landow, Yi Luo, Caroline Ma, Anna Magarelli, Rachel Mirin, et al. 2024. Systematically analyzing prompt injection vulnerabilities in diverse llm architectures. arXiv preprint arXiv:2410.23308. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. 2025. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410. Nadezhda Chirkova, David Rau, Hervé Déjean, Thibault Formal, Stéphane Clinchant, and Vassilina Nikoulina. 2024. Retrieval-augmented generation in multiIn Proceedings of the 1st Worklingual settings. shop on Towards Knowledgeable Language Models (KnowLLM 2024), pages 177188, Bangkok, Thailand. Association for Computational Linguistics. James Chua and Owain Evans. 2025. Inference-timecompute: More faithful? research note. arXiv preprint arXiv:2501.08156. Michal Mimino Danilák. 2020. langdetect: Python port of googles language-detection library. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Silvia Gargova, Irina Temnikova, Ivo Dzhumerov, and Hristiana Nikolaeva. 2022. Evaluation of off-theshelf language identification tools on Bulgarian social media posts. In Proceedings of the Fifth International Conference on Computational Linguistics in Bulgaria (CLIB 2022), pages 152161, Sofia, Bulgaria. Department of Computational Linguistics, IBL BAS. Akash Ghosh, Debayan Datta, Sriparna Saha, and Chirag Agarwal. 2025. The multilingual mind: survey of multilingual reasoning in language models. arXiv preprint arXiv:2502.09457. Fatima Habib, Zeeshan Ali, Akbar Azam, and Fahad Mansoor Pasha. 2024. Language detection using natural language processing. ResearchGate. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou. 2025. Skywork open reasoner series. Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by cross-lingualthought prompting. Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lindén. 2019. Automatic language identification in texts: survey. Journal of Artificial Intelligence Research, 65:675782. Kaggle. 2025. AIME-2025 Dataset. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Yuxi Li. 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274. Frank Weizhen Liu and Chenhui Hu. 2024. Exploring vulnerabilities and protections in large language models: survey. arXiv preprint arXiv:2406.00240. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. 2024. Openai o1 system card. OpenAI. 2024. Gpt-4o mini: advancing cost-efficient intelligence. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Jirui Qi, Raquel Fernández, and Arianna Bisazza. 2025. On the consistency of multilingual context utilization in retrieval-augmented generation. arXiv preprint arXiv:2504.00597. Nishat Raihan, Antonios Anastasopoulos, and Marcos Zampieri. 2024. mhumanevala multilingual benchmark to evaluate large language models for code generation. arXiv preprint arXiv:2410.15037. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Sander Schulhoff, Jeremy Pinto, Anaum Khan, LouisFrançois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Kost, Christopher Carnahan, and Jordan Boyd-Graber. 2023. Ignore this title and HackAPrompt: Exposing systemic vulnerabilities of LLMs through global prompt hacking competition. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 49454977, Singapore. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations. Nakatani Shuyo. 2010. Language detection library for java. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, and Subbarao Kambhampati. 2025. Beyond semantics: The unreasonable effectiveness of reasonless intermediate tokens. arXiv preprint arXiv:2505.13775. R. Valliyammal, S. Nusrath Najeeba, and A. Nandhini. 2024. Python-based text language identification. International Journal of Innovative Research in Technology, 11(2):11471152. Hemish Veeraboina. 2024. Aime problem set 19832024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia From deKulikov, and Zaid Harchaoui. 2024. coding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838. Tong Wu, Chong Xiang, Jiachen Wang, and Prateek Mittal. 2025. Effectively controlling reasoning models through thinking intervention. arXiv preprint arXiv:2503.24370. Arinjay Wyawhare. 2023. Comparative analysis of multilingual text classification & identification through deep learning and embedding visualization. arXiv preprint arXiv:2312.03789. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. Zheng-Xin Yong, M. Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen H. Bach, and Alham Fikri Aji. 2025. Crosslingual reasoning through test-time scaling. Liang Zhang, Qin Jin, Haoyang Huang, Dongdong Zhang, and Furu Wei. 2024. Respond in my language: Mitigating language inconsistency in response generation based on large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 41774192, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Experimental Details",
            "content": "Computational Resources For the evaluation of existing models, we use greedy decoding for best reproducibility. The maximum token number for generation is set to 16392 to maximize the reasoning capability. Most our experiments are conducted on A100 with 40GB using VLLM (Kwon et al., 2023) with approximately 7,000 GPU hours. And 2000 A/H100 80GB hours for 32B models inferencing and roughly 1000 GPU hours for training. Accuracy Evaluation We implement the exact matching metric for evaluating answer accuracy. Regex is adopted to extract the answer field from the ans. (i.e., boxed{ans.}), the answer format these LRMs are pretrained to output."
        },
        {
            "title": "B XReasoning Benchmark Examples",
            "content": "The examples of questions in our established XReasnoing benchmark are shown in Table 4, where AIME and GPQA questions are more challenging than MGSM."
        },
        {
            "title": "Hard",
            "content": "Let ABC be triangle inscribed in circle ω. Let the tangents to ω at and intersect at point D, and let AD intersect ω at . If AB = 5, BC = 9, and AC = 10, AP can be written as the form , where and are relatively prime integers. Find + n. Problem: Two quantum states with energies E1 and E2 have lifetime of 109 sec and 108 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they can be clearly resolved? (A) 1011 eV (B) 108 eV (C) 109 eV (D) 104 eV"
        },
        {
            "title": "Answer",
            "content": "113 D"
        },
        {
            "title": "MGSM Easy",
            "content": "If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? 5 Table 4: Examples AIME, GPQA and MGSM instances in our proposed XReasoning benchmark. Difficulty level is assigned based on reasoning intensity by the authors."
        },
        {
            "title": "C Prompts and Instructions",
            "content": "To ensure the model responses are always in the query language, we follow previous works (Chirkova et al., 2024; Zhang et al., 2024; Qi et al., 2025) and adopt language-specific instructions to explicitly and implicitly guide the model to generate thinking trace in the user-specified languages. The examples in English, Spanish, Chinese and Japanese are listed in Table 5."
        },
        {
            "title": "D Prompt Hacking Prefix",
            "content": "The English, Spanish, Chinese and Japanese prefixes for prompt hacking are listed in Table 6."
        },
        {
            "title": "E Reliability of Automatic Language Detector",
            "content": "For the reliability of the automatically detected thinking languages, we evaluate the alignment of LANGDETECT predictions with human beings. Specifically, we focus on the models thinking traces when prompted to think in Japanese, another struggling thinking language for the tested LRM, where the languages of most thinking traces are predicted as Chinese. We invite native speakers to manually check if these cases are Chinese segments. Feedback confirms that all predictions are correct. The LRM is indeed thinking in Chinese on these questions, even though prompted to think in Japanese, e.g., below is question queried in Japanese but got the thinking trace in Chinese: <think>好我现在要解决这个几何问题题目 是关于三角形ABC内接于圆ω点B和C处的切线在点D相交线段AD与圆ω再次交于点P已"
        },
        {
            "title": "Instruction",
            "content": "EN ES ZH JA Please always think in English. Solve the following mathematics problem step by step. At the end, provide your final answer enclosed in boxed{}. Por favor, siempre piensa en español. Resuelve el siguiente problema matemático paso paso. Al final, proporciona tu respuesta final encerrada en boxed{} 请始终用中文思考逐步解决以下数学问题最后将您的最终答案放 在boxed{} 中 常に日本で考えてください以下の数学をステップバイステップで解 いてください最後に最的な答えをboxed{} でんで提供してくだ さい Table 5: The examples of the adopted instructions for guiding LRMs to generate thinking traces in the user languages."
        },
        {
            "title": "Instruction",
            "content": "EN ES ZH JA <Assistant><think> By request, will start thinking in English. <Assistant><think> petición, empezaré pensar en español. <Assistant><think> 应要求我将开始用中文思考 <Assistant><think> 要求があれば日本で考え始めます Table 6: The examples of the prefixes used in prompt hacking experiments for promoting LRMs to think in the user-specified languages. 知AB=5BC=9AC=10要求AP的长度表示为最简分数m/n然后求m+n[...]</think>. This demonstrates the reliability of our experiments where LANGDETECT is adopted for automatic language detection. Details of Post-Training Experiments F.1 Translated LIMO Examples For the post-training in Japanese, Telugu, and Thai, we translate the LIMO (Ye et al., 2025) into the three languages by GPT-4O-MINI. The examples of the translated Japanese instances are illustrated in Table 7. For full translated instances in Japanese, Telugu, and Thai, please refer to https://huggingface.co/ collections/shanchen/xreasoning-models-68377e15a2e86143dc4b0383. F.2 Post-Trained LRMs and Accessibility Overall, seven models are evaluated in our work: the base model, plus two variants (post-trained with 100 or 250 instances) each for Japanese (JA), Telugu (TE), and Thai (TH). low learning rate (1e-5) is applied to minimize overfitting and catastrophic forgetting during our post-training. All these posttrained LRMs are also publicly accessible via https://huggingface.co/collections/shanchen/ xreasoning-681e7625c7a9ec4111a634b6. Full Results of Post-Training The full results on the LRMs post-trained with Japanese, Telugu and Thai instances are shown in Table 8, 9 and 10, where the LRMs are alway prompt to think in the same language as that of the training data, given queries in different languages. Language Question JA JA ル ド ル フ は 一 定 の速度で自 を漕 ぎ マイルの わ りに5分の休憩を 取 り ま す ジ ェ ニ フ ァ は ル ド ルフの速度の4分 の3の一定の速度 で自 を漕ぎます が ジ ェ ニ フ ァ は2マイルのわり に5分の休憩を取 り ま す ジ ェ ニ フ ァ と ル ド ル フ は同 に自 を漕ぎ 始 め 50マ イ ル の 地 点 に ち ょ う ど同じ に到着し ま す 彼 ら は 何 分 か か っ た の で しょうか 4 , 9 放 物 の 点 が(cid:0) 1 (cid:1) で 8 あ り 方 程 式 がy = ax2 + bx + で あ る と し ま す た だ し > 0 で あ り + + は 整 数 で す の 最 小 可 能 は の 形 で 表 す こ と が で き こ こ でp とq は 互 い に 素 な 正 の 整 数 で すp + を求め なさい Model Response <Assistant><think> 重要な指示 1. すべてのマクダウン形式をそのまま保持する*#などのマクダウン文を翻しない 2. すべての数学的表を保持する例えば 025 のような表は更しない 3. すべてのコドブロックとその文を保持する 4. すべての数名数字技用を保持する 元のテキスト さてこのをステップバイステップで解してみましょうルドルフとジェニファが50マイル地点まで自を漕いで同に到着しま す彼らがどれくらいのがかかったのかをつける必要があります [...] </think> Answer: わかりましたこれをステップバイステップで分解しましょうルドルフとジェニファがそれぞれの自の速度と休憩ス ケジュルを考して50マイルを自で移するのにかかるを求める必要があります彼らは一に出し50マイル地点に同に到着しま す **ステップ1: 速度とを定する** - ルドルフの自の速度をR マイル/分 [...] - **ジェニファの**: 200 3 2 15 者は同に到着するのでかかったは**620 分** です + 120 = 500 + 120 = 620 分 620 <Assistant><think> 重要な指示 [...] わかっただから点が(1/4, -9/8) にあり方程式がy = ax2 + bx + でa > 0 かつa + + が整数である放物のa の最小可能をつ ける必要がある次にこの最小のa を分された分数p/q として表しp + を求めるうん放物の点形式を思い出すことから 始めよう [...] </think> Answer: 最小の可能なa を求めるために点(cid:0) 1 いて考えますまず放物の点形式から始めます (cid:1) を持ち方程式y = ax2 + bx + でa > 0 かつa + + が整数である放物につ 4 , 9 8 (cid:18) = (cid:19)2 1 4 9 [...] したがって最小のa は 2 9 でありp = 2 およびq = 9 ですしたがってp + = 2 + 9 = 11 11 Answer 620 Table 7: The examples of the translated LIMO training instances in Japanese (JA). Eval Metric # Ins. AVG Query Language EN FR DE ZH JA RU ES BN TH SW TE Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) None 100 250 None 100 250 None 100 250 None 100 250 None 100 None 100 250 41.5 76.4 65.0 14.6 4.9 9.0 29.2 61.6 46.8 27.4 12.1 9.3 58.6 64.8 77. 50.7 46.4 42.9 25.5 95.6 89.4 22.2 5.6 6.7 14.5 81.0 67.2 34.5 11.4 11.4 80.5 95.6 99. 71.6 66.3 64.3 38.3 93.9 73.9 20.5 7.8 8.9 25.3 75.6 57.3 29.9 16.5 10.6 58.5 75.9 83. 60.4 54.8 55.3 AIME 51.1 72.8 28.3 27.2 94.4 96.7 18.9 6.7 20.5 21.1 3.3 5.5 GPQA 12.3 38.0 2.2 32.3 86.5 80.6 30.6 19.4 16.0 28.6 9.6 5.8 MGSM 25.7 24.5 44.8 65.5 63.6 60. 67.2 94.9 98.1 67.2 59.3 53.5 73.3 93.3 98.9 8.3 4.4 4.4 72.4 87.7 92.1 23.7 12.8 7. 93.5 99.2 99.7 50.0 51.6 46.1 37.2 92.8 79.4 16.1 7.2 8.9 22.9 76.9 48.7 28.1 11.1 9. 68.5 81.7 94.4 59.3 58.5 51.9 38.3 93.3 94.9 17.2 7.2 6.1 23.4 78.1 60.1 29.1 10.6 8. 65.7 88.1 91.3 61.2 57.1 62.5 29.4 11.1 1.7 16.1 3.3 21.1 23.7 10.3 0.8 28.9 10.4 12. 39.9 6.7 33.9 44.2 40.4 32.8 43.3 65.6 98.3 8.8 4.4 5.5 29.8 38.4 77.3 24.8 9.5 5. 36.5 41.2 87.1 49.3 46.2 53.7 55.6 68.3 32.2 3.9 1.7 0.6 33.3 70.9 20.9 21.2 12.6 6. 44.0 64.4 54.4 5.5 3.6 3.6 37.2 58.9 21.7 8.3 1.7 10.6 30.5 33.8 7.8 22.3 9.8 8. 64.4 40.9 71.2 22.5 11.2 15.3 Table 8: Model performance on AIME/GPQA/MGSM dataset with Deepseek-Distill-R1-7B post-trained on 100/250 few Japanese instances. The LRM is prompted to think in Japanese. Eval Metric # Ins. AVG Query Language EN FR DE ZH JA RU ES BN TH SW TE Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) None 100 250 None 100 None 100 250 None 100 250 None 100 250 None 100 250 29.1 99.6 91.9 18.6 4.6 8. 25.6 98.0 82.3 27.5 9.1 11.6 71.5 97.6 98.3 47.9 36.7 45.5 17.2 100.0 95.0 29.4 6.7 12. 24.4 100.0 89.4 25.6 5.0 8.3 18.4 99.3 80.5 33.5 10.1 13.3 69.9 94.0 99.6 69.9 57.7 72. 19.2 98.8 83.8 26.6 8.8 10.9 63.5 95.5 94.9 55.3 45.6 54.8 AIME 9.5 99.4 100. 21.7 4.5 8.3 GPQA 8.3 99.3 98.5 29.8 6.1 7.1 MGSM 58.4 98.8 99.9 65.2 45.9 57.7 30.6 99.4 90. 25.0 2.2 11.1 19.7 97.5 70.5 29.1 9.9 15.5 67.6 98.7 94.5 57.9 46.8 58.8 25.6 100.0 99. 14.5 4.4 8.3 25.0 100.0 93.3 23.9 5.5 8.9 25.6 100.0 88.9 22.2 7.2 9.4 20.5 99.1 97. 26.3 10.4 11.3 73.3 99.9 99.7 38.9 29.5 43.2 19.7 99.7 69.7 28.3 9.9 15.3 75.0 99.7 99. 57.3 40.7 51.9 17.4 99.0 80.8 31.3 9.1 13.0 65.9 97.5 97.3 62.7 50.5 63.6 33.3 99.4 85. 13.9 4.4 7.8 30.5 96.5 85.2 24.6 10.4 9.4 80.7 99.5 99.2 35.3 25.0 31.7 39.5 100.0 100. 14.5 5.6 6.1 49.8 99.3 98.3 27.6 11.1 8.6 88.5 99.3 99.9 50.0 47.2 54.0 54.4 99.4 81. 3.3 0.6 0.6 42.2 98.0 58.6 25.3 7.3 11.6 75.5 98.3 98.5 5.5 3.1 4.8 35.5 98.3 87. 11.1 2.8 6.7 35.7 91.4 82.2 22.6 7.6 10.6 67.7 94.4 98.3 21.7 13.5 12.9 Table 9: Model performance on AIME/GPQA/MGSM dataset with Deepseek-Distill-R1-7B post-trained on 100/250 few Thai instances. The LRM is prompted to think in Thai. Eval Metric # Ins. AVG Query Language EN FR DE ZH JA RU ES BN TH SW TE Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) Matching Rate (%) Answer Acc. (%) None 100 250 None 100 250 None 100 250 None 100 None 100 250 None 100 250 15.8 99.6 100.0 24.3 0.2 0.8 3.3 99.4 100.0 31.7 1.1 0. 7.8 98.3 100.0 33.3 0.0 0.0 5.4 99.2 99.8 33.5 3.0 1.6 94.6 99.6 99.9 31.7 5.5 7. 0.0 98.5 99.8 41.9 2.8 1.7 97.3 99.5 100.0 58.3 9.3 12.9 2.0 98.0 99.8 37.7 3.7 2. 98.3 99.5 99.6 37.1 5.6 11.2 AIME 0.6 99.4 100.0 27.8 0.0 1.7 GPQA 0.3 99.5 99. 33.3 2.0 1.0 MGSM 87.5 99.9 100.0 35.9 5.3 8.1 21.1 100.0 100.0 25.6 0.6 1.1 18.4 99.7 99. 26.8 2.2 1.7 99.1 99.9 99.7 20.1 3.3 5.3 6.7 100.0 100.0 28.3 0.0 1.1 0.0 98.5 100. 38.7 2.7 1.5 94.8 99.5 99.6 38.3 5.6 9.3 11.1 100.0 100.0 28.3 0.0 1.1 1.5 99.7 100. 34.4 3.0 1.7 97.7 100.0 100.0 36.8 3.7 7.6 6.7 100.0 100.0 30.6 0.0 0.6 6.7 99.4 100. 23.3 0.0 0.6 23.3 98.9 100.0 23.9 0.0 0.6 0.5 97.7 99.8 36.0 3.2 1.8 93.7 99.9 99. 39.2 7.5 9.6 0.7 99.2 99.7 32.6 2.5 0.7 77.7 99.3 99.2 35.5 6.3 8.4 8.1 99.8 99. 32.6 3.0 1.3 97.3 99.9 100.0 20.4 3.1 4.8 32.8 100.0 100.0 54.4 100.0 100.0 5.6 0.6 0. 1.3 99.3 99.7 25.2 4.0 1.8 96.7 99.2 99.7 3.4 0.4 1.1 8.9 0.0 0.6 26.8 100.0 100. 24.6 4.4 1.6 100.0 100.0 100.0 24.4 10.8 10.1 Table 10: Model performance on AIME/GPQA/MGSM dataset with Deepseek-Distill-R1-7B post-trained on 100/250 few Telugu instances. The LRM is prompted to think in Telugu."
        }
    ],
    "affiliations": [
        "Boston Childrens Hospital",
        "Harvard University",
        "Mass General Brigham",
        "University of Amsterdam",
        "University of Groningen"
    ]
}