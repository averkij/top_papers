{
    "paper_title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "authors": [
        "Murat Bilgehan Ertan",
        "Marten van Dijk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{or}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$, and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \\to \\infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 7 3 2 0 1 . 1 0 6 2 : r Fundamental Limitations of Favorable PrivacyUtility Guarantees for DP-SGD Murat Bilgehan Ertan CWI Amsterdam Amsterdam, Netherlands bilgehan.ertan@cwi.nl Marten van Dijk CWI Amsterdam Amsterdam, Netherlands marten.van.dijk@cwi.nl"
        },
        {
            "title": "Abstract",
            "content": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the -differential privacy framework, which characterizes privacy via hypothesistesting trade-off curves, and study shuffled sampling over single epoch with gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces geometric lower bound on the separation κ, which is the maximum distance between the mechanisms trade-off curve and the ideal random-guessing line. Because large separation implies significant adversarial advantage, meaningful privacy requires small κ. However, we prove that enforcing small separation imposes strict lower bound on the Gaussian noise multiplier σ, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy σ 1 2 ln or κ (cid:18) 1 8 1 1 4π ln (cid:19) , and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as , the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing critical bottleneck in DP-SGD under standard worst-case adversarial assumptions."
        },
        {
            "title": "Introduction",
            "content": "The study of differentially private stochastic gradient descent (DP-SGD) [2] has long been motivated by the need to reconcile data privacy with the accuracy of deep neural networks. However, despite significant theoretical and empirical progress, the tension between privacy and utility remains unresolved. In particular, the degradation of training stability and test accuracy due to injected DP noise fundamentally conflicts with the requirement to add sufficient noise for rigorous differential privacy guarantees. Existing analyses of DP-SGD have primarily relied on the Poisson subsampling model [2, 9, 65, 46], in which each record is independently included in mini-batch with fixed probability. This assumption, introduced by Abadi et al. [2], enables privacy accounting techniques like Renyi Differential Privacy [45] and privacy-loss distributions for numerically tight composition [37, 43]. However, it is Affiliated with Vrije Universiteit Amsterdam. 1 β r I y Legend Ideal Random-guessing line Suboptimal upper bound Achievable -DP trade-off Separation κ κ 0 type error 1 Figure 1: Trade-off view of privacy in the -DP framework [21]. The black line shows the ideal random-guessing trade-off between type and type II errors. The vertical red segment κ denotes the the maximum distance between the achievable -DP trade-off and the ideal limit. primarily modeling convenience: in practice, modern deep learning systems do not sample examples independently but instead shuffle the entire dataset and partition it into fixed-size mini-batches each epoch. This shuffling-based batching is far more efficient computationally but breaks the independence assumptions exploited in Poisson analyses, making theoretical guarantees harder to derive. As result, most practical DP-SGD implementations train using shuffled batches while reporting privacy parameters as if Poisson subsampling were used [49, 14, 15]. This persistent mismatch between analytical assumptions and real implementations motivates our study of random shuffling as more realistic sampling mechanism for understanding DP-SGDs privacy behavior. Building on these observations, our analysis is carried out entirely within the -DP framework [21], which provides complete hypothesis-testing description of privacy and allows us to reason directly about the trade-off curves induced by shuffled DP-SGD. We focus on the single-epoch shuffling regime under the standard worst-case adversarial model, in which each data record appears in exactly one batch per epoch. In the standard worst-case adversarial model, the adversary may use the observable noisy batch updates together with any auxiliary information (such as batch size) available to infer the contribution of the differing data record; the formal model is given in Section 4. This setting eliminates the independence structure that underpins Poisson-based analyses and introduces fundamental dependencies in the adversarys observation model. We explicitly formalize this adversarial observation model, which is typically left implicit in prior work, and analyze its implications through the geometry of the -DP trade-off curve. Finally, while our primary results concern shuffled DP-SGD, we show via rigorous mixture argument that the same limitations extend, up to constant factors, to Poisson subsampling. We additionally derive an asymptotic separation bound for Poisson subsampling within the µ-GDP framework [21] which relies on asymptotic convergence without an explicit rate. Together, these results yield unified worst-case picture covering both the theoretical (Poisson) and practical (shuffling) sampling schemes, and demonstrate that our findings propagate to the privacy parameters reported in standard DP-SGD practice. Before presenting our main claim, we briefly recall how privacy is parameterized in DP-SGD. In the classical (ε, δ)-DP formulation, randomized algorithm is private when its output distributions on two neighboring datasets are nearly indistinguishable to any adversary; smaller values of ε therefore correspond to stronger privacy guarantees. In DP-SGD, this effect is achieved by two mechanisms: each per-example gradient is clipped to an ℓ2 radius C, which bounds its maximum possible influence, and Gaussian noise with standard deviation Cσ is added to every batch update. The clipping constant controls sensitivity, while the noise multiplier σ > 0 is the dominant driver of the resulting privacy parameters (ε, δ). Smaller σ improves test accuracy but necessarily weakens privacy, whereas larger σ strengthens privacy at the expense of utility. 2 In the -DP framework [21], an adversary attempting to distinguish two neighboring datasets forms test with false positive rate (FP) α and false negative rate (FN) β. The resulting trade-off function (α) gives the minimum achievable FN error among all tests with FP error at most α. Thus, captures the fundamental FPFN trade-off faced by the strongest possible adversary. The ideal privacy baseline is the random-guessing line β = 1 α, corresponding to zero leakage. Section 3.3 provides further background on the -DP formalism. key geometric quantity in this picture is the separation between the trade-off curve and the random-guessing line (perfect privacy). We define sep(f ) = max α[0,1] min γ[0,1] (cid:13) (α, (α)) (γ, 1 γ) (cid:13) (cid:13) (cid:13)2, that is, the maximum distance between point on the trade-off curve (α, (α)) and the ideal line (γ, 1 γ). When is symmetric and convex, this separation is attained at the fixed point ˆa satisfying 2. Smaller values of sep(f ) (and hence (ˆa) = ˆa, and we can write κ = sep(f ) = (1 2ˆa)/ smaller κ) correspond to stronger privacy, as the entire trade-off curve lies closer to random guessing. This privacy interpretation will be formalized and analyzed rigorously later. This notion of separation follows the -DP interpretation introduced by Dong et al. [21, 12] and later formalized by Zhu et al. [64]. Figure 1 provides an early geometric view of this hypothesis-testing landscape for shuffled DP-SGD. The vertical segment κ illustrates the separation sep(f ) for the corresponding trade-off curve. In the remainder of the paper, we show that under the standard worst-case adversarial model, our explicit upper bound on the shuffled trade-off curve implies lower bound on κ = sep(f ). Combining these elements yields our main conceptual consequence: an unavoidable or constraint for one-epoch shuffled DP-SGD with rounds. Under the standard worst-case adversarial model, the mechanism must satisfy: σ 1 2 ln or κ (cid:18) 1 8 1 (cid:19) 1 4π ln Both cannot be made small simultaneously. large noise multiplier σ hurts accuracy, while large separation κ indicates substantial privacy leakage. While the quantity 1/ ln vanishes asymptotically as , this decay is extremely slow: even for astronomically large values of , the required noise level remains far from zero. Our lower bound thus translates into privacyutility tension: in the shuffled setting and under the standard worst-case adversary of the DP framework, DP-SGD cannot operate in regime where both privacy and utility are simultaneously strong. As our analysis confirms, this tension is not an artifact of shuffling; the same lower bound applies to Poisson subsampling (up to constant factor), suggesting that the bottleneck lies in the worst-case adversarial framework itself. We complement our theoretical results with experiments illustrating the practical severity of this trade-off. In particular, we show that the noise levels implied by our lower bound already lead to substantial degradation in test accuracy under realistic training budgets. These empirical results confirm that the theoretical limitations we identify also manifests concretely at finite parameter values encountered in practice. We emphasize that this limitation is not inherent to DP-SGD as an algorithm, but rather to the adversarial assumption framework under which its privacy is analyzed. In particular, the standard definition implicitly grants the adversary full visibility into every component of the stochastic updates, making certain forms of privacy leakage unavoidable. Our main contributions are as follows: We analyze single-epoch shuffled DP-SGD under the standard worst-case adversarial model in the -DP framework and derive an explicit suboptimal upper bound on its trade-off curve. This yields geometric lower bound on the separation κ, exposing fundamental privacy utility trade-off: the noise multiplier σ and separation cannot be driven below explicit lower bounds at the same time. Then, via mixture argument, we show that Poisson subsampling inherits the same limitation up to constant factor, providing unified worstcase characterization of both sampling schemes. Empirically, we validate the predicted threshold behavior, showing that the noise levels implied by our bounds already cause substantial accuracy degradation across models, datasets, and batch sizes. 3 Organization. The remainder of the paper is structured as follows. Section 2 reviews the most relevant prior work. Section 3 introduces the differential privacy framework, the sampling mechanisms used in practice, and the BatchSGD update model. Section 4 formalizes the worst-case adversarial observation model that underlies our analysis. In Section 5, we introduce suboptimal hypothesis testing framework for DP-SGD and define the separation metric κ formally. Next, Section 6 develops the geometric tools needed to analyze this metric, establishes our main separation lower bound for shuffled DP-SGD, and extends the result to Poisson subsampling via mixture argument and yielding unified worst-case picture for both sampling schemes. Section 7 presents empirical illustrations of our theoretical findings, and Sections 8 and 9 conclude with broader implications and future directions."
        },
        {
            "title": "2 Related Work",
            "content": "Differential Privacy (DP) [2, 26, 24, 25, 23] has emerged as principled framework for preserving individual information in data analysis and model training. Among its applications, Differentially Private Stochastic Gradient Descent (DP-SGD) [2] is the most widely used privacy-preserving method in deep learning, injecting calibrated noise into gradient updates to satisfy target privacy budget. It has also been studied for various machine learning domains [27, 54, 16, 13, 20, 4, 33, 35, 61, 42], and adopted in real-world systems from the U.S. Census [3] to Microsoft telemetry [19] and Googles recent initiative to train large language models from scratch under DP [52]. It is now supported by major privacy libraries such as TensorFlow Privacy, JAX Privacy, and Opacus [1, 8, 60], which provide practical implementations and privacy accounting tools for large-scale training. Analytically, the privacy guarantees of DP-SGD have been refined through composition frameworks such as Renyi Differential Privacy (RDP) [45] and Privacy-Loss Distributions (PLD) [53, 37, 43], which enable tighter accounting of cumulative privacy loss across training iterations. The classical (ε, δ)-DP [26, 25] formulation summarizes privacy using two scalar parameters, whereas the emerging -DP framework [21, 12, 58] describes privacy through the entire trade-off curve of optimal type I/type II errors, providing complete and geometrically interpretable characterization of indistinguishability. Importantly, the trade-off curve is the most informative DP representation: as shown in [21] (Appendix and B), all standard DP notions, including (ε, δ)-DP and divergence-based relaxations, can be derived from it. Every -DP trade-off curve induces corresponding family of (ε, δ)-DP guarantees, and conversely each (ε, δ)-DP guarantee imposes linear constraint on the trade-off curve [21]. This relationship allows techniques developed for (ε, δ)-DP to be imported into the -DP framework. This motivates our decision to conduct all analysis directly in the -DP framework. Privacy amplification by shuffling, introduced by Erlingsson et al. [28] and later refined by Feldman et al. [29, 30], showed that randomly permuting locally privatized reports can strengthen privacy guarantees in the central model. However, this amplification applies to locally randomized mechanisms rather than to DP-SGDs centralized setting. The most closely related analysis of shuffled DP-SGD is due to Chua et al. [14, 15], who study the mechanism under their Adaptive Batch Linear Queries (ABLQ) framework. Their results provide lower bounds on the achievable trade-off function for both singleand multi-epoch training; in the -DP viewpoint, such lower bounds translate to upper bounds on the separation κ, characterizing how private the mechanism can be in the best case. Importantly, they also show that random shuffling in DP-SGD leads to weaker provable privacy guarantees than Poisson subsampling, highlighting discrepancy with the common practice of reporting Poisson-based parameters for shuffled implementations [49]. Our analysis is complementary: we derive an explicit upper bound on the shuffled trade-off function under the standard worst-case adversary, which yields lower bound on separation κ. The random-shuffling regime remains the standard sampling strategy in modern deep learning practice. It has been extensively studied from an optimization perspective, where several works analyze its convergence behavior and compare it to sampling with replacement [48, 32, 44, 31]. Despite its popularity, there is no clear empirical or theoretical evidence that random shuffling systematically outperforms Poisson subsampling, to our knowledge. Its widespread adoption instead reflects training conventions and practical considerations: Poisson subsampling does not scale well to large datasets because random access is inefficient for data that do not fit in memory, and the resulting variable batch sizes are inconvenient to handle in standard deep learning frameworks [14]. As result, shufflingbased batching remains the default in modern training systems, even though the corresponding privacy analyses continue to rely on Poisson-based assumptions [49]. Apart from privacy accounting and sampling considerations, it is now well established that training deep neural networks with DP-SGD often incurs substantial loss in utility, with prior work consistently reporting degradation in accuracy and generalization under meaningful privacy guarantees [56, 16, 39, 41, 10, 51, 5]. common insight across these works is that the injected Gaussian noise, controlled by the noise multiplier σ, is one of the dominant sources of this degradation, acting as barrier to effective training."
        },
        {
            "title": "3 Preliminaries",
            "content": "Differential Privacy (DP) guarantees that randomized mechanism produces statistically similar outputs on any two neighboring datasets differing in single individuals record. Within stochastic gradient descent (SGD), the DP-SGD algorithm [2] enforces this requirement by first bounding the effect of each example via gradient clipping, and then injecting Gaussian noise to obscure the contribution of any individual. DP-SGD modifies the standard mini-batch update in two ways: (i) each per-sample gradient is clipped to fixed threshold, bounding sensitivity, and (ii) Gaussian noise scaled to this threshold is added to the aggregated gradient before updating the model parameters. The precise mechanism is formalized later in Section 3.2. Moreover, the privacy of DP-SGD depends critically on how mini-batches are sampled. We therefore introduce the two canonical sampling mechanisms, namely Poisson Subsampling and Random Shuffling in the following subsection. Scope of analysis. We analyze the privacy guarantees of single epoch of DP-SGD under the standard worst-case adversarial model (see Section 4). In this setting the sampler produces index sets S1, . . . , SM , each used exactly once to form mini-batch. Extensions to multiple epochs require additional composition and are beyond the scope of our core lower bound. 3.1 Sampling Mechanisms in DP-SGD The sampling mechanism plays central role in the privacy analysis of DP-SGD because subsampling provably amplifies differential privacy guarantees: when each update is computed on random mini-batch, given record influences the mechanism only in the rounds where it is sampled, thereby reducing the likelihood of leaking information about that individual [7]. While various sampling strategies have been proposed [49], two have become canonical in both theory and practice: Poisson subsampling and random shuffling. The former is analytically convenient and therefore ubiquitous in theoretical work; the latter is the standard choice in practical deep learning systems. Both determine how mini-batches are formed from dataset = {ξ1, . . . , ξN }, but they exhibit fundamentally different statistical dependencies and therefore lead to different privacy-amplification behaviors. To make these mechanisms explicit we formalize them below and describe the procedures in Algorithms and later. Definition 3.1 (Poisson Subsampling). Given dataset of size , Poisson subsampling includes each element independently with probability (0, 1) at each round = 1, . . . , . This produces random index sets Sj [N ] with expected size E[Sj] = qN , and independence is maintained both across records and across rounds. Definition 3.2 (Random Shuffling). Given dataset of size and fixed number of rounds , random shuffling samples uniform permutation π of the dataset indices and then forms disjoint batches of equal size = N/M by taking consecutive blocks of π. Concretely, for each = 1, . . . , , we have Sj = {π(j1)b+1, . . . , πjb}. So each record appears in at most one batch per epoch, and the last = mod permuted indices are discarded. In the remainder of this work, we focus on the random-shuffling mechanism, as it captures practical DP-SGD implementations and introduces inter-round dependencies that are absent under Poisson subsampling. As we later show, our main limitation results extend to Poisson subsampling up to constant factors, yielding unified worst-case characterization for both sampling schemes. 5 Algorithm Poisson Subsampling 1: Input: Dataset of size , sampling rate (0, 1), number of rounds 2: for = 1, . . . , do Initialize Sj 3: for each index [N ] do 4: 5: 6: 7: end for 8: Output: Sequence of index sets (S1, . . . , SM )"
        },
        {
            "title": "Include i in Sj independently with probability q",
            "content": "end for Algorithm Random Shuffling 1: Input: Dataset of size , number of rounds 2: Sample random permutation π over [N ] 3: Let N/M 4: for = 1, . . . , do 5: 6: end for 7: Output: Sequence of index sets (S1, . . . , SM ) The last = mod indices of π Define index set Sj {π(j1)b+1, . . . , πjb} are discarded. 3.2 DP-SGD Mechanism We formalize the update rule used throughout this work. Let = {ξ1, . . . , ξN } denote the training dataset of size , and let S1, . . . , SM [N ] be index sets produced by sampling mechanism (Poisson subsampling or random shuffling). The corresponding mini-batch at iteration is Bj = {ξi : Sj}. Given loss function ℓ(w; ξ), the empirical risk is L(w) = 1 (cid:88) i= ℓ(w; ξi), and the per-sample gradient at iteration is g(wj; ξi) = wℓ(wj; ξi). Definition 3.3 (Gradient Clipping). Given clipping threshold > 0, the clipped version of per-sample gradient g(w; ξ) is [g(w; ξ)]C = g(w; ξ) min 1, (cid:18) g(w; ξ)2 (cid:19) . Thus [g(w; ξ)]C2 for all ξ, ensuring bounded sensitivity of the aggregated update. Definition 3.4 (Gaussian Noise Mechanism). Given clipping threshold and noise multiplier σ 0, the DP-SGD mechanism adds Gaussian noise N(cid:0)0, (Cσ)2I(cid:1), where is the identity matrix (i.e., independent Gaussian noise per coordinate). The standard deviation Cσ scales proportionally with both the clipping threshold and the noise multiplier. At each iteration j, the mechanism processes the mini-batch Sj by computing clipped per-sample gradients, adding Gaussian noise as in Definition 3.4, averaging by the batch size, and then performing an SGD update step. The Algorithm summarizes the procedural implementation, including batch generation. For σ = 0 and = , this procedure reduces to standard mini-batch SGD; for σ > 0 with finite C, it corresponds to the DP-SGD update of [2]. 3.3 Differential Privacy and -DP We briefly recall the classical (ε, δ) formulation of differential privacy and its refinement through the -DP framework, both of which rely on notion of adjacency between datasets. 6 Algorithm DP-SGD Mechanism [2] 1: Input: Dataset d, number of rounds , learning rate η, clipping bound C, noise multiplier σ (optional) 2: for each epoch do 3: Generate mini-batches {S1, . . . , SM } using either Poisson subsampling (Alg. P) or random 4: 5: 6: 7: 8: shuffling (Alg. S) for each Sj do Compute per-sample gradients g(wj; ξi) for all Sj Clip each gradient: [g(wj; ξi)]C = g(wj; ξi) min (cid:16) 1, g(wj; ξi)2 (cid:17) Compute clipped batch sum: Gj = (cid:88) iSj [g(wj; ξi)]C Add Gaussian noise and average: zj N(cid:0)0, (Cσ)2I(cid:1), Gj = (cid:16) 1 Sj (cid:17) Gj + zj Update model: wj+1 wj η Gj 9: 10: 11: end for end for Two datasets that differ only in the contribution of single individual are called adjacent, written d. This notion can be formalized in several ways, depending on how one models the presence, replacement, or removal of record. We will introduce the standard adjacency relations shortly, but for any such choice, differential privacy is defined as follows. Definition 3.5 (Differential Privacy (DP) [25]). randomized mechanism is (ε, δ)-DP with respect to an adjacency relation if, for all adjacent datasets and all measurable events E, Pr[M(d) E] eε Pr[M(d) E] + δ. The -DP framework characterizes privacy through the entire trade-off function (α), which captures the best achievable Type and Type II error trade-off when distinguishing from d. This viewpoint provides complete geometric representation of privacy, supports tight composition, and matches the Gaussian mechanism central to DP-SGD. In addition, DP relies on formal notion of adjacency between datasets. Several adjacency models appear in the literature; we record them formally below. Definition 3.6 (Substitution Adjacency). Datasets and are substitution neighbors if they have equal cardinality and differ in exactly one record: = {ξ} {ξ}. Thus = d, and the differing individual is replaced rather than removed. Definition 3.7 (Add/Remove Adjacency). Datasets and are add/remove neighbors if one is obtained from the other by inserting or deleting single record. Equivalently, = 1, = {ξ} or = {ξ}. Definition 3.8 (Zero-Out Adjacency [15, 36]). Let denote the space of valid data records, and let := {} be the augmented space, where denotes ghost record whose contribution to any query or gradient is defined to be zero. For dataset = (d1, . . . , dN ) removing the i-th entry. Two datasets d, N index [N ] such that and an index [N ], let di denote the dataset obtained by are said to be zero-out adjacent if there exists an di = i, {di, i} = {, ξ} for some ξ , that is, the datasets are identical except at single position where one contains genuine record and the other contains the ghost record . 7 When DP-SGD uses Poisson subsampling, each element is included in batch independently with probability q. This naturally corresponds to add/remove adjacency (Definition 3.7), since the mechanism literally behaves as if an individual is either present or absent in any given round. Under add/remove adjacency, the ℓ2-sensitivity of the clipped gradient sum is exactly C, and the Gaussian mechanism requires noise of scale Cσ. In contrast, the -DP framework [21, 12, 66] is formulated under substitution adjacency (Definition 3.6), where datasets must have the same size. Translating DP-SGD into this setting means comparing batch from to batch from where the differing record contributes either its gradient or its replacement gradient. Because the batch size Sj is the same in both datasets, the worst-case sensitivity becomes 2C (twice the sensitivity of add/remove adjacency). This imposes an extra factor 2 on the required Gaussian noise scale when using substitution-based -DP analysis. third adjacency notion, zero-out adjacency (Definition 3.8) [15, 36], avoids the sensitivity doubling inherent to substitution adjacency by replacing the differing record with zero-gradient dummy. Under this definition, the ℓ2-sensitivity of the clipped gradient sum is exactly (the distance between gradient vector and zero), rather than 2C. Furthermore, unlike add/remove adjacency, zero-out adjacency preserves the total dataset cardinality , maintaining the fixed-size index structure required for the rigorous analysis of random shuffling. Consequently, adopting zero-out adjacency yields sharper analysis: it avoids the factor of 2 in the required noise scale that commonly arises in substitution-based -DP analyses. For the remainder of this work, we therefore analyze DP-SGD under zero-out adjacency, so that our bounds reflect the tightest possible worst-case privacy guarantees. We note that this modeling choice does not affect our main qualitative conclusions: all results can be translated to substitution adjacency at the cost of factor-of-two increase in the required noise scale."
        },
        {
            "title": "4 Adversarial Model",
            "content": "A key element in any differential privacy analysis is the choice of adversary and the information they are assumed to observe. In DP-SGD, the adversarys view is determined by what portion of the stochastic updates (gradients, batch structure, and added noise) are observable. We consider two notions: practical adversary corresponding to realistic communication constraints, and worst-case adversary required for proving formal differential privacy guarantees. 4.1 Practical Adversary In many distributed or federated training settings, an adversary can observe only the messages transmitted to the server. These messages are the noisy aggregated gradients produced by DP-SGD. The following assumption captures this realistic eavesdropper model. Assumption 4.1 (Practical Adversary). In each round = 1, . . . , , the adversary observes only the (noisy, averaged) update Gj = 1 Sj (cid:88) iSj [g(wj; ξi)]C + (0, (Cσ)2I) , and has no access to individual gradients or intermediate computations. 4.2 Worst-Case Adversary To enable standard DP arguments, analyses in the literature traditionally endow the adversary with additional knowledge that is not realistically available but simplifies the analysis under the adjacency models defined in Section 3.3. We adopt zero-out adjacency (Definition 3.8): neighboring datasets and both have size and share the first 1 records. They differ only in the -th element: in it is ghost record whose gradient is defined to be zero ([g(wj; )]C = 0), while in it is valid record ξN . Then, we give the adversary access to the sum of all clipped gradients except the potentially differentiating one (index ). Furthermore, we assume the batch size Sj is public knowledge (or observable from metadata). 8 Assumption 4.2 (Auxiliary Knowledge Under Zero-out Adjacency). Under zero-out adjacency (Definition 3.8), for each round the adversary knows the batch size Sj and the partial sum of clipped gradients excluding the potentially differing record: G() = (cid:88) [g(wj; ξi)]C. iSj {N } Given the observable update Gj from Assumption 4.1 and the auxiliary information from Assumption 4.2, the adversary can deterministically recover the noisy contribution of the differing record. Proposition 4.3 (Isolation of the Individual Contribution). Let δN Sj denote the indicator that belongs to batch Sj. Under Assumptions 4.1 and 4.2, the adversary can compute Zj := Sj Gj G() . Consequently, when the dataset is (ghost record present), the contribution is zero and the adversary reconstructs pure noise: Zj = (0, (Cσ)2I), and when the dataset is (valid record ξN present), the reconstruction becomes = δN Sj [g(wj; ξN )]C + (0, (Cσ)2I). (1) (2) Consequence. Since the worst-case adversary can mathematically reconstruct these differential terms from the public observations and the auxiliary knowledge under Assumptions 4.1 and 4.2, analyzing the indistinguishability of (1) and (2) is sufficient to bound the privacy leakage of the entire protocol. Hence, without loss of generality, our analysis may focus entirely on the reconstructed variables Zj and for the remainder of this work. For intuition: these are the only quantities that represent the leakage about the differentiating element being in the dataset or not."
        },
        {
            "title": "5 Hypothesis Testing and f -DP Characterization",
            "content": "Having defined the adversarial view, we now characterize privacy leakage as statistical hypothesis testing problem between neighboring datasets and d. This perspective aligns naturally with the - DP framework and reveals structural limitations of DP-SGD under the standard worst-case adversarial model. Throughout this section and the next we work entirely under random shuffling. The Poisson subsampling case will be derived later, in Section 6.3. 5.1 Projection onto One-Dimensional Statistic The adversary observes all transmitted model parameters {wj}. Under zero-out adjacency, the two datasets differ in the -th record: dataset contains ghost record (zero gradient), while dataset contains the valid record ξN . The adversary can compute the clipped gradient of the valid record: = [g(wj; ξN )]C. Under zero-out adjacency, the DP interpretation requires considering the worst-case sensitivity relative to the zero vector. Because the clipped gradient satisfies v2 C, the maximum possible distance between the valid gradient and the ghost gradient is 02 C. Consequently, the noise added to the batch gradient in round is (0, (Cσ)2I) under both datasets. Using terms in Equations (1) and (2), the adversarys observations in round are: Zj = (0, (Cσ)2I) (dataset d, ghost record), = δN Sj + (0, (Cσ)2I) (dataset d, valid record). Because Gaussian noise is rotationally symmetric, all information relevant to distinguishing (pure noise) from (signal plus noise) lies along the direction of the signal vector v. Hence, we can project both Zj and onto this direction. Now, assuming the adversary takes the worst-case direction where v2 = and normalizing by the sensitivity C, we obtain scalar statistic. Conditioned on the round where the record is sampled (N Sj), the distributions differ by shift of exactly 1: (0, σ2) vs. (1, σ2). Finally, dividing by σ for convenience produces the canonical form of the hypothesis test: (0, 1) vs. (σ1, 1). This projection reduces the adversarys task to one-dimensional Gaussian hypothesis test, fully capturing the information relevant to determining whether the differentiating record was included in the batch (signal) or replaced by ghost record (noise). It serves as the basis for the hypothesis-testing formulation presented in the next section. 5.2 Adversarial Hypothesis Formulation Let H0 denote the null hypothesis corresponding to dataset and let H1 denote the alternative corresponding to dataset d. Over an epoch of = N/m rounds, the adversary observes: (x1, . . . , xM ) (cid:40) (N (0, 1))M , H0, (N (u1σ1, 1), . . . , (uM σ1, 1)), H1, where uj := δN Sj indicates whether the valid sample ξN appears in the j-th batch. Under H0, no valid record exists, so effectively uj = 0 for all j. Under H1, conditioning on the event that the valid record is included in the processed batches (i.e., not discarded by the shuffling procedure), it appears in exactly one batch Sj, so exactly one uj = 1, with the index chosen uniformly at random. Under the worst-case adversary, the entire sequence (x1, . . . , xM ) is observable, so the adversary can compute the full likelihood ratio between H0 and H1. The corresponding probability densities are ex2 2π j=1H0] = [(xj)M (cid:89) / (3) , j=1 [(xj)M j=1H1] = = 1 1 (cid:88) j=1 (cid:88) j= e(xj 1/σ)2/2 2π /2 ex2 2π (cid:89) i=1 i=j exj /σ1/(2σ2) (cid:89) i=1 /2 ex2 2π . (4) 5.3 Application of the NeymanPearson Lemma We now recall the classical NeymanPearson framework, which provides the optimal hypothesis test distinguishing between datasets and in terms of Type and Type II errors. Let ϕ denote (possibly randomized) rejection rule for the null hypothesis. Given the observed sequence (xj)M The false positive rate (Type error) of test ϕ is defined as j=1, the rule ϕ outputs 1 when H0 is rejected and 0 otherwise.2 α(ϕ) = E[ϕ d] [0, 1], and represents the probability of incorrectly rejecting the null hypothesis when is true. test of size α(ϕ) is said to have level if α(ϕ) a. The set of all level-a tests is therefore Ta = {ϕ : E[ϕ d] = α(ϕ) a}. The false negative rate (Type II error) is defined as β(ϕ) = 1 E[ϕ d] [0, 1], 2The rejection rule can also be probabilistic: ϕ may output probability value in [0, 1], in which case the decision to reject is made by flipping biased coin with success probability ϕ. 10 which represents the probability of incorrectly accepting the null hypothesis when is true. Among all tests with size at most a, the uniformly most powerful (UMP) test is the one that minimizes the Type II error. Formally, ϕ is UMP among level-a tests if, for every ϕ Ta, β(ϕ) β(ϕ). The optimal trade-off between α and β can thus be summarized by the trade-off function (a) = inf ϕ {β(ϕ) : α(ϕ) a}. This function (a) corresponds directly to the definition of -DP [21]. According to the NeymanPearson lemma, any UMP test of level must be likelihood-ratio test. That is, there exists threshold 0, where P[] denotes probability: ϕ((xj)M ϕ((xj)M j=1) = 1 P[(xj)M j=1) = 0 P[(xj)M j=1 d] > P[(xj)M j=1 d] < P[(xj)M j=1 d], j=1 d], (5) where the equality case (when the two densities are equal) has measure zero under both distributions.3 The lemma guarantees that if test ϕ satisfies α(ϕ) = a, then it is UMP among level-a tests; moreover, all such UMP tests share the same likelihood-ratio form in (5) with common threshold h. Substituting (3) and (4) derived earlier into (5) yields the concrete rejection rule ϕ((xj)M ϕ((xj)M j=1) = 1 1 j=1) = 0 1 (cid:80)M (cid:80)M j=1 exj /σ1/(2σ2) > h, j=1 exj /σ1/(2σ2) < h. (6) In the subsequent sections, we will not use this exact form of ϕ, instead we adopt suboptimal test to derive our bound. 5.4 Geometric Setup and Notation We conclude this section by introducing the geometric framework used in our limitation argument. Our analysis is based on the notion of separation between an -DP trade-off curve and the randomguessing line, and on the observation that separation can be lower bounded using any explicit pointwise upper bound on the true trade-off curve. Definition 5.1 (Pointwise separation). For any trade-off curve g(α), its pointwise separation from the ideal random-guessing line β = 1 α is sepg(α) := (1 α) g(α) 2 . (7) Geometric interpretation. For any α [0, 1], the quantity sepg(α) in (7) is exactly the Euclidean distance from the point (α, g(α)) to the random-guessing line := {(γ, 1 γ) : γ [0, 1]}. In Fig. 2, treating the blue curve as the graph of g, the perpendicular dropped from (α, g(α)) to forms an isosceles right triangle with legs of length x, so that ((1 α) g(α))2 = 2(x)2, yielding (7). Definition 5.2 (Global Separation). The separation of trade-off curve g, denoted sep(g), is the maximum pointwise separation: sep(g) = max α[0,1] sepg(α). (8) Equivalently, using the geometric interpretation above, this corresponds to the maximum Euclidean distance from the curve to the random-guessing line: sep(g) = max α[0,1] min γ[0,1] (cid:13) (cid:13)(α, g(α)) (γ, 1 γ)(cid:13) (cid:13)2. (9) where γ parameterizes the projection of (α, g(α)) onto the random-guessing line L. Equations (8) and (9) therefore define the same quantity. 3Formally, for the set = {(xj)M j=1 : P[(xj)M j=1 d] = P[(xj)M j=1 d]}, we have P[(xj)M j=1 d] = P[(xj)M j=1 d] = 0. 11 1 1 1a 2 1a 2 β r e t π 4 x π 4 κsub κshuf Legend: Ideal Random-guessing line (RL) Suboptimal upper bound trade-off (fsub) κsub Separation between fsub and RL Achievable -DP trade-off (fshuf) κshuf Separation between fshuf and RL Using (7): = (1 a) 1a 2 = 1 8 0 1 4πln type error 1 Figure 2: Illustrative geometry of the suboptimal and true trade-off functions in our impossibility argument. The figure compares the random-guessing line (black), the suboptimal upper bound trade-off fsub (blue), and the true -DP trade-off fshuf (green, dashed). The red segment is the (a) from the suboptimal curve at the analytically tractable point a, pointwise separation sepfsub which forms an isosceles right triangle with base angle 45; the blue segment κsub is the separation sep(fsub) of the entire suboptimal curve; and the green segment κshuf is the separation sep(fshuf) of the true -DP trade-off under random shuffling. The separations satisfy κsub κshuf . We now characterize where the maximum in (8)(9) is attained for the class of symmetric and convex trade-off curves. Lemma 5.3 (Separation). Let : [0, 1] [0, 1] be symmetric and convex trade-off function in the -DP framework. Then the maximum in (9) is attained at the unique fixed point ˆa (0, 1/2] satisfying (ˆa) = ˆa, and the separation equals sep(f ) = 1 2ˆa 2 =: κ. (10) Moreover, for any symmetric and convex that is not identically equal to the random-guessing line, the separation necessarily satisfies 0 < κ < 1 2 . (11) Proof. Because the random-guessing line is the diagonal {(γ, 1 γ) : γ [0, 1]}, the inner minimization in (9) amounts to orthogonally projecting (α, (α)) onto this line. By symmetry of around diagonal and convexity of optimal trade-off curves [21, 12], the orthogonal projection occurs at γ = α, giving the simplified form in (8). sep(f ) = max α[0,1] (1 α) (α) 2 . (12) For symmetric and convex , the farthest point of the trade-off curve from the diagonal occurs at its intersection with the diagonal, i.e., the unique fixed point ˆa satisfying (ˆa) = ˆa. Thus, the separation 2 , 1 is the distance between (ˆa, ˆa) and the midpoint ( 1 2 ) of the random-guessing line. Substituting into (12) yields (10). The absolute bound (11) follows directly from (10) and the fact that any valid symmetric convex trade-off curve satisfies 0 < ˆa < 1/2. 12 Remark. Separation inherits structural properties from -DP such as post-processing because it is derived from the trade-off curve. However, sep(f ) is not DP metric: it lacks composition theorem and cannot track privacy under adaptive composition. It is an interpretable geometric property of an -DP curve. Geometric preview. Figure 2 provides preview of the geometric structure underlying our limitation argument. The black line corresponds to random guessing, the green curve depicts the true -DP trade-off fshuf of shuffled DP-SGD, and the blue curve fsub represents particular explicit trade-off curve that upper-bounds fshuf pointwise. At this stage, the figure should be interpreted illustratively: the precise definition and derivation of fsub are deferred to Section 6.1. The figure highlights three separation quantities. The quantity denotes the pointwise separation at specific point on the upper-bounding curve, κsub denotes the global separation of that upper bound, and κshuf denotes the separation of the true trade-off curve. Independently of how the upper bound is constructed, these quantities necessarily satisfy κsub κshuf , whenever one trade-off curve upper bounds another pointwise. In Section 6.1, we formally construct the suboptimal trade-off curve fsub, verify that it upper bounds fshuf , and make all quantities shown in Figure 2 fully explicit."
        },
        {
            "title": "6 Limitations of Favorable Privacy–Utility Trade-offs",
            "content": "Theorem 6.1 (Lower Bound on Separation for Shuffled DP-SGD). Consider one-epoch random shuffling. Let 1 denote the number of rounds, and let σ > 0 be the noise multiplier. Let κshuf := sep(fshuf ) denote the separation of the true -DP trade-off curve under shuffling defined in (22). Then we have the following statement: κshuf (cid:18) 1 8 1 (cid:19) 1 4π ln OR σ 1 2 ln Thus, unless the assumptions underlying this worst-case analysis are relaxed, improving privacy (i.e., reducing the separation κ) inevitably forces the noise level σ into regime that degrades utility (see Appendix for (ε, δ)-DP derivation). This trade-off is reflected in our experimental results and is consistent with empirical observations in private deep learning. In Section 6.3, we show that the same phenomenon persists under Poisson subsampling (up to constant factors). The proof of Theorem 6.1 is given in Section 6.2. 6.1 Suboptimal Hypothesis Test Instead of using the optimal NeymanPearson likelihood ratio test, we consider suboptimal rejection rule that thresholds the maximum coordinate of the observation vector (x1, . . . , xM ): ϕ((xj)M j=1) = 1 ϕ((xj)M j=1) = 0 max j=1 max j=1 exj /σ1/(2σ2) > h, exj /σ1/(2σ2) < h. This test is equivalent to thresholding the maximum observed statistic: ϕ((xj)M ϕ((xj)M j=1) = 1 maxM j=1) = 0 maxM j=1 xj > h, j=1 xj < h, (13) (14) where = σ ln + 1/(2σ). Although suboptimal, this rule isolates the single coordinate shifted by 1/σ under H1, enabling closed-form -DP trade-off. False-negative and false-positive rates. For 0, define α(h) = Pr β(h) = Pr (cid:20) max j=1 (cid:20) max j=1 (cid:21) xj > d xj < d , (cid:21) . (15) (16) Under (no shift), all xj (0, 1) independently, giving α(h) = 1 Φ(h)M , α1(a) = Φ1(cid:0)(1 a)1/M (cid:1), where Φ is the CDF of (0, 1). Under (one coordinate shifted by 1/σ), the shifted coordinate must satisfy xj < h, which occurs with probability Φ(h 1/σ), while all remaining 1 unshifted coordinates must lie below h, each with probability Φ(h). Independence of the coordinates therefore gives β(h) = Φ(h 1/σ) Φ(h)M 1. Thus, the induced (suboptimal) trade-off function is equal to β(α1(a)): fsub(a) = Φ (cid:16) Φ1(cid:0)(1 a)1/M (cid:1) σ1(cid:17) (1 a)(M 1)/M . (17) ln for some Throughout this section, we work in the regime σ 1/ (cid:112)1/2. Lemma 6.2 (Bound under σ 1/ For any integer 1 and any noise multiplier σ > 0 satisfying σ 1/ 1/ 4π ln such that 2 ln ). Let fsub(a) denote the suboptimal trade-off in (17). 2 ln there exists some 2 ln , and write σ = s/ Proof. Under the standing assumption σ = s/ inner argument of Φ in (17) equals zero, i.e., ln with (cid:112)1/2, we choose such that the fsub(a) = 2 (1 a)(M 1)/M . Φ1(cid:0)(1 a)1/M (cid:1) = σ1, (18) Substituting into (17) gives fsub(a) = Φ(0) (1 a)(M 1)/M = 1 (1 a)(M 1)/M , where we used the symmetry of the standard normal distribution, for which Φ(0) = 1 2 . Applying Φ to both sides of (18), and using that Φ is invertible, yields (1 a)1/M = Φ(1/σ), and hence 1 = Φ(1/σ)M = (1 Φ(1/σ))M . (19) Next, we use the standard Gaussian tail bound and substituting = 1/σ, we obtain Φ(t) < et2/2 2π , > 0, Φ(1/σ) σe1/(2σ2) 2π . Exponentiating both sides with and using (19) yields (cid:32) 1 1 σe1/(2σ2) 2π (cid:33)M . 14 Substituting σ = s/ ln yields (cid:32) 1 1 (cid:33)M 1/(2s2) 2π ln 1 11/(2s2) 2π ln . For (cid:112)1/2, this implies 1 4π ln . This establishes the desired bound. 6.2 Separation Lower Bounds for Shuffled DP-SGD We now instantiate the geometric framework introduced earlier using the analytically tractable suboptimal trade-off curve derived above (cf. (17)). This yields an explicit geometric lower bound on separation. In Figure 2, we consider: the black line β = 1 α (ideal random guessing); the green curve fshuf (α), representing the true trade-off of shuffled DP-SGD under worst-case adversary; the blue curve fsub(α), an explicit suboptimal upper bound derived in (17). Notation: three separations. Using Definition 5.1 and (8), we introduce the three quantities used in the analysis and in the Figure 2: := sepfsub (a), κsub := sep(fsub) = max α[0,1] sepfsub (α), κshuf := sep(fshuf ) = max α[0,1] sepfshuf (α). (20) (21) (22) Here, is the pointwise separation at the analytically tractable point (a, fsub(a)), κsub is the global separation of the suboptimal curve fsub, and κshuf is the true separation of the shuffled trade-off curve fshuf . Then, since fsub(α) fshuf (α) for all α, we have sepfsub (α) sepfshuf for all α, (α) and therefore κsub κshuf . (23) Thus any lower bound obtained for immediately implies the same lower bound for κshuf , the true separation of the shuffled -DP trade-off. We now prove Theorem 6.1 by combining the ingredients developed in the previous sections. The proof proceeds by constructing an explicit, analytically tractable upper bound on the -DP tradeoff curve under one-epoch shuffling using the suboptimal hypothesis test of Section 6.1, and then translating this bound into geometric lower bound on the separation via the framework of Section 5.4. Proof of Theorem 6.1. Assume σ 1/ off curve defined in (17). 2 ln , and let fsub denote the induced (suboptimal) tradeWe now convert Lemma 6.2 into geometric lower bound that reveals an inherent limitation of the privacyutility trade-off. In Figure 2, the suboptimal point (a, fsub(a)) lies on the blue curve representing the analytically tractable upper bound: (a, fsub(a)) = (cid:16) a, 1 2 (1 a)(M 1)/M (cid:17) . Let be the perpendicular distance from this point to the random-guessing line β = 1 α, so that is given by (20). Rewrite fsub(a) = 1 2 (1 a)(M 1)/M = 1 2 (1 a)1/M . 15 By Lemma 6.2, 1/ 4π ln , hence"
        },
        {
            "title": "Define",
            "content": "(1 a)1/M (cid:16) 1 1 4π ln (cid:17)1/M . εM := 2 4π ln . (24) (25) 4π ln < 1/2 for all 2, the elementary inequality (1 t)1/M 1 + 2t Since 1/ (0, 1/2) implies (applying it to the right-hand side of (24) with = 1/ for Consequently, (1 a)1/M 1 + εM . fsub(a) 1 2 (1 + εM ). By definition, the perpendicular distance to the line β = 1 α equals = (1 a) fsub(a) 2 . Substituting (27) yields Using again 1/ 4π ln gives 1 8 (1 εM ). (cid:18) 1 1 8 1 4π ln (cid:19) (1 εM ). 4π ln ) that (26) (27) (28) (29) Finally, by convexity of -DP trade-off functions [21], the geometric separations satisfy κsub κshuf . Therefore, κshuf 1 (cid:18) 1 (cid:19) 1 4π ln (1 εM ) (30) which proves the claimed lower bound. Note that, although the bound above is explicit, the correction factor (1 εM ) converges to 1 at rate O(1/M ); we therefore omit it in the theorem statement and retain it only in the proof for full rigor. Interpretation. The inequality (30) provides concrete lower bound on the geometric separation κshuf between the true -DP trade-off curve and the random-guessing line. Equivalently, it shows that any mechanism whose noise level satisfies σ < 1/ 2 ln must retain at least this amount of separation and therefore cannot make its trade-off curve arbitrarily close to the random-guessing regime. Moreover, although the bound on σ decreases as grows, the decay is extremely slow: even for very large , the required noise level remains far from zero. For example, assuming batch size of 256, the standard ImageNet-1k dataset [17] (1.3M images, 5 103) requires σ 0.24. Scaling to foundation models on LAION-5B [50] (5.8B image-text pairs, 2.3 107), the limit only drops to σ 0.17. Thus, the condition on σ remains substantial across practically relevant values of , revealing true bottleneck in this setting and simply scaling the dataset is insufficient to bypass this fundamental noise bottleneck. 6.3 Extension to Poisson Subsampling While our derivation focuses on the shuffled regime which reflects practical implementations, we now show that the same phenomenon extends to Poisson subsampling. simple mixture argument reveals that Poisson subsampling can be upper bounded by mechanism that, with some probability, leaks no information at all, and otherwise behaves like the shuffled mechanism analyzed above. This immediately transfers the shuffled lower bound on separation to the Poisson setting up to constant factor. We first state the resulting bound for Poisson-sampled DP-SGD, deferring the technical argument to supporting lemma below. 16 Theorem 6.3 (Lower Bound on Separation for Poisson-sampled DP-SGD). Consider DP-SGD trained for one epoch under Poisson subsampling with 1 rounds and noise multiplier σ > 0. Let fpois denote the true -DP trade-off curve under Poisson subsampling, and let sep(fpois) denote its separation. Then the following dichotomy holds: κpois (cid:16) 1 1 (cid:17) 1 8 (cid:18) 1 (cid:19) 1 4π ln OR σ 1 2 ln Lemma 6.4 (Transferring shuffling bounds to Poisson subsampling under zero-out adjacency). Consider DP-SGD trained for one epoch under Poisson subsampling with 1 rounds and per-round sampling probability (0, 1), and assume zero-out adjacency (cf. Definition 3.8). Let fpois denote the true -DP trade-off curve under Poisson subsampling, and let fshuf denote the true -DP trade-off curve under one-epoch random shuffling. Assume that fshuf (α) for all α [0, 1]. shuf (α) Let = (1 q)M be the probability that the differing record is never sampled during the epoch. Then, for all α [0, 1], (31) In particular, if the expected batch size is = qN and number of rounds is = N/b and = b/N = 1/M , then for typical batch sizes we have fpois(α) p(1 α) + (1 p) shuf (α). = (1 q)M = (cid:16) 1 1 (cid:17)M e1. Proof of Lemma 6.4 can be found in Appendix D. We now combine this mixture bound with the separation lower bound established for shuffled DP-SGD to complete the proof of Theorem 6.3. Proof of Theorem 6.3. Under Poisson subsampling, each sample is independently included in each round with probability = 1/M over rounds (one epoch), so that = (1 q)M . Let fshuf denote the true -DP trade-off curve under one-epoch random shuffling. By Lemma 6.4 (applied with shuf := fshuf ), for all α [0, 1], fpois(α) p(1 α) + (1 p) fshuf (α). Geometrically, this shows that fpois lies below convex combination of the random-guessing line (which has zero separation) and the shuffled trade-off curve. Hence, Finally, using e1 from Lemma 6.4 and using Theorem 6.1 proves the claim. κpois (1 p) κshuf . Shuffled and Poisson DP-SGD obey the same worst-case limitation up to constant factor. In particular, Poisson-sampled DP-SGD cannot make its trade-off curve arbitrarily close to random guessing under the standard worst-case DP adversary and the one-epoch regime, unless σ exceeds the same threshold (σ 1/ 2 ln ) as in the shuffled case. Together, Theorems 6.1 and 6.3 establish unified worst-case limitation: under the standard worst-case DP adversary, neither shuffling nor Poisson subsampling can simultaneously achieve noise multiplier below the σ threshold and separation below the corresponding lower bounds."
        },
        {
            "title": "7 Empirical Analysis",
            "content": "Sampling, normalization, and microbatching. For experiments, we consider both Poisson subsampling and random shuffling. For Poisson subsampling, each example is included independently with probability at each step, resulting in random number of participating examples. To have the DP-SGD update consistent with common practice, we normalize gradients and calibrate Gaussian noise using the expected batch size = qN , treating non-sampled examples as contributing zero gradient. This yields one DP-SGD update per Poisson-sampled step with fixed noise scale, while preserving the independence structure assumed in our theoretical analysis. For random shuffling, we use fixed-size batches throughout by shuffling the dataset at each epoch and discarding any incomplete remainder batch which coincides with the standard shuffled DP-SGD mechanism (Definition 3.2). All experiments are implemented using the JAX Privacy DP-SGD framework [8, 11]. We use microbatching with fixed size 32 only to reduce memory usage. Gradients are clipped per example and Gaussian noise is added once per logical batch, so microbatching does not affect the optimization or privacy mechanism. 17 Datasets and Model Architectures. We evaluate our bounds on image (CIFAR-10/100 [38], SVHN [47]) and text (AG News [63]) benchmarks. We employ standard architectures including ResNet [34], Vision Transformers (ViT) [22], and encoder-only Transformer architecture [57]. To analyze how model capacity interacts with privacy noise, we evaluate variants ranging from Tiny to Base by increasing the embedding dimension (the width of the internal vector representations) and depth. Detailed architectural specifications for all models are provided in Appendix B. Experimental protocol and hyperparameter selection. For each epoch budget {1, 10, 25}, we first fix model architecture that is compatible with DP training (e.g., replacing non-DP-friendly components such as LayerNorm where necessary). Using this fixed model, we perform hyperparameter search under clean training, i.e. σ = 0 to identify the best-performing optimizer configuration for each epoch budget. These hyperparameters are then held fixed across all subsequent experiments. The clipping constant is selected by maximizing utility under fixed noise multiplier set to our theoretical lower bound (i.e., σ = 1/ 2 ln ). We then evaluate two training regimes: (i) σ = 0 without clipping (clean training) and (ii) full DP-SGD with clipping and Gaussian noise with σ set to our theoretical lower bound. The purpose of these experiments is not to achieve peak accuracy, but to demonstrate that the noise levels implied by our lower bound are already severe in practice. Additional implementation and tuning details are provided in the Appendix B. Multi-view interpretation. Although our bounds are derived for single epoch, this setting naturally models federated learning scenarios in which each client contributes one local epoch and the server observes many such client updates across rounds. Accordingly, we repeat the one-epoch mechanism over independent views, corresponding to observing independent client updates. Our aim is not to optimize final accuracy, but to empirically illustrate the severity of the lower bound as the number of observed views increases. Results and interpretation. Tables 12 report test accuracy across datasets, architectures, batch sizes, and epoch budgets under shuffling and Poisson subsampling. Across all settings, introducing Gaussian noise at our lower bound induces substantial and persistent utility gap relative to both clean training and clipping-only baselines. This degradation is already pronounced at small epoch budgets and does not vanish with additional epochs, indicating that the noise floor imposed by our lower bound fundamentally constrains optimization rather than merely slowing convergence. While shuffling and Poisson subsampling exhibit comparable behavior in the clean and clipping-only regimes, their DP counterparts consistently suffer similar accuracy losses. Overall, these results empirically corroborate the fundamental limitations predicted by our analysis. Extended experimental results on additional model scales (Tiny, Small, and Base Transformers; ViT-Tiny/Small/Base; ResNet-34, WideResNet28x10) and datasets (AG News, CIFAR-10, and CIFAR-100, SVHN) can be found in Appendix C."
        },
        {
            "title": "8 Discussion and Future Directions",
            "content": "Our results expose structural tension in DP-SGD under the standard worst-case adversarial model. This section discusses the implications of this limitation, how it relates to existing privacy frameworks, and which directions appear promising for overcoming or reinterpreting these constraints. Is the adversary too strong? The lower bounds we derive are with respect to the standard worstcase DP adversary, who is allowed arbitrary side information and observes all noisy updates. From this perspective, our results may be interpreted less as an indictment of DP-SGD itself and more as limitation of the adversarial model. This raises the question of whether different privacy notions such as instance-based PAC-privacy [59] can meaningfully relax the trade-off without abandoning rigorous protection. An appealing direction is to understand how separation behaves when privacy is required only with high probability over the data distribution, rather than uniformly over all neighboring datasets. Algorithmic alternatives to noise scaling. key takeaway from both our theory and experiments is that increasing the noise multiplier alone is blunt instrument. Improving privacyutility trade-offs may require algorithmic changes rather than purely stronger noise calibration. Promising directions include modifying how gradients are clipped and aggregated, reducing the effective dimensionality 18 Table 1: CIFAR-10 / ResNet-18. Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. shuffling poisson BS Epoch 128 256 512 2048 4096 1 10 25 1 10 25 1 10 25 1 10 1 10 25 1 10 25 σ = 0 DP (σ) σ = 0 DP (σ) 48.0 80.1 82.2 44.9 68.6 78.7 39.9 70.5 79.5 29.5 60.4 77.9 27.8 53.7 71.4 15.6 45.9 54. 38.0 (0.29) 40.4 (0.29) 43.4 (0.29) 40.8 (0.31) 50.0 (0.31) 41.7 (0.31) 35.3 (0.33) 59.8 (0.33) 61.9 (0.33) 28.6 (0.36) 56.7 (0.36) 67.0 (0.36) 23.2 (0.40) 47.6 (0.40) 61.9 (0.40) 14.4 (0.45) 38.3 (0.45) 48.3 (0.45) 47.9 79.4 81.8 47.6 77.8 79.2 42.6 71.9 79.0 32.6 61.2 72.8 29.0 55.3 69.2 15.1 46.9 57. 38.9 (0.29) 40.5 (0.29) 45.7 (0.29) 39.7 (0.31) 46.4 (0.31) 46.4 (0.31) 36.2 (0.33) 60.3 (0.33) 62.7 (0.33) 28.7 (0.36) 57.5 (0.36) 65.7 (0.36) 23.5 (0.40) 46.9 (0.40) 61.9 (0.40) 14.5 (0.45) 40.9 (0.45) 50.0 (0.45) Table 2: AG News / Transformer-Base (136M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = DP (σ) σ = 0 DP (σ) 128 256 1 10 25 1 10 25 1 10 25 86.8 90.7 91.4 86.8 91.0 90.8 84.8 89.1 90. 74.7 (0.27) 53.9 (0.27) 82.6 (0.27) 76.1 (0.29) 27.1 (0.29) 76.3 (0.29) 76.6 (0.30) 84.1 (0.30) 82.7 (0.30) 87.6 89.9 91.1 85.9 90.4 89.5 82.9 89.6 90. 74.1 (0.27) 56.8 (0.27) 82.7 (0.27) 75.9 (0.29) 25.9 (0.29) 73.4 (0.29) 75.2 (0.30) 83.6 (0.30) 82.3 (0.30) or sparsity of updates before noise injection, and reconsidering training schedules such as stopping early or running only partial epochs. An open question is whether such interventions can qualitatively weaken the adversary and yielding different privacyutility behavior rather than merely shifting constants within the same worst-case regime. Multi-epoch behavior. Although our main lower bound is derived for single epoch, multi-epoch training is the norm in practice. Existing asymptotic analyses within the µ-GDP framework [21] suggest that privacy loss under Poisson subsampling converges to Gaussian limit under composition. However, such results do not provide explicit non-asymptotic rates, nor do they directly capture the separation metric that governs worst-case distinguishability (see Appendix for detailed analysis). Understanding how separation evolves under repeated composition in the multi-epoch setting remains an open problem, and extending our separation bounds to this regime is an important direction for future work. Implications for practice and the broader picture. Empirically, we observe that the noise levels implied by our lower bounds already induce substantial accuracy degradation at realistic batch sizes 19 and model scales. Importantly, this does not imply that private learning is infeasible in practice; rather, it clarifies the concrete costs imposed by insisting on strong worst-case privacy guarantees within standard SGD-based training pipelines. More broadly, our results highlight that the combination of worst-case adversarial definitions and conventional optimization dynamics imposes real and sometimes underappreciated constraints on achievable privacyutility trade-offs. central open question is whether future progress will primarily come from new algorithmic designs, weaker but still meaningful privacy notions, or principled combination of both. We view this work as step toward understanding the boundary of what DP-SGD can and cannot provide under worst-case semantics, and toward motivating alternative frameworks."
        },
        {
            "title": "9 Conclusion",
            "content": "We analyzed DP-SGD under realistic shuffling and Poisson subsampling through the -DP hypothesistesting framework and established fundamental limitation under the standard worst-case adversarial model: the noise multiplier and worst-case distinguishability cannot be simultaneously driven below explicit lower bounds. This geometric limitation applies to both sampling schemes with empirical results confirming that the implied noise levels already cause substantial utility degradation at practical scales. Importantly, these findings should not be read as negative statement about private learning per se; rather, they describe the boundary of what DP-SGD can achieve under strong worst-case definitions, and motivate the exploration of alternative algorithms, adaptive training mechanisms, or weaker but still meaningful privacy notions when this boundary proves too restrictive."
        },
        {
            "title": "Acknowledgments",
            "content": "The contribution of Marten van Dijk and Murat Bilgehan Ertan to this publication is part of the project CiCS of the research program Gravitation which is (partly) financed by the Dutch Research Council (NWO) under the grant 024.006.037. We acknowledge the use of the DAS-6 High-Performance Computing cluster at Vrije Universiteit Amsterdam for GPU-based experiments [6]."
        },
        {
            "title": "References",
            "content": "[1] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow. org/. Software available from tensorflow.org. [2] Martın Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, and Shai Halevi, editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, pages 308318, Vienna, Austria, 2016. ACM. doi: 10.1145/2976749.2978318. URL https://doi.org/10.1145/2976749.2978318. [3] John M. Abowd. The u.s. census bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 18, page 2867, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3226070. URL https://doi.org/10.1145/ 3219819.3226070. [4] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private bert, 2021. URL https://arxiv.org/abs/2108.01624. [5] Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information 20 Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1545315462, Vancouver, BC, Canada, 2019. NeurIPS. URL https://proceedings.neurips.cc/paper/2019/ hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html. [6] Henri E. Bal, Dick H. J. Epema, Cees de Laat, Rob van Nieuwpoort, John W. Romein, Frank J. Seinstra, Cees Snoek, and Harry A. G. Wijshoff. medium-scale distributed system for computer science research: Infrastructure for the long term. Computer, 49(5):5463, 2016. doi: 10.1109/MC.2016.127. URL https://doi.org/10.1109/MC.2016.127. [7] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pages 62806290, Montreal, Canada, 2018. NeurIPS. URL https://proceedings.neurips.cc/paper/2018/hash/ 3b5020bb891119b9f5130f1fea9bd773-Abstract.html. [8] Borja Balle, Leonard Berrada, Zachary Charles, Christopher Choquette-Choo, Soham De, Vadym Doroshenko, Dj Dvijotham, Andrew Galen, Arun Ganesh, Sahra Ghalebikesabi, Jamie Hayes, Peter Kairouz, Ryan McKenna, Brendan McMahan, Aneesh Pappu, Natalia Ponomareva, Mikhail Pravilov, Keith Rush, Samuel Smith, and Robert Stanforth. JAXPrivacy: Algorithms for privacy-preserving machine learning in jax, 2025. URL http: //github.com/google-deepmind/jax_privacy. [9] Jeremiah Birrell, Reza Ebrahimi, Rouzbeh Behnia, and Jason Pacheco. Differentially private stochastic gradient descent with fixed-size minibatches: Tighter RDP guarantees with or without replacement. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, pages 1108711131, Vancouver, Canada, 2024. NeurIPS. URL http://papers.nips.cc/paper_files/paper/2024/ hash/14fef58f09f2ebe69306e0a322e3be2b-Abstract-Conference.html. [10] Franziska Boenisch, Philip Sperl, and Konstantin Bottinger. Gradient masking and the underestimated robustness threats of differential privacy in deep learning. CoRR, abs/2105.07985:113, 2021. URL https://arxiv.org/abs/2105.07985. [11] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. [12] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J. Su. Deep learning with gaussian differential privacy. CoRR, abs/1911.11607, 2019. URL http://arxiv.org/abs/1911.11607. [13] Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: gradient-sanitized approach for learning differentially private generators, 2021. URL https://arxiv.org/abs/ 2006.08265. [14] Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, and Chiyuan Zhang. How private are DP-SGD implementations? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 89048918, Vienna, Austria, 2127 Jul 2024. PMLR. URL https://proceedings.mlr.press/v235/chua24a.html. [15] Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, In Amir and Chiyuan Zhang. Scalable DP-SGD: shuffling vs. poisson subsampling. Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, 21 Vancouver, BC, Canada, December 10 - 15, 2024, pages 7002670047, Vancouver, BC, Canada, 2024. NeurIPS. URL http://papers.nips.cc/paper_files/paper/2024/ hash/81c252b45b3bd7d9bf080eb27794b762-Abstract-Conference.html. [16] Soham De, Leonard Berrada, Jamie Hayes, Samuel L. Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale, 2022. URL https: //arxiv.org/abs/2204.13650. [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248255, Miami, FLorida, USA, 2009. IEEE Computer Society. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10.1109/CVPR.2009.5206848. [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, MN, USA, 2019. Association for Computational Linguistics. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. [19] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately, 2017. URL https://arxiv.org/abs/1712.01524. [20] Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis. Differentially private diffusion models, 2023. URL https://arxiv.org/abs/2210.09929. [21] Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. CoRR, abs/1905.02383, 2019. URL http://arxiv.org/abs/1905.02383. [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, Austria, May 3-7, 2021, 2021. OpenReview.net. URL https://openreview.net/forum?id=YicbFdNTTy. [23] Cynthia Dwork. firm foundation for private data analysis. Commun. ACM, 54(1):8695, 2011. doi: 10.1145/1866739.1866758. URL https://doi.org/10.1145/1866739.1866758. [24] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211407, 2014. doi: 10.1561/0400000042. URL https: //doi.org/10.1561/0400000042. [25] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Serge Vaudenay, editor, Advances in Cryptology - EUROCRYPT 2006, 25th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28 - June 1, 2006, Proceedings, volume 4004 of Lecture Notes in Computer Science, pages 486503, St. Petersburg, Russia, 2006. Springer. doi: 10.1007/11761679 29. URL https://doi.org/10. 1007/11761679_29. [26] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In Shai Halevi and Tal Rabin, editors, Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings, volume 3876 of Lecture Notes in Computer Science, pages 265284, New York, NY, USA, 2006. Springer. doi: 10.1007/11681878 14. URL https://doi.org/10.1007/ 11681878_14. [27] Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, CCS 14, page 10541067, New York, NY, USA, 2014. 22 Association for Computing Machinery. ISBN 9781450329576. doi: 10.1145/2660267.2660348. URL https://doi.org/10.1145/2660267.2660348. [28] Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via anonymity. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 24682479, San Diego, California, USA, 2019. SIAM. doi: 10.1137/1.9781611975482. 151. URL https://doi.org/10.1137/1.9781611975482.151. [29] Vitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: simple and nearly optimal analysis of privacy amplification by shuffling. In 62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 954964, Denver, CO, USA, 2021. IEEE. doi: 10.1109/FOCS52979.2021.00096. URL https://doi.org/10.1109/FOCS52979.2021.00096. [30] Vitaly Feldman, Audra McMillan, and Kunal Talwar. Stronger privacy amplification by shuffling for renyi and approximate differential privacy. In Nikhil Bansal and Viswanath Nagarajan, editors, Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023, pages 49664981, Florence, Italy, 2023. SIAM. doi: 10.1137/1.9781611977554.CH181. URL https://doi.org/10.1137/1.9781611977554. ch181. [31] Mert Gurbuzbalaban, Asuman E. Ozdaglar, and Pablo A. Parrilo. Why random reshuffling beats stochastic gradient descent. Math. Program., 186(1):4984, 2021. doi: 10.1007/ S10107-019-01440-W. URL https://doi.org/10.1007/s10107-019-01440-w. [32] Jeff Z. HaoChen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 26242633, Long Beach, California, USA, 2019. PMLR. URL http://proceedings.mlr.press/v97/haochen19a.html. [33] Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. Exploring the limits of differentially private deep learning with group-wise clipping, 2022. URL https://arxiv.org/abs/2212.01539. [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770778, Las Vegas, NV, USA, 2016. IEEE Computer Society. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR. 2016.90. [35] Timour Igamberdiev, Doan Nam Long Vu, Felix Kunnecke, Zhuo Yu, Jannik Holmer, and Ivan Habernal. Dp-nmt: Scalable differentially-private machine translation, 2024. URL https://arxiv.org/abs/2311.14465. [36] Peter Kairouz, Brendan Mcmahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffling. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 52135225, Virtual, 1824 Jul 2021. PMLR. URL https://proceedings.mlr.press/v139/kairouz21b.html. [37] Antti Koskela, Joonas Jalko, and Antti Honkela. Computing tight differential privacy guarantees using FFT. In Silvia Chiappa and Roberto Calandra, editors, The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pages 25602569, Palermo, Sicily, Italy, 2020. PMLR. URL http://proceedings.mlr.press/ v108/koskela20b.html. [38] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. URL https://www.cs.utoronto.ca/kriz/learning-features-2009-TR.pdf. 23 [39] Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta. Toward training at imagenet scale with differential privacy. CoRR, abs/2201.12328: 125, 2022. URL https://arxiv.org/abs/2201.12328. [40] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario ˇSaˇsko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21. [41] Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, and Hong Xing. An improved privacy and utility analysis of differentially private SGD with bounded domain and smooth losses. CoRR, abs/2502.17772:119, 2025. doi: 10.48550/ARXIV.2502.17772. URL https://doi.org/10. 48550/arXiv.2502.17772. [42] Ryan McKenna, Yangsibo Huang, Amer Sinha, Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Badih Ghazi, George Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, and Chiyuan Zhang. Scaling laws for differentially private language models, 2025. URL https://arxiv. org/abs/2501.18914. [43] Sebastian Meiser and Esfandiar Mohammadi. Tight on budget?: Tight bounds for r-fold approximate differential privacy. In David Lie, Mohammad Mannan, Michael Backes, and XiaoFeng Wang, editors, Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, CCS 2018, Toronto, ON, Canada, October 15-19, 2018, pages 247264, Toronto, ON, Canada, 2018. ACM. doi: 10.1145/3243734.3243765. URL https: //doi.org/10.1145/3243734.3243765. [44] Qi Meng, Wei Chen, Yue Wang, Zhi-Ming Ma, and Tie-Yan Liu. Convergence analysis of distributed stochastic gradient descent with shuffling. Neurocomputing, 337:4657, 2019. doi: 10.1016/J.NEUCOM.2019.01.037. URL https://doi.org/10.1016/j.neucom.2019.01. 037. [45] Ilya Mironov. Renyi differential privacy. In 30th IEEE Computer Security Foundations Symposium, CSF 2017, Santa Barbara, CA, USA, August 21-25, 2017, pages 263275, Barbara, CA, USA, 2017. IEEE Computer Society. doi: 10.1109/CSF.2017.11. URL https://doi. org/10.1109/CSF.2017.11. [46] Ilya Mironov, Kunal Talwar, and Li Zhang. Renyi differential privacy of the sampled gaussian mechanism. CoRR, abs/1908.10530, 2019. URL http://arxiv.org/abs/1908.10530. [47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford. edu/housenumbers/nips2011_housenumbers.pdf. [48] Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten van Dijk. unified convergence analysis for shuffling-type gradient methods. J. Mach. Learn. Res., 22: 207:1207:44, 2021. URL https://jmlr.org/papers/v22/20-1238.html. [49] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Guha Thakurta. How to dp-fy ML: practical guide to machine learning with differential privacy. J. Artif. Intell. Res., 77:1113 1201, 2023. doi: 10.1613/JAIR.1.14649. URL https://doi.org/10.1613/jair.1.14649. [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, New Orleans, LA, USA, 2022. NeurIPS. URL http://papers.nips.cc/paper_files/paper/2022/hash/ a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html. [51] Yinchen Shen, Zhiguo Wang, Ruoyu Sun, and Xiaojing Shen. Towards understanding the impact of model size on differential private classification. CoRR, abs/2111.13895:114, 2021. URL https://arxiv.org/abs/2111.13895. [52] Amer Sinha, Thomas Mesnard, Ryan McKenna, Daogao Liu, Christopher A. ChoquetteChoo, Yangsibo Huang, Da Yu, George Kaissis, Zachary Charles, Ruibo Liu, Lynn Chua, Pritish Kamath, Pasin Manurangsi, Steve He, Chiyuan Zhang, Badih Ghazi, Borja De Balle Pigem, Prem Eruvbetine, Tris Warkentin, Armand Joulin, and Ravi Kumar. Vaultgemma: differentially private gemma model, 2025. URL https://arxiv.org/abs/2510.15001. [53] David M. Sommer, Sebastian Meiser, and Esfandiar Mohammadi. Privacy loss classes: The central limit theorem in differential privacy. Proc. Priv. Enhancing Technol., 2019(2):245269, 2019. doi: 10.2478/POPETS-2019-0029. URL https://doi.org/10.2478/popets-2019-0029. [54] Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, and Prateek Mittal. Private fine-tuning of large language models with zeroth-order optimization, 2025. URL https: //arxiv.org/abs/2401.04343. [55] TensorFlow. TensorFlow Datasets, collection of ready-to-use datasets. https://www. tensorflow.org/datasets. [56] Nurislam Tursynbek, Aleksandr Petiushko, and Ivan V. Oseledets. Robustness threats of differential privacy. CoRR, abs/2012.07828:116, 2020. URL https://arxiv.org/abs/ 2012.07828. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, Long Beach, CA, USA, 2017. NeurIPS. URL https://proceedings.neurips. cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. [58] Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, and Weijie J. Su. Unified enhancement of privacy bounds for mixture mechanisms via f-differential privacy. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, pages 5505155063, New Orleans, LA, USA, 2023. NeurIPS. URL http://papers.nips.cc/paper_files/paper/2023/hash/ acb3e20075b0a2dfa3565f06681578e5-Abstract-Conference.html. [59] Hanshen Xiao and Srinivas Devadas. PAC privacy: Automatic privacy measurement and control of data processing. In Helena Handschuh and Anna Lysyanskaya, editors, Advances in Cryptology - CRYPTO 2023 - 43rd Annual International Cryptology Conference, CRYPTO 2023, Santa Barbara, CA, USA, August 20-24, 2023, Proceedings, Part II, volume 14082 of Lecture Notes in Computer Science, pages 611644, Santa Barbara, CA, USA, 2023. Springer. doi: 10. 1007/978-3-031-38545-2 20. URL https://doi.org/10.1007/978-3-031-38545-2_ 20. [60] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in pytorch. CoRR, abs/2109.12298, 2021. URL https://arxiv.org/abs/2109.12298. 25 [61] Xiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. Synthetic text generation with differential privacy: simple and practical recipe, 2023. URL https://arxiv.org/abs/2210.14348. [62] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1236012371, Vancouver, BC, Canada, 2019. NeurIPS. URL https://proceedings.neurips.cc/paper/2019/hash/ 1e8a19426224ca89e83cef47f1e7f53b-Abstract.html. [63] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649657, Montreal, Quebec, Canada, 2015. NeurIPS. URL https://proceedings.neurips.cc/paper/2015/hash/ 250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html. [64] Chaoyi Zhu, Jiayi Tang, Juan F. Perez, Marten van Dijk, and Lydia Y. Chen. DP-TLDM: differentially private tabular latent diffusion model. In Mila Dalla Preda, Sebastian Schrittwieser, Vincent Naessens, and Bjorn De Sutter, editors, Availability, Reliability and Security - 20th International Conference, ARES 2025, Ghent, Belgium, August 11-14, 2025, Proceedings, Part I, volume 15992 of Lecture Notes in Computer Science, pages 337357, Ghent, Belgium, 2025. Springer. doi: 10.1007/978-3-032-00624-0 17. URL https://doi.org/10.1007/ 978-3-032-00624-0_17. [65] Yuqing Zhu and Yu-Xiang Wang. Poission subsampled renyi differential privacy. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7634 7642, , 0915 Jun 2019. PMLR. URL https://proceedings.mlr.press/v97/zhu19c. html. [66] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event, volume 151 of Proceedings of Machine Learning Research, pages 47824817, Virtual Event, 2022. PMLR. URL https://proceedings.mlr.press/v151/ zhu22c.html."
        },
        {
            "title": "A Artifact",
            "content": "Even though our core contribution is theoretical, the experiments provide supporting evidence. To facilitate independent verification, we provide complete artifact to reproduce all empirical results reported in Section 7 and Appendix and F4. Our implementation is based on open-source frameworks and public datasets, and all experiments are run with Python 3.11 (the required packages is provided in the artifact). Link to the Artifact. A.1 Main Empirical Results (Section 7 and Appendix E) All datasets used in our experiments are public and are downloaded automatically when running the code. Specifically, CIFAR-10, CIFAR-100, and SVHN are loaded via TensorFlow Datasets (TFDS) [55]. AG News is loaded via torchtext if available, and otherwise via HuggingFace datasets (ag news) [40]. None of these require authentication. Detailed documentation can also be found in the README.md file in the artifact. One can reproduce our results in two ways: Reproduce from existing runs (recommended). For each model/dataset pair and batch size, we provide corresponding script in folder reproduce scripts/ that calls reproduce.py on the appropriate run directories under results/. This script reads the saved experimental specification from each run directory, including: (i) the per-epoch hyperparameters used in that run (recorded per row in utility sigma th.csv), (ii) the selected clipping constant , (iii) the noise level σ (derived from our bound), and (iv) the deterministic seed plan in seeds.json. It then re-runs training and evaluation and writes under the run directory: reproduce/. Run from scratch (optional). To generate new run directories, reviewers can execute run dp.py, which (a) selects using 1-epoch clip-only procedure, and (b) evaluates two training conditions (clean training and DP-SGD) for each epoch budget in {1, 10, 25}. Each epoch is an independent run (trained from the same initialization scheme but with its own derived seed, and hyperparameters), and these runs are executed sequentially. Hardware and determinism. Depending on dataset size, model architecture, and batch size, GPU memory requirements vary (approximately 1624GB for some configurations and up to 80GB for others). While we aim for determinism via fixed seeds, minor numerical differences across GPU/driver stacks and mixed precision behavior may lead to negligible deviations in reproduced metrics. A.2 Figures in Appendix To reproduce the plots in Appendix F, Python 3 with the matplotlib, numpy, and scipy libraries is required. The figures can be generated by running the script appendix f.py provided in the artifact above."
        },
        {
            "title": "B Implementation Details",
            "content": "This appendix provides detailed description of our experimental pipeline, including the DP-SGD implementation, sampling mechanisms, microbatching strategy, and the neural network architectures used in our experiments. All implementations are designed to closely reflect practical DP-SGD deployments while preserving alignment with the theoretical assumptions used in the main body of the paper. One can find our artifact at Artifact Link. B.1 Experimental Pipeline and DP-SGD Implementation All experiments are implemented in JAX using the official DP-SGD primitives from the jax-privacy library.5 We rely on the librarys reference implementation of DP-SGD without modifying its core 4https://github.com/bilgehanertan/dpsgd-fundamental-limitations 5For DP-SGD computing, we use the jax-privacy implementation at version 1.0.0. For microbatching, we use the latest branch. components. We evaluate our bounds on standard image and text benchmarks, including CIFAR10/100 [38] (50k training examples), SVHN [47] (73k training examples), and the AG News corpus [63] (120k training examples). B.2 Microbatching To efficiently compute per-example clipped gradients, we use the unchanged microbatching mechanism provided by jax-privacy. Microbatching splits logical batch into smaller microbatches, computes gradients for each microbatch, and accumulates clipped gradients incrementally. Crucially, this microbatching strategy is an implementation optimization only: it does not alter the effective privacy mechanism. Padding examples are explicitly handled by inserting dummy indices (marked by 1) whose gradients are zeroed out, ensuring that padding has no effect on either optimization or privacy. Throughout all experiments, microbatching is used solely to reduce memory usage and compilation overhead, and does not change the noise scale, clipping bound, or privacy accounting. B.3 Sampling Mechanisms We explicitly control the batch formation mechanism to instantiate the sampling regimes studied in the main paper, up to standard practical considerations required for efficient accelerator execution. Fixed-size shuffling. In the shuffled regime, at the beginning of each epoch we draw fresh uniform random permutation of the training examples and iterate through it in contiguous fixed-size batches of size b. To keep step shapes constant, we discard the final incomplete remainder batch and thus use only the first N/b examples per epoch, yielding exactly N/b updates per epoch. This corresponds to the standard shuffled DP-SGD mechanism with equal-sized, disjoint batches, applied to an effective dataset of size N/b b. Poisson subsampling. In the Poisson regime, at each step we include each example independently with probability q, yielding randomly sized sampled index set. For JIT compatibility, the resulting variable-length index sets are padded after sampling with dummy indices (marked by 1) to fixed compiled shape (a multiple of the microbatch size). Padded indices are mapped to safe index for data loading and are accompanied by an is padding example mask, ensuring that their contributions to the gradient are exactly zero. This padding procedure also handles the corner case of empty sampled batches by producing non-empty dummy batch of fixed shape. Normalization under Poisson sampling. In our DP-SGD implementation, we normalize gradients and calibrate Gaussian noise using the expected batch size = qN under Poisson subsampling, treating non-sampled examples as contributing zero gradient. For shuffling-based training, each update operates on exactly examples, and normalization uses the realized (non-padding) batch size. B.4 Vision Transformer Architectures For image classification experiments, we use minimal Vision Transformer (ViT) adapted to 32 32 images (e.g., CIFAR-style datasets). Images are split into non-overlapping patches using convolutional patch embedding layer with kernel size and stride equal to the patch size. The embedded patches are flattened, prepended with learnable [CLS] token, and combined with learned positional embeddings. The resulting sequence is processed by stack of Transformer encoder blocks using pre-layer normalization design. Each block consists of multi-head selfattention followed by feed-forward MLP with GELU activations and residual connections. To simplify experiments, all dropout rates are set to zero. The final representation of the [CLS] token is passed through linear classification head. We evaluate multiple ViT configurations (Tiny, Small, Base) that vary in depth, width, and number of attention heads, while keeping the architectural structure fixed. 28 B.4.1 Vision Transformer Variants We evaluate multiple Vision Transformer (ViT) variants that share the same architectural template but differ in width, depth, and attention capacity. All variants operate on 32 32 images and use fixed patch size of 4 4, resulting in 8 8 = 64 image patches per input. Common Architecture. All ViT variants follow the same high-level structure: Patch embedding via convolutional layer with kernel size and stride equal to the patch size. learnable [CLS] token prepended to the patch sequence. Learned absolute positional embeddings. stack of Transformer encoder blocks with pre-layer normalization. Classification via the final [CLS] representation. ViT-Tiny. The vit tiny cifar variant is designed to be lightweight and DP-friendly. It uses narrow embedding dimension and few attention heads, reducing both parameter count and gradient variance. Embedding dimension: dmodel = 192 Number of layers: 12 Attention heads: 3 MLP expansion ratio: 4 ViT-Small. The vit small cifar variant increases representational capacity while retaining the same depth. Embedding dimension: dmodel = 384 Number of layers: 12 Attention heads: 6 MLP expansion ratio: 4 ViT-Base. The vit base cifar variant mirrors standard ViT-Base scaling laws adapted to small images. Embedding dimension: dmodel = 768 Number of layers: 12 Attention heads: 12 MLP expansion ratio: 4 ViT Architecture Schematic. For clarity, the shared architecture of all ViT variants is summarized below: Input Image (32x32xC) Patch Conv (4x4, stride 4) 64 Patch Embeddings (d_model) [CLS] Token + Positional Embedding Transformer Encoder layers LayerNorm CLS Token Linear Classifier 29 B.5 Text Transformer Architectures For text classification tasks, we use encoder-only Transformer models with RMS normalization and SwiGLU feed-forward layers. Input tokens are embedded using learned token and positional embeddings and processed by stack of Transformer blocks. Each block employs RMSNorm [62] instead of LayerNorm to match common practice in modern Transformer implementations. The feed-forward sublayer uses SwiGLU activation, which empirically improves optimization stability under noisy gradients. As with the vision models, dropout is disabled throughout. Attention masks are used to prevent padded tokens from contributing to attention scores. Classification is performed by pooling the representation of the first token and applying linear output head. We evaluate multiple model scales by varying depth, hidden dimension, and sequence length. B.5.1 Tokenization and Vocabulary Construction To strictly isolate the impact of privacy noise on the optimization process, we train all text models from scratch with random initialization rather than fine-tuning pretrained language models (e.g., BERT [18]). Hence, we employ standalone tokenization and vocabulary construction pipeline instead of relying on pretrained tokenizers. We use simple frequency-based tokenizer with fixed vocabulary size of = 30,000. Text is first lowercased and tokenized via whitespace splitting. The vocabulary is then constructed by selecting the most frequent tokens in the training corpus. All remaining tokens are mapped to generic [UNK] symbol, and sequences are padded with dedicated [PAD] token. special [CLS] token is prepended to each sequence for classification. B.5.2 Text Transformer Variants For text classification, we evaluate family of Transformer encoder models that differ in sequence length, width, and depth. All models share the same architectural backbone and are trained from scratch. Common Architecture. All text models consist of: Token and positional embeddings. stack of Transformer blocks with pre-normalization using RMSNorm. Multi-head self-attention followed by SwiGLU feed-forward network. Classification via the first (CLS) token. Tiny-128. The tiny 128 model is compact Transformer. Maximum sequence length: 128 Embedding dimension: 256 Number of layers: 4 Attention heads: 4 Feed-forward dimension: 1024 Small-128. The small 128 variant increases depth and width while keeping the same sequence length. Maximum sequence length: 128 Embedding dimension: 512 Number of layers: 6 Attention heads: 8 Feed-forward dimension: 2048 30 BaseEmbedding dimension: 768 Number of layers: 12 Attention heads: 12 Feed-forward dimension: 3072 Maximum sequence length: 256 Increasing sequence length substantially raises computational cost and attention sensitivity. Text Transformer Architecture Schematic. The shared structure of all text Transformer variants is summarized below: Input Tokens (B T) Token + Positional Embedding [RMSNorm Self-Attention Residual] [RMSNorm SwiGLU FFN Residual] layers RMSNorm CLS Token Linear Classifier B.6 Convolutional Architectures For convolutional image classification experiments, we use ResNet-style and Wide ResNet architectures adapted to small-resolution images (e.g., CIFAR-10, CIFAR-100, SVHN). All convolutional models are trained from scratch and use normalization layers that do not rely on batch-dependent statistics, making them compatible with DP-SGD. Normalization. By default, we replace Batch Normalization with Group Normalization. Specifically, we use an adaptive GroupNorm variant that selects the largest valid number of groups not exceeding 32 based on the channel dimension. This choice avoids batch-dependent running statistics while remaining close in spirit to standard GroupNorm. BatchNorm is not used in DP experiments. CIFAR-style stem. All convolutional models use CIFAR-style stem consisting of single 3 3 convolution with stride 1 and no max-pooling, followed by normalization and ReLU. B.6.1 ResNet Architectures We evaluate ResNet-18 and ResNet-34 architectures adapted to CIFAR-style inputs. Both models follow the standard residual block structure with two 3 3 convolutions per block and identity skip connections, with projection shortcuts used when the spatial resolution or channel dimension changes. ResNet-18. ResNet-18 consists of four stages with {2, 2, 2, 2} residual blocks and channel widths {64, 128, 256, 512}. Spatial downsampling is performed by strided convolutions at the beginning of each stage (except the first). Global average pooling is applied before linear classification head. ResNet-34. ResNet-34 increases depth using {3, 4, 6, 3} residual blocks while keeping the same stage-wise channel configuration. This model is used primarily on CIFAR-100 to evaluate the effect of increased depth under DP-SGD. Activation and regularization. All ResNet variants use ReLU activations. Dropout is disabled throughout to avoid introducing additional sources of randomness beyond DP noise. 31 B.6.2 Wide ResNet Architectures To study the effect of increased width, we evaluate Wide ResNet with depth 28 and widening factor 10 (WideResNet-2810). This architecture follows the standard Wide ResNet design, consisting of 6n + 4 layers grouped into three stages with increasing channel widths. Each residual block contains two 3 3 convolutions with normalization and ReLU activations, and projection shortcuts are used when dimensions mismatch. As with standard Wide ResNets, widening increases representational capacity without increasing depth. WideResNet-2810 is evaluated on SVHN, where wider architectures are known to perform well, allowing us to examine the interaction between model width and DP-SGD noise. Architectural schematic. The common structure of the convolutional models is summarized below: Input Image (32x32xC) 3x3 Conv + Norm + ReLU Residual Blocks stages Global Average Pooling Linear Classifier B.7 Reproducibility and Determinism All experiments use explicitly seeded random number generators for sampling, shuffling, and noise generation. Batch indices, padding behavior, and microbatch ordering are deterministic given the seed. This ensures reproducibility across runs and enables controlled comparisons between Poisson subsampling and shuffling-based regimes."
        },
        {
            "title": "C Extended Experimental Results",
            "content": "Tables 39 present the complete evaluation across all architecture variants (Transformer-Tiny/Small, ViT-Tiny/Small/Base, WideResNet-2810, and ResNet-34) and datasets (SVHN, AG News, CIFAR10, and CIFAR-100) omitted from the main text. Across all settings, we observe the same behavior as discussed in Section 7. This effect persists across model scales, batch sizes, and sampling mechanisms, and is visible even for large-capacity models such as WideResNet-2810 and ViT-Base. Moreover, random shuffling and Poisson subsampling consistently yield comparable utility under DP, indicating that the observed degradation is not an artifact of the sampling scheme but rather consequence of the underlying privacy constraints. These extended results reinforce our central claim that the privacyutility trade-off identified by our analysis manifests broadly across architectures and datasets. Proof of Lemma 6.4 Proof of Lemma 6.4. We upper bound the Poisson-subsampled trade-off curve by conditioning on the number of times the distinguishing record is sampled and reducing the resulting experiment to two-branch mixture. First, recall from Section 5.3 that for any (possibly randomized) test ϕ, α(ϕ) = E[ϕ d], β(ϕ) = 1 E[ϕ d], and the trade-off function is (a) = inf ϕ: α(ϕ)a β(ϕ). Step 1: Conditioning on and coarsening {K 1}. Let Binomial(M, q) denote the number of rounds in which the differentiating record is included during the epoch, and let := Pr[K = 0] = (1 q)M . 32 Table 3: AG News / Transformer-Tiny (3M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = 0 DP (σ) σ = 0 DP (σ) 128 256 512 1 10 25 1 10 25 1 10 25 1 10 25 87.5 90.9 91.2 87.0 91.2 90. 85.3 90.3 91.1 80.5 87.3 90.4 77.3 (0.27) 83.3 (0.27) 81.1 (0.27) 79.1 (0.29) 83.6 (0.29) 85.8 (0.29) 80.1 (0.30) 84.2 (0.30) 85.2 (0.30) 75.1 (0.32) 86.4 (0.32) 86.3 (0.32) 86.5 90.2 90.3 87.3 90.3 91.4 84.1 87.6 90.0 81.9 86.7 90.1 76.1 (0.27) 83.6 (0.27) 79.3 (0.27) 78.0 (0.29) 80.6 (0.29) 85.2 (0.29) 78.5 (0.30) 84.3 (0.30) 85.4 (0.30) 74.8 (0.32) 85.8 (0.32) 87.0 (0.32) Table 4: AG News / Transformer-Small (40M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = 0 DP (σ) σ = 0 DP (σ) 256 512 1024 1 10 25 1 10 25 1 10 1 10 25 87.9 91.3 90.8 87.5 90.9 90.0 86.2 90.3 90.9 82.1 89.5 87.5 76.5 (0.27) 71.4 (0.27) 79.0 (0.27) 77.3 (0.29) 81.8 (0.29) 66.4 (0.29) 77.8 (0.30) 83.7 (0.30) 83.0 (0.30) 75.1 (0.32) 86.7 (0.32) 87.8 (0.32) 87.4 90.4 91.0 86.6 90.0 90.3 84.6 90.4 90. 81.2 89.7 87.5 75.3 (0.27) 70.3 (0.27) 77.0 (0.27) 75.7 (0.29) 81.4 (0.29) 64.2 (0.29) 76.0 (0.30) 83.2 (0.30) 83.6 (0.30) 75.0 (0.32) 86.8 (0.32) 87.6 (0.32) Table 5: CIFAR-10 / ViT-Tiny (12M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = 0 DP (σ) σ = 0 DP (σ) 128 256 512 1 10 25 1 10 25 1 10 25 1 10 25 43.8 58.3 66.1 42.6 57.1 65. 40.2 56.5 59.6 35.7 54.4 60.9 38.1 (0.29) 40.8 (0.29) 48.5 (0.29) 40.7 (0.31) 34.8 (0.31) 37.1 (0.31) 39.3 (0.33) 45.4 (0.33) 52.0 (0.33) 35.8 (0.36) 50.5 (0.36) 55.0 (0.36) 38.8 57.8 67.6 38.5 58.4 68.1 39.5 57.8 62.7 35.3 55.6 61.8 38.9 (0.29) 41.8 (0.29) 48.8 (0.29) 39.3 (0.31) 31.9 (0.31) 29.4 (0.31) 38.4 (0.33) 46.6 (0.33) 52.8 (0.33) 35.3 (0.36) 50.5 (0.36) 56.5 (0.36) 33 Table 6: CIFAR-10 / ViT-Small (19M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = 0 DP (σ) σ = 0 DP (σ) 128 256 512 1024 1 10 25 1 10 1 10 25 1 10 25 44.7 61.0 64.0 41.9 57.7 63.4 39.4 56.1 63.4 37.1 55.6 61. 38.1 (0.29) 43.5 (0.29) 51.1 (0.29) 40.3 (0.31) 36.8 (0.31) 27.8 (0.31) 37.9 (0.33) 50.5 (0.33) 51.7 (0.33) 33.9 (0.36) 49.7 (0.36) 55.4 (0.36) 45.8 59.1 62.5 43.3 57.4 62. 42.3 56.9 60.9 36.8 55.1 60.7 39.0 (0.29) 36.0 (0.29) 43.3 (0.29) 40.0 (0.31) 38.7 (0.31) 36.6 (0.31) 38.2 (0.33) 50.3 (0.33) 47.0 (0.33) 33.5 (0.36) 49.3 (0.36) 54.4 (0.36) Table 7: CIFAR-10 / ViT-Base (85M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = DP (σ) σ = 0 DP (σ) 128 256 1024 1 10 25 1 10 25 1 10 25 1 10 25 42.4 60.3 61. 37.9 57.5 61.8 42.2 56.1 56.0 36.8 55.5 56.5 39.1 (0.29) 36.5 (0.29) 9.8 (0.29) 40.2 (0.31) 33.4 (0.31) 31.5 (0.31) 38.8 (0.33) 48.9 (0.33) 50.2 (0.33) 34.7 (0.36) 50.8 (0.36) 52.0 (0.36) 43.9 59.0 61.1 41.5 55.9 63.2 40.2 54.9 56.3 36.1 53.4 56.3 38.6 (0.29) 33.5 (0.29) 10.3 (0.29) 40.2 (0.31) 31.6 (0.31) 31.8 (0.31) 39.4 (0.33) 49.7 (0.33) 47.1 (0.33) 34.6 (0.36) 49.9 (0.36) 52.3 (0.36) Table 8: CIFAR-100 / ResNet-34. Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. shuffling poisson BS Epoch 128 256 512 1024 1 10 1 10 25 1 10 25 1 10 25 σ = 0 DP (σ) σ = DP (σ) 8.7 42.8 48.5 6.0 40.5 44.8 4.6 37.2 46.7 2.6 28.5 35.7 2.5 (0.29) 7.8 (0.29) 10.6 (0.29) 3.0 (0.31) 13.0 (0.31) 13.1 (0.31) 3.1 (0.33) 16.7 (0.33) 20.0 (0.33) 2.1 (0.36) 15.6 (0.36) 21.9 (0.36) 8.7 40.9 48.4 7.1 37.6 48.1 4.0 36.2 46. 3.1 28.3 32.9 2.6 (0.29) 7.6 (0.29) 9.9 (0.29) 3.2 (0.31) 13.7 (0.31) 14.4 (0.31) 3.0 (0.33) 16.1 (0.33) 18.3 (0.33) 1.7 (0.36) 15.3 (0.36) 24.2 (0.36) Table 9: SVHN / WideResNet-28x10 (36M parameters). Rows correspond to batch size (BS) and epoch budget; columns compare Random Shuffling vs. Poisson Subsampling under σ=0 and DP-SGD at the shown σ. BS Epoch shuffling poisson σ = DP (σ) σ = 0 DP (σ) 128 256 1 10 25 1 10 25 1 10 25 94.9 97.7 97.7 94.8 97.5 97.4 92.3 97.2 97. 85.9 (0.24) 89.0 (0.24) 89.8 (0.24) 87.4 (0.25) 91.4 (0.25) 91.7 (0.25) 88.7 (0.27) 92.0 (0.27) 93.2 (0.27) 94.6 97.7 97.6 95.0 97.4 97.3 91.9 97.4 96. 83.7 (0.24) 89.0 (0.24) 89.5 (0.24) 85.4 (0.25) 91.4 (0.25) 92.3 (0.25) 87.3 (0.27) 92.3 (0.27) 93.1 (0.27) We write the Poisson experiment as mixture over K. On the event {K 1}, the record is included at least once; to obtain an upper bound it is convenient to coarsen this event by replacing the conditional law given {K 1} by the conditional law given {K = 1}. Intuitively, this discards information because observing the record multiple times can only help the adversary. Thus, the true Poisson trade-off curve is upper bounded by the trade-off curve of the two-branch mixture experiment with = 0 with probability p, = 1 with probability 1 p, where on the second branch we use the conditional law of the original mechanism given = 1. We denote this mixture trade-off curve by fmix; then fpois(α) fmix(α) for all α [0, 1]. (32) Step 2: Analysis of the = 0 branch (never sampled). We analyze the hypothesis-testing problem conditional on = 0, i.e., the event that the distinguishing record is never included in any sampled batch over the rounds. Under zero-out adjacency, the neighboring datasets satisfy = (d d) {}, = (d d) {ξN }, where is the ghost record with identically zero gradient contribution and ξN is the real differing record. Conditioned on = 0, the distinguishing location (the th record position) is never sampled. Hence, on the event = 0, the gradients actually computed in all rounds depend only on the common subset d. As result, under both H0 and H1 d, the joint distribution of the observed outputs (x1, . . . , xM ) depends only on and is identical under the two hypotheses. Moreover, this distribution coincides with the null-hypothesis likelihood in the shuffling experiment, namely the Gaussian likelihood given in Eq. (3). Then, conditioned on = 0, the two hypotheses are statistically indistinguishable, and the adversary can do no better than random guessing. Then, for any (possibly randomized) rejection rule ϕ, the conditional errors satisfy α0(ϕ) = E[ϕ H0, = 0] = E[ϕ d] , β0(ϕ) = E[1 ϕ H1, = 0] = 1 E[ϕ d] . (33) (34) In particular, β0(ϕ) = 1 α0(ϕ). Finally, by zero-out adjacency, the presence or absence of the th record does not affect the shuffling experiment: shuffling on the full dataset induces the same distribution of (x1, . . . , xM ) as shuffling on d. Therefore, E[ϕ d] = E[ϕ d] = αshuf (ϕ), and we may identify the = 0-branch errors as α0(ϕ) = αshuf (ϕ), β0(ϕ) = 1 αshuf (ϕ). 35 Step 3: Analysis of the = 1 branch (sampled exactly once). Now condition on = 1. Then the distinguishing record is included in exactly one of the rounds. By symmetry of Poisson subsampling across rounds, the (random) round index in which it is included is uniform over {1, . . . , }. Consequently, conditional on = 1, the induced observation model is exactly the same as in the one-epoch shuffling analysis where the distinguishing record ξN is included in exactly one of the rounds: under H0 no round contains shifted contribution, while under H1 exactly one round contains shift of magnitude 1/σ, uniformly over the possible rounds. Equivalently, the conditional likelihoods of the observed outputs (x1, . . . , xM ) coincide with the shuffling likelihoods in Eqs. (3)(4). Therefore, for every (possibly randomized) rejection rule ϕ, the conditional type-I/type-II errors in the = 1 branch equal the corresponding shuffling errors: α1(ϕ) := Pr[ϕ = 1 H0, = 1] = αshuf (ϕ), β1(ϕ) := Pr[ϕ = 0 H1, = 1] = βshuf (ϕ). (35) (36) Step 4: Mixture errors expressed via shuffling quantities. Fix any (possibly randomized) rejection rule ϕ applied to the observed outputs (x1, . . . , xM ). From Steps 2 and 3 we have the exact identifications α0(ϕ) = αshuf (ϕ), β0(ϕ) = 1 αshuf (ϕ), and Therefore, the mixture type-I and type-II errors satisfy α1(ϕ) = αshuf (ϕ), β1(ϕ) = βshuf (ϕ). αmix(ϕ) = αshuf (ϕ) + (1 p) αshuf (ϕ) = αshuf (ϕ), and βmix(ϕ) = p(cid:0)1 αshuf (ϕ)(cid:1) + (1 p) βshuf (ϕ). By definition of the trade-off function, fmix(α) = inf ϕ: αmix(ϕ)α βmix(ϕ) = inf ϕ: αshuf (ϕ)α (cid:104) p(cid:0)1 αshuf (ϕ)(cid:1) + (1 p) βshuf (ϕ) (cid:105) . (37) Step 5: Reduction to the shuffling trade-off When minimizing βshuf subject to the constraint αshuf (ϕ) α, the infimum is attained (or approached) by rejection rules satisfying αshuf (ϕ) = α. Hence both components of the objective are minimized by the same boundary choice αshuf (ϕ) = α. Therefore, p(cid:0)1 αshuf (ϕ)(cid:1) + (1 p)βshuf (ϕ) fmix(α) = inf ϕ: αshuf (ϕ)=α (cid:104) p(cid:0)1 αshuf (ϕ)(cid:1) + (1 p)βshuf (ϕ) (cid:105) = p(1 α) + (1 p) inf ϕ: αshuf (ϕ)=α βshuf (ϕ). By the definition of the shuffling trade-off curve, and therefore fshuf (α) = inf ϕ: αshuf (ϕ)=α βshuf (ϕ), fmix(α) = p(1 α) + (1 p)fshuf (α). Step 6: Concluding for Poisson. Finally, using the assumed bound fshuf (α) shuf (α), we obtain Combining this with (32) proves (31). fmix(α) p(1 α) + (1 p) shuf (α). 36 From Separation to (ε, δ)-DP Guarantees Lemma E.1 (µ-GDP induces Gaussian separation). If mechanism is µ-GDP [21], i.e., its -DP trade-off curve equals the Gaussian trade-off curve Gµ, Gµ(α) = Φ(cid:0)Φ1(1 α) µ(cid:1), then its separation satisfies sep(Gµ) = 1 2α 2 = 2Φ(µ/2) 1 2 , (38) where α = Φ(µ/2) is the unique fixed point of Gµ. In particular, the separation is monotone increasing in µ. Proof of Lemma E.1. The fixed point condition Gµ(α) = α is Φ1(α) = Φ1(1 α) µ. Using symmetry Φ1(1 α) = Φ1(α), we obtain µ = 2 Φ1(α), hence α = Φ(µ/2). By Lemma 5.3, sep(Gµ) = 12α 2 , which yields sep(Gµ) = 1 2Φ(µ/2) = 2Φ(µ/2) 1 2 . Monotonicity follows since Φ is increasing. We now show how lower bound on the separation κshuf of the one-epoch shuffled DP-SGD mechanism yields corresponding lower bound on any (ε, δ)-DP guarantee assigned to the same mechanism. Our argument proceeds in three steps: (i) recall the canonical (ε, δ)-DP trade-off curve fε,δ; (ii) compute its separation sep(fε,δ) in closed form; and (iii) compare this separation to κshuf . Under the standard convention δ = 1/N , this comparison produces minimum admissible privacy parameter ε or equivalently, implies that the noise multiplier σ must be larger than our bound. The (ε, δ)-DP trade-off fε,δ. Following [21] (Proposition 2.4-2.5), any (ε, δ)-DP guarantee corresponds to the piecewise-linear trade-off function fε,δ(α) = max (cid:110) 0, 1 δ eεα, eε(1 δ α) (cid:111) , α [0, 1]. (39) This curve is the tightest possible -DP trade-off compatible with (ε, δ)-DP: if mechanism is (ε, δ)-DP, then its true trade-off curve under that mechanism must satisfy fshuf (α) fε,δ(α) α [0, 1], (40) since every hypothesis test available to the adversary must obey the constraints induced by the (ε, δ)-DP definition. Lemma E.2 (Separation of the (ε, δ)-DP trade-off). Let fε,δ be defined as in (39). Then κε,δ := sep(fε,δ) = eε 1 + 2δ (1 + eε) 2 . (41) Proof. Recall that the (ε, δ)-DP trade-off function fε,δ is symmetric and piecewise linear, with two linear branches meeting at unique fixed point. Explicitly, fε,δ(α) = max(cid:8)1 δ eεα, eε(1 δ α)(cid:9). We solve for fixed point ˆa satisfying fε,δ(ˆa) = ˆa. On the left linear branch, fε,δ(α) = 1 δ eεα. The fixed-point equation becomes α = 1 δ eεα, ˆa = 1 δ 1 + eε . 37 Using the fixed-point formula sep(f ) = (1 2ˆa)/ 2, we obtain 2(1 δ) 1 + eε = yielding (41). Notice that the same value of ˆa is obtained when taking the right linear branch of fε,δ, so the resulting separation is unchanged. We remark that this consistency is expected, as every symmetric convex trade-off function has exactly one fixed point. eε 1 + 2δ 1 + eε 1 2ˆa = 1 , Ordering of separations. Since the trade-off of the shuffled mechanism satisfies (40), its pointwise separation from the random-guessing line β = 1 α must satisfy Taking maxima over α and applying Lemma 5.3 gives sepfshuf (α) sepfε,δ (α) α. κshuf κε,δ. (42) On the other hand, Theorem 6.1 provides an explicit lower bound on κshuf : for one-epoch shuffling with σ < 1/ 2 ln , κshuf (cid:18) 1 1 1 4π ln (cid:19) . Combining this with (42), any (ε, δ)-DP pair consistent with the shuffled mechanism must satisfy κε,δ κshuf (cid:18) 1 8 1 1 4π ln (cid:19) . Lemma E.3 (Lower bound on ε from separation). Let κshuf = sep(fshuf ) denote the separation of the shuffled mechanisms -DP trade-off curve. If the same mechanism is also reported as (ε, δ)-DP, then ε must satisfy ε ln (cid:32) 1 + κshuf 1 κshuf 2 2δ 2 (cid:33) . (43) Proof. For any (ε, δ)-DP guarantee, the induced trade-off curve fε,δ has separation κε,δ as given in Lemma E.2: κε,δ = eε 1 + 2δ (1 + eε) 2 . Since fshuf (α) fε,δ(α) for all α, the separations must satisfy κε,δ κshuf . Thus eε 1 + 2δ (1 + eε) κshuf . Rearranging, Since κshuf < 1/ eε(1 κshuf 2) 1 + κshuf 2 2δ. 2 ensures the coefficient of eε is positive, division yields eε and taking logarithms proves (43). 1 + κshuf 1 κshuf 2 2δ 2 , Remark. The formula in (43) is well-defined because Lemma 5.3 ensures that κshuf < 1/ 2. 38 κshuf (M ) Table 10: Minimum ε implied by separation bounds (one epoch, δ = 1/N , = 108). For each number of rounds , we plug the lower bound κshuf from Theorem 6.1 and the induced Poisson bound κpois (1 1/e) κshuf into Lemma E.3 to obtain the smallest ε consistent with our worst-case -DP analysis. Larger true separations would only increase these ε values. εpois min 0.58 0.59 0.59 0.59 0.60 0.60 0.60 0.60 0.60 κpois(M ) 0.200 0.201 0.203 0.204 0.205 0.206 0.207 0.207 0. 1,000 3,000 10,000 30,000 100,000 300,000 1,000,000 3,000,000 5,000,000 0.316 0.318 0.321 0.322 0.324 0.325 0.327 0.328 0.328 εshuf min 0.96 0.97 0.98 0.98 0.99 1.00 1.00 1.00 1.01 Interpretation and numerical illustration. Lemma E.3 shows that the geometric separation κshuf imposes hard constraint on any (ε, δ)-DP description of the same mechanism. In typical DP-SGD practice one fixes δ = 1/N , so for given separation κ the smallest admissible ε is εmin(κ; ) = ln (cid:32) 1 + κ 2 2/N 2 1 κ (cid:33) . Theorem 6.1 provides lower bound on the shuffled separation κshuf . Since εmin(κ; ) is monotonically increasing in κ, plugging in this lower bound yields the smallest ε compatible with our one-epoch worst-case analysis; any larger true separation would force strictly larger ε. We note that while smaller ε is possible, it requires larger σ (violating the assumption of our lower bound), effectively hurting utility. For Poisson subsampling, Theorem 6.3 gives κpois (1 1/e) κshuf , so the same formula with κ = κpois yields the corresponding εmin for the Poisson regime. Table 10 reports these best-case values for representative choice δ = 1/N with = 108. Even at this optimistic one-epoch baseline, shuffled DP-SGD already forces ε 1, while the Poisson regime inherits comparable, though slightly weaker, constraint. In other words, the worst-case adversary possesses substantial advantage in distinguishing datasets, which makes the formal privacy guarantees insufficient for practical deployment. To bypass this limitation, future work must either move away from the standard worst-case adversarial framework or introduce fundamental algorithmic modifications."
        },
        {
            "title": "F Asymptotic Separation Bound for Poisson Subsampling",
            "content": "Before turning to our main results, we first examine what the asymptotic µ-GDP theory [21] predicts for the separation metric under Poisson subsampling. The purpose of this section is not to derive new guarantees, but rather to relate existing µ-GDP results to our geometric notion of separation, to analyze their implications in the proportional-growth regime, and to motivate the form of our subsequent bounds. Concretely, we translate the resulting µ-GDP characterization into an explicit prediction for separation. As simple illustration, we will later specialize this analysis to fixed choice of σ1 corresponding to our theoretical lower bound, in order to illustrate what the asymptotic theory suggests at that noise level. Lemma F.1 (Asymptotic µ-GDP separation for Poisson Subsampling). Fix σ1 > 0 and let (EM )M 1 be sequence of epoch budgets, where EM denotes the total number of epochs performed when the effective dataset size parameter is . Assume that this sequence satisfies the scaling (cid:114) EM c, (44) for some constant > 0 as . Let µ(M, EM , σ1) denote the asymptotic µ-GDP parameter given by Corollary 5.4 of [21] under Poisson subsampling with noise scale σ, and define the corresponding µ-GDPpredicted separation κµ-GDP(M, EM , σ1) := sep(cid:0)Gµ(M,EM ,σ1) (cid:1) . Then, along any sequence (EM )M 1 satisfying (44), the µ-GDPpredicted separation converges as κµ-GDP(M, EM , σ1) sep(Gµ) , 2 µ = (cid:113) eσ2 Φ(1.5σ1) + 3Φ(0.5σ1) 2. where Moreover, for each , κµ-GDP(M, EM , σ1) = 2Φ(cid:0)µ(M, EM , σ1)/2(cid:1) 1 2 . Finally, for any µ > 0, the separation of µ-GDP mechanism satisfies the tail bound sep(Gµ) 1 2 2 π exp(µ2/8) µ . (45) (46) (47) (48) In particular, substituting µ = µ(M, EM , σ1) yields an explicit lower bound on the limiting µ-GDP separation. Proof. Corollary 5.4 of [21] establishes the privacy guarantees for NoisySGD, which denotes the standard stochastic gradient descent algorithm with gradient clipping and Gaussian noise injection. In the context of our work, NoisySGD corresponds to the DP-SGD mechanism (Algorithm B) when instantiated with Poisson subsampling. Specifically, the result states that if /N converges to finite constant c, where is the expected batch size and is the total number of gradient steps, then the mechanism is asymptotically µ-GDP with parameter 2 µ = (cid:113) eσ2 Φ(1.5σ1) + 3Φ(0.5σ1) 2. (49) Using the epoch reparameterization EM := m/N and := N/m, we obtain (cid:114) = EM . Hence, under the scaling assumption (44), µ(M, EM , σ1) converges to the limit µ given in (46). By Lemma E.1, the separation induced by µ-GDP is obtained by evaluating the Gaussian tradeoff curve at its unique fixed point, yielding sep(Gµ) = 2Φ(µ/2)1 . Applying this expression to µ(M, EM , σ1) and assuming gives (45). For the tail bound, Lemma E.1 implies 2 sep(Gµ) = 1 2 2 (1 Φ(µ/2)). Applying the Gaussian tail inequality 1 Φ(t) φ(t)/t for > 0 at = µ/2 yields 1 2 sep(Gµ) 2 φ(µ/2) µ/2 . Since φ(µ/2) = 1 2π exp(µ2/8), rearranging gives (48). 40 Interpretation. Lemma F.1 characterizes the behavior of the µ-GDP approximation under specific joint asymptotic regime in which the epoch budget and the number of rounds grow proportionally. Fix σ1 > 0, and let (EM )M 1 be any sequence of epoch budgets such that (cid:114)"
        },
        {
            "title": "EM\nM",
            "content": "as , for some constant > 0. Define the µ-GDPpredicted separation κµ-GDP(M, EM , σ1) := sep(cid:0)Gµ(M,EM ,σ1) (cid:1) , and write ε(M, EM , σ1) := (cid:12) (cid:12)κµ-GDP(M, EM , σ1) sep(Gµ )(cid:12) (cid:12) for the approximation error relative to the limiting GDP separation. Then Lemma F.1 implies that ε(M, EM , σ1) 0 as , whenever (cid:112)EM /M c. In particular, fix reference pair (M , E) and define := (cid:112)E/M . For any sequence (EM )M 1 satisfying EM = and (cid:112)EM /M c, the quantity (cid:112)EM /M necessarily oscillates around c, with oscillations vanishing as grows. Up to this point, the statement is fully rigorous and follows directly from the asymptotic characterization of µ(M, E, σ1). At heuristic level, this picture suggests the following intuition. If is sufficiently large, then the regime (M , E) already lies close to the asymptotic scaling curve (cid:112)EM /M = c. Consequently, one expects that the µ-GDP prediction κµ-GDP(M , E, σ1) is already close to its asymptotic value, and hence that ε(M , E, σ1) is small. This intuition, however, does not by itself constitute formal guarantee. The -DP framework establishes asymptotic convergence to Gaussian trade-off curve, but does not provide convergence rate results. As result, closeness to the asymptotic regime alone cannot be translated into formal privacy guarantee. F.1 Illustrative Asymptotic Instantiation We now carry out simple asymptotic instantiation to illustrate what Lemma F.1 predicts when the noise level is set according to our theoretical lower bound that will be derived later in Theorem 6.1. Throughout this subsection we fix the one-epoch setting = 1. Let σ = ln , equivalently σ1 = for fixed constant > 0. ln , Substituting this expression into the asymptotic µ-GDP formula of Corollary 5.4 in [21], we obtain µ(M, 1, σ1) = = 2 (cid:114) 1 1 (cid:112)exp(σ2) Φ(1.5σ1) + 3Φ(0.5σ1) 2 (cid:114) exp (cid:16) ln s2 (cid:17) (cid:16) 1.5 Φ ln (cid:17) (cid:16) + 3Φ 0.5 ln (cid:17) 2. (50) To obtain simple and explicit lower bound, we bound the terms inside the square root in conservative manner. First, since Φ(x) 2 for all 0, we have (cid:16) 1.5 Φ (cid:17) ln 1 . Second, since Φ(x) 0 for all R, we may drop the negative-tail term entirely: (cid:16) 3Φ 0.5 0. (cid:17) ln Applying these bounds to (50) yields µ(M, 1, σ1) 2 1 (cid:114) 1 exp (cid:16) ln s2 (cid:17) 2 = 1 (cid:114) 1 2 1/s2 2 = (cid:112) 1/s2 4, 1 (51) 41 Figure 3: Explicit lower bound on the separation sep(cid:0)Gµ(M,1,σ1) rounds per epoch under noise schedules of the form σ = s/ ln , with = 1. (cid:1) as function of the number of where the last equality holds whenever 1/s2 Finally, we simplify the expression further by observing that for sufficiently large , 4. Applying this bound to (51) yields 1/s2 4 2 1/s2 . µ(M, 1, σ1) 1 (cid:113) 1 2 1/s2 = 1 2 1 2s2 1 2 . Substituting this expression into the tail bound (48) gives the fully explicit separation bound sep(cid:0)Gµ(M,1,σ1) (cid:1) 1 2 2 π (cid:16) exp 16 1 s2 1(cid:17) 1 2 2s2 1 2 . (52) This instantiation illustrates that, when the noise multiplier is set at the level σ = s/ ln (and = 1), the asymptotic µ-GDP theory predicts separation that increases with and approaches its maximal value 1/ 2. Figures 3 and 4 visualize the implications of Lemma F.1 and its asymptotic instantiation for the separation metric as the number of rounds per epoch increases. Figure 3 plots the fully explicit lower bound obtained by combining the Gaussian tail bound in Eq. (48) with the explicit lower bound on the µ-GDP parameter derived in Eqs. (51)(52) under the noise schedule σ = s/ ln (with = 1). Figure 4 shows the corresponding µ-GDPpredicted separation κµ-GDP(M, E, σ1) defined in Eq. (47), where µ(M, E, σ1) is given by the asymptotic characterization in Eq. (49) with epoch scaling (44). Both figures demonstrate that, under noise schedules of order σ = Θ(1/ ln ), the separation increases rapidly with and approaches its maximal value 1/ 2. Figure 4: GDP-predicted separation κµ-GDP(M, E, σ1) as function of the number of rounds per epoch under the noise schedule σ = 1/ 2 ln , for several epoch counts E. We emphasize that, unlike the asymptotic µ-GDP approximation discussed above, our main separation bound is non-asymptotic. In particular, it holds for every finite in the single-epoch setting, without requiring any limiting regime or asymptotic approximation."
        }
    ],
    "affiliations": [
        "CWI Amsterdam"
    ]
}