{
    "paper_title": "Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges",
    "authors": [
        "Safal Shrestha",
        "Minwu Kim",
        "Keith Ross"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models."
        },
        {
            "title": "Start",
            "content": "Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges Safal Shrestha* Minwu Kim* Keith Ross New York University Abu Dhabi"
        },
        {
            "title": "Abstract",
            "content": "GSM8K Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose novel grading methodology that distinguishes between logical and non-logical errors, offering more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal significant increase in logical error ratesup to 14 percentage pointsas numerical complexity rises, demonstrating general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide comprehensive evaluation of LLMs mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models."
        },
        {
            "title": "Introduction",
            "content": "Mathematical reasoning with Large Language Models (LLMs) has recently been the subject of significant attention (Wei et al., 2022; OpenAI, 2024; Ahn et al., 2024). However, the current evaluation methodologies for these systems exhibit notable limitations. First, existing benchmarks primarily focus on problems with limited numerical ranges *Equal contribution; order determined by coin toss. Correspondence: keithwross@nyu.edu Code and relevant dataset available at https://github. com/minwukim/GSM-Ranges Judy teaches 5 dance classes every day on the weekdays and 8 classes on Saturday. If each class has 15 students and she charges $15 per student, how much money does she make in 1 week? GSM-Ranges (Level 6 Perturbation) Judy teaches 3,124,213 dance classes every day on the weekdays and 7,832,129 classes on Saturday. If each class has 25 students and she charges $35 per student, how much money does she make in 1 week? Table 1: An example of question generated by GSMRanges tool, derived from base problem from the GSM8K dataset. (Madaan and Yazdanbakhsh, 2022), leaving significant gap between the controlled evaluations and real-world settings. Second, traditional grading approaches typically compare LLMs final answers directly with the ground truth answers (Hong et al., 2024; Shakarian et al., 2023), practice that conflates logical and numerical errors, thereby obscuring deep understanding of the LLMs reasoning capabilities. This motivates the need for better evaluation approach that not only handles numbers across wide numerical ranges but also distinguishes between logical and arithmetic errors. This paper makes three contributions. First, we introduce publicly available GSM-Ranges, tool for generating datasets that are designed to evaluate error rates and error types across diverse numerical ranges. Derived from the GSM8K dataset (Cobbe et al., 2021), GSM-Ranges systematically organizes problems into distinct numerical intervals. Specifically, GSM-Ranges applies six distinct levels of perturbations to GSM8K questions, replacing existing numbers with random values across 5 2 0 2 2 1 ] . [ 1 0 8 6 8 0 . 2 0 5 2 : r six distinct scales. Second, we introduce novel grading methodology that distinguishes between logical and nonlogical errors. We claim that solution should be deemed logically valid if computational inaccuracies can be corrected and the revised answer matches the ground truth. Conversely, if the final answer remains incorrect despite eliminating such errors, we infer fundamental flaw in the reasoning process. To automate this assessment, our methodology employs GPT-4o model (OpenAI, 2024) to translate LLM-generated response into Python code that accurately captures the underlying logic. By executing this code, we isolate non-logical errors and compute the corrected final answer, which is then compared with the ground truth to assess logical correctness. We also perform careful evaluation of our automated grading methodology, and confirm its high level of accuracy for distinguishing between the two error types. Third, using our grading methodology and our GSM-Ranges tool, we analyze various open-source and proprietary LLM models. This leads to several findings: Previous works have shown that arithmetic errors become more pronounced for larger numbers (Qian et al., 2023; Feng et al., 2024). We find that this trend applies to logical errors as well, with the worst-case logical error rate increasing by up to 14 absolute percentage points as perturbation levels rise. This is surprising since the logical reasoning process required to solve the problems remains unchanged despite the numerical modifications. Nevertheless, we still observe an increase in logical errors as perturbation levels rise, suggesting that the logical reasoning in LLMs tends to exacerbate for out-of-distribution numerical values, compromising their robustness in handling broader numerical scales. Previous studies have demonstrated that LLMs achieve high accuracy on standalone arithmetic tasks with in-distribution numbers (e.g., 366=?) (Yang et al., 2023; Maltoni and Ferrara, 2024; Yuan et al., 2023; Mirzadeh et al., 2024; Xie et al., 2024). While we confirm these findings, our results further reveal that the accuracy of arithmetic computations significantly deteriorates when the calculations are embedded within word problems (e.g., 36 apples from Jack6 = ? apples). By introducing GSM-Ranges and novel grading methodology, this work aims to provide more comprehensive evaluation of mathematical reasoning in LLMs. Our approach not only isolates logical and arithmetic errors but also assesses model robustness across broad range of numerical values. These insights pave the way for future research on improving LLMs mathematical reasoning capabilities and developing models that can generalize more effectively across diverse mathematical problem settings."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Sensitivity to Perturbations. Several prior studies (Stolfo et al., 2022; Hooda et al., 2024; Jiang et al., 2024; Guo et al., 2024) have explored the sensitivity of LLMs to perturbations in input problems, demonstrating significant performance degradation even when the underlying logic remains the same. In the domain of mathematical word problems (MWPs), particularly on the GSM8K benchmark, this degradation has been observed when numbers are slightly changed from the original question. (Li et al., 2024a; Mirzadeh et al., 2024; Shi et al., 2023). However, existing approaches often constrain substituted values to limited numerical range (Stolfo et al., 2022) or use numbers that remain comparable to the original values, which tend to be small (Li et al., 2024a; Mirzadeh et al., 2024; Madaan and Yazdanbakhsh, 2022). In this paper, we go beyond the narrow constraints previously studied, providing comprehensive investigation into how different numerical ranges can impact mathematical abilities in LLMs. Evaluation of Mathematical Correctness. Prior correctness evaluations (grading) predominantly rely on ground-truth comparisons (Shakarian et al., 2023; Fu et al., 2023; Hong et al., 2024; Frieder et al., 2024). However, such straightforward approach does not distinguish between logical and non-logical errors, and therefore cannot alone accurately assess an LLMs mathematical reasoning capabilities. Previous studies have explored simple prompting strategies for evaluation, finding that while LLMs perform well in generating correct answers to benchmark questions, they struggle to identify and diagnose errors in solutions to those same questions. This difficulty is particularly pronounced for non-logical errors, highlighting fundamental gap in their problem comprehension (Li et al., 2024b; Zeng et al., 2024). straightforward alternative involves using external tools, such as calculators, to mitigate non-logical errors. However, this approach requires fine-tuning models to follow specific format and does not always produce reliable results (Schick et al., 2023; Cobbe et al., 2021). To address these challenges, we introduce novel automated grading methodology that eliminates the need for fine-tuning while effectively differentiating between logical and non-logical errors. This enables more precise assessment of how mathematical reasoning deteriorates under various numerical conditions. Arithmetic Errors Due to Perturbations. Past studies have shown that LLMs handle basic arithmetic reasonably well when the arithmetic involves small numbers in standalone queries like \"What is + y?\", skill often linked to memorization (Yang et al., 2023; Maltoni and Ferrara, 2024; Yuan et al., 2023; Qian et al., 2023; Feng et al., 2024). Performance drops significantly in more complex cases, such as multi-step equations, large numbers, and multiplication (Kao et al., 2024; Yuan et al., 2023; Yang et al., 2023; Feng et al., 2024; Qian et al., 2023). Background context can further affect arithmetic reasoning, introducing additional inconsistencies (Abedin et al., 2025). Current research applying perturbations to widely used benchmarks like GSM8K involve basic arithmetic; thus, arithmetic errors are assumed to be minimal. (Mirzadeh et al., 2024; Xie et al., 2024; Anand et al., 2024). However, deeper understanding of the nature and frequency of arithmetic errors remains lacking. This study addresses this gap by systematically quantifying arithmetic errors in mathematical word problems and conducting qualitative analysis to identify underlying error patterns."
        },
        {
            "title": "3 GSM-Ranges",
            "content": "The majority of existing mathematical benchmarks are constrained to relatively limited numerical ranges. For instance, Madaan and Yazdanbakhsh (2022) reported that single-digit numbers constitute approximately 50% of the problems in the GSM8K dataset. To further investigate this trend, we analyzed cumulative frequency distribution of numerical values across three widely used benchmarks: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MATH (Hendrycks et al., 2021). As shown in Figure 1, our analysis reveals that in all three Figure 1: Cumulative frequency distribution of numerical values in questions and ground truth answers. Numbers <1,000 account for 94.9% (GSM8K), 97.8% (SVAMP), and 98.0% (MATH) of the values. datasets, numbers below 1,000 (i.e., three digits or fewer) account for 94.9% of values in GSM8K, 97.8% in SVAMP, and 98.0% in MATH. These findings indicate that the mathematical capabilities of current LLMs have primarily been evaluated within limited numerical range. To assess robustness across wider ranges, we introduce GSM-Ranges, tool for generating datasets which encompass wider distribution of numerical values."
        },
        {
            "title": "3.1 Selecting Base Problems from GSM8K",
            "content": "GSM-Ranges systematically modifies numerical values in the GSM8K dataset. From the GSM8K test set of 1,319 questions, we exclude those involving non-integer values or division in the ground truth answers, as such cases could potentially create logically incoherent problems (e.g., Assign 5 people evenly to 2 separate rooms). After filtering, 100 questions are randomly selected, with all numbers in the questions being singleor double-digit."
        },
        {
            "title": "3.2 Perturbation Levels",
            "content": "We convert the 100 sampled questions into Python templates to systematically adjust the numerical values within the questions across various ranges. Specifically, we apply 6 levels of perturbation: same-digit, 1001,000, 1,000 10,000, 10,000100,000, 100,0001,000,000, and 1,000,00010,000,00, which we refer to as level 1 to level 6 perturbation, respectively. In same-digit perturbation (level 1), we randomly replace each number in the problem with randomly chosen number with the same number of digits as the original number, thereby maintaining the similarity of the perturbed problems to the original. Additionally, we ensure that modified numbers are always different from the original values, preventing any duplication of the original problems. In 100-1000 perturbation (level 2), we replace each number in the problem with number randomly chosen in the range 100 to 1000. Levels 3-6 are done in similar manners with their respective ranges. All perturbations ensure non-negative final answers and intermediate values, as, similar to the case of fractional values, they can lead to logically incoherent math problems (e.g. store sells -7 items in day, Eat 10 apples out of 3 apples). In addition, to prevent extreme scaling of final answers, we apply scaling selectively in cases involving multiplication. Specifically, when the final answer is derived from multiplication operation (e.g., (A + B) (C + E)), we scale only one side of the multiplicationeither and or C, and Ewhile keeping the other side within the original numerical ranges. This approach maintains the final answer within manageable limits while introducing numerical variation in the problem, preventing the inclusion of excessively complex or computationally infeasible arithmetic operations for LLMs. An example of problem generated with these crafted templates is shown in Table 1."
        },
        {
            "title": "4 Grading Methodology",
            "content": "While modern LLMs perform well on basic arithmetic operations, their accuracy is reported to diminish substantially as numerical magnitudes increase (Qian et al., 2023). However, conventional evaluation methods, which primarily compare final answers with ground truth values (Hong et al., 2024; Shakarian et al., 2023), fail to provide complete picture of LLMs mathematical understanding as they do not distinguish computational errors from reasoning errors. To address this deficiency, we propose novel and accurate grading methodology which not only determines whether the answer is correct or erroneous, but also categorizes the type of error."
        },
        {
            "title": "4.1 Error Definitions",
            "content": "Our grading methodology first compares the final answer with the ground truth. If the answers match, the response is labeled as correct. Otherwise, there is an error. We define the error types as follows: Figure 2: Illustration of grading process for LLM responses using the GPT-4o model, categorizing outputs into three labels: correct, non-logical error, and logical error. Non-logical error: Errors that do not stem from the reasoning process itself, such as arithmetic errors or number-copy errors. The latter refers to inaccurately reproducing problem values (e.g., misrepresenting 1,337,042 as 13,337,042) and is observed in higher-level perturbations in some models. Logical error: All errors not classified as non-logical, such as but not limited to missing steps, contradictory steps, or operator misuse. It should be noted that responses classified as logical errors may also include non-logical errors."
        },
        {
            "title": "4.2 Methodology Overview",
            "content": "To handle the extensive volume of responses, we employ the GPT-4o model (OpenAI, 2024) to automate the evaluation. For response that fails to match the ground truth answer, we have GPT-4o model translate the reasoning in the response into Python code, which is then executed to generate new answer. The prompt is provided in Appendix A.3. If the new answer aligns with the ground truth, Figure 3: Logical & non-logical error rates across different perturbation levels. The left panel illustrates the increase in logical errors across the datasets, while the right panel depicts the rise in non-logical errors. Error rates are reported relative to the baseline logical and non-logical error rates on the original GSM8K problems. our methodology classifies the response as containing non-logical error; otherwise, it is classified as logical error. detailed illustration of the grading process is shown in figure 2."
        },
        {
            "title": "4.3 Number-Copy Errors",
            "content": "Apart from arithmetic errors, we observe that number-copy errors are non-trivially present in some models under higher levels of perturbations (Appendix A.1.5). For instance, in the case of the Qwen 2.5 7B model (Qwen, 2024), 4 out of 100 randomly sampled responses under level 6 perturbation demonstrate number-copy errors. To enable the GPT-4o model to identify such errors, the model requires access to the numbers provided in the problem. Instead of providing the entire problem, we supply the model with list of extracted numbers from the problem text. This approach is motivated by our observation that providing the full problem tends to lead the model to revise logically flawed answers into logically correct ones. This behavior aligns with the known tendency of LLMs to exhibit biases toward generating correct responses and their difficulty in intentionally producing incorrect answers (Tjuatja et al., 2023; Kumar and Jain, 2024). By limiting access to the original question, we minimize this undesired bias while enabling number-copy error correction."
        },
        {
            "title": "4.4 Validation",
            "content": "To validate our grading methodology, we perform careful manual analysis. We collect responses generated by nine models across six perturbation levels from the experiments in Section 5, along with the corresponding Python code produced by GPT-4o. We randomly sample 200 of these responses and and manually classify each one. We found that our grading methodology correctly classified 197 of the responses correctly, achieving high accuracy of 98.5%. Furthermore, we assessed GPT-4os ability to correct number-copy errors when going from response to Python code. We identify 50 responses across different models that contain such errors and evaluate whether the generated code correctly fixes them. Our manual review confirms that all 50 errors are successfully corrected. These evaluation results establish the reliability of our methodology."
        },
        {
            "title": "5 Experiments and Results",
            "content": "Using GSM-Ranges and our proposed grading methodology, we evaluate the mathematical reasoning capabilities of nine distinct models, including both open-source and closed-source variants. Recall that we begin with 100 randomly selected GSM8K questions. For each question and for each of the six perturbation levels, we generate 50 random variations of the question. This process yields dataset of 5,000 problems per perturbation level. For each perturbation level and each model, we ity. Surprisingly, however, while the degree varies among the models, we observe consistent upward trend in logical errors as the perturbation level increases, across all nine evaluated models (Figure 3) except GPT-4o. To quantify this trend, we calculate the difference in logical error rates between level 6 (1M10M) and level 1 (same digit) perturbations for each model (Figure 4). The most pronounced discrepancy is exhibited by the Gemma 2 2B model, which shows 14% absolute increase in logical error rate. Similarly, the WizardMath 7B v1.1 model demonstrates substantial increase of 10%. Even the relatively more robust models, such as Phi-3 Mini 4K and GPT-3.5 Turbo, still exhibit an increase of approximately 4%, which remains significant deviation. GPT-4o, one of the most advanced models at the time of our study, stands out as the only model with near-zero gap. These results reveal the sensitivity of the models logical reasoning to numerical scales. We conjecture this phenomenon occurs because the models are mostly trained with lower-range numbers, and test problems with large numbers are out of distribution. (A qualitative analysis of additional logical errors induced by increasing numerical values is provided in Appendix A.4.) Another noteworthy observation is that the increase in logical errors becomes more gradual at higher perturbation levels. Across all nine models, the gap between level 3 and level 1 is generally larger than that between level 6 and level 3. This trend aligns with the cumulative frequency patterns observed in Figure 1, which shows that low-range values account for majority of numbers across the most widely used benchmark datasets. While the exact composition of training data for the models remains unknown, if math-problem training data is predominantly concentrated in the lower numerical ranges, it is plausible that beyond certain threshold, further increases in numerical magnitude do not lead to significant difference in model performance. Once numbers exceed this threshold, they may all be similarly unfamiliar to the model due to their low presence in the training data. Further investigation is needed to validate this hypothesis."
        },
        {
            "title": "5.1.2 Potential Data Contamination with",
            "content": "GSM8K Dataset We also observe notable logical error gap between level 1 perturbation and the original questions for many of the evaluated models (Figure 4). The Gemma 2 2B model exhibits the largest gap at 6%, Figure 4: Logical error gaps across perturbation levels. For each model, the top bar represents the percentage point difference in logical errors between Level 6 and Level 1 perturbations, while the bottom bar indicates the percentage point difference between Level 1 and the original GSM8K questions. obtain responses to the 5,000 questions and classify the responses using our grading methodology. We then determine the percentages of correct answers, logical errors, and non-logical errors across the 5,000 responses. To compute confidence intervals, we leverage the structure of our dataset: each perturbation level consists of 50 distinct sets of questions derived from the original 100. We calculate the proportion of each error type within each set and then use these 50 sample proportions to estimate the corresponding confidence intervals, thereby quantifying the variability in model performance across perturbation instances. Additionally, we assess each model on the 100 unmodified base questions from GSM8K, providing standard reference point for performance comparison. All inferences are done in the greedy decoding setting."
        },
        {
            "title": "5.1.1 Rising Trend of Logical Errors",
            "content": "Since the perturbations alter only the numerical values while keeping the question structure intact, the logical reasoning required to solve the problems remains unchanged across all perturbation levels. In principle, all perturbation levels should demand the same level of logical reasoning abilfollowed by Mistral 7B v0.1 with 4% discrepancy. However, this pattern is not consistent across all models. For instance, Qwen 2.5 7B and GPT-4o show gap of only about 1%, demonstrating better robustness. Moreover, Llama 3.2 3B and Phi-3 Mini 4K exhibit -2% gap, indicating an opposite trend. This result points to possibility of data contamination to the GSM8K dataset in certain models. Notably, similar finding was previously reported by Mirzadeh et al. (2024), but our study explores this issue in more depth by providing an explicit definition of numerical-range similarity and establishing clear distinction between logical and non-logical errors, further validating their conclusions. 5.2 Arithmetic Errors 5.2.1 Rising Trend of Arithmetic Errors Previous studies have shown that LLMs exhibit significant decline in arithmetic accuracy as numerical values grow (Qian et al., 2023; Feng et al., 2024), and our result further confirms this trend. As shown in Figure 3, we also observe consistent increase in non-logical errors. Given that numbercopy errors account for only small portion  (Table 7)  , the majority of these errors stem from arithmetic errors. Furthermore, because some responses classified as logical errors also include arithmetic errors, the true prevalence of arithmetic errors exceeds what is suggested in the figure."
        },
        {
            "title": "5.2.2 Arithmetic Errors with Small Numbers",
            "content": "Previous studies have found that state-of-the-art models have arithmetic accuracy on low-range numbers (Henighan et al., 2020; Yuan et al., 2023; Qian et al., 2023; Feng et al., 2024). However, we find that some models still show non-trivial percentages of non-logical errors at level 1, such as Mistral 7B v0.1 at 9% and WizardMath 7B v1.1 at 4%. This motivates further analysis on the patterns of these errors, which is discussed in section 6.3. 6 In-depth Analysis 6.1 Is the Correct Logic Present in the LLM? We observe that larger numerical values in math questions increase the likelihood of logical errors. However, although sampled response may have logical error, the correct logic may nevertheless be present in the models distribution. To investigate this issue, we measure recall rates defined as follows: for each of 100 randomly generated Figure 5: Recall rates across perturbation levels and original GSM8K questions for different sampling sizes (1, 8, 32, 48). Figure 6: Results of o3-mini across perturbation levels. The left plot displays the logical and non-logical error counts, while the right plot shows the mean token counts per 100 responses at each perturbation level. questions, we obtain responses (i.e., perform npasses) in non-zero temperature setting (temperature = 0.8, top_p = 0.95) and, using our grading methodology, count the number of questions for which the correct logic appears in at least one of the passes. We perform this experiment over all six perturbation levels, and for varying number of passes ranging from = 1 to = 48. As shown in Figure 5, for all four models, as the number of passes increases, we observe (1) higher recall rate, and (2) smaller gaps across perturbation levels. At the highest sample size of 48, the gap between level 1 and 6 is no more than 2 for any model, indicating that the correct logic exists within the models distribution despite larger numerical values. This suggests that training on broader numerical ranges or leveraging test-time computation could improve numerical consistency."
        },
        {
            "title": "6.2 Performance of Reasoning Model",
            "content": "The growing prominence of reasoning models (OpenAI, 2025; Guo et al., 2025) naturally raises Model Gemma 2 2B WizardMath 7B v1.1 Mistral 7B v0.1 Mathstral 7B v0.1 Llama 3.2 3B Qwen 2.5 7B Phi 3 Mini 4K GPT-3.5 Turbo GPT-4o Level 1 15/134 (11.2%) 41/117 (35.0%) 73/299 (21.4%) 31/77 (40.2%) 3/72 (4.2%) 7/18 (38.9%) 9/22 (40.9%) 10/18 (55.6%) 1/3 (33.3%) Level 2 126/735 (17.1%) 186/806 (23.1%) 274/1313 (20.9%) 126/509 (24.8%) 127/1480 (8.6%) 52/155 (33.5%) 89/281 (31.7%) 92/224 (41.1%) 20/40 (50%) Table 2: Results of standalone arithmetic assessment on arithmetic errors made by models under Level 1 and Level 2 perturbations. the question of their performance across different perturbation levels. To explore this, we evaluate o3minione of the most advanced reasoning models at the time of our studyon set of 100 problems for each perturbation level. As shown in Figure 6, o3-mini maintains consistently low logical and non-logical error rates across perturbation levels, demonstrating its robustness to varying numerical scales. We also record the average token count for 100 responses at each perturbation level and observe that the model generates more tokens as the perturbation level increases (Figure 6). While this increase may partly stem from larger numerical values requiring more tokens for representation and complex arithmetic, it also suggests that changes in numerical scale might lead the model to perceive the tasks as more challenging, possibly due to its training data being primarily focused on lowerrange numbers. Additionally, the model produces more tokens for same-digit perturbations compared to the original GSM8K questions. This raises the possibility of data contamination, allowing the model to arrive at the correct final answer with less reasoning."
        },
        {
            "title": "6.3 Arithmetic Error Patterns",
            "content": "Previous studies have evaluated the arithmetic accuracy of LLMs in standalone setting, i.e., directly posing arithmetic questions like \"What is 1 + 2?\" (Yang et al., 2023; Maltoni and Ferrara, 2024; Yuan et al., 2023; Qian et al., 2023; Feng et al., 2024). However, little attention has been paid to whether their arithmetic performance remains robust when these operations are embedded within natural language responses. To investigate this, we conduct an experiment by collecting all responses containing arithmetic errors from all models under level 1 and 2 perturbations, and then extracting the specific arithmetic operations that were answered incorrectly. We subsequently prompt the models to solve these arithmetic operations in standalone setting. As shown in Table 2, while the extent varies, the models perform significantly better when the arithmetic task is isolated. We hypothesize that this phenomenon occurs because LLMs predominantly rely on memorization for arithmetic operations, since they train largely on standalone arithmetic data (Yuan et al., 2023; Yang et al., 2023; Maltoni and Ferrara, 2024). This results in degraded performance when these operations are integrated into natural language context, which is out-ofdistribution for the LLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce GSM-Ranges, benchmark designed to evaluate LLMs reasoning abilities across diverse numerical scales. Additionally, we propose novel grading methodology that classifies erroneous into logical and non-logical categories. Through extensive experiments on various models using GSM-Ranges and our grading framework, we find that logical accuracy tend to degrade significantly as perturbation level rises, revealing LLMs sensitivity to numerical scales. Furthermore, while LLMs perform well on isolated arithmetic tasks, their accuracy declines significantly when calculations are integrated into natural language contexts. This study provides more precise assessment of LLMs mathematical reasoning and paves the way for future research on improving mathematical reasoning capabilities and developing models that can generalize more effectively across diverse mathematical problem settings."
        },
        {
            "title": "8 Limitations",
            "content": "Due to resource constraints, our study primarily focuses on small, lightweight models. While we have evaluated GPT-4o and o3-mini, future work could extend the analysis to other advanced models. Additionally, our perturbation study is conducted on the GSM8K dataset, and exploring the impact of varying numerical ranges on performance in more complex mathematical tasks would further enrich the findings. Lastly, while our grading methodology distinguishes between logical and non-logical errors, more granular grading methodology could offer deeper insights into model performance and refinement."
        },
        {
            "title": "Acknowledgments",
            "content": "We express our gratitude to Yik-Cheung (Wilson) Tam, Professor of Practice in Computer Science at New York University Shanghai, for his valuable advice in shaping the ideas for this study."
        },
        {
            "title": "References",
            "content": "Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, and Akbar Karimi. 2025. Arithmattack: Evaluating robustness of llms to noisy context in math problem solving. arXiv preprint arXiv:2501.08203. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 225237, St. Julians, Malta. Association for Computational Linguistics. Avinash Anand, Mohit Gupta, Kritarth Prasad, Navya Singla, Sanjana Sanjeev, Jatin Kumar, Adarsh Raj Shivam, and Rajiv Ratn Shah. 2024. Mathify: Evaluating large language models on mathematical problem solving tasks. arXiv preprint arXiv:2404.13099. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, and Liwei Wang. 2024. How numerical precision affects mathematical reasoning capabilities of llms. arXiv preprint arXiv:2410.13857. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. 2024. Mathematical capabilities of chatgpt. Advances in neural information processing systems, 36. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. Chain-of-thought hub: continuous effort to measure large language arXiv preprint models reasoning performance. arXiv:2305.17306. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Siyuan Guo, Aniket Didolkar, Nan Rosemary Ke, Anirudh Goyal, Ferenc Huszár, and Bernhard Schölkopf. 2024. Learning beyond pattern matching? assaying mathematical understanding in llms. arXiv preprint arXiv:2405.15485. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Advances in Neural Information Processing Systems, volume 34, pages 2424124253. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701. Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria. 2024. Caught in the quicksand of reasoning, far from agi summit: Evaluating llms mathematical and coding competency through ontology-guided interventions. arXiv preprint arXiv:2401.09395. Ashish Hooda, Mihai Christodorescu, Miltiadis Allamanis, Aaron Wilson, Kassem Fawaz, and Somesh Jha. 2024. Do large code models understand programming concepts? black-box approach. arXiv preprint arXiv:2402.05980. Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. 2024. peek into token bias: Large language models are not yet genuine reasoners. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore. Association for Computational Linguistics. Kuei-Chun Kao, Ruochen Wang, and Cho-Jui Hsieh. 2024. Solving for and beyond: Can large language models solve complex math problems arXiv preprint with more-than-two unknowns? arXiv:2407.05134. Ananya Kumar and Siddharth Jain. 2024. Investigating implicit bias in large language models: large-scale study of over 50 llms. arXiv preprint arXiv:2410.12864. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. 2024a. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 29612984. Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, and Fuli Feng. 2024b. Evaluating mathematical reasoning of large language models: focus on error identification and correction. arXiv preprint arXiv:2406.00755. Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. Preprint, arXiv:2209.07686. Davide Maltoni and Matteo Ferrara. 2024. Arithmetic with language models: From memorization to computation. Neural Networks, 179:106550. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482724837. Roy Xie, Chengxuan Huang, Junlin Wang, and Bhuwan Dhingra. 2024. Llm-resistant math word problem generation via adversarial attacks. arXiv preprint arXiv:2402.17916. Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. 2023. Gpt can solve mathematical problems without calculator. arXiv preprint arXiv:2309.03241. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. 2024. Mr-gsm8k: metareasoning benchmark for large language model evaluation. Preprint, arXiv:2312.17080. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Preprint, arXiv:2410.05229. OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2024. Gpt-4o system card. Published: August 8, 2024. Accessed: 2024-12-20. OpenAI. 2025. Introducing OpenAI o1. Accessed February 7, 2025. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, Online. Association for Computational Linguistics. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2023. Limitations of language models in arithmetic and symbolic induction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 92859298, Toronto, Canada. Association for Computational Linguistics. Qwen. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023. An independent evaluation of chatgpt on mathematical word problems (mwp). arXiv preprint arXiv:2302.13814. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 3121031227. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, and Mrinmaya Sachan. 2022. causal framework to quantify the robustness of mathematical reasoning with language models. arXiv preprint arXiv:2210.12023. Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. 2023. Do llms exhibit human-like response biases? case study in survey design. arXiv preprint arXiv:2311.04076."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experiment Results A.1.1 Logical Error Rates Model Baseline Gemma 2 2B GPT-3.5 Turbo GPT-4o Llama 3.2 3B Mathtral 7B v0.1 Mistral 7B v0.1 Phi 3 Mini 4K Qwen 2.5 7B Wizardmath 7B v1.1 o3-mini 18 11 4 17 7 29 10 4 7 Perturbation Levels Lv.1 24.3(0.8) 13.5(0.5) 5.1(0.4) 14.5(0.5) 9.0(0.5) 33.3(0.9) 7.7(0.4) 5.0(0.4) 8.1(0.5) 5 Lv.2 30.9(0.6) 15.0(0.8) 6.9(0.3) 17.0(0.6) 11.6(0.5) 37.5(0.9) 9.1(0.4) 7.8(0.5) 14.0(0.6) 6 Lv.3 32.3(0.7) 17.5(0.6) 6.9(0.3) 19.3(0.7) 12.4(0.5) 38.5(0.9) 10.7(0.4) 9.0(0.5) 15.7(0.7) 5 Lv.4 35.1(0.6) 17.3(0.7) 6.2(0.3) 18.7(0.7) 13.9(0.5) 40.7(0.7) 11.2(0.5) 10.2(0.5) 16.4(0.7) 4 Lv.5 36.8(0.8) 16.9(0.6) 5.0(0.3) 19.4(0.6) 15.2(0.6) 42.4(0.8) 11.2(0.5) 10.1(0.6) 17.6(0.6) Lv.6 38.5(0.8) 17.3(0.6) 5.3(0.3) 19.4(0.6) 14.8(0.6) 43.3(0.8) 12.2(0.5) 9.9(0.5) 19.1(0.6) 4 Table 3: Logical error rates and confidence intervals across different GSM-Ranges perturbation levels. A.1.2 Non-Logical Error Rates"
        },
        {
            "title": "Baseline",
            "content": "Gemma 2 2B GPT-3.5 Turbo GPT-4o Llama 3.2 3B Mathtral 7B v0.1 Mistral 7B v0.1 Phi 3 Mini 4K Qwen 2.5 7B Wizardmath 7B v1.1 o3-mini 3 0 0 2 2 12 1 0 2 0 Lv.1 3.6(0.4) 0.5(0.2) 0.1(0.1) 1.9(0.3) 2.0(0.4) 9.3(0.5) 0.5(0.2) 0.4(0.2) 4.1(0.6) 0 Lv.2 14.7(0.9) 5.1(0.5) 0.8(0.2) 25.1(1.1) 10.1(0.8) 25.1(1.2) 6.2(0.4) 3.8(0.5) 15.5(0.7) 0 Perturbation Levels Lv.3 21.6(0.8) 12.7(0.8) 2.7(0.3) 49.5(1.2) 14.8(0.8) 31.0(1.1) 10.5(0.7) 7.0(0.6) 24.3(1.2) 0 Lv.4 25.9(0.9) 18.1(0.8) 3.6(0.3) 59.1(1.2) 19.3(1.1) 34.9(1.2) 15.8(1.0) 9.9(0.7) 31.0(1.0) Lv.5 29.6(1.1) 35.0(1.0) 5.2(0.4) 61.8(0.9) 23.0(0.9) 38.6(1.2) 21.4(0.9) 12.1(0.9) 35.6(1.2) 2 Lv.6 37.2(1.2) 38.6(1.0) 5.2(0.6) 68.8(0.9) 27.6(1.0) 42.0(1.1) 28.0(1.1) 16.4(0.9) 42.5(1.3) 0 Table 4: Non-logical error rates and & confidence intervals across different GSM-Ranges perturbation levels. A.1.3 Recall Rates for Correct Logics Model Sample Size Gemma 2 2B Mistral 7B v0.1 Mathtral 7B v0.1 Wizardmath 7B v1.1 1 8 32 48 1 8 32 48 1 8 32 48 1 8 32 48 Perturbation Levels GSM8K Baseline Lv.1 Lv.2 Lv.3 Lv.4 Lv.5 Lv.6 59 84 92 94 50 83 93 95 84 91 94 94 78 94 95 95 67 87 92 92 51 84 93 96 85 92 94 94 84 92 94 94 61 84 89 92 59 81 93 93 86 92 92 93 78 90 93 93 74 89 91 92 66 88 93 95 85 92 93 94 84 95 97 97 68 87 89 92 60 87 93 93 86 90 92 93 83 94 95 96 69 87 92 93 61 82 90 91 82 88 88 89 85 93 98 82 92 95 95 66 89 97 97 90 94 96 96 92 98 99 99 Table 5: Recall rates across different sampling sizes and GSM-Ranges perturbation levels. We use A.1.4 Mean Token Counts of o3-mini Responses"
        },
        {
            "title": "Mean Token Count",
            "content": "Baseline Level 1 Level 2 Level 3 Level 4 Level 5 Level 6 579.8 378.8 252.8 287.6 340.6 429.3 501. Table 6: Mean token counts across GSM-Ranges perturbation levels for o3-mini responses A.1.5 Number-Copy Error Analysis Model Qwen 2.5 7B Llama 3.2 3B Mathstral 7B v0.1 Phi 3 Mini 4K Gemma 2 2B GPT-3.5 Turbo GPT-4o Mistral 7B v0.1 Wizardmath 7B v1.1 Lv. 4 Lv. 5 Lv. 6 2 0 0 0 0 0 0 0 0 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 Table 7: Occurrences of Number-Copy Errors in 100 Random Samples Across Levels 4, 5, and 6 for Each Model. For each of the nine base models, we sampled 100 responses per level from the level 4, 5, and 6 perturbations to evaluate number-copy error rates. As shown in Table 7, 4 out of the 8 base models exhibited number-copy errors under level 6 perturbation, while only one model showed errors under level 5, and none were observed at level 4. A.2 Full Prompt for Inference The full prompt used for inferences in the experiments is shown below: Zero-shot Prompt for Inferences As an expert problem solver, solve the following mathematical question step by step. Q: {Question} A: Lets think step by step. A.3 Python Code Generation Prompt Below is the prompt provided to the GPT-4o model for translating LLMs responses into Python code. We introduce step to verbalize the response logic prior to code generation, as this process is found to improve the alignment between the generated code and the original response. The temperature is set to 0 in the code generation process. Python Code Generation Prompt You are tasked with writing Python code that replicates the logic described in given response to math problem. Your code must strictly follow the exact reasoning steps provided in the response, regardless of whether the logic is correct, inconsistent, or flawed. 1. Do not fix or modify the reasoning described in the response, even if they seem incorrect or nonsensical. 2. Develop Python function named solver() that replicates the logic in the response exactly as described: Define and assign all necessary variables within the function. The function must not take any external arguments. The function must return the computed final numerical result. 3. Ensure that all arithmetic operations described in the response are explicitly written as code. Avoid directly copying the results of these operations or the final answer from the response. 4. Refer to the list of numbers extracted from the question provided to ensure any copied numbers in the response match the original numbers. If number in the response is incorrectly copied (e.g., misrepresenting 1333785 as 133785 or 13333785), correct the number in your code and document the correction as comment in the code. 5. Include an explanation in the explain field that describes the steps and logic from the response, regardless of correctness. 6. Provide the output in the following format: { } extracted_answer: <final numerical value of the answer>, explain: <detailed explanation of the response logic>, python_code: ```pythonn<generated Python function>n``` This is the list of numbers extracted from the question: {number_list}. This the response: {response}. A.4 Various Types of Additional Logical Errors in Level 6 Perturbation We conducted qualitative analysis of additional logical errors induced by increasing numerical values, focusing on cases with the greatest rise in errors across perturbation levels among the questions derived from the same base GSM8K questions for each model. Our findings reveal diverse range of errors without consistent pattern. Below, we present examples of logical errors exhibited under level 6 perturbation. While some errors were entirely absent at level 1 and emerged only at level 6, others, though rare at level 1, became significantly more frequent. The displayed LLM responses have been edited for readability, with any arithmetic errors corrected. Example 1: Missing Step Error Question: Laurels friend gave her 8 852 986 baby outfits that her child no longer needed. At her baby shower, Laurel received twice that amount in new baby outfits. Then, Laurels mom gifted her with another 5 309 889 baby outfits. How many outfits does she have for her baby? Ground Truth: Laurels friend gave her 8 852 986 outfits. At the baby shower, she received twice that amount: 2 8852986 = 17705972 Adding all outfits together: 8852986 + 17705972 + 5309889 = 31868847 LLM Response: 1. Compute the baby shower outfits: 8852986 2 = 17705972 2. Compute the total outfits: 17705972 + 5309889 = Error: The model ignores the original 8 852 986 outfits from Laurels friend. Example 2: Operator Error Question: Finn watches 9 360 266 tadpoles swimming in the pond. Suddenly, Finn sees 7 180 820 of them come out of hiding from under lily pad, then sees 12 947 038 of them hide under rock. How many tadpoles can Finn see in the pond now? Ground Truth: Initially, Finn sees 9 360 266 tadpoles. 7 180 820 tadpoles come out from hiding, so the new visible count is: 9360266 + 7180820 = 16541086 Then, 12 947 038 tadpoles hide under rock, reducing the visible count: 16541086 12947038 = 3594048 LLM Response: 1. Compute the new visible tadpoles after some emerge: 9360266 + 7180820 = 2. Since 12947038 more hide under the rock, add them as well: 16541186 + 12947038 = 29488224 Error: The model makes an operator error by incorrectly adding the 12 947 038 tadpoles that hid under the rock instead of subtracting them. Example 3: Contextual Value Error Question: Gloria is shoe shopping when she comes across pair of boots that fit her shoe budget. However, she has to choose between the boots and two pairs of high heels that together cost 4 528 570 dollars less than the boots. If one pair of heels costs 3 392 343 dollars and the other costs twice as much, how many dollars are the boots? Ground Truth: The second pair of heels costs: 3392343 2 = 6784686 The total cost of both pairs of heels: 6784686 + 3392343 = 10177029 The boots cost 4 528 570 dollars more than both pairs of heels: 10177029 + 4528570 = LLM Response: 1. Compute the second pair of heels cost: 3392343 2 = 6784686 2. Compute the total cost of both pairs of heels: 6784686 + 3392343 = 10177029 3. The total cost of the heels is $10177029 less than the boots, so the boots price is: 10177029 + 10177029 = 20354058 Error: The model makes contextual value error by mismatching the numerical value with the corresponding entity. Instead of using 4 528 570 dollars as the price difference between the boots and the two pairs of high heels, it incorrectly uses the total cost of the heels. Example 4: Question Misinterpretation Error Question: Mary is 1 922 674 years younger than Joan, who is 2 112 084 years older than Jessa. If Jessa is 1 840 103 years old, what is the sum of the ages of the three girls? Ground Truth: Joans age: Marys age: Sum of all ages: LLM Response: 1840103 + 2112084 = 3952187 3952187 1922674 = 2029513 1840103 + 3952187 + 2029513 = 7821803 1. Joan is 2 112 084 years old and Jessa is 1 840 103 years old. 2. Mary is 1 922 674 years younger than Joan, which means: 2112084 1922674 = 1919410 3. Now we can find the sum of their ages: 2112084 + 1840103 + 1919410 = 5861607 Error: The model misinterprets the question by incorrectly treating Joans age difference from Jessa as her actual age. Instead of computing Joans age based on her relationship to Jessa, it directly assigned 2 112 084 as Joans age. Example 5: Omitted Information Error Question: Mary is 1 922 674 years younger than Joan, who is 2 112 084 years older than Jessa. If Jessa is 1 840 103 years old, what is the sum of the ages of the three girls? Ground Truth: Joans age: Marys age: Sum of all ages: LLM Response: 1840103 + 2112084 = 3952187 1922674 = 2029513 1840103 + 3952187 + 2029513 = 7821803 1. Mary is 1 922 674 years younger than Joan, who is 2 112 084 years older than Jessa. 2. This can be written as: Joan = Jessa + 2112084 Mary = Joan 1922674 = Jessa + 2112084 1922674 = Jessa + 3. Substituting the expressions for Joan and Mary, we get: Mary + Joan + Jessa = (Jessa + 2112084) + (Jessa + 189410) + Jessa 4. Simplifying this expression, we get: Mary + Joan + Jessa = 3 Jessa + 2301494 Error: The model makes an omitted information error by failing to utilize the given value of Jessas age (1 840 103) in the final computation. Instead of calculating the actual sum of their ages, it leaves the expression in terms of Jessas age without substitution, leading to an incomplete and incorrect result."
        }
    ],
    "affiliations": [
        "New York University Abu Dhabi"
    ]
}