{
    "paper_title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "authors": [
        "Ali Behrouz",
        "Zeman Li",
        "Praneeth Kacham",
        "Majid Daliri",
        "Yuan Deng",
        "Peilin Zhong",
        "Meisam Razaviyayn",
        "Vahab Mirrokni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 3 7 3 2 . 5 0 5 2 : r Atlas: Learning to Optimally Memorize the Context at Test Time Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, and Vahab Mirrokni Abstract Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present Atlas, long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that Atlas surpasses the performance of Transformers and recent linear recurrent models. Atlas further improves the long context performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark."
        },
        {
            "title": "1 Introduction",
            "content": "The attention module (Bahdanau et al. 2014) is critical building block in modern deep learning architectures (Achiam et al. 2023; Behrouz, Zhong, et al. 2024; Kamath et al. 2025; Vaswani et al. 2017), excelling due to its scalability and performance in in-context retrieval tasks. In principle, attention functions as an associative memory, computing direct pairwise token dependencies to store key-value mappings and retrieve them via query-key similarities. Computing this pairwise dependencies, however, while accurate, causes quadratic space and time complexity, limiting their applicability in long context understanding, memorization, or modeling (Dalal et al. 2025; Li, Huang, et al. 2024; Liu, Lin, et al. 2024). Recent research efforts aim to overcome the limitations of Transformersi.e., pure attention-based architecturesin long-context modeling by designing more efficient yet effective recurrent neural networks (Behrouz, Zhong, et al. 2024; Peng, Zhang, et al. 2025; Schlag et al. 2021). These modern recurrent architectures can be unified as associative memory modules optimizing an internal objective termed attentional bias (Behrouz, Razaviyayn, et al. 2025). Unlike Transformers growing KV cache, these models use fixed-size memory, necessitating improved memory management. Consequently, theres growing interest in enhancing RNN memory management through more effective: (i) Learning rules, from additive learning (Katharopoulos et al. 2020) to DeltaNets Delta rule (Schlag et al. 2021); (ii) Forget (Retention) Gates, from RetNets input-independent gating (Sun, Dong, et al. 2023) to adaptive gating in Titans (Behrouz, Zhong, et al. 2024) and RWKV7 (Peng, Zhang, et al. 2025); and (iii) Memory Architectures, from vector-valued memory (Peng, Alcaide, et al. 2023; Sun, Dong, et al. 2023) to neural deep memory modules (Behrouz, Zhong, et al. 2024; Sun, Li, et al. 2024). Despite the success of these improved models in diverse set of downstream benchmarks, they often struggle with long context understanding, in-context retrieval, and extrapolation to longer sequences (Arora, Eyuboglu, Zhang, et al. 2024; {alibehrouz, zemanli, pkacham, dengyuan, peilinz, razaviyayn, mirrokni}@google.com, and majiddl.2099@gmail.com 1 Table 1: summary of the recent modern recurrent neural networks. We compare these architectures based on five characteristics: (1) Dynamic decay; (2) Deep neural memory; (3) non-linear memory capacity; (4) Locally optimal: managing memory by (approximating) the second-order information about tokens; (5) Flexible context: the ability to flexibly memorize the context. 𝜙 () and 𝜙 () represent polynomial and infinite-dimensional feature mappings (see Equation 22). Attentional Bias ℓ (; ) Optimizer Dynamic Decay Deep Memory Non-linear Capacity Flexible Locally Optimal Context Memory Write Operation Model Attention SWA Linear Attention RetNet GLA PolySketchFor. TTT DeltaNet Longhorn Gated DeltaNet RWKV-7 Titans Titans Moneta Memora (cid:205)𝐿 (cid:205)𝐿 𝑎𝑡 Mk𝑡 v𝑡 2 𝑡 =1 2 𝑡 =𝑐 𝑎𝑡 Mk𝑡 v𝑡 2 2 M𝑡 k𝑡, v𝑡 M𝑡 k𝑡, v𝑡 M𝑡 k𝑡, v𝑡 𝑝 𝑡 , v𝑡 M𝑡 M𝑡 (k𝑡 ) v𝑡 2 2 M𝑡 k𝑡 v𝑡 2 2 M𝑡 k𝑡 v𝑡 2 2 M𝑡 k𝑡 v𝑡 2 2 M𝑡 k𝑡 v𝑡 2 2 M𝑡 (k𝑡 ) v𝑡 2 2 M𝑡 (k𝑡 ) v𝑡 2 2 𝑝 M𝑡 (k𝑡 ) v𝑡 𝑝 M𝑡 (k𝑡 ) v𝑡 2 2 DLA DeepTransformer SWDT OmegaNet Dot M𝑡 (𝜙 (k𝑡 )), v𝑡 M𝑡 (𝜙 (k𝑡 )), v𝑡 (cid:205)𝐿 (cid:205)𝐿 (cid:205)𝐿 𝑖=𝑐 M𝑡 (𝜙 (k𝑖 )), v𝑖 𝑖=𝑐 𝛾𝑖 M𝑡 (𝜙 (k𝑖 )) v𝑖 2 2 𝑖=𝑐 𝛾𝑖 M𝑡 (𝜙 (k𝑖 )) v𝑖 2 2 Atlas (cid:205)𝐿 𝑖=𝑐 𝛾𝑖 M𝑡 (𝜙 (k𝑖 )) v𝑖 2 2 Muon NP NP GD GD GD GD GD GD Implicit GD GD GD GD w/ M. GD GD GD GD GD GD GD GD Our Models 𝑡 )𝑝 M𝑡 = M𝑡 1 {(k𝑡, v𝑡 )} M𝑡 = (M𝑡 1 {(k𝑐, v𝑐 )}) {(k𝑡, v𝑡 )} M𝑡 = M𝑡 1 + v𝑡 𝑡 M𝑡 = 𝛼M𝑡 1 + v𝑡 𝑡 M𝑡 = Diag(𝛼𝑡 )M𝑡 1 + v𝑡 𝑡 M𝑡 = M𝑡 1 + v𝑡 (k M𝑡 = M𝑡 1 𝜂ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = (I 𝛽𝑡 k𝑡 𝑡 )M𝑡 1 + 𝛽𝑡 v𝑡 𝑡 M𝑡 = (I 𝛿𝑡 k𝑡 k) M𝑡 1 + (𝛿𝑡 v𝑡 ) k𝑡 M𝑡 = 𝛼𝑡 (I 𝛽𝑡 k𝑡 M𝑡 = (diag(𝛼𝑡 ) 𝛽𝑡 k𝑡 M𝑡 = 𝛼𝑡 M𝑡 1 + S𝑡 S𝑡 = 𝜂𝑡 S𝑡 1 𝜃𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑖 1; k𝑡, v𝑡 ) M𝑡 = 𝜎 (𝛼𝑡 log(M𝑡 1) 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 )) 𝑡 )M𝑡 1 + 𝛽𝑡 v𝑡 𝑡 𝑡 )M𝑡 1 + 𝛽𝑡 v𝑡 𝑡 M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 NS-5(S𝑡 ) S𝑡 = 𝜃𝑡 S𝑡 1 ℓ (M𝑡 1; k𝑡, v𝑡 ) The matrix-valued memory version is considered. NP: Nonparametric 𝛿𝑡 = 𝛽𝑡 1+𝛽𝑡 𝑡 k𝑡 . Gradient Descent with Momentum. Without Normalization. Behrouz, Zhong, et al. 2024; Wen et al. 2024; Yang, Kautz, et al. 2024). We observe these shortcomings arise from three design aspects: (1) The online nature of their memory update, where memory is optimized based on the current token while retaining past memory state, leading to memorization of individual tokens without considering broader context; (2) The limited capacity of memory, where architecture and key-value feature mappings restrict the number of perfectly mappable key-value pairs; and (3) The expressiveness of memory management (i.e., the internal objectives optimizer), as most recent models use gradient descent that relies on the first-order information about the dynamics of tokens, causing the memory to converge to spurious local minima and learn less effective key-value mappings."
        },
        {
            "title": "Memory Perspective",
            "content": "Associative memorythe ability to map different entities or eventsis an inseparable component of learning in humans (Terry 2017) and so has motivated several recent studies to understand the state-of-the-art deep learning architectures through its lens (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Ramsauer et al. 2021; Wang et al. 2025). In this perspective, memory is defined as neural update caused by an input; the more surprising the input is, the more it affects the memory and so is memorable. Therefore, finding an effective surprise metric is critical step towards designing such memory modules. As earlier discussed by Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al. (2024), almost all existing architectures use surprise metric that updates the memory based on the current input. An event (as sequence of tokens), however, might not consistently be surprising through long-period of time although it is memorable. To overcome this issue, Behrouz, Zhong, et al. (2024) suggest breaking the surprise metric into two parts of momentary and past surprise, incorporating the cumulative surprise of past inputs when updating the memory with respect to the current input. This design, however, can miss the context by memorizing individual tokens. To this end, in this work, we present long-term neural memory module that measures the surprise of local (or global) context window, meaning that it learns how to memorize the (token) context at test time. Through the paper, we use the terminology Test Time Memorization because the process involves storing and retrieving in2 formation strictly within the global context, without updating the models core learned parameters (i.e., outer-loop) or initial states from pre-training. Typically, no persistent learning or skill acquisition carries over to new, independent global context once the memory is cleared. Thus, we prefer the use of \"test time memorization\" over using \"test time training\"."
        },
        {
            "title": "Contributions",
            "content": "In this paper, we aim to overcome the abovementioned limitationsi.e., (1) online nature, (2) limited memory capacity, and (3) less expressive memory managementby designing long-term neural memory module with high capacity and the ability to memorize the context, instead of tokens. We further build upon these insights and present family of strictly more powerful Transformers. More specifically: Better Understanding of Memory Capacity and its Bottleneck. To improve the limited memory capacity, we suggest using higher-order feature mappings (e.g., polynomial feature kernels) on input tokens. We provide theoretical justifications on why deeper memory modules and/or higher-order feature mapping can enhance memory capacityi.e., the maximum number of linearly independent key-value associations the memory can perfectly map. New Expressive Learning Rule. To overcome the online nature of recent recurrent models, this work presents sliding window update rule, called Omega rule, that optimizes and updates memory based on all past tokens in given context window, not just the last. This allows the model to better manage its fixed-size memory and memorize local context instead of individual tokens. Strict Generalization of Transformers. Next, we show how our Omega rule formulation connects to global and local softmax attentions (i.e., Sliding Window Attention - SWA) and present new family of Transformer-like architectures, called DeepTransformers and its sliding window variants SWDT, that strictly generalize Transformers (Vaswani et al. 2017). We further present novel baseline of Deep Linear Attention (DLA) to demonstrate the role of deep memory. New Memory Modules with Better Memory Management. Building upon the above improvements, we present OmegaNet, new architecture using polynomial features on its keys and queries, while updating its memory based on Omega and gradient descent. To further enhance memory management, we introduce Atlas, which leverages the popular Muon optimizer (Jordan et al. 2024) for updating the internal memory. We show that both OmegaNet and Atlas can take advantage of parallelizable training algorithms, resulting in fast training without substantial overhead compared to the online version (i.e., context window = 1). To the best of our knowledge, Atlas is the first parallelizable recurrent architecture that optimizes the memory using the (approximation) of second-order information (i.e., has locally optimal memory module). Improvement on Diverse Downstream Tasks. Extensive experiments validate our model designs and proposed techniques, including ablations of modern architectures. We evaluated DeepTransformers, OmegaNet, and Atlas on diverse benchmarkslanguage modeling, common-sense reasoning, recall-intensive, and needle-in-haystack taskswhere they outperformed modern linear RNNs, local attention (SWA), and Transformers. Furthermore, we studied the effects of memory architecture, feature mapping, memory management algorithm (internal optimizer), and Omega rule on memory module capacity and performance in long-context understanding tasks. Proofs, additional experimental results, discussions on related work, and the details of experiments are in Appendix."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we first discuss the notation that we use through the paper and then review the background concepts and related work. Additional discussion on related studies are in Appendix A. Notations. We let 𝑥 R𝑁 𝑑in be the input, M𝑡 be the state of memory at time 𝑡, be the keys, be the values, and be the query matrices. We use bold lowercase letters with subscript 𝑡 to refer to vectors correspond to time 𝑡 (i.e., k𝑡, v𝑡 , and q𝑡 ). Following Behrouz, Razaviyayn, et al. (2025), we use ℓ (M𝑡 ; k𝑡, v𝑡 ) to refer to the attentional bias (i.e., the internal memory objective). Through the paper, we use simple MLPs with LM 1 layers and residual connection as the architecture of the memory module (). Notably, despite this choice, all of our model formulations are simply adaptable to other memory architecture choices; e.g., linear matrix-valued memory (LM = 1). When it is needed, we 3 parameterized the memory module with 𝜽 := {𝑊1, . . . ,𝑊LM in the MLP. , . . . }, which at least includes the parameters of linear layers"
        },
        {
            "title": "2.1 Backgrounds",
            "content": "Attention. Attention is critical component of Transformers that acts as their associative memory (Behrouz, Razaviyayn, et al. 2025; Bietti et al. 2023; Sun, Li, et al. 2024). Given input 𝑥 R𝑁 𝑑in, causal attention computes output R𝑁 𝑑in over input dependent key, value, and query matrices = 𝑥WQ, = 𝑥WK, and = 𝑥WV as: y𝑖 = 𝑖 𝑗=1 (cid:16) exp 𝑖 k𝑗 / (cid:16) (cid:17) v𝑗 𝑑in 𝑑in 𝑖 kℓ / (cid:205)𝑖 ℓ=1 exp (cid:17) = 1 𝑍𝑖 𝑖 𝑗=1 (cid:16) 𝑖 k𝑗 /𝑑in (cid:17) v𝑗, exp (1) where WQ, WK, and WV R𝑑in 𝑑in are learnable parameters, and 𝑍𝑖 = (cid:205)𝑖 is the normalization term. Despite Transformers simple parallelizable training and effectiveness in recall-intensive tasks (Arora, Eyuboglu, Zhang, et al. 2024), their generation process and long-context scaling are significant drawbacks, as attention requires at least 𝑁 𝑑 operations per token to calculate the output (see Equation 1). Therefore, in recent years, there have been an extensive research effort to design alternative architectures. We divide and review these studies into two groups: (1) Linear shallow memory recurrent models, (2) Deep memory modules: ℓ=1 exp 𝑖 kℓ / (cid:16) (cid:17) 𝑑in (Linear) Recurrent Models. Linear RNNs have recently gained attention as efficient Transformer alternatives due to their parallelizable, linear-time training and comparable performance (Peng, Alcaide, et al. 2023; Sun, Dong, et al. 2023). Early modern RNN variants, often based on Hebbian (Hebb 2005) or Delta (Widrow et al. 1988) learning rules, compress data into vector-valued or matrix-valued memory (Kacham et al. 2024b; Katharopoulos et al. 2020; Lim et al. 2024; Liu, Wang, et al. 2024; Schlag et al. 2021; Sun, Dong, et al. 2023). Let M𝑡 R𝑑 𝑛 be the memory (where 𝑛 = 1 yields vector-valued memory), and k, R𝑑 be the keys and values (projections of input 𝑥𝑡 R𝑑 )). simple general formulation for such linear RNNs is: M𝑡 = 𝐴𝑡 M𝑡 1 + v𝑡 𝑡 , (2) where is an arbitrary associative operator and 𝐴𝑡 is data-(in)dependent diagonal or low-rank plus identity matrix (Yang, Wang, Zhang, et al. 2024). Despite the efficient linear recurrent nature of these models, their memory can overflow, particularly with increasing context length. Although forget gates have recently significantly improved memory management in these architectures (Peng, Zhang, et al. 2025; Sun, Dong, et al. 2023), their memorys expressivity remains bounded by its linear structure. Deep Memory Module. To overcome the limited expressivity of memory and to enhance the effective context length recurrent models, recent studies focus on new line of architectures with deep memory modules (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Irie et al. 2021; Sun, Li, et al. 2024). These architectures are built on the meta-learning perspective, where the memory is deep MLP architecture updated by gradient descent (with momentum). Recently, Behrouz, Razaviyayn, et al. (2025) present framework to accurately unifies popular sequence models as the instances of test time memorization. That is, sequence models are associative memory modules that aim to learn the underlying mapping between given keys and values by optimizing an internal memory objective, called attentional bias. This optimization is based on an iterative optimization algorithms such as gradient descent. More formally, associative memory is defined as: Definition 1 (Behrouz, Razaviyayn, et al. (2025)). Given set of keys R𝑑𝑘 and values R𝑑𝑣 , associative memory is an mapping : V. Learning the associative memory is based on an objective L, called Attentional Bias, that determines the type of memory and its priorities: = arg min (M (K); V). (3) Optimizing this objective using an iterative algorithm (e.g., gradient descent) results in the memory update rule. Thus, the sequence model is meta in-context learner with two optimization levels: 4 Figure 1: Comparison of learning to memorize (Left) individual tokens, and (Right) the context. 1. Inner Loop: Where parameters of the memory module are optimized (i.e., 𝜽 = {𝑊1,𝑊2, . . . ,𝑊LM,...}). In the inner optimization loop, all other parameters from the model are considered hyperparameters and are fixed and not optimized. 2. Outer Loop: Where all other parameters of the model are optimized, such as linear projections, MLP layers, convolutions, etc. Our terminology builds on this framework. Therefore, instead of full recurrent formulations, we describe models by their: (1) memory architecture, (2) internal objective (i.e., attentional bias), and (3) memory learning algorithm (optimizer). In most cases, models use matrix-valued memory with online gradient descent; for brevity in such instances, we refer to an architecture solely by its internal memory objective. For additional discussions and examples, see Appendix B."
        },
        {
            "title": "3 Learning to Memorize the Context at Test Time",
            "content": "Long-term associative memory, crucial for human learning (Terry 2017), has inspired many artificial neural architectures (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; He et al. 2024; Hopfield 1982; Krotov and Hopfield 2016; Ramsauer et al. 2021; Schmidhuber and Hochreiter 1997). While many such models use matrixor vector-valued memory to compress past data (Schlag et al. 2021; Von Oswald et al. 2023; Yang, Kautz, et al. 2024), recent studies advocate for deep non-linear neural memory that encodes past abstractions into its parameters (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Dalal et al. 2025; Sun, Li, et al. 2024). For long-context reasoning/understanding, however, these long-term neural memory modules still require: (1) High capacitythe maximum (key, value) pairs storable in parameters (see 3.1); (2) powerful internal memory objective (i.e., attentional bias) to learn complex mapping between keys and values (see 3.2); (3) Powerful memory management for better fixed-size memory management (see 3.2); and (4) An efficient parallel training process for large-scale training on modern accelerators (see 3.3). This section further discusses these challenges and presents Omega rule: an expressive memory update rule with direct access to tokens in local context window, which memorizes context rather than individual tokens."
        },
        {
            "title": "3.1 Associative Memory with Super Linear Capacity",
            "content": "As previously discussed, an effective long-term memory module should store past data abstractions in its parameters. However, with fixed number of memory parameters, key unanswered question remains: what is the maximum number of uncorrelated (key, value) pairs that model can store? To answer this, we start with the simplest case: matrix memory, an ℓ2 regression loss as the attentional bias (i.e., ℓ (M𝑡 ; k𝑡, v𝑡 ) = M𝑡 (k𝑡 ) v𝑡 2 Proposition 1 (Capacity of ℓ2 Attentional Bias). Let be matrix-valued memory with 𝑑𝑣 𝑑𝑘 parameters that optimizes the internal objective of ℓ (M𝑡 ; k𝑡, v𝑡 ) = M𝑡 k𝑡 v𝑡 2 2 with gradient descent. can store the mapping of at most (𝑑𝑘 ) pairs of (k𝑖, v𝑖 ) with linearly independent keys. 2), optimized by gradient descent: The above proposition indicates that matrix-valued memory with delta update rule has sub-linear capacity with respect to its number of parameters. This means that the number of independent patterns that can be stored in fixed-size memory with size 𝑀 is strictly less than 𝑐 𝑀, for some 𝑐 R+. Recent recurrent models suggest using deep memory modules to 5 store the abstraction of the past into the parameters of deep neural network (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Irie et al. 2021; Sun, Li, et al. 2024). While these deep memory architectures can intuitively enhance the expressive power in modeling complex underlying mapping patterns between keys and values, it is still unclear that if they enhance the memory capacity. Theorem 1 (Effect of Deep Memory). Let () be an MLP with LM 2 layers, 𝑑𝑘 input dimension, and 𝑑ℎ hidden dimension. Then, () can store the mapping of at least (𝑑𝑘𝑑𝑣) and at most pairs of (k𝑖, v𝑖 ) with linearly independent keys. 𝑖=1 min{𝑑 ( 𝑗 ) ℎ } 𝑗 𝑖𝑑 ( 𝑗+1) (cid:16) 𝑑𝑘𝑑𝑣 (cid:205)LM (cid:17) ℎ This theorem indicates that deep memory not only improves representational power but also further boosts network capacity, with advantages growing with depth. However, the upper bound remains subquadratic in key and value dimensions, raising the question if long-term memory module can achieve super-linear capacity. As stated earlier, the dimension of k𝑡 is crucial for increasing memory capacity. Simply increasing all key and value dimensions, however, significantly increase the number of parameters (O (𝑑in) per each extra dimension) and memory usage, particularly with long contexts. To address this, building on methods from Kacham et al. (2024a) and Krotov and Hopfield (2016), we suggest using separable kernels 𝜎 (𝑥, 𝑦) = 𝜙 (𝑥)𝜙 (𝑦) for keys and queries. As an example of such kernels, we focus on polynomial kernels of degree at most 𝑝 to increase input dimensionality and thus network capacity. Given 𝑝 N, let 𝜙𝑝 (𝑥) = [𝑥 𝛽 ] 𝛽 𝑝 be polynomial mapping of 𝑥 with degree at most 𝑝. We redefine the associative memory module in Definition 1 by replacing the inner objective of (M (K); V) with (M (𝜙 (K)) ; V). This polynomial mapping enhances representational power by increasing the effective dimensionality of keys without additional parameter overhead for the input projections. Next, we discuss their effect on memory capacity, even with single matrix-valued memory: Proposition 2 (Memory Capacity with Polynomial Mapping). Let 𝜙𝑝 () be polynomial mapping with degree at most 𝑝, and be matrix-valued memory that optimizes the internal objective of ℓ (M𝑡 ; 𝜙𝑝 (k𝑡 ), v𝑡 ) = M𝑡𝜙𝑝 (k𝑡 ) v𝑡 2 2 with gradient descent. can store the mapping of at most (cid:0)𝑑𝑘 𝑝 (cid:1) pairs of (k𝑖, v𝑖 ) with linearly independent keys, where 𝑑𝑘 is the dimension of keys k𝑖 . Beyond the above intuition, polynomial kernels are further motivated by two perspectives: (1) Approximating Softmax using Taylor series; and (2) Input feature gating. For the sake of clarity, we continue with linear memory and two popular attentional biases i.e., ℓ (1) (M𝑡 ; k𝑡, v𝑡 ) = M𝑡 k𝑡, v𝑡 and ℓ (2) (M𝑡 ; k𝑡, v𝑡 ) = M𝑡𝜙 (k𝑡 ) v𝑡 2 2. The same process can be applied on other attentional objectives and deep memory modules. Optimizing these objectives using gradient descent in the inner loop results in the following recurrent formulas: ℓ (1) (M𝑡 ; k𝑡, v𝑡 ) ℓ (2) (M𝑡 ; k𝑡, v𝑡 ) : M𝑡 = M𝑡 1 + 𝜂𝑡 v𝑡𝜙 (k𝑡 ), : M𝑡 = (cid:0)I 𝜂𝑡𝜙 (k𝑡 )𝜙 (k𝑡 )(cid:1) M𝑡 1 + 𝜂𝑡 v𝑡𝜙 (k𝑡 ). (Hebbian Rule) (Delta Rule) Kernel Attention Perspective for the Special Case of Hebbian Rule. The formulation for (Hebbian Rule) is equivalent to kernel linear attentions (Arora, Eyuboglu, Zhang, et al. 2024; Hua et al. 2022; Kacham et al. 2024b; Kasai et al. 2021; Katharopoulos et al. 2020; Wang et al. 2025). In this viewpoint, the role of 𝜙 (.) is to approximate Softmax or more accurately the exponential kernel. Since exponential kernel with normalization (i.e., Softmax) is not separable, it results in Transformers quadratic time and memory complexity. However, Transformers exponential feature map kernel (exp(q 𝑖 k𝑗 )) can be approximated using its Taylor series as: exp (cid:0)q 𝑖 k𝑗 (cid:1) 1 + 𝑖 k𝑗 + (q 𝑖 k𝑗 )2 2! + (q 𝑖 k𝑗 )3 3! + . . . Our polynomial feature map extends this approximation to more general case of: exp (cid:0)q 𝑖 k𝑗 (cid:1) 𝜙𝑝 (q)𝜙 (k) = 𝑎0 + 𝑎1q𝑖 𝑗 + 𝑎2(q 𝑖 k𝑗 )2 + 𝑎3(q 𝑖 k𝑗 )3 + + 𝑎𝑝 (q 𝑖 k𝑗 )𝑝, (4) (5) with learnable parameters 𝑎𝑖 initialized at 𝑎𝑖 = 1 𝑖! , the polynomial kernel can be viewed as an expressive approximator of Softmax attention. This provides theoretical motivation for using polynomial kernels, especially when memory capacity is limited; i.e., with (i) linear memory and (ii) Hebbian learning rule. This intuition, however, further generalizes to more expressive cases using deep memory modules and more complex attentional biases (i.e., Eq. Delta Rule). That is, exp() feature mapping has infinite dimension and provides more powerful similarity measure of keys and queries (i.e., 𝑖 k𝑗 ); however, its computation with normalization can cause additional memory and time complexity to the model. Using polynomial kernels in architectures with deep memory and complex attentional bias can further enhance performance by approximating more powerful representations for keys-queries similarities (i.e., 𝑖 k𝑗 ). See Section 4 for additional discussions on exponential kernels and Transformers. Input Gating Interpretation. Another perspective that motivates the use of polynomial features is their more expressive representational power in modeling complex functions compared to the simple case of 𝜙 (𝑥) = 𝑥. That is, the coefficients of 𝑎𝑖 can be seen as input feature gating, in which 𝑎𝑖 0 means excluding the feature map of [𝑥 𝑗 ] 𝑗 =𝑖 , and 𝑎𝑖 1 means retaining the corresponding feature. This is similar to the gating mechanisms of RNNs but on the input rather than the memory. This gating mechanism clearly provides more representational power as the model can learn to set 𝑎𝑖 0 for all 𝑖 1 and 𝑎1 1, resulting in the simple case of 𝜙 (𝑥) = 𝑥."
        },
        {
            "title": "3.2 Long-term Memory with Context Memorization",
            "content": "As discussed earlier, one of the critical drawback of most existing recurrent models is their online nature, in which they optimize the inner objective (attentional bias) with respect to only the current input while retaining the previous state of the memory (Behrouz, Razaviyayn, et al. 2025; Liu, Wang, et al. 2024), i.e., ℓ (M; k𝑡, v𝑡 ) + Ret𝑡 (M, M𝑡 1), min (6) where Ret(, ) is the retention gate. This online nature while making the optimization of the memory simpler and faster, can cause sub-optimal memorization of the context as memory is greedily memorize individual tokens. In more general case, however, one can optimize the memory at each time stamp with respect to the entire context (input sequence), i.e., 𝑡 𝑖=1 min ℓ (M; k𝑖 ; v𝑖 ). (7) This strict global formulation generally presents two critical limitations: (1) Efficiency: One of the important advantages of recurrent architectures is their efficiency at longer context in both training and inference. Optimizing the memory with respect to all the past tokens (entire context), however, (i) causes additional optimization constraints at each memory update step, resulting in inefficiency at extremely large sequences, and (ii) requires caching the past keys and values at the test time, increasing the memory consumption; (2) Context Pruning: In large context tasks optimizing with all past tokens can cause sub-optimal performance mainly due to the context change (or irrelevant context) in the middle of the input sequence. This observation has resulted to design architectures with retention (forget) gate, enabling models to erase memory when past context is no longer needed (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Peng, Zhang, et al. 2025; Sun, Dong, et al. 2023; Yang, Wang, Shen, et al. 2024). To address these limitations, we present sliding window recurrent model that optimizes its attentional bias w.r.t. window of past tokens. For memory module () and window length 𝑐 1, we optimize the memory internal objective as: 𝑡 𝑖=𝑡 𝑐+1 min 𝛾 (𝑡 ) 𝑖 ℓ (M; k𝑖, v𝑖 ), (8) where ℓ (M; k𝑖, v𝑖 ) measures the predicted mapping for (k𝑖, v𝑖 ) pair and 𝛾 (𝑡 ) is the decay term for the effect of 𝑖-th token in the optimization process. Building upon this formulation, we present Omega rule, which is strictly more powerful than the popular Delta learning rule (Schlag et al. 2021; Widrow et al. 1988): 𝑖 Omega Rule: Let k𝑖 R𝑑𝑘 and v𝑖 R𝑑𝑣 be the input keys and values, and () be neural architecture that serves as the memory module. Given local context length of 𝑐 N1, the updating the memory module using Omega learning rule is defined as optimizing the following loss function with gradient descent: 𝑡 𝑖=𝑡 𝑐+1 min 𝛾 (𝑡 ) 𝑖 (k𝑖 ) v𝑖 2 2 7 (9) Figure 2: The illustration of tokens dependencies in SWA and Atlas or OmegaNet with different context length. Following Behrouz, Razaviyayn, et al. 2025, this update rule can be extended to 𝑞-Omega rule (or other variants) by replacing ℓ2() with ℓ𝑞 (). In the extreme cases of (1) 𝑐 = 1: the update rule becomes online (Delta rule); and (2) 𝑐 = or context length: the update becomes global optimization w.r.t. all past tokens. In this formulation, parameters 𝛾 (𝑡 ) [0, 1] act as hard (direct) gates for the past tokens. That is, 𝛾 (𝑡 ) 𝑖 0 means that the model directly prunes the optimization of 𝑖-th token in the local context, while 𝛾 (𝑡 ) 𝑖 1 means fully incorporating the optimization of memory for 𝑖-th token in the local context. In our design, we use input-dependent parameters for 𝛾 (𝑡 ) , providing in-context pruning ability. Note that, the design of sliding window recurrence allows such flexibility as for each token we need constant number of gates; i.e., {𝛾 (𝑡 ) }𝑐 𝑖=1. Using input-dependent gates for the global optimization (Equation 7), however, can result in significant parameter increase and memory usage, diminishing the advantages of recurrent models. 𝑖 𝑖 𝑖 OmegaNet. We now present OmegaNet, novel sequence model that updates its memory using Omega rule. To enhance the memory capacity of OmegaNet, we use polynomial kernels on ks and qs. Accordingly, optimizing the objective in Equation 9, results in an update rule of OmegaNet as: M𝑡 = 𝛼𝑡 M𝑡 1 𝑡 𝛾 (𝑡 ) 𝑖 (𝜙 (k𝑖 )) v𝑖 2 2 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) , (cid:125) 𝑖=𝑡 𝑐+1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) Surprise of the context or in the spacial case of linear memory: (cid:32) M𝑡 = diag(𝛼𝑡 ) 𝑡 𝑖=𝑡 𝑐+1 𝛾 (𝑡 ) 𝑖 𝜙 (k𝑖 )𝜙 (k𝑖 ) (cid:33) M𝑡 1 𝑡 𝑖=𝑡 𝑐+ 𝛾 (𝑡 ) 𝑖 v𝑖𝜙 (k𝑖 ). (10) (11) From the memory perspective, Omega rule (OmegaNet) does not measure the surprise of token, but the surprise of local context based on the context-aware combination of individual tokens within the context. Beyond Gradient Descent. The concept of Omega rule and test time memorization of context can simply be extended to optimizing the objective in Equation 9 with any arbitrary optimizer, even beyond simple gradient descent. We use two extreme cases for 𝑐 as the illustrations. In the first case, we let 𝑐 = 1, 𝛾 (𝑡 ) 𝑖 = 1, and use gradient descent with momentum as the optimizers, resulting in the following update rule: M𝑡 = 𝛼𝑡 M𝑡 1 + S𝑡 S𝑡 = 𝜃𝑡 S𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ). (12) (13) This update rule is equivalent to the long-term neural memory in Titans (Behrouz, Zhong, et al. 2024). In the second case, using linear memory M, letting 𝛾 (𝑡 ) 𝑖 = 1, and 𝑐 be equal to the context length, the memory update process is equivalent to optimizing the (regularized) least-squares problem: M𝑡 = min 𝑡 𝑖= Mk𝑖 v𝑖 2 2 . 8 (14) Von Oswald et al. (2023) suggest directly optimizing the above objective and use Sherman-Morrison formula (Sherman et al. 1950) to recursively calculate the inverse term in the optimal solution. Despite the optimality of memory, such direct solutions comes with the cost of non-parallelizable training and also are limited to only the linear matrix-valued memory setup. Furthermore, as discussed earlier, the global nature without any direct hard gating terms (i.e., 𝛾 (𝑡 ) s) can force the model to not prune the context, damaging the performance in longer sequences. 𝑖"
        },
        {
            "title": "3.3 Parallelizing Omega Rule",
            "content": "While Omega rule provides more general and expressive formulation for the design of memory modules than Hebbian or Delta learning rules, its applicability to large-scale models relies on its efficiency in training. To this end, we discuss fast parallelizable training algorithms that does not add any significant computational overhead with the online counterpart version (i.e., 𝑐 = 1). naive implementation requires materializing 𝑐 gradients ℓ R𝑑in 𝑑in, which can result in significantly higher memory footprint and I/O cost when 𝑑in is large. Also, to fully utilize hardware accelerators such as TPUs and GPUs, it is important to tensorize computations and maximize the use of matmul operations. Motivated by recent work (Behrouz, Zhong, et al. 2024; Sun, Li, et al. 2024), we propose simple sliding window masking strategy that supports efficient parallel training while avoiding substantial memory overhead. Specifically, we partition the input sequence with length 𝐿 into chunks of size 𝑏 1, each of which is represented by S𝑖 = {x(𝑖 1)𝑏+1, . . . , x𝑖𝑏 }. Then for each chunk, we calculate the gradients with respect to the last state of the previous chunk. For the sake of clarity, we first assume 𝛾 (𝑡 ) 𝑖 = 𝜂𝑡 for all positions in the sequence. When the chunk size is 𝑏 = 1, the update rule is: M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 𝑡 ℓ (M𝑡 1; k𝑖, v𝑖 ), (15) 𝑖=𝑡 𝑐+1 where M𝑡 is the model state at step 𝑡, 𝛼𝑡 and 𝜂𝑡 are the weight decay and learning rate parameters respectively, and (k𝑖, v𝑖 ) denote the input pair at position 𝑖. In practice, we strike balance between the fully recurrent form and the fully parallel form by dividing the sequence into smaller chunks. Within each chunk (intra-chunk), we apply parallel computation, while across chunks (inter-chunk), we adopt recurrent computation scheme. We now define 𝑡 = 𝑡 mod(𝑡, 𝑏). That is, for time steps 𝑡 such that 𝑡 𝑡 < 𝑡 + 𝑏, the update rule within each chunk becomes: M𝑡 = 𝛼𝑡 ...𝛼𝑡 M𝑡 𝑡 𝑛=𝑡 𝛼𝑡 ...𝛼𝑡 𝛼𝑛...𝛼𝑡 𝜂𝑛 𝑛 𝑖=𝑛𝑐+1 (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ℓ (M𝑡 ; k𝑖, v𝑖 ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) 𝐺𝑡 (16) In our implementation, for 𝐺𝑡 , we follow the same gradient computation approach as described in Titans (Behrouz, Zhong, et al. 2024) but additionally apply sliding window mask 𝑀𝑠 during the broadcasting operation (e.g., using einsum). When 𝑐 = 1, the sliding window mask 𝑀𝑠 reduces to the identity matrix. For 𝑐 > 1, 𝑀𝑠 is an identity matrix except that the 𝑐 1 positions immediately preceding each diagonal entry are also set to 1. This allows gradient contributions from window of size 𝑐, enabling efficient computation without materializing all gradients inside the chunk."
        },
        {
            "title": "4 DeepTransformers: Transformers with Deep Memory",
            "content": "Recent studies have extensively discussed Transformer architectures through the lens of associative memory (Behrouz, Razaviyayn, et al. 2025; Sun, Li, et al. 2024; Wang et al. 2025). Accordingly, it is natural to ask how our discussions of memory capacity as well as Omega rule can affect Transformers. In this section, we discuss how our formulation of Omega rule is connected to Transformers and their sliding window counterparts (i.e., SWA). We then further provide two extensions to Transformers, each of which is strict generalization of Transformers."
        },
        {
            "title": "4.1 Online and Local Context Optimization of Memory",
            "content": "Connection to Sliding Window Attention. Softmax attention block can also be reformulated as non-parametric solution to the ℓ2() regression with Nadaraya-Watson estimators (Fan 2018; Zhang et al. 2022): 𝐿 𝐿 = arg min s(k𝑖, q)v𝑖 2 2 = s(k𝑖, q) 𝑗=1 s(k𝑗, q) (cid:205)𝐿 v𝑖, 𝑖=1 (17) 𝑖=1 where 𝐿 is the sequence length. While this formulation optimizes the memory with respect to the entire sequence length, one can limit the optimization process to the past 𝑐 tokens, resulting in: = arg min 𝑡 𝑖=𝑡 𝑐+1 s(k𝑖, q𝑖 )v𝑖 2 2 = 𝑡 𝑖=𝑡 𝑐+1 s(k𝑖, q) 𝑗=𝑡 𝑐+1 s(k𝑗, q) (cid:205)𝑡 v𝑖, (18) which is equivalent to the sliding window attention (SWA). This connection provides an important insight on the difference of attention and recurrent models: Not only attention is non-parametric solution (contrary to the parametric nature of recurrent models), it globally optimizes its internal objective (attentional bias), while most recent modern recurrent models are online learners (Behrouz, Razaviyayn, et al. 2025; Peng, Zhang, et al. 2025; Sun, Li, et al. 2024; Yang, Kautz, et al. 2024)1 . Our formulations of sliding window RNN and Omega rule fill this gap by optimizing the memory with respect to context window of past tokens based on parametric methods, effectively memorizing the context instead of individual tokens. Deep Linear Attention. As novel baseline, we present Deep (Gated) Linear Attention (DLA) that replaces matrix-valued memory in (gated) linear attention (Katharopoulos et al. 2020; Yang, Wang, Shen, et al. 2024) with deep neural network (e.g., 𝑘-layer MLP). As discussed earlier in (Hebbian Rule), using dot product similarity as the internal attentional bias results in linear attention. Thus, leveraging recent deep memory modules (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Sun, Li, et al. 2024), we optimize the memory using gradient descent with dot product attentional bias: M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; 𝜙 (k𝑡 ), v𝑡 ), (19) where ℓ (M𝑡 1; 𝜙 (k𝑡 ), v𝑡 ) = M𝑡 1(𝜙 (k𝑡 )), v𝑡 and 𝜙 () is polynomial kernel. The training of DLA can simply be parallelized using the hybrid of linear and non-linear chunk-wise training, the same as Behrouz, Zhong, et al. (2024) and Sun, Li, et al. (2024) and our discussion in Section 3.3. Sliding Window Linear Attention. Building upon the above intuition and the connection of our formulation to SWA, we present Sliding Window Linear Attention (SWLA) block. Following the formulation of linear attention in associative memory perspective (Behrouz, Razaviyayn, et al. 2025), we use dot product similarity (i.e., ℓ (M𝑡 ; k𝑖, v𝑖 ) = M𝑡 (k𝑖 ), v𝑖 ) as the attentional bias and optimize the loss function using gradient descent. For the sake of clarity, we use linear memory here to derive the closed form: M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 𝑡 𝑖=𝑡 𝑐+1 ℓ (M𝑡 1; 𝜙 (k𝑖 ), v𝑖 ) = M𝑡 1 + 𝑡 𝑖=𝑡 𝑐+ 𝛾 (𝑡 ) 𝑖 v𝑖𝜙 (k𝑖 ) In the online case (𝑐 = 1) and 𝜙 () = (), this recurrence is the same as linear attention (Katharopoulos et al. 2020)."
        },
        {
            "title": "4.2 Memory Capacity and Exponential Kernels",
            "content": "We first recall the formulation of softmax attention in Transformers (i.e., Equation 1): y𝑖 = (cid:205)𝑖 ℓ=1 exp 1 𝑖 kℓ / (cid:16) (cid:17) 𝑑in 𝑖 𝑗=1 (cid:16) 𝑖 k𝑗 /𝑑in (cid:17) v𝑗, exp (20) (21) which its exp() kernel is not separable and so cannot be written as recurrence. Following the discussion in Kacham et al. (2024b), one can see exp() kernel (compared to polynomial kernel 𝜙𝑝 ()) as feature map that maps the input into an 1Two of the exceptions are Titans (Behrouz, Zhong, et al. 2024) and Mesa-layer (Von Oswald et al. 2023), where Mesa-layer optimizes the memory with respect to all past tokens (comes with the cost of slow training), and Titans optimizes the memory with respect to all past tokens but with an implicit decay term (i.e., the result of the momentum) for each past token, maintaining parallelizability. 10 infinite dimension. That is, we define: (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) where 𝑥 𝑝 = 𝑥 𝑥 (𝑝 1) is self-tensoring operator with Kronecker product (Kacham et al. 2024b) and so: 𝜙𝑝 (𝑥) = 𝑥 𝑝, 𝜙 (𝑥) = (cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:171) 3!... , 1 𝑥 1 𝑥 2 2! 𝑥 3 exp(q 𝑡 k𝑡 ) = 𝜙 (q𝑡 )𝜙 (k𝑡 ). (22) (23) Based on the above kernel, we can reformulate the attention (see Equation 21) as: (we remove 1/ simplicity) 𝑑in term for the sake of y𝑖 = (cid:205)𝑖 ℓ=1 exp 1 𝑖 kℓ / 𝑑in (cid:16) (cid:17) 𝑖 𝑗=1 v𝑗𝜙 (k𝑗 )𝜙 (q𝑖 ) = 1 𝑖 kℓ / (cid:16) (cid:17) 𝑑in (cid:32) 𝑖 𝑗=1 (cid:205)𝑖 ℓ=1 exp (cid:33) 𝜙 (v𝑗 k𝑗 ) 𝜙 (q𝑖 ) = M𝑖𝜙 (q𝑖 ), (24) This formulation, provides another important insight on the differences of attention and (kernel) recurrent models: Softmax attention as an associative memory has an unbounded memory and so can better memorize larger context into its parameters. Building upon this insight, we present DeepTransformers by replacing polynomial kernel with 𝜙 () kernel in Deep Linear Attention formulation (Equation 19), resulting in unnormalized formulation of: M𝑡 = M𝑡 1 M𝑡 1(𝜙 (k𝑡 )), v𝑡 . (25) In the special case of linear memory, we can derive the closed form for the above formulation as: M𝑡 = M𝑡 1 M𝑡 1𝜙 (k𝑡 ), v𝑡 = M𝑡 1 + v𝑡𝜙 (k𝑡 ) = 𝑡 𝑖=1 v𝑖𝜙 (k𝑖 ) y𝑡 = M𝑡𝜙 (q𝑡 ) = 𝑡 𝑖= v𝑖 exp(q 𝑖 k𝑖 ), (26) which matches the output of the unnormalized Transformers. Therefore, DeepTransformers are strict generalizations of Transformers with softmax attention (Vaswani et al. 2017)."
        },
        {
            "title": "4.3 Deep Omega Transformer (Dot): Transformers with Omega learning rule",
            "content": "Our above formulation of DeepTransformers is based on the (Hebbian Rule), which is also used in original Transformers. However, as discussed earlier, using more powerful memory management and learning rules in associative memory modules can further enhance their performance. To this end, we extend the above formulation by replacing the Hebbian rule with our Omega learning rule, resulting in an unnormalized formulation of Deep Omega Transformers (Dot): M𝑡 = M𝑡 1 𝑡 𝑖=𝑡 𝑐+1 𝛾 (𝑡 ) 𝑖 (𝜙 (k𝑖 )) v𝑖 2 . We now discuss special instances of Dot to provide further intuition on its generalized formulation. Linear Memory. This setup results in the following unnormalized formulation: (cid:32) M𝑡 = 𝑡 𝛾 (𝑡 ) 𝑖 𝜙 (k𝑖 )𝜙 (k𝑖 ) (cid:33) M𝑡 1 𝑡 𝛾 (𝑡 ) 𝑖 v𝑖𝜙 (k𝑖 ) 𝑖=𝑡 𝑐+1 (cid:32) y𝑡 = M𝑡𝜙 (q𝑡 ) = 𝑡 𝑖=𝑡 𝑐+ 𝑖=𝑡 𝑐+1 (cid:33) 𝛾 (𝑡 ) 𝑖 𝜙 (k𝑖 )𝜙 (k𝑖 ) M𝑡 1𝜙 (q𝑡 ) 𝑡 𝑖=𝑡 𝑐+1 𝛾 (𝑡 ) 𝑖 v𝑖 exp(q 𝑡 k𝑖 ). (27) (28) (29) 11 Online Case with 𝑐 = 1. We now let 𝑐 = 1: M𝑡 = (cid:0)I 𝜂𝑡𝜙 (k𝑡 )𝜙 (k𝑡 )(cid:1) M𝑡 1 𝜂𝑡 v𝑡𝜙 (k𝑡 ) y𝑡 = M𝑡𝜙 (q𝑡 ) = (cid:0)I 𝜂𝑡𝜙 (k𝑡 ) exp(q 𝑡 k𝑡 )(cid:1) M𝑡 1 𝜂𝑡 v𝑡 exp(q 𝑡 k𝑡 ). (30) (31) The above (unnormalized) formulation can be seen as the generalization of Transformers with Delta Rule. Therefore, due to the unbounded memory, Dot not only appends the new keys and values (similar to original Transformers), but it also replaces the new value with its predicted value from the previous state."
        },
        {
            "title": "5 Atlas: A Locally Optimal Memory with High Capacity",
            "content": "Although the design of Omega rule allows the model to memorize the context instead of individual tokens and also the use of polynomial (or exponential) feature mapping increases memory capacity, the memory management (i.e., optimization of mappings between keys and values) is still limited to simple gradient descent. This choice of optimizer can lead the model to low-quality solution at local optima, damaging the performance of the model in longer contexts. To overcome this issue, we suggest using Muon optimizer (Jordan et al. 2024) (with weight decay) that not only approximates second-order information, but it also mostly leverages matrix multiplication and can be parallelized across the sequence. Accordingly, the use of Muon for optimizing the internal objective in Equation 9, results in the following update rule: M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 NewtonShulz-𝑘 (S𝑡 ), 𝑡 S𝑡 = 𝜃𝑡 S𝑡 1 + 𝛾 (𝑡 ) 𝑖 (𝜙 (k𝑖 )) v𝑖 2 2 𝑖=𝑡 𝑐+ , (32) (33) where 𝑐 is the local context length and 𝑘 is the number steps for NewtonShulz operations. For the additional discussion on the algorithm and this operation we refer the reader to Jordan et al. (2024). Following the literature on Muon optimizer, we know that when 𝑘 , then NewtonShulz-𝑘 (S𝑡 ) converges to the nearest semi-orthogonal matrix to the momentum term S𝑡 and so approximate second-order information with lower error. Therefore, interestingly, parameter 𝑘 can be considered as an internal test-time compute parameter in Atlas, where using more steps can potentially result in better memorization."
        },
        {
            "title": "5.1 Parallel Training",
            "content": "In this section, we discussed how the training process of Atlas can be parallelized. For the sake of clarity, we assume 𝑐 = 1. Generalizing the process to arbitrary value for 𝑐 follows the procedure in Section 3.3. We use the same process as we discussed in Section 3.3 and so chunk the sequence and compute all the gradients with respect to the last state of the previous chunk. Accordingly, using the recurrence of Atlas with momentum but without , we have: M𝑡 = 𝛼𝑡 M𝑡 1 + S𝑡 S𝑡 = 𝜃𝑡 S𝑡 1 𝜂𝑡 ℓ (M𝑡 ; k𝑡, v𝑡 ). (34) (35) Since 𝑡 is the last state of the previous chunk, we can calculate all the gradients before hand and so we let 𝑢𝑡 = ℓ (M𝑡 ; k𝑡, v𝑡 ). Therefore, we have: Now by expanding the second recurrence, we have: M𝑡 = 𝛼𝑡 M𝑡 1 + S𝑡 S𝑡 = 𝜃𝑡 S𝑡 1 𝜂𝑡𝑢𝑡 . , S𝑡 = 𝜃𝑡 S𝑡 1 𝜂𝑡 ℓ (M𝑡 ; k𝑡, v𝑡 ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) S𝑡 = 𝜃𝑡 ...𝜃1 (cid:124)(cid:123)(cid:122)(cid:125) 𝛽𝑡 S0 𝑡 𝑖=1 (cid:123)(cid:122) 𝑢𝑡 𝜃𝑡 ...𝜃1 𝜃𝑖 ...𝜃1 12 𝜂𝑖𝑢𝑖 = 𝛽𝑡 S0 Θ 𝐸 𝐺, (36) (37) (38) (39) Figure 3: Visualization of the Atlass (and our other variants) architecture, and its hybrid counterpart with SWA. where 𝐺 is the gradient matrix, 𝐸 and Θ are diagonal matrices with value 𝜃 and 𝜂, and is broadcasting. The main advantage of the above formulation (chunk wise recurrence) is that the recurrence of momentum is independent of the state of memory. That is, we can calculate all the momentum terms in the beginning of the chunk using the above formulation. Now in the Muon case, we want to use Newton-Schulz algorithm on the momentum terms, which results in: 𝑡 Newton-Schulz5(S𝑡 ), M𝑡 = M𝑡 1 + 𝑡 . (40) (41) Since the calculation of all S𝑡 can be done in parallel, the calculation of Newton-Schulz5() can also be done in parallel. Architectural Backbone. As for the architectural backbone, we follow the recent modern recurrent models (Allen-Zhu 2025; Arora, Eyuboglu, Zhang, et al. 2024; Behrouz, Zhong, et al. 2024; Yang, Wang, Zhang, et al. 2024) and use linear layers to project keys, values, and queries, followed by short convolution layers with size 4. We apply normalization on keys and queries to stabilize the training. We also follow Behrouz, Zhong, et al. (2024) and use two hybrid variants of MAL and MAG for our Atlas model. The architectures are illustrated in Figure 3. For models with deep memory architectures we use 2-layer MLP with residual connections: () = () + 𝑊1𝜎 (𝑊2()). (42) We further extend this memory architecture, which is commonly used in recent studies (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Irie et al. 2021), to gated MLP layer as: where 𝑊1,𝑊2,𝑊3 are linear learnable matrices. We refer to Atlas with the above memory architecture as Atlas++. () = () + 𝑊1 (𝜎 (𝑊2()) 𝑊3()) , (43)"
        },
        {
            "title": "6 Experiments",
            "content": "Next, we evaluate the performance of Atlas, OmegaNet, DeepTransformers, and Dot in language modeling, commonsense reasoning, needle in haystack, and in-context recall tasks. Although we also discussed several other variants, such as SWLA, in our experiments we focus on the above models so in addition to comparison with state-of-the-art models, we also answer the following questions: 1. Is deep memory effective for softmax attention? (see Table 2 comparison of Transformer++ and DeepTransformers) 13 2. Does the use of Omega improve the performance softmax attention? (see Table 2 comparison of Transformer++, DeepTransformers, and Dot) 3. Does the Omega rule provide more expressive memory update? (see Table 2 and Table 6 the performance of OmegaNet, and Atlas) 4. Is locally optimal memory update effective? (see Table 2 and Table 6 comparison of OmegaNet, and Atlas) 5. Is non-linear feature mapping effective? (see Table 6) 6. Can the proposed improvements close the gap with Transformers in in-context recall tasks? (see Table 5) 7. What is the effect of the internal optimizer on the memory? (see Figure 6) Setup. We train our models with training context window of size 4K using FineWeb dataset (Penedo et al. 2024). We use model size of 340M, 400M, 790M, and 1.3B parameters and train them on 15B, 15B, 30B, and 100B tokens sampled from the dataset. Baseline results are reported by Behrouz, Razaviyayn, et al. (2025), Behrouz, Zhong, et al. (2024), and Yang, Kautz, et al. (2024). Perplexity is measured on held-out validation data. As for the downstream tasks, we evaluate trained models on Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk et al. 2020), HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark, Cowhey, et al. 2018), SIQA (Sap et al. 2019), and BoolQ (Clark, Lee, et al. 2019). Additional details about the experimental setups and other used datasets are in Appendix E."
        },
        {
            "title": "6.1 Language Modeling and Common-Sense Reasoning",
            "content": "The results for Atlas, and OmegaNet as well as their corresponding baselines of SWDT, DLA, DeepTransformers, and Dot with the size of 760M and 1.3B are reported in Table 2. (see Appendix for the results of small scale). Among non-hybrid models, including Transformer++, our Atlas, and OmegaNet achieve the best performance in both perplexity and accuracy measures. We attribute this performance to their ability to memorize the context rather than individual tokens. Comparing OmegaNet with Titans, that also uses the same momentary objective (i.e., ℓ2 loss), but with context window of 1, we can observe the effectiveness of having non-online learning rule. On the other hand, our models, alone without any attention, can outperform hybrid models, while their hybrid variant of MAG further improve their performance. This performance gain is also related to the use of polynomial kernels that enhance the memory capacity of the model. See Table 6 for more controlled study on the effect of different components. Comparing Transformer++ with our more generalized Transformers (i.e., DeepTransformers, and Dot) we observe consistent performance improvement. We attribute this performance to their deep memory, which makes them more powerful to model the dependencies of tokens. Comparing Dot with DeepTransformers, we can see the advantage of Omega rule, which helps the model to better manage its memory. Figure 4: Performance of Atlas and baselines on BABILong benchmark. Atlas surpasses Titans performance and effectively scale to 10M context length in this task. Figure 5: The effect of local context length (i.e. 𝑐) on the performance of OmegaNet with different global context length. 14 Table 2: Performance of Atlas and baselines on language modeling and common-sense reasoning tasks. Hybrid models are marked with . The best results are highlighted highlighted . Model Wiki. ppl LMB. ppl Transformer++ DeepTransformers (ours) Dot (ours) RetNet DeltaNet TTT Gated DeltaNet Samba Gated DeltaNet-H2 Titans (LMM) Memora SWDT (ours) DLA (ours) OmegaNet (ours) Atlas (ours) Atlas++ (ours) Atlas (MAG) Atlas (MAL) Transformer++ DeepTransformers (ours) Dot (ours) RetNet Mamba2 DeltaNet Gated DeltaNet Samba Gated DeltaNet-H2 Titans (LMM) Memora OmegaNet (ours) Atlas (ours) Atlas++ (ours) 25.21 20.32 19. 26.08 24.37 24.17 21.18 20.63 19.88 20.04 22.28 19.89 23.12 19.16 18.92 19.04 18.62 19.07 18.53 15.67 15.28 19.08 16.56 17.71 16.42 16.13 15.91 15.60 15. 14.91 14.97 14.40 27.64 20.67 20.15 24.45 24.60 23.51 22.09 22.71 20.83 21.96 22.31 21.52 22.09 20.14 21.01 20.03 21.18 21. 18.32 12.63 11.96 17.27 12.56 16.88 12.17 13.29 12.55 11.41 12.04 11.26 10.98 10.72 LMB. PIQA Hella. Wino. ARC-e ARC-c acc_n acc acc_n acc acc acc 760M params / 30B tokens 35.8 36.9 39.0 34.5 37.1 34.7 35.5 39.7 39.2 37.4 38.2 36.2 36. 38.7 39.1 39.7 40.0 38.8 66.9 68.4 69.1 67.2 66.9 67.3 68.0 69.2 69.0 69.3 67.8 68.3 68.0 69.8 69.7 69. 70.3 69.2 42.2 49.8 50.7 41.6 42.0 43.9 44.9 47.4 48.2 48.5 49.3 45.2 47.9 50.0 50.2 51.1 50.5 50. 51.9 52.8 53.1 52.1 50.7 51.0 50.7 52.0 52.6 52.3 53.3 53.0 52.7 53.3 53.5 53.2 53.0 53.6 1.3B params / 100B tokens 42.6 49.4 50.1 40.5 45.7 42.5 46.6 44.9 48.8 49.1 48.7 49.7 50.1 50.8 70.0 72.6 73.3 70.1 71.9 70.7 72.2 70.9 72.2 73.1 73.1 73.4 73.9 73. 50.2 57.0 57.5 49.2 55.7 50.9 55.8 53.4 56.9 56.3 56.0 57.6 57.3 59.4 53.5 58.8 60.4 54.1 55.2 53.3 57.4 55.6 57.8 59.8 57.4 59.7 60.2 61. 60.4 65.7 66.2 63.2 64.9 64.5 66.9 66.9 67.0 66.3 63.6 65.4 65.8 67.8 67.5 68.2 68.1 67.3 68.8 71.1 72. 67.3 72.5 68.5 71.2 68.8 71.4 72.4 71.5 72.6 72.8 71.3 32.5 34.9 37.0 32.8 31.4 33.8 33.1 33.2 35.5 35.8 36.1 34.2 34.6 36.8 37.1 37. 36.5 36.1 35.1 37.5 41.2 33.8 37.9 35.7 38.4 36.2 39.1 40.8 37.9 40.3 41.0 43.7 SIQA BoolQ Avg. acc acc 39.5 40.2 40.3 38.4 39.9 40.2 39.2 39.0 39.4 40.1 40.9 39.5 39.1 39.6 40.7 40.9 41.2 41.0 40.7 41.6 42. 40.8 40.2 40.2 40.6 40.0 41.2 42.1 40.2 42.4 42.9 42.5 60.4 61.8 63.7 57.9 59.0 59.6 59.1 61.2 61.1 62.8 63.0 59.5 59.6 64.4 64.3 64. 65.0 64.5 57.1 61.5 61.4 60.4 60.1 55.3 60.2 62.1 61.6 61.0 61.3 62.1 62.8 61.9 48.69 51.31 52.39 48.46 48.97 47.32 49.69 51.08 51.49 51.56 51. 50.1 50.46 52.56 52.77 53.09 53.08 52.63 52.25 56.19 57.35 52.02 54.89 52.14 55.32 54.00 56.18 56.82 55.87 57.23 57.62 58."
        },
        {
            "title": "6.2 Long Context: Needle In a Haystack",
            "content": "One of our main motivations to design Atlas is to enhance the performance of long-term neural memory module in long context tasks. Accordingly, to evaluate the effectiveness of our designs for improving the effective context length and memory capacity, we perform an experiment on needle-in-haystack tasks of RULER (Hsieh et al. 2024) benchmark. The performance of Atlas and its hybrid variants, as well as our Transformer-like architectures and baselines are reported in Table 3. Atlas shows very good performance compared to the recurrent baselines, outperforming modern recurrent neural networks such as Titans and DeltaNet. Its hybrid variants further improve its effective context length, effectively extrapolating to sequences with 4 of their training context size. We attribute this performance to the proposed enhancements for the capacity of the memory. We further perform ablation studies to validate this claim. Also, our Transformer-like architectures outperforms the baselines, even our hybrid variants of Atlas in longer contexts. This shows the importance of exponential feature mapping in longer sequences."
        },
        {
            "title": "6.3 Long Context: BABILong Benchmark",
            "content": "To compare the effectiveness of Atlas with Titans (Behrouz, Zhong, et al. 2024) in ultra-large sequences, we further evaluate Atlass performance on BABILong benchmark (Kuratov et al. 2024). In this experiment, we follow Behrouz, 15 Table 3: Performance of Atlas and baselines on S-NIAH task from RULER benchmark. The best results among simple and hybrid models are highlighted. Model TTT DeltaNet Titans (LMM) Atlas Samba Gated DeltaNet-H2 Atlas (MAG) Atlas (MAL) DeepTransformers Dot S-NIAH-PK S-NIAH-N S-NIAH-W 2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 98.4 96.8 99.8 100 98.8 99.2 100 99.8 100 100 98.8 98.8 98.4 99. 98.0 97.8 100 99.6 100 100 98.0 98.6 98.2 98.0 97.4 97.4 99.4 98.4 98.2 99.6 88.4 71.4 96.2 97.0 97.2 98.4 98.6 96.8 97.8 98.6 60.2 47.2 100.0 100. 98.8 98.0 100 99.8 100 100 36.6 15.4 99.8 100.0 98.6 97.8 99.2 98.0 98.8 100 10.2 12.8 93.4 93.0 96.2 96.2 97.4 97.2 97.8 97.8 4.4 5.4 80.2 84. 95.6 95.8 97.0 96.8 94.0 96.8 78.8 46.2 90.4 93.2 96.8 97.4 99.4 98.0 95.8 99.0 28.0 20.0 89.4 90.6 90.0 96.8 98.2 98.4 92.2 98.4 4.4 1.6 85.8 86. 84.0 88.4 92.4 92.6 88.4 93.2 Zhong, et al. (2024) and use MAC architecture but without persistent memory tokens. We also follow the original setup in the benchmark and fine-tune our model. The results are reported in Figure 4. While Atlas shows competitive and on par performance with Titans until 1M context length, the performance of Titans drops in 10M. Atlas, however, maintains its performance and achieve +80% accuracy in 10M context length. We attribute this to more powerful memory; in terms of (1) memory management (i.e., the use of Muon), (2) better memory capacity due to polynomial kernels, and (3) its nature to memorize the context, instead of individual tokens. In previous sections, we show the effectiveness of our Transformer-like architectures (i.e., DeepTransformers and Dot) in both language modeling and long-context needle-in-haystack tasks. From now on, we focus on our recurrent architectures (i.e., Atlas, and OmegaNet) to show the importance of presented improvements."
        },
        {
            "title": "6.4 Learnability Experiments",
            "content": "We have also performed some small-scale experiments to analyze the function-learning capability of small MLPs in an online fashion. In this setting, we have sequence of tuples (𝑖1, 𝑜1), . . . (𝑖𝑡, 𝑜𝑡 ) with both 𝑖 𝑗, 𝑜 𝑗 R𝑑 for all 𝑗. We train an MLP in an online fashion to minimize loss𝑗 = 𝑖 𝑗 𝑜 𝑗 2 2 specifically, we compute the gradient at time step 𝑗 as 2/𝑜 𝑗 2 M.paramsloss𝑗 and use standard optimizers such as Adam, Rmsprop and SGD to update the parameters. Such experiments help us understand the representation power of the models we use to represent memory and the power of optimization algorithms to quickly learn the underlying sequence mapping. We study five different sequence to sequence functions: 1. Low Rank Mappings: We sample random low rank matrix = XY with R𝑑 𝑘 and R𝑘 𝑑 . We then sample 𝑖1, . . . , 𝑖𝑡 randomly from Gaussian distribution and set 𝑜 𝑗 = WT 𝑖 𝑗 for all 𝑗 [𝑡]. 2. MLP Mappings: We sample an MLP with 1 input, 1 hidden and 1 output layer which uses GELU non-linearity. We set the hidden dimension to 𝑑 so that there is no expansion. We then sample 𝑖1, . . . , 𝑖𝑡 randomly from Gaussian distribution and then set 𝑜 𝑗 = (𝑖 𝑗 ) for all 𝑗 [𝑡]. 3. Attention+MLP Mapping: We sample (𝑖1, . . . , 𝑖𝑡 ) from Gaussian distribution and an MLP as above. We 𝑖 𝑗 𝑡 as outputs of the causal masked attention mechanism applied on additionally sample three 𝑑 𝑑 matrices WQ, WK and WV and compute 𝑞 𝑗 = WQ for all 𝑗 [𝑡]. We then compute 𝑜 1 {𝑞 𝑗 } 𝑗 [𝑡 ], {𝑘 𝑗 } 𝑗 [𝑡 ], {𝑣 𝑗 } 𝑗 [𝑡 ] and finally compute 𝑜 𝑗 = (𝑜 𝑗 ). 𝑖 𝑗 and 𝑣 𝑗 = WK 𝑖 𝑗 , 𝑘 𝑗 = WK , . . . , 𝑜 4. Attention Outputs as Inputs: We do the same as above except that we output 𝑜 𝑗 as the input sequence and 𝑜 𝑗 as the output sequence. 5. Sliding Window Attention + MLP Mapping: We do the same as in Attention + MLP Mapping setting except that we use sliding window attention instead of full attention. We use sliding window of 512 in our experiments. 16 (a) with 2 hidden layers and no expansion. (b) with 3 hidden layers and no expansion. (c) with 2 hidden layers and 4x expansion. (d) with 3 hidden layers and 4x expansion. Figure 6: Loss curves for different setting with various hyperparameters 17 Table 4: Performance of Atlas, OmegaNet, and baselines on the synthetic benchmark of MAD (Poli et al. 2024). Atlas outperforms all the baselines, including Transformers. Compression (Noisy) ICR Fuzzy ICR Selective Copying Memorization Average Transformers Gated DeltaNet Titans OmegaNet (ours) Atlas (ours) 49.4 44.8 49. 50.9 51.6 100 100 100 100 100 48.2 32.5 49.7 54.2 54.9 95.9 96.2 99. 99.6 99.6 83.8 81.7 83.5 90.2 91.4 75.46 71.04 76.44 78.98 79.50 Note that the settings 3 and 5 are much harder to learn since they require (partially) memorizing the previous inputs and outputs to be able to learn the function that maps 𝑖 𝑗 to 𝑜 𝑗 , whereas the settings 1, 2 and 4 do not need to memorize the previous input-output pairs and just need to learn the underlying low-rank matrix or the MLP that maps the inputs to outputs. The setting 4 is slightly different to setting 2 in that the inputs are not-independent at each time step and are correlated by the attention mechanism we use to compute the inputs. Thus strong learning algorithm maybe able to utilize the underlying correlations to learn the mapping faster in setting 4 versus setting 2. Table 5: The performance of our models (Atlas, and OmegaNet) compared to baselines. While still Transformers achieve the best results in in-context recall tasks, our design of context memorization and polynomial feature maps can close the gap with Transformers. Table 6: Ablation Study on Atlas. All components of Atlas are positively contributing to its performance. SWDE NQ DROP FDA SQUAD TQA Average Transformers Gated DeltaNet Titans OmegaNet (ours) Atlas (ours) 84.9 63.2 65.1 67.4 66.8 23.0 19.1 20.7 21.1 21.9 28.4 26.7 27. 27.2 27.4 72.5 33.4 37.3 39.0 40.7 48.1 39.6 42.6 43.2 44.1 64.4 59.7 61. 60.9 61.3 53.55 40.28 42.31 43.13 43.70 Model Atlas +Gated MLP Memory +Attn (MAG) +Attn (MAL) Linear Memory w/o Muon 𝑐 = 1 w/o Polynomial Mapping Language Modeling C.S. Reasoning ppl 19.97 19. 19.90 20.26 21.03 19.65 21.98 22.14 acc 52.77 53.09 53.08 52. 49.74 52.56 49.26 50.57 We set 𝑑 = 256 and show the loss curves vs sequence position for all the five settings with function learning MLP being defined and trained with different settings in Figure 6. We can see that in all the settings, the model learns non-trivial mappings from inputs to outputs with the 𝑙𝑜𝑠𝑠 𝑗 = 𝑖 𝑗 𝑜 𝑗 2 2 being smaller than 1 eventually. Most notably, the correlations in inputs in setting 4 induced by the attention mechanism makes the model quickly learn the mapping compared to in setting 2 and the models usually learn the best in setting 1 which is the least complex function. 2/𝑜 𝑗 2 The models do the worst in settings 3 and 5 which require the models to (partially) memorize the inputs and outputs to learn the attention mechanism outputs. Surprisingly, the models learn to do better in setting 3 vs setting 5, when we would expect that capacity requirement for setting 3 to be higher than setting 5. We hypothesize that the learning algorithm is unable to make the model forget old inputs which makes the loss worse in sliding window setting when compared to global attention setting. caveat of our analysis is that, the attention computation is done on randomly initialized vectors and hence the attention matrix is usually not spiky, unlike in the attention matrix for trained set of query, key and value vectors in LLMs. This leads to attention outputs being close to the mean of value vectors in the context."
        },
        {
            "title": "Recall",
            "content": "In this section, we first evaluate the performance of our models on MAD benchmark, synthetic benchmark that evaluate the performance of models in recall, memorization, compression, and copying tasks (Poli et al. 2024). The results are reported in Table 4. Atlas achieves the best results in all aspects, particularly in memorization, which shows the importance of its components for enhancing the memory capacity. 18 In-context recall tasks is one of the most challenging benchmarks for recurrent neural networks. In this section, we follow Arora, Eyuboglu, Zhang, et al. (2024) and perform experiments on SWDE (Lockard et al. 2019), NQ (Kwiatkowski et al. 2019), DROP (Dua et al. 2019), FDA (Arora, Yang, et al. 2023), SQUAD (Rajpurkar et al. 2016), and TQA (Kembhavi et al. 2017) to evaluate and compare the performance of Atlas with baselines and Transformers. The results are reported in Table 5. While Transformers still achieve the best results in in-context recall tasks, Atlas and OmegaNet shows competitive performance and performs better than state-of-the-art recurrent models. We again attribute this performance to better memory management and capacity. Figure 7: The results for associative memory recall. Figure 8: Scaling patterns of Atlas, and OmegaNet with respect to (Left) training context length, and (Right) FLOPs. Finally, following Yang, Wang, Zhang, et al. (2024) and Arora, Eyuboglu, Timalsina, et al. (2023) we evaluate the performance of Atlas and Dot in Multi-Query Associative Recall (MQAR) task (Arora, Eyuboglu, Timalsina, et al. 2023). The results are reported in Figure 7. Both models show good performance compared to baselines and Atlas achieve the best performance per memory size compared to state-of-the-art models such as DeltaNet (Yang, Wang, Zhang, et al. 2024)."
        },
        {
            "title": "6.6 Ablation Study and Scaling Patterns",
            "content": "In this section, we perform an ablation study on the differernt components of Atlas, and also evaluate its scaling patterns with respect to the number of parameters and also the context length of the training. The results for ablation study are reported in Table 6. The results show that: (1) more powerful memory architectures such as gated MLP can further enhance the performance of Atlas; (2) The hybrid variants further improve the performance, where MAG shows better improvement compared to MAL architecture; (3) Polynomial mappings as well as deep memory are particularly important when we use context memorization (i.e., Omega rule). Figure 5 also shows the effect of local context length (i.e., 𝑐) on the performance of the model. With the increase of 𝑐 we can achieve better performance, mainly due to the gating parameters of 𝛾 that can prune the context, whenever it is needed. Model Size. Figure 8 shows the scaling pattern of Atlas, and OmegaNet, with respect to number of parameters and compared to baseline. Both models achieve good scaling pattern with increasing the model size, achieving lower perplexity in all scales compared to baselines. Context Length. Figure 8 shows the scaling pattern of Atlas, and OmegaNet, with respect to the context length and compared to baseline. Both models due to high memory capacity can scale well, when increasing the context length."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced Atlas, new long-term memory module designed to address the core limitations of modern recurrent models in long-context understanding: limited memory capacity, online-only updates, and weak memory management. Our proposed sliding window learning rule, higher-order feature mappings, and advanced memory optimizers offer principled and scalable approach to overcoming these challenges. Empirically, our modelsOmegaNet, Atlas, DeepTransformers, and Dotachieve consistent improvements over Transformers and recent RNN variants across diverse benchmarks. Theoretically, we provided insight into memory capacity and optimization dynamics, offering explanations for the context length limitations observed in prior works."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. In: arXiv preprint arXiv:2303.08774 (2023). [2] Zeyuan Allen-Zhu. Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers. In: [3] [4] [5] SSRN Electronic Journal (May 2025). https://ssrn.com/abstract=5240330. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language models. In: arXiv preprint arXiv:2312.04927 (2023). Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re. Simple linear attention language models balance the recall-throughput tradeoff. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=e93ffDcpH3. Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Ré. Language models enable simple systems for generating structured views of heterogeneous data lakes. In: arXiv preprint arXiv:2304.09433 (2023). [6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In: arXiv preprint arXiv:1409.0473 (2014). [7] Eric Baum. On the capabilities of multilayer perceptrons. In: Journal of Complexity 4.3 (1988), pp. 193215. issn: 0885-064X. doi: https://doi.org/10.1016/0885-064X(88)90020-9. url: https://www.sciencedirect.com/ science/article/pii/0885064X88900209. [8] Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Its All Connected: Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization. In: arXiv preprint arXiv:2504.13173 (2025). [9] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. In: arXiv preprint [10] arXiv:2501.00663 (2024). Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In: International conference on machine learning. PMLR. 2020, pp. 864873. [11] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. In: Advances in Neural Information Processing Systems 36 (2023), pp. 15601588. [12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 34. 2020, pp. 74327439. [13] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 29242936. doi: 10.18653/v1/N19-1300. url: https: //aclanthology.org/N19-1300/. [14] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. In: arXiv preprint arXiv:1803.05457 (2018). [15] Thomas M. Cover. Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition. In: IEEE Transactions on Electronic Computers EC-14.3 (1965), pp. 326334. doi: 10.1109/PGEC. 1965.264137. [16] Róbert Csordás, Christopher Potts, Christopher Manning, and Atticus Geiger. Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations. In: Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 2024, pp. 248262. [17] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-Minute Video Generation with Test-Time Training. In: arXiv preprint arXiv:2504.05298 (2025). [18] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In: arXiv preprint arXiv:1903.00161 (2019). Jianqing Fan. Local polynomial modelling and its applications: monographs on statistics and applied probability 66. Routledge, 2018. [19] [20] Xavier Gonzalez, Andrew Warrington, Jimmy Smith, and Scott Linderman. Towards scalable and stable parallelization of nonlinear rnns. In: Advances in Neural Information Processing Systems 37 (2024), pp. 58175849. [21] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid Structural State-Space Models. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum?id=g4OTKRKfS7R. [22] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogerio Feris. CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory. In: arXiv preprint arXiv:2402.13449 (2024). [23] Donald Olding Hebb. The organization of behavior: neuropsychological theory. Psychology press, 2005. [24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). In: arXiv preprint arXiv:1606.08415 (2016). John Hopfield. Neural networks and physical systems with emergent collective computational abilities. In: [25] Proceedings of the national academy of sciences 79.8 (1982), pp. 25542558. [26] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the Real Context Size of Your Long-Context Language Models? In: First Conference on Language Modeling. 2024. url: https://openreview.net/forum?id=kIoBbc76Sy. Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu. Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes. In: arXiv preprint arXiv:2410.23126 (2024). [28] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In: International conference [27] on machine learning. PMLR. 2022, pp. 90999117. [29] Guang-Bin Huang. Learning capability and storage capacity of two-hidden-layer feedforward networks. In: IEEE Transactions on Neural Networks 14.2 (2003), pp. 274281. doi: 10.1109/TNN.2003.809401. [30] Kazuki Irie, Imanol Schlag, Robert Csordas, and Jurgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In: Advances in neural information processing systems 34 (2021), pp. 77037717. [31] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks. 2024. url: https://kellerjordan.github.io/posts/muon/. [32] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/ forum?id=ghYrfdJfjK. [33] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels. In: Proceedings of the 41st International Conference on Machine Learning. Ed. by Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp. Vol. 235. Proceedings of Machine Learning Research. PMLR, July 2024, pp. 2274822770. url: https://proceedings.mlr. press/v235/kacham24a.html. [34] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. In: arXiv preprint arXiv:2503.19786 (2025). [35] M. Karami and V. Mirrokni. Lattice: Learning to Efficiently Compress the Memory. 2025. [36] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning Pretrained Transformers into RNNs. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Ed. by Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 1063010643. doi: 10.18653/v1/2021.emnlpmain.830. url: https://aclanthology.org/2021.emnlpmain.830/. [37] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In: International conference on machine learning. PMLR. 2020, pp. 51565165. [38] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In: Proceedings of the IEEE Conference on Computer Vision and Pattern recognition. 2017, pp. 49995007. [39] Dmitry Krotov. Hierarchical associative memory. In: arXiv preprint arXiv:2107.06446 (2021). [40] Dmitry Krotov and John Hopfield. Dense associative memory for pattern recognition. In: Advances in neural information processing systems 29 (2016). [41] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail Burtsev. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. In: The Thirty21 eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. url: https : //openreview.net/forum?id=u7m2CG84BQ. [42] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. In: Transactions of the Association for Computational Linguistics 7 (2019), pp. 453466. [43] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. survey on long video generation: Challenges, methods, and prospects. In: arXiv preprint arXiv:2403.16407 (2024). [44] Xiaoyu Li, Yuanpeng Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. On the expressive power of modern hopfield networks. In: arXiv preprint arXiv:2412.05562 (2024). [45] Yi Heng Lim, Qi Zhu, Joshua Selfridge, and Muhammad Firmansyah Kasim. Parallelizing non-linear sequential models over the sequence length. In: The Twelfth International Conference on Learning Representations. 2024. url: https://openreview.net/forum?id=E34AlVLN0v. [46] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space models are amortized online learners. In: arXiv preprint arXiv:2407.14207 (2024). [47] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. In: Transactions of the Association for Computational Linguistics 12 (2024), pp. 157173. [48] Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. Openceres: When open information extraction meets the semi-structured web. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019, pp. 30473056. [49] Carlo Lucibello and Marc Mézard. Exponential capacity of dense associative memories. In: Physical Review Letters [50] 132.7 (2024), p. 077301. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture Models. In: International Conference on Learning Representations. 2017. url: https://openreview.net/forum?id=Byj72udxe. [51] William Merrill, Jackson Petty, and Ashish Sabharwal. The Illusion of State in State-Space Models. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=QZgo9JZpLq. [52] Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2. NIPS14. Montreal, Canada: MIT Press, 2014, pp. 29242932. [53] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned neural memory. In: Advances in Neural Information Processing Systems 32 (2019). [54] Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In: Proceedings of the conference. Association for Computational Linguistics. Meeting. Vol. 1. NIH Public Access. 2017, p. 397. [55] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 15251534. doi: 10.18653/v1/P16-1144. url: https://aclanthology.org/P16-1144/. [56] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. 2014. arXiv: 1312.6098 [cs.LG]. url: https://arxiv.org/abs/1312.6098. [57] Guilherme Penedo, Hynek Kydlicek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. In: Advances in Neural Information Processing Systems 37 (2024), pp. 3081130849. [58] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the Transformer Era. In: The 2023 Conference on Empirical Methods in Natural Language Processing. 2023. url: https://openreview.net/ forum?id=7SaXczaBpG. [59] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. In: arXiv preprint arXiv:2404.05892 (2024). [60] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, et al. Rwkv-7\" goose\" with expressive dynamic state evolution. In: arXiv preprint arXiv:2503.14456 (2025). [61] Michael Poli, Armin Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Re, et al. Mechanistic design and scaling of hybrid architectures. In: arXiv preprint arXiv:2403.17844 (2024). [62] DL Prados and SC Kak. Neural network capacity using delta rule. In: Electronics Letters 25.3 (1989), pp. 197199. [63] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In: arXiv preprint arXiv:1606.05250 (2016). [64] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield Networks is All You Need. In: International Conference on Learning Representations. 2021. url: https: //openreview.net/forum?id=tL89RnzIiCd. [65] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. In: arXiv preprint arXiv:2406.07522 (2024). [66] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In: Communications of the ACM 64.9 (2021), pp. 99106. [69] [68] [67] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense Reasoning about Social Interactions. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Ed. by Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 44634473. doi: 10.18653/v1/D19-1454. url: https://aclanthology.org/D19-1454/. Siddhartha Satpathi and Rayadurgam Srikant. The dynamics of gradient descent for overparametrized neural networks. In: Learning for Dynamics and Control. PMLR. 2021, pp. 373384. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In: International Conference on Machine Learning. PMLR. 2021, pp. 93559366. JH Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Accepted for publication in. In: Neural Computation (1992). Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. In: ICANN93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 1316 September 1993 3. Springer. 1993, pp. 460463. Jürgen Schmidhuber and Sepp Hochreiter. Long Short-term Memory. In: Neural Computation MIT-Press (1997). [72] [73] Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, and Jannes Gladrow. Implicit Language [70] [71] [74] [75] [76] Models are RNNs: Balancing Parallelization and Expressivity. In: arXiv preprint arXiv:2502.07827 (2025). Jack Sherman and Winifred Morrison. Adjustment of an inverse matrix corresponding to change in one element of given matrix. In: The Annals of Mathematical Statistics 21.1 (1950), pp. 124127. Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, and Riccardo Grazzi. DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders. In: arXiv preprint arXiv:2502.10297 (2025). Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum? id=Ai8Hw3AXqks. [77] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. In: arXiv preprint arXiv:2407.04620 (2024). [78] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. In: arXiv preprint arXiv:2307.08621 (2023). [79] Scott Terry. Learning and memory: Basic principles, processes, and procedures. Routledge, 2017. [80] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. On the resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer era. In: arXiv preprint arXiv:2402.08132 (2024). [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017. url: [82] https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf. Johannes Von Oswald, Maximilian Schlegel, Alexander Meulemans, Seijin Kobayashi, Eyvind Niklasson, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, et al. Uncovering mesa-optimization algorithms in transformers. In: arXiv preprint arXiv:2309.05858 (2023). [83] Ke Alexander Wang, Jiaxin Shi, and Emily Fox. Test-time regression: unifying framework for designing sequence models with associative memory. In: arXiv preprint arXiv:2501.12352 (2025). [84] Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. In: arXiv preprint arXiv:2402.18510 (2024). [85] Bernard Widrow and Marcian Hoff. Adaptive switching circuits. 1988. [86] David Willshaw, Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic associative memory. [87] [88] [89] In: Nature 222.5197 (1969), pp. 960962. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated Delta Networks: Improving Mamba2 with Delta Rule. In: arXiv preprint arXiv:2412.06464 (2024). Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated Linear Attention Transformers with Hardware-Efficient Training. In: Forty-first International Conference on Machine Learning. 2024. url: https: //openreview.net/forum?id=ia5XvxFUJT. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In: Advances in Neural Information Processing Systems 37 (2024), pp. 115491115522. [90] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Ed. by Anna Korhonen, David Traum, and Lluis Marquez. Florence, Italy: Association for Computational Linguistics, July 2019, pp. 47914800. doi: 10.18653/v1/P19-1472. url: https://aclanthology.org/P19-1472/. [91] Yufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, and Zhaoran Wang. An analysis of attention via the lens of exchangeability and latent variable models. In: arXiv preprint arXiv:2212.14852 (2022)."
        },
        {
            "title": "A Additional Related Work",
            "content": "Modern Linear Recurrent Neural Networks2. Recent research endeavors have concentrated on mitigating the quadratic computational complexity and inherent limitations of Transformer models in processing long-context sequences. This has led to the development of efficient recurrent alternatives, primarily motivated by their rapid inference and training capabilities (Tiezzi et al. 2024). Initial advancements in this domain, exemplified by models such as RetNet (Sun, Dong, et al. 2023), RWKV (Peng, Alcaide, et al. 2023), and S5 (Smith et al. 2023), employed data-independent transition matrices coupled with Hebbian-like update mechanisms. Subsequently, second generation of models emerged, incorporating input-dependent parameters within these linear architectures (e.g., linear RNNs (Hasani et al. 2023; Smith et al. 2023), RWKV6 (Peng, Goldstein, et al. 2024)). These models also explored more expressive memory updating rules, notably those based on the delta rule (Liu, Wang, et al. 2024; Peng, Zhang, et al. 2025; Schlag et al. 2021; Yang, Kautz, et al. 2024; Yang, Wang, Zhang, et al. 2024). Further evolution in this line of research has extended these memory architectures to deeper models, while concurrently utilizing delta-rule-like update mechanisms (Sun, Li, et al. 2024) or data-dependent momentum-based update rules with forget gating (Behrouz, Zhong, et al. 2024). More recently, to augment the performance of delta-rule-based sequential models, Siems et al. (2025) have proposed the application of multiple gradient descent updates per token, thereby yielding more expressive sequence models, particularly in state tracking tasks. In addition to the above fast linear recurrent sequence models, several studies have focused on RNNs with non-linear recurrence (Behrouz, Razaviyayn, et al. 2025; Csordás et al. 2024; Gonzalez et al. 2024; Karami et al. 2025; Lim et al. 2024; Merrill et al. 2024; Schöne et al. 2025; Von Oswald et al. 2023), and how their training can be faster (Gonzalez et al. 2024; Lim et al. 2024; Schöne et al. 2025). Fast Weight Programs. The conceptualization of linear layers as key-value associative memory systems can be traced back to Hopfield networks (Hopfield 1982). This concept was subsequently developed in the context of fast weight programmers, wherein dynamic fast programs are integrated into recurrent neural networks to serve as writable memory stores (Schlag et al. 2021; Schmidhuber 1992; Schmidhuber 1993). Among the learning paradigms for such systems, Hebbian learning (Hebb 2005) and the delta rule (Prados et al. 1989) have emerged as the most prominent. Both learning rules have been the subject of extensive investigation within the existing literature (Irie et al. 2021; Munkhdalai, Sordoni, et al. 2019; Munkhdalai and Yu 2017; Schlag et al. 2021; Schmidhuber 1992; Yang, Kautz, et al. 2024; Yang, Wang, Zhang, et al. 2024). Hopfield Networks. Our formulation is architecturally founded upon the broad concept of associative memory, wherein the primary objective is to learn an underlying mapping between keys and values. Seminal work by Hopfield (1982) on Hopfield Networks introduced one of the earliest neural architectures explicitly based on associative memory, defining it through the minimization of an energy function for storing key-value pairs. Although traditional Hopfield networks have seen diminished applicability in recent years, primarily due to constraints in vector-valued memory capacity and the nature of their energy function, several contemporary studies have focused on enhancing their capacity through various methodologies. These include efforts by Krotov (2021), Li, Li, et al. (2024), and Krotov and Hopfield (2016). Notably, extensions to the energy function of these models, often incorporating exponential kernels, have been explored (Krotov and Hopfield 2016; Lucibello et al. 2024). Furthermore, the relationship between these modernized Hopfield networks and Transformer architectures has been subject of recent investigation (Hu et al. 2024; Ramsauer et al. 2021)."
        },
        {
            "title": "B Miras Framework",
            "content": "As discussed earlier, Behrouz, Razaviyayn, et al. (2025) formalized the concept of associative memory as: Definition 2 (Behrouz, Razaviyayn, et al. (2025)). Given set of keys R𝑑𝑘 and values R𝑑𝑣 , associative memory is an mapping : V. Learning the associative memory is based on an objective L, called Attentional Bias, that determines the type of memory and its priorities: = arg min (M (K); V). (44) 2Note that here the term linear refers to their fast training and inference procedures. This does not refer to their recurrence formula as some models like Titans (Behrouz, Zhong, et al. 2024), Yaad, Moneta, Memora (Behrouz, Razaviyayn, et al. 2025), and TTT (Sun, Li, et al. 2024) are based on non-linear recurrence but fast at training and inference. Optimizing this objective using an iterative algorithm (e.g., gradient descent) results in the memory update rule. Thus, the sequence model is meta in-context learner with two optimization levels: 1. Inner Loop: Where parameters of the memory module are optimized (i.e., 𝜽 = {𝑊1,𝑊2, . . . ,𝑊LM,...}). In the inner optimization loop, all other parameters from the model are considered hyperparameters and are fixed and not optimized. 2. Outer Loop: Where all other parameters of the model are optimized, such as linear projections, MLP layers, convolutions, etc. B.1 Examples As an example, one can define the linear attention as the optimization of dot-product similarity with gradient descent: i.e., ℓ𝑡 := M𝑡 1k𝑡, v𝑡 . That is, M𝑡 = M𝑡 1 𝜂𝑡 ℓ𝑡 (M𝑡 1; k𝑡, v𝑡 ) = M𝑡 1 𝜂𝑡 M𝑡 1k𝑡, v𝑡 = M𝑡 1 + 𝜂𝑡 v𝑡 𝑡 . (45) (46) As an another example, if we use regression loss, instead of the dot-product similarity, we can obtain the DeltaNet (Schlag et al. 2021): M𝑡 = M𝑡 1 𝜂𝑡 M𝑡 k𝑡 v𝑡 2 = 𝜂𝑡 k𝑡 𝑡 M𝑡 1 + v𝑡 𝑡 . (47)"
        },
        {
            "title": "C Supporting Proofs",
            "content": "Proposition 1 (Capacity of ℓ2 Attentional Bias). Let be matrix-valued memory with 𝑑𝑣 𝑑𝑘 parameters that optimizes 2 with gradient descent. can store the mapping of at most (𝑑𝑘 ) the internal objective of ℓ (M𝑡 ; k𝑡, v𝑡 ) = M𝑡 k𝑡 v𝑡 2 pairs of (k𝑖, v𝑖 ) with linearly independent keys. Proof. Let 𝐾 = [k1 k𝑚] R𝑑𝑘 𝑚 and 𝑉 = [v1 v𝑚] R𝑑𝑣 𝑚. The optimization problem becomes minimizing the 2. Exact memorization requires solving the linear system M𝐾 = 𝑉 . Frobenius norm M𝐾 𝑉 2 Vectorizing the expression yields the system (𝐾 𝐼𝑑𝑣 )vec(M) = vec(𝑉 ), which has 𝑚𝑑𝑣 scalar equations in 𝑑𝑘𝑑𝑣 unknowns. When the keys are linearly independent, rank(𝐾) = 𝑚, and hence the system matrix has full row rank 𝑚𝑑𝑣. Solvability thus requires 𝑚𝑑𝑣 𝑑𝑘𝑑𝑣, or equivalently 𝑚 𝑑𝑘 . This matches classic results on the storage capacity of linear associative memories such as the Willshaw model and Hopfield networks, where capacity is tied to the rank of the input embedding (Hopfield 1982; Willshaw et al. 1969). When 𝑚 𝑑𝑘 and 𝐾 has full column rank, one can construct an exact interpolating solution via the MoorePenrose pseudoinverse: = 𝑉 𝐾 . Then M𝐾 = 𝑉 𝐾 𝐾 = 𝑉 , achieving zero training error. Thus the upper bound is tight. Moreover, full-batch gradient descent on this objective with step size 0 < 𝜂 < 2/𝜆max (𝐾𝐾 ) yields iterates M𝑡 +1 = M𝑡 𝜂 (M𝑡 𝐾 𝑉 )𝐾 , which converge to the minimum-norm interpolating solution = 𝑉 𝐾 when 𝑚 𝑑𝑘 . This is well-known implicit bias of gradient descent in overparameterized linear models (Satpathi et al. 2021). Finally, the same rank-based constraint governs the capacity of linear or multi-head attention modules. In such architectures, the output context matrix has rank at most rank(𝐾) 𝑑𝑘 , which directly limits their expressivity. Recent analyses identify this low-rank bottleneck as capacity-limiting effect in Transformers (Bhojanapalli et al. 2020). Theorem 1 (Effect of Deep Memory). Let () be an MLP with LM 2 layers, 𝑑𝑘 input dimension, and 𝑑ℎ hidden dimension. Then, () can store the mapping of at least (𝑑𝑘𝑑𝑣) and at most pairs of (k𝑖, v𝑖 ) with linearly independent keys. 𝑖=1 min{𝑑 ( 𝑗 ) ℎ } 𝑗 𝑖𝑑 ( 𝑗+1) (cid:16) 𝑑𝑘𝑑𝑣 (cid:205)LM (cid:17) ℎ Early theoretical works established that even simple network architectures can memorize significant number of inputoutput mappings, with capacity often related to the number of network parameters (e.g., weights and biases) and the input (cid:7) neurons dimensionality Baum (1988), Cover (1965), and Huang (2003). For instance, Baum (1988) demonstrated that (cid:6) 𝑁 𝑑 are sufficient for single-hidden-layer network with threshold units to memorize 𝑁 input-label pairs from R𝑑 . 26 Networks employing Rectified Linear Units (ReLUs), exhibit piecewise affine behavior. The input space is partitioned into numerous linear regions, and within each region, the network computes distinct affine transformation Montufar et al. (2014) and Pascanu et al. (2014). This structure is pivotal for analyzing their expressive power and storage capacity. The precise relationship between depth, width, the number of linear regions, and the ultimate capacity to store specific key-value associations, especially with constraints like linearly independent keys, remains an active area of research. Proof. Let 𝑚 denote the number of (k𝑖, v𝑖 ) pairs memorized exactly by M, and assume the keys {k𝑖 }𝑚 independent. Let 𝑑 (0) ℎ simplicity. Since 𝜎 (𝑥) = max(0, 𝑥) is piecewise linear, the composition of linear maps and ReLU activations yields piecewise affine function. For any fixed activation pattern (i.e., fixed sign of pre-activations), the MLP acts as: := 𝑑𝑣, and for each layer 1 ℓ LM, define 𝑊 (ℓ ) R𝑑 (ℓ ) 𝑖=1 R𝑑𝑘 are linearly . Biases are omitted for := 𝑑𝑘 , 𝑑 ( LM ) ℎ 𝑑 (ℓ 1) ℎ ℎ () = 𝐴 +𝐵, where 𝐴 = 𝑊 ( LM )𝐷 ( LM 1)𝑊 ( LM 1) 𝐷 (1)𝑊 (1), and each 𝐷 (ℓ ) is diagonal {0, 1} matrix selecting the active units. Therefore, when all keys fall into the same linear region (which occurs generically after small perturbation), is single affine transformation. Let := [k1 k𝑚] R𝑑𝑘 𝑚 and := [v1 v𝑚] R𝑑𝑣 𝑚. Exact memorization implies 𝐴K = V, so: rank(V) rank(𝐴), 𝑚 = rank(K) min{rank(𝐴), 𝑑𝑘 }. Now observe: 𝐴 = 𝑊 ( LM ) 𝐷 ( LM 1)𝑊 ( LM 1) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) 𝑅LM 1 𝐷 (1)𝑊 (1) , (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:123)(cid:122) 𝑅 and thus the rank of 𝐴 is bounded by the minimal width encountered along each path times the immediate input dimension: rank(𝐴) LM (cid:18) 𝑖=1 (cid:19) 𝑑 ( 𝑗 ) ℎ min 𝑗 𝑖 𝑑 (𝑖 ) ℎ = (cid:32) 𝑑𝑘𝑑𝑣 LM 𝑖= 𝑑 ( 𝑗 ) ℎ 𝑑 (𝑖+1) ℎ min 𝑗 𝑖 (cid:33) . Hence, 𝑚 (cid:32) 𝑑𝑘𝑑𝑣 LM 𝑖=1 (cid:33) 𝑑 ( 𝑗 ) ℎ 𝑑 (𝑖+1) ℎ min 𝑗 𝑖 Proposition 2 (Memory Capacity with Polynomial Mapping). Let 𝜙𝑝 () be polynomial mapping with degree at most 𝑝, and be matrix-valued memory that optimizes the internal objective of ℓ (M𝑡 ; 𝜙𝑝 (k𝑡 ), v𝑡 ) = M𝑡𝜙𝑝 (k𝑡 ) v𝑡 2 2 with gradient descent. can store the mapping of at most (cid:0)𝑑𝑘 𝑝 (cid:1) pairs of (k𝑖, v𝑖 ) with linearly independent keys, where 𝑑𝑘 is the dimension of keys k𝑖 . Proof. Let us begin by analyzing the dimension of the lifted feature space induced by 𝜙𝑝 . monomial in 𝑑𝑘 variables of total degree exactly ℓ has the form k𝛼 = (cid:206)𝑑𝑘 𝛼 𝑗 = ℓ. The number of such monomials 𝑗=1 is given by the classical stars-and-bars formula, which counts the number of integer solutions to 𝛼1 + + 𝛼𝑑𝑘 = ℓ, yielding , where 𝛼 N𝑑𝑘 and 𝛼 := (cid:205)𝑑𝑘 𝑗= 𝑘𝛼 𝑗 𝑗 (cid:19) (cid:18)𝑑𝑘 + ℓ 1 ℓ . 27 Summing over all degrees ℓ = 0 to 𝑝 gives the total number of monomials (i.e., the output dimension of 𝜙𝑝 ), 𝐷 = 𝑝 ℓ=0 (cid:19) (cid:18)𝑑𝑘 + ℓ 1 ℓ (cid:18)𝑑𝑘 + 𝑝 𝑝 (cid:19) , = where the final identity follows from the hockey-stick identity in combinatorics. To characterize the memorization capacity, we reformulate the loss in matrix notation. Let Φ := [𝜙𝑝 (k1) 𝜙𝑝 (k𝑚)] R𝐷 𝑚 and 𝑉 := [v1 v𝑚] R𝑑𝑣 𝑚. Then the objective becomes 𝐿(M) = 1 2 MΦ 𝑉 2 . Exact memorization corresponds to the existence of matrix such that MΦ = 𝑉 . This is linear system in which acts on the columns of Φ, so the rank of Φ necessarily limits the number of independent targets v𝑖 that can be fitted exactly. By the sub-multiplicativity of rank, for any matrices 𝐴 and 𝐵, we have Applying this to MΦ yields rank(𝐴𝐵) min{rank(𝐴), rank(𝐵)}. rank(MΦ) rank(Φ) 𝐷. Now consider case where the targets v1, . . . , v𝑚 are linearly independent; for instance, take 𝑉 = [𝑒1, . . . , 𝑒𝑚], the first 𝑚 standard basis vectors. Then rank(𝑉 ) = 𝑚. If 𝑚 > 𝐷, we necessarily have rank(MΦ) < rank(𝑉 ) for every choice of M, implying that the system MΦ = 𝑉 is unsolvable. Hence, the loss remains strictly positive, and exact memorization is impossible. This establishes that no method, regardless of optimization procedure, can memorize more than 𝐷 = (cid:0)𝑑𝑘 +𝑝 𝑝 input-output pairs under degree𝑝 polynomial lifting. Since (cid:0)𝑑𝑘 +𝑝 𝑝 memorization capacity is bounded above by (𝑑𝑝 𝑘 ). (cid:1) independent 𝑘 ) for fixed 𝑝, the result follows: the (cid:1) = Θ(𝑑𝑝"
        },
        {
            "title": "D Detailed Formulations of All Architectures",
            "content": "In this section, for the sake of clarity, we discuss the details of all architectures that we discuss through the paper: D.1 Deep Linear Attention (DLA) We design Deep Linear Attention (DLA)linear attention module that uses deep MLP as the memory (KV cache)as one of the baselines of this study. Given input R𝑁 𝑑in, we project the input into matrices of keys, values and queries: q1 ... = (cid:169) (cid:173) (cid:173) q𝑁 (cid:171) k1 ... k𝑁 v1 ... = (cid:169) (cid:173) (cid:173) v𝑁 (cid:171) (cid:170) (cid:174) (cid:174) (cid:172) where W𝑄, W𝐾, and W𝑉 are learnable linear layers. We then define memory as learning module that optimizes the inner-dot product similarity using gradient descent: i.e., = (cid:169) (cid:173) (cid:173) (cid:171) = xW𝑄, = xW𝐾, = xW𝑉 , (cid:170) (cid:174) (cid:174) (cid:172) (cid:170) (cid:174) (cid:174) (cid:172) (48) min . (k𝑡 ), v𝑡 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) ℓ ( M𝑡 1;k𝑡 ,v𝑡 ) (49) The above optimization using gradient descent results in the following recurrence (we also add weight decay with input-dependent parameter 𝛼𝑡 ): which in the case of linear memory (i.e., M𝑡 = 𝑊𝑡 R𝑑 𝑑 ) it becomes: 𝑊𝑡 = 𝛼𝑡𝑊𝑡 1 + v𝑡 𝑡 , M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) (50) (51) which is the formulation of gated linear attention. We use the same training process as other models (see Section 3.3). 28 D.2 Sliding Window Linear Attention (SWLA) The design of SWLA is the same as the design of DLA, but with the use of sliding window objective. That is, given keys, values, and queries: q1 ... = (cid:169) (cid:173) (cid:173) q𝑁 (cid:171) (cid:170) (cid:174) (cid:174) (cid:172) = xW𝑄, k1 ... k𝑁 (cid:170) (cid:174) (cid:174) (cid:172) = (cid:169) (cid:173) (cid:173) (cid:171) = xW𝐾, v1 ... = (cid:169) (cid:173) (cid:173) v𝑁 (cid:171) (cid:170) (cid:174) (cid:174) (cid:172) = xW𝑉 , we optimize the internal objective of: min 𝑡 M𝑡 1(k𝑖 ), v𝑖 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) . 𝑖=𝑡 𝑐+1 (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) ℓ ( M𝑡 1;k𝑡 ,v𝑡 ) (cid:125) The above formulation, results in: M𝑡 = 𝛼𝑡 M𝑡 1 ℓ (M𝑡 1; k𝑡, v𝑡 ) = 𝛼𝑡 M𝑡 1 𝑡 𝑖=𝑡 𝑐+1 𝜂 (𝑡 ) 𝑖 M𝑡 1(k𝑖 ), v𝑖 , which in the case of linear memory (i.e., M𝑡 = 𝑊𝑡 R𝑑 𝑑 ) it becomes: M𝑡 = 𝛼𝑡 M𝑡 1 𝑡 𝑖=𝑡 𝑐+1 𝜂 (𝑡 ) 𝑖 v𝑖 𝑖 . (52) (53) (54) (55) D.3 OmegaNet In the design of OmegaNet, we use replace the dot-prodcut similarity objective with ℓ (M𝑡 1; k𝑡, v𝑡 ) = (cid:205)𝑡 v𝑖 2 2 ,which results in the recurrence of: 𝑖=𝑡 𝑐+1 M𝑡 1(𝜙 (k𝑖 )) M𝑡 = 𝛼𝑡 M𝑡 1 ℓ (M𝑡 1; k𝑡, v𝑡 ) = 𝛼𝑡 M𝑡 1 𝑡 𝑖=𝑡 𝑐+ 𝜂 (𝑡 ) 𝑖 M𝑡 1(𝜙 (k𝑖 )) v𝑖 2 2 . (56) In the above formulation, 𝜙 (.) is the polynomial feature mapping function. D.4 Atlas In the Atlas, we use the same internal objective as OmegaNet but we optimize it using Muon optimizer (Jordan et al. 2024) with weight decay. That is, M𝑡 = 𝛼𝑡 M𝑡 1 + Newton-schulz5(S𝑡 ) 𝑡 S𝑡 = 𝜃𝑡 S𝑡 1 𝜂 (𝑡 ) 𝑖 M𝑡 1(𝜙 (k𝑖 )) v𝑖"
        },
        {
            "title": "E Experimental Details",
            "content": "𝑖=𝑡 𝑐+1 . (57) (58) In our experimental setup we follow recent studies on linear recurrent models (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Yang, Kautz, et al. 2024), we use Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk et al. 2020), HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark, Cowhey, et al. 2018), SIQA (Sap et al. 2019), and BoolQ (Clark, Lee, et al. 2019). Also, the baselines results are from Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al. (2024). In the training, we use T5 tokenizer with vocabulary size of 32K and use training length of 4K tokens (2K for SWA). We employ AdamW optimizer with learning rate of 4𝑒-4 with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of 0.1. The architectural Table 7: Architectural Details."
        },
        {
            "title": "Token",
            "content": "170M 340M 760M 1.3B 12 24 24 18 768 1024 1536 2048 16 16 16 8 3e-3 1.5e-3 1.25e-3 7e-4 15B 15B 30B 100B details are also reported in Table 7. The baseline results for 1.3B are from Yang, Kautz, et al. (2024) and for 760M are from Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al. (2024). For the memory architecture, unless state otherwise, we use an MLP with 2 layers with expansion factor of 4 and GELU activation function (Hendrycks et al. 2016). We also use residual connections and layer norm at the end of each chunk: (𝑥) = 𝑥 + 𝑊1𝜎 (𝑊2𝑥)."
        },
        {
            "title": "F Additional Experimental Results",
            "content": "In this section, we provide additional experimental results to support the design of our models, understand the effect of different components and also evaluate their performance in long context, in-context recall and MAD tasks. F.1 Language Modeling and Common-sense Reasoning (Small Scale) In Section 6 we presented subset of results on language modeling and common-sense reasoning tasks. In this section, we further report the results for all scales of models. The results are in Table 8. State-of-the-art Results. Looking at the performance of Atlas and OmegaNet, both architectures perform favorably compared to modern linear recurrent models and Transformers, achieving lower perplexity and better accuracy in downstream tasks. Even the fully recurrent version of these models outperform hybrid models such as Samba (Ren et al. 2024) and Gated DeltaNet-H2 (Yang, Kautz, et al. 2024). Using the hybrid variants of MAG and MAL further improve the performance of Atlas, which shows the complementary role of recurrent long-term memory and attention. The Effect of Design. Comparing the performance of Atlas, OmegaNet, and baselines SWLA and DLA, we can see the role of ℓ2 regression loss as the attentional bias. Also, the better performance of SWLA compared to GLA and RetNet indicates the importance of memorizing the context, instead of memorizing individual tokens. Table 8: Performance of Atlas and baselines on language modeling and common-sense reasoning tasks. The best results are highlighted highlighted . SIQA BoolQ Avg. acc acc 36.81 36.79 37.13 35.41 37.96 37.32 34.89 37.29 33.82 34.1 37.1 37.5 37.7 38. 58.24 59.72 58.39 60.07 58.79 59.83 59.54 60.22 60.29 59.29 59.7 59.6 60.4 61.2 42.92 42.54 44.09 43.59 44.04 44.51 45.42 46.44 45.93 45.51 44.76 45.31 46.93 47. Model Wiki. ppl LMB. ppl LMB. PIQA Hella. Wino. ARC-e ARC-c acc_n acc acc_n acc acc acc 340M params / 15B tokens Transformer++ RetNet GLA Mamba DeltaNet TTT Gated DeltaNet Moneta Yaad Memora DLA (ours) SWDT (ours) OmegaNet (ours) Atlas (ours) 31.52 32.50 28.51 30.83 28.65 27.44 27.01 26.19 26.61 27.16 27.93 26.98 26.03 25.88 41.08 49.73 43.02 40.21 47.30 34.19 30.94 29.31 29.11 30.44 35.09 33.95 28.76 28. 30.76 28.24 28.73 29.94 28.43 30.06 34.11 35.70 34.09 33.68 30.8 32.4 35.6 36.1 62.98 62.61 64.05 63.79 63.52 63.97 63.08 63.99 64.93 65.21 62.9 63.1 65.3 64. 34.76 34.15 35.96 35.88 35.95 35.71 38.12 39.23 39.86 39.17 36.2 38.2 39.7 40.1 50.53 50.91 50.00 49.82 49.63 50.08 51.60 52.04 51.12 51.23 50.4 50.9 52.0 52. 45.21 44.27 54.19 49.24 52.68 53.01 55.28 55.96 54.75 53.40 53.5 54.9 56.1 56.4 24.05 23.62 24.29 24.56 25.37 26.11 26.77 27.15 28.64 27.99 26.7 25.9 28.6 28."
        }
    ],
    "affiliations": [
        "Google"
    ]
}