{
    "paper_title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient",
    "authors": [
        "Rui Ming",
        "Haoyuan Wu",
        "Shoubo Hu",
        "Zhuolun He",
        "Bei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 1 3 6 2 . 9 0 5 2 : r ONE-TOKEN ROLLOUT: GUIDING SUPERVISED FINETUNING OF LLMS WITH POLICY GRADIENT Rui Ming1 Haoyuan Wu1,2 Shoubo Hu2 Zhuolun He1,3 Bei Yu1 1The Chinese University of Hong Kong 2Noahs Ark Lab, Huawei 3ChatEDA Tech"
        },
        {
            "title": "ABSTRACT",
            "content": "Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from more fundamental difference: SFT learns from fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce onetoken rollout (OTR), novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as single-step reinforcement learning trajectory. At each step, it performs Monte Carlo rollout by sampling multiple candidate tokens from the current policys distribution. The ground-truth token from the supervised data is then used to provide reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is critical driver of generalization, offering promising new direction for fine-tuning LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Supervised fine-tuning (SFT) has become cornerstone for adapting large language models (LLMs) to downstream tasks (Ouyang et al., 2022; Chung et al., 2022; Zhang et al., 2025). However, growing body of evidence suggests that while SFT excels at mimicking expert demonstrations, it often struggles with generalization compared to methods based on reinforcement learning (RL) (Chu et al., 2025a; Huan et al., 2025; Shenfeld et al., 2025). Recent research Chu et al. (2025a) has proposed the view that SFT memorizes, while RL generalizes. This limitation is particularly concerning as SFT can disrupt the well-formed distributions learned during pre-training, leading to degradation of general capabilitiesa phenomenon sometimes referred to as catastrophic forgetting (Kumar et al., 2022; Huan et al., 2025; Shenfeld et al., 2025). This generalization gap motivates deeper investigation into the fundamental differences between SFT and RL, with the goal of enhancing the generalization of SFT by borrowing principles from RL. Recent advancements in RL have demonstrated that even simplified methods, such as GPG (Chu et al., 2025b), which directly optimize an objective structurally similar to weighted SFT loss, can achieve performance comparable to more complex algorithms like PPO (Schulman et al., 2017) or GRPO (Shao et al., 2024). This suggests that the performance disparity between SFT and RL may not solely stem from the loss function, but also from more fundamental difference: the nature of the data used for updates. SFT typically relies on static, pre-collected set of expert demonstrations, which is known as off-policy data, whereas RL methods utilize on-policy data sampled iteratively from the current policy. Equal Contribution 1 As RL becomes an increasingly popular paradigm for fine-tuning LLMs, the critical role of on-policy data has garnered significant attention (Tajwar et al., 2024; Ren & Sutherland, 2024; Shenfeld et al., 2025). Tajwar et al. (2024) has shown that on-policy sampling is crucial for RL to discover optimal policies, especially when the target behavior lies in low-probability regions of the initial model. It provides more stable and effective learning signal by ensuring that policy updates are made in regions the model can already reach, thereby preventing drastic and potentially harmful shifts in the output distribution (Ren & Sutherland, 2024; Shenfeld et al., 2025). This suggests that the onpolicy nature of RL is key factor contributing to its superior generalization and ability to preserve pre-trained knowledge. Inspired by these insights, we propose one-token rollout (OTR) algorithm, novel fine-tuning method that aims to enhance the generalization of SFT from data-centric perspective. OTR guides the fine-tuning process with the policy gradient method, treating each token-generation step as an individual, on-policy learning event. By performing Monte Carlo rollout at each token position which samples candidate tokens from the current policy and using the ground-truth token as reward signal, OTR transforms the off-policy supervised data into token-level on-policy signal. OTR enhances generalization by narrowing the data-side gap between SFT and RL, while its design as token-level method bypasses the costly generation of complete, sentence-level on-policy training data. Our extensive experiments demonstrate that this on-policy simulation consistently improves the generalization of fine-tuned models across wide array of challenging mathematical, coding, and general reasoning benchmarks. These results not only validate the efficacy of OTR as powerful alternative for fine-tuning LLMs but also provide strong evidence for the critical role that on-policy data plays in the generalization performance of fine-tuned language models. Our contributions can be summarized as follows: We introduce One-Token Rollout, novel fine-tuning algorithm that guides SFT with the policy gradient method. By treating each token generation as single-step reinforcement learning task, OTR improves model generalization without incurring the high computational cost of full sentence generation. We provide new data-centric perspective on the SFT-RL generalization gap, positing that the on-policy nature of training data is critical factor. The success of our token-level on-policy simulation serves as strong evidence for this viewpoint. We conduct extensive experiments on wide array of challenging benchmarks across mathematical, coding, and general reasoning domains. Our results empirically demonstrate that OTR consistently outperforms SFT, validating its efficacy as powerful and practical alternative for fine-tuning LLMs."
        },
        {
            "title": "2.1 SUPERVISED FINE-TUNING",
            "content": "The standard approach for adapting pre-trained LLMs to specific downstream tasks is Supervised Fine-Tuning. Given dataset of prompt-response pairs, where the response is sequence of tokens {x1, x2, . . . , xT }, SFT aims to maximize the conditional probability of the ground-truth sequence. This is typically achieved by minimizing the negative log-likelihood loss, autoregressively training the model πθ to predict the next token xt given the preceding context x1:t1: LSFT(θ) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 log πθ(xtx1:t1). (1)"
        },
        {
            "title": "2.2 POLICY GRADIENT",
            "content": "Policy gradient represents class of reinforcement learning algorithms that directly optimize parameterized policy, πθ. In this framework, the text generation process is modeled step-by-step. At each timestep t, the state st is the sequence of previously generated tokens x1:t1, and the action at is the next token selected by the policy from the vocabulary. 2 The core objective is to adjust the policys parameters, θ, to maximize the expected total reward. This objective function, J(θ), is defined as the expected cumulative reward: J(θ) = Eτ πθ (cid:35) r(st, at) , (cid:34) (cid:88) t=1 (2) where r(st, at) is the scalar reward received after taking action at in state st, and τ is the entire sequence of states and actions (s1, a1, s2, a2, . . . ), known as trajectory. The policy is improved by ascending the gradient of this objective, θJ(θ). The foundational policy gradient theorem provides way for the gradient computation: θJ(θ) = Eτ πθ (cid:34)(cid:32) (cid:88) t=1 θ log πθ(at st) (cid:33) (cid:32) (cid:88) t=1 (cid:33)(cid:35) r(st, at) , (3) where θ log πθ(at st) indicates the direction in the parameter space that would be used to update the policy πθ. This direction is then weighted by the sum of all rewards in the trajectory, effectively reinforcing action sequences that lead to higher overall rewards."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We introduce the One-Token Rollout algorithm, novel fine-tuning method that adapts the principles of Policy Gradient to the token level. OTR reframes the standard fine-tuning process by treating each individual token generation step as complete, single-step trajectory. This conceptual shift allows us to simplify the general policy gradient framework into highly efficient, token-level reinforcement learning algorithm, where the supervised training data is repurposed to provide reward signal."
        },
        {
            "title": "3.1 FROM POLICY GRADIENT TO ONE-TOKEN ROLLOUT",
            "content": "Our starting point is the foundational policy gradient theorem introduced in the Section 2. The core innovation of OTR is to consider the generation of single token from state st to an action at as complete trajectory of length one. In this micro-trajectory, the summations over timesteps present in Equation (3) collapse, as there is only single state-action pair. Consequently, the summations of θ log πθ(at st) and r(st, at) over tokens in the original formula both reduce to terms for an individual token, and sampling full trajectory τ simplifies to sampling single action at from the policy πθ(st). The policy gradient for this single step thus simplifies dramatically to: θJ(θ) = Eatπθ(st) [θ log πθ(at st) r(st, at)] . (4) To implement this, we approximate the expectation E[] using Monte Carlo estimation. At each timestep of the original sequence, we perform rollout by sampling multiple candidate actions from the current policy. This transforms the optimization problem into practical, sample-based loss function."
        },
        {
            "title": "3.2 TOKEN-LEVEL ROLLOUT AND ON-POLICY REWARD",
            "content": "To facilitate the rollout, we first define stochastic sampling policy and reward mechanism. Stochastic Policy for Exploration. For given state st, the LLMs first compute vector of raw, unnormalized scores for every token in the vocabulary . These scores are known as logits. Let la denote the logit corresponding to specific action a. The models base policy, πθ, is typically derived by applying the softmax function directly to these logits. To encourage exploration during the rollout, we create new sampling policy, π temperature parameter κ. The sampling policy is defined as: (cid:18) la κ π θ(ast) = softmax θ, by introducing (5) (cid:19) . Consistent with its common use during model inference, the temperature adjusts the shape of the final probability distribution. We utilize temperature κ > 1 to flatten the distribution, which increases the likelihood of sampling less probable tokens and thereby enhances exploration. 3 Figure 1: An illustration of the computational divergence between SFT and OTR. Rollout and Reward Definition. At each timestep t, we draw set of candidate actions from our exploration policy: j=1, where each (6) Crucially, we use the ground-truth token xt from the supervised dataset to construct an immediate reward signal. Each sampled action t,j is evaluated against xt using the following reward function: t,j π = {a θ(st). t,j}K R(a t,j, xt) = (cid:26)1 if β if t,j = xt, t,j = xt. (7) Here, β is hyperparameter where β < 1. reward of 1 is given for rediscovering the groundtruth token, while lesser reward β is given for all other tokens. We finally set β = 0.1 for our main experiments based on the ablation study detailed in Section 4.4. This design elegantly converts the traditionally off-policy supervised data into an on-policy learning signal at the token level. The actions we evaluate are sampled directly from the current policy π θ, and the fixed ground-truth token xt is used simply to assign real-time reward to these onpolicy actions. This avoids the complexities of importance sampling or other off-policy correction techniques typically required in sentence-level RL."
        },
        {
            "title": "3.3 THE OTR OBJECTIVE FUNCTION",
            "content": "Based on the token-level rollout and policy gradient in Equation (4), the loss at timestep is the Monte Carlo approximation of the negative policy gradient objective, averaged over the samples: Lt OTR(θ) ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) j=1 (cid:2)sg (cid:0)R(a t,j, xt)(cid:1) log πθ(a t,jst)(cid:3) , (8) where πθ is the models original, non-temperature-scaled policy, and sg() is the stop-gradient operator. Given our defined reward function, we can decompose this loss. Let Ngt = (cid:80)K t,j = xt) be the count of times the ground-truth token was sampled. The loss function simplifies to its final form: I(a j=1 Lt OTR(θ) ="
        },
        {
            "title": "1\nK",
            "content": "Ngt log πθ(xtst) + β (cid:88) log πθ(a t,jst) . (9) s.t. t,j =xt This per-timestep objective has an intuitive interpretation. The first term is SFT-like loss for the ground-truth token, but it is dynamically weighted by its sampling frequency Ngt. If the groundtruth is never sampled, its loss contribution is zero. The second term acts as regularizer, weighted by β, which penalizes the model for assigning high probability to the incorrect tokens it sampled. The total loss for an entire sequence of length is the average of these per-timestep losses: LOTR(θ) = Lt OTR(θ)."
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 4 (10) This objective allows OTR to focus its optimization effort, reinforcing correct predictions that are already within the models reach while gently suppressing plausible alternatives, creating more nuanced and effective learning signal than SFT alone. To visually summarize the computational divergence of the OTR update from the strandard SFT, we provide detailed illustration in Figure 1."
        },
        {
            "title": "3.4 COMPARISON WITH DYNAMIC FINE-TUNING",
            "content": "Our work is related to the concurrent dynamic fine-tuning (DFT) method (Wu et al., 2025c), which also seeks to improve the generalization of SFT from reinforcement learning perspective. DFTs motivation stems from the insight that the standard SFT gradient contains an implicit, problematic inverse-probability weighting (1/πθ) that leads to optimization instability. To address this, DFT proposes to rectify the reward by reweighting the loss for the ground-truth token xt with its own model probability πθ(xtst). The resulting per-timestep DFT loss is: Lt DFT(θ) = sg(πθ(xtst)) log πθ(xtst). (11) The OTR framework can be seen as generalization of DFT. This relationship becomes clear when we consider the special case of our OTR objective where the hyperparameter β = 0. In this scenario, the second term in Equation (9), which penalizes incorrect samples, vanishes. Then the OTR loss can be formulated as: Lt OTR(θ)β=0 ="
        },
        {
            "title": "Ngt\nK",
            "content": "log πθ(xtst), (12) where Ngt/K represents the empirical frequency of sampling the ground-truth token during the rollout. This frequency is direct Monte Carlo approximation of the ground-truth tokens probability, i.e., Ngt πθ(xtst). Thus, when β = 0, the OTR objective is functionally equivalent to the DFT objective, as both methods effectively weight the loss of the ground-truth token by its estimated probability. However, when β = 0, OTR extends beyond DFTs formulation. In addition to reinforcing the rediscovered ground-truth token, OTRs objective incorporates crucial second term: regularization penalty applied to the incorrect tokens sampled during the rollout. This allows OTR to not only learn from the positive signal of the ground-truth but also to actively discourage the model from assigning high probability to plausible but incorrect alternatives. Therefore, OTR provides more comprehensive learning signal by leveraging information from both successful and unsuccessful samples within the models own distribution."
        },
        {
            "title": "4.1 EXPERIMENT SETTINGS",
            "content": "Dataset and Models. We conduct experiments on the OpenR1-Math-220k dataset (OpenR1 Team, 2025), which consists of 220,000 mathematical problems with detailed reasoning traces. These traces are generated by the DeepSeek R1 model (DeepSeek-AI et al., 2025) for problems originating from the NuminaMath-1.5 dataset (LI et al., 2024). To efficiently manage computational resources while ensuring data quality, we randomly sample subset of 5,000 instances for our training set. All selected instances have reasoning traces with lengths under 8192 tokens, and their lengths are approximately uniformly distributed across different intervals. We utilize suite of powerful and contemporary open-source LLMs as base models. Specifically, we conduct our experiments on the following models: Qwen2.5-3B (Qwen Team, 2024), Qwen2.5-7B (Qwen Team, 2024), Qwen3-4BBase (Qwen Team, 2025), and Qwen3-8B-Base (Qwen Team, 2025). Training Details. Our implementation is built upon the Verl framework, and to ensure fair comparison, both our proposed OTR algorithm and the SFT baseline are trained using identical settings. We employ the AdamW optimizer with learning rate of 5 106. The learning rate follows cosine decay schedule, which includes warm-up ratio of 0.03 and decays to 1 106. For the training configuration, we use batch size of 64 and maximum sequence length of 10240 tokens. All models are trained for total of 2 epochs. 5 Table 1: Main results on in-domain mathematical reasoning benchmarks. For each model, the best result between SFT and OTR is in bold. The symbol indicates performance degradation compared to the base model. Model Method GSM8K MATH Olympiad Minerva AIME24 AIME AMC23 Average Qwen2.5-3B Qwen2.5-7B Qwen3-4B Qwen3-8B Base SFT OTR Base SFT OTR Base SFT OTR Base SFT OTR 77.90 82.05 82.93 85.36 88.18 89. 86.90 74.13 91.98 90.40 83.77 91.63 42.64 62.50 63. 49.80 67.75 70.45 54.10 63.95 75.30 60.80 77.40 79. 25.20 26.23 27.05 36.40 31.53 35.33 38.20 32.10 40. 40.90 41.70 42.43 23.20 24.90 25.00 28.30 32.53 33. 29.80 29.60 36.68 34.20 37.70 39.35 3.30 7.30 7. 6.70 8.54 8.33 3.30 10.21 10.22 13.30 15.20 14. 0.00 1.65 2.91 3.30 5.00 6.87 6.70 6.24 11. 16.70 15.63 14.17 40.00 37.03 40.78 42.50 43.75 44. 55.00 42.66 52.81 62.50 55.16 59.53 30.32 34.52 35. 36.05 39.61 41.23 39.14 36.98 45.61 45.54 46.65 48. Table 2: Out-of-domain performance on code generation and general reasoning benchmarks. For each model, the best result between SFT and OTR is in bold. The symbol indicates performance degradation compared to the base model."
        },
        {
            "title": "Method",
            "content": "HumanEval+ MBPP+"
        },
        {
            "title": "SuperGPQA",
            "content": "MMLU-Pro"
        },
        {
            "title": "General Reasoning",
            "content": "35.40 57.60 59.30 48.80 68.50 69.00 56.70 70.20 74. 61.60 76.00 77.70 50.30 48.20 49.90 64.00 58.10 59.20 62.40 60.90 62.90 63.50 65.40 66.50 42.85 52.90 54. 56.40 63.30 64.10 59.55 65.55 68.45 62.55 70.70 72. 6.00 7.23 7.88 6.88 10.44 11.28 8.19 9.27 9. 9.91 10.40 10.02 19.28 18.67 19.22 23.93 26.25 26. 28.56 28.11 29.03 32.53 29.03 30.49 33.90 36.15 36. 42.31 51.24 51.22 53.35 53.86 55.96 59.57 19. 20.68 21.10 24.37 29.31 29.61 30.03 30.41 31.57 34. 53.82 56.87 31.08 32.46 Qwen2.5-3B Qwen2.5-7B Qwen3-4B Qwen3-8B Base SFT OTR Base SFT OTR Base SFT OTR Base SFT OTR"
        },
        {
            "title": "4.2 EVALUATION",
            "content": "Our evaluation is designed to accurately reflect the impact of the SFT and OTR fine-tuning algorithms on the base models capabilities. To this end, we utilize suite of challenging benchmarks spanning mathematical, code, and general reasoning domains to test the generalization of the algorithms, and we employ distinct evaluation settings for the base and fine-tuned models. For all evaluations, the maximum generation length is set to 8192 tokens. Benchmarks and Metrics. Our evaluation covers suite of challenging benchmarks across three domains. For mathematical reasoning, our evaluation includes Minerva Math (Lewkowycz et al., 2022), MATH-500 (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), OlympiadBench (He et al., 2024), AMC 2023, AIME 2024, and AIME 2025. For the highly challenging AMC 2023, AIME 2024, and AIME 2025 benchmarks, we report mean@16 accuracy, while for the remaining math benchmarks, we report mean@4 accuracy. For code generation, we use HumanEval Plus (Liu et al., 2023) and MBPP Plus (Liu et al., 2023), with performance measured by the pass@1 metric. Finally, for general domain reasoning, we evaluate on MMLU-Pro (Wang et al., 2024), SuperGPQA (Du et al., 2025a), and BBEH (Kazemi et al., 2025) using Exact Match (EM) accuracy. Base Model Evaluation. To align with standard evaluation practices for base models, we use natural prompt template for testing. Specifically, we employ 5-shot setting for the MATH-500 and 6 Table 3: Ablation study on the hyperparameter β for in-domain mathematical reasoning. Model Method SFT Qwen2.5-3B OTR β = -1.00 -0.10 0.00 0.01 SFT Qwen3-4B OTR β = -1.00 -0.10 0.00 0.01 GSM8K MATH Olympiad Minerva AIME24 AIME25 AMC23 Average 82.05 83.10 82.93 83.65 83.75 74.13 92.15 91.98 91.03 90. 62.50 63.05 63.95 63.10 63.70 63.95 77.75 75.30 76.30 76.30 26.23 26.05 27.05 27.48 27. 32.10 40.60 40.63 40.75 39.35 24.90 25.75 25.00 22.35 24.65 29.60 35.68 36.68 36.88 36. 7.30 6.88 7.71 8.34 5.43 10.21 12.09 10.22 10.20 9.79 1.65 2.91 2.91 2.69 2. 6.24 13.33 11.67 10.63 9.79 37.03 37.66 40.78 39.53 36.72 42.66 53.75 52.81 53.28 51. 34.52 35.06 35.76 35.31 34.89 36.98 46.48 45.61 45.58 44.82 GSM8K benchmarks and use greedy sampling strategy with temperature of 0 for decoding for all benchmarks. Fine-tuned Model Evaluation. For the chat models fine-tuned with SFT and OTR, we use their respective chat templates and 0-shot setting across all benchmarks. The decoding strategy is stochastic sampling with temperature of 0.7 and top-p of 0.8."
        },
        {
            "title": "4.3 RESULTS",
            "content": "We present the main experimental results in Table 1 for in-domain generalization and Table 2 for out-of-domain (OOD) generalization. For all OTR experiments presented in this section, we set the key hyperparameters for our algorithm: the temperature parameter κ = 1.3, the number of rollout candidates = 256, and the reward hyperparameter β = 0.1. The value for β was determined to yield the best overall performance based on our ablation studies detailed in Section 4.4. In-Domain Generalization. As shown in Table 1, OTR consistently demonstrates superior performance over SFT on mathematical reasoning tasks. Across all four model families, OTR achieves higher average score. This highlights OTRs effectiveness in enhancing the specialized capabilities of the models within their training domain. Furthermore, OTR shows greater generalization by mitigating the catastrophic forgetting often observed during fine-tuning. The number of instances where performance degrades below the base model (marked by the symbol) is significantly lower for OTR (4 instances) compared to SFT (10 instances). Even in cases where both methods underperform, OTRs performance drop is considerably milder. For example, on the AMC23 benchmark with Qwen3-8B, SFTs score drops by 7.34 points relative to the base model, whereas OTRs score drops by only 2.97 points. This suggests that OTRs on-policy signal helps preserve the valuable knowledge learned during pre-training. Out-of-Domain Generalization. The advantages of OTR extend to out-of-domain tasks, as detailed in Table 2. On both code generation and general reasoning benchmarks, OTR consistently surpasses SFT in average performance across all models. This trend demonstrates that OTR effectively leverages its on-policy signal to achieve broader, more generalized capabilities that are not confined to its training domain. From the perspective of knowledge preservation, OTR again proves to be more generalizable algorithm. SFT underperforms its base model in 7 OOD instances, particularly showing vulnerability on SuperGPQA and MMLU-Pro with larger models. In contrast, OTR underperforms in 5 instances and shows consistent improvements on general reasoning for the Qwen3-4B model where SFT struggles. This demonstrates that OTR provides more reliable fine-tuning approach that not only enhances target skills but also better maintains the models general intelligence, leading to superior overall generalization. supplementary experiment is detailed in Section A."
        },
        {
            "title": "4.4 ABLATION STUDY",
            "content": "To investigate the impact of the reward hyperparameter β, we conduct comprehensive ablation study. We select four values for analysis, ranging from negative to positive: -1.0, -0.1, 0, and 0.01. The performance across in-domain and out-of-domain benchmarks is presented in Table 3 7 Table 4: Ablation study on the hyperparameter β for out-of-domain generalization. Model Method SFT Qwen2.5-3B OTR β = -1.00 -0.10 0.00 0. SFT Qwen3-4B OTR β = -1.00 -0.10 0.00 0.01 Code General Tasks HumanEval+ MBPP+ Avg BBEH SuperGPQA MMLU-Pro Average 57.60 57.90 59.30 58.90 60.50 70.20 74.20 74.00 73.70 73.20 48.20 49.70 49.90 49.20 49. 60.90 61.10 62.90 61.90 62.20 52.90 53.80 54.60 54.05 55.20 65.55 67.65 68.45 67.80 67. 7.23 7.28 7.88 8.38 7.59 9.27 9.91 9.71 9.54 8.38 18.67 19.07 19.22 19.39 19. 28.11 28.65 29.03 28.24 26.11 36.15 36.26 36.20 36.16 36.49 53.86 56.51 55.96 53.93 50. 20.68 20.87 21.10 21.31 21.05 30.41 31.69 31.57 30.57 28.29 (a) Effect of β on GT token counts. (b) GT token counts for larger models. Figure 2: Analysis of the number of GT tokens sampled during training. (a) Compares OTR with different β values against SFT on Qwen3-4B. (b) Compares OTR and SFT on larger models. and Table 4, respectively. To provide insight into the training process for our subsequent analysis, we also track key diagnostic metric: the number of ground-truth (GT) tokens sampled during the token-level rollout. For direct comparison, we also record this metric for SFT. It is important to note that this measurement is for analysis only and does not alter the standard SFT algorithm. Effect of β on Training Stability. Our first key observation relates to training stability, as depicted in Figure 2(a). While most OTR variants show stable increase in GT token counts, the setting with β = 0.1 exhibits clear training instability. Its GT count initially rises but then collapses in the later stages. We hypothesize that assigning positive reward to incorrectly sampled tokens, especially relatively high one, can mislead the optimization process. This may cause the model to increase the probabilities of all rolled-out tokens indiscriminately, ultimately leading to degradation of the learned distribution. This observed instability motivates us to limit our search space, leading to our selection of β values {-1.0, -0.1, 0, 0.01}, which primarily explores the non-positive range. Impact on Performance and Optimal β Selection. From the performance results in Table 3 and Table 4, it is evident that OTR is robustly superior to SFT. Regardless of the specific β value, OTR variants consistently outperform the SFT baseline in terms of average scores across nearly all domains and models. Among these variants, the setting of β = 0.1 demonstrates the most consistent and high-level performance across both in-domain and OOD tasks. Therefore, we select β = 0.1 as the default value for our main experiments. The Importance of Negative Samples. This study also provides insight into the importance of utilizing negative samples. As analyzed in Section 3.4, OTR with β = 0 can be viewed as Monte Carlo approximation of the DFT method. direct comparison between the β = 0.1 and β = 0 rows in our tables reveals that the former almost universally outperforms the latter. This result provides empirical evidence that incorporating an explicit penalty for negatively sampled tokens is crucial component of OTRs success, contributing to more effective learning signal than what is offered by SFT-like formulations. Analysis of Learning Dynamics. Finally, we analyze the source of OTRs general superiority over SFT by examining the GT token counts at convergence in Figure 2. Across different models, scales, and architectures (as shown in both Figure 2(a) and Figure 2(b)), OTR-trained models consistently converge to higher number of sampled GT tokens than SFT-trained models. higher GT count indicates that the models learned policy assigns higher probability to the ground-truth sequences, which suggests lower perplexity on the training data. We infer from this that OTR enables the model to learn from and utilize the training data more profoundly and efficiently than SFT, potentially unlocking higher performance ceiling."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reinforcement Learning for Language Models. Recently, reinforcement learning has gained significant traction as powerful paradigm for enhancing the capabilities of large language models (Hu, 2025; DeepSeek-AI et al., 2025; Wu et al., 2025a). The success of state-of-the-art models, which have leveraged RL-based algorithms like GRPO (Shao et al., 2024) to achieve substantial improvements in reasoning and cross-domain generalization, has catalyzed surge of interest in these methods. The traditional approach to RL fine-tuning, reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), often relies on complex and computationally intensive algorithms like PPO (Schulman et al., 2017). The inherent instability and implementation complexity of PPO have motivated recent wave of research focused on simplifying the RLHF pipeline. prominent line of work, including methods like DPO (Rafailov et al., 2023) and GEPO (Wu et al., 2025b), elegantly reframes the preference learning objective to create simple loss, eliminating the need for an explicit reward model. In similar spirit, GPG (Chu et al., 2025b) simplifies the RL objective into weighted maximum likelihood form, demonstrating that such simplified approach can match the performance of more complex algorithms. Improving Supervised Fine-tuning. While SFT is the most widely used paradigm for fine-tuning, its limitations, such as catastrophic forgetting and deviation from the pre-trained models distribution, are well-documented (Kumar et al., 2022; Huan et al., 2025). major line of research aims to improve SFT by modifying its objective function. prominent example is proximal SFT (Zhu et al., 2025), which introduces proximal regularization term to the SFT loss to penalize divergence from the initial models policy. This approach is analogous to the KL-divergence constraint in PPO and helps stabilize training and preserve pre-trained knowledge. Another significant line of work seeks to enhance SFT by reformulating it through the lens of reinforcement learning, often by establishing mathematical connection between their objectives. For instance, some studies reframe RLHF as reward-weighted form of SFT (Du et al., 2025b), while others view SFT as an RL method with an implicit reward function (Wang et al., 2025; Qin & Springenberg, 2025). Concurrent to our work, DFT (Wu et al., 2025c) identifies an implicit inverse-probability weighting in the SFT gradient and addresses the resulting instability by re-weighting the loss for the ground-truth token with its own model probability. Although these works build theoretical bridge between SFT and RL, they primarily focus on re-weighting the loss for the static, ground-truth expert data. In contrast, our work offers distinct, data-centric solution. OTR moves beyond loss modification and instead transforms the training data itself into dynamic, on-policy signal by actively sampling from the models current policy."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we investigated the generalization weakness of SFT compared to RL, positing that the disparity stems from the fundamental difference between SFTs static, off-policy data and RLs dynamic, on-policy data. To bridge this gap from data-centric perspective, we introduced OneToken Rollout, novel fine-tuning algorithm. By reframing each token generation as singlestep reinforcement learning trajectory, OTR transforms the static supervised dataset into dynamic, on-policy learning signal, successfully incorporating the advantage of on-policy data into the SFT framework while maintaining its computational efficiency. Our extensive experiments empirically validate this approach, demonstrating that OTR consistently outperforms SFT on wide array of in-domain and out-of-domain benchmarks. Ultimately, we present OTR as powerful and practical alternative for fine-tuning LLMs, providing compelling evidence that simulating on-policy interaction is key direction for developing more generalizable fine-tuned language models."
        },
        {
            "title": "REFERENCES",
            "content": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025a. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. GPG: Simple and Strong Reinforcement Learning Baseline for Model Reasoning. arXiv preprint arXiv:2504.02546, 2025b. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling Instruction-Finetuned Language Models. arXiv preprint arXiv:2210.11416, 2022. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025a. Yuhao Du, Zhuo Li, Pengyu Cheng, Zhihong Chen, Yuejiao Xie, Xiang Wan, and Anningzhe Gao. Simplify rlhf as reward-weighted sft: variational method. arXiv preprint arXiv:2502.11026, 2025b. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning. arXiv preprint arXiv:2507.00432, 2025. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. tuning can distort pretrained features and underperform out-of-distribution. arXiv:2202.10054, 2022. FinearXiv preprint Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 2022. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. NuminaMath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. OpenR1 Team. OpenR1-Math-220k. https://huggingface.co/datasets/open-r1/ OpenR1-Math-220k, 2025. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. Chongli Qin and Jost Tobias Springenberg. Supervised fine tuning on curated data is reinforcement learning (and can be improved). arXiv preprint arXiv:2507.12856, 2025. Qwen Team. Qwen2.5: party of foundation models. September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Qwen Team. Qwen3 technical report. 2025. URL https://arxiv.org/abs/2505.09388. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 2023. Yi Ren and Danica Sutherland. Learning dynamics of llm finetuning. arXiv preprint arXiv:2407.10490, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, 2024. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. RLs Razor: Why Online Reinforcement Learning Forgets Less. arXiv preprint arXiv:2509.04259, 2025. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, and Xipeng Qiu. Implicit Reward as the Bridge: Unified View of SFT and DPO Connections. arXiv preprint arXiv:2507.00018, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 2024. Haoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, and Bei Yu. ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving. arXiv preprint arXiv:2505.12717, 2025a. Haoyuan Wu, Rui Ming, Jilong Gao, Hangyu Zhao, Xueyi Chen, Yikai Yang, Haisheng Zheng, Zhuolun He, and Bei Yu. On-Policy Optimization with Group Equivalent Preference for MultiProgramming Language Understanding. arXiv preprint arXiv:2505.12723, 2025b. 11 Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the Generalization of SFT: Reinforcement Learning Perspective with Reward Rectification. arXiv preprint arXiv:2508.05629, 2025c. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction Tuning for Large Language Models: Survey. arXiv preprint arXiv:2308.10792, 2025. Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, and Pengfei Liu. Proximal Supervised Fine-Tuning. arXiv preprint arXiv:2508.17784, 2025."
        },
        {
            "title": "A ADDITIONAL EXPERIMENT",
            "content": "To assess the robustness of our method and validate its generalization benefits across different training data and training configurations, we conduct an additional experiment. For this analysis, we adopt the training data and hyperparameter settings from the concurrent work, dynamic finetuning (Wu et al., 2025c), which provides distinct training environment to test the efficacy of OTR. This comparative analysis focuses on the Qwen2.5-3B (Qwen Team, 2024) and Qwen3-4B (Qwen Team, 2025) models, with the detailed setup provided below. Dataset. We train with the NuminaMath CoT dataset (LI et al., 2024), which comprises around 860,000 mathematical problems paired with corresponding solutions. To efficiently manage computational resources, we randomly sample 50,000 instances from this dataset for training. Training Details. Our implementation is built upon the Verl framework. For fair comparison, both our proposed OTR algorithm and the SFT baseline are trained using identical settings. Specifically, we employ the AdamW optimizer with peak learning rate of 5 105. The learning rate follows cosine decay schedule with warm-up ratio of 0.1. We use batch size of 256, maximum input length of 4096 tokens, and train all models for 1 epoch. As shown in Table 5, even under the training settings adapted from DFT, our OTR method consistently outperforms the standard SFT baseline across the majority of benchmarks. This finding demonstrates the robustness of the OTR algorithm and suggests that its generalization benefits are not confined to specific set of data and hyperparameters but hold true across different settings. Table 5: Results of SFT and OTR on in-domain math benchmarks when trained under the DFT experimental settings. For each model, the best result is in bold."
        },
        {
            "title": "Method",
            "content": "GSM8K"
        },
        {
            "title": "Minerva",
            "content": "AIME24 AIME25 AMC"
        },
        {
            "title": "Average",
            "content": "Qwen2.5-3B Qwen3-4B SFT OTR SFT OTR 78.50 78.70 88.75 88. 53.25 57.10 64.80 68.65 19.43 21.53 30.60 33.88 16.55 21.50 27.30 25. 2.28 2.70 6.25 9.38 0.83 1.86 4.38 6.46 24.53 28.75 35.78 42. 27.91 30.31 36.84 39."
        },
        {
            "title": "LIMITATIONS",
            "content": "While our experiments demonstrate OTRs consistent advantages across range of models and benchmarks, this work has several limitations. First, due to computational constraints, our study is conducted on models up to 8 billion parameters and trained on subset of mathematics-focused dataset. Consequently, the scalability of OTR to larger-scale models (e.g., 70B+) remains to be validated. Second, our investigation is confined to the text-only modality. The reward mechanism, while effective, is also relatively simple. Future work will aim to address these limitations by scaling OTR to larger models, training on larger datasets, and extending it to broader training domains. We also plan to explore more sophisticated reward functions, investigate the potential of multi-token rollouts, and extend the OTR framework to other modalities, such as vision-language tasks."
        }
    ],
    "affiliations": [
        "ChatEDA Tech",
        "Noahs Ark Lab, Huawei",
        "The Chinese University of Hong Kong"
    ]
}