{
    "paper_title": "Code Aesthetics with Agentic Reward Feedback",
    "authors": [
        "Bang Xiao",
        "Lingjie Jiang",
        "Shaohan Huang",
        "Tengchao Lv",
        "Yupan Huang",
        "Xun Wu",
        "Lei Cui",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach."
        },
        {
            "title": "Start",
            "content": "Bang Xiao1,2 Lingjie Jiang1,3 Shaohan Huang1 Tengchao Lv1 Yupan Huang1 Xun Wu1 Lei Cui1 1 Microsoft Research Asia 2 Zhiyuan College, Shanghai Jiao Tong University 3 Peking University Furu Wei"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B685B parameters, underscoring the effectiveness of our approach. 5 2 0 2 7 2 ] . [ 1 2 7 2 3 2 . 0 1 5 2 : r Figure 1: Performance comparison of different models on the OpenDesign benchmark. Left: static score evaluation. Right: interactive score evaluation. Equal contribution. Corresponding author. Project pape: https://bangx7.github.io/code-aesthetics"
        },
        {
            "title": "Introduction",
            "content": "LLMs have become powerful assistants in our daily lives, helping us polish writing, refine code, and access knowledge [Tea25, DAGY+25, Ope25b]. Recently, coding LLMs have achieved great sucess in various code related fields, such as code completion, bug fixing, and software engineering [Ant25a, GZY+24, HYC+24b, Tea25]. While LLMs have demonstrated remarkable capabilities in single-text-modality coding tasks, they remain inadequate in visually-oriented tasks such as chart generation and webpage design, leading to poor visual outcomes like overlapping elements, inconsistent color schemes, and disorganized structures. Consequently, the aesthetic dimension of visually-oriented code generated by LLMs remains an underexplored area. In this paper, we focus on assessing and improving LLMs ability in visually-oriented coding tasks, which refer to programming tasks in which the correctness or quality of the code is inherently tied to its visual output. Typical examples include tasks that generate or manipulate visual artifacts such as web pages (HTML/CSS), plots and charts (e.g., Matplotlib [Hun07], Seaborn [Was21], Plotly [Inc15]), or graphical scenes (e.g., Python Turtle). Unlike purely algorithmic coding tasks, these tasks require the model to reason about visual structure, spatial layout, and aesthetic consistency, in addition to syntactic or functional correctness. For visually-oriented coding tasks, natural question arises: do LLMs possess any awareness of the aesthetics of their own code? In other words, do they have sense of aesthetics? Building on these insights, we propose the code aesthetics concept, which captures the aesthetic appeal of the execution result of visually-oriented code. Currently, reward methods for training coding LLMs often focus on single textual modality, such as code executability and result correctness [GZC+24, FHY+23, LWG+22, DWZ+25]. These methods have significant limitations when applied to code aesthetics tasks, as they fail to assess visual aesthetics and are unable to interact with rendered visual interfaces like webpages, making them ineffective as reward sources. To address this challenge, we propose agentic reward feedback, new reward system consisting of three agents, (i) execution agent, which checks the code executability, (ii) static aesthetics agent, which assesses the aesthetics based on an image of code execution result, and (iii) interactive aesthetics agent, which is specified to evaluate the function and aesthetics of rendered visual interface while interacting with the elements. When receiving raw model output, the execution agent will try to extract the code blocks from the output and check its executability. If passed, static aesthetics agent and interactive aesthetics agent will then run in parallel to assess the static and interactive aesthetics perspectives respectively. The core idea is simple: adopting multi-agent system to provide comprehensive and systematic reward feedback from textual, visual, and interactive perspectives, thus giving comprehensive feedback to better align the sense of aesthetics of model with human or advanced models. This approach addresses key limitation of most open-source coding LLMs, which are confined to single textual modality and thus lack awareness of the visual rendering of their code. To achieve this goal, we first build large-scale supervised instruction tuning dataset AesCode-358K of two major code aesthetics tasks: Python-based plot generation and webpage design. Given the absense of existing benchmarks for evaluating webpage aesthetics, we construct the OpenDesign benchmark, which consists of 840 real webpage design cases, to evaluate the aesthetics of webpage from both visual (static) and interactive aesthetics using LLM-as-a-judge [ZCS+23, GJS+25] method. Consequently, we perform reinforcement learning using GRPO [SWZ+24] algorithm combined with our Agentic Reward framework (GRPO-AR) to train two models with different parameter scalesAesCoder 4B and AesCoder 7B. After supervised fine-tuning on the AesCode-358K dataset and reinforcement learning with GRPO-AR, our models achieve significant improvement in PandasPlotBench[GTGB25] and OpenDesign, showcasing the effectiveness of the AesCode-358K dataset and GRPO-AR method. The key contributions can be summarized as follows: We introduce the concept of code aesthetics and investigate whether LLM-generated code demonstrates its own design aesthetics. We construct the first dataset for code aesthetics, AesCode-358K, and introduce the first benchmark, OpenDesign, which specifically designed to assess webpage design aesthetics. We propose novel reward system for code aesthetics, agentic reward feedback, and combine it with GRPO algorithm for more effective model training in code aesthetics tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Aesthetics of AI-Generated Contents. With the rapid advancement of generative artificial intelligence [vdZKS13, SK23, JC22], increasing attention has been directed to the aesthetic taste of AI-generated content (AIGC) [CLL+25, WGC+23] and the alignment between AI aesthetics and human preferences [ZWX+24, LLQ+25, OWJ+22]. Previous works include textual aesthetics [JHWW24, Dil16], which investigates methods to provides cleaner layout and better coherence of LLMs output [JHWW24], and image aesthetics [DLT17, WHW24], which focuses on assessing and improving the aesthetic quality of images. However, all these methods rely on evaluating static image(s) and may not capable to assess contents like webpages which need interactions. As the growing maturity of AI agents [AAA+23, HLG+24], it becomes possible to integrate interactive evaluation into the contents generated by large language models, thereby providing more comprehensive and systematic feedback. Reward Systems in Reinforcement Learning. In reinforcement learning, the reward serves as scalar feedback signal that quantitatively evaluates the immediate desirability of an agents actions, thereby guiding the learning process toward behaviors that maximize cumulative long-term return [KLM96]. In the context of training large language models, the sources of reward can be broadly categorized into two main types: (i) Model-based Rewards: This approach utilizes pre-trained reward model to generate feedback [OWJ+22, CLB+17, WXX+24, CYD+23]. These models encode human preferences or expert knowledge, providing an automated and scalable source of reward. (ii) Rule-based Rewards: This type of reward is generated directly from human-defined rules or logic [SWZ+24, XGR+25, MHH+24]. However, in complex tasks, relying solely on single source of reward can induce biased behaviors, ultimately driving optimization in an incorrect direction. Some works have been attempting to use agents, which combine human preference rewards with verifiable signals, to provide more reliable rewards [PQW+25]."
        },
        {
            "title": "3 The AesCode-358K Dataset",
            "content": "To investigate code aesthetics, we focus on domains where both the visual outcome and the implementation style matter. In this context, two representative areas are considered: Python-based plot generation, which emphasizes clarity and expressiveness in visualization, and webpage design, where aesthetic factors directly influence layout and user experience. In this section, we introduce AesCode-358K, large-scale supervised instruction-tuning dataset designed for two key areas of code aesthetics. 3.1 Python-Based Plot Data Construction We adapt instructions from the existing VisCode-200K dataset [NNZ+25]. While the original dataset contains 200K data points, we find that some of the Python code snippets are either not executable or exhibited sub-optimal aesthetics, such as chartlegend overlap and improper font sizes. To ensure high quality, we use Qwen3-Coder-480B-A35B-Instruct-FP8 [Tea25, HYC+24a] to regenerate the Python code. We enforce quality control in two ways. First, we limit the Python environment to essential libraries like matplotlib, seaborn, and plotly to prevent unexpected imports. Second, we validate the codes executability using Jupyter Notebook runtime checks, ensuring that the generated code runs without errors and produces the correct visualizations. After this rigorous filtering, we obtain 158K high-quality plot data points. 3.2 Webpage Design Data Construction We develop four-step process to create large-scale webpage design dataset. First, we use GPT-4o to generate seed keyword corpus across five webpage categories: General Website, 3D Design, Data Visualization, Game Dev, and UI Component. Next, GPT-4o is used to produce diverse webpage design instructions from these keywords. We then project the instructions into an embedding space and apply t-SNE [MH08] visualization to examine category overlap. To remove redundancy, we further apply large-scale clustering and retain only representative samples, resulting in refined 3 Figure 2: Overview of the AesCoder pipeline, which integrates data construction, model training, and weighted scoring mechanism. GRPO-AR coordinates performing GRPO with three specialized reward agentsExecution, Static Aesthetics, and Interactive Aestheticsfor comprehensive reward feedback. instruction dataset (details in Appendix B.2). Finally, we employ GPT-5 [Ope25b] and Qwen3Coder-480B-A35B-Instruct-FP8 [Tea25] to generate HTML code for each instruction. We present dataset statistics and keyword generation prompts in Appendix B. To ensure the quality of the generated HTML code, we first confirmed that it was executable. We then rendered the webpages using playwright and selenium and asked GPT-5 to score the two outputs based on their rendered images. We selected the code with the higher score as our final data."
        },
        {
            "title": "4 Agentic Reward Framework",
            "content": "For coding tasks, mainstream reward signals typically include execution or unit test success [GZC+24, FHY+23], process-aware reward models [LWG+22, DWZ+25], and human preference feedback [SZC+23]. However, these approaches mainly focus on textual modality and lack visually-oriented reward signals, rendering them unsuitable for evaluating code aesthetics. In visually grounded code generation, we highlight three essential dimensions: Code Executability. The generated code must run successfully, which forms the fundamental requirement of all code-related tasks. Static Aesthetics. This dimension captures the visual quality of the rendered output. An effective design should be concise, well-structured, and visually coherent, with elements properly aligned and exhibiting clear sense of design. Interactive Aesthetics. Beyond static visuals, interactive aspects are crucial for webpagesespecially those featuring 3D objects or browser-based games. This dimension ensures https://playwright.dev/ https://www.selenium.dev/ 4 that the generated content does not pursue static aesthetics at the expense of interactivity or functionality, thereby achieving functionally correct interactive aesthetics. Based on these dimensions, we propose an agentic reward framework that leverages multi-agent system to assess each aspect, integrates their evaluations, and generates comprehensive feedback for webpage design from multiple perspectives. 4.1 Execution Agent The execution agent verifies whether the models output is executable and reports the result to the feedback system. Specifically, it assigns sexec = 1 if the output passes all validations, and sexec = 1 otherwise. For raw model output, the agent first attempts to extract the HTML code from the html block; if not found, the entire output is treated as HTML. Given that web browsers tolerate many structural and syntactic errors, strict execution checking is unsuitable for HTML. Instead, we use HTMLHint to implement rule-based HTML checker to validate the basic syntax. The detailed rules can be seen in Appendix H.7. 4.2 Static Aesthetics Agent The static aesthetics agent evaluates visual quality using full-page webpage screenshots. For an HTML file, it first hosts the page locally using playwright in headless mode, then captures full-page screenshot for subsequent visual assessment. We identify three dimensions essential for evaluating webpage screenshot: Instructional Alignment. Evaluates consistency between the pages style and user instructions. Visual Elements. Assesses the effective use of modern design features such as lighting, transparency, and gradients. Layout and Cohesion. Examines whether the structure is functional, responsive, and visually coherent, with concise yet design-aware typography. We select GPT-5 [Ope25b] as the judge for its strong multimodal reasoning ability. Using chainof-thought approach [WWS+23], the judge evaluates the full page screenshot and provides both score and rationale for each dimension. While both scores and explanations are required to ensure reliable evaluation [WWS+23, YZY+23], we retain only the final aggregated score as the output of the static aesthetics agent. The detailed prompts are provided in Appendix H.2. 4.3 Interactive Aesthetics Agent For webpage design, evaluation based only on static screenshots is insufficient, as it overemphasizes visual appearance while neglecting usability. This issue is particularly critical for interactive webpages such as 3D design platforms or browser-based games. To address this, we introduce the interactive aesthetics agent, which autonomously navigates, explores, and interacts with webpages to provide usability-aware feedback. Given the HTML code, the agent launches the page in headless environment, interacts with its elements, and evaluates their interactive aesthetics. We adopt WebVoyager [HYM+24] as the basic web agent framework and GPT-4o [Ope24] as the multimodal model for cost considerations. Agent Planning. At the start of evaluation, the agent generates an initial list of interaction candidates by reasoning about which elements are most relevant to the user instruction and webpage content. It then ranks these candidates and selects the top for execution. To ensure evaluations remain offline, interactions requiring internet access (e.g., social media logins) are excluded, focusing only on the core webpage functionality. Agent Interacting and Scoring. The agent then executes the planned interactions step by step, recording whether each attempt succeeds or fails by carefully comparing the screenshots before and after performing one interaction and judging whether the webpage responds correctly to the given interaction. After completing all interactions, it outputs binary score list indicating success (1) or https://htmlhint.com/ 5 failure (0) for each action, and aggregates them into final interaction score: sinteract = (cid:80)N i=1 si. This score is then returned to the agentic reward framework (see Appendix H.3 for the full prompt). Current web agents can handle most webpage operations [HYM+24], but may still Discussions. struggle with certain corner cases, such as confusing webpage elements or being misled by irrelevant textual content [CPY+25, WMF+24]. Such agent failures lead to score of 0 in the corresponding iteration, since we assign score of 1 only when the webpage responds correctly. This may cause the agent to make incorrect judgments, resulting in scores lower than the true values. On the other hand, agent failures also partially reveal non-standard or sub-optimal aspects of webpage design. Therefore, despite these limitations, using web agents as evaluators provides reasonable proxy for assessing overall webpage aesthetics and interactivity. 4.4 Reward Aggregation The results from the three agents are integrated by the agentic reward framework, which jointly evaluates execution, static aesthetics, and interactive aesthetics to provide comprehensive feedback on each webpage. Let rexec, rstatic, and rinteract denote the rewards from the respective agents. The overall reward is then computed as = wexec rexec + wstatic rstatic + winteract rinteract (1) where represents the weight assigned to each agent."
        },
        {
            "title": "5 AesCoder Training",
            "content": "5.1 Stage I: Supervised Fine-Tuning on AesCode-358K We perform supervised fine-tuning on two different model with different parameter scales on our AesCode-358K dataset: Qwen3-4B-Instruct-2507 [Tea25] and Qwen2.5-Coder-7B-Instruct [HYC+24a]. This validates the generalizability of AesCode-358K dataset and establish robust foundation for next stage reinforcement learning. 5.2 Stage II: Reinforcement Learning with Agentic Reward Feedback After supervised fine-tuning in stage I, the model acquires substantial high-quality knowledge. However, the model at this stage still exhibits limited generalization beyond the training distribution [CZY+25], especially in webpage design tasks. This limitation highlights the necessity of reinforcement learning (RL), which allows the model to adapt more flexibly and robustly to diverse and unseen scenarios. Thus, we perform reinforcement learning using the GRPO-AR method, which integrates the GRPO [SWZ+24] algorithm with our Agentic Reward framework to enhance the models ability. Data Preparation for RL. For avoiding overlap with the data in AesCode-358K, which the model has already seen\" in stage I, we pick 20K RL data from WebSight v0.2 dataset [LTS24], large synthetic dataset containing HTML/CSS codes and LLM-generated descriptions of the webpages. However, the webpage description in WebSight v0.2 are relatively homogeneous, which does not align with the natural expression patterns of human users. So we use the original webpage descriptions as seeds and use GPT-4o [Ope24] to generate user instructions for clearer semantic expression. Prompts refer to Appendix H.6. GRPO with Agentic Reward. To generalize models webpage design ability, we adopt our agentic reward system as reliable and robust reward provider and perform reinforcement learning using GRPO [SWZ+24] algorithm. We call this training method as GRPO-AR. For each prompt in our RL dataset DRL, GRPO-AR samples group of outputs {o1, o2, . . . , oG} from the old policy model πθold and our agentic reward framework will give each output total reward ri from execution, static aesthetics, and interactive aesthetics perspectives respectively, yielding rewards {r1, r2, . . . , rG} respectively. The advantage ˆAi,t can be caculated as follows: ri mean(r) std(r) ˆAi,t = (2) 6 (a) Comparison of relative model rankings between OpenDesign and Design Arena. (b) Agreement rates among GPT and human evaluators. H-i refers to human evaluators. Figure 3: Overall comparison and alignment between OpenDesign and human evaluators. Accordingly, the policy model is optimized by maximizing the GRPO objective under our agentic reward framework (GRPO-AR): JGRPO(θ) = E[p DRL, {oi}G i=1 πθSFT(Op)] 1 (cid:88) i= 1 oi oi (cid:88) (cid:26) t=1 min (cid:20) πθ(oi,tp, oi,<t) πθSFT(oi,tp, oi,<t) ˆAi,t, clip (cid:18) πθ(oi,tp, oi,<t) πθSFT(oi,tp, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:21) ˆAi,t βDKL [πθπref] (cid:27) (3)"
        },
        {
            "title": "6 The OpenDesign Benchmark",
            "content": "Design Arena is widely used platform for benchmarking web page design, supported by community of hundreds of thousands of voters. It allows users to design web pages with various models and receives community feedback through voting. While effective, this voting process is timeconsuming and impractical for large-scale evaluation. To address this limitation, we introduce the OpenDesign Benchmark, which enables efficient and automated assessment of web page aesthetics using large language models. The benchmark includes 840 real-world web page cases and evaluates both static and interactive aspects of design. detailed breakdown of categories and their case counts is provided in the Appendix C. 6.1 Evaluation Mechanism The benchmark assesses model performance from two perspectives: static aesthetics and interactive aesthetics. Static evaluation: given prompt, the HTML generated by model is rendered into static image. The prompt and the image are then assessed by the static aesthetics agent (see Sec. 4.2), which produces static aesthetics score. Interactive evaluation: using the same prompt and HTML code, the interactive aesthetics agent (see Sec. 4.3) assigns an interactive aesthetics score. The final benchmark score for model is obtained by averaging these results across all benchmark cases. 6.2 Reliability Analysis of OpenDesign To evaluate the quality and reliability of the OpenDesign benchmark, we adopt two complementary perspectives: (1) ranking consistency between OpenDesign and Design Arena, and (2) alignment between LLM scoring and human preference. Ranking cosistency between OpenDesign and Design Arena. We compare the rankings of 10 mainstream foundation models against the Design Arena leaderboard. We measure consistency ushttps://designarena.ai/ Rankings are taken as of September 22, 2025; Design Arena updates dynamically. 7 Table 1: Performance comparison between proprietary and open-source models across various benchmarks. In PandasPlotBench, Err., Avg., Good. refer to error rate, average score, good rate respectively. In OpenDesign, Align., Aes., Struct. refer to the three score perspectives: instructional alignment with user instruction, visual elements aesthetics, and structural cohesion respectively. Total. means the total score of the sum of three aspects scores, and InterAes. refers to the score of interactive evaluation stage. Note: Lower is better for Err., higher is better for all other metrics. Best results are in bold, second-best results are underlined (among all open-source models together). Model Size PandasPlotBench Err. () Avg. () Good. () OpenDesign Static Aesthetics Align. () Aes. () Struct. () Total. () InterAes. () Proprietary Models GPT-4o-mini GPT-4o GPT-4.1 GPT-5 (minimal) Claude Sonnet 4 - - - - - Open-Source Large Language Models Qwen3-Coder-30B-A3B GLM-4-32B-0414 GLM-4.5-Air Qwen3-Coder-480B-A35B DeepSeek-V3.1 DeepSeek-R130B 32B 110B 480B 685B 685B Open-Source Small Language Models Qwen3-4B-Instruct-2507 Qwen2.5-Coder-7B-Instruct AesCoder-4B (Ours) AesCoder-7B (Ours) 4B 7B 4B 7B 0.15 0.09 0.09 0.04 0.04 0.07 0.07 0.08 0.05 0.09 0. 0.13 0.22 0.09 0.09 64 68 69 75 74 72 70 71 73 69 70 65 60 70 67 0.57 0.60 0.61 0.66 0.65 0.62 0.59 0.63 0.66 0.58 0. 0.55 0.50 0.63 0.57 14.29 16.90 23.53 30.38 29.60 27.04 24.67 29.29 30.13 29.35 30.02 27.52 16.38 30.42 30.03 14.13 16.05 21.99 25.94 25.92 23.79 22.90 24.83 25.16 24.37 24. 23.01 15.13 26.19 25.98 12.77 15.13 20.27 24.71 25.53 22.75 21.80 24.04 24.62 24.00 24.09 22.73 14.73 25.31 25.18 41.19 48.08 65.79 81.03 81.05 73.66 69.40 78.16 79.90 77.72 78. 73.26 46.27 81.92 81.23 0.40 0.44 0.74 1.37 0.92 0.52 0.48 0.93 0.70 0.88 0.77 0.67 0.38 1.04 0.94 ing Spearmans and Kendalls rank correlation coefficients, obtaining strong agreement: Spearman = 0.98 (p < 1.5 106) and Kendall = 0.91 (p < 3.0 105). Additionally, OpenDesign achieves 66.7% top-3 and 80.0% top-5 overlap with Design Arena. These results indicate that OpenDesign closely reflects large-scale human judgment. Figure 3a plots model ranks across both benchmarks. Points align closely with the diagonal, confirming OpenDesign as reliable proxy for human preferences in webpage aesthetics. Alignment with Human Scoring. We sampled 200 HTML page pairs generated by the 10 models under the same prompts. Two evaluator groupsGPT judge and 10 humans (3 professors, 7 graduate students)performed pairwise comparisons (win/tie/lose), yielding 2,000 annotations. Figure 3b shows agreement ratios: human-human = 68.7%, GPT-human = 80.9%. These are comparable to MT-Bench results (66% and 70%, respectively) [BLB+24, ZCS+23], supporting LLM-as-aJudge as an effective, robust method for assessing code aesthetics."
        },
        {
            "title": "7 Experiments and Results",
            "content": "7.1 Experimental Setup We evaluate the models plot generation using PandasPlotBench [GTGB25] with the head descriptor and vis mode. For each case, the model generates code from an instruction; executability is checked, and if an image is produced, it is compared to the ground truth. GPT-4o scores each case from 0 to 100. This results in three quantitative results, (i) error rate, which refers to the portion of cases do not pass the executability check, (ii) average score, which is the average GPT-4o score among all test cases, and (iii) good rate, which refers to the protion of scores higher than 75. Webpage design ability is assessed using our OpenDesign benchmark (see Section 6). Training settings are provided in the Appendix E. 7.2 Main Results As shown in Table 1, both AesCoder-4B and AesCoder-7B achieve consistent improvements over their respective baselines. On PandasPlotBench, they achieve lower error rates and higher reliabil8 Table 2: Comparison with DPO, RFT, and ablations on Agentic Reward for Qwen3-4B-Instruct2507 and Qwen2.5-Coder-7B-Instruct. Training Strategy Align Aes Struct InterAes Qwen3-4B-InstructSFT RFT DPO GRPO-AR w/o Agentic Reward (ablation) GRPO-AR w/ Agentic Reward (ours) 28.50 29.32 28.79 29.16 30.42 25.27 25.30 25.31 25.20 26.19 Qwen2.5-Coder-7B-Instruct SFT RFT DPO GRPO-AR w/o Agentic Reward (ablation) GRPO-AR w/ Agentic Reward (ours) 28.85 29.73 29.75 28.81 30. 25.23 25.35 25.33 25.02 25.98 24.36 24.67 24.38 24.67 25.31 24.37 24.85 24.87 24.41 25.18 0.62 0.71 0.70 0.71 1.04 0.70 0.75 0.71 0.72 0.94 ity, indicating stronger capability in generating correct plotting code. On OpenDesign, AesCoder achieves substantial improvements in both static aesthetics (alignment, visual appeal, and structure) and interactive aesthetics, surpassing all other open-source models. In particular, AesCoder matches or outperforms models with 30B685B parameters, establishing new state-of-the-art results among open-source systems. When compared with proprietary models, AesCoder-4B not only surpasses GPT-4o and GPT-4.1 on both PandasPlotBench and OpenDesign, but also delivers results competitive with substantially larger systems. Although GPT-5 and Claude Sonnet 4 still retain slight overall advantage, our models achieve comparable scores across several aesthetic dimensions. These findings underscore the effectiveness of GRPO-AR, demonstrating that reinforcement learning with agentic reward feedback consistently enhances performance across different architectures and scales. We further conducted human evaluation (Appendix F), and the results show that our models consistently outperform strong open-source baselines (GLM-4-32B-0414 and Qwen3-Coder-30B-A3BInstruct), which further validates our results. 7.3 Analysis Generalization of agentic reward. We further analyze the reward dynamics during reinforcement learning, as illustrated in Appendix G. Both Qwen2.5-Coder-7B-Instruct-SFT and Qwen34B-Instruct-2507-SFT exhibit steadily increasing reward scores with training steps. This consistent upward trend indicates that the agentic reward framework provides stable and informative feedback, enabling continuous improvement across different model families and sizes. The results highlight the robustness of the framework as general training signal, independent of specific architecture choices. Effect of Agentic Reward. To isolate the contribution of the proposed agentic reward, we conduct controlled comparison against variant that does not incorporate it. Specifically, instead of leveraging the full agentic reward framework, we directly employ the underlying reward model to score model-generated HTML outputs along three static dimensionsInstructional Alignment, Visual Design and Aesthetics, and Structural Coherence and Usabilityand use these scores as the sole reward signal (see Appendix H.4 for the exact prompt). The policy optimization strictly follows the same procedure as in Sec. 5.2, with the updates computed according to Eq. 3, thereby ensuring fair comparison. As reported in Table 2, this simplified variant consistently underperforms the full method that integrates agentic reward feedback. The performance gap highlights that merely reusing the reward model to directly score code in textual modality is insufficient. In contrast, our agentic reward framework, which incorporates multi-perspective evaluations including execution, static, and interactive aesthetics, provides richer and more reliable feedback. These results demonstrate that agentic 9 Figure 4: Case study comparing AesCoder-4B and baseline models on OpenDesign. The categories from top to bottom are: General Website, Data Visualization, 3D Design, Game Dev, UI Component. reward is essential for aligning the model with both functional correctness and human-perceived aesthetics. 10 Comparison with DPO and RFT. To further validate the effectiveness of our proposed method GRPO-AR, we additionally compare it with two RLHF methods: Direct Preference Optimization (DPO) [RSM+24] and Rejection Sampling Fine-Tuning (RFT) [YYL+23]. Both methods are applied to the Stage checkpoint πθSFT, using the same training data as in Stage II to ensure fair comparison. Implementation details of DPO and RFT are provided in Appendix D. As shown in Table 2, our method consistently surpasses both DPO and RFT on OpenDesign across static and interactive aesthetics. These improvements highlight that incorporating agentic reward feedback not only enhances the visual quality of generated webpages but also strengthens their usability and structural robustness, confirming the superiority of GRPO-AR."
        },
        {
            "title": "8 Case Study",
            "content": "We further conduct case studies on the OpenDesign benchmark to qualitatively compare AesCoder4B with Claude Sonnet 4 [Ant25b] and DeepSeek-R1-0528 [DAGY+25]. We select five representative cases from the five categories in OpenDesign for comparison. As illustrated in Figure 4, AesCoder-4B achieves results that are superior to or on par with state-of-the-art models across all five web design task categories. These results highlight the effectiveness of our approach in aligning code generation with both usability and aesthetic quality."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we introduce the concept of code aesthetics and present AesCode-358K, OpenDesign, and an agentic reward framework (GRPO-AR) that jointly enhance executability, static design, and interactivity in code generation. Through supervised tuning and reinforcement learning with GRPO-AR, our AesCoder models achieve state-of-the-art results on PandasPlotBench and OpenDesign, rivaling much larger models. These results demonstrate that multi-agent reward feedback can effectively align coding LLMs with both functional correctness and human-perceived aesthetics, paving the way for more capable and user-friendly coding assistants."
        },
        {
            "title": "References",
            "content": "[AAA+23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [Ant25a] Anthropic. Claude code: Best practices for agentic coding. https:// www.anthropic.com/engineering/claude-code-best-practices, 2025. Accessed: 2025-09-25. [Ant25b] Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025. [BLB+24] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 74217454. Association for Computational Linguistics, 2024. [CLB+17] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [CLL+25] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip Yu, and Lichao Sun. survey of ai-generated content (aigc). ACM Computing Surveys, 57(5):138, 2025. [CPY+25] Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Why do multi-agent llm systems fail?, 2025. 11 [CYD+23] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. 2023. [CZY+25] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. [DAGY+25] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [Dil16] Paul Dilley. Textual aesthetics. The Red Monastery Church: Beauty and Asceticism in Upper Egypt, page 175, 2016. [DLT17] Yubin Deng, Chen Change Loy, and Xiaoou Tang. Image aesthetic assessment: An experimental survey. IEEE Signal Processing Magazine, 34(4):80106, 2017. [DWZ+25] Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, and Lin Yan. Process supervision-guided policy optimization for code generation, 2025. [FHY+23] Qiang Fu, Xiao Han, Wei Yang, Deheng Ye, Kaiwen Xiao, Jiate Liu, and Yiqin Zhu. Rltf: Reinforcement learning from unit test feedback, 2023. [GJS+25] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. [GTGB25] Timur Galimzyanov, Sergey Titov, Yaroslav Golubev, and Egor Bogomolov. Drawing pandas: benchmark for llms in generating plotting code, 2025. 12 [GZC+24] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriele Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. ArXiv, abs/2410.02089, 2024. [GZX+24] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. [GZY+24] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024. [HLG+24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [Hun07] J. D. Hunter. Matplotlib: 2d graphics environment. Computing in Science & Engineering, 9(3):9095, 2007. [HYC+24a] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [HYC+24b] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5coder technical report, 2024. [HYM+24] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models, 2024. [Inc15] Plotly Technologies Inc. Collaborative data science, 2015. [JC22] Mladan Jovanovic and Mark Campbell. Generative artificial intelligence: Trends and prospects. Computer, 55(10):107112, 2022. [JHWW24] Lingjie Jiang, Shaohan Huang, Xun Wu, and Furu Wei. Textual aesthetics in large language models. arXiv preprint arXiv:2411.02930, 2024. [KLM96] Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. [LLQ+25] Zhichao Liao, Xiaokun Liu, Wenyu Qin, Qingyu Li, Qiulin Wang, Pengfei Wan, Di Zhang, Long Zeng, and Pingfa Feng. Humanaesexpert: Advancing multimodality foundation model for human image aesthetic assessment. arXiv preprint arXiv:2503.23907, 2025. [LTS24] Hugo Laurençon, Léo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. [LWG+22] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 13 [MH08] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):25792605, 2008. [MHH+24] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. Advances in Neural Information Processing Systems, 37:108877108901, 2024. [NCW+25] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon, Lina Yao, Branislav Kveton, Thien Huu Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, and Franck Dernoncourt. Gui agents: survey, 2025. [NNZ+25] Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, and Wenhu Chen. Viscoder: FinearXiv preprint tuning llms for executable python visualization code generation. arXiv:2506.03930, 2025. [Ope24] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2025-09-13. [Ope25a] OpenAI. Gptsystem card. https://openai.com/index/ gpt-5-system-card/, August 2025. [Ope25b] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. Accessed: 2025-09-13. [OWJ+22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [PQW+25] Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. arXiv preprint arXiv:2502.19328, 2025. [RSM+24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [SK23] Tam Sakirin and Siddartha Kusuma. survey of generative artificial intelligence techniques. Babylonian Journal of Artificial Intelligence, 2023:1014, 2023. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [SZC+23] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. Pangu-coder2: Boosting large language models for code with ranking feedback, 2023. [SZY+24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [Tea25] Qwen Team. Qwen3 technical report, 2025. [vdZKS13] Tijn van der Zant, Matthijs Kouw, and Lambert Schomaker. Generative artificial intelligence. In Philosophy and theory of artificial intelligence, pages 107120. Springer, 2013. 14 [Was21] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60):3021, 2021. [WGC+23] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. Aigenerated content (aigc): survey. arXiv preprint arXiv:2304.06632, 2023. [WHW24] Xun Wu, Shaohan Huang, and Furu Wei. Multimodal large language model arXiv preprint for text-to-image generation. is human-aligned annotator arXiv:2404.15100, 2024. [WMF+24] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. [WWS+23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [WXX+24] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024. [XGR+25] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [YYL+23] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023. [YZY+23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. [ZCS+23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [ZHQ+25] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Large language model-brained gui agents: survey, 2025. [ZWX+24] Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, et al. Aligning vision models with human aesthetics in retrieval: Benchmarks and algorithms. Advances in Neural Information Processing Systems, 37:8639986434, 2024. [ZZZ+24] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models, 2024."
        },
        {
            "title": "A LLM Usage Statement",
            "content": "A large language model (ChatGPT) was used to aid and polish the writing of the paper, including minor grammar correction and language refinement."
        },
        {
            "title": "B Details of Web Page Data Construction",
            "content": "B.1 Keyword Corpus and Instruction Generation We classified webpages into five categories: General Website, 3D Design, Data Visualization, Game Dev, and UI Component. Using GPT-4o, we generated 9K seed keywords for the General Website category, and 2.5K keywords for each of the remaining four categories. Table 3 summarizes the distribution. Table 3: Seed keywords statistics across categories. Category General Website 3D Design Data Visualization Game Dev UI Component Seed Keywords 9,000 2,500 2,500 2,500 2, Based on the seed corpus, GPT-4o was asked to generate 20 non-redundant and semantically diverse instructions for each keyword. This resulted in total of 400,000 webpage design instructions for further processing. B.2 Semantic Analysis and Deduplication We embedded all instructions using openai-text-embedding-3-large (3072 dimensions). From each category, 2,000 instructions were randomly sampled and visualized with t-SNE (perplexity = 30, max_iter = 1000). As shown in Figure 5, the raw dataset exhibited significant overlaps across categories, along with several dense clusters. To filter out redundancy, we applied K-Means clustering with = 200K on the embedded vectors and kept only the sample nearest to each cluster center. This resulted in refined dataset of 200K instructions. The t-SNE visualization of the refined dataset shows clearer class boundaries and reduced overlap across categories, demonstrating the effectiveness of our filtering. (a) t-SNE of raw data (b) t-SNE of filtered data Figure 5: Visualization of instruction embeddings before and after filtering."
        },
        {
            "title": "C OpenDesign Benchmark Categories",
            "content": "Table 4: Distribution of OpenDesign Benchmark Categories (Total: 840 cases)"
        },
        {
            "title": "General Website",
            "content": "3D Design Data Visualization Game Dev UI Component"
        },
        {
            "title": "Total",
            "content": "60.9% 14.6% 4.8% 13.6% 4.9% 100%"
        },
        {
            "title": "D Implementation Details for DPO and RFT",
            "content": "In this section, we describe the construction pipeline of training data for both DPO and RFT used in 7.3. We adopt the same set of queries as in GRPO-AR for offline sampling. For each query q, we sample responses from the SFT policy πθSFT , yielding O(q) = (cid:8) oi (cid:9)N i=1. reward model Rϕ then scores each response conditioned on q: R(q) = (cid:8) r(oi q) (cid:12) (cid:12) oi O(q) (cid:9), where r(o q) Rϕ(o q). (4) (5) DPO. For DPO, we construct preference dataset by taking, for each q, the highestand lowestscoring responses: DDPO = (cid:110) (q, ow, ol) (cid:12) (cid:12) (cid:12) ow = arg max oO(q) r(o q), ol = arg min oO(q) r(o q) (cid:111) . We then optimize πθ (initialized from πθSFT) with the standard DPO objective [RSM+24]: (cid:16) (cid:16) (cid:17)(cid:17)(cid:105) max θ E(q,ow,ol) DDPO (cid:104) log σ β log πθ(owq) πθSFT (owq) log πθ(olq) πθSFT (olq) , where σ() is the sigmoid and β > 0 is scaling hyperparameter. RFT. For RFT, we select only the top-scoring response per query: DRFT = (cid:110) (q, o) (cid:12) (cid:12) (cid:12) = arg max oO(q) r(o q) (cid:111) . The model is then trained with standard supervised objective: LRFT(θ) = E(q,o) DRFT (cid:88) log πθ(ot q, o1:t1) . (6) (7) (8) (9) t= Implementation. We implement both DPO and RFT with LLaMA-Factory [ZZZ+24]**. For fair comparison with GRPO-AR, we keep the same learning rate, batch size, and the total number of training samples as in Stage II. Training Settings. For stage I, all models are trained for 3 epochs with the AdamW optimizer, employing 10% linear warmup followed by cosine learning rate decay schedule. The maximum learning rate is set to 1e5, with batch size of 128 and maximum sequence length of 8k tokens. Training the 7B model in the SFT phase takes approximately 2 days on 1 nodes of 8xMI300 GPUs. For stage II, we use VeRL [SZY+24] to conduct experiments. By default, we use constant 3106 learning rate together with AdamW optimizer for policy model, and use batch size of 64 and micro batchsize of 8. The rollout stage collects 64 prompts and samples 8 responses for each prompt. We set KL coefficient to 0.001 and ϵ = 0.5 in Eq. 3 in all experiments. The RL phase takes **https://github.com/hiyouga/LLaMA-Factory 17 approximately 7 days on 1 nodes of 8xMI300 GPUs. In agentic reward framework, we set wexec = 0.1, wstatic = 0.8, and winteract = 0.1. Given the currently low success rate of GUI agents [ZHQ+25, NCW+25, HYM+24], we limit the number of interactive elements to 3 during training. Additionally, when the GUI agent lists the interactive elements, we instruct it to prioritize them based on their importance. This ensures that the most critical and prominent elements are interacted with, thereby mitigating the impact of the GUI agents limited success rate on our GRPO-AR training."
        },
        {
            "title": "F Human Evaluation",
            "content": "(a) AesCoder 4B (b) AesCoder 7B Table 5: Human preference result visualization of AesCoder and other models. To validate the effectiveness of our model, we select four mainstream models, Claude Sonnet 4 [Ant25b], GPT-5 [Ope25a], GLM-4-32B-0414[GZX+24] and Qwen3-Coder-30B-A3B-Instruct [Tea25] and randomly sampled 100 test cases from OpenDesign, resulting in 100 HTML pairs πours(p), πothers(p). Then we perform the same human preference annotations as Section 6. Results are shown in Figure 5. AesCoder achieves win rate of over 55% in comparisons with midto large-scale open-source models (GLM-4-32B-0414 and Qwen3-Coder-30B-A3B-Instruct), and maintains near 50% win rate when compared to state-of-the-art proprietary models (Claude Sonnet 4 and GPT-5), demonstrating the effectiveness of our agentic reward framework."
        },
        {
            "title": "G Agentic Reward Training Curve",
            "content": "Figure 6: Reward curves during GRPO-AR."
        },
        {
            "title": "H Prompt",
            "content": "H.1 Prompt Template for pairwise Evaluation"
        },
        {
            "title": "Prompt Template for pairwise Evaluation",
            "content": "You are highly-skilled and impartial AI evaluator. Your task is to distinctively evaluate two HTML webpage images, Image and Image B, generated from the same user instruction but by different models. Your evaluation should emphasize clear differentiation and ranking between the two images, avoiding similar or average scores unless they are truly of equal quality. Always highlight meaningful differences. You will be provided with the following: - The general topic of the generated webpages: {topic} - The original user instruction used to generate the webpages: {user_instruction} - Image A, representing the output of the first model, which will be given later. - Image B, representing the output of the second model, which will be given later. Scoring Criteria (Total: 100 points per image): 1. Alignment with User Instruction (40 points): - Score how well each image aligns with the details and intent of the provided user instruction. - Assess whether all requested elements, content, and functionalities are present and correctly implemented. - Evaluate if the overall structure and layout match the users requirements. 2. Aesthetics and Readability (30 points): - Score the visual appeal, design quality, and overall polish of each webpage. - Assess factors like color scheme, typography, use of whitespace, and visual hierarchy. - Evaluate the ease of reading and understanding the content. Is the text clear? Are the sections well-defined? 3. Structural Integrity and Responsiveness (30 points): - Score the logical organization and structure of the webpage. - Assess the overall layout and how the different components are arranged. - Evaluate how well the design would adapt to different screen sizes (e.g., mobile, tablet, desktop), based on visual cues in the image. Scoring Instructions: - Distinctiveness is required: Avoid giving similar or average scores to both images unless they truly have no meaningful difference. - Justify both high and low scores: If one image is clearly better in any aspect, assign noticeably higher score. - If an image has major flaws, do not hesitate to give low score for that criterion. - Do not use safe scores. Use the full range of the scoring scale if appropriate. Your output must contain specific scores for each criterion of the two images, and the overall comparison symbol. The template of the output should strictly obey the following json format (alignment_score, aesthetics_score, structure_score are just the abbreviation of Alignment with User Instruction score, Aesthetics and Readability score, and Structural Integrity and Responsiveness score): 19 { } \" Image Score \": { \" alignment_score \": score_A_1 , \" aesthetics_score \": score_A_2 , \" structure_score \": score_A_3 , \" Total Score \": total_score_A } , \" Image Score \": { \" alignment_score \": score_B_1 , \" aesthetics_score \": score_B_2 , \" structure_score \": score_B_3 , \" Total Score \": total_score_B } , \" Overall Comparison \": \" comparison_symbol \" \" feedback \": \" feedback \" Where: - For scores: - score_A_1, score_A_2, score_A_3 are the scores for Image in each category. - score_B_1, score_B_2, score_B_3 are the scores for Image in each category. - total_score_A and total_score_B are the sum of the individual scores for each image. - For comparison symbol: - If Image is far superior to B, the comparison symbol should be [[AB]]. - If Image is better than B, the comparison symbol should be [[A>B]]. - If Image and are of equal quality, the comparison symbol should be [[A=B]]. - If Image is worse than B, the comparison symbol should be [[A<B]]. - If Image is far inferior to B, the comparison symbol should be [[AB]]. - For feedback: - concise summary (about 50 words) of your evaluation, explaining the strengths and weaknesses of the webpage in relation to the scores youve given. 20 H.2 Prompt Template for Pointwise Evaluation"
        },
        {
            "title": "Prompt Template for Pointwise Evaluation",
            "content": "You are an expert evaluator tasked with rigorously assessing the quality of an HTML webpage generated by large language model. You will be given an image of the rendered HTML webpage and the original user instruction. Your primary goal is to provide an objective, accurate, and discriminative score, using the full range of the scoring scale (0100). Do not hesitate to give low or moderate scores if the webpage is average or has flaws. Only award high scores to webpages that are truly exceptional and nearly flawless according to professional standards. You will be provided with: - The general topic of the generated webpage: {topic} - The original user instruction: {user_instruction} - Image A, representing the output of the model to evaluate Evaluation Instructions: 1. Carefully analyze the user instruction and the webpage image. 2. Score the webpage on the following criteria (use the full scoring range): Alignment with User Instruction (40 points): - Does the webpage fully and precisely satisfy all explicit and implicit requirements of the users prompt? - Are all requested elements present and correctly implemented? - Does the content and structure directly correspond to the instruction? Aesthetics and Readability (30 points): - Is the webpage visually appealing, modern, and professionally designed? - Are color, font, and spacing choices effective and consistent? - Is the text easy to read and the layout clear? Structural Integrity and Cohesion (30 points): - Is the structure logical, well-organized, and cohesive? - Do all sections flow smoothly and intuitively? - Is the user experience (based on the image) seamless and easy to follow? Scoring Principles (Read Carefully): - Use the full range for each criterion (e.g., 040, 030). Average or flawed webpages should receive average or below-average scores. - High scores (top 20% of each range) should be awarded only for work that meets or exceeds professional standards with virtually no flaws. - If the webpage is missing elements, has visual issues, or organizational problems, score accordingly low. - Provide brief justification for any high or low score. Score Interpretation Reference: - 90100: Outstanding, professional, nearly perfect. - 7089: Good but with noticeable issues or minor flaws. - 5069: Average, with clear limitations or several weaknesses. - 3049: Below average, significant flaws or missing requirements. - 029: Poor, major requirements missing, very low quality. Provide your final output in the following JSON format: 21 { } \" alignment_score \": < score out of 40 > , \" aesthetics_score \": < score out of 30 > , \" structure_score \": < score out of 30 > , \" total_score \": < sum out of 100 > , \" feedback \": \" < concise summary ( about 30 words ) explaining the strengths and weaknesses and justifying the scores >\" Remember: As an expert evaluator, do not inflate scores. Always judge by high professional standards and make full use of the scoring scale. 22 H.3 Prompt Template for Interactive Aesthetics Agent"
        },
        {
            "title": "Prompt Template for Interactive Aesthetics Agent",
            "content": "Imagine you are distinguished website design judger. Now you are given task about evaluating the practicality and aesthetic about the interactivity of webpage. The webpages you are given are all single-paged, offline html files. User will later provide you with the specific topic (Only in these five topics: [\"General website\", \"Game dev\", \"Data visualization\", \"3D design\", \"UI component\"]) and the detailed description of this webpage. You should evaluate the webpages interactivity and aesthetic based on the topic and the detailed description. When evaluating the aesthetic of interactivity of webpage, you should consider the following aspects: - First, think thoroughly about all the ways of interactions with the webpage based on the topic, the detailed description given by the user and the webpage screenshot. Output your planned interations at the beginning of the task in your thought. - Then, evaluate the interactivity of the webpage in order according to your planned interations. For each time of interaction, carefully compare the webpage before and after the interaction. The webpage should change according to the interaction. If the webpage is not changed or the change is not expected, it should not be considered as good webpage. - Since the webpage is offline, we do not expect changes which need internet connection. Specially, for textbox, you should plan both typing in the textbox and clicking the search button. It cannot be considered as successful interation if only you successfully type in the textbox, but the webpage has not changed at all after clicking the search button. - When your interaction does produce feedback, you still need to carefully consider whether that feedback is correct and logical. For example, if you click on list and it merely displays the list, but clicking on an item within the list does not trigger any response, then no points should be awarded. Only correct feedback can earn points. - Sometimes when you click navigation button, the webpage will not change simply because it is already in the page you want to go. You should try to click another navigation button and click back again to check the interactivity of this navigation button. - {GAME_EXTRA_PROMPT} In each iteration, you will receive an Observation that includes screenshot of webpage and some texts. This screenshot will feature Numerical Labels placed in the TOP LEFT corner of each Web Element. Carefully analyze the visual information to identify the Numerical Label corresponding to the Web Element that requires interaction, then follow the guidelines and choose one of the following actions: 1. Click Web Element. 2. Delete existing content in textbox and then type content. 3. Wait. Typically used to wait for unfinished webpage processes, with duration of 1 seconds. 4. Press the up arrow key. (Only can be used when the topic of the webpage is game dev) 5. Press the down arrow key. (Only can be used when the topic of the webpage is game dev) 6. Press the left arrow key. (Only can be used when the topic of the webpage is game dev) 7. Press the right arrow key. (Only can be used when the topic of the webpage is game dev) 8. FINISH. This action should only be chosen when all evaluations in your plan list have been finished. Correspondingly, Action should strictly follow the format: - Click [Numerical_Label] - Type [Numerical_Label]; [Content] - Wait - UP - DOWN - LEFT - RIGHT 23 - FINISH Key Guidelines you must follow: * Action guidelines * 1) To input text, no need to click textbox first, directly type content. After typing, the system automatically hits ENTER key. Sometimes you should click the search button to apply search filters. Try to use simple language when searching. 2) If you have seen scrollbar in the webpage (not for the whole window, since the webpage is always single-paged, but for certain area or element of the webpage, such as 3D object to be rotated or zoomed), do not directly try to scroll it. Instead, find if any interactable element such as button - or + and click the button instead. 3) If you click button and then pop-up window is displayed, you should close the pop-up window and return to the original webpage after you have finished evaluating the interaction. 4) If the topic of the webpage is game dev, it may not have many interactable elements to click. Instead, you can use the up, down, left, right arrow keys to control the game, and plan dynamically when the game running. Dont miss up the role in the game with interactable elements. 5) You must distinguish between textbox and search button, dont type content into the button. If no textbox is found, you may need to click the search button first before the textbox is displayed. 6) Execute only one action per iteration. 7) Strictly avoid repeating the same action if the webpage remains unchanged. You may have selected the wrong web element or numerical label. Continuous use of the Wait is also not allowed. 8) When complex Task involves multiple questions or steps, select FINISH only at the very end, after addressing all of your planned interations. Flexibly combine your own abilities with the information in the webpage. * Web Browsing Guidelines * 1) Dont try to go to other urls. interations can be done offline (without internet connection). 2) Focus on the numerical labels in the TOP LEFT corner of each rectangle (element). Ensure you dont mix them up with other numbers (e.g. Calendar) on the page. Just focus on the given offline html page. All your Your reply should strictly follow the format: For the first iteration (the planning stage): Thought: {Your thorough plan to interact with all the interactable elements of the webpage} For the other iterations (the interaction stage): Thought: {Your brief thoughts (briefly summarize the info that will help you score the previous interaction, and your brief plan for the next interaction)} Numerical_Label: {The numerical label of the previous interaction} Score: {The score of the previous interaction. Only 0, 1, NaN is allowed. 0 means the interaction is failed or incorrect, 1 means successful. Output NaN if no interation is done in this iteration. Specially for textbox, you should output NaN when you finished typing in the textbox, and the actual score when you clicked the search button or something else.} Reasoning: {Your brief reasoning for the score. Similarly, you must output N/A if no interation is done in the previous iteration} Action: {One Action format you choose for the next interaction} Then the User will provide: Observation: {A labeled screenshot Given by User} 24 H.4 Prompt Template for Ablation without Agentic Reward"
        },
        {
            "title": "Prompt Template for Ablation without Agentic Reward",
            "content": "You are an expert evaluator tasked with assessing the quality of an HTML webpage generated by large language model. You will be given the HTML code of the webpage and the original user instructions. You will be provided with: - The general topic of the generated webpage: {topic} - The original user instruction: {user_instruction} - The html code of the webpage: {html} Your objective is to assign precise, rigorous scores, using the full 0100 range. Only award high scores for webpages that are absolutely flawless, meeting all design and functional expectations. Penalize harshly for even the smallest imperfectionsthere is zero tolerance for errors. Key Evaluation Areas: 1. Instructional Alignment (20 points) Evaluate how closely the webpage follows the users instructions. Only this in aspect, your criteria can be relatively low, since we expect some flexibility in interpretation and should more pay more attention in another two aspects (Visual Design and Aesthetics and Structural Coherence and Usability below). Score levels and their explanations: - Good alignment (1020): The webpage almost matches the users instructions. - Severe misalignment (09): The page fails to meet basic requirements. Major elements are missing or misrepresented. 2. Visual Design and Aesthetics (50 points) Assess the overall professionalism and polish of the design. Only award high marks for designs that look flawless, balanced, and intentional. Some golden rules you should obey when scoring: - Always cherish detailed, refined, and innovative design. highly refined design is always better than plain one, which means we value pages with highly rich design elements more than simple and plain designs. This includes an exquisite transparent dynamic background, elements or special effects floating in the background, gradient color text, rich yet beautiful color matching, and so on. - NO PLACEHOLDERS! Always cherish real images and expressive (real or abstract) icons, instead of placeholders. website with rich, real, and appropriate images or icons should score higher(85 or above), while website with placeholders or broken images should score below 50. Abstract modern icon are also preferable, but they should be well-designed and are NOT placeholders. - Simplicity is not lack of content. simple design can still be rich and engaging if it uses space, color, and typography effectively. - The overall impression is important. Make sure the webpage has NO broken/partially visible words or elements. NO partially loaded elements. Score levels and their explanations: - Perfect design (40-50): The design is exceptionally professional, with well-executed color palette, typography, and spacing. The page has polished and intentional feel. - Minor flaws (20-39): The design is good, but there are small issues (e.g., slight inconsistency in font sizes or spacing). These should still impact the score significantly. - Significant flaws (1019): The design has major issues (e.g., poor readability, awkward layout, or jarring color choices). 25 - Unacceptable design (09): The page is unprofessional, with severe flaws such as overlapping text, unreadable fonts, or broken layouts / images. 3. Structural Coherence and Usability (30 points) The page must have logical and intuitive structure. Even the smallest structural mistake (misalignment, broken flow, or inconsistent layout) will severely affect the score. Key scoring rules: - Overall impression comes first. This stresses the importance of adopting modern, concise, refined framework. Encourage websites to use modern, beautiful design frameworks instead of simple, mediocre designs. Webpages with appropriate use framework can score above 85, while those with poor or no framework should score below 50. - Highlight the integrity of the overall structure. Check carefully whether the page has complete structural layout, with no missing elements or broken sections. If the page has any broken sections, it should score below 50. - Flawless structure (2030): The page has perfect structure: well-organized, logical flow, and easy navigation. - Minor structural issues (1519): The structure is good, but there are small usability issues (e.g., slightly misaligned sections or awkward navigation). - Major structural problems (1014): The page has significant usability flaws, such as broken layouts or confusing content organization. - Unusable structure (09): The page has severe structural issues, making it difficult to use or navigate effectively. Fine-Grained Scoring Guidelines: - Strict threshold for high scores: Only give scores above 90 if the webpage is absolutely flawless. If there is even minor issue (e.g., single broken element, misalignment, or poorly chosen font), do not award high marks. Scores 95+ should be reserved for near perfection. - Minor flaws are heavily penalized: If the webpage has any noticeable flaw (such as text overlapping an image, improper spacing, or lack of balance), this will result in low overall score! (e.g., 1030) - Zero tolerance for bad design: If the webpage looks unprofessional (e.g., excessive white space, unaligned content/text, unreadable text, or poor contrast), the overall score should be 0-30! Example Evaluation: For webpage with: - Perfect alignment with instructions (everything is present and correct), - Excellent visual design, but with slightly misaligned text, - Clear structure with one misaligned image, You might score: - Instructional Alignment: 20/20 (perfect alignment with instructions), - Visual Design: 35/50 (good design but minor flawmisaligned text), - Structural Coherence: 20/30 (minor misalignment of an image), - Total Score: 75/100 (not good, but OK). Final Output Format (alignment_score, aesthetic_score, structure_score are just the abbreviation of Instructional Alignment score, Visual Design and Aesthetics score, and Structural Coherence and Usability score): { 26 \" alignment_score \": < score out of 20 > , \" aesthetics_score \": < score out of 50 > , \" structure_score \": < score out of 30 > , \" total_score \": < sum out of 100 > , \" feedback \": \" < brief summary of strengths and weaknesses , with justification for the scores >\" } Strict Scoring Principles: - Minor mistakes are penalized severely. single misplaced element, broken layout, or poor design choice will dramatically affect the score. - High scores (90+) should only be given for perfect webpages with no errors. If there is any imperfection, the score should drop significantly. - No mercy for bad design. Webpages that are visually unappealing or hard to use must receive low scores (09) regardless of other factors. 27 H.5 Prompt for keyword"
        },
        {
            "title": "Prompt Template for Website Keyword and Summary Generation",
            "content": "You are professional website content generator. Generate 300 unique keywords or short summary descriptions (1030 words each) for websites of the type \"{catagory}\". Each summary should: - Reflect unique purpose, functionality, or use case for website. - Be based on creatively chosen theme or industry, covering wide range of domains (e.g., healthcare, education, finance, entertainment, environmental, e-commerce, tourism, tech, art, sports, social impact, etc.) by leveraging your imagination. - Ensure summaries are specific, diverse, and avoid repetition in functionality, theme, or wording. Output as JSON array, where each entry contains: - summary: concise description (1030 words) of the websites purpose or functionality, reflecting the chosen theme. Ensure maximum diversity by exploring unique and imaginative themes, avoiding overlap with common website concepts. Return the result in JSON format. Example output format: [ { } , { } ] \" summary \": \" website for eco - conscious travelers , offering sustainable tourism guides , ethical lodging options , and carbon footprint calculators .\" \" summary \": \" An educational platform providing interactive biology simulations , 3 models , and real - time quizzes for high school students .\" 28 H.6 Prompt Template for Data Rewriting in RL"
        },
        {
            "title": "Prompt Template for Data Rewriting in RL",
            "content": "You are content strategist and creative visionary specializing in conceptualizing innovative digital platforms. Your task is to transform abstract ideas into compelling website concepts that are both unique and inspiring. will provide you with brief description or seed topic. From this, your goal is to generate highly imaginative and detailed website concept. The concept does not need to be directly correlated to the content provide. Feel free to draw inspiration from keywords or abstract elements and create something new and innovative. Your output should focus on the overall content, purpose, and features of the website, without going into specific layout, design, or visual details. Think about the theme, functionality, and interaction possibilities of the site. This will serve as the basis for generating HTML code for the site. It has to be noticed that your instruction should not contain any specific layout, design, or visual elements of the website, but only the content, purpose, and features of the website. You are required to choose one of the following categories for each website concept you create. Please try to think creatively and step outside the General website category when possible: 1. General website: website designed for general use or any topic, focusing on its core content, purpose, and user interaction. 2. Game development: browser-based game concept in HTML. Focus on interactive and engaging content, game mechanics, and user experience. 3. Data visualization: page that presents dynamic and interactive data, such as charts, graphs, or visualized datasets. Focus on how the user will interact with and explore the data. 4. UI component: page dedicated to showcasing single, highly interactive component. Focus on the functionality and purpose of the component, without detailing its visual structure. 5. 3D design: concept for 3D scene or interactive experience, focusing on its content and user interaction, rather than specific rendering or layout details. For each brief description provide, follow this structure: 1. Select category from the list above that best fits the concept. 2. Create detailed and concise description of the website concept, focusing on its content, purpose, features, and interactions. 3. Provide clear instruction (4060 words) for HTML code generation that can be used to implement this concept. Output the response in the following JSON format: { } \" category \": \" < category name from the list >\" , \" instruction \": \" < detailed website concept instruction >\" H.7 Execution Agent Validation Rules"
        },
        {
            "title": "Execution Agent Validation Rules",
            "content": "The following configuration defines validation and linting rules for HTML, CSS, and JavaScript within single HTML file. These rules should be strictly applied when evaluating or generating webpages. { } \" doctype - html5 \": true , doctype declaration \" tagname - lowercase \": true , lowercase tag names \" attr - lowercase \": true , lowercase attribute names \" attr - value - double - quotes \": true , quotes for attribute values \" tag - pair \": true , // Enforce HTML5 // Enforce // Enforce // Enforce double // Enforce all tags must have corresponding closing tag \" tag - self - close \": [\" br \" , \" img \" , \" input \" , \" link \" , \" meta \"] , // Allow self - closing tags for specific elements \" id - unique \": true , attribute is unique in the document \" alt - require \": true , // Ensure id // Enforce alt attribute for all <img > tags for accessibility \" head - script - disabled \": false , tags in the < head > section \" style - disabled \": false , CSS styles within HTML \" no - inline - style \": false , styles within HTML \" no - inline - script \": false , JavaScript within the HTML file \" lang - require \": true , attribute in the < html > tag \" meta - charset - utf -8\": true , charset declaration \" meta - viewport \": true , inclusion of the viewport meta tag \" title - require \": true , inclusion of the < title > tag \" csslint \": { \" important \": false , of ! important in CSS \" order - alphabetical \": false // Allow < script > // Allow inline // Allow inline // Allow inline // Enforce lang // Ensure UTF -8 // Enforce // Enforce // Allow the use // Do not enforce alphabetical order for CSS properties } , \" script - disabled \": false JavaScript ( inline within HTML ) // Allow"
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Peking University",
        "Zhiyuan College, Shanghai Jiao Tong University"
    ]
}