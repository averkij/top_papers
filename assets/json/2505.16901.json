{
    "paper_title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks",
    "authors": [
        "Hongyuan Tao",
        "Ying Zhang",
        "Zhenhao Tang",
        "Hongen Peng",
        "Xukun Zhu",
        "Bingchang Liu",
        "Yingguang Yang",
        "Ziyin Zhang",
        "Zhaogui Xu",
        "Haipeng Zhang",
        "Linchao Zhu",
        "Rui Wang",
        "Hang Yu",
        "Jianguo Li",
        "Peng Di"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 1 0 9 6 1 . 5 0 5 2 : r CODE GRAPH MODEL (CGM): GRAPH-INTEGRATED LARGE LANGUAGE MODEL FOR REPOSITORY-LEVEL SOFTWARE ENGINEERING TASKS"
        },
        {
            "title": "A PREPRINT",
            "content": "Hongyuan Tao *1, Ying Zhang *1,2, Zhenhao Tang *1, Hongen Peng1, Xukun Zhu1,3, Bingchang Liu1, Yingguang Yang1, Ziyin Zhang1,4, Zhaogui Xu1, Haipeng Zhang2, Linchao Zhu3, Rui Wang4, Hang Yu 1, Jianguo Li 1, Peng Di 1 1Ant Group, Hangzhou, China 2ShanghaiTech University, Shanghai, China 3Zhejiang University, Hangzhou, China 4Shanghai Jiaotong University, Shanghai, China"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether opensource LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLMs attention mechanism and map node attributes to the LLMs input space using specialized adapter. When combined with an agentless graph RAG framework, our approach achieves 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.1. Keywords Code LLM Repository-Level Graph"
        },
        {
            "title": "Introduction",
            "content": "The dream of automating software engineering (SE) has long captivated both the SE and artificial intelligence (AI) communities [1, 2, 3]. Recent advancements in Large Language Models (LLMs) have shown promising results, particularly in code generation at the function level, with models achieving resolution rates above 90% on benchmarks such as HumanEval [4]. Unfortunately, real-world SE tasks extend far beyond isolated functions or self-contained code files. This is exemplified by repository-level issue resolution [5, 6], which encompasses not only software maintenanceaddressing bugs and technical debtbut also software evolution, which involves introducing new features and enhancements [7]. The complexity of repository-level coding tasks has led researchers and practitioners to assume that sophisticated strategies are necessary for their completion [8]. Indeed, current leading approaches typically utilize LLM agents powered by proprietary models like GPT-4/4o [9] and Claude 3.5 Sonnet [10]. These agents are designed to leverage tools, execute commands, observe environmental feedback, and plan subsequent actions [11]. Nevertheless, these *Equal Contribution. Correspondence to: <dipeng.dp@antgroup.com>. Hang Yu <hyu.hugo@antgroup.com>, Jianguo Li <lijg.zero@antgroup.com>, Peng Di 1The code is available at https://anonymous.4open.science/r/CGM-EF arXiv Template PREPRINT methods suffer from two problems. First, the agent-driven mechanism introduces unpredictability in decisionmaking [2]. As the reasoning processes become intricate in tackling complex problems, the accumulation of errors can hinder the generation of optimal solutions [12]. Second, the reliance on closed-source models creates substantial barriers for the broader SE community [13, 14], including limited accessibility, inability to enhance or customize models for specific tasks, and serious security concerns regarding the privacy of sensitive code repositories when interacting with external API services. The above two challenges lead to bold question: Can open-source LLMs be employed in an agentless manner to complete repository-level coding tasks? At first glance, this seems improbable. Closed-source agent-based approaches can resolve up to 55% of issues on the popular SWE-bench Lite benchmark2 for issue fixing, whereas existing methods using open-source models have only achieved maximum resolution rate of 30.67% as of May 2025 [15]. Despite these initial reservations, we posit that the answer is Yes, and the key lies in empowering the open-source LLMs to fully comprehend code repositories, not just the information within individual functions and files, but also the dependencies across functions and files. To move forward to this goal, we propose Code Graph Models (CGMs), to jointly model the semantic and structural information of code repositories. Specifically, we first construct code graph for each repository, which characterizes the hierarchical and reference dependencies between code entities. We then develop method to integrate this graph into the LLM through two key mechanisms. (i) Semantic Integration: Node attributes (containing code or comments) are first encoded by pretrained text encoder and then mapped to the LLMs input space via an adapter, enabling the model to understand the semantic information of all nodes. (ii) Structural Integration: The graph structure is incorporated into the LLM through the attention mask, allowing direct message passing only between neighboring nodes in each layer of the LLM, similar to spatial Graph Neural Networks (GNNs) [16]. The entire systemcomprising the text encoder, adapter, and LLM decoderis then fine-tuned using Low Rank Adaptation (LoRA) [17]. The resulting CGM can tackle repository-level coding tasks by using both the code graph and user instructions (text format). To further augment the abilities of the CGM, we develop specially designed Graph Retrieval-Augmented Generation (RAG) framework, consisting of four modules: Rewriter, Retriever, Reranker, and Reader (i.e., CGM). The first three modules focus the CGM on the subgraph that is most pertinent to the users query or issue. Our approach has demonstrated remarkable results on the SWE-bench Lite benchmark, reaching 43.00% resolution rate using the open-source Qwen2.5-72B model and our agentless RAG framework. As of May 2025, this performance ranks first among methods utilizing open-source models, second among methods with open-source code implementations (the underlying model may still be closed-source), and eighth overall. Notably, our approach surpasses the previous best method based on open-source models (Moatless+DeepSeek-V3 [15]) by 12.33%, despite that method employing DeepSeek-V3, which shows stronger performance than Qwen2.5-72B. The main contributions of this work are as follows: We propose CGMs, novel architecture that seamlessly integrates repository code graphs with open-source LLMs through semantic and structural integration. Its modular design allows independent replacement of componentsincluding the encoder and adapterproviding flexibility that may further enhance performance. We develop an agentless Graph RAG framework that enhances the CGMs performance by focusing on the most relevant subgraphs for user queries. Our CGM, armed with the Graph RAG, achieves 43.00% resolution rate on SWE-bench Lite, surpassing most agent-based approaches. We also demonstrate its effectiveness on other repository-level tasks such as code completion."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Large Language Models for Code Recent advancements in LLMs have shown remarkable success in generating code at self-contained function or file levels [3]. This includes powerful closed-source models like GPT-4/4o [9], Gemini-2.0 [18], and Claude 3.5 Sonnet [10], as well as open-source alternatives such as Llama 3.1 [19], Qwen 2.5 [20], and DeepSeek-V3 [21]. Additionally, code-specialized open-source models have also emerged, including StarCoder [22, 23], DeepSeek-Coder [14, 24], and Qwen-Coder [25]. However, these models struggle with repository-level coding tasks that better reflect practical software development scenarios. Even the most capable closed-source models achieve only modest success rates on the SWE-bench Lite benchmark [5] for real-world issue fixing, while open-source models lag further behind with 2https://www.swebench.com/ 2 arXiv Template PREPRINT maximum resolution rate of 26% [26]. Although closed-source models show superior performance, their limited accessibility and data privacy concerns hinder widespread adoption in the SE community. Furthermore, their proprietary nature prevents fine-tuning on task-specific data to improve performance, if even such data is available. For open-source LLMs to better handle repository-level tasks, they must develop comprehensive understanding of both semantic and structural information within codebases. DeepSeek-Coder [14] has attempted to address this challenge by pre-training models on topologically sorted repository codes. However, this approach faces two major limitations: real-world repositories often contain more code than can fit within the models maximum context length; and the conversion of repository structure into text format tends to obscure explicit dependencies that exist in the codebase. To overcome these challenges, we propose representing repositories as text-rich graphs and aligning them with LLMs via self-supervised continual pre-training. This approach preserves code repository structure while enabling more effective processing and understanding of complex dependencies. 2.2 Graphs in Code Language Models The integration of graph structures into code language models can be classified into three main approaches [27]: (1) attention mask modification, (2) graph-to-text conversion, and (3) positional encoding augmentation. In the first approach, models like GraphCodeBERT [28] and StructCoder [29] modify attention masks to capture relationships between code tokens in Abstract Syntax Trees (ASTs) and Data Flow Graphs (DFGs). The second approach, demonstrated by TreeBERT [30] and UniXcoder [31], transforms ASTs or node paths into textual sequences that can be processed by language models. The third approach, exemplified by TPTrans [32], leverages relative positional encodings to represent structural relationships within ASTs. While these approaches have shown promise, they primarily focus on Transformer encoders and small-scale language models (such as BERT or CodeT5) and are limited to fileor function-level tasks. In contrast, our work enhances decoder-only LLMs to handle repository-level tasks. We construct text-rich code graphs for entire codebases, moving beyond simple ASTs or DFGs. Inspired by GraphCodeBERT and StructCoder, we incorporate graph structures through attention masks in LLMs. However, due to the text-rich nature of the graphs, each nodes text or semantic information is processed by pretrained text encoder and then projected onto the LLMs input space via an adapter. 2.3 Agent-drive Methods for Software Engineering LLM-based agents like Devin [33] have shown the potential to solve real-world SE problems through their reasoning [34, 35] and interactive capabilities [36, 37, 38, 11]. Along this direction, researchers have worked to enhance LLM agents through various approaches, including specialized agent-computer interfaces (ACI) [39, 40, 8, 41], fine-grained search [42, 12, 11], and expanded action spaces [43]. However, these agent-based approaches face several drawbacks. First, they typically delegate decision-making to the agents, allowing them to determine both the timing and nature of actions. While agents base their decisions on previous actions and environmental feedback, the expansive action space and complex feedback mechanisms can lead to repetitive behaviors or accumulating errors, ultimately resulting in suboptimal solutions [12]. Second, resolving single issue often requires 30-40 interaction turns, making the process time-consuming and complicating the identification of specific turns that resulted in unsatisfactory outcomes [2]. Third, the inherent unpredictability of agent behavior and reliance on closed-source models creates obstacles for leveraging data to improve performance, despite the abundance of such data in practice, such as issue-patch pairs for issue fixing [5]. While SWE-Gym [44] attempts to address trainability, it may introduce bias by only training with the trajectories that lead the SWE agent to correct answers. As remedy, we propose the CGM, built on open-source LLMs and enhanced through an agentless Graph RAG framework. 2.4 Agentless Methods for Software Engineering Agentless models offer more controlled approach to simulating real-world SE processes by following well-defined, fixed steps rather than relying on LLM agents to make autonomous decisions or use complex tools. They help avoid the issues of unpredictability and lengthy interaction chains. These methods typically operate in two main stages: localization and editing [45]. The localization stage identifies relevant code snippets within repository, while the editing stage generates or modifies code based on these identified sections. This framework is particularly effective for repository-level code completion tasks, especially when combined with RAG [46, 47]. For more complex tasks like issue fixing, enhanced approaches with additional steps exist [2, 1]. For instance, Agentless [2] implements comprehensive ten-step pipeline, dedicating four steps to improving localization accuracy. This method has achieved promising resolution rate of 40.67% on SWE-bench Lite, comparable to state-of-the-art (SOTA) agent-based methods, though it relies on the closed-source model Claude-3.5 Sonnet. arXiv Template PREPRINT Figure 1: An example of our repository-level code graph, where PKG, FUNC, and T-FILE represents PACKAGE, FUNCTION, and TEXTFILE respectively. In this graph, solid lines represent hierarchical dependencies (i.e., contains), while dashed lines represent reference dependencies (calls/imports/extends). Recent research has also focused on enhancing code understanding by incorporating structural information through graph-enhanced repository modeling [48, 49, 45]. However, even when graph structures are used during retrieval, existing methods typically flatten the retrieved code snippets into linear text sequences for downstream model prompting. This flattening process fails to preserve the inherent heterogeneity between graph and text modalities. As remedy, we propose the CGM that explicitly aligns these two distinct modalities, enabling better preservation and utilization of structural information throughout the entire process."
        },
        {
            "title": "3 Code Graph Construction",
            "content": "Before delving into the CGM, it is crucial to understand the repository-level code graph that CGM utilizes and the process of its construction. The primary aim of this code graph is to offer structured representation of the structural and semantic information inherent in complex codebases. We represent each repository as directed graph = (V, E), where is the set of distinct entities in the codebase and is the set of edges between these entities. To be specific, the code graph includes up to seven types of nodes and five types of edges (details are provided in Appendix B). The node types vary in granularity, ranging from the repository level (REPO) to fine-grained attributes. The edge types comprise both hierarchical (i.e., contains) and reference dependencies (calls/imports/extends). As shown in Figure 1, the hierarchical dependencies (i.e., the solid edges) span the code graph. In other words, all nodes are interconnected by edges reflecting hierarchical dependencies, establishing top-down tree structure. This structure mirrors the organization of code entities as dictated by file systems and programming language syntax rules. Building this tree graph begins with AST parsing [48]. During this phase, code entities and their hierarchical dependencies are identified in recursive manner: the root node (i.e., REPO) is added to the graph first, followed by its children (i.e., PKG and T-FILE), until all nodes without descendants (i.e., FUNC) are processed. With each recursion, directed edges are added from parents to children. On the other hand, reference dependencies (i.e., the dashed edges) capture interactions between different entities, such as class inheritance, function calls, and module imports. Unlike hierarchical edges, which maintain vertical hierarchy, reference edges create horizontal connections that may introduce cycles, such as those caused by recursive calls. These edges are typically not part of an AST. To derive them, we conduct lightweight semantic analysis to resolve symbols, such as references or calls to classes and attributes. Once target symbol is identified, an edge is added from the source node to the target node in the code graph. Concerning node attributes, we retain the original content and line range of each node. This approach enables explicit graph traversal and retrieval and facilitates training models with enhanced semantic understanding capabilities. During post-processing, we remove the text contained in the child nodes from the parent nodes within the tree graph derived from the hierarchical dependencies. The resulting code graph is text-rich graph [50] in which each node encapsulates corresponding code snippet."
        },
        {
            "title": "4 Code Graph Models (CGMs)",
            "content": "In this section, we elaborate on the architecture of the Code Graph Model (CGM), the training strategy we adopted, and how we enhance the CGM via the Graph RAG framework. 4 arXiv Template PREPRINT Figure 2: Architecture of CGM and its Graph RAG extension: Given an issue, (a) Rewriter extracts code entities and keywords from the issue (Extractor), and modifies the original issue into more detailed queries (Inferer). Based on this, (b) Retriever locates relevant nodes from the corresponding code graph; then expands these nodes to connected subgraph by including neighboring and upstream nodes. (c) Reranker ranks retrieved results in two stages: File Name Rank and File Skeleton Rank, selecting the most relevant files for modification. Finally, (d) Reader (CGM) takes the retrieved graph and selected files as input. Each nodes code content is encoded by an Encoder E, producing node token via the Adapter A. Node tokens are then concatenated with text tokens in the prompt before entering the LLM decoder D, where the adjacency matrix replaces its original attention mask. 4.1 Model Architecture The architecture of the CGM is illustrated in Figure 2(d). CGM takes the code graph as inputs, enhancing the LLMs comprehension of both semantic and structural information within the graph. Below, we detail how CGM integrates both aspects into the LLM. Semantic Integration: The code graphs are text-rich, with semantic information only residing in the textual contents of the nodes. As shown in Figure 2(d), we integrate the node information into the LLM decoder by transforming node text into node tokens through an encoder and an adapter A. Specifically for the encoder, we utilize the pretrained encoder from CodeT5+ [51], chosen for its proven effectiveness in processing both source code and text (comments and documentation). For nodes containing lengthy text, we segment the content into chunks of 512 tokens each. These chunks are processed independently by the encoder. To maintain graph consistency, we duplicate the source node for each chunk, preserving identical connections to other nodes. The chunks within node are fully connected, and their sequential order is maintained through position embeddings in the LLM decoder D. We fine-tune the encoder using Low-Rank Adaptation (LoRA) [17] to optimize its performance for downstream tasks. The adapter serves as bridge between the encoder and LLM, projecting encoder outputs into the LLMs input embedding space. Following successful practices in Vision Language Models (VLMs) [52, 53], we implement the adapter as two-layer MLP with GELU activation [54]. The adapter is trained from scratch with random initialization. Unlike VLMs, which bridge different modalities, CGMs encoder and decoder are of the same modality, simplifying the alignment process. Furthermore, we compress each 512-token chunk (shown as gray tokens in Figure 2(d)) into single node token (black tokens in Figure 2(d)) for the LLM decoder. This compression effectively extends the LLMs context length by factor of 512, enabling the processing of extensive code repository contexts. Similar techniques, referred to as soft prompt compression, have been shown to enhance long-context modeling in recent studies [55, 56, 57]. Structural Integration: Besides node information, another challenge is integrating the graph structure into the LLM decoder D. While LLMs excel at processing sequential data, they are not inherently designed to capture graph structures [50]. Traditional approaches have attempted to incorporate repository-level structural information by simply linearizing code snippets into sequences [14, 45], but this transformation often fails to preserve the explicit relationships between code entities. To better maintain structural relationships, we introduce graph-aware attention mask to replace the causal attention mask solely between node tokens in the LLM. This mask is derived from the code graphs adjacency matrix, taking into account the node duplication process described earlier. We then fine-tune the LLM with LoRA to adapt it to both the 5 arXiv Template PREPRINT new attention mechanism and the node tokens from the adapter A. This approach ensures that attention occurs only between neighboring nodes in the code graph, mimicking the message passing mechanism frequently used in spatial GNNs [58, 59]. 4.2 Training Strategies Given the pretrained encoder and decoder D, the training of the CGM consists of two main phases: Subgraph Reconstruction Pre-training: This phase focuses on training the CGM to effectively capture both the semantic and structural aspects of code graphs. To achieve this, we introduce novel pre-training task that requires the model to reconstruct code content from its corresponding code graph, process we refer to as Graph-to-Code. In this task, the inputs are subgraphs randomly sampled from large-scale code graphs, with limited number of nodes. This constraint ensures that the corresponding output code remains below 8,000 tokens, allowing for computational efficiency and manageable context sizes during training. To enhance the meaningfulness of the output code, we employ hierarchical approach that preserves the inherent dependencies within the code graphs as they are translated into text. Concretely, for higher-level nodes (e.g., REPO and PACKAGE), we position them at the beginning of the output sequence or their respective files to maintain hierarchical consistency. We then utilize the approach from DeepSeek-Coder [14] to perform topological sorting on all file nodes, thereby establishing structured order for the code content. Lastly, intra-file nodes (e.g., CLASS and FUNCTION) are sorted by line numbers and concatenated within their respective files, culminating in coherent text sequence that accurately represents the sampled subgraph. Noisy Fine-tuning: This phase fine-tunes CGM on real-world issue-patch pairs [5], adapting it to practical software debugging and code editing tasks. As displayed in Figure 2(d), the model learns to generate code patches based on two inputs: (i) subgraph and (ii) text prompt that indicates the oracle filesfiles that require modification according to the ground-truth patch. The subgraph is constructed by combining the oracle files, their downstream nodes, and one-hop neighbors from the repository-level code graph. To improve model robustness, we intentionally introduce noise into the prompts: 10% include an irrelevant file that doesnt require modification, while another 10% omit at least one oracle file. This controlled noise exposure helps the model better generalize to real-world scenarios where inputs may be incomplete or contain irrelevant information. 4.3 The Graph RAG Framework This section presents our Graph RAG framework, streamlined extension of CGM designed for automated resolution of real-world repository tasks. The framework consists of four core modules: Rewriter, Retriever, Reranker, and Reader (the proposed CGM). This compact architecture stands in contrast to the SOTA agentless method, which requires ten distinct steps [2]. As illustrated in Figure 2, the framework operates sequentially. First, Rewriter enhances the original issue description to help Retriever identify relevant nodes in the code graph. Retriever then constructs connected subgraph using both lexical and semantic search techniques. This subgraph serves as input for both Reranker and Reader. Reranker analyzes the subgraph to identify the Top files likely to be modified. Finally, Reader (CGM) generates the code patch using both the subgraph from Retriever and the selected files from Reranker. Rewriter and Reranker are implemented by prompting the open-source Qwen2.5-72B-instruct [20], while the semantic search in Retriever utilizes the open-source CGE-Large model [60]. In Appendix D, we provide case study on how CGM solve specific issue from scratch. Meanwhile, we report the computational costs of our framework, including the cost of code graph construction, in Appendix C.4 Rewriter comprises two subcomponents: Extractor and Inferer, as illustrated in Figure 2(a). Extractor identifies key code elements from the user query, including file names, function names, and relevant keywords. Inferer then enriches the querys semantics by providing more detailed functional descriptions. The specific prompts for both components are detailed in Appendix F. Retriever generates connected subgraph from the code graph for subsequent modules. As shown in Figure 2(b), Extractor nodes (blue nodes) are first identified through string matching with the code elements and keywords extracted earlier. Next, Inferer nodes are located (red nodes) through semantic search, comparing the Inferers output with each nodes textual information. These anchor nodes are then expanded to include their one-hop neighbors, capturing local programming dependencies [61]. To ensure connectivity and incorporate upstream information, these expanded nodes are connected to the Root node (REPO in Figure 1). Finally, each FILE node in the subgraph is expanded to include all its internal nodes, aligning with Rerankers file-level output. The result is repository-enhanced context subgraph representing the user query, asdenoted by the shaded nodes in Figure 2(b). 6 arXiv Template PREPRINT Table 1: Performance comparison of open source system on SWE-bench Lite and Verified. CS-3.5 denotes Claude-3.5Sonnet-20241022, DS-V3 represents DeepSeek-V3, Q2.5C-32B means Qwen2.5-Coder-32B and Q2.5-72B stands for Qwen2.5-72B-Instruct. The icons denote open and closed-source models, respectively. and (a): SWE-bench Lite (b): SWE-bench Verified Method LLM Agent % Rank All Method LLM Agent % Rank All DARS Agent CGM-SWE-PY Lingxi CodeAct-v2.1 PatchKitty-0.9 Composio SK Agentless-v1.5 Moatless Patched.Codes CGM-Multi AppMap Agentless Lite Agentless-v1.5 Moatless SWE-Fixer Lingma SWE-GPT NA CS-3.5 Yes Q2.5-72B No Yes CS-3.5 Yes CS-3.5 Yes CS-3.5 Yes CS-3.5 No CS-3.5 Yes Yes CS-3.5 Q2.5-72B No Yes CS-3.5 No o3-mini No GPT-4o DS-V3 Yes Q2.5-72B Yes Q2.5-72B No 47.00 43.00 42.67 41.67 41.33 41.00 40.67 39.00 37.00 36.67 36.00 32.33 32.00 30.67 24.67 22.00 1 2 3 4 5 6 7 8 9 10 11 13 14 16 26 28 6 8 10 11 12 14 32 19 20 23 24 31 32 35 51 57 OpenHands PatchPilot-v1.1 SWE-Agent Agentless-v1.5 CGM-SWE-PY Composio SK Agentless Lite Composio SK SWE-Agent Agentless-v1.5 SWE-Fixer Lingma SWE-GPT Lingma Agent Lingma SWE-GPT SWE-Agent SWE-Agent NA NA NA CS-3.7 CS-3.5 Q2.5-72B Yes 65.80 NA 64.60 62.40 Yes 50.80 No 50.40 No 48.60 Yes 42.40 No o3-mini 40.60 CS-3.5 Yes 40.20 Q2.5C-32B No 38.80 No GPT-4o 32.80 Yes Q2.5-72B 30.20 No Q2.5-72B 28.80 Yes Q2.5-72B 25.40 No Q2.5-72B 23.20 Yes GPT-4o 22.40 Yes GPT-4 1 2 3 4 5 6 8 10 11 12 14 15 16 18 16 17 1 5 10 25 26 31 39 47 48 50 54 58 60 64 67 Reranker further refines the subgraph generated by Retriever, selecting only the Top files deemed most likely to be revised. This refinement is necessary because Retrievers output includes files that may only be referenced and not modified. Reranker operates in two steps: first, it selects = 10 files based on the original user query and file names; next, it narrows this selection down to = 5 files by individually scoring each one according to how relevant its file skeleton [2] is to the user query. The specific prompt for Reranker can be found in the Appendix F. Reader receives two inputs: the subgraph from Retriever as node tokens (black tokens) and the selected files with their full contents as text tokens (gray tokens), as depicted in Figure 2(d). These inputs are combined using the prompt template in the white box on the left of the figure. The graph and text tokens complement each other by providing global and local information related to the user query. Using this comprehensive information, Reader (i.e., the CGM) generates the final response."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we assess the performance of the CGM on two primary tasks: repository-level issue resolution and code completion, for both Python and Java programming languages. We also conduct series of ablation studies to validate the effectiveness of the model design and training strategies. 5.1 Repository-Level Issue Fixing This section evaluates the proposed CGM against other SOTA methods in resolving real-world software issues. We use three benchmark datasets: SWE-bench Lite, containing 300 issues from 11 Python repositories, SWE-bench Verified, containing 500 issues from 12 Python repositories, and SWE-bench-java Verified, comprising 91 issues from 6 Java repositories. All benchmarks utilize developer-written unit tests to verify the correctness of model-generated patches, ensuring rigorous evaluation. Performance is measured using the resolution rate (% R), defined as the percentage of successfully resolved issue instances. We present results for two variants of our model: CGM-Multi, trained for both issue resolution and code completion tasks across Python and Java repositories, and CGM-SWE-PY, specifically optimized for Python issue resolution. Detailed information regarding the datasets and implementations can be found in Appendix C.5. As shown in Table 1(a), our CGM-SWE-PY model achieves 43% resolution rate on SWE-bench Lite, placing it first among methods utilizing open-source models, second among those that implement open-source methods but use closed-source models, and eighth overall. Notably: (i) When compared to other methods based on open-source models, CGM-SWE-PY outperforms Moatless+DeepSeek-V3 by 12.33% [15], despite DeepSeek-V3s generally superior performance in various coding benchmarks compared to our LLM decoder Qwen2.5-72B [21]. Furthermore, it exceeds Lingma SWE-GPT by 21%, even though the latter employs carefully curated COT (chain-of-thought) data to 7 arXiv Template PREPRINT Table 2: Performance evaluation on SWE-bench-java Verified. DS-V2 denotes DeepSeek-Chat-V2, DSC-V2 represents DeepSeek-Coder-v2, GPT-4o refers to GPT-4o-2024-05-13, DB-128K stands for Doubao-Pro-128k, and GPT-4o-MINI indicates GPT-4o-MINI-2024-07-18. The icons denote open-source and closed-source methods or models, respectively. and METHOD LLM AGENT % RANK CGM-MULTI SWE-AGENT SWE-AGENT SWE-AGENT SWE-AGENT SWE-AGENT Q2.5-72B DS-V2 DSC-V2 GPT-4O DB-128K GPT-4O-MINI NO YES YES YES YES YES 14.29 9.89 7.69 6.59 1.10 1. 1 2 3 4 5 6 Table 3: Performance comparison on CrossCodeEval and ComplexCodeEval benchmarks. DeepSeek-236B represents DeepSeek-V2.5-236B, Mixtral-123B denotes Mistral-Large-Instruct-2411, and Qwen2.5-72B refers to Qwen2.5-72BInstruct. Baseline models are evaluated using FIM (Fill-in-Middle) and one-hop expansion. METHOD CROSSCODEEVAL JAVA PYTHON ES EM EM ES COMPLEXCODEEVAL PYTHON JAVA ES EM ES EM 47.17 82.23 53.92 82.42 37.00 64.81 31.00 62.48 MIXTRAL-123B 44.74 83.81 58.54 85.03 36.00 63.08 32.00 60.60 DEEPSEEK-236B 37.31 78.78 58.50 81.56 26.00 54.14 28.00 57.12 QWEN2.5-72B CGM-MULTI-72B 50.21 80.76 61.20 84.30 47.00 78.86 43.00 72.60 boost Qwen2.5-72Bs effectiveness in issue resolution. (ii) In relation to other agentless frameworks, CGM-SWE-PY slightly surpasses Agentless+Claude-3.5-Sonnet by 2.33% and significantly outperforms Agentless+GPT-4o by 11.00%. This achievement is particularly noteworthy given that Agentless leverages complex ten-step pipeline with more powerful closed-source models, while CGM-SWE-PY operates on simpler four-step Graph RAG framework. We attribute this success to CGMs enhanced capacity to interpret both semantic and structural information within repositories. (iii) While the top methods on SWE-bench Lite are entirely closed-source regarding both models and implementations, CGM-SWE-PYs results are within 10% of these systems. This indicates that CGM-SWE-PY has the potential to compete with leading agent-based methodologies. Compared to other open-sourced model-based methods, CGM significantly narrows the gap between open-source models and closed-source methods in issuefixing scenarios. (iv) Our multi-task model, CGM-Multi, achieves resolution rate of 36.67% on SWE-bench Lite, ranking 23rd overall. The relatively lower performance compared to CGM-SWE-PY can be attributed to its broader focus, which encompasses both issue fixing and code completion tasks across Python and Java repositories. (v) We further apply CGM-SWE-PY to larger Python benchmarkSWE-bench Verified in Table 1(b), where CGM-SWE-PY ranks first again among open weight models, and fifth among methods with open-source system. In the SWE-bench-java evaluation for Java repositories as shown in Table 2, CGM-Multi records resolution rate of 14.29%, significantly outclassing SWE-Agent build upon both closed-source and open-source models. These findings further substantiate the effectiveness of our proposed GCM and the specially designed Graph RAG framework. 5.2 Repository-Level Code Completion In this section, we evaluate the CGMs performance on code completion tasks at the repository level for both Python and Java programming languages. Our evaluation uses two benchmarks: CrossCodeEval and ComplexCodeEval. Concretely, CrossCodeEval focuses on cross-file code completion, while ComplexCodeEval encompasses more intricate tasks, including API recommendations and test case generation. Performance is measured using two metrics: Edit Match (EM) and Edit Similarity (ES), evaluating how similar the generated code is to the ground-truth code. Detailed information regarding the datasets, metrics, and the implementation of baseline models can be found in Appendix C.6. Table 3 presents the results for CGM-Multi, which uses Qwen2.5-72B-instruct as its LLM decoder. We compare it with similarly sized large language models, including Mistral-Large-Instruct-123B, DeepSeek-V2.5-236B, and the standalone Qwen2.5-72B-instruct. For all models, context retrieval for code completion is performed by identifying one-hop neighbors of the target file (that requires completion) in the code graph. While CGM-Multi processes the entire subgraph as input, baseline models only receive the textual content from the nodes. Results show that CGM-Multi performs on par with or exceeds other models on CrossCodeEval. More importantly, it greatly outperforms the 8 arXiv Template PREPRINT Table 4: Comparison of CGM with RAG variants on CrossCodeEval. Results are reported for Java and Python across multiple base models. Evaluation metrics include EM and ES. METHOD CODELLAMA-7B JAVA PYTHON ES EM EM ES DEEPSEEK-CODER-7B PYTHON ES JAVA EM ES EM 20.60 54.50 13.70 44.10 24.20 59.30 19.40 52.50 NORAG 23.42 66.13 21.76 69.09 22.49 66.78 23.30 70.84 BM25 27.92 73.09 REPOFUSE 26.23 67.61 26.60 72.27 26.09 67.31 30.28 74.42 RLCODER R2C2 35.60 58.50 23.60 42.90 41.60 64.60 32.70 54.00 CGM-MULTI 36.42 75.28 31.03 73.90 41.65 74.76 33.88 71.19 24.80 71.05 / / / / METHOD STARCODER-7B JAVA EM ES PYTHON ES EM QWEN2.5-CODER-7B PYTHON JAVA ES EM ES EM 21.60 55.90 17.00 49.50 37.31 78.78 33.63 73.19 22.16 67.80 22.33 69.60 49.37 82.63 43.15 78.66 NORAG BM25 REPOFUSE RLCODER R2C2 CGM-MULTI 37.44 73.77 31.00 71.66 51.61 84.62 46.23 82.16 24.20 70.82 24.73 69.08 25.82 72.11 38.10 63.60 30.90 51.90 / / / / / / / / / / / / / / baseline models on ComplexCodeEval, demonstrating its superior capability in handling complex tasks through comprehensive subgraph analysis. Next, we evaluate CGM against other RAG methods for CrossCodeEval. The comparison includes several established systems: BM25, the default retrieval method in CrossCodeEval [62]; RLcoder [63], which employs reinforcement learning for retrieval optimization; RepoFuse [64], which integrates code graphs during retrieval but converts retrieved code snippets into linear text sequences; and R2C2 [65], which combines retrieved code snippets with Tree-sittergenerated abstract context as the input to the LLM. In our CGM implementation, we still construct input subgraphs by combining target files with their one-hop neighbors. We evaluate these methods using various base models for generation, including CodeLlama-7B, StarCoder-7B, DeepSeek-Coder-7B, and Qwen2.5-Coder-7B. This diverse set of comparison methods enables comprehensive evaluation of CGMs effectiveness in long-context retrieval and understanding. As shown in Table 4, CGM typically outperforms other RAG methods, regardless of the base model used, suggesting that graph-based context retrieval is more effective for code completion tasks. Moreover, CGMs superiority over RepoFuse, which also uses code graphs for retrieval, can be attributed to CGMs explicit integration of structural information within the subgraph, whereas RepoFuse flattens node context into text sequences, obscuring the explicit dependencies among code entities. 5.3 Ablation Studies In this section, we present key findings from our ablation studies, with detailed analysis available in Appendix C.7. Our investigation reveals four crucial insights: (i) Graph RAG: Our assessment of the Graph RAG modules shows that the presence of Rewriter, Retriever, and Reranker is essential for achieving optimal performance on the SWE-bench Lite benchmark. Notably, Reranker plays pivotal role as it dictates which files should be modified. (ii) Semantic Integration: Joint fine-tuning of all three componentsencoder E, the adapter A, and the decoder Dyields superior performance compared to keeping any component fixed. (iii) Structural Integration: The integration of graph structural information through attention masking is essential for optimal performance. (iv) Training Strategies: The subgraph reconstruction task, as described in Section 4.2, significantly contributes to improving the CGMs overall performance. (v) Backbone Generalization: Moreover, CGM can also be generalized on backbones with different sizes, demonstrating its potential for resource-constrained scenarios."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present CGM, novel graph-enhanced LLM architecture designed for comprehensive repository-level code understanding. By seamlessly integrating both semantic and structural information from codebases through specialized encoder-adapter framework and graph-aware attention mechanisms, CGM demonstrates that sophisticated agent-based approaches and closed-source models are not necessarily required for complex SE tasks. When combined 9 arXiv Template PREPRINT with our custom-designed Graph RAG framework, CGM achieves remarkable 43.00% resolution rate in real-world issue-fixing scenarios on SWE-bench Lite, using only open-source models. Our work establishes new direction for developing powerful, transparent, and accessible tools for automated SE."
        },
        {
            "title": "References",
            "content": "[1] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. [2] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [3] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. Unifying the perspectives of nlp and software engineering: survey on language models for code. arXiv preprint arXiv:2311.07989, 2023. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [6] Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, et al. Swe-bench-java: github issue resolving benchmark for java. arXiv preprint arXiv:2408.14354, 2024. [7] Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, and Chao Peng. Marscode agent: Ai-native automated bug fixing. arXiv preprint arXiv:2409.00899, 2024. [8] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. [9] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [10] Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. [11] Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository? arXiv preprint arXiv:2406.01422, 2024. [12] Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement. arXiv preprint arXiv:2410.20285, 2024. [13] Jiya Manchanda, Laura Boettcher, Matheus Westphalen, and Jasser Jasser. The open source advantage in large language models (llms). arXiv preprint arXiv:2412.12004, 2024. [14] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. [15] Aor. Aoatless-tools. https://github.com/aorwall/moatless-tools, 2024. [16] Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, and Kevin Murphy. Machine learning on graphs: model and comprehensive taxonomy. Journal of Machine Learning Research, 23(89):164, 2022. [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [18] Sundar Pichai, Demis Hassabis, agentic new ai model google-gemini-ai-update-december-2024/, 2024. the for and Koray Kavukcuoglu. era. our https://blog.google/technology/google-deepmind/ Introducing gemini 2.0: [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [20] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 10 arXiv Template PREPRINT [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. [23] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. [24] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [25] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [26] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. [27] Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, and Rui Wang. Galla: Graph aligned large language models for improved source code understanding. arXiv preprint arXiv:2409.04183, 2024. [28] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020. [29] Sindhu Tipirneni, Ming Zhu, and Chandan Reddy. Structcoder: Structure-aware transformer for code generation. ACM Transactions on Knowledge Discovery from Data, 18(3):120, 2024. [30] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. Treebert: tree-based pre-trained model for programming language. In Uncertainty in Artificial Intelligence, pages 5463. PMLR, 2021. [31] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. Unixcoder: Unified cross-modal pre-training for code representation. arXiv preprint arXiv:2203.03850, 2022. [32] Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. Integrating tree path in transformer for code representation. Advances in Neural Information Processing Systems, 34:93439354, 2021. [33] Cognition. Introducing devin. https://www.cognition.ai/introducing-devin, 2023. [34] Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. [35] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024. [36] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. [37] Jiaolong Kong, Mingfei Cheng, Xiaofei Xie, Shangqing Liu, Xiaoning Du, and Qi Guo. Contrastrepair: Enhancing conversation-based automated program repair via contrastive test case pairs. arXiv preprint arXiv:2403.01971, 2024. [38] Yifan Xie, Zhouyang Jia, Shanshan Li, Ying Wang, Jun Ma, Xiaoling Li, Haoran Liu, Ying Fu, and Xiangke Liao. How to pet two-headed snake? solving cross-repository compatibility issues with hera. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pages 694705, 2024. [39] Zhipeng Xue, Zhipeng Gao, Xing Hu, and Shanping Li. Acwrecommender: tool for validating actionable warnings with weak supervision. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 18761880. IEEE, 2023. [40] Cheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, and Michael Lyu. unified debugging approach via llm-based multi-agent synergy. arXiv preprint arXiv:2404.17153, 2024. [41] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. arXiv preprint arXiv:2401.07339, 2024. 11 arXiv Template PREPRINT [42] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604, 2024. [43] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [44] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [45] Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024. [46] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pages 3169331715. PMLR, 2023. [47] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. [48] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv preprint arXiv:2212.10007, 2022. [49] Wei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang, Haiyan Zhao, Zhi Jin, and Qianxiang Wang. Graphcoder: Enhancing repository-level code completion via code context graph-based retrieval and language model. arXiv preprint arXiv:2406.07003, 2024. [50] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: comprehensive survey. IEEE Transactions on Knowledge and Data Engineering, 2024. [51] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023. [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [54] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [55] Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, and Wei Zhang. E2llm: Encoder elongated large language models for long-context understanding and reasoning. arXiv preprint arXiv:2409.06679, 2024. [56] Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph Gonzalez, and Raluca Ada Popa. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979, 2024. [57] Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun, Zhichao Duan, Junjie Fang, Rong Han, Zixuan Wang, and Jianyong Wang. Focusllm: Scaling llms context by parallel decoding. CoRR, 2024. [58] Petar Veliˇckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [59] Ahsan Shehzad, Feng Xia, Shagufta Abid, Ciyuan Peng, Shuo Yu, Dongyu Zhang, and Karin Verspoor. Graph transformers: survey. arXiv preprint arXiv:2407.09777, 2024. [60] CodeFuse. Codefuse-cge. https://github.com/codefuse-ai/CodeFuse-CGE, 2024. [61] Zhiyuan Pan, Xing Hu, Xin Xia, and Xiaohu Yang. Enhancing repository-level code generation with integrated contextual information. arXiv preprint arXiv:2406.03283, 2024. [62] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36, 2024. [63] Yanlin Wang, Yanli Wang, Daya Guo, Jiachi Chen, Ruikai Zhang, Yuchi Ma, and Zibin Zheng. Rlcoder: Reinforcement learning for repository-level code completion. arXiv preprint arXiv:2407.19487, 2024. 12 arXiv Template PREPRINT [64] Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Hongwei Chen, Chengpeng Wang, Gang Fan, et al. Repofuse: Repository-level code completion with fused dual context. arXiv preprint arXiv:2402.14323, 2024. [65] Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, et al. R2c2-coder: Enhancing and benchmarking real-world repository-level code completion abilities of code large language models. arXiv preprint arXiv:2406.01359, 2024. [66] Wenhua Li, Quang Loc Le, Yahui Song, and Wei-Ngan Chin. Incorrectness proofs for object-oriented programs via subclass reflection. In Asian Symposium on Programming Languages and Systems, pages 269289. Springer, 2023. [67] Jason Sawin and Atanas Rountev. Assumption hierarchy for cha call graph construction algorithm. In 2011 IEEE 11th International Working Conference on Source Code Analysis and Manipulation, pages 3544. IEEE, 2011. [68] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. [69] OpenAI."
        },
        {
            "title": "Introducing",
            "content": "swe-bench verified. https://openai.com/index/ introducing-swe-bench-verified/, 2024. [70] Jia Feng, Jiachen Liu, Cuiyun Gao, Chun Yong Chong, Chaozheng Wang, Shan Gao, and Xin Xia. Complexcodeeval: benchmark for evaluating large code models on more complex code. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pages 18951906, 2024. [71] model fluent in 80+ programming languages. https://mistral.ai/news/codestral/, 2024. [72] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. 13 arXiv Template PREPRINT"
        },
        {
            "title": "A Case in Issue Fix Scenario",
            "content": "Figure 3: Illustration of real-world issue from pytorch-lightning codebase, where user wants to disable automatic checkpoint loading. Given the original issue, the corresponding diff-formatted patch (bottom left) shows all the code modifications in linear fashion. Compared to code sequences, the relationships between them can be more clear if we represent the code as graph (bottom right), where containment (solid lines), inheritance (dashed lines), and function calls (dotted lines) explicitly demonstrate the connections between different code snippets."
        },
        {
            "title": "B Details of Code Graph",
            "content": "B.1 Node and Edge Types in Code Graph This section provides the node and edge types defined in our code graph. Table 5 and Table 6 detail the categories of nodes and edges, respectively. For now, code graph supports two objected-oriented programming language (Python and Java). Node Type Description Table 5: Node Types in Code Graph. REPO PACKAGE FILE TEXTFILE CLASS FUNCTION ATTRIBUTE Includes global variables, member variables, and local variables Virtual node, represents the root of the repository Virtual node, acts as representation of directory in the file system Files ending with .py Files that do not end with .py, such as Markdown (.md) files and text (.txt) files In object-oriented programming, class is blueprint for creating objects Refers to the function within classes or standalone function Edge Type Description Table 6: Edge Types in Code Graph. contains calls extends imports implements This edge is exclusively applicable to Java, denoting the relation where class Indicating hierarchical relationship in which one entity contains another entity This type of edge captures the dynamic invocation relationship Representing an inheritance relationship, where one class extends another class Represent dependencies where one file imports another class/function implements an interface B.2 Handling of Complex Dependiences During the construction of code graph, we explicitly address both dynamic calls and multiple inheritance in the following way. 14 arXiv Template PREPRINT Dynamic Calls: We employ conservative resolution approach following the over-approximation principle [66]. When encountering base class method calls (e.g., Base.method()), we include all possible overriding implementations from subclasses in the calls set. This ensures we dont miss any potential execution paths. Multiple Inheritance: We utilize the Class Hierarchy Analysis (CHA) algorithm [67] to properly handle inheritance relationships, including cases where classes inherit from multiple parent classes. B.3 Search on Code Graph Graph search can be easily implemented in code graph. The first step usually begins with finding the source node. This can be achieved by many ways, such as indexing, keyword and embedding matching. Starting from the source node, different strategies can be applied, such as one-hop neighborhood, depth-first search (DFS), breadth-first search (BFS), random walk, etc. It is up to the application scenarios to decide which search algorithm is the best. The result of graph search could be sub-graph of the whole repository-level graph, containing the most relevant context for specific problems."
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 Details of Training CGM This section details how we train CGM-Multi (multi-language version), CGM-SWE-PY (tailored for Python issue fixing), and CGM 7B series (based on different 7B base models). C.1.1 Training Data As mentioned in section 4, we construct training data for different training phase of CGM respectively. Meanwhile, to enhance the models ability in code completion, we also construct code completion samples for fine-tuning of code completion task. To explore the performance of CGM across different programming languages, our data includes both Python and Java. When constructing the training data, we filter out the repositories involved in the test sets for testing to avoid data leakage. Data for Subgraph Reconstruction Pre-training: We obtain 500k Python and 360k Java subgraphs (with the maximum length of 8k tokens) from total of 20k high-star Github repositories. Data for Issue Fixing: We collect 200k issue-patch pairs (100k per language), from GitHub pull-requests. Among the 100k Python pairs, 14k are sourced from the SWEBench training set [8]. Data for Code Completion: The code completion samples are self-constructed from the above repositories, 250k per language. C.1.2 CGM-Multi We initialize CGM-Multi with Qwen2.5-72B-Instruct [20] as the base LLM. Then pre-train it using subgraph reconstruction data and fine-tuning data (issue-fixing and code completion) in both two languages (Python and Java). To ensure balance between different languages, we use 360k subgraph reconstruction data for both languages. Training uses 4-bit QLoRA on 64 A100s (batch=32, lr=1e-4, and epoch=2). The encoder combines CodeT5+ [51] with LoRA (rank=64, and alpha=32), and the adapter uses two-layer MLP with GELU activation. The first layer of the adapter maps the CodeT5+ output dimension of 256 to 8192, and the second layer maintains the dimension of 8192, which aligns with the LLMs hidden dimension. We adopt Xformers [68] for efficient attention computation. C.1.3 CGM-SWE-PY CGM-SWE-PY, as model specifically designed for SWE-bench Lite, is pre-trained using python subgraph reconstruction data (the entire 500k) and fine-tuned on specific python issue-fixing data (the 14k sourced from SWEBench training set). Besides, all details of training and parameters are set the same as CGM-Multi. C.1.4 CGM 7B Series We train several small-scale CGMs based on the existing 7B base models to compare with small-scale models on code completion benchmarks. Specifically, we trained CGMs in seperate language based on CodeLlama-7B, StarCoder-7B, DeepSeek-Coder-7B, and Qwen2.5-Coder-7B-Instruct, respectively. For each model in each language, we use training 15 arXiv Template PREPRINT Table 7: Recall performance of each Graph RAG module on SWE-bench Lite and SWE-bench-java Verified. The table shows the recall percentage for Retriever, Reranker Stage 1, and Reranker Stage 2 components. MODULE SWE-BENCH LITE % RECALL SWE-BENCH-JAVA VERIFIED % RECALL RETRIEVER RERANKER STAGE 1 RERANKER STAGE 2 94 89 87 87 74 60 data in the target language during both pre-training and fine-tuning stages. For example, we train CodeLlama-7B with 500k Python subgraph reconstruction data and 250k Python code completion samples, and then evaluate it on the Python test set of crosscodeeval. Except for modifying the parameters in LoRA (set rank=32, and alpha=16), other training/parameter settings are consistent with CGM-Multi. C.2 Recall Results for the Graph RAG Framework We provide the recall of each component of our Graph RAG framework (in the file level), as shown in Table 7. The recall of each component on SWE-bench-java Verified are lower than those on SWE-bench Lite. One possible reason may be that the issues in SWE-bench Lite usually requires modifying one file, while the issues on the SWE-bench-java Verified sometimes need to modify multiple files. C.3 Hyperparameters for Inference We use the same parameter settings for inference with LLMs (CGMs and Qwen2.5-72B-Instruct in the Graph RAG framework), setting topk = 20, topp = 0.8, temperature = 0.7, and repetition penalty = 1.1. C.4 Cost Analysis In this section, we present cost analysis of the overall process, including the time required for code graph construction and computational expenses. C.4.1 Code Graph Construction The construction of repository-level code graph usually takes 3 minutes or more depending on the complexity of the code repository (such as the implementations of different classes and the calling relationships between codes). Since code graph construction can be performed offline, it does not impact real-time inference workflows. Additionally, optimizations such as incremental updates and parallel processing can further reduce latency for large-scale repositories. C.4.2 Cost of Each Module We analyze the runtime and resource requirements of each key module in our system, focusing on LLM inference overhead, memory consumption, and latency scaling. Rewriter: Requires two sequential LLM calls (Qwen2.5-72B-Instruct). Retriever: Anchor node matching and subgraph generation take 37 seconds per issue. Lightweight CPU operation. Reranker: Requires two sequential LLM calls (Qwen2.5-72B-Instruct). Latency additive to Rewriter ( 2 single-call time). Reader (CGM): Table 8 reports the memory consumption and inference latency. 16 arXiv Template PREPRINT Latency increases by 0.50.7s per 1k tokens (1k8k: 3.9s8.6s). Table 8: Inference Time and Memory Cost of CGM (Qwen2.5-72B). # Input Tokens Time (s) Memory (GB) 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 3.934 4.408 5.055 5.808 6.432 7.163 7.838 8.553 68.79 68.98 69.43 70.26 70.77 70.98 71.44 72.02 C.5 Experimental Setup of Issue Fixing C.5.1 Implementation Details of CGM To adapt CGM in the issue-fix scenario, we extend CGM to GraphRAG framework (as described in section 4). In this scenario, the inputs of CGM are the corresponding subgraph and prompt generated by R3 (Rewriter, Retriever, Reranker). In the experiment, we compare two pre-trained CGMs, CGM-Multi and CGM-SWE-PY (see Appendix C.1 for training details), as Reader in our Graph RAG framework. C.5.2 Datasets The following three benchmarks, which focus on repository-level issue fixing, are all evaluated using the Docker executable environment. SWE-bench Lite [5]: SWE-bench Lite contains 300 self-contained repository-level issues from 11 repositories, designed to test the models understanding of repository-level code changes and its ability to generate correct patches primarily focused on Python. It provides realistic software engineering environment, including execution contexts, to evaluate the models ability to resolve real-world issues. SWE-bench Verified [69]: SWE-bench Verified contains 500 self-contained repository-level issues from 12 repositories. This dataset contains samples that have been verified to be non-problematic by human annotators. SWE-bench-java Verified [6]: This dataset include 91 Java issues from 6 repositories, enabling cross-language evaluation. Like SWE-bench Lite, it provides execution environments to validate the correctness of generated patches. C.5.3 Evaluation Metrics Resolve Rate (% R): The metrics used in the above benchmarks is Resolve Rate, which evaluates the correctness of generated patches for the issue-fix task. patch is considered resolved if it correctly addresses the issue and is superset of the ground-truth edits. C.6 Experimental Setup of Code Completion C.6.1 Implementation Details of CGM As simpler scenario than issue fixing, the files that need to be modified are given in the code completion tasks. Therefore, we obtain the input subgraph of CGM through heuristic method, rather than the Graph RAG framework. To be specific, we take the given incomplete file as the center node, obtaining its one-hop ego graph from the repositorylevel code graph. Note that nodes that need to be completed are not considered in this process. The resulting subgraph (graph modalities), and the incomplete files (text modalities), form the inputs to the CGM. In this experiment, we use two size of CGMs for evaluation. Training details for the large-scale CGM-Multi (72B) and small-scale CGM 7B series can be found in the Appendix C.1. arXiv Template PREPRINT C.6.2 Datasets CrossCodeEval [62] is an emerging benchmark for cross-file code completion, which is constructed from wide range of real-world repositories from GitHub in four popular programming languages: Python, Java, TypeScript, and C#. In our experiments, we evaluate the models code completion ability on only two languages, Java and Python. As shown in Table 9, we provide the dataset statistics of the CrossCodeEval benchmark for Java and Python. ComplexCodeEval [70] is new benchmark for evaluating the performance of large code models in complex development scenarios. It includes 3,897 Java samples from 1,055 Java code repositories and 7,184 Python samples from 2,107 Python code repositories. Following the original setup of this benchmark, we randomly selected 100 samples each in Python and Java for the evaluation. Table 13 and Table 14 show the information of the selected samples. When evaluating the code completion capability, unlike the original setup which requires the model to complete the second half of the function, we ask the model to complete the middle line of the function given the contextual information. Table 9: The statistics of the CrossCodeEval for Java and Python."
        },
        {
            "title": "Python",
            "content": "# Repositories # Files # Examples 239 745 2139 471 1368 2665 C.6.3 Evaluation Matrics When evaluating prediction code in comparison to the reference ground truth y, the above benchmarks utilize the following two metrics: the exact match accuracy (EM) and the Levenshtein edit similarity (ES). EM: The exact match accuracy (EM) is determined by an indicator function. This function takes value of when the prediction is exactly equal to the reference y, and 0 otherwise. ES: The Levenshtein edit similarity (ES) is calculated using the formula ES = 1 Lev(y, y) max(y, y) . (1) Here, is used to compute the length of string, and Lev() is employed to calculate the Levenshtein distance between the two strings and y. C.6.4 Baselines: Base Model Given the subgraph same to CGM (see Appendix C.6.1), we textualize the graph into text sequnce and input into the following baselines based on the prompt templates provided by each benchmark [62, 70]. Since these base models have limitations on context length, we perform truncation on text inputs larger than 8k. Mistral-Large-2 [71] is model developed by Mistral AI with 123 billion parameters. It stands out with remarkable 128k tokens context length, proficiently handling dozens of languages and over 80 programming languages, excelling in code generation, mathematics, and reasoning. To be specific, we chose Mistral-Large-Instruct-2411 as the latest version of Mistral-Large to compare with CGM. DeepSeek-V2.5 [14] is strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token. Qwen2.5 [20] is decoder-only LM series whose size varies from 0.5B to 72B trained on 18 trillion tokens. Its context length support up to 128K tokens and can generate up to 8K tokens. Qwen2.5-Coder [25] is series of code-specific language models developed by Alibaba. Derived from Qwen2.5, it comes in six sizes, is trained on vast 5.5-trillion-token corpus, and excels in various code-related tasks, outperforming many models of the same or larger size. Since it uses specific format and tokens for training on the fill-in-middle code completion task, we followed this prompt setting during the evaluation to obtain its true performance. For the inference of the baseline model, we all deployed the above models using the VLLM framework with the models default settings. All models inference on 4 A100s with 80G VRAM, except for DeepSeek-V2.5, which requires 8 A100s. 18 arXiv Template PREPRINT C.6.5 Baselines: RAG Method BM25 [72] is classic information-retrieval algorithm based on the probabilistic model. Its core idea is to rank documents based on the relevance between query and documents. It serves as traditional retrieval method that does not regard the structural information naturally existing in the coding task, and only performs similarity matching based on word frequency and text length. It was used in the original CrossCodeEval dataset to search for cross-file information based on the code snippets. In our experiments, we directly use the BM25 results provided by CrossCodeEval. R2C2-Coder [65] is method that aims to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models. In particular, R2C 2-Enhance reduces cross-file information to skeleton3 text by syntactically analyzing the content of code files. The cross-file context is retrieved using BM25 after forming candidate retrieval pool together with the context obtained from semantic-based retrieval. It takes into account structural information of the code but does not establish graph relations across code files. RepoFuse [64] is solution for the Context-Latency Conundrum in repository-level code completion. It constructs Code Knowledge Graph by analyzing the code graph dependencies in the repository and uses the repository-level graphs for retrieval. It integrates the Rationale Context obtained by analyzing the repository code structure and the Analogy Context based on the retrieval of similar code blocks, and filtering the context by scoring function. RLCoder [63] is reinforcement-learning-based framework for repository-level code completion, which can effectively improve code completion performance and has good generalization ability. During training, the RLRetriever is trained with reward mechanism based on weighted perplexity to learn retrieval, while stop-signal mechanism is introduced to filter candidate codes. In inference, the trained RLRetriever retrieves useful candidate codes from the code repository and inputs them together with the incomplete code into the generator to complete code generation. C.7 Details of Ablation Study In this section, we first conduct an ablation study on our Graph RAG framework to verify the effectiveness of each component by SWE-bench Lite  (Table 10)  . Then, we conduct the other ablation study on CGM itself to evaluate the effectiveness of model design by CrossCodeEval dataset  (Table 11)  . Table 10: Impact of each RAG (Retrieval-Augmented Generation) component on the performance of CGM for issue fixing. Results are reported as the resolve rate (% R) on SWE-bench Lite, demonstrating the contribution of rewriter, retriever, and reranker modules. SETTING - W/O REWRITER - W/O RETRIEVER - W/O RERANKER - W/O R3 - W/O CGM READER (FLATGRAPH) % 34.67 31.67 18.33 9.67 5.33 C.7.1 Variants of Graph RAG Framework The Graph RAG framework, comprising Rewriter, Retriever, Reranker, and Reader, extends CGM to real-world issue fixing. In Table 10, we verify the effectiveness of each component in our Graph RAG framework by removing them. Here, we use CGM-SWE-PY (see Appendix C.1 for training details) as Reader. w/o Rewriter: We directly perform semantic search based on the original issue descriptions, obtain the anchor nodes from the code graph, and provide them to Retriever. Removing Rewriter results in an 8.33% performance drop, which proves its effectiveness in enhancing the original issue descriptions. w/o Retriever: Since there is no Retriever to provide filtered files and subgraphs, we input all the files in the original codebase into Rerankers Stage 1 for selection, and at the same time append the key information output by Rewriter into Rerankers prompts. Based on the files output by Reranker, we build subgraph using these files and their one-hop neighbors, as the graph modality input of CGM. The exclusion of Retriever results in an 11.33% performance degradation, more severe drop than removing Rewriter, highlighting its importance in providing issue-related subgraph. 3The file skeleton is hierarchical structure of the contents of code file, containing class and function declarations, without specific definitions and comments. arXiv Template PREPRINT Table 11: Impact of training strategies on CGMs performance. Results are reported for CrossCodeEval (Java and Python) in terms of EM and ES. Here, denotes the encoder, denotes the adapter, denotes the LLM, and combined refers to the full CGM setup (w/ mask, w/ Recon, + LoRA w/ + D). SETTING CROSSCODEEVAL JAVA EM ES PYTHON EM ES MODULE 17.91 - FREEZE - 41.70 - LORA W/ 46.84 - + LORA W/ 49. 58.28 77.25 82.06 83.21 11.78 34.90 38.76 43.15 51.60 74.54 76.02 79.84 MASK - W/O MASK TRAINING - W/O RECON CGM - COMBINED 48.71 83.40 42.21 80.46 42.78 80. 39.77 75.87 51.61 84.62 46.23 82. w/o Reranker: We use the top 5 files that are most similar to the query in embedding space (during semantic search) from the FILE node obtained by Retriever and provide them to Reader as the files to be modified. Removing Reranker results in the largest performance drop (decreased by -24.67%), emphasizing its importance in improving the precision of retrieval results and providing the right, relevant files to Reader. w/o R3: To evaluate the effectiveness of the RAG module, we create baseline which removes the first three modules (Rewriter, Retriever, and Reranker) and feed the entire (truncated when the length exceeds the context length of the base model) repository graph as input to Reader during fine-tuning. Removing the RAG module leads to poor performance (decreased by 33.33%), possibly due to excessive noise from the unfiltered repository graph and information loss from context-window truncation. w/o CGM Reader (FlatGraph): To verify the effectiveness of CGM Reader in jointly modeling semantics and structure, we create naive graph-based baseline which flattens code snippets based on topological structure [24], representing an alternative Reader with structure-enhanced fine-tuning. The naive graph-based Reader only achieves 5.33% on SWE-bench Lite, far behind the proposed CGM (decreased by 37.67%). C.7.2 Variants of CGM In Table 11, we compare CGM with its variants in the following three aspects. The CGM we use here is trained on Qwen2.5-Coder-7B Instruct (see Appendix C.1 for training details). Semantic Integration: To verify the design of CGM in understanding semantic information, we compare it with four types of variants: (1) freeze all parameters (include Encoder, Adapter, and LLM Decoder) (2) training the Adapter (3) training the LLM Decoder (4) training both the Adapter and LLM Decoder D. Table 11 demonstrates that training the adapter alone leads to significant improvements in the EM performance: 22.26% increase for Java and 21.43% increase for Python when comparing CGM-A with GGM-Freeze. Additionally, further training the LLM decoder in conjunction with the adapter esults in further enhancements, yielding 5.33% improvement for Java and 4.80% improvement for Python. Finally, when the encoder E, the adapter A, and the decoder are all trained together, we observe an additional increase of 5.71% for Java and 6.12% for Python. This data illustrates that fine-tuning the encoder E, the adapter A, and the decoder is essential to effectively align the graph and code modalities. Structural Integration: To verify the design of CGM in integrating structural information, we remove the graph-aware attention mask during training, and use the original causal mask (denoted as w/o MASK). As shown in Table 11, substituting the graph-aware attention mask in the CGM with standard causal mask results in drop of 8.61% in EM performance for Java and 5.56% for Python. This demonstrates the necessity of incorporating the structural information from the code graph into the CGM to maintain optimal performance. Training Strategy: We remove the subgraph reconstruction pre-training task to verify the effectiveness of this task, denoted as w/o RECON. Subgraph reconstruction pre-training plays crucial role, contributing 7.65% to the overall EM improvements. 20 arXiv Template PREPRINT C.8 Generalization of CGM on Different Backbones To evaluate CGM with different backbones, we trained CGM using Llama3.1-70B-Instruct, Qwen2.5-Coder-32BInstruct, and Qwen2.5-Coder-7B-Instruct, in addition to Qwen2.5-72B. The results are summarized in Table 12. We find that the performance of CGM positively correlates with the LLM decoders inherent coding and instructionfollowing abilities. For example, Llama3.1-70B-Instruct CGMs performance decreased 17.67% compared to Qwen2.572B, possibly due to weaker inherent coding abilities (see Table 2 in [20]). Still, it surpassed Lingma-SWEGPT [1] built on Llama3.1-70B-Instruct by 18.33%, demonstrating CGMs power in improving open-source LLMs. Table 12: CGM with Different Backbones. BACKBONE - QWEN2.5-72B-INSTRUCT - LLAMA3.1-70B-INSTRUCT - QWEN2.5-CODER-32B-INSTRUCT - QWEN2.5-CODER-7B-INSTRUCT % 43.00 25.33 28.67 4.00 arXiv Template PREPRINT Table 13: The Repositories and funcitons selected from ComplexCodeEval-Python. Repository IntelLabs/coach scikit-learn-contrib/category_encoders boto/boto3 flink-extended/ai-flow indico/indico aleju/imgaug lucyparsons/OpenOversight williamfzc/stagesepx dj-stripe/dj-stripe biosustain/potion MLBazaar/BTB mljar/mljar-supervised archesproject/arches uber/causalml digiteinfotech/kairon DeepLabCut/DeepLabCut WeblateOrg/weblate oxan/djangorestframework-dataclasses etsy/boundary-layer grafana/oncall trypromptly/LLMStack weihuayi/fealpy django-cas-ng/django-cas-ng lociii/jukebox LAMDA-NJU/Deep-Forest jazzband/django-simple-history fabfuel/ecs-deploy waterdipai/datachecks pfnet/pfrl bhch/django-jsonform allenai/OLMo AI4Finance-Foundation/ElegantRL someengineering/fixinventory ssube/onnx-web IntelAI/nauta scikit-learn/scikit-learn awslabs/aws-embedded-metrics-python amundsen-io/amundsen DataCanvasIO/DeepTables diyan/pywinrm adamchainz/django-perf-rec ihmeuw-msca/CurveFit google-research/weatherbench2 langroid/langroid jina-ai/jcloud tfeldmann/organize georgia-tech-db/evadb sibson/redbeat bread-and-pepper/django-userena betodealmeida/shillelagh kakaoenterprise/JORLDY openstack/neutron mobiusml/hqq django-json-api/django-rest-framework-json-api nasaharvest/presto locuslab/mpc.pytorch Function validate_output_action_space transform document_collections get_conn _process _generate_intersection_points send_email load_frames _resync_instances parse_request _fit from_json save causalsens request interpolate check_component to_internal_value load authenticate process grad get index fit_transform history_form_view assume_role log select_action render sample_nodes init_before_training parse_args run create_tensorboard fit probe init fit build_session set_and_save fit compute load _get_post_params from_string exec is_due process_request supports sample get_total_reservations_map quantize get_paginated_response add_masked_tokens grad_input arXiv Template PREPRINT Lightning-Universe/lightning-flash openxrlab/xrlocalization bentoml/BentoML bayesiains/nflows open-mmlab/mmcv threat9/routersploit hscspring/hcgf martenlienen/torchode arthurmensch/modl pyg-team/pytorch-frame DjangoGirls/djangogirls DataCanvasIO/Hypernets randovania/randovania materialsproject/fireworks LinkedInAttic/naarad gift-surg/NiftyMIC Project-MONAI/MONAILabel griffithlab/pVACtools Giskard-AI/giskard Zero6992/chatGPT-discord-bot intelligent-machine-learning/dlrover florimondmanca/djangorestframework-api-key GhostManager/Ghostwriter allwefantasy/auto-coder caktus/django-treenav simpeg/simpeg arcee-ai/mergekit alex-petrenko/sample-factory RoboSats/robosats pallets/quart michael-lazar/rtv aurelio-labs/semantic-router drivendataorg/deon element-hq/synapse aquasecurity/kube-hunter CarterBain/AlephNull metauto-ai/GPTSwarm ml6team/fondant pytorchbearer/torchbearer intelowlproject/IntelOwl chainer/chainerrl petuum/adaptdl regel/loudml ansible/ansible transform knn_ratio_match from_yaml_file inverse _resize run train from_k split forward save create format run_task generate read_similarities entropy_3d_volume execute run get_cookie_list _save save_model clean merge_code save eval_deriv _make_schedule _save submit_payout_address _create_request_from_scope get_mimetype from_file read generate_config_section is_aws_pod_v2 simulate optimize_swarm write_dataframe save_checkpoint _subquery_weight_org initialize optimize forecast construct_mapping Table 14: The Repositories and funcitons selected from ComplexCodeEval-Java. Repo apache/tajo spring-projects/spring-batch tencentmusic/supersonic tmobile/pacbot microcks/microcks jtalks-org/jcommune spring-projects/spring-data-redis apache/james-project apache/hop Function findScalarFunctions afterPropertiesSet addAliasToSql listAssets createGenericResourceService showNewQuestionPage executeWithStickyConnection from getXml 23 arXiv Template PREPRINT apache/incubator-dolphinscheduler apache/archiva Alfresco/alfresco-repository 52North/SOS kubernetes-client/java xwiki/xwiki-platform ctripcorp/x-pipe digital-preservation/droid IridiumIdentity/iridium sofastack/sofa-acts ProgrammeVitam/vitam revelc/formatter-maven-plugin Hack23/cia immutables/immutables pentaho/pentaho-platform ORCID/ORCID-Source 88250/latke mybatis/guice GoogleCloudDataproc/spark-bigquery-connector gbif/ipt jhy/jsoup neo4j/neo4j PaladinCloud/CE alibaba/SREWorks jenkinsci/plugin-installation-manager-tool apache/syncope apache/hadoop Qihoo360/Quicksql openlookeng/hetu-core zanata/zanata-platform AutoMQ/automq OctoPerf/kraken metamx/druid kiegroup/optaweb-vehicle-routing oceanbase/odc lennartkoopmann/nzyme Stratio/Decision alibaba/velocity-spring-boot-project Aiven-Open/klaw apache/doris-manager apache/shardingsphere-elasticjob apache/rya ixrjog/opscloud4 google/nomulus koraktor/steam-condenser-java wikimedia/wikidata-query-rdf techa03/goodsKill runelite/runelite jenkinsci/blueocean-plugin MyCATApache/Mycat-Server jenkinsci/gitea-plugin gentics/mesh twilio/twilio-java ppdaicorp/das insideapp-oss/sonar-flutter dschulten/hydra-java alibaba/fastjson2 opencast/opencast expandListParameter commit check init index getFileItems analyze getAvailableSignatureFiles generate parseGenTableDatas switchIndex init unmarshallXml oneLiner startup getWorkInfo resolve get hashCode add submit nodeApplyChanges getAssetLists execute installedPlugins getAdminRealmsFilter checkAllVolumes distinctList updateRows getLocales persistentVersionedKeyValueStore list run startSolver bind recordFrame childEvent getMatchOutcome getConsumerGroupDetails createTable init distinct queryMyWorkRole validateDomainName rconExec load getSeckillList onChatMessage validateAccessTokenScopes formatProperties getFileLink getUid fromHttpRequest checkSql define linkTo of multiTrimConcat arXiv Template PREPRINT spring-projects/spring-data-jpa jline/jline3 star-whale/starwhale javaparser/javaparser datavane/datasophon sakaiproject/sakai alswl/yugong zanata/zanata-server aliyun/aliyun-log-java-producer google/mug apache/druid ExpediaGroup/styx apache/kylin dCache/dcache Asqatasun/Asqatasun mybatis/mybatis-3 apache/poi mitreid-connect/OpenID-Connect-Java-Spring-Server dianping/puma alturkovic/distributed-lock twitter/hraven OpenOLAT/OpenOLAT apache/rocketmq RIPE-NCC/whois odpi/egeria ShifuML/shifu ozimov/spring-boot-email-tools NationalSecurityAgency/datawave spring-projects/spring-data-cassandra opennetworkinglab/onos Graylog2/graylog2-server openmrs/openmrs-core webx/citrus removeSubqueries open list solveSymbolInType syncUserToHosts upgradeRoleString queryAndSaveToQueue parseGlossaryFile tryAppend forDoubles wrap equals encrypt map findByAuditAndUrl register setArrayFormula parse copyFromLocal refresh getAppId isSetOfFlashcardExisting addTransactionSubscription parse buildGlossaryTermContext exec mergeTemplateIntoString from addProperty parse authenticate handle getFastConstructor arXiv Template PREPRINT CGM for Issue Fixing: Case Study In this section, we take real issue from the django/django repository as an example to show how CGM solves specific problem. Figure 4 provides the original issue description and the intermediate outputs produced at each stage of our Graph RAG framework4, and Figure 5 gives the generated patches along with the gold patch. To evaluate the effectiveness of graph modality in assisting solving practical issues, we also compare the patches generated by CGM with and without code graph (as shown in Figure 5). For the latter, the input of CGM is only the context files provided by Reranker, and does not include the subgraph generated by Retriever. Figure 4: The given issue and the intermediate outputs produced by Rewriter, Retriever, and Reranker, respectively. 4The complete outputs are provided in an anonymous online repository due to space constraints: https://anonymous.4open. science/r/CGM-EF59/Supplemental/case_study/case_study.md. 26 arXiv Template PREPRINT Figure 5: Patches generated by CGM (with or without code graph), along with the gold patch. Green boxes represent successful patches and the red box represents unsuccessful one."
        },
        {
            "title": "E Limitations",
            "content": "We have limited this work to Python and Javatwo popular object-oriented languagesso the current code graph schema is untested on other paradigms. Although these languages cover large part of real-world issue-fixing scenarios, extending our framework to other paradigms (e.g., multi-paradigm languages like Rust, or functional languages such as Haskell) will require re-examining how code graphs are built to capture paradigm-specific structures."
        },
        {
            "title": "F Prompt Template Example",
            "content": "This section shows the prompt templates used by Rewriter (Figure 6 and Figure 7) and Reranker (Figure 8 and Figure 9) in our Graph RAG framework. 27 arXiv Template PREPRINT Figure 6: Prompt for Extractor in Rewriter. arXiv Template PREPRINT Figure 7: Prompt for Inferer in Rewriter. 29 arXiv Template PREPRINT Figure 8: Prompt for Reranker in Stage 1. 30 arXiv Template PREPRINT Figure 9: Prompt for Reranker in Stage 2."
        }
    ],
    "affiliations": [
        "Ant Group, Hangzhou, China",
        "Shanghai Jiaotong University, Shanghai, China",
        "ShanghaiTech University, Shanghai, China",
        "Zhejiang University, Hangzhou, China"
    ]
}