{
    "paper_title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
    "authors": [
        "Yifei Liu",
        "Jicheng Wen",
        "Yang Wang",
        "Shengyu Ye",
        "Li Lyna Zhang",
        "Ting Cao",
        "Cheng Li",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B, $4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the quantization algorithm execution time, resulting in a $1.6$-$1.8\\times$ increase in inference throughput compared to SOTA."
        },
        {
            "title": "Start",
            "content": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models Jicheng Wen Yang Wang, Shengyu Ye, Yifei Liu,,* Li Lyna Zhang Ting Cao Cheng Li Mao Yang University of Science and Technology of China Microsoft {v-liuyifei, jicheng.wen, Yang.Wang92, v-shengyuye, lzhani, ting.cao, maoyang}@microsoft.com, chengli7@ustc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Table 1: LLM Quantization Algorithm Comparison. VPTQ balances all dimensions and achieves SOTA. Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce Vector PostTraining Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use SecondOrder Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using ChannelIndependent Second-Order Optimization for granular VQ. In addition, by decomposing the optimization problem, we propose brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.010.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.791.5% on LLaMA-2, 1% on Mistral-7B, 1122% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in 1.6-1.8 increase in inference throughput compared to SOTA. Our code is available at https://github.com/microsoft/VPTQ. VPTQ AQLM QuIP# GPTVQ GPTQ AWQ Effective Bitwidth Accuracy @ Low-bit Quantization Time Cost Inference Throughput"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Touvron et al., 2023; Meta, 2024) have shown excellent performance across various complex tasks as their sizes increase. However, the enormous weight of LLMs poses significant challenges for efficient inference and practical deployment. For instance, storing the LLaMA-2 70B model weights in FP16 format requires 140GB of memory, surpassing the capacity of high-end GPUs and necessitating multi-GPU deployment. This huge size significantly affects memory capacity and hard disk storage and requires substantial bandwidth for inference. Weight-only quantization is mainstream model compression technique that effectively reduces the models size by representing floating-point numbers with fewer bits. In weight-only quantization of LLMs, prominent method is Post-Training Quantization (PTQ). PTQ quantizes model weights directly without retraining the model. Typically, PTQ only involves converting model weights into lower-bit fixed-point numbers. Currently, the main approach in PTQ is scalar quantization, which converts each scalar weight in the model into lower bit value. Recent work (Frantar et al., 2023; Lin et al., 2023; Xiao et al., 2023; Lee et al., 2024; Chee et al., 2023) has achieved near-original model accuracy with 3-4 bit quantization. Table 1 summarizes the char- *Contribution during internship at Microsoft Research Corresponding author This paper is the result of an open-source research project, and the majority work of the project is accomplished in April 2024. 4 2 0 2 2 2 ] A . [ 2 6 6 0 7 1 . 9 0 4 2 : r acteristics of typical scalar quantization methods (GPTQ, AWQ) in LLM. However, due to the limitations of numerical representation, traditional scalarbased weight quantization struggles to achieve extremely low-bit levels. For instance, with 2-bit quantization, we can only use four numerical values to represent model weights, which severely limits the range of weight representation. Although BitNet (Wang et al., 2023; Ma et al., 2024) has enabled quantization-aware training that can quantize weights to below 2 bits during the models pre-training phase, this approach requires substantial GPU cluster resources to maintain reasonable accuracy. Recent studies (van Baalen et al., 2024; Tseng et al., 2024; Egiazarian et al., 2024) have explored an efficient method of weight-only quantization known as Vector Quantization (VQ). VQ is data compression technique that maps high-dimensional vectors to set of predefined lower-dimensional vectors stored in codebooks (lookup tables). During encoding, each data point is represented by the index of corresponding vector in the codebook, and during decoding, the original data is approximated using these indices. This method substantially reduces the storage requirements for data while allowing for the quick reconstruction of original vectors through simple index references. VQ achieves more effective data compression than scalar quantization by leveraging correlations and redundancies across different data dimensions. By detecting and leveraging interdependence, VQ can encode complex multidimensional data with fewer bits, thus achieving higher compression ratios and reduced bit width. While Vector Quantization (VQ) shows promise in extreme low-bit weight compression for Large Language Models (LLMs), it faces several significant challenges. Table 1 compares the strengths and weaknesses of various VQ algorithms in multiple dimensions. The first challenge is ensuring the accuracy after extreme low-bit VQ quantization. Unlike scalar quantization, the quantization granularity of VQ algorithms is vector-based. The quantization may introduce additional accumulation errors due to the simultaneous quantization of multiple numbers. For example, GPTVQ (van Baalen et al., 2024) uses the Second-Order Optimization method to implement PTQ. However, GPTVQ accumulates quantization errors within vector quantization, leading to an inevitable increase in quantization errors as the vector length increases. This prevents the use of longer vectors and, consequently, limits the compression ratio. The second challenge lies in efficiently executing VQ quantization on LLMs. VQ can compress vectors in the weight matrix into indices, but these indices are discrete, non-differentiable integers. This introduces difficulties in implementing VQ quantization methods through model training. For instance, AQLM (Egiazarian et al., 2024) employs beam search and backpropagation to quantize and update centroids in lookup tables. VQ necessitates additional gradient estimation, slowing the convergence of model quantization training and requiring intensive training efforts to achieve better accuracy. The third challenge arises as the dequantization overhead in VQ model inference. To reduce quantization errors, complex data preprocessing methods may be used to process weights. QuIP# (Tseng et al., 2024) introduces incoherence processing using the randomized Hadamard transform for the weight matrix before VQ. These preprocessing steps can reduce quantization errors and improve model accuracy. However, postprocessing must be performed in real time during model inference, which can severely impact throughput in inference. VPTQ seeks to bypass the limitations of current VQ by offering lightweight and efficient approach exclusively for extreme low-bit weight quantization. In this paper, we present Vector Post-Training Quantization (VPTQ), novel approach for extremely low-bit quantization of LLMs. 1. VPTQ achieves SOTA accuracy results on extremely low-bit LLMs. We formulate the quantization problem as an optimization problem and employ Second-Order Optimization to guide our quantization algorithm design. By Channel-Independent Second-Order Optimization, VPTQ reduces model quantization perplexity by 0.01-0.34, 4.41-7.34, 0.38-0.5 on LLaMA-2/3/Mistral-7B, respectively, over SOTA at 2-bit, with an accuracy improvement of 0.79-1.5%,11-22%,1%, on LLaMA2/3/Mistral-7B in QA tasks on average. 2. VPTQ can transform LLMs into extremely low-bit models with minor quantization algorithm overhead. Under the guidance of the optimization problem, we transform the quantization algorithm into heuristic algorithm to solve the optimization problem. We also analyze and propose brief and effective codebook initialization algorithm to reduce the extra overhead of centroid training and updates. Experiments show that VPTQ only requires 10.4-18.6% of the quantization algorithm execution time compared to existing SOTA results. 3. VPTQ has low dequantization overhead. VPTQ algorithm quantizes all the weights in every Linear Operator in the model into an index matrix and codebooks. During model inference, we only need to dequantize the weight matrix by reading centroids from the codebook according to the index before executing the operator. The models quantized by VPTQ result in 1.6-1.8 improvement in inference throughput compared to SOTA."
        },
        {
            "title": "2 Background and Motivation",
            "content": "2.1 Post Training Quantization in LLM Post-Training Quantization (PTQ) (LeCun et al., 1989; Hassibi et al., 1993; Hassibi and Stork, 1992; Frantar et al., 2023; Singh and Alistarh, 2020) aims to decrease model weight size by simplifying the numerical representation and seeking to maintain the models accuracy without retraining the model. We can formulate PTQ as the following optimization problem: WT g(W) + WT H(W) arg min E[L(X, + W) L(X, W)] 1 2 where is the original model weights, ˆW is quantized weights, and = ˆW represents the weight quantization error. The loss of the model task is L. The optimization object is to minimize the impact of model quantization on the model task, which means minimizing the expected deviation of the loss function. Figure 1: Vector Quantization in Weight Quantization Higher-order terms exert minor effect on the optimization goal, and we typically disregard interactions among weights between different layers. Consequently, we can simplify the optimization problem by focusing on optimizing the second-order term and then define the following optimization problem: arg min WT H(W) W, s.t. = 0 (1) The objective of the optimization problem is to minimize the second-order error in model quantization, subject to the constraint that the change in model weights is as minimized as possible, i.e., = 0. 2.2 Vector Quantization in Neural Networks VQ is key method for efficient lossy data compression (Gersho, 1979). Its objective is to reduce the distortion by mapping high-dimensional original data to lower-dimensional space represented by lookup table (Eq. 2). VQ maps original vectors (W) from the vector space to finite set of vectors, which is commonly referred to as codebook (lookup table, C). Each vector in the original space approximates the closest vector (centroid Ci) in the codebook. arg min ik Ci2, W (2) PTQ typically employs concise and accurate method for analyzing the above optimization problem: Second-Order Optimization. Following Taylor series expansion, this method breaks down the optimization goal into first-order, second-order, and higher-order terms. g(W) and H(W) represent the gradient and Hessian of task loss L, respectively. It often assumes that the model has already reached local optimum before model quantization, which means that the first-order term is nearly zero. VQ indicates the nearest centroid Ci that minimizes the Euclidean distance between the input vector in the lookup table. The optimization problem aims to find the index that results in the smallest distance between v. Thus, each input vector is represented by the most similar centroids, thus minimizing total distortion. Recent research has explored the use of VQ for model weight quantization (Chen et al., 2020; Cho et al., 2022; Stock et al., 2020, 2021). These studies attempt to compress the embedding layer, the convolution layer, and the classification layer of neural networks using VQ. Figure 1 illustrates an example of applying VQ to compress model weights on weight matrix. For weight matrix with dimensions , we reshape into vectors of length as (step ➊). The number of reshaped vectors should be . Next, we employ k-means or other clustering algorithms to build codebook (step ➋). The constructed codebook contains centroid vectors, each with dimensions. Applying the VQ algorithm directly often does not yield an acceptable accuracy. Typically, PTQ algorithms adjust the model index and centroid to enhance the accuracy of the quantized model (step ➌). During model inference, each operator in the model first dequantizes the original weight matrix from the lookup table (codebook) by index and centroid. Unlike scalar quantization, VQ keeps the index and centroid in quantized weight. The equivalent compression ratio of VQ can be formulated total original model bits/(codebook bits + as: index bits). The equivalent quantization bitwidth is as: original bit width/compression ratio. For example, 4096 4096 FP16 weight matrix with vectors of length = 8 and 256 centroids, the compression ratio is (16 4096 4096)/(8 256 16 + log2(256) 4096 4096/8) = 15.97. The equivalent bitwidth is 1.002 bit. 2.3 Vector Quantization in LLMs While VQ has been applied to weight quantization, the following significant challenges persist when quantizing LLM. We summarize the benefits and weaknesses of recent research (Egiazarian et al., 2024; Tseng et al., 2024; van Baalen et al., 2024) techniques in Table 1. The number of parameters in LLMs is enormous, which requires quantizing the model using lightweight methods to avoid excessive resource consumption. AQLM (Egiazarian et al., 2024) utilizes gradient descent to train each layer of the VQ-quantized model and simultaneously trains across multiple layers using calibration data. It achieves effective compression through additive quantization and joint optimization of the codebook, which can achieve high accuracy. However, due to AQLMs use of backpropagation for model training, significant GPU hours and memory are required to achieve better accuracy, especially when dealing with LLMs with massive parameters. Algorithm 1 VPTQ Algorithm Input: RM Input: RN Output: ˆW RM RM for = 0, B, 2B, . . . do Input weight matrix Hessian matrix Quantized weight matrix Initialize quantization errors Column blocks for = s, + 1, . . . , + 1 do Quantize single column n, fundamentally different from AQLM for = 0, V, 2V, . . . , do Parallel (Residual) Vector Quantization by function Q(v) to vectors in the column ˆWm:m+V,n QV (Wm:m+V,n) end for E:,n (W:,n :,n)/(H1 Update quantization error n,n) W:,n:s+B W:,n:s+B E:,nH1 n,n:s+B Merge quantization error to weights end for W:,s+B: W:,s+B: E:,s:s+BH1 Update all remaining weights s:s+B,s+B: end for GPTVQ (van Baalen et al., 2024) utilizes the Second-Order Optimization method to implement PTQ. However, GPTVQ accumulates quantization errors within vector quantization, leading to an inevitable increase in quantization errors as the vector length increases. It prevents the use of longer vectors and consequently limits the compression ratio. QuIP# (Tseng et al., 2024) introduces an incoherence processing using the randomized Hadamard transform for the weight matrix before VQ. The processed weight matrix approximates subGaussian distribution, allowing for compression with tiny codebook. However, incoherence processing requires significant amount of computation, despite QuIP# being able to compress LLM to extremely low-bit with low accuracy drop. It requires significantly more computation for inference compared to the original LLM, resulting in low inference throughput."
        },
        {
            "title": "3 Vector Post-Training Quantization\n3.1 VPTQ Algorithm\nVPTQ leverages Second-Order Optimization and\nsolves the optimization problem Eq.1 to achieve ex-\ntreme low-bit quantization. Assume that a weight\nmatrix is W ∈ RM ×N , and a Hessian matrix col-\nlected from the current layer is H ∈ RN ×N . We\ndenote the q-th column of the weight matrix as\nˆW:,q. The quantized column ˆW:,q can be repre-\nsented as the transpose of concatenated centroid\nvectors",
            "content": "ˆW:,q = (C0, C1, ..., CM/v)T . When the weight matrix of the model is large, we can first split the weight matrix into multiple groups. Each group has its own independent codebook. This method allows us to flexibly divide the weight matrix into several submatrices ( ˆW:,q:q+(M/group num)) equal to the group number. For clarity, we describe only one group in the following algorithm description. Unlike GPTVQ, we quantize each column of the matrix independently, which we refer to as Channel-Independent Second-Order Optimization. It greatly simplifies the complexity of VQ in Second-Order Optimization. GPTVQ, on the other hand, quantizes columns of the matrix ( ˆWM,v) at once, leading to larger errors and more complex transformations for problem optimization. We use the Lagrange Method to transform the optimization problem 1 into an unconstrained optimization problem. The Lagrangian function L(W), and λ is the Lagrangian multiplier: L(W) = WT H(W)W + λW The dual function g(λ) can be represented as: g(λ) = H1 qq λλT λ( ˆW:,q W:,q) Differentiating g(λ) with respect to λ and setting it to 0, g(λ) = H1 qq λ ( ˆW:,q W:,q)T = 0 we can find that when λT = ( ˆW:,qW:,q) problem reaches an optimal solution. H1 qq , the By substituting λT into the optimization problem, we find that to minimize the error introduced by quantization, we need to minimize the impact on the Lagrangian function. Therefore, we can transform the quantization problem into minimizing: L( ˆW) = (cid:80) C2 2H1 qq We find that when quantizing column vector each time, we only need to consider minimizing (cid:80) C2, which is to find the closest centroid in Euclidean Distance. It precisely aligns with the optimization of VQ. Moreover, since VPTQ quantizes the weight matrix column by column, H1 qq is constant when quantizing each column, so we do not need to consider Hessian when finding the centroid. After quantizing column of the weight matrix, we need to update the current quantization error to the unquantized part through: = ( ˆW:,q W:,q) H1 qq Hq,: It will transform current quantization errors to the following unquantized columns. Since GPTVQ quantizes columns at the same time, quantization error can only spread to other unquantized columns when all columns have been quantized. It will lead to more errors accumulating in the quantization, resulting in decrease in model accuracy. We can have similar conclusions from Table 2. Algorithm 1 provides detailed description of the steps to solve the optimization problem and quantize the weights according to the above analysis. Distinguish VPTQ from GPTQ and GPTVQ: Compared with GPTQ, VPTQ employs vector representations in the quantization, which choose the vector closest to the original matrix to represent the original data. As VQ can use larger codebook to store the quantized data, it covers wider range of numerical distributions compared to the scalar quantization of GPTQ, thereby achieving better accuracy. Table 2 reveals that VPTQ significantly outperforms GPTQ under extremely low bit quantization. Moreover, since GPTVQ quantizes multiple columns simultaneously, the propagation of quantization errors to unquantized columns is more challenging. Furthermore, the quantization errors in GPTVQ accumulate as the vector length increases, hindering GPTVQ from using longer vector lengths for weight compression (limited to only 1-4 bits). It significantly reduces the compression ratio of VQ. On the other hand, VPTQ is capable of compressing weights using longer vectors (> 8 bits) and representing data with larger codebook. Table 2 shows the better accuracy achieved by VPTQ than GPTVQ. 3.2 Optimization in VPTQ 3.2.1 Hessian-Weighted Centroid Initialization VTPQ algorithm requires the initialization of centroids in the codebooks prior to quantization. Properly initializing centroids can reduce quantization errors and improve model accuracy. straightforward method is to perform K-means clustering on the weight matrix as centroids (Eq.2). However, it does not consider the optimization object in Eq.1, leading to significant accuracy drop (van Baalen et al., 2024; Egiazarian et al., 2024). We can transform the optimization object by leveraging the cyclic property of matrix traces and the Hadamard product. We refine the optimization objective as: WT = n1 (cid:88) i=0 hi,iW:,i2 n1 (cid:88) n1 (cid:88) + i=0 j=0,j=i hi,j(W:,iW:,j) Due to the Hessian matrix being predominantly diagonal (Dong et al., 2020), it guides us to split the proxy error into two terms. The first term represents the dominant diagonal elements of the initial error matrix, which significantly impact the quantization error. The second term is the interaction of single value in weight quantization with others. Because the Hessian matrix is predominantly diagonal, we can prioritize optimizing the first term through centroid initialization. We can view the first term as Weighted K-means Clustering problem (Cordeiro de Amorim and Mirkin, 2012; Kerdprasop et al., 2005; Liu et al., 2017). Since this problem is well-studied, we can directly solve it to achieve efficient and accurate centroid initialization. 3.2.2 Residual Vector Quantization We enable Residual Vector Quantization (RVQ) (Barnes et al., 1996; Wei et al., 2014) in VPTQ. RVQ improves vector quantization (VQ) by breaking down the compression of weight matrix into two (or more) stages. Each stage further compresses the residual error vres = Q(v) from the previous quantization stage: Q(vres) = arg min (v Q(v)) Cres Unlike GPTVQ, VPTQ enables RVQ, which quantizes VQ quantization error using separate lookup table for better representation and quantization. By partitioning the encoding into multiple stages and reducing quantization error, RVQ not only achieves superior compression efficiency but also ensures balance between quantization error, the size of lookup tables, and the memory requirements for indices. During the decoding phase, VPTQ simply reads the centroids from these multiple lookup tables and combines them to reconstruct the original weight matrix. Algorithm 2 End to End Quantization Algorithm Require: original model, vector length v, centroid number k, hessian matrices Ensure: quantized model for each layer do Fully parallelized each layer on GPUs for each Linear operator do if outlier is enabled then outlier VPTQ(Woutlier, Coutlier) Initialize outlier centroids Coutlier end if Initialize centroids VPTQ(W, C) if residual is enabled then Initialize residual centroids Cres VPTQ(W W, Cres) end if end for if finetune layer is enabled then Finetune layer end if end for 3.2.3 Outlier Elimination Recent studies on quantization in LLM have consistently observed significant presence of outliers in activation (Xiao et al., 2023; Lin et al., 2023; Lee et al., 2024). Outliers, while small portions (1% of the matrix), heavily affect the quantization error and simulate model accuracy. Outliers typically result in large values in the diagonal elements of the Hessian matrix. During centroid initialization in Sec.3.2.1, VPTQ already considers these Hessian diagonals as weights in K-means, allowing VPTQ to better quantize the error introduced by outliers. Q(voutlier) = arg min voutlier Coutlier 2 Furthermore, VPTQ flexibly partitions the weight matrix and uses separate outlier lookup table to quantify matrix tiles most affected by outliers. It allows us to effectively trade off model accuracy and quantization overhead."
        },
        {
            "title": "4 End to end Quantization Algorithm\nIn this section, we will detail the end-to-end model\nquantization algorithm (Algorithm 2). The algo-\nrithm takes the original model, vector length v,\ncentroid number k, and Hessian matrices H as in-\nputs. It starts by iterating over each layer l of the\nmodel. As each layer’s quantization only relates to\nthe current layer and the Hessian matrix, we can\nfully parallelize the quantization of each layer on\nGPUs.",
            "content": "In each layer, we first quantize the weight of each Linear Operator (matrix multiplication of input and weight). If we enable the outlier option, the algorithm first selects outlier columns following Section 3.2 and initializes the outlier centroids Coutlier. Then, VPTQ is applied to the outlier weights Woutlier using the outlier centroids, generating the quantized weights outlier. Next, the algorithm initializes the centroids for the remaining columns and applies VPTQ to the weights using these centroids to produce the quantized weights W. Lastly, if residual quantization is enabled, the algorithm initializes the residual centroids Cres. It applies VPTQ to the residual error between the original weights and the quantized weights (W W), using the residual centroids. The quantized weight is updated as W. After processing all the operators, the algorithm will fine-tune the layer if we enable layer finetuning. The loss function is the Mean Squared Error (MSE) between the original and quantized computations. In layer-wise fine-tuning, we only update the normalization operator (e.g. RMSNorm) and centroid. These parameters only comprise small fraction of the entire layer, and we can complete the fine-tuning quickly with limited memory. After each layer completes quantization and finetuning, we can further fine-tune the entire model as other PTQ methods used (Tseng et al., 2024; Chee et al., 2023; Egiazarian et al., 2024). Once the algorithm processes all layers, it outputs the quantized model. The end-to-end VPTQ algorithm quantizes all the weights in every Linear Operator in the model into an index and codebook (C). During model inference, we only need to dequantize the weight matrix by reading centroids from the codebook according to the index before executing the operator."
        },
        {
            "title": "5 Experiments and Evaluations",
            "content": "5.1 Settings Algorithm Baseline We focus on weight-only quantization. The detailed quantization parameters (such as vector length and codebook numbers) and fine-tuning parameters of our VPTQ are shown in Appendix . Following (Frantar et al., 2023), our calibration data consists of 128 random segments of the C4 dataset (Raffel et al., 2020). Models and Datasets We benchmark accuracy on LLaMA-2 (Touvron et al., 2023), LLaMA-3 families (Meta, 2024), and Mistral (Jiang et al., 2023). Following previous work (Frantar et al., 2023), we report perplexity on language modeling tasks (WikiText-2 (Merity et al., 2016), C4 (Raffel et al., 2020)). We also employ lm-eval-harness (Gao et al., 2021) to perform zero-shot evaluations on common sense QA benchmarks (PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018)). Detailed configuration is in Appendix A. Baselines For LLaMA-2 and Mistral models, we compare VPTQ against GPTQ, GPTVQ, DBLLM, QuIP#, and AQLM. To account for the different overheads resulting from varying codebook constructions, we provide results with comparable bit widths to facilitate fair comparison. For LLaMA-3 models, we use the results of (Huang et al., 2024). However, due to alignment issues with the C4 dataset, we only show results for WikiText and QA tasks. Because LLaMA-3 models are new and running quantization ourselves is costly, we do not have results for QuIP# and AQLM. 5.2 Accuracy Evaluation Results on LLaMA-2 model: We compare VPTQ with QuIP#, AQLM, GPTVQ, DB-LLM, and GPTQ on the LLaMA-2 model. First, we discuss the results of 2-bit quantization. As shown in Table 2, GPTQ, as scalar quantization method, performs poorly with unusable accuracy. While DB-LLM and GPTVQ perform better, they still experience significant performance drops, with WikiText-2 perplexity increasing by 2. The significant accuracy drop in GPTVQ, despite being vector quantization algorithm, is due to two factors: the use of shorter vector lengths, which introduces higher quantization loss, and the choice to update weights every columns, which leads to cumulative errors. Therefore, we primarily focus on comparing VPTQ with the state-of-the-art QuIP# and AQLM which both choose longer vector lengths. Table 2 includes the average scores for the five QA tasks mentioned in Section 5.1. VPTQ outperforms QuIP# and AQLM on 7B and 13B models. For the 7B model, VPTQ achieves further reduction in WikiText-2 perplexity by 0.5 and 0.3 compared to the previous best results at 2-2.02 bits and 2.26-2.29 bits, respectively. In QA tasks, the VPTQ 2.26-bit model surpasses the AQLM 2.29bit model with an average accuracy increase of 1%. For the 13B model, the VPTQ 2.02-bit model shows slight improvement over QuIP#, and the 2.18-bit model outperforms AQLM in QA accuracy by 1.5%. On the LLaMA-2-70B model, we achieve similar perplexity (< 0.02) and comparable QA results(< 0.4%). The results for 3and 4-bit quanTable 2: LLaMA-2 2bit Quantization Results. The \"N/A\" in the table stands for \"not available,\" with further 1 We use the naive Torch and Triton kernels for inference performance explanation provided in the Appendix A.1. evaluation, without optimizations like CUDA graphs, FlashAttention, or Torch compile. The inference performance for QuIP# and AQLM do not represent their performance with all optimizations enabled. QuIP# and AQLM can achieve high performance when all optimizations are enabled. bit W2 Method FP16 5.12 16 50.75 GPTQ 2.125 6.71 2.25 GPTVQ 7.23 DB-LLM 2.01 QuIP#1 6.19 AQLM 1 6.64 AQLM 1 6.29 6.13 VPTQ 5.95 2 2.02 2.29 2.02 2.26 (a) 7B results C4 AvgQA 6.63 36.76 9.9 9.62 8.16 8.56 8.11 8.07 7. 62.2 39.16 56.14 55.1 58.2 56.5 58.6 58.2 59.4 (b) 13B results tok/s mem(GB) 38.32 19.59 N/A N/A 4.4 19.4 19.6 39.9 35.7 27.22 4.42 N/A N/A 2.25 2.16 2.4 2.28 2.48 bit W2 Method 4.57 16 FP16 43.84 2.125 GPTQ 5.72 GPTVQ 2.25 DB-LLM 2.01 6.19 QuIP#1 5.35 AQLM 1 5.65 AQLM 1 5.41 5.32 VPTQ 5.28 2 1.97 2.18 2.02 2. C4 AvgQA 6.05 23.07 8.43 8.38 7.2 7.51 7.2 7.15 7.04 65.4 43.72 61.56 59.4 62.0 60.6 61.6 62.4 63.1 tok/s mem(GB) 30.03 11.56 N/A N/A 3.5 N/A 16.5 26.9 18.5 63.63 7.92 N/A N/A 3.94 N/A 4.14 4.03 4.31 cost(h) N/A 0.2 1.5 N/A N/A N/A 11.07 2 2.2 cost(h) N/A 0.3 3.7 N/A N/A N/A 22.7 3.2 (c) 70B results bit W2 C4 AvgQA Method 4.97 16 3.12 FP16 2.125 NaN NaN GPTQ 6.9 4.25 GPTVQ 2.25 6.77 4.64 DB-LLM 2.01 QuIP#1 5.71 3.91 AQLM 1 5.72 3.94 5.72 3.93 VPTQ 5.71 3.92 VPTQ 70.2 59.18 68.5 65.8 69.0 68.8 68.6 68.7 2 2.07 2.07 2.11 tok/s multi-gpu 2.38 N/A N/A 1.9 6.9 9.7 9.7 mem(GB) N/A 37.63 N/A N/A 18.36 18.81 19.54 20. cost(h) N/A 2.83 12 N/A 25 183 19 19 Table 3: LLaMA-3 and Mistra-7b 2,3,4-bit Quantization Results. The table shows LLaMA-3 Wikitext2 perplexity (context length 2048) and average zero-shot QA Accuracy, Mistral-7B Wikitext2, C4 perplexity (context length 8192) and average zero-shot QA accuracy. Detailed score for each task see Table 6 and Table 7. LLaMA-3 8B LLaMA-3 70B Mistral 7B bit W2 AvgQA bit W2 AvgQA bit W2 C4 AvgQA FP16 QuIP GPTQ VPTQ QuIP GPTQ VPTQ QuIP 16 4 4 4.03 3 3 3.03 2 DB-LLM 2 2 2.08 2. GPTQ VPTQ VPTQ 6.14 6.5 6.5 6.42 7.5 8.2 6.97 85.1 13.6 2.10E+02 9.29 9.19 68.6 67.1 67.3 68.1 63.7 61.7 66.7 36.8 51.7 36.2 60.2 62.7 16 4 4 4.05 3 3 3.01 2 2.9 3.4 3.3 3.15 4.7 5.2 3.81 13 N/A N/A 11.9 5.6 5.66 2 2.02 2. 75.3 74.5 74.9 74.7 72.6 70.6 73.7 48.7 N/A 45.4 70.9 70.7 16.0 FP16 4.01 QuIP# AQLM 4.02 4.125 GPTQ 4.03 VPTQ 3.0 AQLM 3.03 VPTQ QuIP# 2.01 AQLM 2.01 GPTQ 2.125 GPTVQ 2.25 2.04 VPTQ 4.77 4.85 4.85 4.83 4.81 5.07 4.96 6.02 6.32 1535 8.99 5.64 5.71 5.79 5.79 5.74 5.72 5.97 5.84 6.84 6.93 164 18.6 6.43 68.6 68.7 68.0 68.4 68.2 67.3 67.3 62.2 62.2 44.5 57.7 63.2 tization shown in Table 5 are without end-to-end fine-tuning but are also comparable to AQLM and QuIP# which include end-to-end fine-tuning. The ablation study of quantization parameters is in Appendix C. Results on LLaMA-3 and Mistral model: Table 3 presents VPTQ results on the LLaMA-3 model and Mistral-7b model. In all 2-, 3-, and 4-bit quantizations of LLaMA-3 models, we significantly outperform GPTQ, DB-LLM, and QuIP, whose accuracy drops to unusable levels. VPTQ ensures an accuracy drop of < 8% for the 8B model and < 5% for the 70B model. On the Mistral7B model, our 2-bit performance surpasses both QuIP# and AQLM by 0.8% in QA accuracy. In 3-bit quantization, our perplexity is lower. At 4bit, results are comparable overall. More detailed results are in Table 7. As bit width increases, the advantage of vector quantization diminishes, with GPTQ showing similar WikiText-2 perplexity at 4-bit. Inference throughput and quantization cost: In Table 2, the tok/s column indicates the number of tokens generated per second during the decode phase of inference. VPTQ achieves 2-9 speedup compared to QuIP# because QuIP# uses Hadamard Transform during decoding, which introduces O(n2) multiplications and additions, significantly slowing the inference throughput. Compared to AQLM, VPTQ uses smaller codebook, resulting in lower decoding overhead. Therefore, our inference throughput for the 7B and 13B models is 1.6-1.8 faster than AQLM. As the model size increases, our codebook size becomes comparable to theirs, leading to similar inference throughputs for the 70B model. The mem(GB) column represents the GPU memory usage at runtime. The cost(h) column represents the hours required for model quantization on 4 80GB A100 GPUs. We achieves comparable or even better results than AQLM in only 10.4-18.6% of quantization algorithm execution time."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose Vector Post-Training Quantization (VPTQ), novel approach to achieving extremely low-bit quantization of LLMs by Vector Quantization. Through the application of Second-Order Optimization, we have formulated the LLM Vector Quantization problem and directed the design of our quantization algorithm. By further refining the weights via Channel-Independent Second-Order Optimization, we have enabled more granular VQ. VPTQ also includes brief and effective codebook initialization algorithm, which is achieved by decomposing the optimization problem. We have extended VPTQ to support residual and outlier quantization, which not only improves model accuracy but also further compresses the model size. Our experimental results demonstrate the effectiveness and efficiency of VPTQ. The perplexity of quantized model is reduced by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks. Furthermore, we achieved these results only using 10.4-18.6% of the execution time of the quantization algorithm, leading to 1.6-1.8 increase in inference throughput compared to SOTA. These results underscore the potential of VPTQ as an efficient and powerful solution for the deployment and inference of LLMs, particularly in resourceconstrained settings."
        },
        {
            "title": "7 Limitations",
            "content": "Related research on PTQ (Egiazarian et al., 2024; Tseng et al., 2024; van Baalen et al., 2024) have adopted end-to-end model fine-tuning after the PTQ phase. Compared to other related works, VPTQ can better quantize the model in the PTQ, and it simplifies and reduces the cost and overhead of model fine-tuning. Due to GPU resource constraints, we cannot finetune larger models (70B) for longer iterations and It limits our experimental results, more tokens. which can only achieve similar results to baselines in 70B models. It restricts the demonstration of VPTQs advantages and potential on large models in this paper. We will strive for more GPU resources to fine-tune the VPTQ model for longer periods and with more tokens in the future, allowing for fair comparison. Additionally, since LLaMA-3 models are the latest released models, there is lack of baselines from related works. It is difficult for us to fully demonstrate our performance improvements. We will continue to add more baselines in the future to highlight the advantages of VPTQ. In this paper, we only use AI tools for grammar checking and code completion. Acknowledgement We thank James Hensman for his crucial insights into the error analysis related to Vector Quantization (VQ), and his comments on LLMs evaluation are invaluable to this research. We also thank QuIP# and AQLM for inspiring our paper and the authors for their guidance on implementation."
        },
        {
            "title": "References",
            "content": "C.F. Barnes, S.A. Rizvi, and N.M. Nasrabadi. 1996. Advances in residual vector quantization: review. IEEE Transactions on Image Processing, 5(2):226 262. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. 2023. Quip: 2-bit quantization of large language models with guarantees. Ting Chen, Lala Li, and Yizhou Sun. 2020. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 16171626. PMLR. Minsik Cho, Keivan Alizadeh-Vahid, Saurabh Adya, and Mohammad Rastegari. 2022. DKM: differentiable k-means clustering layer for neural network compression. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Renato Cordeiro de Amorim and Boris Mirkin. 2012. Minkowski metric, feature weighting and anomalous cluster initializing in k-means clustering. Pattern Recognition, 45(3):10611075. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. HAWQ-V2: hessian aware trace-weighted quantization of neural networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large language models via additive quantization. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. OPTQ: accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. 2021. framework for few-shot language model evaluation. Version v0. 0.1. Sept. A. Gersho. 1979. Asymptotically optimal block quantization. IEEE Transactions on Information Theory, 25(4):373380. Babak Hassibi and David G. Stork. 1992. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992], pages 164 171. Morgan Kaufmann. Babak Hassibi, David G. Stork, and Gregory J. Wolff. 1993. Optimal brain surgeon and general network In Proceedings of International Conferpruning. ence on Neural Networks (ICNN88), San Francisco, CA, USA, March 28 - April 1, 1993, pages 293299. IEEE. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. 2024. How good are low-bit quantized llama3 models? an empirical study. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Kittisak Kerdprasop, Nittaya Kerdprasop, and Pairote Sattayatham. 2005. Weighted k-means for densityIn International conference on biased clustering. data warehousing and knowledge discovery, pages 488497. Springer. Yann LeCun, John S. Denker, and Sara A. Solla. 1989. Optimal brain damage. In Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 598605. Morgan Kaufmann. Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. 2024. OWQ: outlier-aware weight quantization for efficient fine-tuning and inference of large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1335513364. AAAI Press. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: activationaware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978. Hongfu Liu, Junjie Wu, Tongliang Liu, Dacheng Tao, and Yun Fu. 2017. Spectral ensemble clustering via weighted k-means: Theoretical and practical evidence. IEEE Transactions on Knowledge and Data Engineering, 29(5):11291143. Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. CoRR, abs/2402.17764. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843. AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Sidak Pal Singh and Dan Alistarh. 2020. Woodfisher: Efficient second-order approximation for neural netIn Advances in Neural Inforwork compression. mation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, and Armand Joulin. 2021. Training with quantization noise for extreme model compression. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham, and Hervé Jégou. 2020. And the bit goes down: Revisiting the quantization of neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. 2024. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough. 2024. Gptvq: The blessing of dimensionality for llm quantization. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. CoRR, abs/2310.11453. Benchang Wei, Tao Guan, and Junqing Yu. 2014. Projected residual vector quantization for ann search. IEEE MultiMedia, 21(3):4151. Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3808738099. PMLR. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Appendix: All Experiments Results A.1 Supplementary Explanation for Main Results Table 2 Table 2 shows our main results. Here we provide an explanation for the N/A entries relative to other works. DB-LLM Since they did not open source their code, we use the AvgQA results from their paper. However, this number does not align with our FP16 results. GPTQ We reproduce the 2-bit results using the official GPTQ repository. As GPTQ quantizes each layer in sequential order, the cost(h) represents the time taken to quantize on single A100 GPU. GPTVQ They do not release their 2-bit quantized model. We reproduce Llama-2, LLama-3 7B and 13B, Mistral 7b 2-bit results using their released GPTVQ code, which only supports singleGPU execution. Therefore, the quantization cost reflects the execution time for quantization on single A100 GPU. Due to the lack of specific logic for loading their quantizers in the released code, we were unable to measure the throughput and runtime memory. AQLM Their 1.97-bit LLaMA-2 13b model has not been open-sourced, so we are unable to measure its inference throughput and runtime memory. QuIP# Due to recent changes in the libraries they rely on, the quantization cost is not measured. The quantization time for the 70B model is estimated based on their original paper. A.2 All Experimental Results In this section, we present all our experimental results, including the perplexity of the quantized model on different context lengths in two datasets, Wikitext2 and C4, and the accuracy on five Commonsense QA tasks (abbreviated as AE for Arc_easy, AC for Arc_challenge, HE for Hellaswag, QA for PIQA, and WI for Winogrande). Table 4 displays all results of LLaMA-2 at 2-bit quantization. Table 5 presents results of LLaMA-2 at 3 and 4 bits quantization. Table 6 displays all results of Llama3 at 2, 3, and 4-bit quantization. Table 7 shows all results of Mistral 7b at 2, 3, and 4-bit quantization."
        },
        {
            "title": "Parameter Settings",
            "content": "Quantization configuration The quantization parameters of all VPTQ 2bit models are shown in Table 8. Layer-wise fine-tuning parameters Layer-wise finetuning trains centroids and layer norm using the input and output of each layer when entering 128 samples of C4 training sets into the full precision model. We train each layer for 100 iterations. Table 9 shows the learning rate and batch size used for each model."
        },
        {
            "title": "C Ablation Study",
            "content": "Table 10 shows results from LLaMA-2 13b on Wikitext2 and C4 (sequence length = 4096) under different quantization parameters. The impact of techniques such as vector length, channelindependent optimization, residual vector quantization, outlier elimination, layer-wise fine-tuning, and end-to-end fine-tuning on quantization results will be discussed. C.1 Parameter Description When performing N% outlier elimination, N% of outliers will be quantized using codebook with vector length of v0 and k0 centroids. For the remaining (100-N)% parameters, the vector length is v1. k1 represents the number of centroids in the first codebook, while k2 represents the number of centroids in the second codebook for residual vector quantization. k2 = 1 indicates no residual vector quantization. C.2 Vector Length and Residual Vector Quantization Compression Ratio Calculation The average bitwidth per element of the index matrix obtained through vector quantization is: Average index bitwidth = log2(k1) v1 + log2(k2) v1 The compression ratio is calculated by: Compression ratio = Total original model bits Codebook bits + Index bits For an original linear weight matrix with parameters, Codebook bits = (v0 k0 + v1 (k1 + k2)) 16 Index bits = % log (100 )% (cid:19) (cid:18) k0 v0 (cid:20) log2(k1) v1 + (cid:21) + log2(k2) v1 The total bitwidth in the table is calculated per transformer block, which for LLaMA-2 includes 4 attention linear and 3 FFN linear layers. Impact of Vector Length First, we discuss the impact of vector length on accuracy. In Table 10 rows #2, #3, #4, and #6 show results for v1 = 2, 4, 6, 8, keeping the average index bit at 2 (i.e., log2(k1/v1) = 2). As v1 increases, the perplexity on Wikitext2 and C4 decreases, but the codebook size also increases exponentially. For v1 = 8 and k1 = 65536, the codebook overhead introduces an additional 0.19 bits. Then, we evaluate the model inference throughput in Table 11. Since we employ weight-only quantization, the main additional overhead of quantized model inference comes from the lookup table for model weights. Table 11 shows models with 2 bits on various throughputs. As the vector length increases (from 2 to 6), the granularity of memory access for reading the lookup table in dequantization increases, which allows memory access to match the GPUs cache line (128 bytes @ L1). This reduces Table 4: LLaMA-2 2bit Quantization Results, 1 We use the naive Torch and Triton kernels for inference performance evaluation, without optimizations like CUDA graphs, FlashAttention, or Torch compile. The inference performance for QuIP# and AQLM do not represent their performance with all optimizations enabled. QuIP# and AQLM can achieve high performance when all optimizations are enabled. VPTQ bit 7B 16 FP16 2.125 GPTQ GPTVQ 2.25 DB-LLM 2.01 QuIP#1 AQLM 2 2.02 2.29 2.02 2.26 bit 13b 16 FP16 2.125 GPTQ GPTVQ 2.25 DB-LLM 2.01 QuIP#1 AQLM1 W2 5.12 50.75 6.71 7.23 6.19 6.64 6.29 6.13 5.95 W2 4.57 43.84 5.72 6.19 5.35 5.65 5.41 5.32 5.28 W2 70b 3.12 FP16 GPTQ 2.125 NaN 4.25 GPTVQ 2.25 4.64 DB-LLM 2.01 QuIP#1 3.91 AQLM1 3.94 3.93 VPTQ 3.92 2 1.97 2.18 2.02 2.18 bit 16 2 2.07 2.07 2.11 VPTQ C4 6.63 36.76 9.9 9.62 8.16 8.56 8.11 8.07 7.87 C4 6.05 23.07 8.43 8.38 7.2 7.51 7.2 7.15 7.04 C4 4.97 NaN 6.9 6.77 5.71 5.72 5.72 5. AC 39.93 20.9 31.2 33.53 34.6 33.28 34.9 35.24 36.43 AC 45.56 23.3 38.7 38.14 39.5 37.8 39.42 40.02 40.96 AC 51.11 35.8 49.4 44.45 48.7 47.93 47.7 48.29 AE 69.28 34.9 66.3 45.2 64.6 61.87 66.5 63.8 64.9 AE 73.23 43.3 73.6 51.64 69.3 69.78 69.15 71.55 71.8 AE 77.74 67 80.47 55.93 77.3 77.68 77.1 77.77 HE 56.69 30.5 46.4 61.98 51.91 49.49 50.88 52.08 52.87 HE 59.71 36 51.6 68.04 56.01 53.74 54.68 56.18 56.89 HE 63.97 51.8 58.26 76.16 62.49 61.79 62.98 62.51 QA 78.35 57.2 72.4 73.18 75.1 73.56 74.92 75.19 76.17 QA 78.73 61.3 75.4 75.14 77.3 76.22 76.22 77.26 77.48 QA 81.12 74.6 79.4 79.27 80.3 80.43 80.3 79.82 tok/s 38.32 19.59 N/A N/A 4.4 19.4 19.6 39.9 35.7 tok/s 30.03 11.56 N/A N/A 3.5 N/A 16.5 26.9 18.5 tok/s WI 66.93 52.3 64.4 61.72 64.9 64.17 65.67 64.33 66.46 WI 69.69 54.7 68.5 64.09 67.7 65.43 68.43 66.85 68.43 WI 77.11 multi-gpu 66.7 75.2 73.32 75.9 75.93 74.98 75. 2.38 N/A N/A 1.9 6.9 9.7 9.7 mem(GB) 27.22 4.42 N/A N/A 2.25 2.16 2.4 2.28 2.48 mem(GB) 63.63 7.92 N/A N/A 3.94 N/A 4.14 4.03 4.31 mem(GB) N/A 37.63 N/A N/A 18.36 18.81 19.54 20.01 cost(h) N/A 0.2 1.5 N/A N/A N/A 11.07 2 2.2 cost(h) N/A 0.3 3.7 N/A N/A N/A 22.7 3.2 4 cost(h) N/A 2.83 12 N/A 25 183 19 19 memory access transactions and decreases cache misses. As the vector length further increases (from 8 to 12) along with the size and levels of the codebook, the codebook size further increases, which results in the codebook not fitting in the L1 cache, thereby reducing the models inference speed. Additionally, we find that reasonable setting (e.g., = 6, = 4096) can achieve throughput similar to the original model for the quantized model, demonstrating the efficiency of the VPTQ design. Residual Vector Quantization Without any fine-tuning, rows #4 and #7 show similar perplexities for v1 = 6, k1 = 4096 and v1 = 12, k1 = k2 = 4096 , with the latter even higher. However, after layer-wise fine-tuning, comparing rows #11 and #13, residual vector quantization (RVQ) reduces the perplexity by 0.3 compared to vector quantization (VQ) due to the increased number of finetunable centroids, showing significant improvement. #5 without it, indicating that channel-independent second-order optimization effectively mitigates quantization error accumulation. C.4 Outlier Elimination Rows #4, #8, #9, and #10 represent the results for eliminating 0%, 1%, 2%, and 5% outliers, respectively. We used codebook with v0 = 4 and k0 = 4096 to quantize N% of outliers, achieving an effective average index bit of 3 bits, while other parameters were 2 bits. Higher N% means more parameters are quantized with 3 bits, leading to larger total bitwidth and lower perplexity. C.5 Fine-tuning Rows #4, #11, and #12 show results without any fine-tuning, with layer-wise fine-tuning, and with end-to-end fine-tuning, respectively. Adding finetuning reduced the perplexity on Wikitext2 from 6.29 to 6.07 and further to 5.32. C.3 Channel-Independent Optimization C.6 Group Number Row #4 with channel-independent optimization shows perplexity decrease of 1 compared to row Rows #14, #15, #16, and #17 show the quantization results when 99% of parameters are divided into 1, Table 5: LLaMA-2 3, 4-bit Quantization Results. The table shows Witext2, C4 perplexity (context length 2048 and 4096) and zeroshot QA Accuracy. 7B bit W2(2k) C4(2k) W2(4k) C4(4k) AC AE HE QA WI 4 4 GPTQ GPTVQ 4.125 QuIP# AQLM 4.04 4.01 VPTQ GPTQ 3 GPTVQ 3.125 QuIP# AQLM 3.04 3.02 VPTQ 3 5.68 5.56 5.64 5.83 5.79 5.82 7.25 7.07 7.13 7.51 7.32 7.33 5.49 5.27 5.19 5.21 5.26 8.06 5.44 5.41 5.46 5.43 7.2 6.88 6.75 6.75 6.8 10.61 7.24 7.04 7.08 7.04 36.8 42.83 40.5 41.0 39.7 31.1 39.93 39.2 38.4 39. 76.6 55.4 66.2 75.17 56.41 77.37 69.1 78.4 78.2 56.0 70.2 78.1 56.0 69.0 71.5 45.2 58.5 74.07 76.17 54.21 68.4 77.3 76.9 54.1 68.1 77.3 54.9 69.1 68.2 69.61 67.6 67.3 67.1 59.2 69.06 66.5 66.9 68.0 13B bit W2(2k) C4(2k) W2(4k) C4(4k) AC AE HE QA WI 4 4 GPTQ GPTVQ 4.125 QuIP# AQLM 3.94 4.02 VPTQ GPTQ 3 GPTVQ 3.125 QuIP# AQLM 3.03 3.03 VPTQ 3 5.68 4.95 4.96 5.11 5.1 5.12 7.25 6.54 6.54 6.83 6.72 6.7 4.78 5.27 4.63 4.65 4.64 5.85 4.8 4.78 4.82 4.79 6.34 6.88 6.13 6.14 6.13 7.86 6.47 6.35 6.37 6.32 42.49 42.83 45.50 44.80 44.37 38.48 44.45 44.00 42.58 42. 77.75 58.67 70.45 75.17 77.37 56.41 73.90 78.90 78.35 59.27 73.32 59.37 77.75 73.19 76.50 53.47 65.66 77.23 77.8 58.18 72.50 78.40 77.26 58.30 70.88 58.42 77.64 73.99 70.01 69.61 69.90 69.85 69.77 63.93 71.98 69.10 68.43 68.67 70B bit W2(2k) C4(2k) W2(4k) C4(4k) AC AE HE QA WI 4 4 GPTQ GPTVQ 4.125 QuIP# AQLM 4.14 VPTQ 4.01 3 GPTQ GPTVQ 3.125 QuIP# AQLM 3.01 3.01 VPTQ 3 5.32 3.38 3.39 5.51 3.56 3.55 5.56 5.57 5.67 5.67 3.35 3.18 3.19 3.19 4.4 3.35 3.36 3.34 5.15 5.02 5.03 5.02 6.26 5.15 5.17 5.15 49.15 50.6 50.68 49.57 44.11 50.9 50 48. 81.23 63.47 76.81 78.1 81.4 81.5 63.69 77.31 63.71 78.16 81.18 78.4 60 72.73 77.7 81.4 81.28 63.23 77.61 63.52 80.9 77.06 75.61 77.1 76.48 76.4 71.82 76.4 77.19 77.51 Table 6: LLaMA-3 Wikitext2 perplexity (context length 2048) and zeroshot QA Accuracy. LLaMA-3 8B LLaMA-3 70B bit W2 AC AE HE QA WI bit W2 AC AE HE QA WI FP16 QuIP GPTQ VPTQ QuIP GPTQ VPTQ QuIP 16 4 4 4.03 3 3 3.03 2 DB-LLM 2 2 2.08 2.24 GPTQ VPTQ VPTQ 6.14 6.5 6.5 6.42 7.5 8.2 6.97 85.1 13.6 2.10E+02 9.29 9.19 50.3 47.4 47.7 49.1 41.0 37.7 45.8 21.3 28.2 19.9 36.9 42.6 80.1 78.2 78.8 78.8 72.9 70.5 77.5 29.0 59.1 28.8 71.0 73.2 60.2 58.6 59.0 59.3 55.4 54.3 58.4 29.2 42.1 27.7 52.2 53.1 79.6 78.2 78.4 78.7 76.8 74.9 78.2 52.9 68.9 53.9 75.1 75.4 16 4 4 4.05 3 3 3.01 2.9 3.4 3.3 3.15 4.7 5.2 3.81 13 66.3 65.7 66.1 66.2 63.9 63.5 65.5 40.9 87.0 86.0 86.3 86.1 83.3 79.6 84.7 48.9 60.1 58.7 58.4 59.0 54.9 52.1 57.3 26.5 80.8 73.1 79.7 73.2 80.7 72.6 74.8 79.8 78.4 72.5 77.1 71.1 79.2 73.4 51.7 61.7 60.4 N/A N/A N/A N/A N/A N/A N/A 59.9 50.5 77.9 65.9 74.0 69.1 82.4 82.5 82.9 82.4 82.3 80.6 81.7 65. 62.7 80.4 80.1 38.9 81.8 83.6 41.0 61.7 61.8 11.9 5.6 5.66 24.6 52.5 54.2 2 2.02 2. Table 7: Mistral-7B-v0.1 Wikitext2, C4 perplexity (context length 2048 and 8192) and zeroshot QA Accuracy Mistral 7b bit W2(2k) W2(8k) C4(8k) AC AE HE QA WI FP16 16 4 GPTVQ 4.125 QuIP# AQLM 4.02 GPTQ 4.125 VPTQ 4.03 GPTVQ 3.125 AQLM 3.04 3.125 GPTQ VPTQ 3.03 QuIP# 2 AQLM 2.01 GPTVQ 2.25 GPTQ 2.125 2.04 VPTQ 5.25 5.38 5.36 5.36 6.42 6.02 5.53 8.2 280 6.32 4.77 4.87 4.85 4.85 4.83 4.81 6.8 5.07 5.88 4.96 6.02 6.32 8.99 1535 5.64 5.71 6.13 5.79 5.79 5.74 5.72 13.28 5.97 6.86 5.84 6.84 6.93 18.6 164 6.43 48.89 50 49.4 48.21 49.57 48.12 40.78 46.67 47.35 46.67 39.76 40.44 37.37 24.49 41.13 78.87 80.43 78.96 77.86 79.5 77.82 75.67 77.61 77.86 77.95 72.14 73.65 71 44.91 72.22 61.12 60.36 60.62 60.27 60.38 60.61 54.18 59.31 58.84 59.91 52.95 52.13 45.43 36.56 56. 80.3 79.65 80.41 79.71 79.54 80.14 77.42 80.14 79.82 79.49 76.71 76.01 70.18 63.33 77.91 73.88 73.4 73.95 73.8 72.85 74.19 67.4 72.69 71.74 72.45 69.3 68.75 64.33 52.96 68.67 Table 8: Parameters for 2-bit Quantization of Llama and Mistral Models. represents the vector length, denotes the codebook size, k1 and k2 correspond to the two codebooks, and group num indicates the number of groups into which PQ (Product Quantization) is divided. bit 2.02 2.26 2.02 2.18 2.07 2.11 2.08 2.24 2.02 2.07 Outlier N% - 4 - 4 4 4 4 4 - 4 0 1 0 2 1 1 1 1 0 1 - 8192 - 8192 8192 8192 4096 8192 - 4096 6 12 6 12 12 12 12 6 12 6 k1 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 Other k2 - 4096 - 4096 4096 4096 4096 - 4096 - group num 1 4 1 4 4 8 1 16 1 16 LLaMA2-7b LLaMA2-13b LLaMA2-70b LLaMA3-8b LLaMA3-70b Table 9: Layer-wise finetuning parameters on 8xH100 number is not significant. finetune lr model 1 104 LLaMA-2-7B LLaMA-2-13B 1 104 LLaMA-2-70B 1 105 1 105 LLaMA-3-8B LLaMA-3-70B 5 106 5 106 Mistral-7B batchsize 32 32 16 16 8 16 C.7 Higher Bitwidth Rows #18 and #19 represent the results for 3-bit and 4-bit quantization, respectively. Compared to the FP16 results in row #1, 4-bit vector quantization incurs almost no loss."
        },
        {
            "title": "D Inference Evaluation",
            "content": "2, 4, and 8 groups, respectively. Each group has its own independent codebook. When divided into 1, 2, and 4 groups, the perplexity on Wikitext2 does not change much, likely because the distribution of the remaining parameters (after removing 1% outliers) is relatively uniform. This is likely because the distributions of different groups overlap after grouping, so the benefit of increasing the group D.1 Throughput Measurement Process We follow the throughput measurement method used in AQLM (Egiazarian et al., 2024). During the prompt phase, we provide 1 token and then have the model generate 256 tokens, calculating the generation time for each output token to determine the throughput in tokens per second (tok/s). Table 10: Ablation Study on Different Quantization Techniques for LLaMA-2 13B Finetune outlier e2e N% v0 bit channel independent #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 FP16 2 2.01 2.02 2.02 2.19 2.04 2.03 2.04 2.07 2.02 2.02 2.04 2.06 2.09 2.17 2.3 3.01 4.02 - Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes layer wise - - No No No No No No No No No No No No No No No No No No Yes No Yes Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes other k1 k2 - 16 256 4096 4096 65536 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 - -1 -1 -1 -1 -1 4096 -1 -1 -1 -1 -1 4096 4096 4096 4096 4096 -1 k0 - - - - - - - 4096 4096 4096 - - - 4096 4096 4096 4096 - - v1 - 2 4 6 6 8 12 6 6 6 6 6 12 12 12 12 12 4 6 group num - 1 1 1 1 1 1 1 1 1 1 1 1 1 2 4 8 1 1 W2() C4() 4.57 14800 7.21 6.29 7.25 5.8 6.32 6.16 6.08 6.02 6.07 5.32 5.71 5.63 5.63 5.63 5.55 4.82 4.64 6.05 13337 9.78 8.29 9.8 7.68 8.29 8.08 8.12 7.96 7.64 7.15 7.52 7.45 7.41 7.38 7.38 6.37 6.13 - 0 0 0 0 0 0 1 2 5 0 0 0 1 1 1 1 0 0 - - - - - - - 4 4 4 - - - 4 4 4 4 - - Table 11: Ablation of Vector Length on Inference Throughput and Peak Memory Usage v1 - 2 4 6 8 k1 - 16 256 4096 65536 4096 k2 - -1 -1 -1 -1 4096 FP16 2 2.01 2.02 2.19 2.04 group num tok/s mem(GB) 30.03 18.85 17.06 32.09 30.64 21.34 63.63 4.17 4 4.02 4.46 4.06 - 1 1 1 1 D.2 Our Dequantization Implementation Our dequantization implementation is divided into In the first phase, which handles two phases. prompts with relatively long sequences, we restore the quantized weights (index and centroid, etc.) to FP16 and then call torch.matmul. In the second phase, during decoding, we fuse the dequantization and GEMV operations into QGemv, eliminating the repetitive reading and writing of FP16 weights."
        }
    ],
    "affiliations": [
        "Microsoft",
        "University of Science and Technology of China"
    ]
}