{
    "paper_title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "authors": [
        "Zhenyu Zhang",
        "Shujian Zhang",
        "John Lambert",
        "Wenxuan Zhou",
        "Zhangyang Wang",
        "Mingqing Chen",
        "Andrew Hard",
        "Rajiv Mathews",
        "Lun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs."
        },
        {
            "title": "Start",
            "content": "2026-01-01 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Zhenyu Zhang*,2, Shujian Zhang1, John Lambert1, Wenxuan Zhou1, Zhangyang Wang2, Mingqing Chen1, Andrew Hard1, Rajiv Mathews1 and Lun Wang1 1Google DeepMind, 2The University of Texas at Austin, *Work done as student researcher at Google DeepMind 5 2 0 2 0 3 ] . [ 1 8 8 9 3 2 . 2 1 5 2 : r Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on humandefined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level steps and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs. Keywords: Reasoning, Sparse Auto-Encoding, Mechanistic Interpretability 1. Introduction Recent advancements in reasoning have significantly expanded the capabilities of large language models (LLMs), moving them far beyond basic language understanding to encompass more complex reasoning tasks. These include competition-level mathematical problem solving (Ahn et al., 2024; AlphaProof and AlphaGeometry, 2024), project-level coding (Jiang et al., 2024), and planning (Huang et al., 2024; Valmeekam et al., 2023). Evidence from model responses suggests that such substantial performance improvements primarily stem from their ability to perform extended chain-of-thought (CoT) reasoning (Wei et al., 2022). Nevertheless, how such lengthy reasoning trajectories contribute to performance remains underexplored. Recent studies have investigated the reasoning process, such as analyzing entropy mechanisms during inference (Fu et al., 2025; Zhang et al., 2025) or at the post-training stage (Cui et al., 2025; Wang et al., 2025b; Zhao et al., 2025), which can be further leveraged to achieve more accurate and concise reasoning (Yang et al., 2025). Other works suggest that current reasoning processes are often verbose, with certain parts being ineffective (Fu et al., 2024; Huang et al., 2025; Sheng et al., 2025). 2026 Google. All rights reserved Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process In addition, another line of studies argues that reasoning ability is tied to specific behaviors (Chen et al., 2025; Gandhi et al., 2025; Venhoff et al., 2025; Ward et al., 2025), such as reflection (i.e., the model revisits and verifies its previous reasoning steps) and backtracking (i.e., the model abandons the current reasoning path and pursues an alternative solution), which appear to be closely associated with the achieved performance gains. From the perspective of mechanistic interpretability, many studies rely on activation engineering methods, particularly the Difference-of-Means (DiffMean) approach (Marks and Tegmark, 2023). This method begins by constructing contrastive dataset with human supervision, such as categorizing samples into safe vs. harmful or positive vs. negative groups. For each category, it then extracts the latent representations of individual samples (i.e., â„+ ğ‘— ) and computes steering vector by â„+ averaging the representations within each category and taking their difference: ğ‘£ = 1 ğ‘– ğ‘+ 1 ğ‘— . The resulting steering vector can then be applied to shift the response style, for example, ğ‘ from sad to happy or vice versa. ğ‘– and â„ (cid:205)ğ‘ ğ‘—= (cid:205)ğ‘+ ğ‘–=1 â„ However, such activation-based approaches are ill-suited for understanding reasoning, as they rely on supervised, predefined concepts. This is feasible in tasks like sentiment classification (Pang et al., 2002), where concepts such as happy and sad are clearly separable. In contrast, reasoning behaviors are fluid, overlapping, and difficult to annotate at scale, which has led prior studies to focus narrowly on small set of predefined behaviors such as reflection or backtracking (Wang et al., 2025d). This restricts both the scope of analysis and the robustness of the resulting steering vectors. Figure 1 Illustration of our RISE framework for unsupervised reasoning behavior discovery. The pipeline consists of two stages: (i) training Sparse Autoencoder (SAE) on unlabeled representations of reasoning steps (Left), and (ii) evaluating causal effects on the original reasoning process (Right). Notably, the intervention process on the right is applied directly, without any additional training. In this work, we address these challenges by introducing an unsupervised framework (i.e., RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning behaviors. Building on the linear representation hypothesis (Park et al., 2023), we define fine-grained reasoning behaviors as linear directions in the activation space, which we refer to as Reasoning Vectors. These vectors can be applied to modify the reasoning mechanism in specific ways and thereby influence response quality. Unlike prior approaches, our framework does not rely on human-supervised labels; instead, it directly identifies reasoning vectors from step-level activations in chain-of-thought sequences. To achieve this, we employ sparse auto-encoders (SAEs), which learn dictionary of sparse latent features that reconstruct hidden states while promoting disentanglement. We show that individual decoder columns correspond to interpretable reasoning behaviors (Figure 1). Visualizations further reveal that these vectors cluster into semantically coherent regions of activation space. Most importantly, we demonstrate controllability: injecting SAE-derived vectors during inference can suppress or amplify behaviors such as reflection, directly altering the reasoning trajectory without additional training. 2 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Beyond semantic behaviors, we also observe evidence of structural organization: decoder columns consistently form clusters aligned with response length, with separability peaking in mid-to-late layers. Furthermore, we show that SAEs can reveal new behaviors that are difficult to define through wordlevel human supervision. As case study, we examine the semantic concept of confidence and find that confidence-related directions are highly concentrated, with interventions on the corresponding vectors exerting clear causal effects on reasoning styles. Our contributions are threefold: We propose an unsupervised framework, i.e., RISE, that captures the structure of reasoning behaviors within the latent space. Unlike prior methods that rely on human-defined concepts, RISE end-to-end models diverse reasoning behaviors directly within the decoder column space. With RISE, we show that SAE-derived vectors align with human-interpretable behaviors and can be used to selectively modulate reasoning at inference time without additional training. In particular, we can directly control reflection and backtracking during reasoning by reducing or enhancing the components of hidden representations along the SAE column directions on the fly. We further demonstrate the ability to discover novel reasoning behaviors that are difficult to define with word-level human supervision. As an example, we consider confidence, behavior that is challenging to specify at the word level. We find that confidence-related reasoning vectors form coherent clusters and causally shift the models response style toward more confident answers. 2. Related Works Reasoning Models. Previous works have demonstrated that enabling models to generate longer outputs can significantly enhance their reasoning ability. This progress starts from the famous Chain-ofThought (CoT) prompting (Wei et al., 2022), to test-time scaling (Snell et al., 2024), and more recent reinforcement learningoptimized reasoning models, such as OpenAIs o-series (Jaech et al., 2024), Anthropics Claude-3.7-Sonnet-Thinking (Anthropic, 2025), and Googles Gemini-2.5-Flash (Google, 2025), as well as notable open-source counterparts (Abdin et al., 2025; Guo et al., 2025; Team, 2024, 2025; Yang et al., 2024). While longer responses generally improve performance, the mechanisms remain unclear. Recent studies link these gains to several metacognitive behaviors (Chen et al., 2025; Gandhi et al., 2025; Venhoff et al., 2025), yet they rely on human-defined behaviors. In this work, we seek to uncover the geometry of reasoning behaviors in an unsupervised manner, thereby avoiding dependence on human-labeled definitions. Activation Steering. Activation steering modifies model outputs by editing internal activations, with notable approaches including representation engineering (Zou et al.), activation patching (Meng et al., 2022), and DiffMean (Marks and Tegmark, 2023). These methods have been effective in domains such as improving LLM truthfulness (Wang et al., 2025c), enhancing privacy (Goel et al., 2025), and controlling sentiment (Han et al., 2023). However, they typically rely on contrastive pairs (e.g., happy vs. sad) to define steering directions, which is suitable when clear oppositional concepts exist. In reasoning, such contrasts are more difficult to define: prior work has targeted specific behaviors like reflection (Chen et al., 2025; Venhoff et al., 2025) or coarse distinctions between short and long responses (Eisenstadt et al., 2025; Huang et al., 2025; Sheng et al., 2025), leaving the broader reasoning behavior space underexplored. In this work, we present pivot study to investigate this space in an unsupervised manner. recent effort also explored reasoning models in an unsupervised way by using Sparse Auto-Encoder (SAE) (Wang et al., 2025a), but primarily treated SAE as bridge to transfer reasoning ability from reasoning model to base model via supervised fine-tuning, offering limited insight into the internal mechanisms of the reasoning process. Our work instead aims to directly analyze these mechanisms. 3 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process 3. Preliminary 3.1. Sparse Auto-Encoder Building on the Linear Representation Hypothesis (Olah and Jermyn, 2024; Park et al., 2023), each atomic reasoning behavior can be mapped to specific direction in the activation space, which we define as reasoning vector. To automatically identify such behaviors, we employ sparse auto-encoders (SAEs), which provide an effective and principled approach. An SAE consists of an encoder and decoder that aim to reconstruct representations in the activation space: Ë†â„ = ğ‘Š decoder ğœ(ğ‘§) + ğ‘decoder; ğ‘§ = ğœ(cid:0)ğ‘Š encoder â„ + ğ‘encoder (cid:1) (1) Here, â„ â„ğ‘‘ denotes the original representation, ğœ is non-linear activation function, and Ë†â„ is the reconstruction. Each row of ğ‘Šdecoder â„ğ·ğ‘‘ corresponds to an atomic vector (i.e., reasoning vector), where ğ· is the hidden dimension of the SAE and ğ‘‘ is the dimension of the original input. The latent feature ğ‘§ represents the corresponding code. For standard SAE (Cunningham et al., 2023), we use ReLU as the non-linear activation function ğœ. The training objective is formulated as = Ë†â„ â„2 2 + ğœ† ğ‘§0, (2) which encourages the SAE to accurately reconstruct the original activations while enforcing sparsity in the latent codes. This ensures that only small number of atomic concepts are used, thereby reducing entanglement across vectors and enhancing interpretability. 3.2. Thought Representation Construction Intuitively, reasoning behaviors cannot be trivially explained by token-level attributes, since the same token can play diverse roles across different contexts. This observation, further supported by recent studies (Venhoff et al., 2025; Ward et al., 2025), motivates our choice of analyzing the reasoning process at the sentence level, where the internal structure of the whole responses can be more meaningfully captured. The construction of thought representations for SAE training involves the following steps: (i) Collecting model responses: we feed each question from the selected training set into the target model to generate responses. (ii) Splitting responses into sentence-level steps: we divide each response into reasoning steps using the delimiter symbol <nn>, producing ğ‘˜ steps per response, where ğ‘˜ varies across samples. (iii) Embedding each step: we re-run inference by feeding both the question and the corresponding response into the model, and extract the hidden representations of the token <nn>. Each representation is then regarded as the activation of its corresponding reasoning step (Chen et al., 2025). The representations are then being concatenated, denote as {â„ğ‘™ ğ‘–}, where ğ‘– {1, . . . , ğ‘} indexes the samples and ğ‘™ denotes the layer index. We specifically use the residual stream representations after each transformer layer, and train the SAE on single chosen layer. 4. Unsupervised Reasoning Vector Discovery 4.1. SAE Discovers Reasoning Behaviors in the Decoder Column Space To begin, we first analyze the ability of the SAE to capture the ground-truth representation structure. For each activation â„ obtained from the original model, the SAE decoder reconstructs it as: Ë†â„ = ğ‘Š ğœ(ğ‘§) + ğ‘decoder. This reconstruction can be interpreted as linear combination of selected rows of ğ‘Šdecoder. Under the linear representation hypothesis, we posit that the true activation admits decoder 4 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process sparse representation over ground-truth dictionary ğ‘Š = [ğ‘¤1, . . . , ğ‘¤ğ‘š] â„ğ‘‘ğ‘š. As established in Theorem 1, the SAE decoder matrix can recover this dictionary in the decoder space up to additional permutation and scaling. More details can be found in Appendices and C. Theorem 1. Suppose hidden representations at delimiter tokens follow the generative model â„ = ğ‘Šğ‘ + ğœ€, (3) where â„ â„ğ‘‘, ğ‘Š = [ğ‘¤1, . . . , ğ‘¤ğ‘š] â„ğ‘‘ğ‘š is dictionary of latent behavior directions, ğ‘ is ğ‘˜-sparse code, ğ‘¤ğ‘–,ğ‘¤ ğ‘— and ğœ€ is bounded noise. Assume: (i) Incoherence: maxğ‘– ğ‘— ğ‘¤ğ‘– ğ‘¤ ğ‘— ğœ‡ < 1. (ii) Sparsity: ğ‘˜ < ğ‘/ğœ‡ for universal constant ğ‘. (iii) Activation: SAE uses ReLU nonlinearity. (iv) Separation: Nonzero coefficients satisfy ğ‘ğ‘– ğ›¼ > 0. Then, as the number of samples ğ‘ , any local optimum of the SAE training objective min ğ‘Šenc,ğ‘Šdec ğ”¼(cid:2)â„ Ë†â„2 (cid:3) + ğœ† ğ‘§0 (4) recovers decoder matrix whose columns align with the true dictionary up to permutation matrix Î  and scaling diagonal matrix ğ·: Î , ğ· 0 s.t. ğ‘Šdec ğ‘ŠÎ ğ·. (5) 4.2. Setup Details We use DeepSeek-R1-Distill-Qwen-1.5B (R1-1.5B) (Guo et al., 2025), along with five hundred randomly sampled training examples from the MATH dataset (Lightman et al., 2023). We follow Section 3.2 to obtain the activations for SAE training. Since our goal is to model various reasoning behaviors, whose complexity is much lower than modeling raw language structure, we adopt relatively small hidden dimension of ğ· = 2048. The SAE is trained with batch size of 1024 and learning rate of 1 104, with warm-up over the initial 10% of training. We use the Adam optimizer (Kingma, 2014) with cosine annealing learning rate decay. To encourage sparsity, we apply sparsity strength of ğœ† = 2 103. Additionally, we extend our analysis to Qwen3-8B and report the results in Figure 5. 4.3. Visualizing the Geometry of the SAE Decoder We then conduct human-supervised analysis of the SAE decoder geometry to assess whether the structures learned by the SAE align with human supervision. Specifically, we classify each Figure 2 Visualization of SAE decoder columns projected onto 2-D plane with UMAP. From left to right, we show the raw SAE decoder rows and the corresponding results with human-defined behaviors highlighted. Results are obtained from the final layer of R1-1.5B. 5 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process representation in {â„ğ‘™ ğ‘–} into one of three categories: reflection, backtracking, or other. The main motivation for emphasizing reflection and backtracking is that, compared to prior chain-of-thought prompting, current reasoning models introduce stages of refining answers through these processes, which are the most prominent features we aim to explore. The classification is performed using an LLM-as-a-judge approach, where for each reasoning step associated with â„ğ‘™ ğ‘–, we prompt the LLM (i.e., GPT-5) with precise definitions of the behaviors: reflection (re-examining earlier steps), backtracking (switching to new approach), and other. For each labeled step, the corresponding representation is then mapped back to the SAE latent feature space. More details on the annotation process and the consistency across different annotation methods are provided in Appendix D. We record the activation patterns and report the top-active channels, highlighting their associated decoder columns. Here, the activity of channel is measured by the largest magnitude of its latent feature. Additionally, we apply Uniform Manifold Approximation and Projection (UMAP) (McInnes et al., 2018) to embed the decoder columns into two-dimensional space. Figure 2 illustrates that the Results. SAE decoder columns encode semantically meaningful behaviors in the latent space. The leftmost subfigure shows the UMAP projection of all decoder vectors without annotations. When overlaid with humanlabeled behavioral concepts, clear semantic structures emerge. We adopt UMAP for visualization because it leverages cosine similarity as the internal metric, emphasizing directional rather than Euclidean differences. This choice is particularly appropriate since activation vectors are primarily meaningful in terms of their direction rather than their magnitude. Furthermore, for behaviors such as reflection and backtracking, which are semantically clearer from human perspective, the corresponding decoder columns cluster more tightly in localized regions. In contrast, for the other category, which potentially encompasses mixture of behaviors, the top-active vectors are less centered and more dispersed across the entire SAE space. Figure 3 Normalized Silhouette scores across different layers of R1-1.5B. Layer-wise Properties of SAE Geometry. Then, we quantify the above analysis by Silhouette scores (Rousseeuw, 1987; Shahapure and Nicholas, 2020). This metric measures both the cohesion of samples within cluster and their separation from other clusters. We normalize the raw scores across layers for better visualization. The results are shown in Figure 3. Two key observations can be made. First, later layers generally achieve higher Silhouette scores than earlier ones, indicating that behavioral concepts become more separable as representations deepen. Interestingly, slight decline occurs near the final layers (except for the very last one), suggesting that mid-to-late representations encode the most distinct behavioral structures. This observation aligns with the oversmoothing phenomenon in current LLMs (Wang et al., 2023), where token representations become excessively similar. Second, the comparison between reflection and backtracking reveals only modest differences, whereas the separation of other behaviors from either reflection or backtracking is substantially stronger. This suggests that reflection and backtracking occupy more overlapping representational subspaces, while both are more clearly distinguished from the residual other category. 4.4. Causal Examination of SAE Decoder Columns Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process 4.4.1. Incorporating SAE Columns during LLM Inference Incorporating SAE Columns during LLM Inference. We next perform an intervention study to examine the causal effect of SAE decoder columns, focusing on those from the final layer. We first filter out decoder columns that exhibit strong activations across multiple behaviors (e.g., reflection and backtracking). From the remaining reflection-specific columns, we compute their average to obtain single reflection vector. During inference, this vector is injected into the hidden representation of the last token at each reasoning step, as shown in Figure 4. Note that all decoder columns are independently normalized to unit length ğ‘¤ğ‘–2 = 1. The intervention is then performed as: â„ = â„ ğ‘¤ğ‘– (ğ‘¤ ğ‘– â„), (6) where ğ‘¤ğ‘– denotes decoder column ğ‘–. In this way, we project out the component of â„ along the reflection direction to examine how the reflection behavior changes in the models responses. Intervention Results. As shown in Figure 6, when DeepSeek-R1-1.5B is prompted with detailed math question, its reasoning style can be effectively shaped while preserving correctness. Across all conditions, the model consistently produces the same final answer (3, ğœ‹/2), yet the reasoning trajectory shifts in predictable manner: negative intervention suppresses meta-cognitive phrases (i.e., reflection) and shortens the whole response process, positive intervention amplifies self-checking behaviors while lengthening it, and vanilla inference lies between. While simple example may appear trivial, we further validate the causal effect of SAE columns across diverse tasks, behaviors, and models. As shown in Figure 5, the results consistently demonstrate the causal influence of SAE columns. Specifically, applying negative interventions leads to consistent decline in the corresponding behavior, whereas positive interventions result in clear enhancement. Moreover, when we manually control the intervention strength with Figure 4 Illustration of our inference process that utilizes SAE decoder columns. For given reasoning behavior, we compute the corresponding centroid in the SAE decoder column space and directly apply it during inference of the original model for examining how the response changes. Figure 5 Statistics of reasoning behavior shifts induced by SAE column interventions are reported across different models and tasks, where the SAE columns are consistent across tasks. 7 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Table 1 Generalization of SAE columns under domain shift. The SAE columns are learned from the MATH500 dataset and applied to other domains. Steering Direction GPQA-Diamond KnowLogic Reflection Backtracking Reflection Backtracking Vanilla Positive Negative 53.23 62.77 45.42 11.83 20.31 6.47 35.56 51.00 25. 5.42 9.38 2.33 scalar value as: â„ = â„ ğ›¼ ğ‘¤ğ‘– (ğ‘¤ ğ‘– â„), where ğ›¼ {1.5, 1, 0, 1, 1.5}, the corresponding number of reflection steps of DeepSeek-R1-1.5B on the AIME25 tasks changes to {58.6, 73.6, 90.5, 131.0, 166.9}, consistently evolving in accordance with the intervention strength. These results further support that SAEs learn meaningful reasoning behaviors such as reflection in an unsupervised manner. Generalization Across Data Domains.The SAE is trained on the MATH500 dataset, after which we evaluate how the learned vectors generalize to other domains. To ensure larger domain shift, we consider commonsense and logical reasoning tasks, specifically GPQA-Diamond (Rein et al., 2024) and KnowLogic (Zhan et al., 2025). Notably, we apply the same SAE columns during inference on these new domains with the R1-1.5B model. The results, reported in Table 1, show that the reflection and backtracking vectors consistently modulate reasoning behaviors across domains. SAE Reveals the Underlying Geometry of Response Length. Inspired by recent findings (Huang et al., 2025; Sun et al., 2025) that response length can be distinguished in the activation space, we are further motivated to explore the geometry of response length in the SAE decoder space. Using the same procedure, the results are presented in Figures 11 and 12. We observe that early layers exhibit weak separation with respect to response length, whereas mid-to-late layers show stronger alignment with this structural property. More details can be found at Appendix E. 5. Discovering New Behaviors with SAE In the previous section, we demonstrated that an unsupervisedly trained SAE can capture semantically meaningful structures, as evidenced by visualizing its weight geometry and its alignment with humandefined reasoning behaviors. In the following, we present an initial attempt showing that SAEs can also be used to discover new behaviors. From the decoder columns of the SAE, the key challenge becomes selecting specific subspace that can be leveraged for particular goal. As starting point, we consider identifying goal that meaningfully impacts the reasoning process. Recent studies on the entropy mechanisms of LLM reasoning (Cui et al., 2025; Fu et al., 2025; Wang et al., 2025b; Zhang et al., 2025; Zhao et al., 2025) highlight that entropy, or equivalently model confidence, plays critical role during the reinforcement stage. For instance, incentivizing an LLM to maximize its confidence in the final answer has been shown to positively influence reasoning ability in certain scenarios. Moreover, confidence can also serve as criterion for filtering out ineffective responses (Fu et al., 2025; Yang et al., 2025; Zhang et al., 2025), thereby improving inference efficiency. Motivated by these insights, we adopt entropy as our proxy objective and seek to identify decoder columns that are closely associated with entropy. Setup. Given trained SAE with decoder ğ‘Šdecoder â„ğ·ğ‘‘, where ğ‘‘ denotes the dimension of the original model and ğ· is the number of learned vectors, we divide the original model into two parts. The splitting point is chosen at the layer ğ‘™ from which the SAE is trained. Accordingly, we denote the 8 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Figure 6 Responses from the R1-1.5B model when intervened with SAE vectors corresponding to reflection behaviors. Reasoning steps associated with reflection are highlighted in red. Zoom in for better visualization. remaining part of the model as â„ = ğ‘“1ğ‘™ (ğ‘¥); ğ‘¦ = ğ‘“ğ‘™ğ¿(â„), where ğ¿ is the total number of layers, ğ‘¥ is the input token and â„ is the activation at layer ğ‘™. Identifying Target SAE Decoder Columns. We assign score vector ğ‘† â„ğ· to the ğ· decoder columns and optimize ğ‘† by minimizing the final entropy. Specifically, given the activation set {â„ğ‘™ ğ‘–} extracted from the MATH500 training set at layer ğ‘™, we solve the following objective: arg min ğ‘† (cid:34) ğ”¼ ğ‘‰ ğ‘˜=1 (cid:35) ğ‘ğ‘˜ log ğ‘ğ‘˜ , ğ‘ğ‘˜ = softmax(cid:0) ğ‘“ğ‘™ğ¿(â„ + ğ‘†ğ‘Šdecoder)(cid:1) , ğ‘˜ (7) where ğ‘ğ‘˜ denotes the predicted probability of token ğ‘˜ and ğ‘‰ is the vocabulary size. We solve this optimization using the Adam optimizer with learning rate of 0.01 for 1000 iterations, together with cosine annealing learning rate decay schedule. The batch size is 256. 9 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Figure 7 (a) Visualization of SAE decoder columns, with top-scoring columns related to entropy highlighted, showing clear clustering. (b) Most frequent tokens during vanilla inference. (c) Most frequent tokens under intervention with the entropy-related vector. Comparing (b) and (c), tokens associated with reflection or backtracking become less frequent (e.g., Wait), while more tokens emerge that correspond to confident mathematical calculation (e.g., numbers) 5.1. Emergence of Confident Reasoning Vectors We conduct experiments on the last layer of DeepSeek-R1-1.5B. As shown in Figure 7, we select the columns with the highest scores in ğ‘†, which implies that these columns play critical role in achieving low entropy. We then inject with the corresponding vector, referred to as the confidence reasoning vector, during inference of the original model. The intervention process follows the same procedure described in the previous section 4.4. Confidence Vectors Concentrated in the SAE Space. First, we observe that the confidence-related columns are predominantly located in the bottom-right region of the visualization, suggesting that this area is particularly important for reasoning confidence. Moreover, these columns partially overlap with the reflection and backtracking regions identified in Figure 2, indicating that reflection and backtracking behaviors may contribute to higher entropy. This observation is consistent with recent findings (Wang et al., 2025b). Further results are provided by Figures 7(bc), where we visualize the most frequent tokens at the beginning of each reasoning step. Without intervention, frequent tokens include reflection-related cues such as Wait and backtracking indicators such as Alternatively. After intervention with the confidence vector, these are replaced by tokens more closely related to detailed mathematical calculation steps. Causal Effect of Confidence Vectors. In addition, we measure the frequency of reflection and backtracking steps before and after intervention. On the AIME25 tasks, reflection steps are reduced from 90.53 to 33.77, and backtracking steps decrease from 35.50 to 5.93. While we observe slight performance drop from 23.33% to 20.00%, this corresponds to only single question out of 30 and is therefore not statistically significant. More importantly, the intervention substantially alters the reasoning style, with large reductions in reflection and backtracking. These results demonstrate that SAEs can be used not only to interpret but also to actively modify the reasoning style of LLMs. Furthermore, as shown in Table 2, we observe that the confidence vector also generalizes across domains, exhibiting clear ability to simultaneously modulate both reflection and backtracking behaviors. This further demonstrates the semantic similarity between the confidence direction and the combined effects of reflection and backtracking. Reasoning Enhancement via Confidence Vectors. We further explore potential applications of RISE for enhancing the reasoning process. Inspired by recent work (Hu et al., 2025) that learns bias term at test time during the prefilling stage, we select the top-3 confidence vectors ğ‘1, ğ‘2, ğ‘3 and learn their combination coefficients at test time to control the reasoning process. As illustrated in Figure 4, this 10 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Table 2 Generalization of confidence vectors across data domains under different steering directions. Steering Direction GPQA KnowLogic Reflection Backtracking Reflection Backtracking Vanilla Positive Negative 53.23 61.37 37.25 11.83 16.18 6.91 35.56 48.16 20.04 5.42 5.98 3. yields sample-dependent steering vector (cid:205)ğ‘–1,2,3 ğ›¼ğ‘–ğ‘ğ‘–. We compare this approach against SEAL (Chen et al., 2025) and TIP (Wang et al., 2025d). As shown in Figure 8, our careful steering strategy improves reasoning accuracy by up to 4.66 points while achieving 13.69% reduction in token usage. These results serve as pivotal case study demonstrating the potential of RISE to enable on-the-fly control of the reasoning process through interpretability. Figure 8 Results on reasoning accuracy and token cost under different steering methods on R1-1.5B. 6. Conclusions We presented an unsupervised framework for uncovering and manipulating reasoning behaviors in LLMs. By training SAEs on chain-of-thought activations, we discovered disentangled latent vectors that correspond to semantic reasoning behaviors, validated through clustering analyses and intervention studies. These results extend the linear representation hypothesis to reasoning, showing that abstract cognitive patterns can be linearly encoded and selectively controlled. In addition, we identify robust length-aware organization in the latent space, where response length emerges as structural axis influencing reasoning trajectories. Our findings suggest new opportunities for both interpretability and control. First, unsupervised discovery offers scalable path toward mapping the cognitive landscape of reasoning models without relying on hand-crafted labels. Second, the controllability of SAE-derived vectors enables fine-grained test-time adaptation, allowing practitioners to modulate reasoning behaviors dynamically. Finally, the emergence of length as an organizing principle highlights an implicit inductive bias in reasoning LLMs, motivating future work on aligning structural and semantic features for more reliable reasoning. 11 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"
        },
        {
            "title": "References",
            "content": "M. Abdin, S. Agarwal, A. Awadallah, V. Balachandran, H. Behl, L. Chen, G. de Rosa, S. Gunasekar, M. Javaheripi, N. Joshi, P. Kauffmann, Y. Lara, C. C. T. Mendes, A. Mitra, B. Nushi, D. Papailiopoulos, O. Saarikivi, S. Shah, V. Shrivastava, V. Vineet, Y. Wu, S. Yousefi, and G. Zheng. Phi-4-reasoning technical report, 2025. URL https://arxiv.org/abs/2504.21318. J. Ahn, R. Verma, R. Lou, D. Liu, R. Zhang, and W. Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. T. AlphaProof and T. AlphaGeometry. Ai achieves silver-medal standard solving international 178 mathematical olympiad problems. DeepMind blog, 179:45, 2024. Anthropic. Claude 3.7 sonnet and claude code, February 2025. URL https://www.anthropic. com/news/claude-3-7-sonnet. Accessed: 2025-03-17. R. Chen, Z. Zhang, J. Hong, S. Kundu, and Z. Wang. Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986, 2025. G. Cui, Y. Zhang, J. Chen, L. Yuan, Z. Wang, Y. Zuo, H. Li, Y. Fan, H. Chen, W. Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. R. Eisenstadt, I. Zimerman, and L. Wolf. Overclocking llm reasoning: Monitoring and controlling thinking path lengths in llms. arXiv preprint arXiv:2506.07240, 2025. Y. Fu, J. Chen, S. Zhu, Z. Fu, Z. Dai, A. Qiao, and H. Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint arXiv:2412.20993, 2024. Y. Fu, X. Wang, Y. Tian, and J. Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. K. Gandhi, A. Chakravarthy, A. Singh, N. Lile, and N. D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. A. Goel, Y. Hu, I. Gurevych, and A. Sanyal. Differentially private steering for large language model alignment, 2025. URL https://arxiv.org/abs/2501.18532. Google. Gemini flash thinking, February 2025. URL https://deepmind.google/technologies/ gemini/flash/. Accessed: 2025-05-11. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. C. Han, J. Xu, M. Li, Y. Fung, C. Sun, N. Jiang, T. Abdelzaher, and H. Ji. Word embeddings are steers for language models. arXiv preprint arXiv:2305.12798, 2023. Y. Hu, X. Zhang, X. Fang, Z. Chen, X. Wang, H. Zhang, and G. Qi. Slot: Sample-specific language model optimization at test-time. arXiv preprint arXiv:2505.12392, 2025. 12 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process X. Huang, W. Liu, X. Chen, X. Wang, H. Wang, D. Lian, Y. Wang, R. Tang, and E. Chen. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716, 2024. Y. Huang, H. Chen, S. Ruan, Y. Zhang, X. Wei, and Y. Dong. Mitigating overthinking in large reasoning models via manifold steering. arXiv preprint arXiv:2505.22411, 2025. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. D. P. Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. S. Marks and M. Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022. C. Olah and A. Jermyn. What is linear representation? what is multidimensional feature. Transformer Circuits Thread, 2024. B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP 02, page 7986, USA, 2002. Association for Computational Linguistics. K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. P. J. Rousseeuw. Silhouettes: graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:5365, 1987. K. R. Shahapure and C. Nicholas. Cluster quality analysis using silhouette score. In 2020 IEEE 7th international conference on data science and advanced analytics (DSAA), pages 747748. IEEE, 2020. L. Sheng, A. Zhang, Z. Wu, W. Zhao, C. Shen, Y. Zhang, X. Wang, and T.-S. Chua. On reasoning strength planning in large reasoning models. arXiv preprint arXiv:2506.08390, 2025. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. C.-E. Sun, G. Yan, and T.-W. Weng. Thinkedit: Interpretable weight editing to mitigate overly short thinking in reasoning models. arXiv preprint arXiv:2503.22048, 2025. doi: 10.48550/arXiv.2503. 22048. 13 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Q. Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. Q. Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36: 7599376005, 2023. C. Venhoff, I. Arcuschin, P. Torr, A. Conmy, and N. Nanda. Understanding reasoning in thinking language models via steering vectors. arXiv preprint arXiv:2506.18167, 2025. S. Wang, J. Asilis, Ã–. F. AkgÃ¼l, E. B. Bilgin, O. Liu, D. Fu, and W. Neiswanger. Resa: Transparent reasoning models via saes. arXiv preprint arXiv:2506.09967, 2025a. S. Wang, L. Yu, C. Gao, C. Zheng, S. Liu, R. Lu, K. Dang, X. Chen, J. Yang, Z. Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. T. Wang, X. Jiao, Y. Zhu, Z. Chen, Y. He, X. Chu, J. Gao, Y. Wang, and L. Ma. Adaptive activation steering: tuning-free llm truthfulness improvement method for diverse hallucinations categories. In Proceedings of the ACM on Web Conference 2025, pages 25622578, 2025c. Y. Wang, H. Chen, Y. Tang, T. Guo, K. Han, Y. Nie, X. Wang, H. Hu, Z. Bai, Y. Wang, et al. Pangu-pi: Enhancing language model architectures via nonlinearity compensation. arXiv preprint arXiv:2312.17276, 2023. Y. Wang, Q. Liu, J. Xu, T. Liang, X. Chen, Z. He, L. Song, D. Yu, J. Li, Z. Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025d. J. Ward, C. Lin, C. Venhoff, and N. Nanda. Reasoning-finetuning repurposes latent representations in base models. arXiv preprint arXiv:2507.12638, 2025. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. C. Yang, Q. Si, Y. Duan, Z. Zhu, C. Zhu, Z. Lin, L. Cao, and W. Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025. W. Zhan, Y. Wang, N. Hu, L. Xiao, J. Ma, Y. Qin, Z. Li, Y. Yang, S. Deng, J. Ding, et al. Knowlogic: benchmark for commonsense reasoning via knowledge-driven data synthesis. arXiv e-prints, pages arXiv2503, 2025. Z. Zhang, X. He, W. Yan, A. Shen, C. Zhao, S. Wang, Y. Shen, and X. E. Wang. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778, 2025. X. Zhao, Z. Kang, A. Feng, S. Levine, and D. Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, et al. Representation engineering: top-down approach to ai transparency, 2023. URL https://arxiv. org/abs/2310.01405. 14 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process A. Clarification of LLM Usage In this work, we use LLMs to polish the writing, correct grammatical errors, and improve fluency. We also employ LLMs to generate the subfigures of the SAE decoder space in Figure 1. All LLM-generated content has been proofread by the authors before being included in this draft. B. Theoretical Justification Proof. 1) Support identifiability by thresholding. For ğ‘— ğ‘†, â„, ğ‘¤ ğ‘— = ğ‘ğ‘–ğ‘¤ğ‘–, ğ‘¤ ğ‘— + ğœ€, ğ‘¤ğ‘— (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ğ‘–ğ‘† ğ‘ ğ‘— ğ‘–ğ‘†{ ğ‘—} ğ‘ğ‘– ğ‘¤ğ‘–, ğ‘¤ğ‘— ğœ€2 ğ›¼ (ğ‘˜ 1)ğœ‡ ğ›¼ ğœ. For ğ‘— ğ‘†, â„, ğ‘¤ ğ‘— ğ‘–ğ‘† ğ‘ğ‘– ğ‘¤ğ‘–, ğ‘¤ ğ‘— + ğœ€2 ğ‘˜ğœ‡ ğ›¼ + ğœ. Hence any ğœ in the stated interval yields exact recovery ğ‘†(â„) = ğ‘†. The separation margin is ğ›¾ := ğ›¼(cid:0)1 (2ğ‘˜ 1)ğœ‡(cid:1) 2ğœ > 0. 2) Stable coefficient recovery on the true support. Condition on ğ‘†: â„ = ğ‘Šğ‘†ğ‘ğ‘† + ğœ€. Then Ë†ğ‘ğ‘† ğ‘ğ‘† = (ğ‘Š ğ‘† ğ‘Šğ‘†) 1ğ‘Š ğ‘† ğœ€ = ğ‘Š+ ğ‘† ğœ€, so Ë†ğ‘ğ‘† ğ‘ğ‘† 2 ğ‘Š+ ğ‘† opğœ€2 = ğœ€2 ğœmin(ğ‘Šğ‘†) . By Gershgorin, ğœ†min(ğ‘Š ğ‘† ğ‘Šğ‘†) 1 (ğ‘˜ 1)ğœ‡, hence ğœmin(ğ‘Šğ‘†) 1 (ğ‘˜ 1)ğœ‡ and Ë†ğ‘ğ‘† ğ‘ğ‘† 2 ğœ 1 (ğ‘˜ 1)ğœ‡ . 3) Realizability of threshold+LS by one-hidden-layer ReLU encoder. The map â„ ğ‘†(â„) is determined by finitely many half-space tests {â„, ğ‘¤ ğ‘— > ğœ}. On each polyhedral region with fixed ğ‘†, the map â„ Ë†ğ‘ğ‘† is affine. Thus ğ‘‡ (â„) is piecewise affine. Because Step 1 provides margin ğ›¾ > 0, ğ‘‡ can be uniformly approximated on the ğ›¾/2-interiors of these regions by one-hidden-layer ReLU network, with the excluded boundary strip having vanishing probability as ğœ 0. Hence there exist encoder parameters whose output ğ‘§ coincides with ğ‘‡ (â„) up to arbitrarily small error while preserving the sparsity pattern ğ‘†(â„). 4) Population objective separates true and false decoders. For any ğ‘Š, define ğœ‚ğ‘– := dist(cid:0)ğ‘¤ğ‘–, span(ğ‘Š)(cid:1) . (a) Span mismatch. If some ğœ‚ğ‘– > 0, then on Eğ‘– (ğ›¼, ğ›½) we can write â„ = ğ‘ğ‘–ğ‘¤ğ‘– + ğ‘Ÿ + ğœ€ with ğ‘Ÿ2 ğ›½, and by Step 1 the index ğ‘– is selected. Any reconstruction using ğ‘Š must approximate ğ‘ğ‘–ğ‘¤ğ‘– from span(ğ‘Š), incurring error at least (ğ›¼ ğ›½)ğœ‚ğ‘–; hence for some absolute ğ‘ > 0, â„ ğœ™ğ‘Š (â„)2 2 (ğ›¼ ğ›½)2ğœ‚2 ğ‘– ğ‘( ğ›½2 + ğœ2). Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Taking expectations and summing over ğ‘– yields strictly positive gap ğ½ (ğ‘Š) minÎ ,ğ·0 ğ½ (ğ‘ŠÎ ğ·) + ğ›¿span > 0 for small enough ğ›½, ğœ. (b) Same-span misalignment. Suppose span(ğ‘Š) = span(ğ‘Š) but ğ‘Š ğ‘ŠÎ ğ·. Then some ğ‘¤ğ‘– is not colinear with any column of ğ‘Š. On Eğ‘– (ğ›¼, ğ›½), reconstructing ğ‘ğ‘–ğ‘¤ğ‘– with ğ‘Š requires combining multiple misaligned columns, causing an oblique-projection loss bounded below by ğ‘ğ‘–ğ‘¤ğ‘– ğ‘Šğ‘‡ (â„)2 2 ğ‘ğ‘– (ğ›¼ ğ›½)2 ğ‘( ğ›½2 + ğœ2), where ğ‘ğ‘– > 0 depends only on the minimal principal angle between ğ‘¤ğ‘– and the columns of ğ‘Š. Averaging gives ğ½ (ğ‘Š) minÎ ,ğ·0 ğ½ (ğ‘ŠÎ ğ·) + ğ›¿align > 0 for small enough ğ›½, ğœ. Combining (a) and (b), for any ğ‘Š {ğ‘ŠÎ ğ·} there exists ğ›¿ > 0 such that ğ½ (ğ‘Š) min Î ,ğ· ğ½ (ğ‘ŠÎ ğ·) + ğ›¿. Note that the â„“0 term does not diminish this gap; for ğœ† > 0 it can only increase it because ğ‘‡ (â„) is fixed. 5) From population to empirical risk. Restrict ğ‘Š to compact set (e.g., each column norm in [ğ‘, ğ¶]). The map ğ‘‡ is piecewise-Lipschitz with finitely many linear regions and ğ‘‡ (â„) 0 ğ‘˜; hence the class 2 + ğœ† ğ‘‡ (â„)0 : ğ‘Š W(cid:9) := (cid:8) â„ â„ ğ‘Šğ‘‡ (â„)2 has finite pseudo-dimension and satisfies uniform law of large numbers: sup ğ‘Š (cid:12)ğ½ğ‘ (ğ‘Š) ğ½ (ğ‘Š)(cid:12) (cid:12) (cid:12) a.s. ğ‘ 0. Therefore empirical local minimizers converge to the population minimizers, which by Step 4 equal {ğ‘ŠÎ ğ·}. C. Empirical Validation of Sparsity and Incoherence Assumptions Furthermore, we empirically validate the key assumptions underlying the above theoretical analysis: ğ‘¤ğ‘–,ğ‘¤ ğ‘— ğ‘¤ğ‘– ğ‘¤ ğ‘— ğœ‡ < 1, and (ii) Sparsity, characterized by ğ‘˜ < ğ‘/ğœ‡ for (i) Incoherence, defined as maxğ‘– ğ‘— universal constant ğ‘. Figure 9 Results on Sparsity and Incoherence Assumptions. Since the ground-truth dictionary ğ‘Š in the theorem 1 is not directly observable, we use the reconstructed activations from the sparse autoencoder for evaluation. Given the consistently low reconstruction error in Figure 9, this serves as reliable proxy. We observe that the maximum cosine similarity among recovered dictionary vectors is clearly bounded by valid value , and the sparsity level (defined as the number of active dictionary vectors used to reconstruct each activation) is also well within the theoretical bound. These results provide empirical support for the assumptions underlying our theoretical analysis. 16 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process D. Annotation Details for Reasoning Behaviors We analyze the consistency of LLM annotation results in Section 4.3, using the following prompt to query GPT-5, GPT-4o, and Claude Sonnet 4.5 for classification. You are helpful expert that is good at classifying reasoning steps . You will be given single reasoning step from math / logic solution . Your task is to classify the reasoning step according to the provided taxonomy and decision rules . The available labels are : (1) reflection : step checking its previous reasoning process and stating its own uncertainty . (2) backtracking : steps that explicitly retract / pivot , proposing an alternative strategy to replace the current one . (3) others : steps that do not fall into the above two categories . You must select the class labels based on the above criteria and assign single class for each step . Your output should be strict label from the above three options : \" reflection \" , \" backtracking \" , or \" others \". If you cannot determine the label , please assign \" others \". Now , please classify the following reasoning step delimited by triple backticks , according to the taxonomy and decision rules provided in the system prompt . Reasoning Step : { text } Table 3 Keywords set for annotation of reasoning steps. Reflection Wait, verify, make sure, hold on, think again, correct, incorrect Let me check, seems right Backtracking Alternatively, think differenly, another way, another approach, another method, another solution, another strategy, another technique Annotation Consistency: To analyze the robustness of the annotation method, we compare annotation results produced by different LLM judges and keyword-matching approach, using the classification criteria defined in Table 3. The results, shown in Figure 10, report the agreement ratio (defined as the proportion of steps receiving the same annotation relative to all steps) exceeding 85% for each pair of methods. Overall, the consistency is high across most comparisons, with GPT-5 and GPT-4o exhibiting particularly strong agreement at approximately 94%. These results validate the reliability and consistency of our annotation methodology. E. SAE Reveals the Underlying Geometry of Response Length. We manually split the step-level activations {â„ğ‘™ ğ‘–} (where ğ‘™ denotes the layer index) into two categories: short responses, with sequence length less than one thousand tokens, and long responses, with sequence length exceeding eight thousand tokens. Following the same procedure, we forward the activations from each category into the SAE and highlight the most active columns. Structures aligned with response length emerge in the SAE column space. Figure 11 presents UMAP visualizations of SAE decoder columns across layers, revealing two distinct clusters that align with response length. Columns associated with long responses form more diverse and dispersed 17 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Figure 10 Results of annotation consistency across different methods. The numbers represent the agreement ratio for each pair of annotation methods. cluster, while those linked to short responses appear compact. This separation is weak in early layers but becomes increasingly clear and stable in mid-to-late layers, peaking just before the output stage. These results suggest that SAE representations progressively encode structural signals about response length alongside behavior-specific features. Also, the normalized silhouette scores across layers, based on response length clustering, are reported in Figure 12. We observe that the early layers exhibit weak separation with respect to response length, while the mid-to-late layers demonstrate stronger alignment with this structural property. 18 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process Figure 11 Visualization of the SAE decoder columns. From left to right, we show the raw SAE decoder columns and the corresponding results with human-defined behaviors highlighted (green/yellow dots represents the columns related with short and long responses, respectively). Results are obtained from DeepSeek-R1-1.5B using MATH-500 training samples. Figure 12 Normalized Silhouette scores across different layers of R1-1.5B."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "The University of Texas at Austin"
    ]
}