{
    "paper_title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation",
    "authors": [
        "Yanwu Yang",
        "Guinan Su",
        "Jiesi Hu",
        "Francesco Sammarco",
        "Jonas Geiping",
        "Thomas Wolfers"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal medical image segmentation models have emerged as a promising paradigm due to their strong generalizability across diverse tasks, showing great potential for a wide range of clinical applications. This potential has been partly driven by the success of general-purpose vision models such as the Segment Anything Model (SAM), which has inspired the development of various fine-tuned variants for medical segmentation tasks. However, fine-tuned variants like MedSAM are trained on comparatively limited medical imaging data that often suffers from heterogeneity, scarce annotations, and distributional shifts. These challenges limit their ability to generalize across a wide range of medical segmentation tasks. In this regard, we propose MedSAMix, a training-free model merging method that integrates the strengths of both generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical image segmentation. In contrast to traditional model merging approaches that rely on manual configuration and often result in suboptimal outcomes, we propose a zero-order optimization method to automatically discover optimal layer-wise merging solutions. Furthermore, for clinical applications, we develop two regimes to meet the demand of domain-specificity and generalizability in different scenarios by single-task optimization and multi-objective optimization respectively. Extensive evaluations on 25 medical segmentation tasks demonstrate that MedSAMix effectively mitigates model bias and consistently improves performance in both domain-specific accuracy and generalization, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 3 0 1 1 . 8 0 5 2 : r MedSAMix: Training-Free Model Merging Approach for Medical Image Segmentation Yanwu Yang*1,2, Guinan Su3, Jiesi Hu4,5, Francesco Sammarco1, Jonas Geiping3,6,7, and Thomas Wolfers1,2 1University of Tubingen, Germany 2German Center for Mental Health (DZPG), partner site, Jena & Tubingen, Germany 3Max Planck Institute for Intelligent Systems, Germany 4,5Harbin Institute of Technology, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China 6,7ELLIS Institute Tubingen, Germany; Tubingen AI Center, Germany yangyanwu1111@gmail.com Abstract Universal medical image segmentation models have emerged as promising paradigm due to their strong generalizability across diverse tasks, showing great potential for wide range of clinical applications. This potential has been partly driven by the success of general-purpose vision models such as the Segment Anything Model (SAM), which has inspired the development of various fine-tuned variants for medical segmentation tasks. However, fine-tuned variants like MedSAM are trained on comparatively limited medical imaging data that often suffers from heterogeneity, scarce annotations, and distributional shifts. These challenges limit their ability to generalize across wide range of medical segmentation tasks. In this regard, we propose MedSAMix, training-free model merging method that integrates the strengths of both generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical image segmentation. In contrast to traditional model merging approaches that rely on manual configuration and often result in suboptimal outcomes, we propose zero-order optimization method to automatically discover optimal layer-wise merging solutions. Furthermore, for clinical applications, we develop two regimes to meet the demand of domain-specificity and generalizability in different scenarios by single-task optimization and multi-objective optimization respectively. Extensive evaluations on 25 medical segmentation tasks demonstrate that MedSAMix effectively mitigates model bias and consistently improves performance in both domain-specific accuracy and generalization, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations."
        },
        {
            "title": "1 Introduction",
            "content": "Universal medical image segmentation has become widely adopted solution for diverse medical imaging tasks, enabling broad applicability without the need for extensive annotations or clinical expertise Butoi et al. (2023); Czolbe and Dalca (2023). These generalizable models facilitate adaptability to domain shifts and even generalize to previously unseen tasks Hu et al. (2025). Fine-tuned models derived from general-purpose *Equal contribution. Senior authors 1 Figure 1: Segmentation performance of various SAM-based models on diverse medical tasks, measured by Dice coefficient. Blue lines indicate the better result between MedSAM and MedicoSAM, while green lines represent SAMs performance. Despite being domain-specific, MedSAM and MedicoSAM still underperform on certain tasks. In contrast, our MedSAMix (red) perform better consistently across tasks. foundation models for segmentation, such as the Segment Anything Model (SAM) Kirillov et al. (2023), represent significant step toward universal medical image segmentation, e.g., MedSAM Ma et al. (2024a) and MedicoSAM Archit et al. (2025). Despite the promising results of these fine-tuned variants in universal medical image segmentation, we find their performance unbalanced across different tasks. As shown in Fig. 1, MedSAM/MedicoSAM achieve strong results on certain familiar tasks, such as optic disk and liver segmentation. However, on others such as kidney and maxillary sinus segmentation they even underperform the original SAM (see additional results in the Results section). This highlights the stronger generalization capability of SAM, despite the absence of domain-specific optimization, and reveals the limited adaptability of its fine-tuned variants. This can be mainly attributed to the inherent complexities of medical imaging data. Due to heterogeneity, domain shifts, and class imbalance, medical imaging datasets introduce complex optimization landscapes during fine-tuning, often leading models to converge to suboptimal local minima Li et al. (2020); Sanjeev et al. (2024). These complexities make fine-tuned models such as MedSAM more susceptible to suboptimal generalization. In contrast, SAM, which is trained on large-scale natural image datasets with smoother optimization landscapes may retain stronger global generalization and thus outperform MedSAM on certain medical tasks. Furthermore, fine-tuned variants are often affected by catastrophic forgetting during adaptation, especially in the absence of strategies to preserve their original generalization capabilities. As result, these models may lose part of their broader segmentation ability when adapted to the medical imaging domain Aleixo et al. (2023); Kemker et al. (2018). This raises critical question: How can we enhance domain-specific capabilities while mitigating the compromise of generalization? Noting that fine-tuned models initialized from the same pre-trained weights often converge to similar loss basins Neyshabur et al. (2020), model merging has emerged as an effective strategy to unify diverse solution modes into single model without additional training Su and Geiping (2025); Su et al. (2025); Yadav et al. (2024). By integrating parameters or representations from multiple models, model merging provides promising approach to improve performance and mitigate single-model biases, resulting in more stable, diverse, and generalizable predictions Akiba et al. (2025); Almakky et al. (2024). As medical models are often trained separately on data from different clinical centers due to privacy constraints, model merging offers promising solution for effectively integrating these models without requiring data 2 sharing, and its potential in the medical domain remains largely underexplored. Moreover, most existing model merging methods in other domains either rely on manually crafted configurations, which often result in suboptimal performance Maron et al. (2022); Yadav et al. (2024), or require computationally intensive merging during the training process Qazi et al. (2024); Sanjeev et al. (2024), making them particularly costly for large foundation models such as SAM and MedSAM. In addition, these methods typically lack support for multi-objective optimization, making it difficult to ensure the generalization ability of the merged models. In this study, we propose MedSAMix, an efficient training-free model merging framework to balance generalization and domain-specific capabilities for SAM-based medical image segmentation. Specifically, we explore the potential of MedSAMix from two perspectives: (1) Expert capability: MedSAMix merges model variants through single-task optimization using only few calibration samples, tailored to task-specific distributions and improved performance without retraining. (2) General capability: We introduce multiobjective optimization to capture diverse aspects of model performance across tasks for improving universal medical image segmentation. MedSAMix employs zero-order optimization approach that selects merging configurations based on their empirical performance during the search, enabling efficient exploration of the solution space given only few samples. This allows MedSAMix to adaptively balance task-specificity and generalization by combining model variants, while mitigating suboptimal generalization and fine-tuning issues. In summary, our main contributions are as follows: We propose MedSAMix, training-free model merging method leveraging the strengths of general and expert knowledge for medical image segmentation. We introduce flexible ways of merging for MedSAMix to accommodate different scenarios: single-task merging focuses on expert capabilities for specific domain tasks, while multi-task merging promotes generalization across diverse tasks by jointly optimizing multiple objectives. Extensive experiments on 25 medical image segmentation tasks show that MedSAMix facilitates the enhancement of expert-level performance and generalization without retraining, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations."
        },
        {
            "title": "2.1 Universal Medical image segmentation",
            "content": "Recently, universal models for medical image segmentation have become powerful tools for medical image segmentation without retraining Butoi et al. (2023); Ma et al. (2024a); Zhao et al. (2025). These models typically leverage image-mask pairs or symbolic prompts such as bounding boxes and points, offering strong zero-shot generalization capabilities. Among them, in-context learning (ICL) methods Gao et al. (2025); Hu et al. (2024); Takaya and Yamamoto (2024) relate rich spatial alignment information by utilizing image-mask pairs as prompts, including UniverSeg Butoi et al. (2023), SegGPT Wang et al. (2023), Neuralizer Czolbe and Dalca (2023), Tyche Rakic et al. (2024) and Neuroverse3D Hu et al. (2025). These models have achieved accurate segmentation performance on unseen data or tasks. However, these models are inherently constrained by their small parameter scale and the computational cost of in-context learning, both of which limit their generalizability. In addition, building on the Segment Anything Model (SAM), variants including MedSAM Ma et al. (2024a), SAM-Med Ye et al. (2023), MedicoSAM Archit et al. (2025) have been proposed by fine-tuning on large-scale medical imaging datasets with more parameters, showing promising performance in various medical tasks. Compared to ICL-based models, SAM-based approaches benefit from larger model capacity 3 Figure 2: Overview of our model merging framework. Given pool of models, MedSAMix searches for optimal merging configurations by using single-task or multi-task performance as rewards. While individual models may exhibit varying behaviors across tasks, MedSAMix adaptively combines them to optimize performance for the target task. and broader training coverage, which contribute to their superior generalization. However, the success of such foundation models often comes at the cost of large data requirements and substantial training overhead. In contrast, our proposed MedSAMix is an efficient and training-free approach that operates at the model level, reducing dependence on large-scale datasets while preserving strong generalization across diverse segmentation tasks."
        },
        {
            "title": "2.2 Model Merging",
            "content": "Early model merging relied on direct weight averaging (Utans, 1996; White, 2016), which lacked finegrained control over model behaviors. Subsequent methods introduced parameter-space transformations, either through adjustment matrices (Jin et al., 2022; Matena and Raffel, 2022) or task vectors defined by fine-tuning deltas (Ilharco et al., 2022), enabling algebraic composition of capabilities across tasks. To address parameter conflicts, sparsity-driven strategies like TIES-Merging (Yadav et al., 2024) and DARE (Yu et al., 2024) selectively retain and rescale parameters based on magnitude. More recent approaches explored parameter-level hyperparameter control (Du et al., 2024; Yang et al., 2023), though still limited to task-vector formulations, while evolutionary merging (Akiba et al., 2024) suffers from high search complexity. In this study, our framework integrates multiple merging approaches with fine-grained hyperparameter control tailored for image segmentation. Unlike prior medical imaging approaches Qazi et al. (2024); Sanjeev et al. (2024), which merge networks during training to produce robust model, our method adopts fundamentally different paradigm: MedSAMix enables efficient post-hoc merging across different SAM fine-tuned models."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present comprehensive overview of our approach. We start by formalizing the problem and introducing an optimization framework built upon three core components: (1) Search Space: We design layer-wise search space specifically for SAM-series models based on Vision Transformers (ViT). This space allows for fine-grained control over merging strategies by supporting multiple merging methods at varying 4 layer granularities across key modules, including the image encoder, prompt encoder, and mask decoder. (2) Optimization Objectives: The framework accommodates both task-specific objectives, enabling targeted enhancements for specialized domains, and multi-objective formulations that aim to identify Pareto-optimal configurations across diverse segmentation tasks. (3) Search Algorithm: We adopt SMAC optimization algorithm Lindauer et al. (2022), which leverages the defined objectives as rewards to steer the search toward effective merge configurations. The following subsections elaborate on each component and explain how they collectively contribute to robust model merging for segmentation applications."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "Given pre-trained segmentation base model Mbase and set of candidate segmentation models = {M1, M2, ..., MK} fine-tuned from the same base architecture, our goal is to construct an optimal merged model that maximizes performance across single dataset or multiple datasets. These combinations are determined by set of hyperparameters ω Ω, where Ω represents the search space of all possible merging configurations. Each configuration ω defines specific strategy to combine components from candidate models to form merged model Mω. The performance of the merged model is evaluated using an objective function (Mω) that measures effectiveness across segmentation tasks. This leads to our optimization problem: ω = arg min ωΩ where ω represents the optimal hyperparameter configuration that yields the best-performing merged model according to the chosen objective. (Mω) (1)"
        },
        {
            "title": "3.2 Search Space",
            "content": "Our candidate models are all based on the Segment Anything Model (SAM) architecture Kirillov et al. (2023). The network is built on the transformer architecture, specifically incorporating Vision Transformer-based Dosovitskiy et al. (2020) image encoder with transformer layers responsible for extracting image features, prompt encoder with convolutional downsampling layers for integrating user interactions, and lightweight mask decoder with transformer layers and transposed convolutional layers that generate segmentation results and confidence scores. The search space Ω encompasses all possible configurations for constructing our merged model. For different components (image encoder, prompt encoder, and mask decoder), we employ varying layer granularity to determine how layers are grouped for merging, where each group shares the same merge hyperparameters. This granularity-based approach allows us to balance the size of the search space with fine-grained control over the merging process. Specifically, we define granularity genc, gprompt, and gdec for the three components respectively, dividing the layers into = l/genc + k/gprompt + (z + p)/gdec groups in total. For each layer group i, we specify merge method di {1, 2, . . . , D} selected from available merging techniques, and associated hyperparameters hi = [hi,1, hi,2, . . . , hi,Pi], where Pi is the number of hyperparameters for merge method di. Therefore, complete configuration ω Ω is represented as: ω = {{(d1, h1), (d2, h2), . . . , (dG, hG)}} (2)"
        },
        {
            "title": "This formulation provides a structured framework for exploring the space of possible model merging",
            "content": "configurations while maintaining computational tractability through hierarchical grouping."
        },
        {
            "title": "3.3 Optimization Objective",
            "content": "To evaluate the quality of the merged model, we define both single-objective and multi-objective optimization that measures the models effectiveness across tasks. Specifically, we measure performance on calibration datasets D, quantifying metrics such as Dice coefficient for segmentation. For single-task optimization, we focus on maximizing performance on specific calibration target task : fsingle(Mω) = L(Mω, DT ) (3) where represents the segmentation loss function (e.g., Dice loss, cross-entropy) evaluated on the target dataset DT . For multi-task optimization across tasks = {T1, T2, . . . , Tm}, we employ Pareto Efficient Global Optimization (ParEGO) (Knowles, 2006) to identify Pareto-optimal solutions: fmulti(Mω, λ) = max {λi fsingle,i(Mω)} i=1,...,m (cid:88) + α λi fsingle,i(Mω) (4) where fsingle,i(Mω) is the i-th objective function, λi is the corresponding weight satisfying (cid:80)m λi 0, and α is small positive constant (typically 0.05). Each task-specific objective is defined as: i=1 λi = 1 and i=1 fsingle,i(Mω) = Li(Mω, DTi) (5) The optimizer outputs Pareto front of merging configurations representing different trade-offs between tasks. For evaluation, we selected the configurations yielding the best Pareto front on the calibration set."
        },
        {
            "title": "3.4 Search Algorithm",
            "content": "To efficiently navigate the large search space Ω and find optimal merging configurations, we employ Bayesian optimization based on SMAC Lindauer et al. (2022) with Random Forest Breiman (2001) as the surrogate model. Given evaluated configurations Ht = {(ω1, (Mω1)), . . . , (ωt, (Mωt))} at iteration t, the next configuration is selected by: ωt+1 = arg max ωΩ where EI(ω) = E[max(f ˆf (ω), 0)] is the Expected Improvement acquisition function, is the best observed value, and ˆf (ω) is the Random Forest prediction. The process iteratively selects promising configurations until convergence. The whole process is described in Alg. 1 EI(ω) (6)"
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. In this study, we incorporate 25 publicly available medical image segmentation datasets across wide range of organs (e.g., liver, kidney, spleen) and imaging modalities (e.g., CT, MRI, MRA). These datasets cover diverse tasks, including brain tumor segmentation (BraTS) Menze et al. (2014), vascular segmentation (Topcow) Yang et al. (2024), optic disc and cup segmentation from retinal fundus images (Fundus) Staal et al. (2004), and abdominal organ segmentation for the kidneys, liver, spleen, pancreas (FLARE22) Ma 6 Algorithm 1 Optimization Process of MedSAMix Require: Base model Mbase, candidate models = {M1, M2, . . . , MK}, calibration datasets D, maximum iterations Tmax Ensure: Optimal merging configuration ω 1: Initialize search space Ω with granularities genc, gprompt, gdec 2: Initialize evaluation history H0 = 3: Randomly sample initial configurations and evaluate to get H1 4: for = 1 to Tmax do 5: Train Random Forest surrogate model on Ht Compute Expected Improvement: EI(ω) = E[max(f ˆf (ω), 0)] Select next configuration: ωt+1 = arg maxωΩ EI(ω) // Model Merging based on Configuration ωt+1 for each layer group = 1 to do"
        },
        {
            "title": "Apply merge method di with hyperparameters hi\nMerge corresponding layers from candidate models",
            "content": "end for // Evaluation if single-task optimization then Compute fsingle(Mωt+1) = L(Mωt+1, DT ) else Compute fmulti(Mωt+1, λ) using ParEGO on tasks = {T1, T2, . . . , Tm} 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end if Update history: Ht+1 = Ht {(ωt+1, (Mωt+1))} Update best configuration: = minωHt+1 (Mω) 20: 21: end for 22: return ω = arg minωHTmax (Mω) et al. (2024b), liver, kidney, and stomach (RAOS) Luo et al. (2024). We also include maxillary sinus, nasal cavity segmentation, and nasal pharynx (Nasal) Zhang et al. (2024), cardiac structure segmentation (MSD) Antonelli et al. (2022), and prostate segmentation (PROMISE) Litjens et al. (2014). For broad and fair comparison as well as to enhance the diversity, we include additional commonly studied segmentation tasks, such as body organ segmentation in rodents (e.g., lungs and intestines) using the Mice dataset Rosenhain et al. (2018), as well as various neuroanatomical structure segmentation from the ADNI dataset Jack Jr et al. (2008), including the cerebral cortex, hippocampus, thalamus, amygdala, lateral ventricles, and putamen. More details of the data size are reported in the Supplementary. Benchmarks. We compare our method with ICL-based baselines, including UniverSeg Butoi et al. (2023), SegGPT Wang et al. (2023), and Neuroverse3D (Neuro3D) Hu et al. (2025), as well as SAM-based models such as SAM-Med2D Ye et al. (2023), SAM Kirillov et al. (2023), MedSAM Ma et al. (2024a), and MedicoSAM Archit et al. (2025). In addition, we use the fully-supervised nnU-Net Isensee et al. (2021) as an upper bound for reference. Additional comparisons are supplied in the Supplementary. Evaluations. For each dataset, 80% is reserved for testing to ensure robust evaluation. The remaining 20% serves multiple purposes: as calibration set for MedSAMix during the merging search, as context prompts for ICL-based models, and as training data for nnU-Net. SAM-based models are evaluated in zero-shot setting without access to training data but are provided with inferred bounding boxes for guidance. All tasks are evaluated using the Dice coefficient. 7 Table 1: Segmentation performance across 25 tasks in terms of Dice coefficient score (%). The values in parentheses in the last row indicate the number of tasks in which each method outperforms the baseline MedicoSAM/MedSAM/SAM model. idx Task Name Brain Tumor Vascular Cerebral Cortex Hippocampus Thalamus Lateral Ventricle Putamen Amygdala FLARE22 Liver 1 2 3 4 5 6 7 8 9 10 FLARE22 Kidney 11 FLARE22 Kidney Maxillary Sinus 12 Nasal Cavity 13 Nasal Pharynx 14 Prostate 15 Mice-Lung 16 Mice-Pancreas 17 Cardiac 18 FLARE22 Spleen 19 20 FLARE22 Pancreas 21 22 23 24 RAOS Liver RAOS Kidney RAOS Stomach Optic Cup Optic Disk Avg. Supervised In-context-learning models SAM-based nnU-Net SegGPT UniverSeg Neuro3D SAM-Med 75.83 85.87 87.72 82.30 83.40 84.76 83.03 73.43 96.59 94.66 95.39 91.23 88.37 91.40 86.78 89.28 83.27 85.00 95.14 79.92 93.74 92.39 87.73 85.68 96.67 87.58 17.24 28.33 47.08 25.52 38.70 45.60 21.92 7.77 68.72 63.24 66.75 51.30 39.83 46.35 50.26 62.17 53.52 43.23 64.57 2.36 70.81 63.14 33.03 74.32 96.27 47.28 19.20 68.21 69.51 71.33 74.57 75.23 73.38 59.37 79.33 84.21 83.33 80.57 69.76 84.87 76.36 75.67 65.21 68.96 70.59 36.38 81.49 80.14 57.92 80.56 95. 71.27 68.05 80.27 87.08 75.92 78.09 82.44 78.41 57.85 90.73 76.34 79.25 60.29 60.36 83.61 33.57 83.25 66.03 64.21 82.61 16.89 81.83 39.74 33.50 - - 67.84 67.26 36.85 31.77 36.17 56.49 53.83 25.74 30.33 92.03 93.95 92.92 60.90 54.23 87.44 88.40 61.08 85.35 77.06 93.46 71.66 88.19 62.34 79.85 71.61 85.53 67.38 Medico SAM MedSAM SAM Ours-S 78.36 73.17 60.70 31.35 58.00 56.13 62.14 52.17 70.98 67.60 75.71 55.74 45.33 36.93 61.92 48.62 94.05 91.83 95.86 90.04 95.06 89.64 84.47 75.54 62.51 60.15 89.01 84.68 91.36 89.16 80.62 65.93 89.20 83.68 84.41 74.31 95.06 92.93 77.66 76.59 92.38 89.90 74.66 72.44 88.74 85.67 87.70 86.42 95.17 93. 70.04 50.86 55.00 53.23 44.08 74.68 32.49 29.18 84.60 95.05 94.25 80.45 58.70 85.62 88.21 71.96 85.20 74.87 92.60 70.26 85.50 69.13 79.23 61.93 83.17 70.56 28.80 48.83 43.36 56.99 27.52 26.26 39.58 85.89 84.95 83.87 56.40 40.42 70.52 79.01 49.04 74.17 66.18 83.86 58.14 84.64 66.01 77.64 86.41 94.01 Ours-M 75.18 62.90 55.03 57.51 67.60 77.60 33.61 52.03 92.88 95.12 94.16 79.61 65.29 88.83 90.64 75.43 88.44 83.03 94.76 78.36 89.82 72.72 86.00 82.11 94.73 72.97 63. 70.81 79.64 (25) 77.34 (18) *Ours-S: MedSAMix-S. The model is merged under specific single-task setting and evaluated on the testing set. *Ours-M: MedSMix-M. The model is merged under multi-task setting and evaluated on the test data across all 25 tasks. Model Selection. We use SAM Kirillov et al. (2023) as the base and merge it with MedSAM Ma et al. (2024a) and MedicoSAM Archit et al. (2025) as representative fine-tuned variants. Although other variants exist, such as the Medical SAM Adapter Wu et al. (2025), they differ architecturally from SAM, making them incompatible with our model merging framework. Objective and Optimizer. For expert capability evaluation, we focus on single-task search. For each task, the model is optimized using the calibration set to obtain the optimal merging configuration, which is then evaluated on the corresponding test set. This procedure is repeated for all 25 tasks. For general capability evaluation, we perform multi-objective optimization across representative subset of tasks to reflect overall performance. To ensure both diversity and representativeness, we select eight tasks for the search process: optic disk, tumor, vascular, lateral ventricle, mice-lung, cardiac, nasal pharynx, and prostate. The merged models obtained from single-task and multi-task settings are referred to as MedSAMix-S and MedSAMix-M, respectively. Implementation details. In this study, we leverage the commonly used base architectures of SAM and its variants, MedSAM, and MedicoSAM for model merging, with = 12 Transformer layers, 768 hidden features, 12 heads, = 4 prompt encoder layers, and lightweight mask decoder with = 2 Transformer layers and = 3 transposed convolutional layers. Our model merging framework adaptively searches Figure 3: Visual examples of segmentation results of implemented models on pancreas, mouse lung, and brain vascular datasets. for optimal layer combinations, with candidate merging techniques including TIES, task arithmetic, linear combination, and SLERP. For single-task optimization, we perform 120 trials using two GPUs, while for multi-task optimization, we conduct 200 trials on four GPUs. The layer granularity is searched within the range [1, 4]. Codes and model weights would be available upon acceptance. Our code and checkpoints are available at Github 1 and Hugging Face2."
        },
        {
            "title": "5.1 Evaluations on domain-specific single tasks",
            "content": "We first evaluated the performance in domain-specific single-task applications. The results are shown in Table 1, where our merged model is denoted as MedSAMix-S. From the results, we can see that MedSAMix-S achieves significant improvements over the original SAM and other fine-tuned models across all the tasks. For instance, on the brain tumor and Nasal Pharynx parcellation tasks, MedSAMix-S outperforms the MedicoSAM by 5.19% and 4.33% in terms of Dice coefficient, respectively. Moreover, for tasks where MedSAM and MedicoSAM already perform well, such as optic disk segmentation, MedSAMix-S still yields further improvements of around 1.2%. This demonstrates the advantage of model merging in leveraging the complementary strengths of different models, even when those models are already well-adapted to the task. In addition, we report the fully supervised results from nnU-Net Isensee et al. (2021) as an upper bound for reference. Compared to nnU-Net, although there remains performance gap of around 8%, our model achieves comparable or even better results on tasks such as brain tumor and prostate segmentation. This underscores the strong potential of foundation models, highlighting their promising few-shot capabilities."
        },
        {
            "title": "5.2 Evaluations on universal segmentation tasks",
            "content": "In addition to the single-task evaluations, we also compare merged MedSAMix-M with other ICL-based and SAM-based models across multiple tasks to assess its generalization capability in medical image segmentation. The results are reported as MedSAMix-M in Table 1, where it outperforms baselines on 18 out of 25 tasks. 1https://github.com/podismine/MedSAMix.git 2https://huggingface.co/guinansu/MedSAMix 9 Among all universal segmentation models, MedSAMix-M achieves the best overall performance, with an average improvement of 4.37% over the second-best model. Furthermore, most ICL-based models perform even worse than the original SAM. As shown in Fig. 3, visual comparisons further illustrate this trend. For example, while SegGPT performs well on certain tasks, its average performance across all tasks remains relatively low, suggesting that these domain-specific foundation models still exhibit limited generalization. These findings underscore the critical importance of generalization capability in medical image segmentation. Table 2: Comparisons with other model merging methods on medical image segmentation in terms of the averaged Dice coefficient (%) score across 25 tasks. Type Baseline Merged Models MedSAM SAM MedicoSAM TIES TA Linear SLERP MedSAMix-S MedSAMix-M Avg. 63.32 70.81 72.97 66.31 72.19 74.00 75.28 79.64 77.34 Improv. - - - -6.66 % -0.78 % 1.03 % 2.31 % 6.67 % 4.37 %"
        },
        {
            "title": "5.3 Comparisons with other model merging methods",
            "content": "Given the effectiveness of model merging in medical image segmentation, we compare our MedSAMix with several model merging baselines, including Task-Arithmetic (TA) Ilharco et al. (2022), TIES Yadav et al. (2024), SLERP White (2016), and linear weighted combination. which we used in our search space. The results are summarized in Table 2, which reports the average Dice coefficient across all 25 tasks, along with the corresponding improvements over the MedicoSAM baseline. Detailed results are provided in the Supplementary Material. The results show that most model merging baselines offer limited improvement over MedicoSAM. In contrast, MedSAMix(-S/-M) consistently outperforms them. These findings highlight the effectiveness and adaptability of MedSAMix in identifying optimal merging configurations for medical image segmentation."
        },
        {
            "title": "5.4 Search space sensitivity analysis",
            "content": "Layer granularity. We conducted sensitivity analysis on layer granularity for both single-task and multitask optimization, varying the granularity from 1 to 4. The results are presented in Fig. 4 (1), where the performance under single-task and multi-task settings is shown in orange and blue, respectively. The results reveal that single-task performance is relatively insensitive to changes in layer granularity, whereas multi-task performance is notably affected. This suggests that for single-task scenarios, there remains redundancy in the network parameters, allowing for merging even with coarse granularity. In contrast, multi-task performance is more dependent on fine-grained control over layer merging. Notably, the best performance in the multi-task setting is achieved when the granularity is set to 2. Number of searched tasks. In addition, we investigated the impact of the number of tasks used for multi-task optimization. As shown in Fig. 4 (2), we find notable improvement when the number of search tasks exceeds six. The best performance is achieved when eight tasks are used. However, further increasing the 10 Figure 4: Sensitivity analysis of different merging settings in terms of (1) layer granularity, (2) task number for multi-task optimization, and (3) different model combinations. number of tasks leads to decline in performance. This may be attributed to the increasing difficulty in maintaining Pareto-optimal solutions and effective model selection as the task number grows. Combinations of models. Finally, we compare model merging on different combinations of models, as shown in Fig. 4 (3). We find that merging MedicoSAM and SAM yields the best pairwise performance, consistent with their strong individual results. Although MedSAM achieves the worst performance among the three models, combining all three models can lead to further improvements, demonstrating the benefit of leveraging complementary knowledge from multiple sources. This demonstrates the great potential of our framework. Incorporating more diverse models is expected to further enhance performance."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "In this study, we propose MedSAMix, zero-order training-free model merging method for medical image segmentation. While domain-specific foundation models achieve strong performance on specialized tasks, they often struggle with generalizability and, in some cases, underperform the original SAM model. In contrast, MedSAMix consistently outperforms all baselines across 25 tasks, demonstrating both superior domain-specific accuracy and broader generalization capabilities. Model merging for medical image analysis. Our work represents pioneering exploration of model merging in medical image analysis. In practice, individual centers can train models for specific organs or modalities and merge them with large-scale foundation models like SAM, yielding models that combine strong generalization with domain-specific expertise. Moreover, such merging strategy offers practical solution to challenges related to data privacy and security, as it eliminates the need for data sharing while integrating knowledge from diverse sources. Generalized vs. specialized models. This study highlights the inherent trade-off between specialization and generalization in medical image segmentation. While expert models like MedicoSAM perform well across tasks, they still underperform the base model in certain cases. In contrast, the original SAM demonstrates strong generalizability, underscoring the value of foundation models. Based on this insight, our MedSAMix effectively balances this trade-off and enhances both multi-task generalization and single-task performance without the need for additional data or retraining. This highlights the power of model merging in mitigating suboptimal generalization and points to the potential of hybrid strategies that achieve performance gains purely through model-level optimization. Efficiency. Unlike fine-tuning methods that require memory-intensive training, MedSAMix merges models by directly searching optimal configurations without any gradient updates. Each merging configuration 11 involves only forward inference, making the process highly parallelizable and GPU-efficient. For example, on the vascular segmentation task, MedSAMix completes 120 trials in 70 minutes using two GPUs, each consuming only 8GB of memory. This averages to 1.5 minutes per trial per GPU. In the multi-task setting, 200 trials are executed over 20 hours on four GPUs. In contrast, other universal segmentation models demand extensive resources and take days of training on 8 A100 GPUs (e.g., MedicoSAM and Neuroverse3D). Our MedSAMix, by comparison, is far more efficient, requiring minimal hardware and no retraining. Limitation. We acknowledge that, despite covering 25 diverse tasks, our study cannot fully represent the entire landscape of medical image segmentation. Future work will expand task diversity and develop systematic methods to assess generalization and robustness."
        },
        {
            "title": "7 Acknowledgment",
            "content": "This work was supported by the German Research Foundation (DFG) Emmy Noether Program (513851350, TW) and the BMBF/DLR project FEDORA (01EQ2403G, TW). We acknowledge the affiliation with the International Max Planck Research School for Intelligent Systems (IMPRS-IS) and the computational resources provided by the de.NBI Cloud, part of the German Network for Bioinformatics Infrastructure (de.NBI)."
        },
        {
            "title": "References",
            "content": "Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187, 2024. Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. Nature Machine Intelligence, 7(2):195204, 2025. Everton Aleixo, Juan Colonna, Marco Cristo, and Everlandio Fernandes. Catastrophic forgetting in deep learning: comprehensive taxonomy. arXiv preprint arXiv:2312.10549, 2023. Ibrahim Almakky, Santosh Sanjeev, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Hu Wang, and Mohammad Yaqub. Medmerge: merging models for effective transfer learning to medical imaging tasks. arXiv preprint arXiv:2403.11646, 2024. Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. Anwai Archit, Luca Freckmann, and Constantin Pape. Medicosam: Towards foundation models for medical image segmentation. arXiv preprint arXiv:2501.11734, 2025. Leo Breiman. Random forests. Machine learning, 45:532, 2001. Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert Sabuncu, John Guttag, and Adrian Dalca. Universeg: Universal medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2143821451, 2023. Steffen Czolbe and Adrian Dalca. Neuralizer: General neuroimage analysis without re-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 62176230, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 12 Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Goh, Ho-Kin Tang, Daojing He, et al. Parameter competition balancing for model merging. Advances in Neural Information Processing Systems, 37:8474684776, 2024. Yunhe Gao, Di Liu, Zhuowei Li, Yunsheng Li, Dongdong Chen, Mu Zhou, and Dimitris Metaxas. Show and segment: Universal medical image segmentation via in-context learning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2083020840, 2025. Jiesi Hu, Yang Shang, Yanwu Yang, Xutao Guo, Hanyang Peng, and Ting Ma. Icl-sam: Synergizing in-context learning model and sam in medical image segmentation. Medical Imaging with Deep Learning, pages 641656, 2024. Jiesi Hu, Hanyang Peng, Yanwu Yang, Xutao Guo, Yang Shang, Pengcheng Shi, Chenfei Ye, and Ting Ma. Building 3d in-context learning universal model in neuroimaging. arXiv preprint arXiv:2503.02410, 2025. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. Clifford Jack Jr, Matt Bernstein, Nick Fox, Paul Thompson, Gene Alexander, Danielle Harvey, Bret Borowski, Paula Britson, Jennifer L. Whitwell, Chadwick Ward, et al. The alzheimers disease neuroimaging initiative (adni): Mri methods. Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine, 27(4):685691, 2008. Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. Joshua Knowles. Parego: hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE transactions on evolutionary computation, 10(1):5066, 2006. Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot. Domain generalization for medical imaging classification with linear-dependency regularization. Advances in neural information processing systems, 33:31183129, 2020. Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Andre Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, Rene Sass, and Frank Hutter. Smac3: versatile bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research, 23(54):19, 2022. Geert Litjens, Robert Toth, Wendy Van De Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram Van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Medical image analysis, 18(2):359373, 2014. Xiangde Luo, Zihan Li, Shaoting Zhang, Wenjun Liao, and Guotai Wang. Rethinking abdominal organ segmentation (raos) in the clinical scenario: robustness evaluation benchmark with challenging cases. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 531541. Springer, 2024. Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024a. 13 Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Mae, Adamo Young, Cheng Zhu, Xin Yang, Kangkang Meng, Ziyan Huang, et al. Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancer abdominal organ quantification: the flare22 challenge. The Lancet Digital Health, 6(11):e815e826, 2024b. Roman Maron, Achim Hekler, Sarah Haggenmuller, Christof von Kalle, Jochen Utikal, Verena Muller, Maria Gaiser, Friedegund Meier, Sarah Hobelsberger, Frank Gellrich, et al. Model soups improve performance of dermoscopic skin cancer classifiers. European Journal of Cancer, 173:307316, 2022. Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:1770317716, 2022. Bjoern Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):19932024, 2014. Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512523, 2020. Mohammad Areeb Qazi, Ibrahim Almakky, Anees Ur Rehman Hashmi, Santosh Sanjeev, and Mohammad Yaqub. In Annual Dynammo: Dynamic model merging for efficient class incremental learning for medical images. Conference on Medical Image Understanding and Analysis, pages 245257. Springer, 2024. Marianne Rakic, Hallee Wong, Jose Javier Gonzalez Ortiz, Beth Cimini, John Guttag, and Adrian Dalca. Tyche: Stochastic in-context learning for medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1115911173, 2024. Stefanie Rosenhain, Zuzanna Magnuska, Grace Yamoah, Wa Rawashdeh, Fabian Kiessling, Felix Gremse, et al. preclinical micro-computed tomography database including 3d whole body organ segmentations. Scientific data, 5 (1):19, 2018. Santosh Sanjeev, Nuren Zhaksylyk, Ibrahim Almakky, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, and Mohammad Yaqub. Fissionfusion: fast geometric generation and hierarchical souping for medical image analysis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 131141. Springer, 2024. Joes Staal, Michael Abr`amoff, Meindert Niemeijer, Max Viergever, and Bram Van Ginneken. Ridge-based vessel segmentation in color images of the retina. IEEE transactions on medical imaging, 23(4):501509, 2004. Guinan Su and Jonas Geiping. Fine, ill merge it myself: multi-fidelity framework for automated model merging. arXiv preprint arXiv:2502.04030, 2025. Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, and Jonas Geiping. Gptailor: Large language model pruning through layer cutting and stitching. arXiv preprint arXiv:2506.20480, 2025. Eichi Takaya and Shinnosuke Yamamoto. In-context learning for medical image segmentation. arXiv preprint arXiv:2412.13299, 2024. Joachim Utans. Weight averaging for neural networks and local resampling schemes. In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press, pages 133138. Citeseer, 1996. Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, et al. Sam-med3d: towards general-purpose segmentation models for volumetric medical images. In European Conference on Computer Vision, pages 5167. Springer, 2024. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Towards segmenting everything in context. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11301140, 2023. Tom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016. Junde Wu, Ziyue Wang, Mingxuan Hong, Wei Ji, Huazhu Fu, Yanwu Xu, Min Xu, and Yueming Jin. Medical sam adapter: Adapting segment anything model for medical image segmentation. Medical image analysis, 102:103547, 2025. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575, 2023. Kaiyuan Yang, Fabio Musio, Yihui Ma, Norman Juchler, Johannes Paetzold, Rami Al-Maskari, Luciano Hoher, Hongwei Bran Li, Ibrahim Ethem Hamamci, Anjany Sekuboyina, et al. Benchmarking the cow with the topcow challenge: Topology-aware anatomical segmentation of the circle of willis for cta and mra. ArXiv, pages arXiv2312, 2024. Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. arXiv preprint arXiv:2311.11969, 2023. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. Yichi Zhang, Jing Wang, Tan Pan, Quanling Jiang, Jingjie Ge, Xin Guo, Chen Jiang, Jie Lu, Jianning Zhang, Xueling Liu, et al. Nasalseg: dataset for automatic segmentation of nasal cavity and paranasal sinuses from 3d ct images. Scientific Data, 11(1):1329, 2024. Jianwei Zhao, Fan Yang, Xin Li, Zicheng Jiao, Qiang Zhai, Xiaomeng Li, De Wu, Huazhu Fu, and Hong Cheng. Segmic: universal model for medical image segmentation through in-context learning. Pattern Recognition, page 112179, 2025."
        },
        {
            "title": "A Experiments",
            "content": "A.1 Data description We provide detailed data descriptions in Table 3, covering 25 tasks. For each dataset and task, we split the data into 80% for evaluation (test set) and 20% as calibration set. It is important to note that the sizes reported refer to the number of 3D images. Since our study conducts comparisons in 2D setting for consistency, each 3D image is further divided into hundreds of slices. A.2 Additional explanation for model selection Since our model merging framework is based on the assumption of same loss basins, models with differing architectures are not considered as candidate models. For example, SAM-Med2D Ye et al. (2023) employs an adapter-based fine-tuning approach, which introduces architectural modifications that deviate from the base structure. Therefore, we select SAM as the base model and use MedSAM and MedicoSAM, both fine-tuned variants with consistent architectures, as candidate models for merging. 15 Table 3: Detailed description of data across 25 tasks. idx Task Name dataset Type 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Brain Tumor Vascular Cerebral Cortex Hippocampus Thalamus Lateral Ventricle Putamen Amygdala FLARE22 Liver FLARE22 Kidney FLARE22 Kidney Maxillary Sinus Nasal Cavity Nasal Pharynx Prostate Mice-Lung Mice-Pancreas Cardiac FLARE22 Spleen FLARE22 Pancreas RAOS Liver RAOS Kidney RAOS Stomach Optic Cup Optic Disk Brats Topcow ADNI ADNI ADNI ADNI ADNI ADNI FLARE22 FLARE22 FLARE22 Nasal Nasal Nasal Promise12 Mice Mice MSD-Heart FLARE22 FLARE22 RAOS RAOS RAOS Fundus Fundus 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 2D 2D Organ Brain Brain Brain Brain Brain Brain Brain Brain Liver Kidney Kidney Nasal Nasal Nasal Prostate Lung Pancreas Cardiac Spleen Pancreas Liver Kidney Stomach Fundus Fundus Modality Scans T1, T2 MRA T1 T1 T1 T1 T1 T1 CT CT CT CT CT CT T2 CT CT CT CT CT CT CT CT OCT OCT 400 90 400 400 400 400 400 400 50 50 50 120 120 120 50 40 40 18 50 50 262 262 262 101 101 A.3 SMAC sampling process SMAC Lindauer et al. (2022) employs random forests (RF) as surrogate models to guide the configuration sampling process. The workflow operates as follows: (1) Surrogate Model Training: Random forests are trained to predict model performance based on previously evaluated configurations, including layer-wise merging selections and model combination strategies. (2) Acquisition-based Sampling: Leveraging the predictions and uncertainty estimates from the surrogate model, SMAC samples new configurations that balance exploitation (favoring configurations with high predicted performance) and exploration (targeting configurations with high predictive uncertainty). (3) Iterative Refinement: As additional configurations are evaluated, the surrogate model is incrementally updated, progressively enhancing its predictive accuracy and enabling more effective sampling in subsequent iterations. A.4 Additional comparison In addition to the baselines presented in the main document, we provide supplementary comparison results with the ICL-based model Neuralizer Czolbe and Dalca (2023) and the SAM-based model SAM-Med3D Wang et al. (2024). The detailed results are reported in Section B-Results. A.5 Descriptions of Existing Model Merging Methods A.5.1 Task Arithmetic Task Arithmetic augments model capabilities by linearly combining task-specific knowledge through vector operations. Given pre-trained model with weights θpre and set of fine-tuned task weights θft t=1, the task vectors are computed as: 16 Table 4: Additional comparison results in terms of Dice coefficient across 25 tasks. idx Task Name ICL Neuralizer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Brain Tumor Vascular Cerebral Cortex Hippocampus Thalamus Lateral Ventricle Putamen Amygdala FLARE22 Liver FLARE22 Kidney FLARE22 Kidney Maxillary Sinus Nasal Cavity Nasal Pharynx Prostate Mice-Lung Mice-Pancreas Cardiac FLARE22 Spleen FLARE22 Pancreas RAOS Liver RAOS Kidney RAOS Stomach Optic Cup Optic Disk Avg. 6.54 8.19 67.14 51.53 71.76 61.80 62.08 46.24 62.64 58.97 55.80 74.31 56.45 76.76 71.30 66.50 40.16 41.63 43.75 5.76 72.95 58.51 27.32 73.65 93.74 54.22 SAM SAMMed3D 46.34 6.01 9.15 30.75 19.11 30.00 14.03 11.02 70.62 84.83 88.33 39.75 15.07 48.66 62.79 21.21 8.40 48.33 86.26 22.09 60.31 30.42 36.18 - - model merging baselines MedSAMix-M SLERP TA TIES Linear run run2 run3 74.14 52.91 55.85 59.59 53.38 76.93 36.30 46.20 92.79 95.15 94.47 79.01 60.16 86.57 89.16 79.26 87.43 76.95 95.03 77.51 90.54 71.31 86.65 74.35 90.46 77.89 34.73 52.27 49.66 67.63 48.83 38.09 49.90 92.40 92.99 92.45 74.05 46.97 83.02 86.83 65.87 85.35 74.77 94.06 71.47 89.68 73.28 85.74 83.80 92.95 68.56 41.47 43.50 40.62 44.53 60.26 27.48 36.80 90.38 92.44 91.35 56.36 46.77 82.73 80.48 66.98 83.34 77.20 91.98 65.07 87.62 52.65 78.10 64.76 86.33 78.12 43.79 54.19 52.84 66.29 64.12 37.95 51.20 93.61 94.45 94.14 75.91 52.34 87.08 88.36 68.91 86.54 79.41 95.04 75.11 90.02 71.78 86.39 80.69 94. 76.20 74.90 53.70 62.90 55.60 54.90 55.90 55.00 69.10 67.30 75.10 77.60 37.40 32.60 53.40 51.50 92.20 90.80 95.40 95.20 94.50 94.20 80.50 80.50 62.80 64.70 87.60 89.20 91.90 91.50 72.70 78.00 89.00 88.40 84.00 84.20 93.80 94.30 77.80 78.00 90.10 90.00 73.70 72.30 87.50 86.70 86.30 84.60 94.10 93.70 75.20 62.90 55.00 57.50 67.60 77.60 33.60 52.00 92.90 95.10 94.20 79.60 65.30 88.80 90.60 75.40 88.40 83.00 94.80 78.40 89.80 72.70 86.00 82.10 94.70 38.68 75.28 72.19 66. 74.49 77.40 77.14 77."
        },
        {
            "title": "The final merged model weights are derived by applying a weighted sum of these task vectors to the base",
            "content": "τt = θft θpre (7) model: θMerge = θpre + λ (cid:88) t=1 τt (8) where λ serves as scaling factor, controlling the extent of task-specific adaptation. A.5.2 TIES-Merging TIES-Merging resolves parameter conflicts through three-step procedure. First, for each task vector τt, the top k% of parameters with the largest magnitudes are selected: Next, consensus sign vector ˆγ is derived by aggregating the directional trends of parameter changes ˆτt = TopK(τt, k) (9) across tasks: ˆγ = sgn (cid:32) (cid:88) (cid:33) ˆτt t=1 17 (10) Finally, the method computes an averaged task vector τ by considering only those task-specific updates whose signs align with the consensus direction: The merged model weights are then obtained by adding the scaled consensus update to the base model: τ = Average(ˆτt : sgn(ˆτt) = ˆγ) (11) θMerge = θpre + λ τ (12) A.5.3 SLERP SLERP (Spherical Linear Interpolation) computes the shortest path on the hypersphere between two sets of model weights, ensuring smooth interpolation in parameter space. Given two models with weights θ1 and θ2, the interpolated weights at position [0, 1] are calculated as: SLERP(θ1, θ2, t) = sin((1 t)ω) sin(ω) θ1 + sin(tω) sin(ω) θ2 (13) where ω = arccos (cid:17) (cid:16) θ1,θ2 θ1θ2 represents the angle between the two weight vectors. A.5.4 Linear Merging Linear Merging performs simple weighted average of model weights. Given set of models with weights θt t=1, the merged weights are computed as: where the weights satisfy (cid:80)n t=1 wt = 1 and wt 0, ensuring convex combination. θLinear = (cid:88) t=1 wtθt (14) A.6 Hyperparameters for model merging candidate methods For Task Arithmetic and TIES-Merging, we set the scaling factor controlling the magnitude of task-specific updates within the range [0.0, 1.0]. In Linear Merging, the scaling factors are uniformly assigned as 1/n, where denotes the number of models being merged. Additionally, for TIES-Merging, we specify ratio to retain parameters with the largest magnitudes, choosing from [0.0, 1.0]."
        },
        {
            "title": "B Results",
            "content": "B.1 Additional results We provide additional comparison results in Table 4, including the ICL-based model Neuralizer, the SAMbased model SAM-Med3D, model merging baselines, and multiple runs of our proposed MedSAMix. We can see that (1) SAM-Med3D underperforms SAM-Med2D across the 25 tasks. This is primarily because SAM-Med2D is fine-tuned on broader range of datasets, enhancing its generalization capability. In contrast, SAM-Med3D is limited by the scarcity of 3D training data, which allows it to excel in specific tasks such as kidney segmentation but leads to poor generalization in other domains. (2) Baseline model merging methods exhibit inconsistent performance across tasks, indicating their sensitivity to task-specific variations. This 18 Table 5: Architecture of our merged MedSAMix-M Layer Patch Embedding Pos Embedding Encoder Transformer-0 Encoder Transformer-1 Encoder Transformer-2 Encoder Transformer-3 Encoder Transformer-4 Encoder Transformer-5 Encoder Transformer-6 Encoder Transformer-7 Encoder Transformer-8 Encoder Transformer-9 Encoder Transformer-10 Encoder Transformer-11 Prompt Encoder-0 Prompt Encoder-1 Prompt Encoder-2 Prompt Encoder-3 Neck Mask decoder layer-0 Mask decoder layer-1 Mask decoder layer-2 Mask decoder layer-3 Mask decoder layer-4 Mask decoder layerMethod TIES SLERP TIES TA Linear TA SLERP TA SLERP TA SLERP Linear TIES Linear Linear Linear SLERP TIES SLERP TIES/TA Linear/SLERP Retain ratio Scaling SAM MedicoSAM MedSAM 0.50 - 0.59 - - - - - - - - - 0.50 - - - - 0.18 - 0.50 - 0. 0.30 - 0.06 - 0.06 - 0.23 - - 0.50 - - - - 0.55 - - - - - - 0.59 - - 0.60 0.59 - 0.93 - 0.95 - 0.77 0.95 - 0.44 0.40 0.50 0.50 - 0. - - - - - - 0.54 - 0.30 0.50 0.50 0.50 - - - 0.55 - - 0.12 - 0.90 - 0.75 - 0.67 0.57 - 0.01 0.50 0.50 - - 0. inconsistency underscores the limitations of traditional merging approaches that apply uniform strategies across all layers, thereby highlighting the advantage of our MedSAMix, which adopts flexible, task-adaptive merging mechanism. (3) Since our approach selects configurations based on Pareto-optimal solutions, we further compare the top three solutions (run-1, run-2, run-3). As shown in Table 4, the performance differences among these runs are minimal. MedSAMix consistently achieves stable performance across different optimization runs. Although it is challenging to guarantee the absolute optimal solution in each run, the results remain robust as long as the selected calibration tasks are representative. B.2 Merged architecture We present the merged architecture of MedSAMix-M in Table 5. The results show that different layers adopt different merging strategies, which is key factor contributing to the superiority of our method. This hierarchical combination allows the model to merge in direction that is more advantageous for overall performance, enabling more adaptive and effective integration compared to uniform merging approaches."
        }
    ],
    "affiliations": [
        "ELLIS Institute Tubingen, Germany",
        "German Center for Mental Health (DZPG), partner site, Jena & Tubingen, Germany",
        "Harbin Institute of Technology, Shenzhen, China",
        "Max Planck Institute for Intelligent Systems, Germany",
        "Peng Cheng Laboratory, Shenzhen, China",
        "Tubingen AI Center, Germany",
        "University of Tubingen, Germany"
    ]
}