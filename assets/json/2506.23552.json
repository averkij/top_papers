{
    "paper_title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "authors": [
        "Mingi Kwon",
        "Joonghyuk Shin",
        "Jaeseok Jung",
        "Jaesik Park",
        "Youngjung Uh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 5 5 3 2 . 6 0 5 2 : r JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching Mingi Kwon* Yonsei University kwonmingi@yonsei.ac.kr Joonghyuk Shin* Seoul National University joonghyuk@snu.ac.kr Jaeseok Jeong Yonsei University jete_jeong@yonsei.ac.kr Jaesik Park Seoul National University jaesik.park@snu.ac.kr Youngjung Uh Yonsei University yj.uh@yonsei.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports wide array of conditioning inputsincluding text, reference audio, and reference motionfacilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing practical solution for holistic audio-visual synthesis. project page"
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of generative models [16], the synthesis of realistic human faces and voices has become increasingly sophisticated. Two major fields have emerged from this trend: talking head generation [711], which animates static portrait images to mimic facial expressions, and text-tospeech (TTS) synthesis [1216], which converts text and short voice reference into natural-sounding speech. While state-of-the-art methods in both domainsranging from GAN-based models [7, 17] for fast inference to diffusion-based models [8, 16] and flow matching-based models [18, 19] for higher fidelityhave made remarkable progress, these two problems have traditionally been treated as separate tasks. Yet in natural human communication, facial motion and speech are deeply interwoven. Movements of the mouth, cheeks, and jaw are not merely visual artifacts but integral components of spoken language. Surprisingly, despite this intrinsic connection, no prior work has jointly addressed talking head generation and speech synthesis in unified model. Existing talking head models typically treat audio as unidirectional condition, while TTS systems remain blind to facial dynamics. In this work, we introduce the first architecture that can simultaneously model, generate, and condition on both modalitiesaudio and facial motionwithin single flow matching-based framework. Our design is inspired by the Multi-Modal Diffusion Transformer (MM-DiT) [20, 21], where we *Equal Contribution, Equal Advising Figure 1: Overview of our JAM-Flow framework for flexible and joint generation of facial motion and speech. The model accepts diverse input combinations, including text, reference motion, and reference audio. These are processed by our novel Motion & Audio Joint MM-DiT, which enables synchronized synthesis of full audio-visual outputs supporting tasks like talking head generation from text, audio-driven animation, and cross-modal reconstruction (e.g., audio from motion). integrate two specialized flow matching models: Motion-DiT for generating implicit facial keypoint sequences, and an Audio-DiT for denoising mel-spectrograms from text and reference speech. These modules are coupled through selective joint attention layers, where only half the layers are fused, allowing effective cross-modal communication while preserving the benefits of modality-specific representations. Furthermore, to inject structural inductive biases into the model, we incorporate rotary positional embeddings (RoPE) [22] with task-specific engineering improvements and attention masking to restrict temporal receptive fields. To efficiently leverage pre-existing knowledge, we initialize the Audio-DiT with high-quality pretrained TTS model (F5-TTS), while training Motion-DiT from scratch using the LivePortrait [7] framework to represent facial motion via compact keypoints. Training is conducted in two stages: the modules are first trained separately, and then fine-tuned jointly with shared attention layers and inpainting-style supervision, enabling robust and flexible generation from partially missing inputs. Our models design leads to range of novel capabilities not supported by prior systems: 1) Given single portrait image and text, it can generate synchronized facial motion and speech; 2) It can be used as talking head model, driven solely by audio; 3) It can reconstruct audio from motion, or motion from audio; and 4) It supports driving by video, with or without audio, retaining the benefits of previous LivePortrait-based pipelines. All of these are possible within single architecture, with no need for task-specific heads or separate pipelines. We argue that this unified design is not only elegant but also highly practical for future real-world applications where multi-modal synthesis and control are essential. As illustrated in Figure 1, our model supports wide range of input-output configurationsincluding talking head generation, TTS, and cross-modal reconstructionwithin single coherent framework. Our contributions are summarized as follows: 1. We propose the first joint architecture for talking head and TTS generation, enabling mutual conditioning across both modalities. 2. We introduce novel MM-DiT-based framework with partial joint attention, RoPE integration, and attention masking tailored for multi-modal flow matching. 3. Our inpainting-based training and flexible conditioning design allow wide range of generation scenarios, including motion-from-text, audio-from-motion, and more."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Flow Matching and Multimodal Diffusion Transformer Flow matching [4, 5, 20] enhances generative modeling efficiency over score-based diffusion models [2, 3]. It learns continuous transformation Zt between distributions via an Ordinary Differential 2 Equation (ODE): dZt/dt = vθ(Zt, t), where vθ is learnable vector field. The objective is to match vθ to target velocity field ut(x). Conditional Flow Matching (CFM) [5] and Rectified Flow (RF) [4] simplify this: for paired samples x0 π0 and x1 π1, they define an intermediate point via linear interpolation, xt = (1 t)x0 + tx1, and set the target velocity ut(xt) to be the difference x1 x0. This results in the CFM loss: LCFM(θ) = Et,x0,x1 (cid:2)vθ(xt, t) (x1 x0)2(cid:3) . (1) The Multimodal Diffusion Transformer (MM-DiT) [20] builds on flow matching for joint multi-modal generation. It uses separate DiT [21] branches per modality, fused via joint self-attention for crossmodal interaction, enabling scalable and expressive generation. Large-scale MM-DiTs like Stable Diffusion 3 [20], Flux [23], and CogVideoX [24] achieve SOTA in various tasks (e.g., text-to-image, video synthesis), but none have jointly synthesized lip motion and audio. 2.2 Talking Head Heneration Talking head generation synthesizes realistic facial animations from conditional signals, mainly through video-driven (facial reenactment) or audio-driven methods. Video-driven approaches [79] animate source portrait using motion cues from driving video. Early methods often used facial landmarks [9] or 3D models [25], while recent works like LivePortrait [7] use implicit keypoints and X-portrait [8] employs diffusion control. In contrast, audio-driven methods [1215] create lip movements synchronized with input audio. Pioneering work like Wav2Lip [12] focused on lip-sync accuracy with GAN, while later methods (e.g., EMO [14], Omni-Human [16]) often use diffusion models for enhanced expressiveness. Current methods typically model uni-directional audio-to-visual flow. However, facial motion and speech are mutually influential in real conversations. Our hybrid approach addresses this by combining joint diffusion-based generation of keypoints and audio with efficient pixel decoding via LivePortrait, enabling bidirectional information exchange and flexible generation. 2.3 Neural Text-to-Speech Generation Neural text-to-speech (TTS) has progressed from attention-based sequence-to-sequence models [26] to more advanced architectures. Early non-autoregressive systems [27, 28] enhanced efficiency via explicit duration modeling. significant shift towards zero-shot voice cloning was pioneered by neural codec language models [29], which autoregressively generate speech tokens from minimal reference audio, despite some inference challenges. Diffusion-based methods have since emerged as powerful paradigm, particularly for zero-shot voice cloning. These include approaches demonstrating high-quality speech via diffusion [3033] and flow matching for efficient text-guided speech infilling [18]. Recent efforts focus on refining speech-text alignment and moving from explicit duration to more flexible frameworks [34, 35]. F5-TTS [18] advances this with flow matching and Diffusion Transformers (DiTs) for efficient non-autoregressive generation. While research continues to address alignment robustness, prosody, and efficiency [19], the simultaneous generation of speech and matching lip motion remains largely unaddressed. 2.4 Automated Video Dubbing Automated video dubbing synthesizes speech from text and video inputs, focusing on aligning generated speech with existing visual content, unlike video generation from audio/text. It extends textto-speech (TTS) by conditioning on visual context, especially lip movements and facial expressions. The main challenge is ensuring temporal synchronization and reflecting visual expressiveness in the synthesized speech. Prior work [3638] has explored aligning visual cues, multi-scale style learning, and audio-visual fusion. Notably, our model, though not explicitly optimized for this task, shows an emergent capability for generating speech well-aligned with lip movements in given videos. 3 Figure 2: LivePortrait framework and mouth-related expression keypoint analysis. LivePortraits motion encoder (Em) infers parameters including 3D expression deformation R213 for 21 canonical keypoints. We find that deforming approximately four specific keypoints (highlighted) primarily dictates mouth articulation. Our Motion-DiT leverages this by generating only the deformation components (emouth e) for these crucial mouth keypoints, enabling efficient lip-sync."
        },
        {
            "title": "3 Preliminaries",
            "content": "3.1 LivePortrait Framework LivePortrait [7] enables image-to-video synthesis by disentangling structural and appearance features from single input image. It employs an appearance encoder Ea to extract global appearance feature app RCDH . In parallel, motion encoder Em infers set (21) of 3D implicit facial keypoints R213 which is parametrized by canonical keypoints xc R213, pose matrix R33, expression deformation R213, scale and translation R213. The final keypoints are then computed as: = (xcR + e) + t. The warping module modifies app with esimated keypoint differences and the decoder projects this warped appearance feature app into target video frame. Visualizing the 21 dimensions of the expression embedding (Figure 2) reveals that approximately four specific dimensions consistently control the mouth region. Isolating these mouth-related components emouth and freezing others modifies only the lip shape. This empirical finding suggests lip-sync generation can be simplified by modeling only this small subset of expression dimensions. We leverage this by training our motion generation module to predict only these mouth-related components, which are then combined with fixed identityand pose-related features for rendering. 3.2 F5-TTS and Conditional Flow Matching F5-TTS [18] is Conditional Flow Matching (CFM) based text-to-speech model. Inspired by inpainting-based approaches such as VoiceBox [33], F5-TTS treats speech generation as mask-andpredict problem, where parts of the mel-spectrogram are randomly masked during training and then reconstructed conditioned on surrounding context and reference signals. Formally, the model receives masked audio segment amasked and conditioning vector ctext consisting of unmasked regions, reference audio features, and text embeddings. These are concatenated and passed through flow-based network trained using the CFM objective described in Eq. 1. This inpainting formulation has two key benefits. First, it enables robust training over diverse conditioning scenarios by randomly varying the masked regions. Secondand crucial for our joint generation taskit naturally accommodates reference audio conditioning, since the network learns to reconstruct missing regions in speaker-consistent manner using available context. As result, F5-TTS can generate high-quality, voice-consistent speech for arbitrary text inputs, and serves as strong foundation for our Audio-DiT module. 4 Figure 3: The training and inference pipeline of the JAM-Flow framework. Our joint MM-DiT comprises Motion-DiT for facial expression keypoints (emouth) and an Audio-DiT for mel-spectrograms (a), coupled via joint attention. The model is trained with an inpainting-style flow matching objective on masked inputs and various conditions (text, reference audio/motion). At inference, it flexibly generates synchronized audio-visual outputs from partial inputs."
        },
        {
            "title": "4 Method",
            "content": "4.1 Overview Our goal is to simultaneously generate temporally aligned speech audio and facial motion from multimodal inputs (text, reference audio, or motion). To this end, we propose dual-stream diffusion architecture composed of Audio-DiT and Motion-DiT, partially fused via joint attention blocks. Figure 3 illustrates the overall architecture with training and inference pipeline. 4.2 Motion-DiT Design The Motion-DiT generates expression embeddings emouth RTframe43 that control lip motion, following the observation from Section 3.1 that 4 of the 21 expression dimensions are sufficient to model mouth dynamics. As shown in Figure 3, we provide two inputs to the motion stream: the target expression excluding mouth-related components, denoted erest RTframe173 and the audio-derived conditioning features faudio from the Audio-DiT stream (if available). During training, the Motion-DiT denoises corrupted emouth vectors via conditional flow-matching process: Lmotion = emouth ,emouth 1 ,t (cid:2)vθ(emouth , t; audio, emasked, erest) (emouth emouth 0 )2(cid:3) (2) where et denotes the noisy input at step t, and vθ is the velocity predicting DiT. 4.3 Audio-DiT and Flow Matching The Audio-DiT generates mel-spectrograms amel RTmeldmel using the Conditional Flow Matching (CFM) objective. Following F5-TTS, we apply random inpainting masks to audio segments and condition on reference audio and text features. Additionally, we adapt the motion-derived conditioning features motion from the Motion-DiT stream (if available). As defined in Eq. (1), the model learns to reconstruct missing regions from context: Laudio = Et,a0,a1 (cid:2)vθ(at, t; motion, amasked, ctext) (a1 a0)2(cid:3) (3) This enables the Audio-DiT to flexibly operate under partial or complete conditions and naturally generalize to reference audio-based speech generation. 5 4. Joint Attention and Temporal Fusion To enable cross-modal interactions, we introduce Njoint layers of joint attention between the audio and motion streams, as illustrated in Figure 3. Tokens from both streams are fused via joint-attention blocks, while the rest of the transformer layers remain modality-specific. To align temporal semantics between modalities, we apply scaled rotary positional embeddings (RoPE). Let La and Lm be the lengths of audio and motion sequences, respectively. We scale the position index in RoPE such that: pscaled = p/L Lref where Lref = max(La, Lm) ensures both modalities share compatible temporal embeddings in joint layers. Notably, we observed that without this RoPE alignment, the model fails to train entirely, underscoring the critical role of compatible temporal embeddings in joint attention. 4.5 Attention Masking Strategy To enable effective joint modeling of audio and motion, we adopt carefully designed attention masking strategy that respects the temporal dynamics and functional roles of each modality. As illustrated in Figure 3, we apply asymmetric masking to Audio-DiT and Motion-DiT to promote modality-specific learning while ensuring coherent cross-modal alignment. For Motion-DiT, we apply local self-attention mask with fixed temporal window size Nwindow, reflecting the fact that facial motion is primarily influenced by temporally adjacent frames. Moreover, each motion token attends only to audio tokens within the same local window, enforcing temporally aligned cross-modal attention. Importantly, audio-to-audio attention is disabled during motion generation, ensuring that motion tokens rely solely on localized audio cues rather than global audio semantics. Conversely, for Audio-DiT, we disable self-attention among motion tokens and restrict cross-modal attention to only those motion tokens that share the same timestamp, thereby preserving temporal alignment. Audio tokens retain full global self-attention, allowing them to integrate information from text and speaker embeddings across the entire utterance. This modality-specific masking is crucial: without it, we observed that the audio and motion pathways tend to diverge, leading to degraded synchronization and incoherent outputs. Our masking strategy ensures tight temporal coupling between modalities while respecting their distinct generation patterns. 4.6 Training Procedure Training is conducted in two stages: Stage 1: We freeze the pretrained Audio-DiT from F5-TTS and train the Motion-DiT from scratch with joint attention layers inserted. This setup allows Motion-DiT to learn features that align with and support the fixed audio representations, while maintaining architectural compatibility (e.g., layer depth, RoPE scale, masking) for stage 2. Notably, even when the Audio-DiT is frozen, the output audio changes as audio loss still backpropagates to update the motion branch parameters, and these updated motion branch parameters affect audio branchs attention internally. Stage 2: We jointly train both DiTs using shared joint attention blocks. Gradients flow between modalities during joint layers, enabling mutual refinement of representations. Note that text conditioning is injected via the Audio-DiT, while the Motion-DiT consumes both exprest and intermediate audio features audio."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We train our model on the CelebV-Dub [38] dataset, which is filtered from CelebV-HQ [48] and CelebV-Text [49]. The training involves two stages: first, the Motion-DiT is trained from scratch using preprocessed keypoint representations from video frames, while the Audio-DiT is initialized from pretrained F5-TTS checkpoints. After these individual models converge, joint training is conducted on paired audio-visual data. All experiments were performed on 4 NVIDIA RTX 6000 Ada GPUs, with single model variant (encompassing both training stages) taking approximately one day to 6 Table 1: Comparison of talking head generation quality on HDTF [39] dataset. Method FID FVD LSE-C LSE-D Ground Truth - - SadTalker [17] DreamTalk [40] AniPortrait [41] Hallo [42] Hallo3 [43] Ours (I2V) Ours (V2V) 22.340 78.147 26.561 20.545 20.359 17.571 11.633 203.860 890.660 234.666 173.497 160.838 192.30 25.07 8.70 7.885 6.376 4.015 7.750 7.252 7.324 8.086 6.597 7.545 8.364 10.548 7.659 8.106 7.777 7. Table 2: Comparison of text-to-speech performance in LibriSpeech-PC testclean [18, 44] benchmark. Method Ground Truth Vocoder Rec. Cosy Voice [45] FireRedTTS [46] E2 TTS [35] F5-TTS [18] MegaTTS 3 [19] Ours Ours WER SIM-o 2.23% 2.32% 0.69 0.66 0.66 3.59% 0.47 2.69% 0.69 2.95% 0.66 2.42% 2.31% 0.70 4.91% 3.38% 0.64 0.63 Table 3: Comparison of automated video dubbing performance in CelebV-Dub [38] dataset. Method LSE-C LSE-D WER spkSIM Ground Truth Zero-Shot TTS [47] HPMDubbing [36] StyleDubber [37] VoiceCraft-Dub [38] Ours 6.73 2. 6.36 3.78 6.05 3.43 7.44 11.68 7.80 10.40 8.33 10.56 4.15% 3.83% - 0.316 24.06% 0.146 0.264 9.48% 0.333 7.01% 6.39% 0.410 train. Our training data features videos up to 30 seconds in length (with much lower average), and we observed optimal performance when generating around 20 seconds of video. Consequently, for evaluation, longer videos are segmented into 20-second clips. For all evaluations, we consistently applied 32 Number of Function Evaluations (NFE), in line with F5-TTS, conducting tests on the CelebV-Dub test splits and the HDTF [39] dataset. Our quantitative assessment relied on established protocols: Word Error Rate (WER ) and reference audio similarity (SIM-o ) were measured following F5-TTS [18]; Lip Sync Error confidence (LSE-C ) and distance (LSE-D ) were determined using Wav2Lip [12] methodologies; and speaker similarity (spkSIM ) was assessed according to VoiceCraft-Dub [38]. Additionally, Fréchet Inception Distance (FID ) [50] and Fréchet Video Distance (FVD ) [51] were computed using community standard implementations. 5.2 Quantitative Evaluation In all tasks, our model benefits from an inpainting-based training paradigm, which enables flexible conditioning strategies. Depending on the task, we either generate both modalities jointly, provide clean inputs for one side while generating the other, or supply partial references as clues. This flexibility allows us to tailor the conditioning inputs to each tasks specific requirementsfor example, using full expression keypoints in video-to-video generation, or fixed text and video for automated dubbing. Talking head generation. We evaluate talking head performance on HDTF using four key metrics: FID, FVD, LSE-C, and LSE-D. As shown in Table 1, our model performs competitively or better with SOTA methods such as SadTalker, AniPortrait, and Hallo3. Notably, our method is primarily designed with video-to-video (V2V) setup in mind, utilizing sequence of 17 non-mouth expression keypoints as clue (following [52]). This keypoint clue is used in both V2V and image-to-video (I2V) configurations: V2V warps frames sequentially from source video, whereas I2V consistently warps an initial source image. 7 Table 4: Ablation study on the number of joint attention blocks, motion attention masking, and Audio-DiT finetuning. # Joint Blocks Attn. Mask LSE-D WER Train Audio-DiT LSE-C SIM-o FID 22 (Full) 11 (Half) 11 (Half) 11 (Half) 11 (Half) X 5.759 5.735 5.748 5.747 5.662 4.81 4.64 6.44 5.76 6.45 8.40 9.07 7.99 8.22 7.73 6.88% 7.25% 7.93% 6.76% 7.28% 0.62 0.62 0.61 0.63 0.64 Text-to-speech generation. TTS performance is evaluated using WER and SIM-o on LibriSpeechPC test-clean [18, 44]. As shown in Table 2, we observe slight degradation in both WER and SIM-o scores, which we primarily attribute to the characteristics of our training dataset: CelebV-Dub. CelebV-Dub utilizes the Whisper ASR model [53] to generate pseudo-captions and Spleeter [54] to suppress background music. Unlike foundational TTS models trained on extensive ground truth (GT) data, our models training on these pseudo-captions and demuxed audio likely contributes to this performance decrease. This hypothesis is supported by results from variant of our model (Ours) with frozen Audio-DiT and no motion attention maska configuration similar to the original F5-TTSwhich indeed achieves better TTS metrics. Automated video dubbing. Thanks to the inpainting-based training paradigm, our model naturally extends to the automated video dubbing task. Given video and target text, the model generates speech that aligns well with the speakers lip movements. Unlike the prior paragraph that performs lip-syncing by adapting pre-recorded speech to match given video, automated video dubbing requires generating speech that is both semantically correct and temporally aligned with the speakers lip movements. In this setting, we observed that commonly used metrics such as LSE-C and LSE-D are insufficient for accurately evaluating the quality of generated audio. As discussed by [55, 56, 38], SyncNet [57] is known to be sensitive to factors such as spatial translation (lacking shift invariance) and variations in audio/video codecs and compression. We specifically observed that SyncNetderived metrics (LSE-C, LSE-D) can exhibit instability with codec alterations and, crucially, with the text-to-speech (TTS) output generated by our model. We refer readers to our supplementary materials for qualitative examples and further analysis. Despite the limitations of current metrics, our model consistently produces speech that matches the visual articulation, and achieves strong WER performance  (Table 3)  , supporting its effectiveness in this challenging task. 5.3 Qualitative Results We refer readers to the supplementary materials for video examples that showcase both audio and visual outputs. Qualitatively, our model demonstrates strong performance across various settings involving joint audio-video generation. First, when evaluated on traditional lip-sync tasks, the model generates lip motion that aligns well with the given audio. Second, in automated video dubbing scenarioswhere video and target text are giventhe synthesized speech exhibits high temporal consistency with the visual articulation. Third, even when modifying only the text while keeping the input video fixed, the generated audio adapts to the speakers lip motion as much as possible, demonstrating the models ability to infer and resolve timing ambiguities. Furthermore, we show that the model can be used to re-dub video using different speakers voice while preserving the original lip motion, or conversely, to make the same speaker utter entirely new content while maintaining voice identity. These flexible capabilities illustrate the models robustness and controllability across range of dubbing tasks. Interestingly, we also observed that when only lip motion is providedwith no reference audio or speaker embeddingthe model can still generate plausible speech, suggesting that it has learned strong implicit mapping between visual articulation and speech acoustics through its inpainting-based training. 5.4 Ablation Studies To assess the impact of our design choices, we conduct controlled ablation experiments along three axes: (1) the degree of joint attention between audio and motion streams, (2) the presence of temporal 8 attention masks in the Motion-DiT, and (3) the finetuning of the Audio-DiT during stage-2 training. Given the aforementioned instability of LSE-C and LSE-D with generated audio, and to better assess commonly adopted audio-driven talking head setup, we calculate LSE-C and LSE-D in the ablation study using generated motion paired with ground truth (GT) audio. WER and SIM-o are computed using both generated motion and audio, while LSE-C and LSE-D use GT audio for stability. Joint Attention Configuration. We compared Full Joint configuration (all DiT layers share attention) with our Half Joint approach (only earlier layers fused). Although Table 4 indicates numerically better scores for the Full Joint setup, we found this configuration to be unstable during training. Based on our internal qualitative evaluations, which consistently showed the Half Joint configuration yielding more consistent and reliable results, we adopted it for all subsequent ablation studies. Attention Masking. We further test the importance of temporal attention masks by removing them entirely. The No Masking variant shows sharp drops in Sync-C and increases in Sync-D, indicating that unconstrained temporal attention weakens the motion-audio alignment. The resulting facial motions are temporally inconsistent and poorly synchronized with speech. Audio-DiT Finetuning We also evaluated the impact of finetuning the Audio-DiT (stage 2), motivated by the desire to preserve the strong prior knowledge in the pretrained F5-TTS. As results in Table 2 and Table 4 suggest, finetuning the Audio-DiT can slightly increase WER, potentially due to exposure to noisier training data from CelebV-Dub. However, allowing the Audio-DiT to adapt during joint training generally leads to improved lip-sync (LSE-C/D) and more active co-adaptation between audio and motion for better overall synchronization. Consequently, our final model variant incorporates half joint blocks, attention masking, and finetuned Audio-DiT."
        },
        {
            "title": "6 Discussion and Ethics",
            "content": "Our experiments show that partial joint attention and localized temporal masking are key to stable and coherent multimodal generation. While full joint attention yields slightly better scores, it often results in unstable training. Our hybrid design balances cross-modal fusion and modality-specific representations, contributing to both robustness and performance. We also observed that the generated speech often reflects emotional cues from facial motion. For instance, smiling motions tend to produce brighter, higher-pitched voicesdespite the absence of explicit emotion supervision. This suggests the model implicitly aligns emotion across modalities, opening possibilities for expressive and emotionally-aware generation. major strength of our model is its versatility. It supports diverse input configurations (e.g., audioonly, motion-only, text + portrait) within single framework, unlike prior models limited to either talking head synthesis or TTS. This flexibility enables applications such as expressive dubbing, silent video revoicing, and adaptive avatar generation. The JAM-Flow model also raises important ethical considerations. While it has clear benefits for accessibility, virtual avatars, and creative tools, it can potentially be misused to generate deceptive content such as deepfakes or amplify biases present in the training data. To mitigate such risks, future work should explore safeguards like robust watermarking techniques for synthetic media attribution. Access to the model and its derivatives will be restricted to academic use under ethical guidelines, with continued monitoring and research into responsible deployment as the technology evolves."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented unified flow-matching-based architecture for joint speech and facial motion generation, capable of handling diverse conditioning inputs including text, audio, and video. Our design combines modality-specific DiT modules with selectively applied joint attention and aligned positional embeddings and attention masking, enabling coherent multimodal synthesis without requiring separate pipelines. Through extensive experiments, we demonstrate that our model performs competitively with specialized state-of-the-art systems across both talking head and TTS tasks. Moreover, it uniquely 9 supports flexible and compositional inference scenarios such as motion-to-audio, audio-to-motion, and full generation from text and image alone. While these results are promising, current limitations mostly stem from the data and compute resources utilized. Our training data, CelebV-Dub [38], for instance, contains inaccurate captions made by Whisper as well as potentially artifact-laden demuxed audio. Another limitation arises from LivePortraits capabilities; while fast, its performance is mostly constrained to facial regions, and complex motions may not be fully captured. Despite these challenges, we believe our findings offer compelling case for multimodal joint modeling. Utilizing more curated datasets along with stronger video diffusion models as backbone [24, 58, 59] is promising direction to further enhance performance and open new possibilities for unified audio-visual generation in real-world applications such as dubbing, virtual avatars, and human-computer interaction."
        },
        {
            "title": "References",
            "content": "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets, 2014. [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. [3] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations (ICLR), 2021. [4] Xingchao Liu, Chengyue Gong, et al. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In International Conference on Learning Representations (ICLR), 2023. [5] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In International Conference on Learning Representations (ICLR), 2023. [6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution In IEEE Conference on Computer Vision and Pattern image synthesis with latent diffusion models. Recognition (CVPR), 2022. [7] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. [8] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH, 2024. [9] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Conference on Neural Information Processing Systems (NeurIPS), 2019. [10] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [11] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [12] K. R. Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C. V. Jawahar. Wav2Lip: lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia (ACM MM), 2020. [13] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. MakeItTalk: Speaker-aware talking-head animation. In ACM SIGGRAPH Asia, 2020. [14] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision (ECCV). Springer, 2024. [15] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Conference on Neural Information Processing Systems (NeurIPS), 2024. 10 [16] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. OmniHuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. [17] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. SadTalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [18] Junyang Chen, Chenpeng Du, Zhenhui Ye, and Yanwei Fu. F5-TTS: High-fidelity text-to-speech via conditional flow matching and inpainting. arXiv preprint arXiv:2411.00000, 2024. [19] Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, et al. Megatts 3: Sparse alignment enhanced latent diffusion transformer for zero-shot speech synthesis. arXiv preprint arXiv:2502.18924, 2025. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning (ICML), 2024. [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [22] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [23] Black Forest Labs. Flux.1. https://blackforestlabs.ai/announcing-black-forest-labs/, 2024. Accessed: November 2024. [24] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [25] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Pérez, Michael Zollhofer, and Christian Theobalt. Stylerig: Rigging stylegan for 3d control over portrait images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 61426151, 2020. [26] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135, 2017. [27] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and controllable text to speech. Conference on Neural Information Processing Systems (NeurIPS), 2019. [28] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020. [29] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. [30] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. Naturalspeech: End-to-end text-to-speech synthesis with human-level quality. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(6):42344245, 2024. [31] Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. arXiv preprint arXiv:2403.03100, 2024. [32] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023. [33] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sarı, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multilingual universal speech generation at scale. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [34] Keon Lee, Dong Won Kim, Jaehyeon Kim, and Jaewoong Cho. Ditto-tts: Efficient and scalable zero-shot text-to-speech with diffusion transformer. arXiv preprint arXiv:2406.11427, 2024. 11 [35] Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, et al. E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 682689. IEEE, 2024. [36] Gaoxiang Cong, Liang Li, Yuankai Qi, Zheng-Jun Zha, Qi Wu, Wenyu Wang, Bin Jiang, Ming-Hsuan Yang, and Qingming Huang. Learning to dub movies via hierarchical prosody models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [37] Gaoxiang Cong, Yuankai Qi, Liang Li, Amin Beheshti, Zhedong Zhang, Anton van den Hengel, MingHsuan Yang, Chenggang Yan, and Qingming Huang. Styledubber: towards multi-scale style learning for movie dubbing. arXiv preprint arXiv:2402.12636, 2024. [38] Kim Sung-Bin, Jeongsoo Choi, Puyuan Peng, Joon Son Chung, Tae-Hyun Oh, and David Harwath. VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models. arXiv preprint arXiv:2504.02386, 2025. [39] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [40] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2(3), 2023. [41] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. [42] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. [43] Jiahui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv preprint arXiv:2412.00733, 2024. [44] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE, 2015. [45] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [46] Hao-Han Guo, Yao Hu, Kun Liu, Fei-Yu Shen, Xu Tang, Yi-Chen Wu, Feng-Long Xie, Kun Xie, and Kai-Tuo Xu. Fireredtts: foundation text-to-speech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283, 2024. [47] Puyuan Peng, Po-Yao Huang, Shang-Wen Li, Abdelrahman Mohamed, and David Harwath. Voicecraft: Zero-shot speech editing and text-to-speech in the wild. arXiv preprint arXiv:2403.16973, 2024. [48] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: large-scale video facial attributes dataset. In European Conference on Computer Vision (ECCV), 2022. [49] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [50] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Conference on Neural Information Processing Systems (NeurIPS), 2017. [51] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [52] Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei, Gangming Zhao, Liang Lin, and Guanbin Li. Identity-preserving talking face generation with landmark and appearance priors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 12 [53] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Learning Representations (ICLR), 2023. [54] Romain Hennequin, Anis Khlif, Felix Voituret, and Manuel Moussallam. Spleeter: fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software, 5(50):2154, 2020. [55] Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Seymanur Akti, Hazım Kemal Ekenel, and Alexander Waibel. Audio-visual speech representation expert for enhanced talking face video generation and evaluation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2024. [56] Urwa Muaz, Wondong Jang, Rohun Tripathi, Santhosh Mani, Wenbin Ouyang, Ravi Teja Gadde, Baris Gecer, Sergio Elizondo, Reza Madad, and Naveen Nair. Sidgan: High-resolution dubbed video generation via shift-invariant learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [57] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pages 251263. Springer, 2017. [58] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 13 Qualitative Comparisons, User Study, and Discussions A.1 Qualitative Analysis We provide extensive qualitative comparisons across five categories to complement the quantitative results in the main paper. The supplementary HTML file contains: Talking Head Generation: 14 test samples from HDTF [39] dataset comparing our method (both I2V and V2V variants) against SadTalker [17], AniPortrait [41], Hallo [42], and Hallo3 [43]. Our V2V variant demonstrates superior lip-sync accuracy and more natural facial dynamics. Text-to-Speech: 10 test samples from LibriSpeech-PC test-clean [44, 18] comparing our method against F5-TTS baseline. While standalone performance trails specialized TTS models due to pseudo-caption training, our audio quality excels as an engine for diverse creative applications, enabling audio-visual generation and cross-modal conditioning beyond what pure TTS models can achieve. Automated Video Dubbing: 15 test samples from CelebV-Dub [38] comparing against HPMDubbing [36], StyleDubber [37], and VoiceCraft-Dub [38]. Our method achieves synchronized audio-visual generation without explicit optimization for this task. Exclusive Use Cases: We showcase some unique generation capabilities that emerge from our joint audio-motion modeling framework. These include: (1) text-to-multimodal synthesis generating both audio and motion from text alone, (2) voice-preserving multimodal generation using reference audio for speaker identity, (3) motion-constrained audio synthesis where frozen motion guides audio generation with different semantic content, and (4) motion-to-audio generation without any text cues, demonstrating the models ability to infer plausible speech from visual patterns alone. These diverse conditioning scenarios highlight the flexibility and cross-modal understanding of our unified approach. Failure Cases: Analysis of current limitations, including (1) synchronization failures when input modalities exhibit significant length mismatcheswhile minor discrepancies are handled through natural interjections, severe misalignments break lip-sync coherence, and (2) degraded performance when LivePortrait base model fails to detect keypoints on nonrealistic inputs such as flat cartoons or highly stylized artwork. A.2 User Study We conducted user study with 26 participants to evaluate perceptual quality across two tasks: Figure A1: Average ranking results for audioconditioned talking head generation on HDTF dataset. Participants ranked six methods from best (1) to worst (6) based on overall quality, including lip-sync accuracy, motion naturalness, and visual fidelity. Lower rank indicates better performance. Figure A2: User preference results for automated video dubbing on CelebV-Dub dataset. Participants selected the best synchronized audio-visual output among four competing methods for each sample. Values indicate the percentage of times each method was chosen as best. (1) Audio-Conditioned Talking Head Generation (HDTF): Participants ranked six methods from best to worst for each sample. As shown in Figure A1, our V2V variant achieved the best average rank 1 of 1.29, followed by our I2V variant (2.28), significantly outperforming SadTalker (5.04), AniPortrait (5.51), Hallo (3.02), and Hallo3 (3.85). (2) Automated Video Dubbing (CelebV-Dub): Participants selected the best model among four methods for each sample. Figure A2 shows that our method received 62.6% of votes, demonstrating strong preference over VoiceCraft-Dub (37.4%), while HPMDubbing and StyleDubber received no votes. Figure A3: Survey Examples. The left shows an example from the Audio-Conditioned Talking Head Generation (HDTF) survey, and the right shows an example from the Automated Video Dubbing (CelebV-Dub) survey. A.3 Additional Discussions Beyond the quantitative metrics and user study results, several interesting patterns emerged during our extensive experiments. In talking head generation scenarios, our method leverages the efficient and compact LivePortrait implicit keypoint warping approach, which not only accelerates inference but also effectively preserves identity and fine-grained facial details. Throughout our evaluations, we observed that while diffusion-based methods such as Hallo and Hallo3 can handle complex scenes, they tend to exhibit temporal flickering artifacts that become particularly noticeable in longer sequences. For audio generation in dubbing scenarios, our approach benefits from the rich knowledge embedded in pre-trained TTS models, which contributes to its superior performance compared to other dubbing methods. The generated audio consistently demonstrates remarkable alignment with lip motions, capturing subtle nuances including natural pauses during speech breaks and maintaining coherence between facial expressions and vocal tone. This emergent coherence was not explicitly supervised during training, suggesting that our joint modeling approach captures deeper cross-modal relationships beyond simple temporal synchronization. An intriguing observation emerges when examining the models behavior during simultaneous audio and motion generation. The system exhibits an asymmetric adaptation strategy, primarily modifying the audio to match the given motion rather than significantly altering the visual content. In practice, this manifests as minimal adjustments to lip movements while the audio undergoes more substantial modifications to achieve perfect synchronization. This behavior likely reflects the inherent challenge of modifying visual motion compared to audio synthesis and represents practical solution that prioritizes visual consistency while ensuring accurate lip-sync. These qualitative advantages, combined with strong user preference results, demonstrate that joint audio-motion modeling within unified architecture produces more natural and synchronized talking head videos compared to existing cascade or single-modal approaches. We encourage readers to explore the Exclusive Use Cases and Failure Cases sections for deeper insights into our models behavior and limitations. A.4 Inference Speed Our methods compact representation space enables remarkably fast inference. We used default settings for all baseline methods and 32 NFE (Number of Function Evaluations) for our method as specified in the main paper. On single RTX A6000 GPU, generating 20-second lip-synced HDTF sample takes: Our method: 45 seconds SadTalker: 2.5 minutes (with GFPGAN) AniPortrait: 7 minutes Hallo: 23 minutes Hallo3: 30 minutes (on H100 GPU) Note that Hallo3 requires GPUs with >48GB memory and was tested on an H100 GPU (known to be several times faster than A6000). Our training utilized 4 RTX 6000 Ada GPUs, while inference benchmarks were conducted on single RTX A6000, except for Hallo3, which required external cloud infrastructure with an H100 GPU. These results demonstrate notable speedup over existing methods, making our approach practical for real-world applications while maintaining superior quality, as validated by our user studies."
        },
        {
            "title": "B Details on used Datasets and Models",
            "content": "Our models, including the first-stage Motion-DiT, were trained exclusively on the CelebV-Dub [38] dataset, which builds upon the CelebV-HQ [48] and CelebV-Text [49] datasets. Both CelebV-Text and CelebV-HQ were sourced from the internet by their respective authors and are available solely for non-commercial research purposes. While CelebV-Dub does not explicitly specify licensing restrictions, we adhere to the same non-commercial usage constraints as its underlying datasets. The only pre-trained model utilized in our work, F5-TTS [18], is licensed under CC-BY-NC. Given these licensing constraints from both our training data and pre-trained models, our model, to be released in the future, will similarly be available under CC-BY-NC license for non-commercial research purposes only. This licensing may be subject to revision should the terms of the underlying datasets or models change."
        },
        {
            "title": "C Details on Joint Attention Implementation",
            "content": "We provide the implementations in Algorithm 1 and Algorithm 2. To support modality-specific masking and enable configurations where audio joint attention can be disabled (e.g., for classifier-free guidance), we implemented the joint attention mechanism as shown in Algorithm 2. During training, all conditioning inputs (text, audio, and motion) are randomly dropped following the standard CFM training strategy. Rotary positional embeddings (RoPE) are scaled according to the length of the audio to ensure temporal alignment across modalities. The code will be publicly released to support reproducibility and further research. 3 Algorithm 1: JointDiTBlock single block of diffusion transformer operating on two sequences with optional joint cross-attention. Element-wise multiplication is denoted by . Input: x1, t1 hidden state & timestep for branch 1 x2, t2 hidden state & timestep for branch 2 B1 = (attn_norm1, attn1, ff_norm1, ff1), B2 = (attn_norm2, attn2, ff_norm2, ff2) mask1, mask2 optional local-window masks rope1, rope2 rotary position embeddings α1, α2 {0, 1} flags: use joint attention for each branch Output: /* 1. Normalization & gating coefficients */ (ˆx1, gmsa (ˆx2, gmsa /* 2. Joint or independent multi-head attention */ (a1, a2) 1 ) B1.attn_norm(x1, t1) 2 ) B2.attn_norm(x2, t2) 2 updated hidden states 1 , gff 2 , gff 1 , eff 2 , eff , sff , sff 1, 2 1 1 a1 2 JOINTATTENTION(B1.attn, B2.attn, ˆx1, ˆx2, mask1, mask2, rope1, rope2, α1, α2) /* 3. Residual connection with MSA gating */ x1 x1 + gmsa x2 x2 + gmsa /* 4. Feed-forward branch stream 1 */ x1 B1.ff_norm(x1) (1 + eff f1 B1.ff(x1) x1 x1 + gff /* 5. Feed-forward branch stream 2 */ x2 B2.ff_norm(x2) (1 + eff f2 B2.ff(x2) x2 x2 + gff return (x1, x2) 1 ) + sff 1 2 ) + sff 2 2 f2 1 f1 Algorithm 2: JointAttention Multi-head attention with optional joint token pooling. Concatenation [ ; ] is along the sequence dimension. SDPA denotes scaled dot-product attention. CUSTOMDIAGMASK is full+diagonal window mask, illustrated in Fig. 3. Input: Attention modules attn1, attn2 with QKV projections Hidden states x1 RBL1d, x2 RBL2d Optional rotary embeddings: rope1, rope2 Optional local window masks: mask1, mask2 Joint attention flags: α1, α2 {0, 1} Output: Attention outputs o1 RBL1d, o2 RBL2d 1. Project input to QKV: (q1, k1, v1) attn1.TOQKV(x1) (q2, k2, v2) attn2.TOQKV(x2) 2. Apply rotary embeddings (if provided): if rope1 exists then (q1, k1) APPLYROPE(q1, k1, rope1) if rope2 exists then (q2, k2) APPLYROPE(q2, k2, rope2) 3. Construct joint token pools: if α1 = 1 then 1 [q1; q2], q 1 [k1; k2], 1) (q1, k1, v1) else (q 1, 1, if α2 = 1 then else 2 [q2; q1], q 2 [k2; k1], (q 2, 2, 2) (q2, k2, v2) 1 [v1; v2] 2 [v2; v1] 1, 2, 4. Split heads and apply masks: 1) SPLITHEADS(q (q (q 2) SPLITHEADS(q if mask1 = α1 = 1 then 1, 2, 1, 2, 1, 1) 2, 2) M1 CUSTOMDIAGMASK(L1, L2, mask1) else M1 if mask2 = α2 = 1 then M2 CUSTOMDIAGMASK(L2, L1, mask2) else M2 1, 2, 1, M1) 2, M2) 5. Compute scaled dot-product attention: 1, 1 SDPA(q 2, 2 SDPA(q 6. Merge heads, trim to original length, and project: o1 MERGEHEADS(o 1)[:, : L1], o2 MERGEHEADS(o 2)[:, : L2], return (o1, o2) o1 attn1.OUTPROJ(o1) o2 attn2.OUTPROJ(o2)"
        }
    ],
    "affiliations": [
        "Seoul National University",
        "Yonsei University"
    ]
}