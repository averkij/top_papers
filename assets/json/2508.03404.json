{
    "paper_title": "Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling",
    "authors": [
        "Xinlei Yu",
        "Zhangquan Chen",
        "Yudong Zhang",
        "Shilin Lu",
        "Ruolin Shen",
        "Jiangning Zhang",
        "Xiaobin Hu",
        "Yanwei Fu",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 0 4 3 0 . 8 0 5 2 : r Visual Document Understanding and Question Answering: Multi-Agent Collaboration Framework with Test-Time Scaling Xinlei Yu1, Zhangquan Chen2, Yudong Zhang3, Shilin Lu4, Ruolin Shen5, Jiangning Zhang6, Xiaobin Hu1*, Yanwei Fu7, Shuicheng Yan1 1National University of Singapore, Singapore 2Tsinghua University, China 3University of Science and Technology of China, China 4Nanyang Technological University, Singapore 5Technical University of Munich, Germany 6Zhejiang University, China 7Fudan University, China xinlei.yu@u.nus.edu, xiaobin.hu@tum.de"
        },
        {
            "title": "Abstract",
            "content": "Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on documentbased tasks. To address this, we propose MACT, MultiAgent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct smallscale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git. Introduction For humans, acquiring and understanding visual information from documentssuch as texts, webpages, charts, and tablesis common and indispensable part of daily life. As derivative task, document-based VQA provides an assessment of the understanding of both textual and visual document information as well as their relations, whose performance is quantifiable. Therefore, numerous studies aim to propose benchmarks for more complex document-based tasks of diverse types to evaluate model performance fairly *Corresponding Author: Xiaobin Hu and comprehensively. Specifically, the types have expanded from single-page documents, simple charts, and tables to real-world multi-page documents, complicated webpages, bar charts, pie charts, line charts, composite tables, matrix tables, and mathematical reasoning. Additionally, the form of the questions has also transformed from straightforward Yes/No, single-choice or direct information extraction questions to diversified question types, including but not limited to visual details detection, cross-page or multihop information extraction and summarization, comparison, multi-step calculation, intersection finding and unanswerable questions, which are answered based on the layout, structure, visual and textual contents. Thus, the latest flagship VLMs of various series are dedicated to expanding these boundaries of understanding documents. Recently, the latest released general VLMs have showcased exceptional understanding and generalization capabilities. For instance, closed-source models, such as GPT4o (Hurst et al. 2024), Gemini-2.0-Pro (DeepMind 2025) and Claude-3.7-Sonnet (Anthropic 2024) have surpassed human-level scores in certain tasks, while open-source models, e.g., Llama-3.2-Vision (Grattafiori et al. 2024), Qwen2.5-VL (Bai et al. 2025), MiMo-VL (Xia et al. 2025), InternVL-3 (Zhu et al. 2025) and Ovis2 (Lu et al. 2024b) have also achieved great performance. However, their performances in document-based tasks remain suboptimal due to the biases or deficiencies in the training data. To further enhance the ability, some researchers have endeavored to develop specialized VLMs. For example, UReader (Ye et al. 2023), TextMonkey (Liu et al. 2024b) and mPLUGDocOwl2 (Hu et al. 2024b) focus on document understanding. However, their overall performance lags far behind that of the latest general VLMs with larger parameters. To summarize, currently existing VLMs, including both generalists and specialists, exhibit three key limitations in understanding visual documents and answering questions: (1) Constrained by Parameter Scale: There are substantial performance gaps between larger-parameter VLMs and smaller counterparts in the document domain, and the potential of smaller-parameter VLMs remains underactivated. (2) (a) Qwen2.5-VL Series Based (b) MiMo-VL Series Based (c) InternVL-3 Series Based Figure 1: Comparisons among the three variants of our MACT, their base models, and larger models in the same series. Deficiency in Self-Correction Capacity: As self-correction is indispensable when humans tackle tasks involving challenging and complex documents, existing methods either fail to adequately address this capability or adopt suboptimal designs. (3) Underperformance on Long Visual Contexts and Reasoning: Results on some document-based benchmarks, which feature long visual contexts or require dense reasoning, are unsatisfactory, with notably low accuracy. Given these issues, we propose multi-agent collaboration framework with dedicated test-time scaling strategy for visual document understanding and question answering. Within this framework, we employ four relatively lightweight collaborative agents: planning, execution, judgment, and answer agents. Through their collaboration and mixed reward modeling, each agent directs attention to both agent-tailored and global objectives, thereby unleashing the potential of the agents with small-scale parameters. For more efficient self-correction capacity, we design mechanism that employs an additional judgment agent. It only checks the correctness of the generated execution plans and execution processes, with corrections implemented by the preceding agents. Additionally, we introduce an agent-wise hybrid test-time scaling approach that considers the distinct functions and features of each agent, markedly improving abilities in handling long visual contexts and complex reasoning. We train three variants of MACT based on different groups of base models using two-stage pipeline, and evaluate them on 15 benchmarks, comprising 10 document-based and 5 non-document-based ones, to comprehensively assess document understanding as well as general and reasoning performance. Compared to models of comparable scale and larger-scale ones, the results indicate that the three variants of MACT show an average increase of at least 3.2% to 5.6%, securing the top three positions and achieving the best performance in 13 out of 15 benchmarks. Furthermore, our models have distinct advantages in long visual contexts and reasoning tasks, as further validated by results on selected non-document benchmarks. Methodology This section provides detailed elaboration of our proposed MACT, divided into three subsections. We first formally define the task, then introduce the four agent components: the planning agent, the execution agent, the judgment agent, and the answer agent, as well as the collaboration mechanisms among them. Finally, we detail the mixed reward modeling and agent-wise hybrid test-time scaling tailored to our multiagent collaboration framework. Task Definition For the question answering task defined in this work, given question and the its corresponding visual inputs = {v1, v2, . . . , vn}, the target is to generate correct answer output O. This answer should leverage the information within and answer the proposed question comprehensively and accurately. Multi-Agent Collaboration Framework As shown in the upper part of Figure 2, MACT consists of four collaborative agents: sequentially planning agent (Aplan), execution agent (Aexe), judgment agent (Ajudg), and answer agent (Aans). Each agent is role-specialized, which accomplishes its functionality and then passes the generated contents to the subsequent agent or outputs the final answers. Planning Agent. Aplan mainly focuses on the analysis and decomposition of the original question, making high-level executive plans. Largely following the analogical prompting principles (Yasunaga et al. 2024), it first generates Np relevant sample problems along with their corresponding plans simultaneously. By referring to each sample problem and its corresponding plan, it produces distinct execution plans for the current problem respectively, providing diverse pathways to accomplishment. It should be noted that this agent only yields high-level execution plans without implementation details. It ensures that Aplan can better formulate the plan from holistic perspective, and avoid interfering with the execution of Aexe. Moreover, it avoids premature binding to specific tools, enabling Aexe to select optimal resources for diverse tasks dynamically. Execution Agent. Aexe is designed to execute plans step by step and output the execution process using the seFigure 2: The overview of MACT. The upper part demonstrates our proposed multi-agent framework, which has four collaborative agents for tailored tasks. When the judgment agent detects mistakes, it redirects to previous agents for corrections. While the lower part illustrates the mixed reward modeling and agent-wise hybrid scaling for the multi-agent framework. lected tools from the tool library. Specifically, it breaks down the steps in the plan into execution units, with information for each unit populated into template. Aexe then executes these units sequentially, deriving the intended outputs from each. Once all steps are completed, it concatenates the full output and the execution processes of all steps in sequence, passing them to subsequent agents. Judgment Agent. To enhance the quality of completing challenging tasks, humans often judge and correct mistakes that arise in planning or executionsuch as reversed sequences, incorrect dependencies, redundant or missing steps, mismatches between plans and processes, and disconnects between steps. Similarly, some multi-agent systems, primarily focused on code generation, incorporate mechanisms that either (a) internally correct errors within the same agent (Yang et al. 2024; Sun et al. 2024) or (b) deploy an additional agent to handle both judgment and correction (Islam, Ali, and Parvez 2024; Li et al. 2025a). However, the internal correction mechanism (a) struggles to identify most mistakes and may fall into cognitive blind spots, as generation and correction rely on the same model. In contrast, the latter approach (b) requires the agent to possess strong capabilities for both judgment and correction, thereby necessitating models with larger parameters and more complex reward modeling designs. Furthermore, using separate agent to regenerate mistaken components may result in incompatibilities or even conflicts with existing parts, given the autonomy of the two agents. In RL, the optimization objective for both (a) and (b), as illustrated in Figure 3, is to pass verification. This can lead to strategic production of vague statements or the omission of details, resulting in corrections that appear correct superficially but are actually misleading. To address the limitations of both approaches, we design novel judgment strategy for multi-agent systems, which introduces an additional judgment agent to separate judgment from correction, thereby fostering specialization of labor. More specifically, Ajudg is primarily responsible for assessing the correctness of steps in previously generated execution plans and processes, without engaging in direct correction. If mistakes are detected in any step, Ajudg identifies the specific problematic step, provides brief description of the error, and routes the issue to the appropriate agenteither Aplan or Aexefor correction. For each plan-process pair that is already correct or has been corrected, Ajudg forwards them to Aans. This strategy, by decoupling judgment from correction, introduces neutral judge with reduced subjective bias, whose focus remains solely on judgment rather than correction. Additionally, the reward design for the judgment agent can be intuitively simplified, without evaluating correction outcomes. To prevent infinite correction loops, the maximum number of corrections Nc is set to 3. Answer Agent. Counterintuitively, Aans incorporates both the correct execution process and incorrect segments from prior processes. This facilitates direct focus on modifications within the corrected process, thereby preventing the omission of error-prone details. the full Collaboration. Throughout implementation of the multi-agent collaboration framework, visual inputs and questions are first fed into Aplan, whose generated plan is then executed by Aexe. Ajudg judges the correctness the resulting execution plan-process pairs and outputs mistake flags. For correct pairs, Aans outputs the final answers, while the information of mistakes will be passed to the previous agent Aplan or Aexe for correction, and the procedure Algorithm 1: Multi-Agent Collaboration Procedure Require: question Q, visual inputs = {v1, v2, . . . , vn}, planning agent Aplan, execution agent Aexe, the tool library of the former agent = {t1, t2, . . . , tn}, judgment agent Ajudg, answer agent Aans, maximum number of corrections Nc Step-wise execution Relevant plans Execution plan p3 PF (Q, si, , M) ei Aexe (V, p3, ) p1 PF (Q) Prelevant Aplan (V, p1) p2 PF (Q, Prelevant, M) Aplan (V, p2) for each step si in do Ensure: answer output to the question 1: Initialize the four agents: Aplan, Aexe, Ajudg, Aans 2: Initialize Nc = 3, = 0 3: Initialize prompts formatter PF 4: Initialize lagplan = lagexe = alse, = empty 5: while true do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end while 28: p5 PF (Q, E, M) 29: Aans (p5) end for {e1, e2, . . . , en} p4 PF (Q, P, E) Ajudg (p4) {f lagplan, lagexe, M} if not lagplan and not lagexe or Nc then + 1 goto line 10 + 1 goto line else if lagplan then else if lagexe then break end if Mistakes in execution process Mistakes in execution plan Judgment Mistakes Execution process Answer ing inference. Existing test-time scaling methods fall into four main categories: parallel scaling, sequential scaling, hybrid scaling, and internal scaling. However, these strategies are originally designed for single models or agents, overlooking the different division of labor among agents and achieving suboptimal performance when applied to multiagent systems. Accordingly, we propose an agent-wise hybrid scaling strategy tailored to multi-agent architectures, which significantly enhances both agent-specific capabilities and collaborative performance. For the first three agents with various functions, we customize different test-time scaling strategies to their unique characteristics: (1) Planning Agent: In our design of Aplan, it naturally provides relevant sample plans as references for formulating multiple execution plans for the original task. We therefore prompt the generation of Np relevant plans independently, yielding Np parallel paths per query. It establishes foundation for scaling subsequent agents and increases the likelihood that at least one path will produce the correct answer. (2) Execution Agent: Given that the exFigure 3: Comparisons of (a) internal correction, (b) an extra agent for both judgment and correction, and (c) our strategy utilizing an independent judgment agent. repeats. For clarity in illustrating the workflow, we set the number of generated relevant plans to 1 and simplify the test-time scaling designs in Algorithm 1, and these details are elaborated in the subsequent section. Mixed Reward modeling To guide RL in our multi-agent system and further enhance both the specialized and collaborative capabilities of each agent, we design mixed reward strategy that integrates agent-specific and global outcome reward signals. Given that the four agents in the framework have distinct functions and reward signal preferences, we first incorporate agent-tailored rewards for each. In particular, for Aplan and Aexe, whose outputs are step-by-step, we employ an effective multi-modal reward model to yield step-wise process reward signals. These step-wise rewards provide instant feedback for each step and hierarchical rewards for the two agents, boosting accuracy on challenging tasks. For Ajudg and Aans, we directly use reward model to generate one reward signal for each output of each agent. Furthermore, we apply global reward, computed based on the final selected answer along the chosen path of the four agents. It strengthens rewards for correct paths, guiding the model to explore valid paths while mitigating adverse effects from erroneous ones. Besides, it mitigates the selfishness of agents to some extent, as individual agents tend to maximize their own rewards without regard for the global objective, and this global reward helps to alleviate such behavior. Agent-Wise Hybrid Test-Time Scaling Since that scaling test-time computation can activate the long-context understanding and reasoning capabilities of small-parameter VLMs, dynamically allocating additional computing resources and enabling multiple paths in the test time, we propose an agent-wise hybrid scaling strategy durecution process is divided by step in Aexe, we treat each step as an evaluation node. For each node, the agent produces Ne candidate executions, which are scored using pretrained reward model. The top-scoring candidate is selected as the base node for subsequent steps, with all others rejected. (3) Judgment Agent: To judge the correctness of the execution plans and processes, Ajudg requires logical analysis and reasoning to comprehensively and accurately detect mistakes. We therefore adopt the budget forcing scaling method (Muennighoff et al. 2025) for this agent, which enforces minimum number of thinking tokens. When token usage falls below the budget, the agent is encouraged to generate additional thinking tokens, thereby promoting accurate judgments. For the answer agent, whose key function is to summarize prior information and generate the final answer, test-time scaling yields only limited improvements, so we do not apply scaling to it."
        },
        {
            "title": "Experimental Settings",
            "content": "Training Pipeline Since the functions and optimization goals of each agent are independent and diverse, we design two-stage SFT and RL pipeline for MACT. To understand visual inputs and better accomplish their labors, we choose VLMs for the Aplan and Aexe, while LLMs for the Ajudg and Aans. For all the four agents, we initiate with pretrained models, and we select three groups of small-parameter base models for different variants of MACT: (1) Qwen2.5-VL Series Based (Bai et al. 2025; Yang et al. 2025): Qwen2.5-VL-7BInstruct and Qwen2.5-7B/3B-Instruct; (2) MiMo-VL Series Based (Xiaomi 2025; Xia et al. 2025): MiMo-VL-7B-SFT and MiMo-7B-SFT; (3) InternVL3 Series Based (Zhu et al. 2025): InternVL3-9B/8B/2B-Instruct. In the first SFT stage, we initially train tuned 11B/7B/7B VLM on the selected document-based or non-document-based datasets, mixing data with or without CoT. It aims to enhance their visual understanding and reasoning abilities, providing more robust models for future multi-agent collaboration. Next, we fine-tune an 8B/7B/7B LLM on judgment labels generated via GPT-4o (Hurst et al. 2024) and rule-based verifications. Finally, we fine-tune another 3B/3B/7B LLM on the outputs from preceding agents and ground-truths to enhance its summary ability. The fine-tuned VLM are applied as Aplan and Aexe respectively in the subsequent stage, while the two LLMs function as the Ajudg and Aans. In the second RL stage, we generate reward signals based on pretrained reward models and optimize our model via GRPO (Shao et al. 2024). Process reward model VisualPRM (Wang et al. 2025c) is utilized for the step-by-step reward signals of Aplan and Aexe, while Skywork-VL-Reward (Wang et al. 2025d) is used for the rewards generation of Ajudg and Aans. More training details are in the Appendix. Datasets To train our model and comprehensively, objectively assess its visual capabilities, we selected 15 datasets spanning four real-world document types and two non-document types are comprised of: (1) types. The four document Text-Based: DocVQA (Mathew, Karatzas, and Jawahar 2021), DUDE (Van Landeghem et al. 2023), SlideVQA (Tanaka et al. 2023), MMLongBench-Doc (Ma et al. 2025); (2) Webpage-Based: VisualMRC (Tanaka, Nishida, and Yoshida 2021), InfographicVQA (Mathew et al. 2022); (3) Chart-Based: ChartQA (Masry et al. 2022), CharXiv (Wang et al. 2024d); (4) Table-Based: TableVQABench (Kim, Yim, and Song 2024), and TableBench (Wu et al. 2025). To retain generalization and enhance reasoning, two non-document types are involved: (1) General: ScienceQA (Saikh et al. 2022), RealWorldQA (xAI 2024); (2) Mathematical: MathVista (Lu et al. 2024a), MathVision (Wang et al. 2024b), and MathVerse (Zhang et al. 2024c). We adhere to the original training and testing splits and use the test splits as evaluation benchmarks. Each instance in these datasets consists of visual input and question (except ScienceQA), spanning difficulty levels from easy to hard and encompassing various question types. More details are provided in the Appendix. Evaluations Following (Li et al. 2025b), we use the reported initial results of reproducible models where configurations align with ours. For other models, we utilize LMMs-Eval (Zhang et al. 2024b) for fair comparisons on both natively supported and our registered benchmarks. For most benchmarks, we employ GPT-4o (Hurst et al. 2024) as judge model to evaluate the correctness of the generated answers, which is achieved in LMMs-Eval. For the remaining benchmarks, we utilize their original evaluation metrics, e.g., ANLS and F1. More details can be seen in the Appendix."
        },
        {
            "title": "Results and Discussions",
            "content": "Main Results We evaluate MACT across 15 benchmarks: 10 documentbased benchmarks to assess its document understanding performance; 2 general benchmarks to gauge generalization ability, mitigating overfitting to specific scenarios; and 3 mathematical benchmarks to evaluate complex reasoning capabilities. For comparison, we select state-of-theart (SOTA) methodsencompassing both generalist models and document-specific specialistsand categorize them by parameter size. As shown in Table 1, the MACT-MiMo-VL-Series-28B variant delivers the best average performance, followed by MACT-InternVL3-Series-28B and MACT-Qwen2.5-VLSeries-24B, which rank second and third, respectively. Despite having fewer than 30B parameters, MACT models outperform all comparative methods with under 100B parameters, as well as closed-source models. Notably, MACTMiMo-VL-Series-28B achieves significant improvements of 5.6% and 5.9% over the top-performing open-source and closed-source models in terms of the average scores. Additionally, the three variants top 13 of the 15 benchmarks, with MACT-MiMo-VL-Series-28B leading on seven. Notably, in MMLongBench-Doc, which features the longest visual context, and across the three mathematical reasoning benchmarks, MACT-MiMo-VL-Series-28B outperforms the Text Document Webpage Chart Table General Mathematical Average Non-Document DVQA DUDE SVQA MMLong VisMRC InfVQA ChartQA CharXiv TabVQA TabBen SciQA RealQA MVista MVision MVerse Average Images Per Question Average Question Length Average Answer Length Closed-Source GPT-4o-latest Claude-3.7-Sonnet Gemini-2.0-Pro Size < 20B DeepSeek-VL2-4.5B LLaVA-OneVision-7B-si Qwen2.5-VL-7B-Instruct MiMo-VL-7B-SFT InternVL3-8B-Instruct Llama-3.2-11B-Vision-Instruct LlaVa-1.6-vicuna-13B InternVL3-14B-Instruct Ovis2-16B 30B Size < 40B Qwen2.5-VL-32B-Instruct LlaVa-1.6-34B Ovis2-34B InternVL3-38B-Instruct 70B Size < 100B LLaVA-OneVision-72B-si Qwen2.5-VL-72B-Instruct InternVL3-78B-Instruct Llama-3.2-90B-Vision-Instruct MMCA-7B UReader-7B mPLUG-DocOwl2-8B TextMonkey-10B M3DocRAG-10B CogAgent-17B MDocAgent-39B MACT -Qwen2.5-VL-Series-24B MACT -MiMo-VL-Series-28B MACT -InternVL3-Series-28B l n s i S O 1.0 8.3 2.1 93.1 94.0 91.8 78.8 86.6 94.6 92.9 91.1 87.3 79.0 90.7 92.4 94.9 87.8 93.6 95.0 91.7 95.7 95.3 91.4 68.9 72.3 87.8 77.2 79.2 80.9 86. 5.7 8.7 3.4 52.7 58.1 54.3 - 48.4 62.3 62.3 60.1 45.0 42.9 61.0 59.6 66.4 50.1 63.1 61.7 62.7 67.0 68.9 58.6 - - 46.5 36.8 39.5 - 58. 72.5 96.6 +2.0 +10.2 94.4 +1.5 96.1 +5.0 70.8 +8.5 69.8 +9.7 20.0 9.0 2.4 81.0 83.6 78.9 - 52.9 76.2 74.2 72.9 66.5 65.6 74.5 73. 75.0 69.5 72.4 76.3 63.9 82.4 80.7 76.7 - - - - 57.9 - 68.6 85.3 +9.1 83.9 +9.7 81.3 +8.4 47.5 16.4 2.8 40.3 33.9 32. - 5.3 22.4 27.5 28.6 8.8 7.9 28.9 29.0 22.9 9.9 31.0 30.2 10.6 24.9 32.4 19.5 - - - - 29.2 - 36.7 43.7 1.0 10.6 9. 86.4 82.5 91.4 70.4 76.1 86.8 87.3 84.4 79.6 77.9 87.1 88.5 86.4 80.1 85.6 89.9 81.4 90.3 90.7 86.0 59.0 70.7 76.1 74.5 71.8 70.3 82.8 92. 1.2 11.5 1.6 79.2 75.3 81.6 65.3 74.8 82.8 82.7 78.5 73.8 74.7 83.1 78.1 83.8 78.4 79.1 84.6 75.2 87.5 86.2 82.3 54.9 66.6 72.2 68.1 67.3 65.3 74. 89.4 1.0 7.2 1.2 86.5 92.2 88.8 74.0 82.3 87.5 86.1 85.4 79.6 76.6 86.2 87.9 87.7 85.3 88.9 88.0 86.1 89.5 89.6 86. 64.4 67.0 80.8 72.5 73.7 69.9 82.8 91.9 +21.3 47.4 +19.9 46. +17.9 +5.2 93.8 +6.5 91.0 +6. +6.6 88.6 +5.9 87.7 +9.2 +4. 91.4 +5.3 90.6 +5.2 1.0 20.4 2.9 78.7 83.5 83. 46.9 54.3 68.2 72.0 67.1 63.6 52.5 75.1 70.7 75.9 60.4 71.9 79.8 63.8 80.6 77.9 74.4 47.5 52.0 52.4 52.0 56.5 54.3 70.2 1.0 10.5 4.7 64.2 70.3 71. 30.8 47.9 60.4 57.5 59.7 50.4 24.9 58.2 59.5 62.7 32.9 61.6 63.1 52.8 68.4 65.5 60.1 - - 25.7 21.7 22.5 21.7 58.8 85.2 +17.0 87.2 +15.2 85.0 +17.9 74.0 +13.6 71.6 +14.1 75.3 +15. 1.0 20.3 8.5 51.9 54.7 59.9 - 33.3 49.2 52.5 52.4 44.6 39.2 51.4 46.8 51.0 44.4 47.1 54.1 41.5 58.1 57.9 49.4 - - 19.8 16.9 16.5 - 48. 57.2 +8.0 62.7 +10.2 61.0 +8.6 0.5 12.1 4.4 83.3 80.4 80. 66.6 72.8 74.0 75.1 76.4 76.9 70.7 77.1 72.5 74.8 76.2 72.0 77.5 75.1 74.4 78.0 78.5 56.6 60.4 65.7 65.9 65.4 58.8 69.8 78.5 +4.5 79.2 +4.1 81.3 +4.9 1.0 11.1 1. 76.2 71.9 70.5 60.7 62.0 68.4 67.9 66.7 60.7 58.1 66.4 67.9 71.7 67.0 70.7 72.8 70.5 74.8 77.9 68.8 52.6 55.7 60.0 63.4 58.9 52.6 64.8 77. +9.3 76.1 +8.2 80.1 +13.4 1.0 15.6 1.2 62.4 69.7 74. 29.5 57.9 67.8 70.7 65.5 56.5 50.2 66.4 69.6 65.8 56.7 74.0 75.2 72.5 77.6 79.3 58.7 - - 41.3 33.7 41.8 - 46.6 1.0 42.3 2.2 30.7 41.2 54. 6.9 17.9 25.5 46.9 28.8 16.3 11.6 36.6 35.6 32.6 15.8 31.4 34.0 25.2 39.8 44.3 23.4 - - 9.2 5.4 14.8 - 20.1 1.0 35.7 1.4 39.4 45.0 56. 7.4 19.1 40.8 53.3 38.9 32.3 27.5 43.5 45.0 46.4 17.0 49.5 47.6 27.0 46.2 48.7 35.0 - - 14.4 11.7 20.8 - 29.3 5.7 16.0 3.3 67.2 69.1 71. - 52.8 64.5 67.3 63.8 56.1 50.6 65.7 65.1 66.5 55.4 66.1 68.7 60.0 70.5 71.6 63.3 - - - - 47.7 - 59.9 81.2 +13.4 85.4 +14.7 83.8 +18.3 41.8 +16.3 60.1 +13.2 45.8 +17. 54.7 +13.9 65.3 +12.0 54.0 +15.1 74.8 +10.3 77.2 +9.9 75.3 +11.5 Table 1: DVQA, SVQA, MMLong, InfQA, VisMRC, TabVQA, TabBen, SciQA, RealQA, MVista, MVision, and MVerse denote DocVQA, SlideVQA, MMLongBench-Doc, InfographicVQA, VisualMRC, TableVQA-Bench, TableBench, ScienceQA, RealWorldQA, MathVista, Math-Vision, and MathVerse benchmarks, respectively. The values with green, blue, and gray colors denote the best, the second best, and the third best values. Results with - indicate exceeding the maximum context token limit or not being equipped with specific ability. , , correspond to the base models of MACT for comparisons. second-highest scorer by 7.1%, 10.6%, 5.9%, and 8.7%, respectively. These results highlight the superior overall performance of MACT, as well as its strong capabilities in longcontext understanding and reasoning. Radar charts in Figure 1 compare three variants of MACT with their base models and larger models from the same series. As shown, MACT models significantly outperform their base models by 10.3%, 9.9%, and 11.5% on average across the 15 benchmarks. Compared to their corresponding larger-parameter counterparts, i.e., Qwen2.5-VL-72BInstruct and InternVL3-78B-Instruct, our MACT-Qwen2.5VL-Series-24B and MACT-InternVL3-Series-28B achieve average performance gains of at least 6.6% and 3.7%, respectively, with more pronounced improvements observed on long-context and reasoning tasks."
        },
        {
            "title": "Additional Quantitative Analysis",
            "content": "MMLong TabBen MVision Average w/o Multi-Agent Collaboration 32.5 11.2 50.8 6.4 32.4 9.4 66.2 8.6 w/o Multi-Agent Collaboration 24.7 19.0 44.4 12.8 26.0 15.8 58.6 16.2 38.3 5.4 54.2 3.0 36.7 5.1 71.4 3.4 w/o Mixed Reward Modeling w/o Agent-Wise Hybrid Scaling 34.9 8.8 50.8 6.4 34.5 7.3 71.1 3."
        },
        {
            "title": "MACT",
            "content": "43.7 57.2 41.8 74.8 Table 2: Results of ablations. denotes single agent that directly outputs answers, while indicates single agent that incorporates all prompts as workflow. test-time scaling strategies. Experiments here are conducted using MACT-Qwen2.5-VL-Series-24B. Specifically, we select three lowest-scoring benchmarks, which are characterized by long visual contexts and reasoning demands, and the average results across all benchmarks to assess their impact on both specific abilities and overall performance. To further demonstrate the effectiveness of our methodologies, we perform ablation experiments and analyses on the proposed multi-agent collaboration, reward modeling, and Ablation Studies The ablation settings are as follows: (1) w/o Multi-Agent Collaboration: Use single agent to directly execute tasks and output answers, with our proAplan + Aexe Ajudg Aans MMLong TabBen MVision Average"
        },
        {
            "title": "MMLong TabBen MVision Average",
            "content": "(cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 36.3 42.2 36. 43.7 48.5 56.7 48.6 57.2 34.9 40.2 35.4 41.8 68.4 73.9 68. 74.8 Table 3: Results of different combinations of agents. posed scaling and reward strategies retained. (2) w/o MultiAgent Collaboration: Use single agent, which incorporates all prompts from our four proposed agents as unified workflow, with our proposed scaling and reward strategies retained. (3) w/o Agent-Wise Hybrid Scaling: Retain the multi-agent collaborative framework and apply the mixed reward strategy directly, without test-time scaling. (4) w/o Mixed Reward Modeling: Retain the multi-agent collaborative framework and agent-wise hybrid scaling, without reward signals. As shown in Table 2, the multi-agent collaborative approach contributes the most, outperforming the single-agent system by 8.6%. Notably, consolidating all functions into single model results in suboptimal performance, which is even worse than the base model, highlighting the necessity of multi-agent collaborative framework. Both the mixed reward modeling and agent-wise hybrid scaling strategies boost average accuracy by at least 3%. Specifically, the latter yields more significant gains on complex tasks compared to average ones, with especially notable improvements in reasoning tasks. Analysis of Multi-Agent Collaboration To assess the utility of each agent and their collaboration, we perform additional experiments on different agent combinations, which are listed in Table 3. The combination of Aplan and Aexe results in 3.9% average increase over the base model, while adding Ajudg further boosts performance by 5.5%. Aans extraly improves the scores by 0.9%, with this gain expanding to 1.5% for long visual context. Additionally, as in Figure 4a, we compare our judgment agent strategy with the other two counterparts. Our proposed strategy outperforms the others by at least 2.6% on average, while requiring an average of 0.3 fewer corrections. Moreover, our method achieves optimal correction results when the maximum number of corrections Nc is set to 3, whereas the alternatives require Nc to be set to 5, highlighting its greater accuracy and efficiency. It could also be observed that when setting higher Nc, the performance will not improve continuously, primarily because excessive corrections may confuse the agent and obscure correct answers. Analysis of Reward modeling and Test-Time Scaling We perform experiments on our proposed mixed reward modeling and agent-wise hybrid test-time scaling. For the former, we separately use the agent-wise reward and global reward for comparison. For the latter, we benchmark our method with the other four effective test-time scaling strategies. Here, we select budget forcing (Muennighoff et al. 2025) as the internal scaling. Although the improvement from global reward is relatively limited, it avoids the selfishness of agents that only use agent-specific reward, yielding No Reward Agent-Specific Reward Global Reward"
        },
        {
            "title": "No Scaling\nParallel Scaling\nSequential Scaling\nHybrid Scaling\nInternal Scaling",
            "content": "Agent-Wise Hybrid Scaling & Mixed Reward 38.3 42.8 34.1 34.9 39.1 41.3 39.8 41.8 43.7 54.2 56.3 51.6 50.8 55.2 54.7 55.5 55. 57.2 36.7 40.5 35.1 34.5 37.5 40.0 39.4 41.6 41.8 71.4 72.7 70.2 71.1 72.0 72.4 73.0 72. 74.8 Table 4: Results of different reward modeling and test-time scaling strategies. (a) Analysis of Corrections (b) Impact of Np and Ne Figure 4: (a) Line graph shows the impact of various maximum numbers of corrections, with dashed and dotted lines denoting average values across all and three selected benchmarks, respectively. The bar charts show the average judgment numbers when the maximum is set to 3. (b) The line graphs represent the impacts of Np and Ne. further 1.3% increase in average scores. Our strategy, which designs tailored scaling forms for each agent according to its functions and specificity, outperforms all four existing strategies and improves the average scores by at least 1.8%. As demonstrated in Figure 4b, higher values of Np and Ne increase the likelihood of finding the correct answers, which leads to more attempts at plans and execution steps. Conclusion Our proposed MACT is an innovative multi-agent collaboration framework with test-time scaling strategy for visual document understanding and question answering. Comprising four collaborative agents, this framework achieves effective functional role division through synergistic collaboration. Notably, the judgment agent strategy outperforms existing self-correction mechanisms while requiring fewer corrections. Additionally, mixed reward modeling and agent-wise hybrid scaling are particularly well-suited for multi-agent systems, as they strike balance between agentspecific capabilities and collaborative efficiency. Extensive comparative experiments and supplementary analysis validate the superiority and effectiveness of our MACT. Furthermore, this work constitutes meaningful exploration of multi-agent frameworks and test-time scaling strategies, underscoring promising directions for future research. References Anthropic. 2024. Claude 3.7 Sonnet. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.; Podstawski, M.; Gianinazzi, L.; Gajda, J.; Lehmann, T.; Niewiadomski, H.; Nyczyk, P.; et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 1768217690. Biten, A. F.; Tito, R.; Mafla, A.; Gomez, L.; Rusinol, M.; Valveny, E.; Jawahar, C.; and Karatzas, D. 2019. Scene In Proceedings of the text visual question answering. IEEE/CVF International Conference on Computer Vision (ICCV), 42914301. Brown, B.; Juravsky, J.; Ehrlich, R.; Clark, R.; Le, Q. V.; Re, C.; and Mirhoseini, A. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Chen, J.; Hu, X.; Liu, S.; Huang, S.; Tu, W.-W.; He, Z.; and Wen, L. 2024a. Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments. arXiv preprint arXiv:2402.16499. Chen, X.; Lin, M.; Scharli, N.; and Zhou, D. 2024b. Teaching Large Language Models to Self-Debug. In International Conference on Learning Representations (ICLR). Cheng, P.; Dai, Y.; Hu, T.; Xu, H.; Zhang, Z.; Han, L.; Du, N.; and Li, X. 2024. Self-playing adversarial language game enhances llm reasoning. Advances in Neural Information Processing Systems (NeurIPS), 37: 126515126543. Cho, J.; Mahata, D.; Irsoy, O.; He, Y.; and Bansal, M. 2024. M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding. arXiv preprint arXiv:2411.04952. DeepMind, G. 2025. Gemini 2.0 Pro. Dong, X.; Zhang, P.; Zang, Y.; Cao, Y.; Wang, B.; Ouyang, L.; Zhang, S.; Duan, H.; Zhang, W.; Li, Y.; et al. 2024a. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. Advances in Neural Information Processing Systems (NeurIPS), 37: 4256642592. Dong, Y.; Liu, Z.; Sun, H.-L.; Yang, J.; Hu, W.; Rao, Y.; and Liu, Z. 2024b. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432. Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch, I. 2024. Improving factuality and reasoning in language models through multiagent debate. In International Conference on Machine Learning (ICML). Faysse, M.; Sibille, H.; Wu, T.; Omrani, B.; Viaud, G.; Hudelot, C.; and Colombo, P. 2025. Colpali: Efficient document retrieval with vision language models. In International Conference on Learning Representations (ICLR). Feng, H.; Liu, Q.; Liu, H.; Tang, J.; Zhou, W.; Li, H.; and Huang, C. 2024. Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding. Science China Information Sciences, 67(12): 114. Gou, Z.; Shao, Z.; Gong, Y.; yelong shen; Yang, Y.; Duan, N.; and Chen, W. 2024. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. In International Conference on Learning Representations (ICLR). Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Vaughan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Guan, X.; Zhang, L. L.; Liu, Y.; Shang, N.; Sun, Y.; Zhu, Y.; Yang, F.; and Yang, M. 2025. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking. arXiv preprint arXiv:2501.04519. Han, S.; Xia, P.; Zhang, R.; Sun, T.; Li, Y.; Zhu, H.; and Yao, H. 2025. Mdocagent: multi-modal multi-agent arXiv preprint framework for document understanding. arXiv:2503.13964. Hong, W.; Wang, W.; Lv, Q.; Xu, J.; Yu, W.; Ji, J.; Wang, Y.; Wang, Z.; Dong, Y.; Ding, M.; et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1428114290. Hu, A.; Xu, H.; Ye, J.; Yan, M.; Zhang, L.; Zhang, B.; Zhang, J.; Jin, Q.; Huang, F.; and Zhou, J. 2024a. mPLUGDocOwl 1.5: Unified Structure Learning for OCR-free DocIn Findings of the Association for ument Understanding. Computational Linguistics: EMNLP 2024, 30963120. Association for Computational Linguistics. Hu, A.; Xu, H.; Zhang, L.; Ye, J.; Yan, M.; Zhang, J.; Jin, Q.; Huang, F.; and Zhou, J. 2024b. mplug-docowl2: Highresolution compressing for ocr-free multi-page document understanding. arXiv preprint arXiv:2409.03420. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Islam, M. A.; Ali, M. E.; and Parvez, M. R. 2024. MapCoder: Multi-Agent Code Generation for Competitive Problem Solving. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 1: Long Papers), 49124944. Association for Computational Linguistics. Kim, Y.; Yim, M.; and Song, K. Y. 2024. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205. Laurencon, H.; Marafioti, A.; Sanh, V.; and Tronchon, L. 2024. Building and better understanding vision-language models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models. Li, B.; Wang, Y.; Gu, J.; Chang, K.-W.; and Peng, N. 2025a. Metal: multi-agent framework for chart generation with test-time scaling. arXiv preprint arXiv:2502.17651. Li, B.; Zhang, Y.; Guo, D.; Zhang, R.; Li, F.; Zhang, H.; Zhang, K.; Zhang, P.; Li, Y.; Liu, Z.; and Li, C. 2025b. LLaVA-OneVision: Easy Visual Task Transfer. Transactions on Machine Learning Research. Li, H.; Xu, T.; Chang, E.; and Wen, Q. 2025c. Knowledge tagging with large language model based multi-agent sysIn Proceedings of the AAAI Conference on Artificial tem. Intelligence, volume 39, 2877528782. Li, Z.; Yang, B.; Liu, Q.; Ma, Z.; Zhang, S.; Yang, J.; Sun, Y.; Liu, Y.; and Bai, X. 2024. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2676326773. Liu, F.; Wang, X.; Yao, W.; Chen, J.; Song, K.; Cho, S.; Yacoob, Y.; and Yu, D. 2024a. MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies (Volume 1: Long Papers), 12871310. Association for Computational Linguistics. Liu, Q.; Zhou, W.; Xu, N.; Huang, J. Y.; Wang, F.; Zhang, S.; Poon, H.; and Chen, M. 2025. Metascale: Testtime scaling with evolving meta-thoughts. arXiv preprint arXiv:2503.13447. Liu, Y.; Yang, B.; Liu, Q.; Li, Z.; Ma, Z.; Zhang, S.; and Bai, X. 2024b. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473. Lu, P.; Bansal, H.; Xia, T.; Liu, J.; Li, C.; Hajishirzi, H.; Cheng, H.; Chang, K.-W.; Galley, M.; and Gao, J. 2024a. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. In International Conference on Learning Representations (ICLR). Lu, S.; Li, Y.; Chen, Q.-G.; Xu, Z.; Luo, W.; Zhang, K.; and Ye, H.-J. 2024b. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797. Ma, Y.; Zang, Y.; Chen, L.; Chen, M.; Jiao, Y.; Li, X.; Lu, X.; Liu, Z.; Ma, Y.; Dong, X.; et al. 2025. MMLONGBENCHDOC: Benchmarking Long-context Document Understanding with Visualizations. Advances in Neural Information Processing Systems (NeurIPS), 37: 9596396010. Masry, A.; Do, X. L.; Tan, J. Q.; Joty, S.; and Hoque, E. 2022. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, 22632279. Association for Computational Linguistics. Mathew, M.; Bagal, V.; Tito, R.; Karatzas, D.; Valveny, E.; and Jawahar, C. 2022. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 16971706. Mathew, M.; Karatzas, D.; and Jawahar, C. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 22002209. Muennighoff, N.; Yang, Z.; Shi, W.; Li, X. L.; Fei-Fei, L.; Hajishirzi, H.; Zettlemoyer, L.; Liang, P.; Cand`es, E.; and s1: Simple test-time scaling. arXiv Hashimoto, T. 2025. preprint arXiv:2501.19393. OpenAI. 2024. GPT-4V. Pan, B.; Lu, J.; Wang, K.; Zheng, L.; Wen, Z.; Feng, Y.; Zhu, M.; and Chen, W. 2024. AgentCoord: Visually exploring coordination strategy for llm-based multi-agent collaboration. arXiv preprint arXiv:2404.11943. Renze, M. 2024. The Effect of Sampling Temperature on In Findings Problem Solving in Large Language Models. of the Association for Computational Linguistics: EMNLP 2024, 73467356. Association for Computational Linguistics. Saikh, T.; Ghosal, T.; Mittal, A.; Ekbal, A.; and Bhattacharyya, P. 2022. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3): 289301. Setlur, A.; Rajaraman, N.; Levine, S.; and Kumar, A. 2025. Scaling test-time compute without verification or rl is suboptimal. arXiv preprint arXiv:2502.12118. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shi, H.; Ye, S.; Fang, X.; Jin, C.; Isik, L.; Kuo, Y.-L.; and Shu, T. 2025. Muma-tom: Multi-modal multi-agent theory of mind. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 15101519. Snell, C. V.; Lee, J.; Xu, K.; and Kumar, A. 2025. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning. In International Conference on Learning Representations (ICLR). Son, G.; Hong, J.; Ko, H.; and Thorne, J. 2025. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407. Sun, S.; Liu, Y.; Wang, S.; Iter, D.; Zhu, C.; and Iyyer, M. 2024. PEARL: Prompting Large Language Models to Plan In Proceedand Execute Actions Over Long Documents. ings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL) (Volume 1: Long Papers), 469486. Association for Computational Linguistics. Tanaka, R.; Nishida, K.; Nishida, K.; Hasegawa, T.; Saito, I.; and Saito, K. 2023. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, 13636 13645. Tanaka, R.; Nishida, K.; and Yoshida, S. 2021. Visualmrc: In Machine reading comprehension on document images. Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 1387813888. Tao, W.; Zhou, Y.; Wang, Y.; Zhang, W.; Zhang, H.; and Cheng, Y. 2024. Magis: Llm-based multi-agent framework for github issue resolution. Advances in Neural Information Processing Systems (NeurIPS), 37: 5196351993. Van Landeghem, J.; Tito, R.; Borchmann, Ł.; Pietruszka, M.; Joziak, P.; Powalski, R.; Jurkiewicz, D.; Coustaty, M.; Anckaert, B.; Valveny, E.; et al. 2023. Document underIn Proceedings of standing dataset and evaluation (dude). the IEEE/CVF International Conference on Computer Vision (ICCV), 1952819540. Wang, E. Z.; Cassano, F.; Wu, C.; Bai, Y.; Song, W.; Nath, V.; Han, Z.; Hendryx, S. M.; Yue, S.; and Zhang, H. 2025a. Planning in Natural Language Improves LLM Search for Code Generation. In International Conference on Learning Representations (ICLR). Wang, J.; Xu, H.; Jia, H.; Zhang, X.; Yan, M.; Shen, W.; Zhang, J.; Huang, F.; and Sang, J. 2024a. Mobile-Agentv2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration. Advances in Neural Information Processing Systems (NeurIPS), 37: 26862710. Wang, K.; Pan, J.; Shi, W.; Lu, Z.; Ren, H.; Zhou, A.; Zhan, M.; and Li, H. 2024b. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems (NeurIPS), 37: 9509595169. Wang, Q.; Ding, R.; Chen, Z.; Wu, W.; Wang, S.; Xie, P.; and Zhao, F. 2025b. Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017. Wang, W.; Gao, Z.; Chen, L.; Chen, Z.; Zhu, J.; Zhao, X.; Liu, Y.; Cao, Y.; Ye, S.; Zhu, X.; et al. 2025c. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291. Wang, X.; Pei, J.; Shen, W.; Peng, Y.; Hao, Y.; Qiu, W.; Jian, A.; Xie, T.; Song, X.; Liu, Y.; et al. 2025d. Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning. arXiv preprint arXiv:2505.07263. Wang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y.; Wang, J.; Zhang, F.; Wang, Y.; Li, Z.; Yu, Q.; et al. 2024c. Emu3: arXiv preprint Next-token prediction is all you need. arXiv:2409.18869. Wang, Y.; Zhou, W.; Feng, H.; Zhou, K.; and Li, H. 2023. Towards improving document understanding: An exploration on text-grounding via mllms. arXiv preprint arXiv:2311.13194. Wang, Z.; Xia, M.; He, L.; Chen, H.; Liu, Y.; Zhu, R.; Liang, K.; Wu, X.; Liu, H.; Malladi, S.; et al. 2024d. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems (NeurIPS), 37: 113569113697. Wu, X.; Yang, J.; Chai, L.; Zhang, G.; Liu, J.; Du, X.; Liang, D.; Shu, D.; Cheng, X.; Sun, T.; et al. 2025. Tablebench: comprehensive and complex benchmark for table question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2549725506. Wu, Z.; Chen, X.; Pan, Z.; Liu, X.; Liu, W.; Dai, D.; Gao, H.; Ma, Y.; Wu, C.; Wang, B.; et al. 2024. Deepseek-vl2: Mixture-of-experts vision-language models arXiv preprint for advanced multimodal understanding. arXiv:2412.10302. xAI. 2024. Realworldqa: benchmark for real-world spatial understanding. Xia, B.; Shen, B.; Zhu, D.; Zhang, D.; Wang, G.; Zhang, H.; Liu, H.; Xiao, J.; Dong, J.; Zhao, L.; et al. 2025. MiMo: Unlocking the Reasoning Potential of Language arXiv preprint ModelFrom Pretraining to Posttraining. arXiv:2505.07608. Xiaomi, L.-C.-T. 2025. MiMo-VL Technical Report. arXiv preprint arXiv:2506.03569. Yang, Q. A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang, J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.; Dang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue, M.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.; Xia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan, Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5 Technical Report. Yang, Z.; Zhou, Z.; Wang, S.; Cong, X.; Han, X.; Yan, Y.; Liu, Z.; Tan, Z.; Liu, P.; Yu, D.; Liu, Z.; Shi, X.; and Sun, M. 2024. MatPlotAgent: Method and Evaluation for LLMBased Agentic Scientific Data Visualization. In Findings of the Association for Computational Linguistics: ACL 2024, 1178911804. Association for Computational Linguistics. Yasunaga, M.; Chen, X.; Li, Y.; Pasupat, P.; Leskovec, J.; Liang, P.; Chi, E. H.; and Zhou, D. 2024. Large language models as analogical reasoners. In International Conference on Learning Representations (ICLR). Ye, J.; Hu, A.; Xu, H.; Ye, Q.; Yan, M.; Xu, G.; Li, C.; Tian, J.; Qian, Q.; Zhang, J.; Jin, Q.; He, L.; Lin, X.; and Huang, F. 2023. UReader: Universal OCR-free Visuallysituated Language Understanding with Multimodal Large Language Model. In Findings of the Association for Computational Linguistics: EMNLP 2023, 28412858. Association for Computational Linguistics. Ye, J.; Xu, H.; Liu, H.; Hu, A.; Yan, M.; Qian, Q.; Zhang, J.; Huang, F.; and Zhou, J. 2025. mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models. In International Conference on Learning Representations (ICLR). Yu, P.; Xu, J.; Weston, J.; and Kulikov, I. 2024a. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023. Yu, X.; Wang, C.; Jin, H.; Elazab, A.; Jia, G.; Wan, X.; Zou, C.; and Ge, R. 2025. CRISP-SAM2: SAM2 with CrossModal Interaction and Semantic Prompting for Multi-Organ Segmentation. arXiv preprint arXiv:2506.23121. Yu, Y.; Yao, Z.; Li, H.; Deng, Z.; Jiang, Y.; Cao, Y.; Chen, Z.; Suchow, J.; Cui, Z.; Liu, R.; et al. 2024b. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems (NeurIPS), 37: 137010137045. Yue, S.; Wang, S.; Chen, W.; Huang, X.; and Wei, Z. 2025. Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 25796 25804. Zhang, J.; Xu, X.; Zhang, N.; Liu, R.; Hooi, B.; and Deng, S. 2024a. Exploring Collaboration Mechanisms for LLM Agents: Social Psychology View. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 1: Long Papers), 1454414607. Association for Computational Linguistics. Zhang, K.; Li, B.; Zhang, P.; Pu, F.; Cahyono, J. A.; Hu, K.; Liu, S.; Zhang, Y.; Yang, J.; Li, C.; et al. 2024b. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772. Zhang, Q.; Lyu, F.; Sun, Z.; Wang, L.; Zhang, W.; Hua, W.; Wu, H.; Guo, Z.; Wang, Y.; Muennighoff, N.; et al. 2025a. Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well? arXiv preprint arXiv:2503.24235. Zhang, R.; Jiang, D.; Zhang, Y.; Lin, H.; Guo, Z.; Qiu, P.; Zhou, A.; Lu, P.; Chang, K.-W.; Qiao, Y.; et al. 2024c. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision (ECCV), 169186. Springer. Zhang, Y.; Ma, Z.; Ma, Y.; Han, Z.; Wu, Y.; and Tresp, V. 2025b. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2337823386. Zhang, Y.; Zhang, R.; Gu, J.; Zhou, Y.; Lipka, N.; Yang, D.; and Sun, T. 2023. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107. Zhang, Z.; Rossi, R.; Yu, T.; Dernoncourt, F.; Zhang, R.; Gu, J.; Kim, S.; Chen, X.; Wang, Z.; and Lipka, N. 2024d. VipAct: Visual-perception enhancement via specialized vlm agent collaboration and tool-use. arXiv preprint arXiv:2410.16400. Zhao, J.; Liu, R.; Zhang, K.; Zhou, Z.; Gao, J.; Li, D.; Lyu, J.; Qian, Z.; Qi, B.; Li, X.; et al. 2025. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891. Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Duan, Y.; Tian, H.; Su, W.; Shao, J.; et al. 2025. InternVL3: Exploring Advanced Training and Test-Time arXiv Recipes for Open-Source Multimodal Models. preprint arXiv:2504.10479. Zuo, Y.; Zhang, K.; Qu, S.; Sheng, L.; Zhu, X.; Qi, B.; Sun, Y.; Cui, G.; Ding, N.; and Zhou, B. 2025. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084."
        },
        {
            "title": "Appendix\nRelated Works",
            "content": "General VLMs Recent advancements in general VLMs have largely bridged the gap between visual and linguistic information, demonstrating impressive abilities in visual understanding, reasoning, and question answering. For example, latest GPT4o (Hurst et al. 2024), GPT-4V (OpenAI 2024), Gemini2.0-Pro (DeepMind 2025), DeepSeek-VL2 (Wu et al. 2024), Qwen2.5-VL (Bai et al. 2025), Claude-3.7-Sonnet (Anthropic 2024), InternVL-3 (Zhu et al. 2025), Llama3.2-Vision (Grattafiori et al. 2024), Ovis2 (Lu et al. 2024b) and MiMo-VL (Xiaomi 2025) contribute to push the performance boundaries in many aspects. However, there is still room for improvement for these general VLMs in terms of specific downstream tasks in various scenarios. Furthermore, their derivative models, such as LLaVA-OneVision (Li et al. 2025b), Monkey (Li et al. 2024), Emu3 (Wang et al. 2024c), InternLM-XComposer24KHD (Dong et al. 2024a) and mPLUG-Owl3 (Ye et al. 2025), contribute to push the performance boundaries in many aspects. Specialized VLMs To better promote visual understanding and question answering, especially in document-based tasks, such as texts, webpages, charts, and tables, researchers have designed numerous task-specialized VLMs. For instance, TGDoc (Wang et al. 2023) and MMCA (Liu et al. 2024a) are explorations of document-based or chart-based understanding via largescale collected instruction tuning data, and DocPedia (Feng et al. 2024) deals with the visual inputs at the frequency domain. To improve the comprehension of screenshots and webpages, UReader (Ye et al. 2023) designs cropping strategy, while CogAgent (Hong et al. 2024) and TextMonkey (Liu et al. 2024b) use two resolution branches or shifted window to accommodate document-oriented tasks. The series of works by mPLUG-DocOwl (Hu et al. 2024a,b) design high-resolution compressor for multi-page document understanding with reduced visual token costs. Besides, ColPali (Faysse et al. 2025), M3DocRAG (Cho et al. 2024), and MDocAgent (Han et al. 2025) introduce efficient and novel retrieval mechanisms for VLVMs to comprehend long documents. Multi-Agent Models Agentic large models have gained tremendous popularity; however, single-agent systems are increasingly unable to meet the demands of complex and novel tasks. Thus, communicative agents mechanisms are implemented to improve cooperation and reaction between agents by two primary forms, i.e., debate (Du et al. 2024; Zhang et al. 2024a; Yue et al. 2025; Pan et al. 2024) and competition (Cheng et al. 2024; Chen et al. 2024a). Although the domain of LLM-based multi-agent models has expanded to include code generation (Islam, Ali, and Parvez 2024; Tao et al. 2024), financial decision-making (Yu et al. 2024b), and other areas, they still suffer from lack of visual understanding. Thus, more recently, some recent works attempt to explore how multi-agent interaction benefits VLMs. For instance, Insight-V (Dong et al. 2024b) divides the functions on reasoning and summary agents, while MobileAgent-v2 (Wang et al. 2024a) and (Li et al. 2025c) introduce specialized agents for planning. To accommodate different tasks, METAL (Li et al. 2025a), WebPilot (Zhang et al. 2025b), ViDoRAG (Wang et al. 2025b), MatPlotAgent (Yang et al. 2024), and VipAct (Zhang et al. 2024d) further break down the labor of single agent into more manageable sub-instances and specialise the function of each role-tailored agent. Additionally, MuMA-ToM (Shi et al. 2025) and MDocAgent (Han et al. 2025) design separate image and text agents and then fuse them, ignoring the correlations between the two modalities. However, practical multi-agent collaboration framework for visual document understanding and question answering is still deficient. Test-Time Scaling As promising and burgeoning research focus, test-time scaling can further elicit various capabilities of models with relatively lower computational cost (Zhang et al. 2025a). The scaling could be categorized into four types: parallel scaling (Renze 2024; Brown et al. 2024; Wang et al. 2025a; Liu et al. 2025), generating multiple candidates and aggregating them; sequential scaling (Gou et al. 2024; Chen et al. 2024b; Yu et al. 2024a), iteratively updating intermediate states and triggering self-correction; Hybrid scaling (Besta et al. 2024; Guan et al. 2025; Snell et al. 2025) is another realization, which exploits their complementary benefits; internal scaling, which autonomously allocates computational tokens, e.g. budget forcing strategy (Muennighoff et al. 2025; Son et al. 2025; Li et al. 2025a). To further enhance the ability of models to scale at test time, supervised fine-tuning (SFT) and reinforcement learning (RL) are also widely employed to improve scaling performance. For example, an intuitive and efficient strategy is to set reward model, such as process reward modeling (Snell et al. 2025; Zhao et al. 2025) and outcome reward modeling (Liu et al. 2025; Zuo et al. 2025; Setlur et al. 2025), offering quantitative reward values for test-time scaling."
        },
        {
            "title": "Experimental Settings",
            "content": "Training Pipeline In this work, we design two-stage training pipeline to harness the full potential of our framework, specifically the SFT stage and RL stage. Moreover, we select three groups of base models for different variants, which are shown in Table 5. To enhance the general abilities, during the first stage, we employ AdamW optimizer with β = (0.9, 0.95), and the initial learning rate and weight decay are set to 2e5 and 0.1 respectively. We train our model for one epoch with batch size of 128. In the second RL stage, we adopt the GRPO (Shao et al. 2024) strategy for further optimisation. Following its original configurations, we set the learning rate, KL coefficient, and number of iterations as e6, 0.04, and 1, respectively. The reward signals are generated by VisualPRM (Wang et al. 2025c) and Skywork-VLReward (Wang et al. 2025d); the former provides step-bystep signals for planning and execution agents, while the latter generates rewards for judgment and answer agents. During the process, we set the temperature parameter to 0.75 to ensure diversity and creativity in the generated solutions. It should be noted that reward models and our framework are loaded on different servers, thereby avoiding the repeated loading and offloading of model weights. All the training and inference are conducted on 8 NVIDIA A100 GPUs. Datasets Our selected datasets cover four document types: text-based, webpage-based, table-based, and chart-based benchmarks, and also two non-document types: general and mathematical benchmarks. For each type, two to four datasets are included for both training and evaluation benchmarks. To better illustrate the datasets, statistical information provided in Table 6 and brief introduction of each dataset are listed as follows: DocVQA (Mathew, Karatzas, and Jawahar 2021). It is classic document-based dataset, with more than 12K images from various documents and 50K related questions. Each question corresponds to single-page document, which requires relatively simple visual understanding. The average lengths of questions and answers are 8.3 and 2.1 words. DUDE (Van Landeghem et al. 2023). It is multipage document question answering dataset, comprising 41,541 questions and over 5,000 documents. The average number of pages in the document of this dataset is 5.72, while the average lengths of the questions and answers are 8.7 and 3.4, respectively. SlideVQA (Tanaka et al. 2023). It is slide question answering dataset, which incorporates 52K images from slides and about 15K questions. The average lengths of the questions and answers are 9.0 and 2.4 words, and they include certain proportion of multi-hop and numerical reasoning. MMLongBench-Doc (Ma et al. 2025). It introduces multi-page dataset for document understanding, consisting of about 6K document images and 1K questions. Depending on multi-hop evidence, the questions and answers have 16.4 and 2.8 words on average, and almost half of the questions are cross-page, while one-fifth of the questions are unanswerable. VisualMRC (Tanaka, Nishida, and Yoshida 2021). It collects 10K images sourced from 35 domains of webpages and more than 30K annotated questions with an average length of 10.6 words. The answers, with an average length of 9.5 words, in this dataset include both word spans in the context and the layouts of the webpages, which are more complex and diverse, with more abstract questions. InfographicVQA (Mathew et al. 2022). It consists of more than 5K images collected from webpages and 30K questions, constructed with textual, graphical, and visual elements. Most questions in this dataset are extractive and numerical, testing basic reasoning and arithmetic skills, with 11.5 and 1.6 words in questions and answers on average. ChartQA (Masry et al. 2022). It introduces dataset with 22K images and 34K questions, involving bar, line, pie charts, and composite plots. The questions in this dataset require visual, logical, and arithmetic ability, with an average length of questions and answers of 7.2 and 1.2 words. CharXiv (Wang et al. 2024d). It presents dataset for chart understanding and answering, with 2K chart images and 13K curated questions. It includes both descriptive and reasoning sub-datasets, and we merge them. The questions and answers have 22.6 and 2.8 words evenly. Figure 5: Illustrations of representative samples from our selected 15 benchmarks on both document and non-docuement tpyes. TableBench (Wu et al. 2025). It is dataset with tabular and textual hybrid contents, constructed with over 4K tables and 21K questions, with numerical reasoning, such as computing, comparison, and sorting. Its questions and answers have lengths of 20.3 and 8.5 words on average. TableVQA-Bench (Kim, Yim, and Song 2024). It establishes dataset for table-based question answering, extended with four existing sources with 1K tables and 2K questions. Consisting of diverse types of questions, the average lengths of questions and answers are 10.5 and 4.7 words. ScienceQA (Saikh et al. 2022). It is dataset that includes about 21K multiple-choice questions based on more than 10K images from diverse scientific fields, and the length of the questions and answers is 12.1 words on average. RealWorldQA (xAI 2024). It consists of about 1K questions and 1K images. The average length of the questions in this dataset is 11.1 words, while that of the answers is only 1.2 words. MathVista (Lu et al. 2024a). It proposed dataset integrating diverse mathematical and visual tasks with 5K images and 6K questions, demanding fine-grained visual understanding and compositional reasoning. On average, the length of the questions and answers is 15.6 and 1.2 words, respectively. Math-Vision (Wang et al. 2024b). It is dataset integrating diverse mathematical tasks with about 3K images and 3K questions from real exams, spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty. It has the longest average length of questions (42.3 words) in our selected datasets, and 2.2 words in answers on average. MathVerse (Laurencon et al. 2024). It is mathematical dataset with about 3K images and 16K questions. On average, the lengths of the questions and answers in this dataset are 35.7 and 1.4 words, respectively. For some datasets, we use pre-treatments to handle the visual inputs. For the document in some datasets, which is provided in PDF format, we transform these files into visual"
        },
        {
            "title": "Type",
            "content": "Qwen2.5-VL Series MiMo-VL Series (Bai et al. 2025; Yang et al. 2025) (Xiaomi 2025; Xia et al. 2025) InternVL3 Series (Zhu et al. 2025) Planning Agent VLM Execution Agent VLM Judgment Agent LLM* LLM* Answer Agent Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-3B-Instruct"
        },
        {
            "title": "Size",
            "content": "- 7B+7B+7B+3B =24B MiMo-VL-7B-SFT MiMo-VL-7B-SFT MiMo-7B-SFT MiMo-7B-SFT 7B+7B+7B+7B =28B InternVL3-9B-Instruct InternVL3-9B-Instruct InternVL3-8B-Instruct InternVL3-3B-Instruct 9B+9B+8B+2B =28B Table 5: The base models of each variant of MACT. * indicates that we use VLMs, which remain powerful textual ability, because in InternVL3 series the model family does not provide LLMs."
        },
        {
            "title": "Num I Num Q Avg Q Avg A Avg I Per Q",
            "content": "DocVQA (Mathew, Karatzas, and Jawahar 2021) DUDE (Van Landeghem et al. 2023) SlideVQA (Tanaka et al. 2023) MMLongBench-Doc (Ma et al. 2025) VisualMRC (Tanaka, Nishida, and Yoshida 2021) Webpage InfographicVQA (Mathew et al. 2022) Webpage Chart ChartQA (Masry et al. 2022) Chart CharXiv (Wang et al. 2024d) Table TableVQA-Bench (Kim, Yim, and Song 2024) Table TableBench (Wu et al. 2025)"
        },
        {
            "title": "Text\nText\nText\nText",
            "content": "ScienceQA (Saikh et al. 2022) RealWorldQA (xAI 2024) MathVista (Lu et al. 2024a) Math-Vision (Wang et al. 2024b) MathVerse (Zhang et al. 2024c)"
        },
        {
            "title": "General\nGeneral\nMathematical\nMathematical\nMathematical",
            "content": "12K 5K 52K 6K 10K 5K 22K 2K 1K 21K 10K 1K 5K 3K 3K 50K 41K 15K 1K 30K 30K 34K 13K 2K 4K 21K 1K 6K 3K 16K 8.3 8.7 9.0 16.4 10.6 11.5 7.2 20.4 10.5 20.3 12.1 11.1 15.6 42.3 35. 2.1 3.4 2.4 2.8 9.5 1.6 1.2 2.9 4.7 8.5 4.4 1.2 1.2 2.2 1.4 1.0 5.7 20.0 47.5 1.0 1.2 1.0 1.0 1.0 1.0 0.5 1.0 1.0 1.0 1.0 Table 6: Statistical information of our selected datasets. Here, Num and Avg indicate the total number and the average length, while I, Q, and represent images, questions, and answers, respectively. images as input. Besides, we design standardised visualisation process for part of the tables in the TableBench (Wu et al. 2025) dataset, which only contains HTML format data without authentic visual images. Additionally, we utilize the PDF2Image library to convert PDF files into images for input. Evaluations We onboard the unavailable models in LMMs-Eval (Zhang et al. 2024b) to evaluate their performances with the consistent configurations. We directly use the natively supported benchmarks in the LMMs-Eval framework, namely DocVQA (Mathew, Karatzas, and Jawahar 2021), InfographicVQA (Mathew et al. 2022), ChartQA (Masry et al. 2022), ScienceQA (Saikh et al. 2022), RealWorldQA (xAI 2024), MathVista (Lu et al. 2024a), and MathVerse (Zhang et al. 2024c). In terms of unavailable benchmarks in LMMsEval, namely DUDE (Van Landeghem et al. 2023), SlideVQA (Tanaka et al. 2023), MMLongBench-Doc (Ma et al. 2025), VisualMRC (Tanaka, Nishida, and Yoshida 2021), CharXiv (Wang et al. 2024d), TableVQA-Bench (Kim, Yim, and Song 2024), TableBench (Wu et al. 2025), and MathVision (Wang et al. 2024b), we onboard the benchmarks and register them by YAML configurations. To evaluate the performance of models on these benchmarks fairly and equitably, we employ range of evaluation metrics. For DocVQA (Mathew, Karatzas, and Jawahar 2021), DUDE (Van Landeghem et al. 2023), InfographicVQA (Mathew et al. 2022) benchmarks, we follow the original evaluation metrics, i.e., ANLS (Average Normalised Levenshtein Similarity) (Biten et al. 2019), to measure the difference between the predicted answers and ground truths. Moreover, the F1 score is utilized for SlideVQA (Tanaka et al. 2023). For other selected benchmarks, we use powerful GPT-4o (Hurst et al. 2024) as judge model to evaluate the answers by LMMs-Eval, which generates scores from 1 to 10, and then the scores will be scaled to an accuracy between 0 and 100 in the dimension of benchmarks. Prompt Templates As shown in Figure 6, there are prompts for the four agents, including the system prompts and the prompts for each process. Apart from the system prompts, Aplan and Aexe have two extra prompts to accomplish their functions, while Ajudg and Aans have only one. In the separating prompt of Aexe, each execution unit is comprised of: (1) specific definition; (2) expected target and output; (3) existing inputs or results from the previous step."
        },
        {
            "title": "Results and Discussions",
            "content": "Additional Quantitative Results For the line graphs and bar charts shown in Figure 4a and the line graphs in Figure 4b, we list the quantitative results Figure 6: Prompts for the four agents. in Table 7 and Table 8, respectively, for clarity. Compared with the baseline, our judgment agent strategy for correction improves the scores by 6.2% on average when the maximum number of corrections is set to 3. Furthermore, it still has 5.3% and 2.6% increases when compared to the other two correction strategies. Besides, our strategy achieves the best performance with fewer average number of corrections, indicating high efficiency and accuracy. In terms of the impact of the number of generated plans Np and the number of candidate executions Ne, it is evident that higher values of the two parameters cause superior performance."
        },
        {
            "title": "Max Num of Corrections Avg Num of\nCorrections",
            "content": "10 3 5 7 Baseline Internal Correction Agent for Judgment and Correction 40.2 40.8 42.0 41.5 45.1 46.7 44.6 Independent Judgment Agent (Ours) 47.6 47.5 45."
        },
        {
            "title": "Baseline\nInternal Correction\nAgent for Judgment and Correction",
            "content": "68.8 69.5 70.3 69.6 72.2 72.6 71.9 Independent Judgment Agent (Ours) 74.8 74.7 73.7 38.2 41.2 42.9 64.2 68.9 71.2 - 2.8 2.3 1.9 - 2.3 1.6 1.3 Table 7: Comparisons among the three correction strategies of various maximum numbers of corrections and the average number of corrections when the limitation is set to 3. Line with denotes the results on the three selected benchmarks, while another line is the average results on all benchmarks. Ne Np 1 2 4 8 16 1 2 8 16 70.2 70.8 72.0 72.5 72.8 73.4 - 71.1 73.9 - 71.5 72.0 74.5 - 72.2 72.9 73.4 74.1 74.8 - - - - - - Table 8: Results of different values of Np and Ne on all benchmarks."
        }
    ],
    "affiliations": [
        "Fudan University, China",
        "Nanyang Technological University, Singapore",
        "National University of Singapore, Singapore",
        "Technical University of Munich, Germany",
        "Tsinghua University, China",
        "University of Science and Technology of China, China",
        "Zhejiang University, China"
    ]
}