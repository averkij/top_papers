{
    "paper_title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
    "authors": [
        "Sukannya Purkayastha",
        "Zhuang Li",
        "Anne Lauscher",
        "Lizhen Qu",
        "Iryna Gurevych"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)"
        },
        {
            "title": "Start",
            "content": "LAZYREVIEW Dataset for Uncovering Lazy Thinking in NLP Peer Reviews Sukannya Purkayastha1, Zhuang Li2, Anne Lauscher3, Lizhen Qu4, Iryna Gurevych1 1 Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt 2 School of Computing Technologies, Royal Melbourne Institute of Technology, Australia 3 Data Science Group, University of Hamburg 4 Department of Data Science & AI, Monash University, Australia www.ukp.tu-darmstadt.de"
        },
        {
            "title": "Abstract",
            "content": "Peer review is cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of quick heuristics, referred to as lazy thinking, has emerged as recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LAZYREVIEW, dataset of peer-review sentences annotated with finegrained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in zeroshot setting. However, instruction-based finetuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community."
        },
        {
            "title": "Introduction",
            "content": "Peer Reviewing is widely regarded as one of the most effective ways to assess the quality of sciIt is entific papers (Ware and Mabe, 2009). distributed procedure where the experts (reviewers) independently evaluate whether submitted manuscript adheres to the standards of the field. With the Mathew effect (Merton, 1968) in science (rich get richer), where the researchers receive benefits throughout their career for having papers at prestigious venues, it is of utmost importance to ensure sound practices in the reviewing process. Figure 1: Illustration of lazy thinking in ARR-22 reviews sourced from NLPEER (Dycke et al., 2023). The first review segment belongs to the class The results are not novel. The last segment pertains to, The approach is tested only on [not English], so unclear if it will generalize to other languages. as per ARR-22 guidelines. With the growing load of paper submissions, reviewers often face an overwhelming workload (Landhuis, 2016; Künzli et al., 2022) to assess multiple manuscripts quickly. When presented with cognitively hard task (e.g., reviewing) coupled with information overload and limited time, humans often end up using simple decision rules, also known as heuristics (Tversky et al., 1974). Though simple and efficient, these heuristics can often lead to errors and unfair evaluations (Raue and Scholl, 2018). The usage of such heuristics to dismiss research papers in the Natural Language Processing (NLP) Community has been termed as lazy thinking (Rogers and Augenstein, 2021). One such example is shown in Fig 1. Here, the reviewer dismisses the paper in the first review segment for not being an eye-opener. However, they do not provide any references regarding similar prior work or feedback to improve the paper. This corresponds to the lazy thinking class The results are not novel. In 2021, when ACL Rolling Review (ARR) was adopted as one of the main reviewing platforms for major NLP conferences, these heuristics were added to the guidelines (Rogers and Augenstein, 2021), aiming to discourage reviewers from relying on such approaches.2 However, in their ACL 2023 report, Rogers et al. (2023) identified the usage of these heuristics as one of the leading factors (24.3%) of author-reported issues in peer reviews. 1Code available here: https://github.com/UKPLab/ arxiv2025-lazy-review 2https://aclrollingreview.org/ 5 2 0 2 5 1 ] . [ 1 2 4 0 1 1 . 4 0 5 2 : r Therefore, in this work, we focus on this pertinent issue, lazy thinking in NLP peer reviewing and heed Kuznetsov et al. (2024)s call for automated methods to signal such occurrences in order to improve the overall reviewing quality. To have finer look at the problem of lazy thinking in NLP peer reviews, we introduce LAZYREVIEW, dataset with 500 expert-annotated review segments and 1276 silver annotated review segments from the best-performing model tagged with lazy thinking classes with review segment consisting of 1 or more review sentences. We develop this dataset over three rounds of annotation and incrementally sharpen the guidelines guided by interannotator agreements. We further provide positive examples, i.e., annotated examples for each class, to enhance the understanding of annotators for this task and reach annotation consensus sooner. Annotating review segments by new batch of annotators who were not involved in developing the guidelines resulted in Cohens κ (Landis and Koch, 1977) values of 0.32, 0.36, and 0.48, respectively. The steady increase in agreement across rounds justifies the effectiveness of the developed guidelines. With this dataset, we further test the zero-shot capabilities of LLMs in identifying lazy thinking, which can be leveraged downstream to identify rule-following behaviour (Sun et al., 2024b) in peerreview scenarios. Despite likely exposure to peer reviews via platforms like OpenReview during pretraining, LLMs struggle to accurately identify the type of lazy thinking when presented with the current guidelines. To enhance their comprehension of lazy thinking, we employ instruction-tuning on the LLMs using our dataset, leading to significant performance gainsaround 10-20 accuracy points compared to their zero-shot and few-shot performances. Finally, we perform controlled experiment where human reviewers rewrite peer reviews with(out) using annotations of lazy thinking from our dataset. Human preference-based evaluations reveal that reviews written with the lazy thinking guidance are more comprehensive and actionable than their vanilla counterparts. Contributions. We make the following contributions: (1) Introduce LAZYREVIEW, dataset annotated with lazy thinking classes for new task (2) Enin model development and evaluation. hance annotation guidelines to improve both automated and human annotations. (3) Demonstrate that positive examples boost annotation quality and in-context learning. (4) Show that instruction tuning on our dataset enhances model performance. (5) Find that annotated lazy thinking classes improve review quality."
        },
        {
            "title": "Lazy Thinking in Peer Reviews",
            "content": "In line with Rogers and Augenstein (2021), we first define lazy thinking as follows. Definition. Lazy thinking, in the context of NLP research paper reviews, refers to the practice of dismissing or criticizing research papers based on superficial heuristics or preconceived notions rather than thorough analysis. It is characterized by reviewers raising concerns that lack substantial supporting evidence and are often influenced by prevailing trends within the NLP community. Rogers and Augenstein (2021) enlist total of 14 types of lazy thinking heuristics in the ARR 2021 guidelines adapted from the study in Rogers and Augenstein (2020). We show some of the classes as described in the guidelines in Table 1.3 Reiterating the example in Fig 1, we note that such claims about novelty need to be backed up with supporting evidence and hence constitute classic example of lazy thinking as per the guidelines. In this section, we describe the creation of our dataset, LAZYREVIEW guided by the ARR 2022 (Rogers and Augenstein, 2021) and the EMNLP 2020 guidelines (Liu et al., 2020). We describe the dataset curation process followed by an analysis of the dataset. This is the first dataset of annotated instances with fine-grained lazy thinking classes for NLP peer reviews."
        },
        {
            "title": "2.1 Review Collection and Sampling",
            "content": "We use the ARR 2022 reviews from the NLPEER Dataset (Dycke et al., 2023) in which the reviews are collected using the 3Y-pipeline (Dycke et al., 2022) and as such has clear licenses attached. The dataset comprises reviews from five venues: CONLL 2016, ACL 2017, COLING 2020, ARR 2022, and F1000RD (an open-source science journal), with 11K review reports sourced from 5K papers. Focusing on the lazy thinking definition in the ARR 2022 guidelines (Rogers and Augenstein, 2021), we consider only the ARR-22 reviews, using 684 reviews from 476 papers with 11,245 sentences in total. Each review is divided into sections like paper summary, summary of strengths, summary of weaknesses, and comments, typos, and 3The full table is provided in Table 7 in Appendix A."
        },
        {
            "title": "The paper has language errors",
            "content": "Many findings seem obvious in retrospect, but this does not mean that the community is already aware of them and can use them as building blocks for future work. Transfer learning does not look to bring significant improvements. Looking at the variance, the results with and without transfer learning overlap. This is not surprising. Such broad claims need to be backed up with references. The approach the authors propose is still useful but not very novel. As long as the writing is clear enough, better scientific content should be more valuable than better journalistic skills. The paper would be easy to follow with English proofreading even though the overall idea is understandable. Table 1: Descriptions for some of the lazy thinking classes sourced from ARR 2022 guidelines (Rogers and Augenstein, 2021). We present some examples corresponding to these classes from our dataset, LAZYREVIEW. suggestions, with reviewers completing the relevant sections based on their evaluation. However, there is no standardized format for expressing concerns, so we use automated methods to extract relevant review segments, as detailed later. confidence levels: low, medium, or high. To aid in understanding, we added two additional classes: None for no lazy thinking, and Not Enough Information for instances lacking specificity or needing the paper for proper classification."
        },
        {
            "title": "2.2 Review Formatting",
            "content": "We utilize GPT-4 (OpenAI et al., 2024) to prepare the instances (review segments) for subsequent annotations. We instruct GPT-4 to extract the review segments from the Summary of Weaknesses section of the review that can be classified into one of the lazy thinking classes, as outlined in the ARR guidelines4. Since lazy thinking often contributes to reasons for rejecting paper, we believe it will appear only in the Summary of Weaknesses section. We obtain 1,776 review segments after this step, with each segment having varied lengths from 1 to 5 review sentences, as described later in Sec 2.5. To validate the quality of the extracted segments, we sample set of 100 segments from this pool and task three annotators to independently annotate each segment within the context of the entire review to decide on their candidacy towards lazy thinking. The final label is chosen based on majority vote. We obtain Cohens κ of 0.82 and obtain precision, recall and F1 scores as 0.74, 1.00 and 0.85, respectively. Intuitively, this means that GPT-4 samples more candidates than required, giving us larger pool of candidates for annotation. We therefore introduce another class to our labelling schema, None to annotate non-lazy thinking candidates."
        },
        {
            "title": "2.3 Annotation Protocol",
            "content": "Annotators are given the full review and the target review segment (highlighted) to classify according to incrementally developed lazy thinking guidelines based on ARR 2022. They also indicate their 4The prompt for GPT-4 is provided in Appendix A.4 Two Ph.D. students, both fluent in English and experienced in NLP peer reviewing, are tasked with annotating review segments iteratively, given that peer reviewing requires specialized expertise. senior PostDoc with an NLP background acts as third annotator to resolve any disagreements between the initial annotations. The guidelines evolve over multiple rounds. Once the guidelines are refined and finalized, we recruit new batch of annotators to re-validate them by asking them to annotate the same review segments. These new annotators follow the same set of guidelines used in the earlier rounds to ensure consistency. After this validation process, the original Ph.D. annotators are retained to annotate additional instances."
        },
        {
            "title": "2.4 Evolving Guidelines",
            "content": "We incrementally improve the guidelines guided by the inter-annotator agreements. Given the high subjectivity of this domain, we consider the guidelines to be precise once we achieve substantial agreement on annotating the instances further. Round 1: ARR 2022 Guidelines. In this round, the annotators are provided with the existing ARR (Rogers and Augenstein, 2021) guidelines and asked to annotate the lazy thinking classes. We sample set of 50 instances from the review segments extracted by GPT-4 as described in Sec 2.1. After the first round of annotation, we obtain Cohens kappa, κ of 0.31. This is substantially low agreement, revealing an ambiguity in the reviewing guidelines. The confusion matrix for the first round of annotation is shown in Fig 16a (cf. A.9). We find that there is high agreement for the None class, which implies that the annotators can easily detect review segment that is not problematic. However, they struggle to determine the fine-grained label of the problematic review segments. Further analysis of the confidence level of the annotators reveals that for most of the cases, the annotators have low confidence, as shown in Fig 17a (cf. A.9), which points towards ambiguity in the guidelines. Round 2: ARR 2022 and EMNLP guidelines. We further explored the initial reviewing guidelines released during EMNLP 2020 (Liu et al., 2020). We identify some additional classes that are now missing from the ARR guidelines, namely Non-mainstream Approaches (rejecting papers for not using current trending approaches) and Resource Paper (favoring resource papers lesser for ACL conferences). Additionally, we extend descriptions of some of the classes such as, This has no precedent in existing literature, The method is too simple using the guidelines in Liu et al. (2020). Moreover, we extend the name of some of the class labels based on the EMNLP 2020 guidelines, such as The paper has language errors with Writing Style; The topic is too niche with Narrow topics, which have similar meanings. We show the extended descriptions for those classes in Table 8 (cf. Appendix A.1). We annotate the same set of instances as in Round 1 and eventually calculate agreements. We obtain Cohens κ of 0.38, which is significantly higher than the previous round (0.31). We observe higher agreements for the classes having extended names such as, The paper has language errors and Niche Topics, as illustrated in Fig 16b (cf. A.9). The confidence level of the annotators also substantially increased from low to medium in this round, as can be seen in Fig 17b (cf. A.9). Round 3: Round 2 guidelines with positive examples. To promote quick learning through worked examples\" (Atkinson et al., 2000), we refine our annotation round by fixing the guidelines and incorporating positive examples for each lazy thinking class. We evaluated several techniques to select representative examples, measuring effectiveness using Cohens κ. The methods include (a) Random: selecting the shortest or longest review segments, and (b) Diversity: encoding segments with SciBERT (Beltagy et al., 2019), clustering them using K-Means, and choosing the cluster center with the lowest cosine distance. After pairwise comparisons, the random shortest method was preferred, achievFigure 2: Distribution of lazy thinking labels in our dataset, LAZYREVIEW. ing Cohens κ of 0.86. For round 3, we sampled 50 new review segments and included annotated examples from the previous round, resulting in Cohens κ of 0.52, indicating substantial agreement despite the tasks subjectivity, as shown in Fig 16c (cf. A.9). We also obtain higher annotator confidence levels as shown in Fig 17c (cf. A.9).5 Final Validation. To validate the guidelines developed over three rounds, we hired new group of English-speaking PhD students with NLP peer review expertise to annotate the same set of instances. These annotators were not involved in the development of the guidelines. Re-annotation across rounds 1, 2, and 3 resulted in inter-annotator agreement (κ) values of 0.32, 0.36, and 0.48, respectively. This steady increase in κ aligns with previous results of 0.31, 0.38, and 0.52, validating the iterative improvement of the guidelines based on stronger inter-annotator agreement. Agreement scores of 0.4 or higher are considered substantial due to the inherent subjectivity of the task and are consistent with agreement levels reported in previous studies within the peer-reviewing domain (Kennard et al., 2022; Purkayastha et al., 2023). We retain the annotators from the initial rounds who developed the guidelines to annotate total of 500 review segments as part of our dataset, LAZYREVIEW. The overall annotation time amounted to 20 hours. This corresponds to 25 minutes per example, ranging from less than minute for annotating shorter segments up to half an hour for longer ones."
        },
        {
            "title": "2.5 Dataset Analysis",
            "content": "Our dataset comprises 500 expert-annotated review segments tagged with 18 classes. Out of all the labels, 16 classes in our dataset correspond to ex5The examples used in this round are in Table 9 in A.1 plicit lazy thinking. The distribution of our dataset is shown in Fig 2 corresponding to these 16 classes. The most frequent lazy thinking is Extra Experiments, where the reviewers ask the authors to conduct additional experiments without proper justification. This trend reflects the current expectations in the NLP field, which has rapidly shifted towards machine learning models and methods that often emphasize extensive empirical evaluations (Gururaja et al., 2023). The next most frequent classes are Not Enough Novelty and Language Errors. The ACL review report does not state the individual distribution of these classes but constitutes around 24.3% of all reported issues. We further analyze the sentence length of the review segments within these classes. This is illustrated in Fig 18 of A.13. We observe that most of these review segments have length of 1 sentence, underscoring the use of shorter arguments to dismiss papers. The lazy thinking class Extra Experiment is the most common with variable segment lengths."
        },
        {
            "title": "3 Experiments",
            "content": "We use the LAZYREVIEW dataset to assess the performance of various open-source LLMs in detecting lazy thinking in NLP peer reviews."
        },
        {
            "title": "Experimental Setup",
            "content": "Tasks We propose two formulations for detecting lazy thinking in peer reviews: (i) Coarse-grained classification: binary task to determine if an input segment is lazy thinking, and (ii) Fine-grained classification: multi-class task to classify into one of the specific lazy thinking classes, ci C. Models. Since the guidelines for NLP conferences are mainly instructions, we explore various opensource instruction-tuned LLMs for this task. We experiment with the chat versions of LLaMa-2 7B and 13B (Touvron et al., 2023) (abbr. LLaMa, LLaMaL), Mistral 7B instruct (Jiang et al., 2023) (abbr. Mistral), Qwen-Chat 1.5 7B (Bai et al., 2023) (abbr. Qwen), Yi-1.5 6B instruct (AI et al., 2024) (abbr. Yi-1.5), Gemma-1.1 7B instruct (Team et al., 2024) (abbr. Gemma), and SciTülu 7B (Wadden et al., 2024) (abbr. SciTülu).6 Prompts. Following our annotation protocol, we prompt the LLMs with guidelines and instructions for each round. For coarse-grained classification, the model determines whether the input is an in6The justification for the choice of these models along with implementation details are in Appendix A.2 stance of lazy thinking or not. For fine-grained one, the model selects the best-fitting lazy thinking class from the provided options. We test two input types: the target segment (T) alone, or the combination of the review and target segment (RT).7 Metrics. To evaluate LLM outputs, we use both strict and relaxed measures because LLMs sometimes do not produce exact label phrases. The strict measure, as defined by Helwe et al. (2024), uses regular expressions to check for matches between the gold label and predictions, reporting accuracy and macro-F1 (string matching). The relaxed measure employs GPT-3.5 to judge whether the provided answer is semantically equivalent to the ground-truth class, outputting yes or no decision. This approach follows previous work on evaluating free-form LLM outputs (Wadden et al., 2024; Holtermann et al., 2024), and reports accuracy based on the number of yes responses. Both metrics determine whether predictions are correct or incorrect. We also performed an alignment study on 50 responses from every model to validate the reliability of the evaluators. We find that the stringmatching-based evaluator underestimates the correct predictions, whereas the GPT-based evaluator overestimates them, rendering correct balance of lower and upper bounds for the model predictions.8 RQ1: How effective are the improved guidelines in enhancing zero-shot performance of LLMs? Since the first two rounds of our annotation study are dedicated to fixing the guidelines for annotations, we evaluate the understanding of the LLMs on the same 50 instances on both annotation rounds, i.e., rounds 1 and 2, respectively. This validates whether the improved guidelines actually influence the zero-shot performance of LLMs. Modelling Approach. We prompt the LLMs for round 1, with the definition of lazy thinking classes, as shown in Table 1, representing the existing ARR guidelines. For round 2, we prompt the LLMs with the new guidelines as described in Sec A.1 where we added new classes and extended descriptions of the existing classes in the ARR guidelines using the EMNLP 2020 Blog (Liu et al., 2020). Results. We present results for fine-grained and coarse-grained classification of zero-shot LLMs in Table 2.9 Using only the target segment (T) as input generally outperforms using both the tar7Full details are in Appendix A.6. 8Details about the study are in Appendix A.5. 9Full results in Tables 10 and 11 of Appendix A.11. Models Fine-gr. Coarse-gr. R1 R1 R2 S.A G.A S.A G.A S.A G.A S.A G.A 7.11 RANDOM 11.1 MAJORITY 22.2 Gemma + 12.2 Gemma + RT 12.2 LLaMa + 12.2 LLaMa + RT LLaMaL + 26.7 LLaMaL + RT 15.6 27.8 Mistral + 12.2 Mistral + RT 21.1 Qwen + 12.2 Qwen + RT 35.3 Yi-1.5 + 34.4 Yi-1.5 + RT 14.4 SciTülu + 15.6 SciTülu + RT - - 52.2 46.7 15.6 25.6 44.4 41.1 47.8 28.9 46.7 43.3 56.7 51.1 18.1 17.3 4.34 7.34 26.7 11.6 22.2 13.2 26.7 17.6 28.8 16.6 22.7 13.3 37.6 32.8 25.3 18. - - 58.1 51.1 30.6 33.7 45.3 40.4 51.1 35.9 50.0 42.6 60.0 52.2 29.4 23.7 40.7 51.4 44.3 48.1 57.7 53.3 60.2 68.6 57.8 55.4 68.9 53.3 64.4 63.3 57.8 55.6 - - 51.1 47.4 70.0 55.1 73.1 69.4 64.8 53.8 74.1 53.3 71.1 65.1 57.8 55.6 40.7 51.4 46.1 50.4 60.0 60.0 62.2 70.2 58.8 57.4 70.4 56.5 68.7 68.3 58.3 58.7 - - 54.4 49.1 75.0 67.7 75.4 70.2 66.3 56.0 76.1 56.5 73.4 70.4 58.3 58.7 Table 2: LLM performance across annotation rounds in terms of string-matching (S.A) and GPT-based (G.A) accuracy for fine-grained (Fine-gr.) and coarse-grained (Coarse-gr.) tasks. uses only the target sentence, RT combines review and target. R1 and R2 represent Round 1 and Round 2. Increments from R1 to R2 are highlighted in red; decreases or no change in gray. get segment and review (RT) across most models, likely due to spurious correlations from longer inputs, as noted by Feng et al. (2023). All models improve from round 1 (R1) to round 2 (R2), with Gemma gaining 4.5 points in string accuracy (S.A). SciTülu, however, benefits from the broader context of RT, possibly due to its pre-training on longcontext tasks. Coarse-grained classification scores are consistently higher than fine-grained, showing LLMs can serve as basic lazy thinking detectors. RQ2: How effective are positive examples in improving the performance of LLMs? To test the effect of using positive examples on the performance of LLMs, we leverage in-context learning to test the models with the same 50 instances as used in round 3 of our annotation study. Modelling Approach. We prompt LLMs using positive examples from previous rounds and Round 2 guidelines, combining target segments (TE) and the combination of review and target segments (RTE) as in-context learning examples. Since LLMs have fixed context window, we employ two setups for selecting examples: (i) Static selection of fixed random examples for all inferences, and (ii) Dynamic selection on per test case basis using following methods: (a) BM25: uses BM25 to select examples, (b) Top K: uses embedding space similarity, and (c) Vote-K: penalizing already selected examples. We tested with 1, 2, and 3 exemplars.10 Results. We plot results in Fig 3 and 4 for fine10We leverage OpenICL (Wu et al., 2023) for retrieving exemplars and GPT2-XL as the embedding generator. Models Fine-gr. Coarse-gr. S.A G.A S.A G.A 2.46 RANDOM 5.11 MAJORITY 24.45.5 Gemma + TE 17.32.9 Gemma + RTE 15.64.5 LLaMa + TE 14.22.0 LLaMa + RTE LLaMaL + TE 24.413.3 LLaMaL + RTE 18.88.1 30.01.8 Mistral + TE 27.85.6 Mistral + RTE 31.12.2 Qwen + TE 27.81.1 Qwen + RTE 30.03.3 Yi-1.5 + TE 24.43.2 Yi-1.5 + RTE 23.31.1 SciTülu + TE 19.70.8 SciTülu + RTE - - 41.18.9 32.80.2 38.93.3 30.81.9 41.15.5 34.42.2 55.61.2 52.21.1 56.412.0 44.20.9 54.91.1 52.71.4 44.82.6 41.12.2 43.3 52.3 75.620.0 71.15.5 84.44.4 75.32.0 73.11.9 70.31.5 86.712.3 68.86.6 86.74.0 62.22.0 74.53.2 70.12.0 72.221.1 88.817.7 - - 88.931.7 82.216.6 89.13.0 81.14.4 71.110.0 61.19.9 86.711.5 68.86.6 86.74.0 62.22.0 73.81.5 72.42.3 72.220.0 91.122.3 Table 3: Performance of LLMs for round 3 in terms of the metrics used in Table 2 for fine-grained (Finegr.) and coarse-grained (Coarse-gr.) tasks. denotes adding in-context exemplars to input types: (target sentence) and RT (review + target sentence). Red: Increments with exemplars. Subscripts represent increments as compared to the zero-short versions. grained and coarse-grained classification (cf. A.7), finding that the static strategy outperforms all other setups, especially for Mistral and Qwen. Increasing exemplars does not enhance performance, supporting Srivastava et al. (2024) that random examples yield better results than similarity-based methods. This also can be attributed to the ability of LLMs to learn the format of the task from exemplars rather than learning to perform the task (Lu et al., 2024). We adopt static in-context learning method using 1 exemplar for the rest of the paper. We show the results using 1 static example in Table 3. ICL-based methods surpass zero-shot models, with notable gains in coarse classification (20 points for Gemma, 21 points for SciTülu stringbased accuracy (S.A.)). We also obtain positive increments across the board in fine-grained classifications. SciTülu excels in the coarse-grained classification, possibly due to the domain specific pre-training. Qwen leads in fine-grained classification, likely due to the multi-lingual pre-training data and high-quality data filtering performed for the pre-training phase (Bai et al., 2023). RQ3: How effective is instruction-tuning in improving the performance of LLMs? Building on previous work in aligning LLMs with new tasks (Ivison et al., 2023; Wadden et al., 2024), we apply instruction-based fine-tuning for lazythinking detection. Using bottom-up approach, we determine the data requirements using 3-fold cross-validations for maximum performance on small validation set. We then identify optimal data mixes for the test set. We subsequently use similar mix to train the models and then compare their performance to their non-instruction tuned counterparts from the previous annotation rounds and obtain silver annotations from the best model. Modelling Approach. We apply instruction-based finetuning for the same models as detailed in Sec 3. To optimize our limited computational resources, we employ LoRa (Hu et al., 2022) for parameter-efficient finetuning and utilize openinstruct (Wang et al., 2023).11 We use the same prompt (cf. 3) as instruction along with the round 2 guidelines (combination of guidelines of ARR and EMNLP blog) while performing this experiment. We use multiple data sources to create data mixes: (i) TÜLU V2 (Ivison et al., 2023) with 326,154 samples, (ii) SCIRIFF (Wadden et al., 2024) with 154K demonstrations across 54 tasks, and (iii) LAZYREVIEW with 1000 samples evenly split between coarse and fine-grained classification. LAZYREVIEW is divided into 70% training (700 examples), 10% validation (100 examples), and 20% testing (200 samples) in 3-fold crossvalidation setup. We train on 700 examples in the NO MIX setting. We use an equal number of instances (700) from each data source to balance the other mixtures. We create SCIRIFF MIX (1400 samples) and TÜLU MIX (1400 samples) by combining LAZYREVIEW with the other datasets and FULL MIX (2100 samples) integrating all three. We test the performance on the same cross-validated test sets for each mix. We use proportion of 0.3 for the (using only target segment) setup and 0.7 for the RT setup (using combination of review and target segment) from the different mixes to optimize performance and data efficiency based on hyper-parameter search on different validation sets.12 Results. We compare instruction-tuned models to their zero and few-shot counterparts for finegrained and coarse-grained classification using 3fold cross-validation, as shown in Tables 12 and 13 (cf. A.10). Instruction tuning significantly enhances model performance. The LLaMa models and SciTülu excel with the SCIRIFF MIX, benefiting from pre-training on SCIRIFF and TÜLU. Gemma and Qwen achieve their best results with the TÜLU MIX, with Qwen outperforming Gemma (45.5 vs. 31.6 S.A. accuracy), likely due to Qwens 11https://github.com/allenai/open-instruct 12Details on hyper-parameters are in A.8, with tuning methods described in A.8.3. Models Fine-gr. Coarse-gr. R1 R2 R3 R2 R3 Qwen Mistral 27.8 it + 35.47.6 it + RT 31.23.4 21.1 it + 45.924.8 it + RT 41.220. 26.7 LLaMaL it + 45.819.1 it + RT 41.214.5 12.2 LLaMa it + 43.831.6 it + RT 43.231.0 7.11 RAND. 11.1 MAJ. 22.2 Gemma it + 31.49.2 it + RT 28.26.0 40.7 51.4 48.1 57.89.7 55.67.5 57.7 62.75.0 61.23.5 68.6 74.35.7 70.21.6 57.8 60.22.4 65.37.5 68.9 75.46.5 73.24.3 64.4 69.55.1 67.22.8 57.8 66.38.5 62.44.6 43.3 52.3 75.6 81.25.6 78.83.2 84.4 85.41.0 81.33.1 73.1 75.32.2 73.30.2 86.7 86.40.3 88.21.5 86.7 88.41.7 86.30.4 74.5 78.43.9 73.21.3 88.8 91.22.4 87.21.6 4.34 7.34 26.7 38.812.1 35.79.0 22.2 47.825.6 45.323.1 26.7 47.821.1 45.218.5 28.8 37.48.6 35.26.4 22.7 48.425.7 42.419.7 37.6 47.810.2 45.37.7 25.3 48.623.3 42.617. 40.7 51.4 50.4 61.410.0 59.49.0 60.0 65.45.0 63.13.1 70.2 74.64.4 71.81.8 58.8 62.23.4 68.29.4 70.4 76.35.9 74.13.7 68.7 74.25.5 69.40.7 58.3 68.410.1 65.67.3 2.46 5.11 24.4 34.610.2 32.88.4 15.6 44.729.1 41.826.2 24.4 50.526.1 47.322.9 30.0 42.412.4 37.87.8 31.1 59.428.3 47.816.7 30.0 47.917.9 46.316.3 23.3 54.331.0 51.428.1 Table 4: Performance of LLMs after instruction tuning (it) for fine-grained classification using target segment (T) and the combination of review and target segment (RT) in terms of string-matching accuracy (St. (Acc)). The first row of each model states the best results obtained previously as detailed in Tables 2 and 3 respectively. Subscripts represent increment or decrement compared to the non-instruction tuned versions. 15.6 SciTülu it + 45.730.1 it + RT 41.425.8 35.3 it + 45.19.8 it + RT 43.27.9 Yi-1.5 multilingual training on 2.4T tokens (Bai et al., 2023) compared to Gemmas English-only training (Team et al., 2024). Mistral and Yi perform best with the full dataset mix, with Yi surpassing Mistral (35.7 vs. 26.8 S.A.), possibly due to its larger vocabulary size. Qwen and Yi lead in fine-grained classification, while SciTülu excels in coarse classification, consistent with earlier findings. Instruction tunings effectiveness relies on data composition; blending general-purpose instruction data from TÜLU with scientific data from SCIRIFF optimizes LLM performance (Shi et al., 2023) on this task. However, including tasks from all sources (FULL MIX) can occassionally underperform, likely due to negative task transfer (Jang et al., 2023; Wadden et al., 2024). We train models using optimal data mixes from our cross-validation experiments to obtain silver annotations for the rest of the 1,276 review segments. We test the performance on the annotated instances from previous rounds (details in Appendix A.8). We ensure no leakage between the annotation rounds and the training data. The instructiontuned models results for fine-grained classification  (Table 4)  , demonstrate significant improvements over zero-shot and few-shot baselines (best results) from previous rounds (full results in Tables 10 and 11). Qwen achieves the best performance, with gains up to 31 pp. for SciTülu. Coarse-grained classification also shows positive increments, though improvements are smaller  (Table 4)  . This highlights the effectiveness of instruction-tuning in guiding models towards accurate outputs (Wadden et al., 2024; Ivison et al., 2023). We use the instruction-tuned Qwen model to label the rest of the 1,276 review segments and release these silver annotations as part of our dataset. Detailed analyses on this portion are provided in Appendix A.14. RQ4: How effective are lazy thinking guidelines in improving review quality? Setup. To improve review quality by addressing lazy thinking, we focus on assessing peer reviews created with and without lazy thinking annotations. In controlled experiment, we form two treatment groups, each with two PhD students experienced in NLP peer reviewing. We sample 50 review reports from NLPEER for which we have explicit lazy thinking annotations. One group rewrites these reviews based on the current ARR guidelines (Rogers and Augenstein, 2021), while the other group rewrites the same reviews using the same guidelines with our lazy thinking annotations included. Following previous studies (Yuan et al., 2022; Sun et al., 2024a), we evaluate the reviews based on Constructiveness, ensuring actionable feedback is provided, and Justified, requiring clear reasoning for arguments. Additionally, we introduce Adherence to assess how well the reviews align with the given guidelines. senior PostDoc and PhD compare the reviews from both groups and the original reviews pairwise. We split the reviews into equal portions (25 reviews each) while keeping 10 reviews for calculating agreements.13 Results. The win-tie-loss results in Table 5 show that reviews using lazy thinking signals outperform original reviews across all measures, with 90% win rates for adherence and 85% for constructiveness and justification. This suggests that lazy thinking annotations help to provide actionable, evidencebacked feedback. When compared to guidelinebased rewrites (75% for adherence and 70% for constructiveness and justified), the win rates are lower. This is likely because reviewers often exercised greater caution when rewriting reviews based on guidelines by shifting concerns from the weakness section to the comments section to soften criticism rather than fully rewriting the reviews. This 13Annotation instructions are provided in Appendix A.12 Adh."
        },
        {
            "title": "Type",
            "content": "Orig. vs lazy Orig w. gdl vs lazy Constr. Justi. W/T/L W/T/L W/T/L 90/5/5 85/10/5 85/5/10 70/5/25 75/5/20 70/5/25 Table 5: Pair-wise comparison of rewrites based on Win (W), Tie (T), and Loss (L) rates across metrics. The first row compares lazy thinking rewrites (lazy) with original reviews (Orig), while the second compares lazy rewrites with guideline-based rewrites (Orig w. gdl). approach resulted in more constructive and justified feedback. We also train Bradley-Terry preference ranking model (Bradley and Terry, 1952) on adherence preference data, which reveals strengths of 1.6 for lazy thinking rewrites, -1.5 for original reviews, and 0.4 for guideline-based rewrites. This translates to 95.6% win rate for lazy thinking rewrites over original texts and 76.8% over guideline-based rewrites, further confirming the effectiveness of these annotations. We obtain Krippendorffs α scores of 0.62, 0.68, and 0.72 for constructiveness, justified, and adherence respectively."
        },
        {
            "title": "4 Related Work",
            "content": "Rule Following. Rules are essential for everyday reasoning (Geva et al., 2021; Wang et al., 2024), with much research focusing on evaluating rulefollowing behavior in generated outputs. Prior studies have examined inferential logic in questionanswering (Wang et al., 2022a,b; Weir et al., 2024; Sun et al., 2024b) and detecting forbidden tokens in red-teaming (Mu et al., 2024). However, identifying rule adherence in peer-review reports requires more abstract reasoning than in typical rule-based scenarios which makes our work more challenging. Review Quality Analysis. As research outputs continue to rise, interest in assessing review quality has grown significantly within the academic community (Kuznetsov et al., 2024). Previous studies have examined various aspects of review reports, including the importance of politeness and professional etiquette (Bharti et al., 2023, 2024), thoroughness (Severin et al.; Guo et al., 2023), and comprehensiveness (Yuan et al., 2022). There have been efforts to automate the peer-reviewing process using large language models (Du et al., 2024; Zhou et al., 2024). However, there has been no focus on analyzing the usage of heuristics within the reviews. This work focuses on lazy thinking, common issue affecting the quality of peer reviews in Natural Language Processing (NLP). While Rogers and Augenstein (2020) have qualitatively analyzed heuristics in NLP conferences, we are the first to present dedicated dataset for identifying such practices and to propose automated methods for detecting low-quality reviews, thereby enhancing the effectiveness of the peer-review process in NLP. the lazy thinking framework presents compelling avenue for future research."
        },
        {
            "title": "5 Conclusion",
            "content": "Aiming to address the pressing issue of lazy thinking in review reports, we have presented LAZYREVIEW, novel resource for detecting lazy thinking in peer reviews. We conduct extensive experiments and establish strong baselines for this task. Additionally, our controlled experiments demonstrate that providing explicit lazy thinking signals to reviewers can significantly improve review quality. We hope our work will inspire further efforts to improve the overall quality of peer reviews."
        },
        {
            "title": "Limitations",
            "content": "In this work, we introduce LAZYREVIEW, novel resource for detecting lazy thinking in NLP peer reviews. While the resource is aimed at promoting sound reviewing practices and enhancing overall review quality, it has some limitations. First, we adopt the definition and categories of lazy thinking from the ARR guidelines (Rogers and Augenstein, 2021) and the EMNLP Blog (Liu et al., 2020), which are specific to NLP conference reviews. Therefore, the resource does not encompass peer-reviewing venues beyond ARR, and it would need adaptation to suit other domains with differing definitions of lazy thinking. Second, from task and modeling perspective, we focus on the under-explored area of detecting lazy thinking in peer reviews. Although we conduct thorough experiments and analyses, the findings may not be fully generalizable to other classification tasks in the scientific domain, so results should be interpreted with caution. Additionally, ACL ARR guidelines currently characterize lazy thinking specifically within the weakness section of review; future research could explore how such patterns manifest in other parts of the review as well. In addition, lazy thinking patterns may extend into subsequent authorreviewer discussions. However, as our starting dataset, NLPEER, does not include these interactions, we refrain from exploring this aspect, leaving it as potential direction for future investigation. Furthermore, this study focuses exclusively on reviews authored prior to the widespread adoption of large language models (i.e., before 2023). As AI-generated reviews become increasingly prevalent, exploring their impact within"
        },
        {
            "title": "Ethics Statement",
            "content": "Our dataset, LAZYREVIEW will be released under the CC-BY-NC 4.0 licenses. The underlying ARR reviews have been collected from NLPEER (Dycke et al., 2023), which is also licensed under the CCBY-NC-SA 4.0 license. The analysis and automatic annotation do not require processing any personal or sensitive information. We prioritize the mental health of the annotators and ensure that each annotator takes break every hour or whenever they feel unwell."
        },
        {
            "title": "Acknowledgements",
            "content": "This work has been funded by the German Research Foundation (DFG) as part of the Research Training Group KRITIS No. GRK 2222, along with the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts, within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We gratefully acknowledge the support of Microsoft with grant for access to OpenAI GPT models via the Azure cloud (Accelerate Foundation Model Academic Research). The work of Anne Lauscher is funded under the Excellence Strategy of the German Federal Government and the Federal States. We want to thank Viet Pham, Yu Liu, Hao Yang, and Haolan Zhan for their help with the annotation efforts. We are grateful to Qian Ruan and Gholamreza (Reza) Haffari for their initial feedback on draft of this paper."
        },
        {
            "title": "References",
            "content": "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. ArXiv preprint. Robert K. Atkinson, Sharon J. Derry, Alexander Renkl, and Donald Wortham. 2000. Learning from examples: Instructional principles from the worked examples research. Review of Educational Research, 70(2):181214. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. ArXiv preprint. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615 3620, Hong Kong, China. Association for Computational Linguistics. Prabhat Kumar Bharti, Mayank Agarwal, and Asif Ekbal. 2024. Please be polite to your peers: multitask model for assessing the tone and objectivity of critiques of peer review comments. Scientometrics, 129(3):13771413. Prabhat Kumar Bharti, Meith Navlakha, Mayank Agarwal, and Asif Ekbal. 2023. Politepeer: does peer review hurt? dataset to gauge politeness intensity in the peer reviews. Language Resources and Evaluation, 58(4):12911313. Ralph Allan Bradley and Milton E. Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. ArXiv preprint. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Cheng Jiayang, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. 2024. LLMs assist NLP researchers: Critique paper (meta-)reviewing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 50815099, Miami, Florida, USA. Association for Computational Linguistics. Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-yes-yes: Proactive data collection for ACL rolling review and beyond. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 300318, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023. NLPeer: unified resource for the computational study of peer review. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5049 5073, Toronto, Canada. Association for Computational Linguistics. Tao Feng, Lizhen Qu, and Gholamreza Haffari. 2023. Less is more: Mitigate spurious correlations for open-domain dialogue response generation models by causal discovery. Transactions of the Association for Computational Linguistics, 11:511530. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346 361. Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, and Chloé Clavel. 2023. Automatic analysis of substantiation in scientific peer reviews. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1019810216, Singapore. Association for Computational Linguistics. Sireesh Gururaja, Amanda Bertsch, Clara Na, David Widder, and Emma Strubell. 2023. To build our future, we must know our past: Contextualizing paradigm shifts in natural language processing. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1331013325, Singapore. Association for Computational Linguistics. Chadi Helwe, Tom Calamai, Pierre-Henri Paris, Chloé Clavel, and Fabian Suchanek. 2024. MAFALDA: benchmark and comprehensive study of fallacy detection and classification. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 48104845, Mexico City, Mexico. Association for Computational Linguistics. Carolin Holtermann, Paul Röttger, Timm Dill, and Anne Lauscher. 2024. Evaluating the elementary multilingual capabilities of large language models with MultiQ. In Findings of the Association for Computational Linguistics: ACL 2024, pages 44764494, Bangkok, Thailand. Association for Computational Linguistics. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. ArXiv preprint. Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the benefits of training expert language models over instruction tuning. In Proceedings of the 40th International Conference on Machine Learning, ICML 2023, Honolulu, Hawaii, USA, July 23-29 2023. PMLR. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv preprint. Neha Nayak Kennard, Tim OGorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, and Andrew McCallum. 2022. DISAPERE: dataset for discourse structure in peer review discussions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 12341249, Seattle, United States. Association for Computational Linguistics. Nishant Kumar, Zulfiqar Ali, and Rudrashish Haldar. 2023. Novelty in research: common reason for manuscript rejection! Indian Journal of Anaesthesia, 67(3):245246. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K. Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Mausam, Margot Mieskes, Aurélie Névéol, Danish Pruthi, Lizhen Qu, Roy Schwartz, Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna Rogers, Nihar B. Shah, and Iryna Gurevych. 2024. What can natural language processing do for peer review? ArXiv preprint. Nino Künzli, Anke Berger, Katarzyna Czabanowska, Raquel Lucas, Andrea Madarasova Geckova, Sarah Mantwill, and Olaf von dem Knesebeck. 2022. do not have time-is this the end of peer review in public health sciences? Public Health Reviews, 43(11):1605407. Esther Landhuis. 2016. Scientific literature: Information overload. Nature, 535(7612):457458. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159174. Yang Liu, Trevor Cohn, Bonnie Webber, and Yulan He. 2020. Advice on Reviewing for EMNLP. EMNLP 2020 blog. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. 2024. Are emergent abilities in large language models just incontext learning? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50985139, Bangkok, Thailand. Association for Computational Linguistics. Robert Merton. 1968. The matthew effect in science: The reward and communication systems of science are considered. Science, 159(3810):5663. Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2024. Can llms follow simple rules? ArXiv preprint. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. ArXiv preprint. Pallets. 2024. Jinja. https://github.com/pallets/ jinja/. GitHub repository. Sukannya Purkayastha, Anne Lauscher, and Iryna Gurevych. 2023. Exploring jiu-jitsu argumentation In Proceedings for writing peer review rebuttals. of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1447914495, Singapore. Association for Computational Linguistics. Martina Raue and Sabine G. Scholl. 2018. The Use of Heuristics in Decision Making Under Risk and Uncertainty, pages 153179. Springer International Publishing. Anna Rogers and Isabelle Augenstein. 2020. What can we do to improve peer review in NLP? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 12561262, Online. Association for Computational Linguistics. Anna Rogers, Marzena Karpinska, Jordan Boyd-Graber, and Naoaki Okazaki. 2023. Program chairs report on peer review at acl 2023. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages xllxxv, Toronto, Canada. Association for Computational Linguistics. Anne Rogers and Isabelle Augenstein. 2021. How to review for acl rolling review. ACL Rolling Review. Anna Severin, Michaela Strinzel, Matthias Egger, Tiago Barros, Alexander Sokolov, Julia Vilstrup Mouatt, and Stefan Müller. Journal impact factor and peer review thoroughness and helpfulness: supervised machine learning study. ArXiv preprint. Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022. Compute trends across three eras of machine learning. In Proceedings of the International Joint Conference on Neural Networks, IJCNN 2022, July 18-23, Padua, Italy, pages 18. IEEE. Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, and Deng Cai. 2023. Specialist or generalist? instruction tuning for specific NLP tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1533615348, Singapore. Association for Computational Linguistics. Pragya Srivastava, Satvik Golechha, Amit Deshpande, and Amit Sharma. 2024. NICE: To optimize incontext examples or not? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5494 5510, Bangkok, Thailand. Association for Computational Linguistics. Lu Sun, Aaron Chan, Yun Seo Chang, and Steven Dow. 2024a. Reviewflow: Intelligent scaffolding to support academic peer reviewing. In Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI 2024, March 18-21, 2024, Greenville, South Carolina, USA, pages 120137. Association for Computing Machinery. Wangtao Sun, Chenxiang Zhang, Xueyou Zhang, Ziyang Huang, Haotian Xu, Pei Chen, Shizhu He, Jun Zhao, and Kang Liu. 2024b. Beyond instruction following: Evaluating rule following of large language models. ArXiv preprint. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Zhou, Zhongyu Wei, Zhumin Chen, and Nan Duan. 2022a. From lsat: The progress and challenges of IEEE/ACM Transactions on complex reasoning. Audio, Speech, and Language Processing, 30:2201 2216. Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024. Can llms reason with rules? logic scaffolding for stress-testing and improving llms. ArXiv preprint. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022b. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Linguistics: ACL 2022, pages 16191629, Dublin, Ireland. Association for Computational Linguistics. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. ArXiv preprint. Mark Ware and Michael Mabe. 2009. An overview of scientific and scholarly journal publishing. The STM report, 1082:1083. Nathaniel Weir, Peter Clark, and Benjamin Van Durme. 2024. Nellie: neuro-symbolic inference engine for grounded, compositional, and explainable reasoning. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, August 3-9, 2024, Jeju, Korea, pages 36023612. ijcai.org. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023. OpenICL: An open-source framework for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 489498, Toronto, Canada. Association for Computational Linguistics. Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2022. Can we automate scientific reviewing? Journal of Artificial Intelligence Research, 75:171212. Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is LLM reliable reviewer? comprehensive evaluation of LLM on automatic paper reviewing tasks. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024, May 20-25, 2024, pages 93409351. ELRA and ICCL. Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. ArXiv preprint. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. ArXiv preprint. Amos Tversky, Daniel Kahneman, and Paul Slovic. 1974. Judgment under uncertainty: Heuristics and biases. Science, 185(4157):11241131. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, and Arman Cohan. 2024. Sciriff: resource to enhance language model instruction-following over scientific literature. ArXiv preprint. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Model LLaMa 2 (Touvron et al., 2023) LLaMa 2 (Touvron et al., 2023) Gemma 1.1 (Team et al., 2024) Mistral v0.1 (Jiang et al., 2023) Qwen-1.5 (Bai et al., 2023) Yi-1.5 (AI et al., 2024) SciTülu (Wadden et al., 2024) meta-llama/Llama-2-7b-chat-hf Size Link 7B 13B meta-llama/Llama-2-13b-chat google/gemma-1.1-7b-it 7B mistralai/Mistral-7B-Instruct-v0.1 7B Qwen/Qwen-7B-Chat 7B 01-ai/Yi-6B-Chat 6B allenai/scitulu-7b 7B Table 6: Overview of models used in our work along with their sizes and links."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Guidelines used in multiple Annotation"
        },
        {
            "title": "Rounds",
            "content": "The existing ARR guidelines (Rogers and Augenstein, 2021) are shown in Tab 7, which we used in Round 1 of our annotation study. The guidelines extended using the EMNLP Blog (Liu et al., 2020) is shown in Table 8. The positive examples that we provided the annotators for completing round 3 of our annotation study are shown in Table 9. A.2 Model and Implementation details We select the LLMs based on multiple criteria: (a) all the models should be open-sourced in order to deploy them in real-world reviewing systems, (b) their size should be reasonable in order to perform full fine-tuning using LoRA (Hu et al., 2022). c) they must be state-of-the-art on the existing LLM leaderboards (Chiang et al., 2024). Based on these criteria, we select the chat version of the various models shown in Table 6. We use vLLM for fast inference of all the models.14. We set the temperature as 0 to have consistent predictions throughout and limit the output tokens to 30. A.3 Computational Budget We ran all the experiments on Nvidia A100 80GB GPUs. None of the experiments consumed more than 36 hours. A.4 Prompt for GPT-based review segment extraction We describe the prompt for GPT-based review segment extraction in Fig 5. We provide the classes from the existing ARR guidelines and ask the model to provide us list of review segments that can likely be related to lazy thinking. We show subset of the classes in the prompt due to space constraints. 14https://github.com/vllm-project/vllm A.5 Metric alignment study To verify the reliability of our metrics, we follow Holtermann et al. (2024) and perform an alignment study. We sample 50 responses from various models and evaluate them with both string matching and GPT-based methods. We report the results in Tables 14 and 15 respectively. We find that the string-based evaluator is precise for correct answers but tends to underestimate them, while the GPT-based evaluator is better at identifying incorrect answers but may overestimate correct ones. As such, the string-based evaluator serves as lower bound while the GPT-based evaluator serves as an upper bound. Manual analysis shows that the GPT-based evaluator struggles with similar classes, like \"not surprising\" versus \"not novel,\" while the string-based evaluator misses correct predictions such as \"The writing needs to be clear\" and \"Language errors/Writing style.\" However, for the coarse-grained classification, both the evaluators perform equally well. A.6 Prompts for Coarse-grained and Fine-grained classification We use fixed prompt template to describe the lazy thinking as shown in Fig 6. We then add the task-specific prompt for fine-grained and coarsegrained classification as shown in Fig 7 and Fig 8, respectively. For the first round, we add the classes in the fixed template from the initial ARR guidelines  (Table 7)  . In the second round, the classes are added from the enhanced guidelines in Table 8. We use the full review and target segment as input for the RT setup and only the target segment for the setup as described in Sec 3. The prompt templates for In-Context Learning as used in Round 3 of our annotation study, shown in Figures 9 and 10 for fine-grained and coarse-grained classification, respectively. A.7 In-context Learning Experiment for Round 3 We use different types of methods to generate incontext learning exemplars, namely Static, Random, Top-K, and Vote-K, as described in Sec 3. We use 1,2, and 3 exemplars for this experiment. The performance of the models for the fine-grained and coarse-grained classification are shown in Fig 3 and 4, respectively. We observe that the performances donot change on increasing the number of exemplars or using different strategies. (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B Figure 3: Performance of LLMs on using different In Context learning (ICL) methods for Round 3 of our annotation study for fine-grained classification. Error bars indicate using only the target segment (T) as the information source. Acc refers to using GPT-based accuracy (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B Figure 4: Performance of LLMs on using different In Context learning (ICL) methods for Round 3 of our annotation study for the coarse classification task. Error bars indicate using only the target segment (T) as the information source. Acc refers to using GPT-based evaluator accuracy. A.8 Training Details to perform instruction tuning A.8."
        },
        {
            "title": "Instructions",
            "content": "We use the same instructions from Round 2 of our annotation study due to higher inter-annotator agreement (Sec A.1). During fine-tuning, we follow the SCIRIFF template using Jinja (Pallets, 2024), providing task descriptions and data as prompts, with output labels as responses. Two instruction formats are used: one with just the target segment (T) and another with both the target Prompt for GPT-based segment extraction Prompt: As the number of scientific publications grows over the years, the need for efficient quality control in peer-reviewing becomes crucial. Due to the high reviewing load, the reviewers are often prone to different kinds of bias, which inherently contribute to lower reviewing quality and this overall hampers the scientific progress in the field. The ACL peer reviewing guidelines characterize some of such reviewing bias. These are termed lazy thinking. The lazy thinking classes and the reason for them being problematic are provided as dictionary below: {The results are not surprising: Many findings seem obvious in retrospect, but this does not mean that the community is already aware of them and can use them as building blocks for future work., The results contradict what would expect: to accept data contradicting your prior beliefs., You may be victim of confirmation bias, and be unwilling The results are not novel: Such broad claims need to be backed up with references., This has no precedent in existing literature: Believe it or not: papers that are more novel tend to be harder to publish. Reviewers may be unnecessarily conservative., The results do not surpass the latest SOTA, SOTA results are neither necessary nor sufficient for scientific contribution. An engineering paper could also offer improvements on other dimensions (efficiency, generalizability, interpretability, fairness, etc., The results are negative: The bias towards publishing only positive results is known problem in many fields, and contributes to hype and overclaiming. If something systematically does not work where it could be expected to, the community does need to know about it., This method is too simple: solutions are in fact preferable, as they are less brittle and easier to deploy in real-world settings., The goal is to solve the problem, not to solve it in complex way. Simpler The paper doesnt use [my preferred methodology], e.g., deep learning: NLP is an interdisciplinary field, relying on many kinds of contributions: models, resource, survey, data/linguistic/social analysis, position, and theory., [...] You are given review, and your task is to identify the segments (1 or more sentences) within the summary of weaknesses section that can correspond to any lazy thinking class as described above. The output should be list of these review segments. Review: paper summary: This work provides broad and thorough analysis of how 8 different model families and varying model sizes for total of 28 models perform on the oLMpics benchmark and the psycholinguistic probing datasets from Ettinger (2020). It finds that all models struggle to resolve compositional questions zero-shot and that attributes such as model size, pretraining objective, etc are not predictive of models linguistic capabilities. summary of strengths:- The work is well-motivated and clear -A vast selection of models are investigated. summary of weaknesses: While the findings are interesting, there is little to no qualitative analysis to provide insight into why these effects might occur -I would expect an analysis such as this to have at least 3 runs with varying random seeds per model to give greater confidence in the models abilities. growing body of work indicates that models linguistic abilities can vary considerably even across initialisations -The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited. commnets, suggestions, and typos: NA Figure 5: Prompt for GPT based review segment extraction segment and full review (RT). The output is either fine-grained lazy thinking class or coarse label (not lazy thinking or lazy thinking). Demonstration instances are excluded to ensure consistency with SCIRIFF and TÜLU V2. A.8.2 Hyper-parameters for LoRa training We train all the models for 3 epochs. For LoRa, we use the rank of 64, alpha of 16, and dropout of 0.1. The models are trained with cosine learning rate scheduler and warmup ratio of 0.03. We use BF16 and TF32 to ensure training precision. The learning rate is set as 1e 4. A.8.3 Hyper-parameter search We train models using various data proportions: 0.2, 0.4, 0.6, 0.8, and 1.0 from the different mixes. We then evaluate their performance on multiple validation sets. The performance results are plotted in two ways: using only the target segment (T) in Fig 11 and using combination of review and target segment (RT) in Fig 12 corresponding to the fine-grained classification task. For the setup, where only the target segment is used, the models"
        },
        {
            "title": "Prompt Template for Lazy Thinking",
            "content": "Prompt: As the number of scientific publications grows over the years, the need for efficient quality control in peer-reviewing becomes crucial. Due to the high reviewing load, the reviewers are often prone to different kinds of bias, which inherently contribute to lower reviewing quality and this overall hampers the scientific progress in the field. The ACL peer reviewing guidelines characterize some of such reviewing bias. These are termed lazy thinking. The lazy thinking classes and the reason for them being problematic are provided as dictionary below: {The results are not surprising: Many findings seem obvious in retrospect, but this does not mean that the community is already aware of them and can use them as building blocks for future work., The results contradict what would expect: to accept data contradicting your prior beliefs., You may be victim of confirmation bias, and be unwilling The results are not novel: Such broad claims need to be backed up with references., This has no precedent in existing literature: Believe it or not: papers that are more novel tend to be harder to publish. Reviewers may be unnecessarily conservative., The results do not surpass the latest SOTA, SOTA results are neither necessary nor sufficient for scientific contribution. An engineering paper could also offer improvements on other dimensions (efficiency, generalizability, interpretability, fairness, etc., The results are negative: The bias towards publishing only positive results is known problem in many fields, and contributes to hype and overclaiming. If something systematically does not work where it could be expected to, the community does need to know about it., This method is too simple: solutions are in fact preferable, as they are less brittle and easier to deploy in real-world settings., The goal is to solve the problem, not to solve it in complex way. Simpler [...] Figure 6: Fixed Prompt for defining lazy thinking Prompt Template for Fine-Grained Classification Task: Given full review and target segment corresponding to that review, you need to classify the target sentence into one of the lazy thinking classes eg., The topic is too niche, The results are negative. Full Review: <review> Target Segment: <target segment> Figure 7: Prompt for fine-grained classification Prompt Template for Coarse-Grained Classification Task: Given full review and target segment corresponding to that review, you need to classify the target sentence into whether it is lazy thinking or not lazy thinking Full Review: <review> Target Segment: <target segment> Figure 8: Prompt for coarse-grained classification achieve optimal performance with 0.2 to 0.4 of the data. Performance tends to stagnate with more data indicative of reaching the maxima and is in line with the findings in Wadden et al. (2024). In the RT setup, where both the target segment and review are used, the models perform best with 0.6 to 0.8 of the data. This is because the combined prompts provide more context and take longer for the model to fully grasp, allowing it to reach peak performance with larger dataset. As result, we select 0.3 as the optimal data proportion for the setup and 0.7 for the RT setup, balancing performance and avoiding the issues observed with different data sizes. We obtain similar findings for Prompt Template for Fine-Grained Classification (ICL Based) Task: Given full review and target segment corresponding to that review, you need to classify the target sentence into one of the lazy thinking classes eg., The topic is too niche, The results are negative. An example is shown below. Full Review: paper summary: This work provides broad and thorough analysis of how 8 different model families and varying model sizes for total of 28 models perform on the oLMpics benchmark and the psycholinguistic probing datasets from Ettinger (2020). It finds that all models struggle to resolve compositional questions zero-shot and that attributes such as model size, pretraining objective, etc are not predictive of models linguistic capabilities. summary of strengths:- The work is well-motivated and clear -A vast selection of models are investigated. summary of weaknesses: While the findings are interesting, there is little to no qualitative analysis to provide insight into why these effects might occur -I would expect an analysis such as this to have at least 3 runs with varying random seeds per model to give greater confidence in the models abilities. growing body of work indicates that models linguistic abilities can vary considerably even across initialisations -The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited. commnets, suggestions, and typos: NA Target Segment: The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited. Class: The authors should do extra experiment [X] Full Review: <review> Target Segment: <target segment> Figure 9: Prompt for fine-grained classification based on In-Context Learning (ICL) as used in Round 3 of our study Prompt Template for Coarse-Grained Classification (ICL Based) Task: Given full review and target segment corresponding to that review, you need to classify the target sentence into whether it is lazy thinking or not lazy thinking Full Review: paper summary: This work provides broad and thorough analysis of how 8 different model families and varying model sizes for total of 28 models perform on the oLMpics benchmark and the psycholinguistic probing datasets from Ettinger (2020). It finds that all models struggle to resolve compositional questions zero-shot and that attributes such as model size, pretraining objective, etc are not predictive of models linguistic capabilities. summary of strengths:- The work is well-motivated and clear -A vast selection of models are investigated. summary of weaknesses: While the findings are interesting, there is little to no qualitative analysis to provide insight into why these effects might occur -I would expect an analysis such as this to have at least 3 runs with varying random seeds per model to give greater confidence in the models abilities. growing body of work indicates that models linguistic abilities can vary considerably even across initialisations -The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited. commnets, suggestions, and typos: NA Target Segment: The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited. Class: Lazy Thinking Full Review: <review> Target Segment: <target segment> Figure 10: Prompt for coarse-grained classification based on In-Context Learning (ICL) as used in Round 3 of our study coarse-grained classification as shown in Fig 13 (T) and Fig 14 (RT), respectively. A.8."
        },
        {
            "title": "Instruction Tuning Training Setup for\nAnnotation Rounds",
            "content": "We train models with the mixes identified in the experiments with 3-fold cross-validation for optimal performance. We use the same instructions that are used in the annotations and zero-shot prompting for finetuning the models. We use data proportion of 0.3 to train models within the target segmentbased prompting setup (T) and 0.8 for the RT (full review with target segment) setup from the best mizes identified fro each model. In line with the results on the test sets, we use SCIRIFF MIX to train LLaMa and SciTülu models, TÜLU MIX for the Gemma and Qwen models and Full Mix for Mistral and Yi models. A.9 Confusion matrices for Annotation"
        },
        {
            "title": "Labels and Confidences across annotation\nrounds",
            "content": "We show the confusion matrices for annotation labels and confidences in Figures 16 and 17 respectively. A.10 Results for the 3-fold cross validation The results for the 3-fold cross-validation for finegrained and coarse-grained classification is shown in Tables 12 and 13 respectively. The random and (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B (g) Yi 6B Figure 11: Performance of instruction-tuned LLMs for fine-grained classification on the dev set with multiple percentages of dataset mixes using target segment (T) as the source of information in the prompt. (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B (g) Yi 6B Figure 12: Performance of instruction-tuned LLMs for fine-grained classification on the dev set with multiple percentages of dataset mixes using the combination of review and target segment (RT) as the source of information in the prompt. majority baseline scores for fine-grained classification are 4.115.2 and 6.75.4 respectively. The random and majority baseline scores for coarsegrained classification are 44.113.2 and 47.73.2 respectively. Thus, all the models significantly outperform the majority and random baselines for both the tasks. A.11 Results for all the experiments across the annotation rounds We report the full results for the fine-grained and coarse-grained classification in Tables 10 and 11 respectively. A."
        },
        {
            "title": "Instructions to Annotators for Rewriting\nReviews and Evaluation",
            "content": "As discussed in Sec 3, we form two control groups to rewrite reviews. The first control group is asked (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B (g) Yi 6B Figure 13: Performance of instruction-tuned LLMs for coarse-grained classification on the dev set with multiple percentages of dataset mixes using target segment (T) as the source of information in the prompt. (a) Gemma 7B (b) LLaMa 7B (c) LLaMa 13B (d) Mistral 7B (e) SciTülu 7B (f) Qwen 7B (g) Yi 6B Figure 14: Performance of instruction-tuned LLMs for coarse-grained classification on the dev set with multiple percentages of dataset mixes using the combination of review and target segment (RT) as the source of information in the prompt. to rewrite reviews based on the current ARR guidelines. While the other control group is also provided with lazy thinking annotations. We provide both the control groups with the paper and the reviews corresponding to that paper. Finally, senior Ph.D and PostDoc evaluate the rewrites pair-wise based on various measures. Instructions for re-writing based on current ARR guidelines. Given the review and the corresponding manuscript, your task is to re-write the reviews to comply with the current ARR guidelines.15. Please feel free to edit or remove the content from any of the reviewing sections such as summary of weaknesses, comments, suggestions and typos if applicable. Instructions for re-writing based on current ARR guidelines and lazy thinking annotations. Given the review and the corresponding manuscript, you have been provided annotated instances of lazy 15https://aclrollingreview.org/reviewertutorial Figure 15: Distribution of lazy thinking labels in the silver annotated data ers often using it to reject papers without sufficient justification (Kumar et al., 2023). thinking. The definition of lazy thinking is as follows Lazy thinking, in the context of NLP research paper reviews, refers to the practice of dismissing or criticizing research papers based on superficial heuristics or preconceived notions rather than thorough analysis. It is characterized by reviewers raising concerns that lack substantial supporting evidence and are often influenced by prevailing trends within the NLP community. According to ARR guidelines, reviewers are explicitly discouraged from using such heuristics in their review reports. In line with the ARR guidelines, your task is to re-write the reviews to comply with the guidelines. Please feel free to edit or remove the content from any of the reviewing sections such as summary of weaknesses, comments, suggestions and typos if applicable. Instruction for evaluating the rewrites You are provided paper along with pair of reviews written for the same paper. Your task is to perform pair-wise comparison of the reviews based on: (1) Constructivenessreviewers should provide actionable feedback for the authors to improve on the paper, (2) Justified - reviewers should clearly state the reason for their arguments rather than putting out vague statements. Additionally, we introduce measure, (3)A dherence, which judges how well the reviews are written based on the current ARR guidelines. A.13 Distribution of review segment lengths in LAZYREVIEW We plot the distribution of review segment lengths in Fig 18. We find that Extra Experiments is the most common with variable segment lengths. A.14 Analysis on the Silver Annotations As discussed in RQ3 (cf. 3), we use the bestperforming model (Qwen) to annotate the remaining 1,276 review segments in our dataset, generating silver annotations. One of the annotators reviews sample of 100 segments, yielding Cohens κ of 0.56 with the silver annotations, which is substantial given the subjectivity of this task and the domain. The class label distribution is presented in Fig 15, where Extra Experiment remains the most frequent lazy thinking class. Additionally, we see increases in Not SOTA and Not Novel instances, likely reflecting the fast-paced advancements in ML that quickly render state-of-the-art methods obsolete (Sevilla et al., 2022). The notion of novelty is also frequently debated, with review- (a) Round (b) Round 2 (c) Round 3 Figure 16: Confusion matrices for the lazy thinking classes from the ARR guidelines and EMNLP Blog used for annotation across multiple rounds. The class names are shortened due to space constraints. (a) Round 1 (b) Round 2 (c) Round Figure 17: Confusion matrices for the confidence levels among annotators across multiple rounds of annotation. Figure 18: Segment lengths"
        },
        {
            "title": "Description",
            "content": "Many findings seem obvious in retrospect, but this does not mean that the community is already aware of them and can use them as building blocks for future work. You may be victim of confirmation bias, and be unwilling to accept data contradicting your prior beliefs."
        },
        {
            "title": "The results are not novel",
            "content": "Such broad claims need to be backed up with references."
        },
        {
            "title": "This method is too simple",
            "content": "The paper doesnt use [my preferred methodology], e.g., deep learning"
        },
        {
            "title": "The topic is too niche",
            "content": "The approach is tested only on [not English], so unclear if it will generalize to other languages"
        },
        {
            "title": "The paper has language errors",
            "content": "The paper is missing the comparison to the [latest X] The authors could also do [extra experiment X] The authors should have done [X] instead Believe it or not: papers that are more novel tend to be harder to publish. Reviewers may be unnecessarily conservative. SOTA results are neither necessary nor sufficient for scientific contribution. An engineering paper could also offer improvements on other dimensions (efficiency, generalizability, interpretability, fairness, etc.) The bias towards publishing only positive results is known problem in many fields, and contributes to hype and overclaiming. If something systematically does not work where it could be expected to, the community does need to know about it. The goal is to solve the problem, not to solve it in complex way. Simpler solutions are in fact preferable, as they are less brittle and easier to deploy in real-world settings. NLP is an interdisciplinary field, relying on many kinds of contributions: models, resource, survey, data/linguistic/social analysis, position, and theory. main track paper may well make big contribution to narrow subfield. The same is true of NLP research that tests only on English. Monolingual work on any language is important both practically (methods and resources for that language) and theoretically (potentially contributing to deeper understanding of language in general). As long as the writing is clear enough, better scientific content should be more valuable than better journalistic skills. Per ACL policy, the authors are not obliged to draw comparisons with contemporaneous work, i.e., work published within three months before the submission (or three months before re-submission). It is always possible to come up with extra experiments and follow-up work. This is fair if the experiments that are already presented are insufficient for the claim that the authors are making. But any other extra experiments are in the niceto-have category, and belong in the suggestions section rather than weaknesses. A.k.a. would have written different paper. There are often several valid approaches to problem. This criticism applies only if the authors choices prevent them from answering their research question, their framing is misleading, or the question is not worth asking. If not, then [X] is comment or suggestion, but not weakness. Table 7: Full ARR 2022 guidelines on lazy thinking sourced from Rogers and Augenstein (2021)."
        },
        {
            "title": "This method is too simple",
            "content": "The paper doesnt use [my preferred methodology], e.g., deep learning The topic is too niche / Narrow Topics The approach is tested only on [not English], so unclear if it will generalize to other languages The paper has language errors / Writing Style Non-mainstream approaches"
        },
        {
            "title": "Resource paper",
            "content": "The papers topic is completely new, such that theres no prior art or all the prior art has been done in another field. We are interested in papers that tread new ground. Believe it or not: papers that are more novel tend to be harder to publish. Reviewers may be unnecessarily conservative. The papers method is too simple. Our goal is not to design the most complex method. Again, think what the papers contributions and findings are. Often the papers with the simplest methods are the most cited. If simple method outperforms more complex methods from prior work, then this is often an important finding. The goal is to solve the problem, not to solve it in complex way. Simpler solutions are in fact preferable, as they are less brittle and easier to deploy in real-world settings. The paper does not use particular method (e.g., deep learning). No one particular method is requirement for good work. Please justify why that method is needed. Think about what the papers contributions are, and bear in mind that having diversity of methods used is not bad thing. NLP is an interdisciplinary field, relying on many kinds of contributions: models, resource, survey, data/linguistic/social analysis, position, and theory. The papers topic is narrow or outdated. Please be open minded. We do not want the whole community to chase trendy topic. Look at the papers contributions and consider what impact it may have on our community. It is easier to publish on trendy, scientifically sexy topics (Smith, 2010). In the last two years, there has been little talk of anything other than large pretrained Transformers, with BERT alone becoming the target of over 150 studies proposing analysis and various modifications (Rogers et al., 2020). The hot trend forms the prototype for the kind of paper that should be recommended for acceptance. Niche topics such as historical text normalization are downvoted (unless, of course, BERT could somehow be used for that). The papers work is on language other than English. We care about NLP for any language. The same is true of NLP research that tests only on English. Monolingual work on any language is important both practically (methods and resources for that language) and theoretically (potentially contributing to deeper understanding of language in general). As long as the writing is clear enough, better scientific content should be more valuable than better journalistic skills. Since mainstream *ACL paper currently uses DL-based methods, anything else might look like it does not really belong in the main track - even though ACL stands for Association for Computational Linguistics. That puts interdisciplinary efforts at disadvantage, and continues the trend for intellectual segregation of NLP (Reiter, 2007). E.g., theoretical papers and linguistic resources should not be priori at disadvantage just because they do not contain DL experiments. The paper is resource paper. In field that relies on supervised machine learning as much as NLP, development of datasets is as important as modeling work. Table 8: Lazy thinking classes with extended names, extended descriptions and new additions used in Round 2 of our annotations. The rest of the class names and descriptions are the same as in Round 1 of our annotation sourced from Rogers and Augenstein (2021)."
        },
        {
            "title": "This method is too simple",
            "content": "The topic is too niche / Narrow Topics The authors could also do [extra experiment X]"
        },
        {
            "title": "Positive examples",
            "content": "Although the experiments are very comprehensive, this paper lacks technical novelty. The optimal data selection strategy also seems to offer limited improvement over random data selection. Although the paper empirically shows that the baseline is not as effective as the proposed method, - expect more discussion on why using activation values is not good idea, this contradicts my prior assumption.-One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper) The novelty of the approach is limited. The attempt to train the document retrieval and outcome prediction jointly is unsuccessful. There are lot of problems that can imagine for real-world large scale models such as GPT3. To mention few: (1) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization. have two concerns about the experimental results. 1. In the few-shot learning, when the samples are increased from 24 to 1K, the attribute relevance drops by 2 points for positive class as shown in Table 1, and the toxicity metric becomes worse for the detoxification task as shown in Table 2. Please give some explanations. 2. In human evaluation, the interannotator agreement on the sentiment task and the AGNews 482 task is only 0.39 and 0.30 in Fleiss κ, which is low to guarantee high quality of the evaluation data. There is unfortunately not whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered hyperparameter. While have no problem with what is currently in the paper, am just not sure that this is enough to form long paper proposing new method. 1It is not clear how this task and the approach will perform for new information (updates) about an event (even if it is contradictory to what is known about an event) 2The approach operates on clusters. New events may not have clusters. 1. According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades lot after incorporating text smoothing, why? 2. Lack of experimental results on more datasets: suggest conducting experiments on more datasets to make more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged. Table 9: Positive examples used during Round 3 of our annotations. The review segment that corresponds to lazy thinking is highlighted. We display only the weakness section of the reviews rather than the full review due to space constraints. Method Models Fine-grained S.A RANDOM MAJORITY Zero-Shot Instruction Tuned - - Gemma-1.1 7B (T) Gemma-1.1 7B (TE) Gemma-1.1 7B (RT) Gemma-1.1 7B (RTE) LLaMa 7B (T) LLaMa 7B (TE) LLaMa 7B (RT) LLaMa 7B (RTE) LLaMa 13B (T) LLaMa 13B (TE) LLaMa 13B (RT) LLaMa 13B (RTE) Mistral 7B (T) Mistral 7B (TE) Mistral 7B (RT) Mistral 7B (RTE) Qwen 7B (T) Qwen 7B (TE) Qwen 7B (RT) Qwen 7B (RTE) Yi-1.5 6B (T) Yi-1.5 6B (TE) Yi-1.5 6B (RT) Yi-1.5 6B (RTE) SciTülu 7B (T) SciTülu 7B (TE) SciTülu 7B (RT) SciTülu 7B (RTE) Gemma-1.1 7B (T) Gemma-1.1 7B (RT) LLaMa 7B (T) LLaMa 7B (RT) LLaMa 13B (T) LLaMa 13B (RT) Mistral 7B (T) Mistral 7B (RT) Qwen 7B (T) Qwen 7B (RT) Yi-1.5 6B (T) Yi-1.5 6B (RT) SciTülu 7B (T) SciTülu 7B (RT) Acc R2 (R1) 4.34 (-2.77) 7.34 (+3.00) 26.7 (+4.50) - 11.6 (-0.60) - 22.2 (+10.0) - 13.2 (+1.00) - 26.7 (+0.00) - 17.6 (+3.00) - 28.8 (+1.10) - 16.6 (+4.40) - 22.7 (+1.60) - 13.3 (+1.10) - 37.6 (+2.3) - 32.8 (-1.60) - 25.3 (+10.9) - 18.3 (+2.70) - 38.8 (+7.40) 35.7 (+7.50) 47.8 (+2.00) 45.3 (+2.10) 47.8 (+2.00) 45.2 (+4.00) 37.4 (+2.00) 35.2 (+4.00) 48.4 (+2.50) 42.4 (+1.20) 47.8 (+2.70) 45.3 (+2.10) 48.6 (+2.90) 42.6 (+1.20) R1 7.11 11.1 22.2 - 12.2 - 12.2 - 12.2 - 26.7 - 15.6 - 27.8 - 12.2 - 21.1 - 12.2 - 35.3 - 34.4 - 14.4 - 15.6 - 31.4 28.2 43.8 43.2 45.8 41.2 35.4 31.2 45.9 41.2 45.1 43.2 45.7 41.4 R3 2.46 (-1.88) 5.11 (-2.23) 18.9 24. (+5.5) 14.4 17.3 11.1 15.6 (+2.9) (+4.5) (+2.0) 12.2 14.2 11.1 24.4 (+13.3) (+8.1) (+1.8) (+5.6) (+2.2) (+1.1) (+3.3) (+2.2) (+1.1) (+0.8) 10.7 18.8 28.2 30.0 22.2 27.8 28.9 31.1 26.7 27.8 26.7 30. 21.2 24.4 22.2 23.3 18.9 19.7 34.6 32.8 44.7 41.8 50.5 47.3 42.4 37.8 59.4 47.8 47.9 46.3 54.3 51.4 F1 R2 (R1) 2.39 (-4.76) 7.17 (-1.88) 24.8 (+1.7) - 10.4 (+0.89) - 20.3 (+10.79) - 9.89 (+0.38) - 23.8 (+3.4) - 13.8 (+2.1) - 19.7 (+2.2) - 12.8 (+3.29) - 19.8 (+2.0) - 13.4 (+3.89) - 29.7 (+6.2) - 27.8 (+5.1) - 22.5 (+12.0) - 16.7 (+3.9) - 31.6 (+1.8) 33.7 (+6.9) 37.2 (+0.4) 33.8 (+0.7) 38.2 (+0.7) 36.7 (+3.3) 30.4 (+1.6) 27.4 (+2.9) 40.2 (+1.6) 36.4 (+2.1) 38.2 (+1.8) 33.5 (+1.8) 40.2 (+1.6) 35.6 (+1.3) R1 7.15 9.05 23.1 - 9.51 - 9.51 - 9.51 - 20.4 - 11.7 - 17.5 - 9.51 - 17.8 - 9.51 - 23.5 - 22.7 - 10.5 - 12.8 - 29.8 26.8 36.8 32.1 37.5 32.4 28.8 24.5 38.6 34.3 36.4 31.7 38.6 34.3 R3 1.28 3.45 16.7 19.4 (+2.7) (+2.0) 10.8 12.8 9.12 12.4 (+3.28) (+1.4) 10.1 11.5 9.11 20.8 (+11.7) 8.89 17.5 (+8.61) 24.3 26.1 (+1.8 ) (+3.8) (+2.3) (+1.9) (+2.7) (+1.2) (+1.6) (+0.6) 19.4 23.2 24.5 26.8 22.8 24.7 22.1 24.8 19.4 20.6 16.2 17. 14.9 15.5 28.1 24.3 37.2 32.3 40.1 42.6 36.4 32.5 57.2 35.3 38.2 34.6 48.5 42.3 G.A Acc R2 (R1) - - 58.1 (+5.90) - 51.1 (+4.40) - 30.6 (+15.0) - 33.7 (+7.90) - 45.3 (+0.90) - 40.4 (-1.10) - 51.1 (+3.30) - 35.9 (+6.90) - 50.0 (+3.30) - 42.6 (-1.60) - 60.0 (+3.30) - 52.2 (+1.20) - 29.4 (+11.3) - 23.7 (+6.4) - 53.4 (+3.80) 51.2 (+6.50) 65.4 (+4.60) 61.3 (+2.90) 51.2 (+0.90) 48.8 (+0.60) 54.3 (+5.10) 50.4 (+6.20) 55.8 (+4.60) 50.4 (+1.10) 53.8 (+2.60) 61.2 (+1.80) 52.6 (+1.30) 50.2 (+0.80) R1 - - 52.2 - 46.7 - 15.6 - 25.6 - 44.4 - 41.1 - 47.8 - 28.9 - 46.7 - 43.3 - 56.7 - 51.1 - 18.1 - 17.3 - 49.6 44.7 60.8 58.4 50.3 48.2 49.2 44.2 51.2 49.3 51.2 59.4 51.3 49.4 R3 - - 32.2 41.1 (+8.9) 32.2 32.8 (+0.6) 35.6 38.9 (+3.3) 28.9 30.8 (+1.9) 35.6 41.1 (+5.5) 32.2 34.4 (+2.2) 54.4 55.6 (+1.2) 51.1 52.2 (+1.1) 44.4 56.4 (+12.0) 43.3 44.2 (+0.9) 53.8 54.9 (+1.1) 51.3 52.7 (+0.6) 42.2 44.8 (+2.6) 38.9 41.1 (+2.2) 46.9 42.8 48.9 45.4 54.2 52.2 58.3 56.3 62.3 47.3 52.2 58.4 54.8 51.3 Table 10: Performance of LLMs on different rounds of annotation. S.A represents the string-matching evaluator, and G.A represents the GPT-based evaluator. represents prompting with only the target sentence, RT represents the combination of the review and the target sentence. Adding demonstrations to the prompt is represented by E. R1 represents Round 1, R2 represents Round 2, and R3 represents Round 3 respectively."
        },
        {
            "title": "Models",
            "content": "Coarse-grained S.A"
        },
        {
            "title": "RANDOM\nMAJORITY",
            "content": "Zero-Shot"
        },
        {
            "title": "Instruction Tuned",
            "content": "- - Gemma-1.1 7B (T) Gemma-1.1 7B (TE) Gemma-1.1 7B (RT) Gemma-1.1 7B (RTE) LLaMa 7B (T) LLaMa 7B (TE) LLaMa 7B (RT) LLaMa 7B (RTE) LLaMa 13B (T) LLaMa 13B (TE) LLaMa 13B (RT) LLaMa 13B (RTE) Mistral 7B (T) Mistral 7B (TE) Mistral 7B (RT) Mistral 7B (RTE) Qwen 7B (T) Qwen 7B (TE) Qwen 7B (RT) Qwen 7B (RTE) Yi-1.5 6B (T) Yi-1.5 6B (TE) Yi-1.5 6B (RT) Yi-1.5 6B (RTE) SciTülu 7B (T) SciTülu 7B (TE) SciTülu 7B (RT) SciTülu 7B (RTE) Gemma-1.1 7B (T) Gemma-1.1 7B (RT) LLaMa 7B (T) LLaMa 7B (RT) LLaMa 13B (T) LLaMa 13B (RT) Mistral 7B (T) Mistral 7B (RT) Qwen 7B (T) Qwen 7B (RT) Yi-1.5 6B (T) Yi-1.5 6B (RT) SciTülu 7B (T) SciTülu 7B (RT)"
        },
        {
            "title": "Acc",
            "content": "R2 (R1) 40.7 (+0.00) 51.4 (+0.00) 46.1 (+1.8) - 50.4 (+2.30) - 60.0 (+2.30) - 60.0 (+6.70) - 62.2 (+2.00) - 70.2 (+1.60) - 58.8 (+1.00) - 57.4 (+2.00) - 70.4 (+1.40) - 56.5 (+3.20) - 68.7 (+4.3) - 68.3 (+5.0) - 58.3 (+0.50) - 58.7 (+3.10) - 61.4 (+3.60) 59.4 (+3.80) 65.4 (+2.70) 63.1 (+1.90) 74.6 (+0.30) 71.8 (+1.60) 62.2 (+2.00) 68.2 (+2.90) 76.3 (+0.90) 74.1 (+0.90) 74.2 (+5.30) 69.4 (+2.20) 68.4 (+2.10) 65.6 (+3.20) R1 40.7 51.4 44.3 - 48.1 - 57.7 - 53.3 - 60.2 - 68.6 - 57.8 - 55.4 - 68.9 - 53.3 - 64.4 - 63.3 - 57.8 - 55.6 - 57.8 55.6 62.7 61.2 74.3 70.2 60.2 65.3 75.4 73.2 69.5 67.2 66.3 62.4 R3 43.3 52.3 55.6 75.6 (+20.0) 65.6 71.1 80.0 84.4 73.3 75.3 71.2 73.1 68.8 70.3 74.4 86.7 62.2 68.8 82.7 86.7 60.2 62.2 71.3 74.5 (+5.5) (+4.4) (+2.0) (+1.9) (+1.5) (+4.0) (+6.6) (+4.0) (+2.0) (+3.2) (+2.0) 68.1 70.1 51.1 72.2 (+21.1) R1 35.6 44.4 38.7 - 42.2 - 52.4 - 48.4 - 53.4 - 60.2 - 51.2 - 48.4 - 61.2 - 47.1 - 62.2 - - 51.2 - 71.1 88.8 (+17.7) - 81.2 78.8 85.4 81.3 75.3 73.3 86.4 88.2 88.4 86.3 78.4 73.2 91.2 87.2 50.2 53.2 52.1 55.4 65.3 62.4 54.6 57.2 68.2 65.1 68.2 66.2 58.1 56.1 F1 R2 (R1) 35.6 (+0.0) 48.2 (+3.8) 40.2 (+1.5) - 45.3 (+3.1) - 55.3 (+0.9) - 51.6 (+3.2) - 55.7 - 65.7 (+5.5) - 53.2 (+2.0) - 50.2 (+1.8) - 63.4 (+2.2) - 49.2 (+2.1) - 66.6 (+4.4) - - 52.6(+1.4) - - 54.6 (+4.4) 52.5 (-0.7) 56.3 (+4.2) 57.4 (+2.0) 67.2 (+1.9) 63.7 (+1.3) 56.9 (+2.3) 59.4 (+2.2) 69.0 (+0.8) 68.2 (+3.1) 72.1 (+3.9) 70.1 (+3.9) 60.2 (+2.1) 58.1 (+2.0) R3 38.2 49.3 45.3 67.8 (+12.5) 54.2 65.4 (+11.2) 67.2 70.3 (+3.1) 58.3 60.2 64.3 67.2 58.1 62.2 65.2 70.2 54.3 58.4 73.2 74.4 51.2 55.4 68.3 72. (+1.9) (+2.9) (+4.1) (+5.0) (+4.1) (+1.2) (+4.2) (+4.5) (+2.4) 69.2 71.6 45.3 64.3 (+19.0) 65.2 76.4 (+11.2) 75.3 71.5 68.3 65.2 69.1 65.9 68.4 75.3 76.2 74.2 74.1 74.1 80.3 76.1 R1 - - 51.1 - 47.4 - 70.0 - 55.1 - 73.1 - 69.4 - 64.8 - 53.8 - 74.1 - 53.3 - 71.1 - 65.1 - 57.8 - 55.6 - 55.7 54.9 75.4 71.3 75.2 71.2 70.2 68.2 74.8 72.8 72.3 69.3 76.8 72.3 G.A"
        },
        {
            "title": "Acc",
            "content": "R2 (R1) - - 54.4 (+3.30) - 49.1 (+1.70) - 75.0 (+5.00) - 67.7 (+12.6) - 75.4 (+2.30) - 70.2 (+0.80) - 66.3 (+1.50) - 56.0 (+2.20) - 76.1 (+1.20) - 56.5 (+3.20) - 73.4 (+2.3) - 70.4 (+5.30) - 58.3 (+0.50) - 58.7 (+3.10) - 58.8 (+3.10) 56.6 (+1.70) 79.3 (+3.90) 77.2 (+5.90) 77.8 (+2.60) 72.3 (+1.10) 74.3 (+4.10) 70.2 (+2.00) 76.2 (+1.40) 74.0 (+1.20) 75.8 (+3.50) 72.2 (+2.90) 78.8 (+2.00) 73.3 (+1.00) R3 - - 57.8 88.9 (+31.1) 65.6 82.2 (+16.6) 86.1 89.1 (+3.0) 76.7 81.1 (+4.4) 61.1 71.1 (+10.0) 51.2 61.1 (+9.9) 75.2 86.7 (+11.5) 62.2 68.8 (+6.6) 82.7 86.7 (+4.0) 60.2 62.2 (+2.0) 72.3 73.8 (+1.5) 70.1 72.4 (+2.3) 52.2 72.2 (+20.0) 68.8 91.1 (+22.3) 91.2 89.2 91.1 88.6 75.2 74.2 88.1 89.2 89.4 86.2 79.3 75.2 92.3 89.9 Table 11: Performance of LLMs on different rounds of annotation for the coarse classification task. S.A represents the string-matching evaluator, and G.A represents the GPT-based evaluator. represents prompting with only the target sentence, RT represents the combination of the review and the target sentence. Adding demonstrations to the prompt is represented by E. R1, R2 and R3 represent Round 1, Round 2, and Round 3 respectively. Method Mode Mix Zero-shot TE - - Instruction Tuned No Mix SciRiff Mix Tülu Mix Full Mix Zero-shot RT RTE - - Instruction Tuned RT No Mix SciRiff Mix Tülu Mix Full Mix Gemma-1.1 7B 18.13.2 22.23.4 (+4.1) 29.53.4 (+11.4) 23.53.4 (+5.4) 31.63.0 (+13.5) 25.43.0 (+7.3) 17.24.1 18.73.4 (+1.5) 26.83.1 (+9.6) 22.23.2 (+5.0) 25.73.0 (+8.5) 21.43.2 (+4.2) 23.04.6 27.13.4 (+4.1) 36.53.4 (+13.5) 29.53.3 (+6.5) 39.53.1 (+16.5) 30.03.0 (+7.0) 21.84.6 25.83.6 (+4.0) 28.83.2 (+7.0) 26.03.2 (+4.2) 30.63.0 (+8.8) 27.03.1 (+5.2) LLaMa 7B LLaMa 13B Mistral 7B Qwen 7B Yi-1.5 6B SciTülü 7B 14.24.5 22.25.4 (+8.0) 25.44.8 (+11.2) 38.64.1 (+10.6) 21.74.2 (+7.5) 28.34.2 (+14.1) 10.15.2 12.45.4 (+2.3) 22.94.4 (+12.8) 33.64.1 (+28.5) 28.64.2 (+18.5) 22.84.0 (+11.7) 24.55.6 28.45.8 (+3.9) 30.54.6 (+6.0) 46.34.6 (+6.0) 26.54.4 (+2.0) 35.54.0 (+11.0) 16.36.1 20.45.1 (+4.1) 28.84.4 (+12.5) 42.34.6 (+30.0) 33.14.0 (+16.8) 29.04.0 (+12.7) 17.82.9 21.42.2 (+3.6) 29.73.3 (+11.9) 32.03.4 (+14.2) 28.42.9 (+10.6) 31.53.1 (+13.7) 14.53.1 21.52.1 (+7.0) 28.41.4 (+13.9) 31.53.9 (+17.0) 27.23.0 (+12.7) 25.43.0 (+10.9) 26.53.4 29.32.4 (+2.8) 35.53.6 (+11.0) 36.03.2 (+10.5) 33.03.0 (+6.5) 37.03.2 (+10.5) 28.13.1 29.12.9 (+1.0) 30.61.6 (+12.5) 37.33.6 (+11.2) 33.13.0 (+5.0) 29.02.9 (+0.9) 21.04.6 18.64.7 (+2.4) 27.14.4 (+6.1) 24.04.2 (+3.0) 25.74.4 (+4.7) 28.64.2 (+7.6) 17.84.1 18.44.1 (+0.6) 24.14.4 (+6.3) 18.64.4 (+0.8) 21.03.8 (+3.2) 27.64.2 (+9.8) 23.03.8 25.54.9 (+2.5) 30.54.4 (+7.5) 26.54.2 (+3.5) 26.54.4 (+3.5) 32.34.4 (+9.3) 22.04.9 23.34.6 (+1.3) 27.64.4 (+5.6) 24.44.2 (+2.4) 25.03.8 (+3.0) 28.34.4 (+6.3) 21.11.1 22.21.1 (+1.1) 27.91.4 (+6.8) 27.61.4 (+6.5) 45.51.1 (+24.4) 31.91.0 (+10.8) 18.01.1 19.31.1 (+1.3) 22.61.4 (+4.6) 22.50.4 (+4.5) 25.11.2 (+7.1) 22.71.1 (+4.7) 24.51.1 28.21.5 (+3.7) 30.51.4 (+6.0) 31.51.6 (+7.0) 51.01.3 (+26.5) 35.51.0 (+11.0) 22.81.9 26.61.1 (+3.8) 29.61.1 (+6.8) 26.40.6 (+3.6) 31.31.6 (+8.5) 28.01.2 (+5.2) 24.21.1 29.11.8 (+5.9) 44.80.8 (+20.6) 45.41.2 (+21.2) 27.21.0 (+3.0) 45.70.8 (+11.5) 21.61.1 25.40.8 (+3.8) 28.70.4 (+7.1) 38.51.4 (+16.9) 35.80.8 (+14.2) 25.40.8 (+3.8) 30.51.2 36.52.0 (+6.0) 50.51.4 (+20.0) 51.51.7 (+21.0) 31.01.0 (+1.5) 50.00.9 (+19.5) 29.81.1 31.10.6 (+1.3) 36.90.4 (+7.1) 41.31.7 (+11.5) 40.60.9 (+10.8) 32.20.9 (+1.4) 11.12.2 11.22.4 (+0.1) 15.22.4 (+4.1) 28.82.4 (+17.7) 16.52.2 (+5.4) 20.42.0 (+9.3) 08.22.1 12.72.1 (+4.5) 31.53.4 (+23.3) 24.82.4 (+16.6) 28.72.0 (+20.5) 26.82.2 (+18.6) 14.02.4 20.62.6 (+6.6) 23.02.6 (+9.0) 36.32.4 (+22.3) 21.52.4 (+7.5) 26.52.2 (+12.5) 14.42.1 18.82.2 (+4.4) 34.4 (+20.0) 32.32.4 (+17.9) 35.62.2 (+21.2) 31.42.4 (+17.0) Table 12: Performance of LLMs for fine-grained classification with 3-fold cross-validation on LAZYREVIEW test sets. represents the string-matching evaluator, and represents the GPT-based evaluator reporting accuracies. represents prompting with only the target sentence, RT represents the combination of the review and the target sentence. Adding demonstrations to the prompt is represented by E. The best results for this task is highlighted in cyan. Increments are shown with the classic zero-shot setup without exemplars (first row for zero-shots) with or RT. Task Mode Mix Zero-Shot TE - - Instruction Tuned No Mix SciRiFF Mix Tülu Mix Full Mix Zero-Shot RT RTE - - Instruction Tuned RT No Mix SciRiFF Mix Tülu Mix Full Mix Gemma-1.1 7B 57.02.9 63.13.1 (+6.0) 69.33.2 (+12.3) 69.43.0 (+12.4) 69.63.2 (+12.6) 69.43.3 (+12.4) 58.23.1 59.93.0 (+1.7) 62.83.2 (+4.6) 63.43.2 (+5.2) 64.33.1 (+6.1) 64.33.3 (+6.1) 66.02.8 65.23.2 (-0.8) 71.53.2 (+5.5) 71.53.0 (+5.5) 72.43.2 (+6.4) 71.53.4 (+5.5) 61.33.1 63.43.0 (+2.1) 65.93.1 (+4.6) 66.33.0 (+5.0) 66.33.2 (+5.0) 66.93.5 (+5.6) LLaMa 7B LLaMa 13B Mistral 7B Qwen 7B Yi-1.5 6B SciTülü 7B 52.23.9 58.24.2 (+6.0) 69.24.3 (+12.0) 69.83.0 (+17.6) 65.43.8 (+13.2) 69.84.1 (+17.6) 51.54.2 56.14.0 (+13.3) 64.84.0 (+13.3) 66.53.8 (+1.7) 65.44.2 (+13.9) 65.34.2 (+13.8) 57.83.9 60.24.4 (+2.4) 71.54.1 (+13.8) 72.53.0 (+14.7) 67.03.8 (+9.2) 71.54.4 (+13.7) 55.64.2 58.24.1 (+11.3) 66.94.2 (+11.3) 66.93.9 (+11.3) 67.54.3 (+11.9) 66.94.3 (+11.3) 51.82.6 59.43.2 (+7.6) 60.42.9 (+8.6) 68.42.9 (+16.6) 65.32.8 (+13.5) 68.43.4 (+16.6) 54.83.6 58.43.8 (+3.6) 60.22.7 (+5.4) 58.23.0 (+3.4) 62.82.8 (+8.0) 65.43.2 (+9.6) 58.62.8 61.13.0 (+2.5) 65.03.0 (+6.4) 72.03.0 (+13.4) 67.32.8 (+8.7) 68.23.6 (+9.6) 57.63.4 61.33.6 (+3.7) 62.63.0 (+5.0) 60.62.8 (+3.0) 66.92.8 (+9.3) 66.93.4 (+9.3) 51.13.8 59.14.0 (+8.0) 61.04.2 (+9.9) 68.43.0 (+17.3) 56.23.8 (+5.1) 71.34.0 (+20.2) 58.84.2 57.44.0 (-1.4) 65.44.0 (+6.6) 66.43.4 (+7.6) 63.92.8 (+5.1) 64.84.3 (+6.0) 61.13.6 63.44.2 (+2.3) 65.24.0 (+4.1) 70.53.0 (+9.4) 62.53.8 (+1.4) 72.04.2 (+10.9) 60.04.0 61.14.1 (+1.1) 67.54.0 (+7.5) 68.13.3 (+8.1) 66.92.8 (+6.9) 66.94.4 (+6.9) 53.41.0 61.60.8 (+8.2) 63.30.8 (+15.9) 60.43.0 (+7.0) 69.30.8 (+15.9) 63.80.8 (+10.4) 51.00.8 54.50.7 (+3.5) 59.21.0 (+8.2) 58.51.0 (+7.5) 59.22.8 (+8.2) 56.40.4 (+5.0) 63.51.0 65.70.7 (+2.2) 64.01.0 (+0.5) 64.53.0 (+1.0) 72.01.0 (+8.5) 64.00.6 (+0.5) 55.00.7 57.80.8 (+2.8) 61.90.8 (+6.9) 60.60.8 (+5.6) 61.02.8 (+6.0) 58.10.8 (+3.1) 57.21.2 62.21.4 (+5.0) 68.21.0 (+11.0) 69.43.0 (+12.2) 66.31.5 (+9.1) 69.81.8 (+12.6) 50.41.0 51.81.2 (+1.4) 63.81.1 (+13.4) 64.31.2 (+13.9) 62.42.8 (+12.0) 62.81.8 (+12.4) 65.51.4 67.81.6 (+2.3) 71.51.2 (+6.0) 71.53.0 (+6.0) 68.01.7 (+2.5) 72.01.2 (+6.5) 52.51.1 54.51.4 (+2.0) 65.01.2 (+12.5) 66.21.4 (+13.7) 66.32.8 (+13.8) 65.61.6 (+13.1) 27.32.6 29.12.6 (+1.8) 69.62.8 (+42.0) 69.83.0 (+42.5) 65.22.8 (+37.9) 60.82.8 (+33.5) 27.82.4 32.82.8 (+5.0) 64.62.4 (+36.8) 64.32.6 (+36.5) 63.82.8 (+36) 64.52.4 (+36.7) 32.02.8 34.82.9 (+2.8) 71.52.9 (+39.5) 73.53.0 (+41.5) 67.02.8 (+35.0) 68.52.7 (+36.5) 31.82.6 36.92.9 (+5.1) 66.92.4 (+35.1) 66.92.8 (+35.1) 66.92.8 (+35.1) 66.92.6 (+35.1) Table 13: Performance of LLMs on 3-fold cross-validation on LAZYREVIEW test sets for coarse-grained classification. represents the string-matching evaluator, and represents the GPT-based evaluator reporting accuracies. represents prompting with only the target sentence, RT represents the combination of the review and the target sentence. Adding demonstrations to the prompt is represented by E. The best results for this task is highlighted in blue. Increments are shown with the classic zero-shot setup without exemplars (first row for zero-shots) with or RT. Model gemma-1.1-7b-it Llama-7B-chat Llama-13b-chat Mistral-7B-instruct Qwen-7B-chat Yi-6B-chat SciTülu 7B Class Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Fine-grained 0.56 1.00 1.00 1.00 0.88 1.00 0.83 1.00 0.86 1.00 1.00 0.97 1.00 1. 1.00 0.91 1.00 1.00 1.00 0.97 1.00 0.97 1.00 0.97 0.92 1.00 0.95 1.00 F1 0.71 0.95 1.00 1.00 0.93 0.99 0.91 0.91 0.92 0.99 0.96 0.98 0.98 0.98 Coarse-grained F1 0.93 0.92 0.95 0.95 0.92 1.00 0.97 0.98 0.95 1.00 1.00 1.00 0.96 0.96 0.95 1.00 1.00 1.00 0.95 0.98 0.92 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.98 0.94 0.91 1.00 1.00 1.00 Table 14: Manual evaluation of the string-based evaluator for each model in terms of Precision (P), Recall (R), and F1 scores. Model Gemma-1.1-7b-it LlaMa-7B-chat LlaMa-13b-chat Mistral-7B-instruct Qwen-7B-chat Yi-6B-chat SciTülu 7B Class Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Correct Incorrect Fine-grained 0.69 0.89 0.76 0.89 0.76 1.00 0.82 1.00 0.89 1.00 0.88 1.00 0.83 1.00 0.96 0.42 1.00 0.42 1.00 0.44 1.00 0.36 1.00 0.42 1.00 0.76 1.00 0.86 F1 0.81 0.57 0.87 0.57 0.87 0.61 0.90 0.53 0.94 0.59 0.93 0.87 0.94 0. Coarse-grained 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 F1 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Table 15: Manual evaluation of the GPT-3.5 evaluator for each model in terms of Precision (P), Recall (R), and F1 scores."
        }
    ],
    "affiliations": [
        "Data Science Group, University of Hamburg",
        "Department of Data Science & AI, Monash University, Australia",
        "School of Computing Technologies, Royal Melbourne Institute of Technology, Australia",
        "Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt"
    ]
}