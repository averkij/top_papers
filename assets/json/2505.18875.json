{
    "paper_title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
    "authors": [
        "Shuo Yang",
        "Haocheng Xi",
        "Yilong Zhao",
        "Muyang Li",
        "Jintao Zhang",
        "Han Cai",
        "Yujun Lin",
        "Xiuyu Li",
        "Chenfeng Xu",
        "Kelly Peng",
        "Jianfei Chen",
        "Song Han",
        "Kurt Keutzer",
        "Ion Stoica"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 7 8 8 1 . 5 0 5 2 : r Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation Shuo Yang Haocheng Xi Yujun Lin"
        },
        {
            "title": "Ion Stoica",
            "content": "University of California, Berkeley MIT"
        },
        {
            "title": "Stanford University",
            "content": "Figure 1: SVG2 accelerates video generation while maintaining high quality. On single H100, for HunyuanVideo and Wan 2.1, SVG2 achieves up to 2.30 and 1.89 end-to-end speedup, with PSNR up to 30 and 26."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, training-free framework that maximizes identification accuracy and minimizes computation waste, achieving Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both precise cluster Equal contribution. Preprint. Under review. representation, improving identification accuracy, and densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30 and 1.89 speedup while maintaining PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively."
        },
        {
            "title": "Introduction",
            "content": "Diffusion Transformers (DiTs) have demonstrated significant efficacy in generative tasks, particularly excelling in generating high-quality images and videos [1, 2]. However, the computational efficiency of DiTs remains major bottleneck, primarily due to the quadratic computational complexity introduced by 3D spatio-temporal attention mechanisms [3]. For instance, generating just five-second video using HunyuanVideo on an NVIDIA A100 GPU takes nearly an hour, where the 3D attention accounts for more than 80% of end-to-end runtime. This inefficiency severely limits the practical deployment of DiT-based generative models. Figure 2: Trade-off curves between generation quality (PSNR) and efficiency (density). SVG2 consistently surpasses existing methods given the same density, achieving Pareto frontier. To mitigate the quadratic computational complexity, previous studies have observed that self-attention mechanisms are naturally sparse, where only small portion of computations significantly influence the final output [4, 5, 6]. Therefore, the computational costs can be dramatically reduced (up to 8) with negligible degradation in generation quality by only computing the critical tokens [3, 7]. To effectively identify these critical tokens, existing approaches introduce an identification step where activations from each token are used to estimate attention scores [8, 9]. Tokens with the highest scores are then selected as critical and processed by the following sparse attention. To minimize overhead, the identification is typically processed at the block granularity, treating consecutive tokens as an aggregated token, which are selected or ignored as whole [10, 11]. However, we observe that given the same computational budget (i.e., the number of selected critical tokens), existing sparse attention methods significantly fall behind the oracle generation quality, where the critical tokens are selected assuming the attention scores are known in advance rather than estimated ( 3.2). We identify that this performance gap arises from two primary challenges: 1. Inaccurate identification: existing block-wise identification methods are ineffective in precisely identifying critical tokens. Because tokens are clustered into blocks based on positions rather than semantic similarities, tokens within the same block may have dramatically different activations in the latent space. Consequently, the aggregated activations become less representative [10], leading to inaccurate estimations of attention scores and thus incorrect identification of critical tokens. E.g., widely used techniques such as mean pooling [8] and max pooling [4] are prone to inaccuracies, particularly when applied to distinct tokens. 2. Computation waste: existing methods cause computation waste even if critical tokens could be perfectly identified. This is because of the mismatch between sparse computation and hardware specifications [12]. For instance, tensor cores on NVIDIA GPUs require minimum matrix multiplication shape of 16 16 8 [13], which necessitates batch size of 16 tokens. Thus, even if only subset of block of 16 tokens are critical, the entire block must still be computed to utilize tensor cores, causing computation waste. Our empirical evaluations show that up to 80% of computation can be wasted on non-critical tokens ( 3.2). To bridge this gap, we propose SVG2, training-free sparse attention approach specifically designed to accelerate video generation for DiT-based models, achieving Pareto frontier trade-off between generation quality and computational efficiency (as shown in 5.4). Our key insight is to leverage semantic-aware permutation to maximize the accuracy of critical token identification and minimize the computation waste of sparse computation. Specifically, semantic-aware permutation clusters tokens into blocks according to the semantics of activations rather than positions. Consequently, tokens within each block exhibit closely aligned activations, ensuring more accurate aggregated representations and thereby significantly improving identification accuracy. Additionally, semanticaware permutation densifies sparse computations by consolidating scattered critical tokens into 2 compact, dense blocks. Due to their semantic similarity, tokens in single block tend to be either all critical or all non-critical. This property ensures that computation is not wasted on blocks containing mix of critical and non-critical tokens, thus improving computational efficiency. To integrate semantic-aware permutation into an end-to-end framework, SVG2 introduces three key techniques. First, to implement semantic-aware permutation, SVG2 applies k-means clustering on the Query, Key, and Value vectors of each head and layer before the identification step. The resulting clusters are then permuted so that tokens within the same cluster are grouped together, ensuring semantically coherent blocks. Second, to enable dynamic allocation of the computational budget, SVG2 adopts Top-p critical token selection strategy inspired by [10, 14]. Specifically, SVG2 uses the centroids of clusters to approximate attention scores for each cluster, selecting tokens with the highest estimated scores until their cumulative sum reaches p. This approach enables dynamic budget allocation without manual adjustments. Third, to support dynamic block sizes for sparse attention, SVG2 introduces customized kernel implementation. This is essential because the clusters formed by semantic-aware permutation naturally vary in size, and existing block sparse attention kernels, which are limited to fixed block sizes, cannot efficiently handle such variability. We prototype SVG2 based on an open-sourced video generation framework [3] and customize kernels with FlashInfer [15]. We evaluate SVG2s quality and efficiency on representative video generative models including HunyuanVideo [1] and Wan 2.1 [2]. Results demonstrate that SVG2 consistently achieves Pareto frontier, delivering superior generation quality at any given computational budget. Specifically, SVG2 delivers significant efficiency improvements, achieving an end-to-end speedup of up to 2.30 and 1.84 speedup while maintaining high visual quality with PSNR of up to 30 and 26 on HunyuanVideo and Wan2.1-I2V, outperforming all prior methods."
        },
        {
            "title": "2.1 Efficient Sparse Attention",
            "content": "Sparse attention for video DiTs. Sparse attention mechanisms for accelerating DiTs fall into two categories: static and dynamic, depending on whether to select critical tokens dynamically during runtime or statically offline. Static methods [7, 3] predefine sparse patterns offline, such as identifying recent tokens as critical [3]. These methods lack adaptability to diverse sparsity patterns, leading to suboptimal performance. Dynamic methods [8, 9, 16] determine sparse patterns at runtime, selecting critical tokens through an additional identification step. However, existing dynamic methods fail to achieve both high identification accuracy and low computation waste. In contrast, SVG2 consistently achieves superior performance under the same computation budget. Sparse Attention for Large Language Models (LLMs). Sparse attention for LLMs falls into two categories: memory-efficient and compute-efficient. Memory-efficient methods [5, 4, 6, 17, 18] reduce memory load to accelerate decoding but are ineffective for compute-bound DiT-based video generation. Compute-efficient methods [19, 20, 21, 11] focus on processing only critical tokens, yet cannot directly optimize video DiTs due to unique sparse patterns of video data. Notably, MMInference [11] introduces modality-aware permutation for multi-modal LLMs. This permutation is rule-based, designed to permute inter-modality tokens."
        },
        {
            "title": "3.1 Attention in DiTs is Inherently Sparse",
            "content": "Attention operation in DiTs is costly. During each denoising step, DiTs transform the input activations with hidden dimension into Query (Q), Key (K), and Value (V ) tensors, followed by self-attention operation to produce the final output [22]: = V, = softmax (cid:18) QK (cid:19) , dim = 1 where the attention score captures the relationship among tokens However, computing has quadratic complexity relative to the sequence length. State-of-the-art DiTs typically process thousands of tokens per frame across multiple frames, creating significant performance bottleneck. 3 Figure 4: Illustration of how existing methods fall short due to the inaccurate identification and computation waste, assuming computation unit of 4 4 block. (a) Original attention map of demonstration example. (b) Position-based clustering groups distinct tokens within the same clustering, causing the imprecise representation of mean-pooling or max-pooling. Therefore, blocks with smaller number of critical tokens are ignored, causing lower recall of attention scores. (c) Due to the scattered layout of critical tokens, even if achieving high attention recall, each compute block processes both critical and non-critical tokens, thus causing computation waste and decreasing effective compute on critical tokens. (d) Semantic-aware permutation clusters and reorders similar tokens into contiguous layout, thus achieving high attention recall while minimizing computation waste. E.g., generating 33-frame video using HunyuanVideo-T2V-13B requires over 80% of the total end-to-end time to be spent on the attention alone [3, 7]. Attention operation in DiTs is highly sparse. Fortunately, attention is inherently sparse, where only small subset of computations significantly contributes to the final output. This sparsity arises from the characteristics of the softmax function, where few largest values in dominate the attention score , which in turn dictates the final weighted output [5, 10]. To quantitatively assess this sparsity, we collect attention maps from Wan2.1-I2V-14B video generation and visualize the average recall of attention scores under varying computational budgets, defined by the number of critical tokens. Specifically, the critical tokens are selected following an oracle policy, where tokens are ranked in descending order based on their attention scores. This approach illustrates the upper bound of achievable recall under constrained computational budget. As depicted in Figure 3, the attention is highly sparse, where only 13% of the computations are sufficient to achieve an attention recall of 95%, maintaining nearlossless PSNR of 27 while providing up to 2 theoretical end-to-end speedup. This observation highlights an opportunity to leverage the trade-off between generation quality and computational efficiency. Figure 3: Comparison of attention recall versus density (i.e., number of sparse computation normalized by total computation) for the oracle policy, SVG, and SpargeAttention. Notably, the significant gap between the oracle policy and existing methods highlights the potential for improvement."
        },
        {
            "title": "3.2 Existing Sparse Attention Fails to Match the Oracle Policy",
            "content": "Despite the potential of sparsity in reducing computational cost, directly adopting the oracle policy is impractical. This is because identifying critical tokens requires calculating the full attention scores by K, thus providing no actual speedup. To achieve practical efficiency, state-of-theart approaches [8, 9] implement coarse-grained identification strategy. Specifically, they cluster consecutive tokens into large blocks and calculate attention scores at the block level, providing an approximation of the original . This approach significantly reduces the identification overhead, with less than 1% computation compared to the full attention when using block size of 128. However, existing coarse-grained approaches reduce identification accuracy and lead to computation waste. As illustrated in Figure 3, existing sparse attention mechanisms consistently fall significantly short of achieving the attention recall of the oracle policy, regardless of the computation budget. This performance gap arises primarily from two key factors: Position-based clustering leads to inaccurate identification. Existing methods reduce identification overhead by clustering consecutive tokens into blocks. For instance, SpargeAttention [8] groups 4 every 128 query tokens and 64 key tokens, using mean pooling to create single representation for each block, which is then used to approximate . However, this position-based clustering does not guarantee semantic similarity among tokens. Tokens within the same block can exhibit vastly different activations in the latent space. For example, two physically close objects in video frame, such as an apple and cake, may have no semantic relationship. This variability within block degrades the quality of the aggregated block-wise representation, leading to reduced identification accuracy. We illustrate this issue in Figure 4 and provide quantitative analysis of the identification accuracy in 5.5. To address this problem, we propose using semantic-aware clustering instead of position-based clustering, as detailed in 4.1. Scattered critical tokens cause computation waste. Even if all critical tokens are perfectly identified, existing sparse attention mechanisms cannot achieve the theoretical computational savings promised by the oracle policy due to mismatch between scattered sparse computations and hardware specifications. Since the criticality of tokens is determined by semantics, critical tokens are naturally scattered across the tensor rather than being contiguous. However, modern ML accelerators, such as NVIDIA GPU tensor cores, are optimized for dense matrix multiplication, which requires contiguous input dimensions [13, 23]. As result, scattered critical tokens must be padded with non-critical tokens to maintain contiguous layout, leading to significant computation waste. This issue is visualized in Figure 4, and we further quantify the computation waste in 5.5. To approach the performance of the oracle policy, an automatic permutation is required to rearrange scattered critical tokens into dense layout to minimize computation waste, as detailed in 4.1."
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we introduce SVG2, training-free sparse attention framework designed to use semantic-aware permutation to achieve Pareto frontier trade-off between the generation quality and computational efficiency for video DiTs. We visualize the workflow of SVG2 in Figure 5. At the core of SVG2 is semantic-aware permutation, which aims to maximize identification accuracy of critical tokens and minimize computation waste ( 4.1). To dynamic select and adjust the computation budget, SVG2 proposes centroid-based top-p selection, which enables practical deployment ( 4.2). Additionally, SVG2 investigates several system-algorithm co-designs, such as fast k-means and attention kernel, significantly accelerating video generation ( 4.3)."
        },
        {
            "title": "4.1 Semantic-Aware Permutation with k-means Clustering",
            "content": "As discussed in 3.2, existing sparse attention mechanisms suffer from inaccurate identification due to the position-based clustering. To this end, SVG2 proposes using semantic similarity to cluster rather than position, by performing k-means on the activations of the input tokens. Specifically, for each attention head and transformer layer, k-means is independently applied to query tokens (Q RNqd) and key tokens (K RNkd), where Nq and Nk represent the number of tokens in and K, creating Cq query clusters Q1, . . . , QCq and Ck key clusters K1, . . . , KCk . This approach enables tokens within each cluster to share similar semantics, improving the precision of centroid representation for better identification, as detailed in 4.2. Furthermore, to densify the sparse computation of scattered critical tokens, SVG2 performs semanticaware permutation based on the k-means clustering. Although the semantically similar tokens are logically clustered, they are physically scattered in the tensors, resulting in substantial computational waste as described in 3.2. To address this, SVG2 permutes tokens within each cluster into contiguous layout. Such cluster-wise contiguous layout can be efficiently computed by the underlying ML accelerators, thus reducing computation waste. We detail the permutation algorithm and the mathematical equivalence for the attention output in the following formulations. Assuming Πq RNqNq and Πk RNkNk be the permutation matrices such that ΠqΠ = I, the permuted tokens are then = ΠqQ, = ΠkK, and = ΠkV , where and share the same = and ΠkΠ 5 Figure 5: Overview of SVG2. (a) Original attention map of demonstration example, with various colors representing various semantics. Only tokens with similar semantic attend to each other, having high attention scores thus selected as critical tokens. (b) After k-means clustering, semantic-similar tokens (i.e., similar colors) are grouped into the same cluster, with the query and key centroids to precisely represent the cluster-level semantic. These centroids are then used to approximate the attention score for accurate identification of critical tokens. (c) Combined with Top-p selection, critical tokens can be dynamically identified in contiguous layout. permutation Πk to guarantee the output equivalence. The permuted attention output is: =Π Attention(Q, , ) = Π softmax =(Π Πq)softmax (cid:19) (cid:18) QK (Π Πk)V = softmax"
        },
        {
            "title": "4.2 Centroid-Based Top-p Selection",
            "content": "(cid:18) (ΠqQ)(ΠkK) (cid:19) ΠkV (cid:18) QK (cid:19) = Despite the semantic-coherent clusters provided by the semantic-aware permutation, it remains impractical to deploy SVG2 without addressing two critical challenges: (1) how to effectively estimate the criticality of clusters, and (2) how to dynamically determine the number of selected critical clusters (i.e., the number of critical tokens) to satisfy arbitrary accuracy requirements. Accurate and efficient estimation of criticality. To estimate the criticality of each cluster, SVG2 introduces centroid-based estimation of attention scores . Specifically, it approximates the criticality of each token by estimating its using the centroids of its cluster, mimicing the oracle policy defined in 3.1. As formulated in Equation 1, this approach calculates the pre-softmax scores using the centroids of each cluster. These scores are then weighted by the number of tokens within the cluster (i.e., the size of the cluster), to generate an approximate attention score in Equation 2, providing an estimation of the actual . Sij = centroid(Qi) centroid(Kj)T dk (1) ij = Kj exp(Sij) k=1 Kk exp(Sik) (cid:80)Ck (2) Since tokens within the same cluster already share similar semantics, the centroids can serve as highly accurate representations of the actual activations, ensuring the reliability of such estimation. Furthermore, because the typical number of clusters (i.e., Cq and Ck) is less than 1024, the computational overhead for this approximation is negligible compared to the full attention calculation, typically accounting for less than 1% of the total computational cost. Dynamic adjustment of computation budget. To dynamically adjust the number of critical tokens instead of pre-defined constant, SVG2 employs Top-p selection strategy based on the approximated . SVG2 first sorts all potential clusters in descending order according to their corresponding . It then selects clusters sequentially until the accumulated reaches predefined target."
        },
        {
            "title": "4.3 Efficient System-Algorithm Co-design",
            "content": "Fast k-means with centroid cache. While k-means clustering is essential to semantic-aware permutation, its iterative process can introduce substantial latency if the number of iterations is large before convergence. For example, with the state-of-the-art GPU implementation of k-means++ [24], it 6 Figure 6: Visualization of attention maps from different attention heads in Wan2.1 when generating videos from VBench [29]. (a) Original attention maps with diverse sparse patterns, assuming critical tokens highlighted in red. (b) Permuted attention maps. After semantic-aware permutation, critical tokens are permuted into contiguous layout based on the k-means clustering, enabling efficient block-wise computation without waste. (c) Recovered attention maps after applying centroid-based top-p selection and undoing the permutation. The high similarity between the original and recovered attention maps demonstrates the effectiveness of SVG2. takes more than 100 iterations to converge, consuming 50% or even comparable time to the attention computation. Fortunately, DiTs are known to be similar between consecutive denoising steps [25, 26], enabling reusing the centroids from the previous step as the fast initialization for k-means in the next step. Based on this observation, SVG2 implements centroids cache, which automatically caches and reuses centroids between consecutive steps. This technique reduces the runtime of k-means by up to 76, as evaluated in 5.3. Efficient sparse attention kernel for varied block-sizes. While existing efficient attention implementations (e.g., FlashAttention [27], FlexAttention [28], and FlashInfer [15]) support block-wise sparse computation, they only support static block size (e.g., 128 128). However, the sizes of clusters after semantic-aware permutation are naturally dynamic and diverse, causing computation waste with the static block size. For example, SVG2 could generate query cluster with 128 tokens with key cluster with 32 tokens. Such 128 32 computation needs to be padded into 128 128 to use existing kernels, which causes 75% computation waste. To address this, SVG2 implements customized attention kernels that accepts dynamic block-sizes as input. By gathering blocks with various sizes on on-chip memory from global memory, SVG2 is able to utilize the accelerators (e.g., tensor cores in NVIDIA GPUs). We evaluate the kernel efficiency in 5.3."
        },
        {
            "title": "5.1 Setup",
            "content": "Models. We evaluate SVG2 on open-sourced state-of-the-art video generation DiT models including Wan2.1-I2V/T2V-14B [2], and HunyuanVideo-T2V-13B [1] to generate videos with 720p resolution. After being tokenized by 3D-VAE, Wan2.1 generates 21 frames with 3600 tokens per frame, while HunyuanVideo processes 33 frames with 3600 tokens per frame. Metrics. We assess the similarity of generated video compared to full attention using the following metrics: Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), and Structural Similarity Index Measure (SSIM). We use VBench [29] to evaluate the video quality, such as background consistency and motion smoothness. To quantify the efficiency of sparse attention mechansims (i.e., computational budget), we use density, which is defined as the sparse attention computation divided by the full attention computation. To assess end-to-end efficiency, we use the total amount of computation (i.e., FLOPs) needed for generating videos. Datasets. For text-to-video generation, we adopt the prompt in Penguin Benchmark after prompt optimization provided by VBench team. For image-to-video generation, we adopt the prompt-image pairs provided by VBench [29] and crop images to 16 : 9 ratios for 720p resolution. Baselines. We compare SVG2 against state-of-the-art sparse attention algorithms including static method Sparse VideoGen (SVG) [3], and dynamic methods SpargeAttention [8] and XAttention [9]. 7 Table 1: Quality and efficiency benchmarking results of SVG2 and baselines over VBench. Model Config PSNR SSIM LPIPS Smoothness Consistency Density FLOPs Speedup Wan 2.1 14B, 720P, Image-to-Video - SpargeAttn SVG Ours Ours-Turbo 21.181 24.059 26.562 24. Wan 2.1 14B, 720P, Text-to-Video - SpargeAttn SVG Ours Ours-Turbo 20.519 22.989 25.808 23.682 Hunyuan 13B, 720P, Text-to-Video - SpargeAttn XAttention SVG Ours 27.892 28.892 29.157 30.452 - 0.665 0.813 0.861 0. - 0.623 0.785 0.854 0.789 - 0.884 0.898 0.905 0.910 - 0.333 0.174 0.138 0. - 0.343 0.199 0.138 0.196 - 0.151 0.120 0.120 0.117 98.46 97.81 98.46 98.39 98. 98.96 98.31 98.98 98.89 98.82 99.39 99.30 99.36 99.32 99.24 96.07 92.82 95.95 95.88 95. 98.46 97.48 96.67 96.15 95.93 93.85 92.41 93.13 93.15 92.74 100% 526.76 PFLOPs 1x 38.99% 30.25% 31.28% 14.13% 366.80 PFLOPs 343.88 PFLOPs 346.59 PFLOPs 301.62 PFLOPs 1.47x 1.58x 1.58x 1.84x 100% 658.46 PFLOPs 1 42.03% 30.25% 29.51% 12.87% 468.46 PFLOPs 429.86 PFLOPs 427.43 PFLOPs 372.89 PFLOPs 1.44x 1.58x 1.58x 1.89x 100% 612.38 PFLOPs 1x 42.62% 470.40 PFLOPs 39.32% 459.58 PFLOPs 351.75 PFLOPs 29.86% 335.36 PFLOPs 25.45% 1.53x 1.56x 1.91x 2.30x Figure 7: Efficiency evaluation for fast k-means with centroids cache and customized attention kernel. Note that we skip the evaluation of XAttention on Wan2.1 as it is not supported yet. For SVG, SpargeAttention, and XAttention, we use their official configurations. Implementations. We prototype SVG2 as an end-to-end framework with customized kernels from FlashInfer [15] and benchmark on NVIDIA H100 GPU with CUDA 12.8. For SVG2, we choose Cq = 100 and Ck = 500, which empirically generalizes well. To showcase the trade-off between generation quality and efficiency, we evaluate on various accuracy target (i.e., attention score recall) as detailed in 5.4. We also sample single data point for detailed comparison as shown in Table 1. We skip performing sparse attention on the first 30% denoising steps for all methods as they are critical generation quality, following previous work [26, 30, 31, 25]."
        },
        {
            "title": "5.2 Quality Evaluation",
            "content": "We first qualitatively showcase the effectiveness of our proposed method by showing the visualization of attention maps. As shown in Figure 6, we collect attention maps from different attention heads when running Wan2.1 on prompts from VBench. Despite the diversity of the sparse patterns (i.e., different columns in Figure 6), semantic-aware permutation effectively densifies critical tokens into contiguous layout, which enables efficient computation without waste. Furthermore, by applying centroid-based top-p selection and then undoing the permutation, the permuted attention map is recovered into the original layout, which shows high similarity between the original attention map. To quantitatively assess the generation quality, we evaluate the quality of videos generated by SVG2, when compared to baselines and report the results in Table 1. SVG2 consistently outperforms all baseline methods in terms of PSNR, SSIM, and LPIPS, while still maintaining the highest speedup. Specifically, SVG2 achieves an average PSNR of 26.5 on Wan2.1 and 30.4 on HunyuanVideo, demonstrating its effectiveness on generating highly consistent and smooth videos."
        },
        {
            "title": "5.3 Efficiency Evaluation",
            "content": "Efficient k-means with centroids cache. To demonstrate the effectiveness of centroids cache in improving efficiency of k-means, we compare the density achieved by SVG2 to reach the 90% attention recall, when varying different number of execution time (i.e., number of k-means iterations). We use widely-used algorithm k-means++ [24]. Since the k-means quality directly determine the accuracy of critical token identification, it also determines the achieved density. The lower the density is, the better the k-means is. As shown in Figure 7(a), enabling centroids cache reduces the end-to-end latency of k-means by 76 when reaching comparable or lower density. This demonstrates the effectiveness of centroids cache, which greatly reduce the initialization time. Efficient attention kernel with dynamic block-sizes. To showcase the efficiency of our customized attention kernels with dynamic block-sizes, we evaluate the computation FLOPs of our implementation compared with state-of-the-art attention library, FlashInfer [15]. We vary different combination of hyper-parameters (i.e., number of clusters Cq and Ck) and apply centroid-based top-p selection to generate the practical workloads of dynamic block sizes. We fix the attention recall as 90%. As shown in Figure 7(b), our customized kernels achieve an average of 1.48 computation reduction. On practical setup with Cq = 100, Ck = 500, ours achieves 1.88 reduction of computation waste. End-to-end speedup evaluation. To showcase the end-to-end speedup of SVG2, we incorporate several efficiency metrics, including density, FLOPs, and end-to-end speedup into Table 1. SVG2 achieves an average speedup of 1.82 while maintaining the highest generation quality. We also include SVG2-Turbo to showcase the efficiency potential, which maintains similar generation quality as baselines but achieves much higher speedup. Specifically, SVG2-Turbo achieves 2.5 smaller density compared to SVG while achieving an even better PSNR of 23.7. Such results can be cross-validated with the sensitivity evaluation in 5.4."
        },
        {
            "title": "5.4 Sensitivity Test on Quality-Efficiency Trade-off",
            "content": "To validate the effectiveness of SVG2, we conduct comprehensive evaluation on Wan2.1-I2V-14B, comparing it against baseline methods across wide range of computational budgets (i.e., density). As shown in Figure 2, SVG2 consistently achieves better generation quality at any given density, positioning it on the Pareto frontier of the quality-efficiency trade-off. Notably, SVG2 reduces density by up to 2.3 while maintaining the same PSNR."
        },
        {
            "title": "5.5 Ablation Study on Semantic-Aware Permutation",
            "content": "Effectiveness on improving identification accuracy. To assess the effectiveness of semantic-aware permutation in improving the accuracy of critical token identification, we measure attention recall by comparing semantic-aware permutation-enabled and semantic-aware permutationdisabled configurations across varying computational budgets. Both methods use mean-pooling and maintain the same cluster size for consistency. As shown in Figure 8, semantic-aware permutation consistently achieves higher attention recall, indicating more accurate identification of critical tokens. This improvement is attributed to the semantic-coherent clusters generated by semantic-aware permutation, which offer precise representations. Figure 8: Attention recall across various densities. Enabling permutation consistently surpasses disabling permutation. Effectiveness on reducing computation waste. We further investigate the impact of semantic-aware permutation on reducing computational waste. Specifically, we use the same set of critical tokens selected by centroid-based top-p selection. For the semantic-aware permutation-enabled configuration, we feed the contiguous layout generated after permutation into GPUs, while for the semantic-aware permutation-disabled configuration, we use the scattered layout before permutation. As shown in our results, enabling semantic-aware permutation reduces computational overhead by an average of 36%, while maintaining the same set of critical tokens."
        },
        {
            "title": "6 Conclusion & Limitation",
            "content": "In this paper, we proposed SVG2, training-free sparse attention approach for accelerating DiT-based video generation. By clustering tokens based on semantic similarity, SVG2 accurately identifies critical tokens. By permuting critical tokens into contiguous layout, SVG2 effectively reduces the computation waste. Comprehensive evaluations show that SVG2 achieves superior trade-off between generation quality and efficiency, making video generation more efficient and practical. The major limitation of this paper lies in the lack of discussion and evaluation on whether the proposed methods can be extended to attention mechanisms other than DiTs."
        },
        {
            "title": "References",
            "content": "[1] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3, 7 [2] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 7 [3] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. In ICML, 2025. 2, 3, 4, 7 [4] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In ICML, 2024. 2, 3 [5] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models, 2023. 2, 3, [6] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In ICLR, 2024. 2, 3 [7] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. 2, 3, 4 [8] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025. 2, 3, 4, [9] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. In ICML, 2025. 2, 3, 4, 7 [10] Kan Zhu, Tian Tang, Qinyu Xu, Yile Gu, Zhichen Zeng, Rohan Kadekodi, Liangyu Zhao, Ang Li, Arvind Krishnamurthy, and Baris Kasikci. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms, 2025. 2, 3, 4 [11] Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, and Lili Qiu. Mminference: Accelerating pre-filling for long-context vlms via modality-aware permutation sparse attention, 2025. 2, 3 [12] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Yuqing Yang, Lingxiao Ma, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, and Lidong Zhou. Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation, 2023. 2 [13] NVIDIA. Nvidia a100 tensor core gpu. https://www.nvidia.com/content/dam/en-zz/ Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web. pdf, 2021. 2, 5 [14] Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, and Mingyu Gao. Twilight: Adaptive attention sparsity with hierarchical top-p pruning, 2025. 3 [15] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving, 2025. 3, 7, 8, 9 [16] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation, 2025. 3 [17] Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, and Lianmin Zheng. Post-training sparse attention with double sparsity, 2024. 3 [18] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads, 2024. [19] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. NeurIPS, 2024. 3 [20] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference, 2025. 3 [21] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden KwokHay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms, 2025. 3 [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [23] Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. Sparsetir: Composable abstractions for sparse compilation in deep learning, 2023. 5 [24] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 07, page 10271035, USA, 2007. Society for Industrial and Applied Mathematics. 6, 9 [25] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In CVPR, 2025. 7, 8 [26] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast, 2025. 7, [27] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. NeurIPS, 2024. 7 [28] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels, 2024. 7 [29] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models, 2023. 7 [30] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In CVPR, 2024. [31] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee K. Wong. Fastercache: Training-free video diffusion model acceleration with high quality. 2024."
        },
        {
            "title": "A Visualization of the generated videos",
            "content": "We provide visualization comparison between SVG2 and Dense Attention on HunyuanVideo and Wan 2.1. Results in Figure 9 and Figure 10 demonstrate that SVG2 can preserve high pixel-level fidelity, achieving similar generation quality compared with the dense attention. Real video samples are provided in the supplementary materials."
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG2 Figure 9: Comparion of Dense Attention and SVG2 on HunyuanVideo and Wan 2.1 Text-to-Video generation. 12 SVG2 SVG2 SVG SVG2 SVG2 SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG"
        },
        {
            "title": "Dense Attention",
            "content": "SVG2 Figure 10: Comparison of Dense Attention and SVG2 on Wan 2.1 Image-to-Video generation."
        }
    ],
    "affiliations": [
        "MIT",
        "University of California, Berkeley"
    ]
}