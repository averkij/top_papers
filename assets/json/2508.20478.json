{
    "paper_title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
    "authors": [
        "Yuan Xie",
        "Tianshui Chen",
        "Zheng Ge",
        "Lionel Ni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding."
        },
        {
            "title": "Start",
            "content": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding Yuan Xie1,3, Tianshui Chen2*, Zheng Ge3, Lionel Ni1 1The Hong Kong University of Science and Technology (Guangzhou) 2Guangdong University of Technology 3StepFun AI yxie688@connect.hkust-gz.edu.cn tianshuichen@gmail.com 5 2 0 2 8 2 ] . [ 1 8 7 4 0 2 . 8 0 5 2 : r Abstract Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose VideoMTR, reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding. Introduction As foundational computer vision task, video understanding finds widespread applications in numerous domains, ranging from intelligent surveillance, content-based retrieval, to autonomous driving. With the explosive growth of usergenerated videos and the ubiquity of cameras in daily life, the demand for robust and scalable video-understanding tools has grown substantially. Owing to the advanced reasoning capabilities, Multimodal Large Language Models (MLLMs) (Dai et al. 2023; Wu and Xie 2024; Weng et al. 2024; Chen et al. 2024b) have demonstrated breakthroughs in visual understanding tasks for images and short videos in recent years. However, long-form video understanding, characterized by multiple events and long-range temporal dependencies, still presents significant challenges. Existing approaches (Wang et al. 2024b; Lin et al. 2023; Feng et al. 2025) either employ instruction tuning or inte- *Corresponding author grate reinforcement learning to adapt current MLLMs for long-term temporal reasoning. However, these methods primarily transfer training paradigms designed for language and image modalities, relying on static reasoning approach that generates predictions based on fixed, uniform set of sampled frames in single turn. This single-turn, uniform sampling strategy becomes bottleneck for downstream reasoning tasks when dealing with long-form videos, as it risks omitting critical information due to the extended video duration. Alternatively, other approaches (Fan et al. 2024; Wang et al. 2024a) explore the agentic paradigm, where large language models (LLMs) serve as agents, utilizing external visual-language models (VLMs) (Radford et al. 2021; Zhao et al. 2023) to identify key video segments. These methods depend on pretrained VLMs and carefully designed pipelines. While they achieve superior performance, they are hindered by high complexity due to the reliance on heterogeneous external components and sub-optimal tool usage strategies, as they lack end-to-end training. In this work, we propose Video-MTR, reinforced multiturn reasoning framework that leverages the intrinsic capabilities of MLLMs, equipped with bi-level rewards, for iterative key video segment selection and question comprehension within unified model. Unlike existing video reasoning models, Video-MTR enables iterative selection of key video segments based on the current state, which is derived from previously selected segments and the question. This approach facilitates the progressive identification of more informative video segments. Compared to the agentic paradigm, Video-MTR eliminates the reliance on external VLMs and carefully design pipelines, enabling end-to-end training that optimizes video segment selection and, in turn, enhances question comprehension. Formally, Video-MTR builds upon existing MLLM model, Qwen2.5-VL-7B (Bai et al. 2025) and is trained to develop iterative video reasoning capabilities through an end-to-end reinforcement learning strategy. However, current reward systems based solely on answer accuracy offer limited guidance for intermediate video segment selection, particularly in complex long videos. To address this challenge, we introduce novel gated bi-level reward system, consisting of trajectory-level rewards based on answer correctness and turn-level rewards that capture frame-query relevance. This reward system relies on key segment annotations for turn-level rewards and the final answer for trajectory-level rewards. To enable this, we leverage the limited-scale QA-grounded corpus and augment it with curated video temporal grounding dataset, using tailored curation pipeline to align the original annotations with our QAcentric paradigm. Besides, to maintain video understanding as the primary optimization objective, we anchor frame-level rewards exclusively to final answer correctness, enforcing that intermediate operations must genuinely contribute to the core task. The contributions of this work are three-fold. First, we introduce Video-MTR, reinforced multi-turn reasoning framework designed for long-form video understanding, enabling iterative video segment selection and question comprehension. To the best of our knowledge, this is the first attempt to incorporate multi-turn reasoning in the context of long video understanding. Second, we propose novel gated bi-level reward mechanism, which includes trajectorylevel rewards based on answer correctness and turn-level rewards focused on frame-query relevance, facilitating more effective and informed video segment selection and improving reasoning accuracy. Finally, we conduct extensive experiments on several video understanding benchmarks, including VideoMME (Fu et al. 2025), MLVU (Zhou et al. 2024), and EgoSchema (Mangalam, Akshulakov, and Malik 2023), demonstrating the effectiveness and robustness of Video-MTR. Codes, trained models, and dataset will be released for further research. Related works MLLMs for Video Understanding Building on image MLLMs visual reasoning capabilities, researchers develop temporal extensions for video understanding. However, long-form videos remain challenging due to their extended duration exceeding contemporary MLLMs context windows. Approaches like VideoLLaVA (Lin et al. 2023), ShareGPT4Video (Chen et al. 2024a), InternVideo2 (Wang et al. 2024b) and VideoR1(Feng et al. 2025) still resort to uniformly sampling the entire video and rely on post-training with large-scale video-instruction data to boost reasoning abilities. Yet the inevitable loss of information at the input stage creates performance ceiling. Other approaches explicitly address this bottleneck. One category of methods, exemplified by LongVA (Zhang et al. 2024), LLaMA-VID (Li, Wang, and Jia 2024), Kangaroo (Liu et al. 2024) and VideoXL (Shu et al. 2025), employs token compression techniques to extend context windows, enabling direct processing of hour-long videos. However, this approach floods the model with redundant information and sacrifices interpretability. Another category, like VideoAgent (Wang et al. 2024a), VideoMemAgent (Fan et al. 2024) and DrVideo (Ma et al. 2025) adopt agent mechanisms (Li et al. 2023; Wu et al. 2023) that dynamically integrate external tools, including video captioning, video object tracking, and key-frame search, through single-turn or multi-turn iterations. Despite outperforming uniform sampling baselines, these systems exhibit high complexity from heterogeneous external components and suboptimal tool utilization due to the absence of end-to-end training. MLLMs with Reinforcement Learning Recent studies (Shen et al. 2025; Meng et al. 2025), inspired by advances in the text domain, have explored reinforcement learning (RL) to improve the reasoning abilities of MLLMs. VLM-R1 (Shen et al. 2025) extends the DeepSeekR1 paradigm (Guo et al. 2025), showing that an RL-trained MLLM can outperform supervised fine-tuning baseline and generalize better on visual tasks. DeepEyes (Zheng et al. 2025) incentivizes thinking with images over multiple turns via RL. In the video domain, VideoChat-R1 (Li et al. 2025) enhances spatio-temporal perception through reinforcement fine-tuning (RFT) with GRPO, while VideoR1 (Feng et al. 2025) employs tailored T-GRPO algorithm to emphasize temporal cues. However, these methods primarily target static images or short clips, leaving long-form video understanding largely unaddressed."
        },
        {
            "title": "Methods",
            "content": "Overview We propose Video-MTR, framework that reconceptualizes long-form video understanding as multi-turn interactive reasoning task, closely aligned with the way humans process complex visual information. When presented with video and question, humans typically begin by forming holistic understanding of the overall content, then iteratively attend to specific segments to gather more informative details, and finally integrate the accumulated evidence to derive an answer. To instantiate this reasoning paradigm, we formulate the task as reinforcement learning problem. In this formulation, the video functions as dynamic environment that updates the set of observed frames in response to retrieval actions. An MLLM serves as the decision-making agent, interacting with the environment through learned policy πθ. As illustrated in Figure 1, the agent operates in multi-turn manner, and at each step it samples an action ak πθ(sk) to either retrieve additional frames or produce the final answer. The state sk is multimodal context that concatenates (i) the last interactions and (ii) the currently observed frames, providing both temporal history and updated visual evidence, and can be represented as sk = (Fkw, xkw, ykw, . . . , Fk1, xk1, yk1, Fk, xk) where is the text instruction, is the set of observed frames, is the generated response that consists of reasoning process and executable action a. The environment is initialized by uniformly sampling n0 frames to form F0 from the whole video. Thereafter, the environment responds to each retrieval action with new set of frames that become the observation for the next turn. The agent may execute multiple retrieval actions until it is either confident enough to answer or the turn limit Kmax is reached. The complete trajectory is recorded as: τ = { (Fk, xk, yk) }K k=0. Figure 1: Overview of the proposed Video-MTR framework. Left: The lower part shows the multi-turn interaction loop between the MLLM agent and the video environment, while the upper part visualizes the collected trajectory and the gated bi-level reward shaping process during optimization. Right: Detailed logs of the agents interaction steps across turns. The agent iteratively improves its reasoning by retrieving increasingly relevant frames, ultimately leading to correct conclusion. where indexes the turns starting from the initial turn = 0, and denotes the terminal turn, with 0 Kmax. The complete rollout process is outlined in Algorithm 1. Algorithm 1: Rollout of Multi-turn Reasoning Trajectory Input: Long video , Policy MLLM πθ, Input question x0, Input frame set F0 , Maximum turn Kmax, turn counter Output: Final trajectory τ Initialize: 0, rollout trajectory τ (F0, x0) Generate response yk πθ( sk) τ τ + yk reasonk, ak arse(yk) if ak matches \"Retrieval\" format then Extract (tstart, tend) from ak Fk+1 RETRIEVEFRAMES(V, tstart, tend) xk+1 x0 τ τ + (Fk+1, xk+1) question remains unchanged else if ak matches \"Answer\" format then Get final answer 1: while < Kmax do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: break else xk Invalid action. Let me rethink. Regenerate response for invalid action τ τ + (xk) end if + 1 14: 15: 16: 17: end while 18: Collect final trajectory τ While prior studies have applied reinforcement learning to MLLMs for temporal reasoning tasks, they predominantly adopt single-turn reasoning settings. However, standard RL frameworks for MLLMs struggle with multi-turn optimization due to uniform credit assignment of sparse terminal rewards across turns. This hinders the learning of nuanced intermediate behaviors that are critical to final success. Furthermore, optimizing solely based on final-task accuracy generally demands extensive training data because terminal supervision is sparse. To address these challenges in multi-turn reasoning, we introduce gated bi-level reward mechanism that augments conventional trajectory-level rewards with turn-level rewards. These turn-level rewards capture framequery relevance, explicitly separating intermediate from final reasoning stages. As most video question answering datasets provide only QA annotations, we increase data diversity by incorporating video temporal grounding dataset and curating it to our QA-centric setup. Additionally, observing limited proactive frame retrieval in pretrained MLLMs, we adopt dynamic exploration-bootstrapping strategy to encourage multi-turn evidence seeking. Together, this design improves reasoning performance and reduces data requirements by providing more informative and discriminative reward signals. Gated Bi-Level Reward This section details our fine-grained reward design for RL training. We first describe the computation of the basic bilevel reward. We then present goal-gated mechanism that prioritizes trajectory-level signals over turn-level signals to align intermediate decisions with final goal, fostering coherent, goal-oriented multi-turn reasoning. Bi-level Reward This bi-level architecture comprises two complementary components: trajectory-level reward Racc providing global supervision, and intermediate turn-level rewards to deliver localized feedback within individual turns. Racc is determined by final answer correctness: Racc = (cid:40)1 correct final answer otherwise, Rf quantifies turn-level frame retrieval quality. At each intermediate turn k, we measure the relevance of the selected frames Fk to the QA pair. The relevance is quantified as IoU with the ground-truth relevant frames G. turn reward is given only when this relevance improves over the initial turn: Rk fms = (cid:40)0.5 if IoU(cid:0)Fk, G(cid:1) > IoU(cid:0)F0, G(cid:1), otherwise, Goal-Gated Reward Shaping To ensure intermediate actions serve the core goal of video understanding, we further introduce goal-gated reward shaping mechanism. Frameretrieval rewards are granted conditionally based on final answer accuracy, only frame operations contributing to correct answers are essential. This design couples retrieval and answering in the policy instead of optimizing each in isolation. Moreover, in multi-turn rollouts the agent may issue multiple intermediate retrievals yet answer only once. Without constraints, the model could exploit repeated frame rewards for reward hacking while neglecting the primary goal, video understanding accuracy. We therefore enforce single-intermediate-reward constraint, only the retrieval turn with the highest relevance reward retains with its frame reward and format reward, while all other turn-level rewards are zeroed. This ensures cumulative trajectory rewards align with the terminal answer rewards one-time nature, permitting exactly one frame retrieval reward and one answer reward per trajectory. R(τ ) = Racc + 1{Racc>0} max k=0 (Rk fms + Rk format) We aggregate the refined rewards into final rewarded trajectories, which then serve as training data for policy optimization."
        },
        {
            "title": "Reinforcement Learning",
            "content": "The standard RL objective function of the trajectory is defined as follows: maxEτ πθ (R(τ )) We train the policy with Proximal Policy Optimization (PPO) and extend its default formulation to accommodate multi-turn reasoning. The multi-turn interactions trajectory would be treated as an entire token sequence = (s0, s1, ..., sT ). Instead of relying solely on sparse finalstep feedback, the bi-level rewards are applied at every turn boundary and then propagated across all tokens st, enabling effective end-to-end learning. Specifically, two discount factors jointly shape the rewards during the calculation of token-level advantages AGAE : γturn: cross-turn discount factor for the accuracy reward Racc, decaying the final answer signal back to earlier retrieval turns. The reward assigned at the boundary of turn is : γKk turn Racc + Rk fms where denotes the final (answer) turn. γtoken: within-turn discount factor that propagates the turn boundary reward to tokens within the same turn. PPO optimizes the policy model πθ by maximizing the following objective: rt(θ) = πθ (st s<t) πold (st s<t) JP O(θ) = (cid:88) min(rt(θ)AGAE , clip(rt(θ)AGAE ) The optimization poses two core challenges: (1) precisely estimating the intermediate frame retrieval rewards; and (2) shifting model originally biased toward single-turn reasoning into multi-turn paradigm. We address these challenges with two strategies: high-quality data-curation pipeline that delivers fine-grained temporal supervision, and an exploration bootstrapping mechanism that incentivizes multiturn retrieval behavior during early training. Data Curation Computing turn-level frame-retrieval rewards requires temporally grounded annotations aligned with the problem. However, most video-understanding datasets lack such labels tied to the original QA pairs. notable exception is NExT-GQA (Xiao et al. 2024), which provides 10.5K explicit grounding annotations. We retain instances with relevant-segment ratio below 0.5 to enforce tighter temporal grounding, yielding roughly 5K high-quality samples. To scale and diversify training data, we additionally leverage video temporal grounding (VTG) datasets such as QVHighlights (Lei, Berg, and Bansal 2021), which provide precise temporal annotations for queryrelevant segments. To adapt this dataset to our QA-centric training, we employ GPT-4o (Hurst et al. 2024) to convert each query into corresponding QA pair, while preserving alignment with the original temporal annotations. To ensure sample quality, we apply filtering pipeline: (1) the LLM first judges whether query is suitable for QA conversion by checking for sufficient discriminative detail, overly short or generic queries are discarded; (2) we also keep only instances with relevant-segment ratio below 0.5. This process yields nearly 3K QA-grounded high-quality samples from QVHighlights. In total, we curate compact yet supervision-rich dataset of 8K temporally grounded training examples. Departing from conventional paradigms that rely on large-scale data collection, our approach prioritizes reward signal fidelity over data volume. By delivering more discriminative and temporally precise feedback, we enable highly efficient RL training that achieves competitive performance with significantly less data. We further validate this design in the experiments section, where we compare its efficiency and effectiveness against alternative approaches that rely on largerscale data. Exploration Bootstrapping During early rollouts, we observe that the pretrained MLLM rarely initiates evidenceseeking behavior. Unlike prior work, we omit supervised instruction tuning and introduce an adaptive exploration bonus for multi-turns exploration: within each mini-batch, if the agents frames retrieval rate falls below preset threshold, every retrieval action receives small positive reward, regardless of frame relevance. Once the model routinely triggers retrievals, the bonus is automatically disabled. Such dynamic reward shaping bootstraps exploration, enabling pure RL to cultivate multi-turn evidence-seeking behavior."
        },
        {
            "title": "Experiments",
            "content": "Implementation Details Video-MTR is built upon the Qwen2.5-VL-7B and trained using the VAGEN framework, which supports multi-turn reinforcement learning. We set turn limit of three and retain the last two turns as context. The initial turn uniformly samples 16 frames from the full video, and each subsequent turn retrieves up to eight additional frames, capping the total at 32 frames per trajectory. The policy is optimized with PPO using batch size of 32, actor learning rate of 1e-6, and critic learning rate of 1e-5. Experiments are conducted on single server equipped with eight NVIDIA A800-80GB GPUs. Benchmarks We select several representative long-form video benchmarks for comprehensive evaluation: VideoMME(Fu et al. 2025) It is an extensive benchmark that covers videos of diverse genres and lengths, encompassing Short( 2mins), Medium (4 - 15mins), Long (30 - 60mins) categories. It includes 2,700 manually annotated QA pairs from 900 videos. MLVU(Zhou et al. 2024) challenging benchmark built on long videos (3 mins to 2 hours, avg. 15mins), significantly exceeding most datasets in duration. Its test set is challenging, offering 11 distinct tasks and extended multiple-choices of six options.Even the best performing model GPT-4o barely scores 54.9% on M-AVG. EgoSchema(Mangalam, Akshulakov, and Malik 2023) An egocentric video benchmark covers wide range of human activities. Each video lasts 3 minutes. It comprises only test set, including subset of 500 multiple-choice questions with publicly available labels. We report results on this test subset. Performance of Long-form Video Understanding Main Results To ensure objectivity and standardizes assessment, we employ objective questions as the evaluation tasks. Results are reported as average multi-choice accuracy (M-AVG). The main results are reported in Table 1. We benchmark Video-MTR against two groups of SOTA: proprietary models, including agents built on them, and opensource models. Across all tasks, Video-MTR consistently delivers competitive or superior performance. We additionally report model size and the number of input frames to enable fair comparisions and comparative analysis. Overall, scaling model parameters or increasing input frames proves effective for performance gains. Under matched configurations (identical parameters/frames), Video-MTR demonstrates clear advantages over comparable methods on three benchmarks. Notably, MLVU-Test emerges as highly challenging benchmark, even proprietary models like GPT-4o attain merely 54.9% accuracy when processing 300+ frames (0.5 fps). Our model achieves 48.4% accuracy with just 32 frames, ranking second only to GPT-4o and highlighting the critical importance of strategic frame selection. On VideoMME, our model also performs well, particularly excelling on its Long subset and outperforming all other opensource models. EgoSchema is distinct dataset entirely composed of egocentric viewpoints. Our model achieves high accuracy of 62.4% here, second only to GPT-4o and Gemini-1.5-Pro. Notably, this strong performance was achieved without incorporating additional ego-centric data during training, demonstrating effective generalization. By contrast, Video-R1, which shares the same Qwen2.5-VL-7B base model and undergoes additional post-training, drops significantly to 48.8%, even underperforming its original base model (59.4%). Video-R1s training recipe applies large-scale supervised fine-tuning in the first stage, followed by brief RL phase. These results show that our multiturn scheme, combined with RL on small amount of highquality data, fosters superior generalization capabilities. Beyond accuracy, we compare training paradigms and data requirements across approaches in Table 2. Proprietary systems (e.g., GPT-4o, Gemini-1.5-Pro) are trained on massive multimodal corpora, whereas open-source counterparts rely on hundreds of thousands to millions of supervised multimodal pairs. Conversely, Video-MTR uses 7B backbone and is post-trained via single RL stage on only 8K supervision-rich examples. Despite this substantial reduction in data scale, our model matches or surpasses methods trained on massive datasets on mainstream longvideo benchmarks. These results underscore the effectiveness of pairing high-quality, signal-dense supervision with reinforcement learning, while using only fraction of the training data. Case Study Figure 2 illustrates Video-MTRs multi-turn reasoning on 54-minute video for single-detail query hinging on critical plot point. In Turn 1, frames are uniformly sampled across the entire video. Noting that key evidence is missing, Video-MTR autonomously retrieves densely sampled segments semantically aligned with the query. In Turn 2, it re-examines the refined, query-relevant frames, extracts the required detail, and outputs the correct answer. This case shows how iterative retrieval and focused inspection overcome the limitations of uniform sampling in long videos. Ablation Study We further explore the contributions of the essential components of Video-MTR through comprehensive ablation studies on VideoMME and MLVU. Analysis of the Multi-turn Reasoning We analyze the advantages of the proposed multi-turn reasoning framework over the conventional single-turn paradigm. Since VideoMTR is built on Qwen2.5-VL-7B, we compare directly against this base model to isolate performance gains. As multi-turn reasoning is expected to be particularly beneficial for complex tasks, we empirically assess its impact across diverse task types and video durations. (1)Task types. Using the MLVU benchmark, which categorizes evaluation tasks into three types: holistic tasks (global understanding of Model"
        },
        {
            "title": "Frames",
            "content": "VideoMME (w/o sub.) MLVU EgoSchema"
        },
        {
            "title": "Subset",
            "content": "Proprietary Models GPT-4o (Hurst et al. 2024) Gemini-1.5-Pro (Team et al. 2024) VideoAgent(GPT-4) (Wang et al. 2024a) - VideoMemAgent(GPT-4) (Fan et al. 2024) - - - - - 0.5 fps / 384 0.5 fps 87 72 Open-source Models Video-LLaVa (Lin et al. 2023) VideoChat2 (Li et al. 2024) VideoLLaMA2 Qwen2.5-VL-7B (Bai et al. 2025) Video-CCAM (Fei et al. 2024) Video-R1 (Feng et al. 2025) LongVA (Zhang et al. 2024) Video-XL (Shu et al. 2025) Video-MTR(Ours) 7B 7B 87B 7B 9B 7B 7B 7B 7B 8 16 16 32 96 32 128 / 256 128 / 256 32 - - 49.0 52.7 - 33.2 42.1 44.7 39.6 50.2 46.2 49.2 51. 71.9 75.0 56.0 57.4 39.9 39.5 46.0 53.6 50.3 59.3 52.6 55.5 59.0 54.9 - - - 30.7 30.1 45.6 41.6 42.9 45.4 41.1 45.6 48. 72.2 71.1 60.2 - 36.8 - - 59.4 - 48.8 - - 62.4 Table 1: Performance on mainstream long-video benchmarks. VideoMME results are under the no-subtitles (w/o sub.) condition. Best and second-best per category (proprietary / open-source) are bolded and underlined, respectively. Method Paradigm Modalities Volume Video-CCAM VideoChat2 LongVA Video-XL Video-R1 SFT SFT SFT SFT SFT+RL (S) img/vid-text img/vid-text img-text img/vid-text img/vid-text Video-MTR(Ours) RL(M) vid-text 4.4M 2M 1.3M 257K 260K 8K Table 2: Comparison of training paradigms, data modalities and volumes. (M)/(S) denote multi-turn and single-turn respectively. All SFT implementations are instruction tuning. For strictly fair comparison, we only compare the data used during the fine-tuning stage. Model VideoMME (w/o sub.) Short Medium Long Qwen2.5-VL-7B Video-MTR(Ours) 65.8 70.4+4.6 50.3 55.6+5.3 44.7 51.0+6. Table 3: Performance comparisons across video durations. the entire video), single-detail tasks (focusing on one critical plot), and multi-detail tasks (requiring reasoning over multiple events), we observe distinct trends in Figure 3. For holistic tasks, typically lower in complexity, the base model achieves up to 72% accuracy, with Video-MTR providing modest improvement of +3.8%. In contrast, detailoriented tasks are substantially harder. The base model remains below 40% accuracy, while Video-MTR yields larger gains: +7.5% on single-detail and +8.1% on multi-detail. These results suggest near-linear relationship between task complexity and the benefits of multi-turn reasoning. (2)Video durations. We further examine the impact of duration on VideoMME. We also observe positive correlation between video length and performance gains. As shown in Table 3, Video-MTR achieves accuracy improvements of +4.6% (Short), +5.3% (Medium), and +6.3% (Long) over Qwen2.5-VL-7B. Across both task types and durations, these results validate the effectiveness of iterative reasoning in complex scenarios: gains increase with task complexity and video length, providing the greatest benefits on long, complex videos. To ensure fair comparison, we further post-train Qwen2.5-VL-7B on the same data as Video-MTR. This yields our single-turn baseline, which processes the same number of uniformly sampled frames in single forward pass. Compared with Video-MTR, it uses the same accuracy-based reward but removes multi-turn instructions from the prompts. Both models use identical optimization hyperparameters. Results for the single-turn baseline are reported in the third row of Table 4. While this singleturn variant yields modest improvements over Qwen2.5-VL7B, it falls short when compared to Video-MTR, particularly on complex tasks in MLVU and long-form videos in VideoMME, consistent with our earlier analysis. This performance gap highlights the effectiveness of the multi-turn reasoning paradigm for complex inference. Effectiveness of Bi-level Reward We evaluate the bi-level reward design against multi-turn variant that omits this component, which removes turn-level supervision and relies solely on the final accuracy reward to guide the multi-turn behavior. As shown in Table 4, even with identical prompts and preserved multi-turn behavior, accuracy declines across benchmarks (including significant 4% drop on MLVU). These findings highlight that, without precise intermediate supervision, relying solely on final accuracy reward is insufficient to guide the model toward effective temporal localization, thereby limiting its reasoning capability. Figure 2: Illustration of Video-MTRs Multi-turn Reasoning Process, visualizing sampled frames, reasoning process, and model actions per turn. The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video, reflecting the models frame selection strategy at each reasoning turn. Figure 4: Reward hacking example. The red curve shows rising total reward (left) while QA accuracy (right) declines. By contrast, the green curve shows consistent increase in both total reward and QA accuracy. cial for preventing superficial reward exploitation and preserving genuine video understanding capability."
        },
        {
            "title": "Conclusion",
            "content": "We present Video-MTR, reinforced multi-turn reasoning framework for long-form video understanding. To the best of our knowledge, it is the first work to integrate end-toend reinforcement learning with explicit multi-turn reasoning in this domain. At the core of the framework is gated bi-level reward mechanism, designed to incentivize both relevant frame retrieval and step-by-step reasoning. Extensive experiments on VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR achieves strong and robust performance across diverse task types and varying temporal lengths. Notably, the framework exhibits excellent temporal scalability, yielding higher gains as video duration increases, highlighting its particular advantage in extra-long video understanding. Future work includes extending the framework to even longer videos and more complex reasoning tasks, pushing the boundaries of long-video understanding. Figure 3: Task Diagnose on MLVU. Ablation Setting VideoMME (w/o sub.) MLVU Ours Ours Multi-turn w/o Bi-Level Reward Ours Single-turn 70.4 68.4 68.8 55.6 55.2 54.8 51.0 49.4 47.9 59.0 57.8 57.2 Short Medium Long Overall Test 48.4 44.4 44.3 Table 4: Ablation study. The first variant keeps the multiturn paradigm but removes the bi-level reward. The second variant switches to single-turn paradigm. Necessity of Goal-Gated Reward Shaping To assess the effectiveness of our goal-gated reward shaping in mitigating reward hacking, we compare Video-MTR with an ablated variant that removes this mechanism and instead receives unconditioned turn-level rewards. Figure 4 shows the resulting failure mode: during training, the ablated models total reward rises while QA accuracy declines, indicating that the agent inflates reward by repeatedly retrieving relevant frames rather than answering correctly. By contrast, the goal-gated model keeps reward and task success closely aligned. These results confirm that goal-gated shaping is cruReferences Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Chen, L.; Wei, X.; Li, J.; Dong, X.; Zhang, P.; Zang, Y.; Chen, Z.; Duan, H.; Tang, Z.; Yuan, L.; et al. 2024a. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37: 1947219495. Chen, Y.; Xue, F.; Li, D.; Hu, Q.; Zhu, L.; Li, X.; Fang, Y.; Tang, H.; Yang, S.; Liu, Z.; et al. 2024b. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188. Dai, W.; Li, J.; Li, D.; Tiong, A.; Zhao, J.; Wang, W.; Instructblip: ToLi, B.; Fung, P. N.; and Hoi, S. 2023. wards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36: 4925049267. Fan, Y.; Ma, X.; Wu, R.; Du, Y.; Li, J.; Gao, Z.; and Li, Q. 2024. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, 7592. Springer. Fei, J.; Li, D.; Deng, Z.; Wang, Z.; Liu, G.; and Wang, H. 2024. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023. Feng, K.; Gong, K.; Li, B.; Guo, Z.; Wang, Y.; Peng, T.; Wu, J.; Zhang, X.; Wang, B.; and Yue, X. 2025. Videor1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Fu, C.; Dai, Y.; Luo, Y.; Li, L.; Ren, S.; Zhang, R.; Wang, Z.; Zhou, C.; Shen, Y.; Zhang, M.; et al. 2025. Videomme: The first-ever comprehensive evaluation benchmark In Proceedings of of multi-modal llms in video analysis. the Computer Vision and Pattern Recognition Conference, 2410824118. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Lei, J.; Berg, T. L.; and Bansal, M. 2021. QVHIGHLIGHTS: detecting moments and highlights in videos via natural language queries. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS ISBN 21. Red Hook, NY, USA: Curran Associates Inc. 9781713845393. Li, G.; Hammoud, H.; Itani, H.; Khizbullin, D.; and Ghanem, B. 2023. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36: 5199152008. Li, K.; Wang, Y.; He, Y.; Li, Y.; Wang, Y.; Liu, Y.; Wang, Z.; Xu, J.; Chen, G.; Luo, P.; et al. 2024. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2219522206. Li, X.; Yan, Z.; Meng, D.; Dong, L.; Zeng, X.; He, Y.; Wang, Y.; Qiao, Y.; Wang, Y.; and Wang, L. 2025. Videochatr1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958. Li, Y.; Wang, C.; and Jia, J. 2024. Llama-vid: An image In European is worth 2 tokens in large language models. Conference on Computer Vision, 323340. Springer. Lin, B.; Ye, Y.; Zhu, B.; Cui, J.; Ning, M.; Jin, P.; and Yuan, L. 2023. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Liu, J.; Wang, Y.; Ma, H.; Wu, X.; Ma, X.; Wei, X.; Jiao, J.; Wu, E.; and Hu, J. 2024. Kangaroo: powerful videolanguage model supporting long-context video input. arXiv preprint arXiv:2408.15542. Ma, Z.; Gou, C.; Shi, H.; Sun, B.; Li, S.; Rezatofighi, H.; and Cai, J. 2025. Drvideo: Document retrieval based long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1893618946. Mangalam, K.; Akshulakov, R.; and Malik, J. 2023. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36: 4621246244. Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Han, T.; Shi, B.; Wang, W.; He, J.; et al. 2025. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PmLR. Shen, H.; Liu, P.; Li, J.; Fang, C.; Ma, Y.; Liao, J.; Shen, Q.; Zhang, Z.; Zhao, K.; Zhang, Q.; et al. 2025. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615. Shu, Y.; Liu, Z.; Zhang, P.; Qin, M.; Zhou, J.; Liang, Z.; Huang, T.; and Zhao, B. 2025. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2616026169. Team, G.; Georgiev, P.; Lei, V. I.; Burnell, R.; Bai, L.; Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.; et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Wang, X.; Zhang, Y.; Zohar, O.; and Yeung-Levy, S. 2024a. Videoagent: Long-form video understanding with large lanIn European Conference on Comguage model as agent. puter Vision, 5876. Springer. Wang, Y.; Li, K.; Li, X.; Yu, J.; He, Y.; Chen, G.; Pei, B.; Zheng, R.; Wang, Z.; Shi, Y.; et al. 2024b. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, 396416. Springer. Weng, Y.; Han, M.; He, H.; Chang, X.; and Zhuang, B. 2024. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, 453470. Springer. Wu, P.; and Xie, S. 2024. V*: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1308413094. Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Zhang, S.; Zhu, E.; Li, B.; Jiang, L.; Zhang, X.; and Wang, C. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 3(4). Xiao, J.; Yao, A.; Li, Y.; and Chua, T.-S. 2024. Can trust your answer? visually grounded video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1320413214. Zhang, P.; Zhang, K.; Li, B.; Zeng, G.; Yang, J.; Zhang, Y.; Wang, Z.; Tan, H.; Li, C.; and Liu, Z. 2024. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852. Zhao, Y.; Misra, I.; Krahenbuhl, P.; and Girdhar, R. 2023. Learning video representations from large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 65866597. Zheng, Z.; Yang, M.; Hong, J.; Zhao, C.; Xu, G.; Yang, L.; Shen, C.; and Yu, X. 2025. DeepEyes: Incentivizing Thinking with Images via Reinforcement Learning. arXiv preprint arXiv:2505.14362. Zhou, J.; Shu, Y.; Zhao, B.; Wu, B.; Xiao, S.; Yang, X.; Xiong, Y.; Zhang, B.; Huang, T.; and Liu, Z. 2024. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv e-prints, arXiv2406."
        },
        {
            "title": "Training Details",
            "content": "Prompt Design This section details our prompt design and provides an illustrative example in Figure 6. To incentivize multi-turn reasoning, we craft an instruction template that guides the MLLM to follow predefined interaction protocol. The prompt is multimodal: visual tokens corresponding to frames observed in the current turn are inserted immediately after their textual description. We then append format template that constrains the models output to structured schema. We define two actions per turn: (i) answer, which outputs only the single option letter; and (ii) retrieve, which outputs start frame and end frame. In each turn, the model is explicitly required to first provide brief rationale and then emit the action in the specified format. Frame Retrieval Protocol We next describe the frame-retrieval format and implementation. At preprocessing, we uniformly subsample up to frames from each video to form candidate pool Fall and index them accordingly; in our implementation we set = 64 , which worked well empirically. In the initial turn, the agent receives sparse overview of 16 uniformly spaced frames. In subsequent turns, the agent may issue retrieval action that selects temporal interval by outputting start frame and end frame (Fall). The environment then returns frames from this interval at an appropriate stride, capped at most 8 frames. This procedure allows the model to iteratively focus on key segments by selecting targeted subsets of frames. Implementation of Exploration Bootstrapping To address the lack of proactive evidence seeking in early training, we introduce an adaptive exploration bonus that bootstraps multi-turn retrieval. We compute statistics at the mini-batch level (batch size = 32) and use two-stage schedule. For each mini-batch, if the retrieval rate (fraction of turns issuing retrieve action) falls below stagespecific threshold, we add fixed bonus to every retrieval action in that batch, irrespective of frame relevance. Stage (cold start): threshold = 0.1, bonus = +1.0. Stage II (bootstrapping): threshold = 0.5, bonus = +0.5. Once the retrieval rate remains above the Stage-II threshold for several consecutive mini-batches, the bonus is disabled. As shown in Figure 5, this dynamic shaping reliably kick-starts and sustains multi-turn evidence-seeking behavior under pure RL. Datasets This section details the construction and statistics of our temporally grounded supervision dataset for reinforcement learning (RL) training. The dataset comprises two components: one curated from video-understanding dataset NExT-GQA (Xiao et al. 2024) and one adapted from video temporal grounding dataset QVHighlights (Lei, Berg, and Bansal 2021): Figure 5: Exploration bootstrapping enables multi-turn behavior. With the bonus (pink), rewards grow as multi-turn retrieval is maintained; without it (gray), the policy stabilizes at single-turn reasoning. Source Pre Filter QA Converted Post Filter NExT-GQA QVHighlights 8.9K 7.2K - 3.5K 4.9K 3.0K Table 5: Dataset composition and filtering statistics. Counts denote thousands of samples. NExT-GQA is directly used as QA pairs. NExT-GQA Starting from 10.5K explicit temporal grounding annotations (consolidated into 8.9K QA pairs), we retain instances with relevant-segment ratio < 0.5 and video duration > 30s, yielding 5K highquality samples. QVHighlights We use GPT-4o (Hurst et al. 2024) to convert each original query into QA pair aligned with its temporal annotations, and apply two-stage quality filter: (i) discriminative-adequacy screening; and (ii) relevant-segment ratio < 0.5 and video duration > 30s, resulting in 3K QA-grounded samples. In total, we obtain 8K training instances that are compact yet supervision-dense. Table 5 reports per-source composition and retained counts at each step to facilitate reproduction and extension. Figure 7 illustrates the GPT-4o prompt design for rewriting and provides before/after examples. Case Studies We present additional case studies drawn from three evaluation benchmarksVideoMME (Fu et al. 2025), MLVU (Zhou et al. 2024), and EgoSchema (Mangalam, Akshulakov, and Malik 2023) to give comprehensive picture of Video-MTRs multi-round reasoning process; these examples include both successes and failures. frame idx:0, <image 0>,frame idx:4, <image 4>,frame idx:8, <image 8>, System: conversation between User and Assistant. The user asks question, and the Assistant solves it. You are an assistant in analyzing videos. Your will be given video and question. Goal: Answer the question correctly with no more than 3 turns. User: Turn 1. Now you are given 16 selected frames from the video, with frame idx list: [ 0 4 8 12 16 21 25 29 33 37 42 46 50 54 58 63]. Frames: <image 54>,frame idx:58, <image 58>,frame idx:63, <image 63> Answer the following problem based on the frames: What is the event in the video? Options: [A. Womens synchronized diving, B. Individual diving, C. Swimming D. Mens synchronized diving, E. Swimming relay race, F. Synchronized swimming] You can choose to retrieve more frames or provide your answer. Format Template: <think>...</think><answer>...</answer> or <think>...</think><retrive>...</retrive> Please think about this question as if you were human pondering deeply. Engage in an internal dialogue using expressions such as let me think, wait, Hmm, oh, see, lets break it down, etc, or other natural language thought expressions. Its encouraged to include self-reflection or verification in the reasoning process.Provide your detailed reasoning between the <think> and </think> tags. If you have enough information, Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags. If you lack some information, think about the most relevant frame index range of the information you need, then you can retrieve dense frames in the range by sending retrive request by <retrive> start frame, end frame </retrive>. IMPORTANT: start frame and end frame must be integers smaller than 63. frame idx:54, ..., Video-MTR: <think>I need to identify the event and verify if it is mens synchronized diving. Lets look for any indication that this is synchronized diving event and verify if it is mens synchronized diving.</think><retrive>12,33</retrive> User: Turn 2. Now you are given 8 selected frames from the video, with frame idx list: [12 15 18 21 24 27 30 33]. Frames: frame idx:12, <image 12>,frame idx:15, <image 15>, ..., frame idx:27, <image 27>,frame idx:30, <image 30>,frame idx:33, <image 33>, Answer the following problem based on the frames: What is the event in the video? ... If you lack some information, think about the most relevant frame index range of the information you need, then you can retrieve dense frames in the range by sending retrive request by <retrive> start frame, end frame </retrive>. Video-MTR: <think>I need to verify if this is synchronized diving event and check if it is mens synchronized diving. The frames show two divers performing synchronized dive, which indicates synchronized diving event. </think><answer>D</answer> Figure 6: An example of Video-MTR with interactions of 2 turns. <image n> will be replaced with the visual tokens of the specific frames."
        },
        {
            "title": "Successful Cases",
            "content": "From each dataset we randomly selected one correctly solved example. As illustrated in Figure 8, all three examples exhibit consistent evidence-seeking pattern with the following characteristics: (i) an initial global pass over the video produces tentative hypothesis that roughly answers the question; (ii) the model then proposes targeted temporal segment for closer inspection to obtain discriminative evidence; and (iii) after observing this segment, the model updates or confirms the hypothesis and outputs the final answer. Case (role identification). The query asks for the identities of two people. After the first pass, Video-MTR hypothesizes that the pair may be teacher and student based on coarse contextual cues from the full video. It then narrows attention to their interaction segment for verification. In that focused clip, the person in white shirt is seen giving instructions, and the standing man in black shirt follows the instructions and plays the instrument. This instructional exchange provides role-asymmetric signals: directive speech acts, demonstrative gestures, and actionresponse ordering, yielding temporally grounded, discriminative evidence that confirms the teacherstudent hypothesis. Case (event recognition). The question asks which event is shown, with candidates including individual/synchronized diving, swimming, relay, and synchronized swimming. After global pass, Video-MTR sets verification subgoal: to confirm synchronized divingand proposes discriminative interval for inspection. Focusing on this clip, the model observes two divers executing the same dive with mirrored body alignment, thereby ruling out individual diving and all swimming events. The model confirms the hy- (i) lengthen the dialogue loop to support deeper chains of reasoning that solve multi-stage tasks, and (ii) incorporate hierarchical temporal-to-spatial strategy that begins with coarse video sweeps and adaptively zooms into highresolution frame crops, thereby securing reliable evidence at both event-level and micro-action scales. pothesis and outputs (D) Mens synchronized diving. Case (goal reasoning). The query seeks concise account of Cs objective and decisions. After first pass, Video-MTR hypothesizes that is choosing what to wear and proposes targeted interval for verification. In this segment, looks at various clothes, picks them up, and appears to be deciding what to wear, with no behaviors indicative of folding, packing, ironing, or washing. The model confirms the hypothesis and outputs (C) deciding what clothes to wear. Error Analysis and Limitations We also examine failure cases to diagnose error sources and outline potential remedies. Two representative cases, one involving multi-detail reasoning and the other requiring finegrained perception are illustrated in Figure 9. Case (Action Order). This example falls under the action-order category, multi-detail task requiring inspection of multiple, disjoint segments. In Rounds 12 the sampled frames do not cover all events referenced by the options; nevertheless, the model commits to prediction, exhibiting hallucination under insufficient evidence. More retrieval rounds are needed to reach reliable decision. likely cause is training-distribution bias: in our data, one to three rounds typically suffice to locate relevant frames and answer correctly, which encourages early stopping even when evidence is incomplete. straightforward remedy is to expand the curriculum with more sequences that demand four to six retrieval rounds and span widely separated events, prompting the model to keep searching until each candidate answer has been either supported or ruled out. Case (Fine-grained Procedural Reasoning). This task requires interpreting micro-actions (e.g., dipping or swishing in cup versus mixing on palette) and linking them causally to paint subtlety. Under the current frameprocessing pipeline, which must accommodate long temporal sequences, the spatial resolution is kept relatively coarse; as result, these discriminative cues are likely to appear heavily blurred. To address this limitation, the retrievaland-reasoning loop at the frame-selection level could be augmented with hierarchical temporal-to-spatial reasoning mechanism: once relevant frame segment is identified, the system would crop the corresponding frames and re-analyse high-resolution regions of interest, enabling direct verification of micro-movements before any answer is produced. These failure cases several structural weaknesses that limit the current version of Video-MTR in complex scenarios. Together, these issues indicate that Video-MTR needs deeper temporal search policies, hierarchical zoom-in vision modules to handle multi-event reasoning and fine-grained perception reliably. Future Work Although Video-MTR demonstrates strong reasoning performance on current long-form benchmarks, ample room for improvement remains when tackling more challenging queries and much longer videos. Future work should therefore advance the multi-round framework on two fronts: Goal: Given declarative sentence to serve as query for retrieving relevant video segments, generate multiple choice question. Follow these rules: 1. Suitability Check: Return False if the sentence is too short or Lacks distinctive details for discriminative options. Else, return True and proceed. 2. Question Format: Use one of these interrogatives: Where, How, Why, What, When, Who 3. Options: Derive one correct answer and three incorrect answers from the sentence. 4. Answer: The correct answer to the question. Format { suitable: bool, # True/False question: str, # MCQ text (if suitable) options: list, answer: str # Correct option } Examples - Sentence:A man in white shirt discusses the right to have and carry firearms. - Output:{ suitable: True question: What is the man in white shirt discussing? options: [A. The war happens in Europe., B. The recent massacre in the US., C. The right to have and carry firearms., D. The recent crime in the US.] answer: } - Sentence: Woman holds her shopping bags. - Output:{ suitable: False question: options: answer: } QA Converted Examples - query:Asian chef with dyed pink hair cooks food. - question: What is the Asian chef with dyed pink hair doing? - options: [A. Preparing ingredients, B. Serving customers, C. Cleaning the kitchen, D. Cooking food], - answer : - query: Two people from the same show interview man at his house. - question: Where do two people from the same show interview man? - options: [A. At his house, B. In studio, C. Outside, D. In an office] - answer : Figure 7: The GPT-4o prompt template for converting declarative queries into multiple-choice QA pairs with suitability check, options generation, and converted QA examples. Figure 8: Representative success cases from (A) VideoMME, (B) MLVU, and (C) EgoSchema. The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video. Figure 9: Representative failure cases: (A) action-order reasoning error and (B) fine-grained procedural misrecognition.The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video."
        }
    ],
    "affiliations": [
        "Guangdong University of Technology",
        "StepFun AI",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}