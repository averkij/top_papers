{
    "paper_title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
    "authors": [
        "Xiaoyi Zhang",
        "Zhaoyang Jia",
        "Zongyu Guo",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 2 9 7 0 8 1 . 5 0 5 2 : r Deep Video Discovery : Agentic Search with Tool Use for Long-form Video Understanding Xiaoyi Zhang 1 Zhaoyang Jia2 Zongyu Guo1 Jiahao Li1 Bin Li1 Houqiang Li2 Yan Lu1 1Microsoft Research Asia 2University of Science and Technology of China {xiaoyizhang, zongyuguo, jiahaoli, binli, yanlu}@microsoft.com {jzy_ustc, lihq}@ustc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing rigid workflow, our approach emphasizes the autonomous nature of agents. By providing set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools with appropriate parameters for actions in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code will be released later as an MCP service. Figure 1: Left: Illustration of our Deep Video Discovery agent, which autonomously reasons on user query, iterative use tools to obtain the final answer. Right: Performance comparison on LVBench. Equal contribution. Change Log is provided at the end of the main text. This work was done during the internship at Microsoft Research Asia as an open-source project. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Long-form videos are ubiquitous in everyday life, spanning diverse domains such as movies, meeting recordings, sports games, and variety shows. Accurately comprehending and interpreting content within these extensive videos remains an intrinsically challenging task [8, 26, 31], demanding an ability to simultaneously integrate and reason about intricate spatiotemporal details across broad global contexts. Effective retrieval of relevant information from hour-long or even longer sequences not only necessitates attending to fine-grained local details but also simultaneously interpreting subtle semantic relations distributed throughout extended temporal intervals. Recent advancements in Large Language Models (LLMs) and Large Vision-language Models (VLMs) have notably improved capabilities in video understanding [17, 4, 28] and increased context length handling more than one million tokens [17, 25, 33]. However, even this extended context length remains insufficient for comprehending the information density typically found in long-form videos of hour-long duration. Empirical observations [17] also suggest decline in the models effective instruction-following ability and reasoning clarity as the temporal dimension and information density increase. Concurrently, recent breakthroughs [11, 18] on reasoning capability of LLMs have facilitated advances in agentic systems capable of complex information gathering tasks, such as Deep Research [16, 10, 20] or Deep Search [2, 3]. These agentic approaches demonstrate how decomposing difficult tasks into modular sub-tasks enables iterative reasoning, information searching, and content synthesis. Inspired by these successes, we view the problem of understanding extremely long videos as analogous to multi-step complex search tasks, where the video is segmented into multiple shorter video clips serving as manageable units of information, named as Deep Video Discovery (Fig. 1, left). While existing video agent frameworks [34, 7, 19, 30] incorporate searching processes in their designs, they manually design the search process with their human prior. For instance, both VideoTree [30] and VCA [34] employ tree-based search strategies that navigate from root nodes to leaf nodes. This approach alleviates the context length limitations of LMMs but is inefficient for fine-grained queries since traversing the tree from root to the leaf is costly, which might benefit more from direct retrieving among leaf nodes. Additionally, semantically relevant entities may not exhibit temporal proximity, potentially diminishing the efficiency of backdate mechanism in tree-based search methods. In contrast to existing systems that typically rely on manually defined, rigid workflows, our approach is distinctly designed around an autonomous and flexible agentic search paradigm. Instead of explicitly prescribing task workflows or search behaviors, We develop modular search tools that operate at multiple granularities, including (1) Global Browse, (2) Clip Search, and (3) Frame Inspect. Global Browse enables global summarization and indexing of subjects and global contexts across the entire video. Clip Search implements efficient semantic retrieval of relevant events within segmented clips. Specifically, Frame Inspection empowers the agent to extract fine-grained details directly from pixel-level information in specified temporal range. With provided this search-centric toolkit and multi-granular video database, our agent is inherently capable of autonomous reasoning, dynamic strategy formation, and iterative decision-making to proactively discover and extract crucial evidence. By leveraging the sophisticated reasoning capabilities intrinsic in the latest LLM, our agent does not merely use these tools independently, but adaptively combines their complementary strengths into chain of thoughts and tool uses, effectively addressing diverse temporal-spatial and complex questions for long video. In the end, Deep Video Discovery can autonomously reason, plan, and retrieve pertinent information for video understanding. We conduct comprehensive evaluations on long video benchmarks, demonstrating the efficiency and strong performance of our agent. In particular, on the challenging LVBench, we push forward the state-of-the-art performance by large margin to 74.2% (as shown in Fig. 1, right), further achieving 76.0% with auxiliary transcripts. We also set series of ablation studies that show the effectiveness of our tool design. In addition, we analyze the behavior patterns of different reasoning models in tool use sequences, providing future insight of developing agents for long video understanding tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Long Video Understanding. Long video understanding remains formidable challenge due to the intricate demands of temporal and spatial reasoning over extended durations and the complexity of information retrieval [31, 26]. Recent efforts in VLM for long video understanding primarily 2 Figure 2: Deep Video Discovery consists of two stages: 1) Multi-granular Video Database Construction. We extract video information from different levels to enable comprehensive understanding, efficient retrieval, and preservation of original content. 2) Agentic Search and Answer. The agent iteratively reasons on user query and leverage the tailored toolset to gather information to answer. tackle challenges of limited input frame number by extending the context length of models [25, 6] or minimizing video redundancy to reduce visual token numbers [12, 14, 28]. Approaches such as AdaRETAKE [28] dynamically compress visual token by allocating adaptive compression ratios across time and model layers, thus significantly expanding the effective input frame number. However, token compression inherently introduces uncertainty regarding information loss, and models continue to face difficulties when answering complex queries under elongated context windows. In parallel, given the sparsity of key information about the given query, some works [27, 7, 34, 30, 19, 32] propose to explore the video content by agentic system. But they usually manually guide the agent about the search workflow by their priors [34, 30] or only allow the agent at simplex frame granularity to search [27], which cannot make full use of the reasoning capability of LLMs, resulting in suboptimal search efficiency and lack of holistic, global comprehension of the long video content. Agent and tool use. Recent advancements in large language models (LLMs), particularly their enhanced reasoning and planning capabilities, have significantly accelerated the development of autonomous agents [35, 39, 38]. The ability to leverage external tools [23, 22, 21] further narrows the gap between general-purpose LLMs and real-world applications, enabling LLMs to acquire information, perform planning, and execute actions in complex environments. Our work extends this line of research to long video understanding, contributing to the broader investigation of solving complex video understanding tasks by integrating the advanced reasoning capabilities of LLMs with sophisticated tool use. We introduce suite of search-centric tools that allow LLMs to autonomously gather information at varying levels of granularity. By dynamically composing these tools, the agent can construct multi-step tool-use chains to improve the ability to answer complex queries effectively."
        },
        {
            "title": "3 Deep Video Discovery",
            "content": "Overview. To solve the long-form video understanding problem in an agentic search way, we first build multi-grained structured database from the long-form video. The database then serves for search-centric tools that work at different granularities. Specifically, our Deep Video Discovery agent consists of three main components: the multi-grained video database D, search-centric toolset , and the LLM as the agents orchestrator. Given the user query Q, the agent reasons iteratively to choose an action Ai {ANSWER} with parameters to gather information for the video database or make decision to answer the query by referring to the accumulated information in this process. In the following subsections, we sequentially introduce the multi-grained video database construction and Agentic Search and Answer with Tool Use. 3.1 Multi-granular Video Database Construction Given an ultralong input video , our goal is to transform it into database that can provide efficient fast retrieval and also provide the original pixels of video for detailed information when 3 necessary. Hence, we design it in multiple granularity style which can provide different levels of video information for corresponding search tools. Specifically, we first segment the video into clips as the basic information unit then make the database include global summarized information to cover the whole video, clip-based caption corpus and indexed frames from the clip. Fig. 2 (left) provides an overview. We introduce these components sequentially. Temporal segmentation. We start by uniformly partitioning the input video into temporal sequence of non-overlapping short clips {vi}N . Empirically, we set = 5 seconds to provide an adequate balance between computing cost and semantic and action completeness. Then all the video clips are decoded into frames {fi}N i=1 under 2 frames per second for further process. i=1, where the total segments = len(V ) Multi-granular information extraction. Our multi-granular video information is designed as three levels: global video level, clip level and frame level. Specifically, at the global level we summarize the video content into compact, subject-centric representation. At the clip level, we leverage textual captions to facilitate efficient information retrieval, while at the frame level we preserve original decoded frames indexed according to their corresponding clips, enabling precise reference and detailed analysis when required. To derive the subject-centric global representation while minimizing redundancy in caption generation, we maintain an progressive structured subject registry throughout the clip captioning process. Specifically, given video clip vi and decoded frame fi, we prompt large VLM to generate the captioning ci and evolve registry whenever new subjects appear. The process is denoted as Si, ci = LM (fi, Si1) where S0 is initialized as empty, and at the conclusion of the captioning process, the final subject registry is denoted by = SN . Each subject within the registry is represented by comprehensive set of attributes, including name, physical appearance, identity descriptors, associated actions, and corresponding temporal spans in the video. The obtained caption ci is subsequently embedded into dense semantic vector ei Rd using language embedding model, facilitating fast retrieval in downstream applications. Despite careful design choices, perceptual compression inherent in caption generation inevitably entails some information loss. To mitigate this when necessary, we explicitly retain the decoded frames fi alongside their corresponding textual captions and embeddings. Outcome. The finalized database therefore encapsulates the decoded frames, captions and corresponding embedding triples, thus forming structured database = {S, {fi, ci, ei}N i=1}. This offline construction procedure transforms lengthy raw video into structured set of textually searchable embeddings with associated clips, while simultaneously preserving the complete visual content at pixel resolution. The resulting database becomes the basis for adaptive tool usage, enabling global information browsing, efficient semantic retrieval at the video-clip scale, and comprehensive grounding of generated outputs back to their source frames. 3.2 Agentic Search and Answer with Tool Use With the built multi-granular video database, we design set of search-centric tools that can enable global information understanding, efficient clip retrieval by semantic query, and details exploration on original video content. By equipping reasoning large language model with this toolset, we build our DVD that can address complex user query on long video though autonomous planning and strategical search tool combination, as shown in Fig. 2 (right). We refer to this stage as Agentic Search and Answer with Tool Use (ASA). We introduce this stage through two subsections: Search-centric Tool Preparation and Agent Design. 3.2.1 Search-centric Tool Preparation Leveraging the established video database, we have developed suite of tools designed to efficiently gather information from video data at varying levels of granularity. Specifically, we divide long videos into three distinct hierarchical levels and introduce corresponding specialized tools: (1) Global Browse, (2) Clip Search, and (3) Frame Inspect. Given the significant computational cost associated with processing lengthy videos using VLMs, our tool design carefully balances efficiency and performance. Central to our approach is an agentic search paradigm, wherein the agent decomposes 4 the user query and strategically chains up tools with synthesized parameters, enabling iterative reasoning and information collection to resolve the task. Through the effective integration and coordinated use of these tools, the agent progressively enhances its understanding of user intent and precisely locates relevant information within extensive video content. We introduce the three tools sequentially in the following paragraphs. Tool: Global Browse. The Global Browse tool takes the video database and the original user query as input, and returns global summaries capturing high-level contextual information. We construct two distinct types of global information: subject-centric and event-centric summaries. For subject-centric summarization, we pre-construct it when building the multi-granular video dataset as mentioned in Section 3.1 since it is query-irrelevant. For event-centric summarization, we uniformly sample frames across the entire video and feed these sampled frames into the VLM. We instruct the VLM to describe noteworthy events explicitly related to the original user query. Upon invocation by the agent, the Global Browse tool efficiently retrieves and returns these global representations, providing the agent immediate access to high-level global context information. Tool: Clip Search. Clip Search provides mid-level granularity retrieval capability, enabling fast and efficient exploration of video content via caption embedding. Given query ˆQ synthesized based on the agents current internal reasoning context, this module retrieves ranked list of top-k relevant video clips along with their captions. Specifically, the tool computes the cosine similarity between the embedding of the provided query and the pre-computed embeddings of all video clip captions, returning the clips corresponding to the highest-ranked caption matches. Each retrieved observation contains both the corresponding caption and the time ranges of the associated video clip. To achieve an accurate and detailed understanding, the agent can iteratively invoke this tool, progressively refining temporal constraints or reformulating its queries based on newly acquired contextual knowledge. This iterative chain-of-query approach effectively guides the agent toward precise temporal segments relevant to the original high-level query. Tool: Frame Inspect. Frame Inspect receives temporal range [ts, te] within the video and an sub-query freely defined by the agent as input, returning an open format visual-question-answering (VQA) response. The agent can invoke this tool whenever explicit frame-level details such as subtle attributes, object counting, or fine-grained spatial relationships, are required but not clearly depicted in captions or global summaries. The open-ended query format allows significant freedom for the agent to leverage its reasoning capability, enabling highly adaptable visual inspection. Specifically, the Frame Inspect tool loads raw frames from the requested interval and prompts VLM with these frames and agent-synthesized query. To ensure computational efficiency, we limit processing to maximum of 50 frames, uniformly sampling from frames exceeding this limit. The resulting response thus equips the agent with accurate, visually-grounded evidence essential for detailed reasoning tasks. 3.2.2 Agentic Design To maximally leverage the reasoning and planning capacity intrinsic to modern LLMs, we intentionally abstain from manually instructing explicit seaching workflow or tool utilization patterns. Instead, we enable the agent to reason, plan, and take actions through streamlined iterative observe-reasonact loop, similar to ReAct[35]. For given query, the agent reasons about its current observation state, strategically selects search tools, formulates appropriate parameters for actions, and dynamically refines its internal reasoning in light of the gathered evidence. Within ASA, the LLM acts as sophisticated cognitive driver, taking actions at each iteration based on cumulative knowledge and reasoned evidence, thereby reinforcing its pivotal role in adaptively navigating the discovery process. Specifically, as illustrated in Algorithm 1, given an initial user query Q, predefined action space = {ANSWER}, and maximum allowable step count , our agent performs iterative reasoning to strategically navigate the available actions. The agent leverages an LLM to reason upon the current dialogue history, plan its immediate action, interact with the toolset = {GLOBALBROWSE, CLIPSEARCH, FRAMEINSPECT}, and collect observations Oi. More concretely, at each step t, the agent maintains historical context Hi, reflects to generate reasoning step Ri, selects an action Ai {ANSWER} accompanied by relevant parameters Pi, and receives subsequent observation outcomes Oi from the environment. These components, reasoning, action, and obtained outcomes, are successively appended to the interaction history Hi, enriching the context Table 1: Action space overview of our DVD. The first three actions are from our toolset and the final ANSWER action is designed as stop criterion. Action Parameter GLOBAL BROWSE video database user query CLIP SEARCH FRAME INSPECT video database agent synthesized query ˆQ return top-k captions video database agent synthesized query ˆQ temporal range [ts, te] ANSWER the answer to user query Algorithm 1: Agentic Search and Answer. Input :Initial query Q, max step , LLM , tool set , action space = {ANSWER} Output :Answer to Initialize history H0 {Q, A} for 1 to do Ri M.reason(Hi1) Ai, Pi M.call(Ri, Hi1) where Ai if Ai = ANSWER then break end Oi Ai(Pi) Hi Hi1 {(Ri, Ai, Oi)} if = then Pi M.answer(Hi) end end return ANSWER(Pi) for subsequent iterations of inference. The iterative process terminates either when the agent explicitly selects the ANSWER action, or upon reaching the step limit , at which prompts the agent directly generates final answer prediction. The agent then outputs the final answer to the original user query. By positioning the LLMs sophisticated reasoning at the core of this iterative loop, this approach endows the agent with an inherently autonomous, evidence-guided, and flexible action-taking mechanism. This autonomous and iterative paradigm fosters strategic and context-sensitive inquiry cycle, thereby enabling the agent to effectively leverage the available tools to iteratively decompose the original query into progressively refined sub-queries, updating and improving the query representation as it receives new observations. Through iterative reasoning and interaction cycles, guided by deeper and increasingly comprehensive observations collected from prior tool usage, the agent systematically enhances its understanding and interpretation of the task context, ultimately leading to more accurate and informed answers to the given question."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Benchmarks We assess the long-form video understanding capabilities of Deep Video Discovery using several established long video benchmarks. Our primary evaluation benchmark, LVBench [26], includes 1,549 multiple-choice questions across 103 hour-long videos. It stands as one of the most comprehensive and challenging benchmarks for extreme long-form video understanding. LongVideoBench [31] features 6,678 questions from 3,763 videos, ranging in duration from few seconds to an hour. We emphasize the longest subset with durations in (900s, 3600s] (denoted as the Long subset), comprising 564 questions from 188 videos. Video MME [8] is segmented by video duration; we concentrate on the Long subset without subtitles to isolate long-video comprehension, covering 300 videos of 30 to 60 minutes with 900 questions. Finally, EgoSchema [15] serves as diagnostic benchmark for long-video understanding, where we evaluate on its validation split of 500 videos with 500 questions. 4.2 Implementation Details Baselines. We compare Deep Video Discovery with range of long-video understanding systems, including both VLM-based [24, 1, 18, 9, 36, 29, 37, 4, 13, 28] and agent-based approaches [30, 7, 34, 19]. Most baseline results are taken from official leaderboards or published reports, except for the recently released OpenAI o3 [18], which has not yet been evaluated on these benchmarks. Following [19], we uniformly sample 256 frames per video to evaluate OpenAI o3. Deep Video Discovery flexibly integrates different models depending on the needs of each component. For the VLM in video database construction, we use GPT-4.1 [17] to produce high-quality captions on LVBench, and GPT-4.1-mini for other benchmarks to reduce cost. During agentic search and 6 Table 2: Comparison on LVBench. Methods ER EU KIR TG Rea Sum Overall Commercial VLMs Gemini-1.5-Pro [24] Gemini-2.0-Flash [24] GLM-4V-Plus [9] GPT-4o [1] OpenAI o3 [18] Open-Source VLMs InternVL2.5-78B [29] VideoLLaMA3-7B [37] Qwen2.5-VL-72B [4] VideoChat-Flash [13] AdaRETAKE [28] Video Agents and Others VideoTree [30] VideoAgent [27] VCA [34] MR. Video [19] Deep Video Discovery (Ours) + Auxiliary transcripts 32.1 47.4 46.2 48.9 57.6 43.8 45.8 - 51.1 53.0 30.3 28.0 43.7 59.8 73.4 75.5 30.9 48.5 47.8 49.5 56.4 42.0 42.4 - 46.0 50. 25.1 30.3 40.7 57.4 73.3 77.1 39.3 56.8 54.1 48.1 62.9 42.1 47.8 - 49.0 62.2 26.5 28.0 37.8 71.4 80.4 79. 31.8 39.3 42.7 40.9 46.8 36.8 35.9 - 38.9 45.5 27.7 29.3 38.0 58.8 72.3 72.7 27.0 44.4 46.5 50.3 50.8 51.0 45.8 - 48.5 54. 31.9 28.0 46.2 57.7 70.7 68.7 32.8 41.4 37.9 50.0 67.2 37.9 36.2 - 34.5 37.9 25.5 36.4 27.3 50.0 74.1 84. 33.1 48.6 48.7 48.9 57.1 43.6 45.3 47.7 48.2 53.3 28.8 29.3 41.3 60.8 74.2 76.0 Table 3: Comparison on long video benchmarks. Methods Commercial VLMs Gemini-1.5-Pro [24] Gemini-2.0-Flash [24] GPT-4o [1] OpenAI o3 [18] Open-Source VLMs mPLUG-Owl3 [36] InternVL2.5-78B [29] Qwen2.5-VL-72B [4] AdaRETAKE [28] Video Agents and Others VideoTree [30] VideoAgent [27] VCA [34] MR. Video [19] Deep Video Discovery (Ours) LVBench LongVideoBench (Val) Overall Overall Long 33.1 48.3 48.9 57.1 43.5 43.6 47.7 53.3 28.8 29.3 41.3 60.8 74.2 64.0 - 66.7 67. 59.8 63.6 60.7 67.0 - - - - 71.6 58.6 45.7 60.9 60.6 - - - - - - - 61. 68.6 Video MME Long (w/o sub) EgoSchema Val 67.4 63.0 65.3 64.7 50.1 62.6 63.9 65.0 - - - 61. 67.3 - 71.2 70.4 63.2 - - - - 67.0 63.2 73.6 73.0 76.6 answering, we employ OpenAI o3 as LLM for its strong reasoning ability, including in the Frame Inspect module for fine-grained VQA. All frames are resized to 720p to maintain visual details. In Clip Search, we set 16 as the default value of top-k while leaving the flexibility for LLM to change it. Maximum reasoning step is set to = 15. To explore the upper bound of understanding ability, we additionally evaluate LVBench using auxiliary transcripts. Audio is transcribed with WhisperX[5], and transcripts are used to guide video segmentation and enrich captions. This audio-visual fusion enhances understanding of long, complex content, leading to stronger results. API Content filtering. We use LLM API via Azure OpenAI Service. We observe that the safety content filtering mechanism of the service misjudges small part of data from the benchmark as offensive and block the request, which leads to the reduced performance of both OpenAI o3 baseline and our DVD agent. We provide more details and mitigation strategies in our supplementary material. 7 Table 4: Ablation on used models. Mdatabase for captioning in database construction, Mreasoning for reasoning in ASA, Mtool for Frame Inspect. Table 5: Ablation on the search-centeric tools . Note that the anchor uses 4.1-mini for Mdatabase, and o3 for both Mreasoning and Mtool. Adopted models Mdatabase Mreasoning Mtool LVBench w/ transcripts 4.1 4.1 4.1 4.1-mini o3 o4-mini 4o o3 4.1-mini o3 o3 o3 4.1 o3 72.3 70.2 62.3 71.9 76.0 Search-centeric Tools LVBench Global Browse Clip Search Frame Inspect w/ transcripts 69.0 59.6 63.5 71.9 4.3 Main Results Table 2 presents the comparison results on LVBench. DVD significantly outperforms all baselines, surpassing the previous SOTA MR. Video by 13.4%. Compared to the prior leading video agent VCA, our method achieves remarkable 32.9% gain. Against our base VLM, OpenAI o3, our full system delivers substantial 17.1% gain, highlighting the importance of agentic reasoning. Incorporating transcript information provides an additional 1.8% boost. These results highlight the effectiveness of our search-centric agentic reasoning framework in handling ultra-long video understanding tasks. Table 3 provides comprehensive evaluation across several long-video benchmarks. On LongVideoBench, DVD outperforms the previous SOTA by 4.1% overall and 7.0% on the longestduration subset. On the Video MME Long subset, it beats the best open-source VLM, AdaRETAKE, by 2.3%, and MR. Video by 5.5%, approaching the performance of Gemini-1.5-Pro. On EgoSchema, our method exceeds the previous best by 3.0%. Notably, it exceeds reported human-level accuracy of 76% on this benchmark. Across all datasets, our system consistently outperforms the base VLM OpenAI o3, confirming the effectiveness and generalizability of our agentic reasoning framework. 4.4 Ablation Study We evaluate the impact of different model choices across system components. By default, GPT-4.1 is used for captioning and subject extraction during Multi-granular Video Database Construction and OpenAI o3 serves as the reasoning model in the Agentic search and Answer with tool use process while Frame Inspect tool also leverages OpenAI o3 to query the fine-grained details on the frame pixels. We denote the three models as Mdatabase, Mreasoning and Mtool in Table 4. Replacing GPT-4.1 with GPT-4.1-mini for database construction or Frame Inspect tool results in moderate drops of 4.1% and 3.7%, respectively, indicating relatively minor impact. For reasoning model in agentic search, switching to OpenAI o4-mini [18] leads to 5.8% drop, while GPT-4o causes substantial 13.7% decline. It highlights the reasoning model as the most critical component in our agentic system because our system is designed surrounding and to make full use of the reasoning capability of LLM. The lack of reasoning ability leads to the collapse of agent behavior, as analyzed further in the subsequent subsection. We also assess the contribution of each tool in the agentic search and answer phase  (Table 5)  . Removing Global Browse which is responsible for global summarization and long-range event linking leads to 2.9% drop. Disabling Frame Inspect with the fine-grained VQA results in 8.4% decline, highlighting its role in fine-grained understanding. Removing Clip Search causes the largest drop of 12.3%, as it breaks the searching ability for iteratively refine reasoning. These results underscore the importance of tool integration in our search-centric framework. 4.5 Analysis on Agentic Reasoning Behavior The reasoning model is the most critical component in DVD. During the observe-reason-act loop, the agent autonomously integrates current context and flexibly decides the next tool to invoke. To better understand this, we analyze the tool-calling behavior during the agentic search and answer phase and category it into five types for analysis (see Fig. 3). Global Browse Only means the agent answers immediately after single Global Browse call, reflecting strong confidence in the global context. Though rare, this behavior reaches high accuracy. 8 Figure 3: Analysis of the behavior of Deep Video Discovery using different reasoning models. We categorize tool-calling behavior into five types. For each type, we report its proportion (Ratio, sector angels), average reasoning steps (Steps, sector radius) and score (Score, dashed lines). clear correlation emerges among behavior patterns, reasoning depth, and score (see Section 4.5 for details). Simple Action involves at most two continuous Clip Search and two continuous Frame Inspect calls, following straightforward search-query-answer logic. This is the dominant strategy, covering over half the queries with strong accuracy. Iterative Search means the agent iteratively alternates between Clip Search and Frame Inspect to search for new contexts, indicating difficulty in finding sufficient information in early steps. It shows longer reasoning chains (e.g., 8.0 iters vs. 5.2 for Simple Action with o3) and slightly lower accuracy. Frame Inspect Trap means the agent invokes more than three consecutive Frame Inspect without concluding, becoming stuck in fine-grained analysis. This leads to long reasoning and low accuracy. Clip Search Trap means the agent repeatedly calls more than three consecutive Clip Search without reaching conclusion, e.g., when the key information misses in the database, causing the agent to loop without progress. This is frequent for o3 and accounts for most failures of it. From these results, we further draw two key insights, which we believe will provide valuable insights for the development of future autonomous video agent systems. Insight 1: Reasoning Steps v.s. Accuracy. Within the same model, longer reasoning chains often imply uncertainty and lower accuracy. However, across models, better performance is typically associated with more thorough and longer reasoning. Insight 2: Overconfidence and Behavioral Collapse. GPT-4o underperforms significantly compared to o3 and o4-mini. Its behavior collapses to Simple Action in 91.4% queries and rarely explores alternative strategies. With an average of just 4.6 reasoning steps, it tends to conclude prematurely. This suggests overconfidence and limited flexibility, likely leading to its lower accuracy."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduces the proposed Deep Video Discovery agent for long-form video understanding, utilizing multi-granular search tools on constructed database for iterative search and reasoning over extensive video content. Our approach outperforms prior methods by adaptively integrating global browsing, clip search, and frame inspection, as demonstrated by state-of-the-art results on multiple benchmarks. Ablation studies confirm the effectiveness of our tool design, while analyses of reasoning model behavior provide insight into model reasoning patterns. Overall, our framework offers scalable and flexible solution for comprehensive analysis of long videos. Limitations. While our agent significantly improves long video understanding, the iterative reasoning introduces higher computational overhead. In future work, we will explore more effective database construction and searching to reduce reasoning difficulty and thereby lower computational costs."
        },
        {
            "title": "Change Log",
            "content": "v1 (2025-05-23): Initial submission. v2 (2025-05-28): Fixed the evaluation code to correctly account for answers enclosed in parentheses, resulting in consistently improved reported accuracy."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] J. AI. DeepSearch - Jina AI. https://jina.ai/deepsearch/, 2025. [3] X. AI. Grok 3 Beta The Age of Reasoning Agents. https://x.ai/news/grok-3, 2025. [4] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. [5] M. Bain, J. Huh, T. Han, and A. Zisserman. Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023, 2023. [6] Y. Chen, F. Xue, D. Li, Q. Hu, L. Zhu, X. Li, Y. Fang, H. Tang, S. Yang, Z. Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [7] Y. Fan, X. Ma, R. Wu, Y. Du, J. Li, Z. Gao, and Q. Li. VideoAgent: memory-augmented multimodal agent for video understanding. In ECCV, 2024. [8] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis. arXiv preprint arXiv:2405.21075, 2024. [9] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao, et al. ChatGLM: family of large language models from GLM-130B to GLM-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [10] Google. Gemini Deep Research - your personal research assistant. https://gemini.google/ overview/deep-research, 2025. [11] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Y. Han, Q. Guo, L. Pan, L. Liu, Y. Guan, and M. Yang. Dynfocus: Dynamic cooperative network empowers llms with video understanding. arXiv preprint arXiv:2411.12355, 2024. [13] X. Li, Y. Wang, J. Yu, X. Zeng, Y. Zhu, H. Huang, J. Gao, K. Li, Y. He, C. Wang, Y. Qiao, Y. Wang, and L. Wang. VideoChat-Flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [14] X. Liu, Y. Shu, Z. Liu, A. Li, Y. Tian, and B. Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. [15] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023. [16] OpenAI. Introducing deep research. https://openai.com/index/ introducing-deep-research/, 2025. [17] OpenAI. Introducing GPT-4.1 in the API. https://openai.com/index/gpt-4-1/, 2025. Accessed: 2025-04-14. [18] OpenAI. Introducing OpenAI o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. Accessed: 2025-05-15. 10 [19] Z. Pang and Y.-X. Wang. Mr. video:\" mapreduce\" is the principle for long video understanding. arXiv preprint arXiv:2504.16082, 2025. [20] Perplexity. Introducing Perplexity Deep Research. https://www.perplexity.ai/hub/ blog/introducing-perplexity-deep-research, 2025. [21] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, X. Zhou, Y. Huang, C. Xiao, et al. Tool learning with foundation models. ACM Computing Surveys, 57(4):140, 2024. [22] C. Qu, S. Dai, X. Wei, H. Cai, S. Wang, D. Yin, J. Xu, and J.-R. Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343, 2025. [23] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [24] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [25] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [26] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [27] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy. VideoAgent: Long-form video understanding with large language model as agent. In ECCV, 2024. [28] X. Wang, Q. Si, J. Wu, S. Zhu, L. Cao, and L. Nie. Adaretake: Adaptive redundancy reduction to perceive longer for video-language understanding. arXiv preprint arXiv:2503.12559, 2025. [29] Y. Wang, X. Li, Z. Yan, Y. He, J. Yu, X. Zeng, C. Wang, C. Ma, H. Huang, J. Gao, et al. InternVideo2.5: Empowering video MLLMs with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [30] Z. Wang, S. Yu, E. Stengel-Eskin, J. Yoon, F. Cheng, G. Bertasius, and M. Bansal. VideoTree: Adaptive tree-based video representation for LLM reasoning on long videos. arXiv preprint arXiv:2405.19209, 2024. [31] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2024. [32] Y. Yan, S. Jiang, T. Cao, Y. Yang, Q. Yang, Y. Shu, Y. Yang, and L. Qiu. Empowering agentic video analytics systems with video language models. arXiv preprint arXiv:2505.00254, 2025. [33] A. Yang, B. Yu, C. Li, D. Liu, F. Huang, H. Huang, J. Jiang, J. Tu, J. Zhang, J. Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. [34] Z. Yang, D. Chen, X. Yu, M. Shen, and C. Gan. VCA: Video curious agent for long video understanding. arXiv preprint arXiv:2412.10471, 2024. [35] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023. [36] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mPLUG-OWL3: Towards long image-sequence understanding in multi-modal large language models. In ICLR, 2024. [37] B. Zhang, K. Li, Z. Cheng, Z. Hu, Y. Yuan, G. Chen, S. Leng, Y. Jiang, H. Zhang, X. Li, P. Jin, W. Zhang, F. Wang, L. Bing, and D. Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 11 [38] K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin. Codeagent: Enhancing code generation with toolintegrated agent systems for real-world repo-level coding challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1364313658, 2024. [39] Z. Zhang, X. Zhang, W. Xie, and Y. Lu. Responsible task automation: Empowering large language models as responsible task automators. arXiv preprint arXiv:2306.01242, 2023."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "University of Science and Technology of China"
    ]
}