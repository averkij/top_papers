{
    "paper_title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Kai Yang",
        "Saiyong Yang",
        "Yankai Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 5 2 1 2 1 . 2 0 6 2 : r February 13, Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation Wenkai Yang1,, Weijie Liu2, Ruobing Xie2, Kai Yang2, Saiyong Yang2, Yankai Lin1, 1Gaoling School of Artificial Intelligence, Renmin University of China 2LLM Department, Tencent (cid:66) {wenkaiyang,yankailin}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "On-policy distillation (OPD), which aligns the student with the teachers logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (GOPD) framework, which extends the standard OPD objective by introducing flexible reference model and reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teachers performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling smaller student from larger teacher), performing reward correction by choosing the reference model as the teachers base model before RL yields more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teachers pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.1 (a) Multi-teacher distillation results, student model is Qwen34B-Non-Thinking, teachers are domain-specific RL variants (b) Strong-to-weak distillation results, Qwen3-30B-A3B-Instruct-2507 teacher model is Figure 1: The empirical effectiveness of our method ExOPD compared with off-policy distillation (SFT), standard OPD, and the weight-extrapolation method ExPO (Zheng et al., 2025) in multi-teacher and strong-to-weak distillation settings (results averaged over 4 math reasoning and 3 code generation benchmarks). (a) When merging multiple domain expertsobtained by applying domain-specific RL to the same base modelback into the original base model, ExOPD is the only method that yields unified student that consistently outperforms all domain teachers. (b) ExOPD also yields significant improvements over standard OPD when distilling smaller student from larger teacher. Moreover, applying reward correction in ExOPD can further boost distillation performance (Figure 6). Work done during an internship at Tencent. Corresponding author. 1Code is available at https://github.com/RUCBM/G-OPD."
        },
        {
            "title": "Introduction",
            "content": "Recently, on-policy distillation (OPD) (Agarwal et al., 2024; Yang et al., 2025a; Lu & Lab, 2025) has emerged as an effective post-training paradigm for improving capabilities of Large Language Models (LLMs). Unlike prior off-policy distillation methods (Taori et al., 2023; Guha et al., 2025) that train the student on teacher-generated trajectories, OPD allows the student to learn from the teachers supervision (i.e., predicted logits) on studentgenerated tokens. Previous studies have shown that OPD can not only serve as promising multi-task post-training paradigm to (near-)losslessly merge the capabilities acquired by different RL variants across domains back into the original base model (Xiao et al., 2026), but also be effective and efficient in distilling the capabilities of larger teacher into smaller student (Gu et al., 2024; Yang et al., 2025a). Despite its empirical effectiveness, mechanistic understanding of OPD remains limited in the field, leaving its full potential under-explored. In this work, we bridge this gap by establishing theoretical connection between OPD and dense reinforcement learning (RL), and by extending standard OPD into generalized formulation. First, we make derivations to show that OPD is essentially special case of the standard dense RL with KullbackLeibler (KL) constraint, where the token-level reward function is always weighted equally with the KL regularization and the reference model can be chosen arbitrarily. Building on this insight, we generalize the OPD objective to more universal formulation by further introducing reward scaling factor that controls the relative weight of the reward term against the KL regularization, in addition to the flexible reference model. We refer to this generalized formulation as the Generalized On-Policy Distillation (G-OPD) framework. Based on the G-OPD framework, we theoretically analyze how the reward scaling factor and the choice of reference model affect distillation effectiveness across different settings, supported by comprehensive experiments in both math reasoning and code generation domains. In the first setting, the teacher is obtained by applying domain-specific RL to the student, and the reference model is naturally fixed to the students initial state. We show that (1) when the reward scaling factor lies in (0, 1) (i.e., reward interpolation), the distilled student exhibits behaviors (e.g., performance and response length) that fall between the reference and teacher models; (2) when the reward scaling factor is greater than 1 (i.e., reward extrapolation), the student can learn beyond the teachers capability boundary and outperform teacher in domain tasks. We refer to the reward extrapolation variant as ExOPD. We further show that ExOPD extends well to the multi-teacher distillation setting, enabling unified student to surpass all domain teachers. Second, we study the strong-to-weak distillation setting, where smaller student is distilled from larger teacher. In this setting, we demonstrate that replacing the reference model from the students initial policy to the teachers pre-RL variant (i.e., reward correction) in ExOPD yields more accurate reward signal and further improves distillation performance. However, the limitations of this practice are that it assumes access to an additional model (the teachers pre-RL variant) and incurs more computational cost on computing the log-probabilities of the larger reference model. Despite these limitations, ExOPD and ExOPD with reward correction significantly outperform standard OPD in the strong-to-weak distillation setting."
        },
        {
            "title": "2 Related Work",
            "content": "Off-Policy Distillation. Knowledge distillation (KD) (Hinton et al., 2015) is widely used technique for transferring knowledge from domain expert (teacher) to student model. Most prior studies focus on off-policy distillation, where the student is trained on trajectories generated by the teacher, either by aligning the students logits distribution with the teachers via KullbackLeibler (KL) divergence loss on token logits (Sanh et al., 2019; Kim & Rush, 2016; Guo et al., 2025b), or by directly performing supervised fine-tuning (SFT) with cross-entropy loss on the teacher-generated tokens (Taori et al., 2023; Zhou et al., 2023; Guha et al., 2025). This practice has been shown to effectively improve the student model across broad range of capabilities (Ding et al., 2023; Yang et al., 2025e; Ye et al., 2025b) in the LLM era. On-Policy Distillation. By sampling trajectories from the student and aligning the student with the teachers logit distribution on each token of these student-generated trajectories, on-policy distillation (OPD) (Agarwal et al., 2024; Gu et al., 2024) realizes dense on-policy learning. Empirically, OPD has been shown to achieve faster and more effective distillation than off-policy distillation (Yang et al., 2025a; Lu & Lab, 2025). Recent OPD studies have explored distillation across different model families (Patino et al., 2025), developed black-box on-policy distillation methods that do not require access to the teachers logits (Ye et al., 2025a), and investigated the self-distillation paradigm that leverage the LLMs in-context capabilities to distill textual context information into its parameters (Yang et al., 2025c; Hubotter et al., 2026; Shenfeld et al., 2026; Zhao et al., 2026)."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminaries In this section, we start with brief review of relevant preliminaries. 2 Off-Policy Distillation. Let denote the input distribution, and let πθ and π denote the student and teacher policies, respectively. The general form of Knowledge Distillation (KD) (Hinton et al., 2015) can be written as JKD(θ) = min θ xD,yπ(x) (cid:104) (cid:0)π(yx) (cid:13) (cid:13) πθ(yx)(cid:1)(cid:105)"
        },
        {
            "title": "DKL",
            "content": ", (1) where DKL denotes the KullbackLeibler (KL) divergence loss. In the era of LLMs, obtaining the teachers full output distribution (e.g., logits) is often expensive or even infeasible. As result, KD is commonly implemented as supervised fine-tuning (SFT) of the student on trajectories generated by the teacher. Though effective, the major drawback of this paradigm is its off-policy nature: the student is trained to imitate the teachers behavior, rather than to learn from reward signals induced by its own actions. As result, it may fail to adapt and generalize from its own experience at test time, when faced with similar problems. On-Policy RL. We use πθ to denote the policy model to be optimized. The RL objective can be formulated as JRL(θ) = max θ xD,yπθ(x) (cid:104) r(x, y) βDKL(πθ πref) (cid:105) . (2) In the above formulation, the trajectories are sampled from the current policy model, making the training remain on-policy. r(x, y) is the reward function that measures the quality of response sequence = (y1, , yT) to query x. Depending on the setting, it can be either (i) parameterized neural reward model trained on the specific preference data for open-domain alignment (Cai et al., 2024; Dong et al., 2024; Liu et al., 2025a), or (ii) rule-based, deterministic outcome verifier commonly used in verifiable LLM reasoning tasks (Guo et al., 2025a; Hu et al., 2025; Liu & Zhang, 2025; Yang et al., 2025b). DKL(πθ πref) prevents the policy model πθ from drifting too far from reference model πref, and the coefficient β controls the strength of this constraint. To solve Eq. (2), common approach is to apply policy gradient (Sutton et al., 1998), updating the policy parameters using an estimated gradient of the form θJRL(θ) = xD, yπθ(x) Atθ log πθ(ytx, y<t) (cid:105) , (3) (cid:104) t=1 where At is the relative advantage of token yt over baseline value. In practice, the reward signal in RL is often sparse: the policy model only receives reward at the final token after the response is completed, which may make optimization inefficient and ineffective (Cui et al., 2025). On-Policy Distillation. On-Policy Distillation (OPD) (Agarwal et al., 2024; Gu et al., 2024; Lu & Lab, 2025) inherits the on-policy nature of policy training and the advantage of dense credit assignment, making it an efficient post-training paradigm (Yang et al., 2025a; Xiao et al., 2026). The main idea of OPD is to let the student generate its own trajectories, and then minimize the reverse KL divergence between the student and the teacher π on those student-generated trajectories: JOPD(θ) = min θ xD,yπθ(x) (cid:104) (cid:16) DKL πθ(yx) (cid:13) (cid:13) π(yx) (cid:17)(cid:105) . (4) Notice that in Eq. (4), the trajectories are generated by the policy model itself, resulting in the on-policy training. Also, we can get the gradient of OPD as2 θJOPD(θ) = xD,yπθ(x) (cid:104) t=1 (cid:16) =t (cid:0) log πθ(yt x, y<t ) log π(yt x, y<t )(cid:1)(cid:17) θ log πθ(ytx, y<t) (cid:105) . (5) In practice, current studies (Lu & Lab, 2025; Xiao et al., 2026) use discount factor of 0 (focus on next-token optimization only) and approximate the gradient as θJOPD(θ) = xD,yπθ(x) (cid:104) t=1 (cid:0) log πθ(ytx, y<t) log π(ytx, y<t)(cid:1) θ log πθ(ytx, y<t) (cid:105) . (6) Comparing Eq. (6) with Eq. (3), we can see that (cid:0) log πθ(ytx, y<t) log π(ytx, y<t)(cid:1) can be regarded as the token-level advantage in OPD, thereby providing dense credit assignment for each token-level action. 3.2 Generalized On-Policy Distillation In this section, we first start from Eq. (4) and derive generalized formulation of OPD. 2Detailed derivations are in Appendix A. First, we re-formulate the OPD objective as JOPD(θ) = min θ = min θ = max θ = max θ = max θ E xD,yπθ(x) xD,yπθ(x) (cid:104) (cid:104) (cid:0)πθ(yx) (cid:13) (cid:13) π(yx)(cid:1)(cid:105)"
        },
        {
            "title": "DKL",
            "content": "log πθ(yx) log π(yx) (cid:105) (cid:104) (cid:105) E xD,yπθ(x) xD,yπθ(x) xD,yπθ(x) log π(yx) log πθ(yx) (cid:104)(cid:0) log π(yx) log πref(yx)(cid:1) (cid:0) log πθ(yx) log πref(yx)(cid:1)(cid:105) (cid:104) DKL (cid:0)πθ(yx) (cid:13) (cid:13) πref(yx)(cid:1)(cid:105) . log π(yx) πref(yx) (7) Therefore, we have the following remark: Remark By introducing third reference model πref, the OPD objective in Eq. (4) becomes equivalent to specific KL-constrained RL objective in Eq. (2), where the reward function r(x, y) = log π(yx) πref(yx) , the KL regularization is applied between the policy model πθ and the reference model πref, and the reward and KL terms are weighted equally (i.e., β = 1 in Eq. (2)). From the above remark, we establish the connection between OPD and RL. However, we emphasize that OPD differs from standard RL in the following key respects: (1) Dense rewards. As discussed above, in standard RL the model typically receives an effective reward only at the final token, while the rewards for all other tokens are zero: rRL = (cid:26)0 Outcome Reward = 1, , 1, = T. However, in OPD, each token-level action receives an effective reward rOPD = log π(ytx, y<t) πref(ytx, y<t) , = 1, , T. (8) (9) This token-level reward takes essentially the same form as the implicit reward defined in Rafailov et al. (2023). Implicit reward is initially derived from the closed-form solution of Eq. (2), which can be written as r(x, y) = β log πθ(yx) πref(yx) + β log Z(x), where Z(x) = πref(yx) exp( 1 β r(x, y)). (10) As we can see, since log Z(x) is constant depending only on x, log πθ(yx) πref(yx) can be regarded as well-defined proxy of the true reasoning reward, and this idea is adopted in previous studies (Yuan et al., 2024; Cui et al., 2025; Yang et al., 2025d; Liu et al., 2025c) to provide dense supervision for RL. However, in OPD, the implicit reward log π(yx) πref(yx) does not require π to be obtained by applying RL starting from πref. In fact, π and πref can even be models of different sizes. Nevertheless, this reward function still captures the log-probability shift from the reference (πref) distribution to the expert (π) distribution, and thus provides meaningful training signal. (2) Fixed weighting between the reward function and the KL regularization. As revealed in the remark, in OPD, the reward term and the KL regularization are always weighted equally. In what follows, we present and discuss our generalized OPD formulation by introducing reward scaling factor that allows us to adjust the relative weight of the reward term against the KL regularization. (3) Flexible choice of the reference model. In RL (i.e., Eq. (2)), the reference model is typically initialized as the policy models starting checkpoint. However, we note that in OPD (i.e., Eq. (11)), the introduced reference model can be any model, since this choice does not affect the final simplification of the objective back to its original form in Eq. (4). In what follows, we discuss how different choices of πref affect our proposed generalized OPD framework. By default, the reference model is selected as the students initial policy. From the above discussion, we can see that OPD offers two key advantages over RLdense reward signals and flexible choice of reference modelyet it fixes the relative weighting between the reward function and the KL regularization to 1 : 1. This motivates us to follow Eq. (2) and generalize the original OPD objective in Eq. (4) into general dense RL objective with flexible KL constraint, by introducing both third reference model and an additional reward scaling factor λ: JG-OPD(θ) = max θ xD,yπθ(x) (cid:104) λ log π(yx) πref(yx) DKL (cid:0)πθ(yx) (cid:13) (cid:13) πref(yx)(cid:1)(cid:105) . (11) The above Eq. (11) presents our Generalized On-Policy Distillation (G-OPD) formulation, where λ controls the relative weight of the reward term against the KL regularization in the objective, and is essential 1 β in Eq. (2). As we can see, compared to RL, G-OPD enables dense credit assignment and more flexible choice of reference model; compared to OPD, it further allows more general control over the reward weight. In the following, we discuss in detail about the two crucial components, λ and πref, in G-OPD. Reward interpolation and extrapolation in G-OPD. The optimal solution to G-OPD in Eq. (11) satisfies that log πθ(yx) = λ log π(yx) + (1 λ) log πref(yx) = log π(yx) + (λ 1)(log π(yx) log πref(yx)). (12) This reveals that, (1) when 0 < λ < 1, G-OPD encourages the student models log-probability distribution to match linear interpolation between that of the teacher and reference models. This can also be interpreted as replacing the reward in Eq. (7) with λ + (1 λ) 0. Therefore, we refer to this case as reward interpolation. We conjecture that, under this setting, the student trained with G-OPD may exhibit behavior (e.g., performance, response length, etc.) that lies between the reference model and the standard OPD with λ = 1. (2) When λ > 1, G-OPD encourages the students log-probability distribution to go beyond matching the teachers log-probabilities by additionally fitting an extra shift term (λ 1)(log π log πref). From the perspective of rewards, G-OPD with λ > 1 performs an extrapolation of the reward functions weight in Eq. (7); thus, we refer to this regime as reward extrapolation. We wonder whether reward extrapolation can outperform standard OPD, and in special case, when the teachers are domain experts obtained by applying RL to the same student (Xiao et al., 2026) in different domains, can reward extrapolation in G-OPD distill unified student that surpasses all the domain teachers? Reward correction in strong-to-weak distillation. When the reward scaling factor λ = 1, different choices of the reference model πref in Eq. (11) lead to different objectives. Based on distillation settings, in the following, we discuss the choices of πref in two cases: (1) One application of G-OPD is to merge the capabilities of several experts, each obtained by applying domain-specific RL starting from the same base model, back into the original base model (Xiao et al., 2026). In this setting, πref is naturally chosen as the original base model, and the reward function in G-OPD is exactly the implicit reward defined in Eq. (10). (2) Another distillation setting is strong-to-weak distillation (Yang et al., 2025a), i.e., distilling large teacher into smaller student. In this case, πref admits two choices: (i) the students base model, πstudent , which corresponds to the default setting where we only have access to π and πstudent (i.e., the teacher before post-training), assuming it is available. To compare these two choices, we first rewrite the G-OPD objective into an equivalent form: ; or (ii) the teacher experts pre-RL base model, πteacher base base base JG-OPD(θ) = max θ = max θ xD,yπθ(x) xD,yπθ(x) (cid:104) λ log π(yx) πref(yx) (cid:104) (λ 1) log π(yx) πref(yx) DKL (cid:0)πθ(yx) (cid:13) (cid:13) πref(yx)(cid:1)(cid:105) DKL (cid:0)πθ(yx) (cid:13) (cid:13) π(yx)(cid:1)(cid:105) (13) . Now, under the same KL regularization strength, we can see that choosing πref = πteacher reason is that the reward log π is more reasonable. The corresponds to the implicit reward induced by the teachers RL post-training, base and is thus well-defined according to Eq. (10). In contrast, log can be noisier, since there exists fundamental gap between the internal knowledge and capacity of teacher and student base models. Therefore, in the strong-toweak distillation setting, we think that applying reward correction to the default reward log by adding π πstudent base πteacher base π πstudent base can lead to better distillation performance. The limitations, however, are that this πteacher base and incurs additional computation, since computing log πteacher requires more cost than base log πstudent base πteacher base to obtain log π requires access to πteacher base computing log πstudent . base Remark By introducing reference model πref and reward scaling factor λ, we formulate the Generalized On-Policy Distillation framework as JG-OPD(θ) = max xD,yπθ(x) (cid:104) λ log π(yx) πref(yx) DKL (cid:0)πθ(yx) (cid:13) (cid:13) πref(yx)(cid:1)(cid:105) . θ We have two observations: (1) For 0 < λ < 1, G-OPD yields student whose behavior lies between that of the reference model and that of the student trained by standard OPD (i.e., λ = 1). In contrast, λ > 1 can potentially deliver larger gains than standard OPD, and may even produce student that outperforms the teacher in certain cases. Note that setting λ = 1 incurs additional computational cost on computing log πref. 5 (2) In strong-to-weak distillation, choosing the teachers pre-RL base model πteacher as the reference may yield better distillation performance. However, this comes with two limitations: it requires access to an additional model πteacher requires more computational cost than computing log πstudent , and it increases computational cost because computing log πteacher base base base . base Finally, the approximated gradient of G-OPD can be written as θJG-OPD(θ) = xD,yπθ(x) (cid:104) t=1 AG-OPD θ log πθ(ytx, y<t) (cid:105) , (14) where AG-OPD = (cid:0) log πθ(ytx, y<t) log π(ytx, y<t)(cid:1) + (λ 1)(cid:0) log πref(ytx, y<t) log π(ytx, y<t)(cid:1)."
        },
        {
            "title": "4 Experiments and Analysis",
            "content": "In this section, we conduct series of extensive experiments on math reasoning and code generation tasks to analyze the properties of the proposed G-OPD framework and assess the effectiveness of ExOPD. We begin with preliminary experiments on same-size teacher-student pairs in Section 4.1.2, where we investigate the impact of the reward scaling factor within G-OPD. We then explore the effectiveness of ExOPD in the multi-teacher distillation setting in Section 4.1.3. Finally, we present experimental results in the strong-to-weak distillation setting in Section 4.2. 4.1 Experiments with Same-Sized Student and Teacher Here, we consider the scenario where the domain teachers are reinforced models derived from the student through domain-specific RL. 4.1.1 Experimental Settings Base Model. We primarily conduct experiments using the Qwen3-4B-Non-Thinking (Yang et al., 2025a) model. The student model is initialized as Qwen3-4B-Non-Thinking, while the domain teachers are derived by applying RL separately to Qwen3-4B-Non-Thinking on domain-specific data. Training Datasets. We filter the DeepMath (He et al., 2025) dataset to select 57K samples with difficulty level greater than or equal to 6 to form the math RL data, and use Eurus-RL-Code (Cui et al., 2025) as the code RL data, which consists of 25K samples. We then apply RL to the base model on two datasets separately to get domain teachers, Qwen3-4B-Non-Thinking-RL-Math and Qwen3-4B-Non-Thinking-RL-Code. The distillation data is the same as the RL data. Training Settings. We apply Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to obtain domain teachers. In RL, reward of 1.0 is given when the final answer is correct in math reasoning or when all unit tests pass in code generation; otherwise, the reward is 0.0. Detailed training hyper-parameters in GRPO are in Appendix B. After this, we implement G-OPD on the original student model (i.e., Qwen3-4B-Non-Thinking) with different reward scaling factors λ {0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5}. Note that λ = 0.0 corresponds to the initial state Qwen3-4B-Non-Thinking, and λ = 1.0 corresponds to standard OPD. The reference model here is fixed naturally as Qwen3-4B-Non-Thinking. Detailed training hyper-parameters in G-OPD are in Appendix B. In both GRPO and G-OPD, we implement token-level rollout correction (Liu et al., 2025b) to mitigate training-inference mismatch. Our experiments are based on verl (Sheng et al., 2024) framework. Evaluation. For the evaluation of math reasoning, we select four competition-level benchmarks: AIME24 (AIMO, 2024), AIME25 (OpenCompass, 2025), HMMT25 (February) (Balunovic et al., 2025), and HMMT25 (November) (Balunovic et al., 2025). For the evaluation of code generation, we select three test sets: HumanEval+, MBPP+ (Liu et al., 2023), and LiveCodeBench (v6 only, February 2025May 2025) (Jain et al., 2024). In all evaluations, we set the temperature to 1.0, top-p to 1.0, and the maximum generation length to 16,384. On each math reasoning benchmark, we sample 32 solutions for each problem; whereas each code generation benchmark, we sample 4 solutions per problem. We then report the average accuracy of each model on each benchmark. We adopt Math-Verify3 as rule-based verifier to validate answer correctness for math reasoning benchmarks. 4.1.2 Results of Single-Teacher Distillation We first explore the impact of reward scaling factor λ in G-OPD in the same-sized single-teacher distillation setting as the preliminary experiments (i.e., distilling Qwen3-4B-Non-Thinking-RL-Math or Qwen3-4B-Non-Thinking-RLCode back into Qwen3-4B-Non-Thinking). The evaluation results in math reasoning and code generation domains 3https://github.com/huggingface/Math-Verify Figure 2: On-policy distillation results on four math reasoning benchmarks under different choices of reward scaling factor λ. Figure 3: On-policy distillation results on three code generation benchmarks under different choices of reward scaling factor λ. (a) Results on AIME24 (b) Results on AIME25 (c) Results on HMMT25 (Feb.) (d) Results on HumanEval+ (e) Results on MBPP+ (f) Results on LiveCodeBench Figure 4: Trends in the average number of tokens and the average accuracy of the on-policy distilled models across different benchmarks under varying reward scaling factors. The teacher for math reasoning tasks is Qwen3-4B-Nonthinking-GRPO-Math, while the teacher for code generation tasks is Qwen3-4B-Non-thinking-GRPO-Code. are in Figure 2 and Figure 3 respectively. We also visualize the relationship between accuracy and response length of each model in Figure 4 for deep analysis. We can draw the following conclusions: (1) Standard OPD can fully recover the post-training behavior. As we can see, the student produced by OPD closely matches the evaluation accuracy and response length of the domain teacher. (2) Reward interpolation (0 < λ < 1) produces student whose behavior (performance and response length) lies between the base model and the teacher model. Also, both the performance and response length increase monotonically as λ grows, approaching the behavior of the teacher. This property can be leveraged to achieve budget-controlled reasoning (Yang et al., 2025e; Liang et al., 2026). (3) Reward extrapolation (λ > 1) outperforms standard OPD and has the potential to produce student that surpasses the domain teacher. As observed, ExOPD with appropriate reward extrapolation (i.e., λ = 1.25) consistently outperforms OPD and the domain teacher in all settings (also see Table 2), while excessive reward extrapolation (i.e., λ = 1.5) may lead to instability and degrade performance. This can be explained by the fact that continuously increasing λ introduces the risk of the student hacking the implicit reward in Eq. (9), by aggressively fitting the peak of the log ratio, even if some tokens have excessively large log ratios due to bias. Furthermore, we can see that the response lengths of Table 1: Comparison against the math domain teacher with continued RL. Each numerical subscript indicates the absolute improvement or degradation compared to the teacher model. Method"
        },
        {
            "title": "Teacher",
            "content": "+ continued RL (100 steps) ExOPD (50 steps) AIME24 AIME25 HMMT25 (Feb.) HMMT25 (Nov.) Avg. 58.0 60.9+2.9 62.7+4.7 54.6 55.6+0.5 56.1+1. 32.5 32.8+0.3 33.9+1.4 38.9 38.40.5 39.3+0.4 46.0 46.9+0.9 48.0+2.0 Table 2: Comparison against off-policy distillation (SFT) and weight extrapolation (ExPO) methods in both singleteacher and multi-teacher settings with same-sized teacher-student pairs. Teacher represents the performance of the domain teacher model (Qwen3-4B-Non-thinking-GRPO-Math for math reasoning and Qwen3-4B-Non-thinkingGRPO-Code for code generation), Student represents the initial performance of student model Qwen3-4B-NonThinking. Each numerical subscript indicates the absolute improvement or degradation compared to the domain teacher model. Method AIME24 AIME25 HMMT25 (Feb.) HMMT25 (Nov.) Avg. HumanEval+ MBPP+ Math Reasoning Code Generation Teacher Student 58.0 21. 54.6 21.9 Single-Teacher Distillation 55.2+0.6 58.7+0.7 55.0+0.4 60.7+2.7 56.1+1.5 62.7+4.7 ExPO OPD ExOPD Multi-Teacher Distillation 53.31.3 58.5+0.5 54.50.1 57.50.5 54.10.5 60.6+2.6 56.0+1.4 61.0+3.0 SFT ExPO OPD ExOPD 32.5 10. 32.40.1 32.40.1 33.9+1.4 30.71.8 31.70.8 32.5+0.0 34.4+1.9 38.9 8.0 37.01.9 37.91.0 39.3+0.4 34.84.1 36.32.6 38.30.6 39.2+0.3 46.0 15. 86.0 74.7 70.2 64.7 LCB 27.3 17.9 Avg. 61.2 52. 45.80.2 46.5+0.5 48.0+2.0 44.31.7 45.01.0 46.4+0.4 47.7+1.7 84.81.2 85.20.8 86.9+0.9 86.4+0.4 86.7+0.7 84.61.4 86.3+0.3 70.2+0.0 69.90.3 70.7+0.5 69.60.6 72.0+1.8 69.50.7 70.6+0. 28.0+0.7 27.3+0.0 28.6+1.3 61.00.2 60.80.3 62.1+0.9 26.40.9 29.0+1.7 27.6+0.3 29.0+1.7 60.80.4 62.6+1.4 60.60.6 62.0+0.8 the students produced by ExOPD continue to increase, which may be due to the length bias issue of the implicit reward (Yang et al., 2025d). To demonstrate that the improvement of ExOPD over the teacher is not due to less training of the teacher, we compare the evaluation performance of ExOPD and the teacher after an additional 100 steps of RL training. The results in Table 1 show that the teacher with more continued RL training show smaller improvement compared to ExOPD with fewer steps. 4.1.3 Results of Multi-Teacher Distillation Based on above analysis, we conduct experiments in the multi-teacher distillation setting, where we aim to merge the capabilities from different domain teachers, obtained by applying domain-specific RL to the same base model, into the original base model through OPD (Xiao et al., 2026). This has been demonstrated to be an effective new multi-task post-training paradigm. Specifically, the domain teachers are the above RL variants Qwen3-4B-NonThinking-RL-Math/Code, and the student model is Qwen3-4B-Non-Thinking. From the preliminary results in Section 4.1.2, we can see that λ = 1.25 in ExOPD consistently leads to better performance than OPD. Thus, in all subsequent experiments, we fix λ = 1.25 for ExOPD without any further specific tuning. Besides OPD, we also compare against two baselines: (1) Supervised fine-tuning (SFT), which trains the student on the teachers generated trajectories via Cross-Entropy Loss. We ensure that the number of trajectories used for SFT is consistent with those in OPD and ExOPD. More details can be found in Appendix B. (2) ExPO (Zheng et al., 2025), weight extrapolation method. We implement ExPO by first averaging the weights of all domain teachers, then extrapolating the weights against the student model using an extrapolation factor α, which is tuned from {0.25, 0.5} following the recommendations. For fair comparison, we downweight the sample size of the math RL data to match that of the code RL data in both OPD and ExOPD here, ensuring that each domain has the same sample size. The results of multi-teacher distillation are shown in Table 2. As we can see, SFT produces sub-optimal student, while the performance ceiling of OPD is typically bounded by the teachers. ExPO, though training-free, cannot ensure that the weight-extrapolated student consistently surpasses all domain teachers, lacking good controllability. However, our method ExOPD consistently outperforms OPD and is the only method that produces unified student capable of surpassing both domain teachers on all benchmarks. Furthermore, we analyze the training dynamics of ExOPD compared to OPD to gain deeper understanding of ExOPD. We put the comparison in Figure 5. ExOPD achieves higher training rewards but makes the student generate 8 (a) Training rewards (b) Response length (c) Entropy Figure 5: Training dynamics of OPD and ExOPD in multi-teacher distillation experiments. We visualize using Exponential Moving Average (EMA) smoothing with coefficient of 0.5. Table 3: Evaluation accuracy on four math reasoning benchmarks in the strong-to-weak distillation setting. Teacher model is Qwen3-30B-A3B-Instruct-2507. The numerical subscript indicates the absolute improvement or degradation compared to the standard OPD."
        },
        {
            "title": "Method",
            "content": "AIME24 AIME25 HMMT25 (Feb.) HMMT25 (Nov.) Teacher 74.7 62. 44.2 57.2 Student: Qwen3-1.7B-Non-Thinking Base SFT OPD ExOPD 12.3 18.1 33.0 37.3+4.3 11.4 20.5 28.7 31.5+2. Student: Qwen3-4B-Non-Thinking Base SFT OPD ExOPD 21.5 45.4 55.0 58.7+3.7 21.9 40.9 48.0 50.8+2.8 6.8 9.2 15.7 16.2+0.5 10.0 22.4 29.8 33.0+3. 4.5 6.3 14.9 16.5+1.6 8.0 31.6 37.7 38.8+1.1 Avg. 59.7 8.8 13.5 23.1 25.4+2.3 15.4 35.1 42.6 45.3+2. longer response lengths, which is consistent with the evaluation results shown in Figure 4. We also observe that the response entropy of the student trained by ExOPD is higher than that trained by OPD. We attribute this to the fact that the former tends to generate longer responses, increasing the response diversity. 4.2 Experiments in the Strong-to-Weak Distillation Setting Another practical usage of OPD is for strong-to-weak distillation (Yang et al., 2025a), i.e., distilling capabilities from larger teacher into smaller student. Thus, in this section, we explore the effectiveness of ExOPD and the additional reward correction practice in the strong-to-weak distillation setting. 4.2.1 Experimental Settings We select Qwen3-30B-A3B-Instruct-2507 as the teacher model and perform distillation on Qwen3-1.7B-NonThinking and Qwen3-4B-Non-Thinking, respectively. We primarily conduct experiments in the math reasoning domain, where the training and evaluation datasets are the same as those used in Section 4.1. The training details are in Appendix B. In ExOPD, we first conduct experiments in the default setting (Section 4.2.2), where we assume the availability of only two models: the student base model and the stronger teacher model. Thus, in this default setting, we set the reference model in ExOPD to the student base model. We also explore the effectiveness of the reward correction technique in Section 4.2.3, where we assume extra access to the teachers pre-RL variant, which serves as the reference model in ExOPD. We compare ExOPD against standard OPD and off-policy distillation (SFT). 4.2.2 Results of Strong-to-Weak Distillation The results in default strong-to-weak distillation setting are put in Table 3. The main conclusion is that ExOPD can bring significant improvements in strong-to-weak distillation, outperforming off-policy distillation and standard OPD by large margin. The results reveal that, although the implicit reward log π may contain πstudent base 9 (a) Results averaged on four math reasoning benchmarks, student is Qwen3-1.7B-Non-Thinking, teacher is Qwen3-4BNon-thinking-GRPO-Math (b) Results averaged on three code generation benchmarks, student is Qwen3-1.7B-Non-Thinking, teacher is Qwen3-4BNon-thinking-GRPO-Code Figure 6: Effect of reward correction in the strong-to-weak distillation setting. noise due to the intrinsic knowledge gap and distribution bias between the small and large models, extrapolating the rewards can still push the limits of OPD in strong-to-weak distillation. 4.2.3 Reward Correction in Strong-to-Weak Distillation As shown above, the default ExOPD with the reference model fixed as the student base model can already bring significant improvement over OPD. However, as discussed in Remark 3.2, setting the reference model to the teachers pre-RL variantif availablemay further enhance the distillation performance. Here, we conduct experiments to validate this analysis. Specifically, since we cannot get the pre-RL variant of Qwen3-30B-A3B-Instruct-2507, we choose our trained Qwen3-4B-Non-Thinking-RL-Math/Code as the teachers and take Qwen3-4B-Non-Thinking as the pre-RL variant. The student model is Qwen3-1.7B-Non-Thinking. The comparison results are displayed in Figure 6. The results validate the effectivenss of the reward correction practice, which consistently boosts the performance of ExOPD. However, we reiterate that reward correction requires access to πteacher and incurs higher computational cost, since it requires computing log-probabilities under larger reference model than in the default ExOPD. base"
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In this work, we conduct an in-depth analysis of the on-policy distillation paradigm. We first establish an interesting connection between OPD and dense KL-constrained RL. Building on this insight, we propose generalized OPD framework (G-OPD) by introducing (i) flexible reference model for the implicit reward function and (ii) reward scaling factor that controls the relative weight of the reward term versus KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we provide several novel insights: (1) Appropriate reward extrapolation (i.e., setting the reward scaling factor to be larger than 1) can improve OPD performance, and in same-sized multi-teacher distillation it enables learning unified student that surpasses all domain-specific teachers. We refer to this variant as ExOPD. (2) Moreover, in strong-to-weak distillation, replacing the students initial policy with the teachers pre-RL policy as the reference model can further boost the performance of ExOPD. Regarding future work, we believe it is practical to explore: (1) validating the generalizability of ExOPD on larger-scale models; (2) assessing the robustness of ExOPD in multi-teacher distillation with broader and more diverse set of domain teachers; and (3) evaluating the effectiveness of ExOPD for on-policy distillation across different model families."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. AI-MO. Aime 2024. https://huggingface.co/datasets/AI-MO/aimo-validation-aime, 2024. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharena.ai/. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 30293051, 2023. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=5h0qf7IBZZ. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, and Yankai Lin. Learning to focus: Causal attention distillation via gradient-guided token pruning. arXiv preprint arXiv:2506.07851, 2025b. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasonerzero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Jonas Hubotter, Frederike Lubeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, et al. Reinforcement learning via self-distillation. arXiv preprint arXiv:2601.20802, 2026. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. 13171327, 2016. Kun Liang, Clive Bai, Xin Xu, Chenming Tang, Sanwoo Lee, Weijie Liu, Saiyong Yang, and Yunfang Wu. Orbit: On-policy exploration-exploitation for controllable multi-budget reasoning. arXiv preprint arXiv:2601.08310, 2026. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, et al. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025a. 11 Jiacai Liu, Yingru Li, Yuqian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025b. URL https://richardli.xyz/ rl-collapse. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=1qvx610Cu7. Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, and Jianbin Jiao. Agentic reinforcement learning with implicit step rewards. arXiv preprint arXiv:2509.19199, 2025c. Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20251026. https://thinkingmachines.ai/blog/on-policy-distillation. OpenCompass. Aime 2025. https://huggingface.co/datasets/opencompass/AIME2025, 2025. Carlos Miguel Patino, Kashif Rasul, Quentin Gallouedec, Ben Burtenshaw, Sergio Paniego, Vaibhav Srivastav, Thibaud Frere, Ed Beeching, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Unlocking on-policy distillation for any model family, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Idan Shenfeld, Mehul Damani, Jonas Hubotter, and Pulkit Agrawal. Self-distillation enables continual learning. arXiv preprint arXiv:2601.19897, 2026. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, et al. Mimo-v2-flash technical report. arXiv preprint arXiv:2601.02780, 2026. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen. Deepcritic: Deliberate critique with large language models. arXiv preprint arXiv:2505.00662, 2025b. Wenkai Yang, Yankai Lin, Jie Zhou, and Ji-Rong Wen. Distilling rule-based knowledge into large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 913932, 2025c. Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, and Yankai Lin. Laser: Reinforcement learning with last-token self-rewarding. arXiv preprint arXiv:2510.14943, 2025d. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025e. Tianzhu Ye, Li Dong, Zewen Chi, Xun Wu, Shaohan Huang, and Furu Wei. Black-box on-policy distillation of large language models. arXiv preprint arXiv:2511.10643, 2025a. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025b. 12 Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, and Aditya Grover. Self-distilled reasoner: On-policy self-distillation for large language models. arXiv preprint arXiv:2601.18734, 2026. Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Model extrapolation expedites alignment. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10251041, 2025. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36: 5500655021, 2023."
        },
        {
            "title": "A Detailed Math Derivations",
            "content": "Here, we make mathematical derivations to calculate the expected gradients of OPD objective in Eq. (4)."
        },
        {
            "title": "Since",
            "content": "JOPD(θ) = min θ = min θ xD,yπθ(x) xD,yπθ(x) (cid:104) (cid:104) (cid:0)πθ(yx) (cid:13) (cid:13) π(yx)(cid:1)(cid:105)"
        },
        {
            "title": "DKL",
            "content": "log πθ(yx) log π(yx) (cid:105) ."
        },
        {
            "title": "We can get",
            "content": "θJOPD(θ) = θ (cid:104) log πθ(yx) log π(yx) (cid:105) xD,yπθ(x) (cid:104) = θ Ex πθ(yx)(cid:0) log πθ(yx) log π(yx)(cid:1)(cid:105) = Ex (cid:104) θ πθ(yx)(cid:0) log πθ(yx) log π(yx)(cid:1)(cid:105) = Ex (cid:104) (cid:0)θπθ(yx)(cid:1)(cid:0) log πθ(yx) log π(yx)(cid:1) + πθ(yx)θ log πθ(yx) (cid:105) . y"
        },
        {
            "title": "Notice that",
            "content": "(cid:104) Ex πθ(yx)θ log πθ(yx) (cid:105) = Ex = Ex = Ex = Ex = 0. (cid:105) θπθ(yx) πθ(yx) (cid:105) πθ(yx) θπθ(yx) (cid:104) (cid:104) (cid:104) (cid:104) θ πθ(yx) (cid:105) (cid:105) θ1 (15) (16) (17) Therefore, Eq. (16) can be reduced to θJOPD(θ) = Ex (cid:104) θπθ(yx)(cid:0) log πθ(yx) log π(yx)(cid:1)(cid:105) = Ex (cid:104) πθ(yx)θ log πθ(yx)(cid:0) log πθ(yx) log π(yx)(cid:1)(cid:105) = = xD,yπθ(x) xD,yπθ(x) (cid:104)(cid:0) log πθ(yx) log π(yx)(cid:1)θ log πθ(yx) (cid:104) t= ) log π(yt (cid:0) log πθ(yt x, y<t =1 x, y<t (cid:105) Now lets denote and consider each term = (cid:0) log πθ(yt (cid:104) xD,yπθ(x) ) log π(yt x, y<t )(cid:1), < t: where x, y<t θ log πθ(ytx, y<t) (cid:104) (cid:104) (cid:105) = Ex,y Eyt (cid:105) θ log πθ(ytx, y<t) Ex,y (cid:104) )(cid:1) θ log πθ(ytx, y<t) (cid:105) . (18) (19) θ log πθ(ytx, y<t)(cid:12) (cid:104) θ log πθ(ytx, y<t)(cid:12) (cid:12)x, y<t (cid:12)x, y<t (cid:105)(cid:105) (cid:105)(cid:105) Eyt ytπθ(x,y<t) (cid:2)θ log πθ(ytx, y<t)(cid:3)(cid:105) (cid:105) (cid:105) θπθ(ytx, y<t) πθ(ytx, y<t) yt θ yt (cid:105) θ1 (cid:104) (cid:104) (cid:104) (cid:104) (cid:104) t = Ex,y = Ex,y = Ex,y = Ex,y = Ex,y = 0. 14 Table 4: Training hyper-parameters of GRPO in math RL. Table 5: Training hyper-parameters of GRPO in code RL. Hyper-parameter Train Batch Size Micro Batch Size Rollout Maximum Prompt Length Maximum Response Length Temperature Top-p LR Optimization Steps KL Coefficient"
        },
        {
            "title": "Value",
            "content": "128 128 8 2048 16,384 1.0 1.0 1 106 500 0.0 Hyper-parameter Train Batch Size Micro Batch Size Rollout Maximum Prompt Length Maximum Response Length Temperature Top-p LR Optimization Steps KL Coefficient"
        },
        {
            "title": "Value",
            "content": "128 128 8 2048 8192 1.0 1.0 1 106 300 0.0 Table 6: Training hyper-parameters of G-OPD in both math and code domains. Table 7: Training hyper-parameters of SFT in both math and code domains. Hyper-parameter Batch Size Rollout Maximum Prompt Length Maximum Response Length Temperature Top-p LR"
        },
        {
            "title": "Value",
            "content": "1024 1 2048 16,384 1.0 1.0 1 105 Therefore, Eq. (18) can be reduced to Hyper-parameter Batch Size Maximum Sequence Length Warm-up Ratio LR"
        },
        {
            "title": "Value",
            "content": "1024 32,768 0.05 1 105 θJOPD(θ) = xD,yπθ(x) (cid:104) t=1 (cid:16) =t (cid:0) log πθ(yt x, y<t ) log π(yt x, y<t )(cid:1)(cid:17) θ log πθ(ytx, y<t) (cid:105) . (20) In practice, recent studies (Lu & Lab, 2025; Xiao et al., 2026) use discount factor of 0 and approximate the gradient as θJOPD(θ) = xD,yπθ(x) (cid:104) t=1 (cid:0) log πθ(ytx, y<t) log π(ytx, y<t)(cid:1) θ log πθ(ytx, y<t) (cid:105) . (21) Similarly, the approximated gradient of G-OPD in Eq. (11) can be written as θJG-OPD(θ) = xD,yπθ(x) (cid:104) t=1 AG-OPD θ log πθ(ytx, y<t) (cid:105) , (22) where AG-OPD = (cid:0) log πθ(ytx, y<t) log π(ytx, y<t)(cid:1) + (λ 1)(cid:0) log πref(ytx, y<t) log π(ytx, y<t)(cid:1)."
        },
        {
            "title": "B Detailed Training Settings",
            "content": "The training hyper-parameters in math and code RL training are put in Table 4 and Table 5 respectively. The training hyper-parameters in G-OPD in both domains are in Table 6. In preliminary experiments, we find that under the same prompt size rollout conditions, setting larger prompt size leads to smoother convergence. The number of optimization steps for G-OPD in all experiments with same-size teacher-student pairs (Section 4.1) is set to 50, while it is set to 100 for experiments in the strong-to-weak distillation setting (Section 4.2). We find that further increasing the number of distillation steps may degrade generalization performance due to overfitting. The training hyper-parameters in SFT are in Table 7. We make sure the number of trajectories to each problem generated by the teacher in SFT is consistent with that generated by the student in OPD and ExOPD. We keep the number of optimization steps consistent with the corresponding G-OPD experiment for fair comparison."
        },
        {
            "title": "C Prompt Templates",
            "content": "We show the prompt templates used in our experiments in the end. 16 Training and Evaluation Prompt Template for Math Reasoning < im start >user {question} Please reason step by step, and put your final answer within boxed{}.< im end > < im start >assistant Training and Evaluation Prompt Template for Code Generation < im start >user {question} Write Python code to solve the problem. Present the code in python Your code at the end. You need to think first then write the Python code.< im end > < im start >assistant"
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "LLM Department, Tencent"
    ]
}