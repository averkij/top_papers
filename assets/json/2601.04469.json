{
    "paper_title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
    "authors": [
        "Iaroslav Chelombitko",
        "Ekaterina Chelombitko",
        "Aleksey Komissarov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 9 6 4 4 0 . 1 0 6 2 : r SampoNLP: Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers Iaroslav Chelombitko DataSpike, aglabx Neapolis University Pafos Paphos, Cyprus i.chelombitko@nup.ac.cy Ekaterina Chelombitko DataSpike Dubai, UAE ekaterina@dataspike.io Aleksey Komissarov aglabx Paphos, Cyprus ad3002@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, corpus-free toolkit for morphological lexicon creation using MDLinspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct systematic evaluation of BPE tokenizers across range of vocabulary sizes (8k256k). We propose unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available1."
        },
        {
            "title": "Introduction",
            "content": "The performance of subword tokenization algorithms like Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is cornerstone of modern Natural Language Processing (NLP). While highly effective for many languages, their purely statistical nature poses significant challenge for morphologically rich, agglutinative languages (Bostrom and Durrett, 2020; Rust et al., 2021). In the Uralic family, group of languages known for its complex morphology and diverse linguistic phenomena (Hämäläinen, 1https://github.com/AragonerUA/SampoNLP 2019), words are often long concatenations of morphemes (e.g., Finnish talo-i-ssa-ni-ko-kaan - \"not in my houses either?\"). For such languages, the quality of tokenization is not just an engineering detail but critical factor that determines models ability to grasp grammatical structure and generalize effectively (Hämäläinen et al., 2021; Gerz et al., 2018). This raises pressing, yet under-explored, practical question, known to be challenge in Uralic NLP: What is the optimal tokenizer vocabulary size (k) to achieve robust morphological representation? The importance of this question was highlighted by recent work demonstrating the benefits of specialized tokenizers for these languages (Chelombitko and Komissarov, 2024). Addressing this question reveals more fundamental problem: the scarcity of high-purity morphological resources for evaluation. While lexical data is available in spell-checking dictionaries, their raw combination of stems and affixes results in noisy candidate list. Manual curation is not scalable, and established corpus-based methods like Morfessor (Creutz and Lagus, 2007) are illsuited for the many low-resource Uralic languages (Arkhangelskiy, 2019). To address this challenge, we present SampoNLP, toolkit based on corpus-free and selfreferential pipeline for refining morphological lexicons. The proposed method, \"MDL-inspired SelfReferential Atomicity Scoring,\" draws its theoretical motivation from the Minimum Description Length principle (Rissanen, 1978), but adapts it to type-only setting. The core algorithm iteratively estimates the atomicity of each candidate, distinguishing between simple and composite forms by analyzing internal structural patterns within the dataset itself. This lightweight and reproducible approach offers practical way to produce cleaner morphological resources, recognized need for data-scarce environments where traditional corpusbased methods are not viable (Hämäläinen, 2019). Having established robust methodology for resource creation, we leverage our generated lexicons to address the core problem of this paper: the vocabulary-morphology trade-off inherent in BPE tokenization (Bostrom and Durrett, 2020). We conducted systematic evaluation of BPE tokenizers for Finnish, Hungarian, and Estonian across vocabulary sizes from 8k to 256k. The development of novel evaluation frameworks that go beyond downstream performance is growing area of research (Chelombitko et al., 2024). In line with this, to precisely navigate the aforementioned trade-off, we introduce the Integrated Performance Score (IPS), single metric that balances Lexical Morpheme Coverage (LMC) against the Over-Split Rate (OSR). This allows us to model the performance curve and identify the optimal vocabulary range, providing principled answer to our central research question. Our contributions are thus twofold and equally significant: 1. Corpus-Free Morphological Method: We introduce fully automatic and reproducible pipeline for refining morphological lexicons without relying on corpus frequencies or external resources, released as an open-source toolkit, SampoNLP. 2. Quantitative Evaluation: We conduct systematic analysis of BPE tokenizers for Finnish, Estonian, and Hungarian, examining how vocabulary size affects morphological granularity through newly defined metrics of coverage and over-segmentation."
        },
        {
            "title": "2 Related Work",
            "content": "The evaluation and optimization of subword tokenization for morphologically rich languages intersects several research areas: subword tokenization algorithms, unsupervised morphological analysis, rule-based analyzers, and language-specific NLP for Uralic languages."
        },
        {
            "title": "2.1 Subword Tokenization and Morphology",
            "content": "Byte-Pair Encoding (BPE) (Sennrich et al., 2016) has become the de facto standard for subword tokenization in modern NLP. Alongside it, methods like the Unigram Language Model (Kudo, 2018) have been proposed, but the purely statistical nature of these approaches presents well-documented challenges for morphologically complex languages. The work of (Bostrom and Durrett, 2020) demonstrated that BPE tokenizers often fail to align with linguistic morpheme boundaries. Interestingly, parallel challenges in identifying meaningful subsequence units have been explored in domains beyond NLP, such as the tokenization of biological sequences like primate genomes (Popova et al., 2025). The question of optimal vocabulary size has often been guided by heuristics or evaluated indirectly via downstream task performance (Mielke et al., 2021). Our work directly addresses this gap by proposing methodology for intrinsic, morphologically-grounded evaluation to provide data-driven recommendations for Uralic languages. 2.2 Unsupervised Morphological Analysis The unsupervised discovery of morphological structure has rich history. One major family of approaches relies on statistical cues from corpora to identify boundaries. Classic methods such as Branching Entropy and Accessor Variety (Chen et al., 2004) analyze the predictability of subsequent characters to hypothesize morpheme breaks. Another prominent family of methods is based on the Minimum Description Length (MDL) principle. Morfessor (Creutz and Lagus, 2007) and its variants represent the canonical probabilistic approach, finding lexicon that best compresses text corpus. While successful, these methods are fundamentally corpus-based, requiring token frequency information that may not be available in low-resource settings. Our approach, while MDL-inspired, operates in corpus-free, type-only regime. It represents different paradigm: self-referential filtering of candidate list. By operating purely on the internal structure of candidate set, we provide lightweight method suited to resource-scarce scenarios, persistent challenge in Uralic NLP (Arkhangelskiy, 2019)."
        },
        {
            "title": "2.3 Rule-Based Analyzers and Tokenization",
            "content": "for Uralic Languages For Uralic languages, rule-based morphological analyzers built on Finite-State Transducers (FSTs) like Omorfi (Pirinen, 2015) and the GiellaLT2 infrastructure (Jauhiainen et al., 2020) are invaluable resources. While their generative outputs are linguistically comprehensive, they are not directly optimized for use as minimal reference morphemes lexicon. Our IMDP pipeline offers contrasting 2https://giellalt.github.io/ approach: data-driven methodology for distilling such lexicon from type-only candidate list, as can be extracted from dictionary-based resources like Hunspell, without requiring token frequencies from corpus. The challenge of effective tokenization for this language family has recently gained significant attention. Broader findings have established that language-specific modeling is crucial for morphologically rich languages, with studies on Finnish demonstrating clear benefits of monolingual models like FinBERT over multilingual ones (Virtanen et al., 2019). Building on this principle, recent study by (Chelombitko and Komissarov, 2024) specifically addressed the severe underrepresentation of Uralic languages in large multilingual models. They demonstrated that training specialized, large-vocabulary monolingual tokenizers yields substantial improvements in compression efficiency. However, while establishing the need for specialized resources, their work left the question of how to determine an optimal vocabulary size open for future investigation. Concurrently, the need for better evaluation metrics has become prominent research topic. The Qtok framework (Chelombitko et al., 2024), for instance, proposed comprehensive approach to evaluating multilingual tokenizer quality, while other studies have also advocated for moving beyond downstream task performance towards more intrinsic, linguistically-informed measures (Beinborn and Pinter, 2023). Our Integrated Performance Score (IPS) directly addresses this call from the community for more morphologically-grounded metrics. Our current work builds on these foundations. It utilizes similar high-quality data sources as those in (Chelombitko and Komissarov, 2024) to train the tokenizers being evaluated. Furthermore, by proposing concrete methodology, it answers the call for better evaluation and finds the optimal vocabulary sizes that the former study alluded to, thus providing logical next step in this line of research."
        },
        {
            "title": "3 Methodology. The IMDP Pipeline",
            "content": "To create high-purity morpheme lexicon from noisy, raw list of candidate forms, we propose the Iterative Morphological Decomposition Pipeline (IMDP). Our approach is designed to be fully automatic and operates in corpus-free, type-only regime, requiring only the candidate list as input. The core of the pipeline is method we term \"MDL-inspired Self-Referential Atomicity Scoring,\" which iteratively evaluates how \"fundamental\" each candidate is relative to the entire set. The entire process is visualized in Figure 1. The pipeline consists of three main stages: (1) Prefiltering and Initial Scoring, (2) Iterative Score Refinement, and (3) Final Filtering via Automated Thresholding. 3.1 Stage 1: Candidate Pre-filtering and Initial Scoring This initial stage aims to drastically reduce nonlinguistic noise and establish baseline score for each plausible candidate. 3.1.1 Hard Pre-filtering First, we apply series of deterministic filters to the raw input list Craw. token Craw is discarded if it: 1. Contains symbols from non-target script (e.g., Cyrillic in Latin-based list). We define valid character set Σ for each language (e.g., [a-záéíóöoúüu] for Hungarian). 2. Contains any non-alphabetic characters (e.g., numbers, punctuation, URLs), excluding initial/final hyphens used to mark affixes. 3. Is proper noun or acronym (heuristic: starts with capital letter or consists of multiple uppercase letters). 4. Is excessively long (t > 30) or too short (t < min_length), unless is single character present in language-specific whitelist of valid one-character morphemes W."
        },
        {
            "title": "3.1.2 Type-support Filtering",
            "content": "To filter out typographical errors and other singleton noise, we apply \"type-support\" criterion to the remaining set of candidates C. candidate is kept only if it appears as substring in at least other unique candidates in C. This ensures that we only consider patterns that are structurally recurrent within the dataset itself. support(t) = {c Ct is substring of c} We retain if support(t) (we use = 3). The resulting set is our final candidate pool C. Figure 1: An overview of the Iterative Morphological Decomposition Pipeline (IMDP)."
        },
        {
            "title": "3.2 Stage 2: Iterative Score Refinement",
            "content": "Sk+1(t) = Initial Atomicity Scoring 3.1.3 Each surviving candidate is assigned an initial Atomicity Score S0(t). This score is based on the MDL-inspired principle that, all else being equal, shorter forms are more likely to be fundamental morphemic units. The score is defined as the inverse of the tokens length: S0(t) = 1 , where is the number of characters in t. This is the core of our method. We iteratively refine the Atomicity Scores until they converge. In each iteration + 1, the score of every token is re-calculated based on its \"explainability\" by other tokens in the set."
        },
        {
            "title": "3.2.1 Optimal Decomposition and Best\nExplanation Power (BEP)",
            "content": "For each token t, we find its optimal decomposition into sequence of smaller tokens (m1, m2, ..., mn) where each mi C. The optimal decomposition is the one that maximizes the sum of the scores of its constituents (taken from the previous iteration, Sk). We find this maximum sum using dynamic programming algorithm and term it the Best Explanation Power, BEPk(t). BEPk(t) = max t=m1mn n2 (cid:88) i=1 Sk(mi). The search space for decompositions is constrained by two rules: 1. Multi-component: The algorithm considers segmentations into any number of parts, not just two. 2. Degeneracy Prevention: Segments of length 1 are only considered if they are in the whitelist . 3.2.2 Score Update Rule The new score Sk+1(t) is calculated by comparing the tokens own score with its explainability. token is penalized only if the \"evidence\" for it being composite (BEPk(t)) is stronger than the evidence for it being an atom (Sk(t)). Sk(t), S0(t) 1 + BEPk(t) , if BEPk(t) Sk(t), if BEPk(t) > Sk(t). This update rule creates competitive dynamic where atomic morphemes retain high scores, while composite words are iteratively penalized towards zero."
        },
        {
            "title": "3.2.3 Convergence\nThe iterative process continues until the system\nreaches a stable state. We define convergence as the\npoint where the maximum absolute change in any\ntoken’s score between two consecutive iterations\nfalls below a small threshold",
            "content": "(cid:12)Sk+1(t) Sk(t)(cid:12) (cid:12) (cid:12) < ε max tC We use ε = 1e 7 and safeguard limit of max_iterations = 100."
        },
        {
            "title": "Thresholding",
            "content": "After the scores converge, the final distribution of scores typically shows heavy concentration of composite candidates at very low scores, while atomic candidates retain higher scores. To automatically and reproducibly determine separation threshold between these groups, we employ Otsus method (Otsu, 1979). Originally developed for image processing to separate foreground from background, this algorithm finds an optimal threshold τ for distribution by maximizing the inter-class variance between the two resulting classes (in our case, \"atomic\" vs. \"composite\"). This data-driven approach avoids manual parameter tuning and adapts to the specific score distribution of each dataset. All tokens with final score Sf inal(t) >= τ are classified as atomic and form our final, highpurity morpheme lexicon. Lang Fin Est Hung Initial Atomic Reduct Reduct Cands Morphs Factor 99.23% 129.8x 3,850 97.97% 49.3x 5,705 96.91% 32.4x 3, 499,647 281,256 103,317 % Table 1: Efficiency of the IMDP pipeline in cleaning and reducing morpheme candidate lists."
        },
        {
            "title": "4 Experimental Setup",
            "content": "To evaluate the impact of vocabulary size on morphological coverage, we conducted systematic analysis for three Uralic languages: Finnish, Hungarian, and Estonian. Our experimental setup consists of three main stages: creating the reference morphemes, training the tokenizers, and defining the evaluation metrics."
        },
        {
            "title": "4.1 Data",
            "content": "Our methodology requires two types of data for each language: raw list of morpheme candidates for cleaning and large text corpus for tokenizer training. 1. Morpheme Candidate Lists: The initial \"dirty\" lists of candidates were constructed from authoritative, open-source spellchecking dictionaries based on the Hunspell framework3. For Hungarian and Estonian, we utilized the comprehensive dictionaries curated by The LibreOffice Project4. For Finnish, which requires special handling of compounds, we used the dedicated dictionary from the hunspell-fi project5. For each language, the full set of unique stems (from.dicfiles) and affixes (from.afffiles) was merged to create comprehensive but structurally noisy candidate list, which serves as the input to our IMDP pipeline. This approach 3https://hunspell.github.io/ 4https://github.com/LibreOffice/dictionaries 5https://github.com/fginter/hunspell-fi of leveraging widely available dictionary resources provides practical starting point for morphological analysis. 2. Text Corpora: For training the BPE tokenizers, we used large, pre-processed corpora derived from Wikipedia snapshots6. Our choice of data source and preprocessing methodology aligns with previous work on creating specialized Uralic tokenizers (Chelombitko and Komissarov, 2024), ensuring comparable basis for our analysis. It is critical to emphasize that these corpora were used exclusively for training the BPE tokenizers and were not used in any stage of our morpheme list refinement pipeline, thus preserving the corpus-free nature of the IMDP method. 4.2 Reference Lexicon Creation For each of the three languages, we applied our Iterative Morphological Decomposition Pipeline (IMDP), as described in Section 3, to the corresponding raw candidate list. The pipeline was configured with the following parameters: minimum morpheme length min_length = 1, minimum type-support = 3, and convergence threshold ε = 1e 7. The process was run until convergence. The final filtering was performed using the automatically determined Otsu threshold (Otsu, 1979). This procedure yielded three high-purity reference morpheme lexicons (Gf in, Ghun, Gest), the statistics of which are summarized in Table 1."
        },
        {
            "title": "4.4 Evaluation Metrics",
            "content": "To provide nuanced and rigorous evaluation of tokenizer quality, we must account for the fundamental trade-off between morphological coverage 6https://dumps.wikimedia.org 7https://github.com/huggingface/tokenizers 4) clearly show the performance profile for each language. Supplementary details on the component metrics (LMC and OSR) available in Figures 2 and 3. and over-segmentation. tokenizer that perfectly represents all morphemes (high coverage) but also excessively splits common words is not optimal. To capture this balance in single, unified score, we introduce the Integrated Performance Score (IPS). The IPS models this trade-off geometrically. We consider 2D space where the ideal tokenizer resides at the point (Coverage=1, OverSplit=0). The IPS of any real tokenizer is its normalized Euclidean distance from this ideal point, scaled to [0, 1] range where 1 is perfect. First, we define the two core components: 1. Lexical Morpheme Coverage (LMC): The fraction of atomic morphemes from our reference lexicon that are perfectly represented as single token in the tokenizers vocabulary Vk. This measures the tokenizers lexical \"knowledge\" of fundamental morphological units. LMC = (cid:12){ Vk }(cid:12) (cid:12) (cid:12) . Figure 2: Lexical Morpheme Coverage (LMC) across different vocabulary sizes (k). LMC represents the percentage of reference morphemes found as single, complete tokens in the tokenizers vocabulary. 2. Over-split Rate (OSR): The fraction of morphemes from that the tokenizer fails to represent as single tokens, thus always splitting them into multiple pieces. (cid:12) (cid:110) (cid:12) (cid:12) OSR = (cid:12) (cid:12) (cid:12) occurs in 1 word never as single token { in 1 word} (cid:111)(cid:12) (cid:12) (cid:12) . From these, the Integrated Performance Score (IPS) is calculated as: IP = 1 ( (1LM C)2+OSR2 2 ) This single metric allows for clear and direct comparison of tokenizers across different vocabulary sizes. higher IPS indicates better balance between representing morphemes and avoiding excessive fragmentation. Our final analysis of optimal vocabulary sizes is based on identifying the \"elbow point\" on the IPS vs. vocabulary size curve."
        },
        {
            "title": "5 Results and Analysis",
            "content": "Our experiment yielded clear and significant patterns regarding the relationship between tokenizer vocabulary size and morphological performance. To capture the fundamental trade-off between coverage and over-segmentation, we analyzed the Integrated Performance Score (IPS) for each language. The resulting IPS curves for Estonian (Figure 5), Finnish (Figure 6), and Hungarian (Figure Figure 3: Over-Split Rate (OSR) as function of vocabulary size (k). OSR denotes the fraction of reference morphemes that occur in words but never appear as single token in any tokenization."
        },
        {
            "title": "Profile",
            "content": "The IPS curves for all three languages exhibit classic logarithmic growth pattern, demonstrating the law of diminishing returns. The score increases rapidly for smaller vocabulary sizes, indicating that initial additions to the vocabulary are highly efficient at capturing morphological structure. However, the rate of improvement progressively slows, showing that ever-larger vocabularies provide only marginal gains at significant cost to model size. This confirms that \"sweet spot\" or an optimal range exists for each language. 5.2 Cross-Linguistic Analysis: Three Distinct Performance Tiers The results reveal three distinct performance tiers, highlighting the varying degrees to which standard BPE can model the morphology of these languages. 1. Hungarian (hu): As shown in Figure 4, Hungarian demonstrates by far the best performance. Its IPS curve starts at 0.29 and rises sharply, reaching maximum of 0.73. This high score suggests that BPE is reasonably effective at learning the statistical regularities of Hungarian morphology. 2. Estonian (et): Estonian occupies the middle tier, with its IPS curve depicted in Figure 5. The score starts at 0.22 and reaches maximum of 0.39. While better than Finnish, this score indicates that less than 40% of the \"ideal\" tokenizer performance is achieved, even with large vocabulary. 3. Finnish (fi): Figure 6 illustrates the most challenging profile for Finnish. With maximum IPS of only 0.31, the results quantitatively demonstrate that standard BPE is fundamentally ill-suited for capturing the complexities of Finnish morphology. (k_elbow), identified by the Kneedle algorithm (Satopää et al., 2011), which marks the point of diminishing returns. The upper bound is the 90% quality point (k_q90), where 90% of the maximum observed IPS is achieved. As shown in Figures 4, 6, 5, and summarized in Table 2, this analysis leads to the following recommendations: 1. Hungarian (hu): The IPS curve for Hungarian (Figure 4) shows clear optimal range between k=80,000 and k=128,000. The elbow is found at 80k, and 90% of the maximum performance is reached at 128k. As visualized on the plot, expanding the vocabulary beyond this range yields only minimal performance gains. 2. Estonian (et): For Estonian (Figure 5), the recommended range is also k=80,000 to k=128,000. Similar to Hungarian, the elbow is at 80k and the 90% quality mark is at 128k, establishing this as the zone of best compromise between performance and size. 3. Finnish (fi): The analysis for Finnish (Figure 6) indicates need for larger vocabulary. The elbow is at k=80,000, but to achieve 90% of the (albeit low) maximum performance, vocabulary of k=150,000 is required. This suggests that for Finnish, the optimal range is k=80,000 to k=150,000, reflecting the languages high morphological complexity. Figure 4: IPS vs. vocabulary size (k) for Hungarian. Hungarian shows the most consistent improvement in IPS, reflecting its comparatively transparent agglutinative structure with fewer morphophonological alternations. The elbow point is at 80k, and the 90% quality threshold at 128k, yielding recommended range of 80k128k. 5.3 Identifying the Optimal Vocabulary Range (k*) To determine practical and effective vocabulary size, we define recommended range for k*. The lower bound of this range is the \"elbow\" point Figure 5: IPS vs. vocabulary size (k) for Estonian. While the overall pattern of diminishing returns is similar to Hungarian, the lower IPS plateau indicates reduced learnability due to Estonians extensive morphophonological alternations, which obscure orthographic morpheme boundaries. The recommended range remains 80k128k. These findings provide quantitative foundation for the critical decision of vocabulary sizing, transforming it from heuristic-based choice into principled optimization problem. Complete numerical Lang Hung Est Fin Max Gain Point (k_gain) 40,000 16,000 64,000 Elbow Point (k_elbow) 80,000 80,000 80,000 90% Quality Point (k_q90) 128,000 128,000 150,000 Recommend k* Range 80k 128k 80k 128k 80k 150k Table 2: Key points on the IPS curve for determining the optimal vocabulary range. results for all evaluated vocabulary sizes are provided in Appendix  (Table 3)  for reference."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we addressed the dual challenge of creating high-purity morphological resources in corpus-free setting and using them to evaluate subword tokenizers for Uralic languages. We introduced SampoNLP, toolkit featuring novel pipeline based on \"MDL-inspired Self-Referential Atomicity Scoring,\" which successfully refines noisy candidate lists into clean morpheme lexicons. Applying these lexicons, our systematic evaluation of BPE tokenizers yielded two key findings. First, we provide an empirically-grounded recommendations for optimal vocabulary sizes, identifying range of 80k-128k for Hungarian and Estonian, and 80k-150k for Finnish, as the most effective trade-off between performance and model size. Second, our results quantitatively demonstrate the severe limitations of standard BPE for highly agglutinative languages like Finnish, where performance plateaus at strikingly low level. This study confirms that while vocabulary size optimization is crucial step, it is not panacea. We release our SampoNLP library and the generated morpheme lists to the community to facilitate reproducible research and encourage the development of more morphologically-aware tokenization methods for the Uralic language family."
        },
        {
            "title": "Discussion",
            "content": "Our results yield two key insights. First, the effectiveness of BPE varies dramatically by language: while Hungarian achieves high IPS (max 0.73), the low scores for Finnish (0.31) and Estonian (0.39) quantitatively demonstrate the algorithms fundamental limitations for these highly agglutinative languages. Second, for all languages, an empirically identifiable \"sweet spot\" for vocabulary size exists, beyond which performance gains diminish. Here, optimality is understood as morphological sufficiency - the point at which the tokenizer capFigure 6: IPS vs. vocabulary size (k) for Finnish. Finnish exhibits the lowest IPS plateau, consistent with its rich system of consonant gradation and stem alternations, which make orthographic segmentation less stable for BPE. The elbow is at 80k, while 90% of the maximum IPS is reached at 150k, suggesting recommended range of 80k150k. tures the productive structure of language with minimal redundancy. This notion is intrinsic by design, offering language-level criterion rather than task-specific optimization. We acknowledge the limitations of our approach. The IPS metric abstracts away qualitative segmentation differences - necessary compromise for scalability. Our use of clean, standardized corpora also isolates the variable of vocabulary size but does not reflect the noise of real-world data. These aspects represent clear avenues for future work. While our method produces refined set of recurrent sub-lexical units, we do not claim full linguistic morpheme correctness. The IMDP segmentation is orthographic and self-referential in nature, providing practical approximation rather than phonologically grounded morphological analysis. In conclusion, our findings suggest that while optimizing k* is crucial step, it may be insufficient for languages like Finnish. The low performance ceiling for BPE underscores the need for morphologically-aware tokenization methods. We believe our SampoNLP toolkit and the generated lexicons provide the community with reproducible benchmark to develop and test such new strategies."
        },
        {
            "title": "References",
            "content": "Timofey Arkhangelskiy. 2019. Corpora of social media in minority Uralic languages. In Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages, pages 125140, Tartu, Estonia. Association for Computational Linguistics. Lisa Beinborn and Yuval Pinter. 2023. Analyzing cognitive plausibility of subword tokenization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 44784486, Singapore. Association for Computational Linguistics. Kaj Bostrom and Greg Durrett. 2020. Byte pair encoding is suboptimal for language model pretraining. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 46174624, Online. Association for Computational Linguistics. Iaroslav Chelombitko and Aleksey Komissarov. 2024. Specialized monolingual BPE tokenizers for Uralic languages representation in large language models. In Proceedings of the 9th International Workshop on Computational Linguistics for Uralic Languages, pages 8995, Helsinki, Finland. Association for Computational Linguistics. Iaroslav Chelombitko, Egor Safronov, and Aleksey Komissarov. 2024. Qtok: comprehensive framework for evaluating multilingual tokenizer quality in large language models. Preprint, arXiv:2410.12989. Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for chinese word extraction. Computational Linguistics, 30:7593. Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Trans. Speech Lang. Process., 4(1). Daniela Gerz, Ivan Vulic, Edoardo Ponti, Jason Naradowsky, Roi Reichart, and Anna Korhonen. 2018. Language modeling for morphologically rich languages: Character-aware modeling for word-level prediction. Transactions of the Association for Computational Linguistics, 6:451465. Mika Hämäläinen, Niko Partanen, Jack Rueter, and Khalid Alnajjar. 2021. Neural morphology dataset and models for multiple languages, from the large to the endangered. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 166177, Reykjavik, Iceland (Online). Linköping University Electronic Press, Sweden. Mika Hämäläinen. 2019. Uralicnlp: An nlp library for uralic languages. Journal of Open Source Software, 4(37). T. Jauhiainen, Krister Linden, Niko Partanen, and . . . . 2020. Uralic language identification (uli) 2020 shared task: Wanca 2017 web corpora for uralic languages. Proceedings of the VarDial Workshop at LREC 2020. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6675, Melbourne, Australia. Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 6671, Brussels, Belgium. Association for Computational Linguistics. Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, and Samson Tan. 2021. Between words and characters: brief history of open-vocabulary modeling and tokenization in nlp. Preprint, arXiv:2112.10508. Nobuyuki Otsu. 1979. threshold selection method from gray-level histograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):6266. Tommi Pirinen. 2015. Omorfi free and open source morphological lexical database for Finnish. In Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015), pages 313 315, Vilnius, Lithuania. Linköping University Electronic Press, Sweden. Marina Popova, Iaroslav Chelombitko, and Aleksey Komissarov. 2025. When repeats drive the vocabulary: byte-pair encoding analysis of t2t primate genomes. Preprint, arXiv:2505.08918. Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14(5):465471. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. 2021. How good is your tokenizer? on the monolingual performance of multilingual language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 31183135, Online. Association for Computational Linguistics. Ville Satopää, Joshua Albrecht, David Irwin, and Barath Raghavan. 2011. Finding kneedle in haystack: Detecting knee points in system behavior. In Proceedings of the 31st International Conference on Distributed Computing Systems Workshops, pages 166171. IEEE. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics. Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo. 2019. Multilingual is not enough: Bert for finnish. Preprint, arXiv:1912.07076."
        },
        {
            "title": "A Appendix",
            "content": "Language Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Estonian Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Finnish Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian Vocabulary Size (k) 8,000 16,000 32,000 40,000 50,000 64,000 80,000 100,000 128,000 150,000 180,000 200,000 220,000 240,000 256,000 8,000 16,000 32,000 40,000 50,000 64,000 80,000 100,000 128,000 150,000 180,000 200,000 220,000 240,000 256,000 8,000 16,000 32,000 40,000 50,000 64,000 80,000 100,000 128,000 150,000 180,000 200,000 220,000 240,000 256,000 Total Morphemes 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 5,705 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,850 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 3,189 Morpheme Coverage % 11.27% 13.71% 16.49% 17.48% 18.18% 19.30% 20.68% 21.74% 22.99% 23.79% 24.78% 25.38% 25.74% 26.23% 26.81% 7.85% 9.76% 12.20% 12.95% 13.95% 15.53% 16.63% 17.84% 19.11% 20.21% 21.51% 22.16% 22.93% 23.38% 23.73% 25.15% 34.34% 45.03% 49.23% 52.46% 56.98% 60.90% 64.88% 69.24% 71.97% 74.29% 75.67% 77.08% 78.43% 79.12% Over-Split Rate % 65.79% 59.13% 53.65% 51.98% 51.43% 50.32% 49.13% 48.49% 47.86% 47.46% 47.06% 46.43% 46.19% 46.19% 46.11% 78.96% 74.37% 69.13% 68.04% 66.62% 64.73% 63.36% 62.46% 61.61% 61.18% 60.52% 60.28% 60.05% 59.95% 59.81% 67.72% 57.01% 46.14% 42.17% 39.97% 37.84% 35.72% 34.33% 33.27% 32.37% 32.28% 32.16% 32.00% 31.92% 32.04% Table 3: Detailed experimental results for BPE tokenizers of varying vocabulary sizes across three Uralic languages. Morpheme Coverage represents the percentage of reference morphemes found in the vocabulary (LMC). Over-Split Rate is the percentage of reference morphemes with support in that never appear as single token in any tokenization."
        }
    ],
    "affiliations": [
        "DataSpike",
        "Neapolis University Pafos",
        "aglabx"
    ]
}