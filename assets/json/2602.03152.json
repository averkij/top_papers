{
    "paper_title": "FASA: Frequency-aware Sparse Attention",
    "authors": [
        "Yifei Wang",
        "Yueqi Wang",
        "Zhenrui Yue",
        "Huimin Zeng",
        "Yong Wang",
        "Ismini Lourentzou",
        "Zhengzhong Tu",
        "Xiangxiang Chu",
        "Julian McAuley"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\\times$ speedup using just 18.9\\% of the cache on AIME24."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . [ 1 2 5 1 3 0 . 2 0 6 2 : r Published as conference paper at ICLR 2026 FASA: FREQUENCY-AWARE SPARSE ATTENTION Yifei Wang1 Yueqi Wang2 Zhenrui Yue3 Huimin Zeng3 Yong Wang1 Julian McAuley2 Ismini Lourentzou3 Zhengzhong Tu4 Xiangxiang Chu1 1 AMAP, Alibaba Group 2 UCSD 3 UIUC 4 Texas A&M University"
        },
        {
            "title": "ABSTRACT",
            "content": "The deployment of Large Language Models (LLMs) faces critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides robust and computationally free proxy for identifying salient tokens. Building on this insight, FASA first identifies critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. Across spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100% of full-KV performance when only keeping 256 tokens, and achieves 2.56 speedup using just 18.9% of the cache on AIME24."
        },
        {
            "title": "INTRODUCTION",
            "content": "Despite recent advances in Large Language Models (Dao et al., 2022; Ainslie et al., 2023; Liu et al., 2024a; Wang et al., 2025c) in long-context processing, requirements such as repository-level code analysis (Chen et al., 2021; Shi et al., 2025; Wang et al., 2026; Chen et al., 2025b) and document summarization (Goyal & Durrett, 2020) pose both memory and computational challenges, especially the linear growth of the KV cache. As the sequences grow, each token generation requires accessing the entire KV cache, leading to increased memory I/O latency. This memory-bound process underutilizes high-performance GPUs, ultimately limiting the overall throughput. To optimize KV cache management, previous studies have proposed mainly five directions: token eviction (Akhauri et al., 2025; Fang et al., 2025), low-rank compression (Chang et al., 2025; Singhania et al., 2024; Zhang et al., 2025), quantization (Hooper et al., 2025b; Liu et al., 2024d), KV merging (Wang et al., 2025d; Wan et al., 2025; Liu et al., 2024b), and budget allocation (Cai et al., 2025b). Among these, an intuitive and widely explored approach is token eviction (LI et al., 2025; Liu et al., 2023). The rationale is that only small subset of tokens contributes significantly to outputs, enabling the selective removal of trivial ones. Existing token eviction methods can be classified into three types: (1) Static strategies remove tokens with fixed rules (Xiao et al., 2024), therefore risking irreversible information loss; (2) Adaptive strategies either permanently evict less critical tokens (Zhang et al., 2023; Li et al., 2024) or preserve the full cache while retrieving subset of entries (Tang et al., 2024; Ge et al., 2024). Yet such heuristic rankings provide an imperfect proxy for the truly dynamic nature of token importance; (3) Learning-based strategies (Akhauri et al., 2025; Yang et al., 2025; Chen et al., 2025a) rely on trained token predictor, suffering from poor generalization on different datasets. Can token predictor achieve query-awareness without resorting to costly training? Project lead and corresponding author. 1 Published as conference paper at ICLR 2026 In response to this question, we introduce FASA (Frequency-Aware Sparse Attention), trainingfree, high-granularity, query-aware predictor designed to evaluate token significance during the decoding phase, in training-free manner. The design of FASA is rooted in an intriguing observation that differential frequencies within RoPE (Su et al., 2023) induce functional sparsity among frequency chunks (FCs). Only sparse subset of FCs, termed as dominant FCs, contribute significantly to contextual awareness, while others construct robust positional patterns. We empirically verify that these dominant FCs are sparse, universal, and task-agnostic in Section 3.3, thereby providing robust foundation for accurately predicting token importance. Building upon this insight, FASA employs two-stage framework for efficient inference. The first stage, Token Importance Prediction, harnesses dominant FCs to dynamically estimate attention scores, obtaining critical tokens. At the second stage, Focused Attention Computation then performs precise and focused token generation on this reduced set. The overhead of FASA is minimal because the identification of dominant FCs is one-time and taskinvariant process. Ultimately, FASA achieves high efficiency by fetching only small fraction of the KV cache, which significantly reduces the data transferred between memory and the processor and thereby lowers memory bandwidth consumption. The overview of FASA is in Figure 2. Grounded on the same principles above, we introduce two variants of FASA: FASA-M and FASA-C. While they differ in implementation strategies, both achieve equivalent downstream task performance while offering different efficiency profiles, specializing in memory and computation, respectively. Crucially, despite FASA leverages low-rank subspace, its primary objective is the dynamic prediction of token importance, not mere dimensionality reduction. This design makes FASA orthogonal to and compatible with most other KV cache compression methods. For example, it can be seamlessly integrated with layer-wise budget allocation schemes like PyramidKV (Cai et al., 2025b). We evaluated FASA across range of LLMs with varying KV cache budgets, concentrating on three core tasks: long-context benchmark, long-sequence modeling, and long chain-of-thought (LongCoT) reasoning. Our method achieves performance comparable to that of full KV cache, with reduction of less than 0.7%, while consistently surpassing all baseline methods across these tasks. FASA-M provides an 8 compression of the KV cache, substantially optimizing memory usage. and FASAC delivers 2.6 speedups, enhancing computational efficiency, with 25% of FCs selected. Our contributions are summarized as follows: We are the first to uncover an intriguing finding: functional sparsity at FC-level induced by RoPE. Leveraging the functional sparsity of FCs, we introduce FASA, training-free framework for dynamically predicting token importance. We present two variants of FASA: FASA-M, optimized for settings with memory constraints, and FASA-C, designed for scenarios with computational constraints. Extensive experiments across three paradigm tasks demonstrate that FASA consistently achieves near-oracle accuracy in both long-context and long-generation tasks."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Token Eviction. central theme in recent KV cache optimization (Hooper et al., 2025a; Wang et al., 2025a) is the exploitation of inherent, query-dependent attention sparsity (Liu et al., 2024c; 2025a; Behnam et al., 2025). Stream (Xiao et al., 2024) employs rigid heuristic, preserving only initial and recent tokens, which invariably discards potentially crucial information from intermediate positions. SnapKV (Li et al., 2024) improves on this by introducing one-time, prefill-stage filtering based on empirically estimated attention scores. However, the static nature of this estimation cannot adapt to the evolving relevance of tokens as generation progresses. Quest (Tang et al., 2024) offers more dynamic solution by organizing the KV cache into pages and selectively fetching them. Despite its dynamism, its efficacy is hampered by coarse, page-level granularity, which incurs significant overhead by forcing the retrieval of entire pages even when only few tokens are needed. Low-rank Compression. Another prominent paradigm for KV cache compression is low-rank approximation (Zhang et al., 2025; Dong et al., 2024), predicated on the observation that the caches information content is concentrated in low-dimensional subspace (Sun et al., 2025; sax, 2024; Behnam et al., 2025). For instance, SparQ (Ribar et al., 2024) employs heuristic that selects key dimensions based on high query-vector magnitudes, strategy that proves suboptimal due to its head-agnostic nature and its simplistic reliance on magnitude as proxy for importance. Similarly, 2 Published as conference paper at ICLR 2026 LoKi (Singhania et al., 2024) leverages Principal Component Analysis (PCA) to project key states into compact subspace for efficient computation, but at the cost of significant memory overhead from storing the requisite projection matrices. In contrast, our proposed FASA circumvents these limitations by operating in-place on the KV cache, thereby incurring no auxiliary memory overhead."
        },
        {
            "title": "3.1 PRELIMINARY: ROTARY POSITIONAL ENCODINGS (ROPE)",
            "content": "RoPE embeds relative position information into the self-attention computation. Specifically, for query vector qt1 and key vector kt2 at positions t1 and t2, the attention score is formulated as At1,t2 = (qt1Rt1)(kt2 Rt2) = qt1Rtk t2. Due to the orthogonality, the product of Rt1 and Rt2 elegantly simplifies to single rotation matrix parameterized solely by the relative offset = t1 t2. Frequency-Chunk Perspective on RoPE. From frequency-domain perspective, the RoPE mechanism can be interpreted through the concept of frequency chunks (FCs). This framework posits that any d-dimensional vector Rd (e.g., query and key) is partitioned into d/2 orthogonal 2D subspaces. We denote the i-th such subspace, or FC, as v[i] = (v2i, v2i+1)T . Each FC is associated with unique base angular frequency, calculated as θi = B2(i1)/d for {1, . . . , d/2}, where is predefined frequency base. This design establishes direct mapping from chunks dimensional indices (2i, 2i + 1) to its rotational frequency. Lower dimension indices (i) result in higher frequencies, which implies that the corresponding FCs rotate very quickly physically. For token at absolute position m, its i-th FC is rotated by an angle mθi through specific 2 2 rotation matrix Rm,θi. The global rotation matrix Rt is block-diagonal, where each diagonal block is 2 2 rotation matrix Rt,θi and defined as Rt = Diag(Rt,θ1, Rt,θ2, . . . , Rt,θd/2) = (cid:76)d/2 i=1 Rt,θi . vm = d/2 (cid:77) k=1 v[i] = d/2 (cid:77) k=1 (v2i, v2i+1)T , Rm,θi = (cid:18)cos(mθi) sin(mθi) cos(mθi) sin(mθi) (cid:19) . (1) 3.2 MOTIVATION AND HYPOTHESIS Position vs. Semantics: Different Roles of FCs. The varying rotational velocities across FCs inherently lead to functional heterogeneity. This principle is substantiated by two key observations from prior literature. First, distinct division of labor exists within RoPE (Barbero et al., 2025; Wei et al., 2025), where high-frequency FCs (in low dimensions) are primarily responsible for constructing robust positional patterns, and in contrast, low-frequency counterparts specialize in carrying the semantic information and model long-range dependencies. Second, this functional specialization is structurally reflected by RoPE-induced concentration of high-magnitude values within specific query and key dimensions (Sun et al., 2024), reinforcing the non-uniform functional importance of FCs. This functional heterogeneity suggests that FCs can be grouped into two distinct categories: 1. Contextual FCs: small, critical subset responsible for dynamic, context-specific attention. These FCs identify which tokens are semantically relevant to the current query. 2. Structural FCs: The remaining majority primarily injects inherent, positional attention patterns, mainly recency bias (Peysakhovich & Lerer, 2023) and attention sinks (Xiao et al., 2024). Hypothesis: The models contextual awareness is overwhelmingly driven by the Contextual FCs. few contextual FCs could replicate the contextual selection behavior of full attention head. If their index set is denoted as Idom {1, . . . , d/2}, the full attention dot product can be effectively approximated by summing only over Idom, namely At1,t2 = qt1 RtkT t2 t1 Rt,θi k[i] q[i] (cid:80) . iIdom 3.3 QUANTIFYING FUNCTIONAL SPARSITY Quantifying our hypothesis of FC-level functional sparsity requires metric to assess the dominance of individual FCs. Therefore, we propose the Contextual Agreement (CA) metric, which measures the alignment between the attention pattern from single FC and that of the full attention head. Formal Setup. For query qt Rd and key matrix K1:t Rdt in an attention head (l, h), we define two raw score vectors: the standard full-head scores αl,h and the single-FC scores α(i) l,h. The Published as conference paper at ICLR 2026 Figure 1: Functional sparsity of FCs revealed by Contextual Agreement (CA) heatmaps. Each heatmap shows CA per FC (x-axis) across all heads (y-axis). few dominant FCs (bright vertical bands) consistently capture contextual information across attention heads. Results on Qasper (K = 256); see Appendix A. latter are computed using only the 2D components of the i-th FC. These are expressed as: αl,h(qt, K1:t) = [qt Rt1 (k0)T , , qt R0 (kt)T ]T R0,θi k[i] Rt1,θi k[i] α(i) , quantifies the agreement between the full-head αl,h and , , q[i] ]T (3) (2) 0 l,h scores by measuring the normalized intersection of their top-K token index sets: (qt, K1:t) = [TopK-I(αl,h(qt, K1:t), K) TopK-I(α(i) l,h(qt, K1:t), K)]/K, (4) l,h(qt, K1:t) = [q[i] Metric Definition. The CA score, CAl,h,i single-FC α(i) CAl,h,i where the operator TopK-I(α, K) retrieves the top-K values of vector α. To assess an FCs importance robustly, we compute its mean CA score, by averaging across several samples from specific dataset. Figure 1 reveals the distinct functional contribution of each FC across all heads. Table 1: Compound CA scores under varying number of selected FCs (F ) and KV cache budgets (K). Each head has 64 FCs in total. Sparse and Universal Idom. Empirical analysis reveals three properties: (1) Sparsity: small subset of FCs (dominant FCs) exhibits disproportionately high agreement with full attention patterns. Conversely, the CA scores for the vast majority of other FCs are negligible (typically < 0.1); (2) Universality: The functional sparsity is widely observed across Llama, Mistral, and Qwen, and model scales from 3B to 32B (Appendix A.1); (3) TaskInvariance: The set of dominant FCs is largely task-agnostic. As shown in Figure 10, the saliency maps derived from tasks such as QA and summarization are consistent, suggesting that the functional roles of FCs are intrinsic to the RoPEs mechanics, rather than being task-specific adaptations. Idom Random Stream SnapKV = 8 (1/8) = 10 = 12 = 14 = 16 (1/4) 19.1 26.5 45.4 58.8 61.1 63.4 65.2 66.9 25.5 30.7 49.5 62.6 64.8 66.8 68.5 70.1 6.4 24.4 41.9 54.3 56.6 58.9 60.9 62.8 3.6 26.8 40.9 49.4 52.1 54.7 56.9 59. 2.0 34.4 37.9 43.0 46.4 49.7 52.4 55.3 51.1 53.9 66.6 76.1 77.5 79.0 80.2 81.4 1024 512 256 64 2048 Reconstructing Functionality from Idom. The analysis above supports that the functionality of full attention head can be reconstructed using only its most dominant components l,h dom = TopK-I({CAl,h,i 0 < d/2}, ). Therefore, we measure the collective efficacy of this subset using compound CA score, CAl,h,Idom , and present the results in Table 1. For comparison, we benchmark against token-eviction methods, which serve to emphasize the capability of predicting token importance. Our method demonstrates remarkable efficiency: with just 1/8 of the components selected under tight budget 64, Idom achieves an accuracy of 43%, surpassing the strong baseline SnapKV (Li et al., 2024) by an average of 10.3% across all budget levels. K"
        },
        {
            "title": "4 METHOD",
            "content": "Grounded in the functional sparsity of FCs, our training-free framework FASA employs twostage, coarse-to-fine strategy to circumvent the prohibitive cost of full self-attention. First, the Token Importance Predictor (TIP) stage utilizes computationally frugal proxy, defined by pre-calibrated set of dominant FCs, Idom, to efficiently identify small subset of contextually salient tokens. Subsequently, the Focused Attention Computation (FAC) stage performs full-fidelity attention computation exclusively on this salient subset, preserving high generation fidelity while drastically mitigating the computational and memory overhead of standard attention. 4 Published as conference paper at ICLR 2026 Figure 2: Method Overview of FASA. First, the TIP stage leverages only dominant FCs to efficiently estimate token importance and select critical subset of tokens. Then, the FAC stage performs full-dimensional attention exclusively on this reduced subset to generate the next token. See discussion about design in Appendix D.2."
        },
        {
            "title": "4.1 TOKEN IMPORTANCE PREDICTOR (TIP)",
            "content": "The TIP stage operates on the principle that dominant frequencies are an efficient proxy for token importance, where the dominant indices Idom are identified via one-time offline calibration. Offline Calibration: Identifying Idom. The objective of the offline calibration is to identify small, head-specific set of dominant frequencies, l,h dom, for each attention head (l, h). We formulate this process as search problem over frequency indices. Given small calibration dataset Ω and target size Ntip, our goal is to find the subset of FCs of cardinality Ntip that maximizes the expected average of CA scores. The objective is defined as: l,h dom = argmax I{0,...,d/21},I=Ntip Eq,KΩ (cid:34) (cid:88) (cid:35) (q, K) CAl,h,i . iI (5) This calibration is highly efficient, one-time offline process because the resulting Idom is empirically found to be task-agnostic and can be robustly identified from minimal number of samples. Its associated computational cost is negligible. The detailed algorithm is provided in Algorithm 1. Online Prediction: Importance Scoring via Frequency Subspace Aggregation. During the online prediction phase at given decoding step t, we leverage the pre-calibrated set of dominant frequencies, l,h dom, to efficiently estimate token importance in training-free manner. Conceptually, the full attention score for query qt and keys K1:t can be decomposed into sum of contributions from all d/2 frequency components: αl,h(qt, K1:t) = (cid:80)d/21 i=0 αl,h,i(qt, K1:t). Instead of performing this computationally expensive summation, our method constructs an importance score vector Sl,h , by exclusively aggregating the contributions from the pre-identified dominant frequencies, i.e., Sl,h αl,h,i(qt, K1:t). This formulation strategically bypasses computation for nondominant frequencies. Finally, based on these scores, we identify the set of top-Nf ac most important token indices, Tt, for the subsequent FAC stage: Tt = TopK-I(Sl,h , Nf ac). (cid:80) iIl,h dom 4.2 FOCUSED ATTENTION COMPUTATION (FAC) Following the identification of the contextually important token set Tt by the TIP module, this stage executes an attention computation on Tt, enabling the model to concentrate its computational resources on the most salient parts of the context. Specifically, for the current query vector qt at decoding step t, instead of using the full key and value matrices (K1:t, V1:t) from the entire past context, we first gather the keys and values corresponding to the indices in Tt: KTt = Gather(K1:t, Tt), VTt = Gather(V1:t, Tt) (6) where the Gather() operation selects the rows from the original matrices specified by the index set Tt. The attention scores for each head (l, h) are then computed using only these selected keys. The final output vector for the head is subsequently produced by weighting the selected value vectors: ˆαl,h FAC = Softmax (cid:16) qtKTt / (cid:17) 5 , Ol,h = ˆαl,h FACVTt (7) Published as conference paper at ICLR 2026 Critically, the original absolute positions of the tokens in Tt are preserved. This directly maintains the integrity of their position embeddings and the vital spatial information they encode, preventing the performance degradation associated with positional distortion. In essence, the FAC stage functions as high-fidelity computational filter, restricting full-precision attention to the most salient tokens to achieve compelling balance between computational efficiency and predictive accuracy."
        },
        {
            "title": "4.3 TWO IMPLEMENTATIONS OF FASA",
            "content": "We introduce two specialized, hardware-aware variants of FASA that offer trade-off between memory and speed: (1). FASA-M (Memory-Optimized) minimizes its GPU memory footprint by strategically offloading the value cache and non-dominant key components to CPU memory, making it ideal for VRAM-constrained environments. To mitigate the latency from CPU-GPU data transfer, this approach can be effectively paired with prefetching techniques. (2) FASA-C (ComputationOptimized) prioritizes inference speed by retaining the full cache on-GPU but accessing only sparse subset of key states, drastically reducing memory I/O for significant acceleration. (See Appendix D.1 for details and memory analysis of FASA-M). 4.4 EFFICIENCY ANALYSIS OF FASA Computational Analysis. At the generation step t, the complexity of computing qtKT 1:t is O(td) and the complexity of multiplying the value states with attention scores is O(td) per head. For FASA, (1) the complexity of the TIP stage is O(2tNtip) (each FC takes up 2 dimensions), since this stage operates in low-dimensional subspaces, and (2) the FAC stage performs attention on reduced set of Nf ac tokens, leading to complexity of O(Nf acd). Additionally, the detection of dominant frequencies Idom is offline, one-time, and applicable for various tasks and the burdens from this part could be neglected. Assuming the complexity of selecting the top-k tokens is small, the overall complexity of FASA is O(2tNtip + 2Nf acd). The theoretical speedup at decoding stage is in Equation 8. Figure 3: Decoding latency dominates total latency in auto-regressive generation. Speedup = 2td 2tNtip + 2Nf acd = 1 Ntip/d + Nf ac/t , Speedup Ntip if Nf ac (8) Memory Movement Reduction. The auto-regressive decoding stage is notoriously memory-bound, as requiring loading the entire KV cache, creating significant latency bottleneck. This is confirmed in Figure 3, where decoding constitutes 90% of the total latency at 32K context. FASA, directly mitigates this bottleneck by drastically reducing memory traffic. At decoding step t, standard attention loads 2tm bytes from the KV cache (with as the byte size per state vector) while FASA accesses only t(2Ntip/d m) bytes (only keys) for the TIP and 2Nfacm bytes for the FAC. The fraction that FASA must load is therefore: (2tmNtip/d + 2Nf acm)/2tm = Ntip/d + Nf ac/t Ntip/d(Nf ac t), which alleviates the memory-bound constraint of long-context decoding."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTING Baselines and Models. To comprehensively evaluate FASAs performance, we benchmark it against into two groups of robust baselines: (1) State-of-the-art methods: We compare against leading token eviction methods in efficient KV cache management, including Stream (Xiao et al., 2024), SnapKV (Li et al., 2024), RKV (Cai et al., 2025a), Quest (Tang et al., 2024), H2O (Zhang et al., 2023); (2) Upper bounds: two theoretical bounds, FKV, which represents standard inference with the complete, uncompressed KV cache, serving as the absolute performance ceiling due to no information loss, and Oracle, more pragmatic upper bound for eviction-based methods, assuming ideal knowledge to retain only the most critical tokens based on full-head scores. Our experiments 6 Published as conference paper at ICLR 2026 Table 2: Performance of FASA on diverse models on LongBench-V1 benchmarks. For baselines, we retain constant token budget (256) and 25% FCs for FASA. FKV and Oracle are full and look-ahead upper bounds. Summarize Single-Doc QA Multi-Doc QA Code Method MF-en 2Wiki NQA Qasp Hqa Tqa FKV 26.0 40.7 50.4 32.2 29.6 15.1 33.5 22.9 25.3 71.5 88.9 Oracle 26.6 41.2 49.8 31.9 29.9 16.2 32.6 22.2 25.0 71.5 89.3 23.3 18.1 25.1 34.5 52.9 6.5 8.7 Quest 19.5 23.6 12.9 15.9 Stream 13.2 19.7 23.6 18.1 22.7 18.2 17.9 17.9 49.0 83.7 7.8 SnapKV 23.5 28.9 45.6 17.7 22.9 11.8 21.7 20.9 21.1 61.0 88.5 25.6 38.9 49.9 29.7 31.2 14.8 28.0 24.2 26.1 71.5 89.2 FASA 3 l 3 - 2 . Mult Musi Qsum GovR AVG. RB-P Pcnt Lcc 3.5 87.8 52.0 54.2 42.2 3.5 88.0 53.7 54.4 42.40.2 6.5 38.3 53.7 43.6 25.516.7 3.5 85.7 49.3 45.9 31.810.4 3.5 88.0 50.7 48.6 37.05.2 3.6 86.9 53.2 50.5 41.50. Summarize Synthetic Trec Pre FKV 24.2 43.5 52.1 55.9 46.9 28.6 31.8 23.1 23.9 71.5 89.3 24.4 43.0 52.3 57.8 46.9 30.1 31.6 23.9 24.1 72.5 89.7 Oracle Quest 26.8 19.9 24.4 41.8 66.7 8.8 9.1 Stream 18.1 24.2 26.5 41.2 36.4 17.3 18.4 18.3 15.4 45.0 82.9 SnapKV 26.6 36.0 50.8 55.6 43.8 26.5 21.9 21.9 19.3 58.0 86.2 28.3 43.8 51.9 57.4 46.0 30.1 31.2 22.8 24.3 72.0 89.4 FASA 24.5 30.4 24.7 24.1 . 7 - 5 2 Q 3 FKV 29.1 41.6 52.9 49.4 39.5 29.1 34.8 25.7 27.8 76.0 88.6 Oracle 31.0 40.2 52.4 50.3 39.4 28.8 34.0 25.74 27.2 76.0 89.4 Quest 15.7 30.7 41.0 37.4 27.1 11.9 29.3 21.3 26.6 57.0 80.7 Stream 11.8 15.3 20.9 32.1 27.1 10.6 20.2 17.3 20.1 44.5 69.0 SnapKV 25.5 32.6 53.7 48.4 37.3 25.9 22.7 23.6 23.1 62.5 89.4 29.9 42.3 53.7 51.1 39.1 28.7 34.0 24.8 28.2 76.0 89.4 FASA . 0 - 7 - t 7.5 92.0 60.2 66.5 47.8 8.0 100.0 60.5 65.3 48.70.9 4.4 77.6 46.5 42.0 31.416.4 8.5 24.0 49.6 52.2 31.915.9 8.0 98.5 55.6 60.6 42.65.2 8.0 99.5 60.3 64.0 47.90.1 5.5 98.0 58.4 59.7 47.4 5.0 98.0 59.3 61.0 47.90.5 5.0 85.5 56.9 53.0 38.68.8 1.6 6.5 94.5 57.3 57.0 44.03.4 5.0 98.0 57.8 58.0 47.80.4 56.5 49.8 26.7 20. 3.2 FKV 30.0 45.3 55.6 55.8 43.7 30.2 35.1 25.4 27.0 72.5 91.7 Oracle 30.3 44.5 55.0 54.9 44.6 32.0 34.8 25.1 26.9 72.5 91.5 Quest 13.7 33.1 38.4 35.8 32.2 12.8 26.5 20.9 26.7 38.0 65.6 Stream 21.9 23.4 31.8 45.1 36.7 24.3 20.0 21.0 19.3 45.5 87.9 SnapKV 27.5 34.5 51.6 52.3 44.3 28.3 23.9 24.0 22.7 62.5 90.9 29.3 43.7 54.1 54.8 43.9 30.8 33.5 24.7 27.0 72.0 91.1 FASA 8 - 1 . 3 l 7.1 99.5 63.0 56.3 48.7 7.0 99.5 63.3 57.4 48.70.0 3.8 95.0 52.5 45.7 35.413.3 6.9 99.5 59.4 49.1 38.89.9 7.5 99.5 60.1 52.6 45.03.7 7.5 99.5 61.8 52.7 48.20.5 FKV 28.7 46.2 53.8 65.2 64.5 43.6 43.5 23.3 22.7 80.5 89.5 11.0 100.0 32.3 37.5 50.3 Oracle 28.5 46.3 54.3 64.3 63.6 44.7 31.5 22.9 22.7 81.0 88.4 10.0 100.0 33.6 39.7 49.40.9 Quest 14.5 31.9 39.1 38.8 36.6 16.2 16.2 20.1 25.2 43.5 72.7 10.0 88.8 35.0 34.0 34.915.4 Stream 19.6 26.9 29.4 46.5 48.3 29.6 17.8 18.4 15.0 46.5 82.5 12.5 72.1 28.7 31.2 35.315.0 SnapKV 26.3 40.5 51.2 63.2 62.2 43.3 22.5 22.0 18.3 63.5 87.5 11.5 100.0 30.4 36.0 45.94.4 27.2 45.5 54.5 64.4 63.9 44.5 30.4 22.8 21.9 80.0 87.5 15.5 100.0 30.5 36.1 49.21.1 FASA 1 - 4 1 - 5 . 2 Q span variety of cutting-edge architectures and model sizes, specifically Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), and Qwen (Bai et al., 2023). Evaluation Benchmarks. To rigorously assess the capabilities of FASA across diverse long-context scenarios (Liu et al., 2025b), we conduct comprehensive evaluations spanning three paradigms: (1) Long-context understanding: We use diverse, real-world tasks from LongBench (Bai et al., 2024) to assess the ability to identify critical information within lengthy contexts. (2) Long-Sequence Modeling: We measure perplexity on PG-19 (Rae et al., 2019), WikiText (Merity et al., 2017), and C4 (Raffel et al., 2019) corpus to evaluate generative fidelity over long dependencies. (3) Long-CoT Reasoning: To test performance in long-generation scenarios, we evaluate on complex mathematical reasoning tasks from MATH500 (Hendrycks et al., 2021) and AIME24 (MAA, 2024) on R1-LLMs. 5.2 PERFORMANCE COMPARISON ON LONG-CONTEXT TASKS. Figure 4: Perplexity results of FASA in comparison with FKV, Oracle, Stream, and Quest on Wikitext (top), PG19 (middle), and C4 corpus (bottom). Token sparsity indicates the retained ratio of tokens. FASA achieves near-lossless performance under various budgets. FASA consistently outperforms all baselines across various budgets (Appendix C.1 and 5), preserving contextual integrity even under extreme compression  (Table 2)  . In stark contrast, existing token-eviction methods suffer catastrophic performance degradation; for instance, Quests accuracy plummets by 13.4% on NarrativeQA, underscoring their inability to retain critical information. Remarkably, under extreme budgets, FASA Published as conference paper at ICLR 2026 Table 3: Performance and output length of FASA compared to baseline models on the MATH500 and AIME24 Ntip = 16. AIME24 results are reported as pass@1, based on 16 responses per question. PREF* and DEC* denote the prefill and decoding lengths, respectively. FKV and Oracle are full and look-ahead upper bounds. Methods MATH500 AIME24 Fixed Budget Len Stats Fixed Budget Len Stats 300 500 700 1000 PREF* DEC* TOTAL. 500 1000 1500 2000 2500 PREF* DEC* TOTAL. DeepSeek-R1-Distill-Llama-8B - - FKV 72.4 72.4 Oracle 70.4 72.6 74.2 71.8 6.8 33.0 53.87 42.8 H2O 9.6 24.6 40.4 47.4 Stream SnapKV 21.6 32.6 46.8 54.6 24.0 39.4 49.2 57.0 RKV 62.2 68.8 69.4 71.8 FASA 127 2977 3195 8244 3520 7047 7005 3171 3104 3321 8370 3647 7174 7132 3298 - - 43.9 43.9 - 30.0 36.7 37.3 39.3 36.0 4.7 11.3 14.0 20.0 0.7 8.0 10.7 15.3 3.3 0.0 4.0 8.0 16.0 23.3 29.1 6.7 10.7 14.0 21.7 23.3 20.6 34.4 40.2 35.8 38.0 - - FKV 92.4 92.4 Oracle 92.2 92.4 92.4 92.2 29.6 50.2 62.8 77.0 H2O Stream 27.8 44.0 57.8 64.4 SnapKV 34.2 55.8 69.4 79.4 57.8 74.0 80.8 86.4 RKV 86.6 88.8 90.2 91.2 FASA DeepSeek-R1-Distill-Qwen-14B 127 2784 2985 3413 2801 3586 3865 3139 2914 3112 3540 2928 3713 3992 3266 - - 66.6 66.6 - 67.9 66.7 67.3 70.7 67.3 5.3 20.5 37.3 46.0 52.7 2.0 4.0 16.7 22.7 29.3 10.0 23.3 40.0 46.0 52.7 20.7 30.0 46.7 55.4 62.0 54.0 60.6 59.3 62.7 63. DeepSeek-R1-Distill-Qwen-32B - - FKV 92.6 92.6 Oracle 92.4 91.4 91.4 91.2 47.2 50.0 68.3 74.4 H2O Stream 43.6 57.6 65.6 73.4 SnapKV 49.6 66.0 74.8 80.8 75.0 72.2 78.4 83.6 RKV 86.4 90.2 90.2 91.2 FASA 127 2717 2886 3841 2773 3704 4229 2846 3013 3968 2900 3831 4356 3014 - - 72.8 - 72.8 68.0 70.1 70.0 76.7 69.2 6.7 16.7 38.4 45.6 55.6 0.7 6.7 18.7 23.3 24.7 10.0 23.3 40.0 46.0 52.7 14.7 32.7 43.3 55.3 61.3 60.7 62.0 66.3 70.0 73.2 161 156 13231 13392 15638 15799 21099 21260 10191 10352 17359 17520 22916 23077 17166 17327 11039 11204 11546 11711 9684 9519 8468 8633 11922 12083 16274 16439 11553 11709 10461 10626 11545 11710 10904 11069 10732 10897 13650 13815 18078 18243 11735 11891 occasionally surpasses the FKV baseline (e.g., on Mistral-7B). We attribute this phenomenon to the mitigation of attentional distraction from irrelevant tokens. This hypothesis is corroborated by the Oracle baseline, which also outperforms FKV sometimes, thereby validating our frequency-chunkbased frameworks efficacy in precisely identifying semantically pivotal regions. FASA models complex long-term dependencies. We simulate token-by-token decoding process wherein the eviction strategy is iteratively applied before token prediction. The fixed-rule approach of Stream (Xiao et al., 2024), which relies on attention sinks, severely compromises its ability to capture long-range dependencies, leading to drastic increase in perplexity as shown in Figure 4. Similarly, Quests coarse, page-level granularity prevents it from adaptively retaining critical, noncontiguous tokens. In contrast, FASAs fine-grained, query-dependent mechanism accurately identifies salient tokens, achieving performance comparable to FKV, even under aggressive compression. FASA excels at long-CoT reasoning. The chain of thought in long-form reasoning is fragile thread, requiring the preservation of dynamically shifting \"thought traces\", thread that prominent baselines consistently sever. As shown in Table 3, their static compression heuristics, blind to the evolving importance of tokens, lead to precipitous drop in performance. On R1-Llama, SnapKVs accuracy collapses to 21.6, stark contrast to the FKVs 72.4, demonstrating fundamental failure to sustain the very logical dependencies required for reasoning. Conversely, FASA operates with surgical precision. It surpasses not only standard baselines but also R-KV, highly specialized method for CoT compression. It achieves an impressive 86.4% accuracy on scant 10% context budget, narrowly trailing the 92.6% FKV upper bound. This feat cements its status as superior framework, one that can navigate the intricate web of complex reasoning without severing the essential threads of logic. Figure 5: FASA under various token budgets (Ntip = 16). 5.3 IN-DEPTH ANALYSIS Table 4: Compatibility of FASA. Table 5: Ablation on K. Table 6: Ablation of offline calibration. Budget 256 512 1024 2048 Qasp. FASA 43.7 +PyKV 44.40.7 44.50.5 45.81.1 45.80.1 Lcc 44.0 44.7 45.7 FASA 61.8 +PyKV 62.20.4 63.60.2 64.70.3 64.90.1 64. 63.4 64.4 Token Budget AVG. Offline 128 256 512 1024 2048 128 42.5 43.6 44.9 45.7 45.6 44.5 256 42.6 43.7 44.0 44.7 45.3 44.1 512 41.9 43.5 43.7 44.9 45.3 43.9 1024 42.2 44.2 44.3 44.7 45.0 44.1 Base Nqa Qasp. Musi Self CV S-Doc QA M-Doc QA 2Wiki Musi Hqa Qasp. MF_en Nqa 55.6 29.9 43.7 30.2 55.8 45.3 55.8 29.2 44.5 31.6 55.0 44.2 54.6 29.1 43.0 31.0 54.1 44.0 54.6 29.6 43.8 30.8 55.1 44.8 54.4 29.2 43.5 30.8 55.3 43.9 .011 .007 .014 .012 .010 . 8 Published as conference paper at ICLR 2026 Figure 6: Evaluation of FASA on TREC (left) and MATH (right) datasets. The plots show the synergistic effects under varying numbers of selected FCs and different token budgets. Effect on Generation Length. neglected aspect of compression methods is the impact on output length. Some compression methods, like H2O, induce generative verbosity, imposing an overlooked computational burden  (Table 3)  . Conversely, others, such as Stream, prematurely terminate generation, which truncates valid reasoning and degrade performance. In contrast, FASA maintains output lengths nearly identical to the FKV while preserving high performance, demonstrating superior balance. Compatiblility of FASA. By design, FASA is orthogonal to and synergistic with other KV cache optimization paradigms. We demonstrate this by integrating it with PyramidKV (Cai et al., 2025b), which allocates varied budgets across layers. While PyramidKV determines how many tokens to keep per layer, FASA decides which tokens are most critical. As shown in Table 4, this complementary pairing yields consistent performance gains, confirming FASAs high compatibility and modularity. Efficiency Analysis. We assess the efficiency of our two FASA variants. FASA-Ms memory savings are particularly pronounced in long sequences, as the KV caches footprint grows to dominate and dwarf the static memory costs of model parameters and activations. While its CPUGPU data transfer introduces slight latency overhead, this can be effectively mitigated by prefetching techniques that asynchronously load the required KV pairs in advance. FASA-C, implemented with Triton (based on Ribar et al. Figure 7: Memory vs. latency (Ntip = 16). (2024)), delivers substantial inference acceleration. The speedup effect intensifies with longer sequences, achieving up to 2.56 with Ntip = 16 under 64K. 5.4 ABLATION STUDIES Robustness to Calibration Window K. Our method exhibits remarkable robustness to the calibration window size, K. Performance is largely insensitive to K, with smaller values often yielding slightly superior results  (Table 5)  . This suggests that due to the inherent sparsity of attention, even small calibration window provides sufficiently robust signal to identify the dominant FCs. Trade-off between Ntip and Nf ac. The hyperparameters Ntip (token selection precision) and Nf ac (retention budget) govern trade-off between the fidelity of token identification and the volume of retained context. As depicted in Figure 6, optimal performance can be achieved either with high-precision selection (large Ntip) and small budget, or more lenient selection (small Ntip) compensated by larger one. Empirically, on the TREC dataset, we found that using just 10 dominant FCs (15.6% of dimensions) with Nf ac = 500 is sufficient to match the FKVs performance. Impact of Offline Calibrated Data. As shown in Table 6, our method exhibits remarkable robustness to the choice of calibration data. The minimal performance variation across different calibration datasets, as quantified by low Coefficient of Variation (CV), confirms that our FC detection mechanism is stable and not reliant on specific calibration source."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we addressed the memory footprint and bandwidth introduced by the KV cache in LLMs. Firstly, we cover an intriguing phenomenon: the functional sparsity of FCs. subset of dominant FCs 9 Published as conference paper at ICLR 2026 could show high contextual awareness. Based on this discovery, we introduce FASA, coarse-to-fine two-stage freamwork. The first stage utilizes the dominant FCs to perform dynamic, query-aware token selection without costly training. Then, the second stage perform focused and precise attention computation on this reduced subset. Our experiments indicate that FASA attains performance nearly on par with full KV even under constrained budgets. The memoryand speed-optimized variants of FASA offers practical and effective solution for efficient long-context inference."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our research is focused on enhancing the computational efficiency of Large Language Model (LLM) inference by optimizing KV cache management. The primary positive impact of our work, FASA, is to make large-scale models more accessible, affordable, and environmentally sustainable. By significantly reducing memory and computational overhead, our method can enable researchers and institutions with limited resources to develop and deploy powerful long-context models, thereby fostering broader innovation and democratization in the field of AI. We acknowledge the dual-use nature of efficiency-enhancing technologies. While our goal is positive, lowering the barrier to running large models could inadvertently make it easier for malicious actors to deploy them for harmful purposes, such as generating misinformation or spam at scale. It is important to note, however, that our work is foundational and does not create new capabilities for generating harmful content; it merely optimizes the performance of existing models. All experiments were conducted on publicly available benchmarks (LongBench, MATH, AIME) and open-source pre-trained models. We did not use any private, sensitive, or user-generated data. We recognize that the foundation models used in our evaluation may reflect and perpetuate societal biases present in their vast training corpora. Our method operates orthogonally to the challenge of model-level bias and does not address it directly, but we encourage users to be mindful of the inherent limitations of the models they deploy with our technique."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our work, we provide detailed account of all models, datasets, experimental setups, and evaluation protocols, all of which are publicly available. An overview of the experiments is provided in Section 5.1, with more comprehensive details described across several appendices. Specifically, the configurations for all baselines and the detailed hyperparameters for FASA are presented in Appendix B.1. The descriptions of all benchmarks and their corresponding evaluation protocols are detailed in Appendix B.2 and Appendix B.3, respectively. Furthermore, the implementation and design choices for FASA are explained in Appendix B.4. Finally, the specific algorithms for FASA-M and other core functions are provided in Appendix D.1 and Appendix D.3."
        },
        {
            "title": "REFERENCES",
            "content": "Eigen attention: Attention in low-rank space for kv cache compression, 2024. URL https: //arxiv.org/abs/2408.05646. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=hmOwOZWzYE. Yash Akhauri, Ahmed AbouElhamayed, Yifei Gao, Chi-Chih Chang, Nilesh Jain, and Mohamed S. Abdelfattah. Tokenbutler: Token importance is predictable, 2025. URL https://arxiv.org/ abs/2503.07518. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context 10 Published as conference paper at ICLR 2026 understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, 2024. Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=GtvuNrk58a. Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, and Alexey Tumanov. Rocketkv: Accelerating long-context llm inference via two-stage kv cache compression, 2025. URL https://arxiv.org/abs/2502.14051. Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, LiWen Chang, Jiuxiang Gu, et al. R-kv: Redundancy-aware kv cache compression for training-free reasoning models acceleration. arXiv preprint arXiv:2505.24133, 2025a. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2025b. URL https://arxiv.org/abs/2406.02069. Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed S. Abdelfattah, and Kai-Chiang Wu. Palu: KV-cache compression with low-rank projection. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=LWMS4pk2vK. Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, and Chao Huang. Sepllm: Accelerate large language models by compressing one segment into one separator, 2025a. URL https://arxiv.org/abs/2412.12094. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. Swe-exp: Experience-driven software issue resolution. arXiv preprint arXiv:2507.23361, 2025b. Yanqi Dai, Yuxiang Ji, Xiao Zhang, Yong Wang, Xiangxiang Chu, and Zhiwu Lu. Harder is better: Boosting mathematical reasoning via difficulty-aware grpo and multi-aspect question reformulation. arXiv preprint arXiv:2601.20614, 2026. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/ 2205.14135. Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen. Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference, 2024. URL https://arxiv.org/abs/2402.09398. Yixiong Fang, Tianran Sun, Yuling Shi, and Xiaodong Gu. Attentionrag: Attention-guided context pruning in retrieval-augmented generation. arXiv preprint arXiv:2503.10720, 2025. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=uNrFpDPMyo. 11 Published as conference paper at ICLR 2026 Tanya Goyal and Greg Durrett. Evaluating factuality in generation with dependency-level entailment, 2020. URL https://arxiv.org/abs/2010.05478. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, Sebastian Zhao, June Paik, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. Squeezed attention: Accelerating long context length llm inference, 2025a. URL https://arxiv.org/abs/ 2411.09688. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization, 2025b. URL https://arxiv.org/abs/2401.18079. Dongsheng Jiang, Yuchen Liu, Songlin Liu, Jine Zhao, Hao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and Hongkai Xiong. From clip to dino: Visual encoders shout in multi-modal large language models. arXiv preprint arXiv:2310.08825, 2023. Haoyang LI, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole HU, Wei Dong, Li Qing, and Lei Chen. survey on large language model acceleration based on KV cache management. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=z3JZzu9EA3. Renda Li, Hailang Huang, Fei Wei, Feng Xiong, Yong Wang, and Xiangxiang Chu. Adacurl: Adaptive curriculum reinforcement learning with invalid sample mitigation and historical revisiting. arXiv preprint arXiv:2511.09478, 2025. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-ofexperts language model. arXiv preprint arXiv:2405.04434, 2024a. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: KV cache compression in depth dimension for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview. net/forum?id=sgVOjDqUMT. Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, and Lili Qiu. Retrievalattention: Accelerating long-context llm inference via vector retrieval, 2024c. URL https://arxiv.org/abs/2409.10516. Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, and Minyi Guo. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression, 2025a. URL https://arxiv.org/ abs/2412.03213. Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, et al. Attention as compass: Efficient exploration for processsupervised rl in reasoning models. arXiv preprint arXiv:2509.26628, 2025b. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time, 2023. URL https://arxiv.org/abs/ 2305.17118. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024d. 12 Published as conference paper at ICLR 2026 MAA."
        },
        {
            "title": "American\nInvitational Mathematics",
            "content": "invitational mathematics American ary URL american-invitational-mathematics-examination-aime. 2024. In - Examination AIME Februhttps://maa.org/math-competitions/ examination - aime. 2024, Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture In International Conference on Learning Representations, 2017. URL https:// models. openreview.net/forum?id=Byj72udxe. Alexander Peysakhovich and Adam Lerer. Attention sorting combats recency bias in long context language models. arXiv preprint arXiv:2310.01427, 2023. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. URL https://arxiv.org/abs/ 1911.05507. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL https://openreview. net/forum?id=Ue8EHzaFI4. Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Longcodezip: Compress long context for code language models. arXiv preprint arXiv:2510.00446, 2025. Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=raABeiV71j. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104. 09864. Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference, 2025. URL https://arxiv.org/abs/2410.21465. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=F7aAhfitX6. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In International Conference on Machine Learning, pp. 4790147911. PMLR, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, and Mi Zhang. $text{D}_{2}text{O}$: Dynamic discriminative In The Thirteenth operations for efficient long-context inference of large language models. International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=HzBfoUdjHt. Ao Wang, Hui Chen, Jiaxin Li, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, and Guiguang Ding. Prefixkv: Adaptive prefix kv cache is what vision instruction-following models need for efficient generation, 2025a. URL https://arxiv.org/abs/2412.03409. Published as conference paper at ICLR 2026 Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. Unveiling factual recall behaviors of large language models through knowledge neurons. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 73887402, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.420. URL https://aclanthology.org/2024.emnlp-main.420/. Yifei Wang, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. Uncertainty unveiled: Can exposure to more in-context examples mitigate uncertainty for large language models? In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 2065920678, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 1062. URL https://aclanthology.org/2025.findings-acl.1062/. Yifei Wang, Feng Xiong, Yong Wang, Linjing Li, Xiangxiang Chu, and Daniel Dajun Zeng. POSITION BIAS MITIGATES POSITION BIAS: Mitigate position bias through inter-position knowledge distillation. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 14951512, Suzhou, China, November 2025c. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.78. URL https://aclanthology.org/2025.emnlp-main.78/. Yuhang Wang, Yuling Shi, Mo Yang, Rongrui Zhang, Shilin He, Heng Lian, Yuting Chen, Siyu Ye, Kai Cai, and Xiaodong Gu. Swe-pruner: Self-adaptive context pruning for coding agents. arXiv preprint arXiv:2601.16746, 2026. Zheng Wang, Boxiao Jin, Yuming Chang, Zhongzhi Yu, and Minjia Zhang. Model tells you where to merge: Adaptive KV cache merging for LLMs on long-context tasks, 2025d. URL https://openreview.net/forum?id=Q5VlpYRxGF. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. VideoroPE: What makes for good video rotary position embedding? In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=tO7OVZkCo1. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6/. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, and Xiangxiang Chu. Hs-star: Hierarchical sampling for self-taught reasoners via difficulty estimation and budget reallocation. arXiv preprint arXiv:2505.19866, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. URL https://doi.org/10.48550/arXiv.2412.15115. Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, and Bin Li. Attentionpredictor: Temporal pattern matters for efficient llm inference, 2025. URL https://arxiv.org/abs/2502.04077. 14 Published as conference paper at ICLR 2026 Rongzhi Zhang, Kuan Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and yelong shen. LoRC: Low-rank compression for LLMs KV cache with progressive compression strategy, 2025. URL https://openreview.net/forum?id=NI8AUSAc4i. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=RkRrPp7GKO. Published as conference paper at ICLR"
        },
        {
            "title": "A INVESTIGATION RESULTS OF DOMINANT FREQUENCY CHUNKS",
            "content": "A.1 FURTHER GENERALIZATION ON MODEL SCALES AND ARCHITECHTURES Figure 8: Functional sparsity is maintained on Qwen2.5 series models (Yang et al., 2024). Heatmaps visualize the Mean Contextual Agreement (CAK=256) for each Frequency Chunk (FC, x-axis) across all attention heads (y-axis) in representative layer. We compare the standard Qwen2.5-14B-Instruct model (left) with its long-context variant, Qwen2.5-14B-Instruct-1M (right), both calibrated on the Qasper dataset. The remarkable similarity between the two heatmaps demonstrates that the functional sparsity of FCs is robust property, consistently maintained even after long-context fine-tuning. Figure 9: Functional sparsity persists across model scales. Heatmaps show the Mean Contextual Agreement (CAK=256) for increasing scale (3B and 32B). The remarkable stability of the dominant FC patterns (bright vertical columns) across these scales demonstrates that functional sparsity is fundamental and scalable characteristic of RoPE. Conclusions: Our cross-architectural (Figure 8) and cross-scale (Figure 9) analysis reveals striking finding: the functional sparsity of FCs is universal and stable property. This powerful evidence suggests that the observed functional hierarchy is not an emergent artifact of specific models training dynamics or size, but rather an intrinsic characteristic deeply embedded within the RoPE mechanism itself. The roles of different frequencies appear to be fundamental and pre-determined, providing robust and predictable foundation for developing model-agnostic efficiency optimizations. A.2 TASK-INVARIANCE PROPERTY OF FUNCTIONAL SPARSITY We find that the saliency of dominant FCs is largely task-agnostic. This property is evidenced by the strong alignment between saliency maps generated for distinct downstream tasks, as shown in Figure 10. Despite the functional differences between question answering (left) and summarization (right), the resulting importance rankings are highly consistent. This indicates that these FCs perform fundamental role inherent to the models architecture, rather than one adapted for specific task. A.3 MORE ANALYSIS RESULTS Functional Sparsity across Layers. While the principle of functional sparsity is universal, the specific set of dominant FCs is far from static in Figure 11; instead, it exhibits high degree of 16 Published as conference paper at ICLR 2026 (a) Qasper (b) GovReport Figure 10: Heatmaps of agreement score (CA, = 256) across attention heads for the Qasper (Left) and GovReport (Right) from LongBench-V1 (Bai et al. (2024)) on Mistral-7B-Instruct-v0.3. Figure 11: Heatmaps of agreement score (CA, = 256) across different layers. specialization across both model depth and individual attention heads. This dynamic behavior reveals sophisticated division of labor within the transformer architecture."
        },
        {
            "title": "B EXPERIMENTS DETAILS",
            "content": "B.1 EXPERIMENT CONFIGURATIONS. Baseline Configurations. As FASA is designed to optimize the decode phase, we forgo any KV cache optimizations during prefilling for all methods under evaluation. This experimental design isolates the performance impact of decode-stage acceleration, ensuring that our comparisons are direct and fair. For all baselines, we adopted configurations that are either standard in their original papers or represent fair and strong setup for comparison. Oracle: serves as an oracle baseline to demonstrate the upper-bound performance of Top-k sparse attention. This method operates under the ideal assumption that the most important KV tokens for each query can be identified perfectly and at no computational cost. Consequently, given token budget directly corresponds to this optimal Top-k set. Stream (Xiao et al., 2024): This method is based on the \"attention sink\" phenomenon, preserving fixed number of initial tokens and sliding window of recent tokens. Following its standard setup, we set the initial \"start_size\" to 8 and the \"recent_size\" to \"budget - 8\". SnapKV (Li et al., 2024): SnapKV estimates token importance based on accumulated attention scores within observation window during prefilling. We adopted its \"maxpool\" strategy with window size of 32 and kernel size of 7. As its original design performs one-time filtering, it is not directly suited for long-generation tasks. We therefore adapted it, following the methodology in (Cai et al., 2025a), by re-applying the filtering mechanism every generated tokens. Quest (Tang et al., 2024): Quest organizes the KV cache into pages and retrieves them based on coarse-grained query-page similarity. We set the page size to 16, value reported as near-optimal, to balance the trade-off between retrieval granularity and overhead. RKV (Cai et al., 2025a): RKV is state-of-the-art method for reasoning tasks that also employs retrieval mechanism. We set its core hyperparameter λ, which balances between recent and important tokens, to 0.1 as recommended for optimal performance. FASA Configurations. Our configuration for FASA is designed for both effectiveness and practical efficiency. Unless otherwise specified, the following setup was used across all experiments. 17 Published as conference paper at ICLR 2026 Dominant FC Identification: core principle of FASA is that the set of dominant FCs is universal, task-agnostic property of the model architecture itself. Consequently, these indices (Idom) can be determined via highly efficient, one-time offline calibration. For our LongBench experiments, this calibration was performed on just single data sample from the Qasper dataset. We found this minimal setup to be remarkably robust, as the generated response provides sufficient signal to identify the dominant FCs. The universality of these calibrated indices is empirically validated by FASAs strong performance across diverse tasks, from summarization to code completion. For Long-CoT reasoning, similar single-instance calibration was performed on question from the MATH500 dataset. Hyperparameter Settings: For architectural simplicity and to maximize computational parallelism, we employ uniform configuration across all heads and layers. The number of dominant FCs to retain, denoted as Ntip, was consistently set to 16. This choice represents balance between preserving sufficient contextual information and maximizing computational. Task Configurations: We configured the maximum sequence length to 32k for the AIME24 benchmark, reflecting its higher reasoning complexity, and to 16k for MATH500. For the LongBench benchmark, we set the maximum prompt length to 127.5k for Llama3/Qwen2.5 series models and 31.5k for Mistral-7B-Instruct-v0.2. B.2 BENCHMARK DETAILS LongBench (Bai et al., 2024) is comprehensive, multi-task benchmark designed to evaluate the long-context understanding capabilities of Large Language (Wang et al., 2024; 2025b). It comprises diverse set of tasks, including single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. In our experiments, we report the average performance across all relevant tasks to provide holistic measure of models ability to process and reason over extended contexts, with sequence lengths ranging from 4K to over 100K tokens. MATH500 (Hendrycks et al., 2021) is challenging benchmark for evaluating mathematical reasoning. It consists of 12,500 problems sourced from high school math competitions, spanning subjects like Algebra, Geometry, Number Theory, and Precalculus. Each problem is accompanied by step-by-step solution, making it highly suitable for assessing CoT reasoning capabilities. We utilize the MATH500 subset for our long-CoT generation experiments, where models must produce detailed reasoning chains to arrive at the final answer. AIME (MAA, 2024) represents significant step-up in reasoning complexity compared to the MATH dataset. It consists of problems from the AIME competition, which are known for their non-routine, multi-step solutions requiring deep mathematical insight and creativity (Li et al., 2025; Dai et al., 2026). These problems serve as stress test for models most advanced reasoning and long-chain generation abilities (Xiong et al., 2025). Following standard practice, we evaluate performance using the pass@k metric, specifically reporting pass@1 based on 16 generated responses per question. C4 (Raffel et al., 2019) is massive, general-domain English text dataset derived from the Common Crawl web scrape. The \"clean\" version is created by applying series of heuristics to filter out boilerplate content, code, and offensive language, resulting in high-quality, natural language corpus. PG19 (Rae et al., 2019) is long-form text dataset derived from books in the Project Gutenberg library. It is specifically curated for evaluating long-range sequence modeling. Each example in the dataset is full book text, making it an ideal benchmark for assessing models ability to handle and maintain coherence over very long dependencies, often exceeding the context windows of LLMs. WikiText(Merity et al., 2017) is large-scale language modeling corpus sourced from high-quality \"Good\" and \"Featured\" articles on Wikipedia. Unlike raw web text, WikiText is well-formatted, grammatically correct, and retains its original punctuation and case. It is split into training, validation, and test sets at the article level. B.3 EVALUATION PROTOCOLS To provide comprehensive and rigorous assessment of model performance, we employ set of standard metrics tailored to each evaluation paradigm. 18 Published as conference paper at ICLR Long-Context Understanding (LongBench). For the diverse tasks within the LongBench benchmark (Bai et al., 2024), we follow its official evaluation protocol. Specifically, we use: f1 score for question-answering tasks. rouge_score for summarization tasks. code_sim_score for code completion tasks. The final reported score for LongBench is the average performance across all constituent tasks. Long-Sequence Modeling. To evaluate models ability to maintain generative fidelity over long dependencies, we use perplexity (PPL). Perplexity measures how well probability model predicts sample. For sequence of tokens = (w1, w2, . . . , wN ), PPL is defined as the exponential of the average negative log-likelihood in Equation 9. lower PPL indicates better model, as it signifies higher confidence and accuracy in predicting the next token. (cid:32) PPL(W ) = exp (cid:33) log (wiw<i)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (9) Long CoT Reasoning. For complex mathematical reasoning tasks such as MATH500 and AIME2024, we evaluate the models performance in long-generation setting. This paradigm is distinct from conventional long-context understanding tasks. Instead of processing long static input, the model must maintain logical coherence and track thought traces across an extended, auto-regressive generation process to produce the correct final answer. Performance is reported as pass@1 (Dai et al., 2026). For MATH500, we report pass@1, where single generation is sampled for each problem. For AIME2024, which features more challenging problems, we also report pass@1, but the result is determined by checking if at least one correct answer exists within = 16 independent generations for each question. This sampling strategy is standard for estimating performance on complex reasoning benchmarks. Figure 12: The FASA Pipeline: An Efficient, FlashAttention-Compatible Approach. The algorithm details our two-stage process. key design feature is that the FAC stage seamlessly integrates with the standard FlashAttention API, leveraging its performance while enabling sparse computation. B.4 IMPLEMENT DETAILS Implementation Details Our implementation of FASA is built upon the HuggingFace Transformers library (Wolf et al., 2020). We employ non-invasive monkey patching approach to integrate our 19 Published as conference paper at ICLR 2026 logic. Specifically, we intercept the forward pass of the FlashAttention2 class within the models modeling.py file. The core of our method resides in two components. First, leveraging the universal nature of dominant FCs, their pre-computed indices are stored in globally accessible dictionary, shared across all layers and heads. Second, the Token Importance Prediction (TIP) logic, which performs the critical token selection, is encapsulated within our core_module_with_padding function. key advantage of our design is its simplicity and minimal intrusion. The integration requires inserting just single line of code, the token selection logic, into the original attention function, making FASA easy to deploy and adapt. This minimal intrusion makes FASA highly portable and easy to adapt. The corresponding pseudocode is provided in Figure 12."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "C.1 PERFORMANCE ANALYSIS ON DIFFERENT BUDGETS Figure 13: FASA on Qwen2.5-7B-Instruct under various token budgets (Ntip = 16). Figure 14: FASA on Meta-3.1-Llama-8B-Instruct under various token budgets (Ntip = 16). Comparison with Low-Rank Methods closely related work to FASA is SparQ (Ribar et al., 2024), which also performs form of dimension selection. SparQ operates on the heuristic that high-magnitude dimensions in query vector are the most indicative of importance, and thus selects corresponding key dimensions as proxy for token prediction. However, as our experiments in Figure 15 demonstrate, this heuristic proves to be poor substitute for true contextual awareness. Under constrained budget of 256 tokens, SparQs performance collapses, indicating its inability to reliably identify critical tokens based solely on query magnitudes. Furthermore, from an efficiency standpoint, SparQ incurs significant overhead as it must re-evaluate high-magnitude dimensions for every new query. In stark contrast, FASA leverages one-time, offline calibration, making its per-token inference cost substantially lower."
        },
        {
            "title": "D DISCUSSION ON FASA",
            "content": "D.1 VARIANTS OF FASA FASA-M (Memory-Optimized) The memory-optimized variant, FASA-M, is specifically engineered for scenarios with constrained GPU memory, such as consumer-grade hardware. As detailed in Algorithm 2, its core strategy is to minimize the on-GPU memory footprint by strategically keeping only the most essential data on the GPU. 20 Published as conference paper at ICLR 2026 Figure 15: Comparision with SparQ on LongBench. Specifically, only the dominant parts of the Key cache (C dom key ), which are required for the initial token importance prediction, are retained in GPU memory. The non-dominant parts of the Key cache (C nondom ) and the entire Value cache (Cval) are offloaded to and managed in the much larger CPU key memory. During the Focused Attention Computation (FAC) stage, once the critical token indices (Tt) are identified, only the small, required subsets of the non-dominant key and value caches are transferred from the CPU to the GPU for the final attention calculation. This \"just-in-time\" data transfer ensures that the GPU memory is primarily occupied by the most critical components, leading to substantial memory savings. Memory Footprint Analysis The GPU memory footprint of the KV cache in FASA-M can be formulated as follows. Let be the total sequence length, the token budget, the models hidden dimension, and Nlayers the number of layers. Let ddom be the dimension of the dominant FCs and dnondom be the dimension of the non-dominant FCs (d = ddom + dnondom). The memory occupied by the KV cache on the GPU is: MemGPU Nlayers ddom (cid:125) (cid:123)(cid:122) (cid:124) Dominant Keys + dnondom (cid:125) (cid:123)(cid:122) (cid:124) Non-dominant Keys bytes_per_param + (cid:124) (cid:123)(cid:122) (cid:125) Values (10) Compared to full KV cache, which occupies Nlayers 2d bytes_per_param, FASA-M significantly reduces the memory burden, especially when the non-dominant and value components constitute large portion of the cache. For instance, if ddom is 25% of and the budget is 10% of L, the memory savings can be substantial, approaching an 8 reduction in typical configurations. D.2 DESIGN CHOICES On the Role of FC-Scores: Proxy for Ranking, Not Substitute for Attention. crucial design principle we validated is that our FC-based scores (Sl,h ) are not calibrated to function as direct attention weights. Although they provide remarkably accurate relative ranking of token importance, their direct substitution for attention probabilities leads to catastrophic performance degradation. This reveals their fundamental role as selectora mechanism to identify salient tokens rather than an approximator of the final attention distribution. On the Indivisibility of Frequency Chunks. We investigated whether individual dimensions could serve as selection units, and the answer is definitive no. pipeline based on selecting \"dominant dimensions\" suffers catastrophic performance degradation. This empirically validates that the Frequency Chunk (FC) is an indivisible functional unit for this process. This principle is not coincidental but is direct corollary of RoPEs core mechanism, which encodes position by applying rotations to coupled pairs of dimensions. Disrupting these pairs severs the positional encoding, leading to model failure. In summary, these two findings underscore two core design principles of FASA. First, an efficient proxy for token importance does not necessarily serve as valid substitute for attention weights. Second, any optimization for RoPE-based models must respect the inherent coupling of dimension pairs, treating the Frequency Chunk as an indivisible functional unit. 21 Published as conference paper at ICLR 2026 D.3 ALGORITHM ON FASA See the algorithm of offline calibration in Algorithm 1; see the algorithm of FASA-M in Algorithm 2. Algorithm 1: Offline Calibration for Dominant FCs Input: calibration dataset Ω; number of dominant FCs to select k. Output: The set of dominant FC indices, Idom. // Stage 1: Collect Contextual Agreement (CA) scores Initialize an empty map to store CA scores for each (l, h, i) triplet foreach example in Ω do foreach token generation step do foreach layer do foreach head do Compute full attention scores αl,h(qt, K1:t) foreach FC index do Compute single-FC scores α(i) Calculate the CA score CAl,h,i Store CAl,h,i in [l][h][i] l,h(qt, K1:t) using Eq. 4 end end end end end // Stage 2: Select Dominant FCs Initialize an empty map for mean CA scores foreach (l, h, i) in do [l][h][i] Mean(M [l][h][i]) end Idom TopK-Indices(M , k) // Select top-k indices based on CA return Idom"
        },
        {
            "title": "E LLM USAGE",
            "content": "During the preparation of this manuscript, we utilized the AI-based language model ChatGPT, developed by OpenAI. Its use was strictly limited to language refinement, including grammar correction, stylistic enhancement, and rephrasing for clarity. All scientific concepts, experimental designs, data analyses, and conclusions presented herein are the original work of the authors and were conceived and executed without any substantive contribution from the language model. 22 Published as conference paper at ICLR 2026 Algorithm 2: Inference with FASA-M (Memory-Optimized Variant) Input: Current query qt; Current key kt; Current value vt Dominant FC indices Idom Token budget Past KV cache: dom Output: Next hidden state ht+1 Updated KV cache: dom key (GPU), nondom (CPU), Cval (CPU) key , nondom key , Cval key t ) 1:t 1:t key , Tt) (K dom 1:t ) , knondom , knondom Split(kt, Idom) SelectTokens(K dom Select(qt, Idom) 1:t UpdateCache(C dom UpdateCache(C nondom LoadFromCPU(C nondom // Stage 1: Token Importance Prediction (TIP) // Split key by dominant FCs kdom // Select corresponding query dimensions qdom dom key , kdom // Approximate scores using dominant parts ˆSt qdom // Identify indices of most salient tokens Tt TopK-Indices(ˆSt, b) // Stage 2: Focused Attention Computation (FAC) // Select dominant key parts on GPU dom Tt // Update non-dominant cache on CPU nondom key nondom // Select non-dominant key parts on CPU nondom , Tt) // Update value cache on CPU Cval UpdateCache(Cval, vt) V1:t LoadFromCPU(Cval) // Select values on CPU VTt SelectTokens(V1:t, Tt) // Offload required non-dominant keys to GPU nondom TransferToGPU(K nondom // Offload required values to GPU VTt TransferToGPU(VTt ) // Reconstruct full keys for selected tokens KTt Combine(K dom , nondom Tt // Compute full attention on the subset αfac Softmax(qtK dk) Tt ht+1 WO(αfacVTt) return ht+1 and updated caches SelectTokens(K nondom , Idom) / key ) 1:t Tt Tt Tt Tt ) )"
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Texas A&M University",
        "UCSD",
        "UIUC"
    ]
}