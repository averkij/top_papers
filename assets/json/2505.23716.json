{
    "paper_title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "authors": [
        "Lihan Jiang",
        "Yucheng Mao",
        "Linning Xu",
        "Tao Lu",
        "Kerui Ren",
        "Yichen Jin",
        "Xudong Xu",
        "Mulin Yu",
        "Jiangmiao Pang",
        "Feng Zhao",
        "Dahua Lin",
        "Bo Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/"
        },
        {
            "title": "Start",
            "content": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views Lihan Jiang1,2* Yucheng Mao2* Xudong Xu2 Mulin Yu2 Linning Xu3 Tao Lu4 Kerui Ren2,5 Yichen Jin2 Jiangmiao Pang2 Feng Zhao1 Dahua Lin3 Bo Dai6 5 2 0 2 9 2 ] . [ 1 6 1 7 3 2 . 5 0 5 2 : r 1The University of Science and Technology of China 2Shanghai Artificial Intelligence Laboratory 3The Chinese University of Hong Kong 4Brown University 5Shanghai Jiao Tong University 6The University of Hong Kong Figure 1. AnySplat lifts multi-view captures, from sparse to dense, into ready-to-view 3D scenes represented with 3D Gausssians [17]. Unlike previous multi-view reconstruction and neural rendering methods, which rely on precise camera calibration, tedious per-scene optimization, and are often sensitive to input noise, AnySplat robustly handles wide variety of capture scenarios in just seconds."
        },
        {
            "title": "Abstract",
            "content": "We introduce AnySplat, feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense viewsour model predicts everything in one shot. single forward pass yields set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baseEqual contribution, Alphabetical Order. Corresponding author. lines in both sparseand dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduce rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/. 1. Introduction Recent advances in 3D foundation models [38, 40, 47] have reshaped how we view the problem of reconstructing 3D scenes from 2D images. By inferring dense point clouds from single view to thousands within seconds, these methods streamline or even eliminate traditional multi-stage reconstruction pipelines, making 3D scene reconstruction more accessible across wider range of applications. Despite their powerful geometry priors, current foundation models often struggle to capture fine detail, photorealism, and geometric consistencyespecially when processing highly overlapping inputs, which can yield misaligned or noisy reconstructions. By contrast, novel-view synthesis (NVS) methods such as NeRF [26] and its recent extensions [17] deliver exceptional rendering fidelity, but only by offloading the hard work to costly preprocessing stage. These pipelines first estimate camera poses via structurefrom-motion and then perform per-scene neural field optimization. This delay between capture and usable output, along with computation costs that grow with the number of input frames, limits their practical applicability in many real-world scenarios. Witnessing this paradigm shift brought by feed-forward architectures like ViT [7] in 3D modeling, we ask: can novel-view synthesis (NVS) from multiview captures naturally benefit? To bridge the gap between geometry priors and ready-to-see output, as examplified in Fig. 1, we augment the foundation model with lightweight rendering head. During training, this head refines and synthesizes appearance via self-supervised proxy task, no ground-truth 3D annotations required, thereby injecting texture priors and enforcing geometric coherence in single, end-to-end pass. This training strategy paves the way for extending the reach of 3D foundation models [38, 40, 47] far beyond finite, annotated datasetsenabling seamless generalization to unbounded new scenes with minimal overhead. Specifically, we propose AnySplat, feed-forward network for novel view synthesis trained on unconstrained and unposed multi-view images. AnySplat employs geometry transformer to encode these images into high-dimensional features, which are then decoded into Gaussian parameters and camera poses. To improve efficiency, we introduce differentiable voxelization module that merges pixel-wise Gaussian primitives into voxel-wise Gaussians, eliminating 3070% of redundant primitives while maintaining comparable rendering quality. Since 3D annotations in real-world scenarios are often noisy, we design novel self-supervised knowledge distillation pipeline. In this framework, we distill camera and geometry priors from pretrained VGGT [38] backbone as external supervision. As result, AnySplat can be trained without any 3D SfM or MVS supervision, relying solely on uncalibrated images, make it promising to scale up to unconstrained capture at hands. We train AnySplat on nine diverse and large-scale datasets, exposing the model to wide range of geometric and appearance variations. As result, our method demonstrates superior zero-shot generalization performance on unseen datasets. Experimental results show that AnySplat achieves excellent novel view synthesis quality, more consistent geometry, more accurate pose estimation, and faster inference times compared to both state-of-the-art feed-forward and optimization-based methods. In summary, our key contributions are: Feed-forward reconstruction and rendering. Our model takes uncalibrated multi-view inputs and simultaneously predicts 3D Gaussian primitives and their camera intrinsics/extrinsics, delivering higher-quality reconstructions than prior feed-forward methods, and even outperforming optimization-based pipelines in challenging scenarios. Efficient self-supervised knowledge distillation. We distill geometry priors from pretrained VGGT model via novel, end-to-end training pipeline, without any 3D annotations, unlocking high-fidelity rendering and enhanced multi-view consistency in under one day on 816 GPUs. Differentiable voxel-guided Gaussian pruning. Our custom voxelization strategy eliminates 3070% of Gaussian primitives while preserving rendering quality, yielding unified, compute-efficient model that gracefully handles both sparse and dense capture setups. 2. Related Work 2.1. Optimization-based Novel View Synthesis"
        },
        {
            "title": "Methods",
            "content": "Neural Radiance Fields (NeRF) [1, 26, 27] pioneered highquality novel view synthesis by learning continuous volumetric density and radiance fields via coordinate-based networks, but its reliance on expensive volume rendering precludes real-time performance. In contrast, 3D Gaussian Splatting (3DGS) [14, 17, 22, 31, 52] explicitly represents scenes with millions of anisotropic Gaussians and exploits differentiable rasterization to render photorealistic views at over 30 FPS (1080p). Its core advancesadaptive density control for geometry refinement and spherical harmonics for view-dependent shadingenable real-time playback. Despite these advances, most NeRF and 3DGS methods assume access to accurate camera poses, typically obtained via classical Structure-from-Motion tools such as COLMAP [34]. This requirement introduces an implicit preprocessing step that conceals the significant time and logistical costs of large-scale, multi-view data acquisition and registration. To address these limitations, recent approaches attempt to jointly optimize camera poses and scene representation. However, they either require incremental image sequences and intrinsics [10, 16, 23, 24, 46] as input or are limited to scenarios with minimal motion or sparse view coverage [8]. Furthermore, these methods still involve redundant optimization processes. In contrast, AnySplat can directly predict 3D Gaussians and camera parameters within seconds, significantly accelerating the 3D reconstruction process. 2 2.2. Generalizable 3D Reconstruction Methods 3.1. Problem Setup Most view synthesis methods require tens of minutes or even hours to optimize on densely captured data. Recently, several generalizable 3D reconstruction methods have been proposed, which can be broadly categorized into two types: pose-aware methods, which assume known camera parameters, and pose-free methods, which jointly infer both geometry and camera poses. Pose-aware generalizable methods. Pose-aware generalizable methods rapidly reconstructed 3D models from calibrated images and their corresponding poses. These approaches can be broadly categorized into three method- (1) 3D Gaussian Splatting based techological strands: niques [35, 41, 44] which directly predict 3D Gaussian primitive as the scene representation, (2) neural network based frameworks [9, 15] employing neural network to infer the appearance of the novel view image without any 3D representation, and (3) the emerging LRM architecture family [12, 53]. Despite these pose-aware reconstruction methods significantly reducing optimization time and improving performance under sparse-view conditions, their broader applicability remains limited due to the necessity for accurate image poses as input. Pose-free generalizable methods. To achieve truly endto-end 3D reconstruction, pose-free generalizable methods rely solely on images as input, and most of them simultaneously predict image poses alongside the reconstructed 3D model. Among them, DUSt3R [40] and extended by MASt3R [19], replace traditional multi-stage pipelines with single large-scale model that jointly predicts depth and fuses it into dense scene. More recent methods, including CUT3R [39], VGGT [38], and Fast3R [47], cascade transformer blocks to jointly infer camera poses, point trajectories, and scene geometry in single forward pass, achieving substantial improvements in both accuracy and runtime. While these methods highlight the potential for efficiently scaling up 3D asset reconstruction, they generally struggle in poor texture representation and multi-view misalignment problem, which significantly hinder their novel view synthesis performance. 3. Method We propose AnySplat, transformer-based neural network designed for rapid 3D scene reconstruction tailored for novel-view synthesis. Given any number of uncalibrated images, from single view up to hundreds, AnySplat directly predicts set of 3D Gaussian primitives that compactly represent the reconstructed scene. In the following sections, we first formalize our problem setup in Sec. 3.1, detail the models architecture and pipeline in Sec. 3.2, and finally present our training and inference strategies in Sec. 3.3. 3 Consider uncalibrated views of single 3D scene, given i=1, where Ii RHW 3, AnySplat aims to as images {Ii}N jointly reconstruct the scene geometry and appearance by predicting a) collection of anisotropic 3D Gaussians (cid:8)(µg, σg, rg, sg, cg)(cid:9)G g=1, where each Gaussian is parameterized by center position µ R3, positive opacity σ R+, an orientation quaternion R4, an anisotropic scale R3, and color embedding R3(k+1)2 represented via spherical-harmonic coefficients of degree k, following practice of [17]; and 2) the camera parameters for each view {pi R9}N i=1, with pi encoding the intrinsics and extrinsics of image Ii. Formally, our model implements the mapping: (cid:110) (cid:111)G g=1 {pi}N fθ : {Ii}N i=1 (cid:55) (µg, σg, rg, sg, cg) i=1. (1) We evaluate our model on two core tasks: novel view synthesis and multi-view camera pose estimation. Notably, this pipeline also produces several useful by-productssuch as global point map, per-frame depth maps, and associated confidence scoresthat can support variety of downstream applications. 3.2. Pipeline Fig. 2 illustrates the overall pipeline of framework. In nutshell, our model begin by encoding set of uncalibrated multi-view images into high-dimensional feature representations, which are then decoded into both 3D Gaussian parameters and their corresponding camera poses. To manage the linear growth in per-pixel Gaussians under dense views, we introduce differentiable voxelization module that clusters primitives into voxels, significantly reducing computational cost and facilitating smoother gradient flow. Geometry Transformer. Following VGGT [38], we begin by patchifying each image Ii into lI = tokens p2 of dimension using DINOv2 [28], where = 14 and RlI d, = 1024. To each images token sequence tI we prepend learnable camera token tg R1d and four R4d; for the first view only, we omit register tokens tR positional encodings on these tokens. The combined tokens (cid:2)tI (cid:3) from all views are processed by an Llayer Alternating-Attention transformer: each layer applies frame attention over tokens of shape RN (lI +5)d, then global attention over all views jointly as R1N (lI +5)d. ; tR i ; tg Figure 2. Overview of AnySplat. Starting from set of uncalibrated images, transformer-based geometry encoder is followed by three decoder heads: FG, FD, and FC , which respectively predict the Gaussian parameters (µ, σ, r, s, c), the depth map D, and the camera poses p. These outputs are used to construct set of pixel-wise 3D Gaussians, which is then voxelized into pre-voxel 3D Gaussians with the proposed Differentiable Voxelization module. From the voxelized 3D Gaussians, multi-view images and depth maps are subsequently rendered. The rendered images are supervised using an RGB loss against the ground truth image, while the rendered depth maps, along with the decoded depth and camera poses p, are used to compute geometry losses. The geometries are supervised by pseudo-geometry priors ( D, p) obtained by the pretrained VGGT [38]. Camera Pose Prediction. Camera pose estimation is essential for self-supervised geometry reconstruction via novel-view rendering. The refined camera tokens ˆtg are passed through the camera decoder FC, which consists of four additional self-attention layers followed by linear projection head, to predict each camera parameters pi. As in prior work, we set the first camera pose to the identity transformation and express all remaining poses in that shared local coordinate frame. Pixel-wise Gaussian Parameter Prediction. As shown in Fig. 2, we adopt dual-head design based on the DPT decoder [29] to predict all Gaussian parameters. The depth head, FD, ingests the image tokens ˆtI and outputs per-pixel depth maps Di (with associated confidence these depths are then back-projected through the predicted camera poses pi to yield each Gaussians center {µg}G g=1. The Gaussian head FG combines DPT features via Fd(ˆtI ) with shallow CNNextracted appearance features Fa(I), and feeds their sum into final regression CNN Fb to predict opacity σg, orientation rg, scale sg, SH color coefficients cg, and per-gaussian confidence Cg. Formally: ); (Di, i ) = FD(ˆtI ), {µg} = proj(cid:0){pi}, {Di}(cid:1), (cid:0)F2 d({ˆtI }) + Fa({Ii})(cid:1). { σg, rg, sg, cg, Cg} = Fb (2) Differentiable Voxelization. Existing forward 3DGS methods typically assign one Gaussian per pixel, feed 4 which works for sparse-view inputs (216 images) but struggles with scaled-up complexity once more than 32 views are used. To address this, build upon [22], we introduce differentiable voxelization module that clusters the Gaussian centers {µg} into voxels of size ϵ: {V s}S s=1 = (cid:36) {µg}G ϵ g=1 (cid:39) , (3) where {1, . . . , S} denotes the voxel index of Gaussian g. To keep voxelization differentiable, each Gaussian also predicts confidence Cg. We convert these scores into intra-voxel weights via softmax: wgs = exp(Cg) h:V h=s exp(Ch) (cid:80) . (4) Finally, any per-pixel Gaussian attribute ag (e.g., opacity or color) is aggregated into its voxel by (cid:88) as = wgs ag. g:V g=s (5) The output of our pipeline is parameterized by the Gaussian attribute {(µv, σv, rv, sv, cv)}s of each voxel {1, . . . , S}. We can efficiently render the Gaussians predicted by our model using differentiable gaussian rasterization [17, 50]. This strategy dramatically reduces the number of primitives to process and yields smoother gradients for end-to-end learning. 3.3. Training and Inference Geometry Consistency Enhancement. Predicting depth maps and camera poses simultaneously introduces subtle ambiguities that stem from multiview alignment and aggregation: when lifting per-image predictions to 3D, these inconsistencies manifest as layered sheets in the reconstructed point cloud, which may go unnoticed in raw point-cloud form but become glaringly obvious in rendered views. Such layering not only degrades visual fidelity but also prevents our outputs from meeting human-perceptual quality standards. To mitigate this, we introduce geometry consistency loss that enforces agreement between rendered appearances and the underlying depth predictions, effectively smoothing out these layers and restoring coherent surface geometry. Specifically, we enforce alignment between the depth maps Di obtained from the DPT head FD and the rendered depth maps ˆDi from 3D Gaussians. Since Di can be unreliable in challenging regions, e.g., the sky or reflective surfaces, we utilize the jointly learned confidence map and apply supervision only to the top % of pixels with the confidence, ensuring that supervision focuses on the most trustworthy predictions. We align two depth maps as: Lg = 1 (cid:88) (Di[M ] ˆDi[M ])2, i=1 (6) where is geometry mask corresponding to the top - quantile of the confidence map, we set = 30% in our experiments. Furthermore, we observed that, in the absence of supervision from novel views, the model tends to overfit to context views in an attempt to avoid interference from varying viewpoints. This results in poor generalization and leads to failures in depth and camera prediction. To mitigate this, we leverage powerful pre-trained transformer network [38] to distill both camera parameters and scene geometry for stable training. Specifically, we regularize the camera parameters using the following loss function: Lp = 1 (cid:88) i=1 pi piϵ , (7) Training Objective. To avoid noises in the input data and better scale up data, AnySplat is trained without any 3D supervision, using self-supervised training approach. Specifically, given set of unposed and uncalibrated multiview images {Ii}N i=1 as input, our method first predicts their camera intrinsics and extrinsics. These predicted parameters are first used to project the positions of Gaussian primitives, and then rendered to produce the final outputs { ˆIi}N i=1. Note that, although our model trains with only context views without novel views, AnySplat presents great performance in novel view rendering due to the distill functions and great scene modeling capacity. Finally, we optimize our model using set of unposed images. We minimize the following loss function: = Lrgb + λ2 Lg + λ3 Lp + λ4 Ld Lrgb = MSE(I, ˆI) + λ1 Perceptual(I, ˆI) (9) Test-Time Camera Pose Alignment (Calculating rendering metrics only). During inference, both the context views Ic and target views It are provided as inputs, where Ic It = . We assume the first frame of Ic is identical to the first frame of Ic It. Consequently, the rotation of Ic and the context portion of Ic It remains the same; the only distinction lies in their scale. To address this, we compute the average context scale factor from Ic and the average scale factor ˆs from Ic It. The target scale is then normalized by multiplying it by the ratio s/ˆs. Post Optimization (Optional). We also include an optional post-optimization stage to further refine reconstructions, especially when inputs are dense. After AnySplat predicts the initial set of Gaussians and camera parameters, we first prune Gaussians with low opacity value (less than 0.01), and then render images from the input camera views and compute the MSE loss and the SSIM loss between the rendered and input images. We back-propagate the gradients through the Gaussian and camera parameters. The learning rates are set as follows: 1.6e-4 for position, 5e-3 for scale, 1e-3 for rotation, 5e-2 for opacity, 2.5e-3 for color, and 5e-3 for camera pose. where pi represents the pseudo ground-truth pose encoding, and ϵ denotes the Huber loss. We then distill geometric information using: 4. Experiments 4.1. Experimental Setup Ld = 1 (cid:88) ( Di[M ] ˆDi[M ])2, i=1 (8) where is the pseudo depth map obtained from the pretrained model [38]. Experimental results show that this distillation loss significantly improves training stability and helps avoid convergence to poor local minima. Datasets. Following CUT3R [39] and DUST3R [40], we train our model using images from nine public datasets: Hypersim [32], ARKitScenes [2], BlendedMVS [48], ScanNet++ [51], CO3D-v2 [30], Objaverse [6], Unreal4K [36], WildRGBD [43], and DL3DV [21]. These datasets collectively span synthetic and real-world content, indoor and outdoor scenes, and objectto city-scale settings. This diverse 5 Figure 3. Qualitative comparisons against baseline methods: for sparse-view inputs, we benchmark against the state-of-the-art FLARE [55] and NoPoSplat [49]; for dense-view inputs, we include 3DGS [17] and MipSplatting [52] as representative comparisons. The slight misalignment between the rendered novel-views and the ground-truth is likely caused by pose-free reconstruction methods estimated pose not perfectly matching the annotated groud-truth camera poses. data composition exposes the model to wide-ranging geometric and appearance variations, enhancing its generalization to unseen scenarios. Training View Sampling Strategy. View-sampling strategy is crucial for model robustness. We apply three different strategies based on the dataset type. For objectcentric datasets such as CO3D-v2 [30], Objaverse [6], and WildRGBD [43], we randomly sample views within selected capture sequence. For sequential datasets like ARKitScenes [2] and DL3DV [21], we first define minimum and maximum temporal gaps, then randomly select value within this range to determine the interval between the first and last frames; views are then randomly sampled from within this interval. For unordered datasets like Hypersim [32], BlendedMVS [48], ScanNet++ [51], and Unreal4K [36], we randomly choose reference frame, compute the pose distance from all other frames to this reference, and sample views based on predefined distance threshold. Implementation details. We set layer number = 24 for the Alternating-Attention Transformer and initialize the geometry transformer and depth DPT head with weights from VGGT [38], while the remaining layers are initialized randomly. During training, we freeze the patch embedding weights. The model has approximately 886 million parameters in total. For differentiable voxelization, we set the voxel size ϵ to 0.002. We train the model using the AdamW optimizer for 15K iterations. cosine learning rate scheduler is employed, with peak learning rate of 2e-4 and warmup phase of 1K 6 Table 1. Training Datasets Statistics. We report the sampling distribution over our nine training datasets: at each iteration, we randomly select one dataset according to the probabilities listed in the Prob column, which reflects the relative frequency with which each dataset is drawn during training. Dataset Scene Type Real/Synthetic # of Frames # of Scenes Training Prob. (%) ARKitScenes [2] ScanNet++ [51] BlendedMVS [48] Unreal4K [36] CO3Dv2 [30] DL3DV [21] WildRGBD [43] Objaverse [6] Hypersim [32] Indoor Indoor Mixed Mixed Object Mixed Object Object Indoor Real Real Real Synthetic Real Real Real Synthetic Synthetic 9.2M 1.0M 114K 16K 5.5M 3.4M 3.9M 8M 73K 4406 935 467 18 27520 9894 11050 199K 744 16.7 16.7 8.3 8.3 8.3 16.7 8.3 8.3 8.3 iterations. For layers initialized from VGGT, the learning rate is scaled by factor of 0.1. We train AnySplat on 16 NVIDIA A800 GPUs for approximately one day. To save GPU memory and accelerate training, we use FlashAttention, bfloat16 precision, and gradient checkpointing. For stable training, we also skip optimization steps where the total loss exceeds 0.2 after the first 1K iterations. In each iteration, we first select training dataset at random, where each dataset is sampled according to predefined weight  (Fig. 1)  . From the chosen dataset, we randomly sample between 2 and 24 frames, while maintaining constant total of 24 frames per GPU. The maximum input resolution is set to 448 pixels on the longer side. The aspect ratio is randomized between 0.5 and 1.0. Additionally, we apply intrinsic augmentation by randomly center-cropping each image to between 77% and 100% of its original size. Images are also augmented via random flipping. For the training objective, we set λ1 = 0.05, λ2=0.1, λ3=10.0, and λ4=1.0. Baselines. We establish our sparse-view novel view synthesis baseline using previous state-of-the-art pose-free feed-forward methods, including FLARE [55] and NoPoSplat [49]. For each evaluation dataset, we select three sparse-view subsets per scene. For VRNeRF [45] dataset, we randomly seleect 5 scene for evaluation, we use all the 2 scene proviede by Deep-Blending [11] dataset. To partition each scene inot none-overlapped input and test views, we first random choose rederence camera, then compute pose-distance score for every other cameras. Cameras are ranked in descending order. Given target split of input views and test views, we take the top + ranked cameras and randomly assign of them as inputs and the remaining as tests. Our evaluation spans both sparse-view and dense-view in the sparse regime, we use input novel-view synthesis: configurations of 3, 6, and 16 views with 2, 3, and 4 heldout test views respectively, sampling three distinct subsets per scene on VRNeRF [45] and one subset per scene on Deep-Blending [11] due to its limited overlap; in the dense regime, we employ 32, 48, and 64 input views with 4, 5, and 6 held-out views, fixing one high-density subset per scene to stress-test reconstruction quality. Notably, prior methods require post-optimization step during evaluation to align predicted camera poses with ground truth. However, we observe that this often failsespecially when there is limited overlap between training viewsand can even degrade performance by overfitting to regions not visible during training. To ensure fair comparison, we propose more robust alignment strategy: we fix the first predicted camera as the identity and transform all other predicted rotations into this reference coordinate system. For translation alignment, we compute the median camera distance and estimate relative scale factor to align the predicted and ground truth translations. For our denseview novel view synthesis baseline, we compare against 3D Gaussian Splatting [17] and Mip-Splatting [52], which both train on 30K iterations. We use 32, 48, and 64 views for training, and select 4, 6, and 8 views for evaluation, respectively. Training and testing views are jointly sampled based on camera distance. Since COLMAP [33] is often unreliable under sparse-view conditions, we use VGGT to calibrate the input images and generate point cloud for initialization. Metrics. To evaluate the quality of novel view synthesis, we compute PSNR, SSIM [42], and LPIPS [54] between the predicted images and the ground truth. Additionally, to assess the accuracy of the predicted relative image poses, we use the AUC metric, which measures the area under the accuracy curve across various angular thresholds. In our evaluation, we set thresholds as 5, 10, 20, and 30. Furthermore, to evaluate multi-view geometric consistency, we report two widely used depth consistency metrics: the Absolute Mean Relative Error (AbsRel), defined as: AbsRel = 1 (cid:88) i=1 ˆDi Di Di , and the δ1 accuracy, which measures the percentage of pixels where max (cid:33) (cid:32) ˆDi Di , Di ˆDi < 1.25. 4.2. Novel-view Synthesis Compared to prior pose-free feed-forward methods, which are typically limited to sparse-view inputs (e.g., 224 images), and optimization-based approaches that require up to 10 minutes per scene for dense-view reconstruction, our model generalizes to hundreds of input views and reconstructs 3D Gaussian primitives within just few seconds on unseen scenes. We quantitatively evaluate our method against previous approaches on two zero-shot novel view synthesis datasets: Deep-Blending [11] and VRNeRF [45], under both sparse-view and dense-view settings. 7 Table 2. Quantitative Comparison on Sparse View NVS Setting. We compare our method with baseline approaches under sparseview setting, where the number of input images is fewer than 16. We report both 3D scene reconstruction time and rendering quality metrics. We highlight the best and second-best results. We omit reporting the Times for Deep Blending, as its timings are consistent with the input values (same applied to Tab. 3). Method 3 Views 6 Views 16 Views PSNR SSIM LPIPS Time(s) PSNR SSIM LPIPS Time(s) PSNR SSIM LPIPS Time(s) VRNeRF [45] Dataset NoPoSplat [49] Flare [55] Ours 18.37 18.58 20.63 0.707 0.717 0.738 Deep Blending [11] Dataset NoPoSplat [49] Flare [55] Ours 14.99 18.12 19.83 0.443 0.562 0.579 0.437 0.470 0. 0.612 0.513 0.321 0.119 0.271 0.171 17.57 18.26 21.57 15.38 16.83 21.29 0.704 0.717 0. 0.367 0.477 0.508 0.466 0.477 0.356 0.668 0.605 0.345 0.290 0.415 0.297 17.66 17.02 22. 13.22 16.05 18.53 0.720 0.709 0.784 0.457 0.611 0.524 0.472 0.510 0.304 0.729 0.617 0.435 1.198 1.201 0. Table 3. Quantitative comparison on Dense View Novel View Synthesis. We compare our method with baselines under dense-view setting, where the number of input images is more than 32. We report both the 3D scene reconstruction time and the resulting rendering quality. The best and second-best results are highlighted. Table 4. Camera Pose Estimation on CO3Dv2 [30] with 10 random frames. Method AUC@5 AUC@10 AUC@20 AUC@30 Flare [55] VGGT [38] Ours 27.4 57.9 57.0 50.2 74.4 73.9 69.5 84.9 84.8 77.6 89.1 89.0 Post Optimization. Although AnySplat can efficiently perform end-to-end reconstruction of high-quality Gaussian models, further improvements can be achieved through an optional post-optimization step. As shown in Fig. 5 and Tab. 5, we conduct 200 input views experiment on the Matrixcity dataset [20]. We demonstrate that even with 200 input, applying just 1000 steps of post-optimization (taking less than two minutes) yields improved results and 3000 steps can achieve much better results. Fig 5 shows the qualitative results of our post optimization experiment. Dataset/Method 32 Views 48 Views 64 Views PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time 4.3. Pose Estimation and Multi-view Geometry VRNeRF [45] Dataset 3D-GS [17] Mip-Splatting [52] Ours 22.37 22.41 23.09 Deep Blending [11] Dataset 15.20 3D-GS [17] 15.25 Mip-Splatting [52] 15.75 Ours 0.774 0.768 0.781 0.528 0.521 0.479 0.302 0.316 0.230 0.490 0.504 0. 10min 11min 1.4s 22.86 22.55 22.58 17.78 17.52 17.98 0.780 0.772 0.785 0.575 0.586 0. 0.306 0.314 0.238 0.400 0.451 0.365 10min 11min 2.7s 22.10 21.75 22.13 17.11 16.83 17. 0.770 0.760 0.779 0.557 0.537 0.553 0.315 0.326 0.250 0.471 0.488 0.425 10min 11min 4.1s As shown in Tab. 2 and Fig. 3, AnySplat achieves improved rendering performance on sparse-view zero-shot datasets compared to recent feed-forward methods such as NoPoSplat [49] and Flare [55]. There are two main reasons for this performance: 1) AnySplat is trained on diverse set of datasets and incorporates random input view selection strategy, which contributes to its superior zero-shot generalization; 2) it achieves more accurate geometry and pose estimation, and since rendering quality strongly depends on pose accuracy, this leads to better visual results. Moreover, with an increasing number of input views, our approach demonstrates faster inference times, which is important for real-world application. In the dense-view setting (more than 32 views ), AnySplat continues to outperform optimization-based methods such as 3D-GS [17] and Mip-Splatting [52] (with VGGT initialization), as shown in Tab. 3 and Fig. 3. These optimization-based methods tend to overfit the training views, often resulting in artifacts in novel views. In contrast, our method reconstructs finer, cleaner geometry and delivers more detailed rendering results. Furthermore, AnySplat achieves reconstruction times that are an order of magnitude faster than those of 3D-GS and Mip-Splatting. For more novel-view synthesis result, please refer to Fig 4."
        },
        {
            "title": "Consistency",
            "content": "AnySplat can be applied to relative pose estimation task. We evaluate it in feed-forward setting on the CO3Dv2 dataset using 10 randomly selected frames and compare its performance with VGGT and Flare, as shown in Tab. 4. Our method achieves accuracy only slightly lower than VGGT, as we trade small amount of pose accuracy for improved 3D multiview consistency. However, it significantly outperforms Flare, demonstrating its superior performance. In addition to pose estimation, we assess the multi-view geometric consistency of our approach. While VGGT [38] demonstrates strong performance in monocular depth prediction, it often suffers from poor consistency across views due to the lack of explicit 3D geometry constraints and its sensitivity to low-confidence regions, particularly around In contrast, our method leverages 3D object boundaries. rendering supervision to significantly enhance multi-view consistency. To evaluate this effect, we compare the depth maps rendered from Gaussians ( ˆDi) with those predicted by the DPT head (Di) at both the beginning and end of training on the Hypersim dataset. As illustrated in Fig. 6, the alignment between the two depth sources improves notably over training iterations, highlighting the effectiveness of our self-supervised training strategy. 4.4. Ablation Study In this section, we ablate each individual module to validate their effectiveness. We conduct all the experiments based on the Hypersim Dataset. Quantitative and qualitative results can be found in Tab. 6. 8 Figure 4. Example visualization of our AnySplat reconstruction and novel-view synthesis across spectrum of scene complexities and input frames densities. From top to bottom, the number of input images increasesfrom extremely sparse to medium and dense captures, while the scene scale grows from object-centric setups (LLFF [25], DTU [13]) through mid-scale trajectories (MegaNeRF [37], LERF [18], HorizonGS [14]) to large-scale indoor and outdoor environments (VR-NeRF [45], Waymo [35]). For each setting, we display the input views, the reconstructed 3D Gaussians, the corresponding ground-truth renderings, and example novel-view renderings. Distill Loss. To evaluate the impact of the distillation losses defined in Eq. 7 and 8, we perform an ablation study by removing them from training. As shown in Table 6, this leads to significant drop in both rendering quality and geometric consistency. The results suggest that, in the absence of external supervision and when trained solely on 9 Figure 5. Improved Rendering with Post-Optimization. In our experiments using 200 input views, an optional post-optimization stage yields noticeably higher rendering fidelity, particularly in dense-view scenarios. Figure 6. Improvements of Multiview Consistency. From the initial iteration to 10 training steps, we observe marked enhancement in multiview geometry consistency, clearly visible in the depth renderings, across both the models outputs and the 3D Gaussian Splatting renderings. This confirms the effectiveness of our geometry consistency enhancement design. Table 5. Quantitative comparison with 200 views on Matrixcity dataset [20]. We compare our method, as well as its variants with 1K and 3K iters of post-optimization (Ours 1000 and Ours 3000), against 3DGS and Mip-Splatting. Method PSNR SSIM LPIPS Time 3D-GS Mip-Splatting Ours Ours 1000 Ours 3000 19.10 18.20 19.46 20.81 21.64 0.614 0.556 0.574 0.635 0. 0.450 0.485 0.446 0.519 0.421 10min 11min 33s 2min 7min Table 6. Ablation Study. We evaluate the ablated variants of AnySplat, discussed in Sec. 4.4, by recording their rendering quality, geometric accuracy, and the size of the resulting Gaussian models. We highlight the best and second-best results. Method PSNR SSIM LPIPS AbsRel δ1 #GS (M) Ours w/o Distill Loss Ours w/o Geo. Loss Ours w/o Diff. Voxel Ours frozen AA transformer layers Ours frozen all transformer layers Ours 7.28 18.20 17.77 17.90 17.84 18.25 0.217 0.635 0.609 0.616 0.621 0.648 0.832 0.285 0.303 0.306 0.330 0.279 75.5 94.7 95.8 96.5 95.3 96.3 14.7 7.6 5.7 5.3 6.6 5.9 4.80 3.52 4.82 3.51 3.40 3. unposed images, the model tends to overfit to the context views. Consequently, each view becomes isolated, resulting in deformed and layered geometry and degraded novel view synthesis performance. Geometry Consistency Loss. We further demonstrate the effectiveness of our geometry consistency loss, defined in Eq. 6, by comparing it against variant of our model trained with only the rendering and distillation losses. As shown in the second and last rows of Table 6, incorporating the consistency loss encourages the model to produce more coherent multi-view geometry, resulting in 1.7% reduction in AbsRel and 1.6% improvement in δ1 accuracy. Fig 6 further shows some qualitative results Differentiable Voxelization. To evaluate the impact of the differentiable voxelization module introduced in Sec. 3.2, we conduct an experiment in which this component is removed. Interestingly, the model achieves slightly better performance despite using fewer Gaussian primitives. This improvement can be attributed to the voxelization modules ability to reduce redundancy among Gaussians and alleviate artifacts caused by overlapping primitives. Furthermore, as illustrated in Fig. 7, when differentiable voxelization is used, the number of Gaussians increases more slowly with the number of context views and eventually reaches saturation. This leads to lower GPU memory consumption during rendering compared to pixelwise rendering approaches. Training strategy. We investigate different training strategies by exploring three alternative configurations: (1) Frozen All Transformer: All transformer layers initialized from VGGT are frozen during training, while the remaining parameters are trainable; (2) Frozen AA Transformer: Only the Alternating-Attention layers are frozen, while the vision tokenizer is fine-tuned; (3) Frozen Vision Tokenizer: The vision tokenizer is frozen, and only the Alternating-Attention layers are fine-tuned; Our empirical results show that the third configuration yields the best performance, achieving PSNR gains of 0.41 dB and 0.35 dB over configuration"
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 2 [2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 5, 6, 7 [3] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image In Propairs for scalable generalizable 3d reconstruction. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 3 [4] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [5] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. arXiv preprint arXiv:2411.04924, 2024. 3 [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 5, 6, 7 [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. [8] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2(3):4, 2024. 2 [9] John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clement Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, et al. Quark: Real-time, high-resolution, and general neural view synthesis. ACM Transactions on Graphics (TOG), 43(6):120, 2024. 3 [10] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2079620805, 2024. 2 [11] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 7, 8 Figure 7. Growth of Gaussian Primitives and GPU Memory Usage. As the number of input views increases, the count of Gaussian primitives grows sublinearly and eventually plateaus when using the differentiable voxelization module. In contrast, without this module, the number of Gaussians increases approximately linearly. The GPU memory consumption for rendering mirrors this saturation behavior. and 2, respectively. These findings suggest that preserving pre-trained visual representations while adapting the attention mechanism provides an effective balance between stability and adaptability during training. 5. Conclusion and Future Works In this work, we introduce AnySplat, feed-forward 3D reconstruction model that integrates lightweight rendering head with our geometry-consistency enhancement, augmented by self-supervised rendering proxy and knowledge distillation. We view this as novel way to fully unlock the potential of 3D foundation models and elevate their scalability to broader scope. Our experiments demonstrate AnySplats robust and competitive results on both sparse and dense multiview reconstruction and rendering benchmarks using unconstrained, uncalibrated inputs. Additionally, the model training remains efficient, requiring minimal time and compute, enabling feed-forward 3D Gaussian Splatting reconstructions and high-fidelity renderings in just seconds at inference time. We expect this low-latency pipeline to open new possibilities for future interactive and real-time 3D applications. Despite its improvements, AnySplat still observe artifacts in challenging regions, such as skies, specular highlights, and thin structures; its reconstruction-based rendering loss may be less stable under dynamic scenes or varying illumination, and the computeresolution trade-off (i.e., number of Gaussians scaling alongside input and voxel resolution) can slow performance when handling very high resolution or large numbers of views. Future work should incorporate more diverse real-world captures and high-quality synthetic datasets, featuring varied camera trajectories and scene complexities from object-centric to unbounded environments. On the technical side, enhancing patch-size flexibility, improving robustness to repetitive texture patterns, and streamlining the scaling to thousands of high-resolution inputs, especially paired with rapid advances in mobile capture technology, offer particularly promising directions. 11 [12] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [13] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406413, 2014. 9 [14] Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, and Bo Dai. Horizongs: Unified 3d gaussian splatting for large-scale aerial-toground scenes. arXiv preprint arXiv:2412.01745, 2024. 2, 9 [15] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. 3 [16] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2135721366, 2024. [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2, 3, 4, 6, 7, 8 [18] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In Proceedings of the IEEE/CVF Internaradiance fields. tional Conference on Computer Vision, pages 1972919739, 2023. 9 [19] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 3 [20] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32053215, 2023. 8, 10 [21] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 5, 6, 7 [22] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2, [23] Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, and Andrew J. Davison. Gaussian Splatting SLAM. 2024. 2 [24] Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, and George Drettakis. On-the-fly reconstruction for large-scale novel view synthesis from unposed images. ACM Transactions on Graphics, 44(4), 2025. 2 [25] Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):114, 2019. 9 [26] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [27] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2 [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [29] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. [30] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation In Proceedings of of real-life 3d category reconstruction. the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 5, 6, 7, 8 [31] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 2 [32] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 5, 6, 7 [33] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. 7 [34] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 2 [35] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [36] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 89428952, 2021. 5, 6, 7 [37] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large12 Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. 6, 7, 8 [50] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. gsplat: An open-source library for gaussian splatting. Journal of Machine Learning Research, 26(34):117, 2025. 4 [51] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 5, 6, 7 [52] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1944719456, 2024. 2, 6, 7, 8 [53] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 3 [54] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [55] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. arXiv preprint arXiv:2502.12138, 2025. 6, 7, 8 In Proceedings of scale nerfs for virtual fly-throughs. the IEEE/CVF conference on computer vision and pattern recognition, pages 1292212931, 2022. 9 [38] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: arXiv preprint Visual geometry grounded transformer. arXiv:2503.11651, 2025. 1, 2, 3, 4, 5, 6, 8 [39] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d arXiv preprint perception model with persistent state. arXiv:2501.12387, 2025. 3, 5 [40] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 1, 2, 3, 5 [41] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes. Advances in Neural Information Processing Systems, 37:107326107349, 2024. [42] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [43] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2237822389, 2024. 5, 6, 7 [44] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. 3 [45] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, et al. Vr-nerf: HighIn SIGGRAPH Asia fidelity virtualized walkable spaces. 2023 Conference Papers, pages 112, 2023. 7, 8, 9 [46] Chi Yan, Delin Qu, Dan Xu, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li. Gs-slam: Dense visual slam with 3d In Proceedings of the IEEE/CVF Congaussian splatting. ference on Computer Vision and Pattern Recognition, pages 1959519604, 2024. 2 [47] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. 1, 2, [48] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: largescale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17901799, 2020. 5, 6, 7 [49] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem:"
        }
    ],
    "affiliations": [
        "Brown University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "The University of Science and Technology of China"
    ]
}