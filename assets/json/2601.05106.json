{
    "paper_title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
        "Nuoya Xiong",
        "Yuhang Zhou",
        "Hanqing Zeng",
        "Zhaorun Chen",
        "Furong Huang",
        "Shuchao Bi",
        "Lizhu Zhang",
        "Zhuokai Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 6 0 1 5 0 . 1 0 6 2 : r Token-Level LLM Collaboration via FusionRoute Nuoya Xiong1,3,, Yuhang Zhou1, Hanqing Zeng1, Zhaorun Chen4, Furong Huang5, Shuchao Bi2, Lizhu Zhang1,, Zhuokai Zhao1, 1Meta AI, 2Meta TBD Lab, 3Carnegie Mellon University, 4University of Chicago, 5University of Maryland Work done at Meta, Joint last author Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, robust and effective token-level multi-LLM collaboration framework in which lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes complementary logit that refines or corrects the selected experts next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequenceand token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks. Date: January 9, 2026 Correspondence: First and last authors at nuoyax@andrew.cmu.edu and {zhuokai, lizhu}@meta.com Project Page: https://github.com/xiongny/FusionRoute"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have demonstrated strong performance across an extensive range of tasks, such as mathematical reasoning (Wang et al., 2025b; Zhao et al., 2025; Zhou et al., 2025), code generation (Deng et al., 2025; Huynh and Lin, 2025; Qi et al., 2024; Jiang et al., 2024b; Jimenez et al., 2023), and instruction following (Wang et al., 2025a; Qin et al., 2024). Although sufficiently large, general-purpose LLMs can often deliver balanced performance across diverse domains (Hurst et al., 2024; Team et al., 2025; Yang et al., 2025; Zeng et al., 2025), their computational and monetary costs make them not ideal for every real-world applications. This motivates an alternative paradigm: employing multiple smaller, specialized LLMs, each excelling in particular domain while remaining comparatively efficient. However, the strengths of such smaller LLMs often remain unevenan LLM that performs exceptionally well on one task may Figure 1 Sequence-level collaboration is coarse and inefficient, while prior token-level methods are unstable. FusionRoute achieves fine-grained, efficient, and robust token-level collaboration through complementary routing. 1 underperform on others due to inductive biases (Levine et al., 2021; Si et al., 2023) and domain-specific training distributions (Yuan et al., 2023). As result, achieving general-purpose performance through collection of specialized models hinges on developing mechanisms that can robustly coordinate and leverage their complementary expertise, which has emerged as central challenge in building efficient and broadly capable LLM systems. natural direction toward such collaboration is mixture-of-experts (MoE), in which multiple experts are integrated into unified architecture and trained jointly with routing network (Zhou et al., 2022; Xue et al., 2024; Jiang et al., 2024a; Zeng et al.). While effective, this framework is expensive and inflexible since it typically needs gradient access to all experts and substantial additional end-to-end training, and it usually requires expert models to have similar structures. Another line of work aims to combine the strengths of specialized models through multi-agent systems (MAS), where different LLMs are assigned different roles with the goal of leveraging the best capabilities of each model or agent during generation (Du et al., 2023; Zheng et al., 2025). However, without prior knowledge of which types of questions each model is best suited for, determining how tasks should be allocated in advance becomes inherently challenging. Moreover, existing MAS (Liu et al., 2024; Liang et al., 2024; Ashiga et al., 2025; Huang et al., 2024; Chen et al., 2025) typically operate at coarse response level, where each agent independently generates full response and final answer is selected, merged or subjected to debate post hoc. This approach is inefficient, as it requires multiple full-sequence generations, and can even degrade performance when more agents and discussions are involved due to the heavily increased context length (Zheng et al., 2025). third direction is model merging (Yang et al., 2024; He et al., 2025), which combines multiple specialized models into single set of parameters. Although training-free and architecturally simple, model merging is often sensitive to hyperparameters and suffers from parameter interference, where the merged weights degrade each models specialized capabilities, making it unable to adaptively emphasize different expert behaviors in changing scenarios. To overcome these limitations, recent work has explored token-level multi-agent collaboration, more fine-grained and dynamic paradigm that allows multiple LLMs to jointly produce each token during generation Chakraborty et al. (2025). However, the effectiveness of these works depends heavily on the quality of the underlying models, making them not robust when expert models perform poorly, or the selection strategy is not correct on certain tasks or tokens. These limitations lead to the following key question: Can we develop token-level collaboration paradigm that works robustly, efficiently, and automatically across all scenarios? To address this question, we propose FusionRoute, token-level collaboration framework that unifies expert selection and knowledge complementation within single, lightweight router LLM. FusionRoute learns to identify which specialized model is most suitable for generation each next token, enabling fine-grained, context-dependent use of expert capabilities without requiring expensive joint training or per-token evaluation across multiple models (Zhou et al., 2022). At the same time, FusionRoute is trained to provide its own complementary generation signal, allowing it to refine or correct an expert whenever the expert is uncertain or unreliable. By fusing the routers corrective logits with those of the selected expert during decoding, FusionRoute achieves both robustness by mitigating expert failures, and efficiency through avoiding the overhead that limits prior multi-LLM collaboration methods. This dual design allows FusionRoute to function as an automatic, domain-agnostic coordination mechanism, enabling consistent performance improvements across diverse tasks and datasets. In summary, we have the following contributions: We propose FusionRoute, token-level approach that automatically selects the most suitable expert model at each decoding step while simultaneously providing complementary logit for better generation. Theoretically, we show that expert-only token-level collaboration as in (Chakraborty et al., 2025) cannot in general attain the optimal value function unless strong assumptions hold, revealing fundamental limitation of prior methods. In contrast, FusionRoutes complementary generator overcomes this limitation and enables recovery of the optimal policy. Empirically, FusionRoute consistently outperforms strong baselinesincluding token-level collaboration methods, model merging, and even finetuned modelsacross diverse expert domains (mathematics, coding, and instruction following) as well as general-purpose evaluations. These results highlight FusionRoutes robustness, efficiency, and broad applicability in scenarios where expert strengths differ."
        },
        {
            "title": "2.1 LLM Decoding\nWe formalize the decoding process of a language model as sampling from an autoregressive policy π. Let X\ndenote the space of prompts and Y the vocabulary. A response is represented as a sequence y = (y1, . . . , yT ) ∈\nY T . Given a prompt x ∈ X , the language model induces a conditional distribution over responses through\nthe policy",
            "content": "π(y x) = (cid:89) t=1 π(yt x, y<t), where π(yt x, y<t) specifies the probability of generating token yt given the prompt and the preceding tokens. In both the empirical part and the theoretical part of our paper, we consider the greedy decoding, since it is the simplest and effective way for decoding. To be more specific, at each step greedy decoding selects the token with the highest conditional probability under the policy π: yt = arg max yY π(y x, y<t)."
        },
        {
            "title": "2.2 SFT and RLHF",
            "content": "Supervised Fine-Tuning (SFT) serves as the initialization stage of fine-tuning the language model, where the model is trained to imitate human-written demonstrations using supervised learning. This step enables the model to acquire basic capabilities on one dataset and produce high-quality responses before preference-based alignment is applied. Given the fine-tuned model πref, one widely adopted RLHF approach after that is DPO (Rafailov et al., 2023), which further aligns the model to human preferences using human-annotated preference pairs to improve the general response quality. Formally, given preference pairs (x, y+, y) where y+ and represent the preferred and dispreferred response respectively, DPO derives closed-form objective from the BradleyTerry model, enabling policy updates through purely supervised loss: LDPO(θ) = log σ β (cid:18) (cid:20) log πθ(y+x) log πref(y+x) log πθ(yx) log πref(yx) (cid:21)(cid:19) , (1) where β controls the strength of alignment."
        },
        {
            "title": "3.1 Overall Design",
            "content": "FusionRoute aims to enable effective token-level collaboration among multiple specialized LLMs by selecting the most suitable expert at each decoding step, while remaining robust when individual experts produce unreliable outputs. Pure expert selection is often insufficient: even strong specialist models may fail on out-of-domain queries, and relying solely on expert logits makes the decoding process brittle. FusionRoute addresses this challenge by introducing router that simultaneously identifies the appropriate expert and provides complementary logit capable of correcting or refining the experts prediction. The FusionRoute router model πθ is post-trained from base LLM parameterized by θLM. Given prompt and partial generation yt, FusionRoute processes the sequence and produces two outputs: vector of routing weights wθ Rn, which determines the preferred expert from set of specialized LLMs {π1, . . . , πn}, and set of logits log πθLM ( x, yt), which act as complementary corrective component. The routing weights are generated via lightweight linear projection applied to the final hidden state hθLM (x, yt), namely W, hθLM(x, yt). Despite its simplicity, this mechanism enables token-level expert assignments that adapt smoothly to the evolving context. During inference, FusionRoute first selects the expert with the highest routing weight, and uses πexpert = πI θ = arg maxi wθ,i, as the selected specialist model for the current step. The final next-token distribution θ Figure 2 Top: Overall architectural design of FusionRoute. FusionRoute enables multiple specialized LLMs to work collaboratively through carefully designed router. The router outputs both the weights for token-level routing and the logits to complement the experts output. Bottom: The training process is decomposed into two phases. In the SFT phase (3.2.1), the router learns token-wise mixture weights over the expert models and is jointly fine-tuned to become good response generator. In the CDPO phase (3.2.2), FusionRoute refines the final policy by using preference optimization to improve upon the experts outputs. is then obtained by combining the routers complementary logits with those of the selected expert through logit addition, log πfinal( x, yt) = log πθLM ( x, yt) + log πexpert( x, yt). The next token yt+1 is generated greedily from πfinal. This design preserves the domain-specific strengths of the selected expert while allowing the router to refine, correct, or override expert behavior when necessary. Because the router and routing layer are trained on mixed-domain data, the complementary logits also help anchor generation to distributions observed during training, improving stability and robustness in settings where expert outputs alone are unreliable. Conceptually, FusionRoute transforms token-level collaboration from brittle expert-selection problem into more expressive policy that blends fixed expert knowledge with learned, adaptive corrective component. As shown in 4, this complementary mechanism is essential for overcoming the fundamental limitations of purely expert-based token-level collaboration. (2)"
        },
        {
            "title": "3.2 Training FusionRoute",
            "content": "Having introduced the routing-and-complementation framework of FusionRoute, we now describe how the router is trained. Training FusionRoute is non-trivial since the router must simultaneously satisfy two coupled objectives: 1) it must reliably select the most suitable expert at each token, while 2) also providing complementary logit contribution that corrects expert failures when necessary. Naively optimizing these objectives together can lead to unstable behavior, where improvements in complementary generation degrade routing quality or vice versa. To address this challenge, we adopt staged and decoupled training strategy consisting of supervised fine-tuning (SFT) followed by modified preference-optimization phase. 4 3.2.1 Supervised fine-tuning (SFT) We first perform SFT to establish two foundational properties of the router: next-token prediction capability and token-level expert selection. Importantly, this phase is not intended to train the routers complementary behavior, but rather to produce stable routing mechanism aligned with expert specialization. During SFT, we jointly optimize the base LLM parameters θLM and the routing projection using combination of standard language modeling loss and routing loss. The language modeling loss LLM(θLM; x, y) given the data point (x, y) follows the standard SFT objective and encourages accurate next-token prediction on the training data. To enable the token-level routing ability, we introduce routing loss that favors experts whose predictions are more consistent with the ground-truth sequence. However, not all tokens are informative for routing. Many tokens, such as punctuation or common function words, are predicted identically by all experts and therefore provide little signal about expert specialization. Supervising routing on these tokens can dominate the gradient and bias the router toward uninformative agreement patterns. To mitigate this issue, we restrict routing supervision to token positions where experts disagree. For given prefix (x, yt), let yi,t+1 = arg maxy log πi(y x, yt) denote the next-token prediction of expert πi. We define the informative token set as = {t [T ] : = such that yi,t+1 = yi,t+1}. (3) For these positions, at token t, we first compute the router weight wt θ and compute the aggregated logits as log πagg( x, yt) = (cid:88) i=1 wt θ,i log πi( x, yt), (4) where the weight wt θ, and define the routing loss θ = (wt θ,1, , wt θ,n) represents the output of the linear layer for token under parameter Lexpert(θ; x, y) = (cid:88) iS log πagg(yi x, yi1). The final SFT objective therefore is LSFT(θ) = E(x,y)DSFT [LLM(θLM; x, y) + λLexpert(θ; x, y)] , (5) (6) where λ is some hyperparameter for balancing the scale of two losses. By restricting supervision to informative tokens, this objective ensures that routing gradients are driven by meaningful expert disagreements, allowing the router to capture expert specialization without being dominated by trivial agreement cases. 3.2.2 Complemented Direct Preference Optimization (CDPO) While SFT establishes routing behavior and maintain next-token prediction capability, it does not address how the router should correct expert outputs when the selected expert is locally suboptimal. In practice, experts may produce unreliable logits even when correctly selected. To further improve response quality, we employ an additional preference optimization phase to encourage the router to actively learn complementary logit contribution while treating expert outputs as fixed. We refer to this preference-optimization stage as Complemented Direct Preference Optimization (CDPO). During inference, the final policy is obtained by combining the router and the selected expert model logits through logits addition, as shown in Eq. (2), so that the router can refine or correct the experts output when necessary. To align training with this decoding behavior, we introduce preference-optimization objective that applies Direct Preference Optimization (DPO) (Rafailov et al., 2023) to the routers base model parameters 5 Algorithm 1 Mix Training 1: Initial: Dataset DSFT, DDPO, SFT router πθ, batch size B, learning rate η. 2: Mix two datasets randomly to get Dmix. 3: for = 1, 2, , do 4: 5: 6: 7: 8: Receive batch of data points {d1, , dB} Dmix. Set = 0. for = 1, 2, , do if di = (x, y) DSFT then = + λLLM(θ; di). else = + LCDPO(θLM; di). end if end for Update the parameter θ = θ ηθL. 9: 10: 11: 12: 13: end for θLM. Given preference pairs (x, yw, yl) for the same prompt x, the objective is defined as LCDPO(θLM) = Ex,yw,ylDDPO log σ (cid:34) (cid:32) (cid:88) β log t0 (cid:32) (cid:34) = Ex,yw,ylDDPO log σ πfinal(yt+1 yw,t, x) πref(yt+1 yw,t, x) (cid:88) t0 β log πfinal(yt+1 yl,t, x) πref(yt+1 yl,t, x) (cid:33)(cid:35) (cid:16) (cid:88) t0 (cid:124) β log πθLM (yt+1 yw,t, x) πref(yt+1 yw,t, x) (cid:88) t0 β log (cid:123)(cid:122) (A) (cid:17) πθLM(yt+1 yl,t, x) πref(yt+1 yl,t, x) (cid:125) (cid:16) (cid:88) + t0 (cid:124) β log πexpert(yt+1 yw,t, x) β log πexpert(yt+1 yl,t, x)) (cid:33)(cid:35) (cid:17) , (cid:88) t0 (7) (cid:123)(cid:122) (B) (cid:125) where πref denotes the router after the SFT phase, σ(x) = (1 + ex)1 is the sigmoid function, and β controls the strength of preference alignment. Importantly, we do not backward propagate any gradient for term (B), hence it can be regarded as an extra token-level bias. Optimizing this loss naturally encourages the router to provide complementary logit contribution that compensates for the weaknesses in the expert models. For example, if the expert πexpert already provides strong policy for given prompt x, the log-ratio gap contributed by the expert term (term (B)) is already large, resulting in small gradient with respect to θLM. Conversely, when πexpert is weak for prompt x, the log-ratio gap shrinks, producing larger gradient on θLM that encourages the router to provide stronger corrective logits. As result, the router refines expert predictions precisely when needed, improving robustness while aligning training with the collaborative decoding behavior used at inference time. 3.2.3 Mix Training Since preference optimization updates the routers base model parameters θLM, it can indirectly influence the routing behavior through shared representations. It is therefore necessary to preserve both reliable expert selection and effective complementary logit refinement during training. naive approach would be to apply the DPO objective directly to the entire router, including its final linear routing layer. However, doing so often leads to unstable or degenerate routing behavior, where the linear routing layer overfits to preference-alignment signals and loses its ability to correctly select among experts. To address this issue, we adopt decoupled optimization strategy. Specifically, for SFT samples, all parameters, including the routing layer, are updated using the routing loss LLM. On the other hand, we apply the preference-optimization objective (Eq. (7)) only to the routers base model parameters θLM, while excluding the routing projection. To ensure consistency between the base model and the linear layer, we 6 jointly mix preference-optimization data with supervised SFT data during training. This mixed training scheme preserves expert-selection capability while enabling the router to acquire an effective complementary logit contribution. The full procedure is summarized in Algorithm 1."
        },
        {
            "title": "4.1 Token-level Markov Decision Process\nWe formulate the decoding process as a token-level Markov Decision Process (MDP) M = {S, A, P, r}, where\nS is the state space, A is the action space, P is the transition kernel, and r defines the reward function. In\nthe language model setting, let the prompt space be X and the vocabulary be Y. Starting from an initial\nprompt s0 = x, at step t the state is defined by st = (x, y≤t), and the action is represented by the next token\nat = yt+1. The transition kernel is then defined by P(st+1 | st, at) = I{st+1 = (x, y≤t, yt+1)}. Finally, the\nreward function r(s, a) = r(x, y≤t+1) ∈ [0, 1] is a token-level reward function that maps any text (x, y≤t+1) to\na real number. We also denote R(x, y≤t) = (cid:80)t\ni=1 r(x, y≤i) is the total reward function of the prefix (x, y≤t).\nA language model can be represented as a policy π(a | s) = π(yt+1 | x, y≤t), where it takes the previous text\n(x, y≤t) as the input and generates the next token yt+1. Let τ = (x, y1, · · · , yT ) denote the trajectory, which\nis indeed a full response generated by a language model, the value function V π(s) = V π(x, y≤t) for a state\ns = (x, y≤t) can be defined as",
            "content": "V π(x, yt) = Eτ π(x,yt) (cid:35) r(x, yi) . (cid:34) (cid:88) i=t+1 Similarly, for = yt+1, the function can be defined as Qπ(x, yt, yt+1) = r(x, yt+1) + Eτ π(x,yt+1) (cid:35) r(x, yi+1) . (cid:34) (cid:88) i=t+"
        },
        {
            "title": "4.2 Token-Level Routing through the Lens of Performance Difference Lemma",
            "content": "In this section, we establish conceptual connection between the Performance Difference Lemma (PDL) (Kakade and Langford, 2002) and our token-level routing training , clarifying how supervised routing can be interpreted through the lens of value-function approximation in token-level MDP. At high level, PDL characterizes how deviations from the optimal policy at individual decision steps accumulate into global performance gap. When applied to language model decoding, which is viewed as token-level MDP as shown in 4.1, this lemma suggests that if, at each token position, the routing mechanism selects an expert whose next-token distribution approximately maximizes the optimal action-value function, then the resulting sequence-level policy will be near-optimal. Our routing loss in the SFT phase can be viewed as an imitation-based approximation of this idealized per-token selection rule, trained on trajectories generated by an optimal (or near-optimal) policy. To formalize this connection, we analyze an idealized setting in which the SFT dataset is generated by the optimal policy π( s), where π achieves the optimal value function in the token-level MDP = (S, A, P, r). Hence, the optimal next token at step is given by yt+1 arg maxaY Qπ (x, yt, a), and we refer to Qπ as the optimal Q-function. Since we trained on the SFT dataset which follows trajectories generated by π, at step with prefix (x, yt), the router can be viewed as approximating an expert action that aligns with the action preferred by π , i.e., the action that maximizes the optimal Q-function along the observed trajectory. We assume that the router is well-trained so that the expert model chooses the correct model for each prefix, namely, πexpert( x, yt) = πI ( x, yt), where = arg max Eyt+1πi(x,yt)[Qπ (x, yt, yt+1)]. (8) We also make the following assumption to state that for any prefix x, yt, there is one correct model that achieves approximate optimal value function. 7 Assumption 4.1 (Global Coverage Assumption). For any x, yt, there exists an [n] such that (cid:12) (cid:12) (cid:12) Eyt+1πi(x,yt)[Qπ (x, yt+1)] (x, yt) (cid:12) (cid:12) (cid:12) , (9) where represents the error between the optimal policy and the candidate model. This assumption states that the expert set is assumed to be sufficiently expressive so that one expert always achieves an approximately optimal value. This assumption can be regarded as sanity bound that characterizes the best-case performance of purely token-level routing under unrealistically strong coverage conditions. It assumes the existence of near-optimal expert at each token. First, we introduce the performance difference lemma, which decomposes the value difference into the sum of the Q-function gaps on individual steps. Lemma 4.2 (Performance Difference Lemma (Kakade and Langford, 2002)). Denote ρ (X ) as distribution over the prompt space. Suppose each response has length . Then, for any two policies π, π, we have (cid:104) Exρ π (x) π(x) (cid:105) = 1 (cid:88) t=0 Exρ,ytπ(x) (cid:2)V (x, yt) Eyt+1π(x,yt)[Q(x, yt, yt+1)](cid:3) . Following the Assumption 4.1 and Lemma 4.2, we can get (cid:104) Exρ π (x) π(x) (cid:105) 1 (cid:88) t= = , (10) which means that when O(1/T ), an idealized token-level routing policy can achieve near-optimal expected return under Assumption 4.1. The Eq. (10) shows that well-trained purely token-level routing mechanism can possibly recover near-optimal trajectory by selecting the appropriate expert at each step. We also discuss the difference between our approach and the prior token-level collaboration approach Collab (Chakraborty et al., 2025) in Appendix B. However, it is worth noting that Assumption 4.1 is relatively strong, since it requires global coverage that must hold for all prefixes, including those that may not appear in the supervised training data. Because the router is trained only on trajectories generated by the expert policy, satisfying this condition requires generalization beyond the observed data. more natural assumption is to make the assumption only on the data generated by optimal policy π. In the next section, we show that this relaxed assumption is insufficient for generating an approximately optimal response, demonstrating the fundamental limitations of purely token-level collaboration."
        },
        {
            "title": "4.3 Limitations of Purely Token-Level Collaboration",
            "content": "In the previous subsection, we showed that router training can recover the ideal action. However, Assumption 4.1 must hold for every possible prefix x, yt. This assumption is rather restrictive, as the routing network is trained solely on the training dataset and may therefore suffer from distribution shift. more reasonable requirement is to enforce Eq. (9) only for good prefixes x, yt. We consider two types of prefixes to be good prefixes. First, any prefix x, yt π is considered as good prefix, which we call the single policy coverage assumption. Moreover, any prefix that has the possibility to generate good response is also considered as good prefix, which we call the Generalization Coverage assumption. Together, these two types of prefixes are intended to characterize the kinds of text commonly encountered in the training data. However, under this weaker assumption, we show that although there may exist way to select candidate model that yields near-optimal response, such selection rule cannot be recovered solely from observations of the value function Qπ . Theorem 4.3. Denote {n1, n2, , nt} as path of length t, where ni [n], 1 t. Assume that all π and {πi}i[m] are deterministic policy, the response length is fixed as , and the transition function is also deterministic. Suppose the learners observation space Ot contains (cid:110) x, yt, {Qπ (x, yt, y)}yY (x, yk)}t ot = (cid:111) . k=1, {Qπ 8 at the visited states (x, yt) during decoding. Suppose > 0 is error constant. Then, there is MDP such that no token-level routing algorithm : (cid:55) [n] can achieve πA π /2 + 2 for all possible rewards in this MDP, even if we have the following two good properties: 1. Existence of Near-Optimal Path. There exists small ε [0, 1] and at least one possible token-level routing strategy = {n 1, 2, , } such that π = + ε. (11) 2. Single Policy Coverage. For the optimal policy π, the error constant [0, 1] such that the following approximation holds: Ex,ytπ (cid:12) (cid:12) (cid:12)arg max Eyt+1πi(x,yt) (cid:2)Qπ (x, yt+1)(cid:3) Eyt+1π(x,yt) (cid:2)Qπ (x, yt+1)(cid:3)(cid:12) (cid:12) (cid:12) . (12) 3. Generalization Coverage For any prefix x, yt, if there exists full response x, yT with prefix x, yt and R(x, yT ) π , we have (cid:12) (cid:12) (cid:12)arg max Eyt+1πi(x,yt) (cid:2)Qπ (x, yt+1)(cid:3) Eyt+1π(x,yt) (cid:2)Qπ (x, yt+1)(cid:3)(cid:12) (cid:12) (cid:12) . (13) The impossibility in Theorem 4.3 stems from an identifiability failure, where observing optimal values along trajectories generated by π is insufficient to determine which expert actions actually realize those values. This is actually closely related to known limitations in reinforcement learning and imitation learning, where value information or on-policy demonstrations alone do not uniquely identify optimal actions without additional realizability assumptions (Kakade and Langford, 2002; Ross et al., 2011; Levine et al., 2021) In fact, due to this identifiability failure, this theorem implies that designing reliable method for selecting the optimal model to produce the optimal response is inherently difficult. For example, Theorem 4.3 shows that using SFT to train router that chooses the optimal model is not reliable, because this is essentially equivalent to use behavior cloning for learning the actions that maximize the optimal value function Qπ . The proof is provided in Appendix A. 4.3.1 Benefits of Router Training From the previous theorem, we show that reliable performance cannot be guaranteed under single-policy coverage. The core difficulty is that even if prefix that may lead to good response (theoretically, state has large value Qπ ), this response may not be realizable by the available candidate models. For each token, the set of feasible actions is restricted by the fixed expert models. Consequently, whenever the optimal policy π and the expert models are not perfectly aligned, the resulting approximation error becomes uncontrollable. To solve this issue, we consider adding trainable part to our decoding process. We also train the πrouter to align the optimal model, and add the logits of πrouter as complementary part (Eq. (2)). Now, instead of assuming that some models can achieve approximately maximum optimal value, we can assume that the TV distance of the policy is bounded. In fact, assuming π πa πrouter is the combination of the router base model and the candidate model, we can assume Ex,ytπ [arg min a[M ] TV(π a( x, yt), π( x, yt))] . Then, we can easily get the value difference bound by PDL π π = (cid:88) t=0 ytPπ (x)[Eyt+1π(x,yt)Qπ h((x, yt), yt+1) Eyt+1π(x,yt)Qπ h((x, yt), yt+1)] Qmax RmaxT . Hence, the discussion above shows that adding router as complementary part is good way for overcoming the intrinsic difficulty for Theorem 4.3. This result highlights an important conceptual shift. Instead of requiring that one of the fixed expert models be capable of achieving near-optimal performance on its own, we allow router that serves both selection and complementation roles to compensate for the mismatch by contributing additional, adaptive logits. In this way, the combined policy effectively expands the expressivity of the action space, enabling it to approximate the optimal policy π even when none of the individual candidate models is sufficient. This demonstrates that making the router complementary component in the decoding process is not merely heuristic enhancement, but principled mechanism that circumvents the coverage assumption for all prefixes. Furthermore, when the optimal policy and candidate model are already reasonably well aligned, the routers complementary component need not be highly accurate; even weakly informative router can suffice. In this case, the router is substantially easier to learn, as it only needs to correct the limited set of states where the candidate model deviates from π. On well-aligned states, the candidate models already provide strong responses. Consequently, combining candidate model with trainable router can outperform both fine-tuning single model and pure routing."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first describe the experimental setup in 5.1. We then present cross-domain performance and win rates on general benchmarks in 5.2. Together, these evaluations demonstrate that our FusionRoute enables effective collaboration among multiple models by (a) selecting the most domain-appropriate models for accurate specialized responses and (b) delivering consistently high-quality outputs on general datasets."
        },
        {
            "title": "5.1 Experimental Setup\nBaselines. We compare FusionRoute against a broad set of baselines including sequence-level collaboration,\ntoken-level collaboration, model merging, and single-model fine-tuning. For sequence-level collaboration, we\ncompare with the common Sequence Selection, where each expert independently generates a full response\nusing greedy decoding and an external reward model selects the highest-scoring output. For the token-level\ncollaboration, we include Collab (Chakraborty et al., 2025), which performs controlled decoding by evaluating\ncandidate tokens from multiple models using an external reward signal; we follow the original decoding\nprocedure and replace only the reward model when necessary to ensure fair comparison across model families.\nTo evaluate training-free expert integration, we include two popular model merging approaches, DARE (Yu\net al., 2024) and TaskArithmetic (Ilharco et al., 2022). We additionally include a Fine-tuned Model baseline\nobtained by directly fine-tuning the base LLM using the same SFT and DPO procedures as FusionRoute,\nbut without any routing or collaboration mechanism, thereby isolating the benefit of token-level multi-LLM\ncollaboration and complementary logit contributions.",
            "content": "Models. We conduct experiments on both the Llama-3 (Dubey et al., 2024) and Gemma-2 (Team et al., 2024) families, using the model checkpoints provided by He et al. (2025). For each family, we select three expert models specializing in mathematics, coding, and instruction following, respectively. Specifically, for the Llama-3 family, we use math expert models Llama-3.1-8B-Instruct_math and coding expert models Llama-3.1-8B-Instruct_coding from MergeBench (He et al., 2025) as the two expert models, and Llama-3.1-8B-Instruct as the instruction-following expert. Following Chakraborty et al. (2025), we use the reward model Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback as the external reward for Collab (Chakraborty et al., 2025). For the Gemma-2 family, we use Gemma-2-2B-Instruct_math, Gemma2-2B-Instruct_coding and Gemma-2-2B-Instruct_instruction from MergeBench (He et al., 2025) as three expert models. For fair comparison, we use weqweasdas/RM-Gemma-2B as the external reward for Collab (Chakraborty et al., 2025), instead of larger 7B reward model. Implementation and training. We randomly select 500 samples from the PerfectBlend dataset (Xu et al., 2024) as held-out test set and use the remaining samples as the training pool. For the initial SFT phase, we randomly sample 200k examples from the PerfectBlend training split, which provides balanced mixture of mathematics, coding, and instruction-following tasks. For both the Llama and Gemma families, the router is initialized from the instruction-following expert and fine-tuned by minimizing the loss in Eq. (6) with λ = 1/3 10 and learning rate of 1 105 for one epoch. After obtaining the policy from the SFT phase, we further train the model using the mixed SFTDPO procedure in Algorithm 1. Specifically, we sample 100k examples from the PerfectBlend training split as DSFT and 100k preference pairs from the OpenHermes dataset (Teknium, 2023) as DDPO, using learning rate of 105, β = 0.1, and λ = 1/3 for one epoch. For the directly fine-tuned baseline, we apply the same SFT and DPO procedures and hyperparameters, but without training the routing linear layer. For model merging baselines, we use the default parameters = 0.9 and λ = 1 for DARE (Yu et al., 2024) and λ = 1 for TaskArithmetic (Ilharco et al., 2022)."
        },
        {
            "title": "5.2 Main Results",
            "content": "5.2.1 General-purpose Performance with Specialized Experts In practical deployment, users typically interact with single model without knowing in advance which domain such as mathematics, coding, or instruction followinga given prompt belongs to. We therefore evaluate FusionRoute in general-purpose, mixed-domain setting, where the model must automatically leverage the most appropriate expert at inference time, without any manual checkpoint selection. We consider five benchmark datasets covering diverse expert domains: GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023) for mathematical reasoning, MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) for code generation, and IfEval (Zhou et al., 2023) for instruction following. These datasets allow us to measure model performance across math, coding, and instruction-following tasks, providing comprehensive cross-domain evaluation. We use greedy decoding as the generation approach, more implementation details for each task are provided in Appendix C.1. Our results show that both algorithms consistently achieve the highest average performance across the five domains on both two LLM families and get performance comparable to domain-specific experts on individual tasks. Table 1 summarizes the results on both the Llama-3 and Gemma-2 families. Across both model families, FusionRoute achieves the highest average performance, consistently outperforming sequence-level collaboration, prior token-level collaboration (Collab), model merging methods, and directly fine-tuned models. These results demonstrate that FusionRoute effectively functions as general-purpose model assembled from specialized experts, relieving users from the need to select domain-specific checkpoints while delivering strong and stable performance across heterogeneous tasks. The examples of routing behavior in several domain-specific tasks are provided in Appendix C.2. Importantly, FusionRoute does not sacrifice specialization. On benchmarks where particular expert is expected to perform bestsuch as math experts on GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023) or coding experts on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)FusionRoute remains competitive and often matches or exceeds the corresponding experts performance. This shows that FusionRoute is compatible with expert specialization, while being substantially more robust and generalized in mixed-domain usage. 5.2.2 Performance on General Dataset Figure 3 GPT-4o winrate on general datasets compared to fine-tuned model. (a) Llama3-8B Family (b) Gemma2-2B Family 11 Method GSM8K MATH MBPP HumanEval IfEval Avg Acc. Math Expert Code Expert Instruct Expert Sequence Selection Collab Fine-tuned Model DARE TaskArithmetic FusionRoute (ours) Math Expert Code Expert Instruction Expert Sequence Selection Collab Fine-tuned Model TaskArithmetic DARE FusionRoute (ours) Llama-3 Family Expert Models 0.36 0.15 0.27 0.23 0.34 0. Collaboration Approaches 0.31 0.32 0.26 0.28 0.32 0.33 0.21 0.28 0.36 0.23 0.24 0.36 Gemma-2 Family Expert Models 0.27 0.16 0.08 0.22 0.27 0.23 Collaboration Approaches 0.24 0.22 0.17 0.15 0.10 0.22 0.27 0.26 0.24 0.14 0.14 0. 0.86 0.52 0.74 0.76 0.82 0.75 0.75 0.82 0.82 0.67 0.39 0.19 0.62 0.52 0.54 0.43 0.36 0. 0.37 0.66 0.52 0.45 0.54 0.58 0.26 0.39 0.63 0.36 0.41 0.34 0.37 0.35 0.34 0.28 0.16 0. 0.31 0.48 0.67 0.50 0.55 0.72 0.32 0.35 0.69 0.47 0.52 0.61 0.54 0.45 0.68 0.34 0.36 0. 0.426 0.430 0.512 0.466 0.502 0.536 0.368 0.424 0.566 0.398 0.350 0.290 0.408 0.360 0.394 0.268 0.224 0. Table 1 Cross-domain performance comparison on Llama-3 and Gemma-2 families. Best and second-best performance within each family are highlighted by bold and underline, respectively. To further assess overall response quality beyond task-specific correctness, we evaluate FusionRoute on heldout general dataset using pairwise comparisons. Specifically, we sample 500 prompts from the PerfectBlend (Xu et al., 2024) test set and generate responses with maximum length of 300 tokens. Then, we use GPT-4o to evaluate the overall response quality compared to the response of the directly fine-tuned model (Fine-tuned Model in Table 1). For two responses, the pairwise comparison is selected from win, tie and loss, and the win rate is computed as the proportion of wins, with ties counted as half win, over 500 evaluation examples. Addition details such as evaluation prompt provided in Appendix C.1. As shown in Fig. 3, FusionRoute achieves substantially higher win rate than the fine-tuned baseline on both model families, indicating improved overall response quality on general prompts. This improvement reflects better alignment, fluency, and formatting, complementing the strong task-level performance observed on domain benchmarks. 5.2.3 FusionRoute is More Beneficial at Larger Scale As shown in Fig. 3, the performance gap between FusionRoute and other baselines becomes markedly larger in the 8B Llama-3 family. In this setting, both existing token-level collaboration (Collab (Chakraborty et al., 2025)) and sequence-level selection exhibit substantial degradation in win rate, whereas FusionRoute continues to improve response quality. This suggests that as model capacity increases, purely selecting among fixed expert outputs becomes increasingly brittle, while FusionRoutes complementary routing mechanism is able to effectively leverage the additional representational capacity to refine and correct expert predictions. In contrast, in the 2B Gemma-2 family, the gap between FusionRoute and pure token-level collaboration is smaller. While expert-only token-level collaboration still under-performs FusionRoute, it remains competitive and even outperforms the directly fine-tuned baseline. This indicates that at smaller scales, expert models alone can already capture reasonable portion of the desired behavior, leaving less room for complementary correction. Overall, these results suggest that router-based complementary generation becomes increasingly important as model scale grows, whereas expert-only token-level collaboration may suffice in more capacity-constrained settings."
        },
        {
            "title": "6.1 Ablation on Complementary Logit Contribution",
            "content": "We first examine the role of the routers complementary logit contribution in FusionRoute. In particular, we aim to isolate whether token-level expert selection alone is sufficient for effective collaboration, or whether the routers ability to directly contribute corrective logits is necessary for strong performance. To this end, we construct routing-only baseline that removes the complementary component: we use the router obtained after the SFT phase to select an expert at each token, and generate the next token solely from the selected experts logits, without adding the routers logits. Table 2 reports the cross-domain performance of the routing-only variant compared with the full FusionRoute framework across both the LLaMA-3 and Gemma-2 families. We see that FusionRoute consistently outperforms FusionRoute w/o complementary logits across nearly all benchmarks and both model families. The performance gap is particularly pronounced on coding and instruction-following tasks, where even correctly selected experts can produce locally suboptimal or misaligned tokens that require correction. These results provide direct empirical evidence support for our theoretical analysis in 4. Even when the router is able to identify suitable expert at each token, relying solely on fixed expert logits is insufficient to reliably recover near-optimal policy. In contrast, allowing the router to contribute complementary logits effectively expands the expressive capacity of the decoding policy, enabling it to correct expert failures and achieve stronger, more robust performance across heterogeneous domains. Moreover, we find that the routing-only variant (FusionRoute w/o complementary logits) already outperforms the state-of-the-arts controlled-decoding-based token-level collaboration method (Chakraborty et al., 2025) across most benchmarks. This improvement suggests that directly training the routing component on expert data is crucial for learning accurate and stable token-level routing behaviors. In contrast, controlleddecoding approaches that rely solely on external reward signals tend to exhibit higher instability, limiting their effectiveness in practice. Figure 4 GPT-4o winrate comparison: FusionRoute with vs. without CDPO training on both Llama-3 and Gemma-2 Family."
        },
        {
            "title": "6.2 Ablation on FusionRoute Training Procedure",
            "content": "While the ablation in 6.1 demonstrates the importance of complementary logits at inference time, it does not examine how different stages of the FusionRoute training procedure contribute to this behavior. In this section, we further probe the training pipeline of FusionRoute by ablating the preference-optimization stage, with the goal of understanding the effectiveness of the proposed CDPO over SFT. To this end, we compare three variants: base model, FusionRoute after the SFT phase, and the full FusionRoute framework with CDPO training. We evaluate these variants using GPT-4o win rate on held-out general dataset, which captures overall response quality beyond task-specific correctness. The result are shown in Fig. 4. We observe that while SFT already provides reasonable initialization, applying CDPO training leads to substantial improvement in win rate. In particular, the full FusionRoute model significantly outperforms its SFT-only counterpart, indicating that the complementary component learned during CDPO effectively corrects expert failures and enhances the overall response quality. These 13 Method GSM8K MATH500 MBPP HumanEval IFEval Avg Acc. Llama-3 Family Collab FusionRoute w/o complementary logits FusionRoute 0.82 0.82 0. 0.32 0.32 0.33 Gemma-2 Family Collab FusionRoute w/o complementary logits FusionRoute 0.52 0.62 0. 0.22 0.23 0.22 0.28 0.26 0.36 0.26 0.23 0.25 0.54 0. 0.63 0.35 0.33 0.40 0.55 0.65 0.69 0.45 0. 0.61 0.502 0.522 0.566 0.360 0.384 0.426 Table 2 Cross-domain performance comparison between purely token-level routing and FusionRoute across model families. Best and second-best performance within each family are highlighted by bold and underline, respectively. findings suggest that the performance gains of FusionRoute depend critically on the preference-optimization stage in the training procedure, which substantially improves general response quality."
        },
        {
            "title": "7.1 Multi-LLM Collaboration",
            "content": "Recent advances in LLMs have shown that sufficiently large, general-purpose LLMs can achieve strong performance across wide range of tasks, including mathematics, coding, and instruction following (Hurst et al., 2024; Team et al., 2025; Yang et al., 2025; Zeng et al., 2025). However, such models are often prohibitively expensive to train and deploy, motivating interest in more efficient alternatives. At the same time, smaller and domain-specialized LLMs can perform competitively within their respective domains, but typically struggle to generalize beyond the distributions on which they are trained. Bridging this gap between specialization and generalization without incurring the cost of training ever-larger models has therefore become an important challenge. One line of works study the benefits of multi-agent debate, in which several agents discuss and debate to generate final response (Liu et al., 2024; Chan et al., 2023; Liang et al., 2024). Existing works also try to model the social and economic behavior in the real world using different specialized LLMs (Zhao et al., 2023). Moreover, model ensemble methods (Ashiga et al., 2025; Huang et al., 2024; Chen et al., 2025) combine responses or probability distributions from multiple models in fixed manner to leverage their complementary strengths without modifying individual parameters; however, they lack dynamic, context-dependent collaboration among candidate models. More fine-grained token-level collaboration has been studied in recent years. Collab (Chakraborty et al., 2025) uses controlled decoding-based approach to evaluate the candidate tokens during decoding. Compared to them, our decoding algorithm is much more efficient and also achieves better performance. CO-LLM (Shen et al., 2024) also explores token-level collaboration, but mainly uses one strong math expert to assist small fine-tuned model, and its output depends entirely on the candidate experts. In contrast, we use multiple specialized LLMs, evaluate both cross-domain performance and general win rate, and additionally train the router as complementary generator to improve robustness."
        },
        {
            "title": "7.2 Multi-LLM Integration",
            "content": "Beyond multi-agent LLM collaboration, substantial body of works also consider how to utilize and integrate different models to be comprehensive. The most representative area is Mixture-of-Expert (MoE), in which multiple expert models are combined for each layer with some routing network, allowing the system to leverage the specialized knowledge of each expert for different inputs (Zhou et al., 2022; Xue et al., 2024; Jiang et al., 2024a). However, because the final architecture tightly integrates the layers of multiple models with routing network, these approaches are typically applicable only to models with similar structures. They also require access to the experts gradients and joint training of the entire system, which limits flexibility and incurs 14 significant additional training cost. Model merging is another approach for integrating multiple experts (Yu et al., 2024; Ilharco et al., 2022; Yadav et al., 2023). While it does not incur additional training costs, its performance is often limited and sensitive to hyperparameters. Moreover, its flexibility is also limited, as it requires all models to share the same architecture. There is also line of work on pluralistic alignment, in which multiple models with potentially conflicting objectives, such as safety and helpfulness, are integrated to achieve tradeoffs across multiple objectives (Shi et al., 2024; Xiong and Singh, 2025; Wang et al., 2025c). In contrast to these approaches, FusionRoute does not merge or jointly train expert models, nor does it require architectural compatibility or gradient access. Instead, FusionRoute enables flexible, token-level integration of heterogeneous, off-the-shelf LLMs by learning lightweight router that selects among experts and provides complementary logits during decoding. This design allows FusionRoute to combine the strengths of specialized models while retaining robustness and generalization, without the rigidity or training overhead inherent in MoE and model-merging approaches."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we present FusionRoute, framework that enables multiple candidate models to collaboratively generate higher-quality responses than any individual model. FusionRoute works well automatically across diverse scenarios because of routing mechanism, and provides robustness by letting the router provide complementary logits to refine the overall response quality. In addition, FusionRoute is computationally efficient, as it avoids requiring each model to generate candidate tokens or full responses for selection. Theoretically, we characterize the limitations of purely token-level collaboration and demonstrate the benefits of incorporating complementary logits. Extensive experiments on both domain-specific and general datasets verify the effectiveness of our framework."
        },
        {
            "title": "References",
            "content": "Mari Ashiga, Wei Jie, Fan Wu, Vardan Voskanyan, Fateme Dinmohammadi, Paul Brookes, Jingzhi Gong, and Zheng Wang. Ensemble learning for large language models in text and code generation: survey. arXiv preprint arXiv:2503.13505, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, and Sumitra Ganesh. Collab: Controlled decoding using mixture of agents for llm alignment. arXiv preprint arXiv:2503.21720, 2025. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Ming Li, Likang Xiao, Dingqi Yang, et al. Harnessing multiple large language models: survey on llm ensemble. arXiv preprint arXiv:2502.18036, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, et al. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks? arXiv preprint arXiv:2509.16941, 2025. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Yifei He, Siqi Zeng, Yuzheng Hu, Rui Yang, Tong Zhang, and Han Zhao. Mergebench: benchmark for merging domain-specialized llms. arXiv preprint arXiv:2505.10833, 2025. Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Ting Liu, and Bing Qin. Ensemble learning for heterogeneous large language models with deep parallel collaboration. Advances in Neural Information Processing Systems, 37:119838119860, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Nam Huynh and Beiyu Lin. Large language models for code generation: comprehensive survey of challenges, techniques, evaluation, and applications. arXiv preprint arXiv:2503.01245, 2025. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. 16 Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024b. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pages 267274, 2002. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. arXiv preprint arXiv:2110.04541, 2021. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 1788917904, 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, and Jing Li. Groupdebate: Enhancing the efficiency of multi-agent debate using group discussion. arXiv preprint arXiv:2409.14051, 2024. Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, and James Glass. Quantifying generalization complexity for large language models. arXiv preprint arXiv:2410.01769, 2024. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode collaboratively with In Proceedings of the 62nd Annual Meeting of the Association for Computational multiple language models. Linguistics (Volume 1: Long Papers), pages 1297412990, 2024. Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Hanna Hajishirzi, Noah Smith, and Simon Du. Decoding-time language model alignment with multiple objectives. Advances in Neural Information Processing Systems, 37: 4887548920, 2024. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. Measuring inductive biases of in-context learning with underspecified demonstrations. arXiv preprint arXiv:2305.13299, 2023. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. https://huggingface. co/datasets/teknium/OpenHermes-2.5. Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, et al. Beyond reward hacking: Causal rewards for large language model alignment. arXiv preprint arXiv:2501.09620, 2025a. Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, et al. survey on large language models for mathematical reasoning. arXiv preprint arXiv:2506.08446, 2025b. 17 Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, and Linjun Zhang. Mpo: An efficient post-processing framework for mixing diverse preference alignment. arXiv preprint arXiv:2502.18699, 2025c. Nuoya Xiong and Aarti Singh. Projection optimization: general framework for multi-objective and multi-group rlhf. arXiv preprint arXiv:2502.15145, 2025. Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, et al. The perfect blend: Redefining rlhf with mixture of judges. arXiv preprint arXiv:2409.20370, 2024. Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36:70937115, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and llms evaluations. Advances in Neural Information Processing Systems, 36:5847858507, 2023. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Hanqing Zeng, Yinglong Xia, Zhuokai Zhao, Chuan Jiang, Qiang Zhang, Jiayi Liu, Qunshu Zhang, Lizhu Zhang, Xiangjun Fan, and Benyu Zhang. Smore: Structural mixture of residual experts for parameter-efficient llm fine-tuning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition dynamics in large language model-based agents. arXiv preprint arXiv:2310.17512, 2023. Xutong Zhao, Tengyu Xu, Xuewei Wang, Zhengxing Chen, Di Jin, Liang Tan, Zishun Yu, Zhuokai Zhao, Yun He, Sinong Wang, et al. Boosting llm reasoning via spontaneous self-correction. arXiv preprint arXiv:2506.06923, 2025. Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, and Kun Zhang. Thought communication in multiagent collaboration. arXiv preprint arXiv:2510.20733, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei Wang, Jiayi Liu, Fei Liu, Serena Li, Weiwei Li, et al. Mixture-of-minds: Multi-agent reinforcement learning for table understanding. arXiv preprint arXiv:2510.20176, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Proof of Theorem 4.3 Proof. We assume that the prompt is fixed and that the response length is fixed to . For any , define Pt = (cid:8)(n1, . . . , nt) (cid:12) (cid:12) nk [n], = 1, . . . , t(cid:9) as the set of all possible expert-selection sequences of length t. We further assume that all expert models and transition dynamics are deterministic. In particular, for any expert πi and any prefix (x, yt), the decoding rule πi(x, yt) deterministically outputs the next token. Given any selection sequence = (n1, . . . , nt) Pt, the induced token sequence yp = (yp 1, . . . , yp yp = πnk ) is defined recursively as (cid:1), (cid:0)x, yp = 1, . . . , t, k1 with the convention that yp 0 := . Accordingly, the induced state can be written as sp = (x, yp). Construction Now we construct PT /2 MDPs {MpT /2}pT /2PT /2 Same Structure We first define the shared components of all candidate MDPs. The initial state is s0 = x. The action space corresponds to token generation, with vocabulary Y. Unless otherwise specified, the reward function is defined to be 1 for all prefixes (x, y). as follows: For each expert model [n], define We assign the first-step rewards as follows. For each token y1,i, define y1,i := arg max yY πi(y x). r(x, y1,i) = 1 ε. (14) Otherwise, for any {y1,i}i[n], we assign r(x, y) = 1. Difference Now we show the difference between multiple candidate MDPs {MpT /2}pT /2PT /2 is constructed in the following ways: . Each MDP Mp For path {p PT /2+i 1, has prefix pT /2}, we define r(x, yp For any path {p PT /2+1 does not have prefix pT /2}, r(x, yp For any path {p PT /2+i 2, does not have prefix pT /2}, we define r(x, yp ) = 1 . ) = 1. ) = 0. For all other states that are not mentioned, their reward functions are defaulted to 1. Value Function Under this construction, for any prefix (x, yt), the optimal policy will choose yPR as its next token, and the final optimal value function satisfies (x, yt) = t, prefix (x, yt). This is because the optimal policy π selects the reward-maximizing token at every remaining time step, thereby accumulating reward of 1 at each step until termination. In particular, for all possible model-selection paths Pt, the optimal value function satisfies Similarly, the function (x, yp) = t. Qπ (x, yt, yt+1) = r(x, yt, yt+1) + + 1. (15) 19 In particular, for each policy πi, define the greedy token at state sp as yp := arg max yY πi(y x, yp). Then, the optimal stateaction value under π is given by ) = Qπ (x, ypi) = r(x, yp, yp (x, yp, yp Qπ ) + (x, ypi) = r(x, ypi) + + 1, (16) where Pt+1 denotes the extended path that follows for the first tokens and selects token at step + 1. Since we construct the reward such that r(x, yp) = 1 for any path Pt for /2. Then, for any /2 1, we know that for any pk Pk Qπ (x, ypki) = = (x, yp). (17) Verfication of Assumption Now we verify whether three assumptions Eq. (11), Eq. (12) and Eq. (13) holds for all these MDPs. Eq. (11) By the construction, we can easily know that = . Now, recall that R(x, yT ) = (cid:80)T i=1 r(x, yi). For any token-level routing generated response (full path) pT on MDP Mp (p is path with length /2), we have pT = R(x, ypT ) = (cid:40) ε /2 + 1 ε if pT has prefix else . Hence, on the MDP Mp with PT /2, if path pT PT has prefix p, we know that pT = ε, which implies that pT ε. Eq. (12) Note that both the transition kernel and the optimal policy π are deterministic. As result, all expectations reduce to deterministic quantities. For x, y1, , yT π, we know that yt+1 = arg maxyY r(x, yt, y), which implies that Eyt+1π(x,yt)[Qπ (x, yt+1)] = (x, yt) = t. (18) Also, for 1, by Eq. (15), we have arg max i[n] Eyt+1πi(x,yt)[Qπ (x, yt, yt+1)] = r(x, yt, yt+1) + + 1 = t. The last inequality is because y1 {y1,i}i[n] and then all rewards are equal to 1. Hence, for any 1, we have Ex,ytπ (cid:12) (cid:12) (cid:12)arg max Eyt+1πi(x,yt) (cid:2)Qπ (x, yt+1)(cid:3) Eyt+1π(x,yt) (cid:2)Qπ (x, yt+1)(cid:3)(cid:12) (cid:12) (cid:12) = 0 . Moreover, for = 0, we have arg max i[n] Ey1πi(x)[Qπ (x, y1)] = r(x, y1) + 1 = (1 ε) + 1 = ε. Hence, for = 0, we also have Ex,ytπ (cid:12) (cid:12) (cid:12)arg max Eyt+1πi(x,yt) (cid:2)Qπ (x, yt+1)(cid:3) Eyt+1π(x,yt) (cid:2)Qπ (x, yt+1)(cid:3)(cid:12) (cid:12) (cid:12) = ε . Eq. (13) First, by our construction, for any prefix x, yt, one can follow the optimal policy π to generate response x, yT such that R(x, yT ) = (cid:88) i= r(x, yi) = (cid:88) i=1 r(x, yi) + t. (19) This equality holds because optimal policy will select the reward-maximizing token at every remaining time step, that leads to reward. Now, on particular MDP Mp with path PT /2, we know that for any prefix x, yt, if there is not full response x, yT with prefix x, yt and R(x, yT ) π = , we must have which implies that (cid:88) i=1 r(x, yi) + , (cid:88) i=1 r(x, yi) . (20) By our construction, Eq. (20) holds if and only if /2 + 1 and yT /2 = yp. Hence, we only need to verify Eq. (13) holds when yT /2 = yp. We denote πi(x, yt) is the deterministic next token given prefix x, yt for expert model πi. Then, we know that Eyt+1πi(x,yt) (cid:2)Qπ (x, yt+1)(cid:3) Eyt+1π(x,yt) (cid:2)Qπ (x, yt+1)(cid:3)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)arg max (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = arg max i[n] r(x, yt, πi(x, yt)) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . Situation 1: If = /2 and yt = yp coincides with yp, we have (cid:12) (cid:12) (cid:12) arg max (cid:12) (cid:12) i[n] r(x, yt, πi(x, yt)) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (1 ) 1 = . Situation 2: If 1 and yT /2 = yp, by our construction, we have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) arg max i[n] r(x, yt, πi(x, yt)) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = 0. Situation 3: For = 0, we can have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) arg max i[n] r(x, yt, πi(x, yt)) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (1 ε) 1 = ε. Combining these three conditions above, we successfully verify the Eq. (13). Since the learner only observes the expert value Qπ (x, yt) at the visited states (x, yt), and since these values are identical across all candidate MDPs {Mp}pPT /2 for all /2 1, the learner receives exactly the same observation sequence oT /2 under every Mp. Consequently, the MDPs are indistinguishable to the learner during the first /2 1 steps. We consider token-level routing algorithms : [n] whose observation space at step is defined as = (cid:91) t=1 (cid:91) (cid:110) x, yt, {Qπ Ot = (x, yk)}t k=1, {Qπ (x, yt, y)}yY (cid:111) . t= That is, at each step t, the learners observation ot Ot includes the prompt, the generated prefix, the expert value along the realized trajectory, and the expert values of all possible next-token continuations. Formally, let : [n] be any (possibly randomized) token-level routing algorithm. Then there exists path = (p /2) PT /2 such that 1, . . . , P(cid:2)A(oi) = i+1 for all = 0, . . . , /2 1(cid:3) 1 PT /2 = 1 nT /2 . 21 This follows from simple counting argument. Since the observation sequence is identical across all Mp, the algorithm induces probability distribution over the set of paths PT /2, and thus (cid:88) pPT /2 P[A(oi) = pi+1 for all = 0, . . . , /2 1] 1. Therefore, there must exist at least one path PT /2 whose probability mass is at most 1/PT /2. Hence, for the MDP Mp , this expect value of this token-level routing algorithm will be at most (T /2 + 1 ε) 1 (cid:18) (cid:19) 1 nT /2 + (T ε) 1 nT /2 /2 + 2, which implies that πA /2 + 2. Hence, no token-level routing algorithm with observation space ot = (cid:110) x, yt, {Qπ (x, yk)}t k=1, {Qπ (x, yt, y)}yY (cid:111) can guarantee achieving an approximately optimal value for given prompt. Theoretical Discussion of Prior Token-Level Approaches In 4, we show that training the router on SFT dataset which follows trajectories generated by π can be viewed as approximating an optimal expert selection strategy (Eq. (8)). By the performance difference lemma, Eq. (8) establishes direct connection between the generated responses and the optimal response. However, in recent years, Collab (Chakraborty et al., 2025) provides test-time controlled-decoding based multi-LLM token-level collaboration approaches. During the decoding process, for each token position, Collab first generates multiple candidate tokens from multiple experts π1, , πn. For candidate tokens for each expert πi, Collab then generates few more tokens using πi for evaluating the Qπi function. Finally, they choose the token with the maximum corresponding function from the candidate token pool. In simple words, Collab tends to choose the action that ah = arg max ah Eyt+1πah (x,yt)[Q πah (x, yt+1)]. Since the action is not selected by maximizing the optimal function, it leads to mismatch i(x, yt) = Q(x, yt) Qπi (x, yt) min i[n] between the generated response and the optimal response. This mismatch can lead to non-ideal performance. In fact, consider two different expert policies π1 and π2 with horizon and π1(x, y) = π2(x, y) for any prefix (x, y), we can construct the reward function that satisfies and r(x, yt, yt+1) = I{yt+1 = π1(x, yt)}, 3 , r(x, yt, yt+1) = I{yt+1 = π2(x, yt)}, > 3 . In this setting, the optimal policy is π = π1 for H/2 and π = π2 for H/2 + 1, which has Q(x) = for prompt x. However, at position 0, we have Hence, the mismatch error Q(x) = H, Qπ1(x) = 3 , Qπ2 (x) = 2H . max i[n] i(x, yt) = 3 , which is linear in the horizon H. Compared to Collab, FusionRoute directly finetunes the router on the expert dataset, which is equivalent to which helps us to bound the final error using Performance Difference Lemma. ah = arg max ah Eyt+1πah (x,yt)[Q h(x, yt+1)],"
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Generation C.1.1 Cross-Domain Generation Due to the large number of prompts in certain tasks and computational constraints, for each task we evaluate on subset of their datasets. The generation settings and number of samples are summarized in Table 3. Dataset # of samples # Max New Tokens GSM8K MATH500 MBPP HumanEval Ifeval 400 500 500 164 400 400 400 400 400 400 Table 3 Number of samples and number of max new tokens for each dataset. C.1.2 GPT Evaluation Prompt For GPT-4o evaluation on the general dataset, we use the following text as the prompt provided to the GPT-4o model. The output scores for both responses are then extracted to compute the win rate. GPT-4o Evaluation Prompt Prompt: You are precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question. Please rate the correctness, fluency, and level of detail of their responses. Your evaluation should consider factors such as the accuracy, relevance, depth, creativity, fluency, and level of detail of the response. Bear in mind that the response might be truncated at the end due to length constraints. Each assistant receives an overall float score on scale of 1 to 10, in increments of 0.25. higher score indicates better overall performance. Do not give two responses same score. Do not prefer longer response. Please first output single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by space. In the subsequent line, please provide short explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. [Question] question [The Start of Assistant 1s Answer] answer1 [The End of Assistant 1s Answer] [The Start of Assistant 2s Answer] answer2 [The End of Assistant 2s Answer] 23 C.2 Routers Behavior In this section, we examine the routing behavior of our model on both domain-specific and general datasets to demonstrate the effectiveness of the routers token-level routing capability. In the following text, we use red, blue and softgreen to represent the math, coding and instruction-following model, respectively. C.2.1 Example of Coding Problem Prompt: Below is an instruction that describes task. Write response that appropriately completes the request. Instruction: Write function to find the minimum cost path to reach (m,n) from (0,0) for the given cost matrxi cost[][] and position (m,n) in cost[][]. Assistant: Response: Here is Python solution using dynamic programming: python def minCostPath(cost, m, n): dp=[[0 for in range(n)] for in range(m)] # Initialize the first row and first column for in range(m): dp[i][0] = cost[i][0] for in range(n): dp[0][j] = cost[0][j] # Fill dp[][] in bottom up manner for in range(1,m): for in range(1,n): dp[i][j] = min(dp[i-1][j] + dp[i][j-1] + cost[i][j] return dp[m-1][n-1]\" This function works by creating 2D array dp[][] where dp[i][j] represents the minimum cost to reach the cell (i,j) from (0,0). The first row and first column are initialized with the cost of the cells in the first row and first column respectively. Then, for each cell (i,j) in the rest of the matrix, the minimum cost to reach it is the minimum cost to reach the cell above it (i-1,j) or to the left of it (i,j-1) plus the cost of the cell itself. The function finally returns the minimum cost to reach the cell (m,n). <eot_id> C.2.2 Example of Math Problem Prompt: Below is an instruction that describes task. Write response that appropriately completes the request. Instruction: Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, θ), where > 0 and 0 θ < 2π. Assistant: Response: The polar coordinates $(r,theta)$ of point $(x,y)$ are given by $r =sqrt {xˆ2+yˆ2}$ and $theta= tanˆ{-1}left(frac{y}{x}right)$. In this case, we have $x =0$ and $y=3$, so $r=sqrt{0ˆ2 + 3ˆ2}=3$theta=tanˆ{-1}left(frac{3}{0}right)=frac{pi}{2}$. Therefore, the polar coordinates of the point $(0,3)$ are $boxed{(3,frac}{2})}$. The answer is: (3,frac{pi}{2})<eot_id> C.2.3 Example of Instruction Following Problem Prompt: Below is an instruction that describes task. Write response that appropriately completes the request. Instruction: Write resume for fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholders represented by square brackets, such as [address], [name]. Assistant: Response: Here is sample resume for fresh high school graduate: [Name][Address][City, State Zip][Phone Number][Email Address] Objective: To secure an entry-level position in dynamic and growth-oriented organization where can apply my skills and knowledge to contribute to the companys success. Education: - High School Diploma, [Name of High School], [Graduation Date] - GPA: [GPA] Skills: - Proficient in Microsoft Office (Word], Excel, PowerPoint, Outlook) - Strong communication and teamwork skills - Proficient in [Programming Language or Software] - Experience with [Software or -[Briefly describe Technology] Work Experience: - [Job Title], [Company Name], [Employment Dates] - job responsibilities and accomplishments] - [Job Title], [Company Name], [Employment Dates] [Briefly describe job responsibilities and accomplishments] Volunteer Experience: - [Volunteer Position], [Organization], [Volunteer Dates] - [Briefly describe volunteer work and accomplishments] References: - Available upon request. Note: This is basic template, and you should customize it to fit your specific experiences and qualifications. Remember to proofread your resume multiple times for any errors or typos. Good luck with your job search! Please let me know if you need any further assistance. Also, please note that this is very basic template, and you may want to consider adding more sections or details to make your resume more comprehensive. Some additional sections you may want to consider include: Extracurricular activities - Awards or recognition - Relevant coursework - Projects or presentations - Leadership roles - Relevant certifications or licenses Let me know if you need any help with any of these sections. Also, please keep in mind that this is vaery basic template, and you may want to consider tailoring your resume to the specific job you are applying for. You can do this by highlighting... C.2.4 Example of General Problem in PerfectBlend Dataset Prompt: Below is an instruction that describes task. Write response that appropriately completes the request. Instruction: You start out with some pens. Mike gives you another 22 pens. Cindy, whose father makes pens, decides to double your pens. You give Sharon 19 pens. At the end, you have 65 pens. How many pens did you start with? Response: Lets denote the number of pens you started with as x. According to the problem, Mike gives you 22 pens, so you now have + 22 pens. Cindy doubles your pens, so you now have 2(x + 22) pens. You give Sharon 19 pens, so you now have 2(x + 22) - 19 pens. According to the problem, at the end, you have 65 pens, so we can set up the equation: 2(x + 22) - 19 = 65. Now, lets solve for x: 2x + 44 - 19 = 65 2x + 25 = 65 2x=65 - 25 2x = 40 = 40/2 = 20 So, you started with 20 pens. The answer is: 20.<eot_id> C.2.5 Discussion on the Qualitative Examples Based on the above qualitative examples, we observe that FusionRoute successfully assigns the majority of tokens associated with specific problem types such as coding, mathematical reasoning, and instruction following, to their corresponding expert models. This behavior demonstrates the effectiveness of FusionRoute in performing accurate token-level routing on domain-specific datasets, enabling the model to effectively leverage the complementary expertise of different specialists. Furthermore, when evaluated on general-purpose datasets, we find that FusionRoute exhibits more nuanced routing pattern. Tokens that require mathematical or logical reasoning are predominantly routed to the math and code experts, while tokens related to discourse structure, formatting, or general instruction phrasing (e.g., According to, Lets, So) are more frequently assigned to the instruction-following model. This adaptive token-level routing behavior highlights FusionRoutes ability to dynamically balance specialized reasoning and general linguistic fluency, thereby improving overall generation quality across diverse tasks."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Meta AI",
        "Meta TBD Lab",
        "University of Chicago",
        "University of Maryland"
    ]
}