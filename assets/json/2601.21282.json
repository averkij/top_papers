{
    "paper_title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "authors": [
        "Rishi Upadhyay",
        "Howard Zhang",
        "Jim Solomon",
        "Ayush Agrawal",
        "Pranay Boreddy",
        "Shruti Satya Narayana",
        "Yunhao Ba",
        "Alex Wong",
        "Celso M de Melo",
        "Achuta Kadambi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning."
        },
        {
            "title": "Start",
            "content": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models Rishi Upadhyay1 Howard Zhang1 Shruti Satya Narayana1 Yunhao Ba2 Alex Wong3 Celso de Melo4 Achuta Kadambi1 Jim Solomon1 Ayush Agrawal1 Pranay Boreddy1 6 2 0 2 J 9 2 ] . [ 1 2 8 2 1 2 . 1 0 6 2 : r 1University of California, Los Angeles 2Sony AI 3Yale University 4DEVCOM Army Research Laboratory Figure 1. We introduce WorldBench, video-based benchmark to evaluate world foundation model performance on specific physical concepts/constants and material properties. Prior benchmarks typically either entangle multiple concepts (making it difficult to uniquely identify model failures) or use coarse-grained metrics like binary selection (making it difficult to disambiguate between visually realistic and physically accurate trajectories). Our benchmark leverages both an intuitive physics and physical parameter estimation subset to provide greater insight into world model performance."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in generative foundational models, often termed world models, have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of single physical concept or law at time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning. 1. Introduction Imagine watching tower of blocks teeter and fall, or ball rolling its way down staircase. As humans, we effortlessly predict its motion. However, this intuitive grasp of physical dynamics remains core challenge for AI. Recent world foundation models, most notably NVIDIAs Cosmos [2], promise to learn such skills at scale, suggesting that these models can be used as synthetic data generators for the real world. Rigorously evaluating these claims requires benchmarks that are designed and focused on probing physical understanding at concept-specific level. 1 Figure 2. Overview of our generation and evaluation process. For generation (top), we use Kubric, which uses PyBullet and Blender under the hood. During evaluation (bottom), we first pass the initial frames of the generated video to the world foundation model which completes the video. The completed video is passed to SAM2 along with bounding boxes based on ground truth masks. The segmentations outputted by SAM2 are compared to ground truth segmentations to obtain the final metrics. Existing benchmarks for physical reasoning tasks often provide coarse-grained or binary metrics on scenes that entangle multiple physics concepts, limiting their diagnostic ability. For example, benchmarks such as PHYRE [6] or CLEVRER [60] contain scenes which require core understanding of multiple concepts (perspective accuracy, collision dynamics, and support relations). While some benchmarks like Physion [7] and IntPhys2 [11, 41] do provide some level of granularity in testing physical concepts, they do not contain experiment-level tests of specific physical parameters or material constants. Furthermore, reliance on coarse-grained or binary metrics like object contact prediction limit their ability to capture nuanced physical phenomena, such as object dynamics (velocity, acceleration, rotation, etc.), deformation, or occlusion. In this paper, we introduce WorldBench, novel benchmark designed to rigorously evaluate the disentangled, concept-specific, physical reasoning capabilities of WFMs through video prediction. Our testing framework uses finegrained categories of evaluation to fill in critical gap in in the current research landscape. The video-based output requires models to accurately forecast the physically plausible evolution of full visual scenes over time, providing more robust signal than previous binary metrics. To achieve this nuanced assessment while ensuring repeatable, interpretable outcomes, we design simplified, yet physically rich and visually realistic scenes. Our benchmark is split into two subsets. The intuitive physical understanding subset targets four critical principles: motion physics, object permanence, support relations, and scale/perspective. This subset is meant to benchmark models ability to generate plausible dynamics governed mostly by their respective core principles (e.g. ball rolling behind pillars, object moving towards the camera, etc.). WorldBench also includes physical parameter estimation subset. This feature requires the video output to accurately adhere to specific, known physical parameters that govern the scenes dynamics, such as gravitational acceleration, fluid viscosity, and friction coefficients. By enforcing adherence to these measurable parameters, we provide definitive, quantifiable metric for assessing models true grasp of each underlying physical law. Our proposed task, which we term constrained video prediction, allows for significantly more nuanced and detailed assessment of physical laws than previous work, including more fine-grained diagnosis of which concept model underperforms on, and an accurate representation of object dynamics (velocity, acceleration, rotation, deformation, and occlusion). The first subsets focus on specific, real-world physics properties, such as object permanence and scale/perspectivewhich are absent in prior benchmarks like Physionensures that WorldBench provides richer signal of how close these models are to truly learning and understanding real-world dynamics. The second subsets focus on real-world physical parameters also enables us to directly measure the physical accuracy of world models, which is particularly critical if these models aim to be synthetic data generators. For future work in the evaluation of these world models, our benchmark also leads to wider array of downstream tasks, such as object tracking, anomaly detection, action planning, etc. Using WorldBench, we extensively test state-of-the-art world models, including the Cosmos architecture, revealing substantial gaps in physical consistency and generalization when compared to both real-world captured and simulated, physically accurate expectations. Importantly, we find that models tend to generate visually realistic scene evolutions (e.g. ball follows parabolic trajectory), but fail to adhere to physical parameters (e.g. ball accelerates downward at 2). By leveraging video outputs rather than binary 9.8 2 selection tasks, our benchmark provides way to quantitatively disambiguate between visually plausible and physically accurate model outputs. Our results highlight the critical limitations of current world model architectures and motivate further research into physically grounded learning. Below is summary of our key contributions: We introduce video-based benchmark comprised of real and synthetic videos to evaluate WFM performance on specific physical concepts and parameters. Our first subset provides scenes designed to test models qualitative understanding of foundational concepts such as object permanence, scale/perspective, support relations, and motion physics. Our second subset provides carefully designed experiments which directly measure models ability to reproduce specific well-defined physics constants and behaviors such as gravitational acceleration, fluid viscosity, and friction coefficients. We perform an empirical analysis of SOTA WFMs and Image-to-Video models to identify concept-specific shortcomings and gaps in physical understanding. 2. Related Work 2.1. World Foundation Models significant body of work has emerged around world models, models that can understand and predict the real world, in recent years. Initial work in this space focused on vision-language models [34, 36, 42], but recent work has been on using video generation models [4]. These models typically leverage transformer architectures and either latent diffusion models [8, 9, 13, 18, 26, 27, 35, 43, 51] or autoregressive models [17, 39, 5257, 61] to achieve temporally consistent video synthesis. However, while these models are able to generate visually realistic and aesthetically pleasing outputs, there has also been recent growth of research surrounding physically accurate generations. The recent Cosmos [2, 3] aims to be world foundation model, which can output temporally and physically accurate videos that can be leveraged for training downstream AI models that interact with the physical environment. Cosmos can generate these videos using either transformer-based autoregressive model or transformer-based diffusion model, training large corpus of over 100M video clips, labeled by numerous different vision-language models [47]. Similarly, other models such as Genie [12], also attempt at creating world foundation model capable of generating physically accurate interactive environments. It uses novel video tokenizer and causal action model, passing both the video tokens and action latents to an autoregressive dynamics model for prediction. However, note that Genie is currently closedsource and not available for evaluation under our proposed benchmark. These world foundation models claim to be physically accurate enough for their outputs to be used as simulated data, but little to no evaluations have been developed so far to validate this claim. 2.2. Image-to-Video Models closely related and highly active area of research is Image-to-Video (I2V) generation, which focuses on synthesizing video sequence from single static image input. The fundamental challenge in I2V models is temporal consistency: ensuring that the generated frames maintain the identity, structure, and appearance of the initial image while introducing plausible motion and scene evolution. Modern I2V models typically inflate standard image diffusion models with temporal dimension, using temporal convolution or attention layers [8, 9, 2325, 27, 29, 48, 49, 59]. Notable models include CogVideoX [27, 59], which leverage 3D-VAE for enhanced compression and an expert transformer for better text-video alignment, as well as WAN [48] which leveraged large-scale data training and spatio-temporal VAE to achieve SOTA results on previous benchmarks. While I2V models excel at animating scenes with high visual fidelity and maintaining the initial scenes identity, their primary design objective has been aesthetic realism and adherence to user-specified motion (often via text prompt), rather than physical plausibility. The evaluation of I2V models is typically focused on metrics like Frechet Inception Distance (FID), Inception Score (IS), and temporal coherence measures, which assess visual quality and smoothness but do not inherently check for adherence to physical laws like gravity, friction, or object dynamics. This gap underscores the need for benchmarks like WorldBench, which can systematically test whether the generated motion reflects true understanding of the physical parameters required for real-world simulation. 2.3. Physics Datasets and Benchmarks There has been growing interest in the community to evaluate the physical understanding and reasoning abilities of modern vision models [16, 22, 30, 38, 41, 44]. Datasets like PHYRE [6] focus on simplistic 2D scenarios constructed from balls and rectangular bars, with dynamics like collision, gravity, and friction. CLEVRER [60] is video reasoning benchmark designed with simple structures for tasks including description, explanation, prediction, and counterfactuals. The MOVi set of datasets [21], are multi-object video datasets, targeting object-centric models and their ability to detect and discover object boundaries in videos. More recently, the Physion dataset [7] compiles set of visually realistic videos separated between 8 different physics scenarios: dominoes, support, collide, contain, drop, link, roll, and drape. It leverages the object contact prediction (OCP) task to evaluate the physical understanding ability of 3 Figure 3. Overview of our physical parameter estimation pipeline. Given an input video, we first use checkerboard detection and SAM2 to extract 3D positions for objects in the video. We then fit curves to these parameters to estimate relevant physical properties such as acceleration or terminal velocity. These are then post-processed, if needed, to calculate the relevant physical parameters. Table 1. Our proposed benchmark is the first to have conceptspecific evaluation of both intuitive physics concepts as well as parameter-based experiments. Our proposed WorldBench is also enables distengled evaluation by video outputs, enabling more nuanced and fine-grained evaluation. Benchmark 3D Real-World Data Video Output Disentangled Intuitive Physics Disentangled Parameter Estimation Task PHYRE [6] CLEVRER [60] MOVi [21] Physion [7] IntPhys2 [11, 41] Ours Scene Modification Visual Question Answering Object Tracking Object Contact Prediction Binary Selection Frame Prediction models. While considerable progress has been made in this space, all prior work are deficient in at least one key area. Datasets like PHYRE and CLEVRER lack in visual realism and are made up of overly simplistic objects and structures. The MOVi datasets has visually diverse scenes and objects, but focuses on object discovery rather than physical reasoning tasks. As described in earlier sections, while Physion does have wide variety of different tasks and visually realistic video inputs, the sole use of the OCP task for physical understanding evaluation limits its ability to be used to evaluate the new wave of world foundation models such as Cosmos [2] or Genie [12]. Compared to these, our benchmark is the first benchmark where the inputs and outputs are both video based. This aligns much more closely with the architectures of todays models, making it better fit for physics evaluation. Reference Table 1 for comparison summary of notable physics datasets and benchmarks. 3. Benchmark In order to test concept-specific physical understanding in world foundation models (WFMs), we introduce novel benchmark, WorldBench, designed to evaluate their physics prediction capabilities. The core methodology is to provide these models with short input video and tasking them to generate continuation. Each video is designed to evaluate single physics concept or law. WorldBench is divided into two distinct, yet complementary components, designed to probe both the intuitive and the engineering-grade understanding of the physical world. This structure ensures comprehensive assessment of both high-level intuitive physics and low-level physics constants. All simulated videos in our benchmark are rendered using Kubric, an open-source physics simulation pipeline [20]. Kubric uses PyBullet [15] as the physics simulator and Blender [10] as the renderer. This allows us to combine the physically accurate simulation of PyBullet with the high-quality rendering of Blender. 3.1. Intuitive Physics Understanding The first subset is designed to assess the models implicit understanding of core, foundational physics concepts, often referred to as intuitive physics. This section takes inspiration from developmental psychology, where infants quickly acquire an understanding of the worlds basic operational rules through observation. The goal is to determine if large-scale models, trained on vast quantities of video data, have successfully internalized these necessary cognitive building blocks. Our benchmark focuses on four fundamental physics concepts: Motion Physics (how objects move and interact), Support Relations (how objects are supported or balanced), Object Permanence (understanding that objects continue to exist when hidden), and Scale/Perspective (how size and spatial relationships change with motion/viewpoint). This is not an exhaustive list of physical concepts, but is designed to cover range of common real-world scenarios. The difficulty in the design of this subset is narrowing the object trajectories to ensure consistent scene evolution, but allowing for enough variation for rich and robust benchmark. To that end, for each concept, we construct 3-5 scenarios, where each of these scenarios is hand-designed to capture some element of the concept it is testing. Each scenario has 25 videos, each of which is generated by randomizing various components such as object type, location and material. In addition, we collect 10-14 real videos for each high-level concept, generally from subset of the scenarios. In total, this subset of WorldBench is made up of 469 videos spanning the 4 concepts: 425 simulated, and 44 4 Table 2. Validation of Physics Parameters on the PerfectPhysics subset. We show the evaluation pipeline on our captured ground truth videos here. All estimated parameters are within an acceptably small margin of error Model Free-Fall (m/s2) Parabolic (m/s2) Gravity Glycerine (Pa.s) Viscosity Corn Syrup (Pa.s) Honey (Pa.s) Wood Rubber Friction Sandpaper (80) Sandpaper (3000) Plastic Estimated Ground Truth 9.78 0.38 9.81 9.85 0.36 9. 1.22 0.01 1.2 5.84 0.02 5.0 - 7.0 13.82 0.75 14.1 0.35 0.05 0.2 - 0.5 0.93 0.10 0.5 - 2.0 1.06 0.05 0.7 - 1. 0.30 0.02 0.2 - 0.5 0.22 0.03 0.05 - 0.2 Table 3. Foreground mIoU results on the Physical Principles Understanding subset (Simulated Videos). Since the diffusion models generates 121 frames vs 33 for the autoregressive, we provide both comparisons. Higher is better for all columns Model Params Ball Bounce 2 Obj Fall 2 Obj Para Block/Obj Columns Raised Block Walls Two Ball Cosmos-1 AR [2] Cosmos-1 (33F) [2] Cosmos-1 (121F) [2] Cosmos-2 [37] Cosmos-2 [37] Cosmos-2.5 [3] 5B 7B 7B 2B 14B 2B 0.3759 0.3719 0.1636 0.1903 0.1913 0.1996 0.2675 0.2994 0.1444 0.1780 0.2013 0.3035 0.2268 0.2831 0.2008 0.1401 0.1351 0.1607 0.2643 0.3476 0.2047 0.1271 0.1672 0. 0.7032 0.7349 0.5575 0.6958 0.4758 0.7363 Params Obj Tow. Obj Away Sphere Tow. Sphere Away Dominoes Cosmos-1 AR [2] Cosmos-1 (33F) [2] Cosmos-1 (121F) [2] Cosmos-2 Cosmos-2 Cosmos-2.5 [3] 5B 7B 7B 2B 14B 2B 0.2984 0.3272 0.0996 0.1591 0.1484 0.1341 0.4121 0.4840 0.2330 0.4341 0.3705 0.4321 0.6349 0.7123 0.2453 0.4595 0.3721 0. 0.4799 0.5546 0.1774 0.3337 0.3171 0.3189 0.4605 0.4892 0.1568 0.2425 0.2067 0.2981 0.3798 0.4555 0.3193 0.4514 0.1846 0.4480 Ramp 0.5292 0.4861 0.3802 0.4082 0.4048 0.5049 0.4996 0.5578 0.4155 0.3298 0.2699 0. Table 0.6439 0.4573 0.4215 0.4907 0.4061 0.4984 0.1607 0.2013 0.1403 0.2055 0.1751 0.2055 Avg. 0.4225 0.4508 0.2573 0.3201 0.2684 0.3488 real. Each video is 132 frames long and includes ground truth object segmentations. The synthetic videos additionally include ground truth depth, normals, and optical flow. All meshes and objects used in our simulations were taken from the ShapeNet dataset [14] which includes 51,000 object models across 55 different categories. We sampled across all different categories and models, allowing for diversity in object shapes, textures, sizes, and properties. We will now provide brief description on each of the concepts. More details on individual scenarios are included in the supplemental material. Motion Physics is focused on evaluating the kinematics and dynamics in the generated video, specifically accounting for forces such as gravity and friction. This is common real world scenario, as it is common for these models to have to simulate moving and colliding objects. To test motion physics, we create 3 scenarios: bouncing ball, two object fall, and two object parabolic motion. Object Permanence evaluates whether video generative models understand that objects continue to exist in the scenes even when hidden from the camera. This is fundamental physics property that significantly affects our ability to predict the world (e.g. when driving we understand cars remain even if blocked) and is generally developed in young children between the ages of only 4-7 months old. To test object permanence, we create 5 scenarios: block & object, columns, raised block bounce, wall bouncing, and two ball bounce. Support Relations evaluates how objects physically support one another, e.g. one object preventing another from falling due to gravity or external forces. This includes understanding when certain configurations of objects are stable vs. unstable: for example, large object placed on the middle of table would be stable while the same object placed closer to the edge would be unstable. To test support relations, we designed 3 scenarios: dominoes, ramp block, and table drop. Perspective / Scale Relations is designed to evaluate the accuracy of objects appearance, such as size and location, with respect to the camera viewpoint. We implemented two types of scenes to evaluate whether models can reason about how object size and location change as function of distance from the camera. To evaluate perspective / scale relations, we designed 2 scenarios: obj/sphere moving towards camera and obj/sphere moving away from camera. 3.1.1. Evaluation Methodology Our evaluation for the intuitive physics subset is done by comparing ground truth object segmentations with segmentations obtained from the generated videos. Specifically, our pipeline (visualized in Fig. 2) works as follows: For every scenario, we use the ground truth segmentations to obtain bounding boxes of all objects in the video in the first frame. We then prompt SAM2 [40] with these bounding boxes, and use SAM2 to propogate these boxes/objects through the rest of the video. For every frame, we then compare the ground truth and predicted masks using the mIoU metric. We additionally, use the background region (calculated as all pixels not part of individual object masks) to compute the background RMSE. Since all of our backgrounds remain con5 stant throughout the video, this metric measures how well the models maintain backgrounds. 3.2. Physical Parameter Estimation The second subset shifts the focus from core physics principles to exact parameter estimation. One key goal of WFMs like NVIDIAs Cosmos [2] is replacing physics simulation software (where parameters are hard-coded or manually tuned) as synthetic data generator. To that end, it is crucial that these models are able to generate videos with accurate estimations for key physical parameters, such as gravitational acceleration. For this subset, we carefully designed total of three experimental setups testing gravitational acceleration, friction coefficients, and fluid viscosity as the respective physical parameters. These concepts have 51 videos, 103 videos, and 80 videos respectively, for total of 234 videos. We additionally synthetically generate 30 gravity and 15 friction videos, bring the combined total to 279 videos for the physical parameter estimation set. The difficulty of designing these settings lies in reducing the effect of confounding variables that can influence the physical parameter estimation ability of these models (e.g. depth ambiguity, object motion uncertainty, etc.). To address this, each experimental setup and evaluation pipeline is meticulously designed to ensure consistent evaluation. This setup and pipeline are detailed in Sec. 3.2.1. We validate this pipeline by running it on our collected videos, and ensuring that the output physical parameter is close to 2 for gravitational acceleration). the ground truth (e.g. 9.8 Validation results can be seen in Tab. 2. We will now provide detail for the experimental setups, with additional information in the Supplementary material: Gravity consists of 51 videos, 17 for straight drops and 34 for parabolic motion. Dropped items vary in shape and size. For parabolic motion, launch angle and trajectory vary as well. Note that video completion models are provided enough input frames to estimate the gravitational acceleration. Viscosity consists of 80 videos, 32 for glycerine, 30 for corn syrup, and 18 for honey. In these videos, steel ball is dropped into test tube. The terminal velocity is estimated from the 3d position of the steel ball, which in turn gives us the viscosity. Note that viscosity varies depending on temperature and water absorption. As such, we collect all videos at 75F and within the same session to as not to influence the viscosity of hygroscopic fluids. All controlled parameters are provided to the evaluated model through text conditioning. Additionally, video completion models are provided enough input frames to estimate the terminal velocity of the steel ball through the fluid. Friction consists of 103 videos, 30 for wood, 19 for rubber, 18 for sandpaper (80 grit), 18 for sandpaper (3000 In these videos, steel block grit), and 18 for plastic. is dropped down ramp covered with different materials. The angle of the ramp varies between runs. All controlled parameters are provided to the evaluated model through text conditioning. Additionally, video completion models are provided enough input frames to estimate the acceleration of the steel block down the ramp. 3.2.1. Evaluation Methodology Extracting exact physical parameters from video is difficult task because it requires estimating 3D positions of objects from only monocular video. Estimating these 3D positions requires three key pieces of information: camera intrinsics/extrinsics, the 2D pixel location of the object and the depth of the object. To collect camera intrinsics, we calibrate the camera using traditional checkerboard method prior to collecting data. To estimate the extrinsics dynamically for every video/setting, we place checkerboard in the background of all of our videos. Since we know the 3D locations of the checkerboard corners, we can use them to estimate the camera extrinsics given the intrinsics. We additionally verify the camera intrinsics and extrinsics through manual measurement (e.g. we manually measure the distance between the camera and the checkerboard and compare). To extract the 2D locations of objects, we use SAM2 [40] prompted by manually selected prompt point to track the object through the frames. We take the centroid of the object mask as its 2D pixel location. Robustly estimating the objects depth given just monocular video is difficult, so we instead opt to design our setup so that the depth is always constant and can be measured exactly. We do this by placing our objects at constant depth, and ensuring that they move only in plane parallel to the camera plane. Once we have estimated the 3D position of the object throughout the video, we can use those positions to estimate relevant physics parameters. To do this, we first estimate the acceleration of the object. This is done by fitting quadratic equation to the object positions over time and taking the appropriate coefficients. For our gravity scenarios, we then use this acceleration directly and compare it to g. For the friction setups, we use the following equation to compute friction coefficient from the acceleration: µ = sin θ cos θ where θ is the angle of the ramp and is pre-measured. For viscosity, instead of estimating accleration, we estimate the terminal velocity of the object in the fluid. We do this by fitting line to the object positions and taking the slope. We then use the following equation to estimate viscosity: η = 2r2(ρs ρf )g 9vt 6 Table 4. Estimated Physics Parameters on the Physical Parameter Estimation subset (Real Videos). Both ground truth ranges as well as our real video estimations are provided as comparison (note that our video estimations are all very close to or within the range of acceptable values). For gravity and viscosity, models closest to the ground truth values are bolded. Due to the wide range of acceptable friction values, models closest to our real video estimations are bolded. Model Ground Truth Real Videos Cosmos 1 AR [2] Cosmos 1 DM [2] Cosmos 2 (2B) [37] Cosmos 2 (14B) [37] Cosmos 2.5 [3] Wan 2.2 [48] Hunyuan Video [29] CogVideoX [59] Free-Fall (m/s2) Parabolic (m/s2) Gravity 9.81 9.78 0.38 4.215 3.713 3.506 1.912 8.927 4.791 8.428 3.478 4.778 4.474 0.378 0.784 0.369 0.788 -0.039 0.136 9.81 9.85 0.36 4.297 1.294 7.652 2.927 8.228 3.813 9.145 4.171 5.375 2. 1.682 3.065 0.206 0.407 0.181 0.239 Glycerine (Pa.s) 1.2 1.22 0.01 7.8 1.04 0.603 1.191 0.252 0.394 0.221 0.101 0.802 0.478 3.418 5.119 7.203 3.709 3.286 2.854 Viscosity Corn Syrup (Pa.s) 6.0 5.84 0.02 8.44 2.11 1.538 1.478 1.091 0.309 1.304 0.844 1.091 0. 3.162 3.169 > 50 2.247 2.769 Honey (Pa.s) Wood Rubber 14.1 13.82 0.75 > 50 0.17 0.171 9.845 6.198 1.805 1.813 1.678 0.079 3.281 2.758 > 50 2.672 1. 0.2-0.5 0.35 0.05 0.541 0.124 0.674 0.165 0.696 0.094 0.659 0.162 0.695 0.185 0.668 0.101 0.662 0.086 0.634 0.126 0.5-2.0 0.93 0.10 1.237 0.142 1.295 0.214 1.468 0.26 1.224 0.410 1.449 0.339 1.321 0.223 1.432 0.227 1.474 0.3315 Friction Sandpaper (80) 0.7-1.1 1.06 0.05 1.277 0.185 1.453 0.257 1.262 0.259 1.144 0.473 1.522 0. 1.476 0.242 1.049 0.222 1.288 0.498 Sandpaper (3000) Plastic 0.2-0.5 0.30 0.02 0.528 0.079 0.522 0.139 0.598 0.111 0.556 0.088 0.611 0.119 0.516 0.155 0.531 0.076 0.516 0.134 0.05-0.2 0.22 0.03 0.508 0.101 0.481 0.119 0.614 0.131 0.461 0.393 0.628 0. 0.576 0.105 0.569 0.104 0.502 0.201 Table 5. Estimated Physics Parameters on the Physical Parameter Estimation subset (Simulated Videos). Various different materials are tested for friction. We give the average RMSE error instead. Values closest to the ground truth value or with the lowest error are bolded. Model Params Free-Fall Parabolic Gravity Friction Varied Materials Average Error () Ground Truth Cosmos-1 AR [2] Cosmos-1 DM [2] Cosmos-2 [37] Cosmos-2 [37] Cosmos-2.5 [3] Wan 2.2 [48] Hunyuan Video [29] CogVideoX [59] 5B 7B 2B 14B 2B 5B 13B 5B 9.81 10.831 2.925 7.530 3.672 4.288 5.061 4.230 6.740 10.494 3.934 1.157 2.631 0.939 0.719 -0.226 2.392 9.81 7.408 1.15 6.274 1.595 13.229 4.312 2.930 3.023 12.540 3.296 0.062 1.034 0.677 0.569 0.309 1.631 0.298 0.231 0.232 0.274 0.217 0.253 0.2275 0. where ρs and ρf are the densities of the sphere and fluid respectively and vt is the terminal velocity. 4. Discussion We evaluate the Cosmos family of models and number of image-to-video models on both subsets of WorldBench. Note that as of now, Cosmos is the only video-to-video WFM that is open-source, though we expect its introduction to expand the already growing research area. We evaluate Cosmos-1 (Auto-regressive and Diffusion), Cosmos-2 (2B and 14B) and Cosmos-2.5. For image-to-video models, we evaluate Wan 2.2, Hunyuan Video, and CogVideoX. For the intuitive physics subset, we use two metrics to evaluate the accuracy of generated videos: Foreground mIoU and Background RMSE. Foreground mIoU compares ground truth object segmentations with segmentations extracted from generated videos by SAM2 [40] and gives us information about how accurately the models can predict the dynamics and evolution of the scene. Background RMSE on the other hand, computes the RMSE between the background in the ground truth video and generated video. This metric gives us information about whether the model is able to keep the surrounding scene/environment consistent while objects are in motion. Specific implementation details of our evaluation methods are provided in the supplement."
        },
        {
            "title": "Quantitative results from our evaluations are shown in",
            "content": "Table 3, Table 9, Table 4, and Table 5. Results for the background RMSE are in the Supp. Mat. We summarize some of our learnings about current WFMs below: Across the board, models exhibit an extremely high variance in the parameter estimation subset. Experiments captured in the physical parameter estimation subset contain highly constrained dynamic sequences in an effort to focus solely on the parameter being tested. Despite this, all models (both Cosmos-family and image-to-video) exhibit extremely high variance between rollouts. This is especially apparent in the gravity experiments where objects experience downward acceleration at varying levels, even between rollouts with the same object and object trajectory. Model outputs tend to follow realistic motion trajectories, while not adhering to realistic motion parameters. For example, for gravity, all models tended to exhibit realistic motion paths most of the time (both parabolic trajectories and straight drops). However, they fail to abide by 2. Similar trends appear in the fluid viscosthe proper 9.8 ity and friction experiments. As remarked on before, this highlights the need for benchmarks that can target particular physical parameters, as visual realism alone is not sufficient for synthetic world data generators. Parameter-specific performance trends. Certain models (Cosmos 2 in particular) performed well on the gravitational acceleration subtask, while others tended to under7 Table 6. Foreground mIoU results on the Physical Principles Understanding subset (Real Videos). Higher is better for all columns Model Params Motion Physics Object Perm. Scale Support Relations Avg. Cosmos-1 AR [2] Cosmos-1 [2] Cosmos-2 [37] Cosmos-2 [37] Cosmos-2.5 [3] 5B 7B 2B 14B 2B 0.3826 0.2156 0.2931 0.2716 0.2896 0.3471 0.3644 0.3641 0.3697 0.3114 0.1634 0.044 0.0910 0.0836 0.0905 0.7517 0.6362 0.7648 0.6521 0.6691 0.4112 0.3151 0.3783 0.3443 0. accelerate the object. Models across the board tended to fail at fluid viscosity estimation, likely due to lack of training data involving fluid dynamics. Friction experiments, on the other hand, tended to be easier for models to predict. Importantly, models tended to abide by the correct ordering of terminal velocity from slowest to fastest (sandpaper at 80 grit, rubber, wood, sandpaper at 3000 grit, and plastic). Image-to-video models tended to under-perform on the gravitational acceleration subtask. Image-to-video models suffer from lack of temporal information in the input, and so typically under-performed. However, this trend was exacerbated in the gravitational acceleration subtask, where lack of temporal information led to severely low or even negative gravity estimates (e.g. ball slowing down). Models perform similarly on the synthetic and real versions of both benchmarks We find that across both subsets, all tested models perform similar in synthetic and real scenarios. This suggests that the cause for poor performance is not the distribution gap between real world videos and synthetically generated test cases but rather due to poor physics understanding in the models. Models do not handle material properties on the longtail of real-world distributions well We find that the models typically are unable to accurately simulate objects or materials which are far from average values such as Honey (very high viscosity relative) or Plastic (very low friction coefficient relative). Instead, they tend to simulate these materials closer to the average and do not differentiate well. Additionally, the variance does not meaningfully increase on these materials, showing that the model is consistent in these incorrect predictions. Models perform better on scenarios with longer object interaction periods. In the intuitive physics subset, we find that for scenarios where objects interacts for longer, such as Ramp, Table, and Walls, perform much better than shorter interactions such as in Two Object Fall, Two Object Parabolic Motion, or Dominoes. We can also see this in the intuitive physics subset where the friction tests, in which an object is slowly sliding down ramp, perform quite well compared to viscosity or gravity. Models tend to perform better on scenarios where strong training priors are present. We find that across 8 scenarios and subsets, models rely heavily on training priors for generating future frames. For example, in the intuitive physics subset, we find that the models handle balls rolling down the ramp very well, but have more trouble modeling the interaction of the ball and block at the bottom of the ramp. This is likely because ball rolling has been seen in training data, but obstructions on ramps are less likely. In the physical parameter subset, we also notice that most models tend to perform better on gravity prediction when the object being thrown is basketball compared to block or pool ball. All of this suggests that these models are relying more on priors from training data for their predictions rather than understanding of physical parameters/laws. Limitations The goal of this benchmark is to disambiguate between different core physical concepts and parameters. As such, we would like to continue expanding the number of concepts that we test. As each setup requires careful experimental design and extensive validation (for the parameter estimation subset in particular), this is an ongoing process. We plan to add additional physical tasks (collision mechanics, optics, etc.). Additionally, while our video-based design leads to more nuanced and diagnostic evaluation, requiring visual inputs limits the number of models we can evaluate. We hope that as image/video-tovideo models become more popular with the introduction of Cosmos, they can benefit from our benchmark. 5. Conclusion In this work, we introduce WorldBench, new benchmark designed to evaluate concept-specific physical understanding in todays world-foundation models. Previous benchmarks assessed models visual realism through humanbased metrics, or models dynamic trajectory adherence through coarse-grained metrics like object contact prediction. We leverage our benchmarks video-based and modular framework to directly measure adherence to physical concepts, constants, and material properties. This aids in our goal to 1) disambiguate between visually appealing and physically accurate generations, and 2) diagnose the core concept-specific failures of modern WFMs. We hope this benchmark can highlight the distinct challenges faced by current methods and can help guide the development and understanding of new ones as well."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. 2 [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 1, 3, 4, 5, 6, 7, 8 [3] Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al. World simulation with arXiv preprint video foundation models for physical ai. arXiv:2511.00062, 2025. 3, 5, 7, 8 [4] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and Francois Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37: 5875758791, 2024. 3 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 5 [6] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: new benchmark for physical reasoning. Advances in Neural Information Processing Systems, 32, 2019. 2, 3, [7] Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. 2, 3, 4 [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [9] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. 3 [10] Blender Online Community. Blender - 3d modelling and rendering package. 2002. 4 [11] Florian Bordes, Quentin Garrido, Justine Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments. arXiv preprint arXiv:2506.09849, 2025. 2, [12] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 3, 4 [13] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. 3 [14] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 5 [15] Erwin Coumans and Yunfei Bai. Pybullet, python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 20162021. 4 [16] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale arXiv preprint arXiv:1910.11215, multi-robot 2019. 3 learning. [17] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3 [18] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 73467356, 2023. 3 [19] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. 2 The llama 3 herd of models. [20] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator. 2022. 4 [21] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. 3, 4 [22] Oliver Groth, Fabian Fuchs, Ingmar Posner, and Andrea Vedaldi. Shapestacks: Learning vision-based physical intuition for generalised object stacking. In Proceedings of the european conference on computer vision (eccv), pages 702 717, 2018. [23] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang 9 Jiang, and Hang Xu. denoising for text-to-video generation. arXiv:2309.03549, 2023. 3 Reuse and diffuse: Iterative arXiv preprint [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [25] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 3 [26] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [27] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [28] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 5 [29] Team HunyuanWorld. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint, 2025. 3, 7 [30] Achuta Kadambi, Celso de Melo, Cho-Jui Hsieh, Mani Srivastava, and Stefano Soatto. Incorporating physics into datadriven computer vision. Nat. Mach. Intell., 5(6):572580, 2023. [31] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 3 [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 3 [34] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. 3 [35] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modarXiv preprint els for high-quality video generation. arXiv:2303.08320, 2023. 3 [36] Vincent Micheli, Eloi Alonso, and Francois Fleuret. Transformers are sample-efficient world models. arXiv preprint arXiv:2209.00588, 2022. 10 [37] NVIDIA. Cosmos-predict2: World simulation model for https : / / research . nvidia . com / physical ai. labs/dir/cosmos-predict2/, 2025. 5, 7, 8 [38] Luis Piloto, Ari Weinstein, Dhruva TB, Arun Ahuja, Mehdi Mirza, Greg Wayne, David Amos, Chia-chun Hung, Probing physics knowledge using and Matt Botvinick. arXiv preprint tools from developmental psychology. arXiv:1804.01128, 2018. 3 [39] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. arXiv preprint arXiv:2006.10704, 2020. 3 [40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 5, 6, [41] Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard, and Emmanuel Dupoux. Intphys: framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018. 2, 3, 4 [42] Jan Robine, Marc Hoftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023. 3 [43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [44] Kevin Smith, Lingjie Mei, Shunyu Yao, Jiajun Wu, Elizabeth Spelke, Josh Tenenbaum, and Tomer Ullman. Modeling expectation violation in intuitive physics with coarse probabilistic object representations. Advances in neural information processing systems, 32, 2019. 3 [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3 [46] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3, [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [48] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 7 [49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [51] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. 2023. 3 [52] Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019. [53] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-tovideo generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73957405, 2024. [54] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021. [55] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In European conference on computer vision, pages 720736. Springer, 2022. [56] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Progressive arXiv preprint Feng Liu, Arie Kaufman, and Yang Zhou. autoregressive video diffusion models. arXiv:2410.08151, 2024. [57] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3 [58] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. [59] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 7 [60] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 2, 3, 4 [61] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. 3 [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 11 WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Benchmark Details We provide below more detailed descriptions of the benchmark, including scene descriptions, prompt choice, and experimental setups. A.1. Prompt Choice Regarding prompt selection, we test variety of different prompts, providing varying degrees of scene and setup related detail. The final set of prompts were selected based on performance on the benchmark. The same prompts were used for each model that was tested. The prompts will be provided as part of the benchmark. A.2. Intuitive Physics Subset Here we provide more detailed description of the scenes used in our benchmark: Bouncing Ball sphere, initially at rest at some known height above the ground, is subject to fall freely under gravity. The provided video includes frames after the bounce, ensuring the model has the necessary information for prediction. In this scenario, the initial height of the ball and material (including bounciness) of the ball are randomized. Two Object Fall Two distinct objects are initially at rest at different heights above the ground and at slight horizontal offset. Both objects are released to freely fall under gravity, and then typically collide with each other and the ground. In this scenario, we randomize the shape and initial positions of both objects. Two Object Parabolic Motion This scenario is slight variation of the above scenario, where two distinct objects are placed on opposite sides of the scene. They are then projected towards each other at some randomized initial angle and projectile angle, not necessarily in the same vertical plane. In this scenario, the shapes, locations, and initial velocities of the objects are randomized to generate diverse scenes. Columns An object moving from left to right behind several thin columns. This is very similar to the Block & Obj task, except that the object periodically disappears and reappears as it passes each column. The provided frames generally In this scenario, the type of object and initial velocity are randomized. Raised Block Bounce sphere bouncing vertically behind raised block. sphere appears above the block when approaching its highest vertical position, and below the block when close to the ground, As it bounces, it is periodically occluded by the block and not visible to the camera. In this scenario, the mass, restitution (bounciness), and material of the sphere were randomized. Wall Bouncing sphere rolling horizontally behind block between two walls. As the sphere rolls from one side to another, it eventually collides with wall, bounces off, and rolls back the other direction. The sphere is periodically occluded while behind the block, and reappears in gap as it approaches wall. In this scenario, the mass, initial velocity, and friction coefficient of the sphere were randomized. Two Ball Bounce Two spheres bouncing vertically, with one larger sphere in front and one smaller sphere straight behind it. During this motion, the small sphere is periodically occluded by the large ball as their vertical positions diverge. In this scenario, the mass, restitution, and materials of both spheres were randomized. Dominoes This scenario features an object colliding with series of standing dominoes on surface. Depending on initial velocity, the object knocks over varying number of dominoes, which may subsequently topple onto one another. In this scenario, the type/shape of the object thrown and initial velocity are randomized. Block & Obj An object is moving from left to right behind wall. The movement is linear and predictable, starting left of the wall with randomized initial velocity. The object disappears behind the wall and reemerges on the right side. In this scenario, the type of object along with its initial velocity are randomized. Ramp Block sphere rolling down an incline until it encounters fixed barrier at the end, causing it to stop. This tests two things, one whether the incline supports the ball as it rolls, and then if the block at the bottom supports and stops it. In this situation, the angle and length of the incline and the initial position of the sphere are randomized. 1 Table Drop An object is positioned at tables edge with portion extending beyond the surface. This scenario directly challenges models understanding of the minimum conditions required for stable support and balance. Mere contact with support surface is insufficientthe object requires adequate support distribution relative to its mass distribution. In this scenario, both the type/shape of the object and its location relative to the table were randomized. This also meant that physical parameters such as mass and restitution were in sense randomized as they depended on the object chosen. Obj/Sphere Moving Towards Camera single object (e.g. sphere or miscellaneous irregular object) is launched from the background and moves towards the camera. As the object approaches, it should appear to increase in size due to perspective. In these scenarios, the type/shape of object and the initial velocity are randomized. Obj/Sphere Moving Away From Camera single object (e.g. sphere or miscellaneous irregular object) is launched from the background and moves towards the camera. As the object moves away, it should appear to decrease in size due to perspective. Similarly, in these scenarios, the type/shape of object and the initial velocity are randomized. A.3. Physical Parameter Estimation Subset Setup Details Scenes were all captured using an iPhone 13 slow-motion camera. checkerboard pattern was placed in all scenes for camera intrinsic and extrinsic calibration. Camera poses were manually adjusted to ensure consistent and accurate pose for every scene capture. Depth is estimated through the checkerboard calibration, but also validated through manual measurement for each scene capture. Videos are trimmed and frame rate information is verified through metadata. SAM2 segmentation is used for 2D pixel location. Initial segmentation is provided through prompt point (but optionally can use language-guided prompt as well). Acceleration is estimated from 3D locations using quadratic fit. For viscosity (which uses terminal velocity), linear regression method is used to estimate the terminal velocity (videos are trimmed to ensure object is at terminal velocity at the first frame). All evaluation pipelines are validated before running them on video generation model outputs. This is done by testing them on our collected videos, and ensuring that parameter estimates line up with the generally accepted ground truth values (refer to Table 2 in the main paper). ruler is added on the free-fall, parabolic, and five friction setups to help video generation models get sense of scale. Viscosity experiments include beaker with markings to have sense of scale. It is worth noting addtionally that video-to-video models receive enough input frames for the model to estimate the correct gravitational acceleration, viscosity, or friction coefficient. Gravity For free-fall experiments, an object is dropped from rest and accelerates downward until it exits the frame. The video is trimmed so that the initial frame is already freely in the air. For parabolic motion experiments, objects are pushed up ramp and fall towards the ground. The video is trimmed such that the object is already freely in the air in the initial frame. Ramp angle is randomized to ensure variety. Objects for both experiment types are randomized and chosen to ensure the impact by air resistance is negligible. Viscosity For the viscosity experiments, large beaker containing either glycerine, corn syrup, honey is sitting upright on table. steel ball is dropped into the beaker and falls towards the bottom. The terminal velocity at which the ball flows through the liquid can determine the liquids viscosity. The video is trimmed to ensure that the object is already falling at terminal velocity in the initial frame. The size of steel ball is randomized to ensure variety. Friction For the friction experiments, steel cube is sliding down ramp covered with either wood, rubber, sandpaper (80 or 3000 grit), or plastic. Using the magnitude of the acceleration vector, we can determine the kinetic coefficient of friction of the ramp material. The video is trimmed to ensure that during the initial frame, there are no external forces other than gravity and friction acting on the object. The ramp angle is adjusted and randomized to ensure variety. B. Text-Enhanced Subset To supplement our image-to-video and video-to-video benchmarks, we additionally created language-ased subset of the benchmark. This is to aid in the evaluation of modern vision-language models (VLMs) on physics understanding and prediction. This subset asks models to interpret visual details or predict physical outcomes, allowing us to assess their ability to do physical reasoning in diverse situations. We select subset of 181 videos and write 1 natural language question per video. Questions can be either binary True/False questions or multiple choice with up to 4 choices. Example questions and answers are shown in Fig. 4. B.1. Previous Work on Multi-modal VLMs Accompanying the wave of popularity of large language models is the vision-language model, multi-modal model capable of processing both text and visual input [1, 19, 32, 2 C. Additional Results In this section, we provide additional video generation results on our dataset, including qualitative results showing scene rollouts, background rmse results, as well as scene evolution over time. C.1. Additional Qualitative Results We provide here qualitative results of video generations on both subsets of our benchmark. For the intuitive physics subset, we show qualitative results in Fig. 5, Fig. 7, Fig. 8, and Fig. 6. For the physical parameter estimation subset, we show results in Fig. 9, Fig. 10. Note that, as mentioned in the main paper, results varied greatly between generations, and results in one rollout as seen in the figures is not indicative of broad trends throughout the benchmark. C.2. Background RMSE Results For the intuitive physical understanding subset, we use the foreground mIoU metric to evaluate video generation performance. As an additional metric, we provide here background RMSE results. Results are in Tab. 8 and Tab. 6 C.3. Results Over Time For the intuitive physical understanding subset, rollouts can be especially long (as opposed to the short physics experiments in the second subset). Here, we analyze the differences between the 30 frame and 120 frame rollout length. As seen in Fig. 11, foreground mIoU is inversely related with how far in the future the model is predicting, suggesting compounding error effect in physical accuracy. This highlights the need for accurate physical understanding, as compounding error effects can be especially detrimental if leveraging world models as synthetic data generators. D. SAM Validation For both our benchmark subsets, we rely on SAM2 for object segmentation and tracking, so we perform validation checks to evaluate SAM2 accuracy. Specifically, since we hand-annotate the real part of the intuitive physics subset, we compare those annotations against automatic SAM2 segmentations based on our prompting methods. We find an overall mIoU of 0.9445, showing high overlap between hand-annotated and SAM2 segmentations. Additionally, in Fig. 12, we show SAM2 tracking an object in the object permanence scenario: even though the object disappears behind another object for 20+ frames, SAM2 continues to track it well due to prompting and maintained state. 33, 46, 50, 58, 62]. These models are typically capable of video understanding and reasoning. The recent Gemini model [45, 46] is an example of multi-modal model, capable of flexibly taking in any order of visual, textual, or audio input. The more recent Qwen2.5-VL model [5, 50], from the Qwen series of vision-language models, uses dynamic resolution processing and absolute time encoding, and particularly targets visual agents ability to perform visual reasoning, tool usage, and task execution. To evaluate the dynamics prediction accuracy and physical reasoning abilities of these models, we extend our proposed benchmark with language-based visual reasoning framework. B.2. Results In order to evaluate the language-based subset of WorldBench, we test SOTA closedand open-source models: Qwen2.5 and Gemini. Qwen2.5 comes in 3 sizes, 7B, 32B, and 72B parameters, and is designed to handle vision inputs natively. For Gemini, we test both Gemini 2.5 Flash and Gemini 2.5 Pro. When running the evaluations, all models are provided with system prompt which describes the tasks, defines what the output format should be, and the format of the data provided (9 frames). The Qwen models are run on 1 H100 GPU with the use of vLLM, while the Gemini models are evaluated through the provided API [31]. The total costs for evaluation were approximately $25. The outputs of the models are evaluated by directly comparing against the answers. Results for all the models are shown in Table. 7. Across all 4 scenarios, Gemini 2.5 Pro performs the best overall, achieving 49.72% accuracy. Within the open source models, Qwen2.5 32B surprisingly outperforms the larger 72B model, largely due to very strong performance on the motion physics category. However, overall, all five models perform relatively poorly on the benchmark, achieving results only slightly better than chance. This suggests there is still much work to be done to improve the physical understanding of modern VLMs. B.3. Additional Quantitative VLM Metrics In this section, we expand upon the results in Tab. 3 of the main paper and show the results for each model by scene category. Although all 5 models tested perform similarly in most categories, we notice striking differences in the Walls category where the Qwen models are all near 0.0 while both Gemini models achieve accuracys > 0.6. All models have the most trouble with the Object Permanence scenes and perform the best on Motion Physics scenes overall. This is somewhat expected as it is likely that their training data included more examples of motion physics than of object permanence. 3 Figure 4. Qualitative Examples of the Language-based subset of WorldBench. VLMs are given access to 9 frame video (same as what is inputted to COSMOS) and ask to answer True/False or multiple choice question based on the video and future predictions. Figure 5. Qualitative examples for the Motion Physics scenario of the intuitive physics subset. For the motion physics example, two objects (a vase and knot) are thrown at each other, collide, and then fall to the floor. 4 Figure 6. Qualitative examples for the Support Relations scenario of the intuitive physics subset ball is rolled down ramp towards solid block near the bottom. Table 7. Results of SOTA closed and open models on our language-based benchmark. Gemini 2.5 Pro, closed model performs best overall, and Qwen2.5 32B performs best among open models. Open Models Closed Models Model Qwen2.5-VL-7B [5] Qwen2.5-VL-32B [5] Qwen2.5-VL-72B [5] GLM 4.1V 9B [28] Mistral Small 3.2 24B Llama-3.2-11B-Vision Gemini 2.5 Flash [46] Gemini 2.5 Pro [46] Claude Sonnet 4 GPT 4.1 Motion Phys Obj. Perm. 0.5161 0.8710 0.5806 0.6674 0.4838 0.5161 0.6452 0.6774 0.7096 0.3781 0.2381 0.2738 0.3333 0.3453 0.2500 0.1548 0.3571 0.4048 0.4286 0. 5 Scale/Persp. 0.4474 0.4737 0.4211 0.4473 0.3684 0.3421 0.6053 0.5000 0.5526 0.5000 Support Rel. Avg. 0.3737 0.4641 0.4309 0.4641 0.3315 0.2873 0.4751 0.4972 0.5027 0.3701 0.5357 0.5714 0.5714 0.6071 0.3571 0.3571 0.4643 0.5714 0.4285 0.5000 Figure 7. Qualitative examples for the Object Permanence scenario of the intuitive physics subset. An object is thrown behind sequence of thin columns, appearing and dis-appearing as it goes. Table 8. Background RMSE results for Cosmos models on our benchmark. RMSE is computed over only the ground truth segmentation for background. Lower is better for all columns. Model Params Ball Bounce Two Obj Fall Two Obj Para Block/Obj Columns Raised Block Walls Two Ball Cosmos-1 AR Cosmos-1 Cosmos-2 Cosmos-2 Cosmos-2.5 5B 7B 2B 14B 2B 0.2112 0.2403 0.2459 0.2265 0. 0.2168 0.2221 0.2429 0.2095 0.2361 0.2542 0.2594 0.2370 0.2177 0.2659 0.1943 0.2103 0.1970 0.2086 0.2227 0.1376 0.1104 0.0799 0.1401 0.0853 Params Obj Tow. Obj Away Sphere Tow. Sphere Away Dominoes Cosmos-1 AR Cosmos-1 Cosmos-2 Cosmos-2 Cosmos2.5 5B 7B 2B 14B 2B 0.2226 0.2534 0.2551 0.2108 0. 0.1696 0.2329 0.2298 0.2094 0.2355 0.1848 0.2256 0.1936 0.1886 0.2021 0.1616 0.0998 0.2231 0.2015 0.1952 0.1218 0.1325 0.1760 0.2061 0.1738 6 0.0978 0.1223 0.0344 0.1393 0. Ramp 0.1027 0.2283 0.3182 0.1947 0.1488 0.2299 0.2223 0.2212 0.2027 0.2117 Table 0.1715 0.2586 0.3222 0.2382 0.1669 0.1440 0.1709 0.1368 0.1601 0. Avg. 0.1747 0.1993 0.2075 0.1969 0.1877 Figure 8. Qualitative examples for the Scale/Perspective scenario of the intuitive physics subset. metallic sphere is rolling away from the camera. All models perform well on this sample. Table 9. Background RMSE results on the Physical Principles Understanding subset (Real Videos). Lower is better for all columns Model Params Motion Physics Object Perm. Scale Support Relations Avg. Cosmos-1 AR [2] Cosmos-1 [2] Cosmos-2 [37] Cosmos-2 [37] Cosmos-2.5 [3] 5B 7B 2B 14B 2B 0.0887 0.1011 0.1589 0.1289 0. 0.2432 0.2100 0.2381 0.2015 0.2317 0.2568 0.2935 0.2681 0.2358 0.2386 0.2295 0.3090 0.2646 0.3018 0.2784 0.2045 0.2284 0.2324 0.2170 0.2278 7 Figure 9. Qualitative examples for the Friction scenario of the physical parameter estimation subset. steel block is released from rest down ramp at pre-set elevation and with specific friction coefficient. 8 Figure 10. Qualitative examples for the Motion Physics (top) and Scale/Perspective (bottom) scenarios. For the motion physics example, two objects (a vase and knot) are thrown at each other, collide, and then fall to the floor. The auto-regressive model greatly distorts the object shapes, while the diffusion model hallucinates the vase into tank and adds human hand. For the scale example, metallic sphere is rolling away from the camera. Both models perform well on this sample. 9 Figure 11. mIoU and background RMSE results over time. The foreground mIoU is inversely related with how far in the future the model is predicting. There is sharp drop off after frame 5 or frame 9 when the model first begins predicting (depending on the model) The shaded region shows 1 standard deviation. Figure 12. SAM2 tracking an object in the object permanence scenario. Despite the fact that the object disappears for 20+ frames, SAM2 is able to track it when it re-appears."
        }
    ],
    "affiliations": [
        "DEVCOM Army Research Laboratory",
        "Sony AI",
        "University of California, Los Angeles",
        "Yale University"
    ]
}