{
    "paper_title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "authors": [
        "Zhengcong Fei",
        "Debang Li",
        "Di Qiu",
        "Jiahua Wang",
        "Yikun Dou",
        "Rui Wang",
        "Jingtao Xu",
        "Mingyuan Fan",
        "Guibin Chen",
        "Yang Li",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 3 4 2 0 . 4 0 5 2 : r SkyReels-A2: Compose Anything in Video Diffusion Transformers Zhengcong Fei Debang Li Di Qiu Jiahua Wang Yikun Dou Rui Wang Jingtao Xu Mingyuan Fan Guibin Chen Yang Li Yahui Zhou Skywork AI, Kunlun Inc. Project page: SkyReels-A2.github.io Figure 1: Examples of elements-to-video results from our proposed SkyReels-A2 model. Given reference with multiple images and textual prompt, our method can generate realistic and naturally composed videos while preserving specific identity consistent."
        },
        {
            "title": "Abstract",
            "content": "This paper presents SkyReels-A2, controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elementsto-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose novel image-text joint embedding model to inject multi-element representations Equal Contributions Project Lead Technical Report into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation. The code and model weights of SkyReels-A2 are publicly available at https://github.com/SkyworkAI/SkyReels-A2."
        },
        {
            "title": "Introduction",
            "content": "The emergence of diffusion models has significantly advanced the field of generative modeling, fundamentally transforming the paradigm of content creation [75, 65, 58, 14, 24, 25]. Building upon text-to-image architectures, large-scale pre-trained video diffusion frameworks have achieved remarkable breakthroughs by incorporating temporal consistency mechanisms into their attention designs [39, 5, 73, 32, 6]. Notably, the scalability of diffusion transformer architectures with full 3D attention [58, 49, 102, 103, 91, 60, 86, 84] has propelled the field toward high-quality visual content, enabling broad spectrum of downstream applications [98, 99, 72, 95, 77, 96, 28]. These controllable scenes are impactful in the area of limited subject customization [85, 83, 87, 9, 56, 16], facilitating highly customizable and dynamic visual outputs [27, 62]. Despite these remarkable advancements, releasing video diffusion model capacity into multiple consistent elements generation [89], such as AI drama and virtual e-commerce, remains an ongoing challenge. Contemporary foundational video generation models predominantly address two core tasks: textto-video (T2V) and image-to-video (I2V). T2V usually utilizes T5 [64] or CLIP [63] text encoder to interpret textual instructions and generate visual content that reflects the desired characters, actions, and settings. Although T2V fosters creative and diverse content generation, it frequently struggles to maintain consistent and predictable outcomes due to the inherent stochasticity of diffusion process [60, 91]. Conversely, I2V typically transforms static image into dynamic video by providing an initial frame alongside optional textual descriptions, which is often constrained by the \"copy-paste\" dependency on the initial frame [97, 27, 41, 53]. In this paper, we endeavor to visual elements-to-video task, that synthesize dynamic and natural composed videos that precisely follows multiple reference images and textual prompts, thereby integrating the flexibility of textbased generation with the controllability in image conditioning. It is worth noting that the scope of referenced compositions are not limited to human characters or subjects, but also encompass any animals, landscapes, and various other visual elements. In this work, we first construct data structure consisting of text-reference-video triplets, where reference includes multiple images for different visual elements. We have re-annotated and aligned the video captions to focus on describing the appearance and action of the elements in the video. Additionally, the reference images for elements are not naively taken from single video frame but are sampled and selected across multiple videos, ensuring that the generated video does not simply copy and paste the images. Then, we develop unified video generation framework for any visual element, which is competitive with existing solutions on the market. Specifically, we design joint image-text injection from semantic and spatial ways based on existing foundational video models to ensure effective learning of cross-modal data representations. Furthermore, we propose A2-Bench, novel benchmark for comprehensively evaluating the E2V task, which exhibits statistically significant correlation with human subjective judgments. Finally, we employ training-free acceleration for user-friendly experience. Extensive experiments demonstrate the superiority of our proposed method, highlighting its efficacy as generative control mechanism. In summary, our methodology demonstrates robust flexibility, enabling the seamless integration of diverse visual elements within the synthesized video outputs, making it highly adaptable to diverse applications. The contributions are summarized as below: 2 We propose SkyReels-A2, an elements-to-video (E2V) framework based on video diffusion models, aiming to maintain fidelity from multiple reference images (characters, objects, and background) while enabling precise control through textual instructions. We introduce meticulously curated dataset comprising high-quality text-reference-video triplets, and develop A2-Bench which facilitates comprehensive and automated assessment of elements-to-video tasks. Extensive experiments demonstrate that SkyReels-A2 can generate high-quality, editable, and temporally consistent multi-visual-elements videos, performing favorably against advanced commercial closed-source models in both qualitative and quantitative analyses. To encourage further advancements in this domain, we have publicly released our code, models, and evaluation benchmark."
        },
        {
            "title": "2 Methodology",
            "content": "Formally, given set of input reference images, denoted as {Cn}N n=1, describe 1 individual subjects and background of an image, our goal is to generate high-quality and natural videos composed of these visual elements while following text prompts. We first discuss text-to-video diffusion models and image condition preliminaries in Section 2.1. Following this, we details architecture of SkyReels-A2 for composition video in Section 2.2. Finally, we describe dataset construction pipeline in Section 2.3 and evaluation A2-Bench in Section 2.4, respectively. 2.1 Preliminaries Text-to-video diffusion. Diffusion-based text-to-video models usually learns to iteratively transforms noise ϵ into video x0. Early approaches implement the denoising process directly within the pixel space [75, 38, 74]. However, due to the substantial computational costs, more recent techniques predominantly leverage latent spaces [66, 47, 98, 91, 86] with 3D VAE. The optimization objective can be defined as: Ldif = Ex0,t,y,ϵ ϵ ϵθ (E (x0) , t, τθ(y))2 2 (cid:104) (cid:105) (1) where denotes textual prompt, ϵ is randomly sampled from Gaussian distribution, e.g., ϵ (0, 1), and τθ() is the text encoder, e.g., T5 [64]. Substituting x0 with its latent representation (x0), form the diffusion process. Note that our method following more advanced flowing matching [14] to optimize the velocity with MSE loss. Video generation models based on DiT demonstrates considerable potential in capturing the dynamics of the physical world [8, 91, 103]. Despite the recent advancements achieved through scaling along 1D sequences, research on controllable generation within this framework remains relatively under-explored. In particular, no prior study has investigated the mechanisms underlying the efficacy of DiT in scenarios of multi-visual-elements composition. Conversely, commercial models have consistently demonstrated superior performance in this domain. This work aims to bridge the gap between open-source methodologies and proprietary solutions, addressing this unexplored aspect to advance the state of controllable video generation. Condition with images. Although natural language provides an intuitive means for controlling generative models, it often lacks the specificity required to accurately describe objects. To address this limitation, recent approaches [86, 39] extend text-conditioned diffusion models to condition on reference images. For example, in Wan [84], the image prompt Iimg is first encoded with pre-trained image encoder, i.e., CLIP, to obtain visual embeddings Evision(Iimg), and then the hidden states Himg are projected to the key matrix img (Himg). Analogous to text prompts, the image prompt is integrate with additional cross-attention for queries of video sequence as: (Himg) and values matrix img Softmax( (Htxt) img text (Himg)T ) img (Himg). (2) The final output hidden representation is obtained by summing the outputs of text-prompt crossattention and image-prompt-based attention. Meantime, reference image Iimg is also encoded with VAE to obtain spacial latent features, which is concatenate with input noise in channel dimension before forward into patch embedding. Different image-conditioned methods [39, 86] differ in encoder design and image integration branches. 3 Figure 2: Overview of Skyreels-A2 framework. Our approach initiates by encoding all reference images using two distinct branches. The first, termed the spatial feature branch (represented in red bottom arrow), leverages fine-grained 3D VAE encoder to process per-composition images. The second, identified as the semantic feature branch (represented in red top arrow), utilizes CLIP vision encoder followed by an MLP projection to encode semantic references. Subsequently, the spatial features are concatenated with the noised video tokens along the channel dimension before being passed through the diffusion transformer blocks. Meanwhile, the semantic features extracted from the reference images are incorporated into the diffusion transformers via supplementary cross-attention layers, ensuring that the semantic context is effectively integrated during diffusion. 2.2 Architecture The overall architecture is depicted in Figure 2. Given set of reference composed images and background images, denoted as {Cn}N n=1, we aim to generate high-quality videos that preserve the identity of each reference and background, while following textual prompt with flexible layouts. Our approach leverages an advanced video diffusion transformer architecture, with minimal structural modifications to ensure adaptability across broad spectrum of applications. We segment each subjects in the reference image with white grounding except background reference image, to avoid additional noise. Each input reference subject, Cn, is processed through two-stream structure for cross-modal projection. The first stream use semantic image encoder Eimg to extract visual embedding for global and semantic features. The second stream use original VAE to obtain spacial and local detailed features. Specifically, the semantic encoder is instantiated using CLIP image encoder, which extracts gridbased features from its penultimate layer. projection module subsequently transforms these features into image queries with dimensions aligned to the video sequence queries. The resulting image tokens from all reference images are concatenated and utilized as keys and values within the cross-attention layers, which are integrated after each text-prompt cross-attention block. For the spatial branch, reference images are first concatenated along the frame dimension and zero-padded to match the original frame number. standard 3D VAE is then applied to extract video latents, which are subsequently concatenated with the noise latents along the channel dimension before being passed through the patch embedding module. 2.3 Training and Inference Training objectives. Given the composition prompts {Cn}N n=1 and input text prompt , we first segment the conception in original images without background, train our SkyReels-A2 model to reconstruct the target video in the latent space. Our training objective follows standard diffusion MSE losses as Equation 1. During training, we only optimize the following neural modules: cross-attention, patch embedding, image condtion embedders, while keeping the remain parts frozen. 4 Figure 3: Data processing pipeline in SkyReels-A2. The pipeline begins with preprocessing, where raw videos are filtered by resolution, labels, types, and sources, followed by temporal segmentation based on key-frames. Next, proprietary multi-expert video captioning model generates both holistic descriptions for video clips and structured concept annotations. Subsequently, detection and segmentation models extract visual elements (e.g., humans, objects, environments). To mitigate duplication, reference images are retrieved from other clips based on clip/facial similarity score. Further refinement includes face detection and human parsing to obtain facial/attire elements. Finally, the extracted elements are matched with structured descriptions to form training triplets (visual elements, video clips, and textual captions). Inference acceleration. Following [84], we adopt UniPC multi-step schedule [101] for inference sampling. Meantime, we also consider some effective and advanced acceleration strategy for better practical employment. Generally, parallelizing the processing of neural network activations across multiple GPUs during diffusion model inference is crucial acceleration strategy, particularly as our model scales to 14B parameters, where slow inference in each sampling step becomes bottleneck. Our method employs Context Parallel, CFG Parallel, and VAE Parallel strategies to enhance model efficiency, enabling rapid and lossless video generation while meeting the stringent low-latency requirements of online environments. Additionally, we implement User-Level GPU Deployment, leveraging model quantization and parameter-level offload strategies to significantly reduce GPU memory consumption, thereby accommodating consumer-grade graphics cards with limited VRAM. 2.4 Dataset Construction Data construction plays pivotal role in achieving multi-subject consistent controllable video generation. Unlike conventional text-to-video (T2V) or image-to-video (I2V) tasks, our framework requires additional reference images for various subjects (e.g., humans, objects, and scenes). To address this challenge, we design comprehensive training data pipeline capable of generating high-quality video-caption-multi-reference triplets, which is illustrated in Figure 3. Our pipeline begins with the collection of large-scale video dataset. Each video is segmented into clips based on content coherence. We then employ an in-house multi-expert video captioning model to generate both holistic and structured captions for these clips. The structured captions encapsulate fine-grained details, including distinct subjects (e.g., humans, clothing, and objects), background information, as well as dynamic attributes such as facial expressions, actions, and motion trajectories. https://github.com/chengzeyi/ParaAttention 5 Next, we proceed to construct reference images. For each video clip, generic detection model [45] is first applied to localize humans and objects. For human subjects, we further utilize face detector [12] and human parsing model [46] to extract facial features and apparel details. To align the detected subjects with the structured captions, we leverage the CLIP model [44] to match textual descriptions in the captions with visual entities. To mitigate the \"copy-paste\" effect in generated outputs, we introduce an additional similarity-based filtering step. Specifically, we compute inter-clip similarity for detected subjects using face similarity model [12] (for humans) and CLIP-based similarity model [44] (for objects). This allows us to select diverse reference images for the same subject across different clips. For background reference construction, we identify the frame with maximal background coverage, remove foreground objects via cropping, and retain the purified background image. In conclusion, we compiled dataset of approximately 2 million high-quality video-reference-prompt triplets for training purposes. 2.5 A2-Bench Evaluation Existing video generation evaluation benchmarks, VBench[42] and VBench++[43], have introduced rigorous evaluation framework for text-to-video and image-to-video tasks through well-designed assessment suite and multi-dimensional evaluation criteria. However, comprehensive benchmark remains lacking for downstream video generation task, elements-to-video. In order to thoroughly assess the performance of E2V task across diverse scenarios, we propose an automated, comprehensive, and human-aligned A2-Bench, which provides systematic evaluation framework for assessing composition video generation models across multiple dimensions, ensuring rigorous and reliable performance measurement. We collected 150 reference images from various scenarios as elements for our E2V task, comprising 50 distinct human identities, 50 different objects spanning 12 categories, and 50 unique backgrounds. To construct the benchmark dataset, we randomly paired multiple elements (character, object, and background) into 50 diverse input combinations. Corresponding text prompts were then generated using LLMs to facilitate the dataset creation process.It is important to note that we have meticulously ensured that there is no overlap between the training videos and the constructed A2-Bench. The automatic metrics of A2-Bench comprise three core dimensions: composition consistency, visual quality, and prompt following, evaluated through eight fine-grained metrics. Composition Consistency. Composition consistency is designed as the core metric for evaluating the consistency of video-generated elements in the E2V task. Character ID consistency is used to evaluate the consistency of characters, with straightforward evaluation approach. After detecting the face, face recognition model[13] is employed to extract features and measure cosine similarity. Object consistency assesses the consistency of non-character objects, where we use Grounded-SAM to segment the object parts in the video and compute the similarity between frame-level clip features. Background consistency measures the similarity between the generated video scene and the reference background image. This is done by detecting and segmenting the subject, masking out the subject, and then calculating the similarity of frame-level clip features with the reference background image. Visual Quality. To assess the visual quality of the generated videos, we incorporate comprehensive image quality, aesthetic quality, motion smoothness and dynamic degree, as defined by VBench[42]. These metrics collectively capture both the temporal coherence and visual appeal of the generated content, ensuring comprehensive evaluation of video quality. Prompt Following. We leverage ViCLIP to compute the cosine similarity score between textual descriptions and video content, providing direct measure of text-video alignment. This approach enables an effective evaluation of the semantic consistency between the textual input and the corresponding visual representation. 6 Figure 4: The dimensions covered in A2-Bench. Our evaluation consider both automatic metrics and user study, meantime, it covers multiple perspectives that precisely reflects the quality of E2V task. Comprehensive Score. We incorporate human feedback to derive holistic assessment of composition consistency, visual quality and prompt following, recognizing that each dimension contributes differently to user preferences rather than applying simple average. User Preference Study. For the elements-to-video task, given the high error rates in automated element detection and matching, we conduct user preference study to assess visual quality and element fidelity to complement automated evaluation. We use 50 test samples and present the conditional image, prompt, and results from multiple models (including Keling, Vidu, Pika, and our SkyReels-A2) to multiple participants. For each sample, participants simultaneously view the four results and rate them on scale of 1 to 5 based on different evaluation criteria. Our approach of user study provides more intuitive comparison of the performance differences between models in terms of visual quality and element details. Notably, our user preference study adopts highly detailed evaluation framework, encompassing 10 distinct criteria, e.g., instruction following, face consistency, spatial rationality, and subject coherence. Each sample is assessed through human ratings based on these fine-grained dimensions, ensuring more precise evaluation of model performance. Additionally, we provide the full scoring guidelines in the Table 5 to enhance transparency and facilitate more rigorous and effective assessment of the element-to-video task."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Settings Implementation details. SkyReels-A2 is fine-tuned from video generation foundation model based on the DiT architecture [84]. The T2V and I2V pre-training stages are excluded from this evaluation. We focus on assessing the elements-to-video generation capability, including character, object and background similarity. During training, we drop the video captions and reference conditions with probabilities of 30% and 10% for classifier-free guidance, respectively. We pad with white image when the reference image is not equal to video ratios and the training video segments consist of 81 frames, corresponding to duration of 6 seconds at 15 frames per second (FPS). The training process employs the Adam optimizer with learning rate set to 1e-5 and global batch size of 256. During inference, we utilize 50 steps and set the CFG scale to 5. Baselines. For the elements-to-video task, the currently available state-of-the-art (SoTA) methods are closed source commercial tools. Therefore, we evaluate and compare the latest capabilities of Pika[2], Vidu[71], and Keling[1] products. 7 Figure 5: Consistency and video quality of user study results for elements-to-video generation. We can see that our SkyReels-A2 achieve comparable generative performance compared with toptier closed source commercial video models. Table 1: Comparison of different methods based on the automatic metrics of A2-Bench Evaluation Dimension. It contains three major dimensions including composition consistency, visual quality, and prompt following. We can see that our SkyReels-A2 achieve comparable composition performance, especially in objective consistency. Dimension Pika2.1 Vidu2.1 Keling1.6 SkyReels-A2 ID Consistency Object Consistency Background Consistency Image Quality Aesthetic Quality Motion Smoothness Dynamic Degree Image-Text Similarity Comprehensive Score 0.388 0.788 0.733 0.548 0.628 0.917 0.918 29.128 0.807 0.455 0.796 0. 0.565 0.616 0.960 0.878 28.926 0.821 0.497 0.790 0.700 0.587 0.609 0.879 1.000 27.504 0.826 0.398 0.809 0. 0.683 0.579 0.891 1.000 28.188 0.818 3.2 Quantitative Results The video quality evaluation results, are listed in the Table 1. We can see that for visual consistency perspective, SkyReels-A2 performs lightly poor on the background consistency metrics, while excelling in other metrics such object and character consistency. Moreover, in right part of video quality, SkyReels-A2 leads in overall comparable metrics for dynamic degree and image quality. For the elements-to-video task, due to some error rates in automated subject detection and matching, we also conducted user study. We surveyed multiple users, who rated the methods on scale of 1 to 5 (The score larger, the results in human perception better). The evaluation results, displayed in the Figure 5, show that our proposed models multi-reference images performance is comparable to commercial solutions in several metrics, even with some advantages in dress consistency and human motion smoothness. 3.3 Qualitative Results Here, we present the comparison results of several typical cases in Figure6. Each generated video is displayed with four evenly sampled frames, including the first and last frames. Figures 7 show more results of generating multiple subject consistency from SkyReels-A2. It can be seen that Vidu and SkyReels-A2 exhibit balanced performance in subject consistency, visual effect, and text response. Pika performs poorly in subject consistency with less motion. Keling has notable has noticeable 8 Table 2: Ablation study. We compare different design space in SkyReels-A2, and select the best optimal algorithm, i.e., after VAE with frame repeat, cross-1, and 1:0 data mixing ration, in the final version. In between, α denote the single and multiple visual element data mixing ratio."
        },
        {
            "title": "Before VAE\nAfter VAE\nFrame No Repeat",
            "content": "Cross-2 Cross-1 Full α = 2:1 α = 1:1 α = 0:"
        },
        {
            "title": "Prompt Following",
            "content": "ID Obj. Back. Image Quality Similarity 0.398 0.388 0.372 0.362 0.398 0.389 0.375 0.386 0. 0.809 0.798 0.756 0.732 0.809 0.798 0.772 0.798 0.809 0.677 0.697 0.612 0.612 0.677 0.670 0.642 0.665 0. 0.683 0.667 0.642 0.631 0.683 0.692 0.642 0.671 0.683 28.188 28.186 26.232 27.832 28.188 28.021 26.312 27.561 28. mirror movement effect, from far to near or vice versa. We believe it is the impact caused by data distribution, similarly, our model was trained using more movie level data sources. Overall, our results are balanced across all dimensions, with particular advantage in subject character consistency and motion naturalness. 3.4 Ablation Study In this section, we analyze the effect of different structure design of SkyReels-A2 in detail. Here, we only change the ablation parts and control the residual model and training paradigm, i.e., training data and steps, seed, learning rate and so on, frozen. Spacial feature combination. How to integrate spatial features is worth considering. Firstly, for the given multiple reference images, we attempted to (i) concatenate them in the original pixel space and then pass them through 3D VAE. Considering the compression of the temporal dimension, we also adopted the number of copies of the spatial dimension compression ratio in the pixel space (e.g., repeat for 4 times in Wan) to ensure that the image is not lost after being compressed by 3D VAE; (ii) Independently pass through 3D VAE and then concatenate the obtained features in latent space; (iii) We do not copy the compression temporal dimension, which cause more padding token in pixel space. The results can be seen in the first part of Table 2. We can see that no repeat makes decreasing in image information, leads to significant in composition consistency metrics. Referring to the generated examples, it can also be observed that there is an increase in the occurrence of subject loss without frame repeat. Meantime, we also find that concatenate in latent space may cause interaction loss between different reference images, also leads to marginal performance degration. Learnable parameters set. Generally speaking, the more parameters model is trained on, the greater the damage to its original capabilities and the higher the data requirements. Based on the same training data collected, here, we consider different learnable parameter set strategies. Specifically, we consider: (i) only training the model parameters of cross-attention every two layers; (ii) training all the cross-attention layer learning mechanisms for all layers; (iii) Fine tuning of the entire video diffusion models. Note that we always include the image projection layers and patch embedding as learnable parts. According to the listed results in second part of Table 2, we find that only training every two cross-attention layers leads to significant consistent performance loss, although reduce the memory request. Meantime, fine-tuning all the models helps to improve the image quality and makes the generated composition videos more naturalness, however, consider the performance and data scale balancing, training every cross-attention layers is optimal. Training data mixing ratio. Furthermore, we analyze the impact of data mixing on model performance. It is believed that integration of single ID data helps learn better representation in customization tasks [41]. Here, we first fixed the number of multi-subject data, and then increase the scale of single subject data. We denote single reference and multiple reference images ratio as α 9 Figure 6: Comparative results of elements-to-video generation with closed-source models. We can see that our SkyReels-A2 achieve achieves similar performance in composition and excels in the texture of light and shadow. 10 Figure 7: More generated results of SkyReels-A2. Our model has strong generalization ability and supports reference combinations of any subjects. 11 and modify the above ratios, setting them to 1:1, 2:1, and all multi subject data ratios. According to the results in the third part of Table 2, surprisingly, we found that adding single subject data did not improve the model performance in various composition scenarios. Therefore, we did not use mixing data in model training. We speculate that, similar to supervised fine-tuning, moderate amount of high-quality text-reference-video data can better stimulate the controllability of the model without compromising the originally generated fluency. Effect of inference acceleration and hyper-parameters. Considering that during training, the time step of schedule is set to 1000, and during inference, typically 30-50 is chosen. We need to sample inference step from 1000, and the in weighted schedule of controlled hyperparameter is flow shift. Here, we choose the best model and range this value from {1, 3, 5, 8, 12}. We find that the larger the value of flow shift selection, the more steps in early sampling, and the more reasonable the spatial structure, but the image details will gradually deteriorate. Taking into account the balance of motion and visual quality, we have chosen value of 8 by default. 3.5 Applications As an significant application of video generation models, we analyze their potential in music video creation and virtual E-commerce, with proposed SkyReels-A2. For music video creation, we can select instruments, such as guitars, and define scenes with more imagination (see Figure 7 third to last row) to generate seamless, creative sequences, if given music clips. Additionally, given product imagery (see Figure 7 second to last row), such as i-phone, we can design promotional content by placing celebrities in targeted scenarios. When paired with tailored voice-overs, this approach can effectively boost consumer purchasing intent. We will continue to pioneer innovative controllable generation scenarios to maximize the utility of video diffusion models."
        },
        {
            "title": "4 Related Works",
            "content": "4.1 Text-to-Video Generation The field of conditional video generation has seen remarkable progress, evolving from early approaches based on GANs and VAE to more advanced methodologies, such as diffusion-based models. Initial GAN-driven techniques [80, 70, 78, 11, 94] faced challenges in maintaining temporal consistency, often resulting in discontinuities between adjacent frames. To mitigate these issues, video diffusion models have been developed by adapting U-Net architecturesoriginally designed for text-to-image synthesisthereby improving frame coherence. Recent advancements have introduced diffusion Transformers [58, 24, 21, 26] and their extended variants [3, 22], which replace conventional convolutional U-Net backbones [67] with fully Transformer-based architectures. These models commonly employ spatio-temporal attention mechanisms and comprehensive 3D attention [58, 54, 55, 31], significantly enhancing their capacity to model complex video dynamics and maintain frame continuity [36, 7, 10, 32]. This shift towards Transformer-based architectures has contributed to improved scalability and facilitated more efficient parameter expansion. In parallel, auto-regressive models [90, 39, 79, 47, 88, 52, 23, 17, 18] have leveraged discrete tokens to effectively capture temporal dependencies, demonstrating particular efficacy in generating extended video sequences [93, 81, 100, 37, 76, 104]. This work investigates controlled application of video diffusion Transformers, with specific focus on synthesizing compositions videos. 4.2 Controllable Video Generation Large-scale, pre-trained text-to-video generation models [15, 14] have been propelled by the integration of high-capacity textual representations [63, 64] and advanced attention-based architectures. These innovations have substantially contributed to achieving fine-grained control in the synthesis of temporally coherent video content [50, 57, 4, 61, 59, 28, 19]. In identity-preserving tasks, tuning-based approaches typically involve adapting the pre-trained backbone to individual subjects at inference time [68, 40, 30, 20, 29]. While effective, these methods often incur considerable computational costs due to the need for per-identity fine-tuning. Although recent image and video generation models based on U-Net and DiT architectures have improved personalization capabilities [69, 48, 85, 83, 87, 9, 35, 16], their reliance on continual model adaptation hinders scalability and 12 limits broader deployment. To mitigate the resource requirements inherent in tuning-based pipelines, the image generation domain has seen the emergence of more lightweight, tuning-free solutions [92, 82, 34, 33, 51]. It is important to note that systems such as Keling[1], Vidu[71], Pika[2], and MovieGen [60] are closed-source. In contrast, we build upon the state-of-the-art video diffusion transformers and publicly release all technique details. Our approach facilitates the generation of coherent, high-fidelity, and editable videos while preserving subject-specific consistency, representing significant advancement in scalable and controllable open video composition generation."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce SkyReels-A2, framework developed for elements-to-video generation that integrates multiple visual reference images. Generally, SkyReels-A2 design the joint text-image model injection method from semantic and spacial perspectives based on existing video foundation models, to effectively learn cross-modal data form integration. The empirical results demonstrate that SkyReels-A2 can deliver composition synthesis that is not only of high quality and diversity but also offers robust editability and strong identity fidelity. We anticipate that our method will establish new benchmark in the control video field, providing replicable framework that can be adopted, extended, and optimized by future research efforts."
        },
        {
            "title": "Acknowledges",
            "content": "We would like to express our gratitude to Hao Zhang, Dixuan Lin, Zhiheng Xu for their help for constructing text-video dataset and infra, and Xin Sui, Binlu Zhang for model performance evaluation and insight feedback."
        },
        {
            "title": "References",
            "content": "[1] Keling. image to video elements features. https://klingai.com/image-to-video/ multi-id/new/. 2024. [2] Pikascenes. https://pika.art/ingredients/. 2024. [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, pages 2266922679, 2023. [4] Yuxiang Bao, Di Qiu, Guoliang Kang, Baochang Zhang, Bo Jin, Kaiye Wang, and Pengfei Yan. Latentwarp: Consistent diffusion latents for zero-shot video-to-video translation. arXiv preprint arXiv:2311.00353, 2023. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [8] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. In openai, 2024. [9] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. arXiv preprint arXiv:2407.08674, 2024. 13 [10] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. [11] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019. [12] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In CVPR, 2020. [13] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [16] Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, and He Tang. Motioncharacter: Identitypreserving and motion controllable human video generation. arXiv preprint arXiv:2411.18281, 2024. [17] Zheng-cong Fei. Fast image caption generation with position alignment. arXiv preprint arXiv:1912.06365, 2019. [18] Zhengcong Fei. Partially non-autoregressive image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 13091316, 2021. [19] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa: Joint-embedding predictive architecture can listen. arXiv preprint arXiv:2311.15830, 2023. [20] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. Gradient-free textual inversion. In Proceedings of the 31st ACM International Conference on Multimedia, pages 13641373, 2023. [21] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Flux that plays music. arXiv preprint arXiv:2409.00587, 2024. [22] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [23] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [24] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformer-mamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. [26] Zhengcong Fei, Mingyuan Fan, Li Zhu, Junshi Huang, Xiaoming Wei, and Xiaolin Wei. Masked auto-encoders meet generative adversarial networks and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2444924459, 2023. [27] Zhengcong Fei, Debang Li, Di Qiu, Changqian Yu, and Mingyuan Fan. Ingredients: Blending custom photos with video diffusion transformers. arXiv preprint arXiv:2501.01790, 2025. [28] Zhengcong Fei, Di Qiu, Changqian Yu, Debang Li, Mingyuan Fan, and Xiang Wen. Video diffusion transformers are in-context learners. arXiv preprint arXiv:2412.10783, 2024. [29] Zhengcong Fei, Xu Yan, Shuhui Wang, and Qi Tian. Deecap: Dynamic early exiting for efficient image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1221612226, 2022. [30] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [31] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [32] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. [33] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. [34] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identitypreserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [35] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. [36] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. [37] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. [39] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [40] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [41] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [42] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [43] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [44] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below. [45] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024. [46] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. In European Conference on Computer Vision, pages 206228. Springer, 2024. [47] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 15 [48] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. [49] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, April 2024. [50] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback. In European Conference on Computer Vision, pages 129147. Springer, 2025. [51] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In CVPR, pages 86408650, 2024. [52] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [53] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [54] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling. arXiv preprint arXiv:2305.13311, 2023. [55] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [56] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. [57] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. [58] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [59] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [60] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [61] Di Qiu, Zheng Chen, Rui Wang, Mingyuan Fan, Changqian Yu, Junshi Huan, and Xiang Wen. Moviecharacter: tuning-free framework for controllable character video synthesis. arXiv preprint arXiv:2410.20974, 2024. [62] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. [63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [64] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2019. [65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [66] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 16 [67] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [68] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. [69] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In CVPR, pages 65276536, 2024. [70] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 28302839, 2017. [71] shengshu ai. Vidu. In shengshu-ai, 2024. [72] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate drag-based image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024. [73] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [74] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 22562265. PMLR, 2015. [75] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv, Oct 2020. [76] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. [77] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generationreconstruction cycle. arXiv preprint arXiv:2407.19548, 2024. [78] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. [79] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. [80] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. Advances in neural information processing systems, 29, 2016. [81] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. [82] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [83] Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962, 2024. [84] WanTeam. Wan: Open and advanced large-scale video generative models. 2025. [85] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, pages 65376549, 2024. [86] Qi Tian Weijie Kong and so on. Hunyuanvideo: systematic framework for large video generative models, 2024. 17 [87] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758, 2024. [88] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One arXiv preprint single transformer to unify multimodal understanding and generation. arXiv:2408.12528, 2024. [89] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. [90] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. [91] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [92] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [93] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. [94] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. [95] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224, 2024. [96] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [97] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [98] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. [99] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. [100] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. [101] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. [102] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. [103] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. [104] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. 18 Table 3: Dimensions and Criteria for User Preference Studies. Evaluation Dimension Evaluation Criteria and Details Face Consistency 1: Completely unable to identify as the same individual (facial features are unrelated to the reference image). 3: Noticeable differences are present, but identification is still possible (e.g., variations in eye size or alterations in nose shape). 5: All facial features are fully consistent (e.g., no differences in eye shape, nose curvature, or lip thickness). Dress Consistency Background Consistency 1: The clothing is completely unrelated to the reference image (different style, color, and pattern). For example, the reference wears traditional Hanfu, while the generated image shows swimwear. 2: Significant style differences exist, with partial matches in color or pattern. For instance, the reference wears suit, and the generated image shows athletic wear, both in dark blue. 3: The styles are similar, but key elements have changed (e.g., long sleeves to short sleeves). For example, the reference wears blue jeans, while the generated image features black casual pants with similar cut. 4: The styles are consistent, with minor acceptable differences (e.g., color brightness or slight pattern scaling). For example, the reference wears white dress, and the generated image is in off-white, with matching hem lengths. 5: The clothing style, color, and pattern are identical (e.g., no differences in button placement or pattern details). For instance, the reference wears red plaid shirt, and the generated image matches the plaid pattern perfectly. 1: The background is completely unrelated to the reference image (different scene, lighting, and objects). For example, the reference is in an office, while the generated image is spacecraft. 2: There are significant scene type differences, with only partial matches in elements (e.g., color or material). For instance, the reference is at the beach, while the generated image is forest, both with green vegetation. 3: The scene types are similar, but key elements have changed (e.g., indoor to outdoor, day to night). For example, the reference is in kitchen, while the generated image shows living room, with consistent color scheme. 4: The scene types are consistent, with minor acceptable differences (e.g., lighting or object placement). For instance, the reference is in sunny park, while the generated image is cloudy, but vegetation and bench positions match. 5: The scene elements are identical (e.g., no differences in furniture layout or decorations). For example, the reference is in front of library bookshelf, and the generated image matches the shelf arrangement exactly. Object Consistency 1: Completely unrecognizable as the same object (features unrelated to the reference image). 3: Noticeable differences but still recognizable (e.g., changes in size or shape). 5: All object features are completely consistent (e.g., size, shape, and texture show no differences). Instruction Following Rationality Subject Coherence Motion Smoothness Aesthetic Quality Video Quality 1: Fails to follow the instructions or only minimally aligns; significant deviation in theme and key elements. 2: Partially follows the instructions, but with notable deviations; key elements are present but lack completeness and accuracy. 3: Generally adheres to the instructions; main content and key elements are aligned, presenting the instruction adequately, though with minor details missing or inaccurate. 4: Highly adheres to the instructions; accurately presents key content and elements without deviation. 5: Perfectly follows the instructions and extends beyond; the generated content adheres fully to the theme, elements, and style, with enhancements that improve the overall effectiveness. 1: Spatial relationships are entirely irrational; positions and interactions between subjects and backgrounds or multiple subjects are contradictory, lacking any logical coherence. 2: Many spatial relationships among objects are flawed; interactions between subjects and backgrounds or among multiple subjects show significant unnatural positioning and occlusion, severely impacting the videos realism and credibility. 3: Some spatial relationships are unreasonable; occasional issues with positioning, interactions, or unnatural occlusion arise, but they do not significantly affect the overall presentation. 4: Only few spatial relationships show slight deviations but are generally logical; interactions among multiple subjects are reasonably coherent, and the overall output aligns closely with the instructions. 5: All positional, occlusion, and interaction relationships among objects are highly rational and natural, with meticulous detail handling, resulting in an almost perfect overall presentation. 1: The main subject is completely distorted, or there are significant differences between frames. 2: Multiple areas or large sections show distortion. 3: Minor distortions in details, such as slight finger twisting when holding objects. 4: No distortion or issues with the character. 5: No distortion at all, with additional strengths like natural facial expressions, resulting in excellent overall quality. 1: Movement is very jerky; severe stuttering and noticeable jumps create poor viewing experience with no coherent motion. 2: Clear stuttering and disjointed movement; transitions between actions are abrupt, making most scenes feel unnatural and hindering content understanding. 3: Movement smoothness is average; occasional stutters or disconnections occur, but overall can still follow the motion rhythm, minimally impacting viewing of the main content. 4: Movement is relatively smooth and natural with no significant stuttering; the motion feels fluid, and the trajectory is reasonable, providing good overall experience. 5: Movement is extremely smooth and fluid, resembling natural motion with no stuttering or disconnections, almost indistinguishable from real human movement. 1: Extremely poor quality; severe distortion of the main subject or complete failure to meet prompt requirements, rendering it unusable. 2: Overall quality is low; follows more than half of the instructions but presents incomplete content (process without results or vice versa) and lacks basic aesthetics. 3: Moderate quality; conveys the prompts information reasonably well with some detail flaws that dont impact the main storyline, exhibiting some aesthetic value but not immediately usable. 4: Good overall performance; fully meets instructions and presents the prompt information clearly without distortion or missing elements, suitable for low-cost production. 5: Perfect overall performance. 1: Severe quality issues; the image is blurry with noticeable pixelation or other major visual flaws, making it nearly unwatchable. 2: Poor image quality; blurriness is evident, and while the content can be vaguely recognized, it significantly affects the viewing experience. 3: Average quality; minor flaws like slight blurriness or noise are present but do not hinder understanding of the main content. 4: Acceptable quality for viewing; clear images with no significant flaws, providing good visual effects on various devices. 5: Exceptional quality; professional-level image clarity without flaws in resolution, color, contrast, or detail, suitable for high-quality display and dissemination."
        }
    ],
    "affiliations": [
        "Skywork AI, Kunlun Inc."
    ]
}