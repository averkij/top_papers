{
    "paper_title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation",
    "authors": [
        "Guanghao Li",
        "Kerui Ren",
        "Linning Xu",
        "Zhewen Zheng",
        "Changjian Jiang",
        "Xin Gao",
        "Bo Dai",
        "Jian Pu",
        "Mulin Yu",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 5 5 8 0 . 0 1 5 2 : r ARTDECO: TOWARDS EFFICIENT AND HIGHFIDELITY ON-THE-FLY 3D RECONSTRUCTION WITH STRUCTURED SCENE REPRESENTATION Guanghao Li1,2,3 Kerui Ren1,4 Linning Xu1,5 Zhewen Zheng1,6 Changjian Jiang1,7 Xin Gao1,2 Bo Dai8 1Shanghai Artificial Intelligence Laboratory, 2Fudan University, 3Shanghai Innovation Institute, 4Shanghai Jiao Tong University, 5The Chinese University of Hong Kong, 6Carnegie Mellon University, 7Zhejiang University, 8The University of Hong Kong Jian Pu2 Mulin Yu1 Jiangmiao Pang1 Figure 1: ARTDECO delivers high-fidelity, interactive 3D reconstruction from monocular images, combining efficiency with robustness across indoor and outdoor scenes."
        },
        {
            "title": "ABSTRACT",
            "content": "On-the-fly 3D reconstruction from monocular image sequences is long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design hierarchical Gaussian representation with LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/. Equal contribution Corresponding author."
        },
        {
            "title": "INTRODUCTION",
            "content": "High-fidelity 3D reconstruction from monocular image sequences is long-standing goal in computer vision. Monocular data are relatively inexpensive, ubiquitous, and easy to capture, while accurate 3D scene representations are crucial for downstream applications such as embodied intelligence, AR/VR, or real-to-sim content creation. Recently, 3D Gaussian Splatting (Kerbl et al., 2023a) has emerged as an efficient scene representation with strong empirical results. However, in monocular settings the lack of reliable geometric cues such as ambiguous scale, limited parallax, motion blur, and poor overlap makes it difficult to achieve accuracy, speed, and robustness at the same time. As consequence, many existing systems optimize one objective at the expense of the others, limiting their practicality for online deployment. Current 3DGS-based 3D reconstruction methods fall into two paradigms. Per-scene optimization methods (Matsuki et al., 2024; Huang et al., 2024b; Meuleman et al., 2025b) rely on image poses estimated from Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM), achieving high accuracy but at the cost of substantial computation, with robustness limited by the fragility of these pipelines. In contrast, recent feed-forward models (Tang et al., 2024; Ye et al., 2024; Jiang et al., 2025b; Zhang et al., 2025) leverage large-scale data to learn monocular priors and directly regress poses and Gaussian primitives with attention, enabling fast and robust inference across diverse scenes but with limited rendering fidelity and weak consistency. This tradeoff highlights the need for approaches that combine the efficiency of feed-forward models with the strengths of per-scene optimization methods to deliver accurate, robust, real-time reconstruction. Beyond the efficiencyaccuracy tradeoff, 3DGS pipelines are also highly sensitive to scene scale. As the scene grows, the number of Gaussian primitives required for training and rendering increases rapidly, which reduces efficiency. Prior attempts to address this either apply post-hoc anchor-based pruning, which lowers computation but introduces boundary artifacts and increases memory cost, or add multi-scale Gaussians during training, which mitigates artifacts but lacks explicit structural organization. These limitations underscore the need for principled and practical level-of-detail (LoD) mechanism in 3DGS. ARTDECO1 derives its name from streamlined pipeline that unifies Accurate localization, Robust reconstruction, and Decoder-based rendering, with the aim of enabling on-the-fly 3D scene reconstruction and rendering. The core of ARTDECO is the goal of balancing real-time performance, accuracy, and robustness. It employs feed-forward models as data priors to reduce monocular ambiguities while maintaining the efficiency required for interactive use. To address the global inconsistency often seen in feed-forward approaches, ARTDECO integrates loop detection with lightweight bundle adjustment. Finally, hierarchical semi-implicit Gaussian structure with LoD-aware densification provides level-of-detail control, helping the system scale without excessive loss of fidelity or efficiency. Together, these components support practical real-time 3D reconstruction across diverse indoor and outdoor settings. Our main contributions can be summarized as follows: We present ARTDECO, an integrated system that unifies localization, reconstruction, and rendering into single pipeline, designed to operate robustly across various environments. Notably, we incorporate feed-forward foundation models as modular components for pose estimation, loop closure detection, and dense point prediction. This integration improves localization accuracy and mapping stability while preserving efficiency. We further propose hierarchical semi-implicit Gaussian representation with LoD-aware densification strategy, enabling principled trade-off between reconstruction fidelity and rendering efficiency, critical for large-scale, navigable environments. Extensive indoor and outdoor experiments show that ARTDECO achieves SLAM-level efficiency, feed-forward robustness, and near per-scene optimization quality, validating its effectiveness for practical on-the-fly 3D digitization. 1Beyond the acronym, the name also evokes the Art Deco movement, valued for structure, geometry, and clarity of form. This metaphor reflects our systems emphasis on structured scene representations."
        },
        {
            "title": "2.1 MULTI-VIEW RECONSTRUCTION AND RENDERING",
            "content": "Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) have attracted significant attention in novel view synthesis (NVS). NeRF and its variants (Barron et al., 2021; 2022; 2023; Zhang et al., 2020; Verbin et al., 2022) model continuous volumetric fields and achieve high-quality image synthesis. However, the reliance on expensive volume rendering and large networks results in long training times and hinders real-time applications. To address these limitations, several works (Muller et al., 2022; Xu et al., 2022; Sun et al., 2022) accelerate both training and rendering by introducing hybrid or explicit scene representations. Recently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023b) has shown remarkable progress in high-fidelity reconstruction and real-time rendering by representing scenes with anisotropic Gaussians and leveraging an efficient tile-based rasterizer. Following its introduction, research has largely focused on model compression (Fan et al., 2024; Chen et al., 2025), large-scale scene processing (Ren et al., 2024; Jiang et al., 2025c; Kerbl et al., 2024), and geometry reconstruction (Huang et al., 2024a; Yu et al., 2024c; Guedon et al., 2025; Yu et al., 2024a). Despite these advances in novel view synthesis (NVS), most methods assume access to accurate camera poses, typically estimated via Structure-from-Motion (SfM) (Schonberger & Frahm, 2016; Schonberger et al., 2016; Pan et al., 2024), which imposes considerable preprocessing costs for large-scale or in-the-wild captures. To alleviate this reliance, several works (Lin et al., 2021; Fu et al., 2024; Lin et al., 2025b) propose joint optimization of camera poses and scene parameters. However, these approaches remain computationally intensive, are sensitive to wide-baseline settings, or still depend on costly post-refinement. 2.2 STREAMING PER-SCENE RECONSTRUCTION Classical visual SLAM systems (Mur-Artal & Tardos, 2017; Engel et al., 2017; Teed et al., 2023; Li et al., 2025a) provide online tracking, mapping, and loop closure, but they fall short in producing high-fidelity maps. To overcome this limitation, recent works integrate volumetric rendering techniques into SLAM to enable online NVS. Among these works, NeRF-based SLAM methods (Sucar et al., 2021; Zhu et al., 2022; Zhang et al., 2023; Li et al., 2026) exhibit photorealistic reconstruction but remain computationally expensive due to per-ray volumetric rendering. By contrast, 3DGS has gained traction for SLAM integration thanks to its explicit representation and efficient rendering. Utilizing the differential pipeline of 3DGS, some studies (Matsuki et al., 2024; Hu et al., 2024; Keetha et al., 2024; Yan et al., 2024; Deng et al., 2024; Yu et al., 2025; Li et al., 2025b; Cheng et al., 2025; Lin et al., 2025a) directly propagate the gradient from the rendering loss to pose, while others (Yugay et al., 2024; Huang et al., 2024b; Peng et al., 2024b; Ha et al., 2024; Peng et al., 2024a; Tianci Wen, 2025; Sandstrom et al., 2025) leverage traditional SLAM Modules to provide accurate pose. However, in monocular setting these systems often struggle to balance robustness, accuracy, and runtime efficiency. Recently, On-the-fly NVS (Meuleman et al., 2025b) has shown that GPU-friendly mini-bundle adjustment with incremental 3DGS updates can enable interactive reconstruction, though robustness on casual unposed videos remains limited. 2.3 FEED-FORWARD MODELS Pretrained on large-scale datasets, recent feed-forward models (Wang et al., 2025a; 2024) reconstruct 3D scenes directly, avoiding per-scene optimization. These approaches can be divided into pose-aware and pose-free methods. Pose-aware models take images together with camera poses as input, enabling rapid reconstruction (Charatan et al., 2024; Chen et al., 2024; Zhang et al., 2024; Jiang et al., 2025a). Pose-free models, in contrast, perform fully end-to-end reconstruction from raw images alone, typically representing scenes with either point maps (Wang et al., 2024; Leroy et al., 2024b; Wang et al., 2025a;b; Murai et al., 2024) or 3DGS (Jiang et al., 2025b). Notably, these feed-forward methods offer robustness across diverse scenarios, remove the need for preprocessing, and allow fast inference suitable for interactive use. However, they generally achieve lower accuracy than per-scene optimized methods, and face challenges with maintaining global consistency, handling high-resolution inputs, and processing long sequences. Figure 2: Frontend and backend modules. (a) Frontend: Images are captured from the scene and streamed into the front-end part. Each incoming frame is aligned with the latest keyframe using matching module to compute pixel correspondences. Based on the correspondence ratio and pixel displacement, the frame is classified as keyframe, mapper frame, or common frame. The selected frame, along with its pose and point cloud, is then passed to the back-end. (b) Backend: For each new keyframe, loop-detection module evaluates its similarity with previous keyframes. If loop is detected, the most relevant candidates are refined and connected in the factor graph; otherwise, the keyframe is linked only to recent frames. Finally, global pose optimization is performed with GaussNewton, and other frames are adjusted accordingly. We instantiate the matching module with MASt3R (Leroy et al., 2024b) and the loop-detection module with π3 (Wang et al., 2025b)."
        },
        {
            "title": "3 METHOD",
            "content": "We aim to recover high-fidelity static 3D scene together with the corresponding camera poses from monocular image sequence. Given sequence of monocular RGB frames {Ii}N i=1, with or without known camera intrinsics R33, we estimate the camera poses {Ri ti}N i=1 associated with each image, as well as set of Gaussian primitives {Gj}M j=1 that compactly represent the 3D scene. By default, we assume the scene is static and rigid, and that all geometric information is inferred purely from monocular cues without external sensors. As illustrated in Fig. 2 and 3, ARTDECO processes the sequence in streaming SLAM-style pipeline consisting of three modules: frontend, backend, and mapping. (1) The frontend estimates (2) The relative poses and categorizes frames into common, mapping, or keyframes (Sec. 3.1). backend refines keyframe poses through loop closure and global bundle adjustment (Sec. 3.2). (3) Finally, image-wise pointmaps initialize 3D Gaussians, which are incrementally optimized in the mapping module (Sec. 3.3). 3.1 FRONTEND MODULE For each input frame, the frontend estimates its pose relative to the latest keyframe and categorizes it as common, mapping, or keyframe. We assume pinhole camera with fixed intrinsics and shared optical center. If the focal length is unknown, it is initialized from the first kf GeoCalib (Veicht et al., 2024) estimates and jointly refined during pose estimation. Pose Estimation. MASt3R (Leroy et al., 2024a) serves as our matching module, two-view reconstruction and matching prior, to improve camera tracking and focal length estimation. Following MASt3R-SLAM (Murai et al., 2024), we obtain frame-wise pointmaps, their confidence scores, and pixel correspondences between the current frame and the latest keyframe. The 3D points from the current frame are projected into the keyframe image plane, and the relative pose TKC SIM(3) is estimated by minimizing reprojection residuals with GaussNewton solver. Since MASt3R predictions are less stable near object boundaries, we weight residuals by per-point uncertainty. For each point xc in the current frame, we estimate local covariance Σc R33 from neighbors within radius δ. We then project Σc to the current keyframes measurement space, which is used to filter out unreliable re-projection residuals. Besides, if the focal length is not provided, it is jointly optimized along with the relative pose. Further derivations are given in A.2. Keyframe Selection. After pose estimation, each frame is categorized as common frame, mapper frame, or keyframe. keyframe is created when the number of valid correspondences with the latest keyframe falls below threshold τk, following standard SLAM practice. Keyframes are passed to the backend for pose refinement and to the mapping module for reconstruction. mapper frame is selected when the frame provides sufficient parallax for reliable multi-view reconstruction. We compute the pixel displacement between the current frame and the latest keyframe; if the 70th percentile exceeds τm, the frame is promoted to mapper frame. Mapper frames are first processed by the backend to compute pointmap confidence and are then used in the mapping module to initialize new 3D Gaussians. common frame does not meet either the keyframe or mapper criteria and is therefore used only to refine existing scene details, without introducing new structure; its role will be further elaborated in later sections."
        },
        {
            "title": "3.2 BACKEND",
            "content": "The backend processes keyframes from the frontend to maintain globally consistent scene and camera trajectory. For each incoming keyframe, it evaluates correlations with earlier ones, builds factor graph over the most relevant candidates, and performs global optimization to enforce multiview consistency. In addition, it estimates the confidence of keyframe and mapper pointmaps, which are later used to initialize 3D Gaussian in the mapping module. Loop Closure and Global Bundle Adjustment. Given new keyframe, the backend first updates the factor graph by connecting it to related frames, and then performs PnP-based global bundle adjustment (BA) to refine poses, as illustrated in Fig. 2.(b). If loop closure is detected, the current keyframe is linked to its three most relevant predecessors; otherwise, it is connected only to the latest keyframe. Loop detection is initially performed using the Aggregated Selective Match Kernel (ASMK). loop is declared when previous keyframe has an ASMK score above threshold τloop and is at least klopp keyframes apart. To increase robustness against weak correspondences and noisy inputs, we further leverage the 3D foundation model π3 (Wang et al., 2025b) as our loop-detection module. Specifically, the current frame and the top Na candidates from ASMK are processed by π3 to produce pointmaps, from which we select the three most geometrically consistent keyframes based on angular error following (Murai et al., 2024). These are then connected to the factor graph, yielding more reliable loop closures and reducing drift. More details can be found in A.5. Pointmap Confidence. We estimate pointmap confidence using reprojection error rather than relying on the confidence values predicted by MASt3R. When mapper frame or keyframe is processed, its pointmap is projected onto the Nc previous keyframes with the highest ASMK scores. For each point, we compute the reprojection errors across the Nc keyframes, average them to obtain e, and define the confidence score as = 1 if εc, and = eεc+1 otherwise, where εc is predefined threshold. This reprojection-based confidence provides more reliable measure of geometric consistency across frames."
        },
        {
            "title": "3.3 MAPPING MODULE",
            "content": "The mapping module reconstructs the 3D Gaussian scene from incoming frames, their estimated poses, and pointmaps, as illustrated in Fig. 3. Unlike prior 3DGS-based SLAM methods that rely only on keyframes, we leverage all frames to maximize the use of captured information: keyframes and mapper frames introduce new Gaussians, while common frames refine existing ones. This design enriches both visual and geometric details, which are critical not only for accurate reconstruction but also for high-fidelity rendering. Moreover, we introduce hierarchical Gaussian structure with LoD-aware control. LoD is essential for scalable scene modeling, particularly in large-scale, navigable spaces where SLAM-based applications require consistent detail at varying viewing distances. By combining dense supervision from all frame types with principled level-of-detail management, our mapping module improves fidelity of both reconstructed geometry and rendered views, while maintaining computational efficiency. Probabilistic Selection for 3D Gaussian Insertion. When mapper frame or keyframe arrives from the backend, we determine where to initialize new 3D Gaussians. To avoid redundancy, Gaussians are inserted only in regions that require refinement, rather than at every pixel, guided by imagelevel priors inspired by (Meuleman et al., 2025b). We prioritize high-frequency regions and poorly reconstructed areas by computing an insertion probability at each pixel (u, v) using the Laplacian of 5 Figure 3: Mapping process. When keyframe or mapper frame arrives from the backend, new Gaussians are added to the scene. Multi-resolution inputs are analyzed with the Laplacian of Gaussian (LoG) operator to identify regions that require refinement, and new Gaussians are initialized at the corresponding monocular depth positions in the current view. Common frames are not used to add Gaussians but contribute through gradient-based refinement. Each primitive stores position, spherical harmonics (SH), base scale, opacity, local feature, dmax, and voxel index vid. For rendering, the dmax attribute determines whether Gaussian is included at given viewing distance, enabling consistent level-of-detail control. Gaussian (LoG) operator (Haralock & Shapiro, 1991): Pa(u, v) = max (cid:16) min(2(Gσ) I(u, v), 1) min(2(Gσ) I(u, v), 1), 0 (cid:17) , (1) where and are the ground-truth and rendered images, and Gσ is Gaussian kernel with standard deviation σ. new Gaussian is added when Pa(u, v) exceeds the threshold τa. Gaussian Primitive Initialization. After identifying candidate pixels, we initialize the corresponding 3D Gaussians. Each Gaussian is parameterized by its center µ, spherical harmonics (SH), opacity α, base scale Sb, individual feature fl, and voxel index vid. The µ and SH0 are initialized from the pointmap and pixel color, while opacity is set to 0.2 C(u,v) to down-weight low-confidence regions, where C(u,v) is the confidence score calculated in backend. Following (Meuleman et al., 2025b; Wu et al., 2025; Yu et al., 2024b), the base scale at pixel (u, v) is defined as: Sb = dis , = 1 2(cid:112)min((2Gσ) I(u, v), 1) , (2) where di is the distance from the Gaussian center to the camera and is the focal length. Here, represents an image-space scale, i.e., the expected distance to the nearest neighbor under local 2D Poisson process of intensity min((2Gσ) I(u, v), 1), (Clark & Evans, 1954). To ensure smoother reconstruction, we further refine scale and initialize rotation with two MLPs: = Sb MLPs(fr fl), = MLPr(fr fl), (3) where denotes concatenation, fl is an individual feature initialized as zero, and fr is region feature encoding local voxel context. We voxelize the 3D space with cell size ϵ; when new Gaussian is added to voxel, the corresponding voxel-wise feature is initialized as zero and indexed by vid. This hybrid regionindividual design promotes global consistency of the Gaussian field while preserving local distinctiveness. Levels of Detail Design. To support smooth navigation in large 3D scenes, we organize Gaussians into multiple levels of detail (LoD). Each Gaussian is assigned level N+ with < L, where level 0 denotes the finest resolution and level 1 the coarsest. At initialization, Gaussian at level corresponds to patch of 22l pixels in the original image (e.g., level 0 corresponds to one pixel). We progressively downsample the input frame L1 times and initialize Gaussians from both the downsampled and original images. All Gaussian parameters follow the initialization described earlier, except that (i) the base scale is weighted by 1.42l, and (ii) each Gaussian is assigned distance-dependent parameter dmax = 22l, where is the distance from the Gaussian center to the camera. During rendering, Gaussian is included if dr dmax, excluded if dr > 2dmax, and smoothly faded out for dmax < dr 2dmax by interpolating its opacity as α = (d dmax)/dmax. This distance-aware LoD design suppresses flickering and maintains stable rendering quality across scales while preserving efficiency. 6 Training Strategy. To balance efficiency and reconstruction quality, we adopt staged training scheme. For streaming input, new Gaussians are initialized and the scene is optimized for iterations whenever mapper frame or keyframe arrives, while common frames trigger only K/2 iterations without inserting new Gaussians. Following (Meuleman et al., 2025b; Wu et al., 2025), training frames are sampled with 0.2 probability from the current frame and 0.8 from past frames to mitigate local overfitting. After processing the sequence in streaming manner, we run global optimization over all frames, giving higher sampling probabilities to those with fewer historical updates. Finally, camera poses are optimized jointly with Gaussian parameters, with gradients on positions and rotations propagated to poses, consistent with common practice in on-the-fly reconstruction."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Datasets and Metrics. We evaluate on diverse indoor and outdoor benchmarks. Indoor datasets include 11 TUM scenes (Sturm et al., 2012), 14 scenes from ScanNet++ (Yeshwanth et al., 2023), 8 scenes from VR-NeRF (Xu et al., 2023), and 6 scenes from ScanNet (Dai et al., 2017), with sequence lengths ranging from 325577 image frames. Outdoor datasets include 8 KITTI scenes (Geiger et al., 2013), 9 Waymo scenes (Sun et al., 2020) (both following S3PO-SLAM (Cheng et al., 2025)), 5 scenes from Fast-livo2 (Zheng et al., 2024), and 1 scene from MatrixCity (Li et al., 2023), with lengths 2001363 frames per trajectory. Reconstruction is evaluated with PSNR, SSIM (Wang et al., 2004), and LPIPS (Zhang et al., 2018); pose accuracy with Absolute Trajectory Error (ATE) RMSE; and system efficiency with FPS. Baselines. We evaluate against two categories of state-of-the-art methods. For reconstruction quality, we consider 3D Gaussian Splatting approaches, including OnTheFly-NVS (Meuleman et al., 2025a)), LongSplat (Lin et al., 2025a), S3PO-GS (Cheng et al., 2025), SEGS-SLAM (Tianci Wen, 2025), MonoGS (Matsuki et al., 2024)). For pose estimation, in addition to the aforementioned 3DGS-based SLAM methods, we benchmark against several state-of-the-art SLAM systems, including MASt3R-SLAM (Murai et al., 2024), DPV-SLAM (Lipson et al., 2024), DROID-SLAM (Teed & Deng, 2021), and Go-SLAM (Zhang et al., 2023). Implementation Details. Experiments are run on desktop with an Intel Core i9-14900K CPU and NVIDIA RTX 4090 GPU. Following standard Novel View Synthesis practices, every 8th frame is held out for evaluation: these frames are excluded from mapping but their poses are estimated and optimized for evaluation. In the frontend, we set τk = max(0.333 W, 30), where is the image width. In the backend, we use Na = min(23, Nc) candidate keyframes, where Nc is the number of available candidates. If Na < 11, loop-detection modules are disabled, and the top three ASMKscoring keyframes are directly selected to connect in the factor graph. For pointmap confidence, we fix εc = 3. During mapping, 3D Gaussians are organized into 4 LOD levels by setting = 4."
        },
        {
            "title": "4.2 COMPARISON",
            "content": "Reconstruction Results Analysis. Tab. 1 reports reconstruction results on eight indoor and outdoor benchmarks. Our system achieves state-of-the-art quality, particularly on challenging datasets such as TUM and ScanNet with structural complexity, motion blur, and noise. On higher-quality datasets like VR-NeRF and ScanNet++, where scenes feature diverse multi-scale visuals, all methods improve, yet ARTDECO still delivers the best performance. Outdoor evaluation covers large-scale free-motion captures (Fast-LIVO2) and forward-facing driving datasets (Waymo, KITTI, MatrixCity). ARTDECO consistently outperforms baselines, demonstrating robustness to scale variation. Qualitative comparisons  (Fig. 4)  show that ARTDECO, enabled by its multi-level Gaussian primitive design, captures fine details, large-scale structures, and high-fidelity geometry within compact representation. Additional results are provided in Tabs. 4 - 24, Fig.5 in A.6. Tracking Results Analysis. Tab. 2 summarizes the tracking performance on indoor and outdoor benchmarks. With loop closure and covariance-matrix filtering, ARTDECO achieves markedly higher localization accuracy than other 3DGS-based systems on challenging multi-scale indoor datasets (TUM, ScanNet++). On outdoor datasets such as Waymo, it also delivers competitive performance. Further results on TUM (Second part of Tab. 2) demonstrate that ARTDECO consistently outperforms state-of-the-art non-3DGS SLAM methods, confirming its superior localization 7 Figure 4: Qualitative comparisons against popular on-the-fly reconstruction baselines across diverse 3D scene datasets. ARTDECO consistently preserves high-quality rendering details in complex and diverse environments, particularly in the regions highlighted with colored rectangles. Table 1: Rendering comparisons against baselines across indoor and outdoor datasets. We report visual quality metrics, average running time. Indoor-dataset Method MonoGS S3PO-GS SEGS-SLAM ScanNet++ ScanNet TUM VR-NeRF PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Training Time 16.71 0.682 0.600 18.87 0.780 0.629 17.78 0.602 0.573 13.88 0.560 0.420 14.08 min 20.14 0.797 0.558 22.94 0.820 0.355 12.43 0.642 0.497 41.25 min 19.62 0.656 0.466 19.73 0.839 0.365 19.69 0.743 0.307 31.62 0.896 0.232 10.84 min - - - OnTheFly-NVS 18.01 0.761 0.386 LongSplat 2.29 min 24.94 0.827 0.260 19.27 0.754 0.404 25.09 0.804 0.272 25.74 0.832 0.321 442.96 min 27.30 0.872 0.310 19.72 0.719 0.380 15.36 0.708 0.494 Ours 29.12 0.918 0.167 24.10 0.865 0. 26.18 0.850 0.224 28.57 0.895 0.242 5.33 min Outdoor-dataset Method KITTI Training PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Time Fast-LIVO2 MatrixCity Waymo MonoGS S3PO-GS SEGS-SLAM 14.03 0.463 0.488 19.01 0.698 0.502 24.58 0.773 0.307 25.57 0.784 0.366 19.36 0.593 0.736 16.52 min 21.76 0.661 0.584 34.89 min 8.75 min 19.34 0.752 0.627 27.28 0.865 0. 14.56 0.489 0.767 19.97 0.645 0.410 18.87 0.598 0.699 21.51 0.684 0.445 OnTheFly-NVS 16.89 0.579 0.471 16.86 0.532 0.447 LongSplat 25.53 0.820 0.360 25.61 0.795 0.326 18.76 0.618 0.497 26.37 0.792 0.276 21.36 0.687 0.451 - - - 0.74 min 313.60 min Ours 23.17 0.765 0.299 28.75 0.880 0. 29.54 0.894 0.158 25.62 0.790 0.327 6.58 min *: majority of scenes successful; : majority failed; Only compare fully successful methods. capability. Per-scene metrics, additional tracking results and qualitative trajectory comparisons are provided in Tabs. 2529, Figs. 78 in A.6. Runtime Analysis. We compare runtime across 3DGS-based methods on both indoor and outdoor datasets (Tab. 1). ARTDECO runs faster than all except OnTheFly-NVS, with its extra time cost primarily from pose estimation, trade-off justified by the superior pose accuracy in Tab. 2. 8 Table 2: Tracking comparisons. For tracking evaluation, we compare against SLAMand SFMbased 3D reconstruction methods on indoor and outdoor datasets, as well as state-of-the-art SLAM systems on the TUM dataset (Following MASt3R-SLAM, 9 scenes from TUM fr1). Our method consistently achieves lower ATE RMSE. Dataset MonoGS S3PO-GS SEGS-SLAM MASt3R-SLAM OnTheFly-NVS LongSplat Ours ScanNet++ TUM Waymo 1.217 0.244 7.370 0.632 0.117 1.236 0.245 0.073 - 0.025 0.031 - 0.891 - 3. 0.602 - 4.956 0.018 0.025 1.213 Metric ORB-SLAM3 DPV-SLAM++ DROID-SLAM Go-SLAM MASt3R-SLAM Ours ATE RMSE - 0. 0.038 0.035 0.030 0.028 *: majority of scenes successful; : majority failed; Only compare fully successful methods. Table 3: Quantitative results on ablation studies. We separately listed the rendering metrics and ATE RMSE on ScanNet++ dataset for each ablation described in Sec. 4. Front&Backend Full w/ SLAM (MASt3R π3) w/ Loop (π3 vggt) w/o loop w/ dense key frame ATE RMSE 0.018 0.374 0.096 0.057 0. Mapper Full w/o level-of-detail w/o implicit structure w/o global feat w/o mapper frame w/o common frame PSNR SSIM LPIPS 29.12 0.918 0.167 28.13 0.912 0. 28.54 0.914 0.175 27.95 0.910 0.197 26.38 0.898 0.229 27.20 0.904 0.211 4.3 ABLATION STUDY Ablation on Localization. We analyze the impact of backbone choice, loop closure, and frame categorization strategy on localization, as summarized in Tab. 3. We first ablate the feed-forward model used in the frontend and backend by replacing MASt3R (pairwise inference) with π3 (multiimage inference). Although π3 is trained on more diverse data, it lacks metric-scale capability and performs worse under varying viewpoints. In contrast, MASt3R better preserves consistent object proportions, resulting in more accurate pose estimation. Next, we ablate the loop-closure module by disabling it, which leads to significant degradation in localization accuracy. Finally, we ablate the frame categorization strategy. Here, MF denotes mapper frames and KF denotes keyframes. Using both MFs and KFs (track w/ MF&KF) for inference provides additional temporal information, but unexpectedly reduces pose accuracy. This is because 3D foundation models often struggle with small-parallax inputs, producing ghosting and blur that corrupt point clouds and feature correspondences when the input sequence is overly dense. Ablation on Reconstruction. We further ablate the effects of mapper frames, level-of-detail, and structural Gaussians on reconstruction, as shown in Tab. 3. MFs add richer multi-view constraints, while LoD and structural Gaussians yield more compact, regularized representations, together improving reconstruction fidelity and rendering quality."
        },
        {
            "title": "5 LIMITATIONS",
            "content": "While ARTDECO achieves strong reconstruction and localization, it has several limitations. First, it partly depends on feed-forward 3D foundation models for correspondence and geometry, which, despite enabling fast and scalable inference, reduce robustness under noise, blur, or lighting changes, and suffer when inputs fall outside the training distribution. Second, the system assumes consistent illumination and sufficient parallax; violations such as low-texture surfaces, repetitive structures, or near-degenerate trajectories can cause drift or artifacts. These challenges suggest future work on incorporating uncertainty estimation, adaptive model selection, and stronger priors to improve generalization and reliability in real-world settings."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present ARTDECO, unified framework that advances on-the-fly 3D reconstruction from monocular image sequences. Beyond achieving strong results on standard indoor and outdoor benchmarks, ARTDECO demonstrates that feed-forward priors and structured Gaussian representations can be effectively combined within single system to deliver both accuracy and efficiency. We see ARTDECO as step toward practical large-scale deployment of real-to-sim pipelines, with promising applications in AR/VR, robotics, and digital twins."
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 58555864, 2021. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54705479, 2022. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: In Proceedings of the IEEE/CVF International Anti-aliased grid-based neural radiance fields. Conference on Computer Vision, pp. 1969719705, 2023. David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, 2024. Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Fast feedforward 3d gaussian splatting compression. In The Thirteenth International Conference on Learning Representations, 2025. Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, TatJen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, and Hao Wang. Outdoor monocular slam with global scale-consistent 3d gaussian pointmaps. arXiv preprint arXiv:2507.03737, 2025. Philip Clark and Francis Evans. Distance to nearest neighbor as measure of spatial relationships in populations. Ecology, 35(4):445453, 1954. Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias In Proc. Computer Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. Vision and Pattern Recognition (CVPR), IEEE, 2017. Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, and Weidong Chen. Compact 3d gaussian splatting for dense visual slam. arXiv preprint arXiv:2403.11247, 2024. Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611625, 2017. Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. Advances in neural information processing systems, 37:140138140158, 2024. Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2079620805, 2024. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. 10 Antoine Guedon, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, and Maks Ovsjanikov. Milo: Mesh-in-the-loop gaussian splatting for detailed and efficient surface reconstruction. ACM Transactions on Graphics, 2025. URL https://anttwo.github.io/ milo/. Seongbo Ha, Jiung Yeon, and Hyeonwoo Yu. Rgbd gs-icp slam."
        },
        {
            "title": "In European Conference on",
            "content": "Computer Vision, 2024. Robert Haralock and Linda Shapiro. Computer and robot vision. Addison-Wesley Longman Publishing Co., Inc., 1991. Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Cg-slam: Efficient dense rgb-d slam in consistent uncertainty-aware 3d gaussian field. In European Conference on Computer Vision, 2024. Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024a. doi: 10.1145/3641519.3657428. Huajian Huang, Longwei Li, Hui Cheng, and Sai-Kit Yeung. Photo-slam: Real-time simultaneous localization and photorealistic mapping for monocular, stereo, and rgb-d cameras. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024b. Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, and Georgios Pavlakos. Rayzer: self-supervised large view synthesis model. 2025a. Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views. arXiv preprint arXiv:2505.23716, 2025b. Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, and Bo Dai. Horizon-gs: Unified 3d gaussian splatting for large-scale aerial-to-ground scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2678926799, June 2025c. Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023a. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023b. Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics, 43(4), July 2024. URL https://repo-sam. inria.fr/fungraph/hierarchical-3d-gaussians/. Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding Image Matching in 3D with MASt3R, 2024a. Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024b. Guanghao Li, Yu Cao, Qi Chen, Xin Gao, Yifan Yang, and Jian Pu. Papl-slam: Principal axisanchored monocular point-line slam. IEEE Robotics and Automation Letters, 2025a. Guanghao Li, Qi Chen, Sijia Hu, Yuxiang Yan, and Jian Pu. Constrained gaussian splatting via implicit tsdf hash grid for dense rgb-d slam. IEEE Transactions on Artificial Intelligence, 2025b. 11 Guanghao Li, Qi Chen, Yuxiang Yan, and Jian Pu. Ec-slam: Effectively constrained neural rgb-d slam with tsdf hash encoding and joint optimization. Pattern Recognition, 170:112034, 2026. Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 32053215, 2023. Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 57415751, 2021. Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu. Longsplat: Robust unposed 3d gaussian splatting for casual long videos. arXiv preprint arXiv:2508.14041, 2025a. Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu. LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos, August 2025b. Lahav Lipson, Zachary Teed, and Jia Deng. Deep patch visual slam. In European Conference on Computer Vision, pp. 424440. Springer, 2024. Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, and George Drettakis. On-thefly reconstruction for large-scale novel view synthesis from unposed images. ACM Transactions on Graphics (TOG), 44(4):114, 2025a. Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, and George Drettakis. Onthe-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images. ACM Transactions on Graphics, 44(4), 2025b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. Raul Mur-Artal and Juan D. Tardos. ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics, 33(5):12551262, 2017. doi: 10. 1109/TRO.2017.2705103. Riku Murai, Eric Dexheimer, and Andrew J. Davison. MASt3R-SLAM: Real-time dense SLAM with 3D reconstruction priors. arXiv preprint, 2024. Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Schonberger. Global structure-frommotion revisited. In European Conference on Computer Vision, pp. 5877. Springer, 2024. Chensheng Peng, Chenfeng Xu, Yue Wang, Mingyu Ding, Heng Yang, Masayoshi Tomizuka, Kurt Keutzer, Marco Pavone, and Wei Zhan. Q-slam: Quadric representations for monocular slam. arXiv preprint arXiv:2403.08125, 2024a. Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, and Kun Zhou. Rtgslam: Real-time 3d reconstruction at scale using gaussian splatting. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024b. Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 12 Erik Sandstrom, Ganlin Zhang, Keisuke Tateno, Michael Oechsle, Michael Niemeyer, Youmin Zhang, Manthan Patel, Luc Van Gool, Martin Oswald, and Federico Tombari. Splat-slam: GlobIn Proceedings of the Computer Vision and ally optimized rgb-only slam with 3d gaussians. Pattern Recognition Conference, pp. 16801691, 2025. Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise View Selection for Unstructured Multi-View Stereo. In European Conference on Computer Vision (ECCV), 2016. Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 573580. IEEE, 2012. Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davison. imap: Implicit mapping and poIn Proceedings of the IEEE/CVF international conference on computer sitioning in real-time. vision, pp. 62296238, 2021. Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast conIn Proceedings of the IEEE/CVF Conference on vergence for radiance fields reconstruction. Computer Vision and Pattern Recognition (CVPR), pp. 54595469, June 2022. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24462454, 2020. Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36:3903339051, 2023. Yongchun Fang Tianci Wen, Zhiang Liu. Segs-slam: Structure-enhanced 3d gaussian splatting slam with appearance embedding. arXiv preprint arXiv:2501.05242, 2025. Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. GeoCalib: Single-image Calibration with Geometric Optimization. In ECCV, 2024. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul SriniIn 2022 vasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 54815490. IEEE, 2022. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025a. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning, 2025b. URL https://arxiv.org/abs/2507.13347. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. 13 Songyin Wu, Zhaoyang Lv, Yufeng Zhu, Duncan Frost, Zhengqin Li, Ling-Qi Yan, Carl Ren, Richard Newcombe, and Zhao Dong. Monocular online reconstruction with enhanced detail preservation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 111, 2025. Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, Dahua Lin, Michael Zollhofer, and Christian Richardt. VR-NeRF: High-fidelity virtualized walkable spaces. In SIGGRAPH Asia Conference Proceedings, 2023. doi: 10.1145/3610548.3618139. URL https://vr-nerf. github.io. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 54385448, June 2022. Chi Yan, Delin Qu, Dan Xu, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li. GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting, April 2024. Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: highfidelity dataset of 3d indoor scenes. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved rendering and reconstruction. arXiv preprint arXiv:2403.16964, 2024a. Sicheng Yu, Chong Cheng, Yifan Zhou, Xiaojun Yang, and Hao Wang. Rgb-only gaussian splatting slam for unbounded outdoor scenes. arXiv preprint arXiv:2502.15633, 2025. Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Aliasfree 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1944719456, 2024b. Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Transactions on Graphics, 2024c. Vladimir Yugay, Yue Li, Theo Gevers, and Martin Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070, 2024. Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. European Conference on Computer Vision, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera esIn Proceedings of the Computer Vision and Pattern timation from uncalibrated sparse views. Recognition Conference, pp. 2193621947, 2025. Youmin Zhang, Fabio Tosi, Stefano Mattoccia, and Matteo Poggi. Go-slam: Global optimization for consistent 3d instant reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 37273737, 2023. 14 Chunran Zheng, Wei Xu, Zuhao Zou, Tong Hua, Chongjian Yuan, Dongjiao He, Bingyang Zhou, Zheng Liu, Jiarong Lin, Fangcheng Zhu, et al. Fast-livo2: Fast, direct lidar-inertial-visual odometry. IEEE Transactions on Robotics, 2024. Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022."
        },
        {
            "title": "A SUPPLEMENTARY MATERIAL",
            "content": "We organize the supplementary material as follows: Sec. A.1 describes implementation & evaluation protocol. Sec. A.2 derives Jacobians and covariance transformation used in the Frontend / Backend. Sec. A.3 presents additional experimental details, including π3-based multi-frame ablations and their observations. Sec. A.4 expands the Frontend implementation, covering correspondence formation, residual weighting, Gauss-Newton updates, and focal optimization under unknown intrinsics. Sec. A.5describes Backend loop-closure detection (ASMK + multi-frame 3D priors) and global bundle adjustment. Sec. A.6 reports extended experiments, per-scene quantitative metrics, and additional qualitative results. A.1 MORE IMPLEMENTATION DETAILS Evaluation protocol. Prior works evaluate reconstruction quality on different subsets of frames, as each method selects its own keyframes for mapping. This inconsistency leads to slight but systematic metric bias. To ensure fair and reproducible evaluation, we uniformly sample one frame out of every eight across all sequences as evaluation frames. These frames are excluded from mapping supervision and are used only for pose optimization during inference. We have re-implemented or modified baseline evaluation scripts accordingly so that all reported metrics in Tables are computed under this identical protocol. This guarantees that differences in performance arise solely from algorithmic design rather than evaluation selection bias. A.2 JACOBIAN Let 3D point in the current camera coordinate system be = (X, Y, Z). Its projection to the image plane with logdepth parameterization is defined as π(X, Y, Z) = fxX/Z + cx fyY /Z + cy log , (4) where (fx, fy) are focal lengths and (cx, cy) are principal point offsets. The differential of this mapping governs how small 3D perturbations propagate into pixel-space. Jacobian with respect to predicted 3D points. The Jacobian of the projection function w.r.t. the 3D coordinates is Jπ(x) = . (5) fx 0 0 fxX Z2 fyY fy 0 Z2 1 Given the per-point covariance Σc R33 predicted in the current frame (estimated from local neighborhoods as described in Sec. 3.1), the measurement-space covariance after projection to the keyframe view is Σck = Jπ(xck) Rkc Σc kc Jπ(xck), (6) 15 where Rkc is the rotation matrix of the relative pose Tkc between the current frame and keyframe k. We reject correspondence if det(Σck) > τ , which effectively discards measurements with high geometric uncertainty while remaining rotation-invariant. Jacobian with respect to focal length When the focal length is unknown or weakly calibrated, we jointly optimize it with the pose parameters. Let pixel pc = (uc, vc) in the current frame correspond to the 3D point Pc = (Xc, Yc, Zc). The point reconstructed in normalized camera coordinates is c = Zc (uc cx)/f (vc cy)/f 1 , (7) and its transformation to the keyframe is Pk = Tkc projection back to pixel space is with Tkc = [Rkc, tkc] Sim(3). The π(Pk) = Xk/Zk + cx Yk/Zk + cy log Zk . (8) The focal length influences the projection in two ways: (1) Direct effect: scaling of projected pixel coordinates. (2) Indirect effect: change in the reconstructed 3D point itself, which depends on through normalization. The corresponding partial derivatives are (u, v, log Z) = Xk/Zk Yk/Zk 0 , f = Zc (uccx) 2 (vccy) 2 , Pk = Tkc f . Combining both contributions via the chain rule yields the full Jacobian: Jπ,f (Pk) = (u, v, log Z) (cid:123)(cid:122) direct (cid:125) (cid:124) + (cid:124) (u, v, log Z) Pk (cid:123)(cid:122) indirect Zk Zk 0 + Zk 0 0 = Pk (cid:125) 0 (Zk)2 Zk 0 (Zk)2 1 Zk (9) (10) (uccx) 2 (vccy) 2 0 (11) This joint derivative allows the optimizer to update focal length coherently with the camera pose. A.3 ADDITIONAL EXPERIMENTS Sec. 4.3 of the main paper evaluates how different architectural and procedural choices affect localization and reconstruction. Here we provide detailed background and rationale for one particularly important ablation: replacing the pairwise correspondence model MASt3R with the multi-frame visual-geometry model π3. The goal is to understand whether stronger 3D priors and longer temporal context can further enhance robustness and global consistency in the pipeline. Background. MASt3R (Leroy et al., 2024b) predicts two-view correspondences and metric pointmaps with well-calibrated scale but limited temporal context. In contrast, π3 (Wang et al., 2025b) is permutation-equivariant large-scale geometry model trained on multi-image sets. Its potential advantage lies in leveraging more global spatial constraints and recovering denser, smoother pointmaps across frames. However, because π3 is not explicitly metric-aware, integrating it into streaming SLAM-style system introduces challenges in maintaining scale and temporal consistency. Below we describe how we adapt π3 to our framework and analyze its effect relative to the baseline. 16 Frontend with π3 Inference and Keyframe Scheduling. In the original frontend, each incoming frame is matched against the latest keyframe using MASt3R. To enable multi-frame inference, we accumulate consecutive frames before calling π3. The model jointly estimates pointmaps for all views and provides dense pixel correspondences. After inference, we perform local bundle adjustment (LBA) in the Sim(3) group among these frames. Upon completing this process, we follow the keyframe selection strategy described in Sec. 3.1, and the selected keyframes are then passed to the backend. After the first π3 inference, subsequent updates reuse the most recent keyframes as temporal anchors and wait until kl new frames arrive before triggering the next inference batch. This overlap ensures continuity and smooth transition of geometric priors between windows. Backend with π3-Based Global Optimization. The backend receives keyframes streamed from the frontend. To incorporate π3 priors, it similarly waits until keyframes are accumulated, then performs joint π3 inference to obtain multi-view correspondences among the current and historical keyframes. global bundle adjustment (GBA) then optimizes all selected poses. Compared with the frontends local window, the backends inference window integrates non-adjacent keyframes filtered by the ASMK module from MASt3R-SLAM (Murai et al., 2024), providing long-range loop-closure evidence while keeping runtime tractable. Mapper with Keyframe Reception. The mapping module remains unchanged. The mapper simply receives the keyframes sent by the backend and proceeds with the reconstruction following the process described in Sec. 3.3. A.4 DETAILS IN FRONTEND: POSE AND FOCAL OPTIMIZATION The frontend provides the first stage of the streaming pipeline. We now present detailed formulation of the weighted GaussNewton optimization that simultaneously refines pose and focal length when intrinsics are unknown. For each input frame, MASt3R predicts dense two-view correspondences and associated 3D points between the current frame and the latest keyframe k. We denote the correspondence set as , pm = {(pm , Pm , Pm )}M m=1, (12) , pm R2 are pixel coordinates and Pm R3 are the corresponding 3D points where pm expressed in the coordinate systems of the current frame and keyframe, respectively. The relative transform Tkc = (skc, Rkc, tkc) includes scale, rotation, and translation components. , Pm Residual formulation. For each correspondence m, we compose the similarity transform and projection directly into the residual: ˆpm pm log ˆZ log = πK(f )( ˆPm ˆpm ), = skcRkcPm + tkc, rm = ˆPm (13) (cid:21) (cid:20) , where πK(f )() denotes the pinhole projection with intrinsic matrix K(f ). The objective seeks the optimal parameters that minimize the robust weighted energy Erob = 1 2 (cid:88) m=1 mWrob rm, Wrob = ωmWm. (14) Weighting and robustness. Each correspondence is assigned positive-semidefinite weight matrix Wm R33 derived from measurement covariance, balancing pixel and log-depth residuals. To suppress outliers, we apply Huber kernel: ωm = 1, δ sm + ε , sm δ, sm > δ, (cid:113) sm = mWmrm, (15) with small ε > 0 for numerical stability. 17 Linearization and update. Linearizing Eq. 13 about the current estimate yields the normal equations (cid:16) (cid:88) (cid:17) (cid:88) mWrob Jm θ = mWrob rm, (16) where Jm = rm/θ and θ = {Tkc, }. For pose-only optimization, the dimensionality is = 7; when focal refinement is enabled, = 8. The updates are applied through the exponential map in the Sim(3) Lie group: Tkc expSim(3)(ξsim) Tkc, + f, (17) where ξsim R7 is the minimal increment of the similarity transform. A.5 DETAILS IN BACKEND: LOOP CLOSURE AND GLOBAL BUNDLE ADJUSTMENT The long-term trajectory accuracy requires closing loops and enforcing multi-view consistency across the entire sequence. The backend of ARTDECO detects loop closures, verifies them with 3D priors, and performs global optimization of all keyframe poses in the Sim(3) group. Hybrid Loop-Closure Detection. Given new keyframe Kt, the backend first computes its ASMK similarity to all historical keyframes {Kj}. Candidates with score greater than 0.005 are retained. If the temporal gap between Kt and the nearest candidate exceeds 10 frames, we assume potential loop and re-rank the top Na candidates by similarity. Next, we perform π3 inference jointly on the set {Kt} CNa to obtain dense pointmaps in shared coordinate system. For each candidate keyframe Kj, we compute the angular and depth errors between corresponding 3D points Pm . If the ratio of geometrically consistent correspondences exceeds 0.15, the pair (Kt, Kj) is confirmed as loop closure and added as an edge in the factor graph. and Pm This two-stage scheme achieves balance between robustness and efficiency: ASMK rapidly filters potential loops, and π3 provides dense geometric validation resilient in practice. Global Bundle Adjustment. After loop-closure edges are added, we jointly refine all connected keyframe poses {Twi} = {(si, Ri, ti)} Sim(3) through global bundle adjustment (GBA). For R2 correspondence between frames and j, let Pm the observed pixel in frame j. The transformed and projected quantities are R3 be the 3D point in frame and pm ˆPm = siRiPm + ti, ˆPm = s1 ( ˆPm tj), We define the residual vector as rm = (cid:20) ˆpm pm log ˆZ log (cid:21) , = πK( ˆPm ˆpm ). (18) (19) which combines pixel reprojection and log-depth errors. The objective function sums all residuals weighted by their confidence ωm: EGBA = (cid:88) i,j,m ωm mrm. (20) For each correspondence, the projection Jacobian with respect to the 3D point ˆPm is = (Xj, Yj, Zj) rm ˆPm = fx Zj 0 0 fxXj Z2 fyYj fy Z2 Zj 1 0 Zj . (21) Perturbing poses in Sim(3) by left-multiplicative increments δξi, δξj R7 yields ˆPm δξi = s1 (cid:20) I3 [ ˆPm Pm 0 ] (cid:21) , ˆPm δξj (cid:104) = [ ˆPm ] (cid:105) , (22) where [] denotes the skew-symmetric matrix. By the chain rule, rm δξi = rm ˆPm ˆPm δξi , rm δξj = rm ˆPm ˆPm δξj . (23) After global optimization, each keyframe reprojects its 3D points {Xn keyframes Nk and computes the mean reprojection error }Nk n=1 to all connected en = 1 Nk (cid:88) jNk"
        },
        {
            "title": "A confidence score is then assigned as",
            "content": "un πK(TjkXn )2. cn = 1, 1 en τproj + 1 , en τproj, en > τproj, (24) (25) and the weighted pairs {(Xn , cn)} are sent to the mapping thread. This ensures that subsequent Gaussian-primitive updates favor high-confidence geometric regions while down-weighting uncertain areas. A.6 MORE EXPERIMENTS Below, We provide per-scene quantitative results and qualitative comparisons to further support the main paper. As shown in Tab. 429, ARTDECO consistently achieves the highest PSNR and SSIM and the lowest LPIPS across all datasets, validating its strong generalization from vatious type of indoor scenes (TUM (Sturm et al., 2012), ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), VR-NeRF (Xu et al., 2023)) to large-scale outdoor captures (Waymo (Sun et al., 2020), FastLIVO2 (Zheng et al., 2024), KITTI (Geiger et al., 2013), MatrixCity (Li et al., 2023)). We also provide qualitative results related to reconstruction and tracking trajectories, as shown in Fig. 7 6 8. Table 4: PSNR on the Fast-LIVO2 Dataset Method CBD Building 01 HKU Campus Red Sculpture Retail Street SYSU MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 19.86 26.26 17.47 17.79 29.25 31. 21.70 29.55 25.47 21.46 29.45 30.89 15.50 - 18.89 17.50 24.97 26. 18.05 18.49 24.30 17.67 23.10 29.20 19.23 24.01 21.42 19.39 25.07 30. Table 5: SSIM on the Fast-LIVO2 Dataset Method CBD Building 01 HKU Campus Red Sculpture Retail Street SYSU MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.698 0.880 0.645 0.677 0. 0.940 0.608 0.847 0.705 0.628 0.832 0.871 19 0.518 - 0. 0.590 0.786 0.861 0.554 0.589 0.782 0.558 0.708 0.901 0.610 0.777 0. 0.635 0.742 0.899 Table 6: LPIPS on the Fast-LIVO2 Dataset Method CBD Building 01 HKU Campus Red Sculpture Retail Street SYSU MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.623 0.213 0.592 0.490 0.179 0.108 0.687 0.205 0.448 0.470 0.258 0. 0.778 - 0.505 0.500 0.304 0.205 0.750 0.516 0.232 0.528 0.315 0. 0.658 0.292 0.451 0.498 0.322 0.151 Table 7: PSNR on the TUM Dataset Method f1 360 f1 desk f1 desk2 f1 floor f1 plant f1 room f1 rpy f1 teddy f1 xyz f2 xyz f3 office MonoGS 16.17 SEGS-SLAM 19.43 16.70 S3PO-GS OnTheFly-NVS 18.44 27.07 LongSplat 14.86 19.81 20. 18.91 25.35 14.96 18.44 18.52 18.38 25.48 20.71 21.75 22.69 25.43 28.14 17.46 17.33 18. 15.84 21.27 15.38 - 17.14 17.26 23.52 16.28 18.44 16.67 20.86 22.81 16.50 15.37 19. 16.27 22.36 21.89 21.23 20.68 19.54 22.78 23.06 25.69 20.04 26.75 27.46 20.10 26.14 20.74 19.80 25.80 Ours 26.19 26.04 25.54 29.43 24.06 25. 24.92 23.30 26.50 29.91 26.92 Table 8: SSIM on the TUM Dataset Method f1 360 f1 desk f1 desk2 f1 floor f1 plant f1 room f1 rpy f1 teddy f1 xyz f2 xyz f3 office MonoGS 0.583 SEGS-SLAM 0.751 0.602 S3PO-GS OnTheFly-NVS 0.725 0.826 LongSplat 0.529 0.775 0.680 0.712 0.833 0.552 0.720 0.650 0.713 0.842 0.586 0.752 0.625 0.783 0. 0.581 0.641 0.616 0.600 0.695 0.542 - 0.597 0.662 0.790 0.575 0.718 0.596 0.755 0. 0.539 0.622 0.614 0.594 0.736 0.738 0.698 0.821 0.769 0.762 0.752 0.870 0.730 0.888 0.879 0.698 0.861 0.723 0.760 0. Ours 0.850 0.861 0.859 0.838 0. 0.838 0.847 0.768 0.883 0.922 0.882 Table 9: LPIPS on the TUM Dataset Method f1 360 f1 desk f1 desk2 f1 floor f1 plant f1 room f1 rpy f1 teddy f1 xyz f2 xyz f3 office MonoGS 0.642 SEGS-SLAM 0.361 0.551 S3PO-GS OnTheFly-NVS 0.445 0. LongSplat 0.664 0.244 0.433 0.396 0.266 0.661 0.358 0.506 0.404 0.270 0.736 0.277 0. 0.305 0.305 0.572 0.399 0.461 0.501 0.406 0.679 - 0.571 0.440 0.255 0.509 0.336 0. 0.353 0.267 0.654 0.415 0.482 0.505 0.300 0.326 0.355 0.205 0.270 0.296 0.270 0.199 0.326 0.127 0.152 0.511 0.200 0. 0.307 0.325 Ours 0.279 0.220 0.238 0. 0.263 0.251 0.235 0.298 0.174 0.080 0. 20 Table 10: PSNR on the ScanNet Dataset Method scene0000 00 scene0059 00 scene0106 00 scene0169 00 scene0181 00 scene0207 00 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours - - 17.98 14.88 - 23.28 18.09 - 19.54 16.33 19. 24.74 18.03 17.92 20.27 14.66 19.01 26.34 19.71 20.89 21.24 16.37 19. 23.07 19.37 21.40 21.20 15.47 18.94 21.80 19.16 18.69 20.62 14.47 19. 25.37 Table 11: SSIM on the ScanNet Dataset Method scene0000 00 scene0059 00 scene0106 00 scene0169 00 scene0181 00 scene0207 00 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours - - 0.738 0.742 - 0.824 0.733 - 0.769 0.747 0. 0.863 0.774 0.804 0.816 0.667 0.764 0.905 0.792 0.850 0.815 0.721 0. 0.859 0.823 0.902 0.845 0.667 0.782 0.889 0.776 0.799 0.797 0.705 0. 0.848 Table 12: LPIPS on the ScanNet Dataset Method scene0000 00 scene0059 00 scene0106 00 scene0169 00 scene0181 00 scene0207 00 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours - - 0.700 0.464 - 0.254 0.713 - 0.518 0.477 0. 0.279 0.597 0.405 0.484 0.554 0.384 0.237 0.612 0.352 0.550 0.504 0. 0.278 0.578 0.269 0.513 0.491 0.423 0.288 0.644 0.432 0.583 0.474 0. 0.290 Table 13: PSNR on the Waymo Dataset Method 100613 106762 132384 13476 152706 153495 158686 163453 405841 MonoGS 20.05 SEGS-SLAM 20.14 25. S3PO-GS OnTheFly-NVS 26.95 24.25 LongSplat 20.91 23.60 28.23 27.21 23.70 22.71 22.52 27. 25.31 24.11 19.51 - 24.85 24.34 24.42 21.23 24.11 28.53 25.45 25.66 14.15 21.16 26. 26.41 23.74 20.29 21.22 26.03 26.30 24.84 19.01 - 23.65 23.97 22.69 16.19 19.01 27. 23.79 25.61 Ours 27.98 30.84 30.32 28. 29.80 27.60 26.83 26.91 30.48 Table 14: SSIM on the Waymo Dataset Method 100613 106762 132384 13476 152706 153495 158686 163453 405841 MonoGS 0.758 SEGS-SLAM 0.730 0.828 S3PO-GS OnTheFly-NVS 0.850 0.773 LongSplat 0.816 0.805 0.878 0.854 0.766 0.855 0.824 0.883 0.864 0. 0.713 - 0.778 0.764 0.716 0.792 0.784 0.856 0.795 0.778 0.672 0.734 0.846 0.847 0. 0.723 0.677 0.819 0.837 0.777 0.745 - 0.797 0.785 0.732 0.695 0.698 0.865 0.788 0. Ours 0.865 0.906 0.919 0.856 0. 0.871 0.847 0.866 0.907 Table 15: LPIPS on the Waymo Dataset Method 100613 106762 132384 13476 152706 153495 158686 163453 405841 MonoGS 0.610 SEGS-SLAM 0.484 0.329 S3PO-GS OnTheFly-NVS 0.328 0.356 LongSplat 0.534 0.399 0.276 0.337 0.328 0.451 0.384 0.275 0.361 0.355 0.745 - 0.471 0.378 0. 0.666 0.450 0.427 0.401 0.397 0.710 0.472 0.379 0.348 0.430 0.627 0.495 0.373 0.307 0. 0.664 - 0.411 0.376 0.371 0.633 0.502 0.352 0.400 0.326 Ours 0. 0.237 0.265 0.267 0.304 0.313 0. 0.289 0.216 Table 16: PSNR on the VR-NeRF Dataset Method appartment262 kitchen261 kitchen262 kitchen263 table61 workspace61 workspace62 workspace64 MonoGS SEGS-SLAM S3PO-GS LongSplat OnTheFly-NVS Ours 18.43 26.14 28.45 31.22 30.52 32.98 16.91 31.81 27. 27.10 30.24 30.90 11.66 - 25.16 27.76 27.51 30.23 14.47 32.65 18. 24.04 25.16 15.50 36.55 19.36 24.90 27.77 29.04 29.05 14.80 30.95 22. - 22.08 24.68 15.12 - 23.49 22.32 26.04 24.63 14.80 - 22. 22.84 29.05 27.13 Table 17: SSIM on the VR-NeRF Dataset Method appartment262 kitchen261 kitchen262 kitchen263 table61 workspace61 workspace62 workspace64 MonoGS SEGS-SLAM S3PO-GS LongSplat OnTheFly-NVS Ours 0.646 0.831 0.875 0.905 0.912 0.937 0.627 0.910 0. 0.861 0.903 0.913 0.506 - 0.856 0.888 0.900 0.939 0.599 0.883 0. 0.803 0.847 0.595 0.949 0.737 0.823 0.882 0.913 0.900 0.526 0.905 0. - 0.775 0.842 0.581 - 0.791 0.757 0.855 0.833 0.522 - 0. 0.787 0.898 0.883 22 Table 18: LPIPS on the VR-NeRF Dataset Method appartment262 kitchen261 kitchen262 kitchen263 table61 workspace61 workspace62 workspace MonoGS SEGS-SLAM S3PO-GS LongSplat OnTheFly-NVS Ours 0.631 0.375 0.321 0.292 0.302 0. 0.688 0.218 0.302 0.308 0.277 0.224 0.676 - 0.269 0.265 0.311 0. 0.662 0.192 0.597 0.319 0.311 0.582 0.174 0.474 0.353 0.322 0.198 0. 0.689 0.203 0.385 - 0.385 0.286 0.721 - 0.350 0.359 0.307 0. 0.685 - 0.386 0.352 0.261 0.274 Table 19: PSNR on the KITTI Dataset 02 15.08 14.88 19. 17.00 14.62 22.53 03 16.90 16.71 20.51 17.34 17.97 24. 05 15.66 14.88 20.73 18.08 18.08 23.80 06 16.38 14.65 20. 17.29 18.68 23.59 Table 20: SSIM on the KITTI Dataset 02 0.476 0.458 0.587 0.538 0. 0.707 03 0.487 0.448 0.581 0.529 0.482 0.745 0.491 0.470 0.659 0.606 0.548 0.781 06 0.560 0.510 0.652 0.583 0. 0.779 Table 21: LPIPS on the KITTI Dataset 02 0.761 0.491 0.459 0.501 0.531 0. 03 0.735 0.465 0.498 0.501 0.438 0.311 05 0.753 0.453 0. 0.455 0.428 0.281 06 0.720 0.464 0.401 0.500 0.394 0. 00 16.01 12.69 20.77 15.97 17.84 23.76 00 0.568 0.454 0. 0.594 0.616 0.829 00 0.687 0.492 0.264 0.461 0.375 0. 07 11.38 10.45 20.38 18.21 16.14 23.92 07 0.439 0.405 0. 0.673 0.557 0.827 07 0.826 0.552 0.364 0.388 0.454 0. 08 13.21 14.50 20.27 16.50 16.58 22.86 08 0.480 0.493 0. 0.622 0.543 0.786 08 0.830 0.450 0.345 0.422 0.440 0. 10 11.82 13.51 17.41 14.73 14.93 20.38 10 0.420 0.462 0. 0.483 0.484 0.663 10 0.820 0.533 0.563 0.540 0.517 0. Method MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours Method MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours Method MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 23 Table 22: PSNR on the ScanNet++ Dataset Method 00777c41d4 02f25e5fee 0b031f 126d03d821 1cbb105c6a 2284bf5c9d 2d2e873aa0 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 14.621 - 21.316 16.270 18.465 26. 20.915 28.177 25.158 19.134 26.747 30.671 9.488 - 21.019 21.701 22.484 27.722 21.353 - 23.758 17.572 27.251 32.796 23.716 - 26.614 17.478 28.621 32.406 13.049 - 23.446 18.200 23.348 30.277 16.861 - 23.554 14.173 - 28. Method 303745abc7 41eb967018 46001f434d 4808c4a397 546292a9db 712dc47104 7543973e1a MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 14.899 - 25.840 15.884 - 33.056 18.011 - 21.074 16.247 - 28.608 18.515 - 20.288 16.426 24.437 21. 21.570 - 24.263 18.722 - 30.101 9.464 - 20.612 24.791 24.587 25.868 8.970 - 17.843 14.056 - 27.506 22.547 27.169 26.367 21.540 28.539 32.308 Table 23: SSIM on the ScaNnet++ Dataset Method 00777c41d4 02f25e5fee 0b031f3119 126d03d821 1cbb105c6a 2284bf5c9d 2d2e873aa0 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.559 - 0.695 0.583 0.604 0.855 0.784 0.911 0.853 0.779 0.868 0.941 0.485 - 0.809 0.823 0.816 0.903 0.796 - 0.828 0.753 0.854 0. 0.833 - 0.876 0.749 0.897 0.956 0.622 - 0.833 0.751 0.819 0.937 0.641 - 0.830 0.705 - 0.918 Method 303745abc7 41eb 46001f434d 4808c4a397 546292a9db 712dc47104 7543973e1a MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.723 - 0.878 0.780 - 0.959 0.737 - 0.788 0.705 - 0.905 0.844 - 0.861 0.817 0.900 0.891 0.848 - 0.873 0.826 - 0.938 0.244 - 0.688 0.825 0.793 0.850 0.590 - 0.784 0.732 - 0. 0.836 0.891 0.884 0.831 0.894 0.950 Table 24: LPIPS on the ScanNet++ Dataset Method 00777c41d4 02f25e5fee 0b031f 126d03d821 1cbb105c6a 2284bf5c9d 2d2e873aa0 MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.822 - 0.451 0.524 0.436 0. 0.459 0.148 0.243 0.352 0.179 0.122 0.724 - 0.350 0.310 0.307 0.196 0.450 - 0.346 0.404 0.242 0.143 0.370 - 0.264 0.409 0.179 0.123 0.710 - 0.289 0.386 0.245 0.134 0.769 - 0.316 0.499 - 0. Method 303745abc7 41eb967018 46001f434d 4808c4a397 546292a9db 712dc47104 7543973e1a MonoGS SEGS-SLAM S3PO-GS OnTheFly-NVS LongSplat Ours 0.707 - 0.259 0.382 - 0.115 0.619 - 0.454 0.434 - 0.165 0.485 - 0.419 0.362 0.265 0. 0.408 - 0.300 0.331 - 0.156 0.734 - 0.520 0.256 0.286 0.205 0.707 - 0.449 0.450 - 0.176 0.441 0.216 0.303 0.305 0.200 0.140 24 Table 25: Tracking Results on the ScanNet++ Dataset Method 0077 02f2 0b03 126d 1cbb 2284 2d2e 3037 41eb 4600 4808 5462 712d 7543 OnTheFly LongSplat 0.804 0.157 0.114 0.227 0.227 0.913 1.438 1.186 1.605 3.932 0.486 0.018 0.458 0.907 0.058 1.307 0.041 0.378 0.550 0.023 0.713 1.943 0. - - - - - - - 0.294 0.939 2.019 0.922 1.760 3.540 0.432 MonoGS S3PO-GS SEGS-SLAM 1.364 0.626 0.853 0.642 0.650 0.016 0.093 0.418 0.135 0.441 0.355 0.858 1.674 1.318 0.614 0.909 1.092 0.270 0.485 - MASt3R-SLAM 0.121 0.017 0.059 0.122 0.021 0.027 0.835 0.176 0.020 2.379 0.065 0.042 0.021 0.025 loop with vggt 0.010 0.011 0.014 1.176 0.014 0.022 0.010 0.007 0.030 0.075 0.022 0.032 0.013 0.011 0.009 0.011 0.015 0.019 0.015 0.012 0.010 0.005 0.017 0.060 0.021 0.030 0.014 0.011 0.005 Ours - - - - - - - - - - - - Table 26: Tracking Results on the TUM Dataset Method f1 360 f1 desk f1 desk2 f1 floor f1 plant f1 room f1 rpy f1 teddy f1 xyz f2 xyz f3 office OnTheFly LongSplat 0.187 0.133 - - MonoGS S3PO-GS 0.160 0.093 SEGS-SLAM 0.154 MASt3R-SLAM 0.049 0.040 loop with vggt 0.040 Ours 0.034 0.041 0.016 0.016 0.016 0.016 - - 0.596 0.155 0.013 0.024 0.026 0.025 - - 0.531 0.228 - 0.025 0.026 0.025 - - 0.077 0.040 - 0.020 0.017 0.016 0.874 0.712 0.649 0.552 - 0.061 0.056 0.060 4.136 - 0.032 0.053 - 0.027 0.023 0.022 - - 0.512 0.051 0.293 0.041 0.047 0.045 0.278 0.073 0.103 - 0.017 0.047 0.010 0.016 0.009 0.006 0.009 0.005 0.007 0.005 0.007 0. 1.489 - 0.033 0.045 0.026 0.031 0.024 0.019 Table 27: Tracking Results on the KITTI Dataset Method OnTheFly LongSplat MonoGS S3PO-GS SEGS-SLAM MASt3R-SLAM loop with vggt Ours 00 02 03 05 06 08 10 Avg. 18.297 3.047 11.868 1.196 0.561 - 1.304 1.304 28.230 10. 11.817 2.961 0.897 1.756 2.691 2.691 14.356 9.344 11.195 5.522 0.189 0.397 0.442 0.442 5.027 9.482 4.623 1.459 0.967 0.761 1.103 1.103 9.608 6. 7.638 0.721 4.122 2.279 0.958 0.958 1.550 2.866 4.128 1.009 0.851 1.361 1.257 1.321 9.836 4.480 5.864 2.655 0.477 - 1.160 1.167 9.502 6. 5.109 1.927 0.353 - 1.893 1.893 12.051 6.682 7.780 2.181 1.052 1.351 1.360 Table 28: Tracking Results on the Waymo Dataset Method 100613 106762 132384 152706 153495 163453 405841 Avg. OnTheFly LongSplat MonoGS S3PO-GS SEGS-SLAM MASt3R-SLAM loop with vggt Ours 1.687 7. 3.262 1.165 - - 1.230 1.229 1.051 10.242 6.706 2.471 0.860 - 0.315 0.315 2.022 7.506 15.638 0.214 0.696 2.958 2.762 2.762 14.395 2. 11.093 1.551 2.755 - 1.753 1.753 1.193 4.900 8.286 1.144 1.867 - 0.931 0.931 2.110 2.625 0.585 1.972 - 1.569 1.578 1.578 2.206 3. 6.881 0.963 - 1.098 0.460 0.460 1.872 3.939 10.669 1.085 - 2.334 1.071 1.071 1.523 2.729 3.218 0.560 - 0.873 0.816 0.816 3.118 4. 7.370 1.236 - - 1.213 1.213 25 Table 29: Tracking Results on the TUM Dataset Compared with Non 3DGS SLAM Systems Method 360 desk desk2 floor plant room rpy - ORB-SLAM3 DPV-SLAM++ 0.132 DROID-SLAM 0.111 0.089 MASt3R-SLAM 0.049 0. Go-SLAM Ours 0.017 0.018 0.018 0.016 0.016 0.016 0.210 0.029 0.042 0.028 0.024 0.025 - 0.050 0.021 0.025 0.025 0.025 0.034 0.022 0.016 0.026 0.020 0. - 0.096 0.049 0.052 0.061 0.060 - 0.032 0.026 0.019 0.027 0.022 teddy - 0.098 0.048 0.048 0.041 0.045 xyz 0.009 0.010 0.012 0.010 0.009 0. Avg. - 0.054 0.038 0.035 0.030 0.028 Figure 5: More Qualitative Reconstruction Results. Figure 6: Qualitative Comparison of Trajectories across Different Methods on the KITTI Dataset. 26 Figure 7: Qualitative Comparison of Trajectories across Different Methods on the ScanNet++ Dataset. Figure 8: Qualitative Comparison of Trajectories across Different Methods on the TUM Dataset."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}