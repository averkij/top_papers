{
    "paper_title": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences",
    "authors": [
        "Dmitrii Korzh",
        "Dmitrii Tarasov",
        "Artyom Iudin",
        "Elvir Karimov",
        "Matvey Skripkin",
        "Nikita Kuzmin",
        "Andrey Kuznetsov",
        "Oleg Y. Rogov",
        "Ivan Oseledets"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition."
        },
        {
            "title": "Start",
            "content": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences Dmitrii Korzh1,2,3, Dmitrii Tarasov1,4, Artyom Iudin1,3, Elvir Karimov1,2,3, Matvey Skripkin1,2, Nikita Kuzmin3, Andrey Kuznetsov1,2, Oleg Y. Rogov1,2,3, Ivan Oseledets1,2 1AIRI, 2Skoltech, 3MTUCI, 4HSE Moscow, Russia korzh@airi.net, d.tarasov@airi.net 5 2 0 2 5 ] . [ 1 2 4 5 3 0 . 8 0 5 2 : r Abstract Conversion of spoken mathematical expressions is challenging task that involves transcribing speech into strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR postcorrection models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with particular focus on mathematical content recognition."
        },
        {
            "title": "Introduction",
            "content": "Modern speech recognition models (Baevski et al. 2020; Radford et al. 2023) demonstrate strong performance on general speech but struggle with domain-specific tasks such as converting spoken mathematical expressions and sentences into formal symbolic representations like LaTeX. While simple symbols (e.g., +, , π, ) are often correctly recognized, more complex or nested expressions remain challenging. This limitation is critical in academic and educational contexts, including automatic lecture transcription, multimodal assistant development, and scientific notetaking. Speech-to-LaTeX (S2L) models, which can interpret the structure and semantics of mathematical language in speech, are essential in these applications. Prior work, Figure 1: S2L methods schematic illustration. (a) Postcorrection approach. (b) Multi-modal end-to-end approach (SALMONN). In (a), audio is transcribed by an ASR model, and the result is passed to an LLM for LaTeX conversion. In (b), raw audio is processed by 2 audio encoders and an adapter, and the resulting audio and textual prompt tokens are fed into LLaMA-based LLM to generate the LaTeX. such as MathBridge (Jung et al. 2024), addresses the Textto-LaTeX task using language models trained on textual representations of spoken equations. The S2L problem, however, remains largely underexplored. MathSpeech (Hyeon et al. 2025b) proposes an ASR post-correction approach that transcribes spoken equations into text, followed by Text-to-LaTeX generation via language models. Evaluation was performed on 1.1k spoken expressions from YouTube. However, this pipeline depends on dual ASR transcriptions, supports only isolated equations (not mathematical sentences), lacks multilingual support, and omits end-to-end multimodal approaches. Moreover, the test set is limited in size and diversity, and the underlying training data, voiced-over with TTS from MathBridge equations and pronunciations, is not publicly released. To address these limitations, it is necessary to develop new S2L datasets that contain partial human annotations and employ more robust modeling techniques, including end-to-end systems that integrate ASR, LMs, and audio-based LLMs. introduces the S2L dataset for spoken This paper mathematical language, which consists of 2 subsets: S2L-sentences and S2L-equations. The dataset contains approximately 12k unique mathematical sentences and 10.7k distinct isolated equations, each annotated by up to 3 different speakers (from total of 33 annotators) to capture diverse pronunciations, intonations, and linguistic styles. To further expand and augment the dataset, additional artificially-annotated expressions and sentences were added, resulting in 571k generated audio samples. We develop several S2L methods combining state-of-theart ASR models (Radford et al. 2023; Chen et al. 2022) with post-processing via fine-tuned LMs and end-to-end approaches, based on Audio-LLMs (Tang et al. 2024; Chu et al. 2023). These approaches are illustrated in Figure 1. Our best models achieve an equation CER between 27.7% and 30.0% on English data. Within mathematical sentences, text CER is up to 9.6%, and equation CER is up to 39.7%. These relatively high rates reflect the inherent ambiguity in spoken math. For example, kappa may correspond to kappa (κ) or varkappa (κ); the phrase one over plus two could yield 1 1 x+2 , or 1/x + 2. Despite such ambiguities, our models generate valid LaTeX in most cases, establishing strong performance baseline. + 2, Our contributions might be summarized as follows: We release the first large-scale, open-source 1 dataset of spoken mathematical expressions and sentences (S2L-sentences, S2L-equations) in English and Russian, including 66k human and 571k synthetic audio samples with diverse pronunciations and complexities. We evaluate multiple S2L methods based on ASR postcorrection, few-shot prompting, and audio-LLM integration, demonstrating strong performance across metrics and outperforming MathSpeech on several tasks. We conduct comprehensive evaluation using relevant metrics to establish robust baselines and detailed analysis for future S2L research."
        },
        {
            "title": "2 Related Work\nAutomatic Speech Recognition. CTC loss (Graves et al.\n2006; Amodei et al. 2016) allows alignment between audio\nand text without a precise labelling. While traditional ASR\nsuffers from context insensitivity, the Conformer model\n(Gulati et al. 2020) combines convolutions and self-attention\nto capture both local and global dependencies. Wav2Vec 2.0\n(Baevski et al. 2020) uses contrastive self-supervised learn-\ning to extract high-quality audio features. Whisper (Radford\net al. 2023), a transformer-based architecture, is trained in\na weakly-supervised manner, demonstrating a robust perfor-\nmance across various audio domains.",
            "content": "Language Models. Transformer-based LMs such as BERT (Devlin 2018), T5 (Raffel et al. 2020), and GPT-3 (Radford et al. 2019) have shown strong performance, including in math-related problems. LMs can process structured and ambiguous data, such as chemistry formulas (Ganeeva et al. 2024) or code problems (Li et al. 2024). Recent models such as Qwen2.5-Math (Yang et al. 2024) and InternLM-Math (Ying et al. 2024) are tuned explicitly for mathematical reasoning, leveraging chain-of-thought prompting and large-scale math corpora. However, they still require fine-tuning for the text-to-LaTeX conversion. ASR Post-Correction. Post-correction pipelines are well studied (Ma et al. 2025, 2023; Chen et al. 2023) and effective due to the availability of textual and especially textual math data (e.g., MathBridge), compared to available audio data used for fine-tuning ASR models. These approaches use ASR to transcribe audio and then apply an LM to convert 1https://hf.co/datasets/marsianin500/Speech2Latex. the text into LaTeX. This approach leverages strong LLM priors, which might be pre-trained on relevant mathematical data, without requiring expensive audio annotation. However, performance heavily depends on transcription quality, and ambiguity in mathematical speech remains challenging. Audio-LLMs. Multimodal LLMs (M-LLMs) aim to jointly process audio and text by encoding multiple modalities and feeding them into unified LM. SALMONN (Tang et al. 2024) combines Whisper and BEATs embeddings via Q-former (Li et al. 2023) into LLaMA-based decoder, enabling tasks like ASR and audio QA. Qwen-Audio adapts Whisper encodings to handle instruction-based audio tasks across multiple languages. While promising, these models are not explicitly designed for mathematical LaTeX generation, lack fine control over symbolic precision, and often cannot recognize spoken mathematics completely. OCR LaTeX Recognition. In contrast to S2L, OCRbased LaTeX recognition has received significant attention (Genthial 2024; Blecher et al. 2023; OleehyO 2024). Text-to-Speech (TTS). Modern TTS models (Casanova et al. 2024; Kong, Kim, and Bae 2020) can synthesize natural speech at near-human quality. Recently, MathReader (Hyeon et al. 2025a) proposed pipeline for converting LaTeX to speech via LLM-generated pronunciation and standard TTS. Authors of (Roychowdhury et al. 2025) evaluated 5 TTS systems on math expressions, showing that performance varies significantly by expression category and performs substantially worse than human expert readings. Spoken Mathematics Recognition. Only few works tackle S2L-related problems directly. Mathifier (Batlouni et al. 2011) targeted fixed-template equation recognition, now largely outdated. The work (Wei et al. 2025) introduced the Spoken-MQA benchmark for spoken math reasoning and evaluated several ASR post-correction models and audio-LLMs. While they demonstrated promising results on arithmetic reasoning, LaTeX-style symbolic expressions and advanced expressions were largely absent. MathSpeech (Hyeon et al. 2025b) introduced postcorrection pipeline using 2 ASR transcripts as input. Equations from MathBridge were synthesized via TTS and transcribed with 4 ASR models to produce around 8M samples. 2 T5-small models were trained to correct and convert transcripts into LaTeX. While effective, their approach requires multiple ASRs, lacks sentence-level context, and does not support multilingual or end-to-end modeling. et al. 2024) al. 2023; Weber Datasets. Textual math datasets like Proof-Pile (Azerbayev et and OpenWebMath (Paster et al. 2023) are key for training math-aware LLMs. MathBridge provides 23M LaTeX expressions with artificial text and context, but suffers from low quality and duplicates. OCR-LaTeX datasets like TextTeller (OleehyO 2024) offer highquality image-LaTeX pairs and can potentially support S2L via voice-over. However, large-scale S2L datasets remain missing: MathSpeech offers only 1.1k test set with no training data, and Spoken-MQA contains just 2.3k TTS samples focused on basic arithmetic. No dataset provides large-scale, human-annotated, contextual spoken math data, motivating us to begin with dataset collection."
        },
        {
            "title": "3 Dataset Collection\nIn this section, we describe the pipeline of S2L data collec-\ntion. We combined human-annotated and artificially gener-\nated data to create a robust and diverse dataset. We began by\ncollecting mathematical equations and sentences from mul-\ntiple sources, along with corresponding reference pronunci-\nations. These pronunciations serve both as guidance for non-\nexpert human annotators and as required inputs for artificial\nannotation via TTS or voice-conversion (VC) models.",
            "content": "Each sample is classified by language (English or Russian), annotation type (human or artificial), source (e.g., Proof-Pile, MathBridge, TextTeller, or generated), and format (S2L-equations for isolated expressions vs. S2L-sentences for in-context mathematical sentences)."
        },
        {
            "title": "3.1 Data Sources and Preparation",
            "content": "the S2L-equations, we utilized two existing For sources, MathBridge, and TextTeller, and also generated additional equations. For the S2L-sentences, the primary source was Proof-Pile. We incorporated subset of MathBridge, which offers large-scale textual pairs of equations and pronunciations with surrounding context. However, MathBridge data quality is inconsistent. Common issues include: (i) text instead of formula; (ii) invalid LaTeX; (iii) missing pronunciations; (iv) duplicated entries; (v) pronunciation additionally contains LaTeX; (vi) mismatched formulapronunciation pairs; (vii) lots of nearly duplicated formulas, such as cos(α), . . . , cos(ω). We selected 15,000 candidate equations and filtered them manually, retaining 3,000 high-quality English samples for both human and artificial annotation. Furthermore, we employed heuristic filters and LaTeX compilation checks to automatically clean the whole dataset, reducing it from 23 million to 1.5 million validated samples; of these, 400k were subsequently annotated with TTS in English. The heuristics involved filtering out overly short formulas, text-only entries (which typically contained plain text rather than equations), and cases where the equation was substantially longer than its spoken form. TextTeller provides complex equations used in OCRLaTeX research. We extracted 9,400 unique LaTeX equations and used GPT-4 to generate 4 reference pronunciations (2 English, 2 Russian) per equation, later used for artificial voice synthesis. To enhance diversity across mathematical domains, we prompted GPT-4 to generate LaTeXpronunciation pairs for common study topics (e.g., Calculus, Mechanics). Examples of generated topics and corresponding equations are provided in Appendix A.1. For S2L-sentences, we extracted contextual math sentences from the arxiv subset of Proof-Pile-2. Next preprocessing steps were applied: (i) filtering for inline formulas; (ii) removing LaTeX formatting of text (e.g., cite, textit); and (iii) validating equations via KaTeX compilation. Sentences were stratified by equation length  (Table 1)  and balanced accordingly, resulting in 12.4k clean samples with human-annotation coverage rate of approximately 2. Additionally, we included 1.4k negative examples (no equations) from the LRS3 dataset (Afouras, Chung, and Zisserman 2018) for artificial annotation. S2L-equations are primarily up to 140 LaTeX characters (max is 230), while the maximum S2L-sentence length was 310. The S2L-sentences has the following stratification by formula length  (Table 1)  and by the number of equations per sentence  (Table 2)  . Table 1: Character statistics in S2L-sentences dataset for the unique human-annotated expressions. Eq. length 310 1020 20 3050 50+ Counter 2,752 2,751 2, 2,396 1,941 2: Table Equations S2L-sentences for sentences. per the sentence unique in statistics human-annotated # Eq. 1 2 3 5 611 Counter 4,899 3,726 1, 1,028"
        },
        {
            "title": "3.2 Equations Normalization",
            "content": "All LaTeX equations were normalized using KaTeX (Barabash et al. 2025) fork, with uncompilable samples removed. The process standardized notation, eliminated extraneous spaces, inserted required braces, and unified operator names by parsing and reconstructing formulas via Abstract Syntax Tree. This reduced the CER by 1% on S2L-Equations. See Table 3 for examples. Table 3: Examples of LaTeX Equation Normalization. Original Equation Normalized Equation sum_iˆn frac{ n( n+1 ) }{ 2 } underset{ xi }{ max } Delta zsim1 sum_{i}ˆ{n}i frac{n(n+1)}{2} max_{xi} Delta zsim"
        },
        {
            "title": "3.3 Dataset Compositions and Audio Annotation",
            "content": "Each distinct equation or sentence in our dataset is paired with at least one reference pronunciation, which serves both as TTS input and human reference. For augmentation, multiple pronunciations were collected for several thousand expressions. To reduce annotation cost and augment the data, we explored the viability of training models on artificially generated audio. For this, we used open-source XTTSv2 (Casanova et al. 2024), and proprietary TTS APIs (e.g., SaluteSpeech). XTTSv2 was selected as the primary annotator due to its public availability, high audio fidelity, and voice conversion capability. For human annotation, we used crowd-sourcing platform similar to MTurk. The process showed speakers formula or sentence and reference pronunciations. Overall, 33 unique human annotators were involved. Manual verification was performed for each annotator: 10% of their audio was reviewed, and if more than 15% was rejected due to noise or low quality, only the verified subset was retained. To sum up, the resulting statistics are the following: Human annotation S2L-equations (English): 6,535 unique equations, 27 human-annotators, total 23,196 audio. S2L-equations (Russian): 4,274 unique equations, 10 human-annotators, total 18,134 audio. S2L-sentences (English): 12,395 unique sentences, 20 human-annotators, total 24,794 audio. Artificial annotation S2L-equations (English): 406,122 unique equations (6,535 as for human annotators, 399,587 new), 9 artificial voices, total 450,874 audio. S2L-equations (Russian): 12,669 unique equations (4,274 as for human annotation, 8,395 new), 14,449 reference pronunciations, 8 artificial voices, total 53,109 audio. S2L-sentences (English): 12,064 (10,411 as in human annotation, 1,984 new) unique sentences, 4 artificial voices, total 67,069 audio."
        },
        {
            "title": "3.4 S2L Data Representativeness Discussion\nAssessing dataset representativeness in the context of spo-\nken mathematical expressions is inherently challenging due\nto the breadth of mathematical fields and the diversity of\npronunciations. For example, the spoken phrase ”2 squared\nx2 + 1. One strat-\nfrom x plus 1” can map to either\negy is to include ”parentheses” in the pronunciation explic-\nitly. Some samples adopt this, but many do not, reflecting\nreal-world variability. Our dataset does not aim to cover the\nfull range of scientific disciplines. It rather prioritizes diver-\nsity in structure, notation, and linguistic realization. To this\nend, we were motivated by the following:",
            "content": "x2+1 or 2 2 Symbol and syntax coverage: We ensured broad coverage of commonly used LaTeX symbols and structures, such as alpha, omega, frac{}{}, sqrt{}, left(, etc. Curricular diversity: We prompted GPT to generate equations and pronunciations across typical undergraduate mathematics and physics topics, removing overly simplistic samples and those dominated by textual content (e.g., text{}). Source variability: The dataset draws from 3 distinct formula sources for S2L-equations, and includes real-world academic content via TextTeller and Proof-Pile-2. Language and voice variation: Pronunciations were collected in both English and Russian, using multiple TTS voices as well as human annotators. Pronunciation style diversity: In some cases, we included both phonetic (e.g., equals a) and lexical (e.g., Newtons second law) variants, reflecting natural pronunciations. While GPT-based samples may introduce some bias in both equations and pronunciations, this is partially mitigated through the inclusion of real-world academic data and crowd-sourced spoken annotations. Empirically, we observe that models trained on synthetically generated audio generalize well to human-annotated test cases, with no drastic degradation in performance (see the next section)."
        },
        {
            "title": "4.1 ASR Post-Correction\nWe first evaluated a Whisper-Large v3 ASR-only base-\nline for LaTeX transcription, achieving 88% CER on En-\nglish S2L-equations - deemed insufficient. While shal-\nlow/deep fusion methods could improve performance, we\nexcluded them due to practical limitations: high memory/la-\ntency costs (shallow fusion) and training complexity (deep\nfusion).",
            "content": "Instead, we adopted an ASR post-correction pipeline, strategy previously shown to be effective for transcription improvement and for math-related speech tasks (Hyeon et al. 2025b; Jung et al. 2024; Chen et al. 2023). Among the ASR models evaluated, Whisper-Large v3 provided the most accurate transcriptions for mathematical symbols, particularly Greek letters and structured expressions. Canary and Qwen-Audio (based on Whisper v2) also performed reasonably well, while WavLM and Wav2Vec2.0 produced frequent symbol errors. Please, refer to Table 8 in the Appendix for the transcriptions comparison. Summing up, we used frozen ASR model and fine-tuned LLMs for the postcorrection. For S2L-equations, experiments were conducted using Qwen2.5 and Qwen2.5-Math across English, Russian, and combined splits. All LLMs received the ASR transcription as input and produced LaTeX equations or sentences as output. For S2L-sentences, we fine-tuned the Qwen2.5 (0.5B, 1.5B, and 7B) and Qwen2.5-Math-1.5 instruct models using 3 training splits: artificial, human-annotated, and mixed. To assess few-shot performance, we tested the same models using 5-shot prompt format, evaluating generalization across parameter sizes."
        },
        {
            "title": "4.2 Multimodal Models\nA multimodal S2L pipeline was further explored using\nAudio-LLMs. This approach bypasses phonetic transcrip-\ntion by directly converting raw audio into LaTeX expres-\nsions or sentences. Audio encoders first extract latent fea-\ntures from the waveform; these are then processed by a\nmodality adapter to align with LLM token embeddings.",
            "content": "Table 4: S2L-Equations results. Disjoint split: test equations do not overlap with train equations. A: artificially (TTS) annotated audio except 400k samples extracted from MathBridge; H: human-annotated audio; Mix: combination of and H; CER is calculated for lower-case. Q-αB and Q-math-αB stand for Qwen2.5-αB-instruct and Qwen2.5-math-αBinstruct, respectively. Full implies addition of 400k artificially annotated samples from MathBridge to the set. Model Train Train Test Test: Mix Test: Test: Language Language CER TeXBLEU CER TeXBLEU CER TeXBLEU MS-train H Mix Mix Mix-full H Mix Mix Mix-full MathSpeech Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-Math-1.5B Q-Math-1.5B SALMONN-13B Mix-full Mix-full Gemma-3n-8B Mix Flamingo-3-8B Eng Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng+Rus Eng+Rus Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng 64.04 33.28 34.78 36.91 35.43 31.41 32.33 27.21 31.24 30.73 29.69 30.93 29.76 31.14 25.69 29.57 31.45 17.50 34.24 23.25 83.71 88.61 87.90 87.86 88.06 88.83 88.60 90.20 89.22 88.92 89.41 89.04 89.28 89.37 90.70 90.00 89.25 93.68 89.15 91.32 59.32 33.26 34.94 35.01 33.94 31.06 31.18 27.03 31.37 30.70 27.57 28.85 28.93 30.08 24.91 29.44 30.71 18.17 33.24 23.13 83.64 88.54 87.57 88.25 88.47 88.87 88.89 90.14 89.15 88.73 89.69 89.42 89.44 89.43 90.74 89.80 89.43 93.64 89.23 91. 69.65 33.30 34.59 39.16 37.19 31.82 33.69 27.42 31.08 30.77 32.22 33.39 30.74 32.40 26.61 29.74 32.34 16.70 35.42 23.40 83.80 88.70 88.31 87.38 87.56 88.78 88.24 90.27 89.31 89.16 89.07 88.57 89.09 89.28 90.66 90.23 89.02 93.72 89.06 91.33 The resulting audio tokens are concatenated with the textual prompts tokens and passed to the LLM for decoding. We used Qwen-Audio, Gemma-3n (Team et al. 2025), Audio Flamingo-3 (Goel et al. 2025), and SALMONN-13B for this setting, given their strong benchmark performance. LLMs were fine-tuned using the LoRA technique (Hu et al. 2022), while freezing audio encoders and the adapter. Since the Qwen-Audio fine-tuning pipeline was not publicly available, we prepared it ourselves."
        },
        {
            "title": "4.3 Evaluation",
            "content": "There are cases where semantically equivalent LaTeX formulas differ in syntax, which can distort formal metrics based on raw code. For instance, the expressions int_{a}ˆ{b} f(x) dx and int_aˆbf(x)dx represent the same formula but have high CER. Additionally, capitalization (e.g., phi vs. Phi) and font styles (e.g., mathcal{R} vs. r) introduce further ambiguities. To mitigate these effects, we apply equation normalization as previously described. Additionally, all metrics are evaluated on lowercase text except for TeXBLEU. For S2L-equations, predictions and ground truth are compared in LaTeX form, as illustrated in Table 5. For S2L-sentences, which contain inline formulas within English text, we separately evaluate both equation and text components. All formulas are extracted from the predicted sequences, concatenated, and compared against the reference formulas using character-level metrics. The primary reported metrics are character error rate (CER), and TeXBLEU (Jung et al. 2025) metric, recently specifically proposed for LaTeX comparison."
        },
        {
            "title": "5.1 S2L-equations Results\nTable 4 compares the performance of post-ASR and multi-\nmodal S2L models on the English S2L-equations test\nsubset. Due to the limited space, the complete Table 10 with\nRussian test and additional splits is moved to the Appendix.\nThe key observations are the following:",
            "content": "Table 5: SALMONN-13B prediction examples on S2L-equations, test subset. Prediction Ground Truth CER, % Pronunciation Fµν = µAν ν Aµ Fµν = µAν ν Aµ = n(µ, σ2, t) Var(X) = 1p p2 = N(cid:0)µ, σ2 (cid:1) Var(X) = r(1p) p2 n(γ, θe)/n = δ(θe θj) n(Γ, θe)/n = δ(θe θj) Ei(x) = 1 π 0 cos(cid:0) t3 (cid:82) 3 + xt(cid:1) dt Ai(x) = 1 π 0 cos(cid:0) t3 (cid:82) 3 + xt(cid:1) dt limx5 4x3 x+5 limx5 4x3 x+ (cid:80)n i=1 = n(n+1)(2n+1) 6 (cid:80)n i=1 = n(n+1)(2n+1) 1 u1, u2, b1, v2 1 u1, u2, v1, v2 Models can benefit from multilingual training, but not always. For example, Human English Test CER is 33.94% vs 35.01% for Human-annotated Eng and Eng+Rus train, respectively (Qwen2.5-0.5B). However, for Qwen2.5Math, in contrast, results are 30.71% vs 28.08 %. For all English experiments, the addition of new 400k English TTS samples improves metrics. However, in the case of the Russian test, the results worsened, probably due to the resulting language imbalance. SALMONN achieves superior results compared to other models. Qwen-Audio fails completely, likely due to re-implementation discrepancies. Flamingo-3 performs only on par with smaller post-correction LMs. Gemma performs below even small post-correction LMs. 1.5B models consistently outperform their 0.5B counterparts, as expected. However, 7B models underperform compared to 1.5B models. This discrepancy can be attributed to different training methodologies: the smaller models were fully fine-tuned, while 7B models were trained using LoRA with frozen base weights, which likely constrained their learning capacity. Math-oriented Qwen2.5-Math-1.5B does not perform significantly better than ordinary (Qwen2.5-1.5B) competitor. This is probably due to the specificity of the input, which is presented in lexically as natural language, not through mathematical expressions. We conducted experiments by adding frequently used LaTeX symbols, such as {, }, ˆ, _ as additional tokens not presented in default tokenizer separately. However, this modification did not result in any measurable improvement in model performance. The KaTeX compilation success rate of predicted equations varies from 98% to 99.5%, and failure cases mainly include bracket issues. In summary, despite large nominal CER, the metrics do not reflect the actual situation due to the 0.0 54.5 57.4 13. 1.1 0.6 10.0 0.0 1.3 The field strength tensor for electromagnetism is mu nu equals mu nu minus nu mu. An electric field equals force over charge. of mu, sigma squared over T. For negative binomial distribution, the variance equals times 1 minus divided by squared. of Gamma, theta sub divided by equals delta of theta sub minus theta sub j. Airy function of the first kind, ai of is equal to one divided by pi times the integral from zero to infinity of cosine of cubed divided by three plus times dt. Limit as tends to negative 5 of square root of 4 minus minus 3 divided by plus 5. The sum from equals one to of times equals times plus one times two plus one divided by six. 1 is less than or equal to sub 1, sub 2, sub 1, sub 2, which are all less than or equal to d. considerable ambiguity of possible pronunciation and transcriptions, and our models demonstrate satisfactory quality. For instance, one can assess the generation quality of SALMONN in Table 5. Comparison with MathSpeech. We evaluated our approaches on the MathSpeech benchmark and the MathSpeech model on our S2L-equations test. MathSpeech model tends to use operators that do not affect like displaystyle, the semantics of the equation, operatorname. In contrast, our dataset lacks them, and consequently, our models also lack them. Thus, for fairer evaluation, we additionally normalized predictions and labels of our and MathSpeech models and datasets. This improved MathSpeechs metric on S2L-equations from 92% to 64%, however, it is still drastically worse than our models demonstrate (27.2%) while having just slightly better CER (27.7% vs. 30.0%) on MathSpeech benchmark, as shown in Table 6. We should note that MathSpeech has only 120M parameters, but it was trained on 6-8 million samples. In contrast, our model has 0.5B parameters but was tuned on 550k samples. 6: Comparison with MathSpeech the Table MathSpeech benchmark and S2L-equations (English test). Metric: CER. Qwen: Qwen2.5-0.5B-Instruct (multilingual). SALMONN was tuned only in English. on Model MathSpeech S2L-equations MathSpeech Qwen SALMONN 27.7% 30.0% 27.7% 64.0% 27.2% 17.5% Table 7: S2L-Sentences results. Disjoint split: test sentences do not overlap with train sentences. A: artificially (TTS) annotated audio; H: human-annotated audio; Mix: combination of and H. CER is calculated for lower-case. Q-αB and Q-math-αB stand for Qwen2.5-αB-instruct and Qwen2.5-math-αB-instruct, respectively. Sent. stands for sentence: metric calculated over the whole sentence; Eq: only for the embedded equations; Text: only for the text parts of the sentence. Model Train Test: Test: Mix Q-0.5B Q-0.5B Q-0.5B Q-0.5B (25 shots) Q-1.5B Q-1.5B Mix Q-1.5B Q-1.5B (5 shots) Q-1.5B (25 shots) Q-math-1.5B Q-math-1.5B Mix Q-math-1.5B Q-7B (LoRa) Q-7B (LoRa) Mix Q-7B (LoRa) Q-7B (5 shots) Q-7B (25 shots) SALMONN-13B SALMONN-13B SALMONN-13B Mix CER Text 28.23 23.13 28.83 27.30 27.91 20.69 28.27 20.50 17.26 23.56 18.80 26.79 13.52 14.67 12.36 17.56 14.23 15.05 10.09 9.57 Sent. 33.74 29.18 32.50 30.67 33.28 25.96 33.57 27.94 24.05 29.49 23.78 32.16 20.11 20.72 18.75 24.19 20.00 23.00 16.03 15. Eq. 63.44 56.93 54.07 63.44 59.34 53.13 58.41 63.99 56.77 54.72 45.48 57.83 47.12 46.10 43.75 57.91 47.12 57.69 41.53 39.68 TeXBLEU CER TeXBLEU Eq. Sent. Text Eq. 82.99 83.22 83.90 73.93 84.37 83.79 84.58 76.78 78.57 85.05 85.34 84.36 85.90 84.12 85.46 77.97 80.64 82.62 84.62 85.76 34.34 32.69 32.75 31.47 32.32 27.43 33.95 28.70 24.49 30.35 27.01 32.40 20.54 22.55 19.09 23.44 20.73 17.89 19.68 16.78 28.89 27.05 29.29 27.37 27.59 22.36 29.57 23.69 18.93 24.24 22.25 27.36 15.09 16.66 13.80 18.76 16.38 10.99 13.60 10. 67.38 63.48 53.47 72.45 56.88 55.57 58.07 64.74 57.61 58.48 51.32 59.56 45.92 50.06 43.04 55.88 48.21 49.92 49.69 44.96 Eq. 79.58 78.55 79.89 66.36 80.09 78.90 80.29 70.83 73.43 80.18 79.67 79.88 81.81 79.99 81.49 73.31 75.14 80.31 79.34 81."
        },
        {
            "title": "5.3 Discussion and Limitations\nTo bridge the gap with practical applications, we incorpo-\nrated mathematical sentences (S2L-sentences) and hu-\nman annotation. However, our data does not fully capture\nreal-world lecture conditions, where equations may be para-\nphrased, incomplete, or tied to visual content. Addressing\nthis would require costly, fine-grained annotation of lecture",
            "content": "recordings, which remains out of scope. Our work has several limitations. While S2L results are promising, they remain limited in scope and robustness. Post-processing LLMs often fail when ASR transcriptions are vague, and similarly, Audio-LLMs struggle with unfamiliar audio domains. More diverse training data is likely needed. Additionally, while synthetic data is helpful for augmentation, it remains less effective than human speech due to its lower complexity and variability."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduced S2L, a novel large-scale open-\nsource dataset for Speech-to-LaTeX conversion, consisting\nof 66k human-annotated and 571k TTS-generated audio\nsamples of equations and sentences in English and Russian.\nOur data collection pipeline is openly described and can\nsupport future efforts in speech-driven mathematical under-\nstanding. We proposed and evaluated multiple approaches,\nincluding ASR post-correction and multimodal end-to-end\nmodels. Our models achieved competitive results, outper-\nforming prior work and highlighting the feasibility of S2L\nconversion when supported by high-quality data. We also\ndemonstrated that handling equations embedded within nat-\nural language is substantially more challenging than con-\nverting isolated equations. Future work might be devoted\nto enhancing the dataset with more comprehensive human-\nannotated real-world data, such as lecture recordings, and\nimproving the conversion quality, with the possible applica-\ntion of audio-visual methods.",
            "content": "References Afouras, T.; Chung, J. S.; and Zisserman, A. 2018. LRS3TED: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496. Amodei, D.; et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In International conference on machine learning, 173182. PMLR. Azerbayev, Z.; Schoelkopf, H.; Paster, K.; Santos, M. D.; McAleer, S.; Jiang, A. Q.; Deng, J.; Biderman, S.; and Welleck, S. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631. Baevski, A.; Zhou, H.; rahman Mohamed, A.; and Auli, M. 2020. wav2vec 2.0: Framework for SelfSupervised Learning of Speech Representations. ArXiv, abs/2006.11477. Barabash, K.; Demaine, E.; Randall, R.; et al. 2025. KaTeX is fast, easy-to-use JavaScript library for TeX math rendering on the web. https://github.com/Khan/KaTeX/. Accessed: 2025-06-15. Batlouni, S. N.; Karaki, H. S.; Zaraket, F. A.; and Karameh, F. N. 2011. MathifierSpeech recognition of math equations. In 2011 18th IEEE International Conference on Electronics, Circuits, and Systems, 301304. IEEE. Blecher, L.; Cucurull, G.; Scialom, T.; and Stojnic, R. 2023. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418. Casanova, E.; Davis, K.; Golge, E.; Goknar, G.; Gulea, I.; Hart, L.; Aljafari, A.; Meyer, J.; Morais, R.; Olayemi, S.; et al. 2024. XTTS: Massively Multilingual Zero-Shot Textto-Speech Model. arXiv preprint arXiv:2406.04904. Chen, C.; Hu, Y.; Yang, C.-H. H.; Siniscalchi, S. M.; Chen, P.-Y.; and Chng, E.-S. 2023. Hyporadise: An open baseline for generative speech recognition with large language models. Advances in Neural Information Processing Systems, 36: 3166531688. Chen, S.; Wang, C.; Chen, Z.; Wu, Y.; Liu, S.; Chen, Z.; Li, J.; Kanda, N.; Yoshioka, T.; Xiao, X.; et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6): 15051518. Chu, Y.; Xu, J.; Zhou, X.; Yang, Q.; Zhang, S.; Yan, Z.; Zhou, C.; and Zhou, J. 2023. Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale AudioLanguage Models. arXiv preprint arXiv:2311.07919. Devlin, J. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Ganeeva, V.; Sakhovskiy, A.; Khrabrov, K.; Savchenko, A.; Kadurin, A.; and Tutubalina, E. 2024. Lost in Translation: Chemical Language Models and the Misunderstanding of In Findings of the Association for Molecule Structures. Computational Linguistics: EMNLP 2024, 1299413013. Genthial, G. 2024. im2latex: Converting Images to LaTeX. https://github.com/guillaumegenthial/im2latex. Goel, A.; Ghosh, S.; Kim, J.; Kumar, S.; Kong, Z.; Lee, S.- g.; Yang, C.-H. H.; Duraiswami, R.; Manocha, D.; Valle, R.; et al. 2025. Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models. arXiv preprint arXiv:2507.08128. Graves, A.; Fernandez, S.; Gomez, F.; and Schmidhuber, J. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, 369376. Gulati, A.; Qin, J.; Chiu, C.-C.; Parmar, N.; Zhang, Y.; Yu, J.; Han, W.; Wang, S.; Zhang, Z.; Wu, Y.; and Pang, R. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. ArXiv, abs/2005.08100. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2): 3. Hyeon, S.; Jung, K.; Kim, N.-J.; Ryu, H. G.; and Do, J. 2025a. Mathreader: Text-to-speech for mathematical documents. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Hyeon, S.; Jung, K.; Won, J.; Kim, N.-J.; Ryu, H. G.; Lee, H.-J.; and Do, J. 2025b. Mathspeech: Leveraging small lms for accurate conversion in mathematical speech-to-formula. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2419424202. Jung, K.; Hyeon, S.; Kwon, J. Y.; Kim, N.-J.; Ryu, H. G.; Lee, H.-J.; and Do, J. 2024. MathBridge: Large Corpus Dataset for Translating Spoken Mathematical Expressions arXiv into LaT eX Formulas for Improved Readability. preprint arXiv:2408.07081. Jung, K.; Kim, N.-J.; Ryu, H. G.; Hyeon, S.; Lee, S.-j.; and Lee, H.-J. 2025. Texbleu: Automatic metric for evalIn ICASSP 2025-2025 IEEE Internauate latex format. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Kong, J.; Kim, J.; and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33: 1702217033. Li, H.; Zhang, J.; Liu, H.; Fan, J.; Zhang, X.; Zhu, J.; Wei, R.; Pan, H.; Li, C.; and Chen, H. 2024. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3): 128. Li, J.; Li, D.; Savarese, S.; and Hoi, S. C. H. 2023. BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning. Loshchilov, I.; Hutter, F.; et al. 2017. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5. Ma, R.; Gales, M. J.; Knill, K. M.; and Qian, M. 2023. Nbest t5: Robust asr error correction using multiple input hypotheses and constrained decoding space. arXiv preprint arXiv:2303.00456. Ma, R.; Qian, M.; Gales, M.; and Knill, K. 2025. Asr error correction using large language models. IEEE Transactions on Audio, Speech and Language Processing. OleehyO. 2024. TexTeller: Convert Handwritten Text to LaTeX. https://github.com/OleehyO/TexTeller. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: method for automatic evaluation of machine transIn Proceedings of the 40th annual meeting of the lation. Association for Computational Linguistics, 311318. Paster, K.; Santos, M. D.; Azerbayev, Z.; and Ba, J. 2023. OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text. arXiv preprint arXiv: 2310.06786. Post, M. 2018. call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771. Radford, A.; Kim, J. W.; Xu, T.; Brockman, G.; McLeavey, C.; and Sutskever, I. 2023. Robust speech recognition via In International conference large-scale weak supervision. on machine learning, 2849228518. PMLR. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140): 167. Roychowdhury, S.; Ranjani, H.; Soman, S.; Paul, N.; Bandyopadhyay, S.; and Iyengar, S. 2025. Intelligibility of Textto-Speech Systems for Mathematical Expressions. arXiv preprint arXiv:2506.11086. Tang, C.; Yu, W.; Sun, G.; Chen, X.; Tan, T.; Li, W.; Lu, L.; MA, Z.; and Zhang, C. 2024. SALMONN: Towards Generic Hearing Abilities for Large Language Models. In The Twelfth International Conference on Learning Representations. Team, G.; Kamath, A.; Ferret, J.; Pathak, S.; Vieillard, N.; Merhej, R.; Perrin, S.; Matejovicova, T.; Rame, A.; Rivi`ere, M.; et al. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Weber, M.; Fu, D. Y.; Anthony, Q.; Oren, Y.; Adams, S.; Alexandrov, A.; Lyu, X.; Nguyen, H.; Yao, X.; Adams, V.; Athiwaratkun, B.; Chalamala, R.; Chen, K.; Ryabinin, M.; Dao, T.; Liang, P.; Re, C.; Rish, I.; and Zhang, C. 2024. RedPajama: an Open Dataset for Training Large Language Models. NeurIPS Datasets and Benchmarks Track. Wei, C.; Wang, B.; Kim, J.-j.; and Chen, N. F. 2025. Towards Spoken Mathematical Reasoning: Benchmarking Speechbased Models over Multi-faceted Math Problems. arXiv preprint arXiv:2505.15000. Yang, A.; Zhang, B.; Hui, B.; Gao, B.; Yu, B.; Li, C.; Liu, D.; Tu, J.; Zhou, J.; Lin, J.; et al. 2024. Qwen2. 5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. arXiv preprint arXiv:2409.12122. Ying, H.; Zhang, S.; Li, L.; Zhou, Z.; Shao, Y.; Fei, Z.; Ma, Y.; Hong, J.; Liu, K.; Wang, Z.; et al. 2024. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332. This supplementary material describes the dataset examples and presents additional results."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Dataset Example An overview of the dataset collection pipeline is presented in Figure 2. Figure 2: S2L-equations collection and annotation pipeline overview. Let us compare transcriptions of 5 ASR models for one particular human-annotated audio in Table 8. Table 8: Example of transcription of one particular human-annotated audio of the νAµ = Aµ xν + Γµ νρAρ equation. Model Transcription Whisper-L WavLM Wav2Vec Qwen-audio Canary The covariant derivative of vector mu equals partial mu with respect to nu plus gamma upper rho mu nu times rho. the covarient derivative of vector mou equals partial moo with respect to ex new plus gama upper row moo new times row the covariant derivative of vector mu equals partial mo with respect to x-new plus scamma upper row mo new times row The covariant derivative of vector mu equals partial mu with respect to nu plus gamma upper row mu nu times row The covariant derivative of vector amu equals partial amu with respect to nu plus gamma upper rho moon nu times rho. Let us present several English samples we collected using GPT-4 requests in Table 9. The Posssible Pronounciation is necessary for the TTS models to generate speech and is extremely helpful for the human speech annotators as they can use it for reference if they do not know how to read the equation properly and simplifies the criteria for the human annotator. For the S2L-sentences, let us illustrate the evaluation challenge. Consider the CER between the predicted sequence Given fixed graph , typical problem on large graph on vertices that contains no copy of can have an upper bound on the number of its edges, denoted by X(n, ) and the ground-truth Given fixed graph , typical problem in extremal graph theory asks for the maximum number of edges that large graph on vertices containing no copy of can have, denoted by ex(n, ). The equation-only CER is 27.27%. A.2 Metrics Description We proceed by examining the primary and additional metrics in detail. Table 9: Example of the dataset samples for further annotation by speaker and TTS models. Topic Possible Pronunciation Equation Calculus. Integrals Integral: integral of cubed dx equals to the fourth over 4 plus constant (cid:82) x3 dx = 4 + C"
        },
        {
            "title": "Differential Equations",
            "content": "Field Theory Quantum Mechanics the distance between two points (x1, y1) and (x2, y2) is the square root of (x2 minus x1) squared plus (y2 minus y1) squared = (cid:112)(x2 x1)2 + (y2 y1)2 of is equal to minus 3 divided by squared minus 9 (x) = x3 x"
        },
        {
            "title": "The partial derivative of f with respect to x\nand then y equals d squared f divided by d x\nd y",
            "content": "the cross product of vectors and is vector perpendicular to both 2f xy the solution to over equals negative is equals to the negative the electromagnetic field tensor is given by mu nu equals partial mu nu minus partial nu mu the Schrodinger equation for free particle is bar psi over equals minus bar squared over 2 squared psi over squared dy dx = ky is = Cekx Fµν = µAν νAµ iℏ dψ dt = ℏ2 2m d2ψ dx2 QFT Particle Physics General Physics the Lagrangian density for the gauge field is minus one over four mu nu mu nu = 4 FµνF µν the mass of the boson is approximately 91.2 GeV/c squared mZ 91.2 GeV/c2 Period of pendulum: two pi times square root of length divided by gravitational acceleration = 2π (cid:113) Mathematical Physics Bessel function of the first kind of order zero, sub zero is equal to the sum from equals zero to infinity, of minus one to the power m, divided by factorial squared, times divided by two to the power of 2 Eulers formula, to the power times pi plus one equals zero Trigonometry J0(x) = (cid:80) m= (1)m (m!)2 (cid:0) 2 (cid:1)2m eiπ + 1 = 0 Thermodynamics Gibbs free energy, equals minus TS = Character Error Rate (CER) which is defined as the ratio of the normalized edit distance (Levenshtein distance) between the predicted sequence and the ground truth, normalized by the total number of characters in the reference: CER = + + , (1) where is the number of substitutions, is the number of deletions, is the number of insertions, and is the total number of characters in the reference. The Word Error Rate (WER) is defined similarly to the CER but considers words instead of characters. CER and WER are commonly used in ASR tasks. ROUGE-1 calculates the unigram recall between the predicted output and the reference text. ROUGE-1 = (cid:80) unigramref min(count(unigram), count(unigram pred)) unigramref count(unigram) (cid:80) (2) This metric is widely used for summarization and transcription tasks to evaluate the lexical overlap between predicted and reference outputs. BLEU and sacreBLEU evaluate n-gram precision by comparing the predicted output against the reference. BLEU is computed as: BLEU = BP exp (cid:33) wn log pn (cid:32) (cid:88) n= (3) where BP is the brevity penalty, pn is the precision of n-grams, and wn are weights. SacreBLEU applies different tokenization (Papineni et al. 2002; Post 2018). TeXBLEU is variant of the BLEU score adapted to evaluate LaTeX string generation tasks, particularly mathematical expressions. It penalizes syntactic mistakes and helps measure the quality of generated LaTeX code. chrF and chrF++ are character-based F-scores metrics that compute balance between precision and recall at the character level: chrFβ = (1 + β2) chrP chrR β2 chrP + chrR , (4) Where chrP and chrR represent the arithmetic mean of character n-gram precision and recall across all n-grams; chrP is the percentage of character n-grams in the hypothesis that also appear in the reference, and chrR is the percentage of character n-grams in the reference that are also found in the hypothesis. chrF++ is chrF for = 2. TeXBleu is relatively insensitive to the significant errors. Training Hyperparameteres The default loss function was cross-entropy, and the default optimizer was AdamW (Loshchilov, Hutter et al. 2017). Qwen models for S2L-Equations experiments were trained on 1 A100 GPUs for 1 epoch, and batch size was set to 16 samples per batch. AdamW optimizer was used with weight decay of 0.01 with learning rate 1e 4 and linear learning rate scheduler. For S2L-Sentences experiments, Qwen models were trained on single A100 GPU for 1 epoch, and the batch size was set to 16 samples per batch. AdamW optimizer was used with weight decay of 0.01 with learning rate 1e 4 and linear learning rate scheduler. SALMONN was trained with the LoRA technique on Llama. Target modules were set to attention layers, rank was 8, alpha parameter was 32, and dropout was set to 10%. Whisper and Beats models were frozen. The model was trained on Nvidia H100-80Gb 2 GPUs for 6 epochs. The learning rate was set to 3e-5 with warm-up for 3000 steps and cosine decay. Gradient accumulation was set to 3 iterations. The batch size was set to 12 samples per batch. Automated mixed precision with float16 was used. For the few-shot learning, the following system prompt and examples were used: 1 2 system_prompt = \"\"\"You are mathematics text processing assistant. Your task is to convert informal mathematical expressions into proper LaTeX format while keeping all other text unchanged. Maintain the original structure and wording, only modifying mathematical notation. Respond only with the processed text, no additional commentary. \"\"\" 3 4 % few_shot_examples = [ 5 {user: Our first result is an explicit description of sub Lin in the case where equals square bracket delta is the Stanley-Reisner ring of simplicial complex delta., 6 7 9 10 11 12 assistant: Our first result is an explicit description of $Fblin$ in the case where $M = Bbbk[Delta]$ is the Stanley-Reisner ring of simplicial complex $ Delta$.}, {user: The results show the cystinatic improvability with increasing sub Ln raised to G., assistant: The results show the systematic improvability with increasing $n_ mathrm{mom}ˆG$.}, {user: or any divides and denote by or superscript of the set of orientations with isotropy group isomorphic to the cyclic group of order i.e., assistant: For any $d mid m$ denote by $Orˆ{d}(L)$ the set of orientations with isotropy group isomorphic to $mathbb{Z}_d$, i.e.}, 13 14 15 16 17 18 19 ] {user: Hence, see what is subscript of and minus two wins by the tie-breaking., assistant: Hence, $c_{n-2}$ wins by the tie breaking.}, {user: Let from to be amorphous between smooth varieties over field K., assistant: Let $f: Ylongrightarrow X$ be morphism between smooth varieties over $k$.}"
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Additional Results for the Main Text Tables Full version of the Table 4 from the main text for S2L-equations results is Table 10. Additional few-show results for the S2L-sentences  (Table 7)  are presented in Table 11. C.2 Additional Models We also evaluated InternLM, ProofGPT, and FlanT5 on the subsets of S2L-equations on additional experiments with different splits. Results are presented in Table 12. ProofGPT-1.3B demonstrated good performance, except for the Russian language. In the setting when train and test data both have mix of genuine and artificial audio, and the test set equations have no overlapping with the equations from the train, SALMONN-13B demonstrates the best metrics except CER on all languages, while Qwen2.5 has slight edge over SALMONN regarding CER. For instance, on the English subset, SALMONN leads with the highest Rouge-1 (83.88), sBLEU (60.68), and chrF (71.04) scores. However, its CER (42.42) is slightly higher than Qwen2.5-Math-1.5B, which has the lowest CER (39.54) and ranks second in Rouge-1 (81.43) and chrF (68.34). The QwenAudio performs worse than other methods, probably due to re-implementation nuances. The second part of the table compares Qwen2-0.5B and Qwen2.5-0.5B for English and Russian languages for the random and disjoint (test equations do not overlap train ones) splits. For both languages, Qwen2.5-0.5B consistently outperforms Qwen2-0.5B in terms of Rouge-1 and sBLEU. Interestingly, in the case of the combined English and Russian datasets, the 2 models exhibit very close performance, with Qwen2.5-0.5B showing marginal improvements in accuracy metrics while having slightly higher CER. C.3 Rest of the metrics We tried to train LLM with pronunciations from all 5 ASR systems from Table 8 to make it an ASR-agnostic model, but the models accuracy was worth more than just with Whisper. For results see Table 13. We measured case-sensitive performance (for example, ϕ and Φ mean different symbols). Results are presented in Tables 14 and 15. As we can see, the performance drop is not as severe. This generally means that models were trained well and that data regarding capitalized and non-capitalized symbols was labelled well. The rest of the metrics from the Table 12 are represented in Table 16 as an addition with the lower-cased metrics for the S2L-eqautions part. C.4 Cross-Language Learning. One of the advantages of fine-tuning multilingual language models is the ability to extract information from one language that is not available in another. For example, LaTeX special symbols simeq and hat are not presented in the Russian part of the equations dataset but in English. Qwen2.5, trained in English and Russian, can transcribe approximately equal in Russian to simeq (). Another observation is that the models are primarily English-oriented, so Qwen2.5-Math-1.5B and Qwen2-0.5B trained in Russian can generate only simple formulas in English. The reverse situation works worse - Qwen2.5-0.5B, trained in English, cannot perform post-correction in Russian. Table 10: S2L-equations results. Disjoint split: test equations do not overlap with train equations. A: artificially (TTS) annotated audio except 400k samples extracted from MathBridge; H: human-annotated audio; Mix: combination of and H; CER is calculated for lower-case. Q-αB and Q-math-αB stand for Qwen2.5-αB-instruct and Qwen2.5-math-αBinstruct, respectively. Full implies addition of 400k artificially annotated samples from MathBridge to the set. Model Train Train Test Test: Mix Test: Test: Language Language CER TeXBLEU CER TeXBLEU CER TeXBLEU MS-train H Mix Mix Mix-full H Mix Mix Mix-full MathSpeech Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Mix Q-math-1.5B Mix Q-math-1.5B Mix-full Q-7B Q-7B Q-7B Q-7B Q-7B Q-7B Q-7B Qwen-Audio Mix SALMONN Mix-full Mix-full Gemma 3n H Mix Mix Mix-full H Mix Mix Mix-full H Mix Mix Mix-full Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-0.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Q-math-1.5B Mix Q-math-1.5B Mix Q-math-1.5B Mix-full Q-7B Q-7B Q-7B Q-7B Q-7B Q-7B Q-7B Flamingo 3 SALMONN Mix-full Mix-full Gemma 3n H Mix Mix Mix-full Mix Eng Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng Eng+Rus Eng+Rus Eng Eng Eng Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Rus Eng+Rus Eng+Rus Rus Rus Rus Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Eng Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus Rus 64.04 33.28 34.78 36.91 35.43 31.41 32.33 27.21 31.24 30.73 29.69 30.93 29.76 31.14 25.69 29.44 29.57 30.16 31.45 28.53 27.75 25.01 28.15 27.32 27.97 26.89 26.10 27.78 26.17 71.67 17.50 34.24 32.06 9.63 15.24 6.77 15.34 13.02 8.15 10.05 6.14 4.66 14.50 14.60 4.02 4.55 6.03 11.01 13.23 12.49 5.50 17.25 13.33 3.70 6.14 5.40 21.59 1.59 4.66 7.94 2.01 9.38 15.30 83.71 88.61 87.90 87.86 88.06 88.83 88.60 90.20 89.22 88.92 89.41 89.04 89.28 89.37 90.70 89.61 90.00 89.83 89.25 89.97 89.85 90.90 90.10 90.12 89.99 90.18 90.58 90.11 90.50 83.55 93.68 89.15 91.73 95.14 96.97 97.74 98.50 97.32 97.03 96.77 96.62 99.38 97.38 95.90 99.20 98.89 98.94 98.04 96.51 97.45 98.90 97.24 97.86 99.19 98.39 99.29 96.73 99.57 99.09 97.74 99.88 97.73 97. 59.32 33.26 34.94 35.01 33.94 31.06 31.18 27.03 31.37 30.70 27.57 28.85 28.93 30.08 24.91 30.00 29.44 28.97 30.71 28.08 27.54 25.05 28.07 26.16 26.93 26.43 25.80 26.55 25.96 104.19 18.17 33.24 40.95 17.78 15.87 13.97 15.24 14.60 15.56 19.37 14.60 8.25 14.60 10.79 11.75 13.33 17.14 13.33 2.86 11.75 13.65 12.70 14.29 10.79 16.83 6.98 14.92 4.44 13.65 14.92 0.00 6.51 11.26 83.64 88.54 87.57 88.25 88.47 88.87 88.89 90.14 89.15 88.73 89.69 89.42 89.44 89.43 90.74 89.33 89.80 90.13 89.43 90.13 89.89 90.90 89.96 90.20 90.29 90.36 90.68 90.37 90.51 78.45 93.64 89.23 94.13 96.34 96.76 97.20 97.21 96.78 96.68 96.60 96.74 99.49 96.67 98.00 97.68 96.74 97.02 97.08 99.36 97.78 97.39 97.70 96.72 97.66 96.46 99.33 97.00 98.78 97.36 96.87 100.00 99.55 98.17 69.65 33.30 34.59 39.16 37.19 31.82 33.69 27.42 31.08 30.77 32.22 33.39 30.74 32.40 26.61 28.77 29.74 31.58 32.34 29.05 28.01 24.97 28.25 28.70 29.20 27.44 26.46 29.24 26.43 33.06 16.70 35.42 27.62 5.56 14.92 3.17 15.40 12.22 4.44 5.40 1.90 2.86 14.44 16.51 0.16 0.16 0.48 9.84 18.41 12.86 1.43 19.52 12.86 0.16 0.79 4.60 24.92 0.16 0.16 4.44 3.02 10.81 17. 83.80 88.70 88.31 87.38 87.56 88.78 88.24 90.27 89.31 89.16 89.07 88.57 89.09 89.28 90.66 89.96 90.23 89.46 89.02 89.76 89.79 90.89 90.26 90.03 89.62 89.95 90.45 89.78 90.48 89.84 93.72 89.06 90.53 94.54 97.08 98.01 99.14 97.59 97.21 96.85 96.56 99.33 97.73 94.85 99.96 99.96 99.89 98.52 95.08 97.28 99.66 97.01 98.43 99.96 99.35 99.27 96.60 99.96 99.96 98.17 99.82 96.82 94.48 Table 11: S2L-sentences results for Few-Shot experiments. Disjoint split: test sentences do not overlap with train sentences. A: artificially (TTS) annotated audio; H: human-annotated audio; Mix: combination of and H. CER is calculated for lower-case. Q-αB and Q-math-αB stand for Qwen2.5-αB-instruct and Qwen2.5-math-αB-instruct, respectively. Sent. stands for sentence: metric calculated over the whole sentence; Eq: only for the embedded equations; Text: only for the text parts of the sentence. Model Train Test: Test: CER TeXBLEU CER TeXBLEU Sent. Text Eq. Eq. Sent. Text Eq. 5-shot Q-0.5B Q-0.5B Q-1.5B Q-1.5B Q-math-1.5B Q-math-1.5B Q-7B Q-7B 25-shot Q-0.5B Q-0.5B Q-1.5B Q-1.5B Q-math-1.5B Q-math-1.5B Q-7B Q-7B 35.80 32.09 26.16 27.94 35.94 42.52 23.83 24.19 28.74 30.67 23.65 24.05 37.65 30.58 21.22 20.00 35.76 29.69 20.62 20.50 35.99 42.16 18.44 17.56 25.97 27.30 17.55 17.26 29.11 24.43 15.85 14. 66.22 71.95 58.78 63.99 62.73 73.36 56.25 57.91 61.87 63.44 56.84 56.77 88.22 67.93 50.43 47.12 66.87 65.31 76.84 76.78 71.66 68.23 77.88 77.97 73.77 73.93 78.42 78.57 75.14 75.67 79.88 80.64 34.21 31.83 26.64 28.70 41.90 44.63 23.63 23.44 28.15 31.47 24.10 24.49 36.74 31.26 21.65 20. 34.20 30.16 22.26 23.69 43.85 45.15 19.57 18.76 25.89 27.37 18.82 18.93 27.56 27.51 16.97 16.38 68.30 73.71 60.47 64.74 65.75 78.05 56.03 55.88 65.14 72.45 58.77 57.61 95.89 67.43 51.97 48.21 Eq. 60.41 58.47 70.64 70.83 64.53 61.07 72.64 73. 66.53 66.36 72.39 73.43 69.09 68.44 74.65 75.14 Table 12: S2L-equations (subset) results. SALMONN represent end-to-end Audio-LLMs, while all other models use ASR post-correction via fine-tuned LLMs. denotes artificially (TTS) annotated audio, refers to human-annotated audio, and Mix indicates combination of both. Rand indicates random split where equation-pronunciation-speaker/voice triplets are non-overlapping across train, validation, and test sets. Disj specifies disjoint split where test equations do not appear in the training set. Model Lang Train Test Split CER Rouge-1 sBLEU chrF Qwen2.5-0.5B Qwen2.5-Math-1.5B ProofGPT-1.3B InternLM2-1.8B Flan-T5 SALMONN-13B Qwen2.5-0.5B Qwen2.5-Math-1.5B ProofGPT-1.3B SALMONN-13B Eng Eng Eng Eng Eng Eng Rus Rus Rus Rus Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Qwen2.5-0.5B ProofGPT-1.3B SALMONN-13B Eng+Rus Mix Mix Eng+Rus Mix Mix Eng+Rus Mix Mix Qwen2-0.5B Qwen2.5-0.5B Qwen2-0.5B Qwen2.5-0.5B Qwen2-0.5B Qwen2.5-0.5B Eng Eng Rus Rus Eng+Rus Eng+Rus A H H Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Rand Rand Rand Rand Disj Disj 43.87 39.54 41.60 49.23 64.92 42.42 13.19 10.49 16.48 10. 22.70 23.93 24.27 25.05 23.56 7.09 7.49 30.36 31.13 77.78 81.43 78.04 78.12 53.47 83.88 89.71 90.66 87.82 93. 86.22 84.85 89.93 86.56 86.92 94.44 94.58 83.52 83.60 53.33 57.86 52.31 61.00 11.98 60.68 72.78 74.25 70.82 76. 67.14 65.33 69.62 70.39 71.37 79.59 79.88 61.72 61.73 64.48 68.34 64.30 64.24 28.78 71.04 86.09 88.11 84.04 91. 79.87 78.18 84.10 76.91 77.88 92.79 92.73 72.20 72.22 Table 13: S2L-equations (subset). Metrics results (%) for Qwen trained with 5 ASR models. Model CER Rouge-1 sBLEU chrF WER METEOR BLEU chrF++ Qwen2.5-0.5B 43. 78.49 50.06 60.35 75.33 57.21 47. 58.88 Table 14: S2L-equations (subset). Case-sensitive metrics (%) for different Language Models. Mix means combination of human-annotated and TTS. Lang means the language of the train/validation/test splits. Model Lang Train Test Split CER Rouge-1 sBLEU chrF Qwen2.5-0.5B Qwen2.5-Math-1.5B SALMONN-13B Flan-T5 Qwen-Audio Qwen2.5-0.5B SALMONN-13B Qwen2.5-0.5B SALMONN-13B Eng Eng Eng Eng Eng Rus Rus Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Eng+Rus Mix Mix Disj Eng+Rus Mix Mix Disj 45.79 44.39 44.47 67.52 54.64 13.45 10. 23.39 24.99 77.78 79.29 83.88 53.47 76.63 89.71 93.59 86.22 89.93 50.46 51.02 56.76 10.43 54.79 72.67 76. 66.26 68.69 61.06 61.67 66.70 26.01 57.61 85.47 91.38 78.74 82.82 Table 15: S2L-equations (subset). Remaining case-sensitive metrics (%) for different Language Models. Mix means combination of Human annotated and TTS. Lang means language of train/validation/test splits Model Lang Train Test Split WER METEOR BLEU chrF++ Qwen2.5-0.5B Qwen2.5-Math-1.5B SALMONN-13B Flan-T5 Qwen-Audio Qwen2.5-0.5B SALMONN-13B Qwen2.5-0.5B SALMONN-13B Eng Eng Eng Eng Eng Rus Rus Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Mix Mix Disj Eng+Rus Mix Mix Disj Eng+Rus Mix Mix Disj 79.60 76.78 72.20 111.83 102.91 28.14 18.13 42.46 40.02 56.89 57.52 61.91 20.47 53.67 80.78 84.91 73.63 77. 47.16 47.85 53.08 6.19 42.53 70.55 74.95 63.80 66.77 59.44 60.24 65.06 24.84 55.89 83.68 90.09 78.18 81. Table 16: S2L-equations (subset). Remaining results of lower-case metrics (%) for different models. SALMONN represents the Multimodal approach, while the rest of the models represent ASR post-correction. stands for artificially annotated audio (TTS), human annotated audio, Mix the combination of both and H. Disj split means that test equations do not intersect with the train equations, and Rand split means that train-test split was made randomly over generated pairs and equations from train might occur in the test but should be pronounced with different speakers or TTS models. Model Lang Train Test Split WER METEOR BLEU chrF++ Qwen2.5-0.5B Qwen2.5-Math-1.5B ProofGPT-1.3B SALMONN-13B InternLM2-1.8B Flan-T5 Qwen2.5-0.5B Qwen2.5-Math-1.5B ProofGPT-1.3B SALMONN-13B Eng Eng Eng Eng Eng Eng Rus Rus Rus Rus Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Mix Qwen2.5-0.5B ProofGPT-1.3B SALMONN -13B Eng+Rus Mix Mix Eng+Rus Mix Mix Eng+Rus Mix Mix Qwen2-0.5B Qwen2.5-0.5B Qwen2-0.5B Qwen2.5-0.5B Qwen2-0.5B Qwen2.5-0.5B Rus Rus Eng Eng Eng+Rus Eng+Rus A H H Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Disj Rand Rand Rand Rand Disj Disj 76.85 69.16 69.64 68.90 81.01 109.26 27.14 23.80 32.14 17.94 41.47 43.26 38.80 14.82 13. 40.37 38.54 57.02 58.27 56.89 60.33 55.86 61.91 57.30 20.47 80.78 81.65 79.10 84.91 73.63 72.20 77.24 86.74 86. 73.88 74.59 68.83 68.56 50.42 55.57 49.73 57.55 50.65 7.69 70.64 72.03 68.51 75.05 64.75 62.94 67.85 78.46 78. 68.60 69.71 58.82 58.60 62.71 66.77 62.50 69.20 62.55 27.53 84.34 86.47 82.22 90.36 78.18 76.37 82.62 91.87 91. 76.53 76.53 70.78 70."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE Moscow, Russia",
        "MTUCI",
        "Skoltech"
    ]
}