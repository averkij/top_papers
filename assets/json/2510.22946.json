{
    "paper_title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
    "authors": [
        "Zeyu Wang",
        "Zilong Chen",
        "Chenhui Gou",
        "Feng Li",
        "Chaorui Deng",
        "Deyao Zhu",
        "Kunchang Li",
        "Weihao Yu",
        "Haoqin Tu",
        "Haoqi Fan",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 4 9 2 2 . 0 1 5 2 : r Preprint. Work In Progress. LIGHTBAGEL : LIGHT-WEIGHTED, DOUBLE FUSION FRAMEWORK FOR UNIFIED MULTIMODAL UNDERSTANDING AND GENERATION Zeyu Wang1 Zilong Chen2 Chenhui Gou3 Feng Li4 Chaorui Deng4 Deyao Zhu4 Kunchang Li4 Weihao Yu4 Haoqin Tu1 Haoqi Fan4 Cihang Xie1 1UC Santa Cruz 2Tsinghua University 3Monash University 4ByteDance Seed Project Page: https://ucsc-vlaa.github.io/LightBagel/ Equal Contribution"
        },
        {
            "title": "ABSTRACT",
            "content": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, multimodal learning has witnessed shift from specialized models to new generation of unified multimodal models (UMMs). The central innovation of these models lies in their ability to natively couple language and vision, enabling them to understand both modalities and to generate text and images within single, end-to-end modeling stack. Production-scale systems such as GPT-4o (Hurst et al., 2024) and Gemini 2.0 Flash (Google, 2025) have further demonstrated the promise of this paradigm, with impressive prompt adherence and dialogue-based image generation and manipulation capabilities, highlighting the potential of any-to-any paradigm. Meanwhile, there is also growing push inside the open-source research community to advance UMMs. The early works (Team, 2024; Wang et al., 2024; Zhou et al., 2024; Wu et al., 2025a; Chen et al., 2025c) adopt single-stack transformer over mixed-modality sequences, but jointly optimizing autoregressive and diffusion objectives introduces fundamental training conflicts. Subsequent works (Deng et al., 2025; Wu et al., 2025b; Wang et al., 2025b; Wei et al., 2025) decouples understanding and generation into separate pathways, easing optimization and improving performance, yet requiring large-scale training and substantial compute that limit accessibility. Recent attempts to build UMMs efficientlysuch as lightweight connectors that map final-layer representations from pretrained visionlanguage models (VLMs, i.e., used for understanding) to diffusion transformers (DiTs, i.e., used for generation) for conditional generation (Chen et al., 2025a; Lin et al., 2025)are promising but remain incomplete, as performance and task generality are still limited. *Equal contribution. 1 Preprint. Work In Progress. Figure 1: Token efficiency comparison on T2I and image editing benchmarks. Our LIGHTBAGEL outperforms many leading unified models that uses significantly more tokens for training, showing great token efficiency. Note that we use our best estimate for the number of seen tokens of OmniGen2 and UniPic since their original training recipe is unclear to the public. This work continues the pursuit of efficient UMM construction by introducing novel and powerful fusion strategy. Specifically, inspired by the recent success of LMFusion (Shi et al., 2024) and Bagel (Deng et al., 2025), we retain the original VLM and DiT blocks as they are, and insert zero-initialized multimodal self-attention blocks after each understanding and generation block. This design keeps the strong autoregressive and diffusion capabilities of the base models while enabling early, deep, and continuous cross-modal interaction. In controlled experiments, our method clearly outperforms the widely adopted Shallow Fusion approach (Tang et al., 2025), which conditions the generator only on the final-layer output of the understanding branch (Chen et al., 2025a; Lin et al., 2025). Furthermore, this dual-pathway design allows clean separation of ViT tokens (processed in the understanding pathway) and VAE tokens (processed in the generation pathway), effectively integrating high-level semantic representations with low-level signals. We refer to this mechanism as Double Fusion, as it simultaneously fuses understanding and generation branches, and ViTand VAE-derived features. To better support this architecture, we further curate UMM tuning dataset emphasizing data quality, task balance, and diversity, by combining post-processed public text-to-image and image-editing data with carefully generated synthetic data. Together, these components enable efficient training and deliver strong performance with substantially fewer tokens and compute than prior works, as illustrated in Figure 1. We refer to the resulting model as LIGHTBAGEL. LIGHTBAGEL delivers state-of-the-art results across multiple benchmarks, including GenEval score of 0.91 for compositional text-to-image generation, DPG-Bench score of 82.28 for complex text-to-image generation, and scores of 6.06 on GEditBench and 3.65 on ImgEdit-Bench for image editing. Remarkably, trained on only 35B seen tokens, LIGHTBAGEL achieves performance on par with, or even surpassing, leading models like UniPiC and OmniGen2, which were trained with orders of magnitude more tokens. These results highlight LIGHTBAGELs efficiency and suggest new directions for the design of future UMM architectures. By fully releasing the code, models, and datasets, we seek to promote reproducibility and catalyze future advancements in UMM research."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Text-to-Image Generation. Diffusion models (Ho et al., 2020; Song et al., 2020) have emerged as the dominant paradigm for open-domain image synthesis, surpassing GANs (Goodfellow et al., 2 Preprint. Work In Progress. 2020) with improved stability and semantic fidelity. Seminal works, such as Stable Diffusion and its successors (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024), DALLE (Ramesh et al., 2022), and Imagen (Imagen 3 Team, 2024), demonstrated the power of large-scale pretraining and latent denoising for high-resolution, text-aligned generation. Follow-up studies advanced diffusion-based pipelines by improving model architecture (Peebles & Xie, 2023; Yu et al., 2025), emphasizing data quality (Chen et al., 2024b;a), and enhancing controllability through spatial or semantic constraints (Zhang et al., 2023b; Li et al., 2023; Mou et al., 2024). More recently, flowmatching models (Lipman et al., 2022) have been proposed as powerful alternative to diffusion, modeling vector fields between noise and data distributions, and have achieved strong results in large-scale systems (Labs, 2024; Wan et al., 2025). Image Editing. Image editing has been widely studied as an extension of text-to-image generation. InstructPix2Pix (Brooks et al., 2023) pioneered the supervised paradigm based on {instruction, source image, target image} triplets. Subsequent works improved this approach by diversifying training data and improving quality (Zhang et al., 2023a; Hui et al., 2025; Wei et al., 2024; Ye et al., 2025), or by incorporating stronger control signals (Tan et al., 2023; Liu et al., 2025). Parallel directions include mask-based editing (Ju et al., 2024; Zhuang et al., 2024) and subject-driven generation through lightweight fine-tuning methods (Ruiz et al., 2023; Raj et al., 2023; Ruiz et al., 2024). More recent frameworks such as OmniGen (Xiao et al., 2025) and UniReal (Chen et al., 2025b) unify common image generation tasks (e.g. T2I generation, image editing, personalization) by processing various conditional inputs with modified attention mechanisms, demonstrating the potential of single model for multiple generation tasks. Unified Multimodal Models. UMMs have rapidly gained attention for their ability to support both understanding and generation, enabling smooth cross-modality knowledge transfer. Early works such as Chameleon (Team, 2024), EMU3 (Wang et al., 2024), Transfusion (Zhou et al., 2024), Janus (Wu et al., 2025a; Chen et al., 2025c), and Show-o (Xie et al., 2024) employed single Transformer backbone to process interleaved image and text tokens. Among the, hybrid approaches, including Transfusion (Zhou et al., 2024) and Show-o (Xie et al., 2024), combine autoregressive text decoding with diffusion-based image generation, while others rely on discrete image tokenizers to autoregressively predict both image and text tokens. Recent work has explored how to adapt powerful pretrained models into UMMs. LMFusion (Shi et al., 2024) extends text-only LLMs with parallel image-generation branch, design that inspires our own approach of fusing well-trained understanding and generation models as parallel pathways via additional attention layers. Other approaches such as MetaQueries (Pan et al., 2025) and UniWorld (Lin et al., 2025) propose efficient schemes to combine frozen multimodal LLMs with trainable diffusion models, enabling knowledge-augmented image generation while maintaining strong understanding ability. Finally, large-scale pretraining efforts (Deng et al., 2025; Liao et al., 2025; Wu et al., 2025b; Wang et al., 2025b; Wei et al., 2025) focus on massive interleaved image-text corpora, demonstrating broad generalization across understanding, generation, editing, and other downstream tasks."
        },
        {
            "title": "3 METHODS",
            "content": "3.1 MODEL ARCHITECTURE Inspired by the prior works LMFusion (Shi et al., 2024) and BAGEL (Deng et al., 2025), the core design principle of LIGHTBAGEL is to adopt mixture-of-experts (MoE) style architecture for fusing publicly available models. Specifically, as illustrated in Figure 2, this design interleaves multimodal self-attention blocks across VLM (used for understanding) and DiT (used for generation) blocks. To fully leverage the strength of open-source models with extensive pre-training and post-training, we employ QWen2.5-VL-7B (Bai et al., 2025) for the understanding pathway and Wan2.2-TI2V-5B (Wan et al., 2025) for the generation pathway. Note that given QWen2.5-VL-7B has slightly fewer layers than Wan2.2-TI2V-5B (two layers fewer), its final-layer output is reused as input to the last two multimodal self-attention blocks. In this design, the VLM blocks process understanding tokens (i.e., text and ViT tokens), while the DiT blocks operate on generation tokens (i.e., VAE tokens). The multimodal self-attention blocks span all token types, enabling rich cross-modal interactions. To facilitate this, we adopt the generalized causal attention mechanism from BAGEL (Deng et al., 2025), allowing tokens from different modalities 3 Preprint. Work In Progress. Figure 2: Overview of the LIGHTBAGEL architecture. Text and ViT tokens (understanding pathway) and VAE tokens (generation pathway) are processed by pre-trained VLM and DiT blocks, respectively. At each layer, zero-initialized multimodal self-attention module enables cross-modal interactions without altering the original model architectures. and tokenizers to attend to one another. Importantly, each multimodal self-attention block is zeroinitialized, ensuring that the feature distributions of the VLM and DiT remain intact at the start of training, thereby preserving their strong autoregressive and denoising capabilities. For image editing tasks, both ViT and VAE tokens are extracted from the source image and provided as conditioning signals. The ViT tokens are extracted using the QWen2.5-VL vision encoders. The VAE tokens are extraced by the 3D causal VAE from Wan2.2-TI2V-5B (Wan et al., 2025), which provides 16 spatial compression and 4 temporal compression. We expect this design offer several key advantages: Seamless model integration. The framework incorporates powerful pre-trained VLM and DiT models without altering their architectures, offering straightforward and generalizable method for fusing publicly available models into unified multimodal system. In line with prior findings (Tang et al., 2025), we observe that this deep fusion strategy consistently outperforms shallow fusion approaches while delivering superior token efficiency. Dual-pathway visual representation. By naturally integrating ViT tokens (high-level semantics) and VAE tokens (low-level signals), the architecture achieves precise and consistent image editing, effectively balancing global understanding with fine-grained detail. Information-preserving multimodal interaction. Leveraging hidden states from all understanding and generation layers within the multimodal self-attention avoids compressing conditioning inputs into fixed-length representation (Pan et al., 2025; Chen et al., 2025a; Wei et al., 2025), ensuring rich, loss-free cross-modal interactions. 3.2 DATASET Our training corpus comprises 45 million samples, encompassing both text-to-image generation and image-editing tasks. We source data from publicly available datasets, including BLIP-3o (Chen et al., 2025a), Civitai (civ, 2024), OmniGen (Xiao et al., 2025), OmniEdit (Wei et al., 2024), GPTIMAGE-EDIT-1.5M (Wang et al., 2025c), and UniWorld-V1 (Lin et al., 2025), supplemented by synthetic self-curated dataset of 4.5 million samples. Notably, we apply VLM to refine the editing instructions of public image editing data conditioned on sourcetarget pairs, thereby enhancing instruction precision. 3.3 TRAINING We adopt NaViT-style image processing to preserve native aspect ratios (Dehghani et al., 2023), constraining inputs to minimum short side of 512 pixels and maximum long side of 1024 pixels, thereby improving generation quality. ViT tokens are extracted from input sizes ranging from minimum short side of 224 pixels to maximum long side of 532 pixels. The LIGHTBAGEL model is 4 Preprint. Work In Progress. Table 1: Comparison of different models across understanding, generation, editing, and in-context Generation tasks. refers to the methods using LLM rewriter. For UMMs, refers to only model checkpoints and evaluation code being released; refers to only model checkpoints and training/evaluation code being released; refers to the full suite of all the {model, data, code} being released. Model # Params Openness Understanding Image Generation Image Editing MMB MMMU MM-Vet GenEval DPG ImgEdit GEdit-EN LLaVA-1.5 LLaVA-NeXT SDXL SD3-medium FLUX.1-dev Instruct-P2P MagicBrush AnyEdit Step1X-Edit IC-Edit Unified models Janus-Pro Emu3 UniPic UniPic 2.0 Ovis-U1 MetaQuery-XL Show-o2 OmniGen OmniGen2 BAGEL 1.5B 7B + 2B 2.4B + 1.2B 7B + 1.6B 7B 3.8B 3B + 4B 7B + 7B BLIP3-o 4B BLIP3-o 8B UniWorld-V1 LIGHTBAGEL 3B + 1.4B 7B + 1.4B 7B + 12B 7B + 5B + 3B 36.4 79. 67.8 51.1 36.3 57.4 75.5 58.5 83.5 77.8 83.5 79.3 79.1 85.0 78.6 83.5 83.5 83. 36.3 31.6 58.6 51.1 58.6 48.9 53.1 55.3 46.6 58.6 58.6 58.6 39.8 37.2 67.1 66.7 66.6 61.8 67.2 60.1 66.6 67.1 67.1 0.55 0.62 0.66 0.80 0.66 0.86 0.90 0.89 0.80 0.76 0.68 0.86 0.88 0.81 0.84 0.84 0.91 74.7 84.1 84.0 84.19 80.60 85.50 83.79 83.72 82.05 86.14 81.16 83.57 85.07 79.36 81.60 81.38 82.16 1.88 1.90 2.45 3.06 3.05 3.49 4.06 4.00 2.96 3.44 3.20 3.26 3.77 3.68 1.86 3.21 6.70 4.84 5.83 7.10 6.42 5.06 6.42 6.52 4.85 6.06 trained for 70K steps using the AdamW optimizer, with 2K warmup steps and fixed learning rate of 0.00003. The sequence length is configured between 16,384 (minimum) and 20,480 (maximum) tokens. To enable classifier-free guidance, text tokens, VAE tokens, and ViT tokens are randomly dropped with probabilities of 0.1, 0.1, and 0.5, respectively. The understanding branch is frozen during the entire training time to preserve the strong understanding ability of QWen2.5-VL-7B. Training was conducted with 32 H200 GPUs for approximately one week. We divide the full training process into three stages. In the first stage, large proportion of common T2I data and small proportion of high-quality T2I and editing data are used. In the second and third stages, we progressively increase the ratios of high-quality T2I and editing data, respectively. In practice, this staged setup proves beneficial for improving both text-to-image generation and image-editing performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate the performance of LIGHTBAGEL across diverse set of image understanding, generation, and editing tasks. Sec. 4.1 presents the visual understanding results. Text-to-image generation results are reported in Sec. 4.2, followed by focus on image editing in Sec. 4.3. Sec. 4.4 presents our ablation study, analyzing key design choices of the model. Our experiment results show that LIGHTBAGEL efficiently achieves strong performances over wide spectrum of tasks and benchmarks, demonstrating the efficacy of our double fusion approach. 4.1 VISUAL UNDERSTANDING By freezing the understanding branch, our model preserves the strong multimodal reasoning capabilities of the pre-trained QWen2.5-VL-7B. As reported in Table 1, LIGHTBAGEL attains competitive performances of 83.5 on MMBench (Liu et al., 2024), 58.6 on MMMU (Yue et al., 2024), and 67.1 on 5 Preprint. Work In Progress. Figure 3: Qualitative text-to-image results from LIGHTBAGEL, showcasing high-quality generations with strong fidelity to text prompts and consistent rendering across diverse aspect ratios. MM-Vet (Yu et al., 2023). This architectural choice aligns with recent state-of-the-art designs, such as UniWorld-V1 (Lin et al., 2025), OmniGen2 (Wu et al., 2025b), and UniPiC 2.0 (Wang et al., 2025b), which similarly prioritize maintaining robust understanding performance. As result, LIGHTBAGEL effectively mitigates potential degradation in understanding capabilities and surpasses several strong competitors, including Ovis-U1 (Wang et al., 2025a), Show-o2 (Xie et al., 2025), and Janus-Pro (Chen et al., 2025c). 4.2 TEXT-TO-IMAGE GENERATION We test on widely recognized benchmarks, GenEval and DPG-Bench, to evaluate LIGHTBAGELs text-to-image generation performance. These benchmarks primarily assess the models capabilities in compositional image generation and dense prompt following, respectively. In addition, we provide qualitative visualization examples in Figure 3. Both the quantitative results and qualitative illustrations demonstrate that LIGHTBAGEL is capable of producing high-fidelity, aesthetically compelling images. GenEval. As reported in Table 2, LIGHTBAGEL attains an overall score of 0.91 on GenEval when evaluated with LLM-rewritten prompts, highlighting its strong compositional understanding across diverse generative tasks. This performance surpasses several competitive baselines, including UniPiC (0.86), OmniGen2 (0.86), BAGEL (0.88) and UniPiC2.0 (0.90). Notably, LIGHTBAGEL is trained with over order of magnitude of less seen tokens, underscoring its remarkable efficiency in terms of both data and compute resources. DPG-Bench. As reported in Table 3, on DPG-Bench, LIGHTBAGEL achieves an overall score of 82.16, demonstrating competitive performance in long-prompt adherence and complex scene generation. This result surpasses other strong unified models such as BLIP3-o 8B (81.60) and UniWorld-V1 (81.38). Additionally, detailed breakdowns in Table 3 indicate that LIGHTBAGEL maintains consistently strong performance across multiple dimensions of evaluation, including global coherence, entity recognition, attribute understanding, and relational reasoning. Preprint. Work In Progress. Table 2: Evaluation of text-to-image generation ability on GenEval benchmark. refers to the methods using LLM rewriter. Method Single object Two object Counting Colors Position Color attribution Overall SDv2.1 SDXL IF-XL LUMINA-Next SD3-medium FLUX.1-dev NOVA OmniGen TokenFlow-XL Janus Janus Pro Emu3-Gen Show-o2 MetaQuery-XL UniPic UniPic 2.0 Ovis-U1 BAGEL OmniGen2 BLIP3-o 8B UniWorld-V1 LIGHTBAGEL 0.98 0.96 0.97 0.92 0.99 0.99 0.99 0.98 0.95 0.97 0.99 0.99 1.00 0.98 0.98 0.98 0.99 0.98 1.00 0.51 0.74 0.74 0.46 0.94 0.81 0.91 0.84 0.60 0.68 0.89 0.81 0.87 0.92 0.98 0.95 0. 0.93 0.97 0.44 0.39 0.66 0.48 0.72 0.79 0.62 0.66 0.41 0.30 0.59 0.42 0.58 0.74 0.90 0.84 0.74 0.81 0.93 0.85 0.85 0.81 0.70 0.89 0.74 0.85 0.74 0.81 0.84 0.90 0.80 0.92 0.91 0.92 0.95 0. 0.89 0.94 0.07 0.15 0.13 0.09 0.33 0.20 0.33 0.40 0.16 0.46 0.79 0.49 0.52 0.89 0.79 0.78 0.71 0.74 0.79 0.17 0.23 0.35 0.13 0.60 0.47 0.56 0.43 0.24 0.42 0.66 0.45 0.62 0.72 0.75 0.77 0. 0.71 0.81 0.50 0.55 0.61 0.46 0.74 0.67 0.71 0.68 0.55 0.61 0.80 0.66 0.76 0.80 0.86 0.90 0.89 0.88 0.86 0.84 0.84 0.91 Table 3: Evaluation of text-to-image generation ability on DPG-Bench benchmark. Method Global Entity Attribute Relation Other Overall LUMINA-Next SDXL PlayGroundv2.5 Hunyuan-DiT PixArt-Σ DALLE3 SD3-medium FLUX.1-dev OmniGen TokenFlow-XL Janus Janus Pro Show-o2 EMU3 UniPic UniPic 2.0 Ovis-U1 BAGEL OmniGen2 BLIP3-o 8B UniWorld-V1 LIGHTBAGEL 82.82 83.27 83.06 84.59 86.89 90.97 87.90 82.10 87.90 78.72 82.33 86.90 89.00 85.21 89.65 - 82.37 88.94 88. 83.64 87.13 88.65 82.43 82.59 80.59 82.89 89.61 91.01 89.50 88.97 79.22 87.38 88.90 91.78 86.68 87.78 - 90.08 90.37 88.83 88.39 89.59 86.44 80.91 81.20 88.01 88.94 88.39 88.83 88.70 88.47 81.29 87.70 89.40 89.96 86.84 90.84 - 88.68 91.29 90. 88.44 87.58 80.53 86.76 84.08 74.36 86.59 90.58 80.70 91.10 87.95 85.22 85.46 89.32 91.81 90.22 91.89 - 93.35 90.82 89.37 89.27 90.22 81.82 80.41 83.50 86.41 87.68 89.83 88.68 89.40 83.56 71.20 86.41 89.02 91.64 83.15 91.95 - 85.20 88.67 90. 87.22 90.44 74.63 74.65 75.47 78.87 80.54 83.50 84.08 84.00 81.16 73.38 79.68 84.19 86.14 80.60 85.50 83.79 83.72 85.07 83.57 81.60 81.38 82.16 4.3 IMAGE EDITING We evaluate LIGHTBAGELs image editing capabilities using two widely adopted benchmarks: GEditBench-EN (Liu et al., 2025) and ImgEdit-Bench (Ye et al., 2025). GEdit-Bench-EN consists of real-world user editing instances, while ImgEdit-Bench encompasses nine distinct editing tasks (e.g., add, remove, alter). We further provide qualitative examples of LIGHTBAGELs editing results in Figure 4. Both quantitative and qualitative results demonstrate that LIGHTBAGEL delivers strong performance in instruction-based image editing, excelling in both editing accuracy and content preservation. GEdit-Bench-EN. As shown in Table 4, LIGHTBAGEL achieves an overall score of 6.06, positioning it among the top-tier unified models. The model demonstrates particular strength in semantic consistency (SC), attaining score of 6.34, which reflects robust instruction-following capabilities. 7 Preprint. Work In Progress. Figure 4: Qualitative image editing results generated by LIGHTBAGEL. The model exhibits strong instruction following and content preservation capability across diverse range of editing tasks. Table 4: Evaluation of image editing ability on GEdit-Bench-EN Model SC PQ Overall Gemini-2.0-flash GPT-4o Instruct-Pix2Pix MagicBrush AnyEdit ICEdit Step1X-Edit OmniGen2 BAGEL Ovis-U1 UniPic UniPic 2. UniWorld-V1 LIGHTBAGEL 6.73 7.85 3.58 4.68 3.18 5.11 7.09 7.16 7.36 6.72 4.93 6.34 6.61 7. 5.49 5.66 5.82 6.85 6.76 6.77 6.83 6.18 7.43 7.31 6.32 7.53 3.68 4.52 3.21 4.84 6.70 6.41 6.52 6.42 5.83 7. 4.85 6.06 This result notably surpasses that of UniWorld-V1 (4.93) by large margin, underscoring the effectiveness of our hybrid ViT+VAE feature fusion strategy compared to only using ViT tokens as condition in image editing task. ImgEdit-Bench. As can be seen in Table 5, LIGHTBAGEL achieves an overall score of 3.77, outperforming other strong open-source competitors such as UniWorld-V1 (3.26) and OmniGen2 (3.44). Importantly, LIGHTBAGEL ranks first among open-source models in several key categories, including Add (4.21), Replace (4.55), Remove (3.80), and Hybrid (3.92), which highlights the models robust and consistent editing ability across broad spectrum of tasks. 4.4 ABLATION STUDY For ablation study experiments, we keep the general training setup for the \"Deep Fusion vs Shallow Fusion\" study, while opt for 40K training steps for the others for faster experiment cycles. Deep Fusion vs Shallow Fusion. Previous efficient UMM approaches typically employ lightweight connector that maps the final output of the understanding branch as conditional input to the generation branch. In contrast, our Double Fusion design allows language and visual tokens to interact from the earliest layers, enabling deeper and more continuous cross-modal integration. 8 Preprint. Work In Progress. Table 5: Evaluation of image editing ability on ImgEdit-Bench. Model GPT-4o MagicBrush Instruct-Pix2Pix AnyEdit UltraEdit Step1X-Edit ICEdit OmniGen2 BAGEL Ovis-U1 UniPic UniPic 2.0 UniWorld-V1 LIGHTBAGEL Add Adjust Extract Replace Remove Background Style Hybrid Action Overall 4.61 2.84 2.45 3.18 3.44 3.88 3. 3.74 3.56 4.12 3.66 - 3.82 4.21 4.33 1.58 1.83 2.95 2.81 3.41 3.39 3.54 3.31 3.92 3.51 - 3.66 3. 2.90 1.51 1.41 1.14 2.00 1.76 1.73 1.77 1.88 2.36 2.06 - 2.31 1.83 4.35 1.97 2.01 2.49 2.96 3.40 3. 3.21 2.62 4.09 4.31 - 3.45 4.55 3.66 1.58 1.44 2.21 2.45 2.83 2.93 2.77 2.88 3.57 2.77 - 3.02 3. 4.57 1.75 1.44 2.88 2.83 3.16 3.08 3.57 3.44 4.22 3.77 - 2.99 4.15 4.93 2.38 3.55 3.82 3.76 6.63 3. 4.81 4.49 4.69 4.76 - 4.71 4.66 3.96 1.62 1.20 1.56 1.91 2.52 2.04 2.30 2.38 3.23 2.56 - 2.96 3. 4.89 1.22 1.46 2.65 2.98 2.52 3.68 4.14 4.17 3.61 4.04 - 2.74 3.60 4.20 1.90 1.88 2.45 2.70 3.06 3. 3.43 3.20 3.98 3.49 4.06 3.26 3.77 Figure 5: Deep fusion vs. shallow fusion design choices. Regions of different colors represent different training stages. The 0% Depth deep fusion approach in our LIGHTBAGEL consistently outperforms other options. To systematically compare the two strategies, we vary the depth at which features from the VLM are injected into the generation pathway. Specifically, 0% Depth denotes the case where the i-th DiT block is conditioned on the i-th VLM blocks output. When the VLM blocks are exhausted, the final VLM output is repeated as input for the remaining DiT blocks. Conversely, 100% Depth corresponds to conditioning all DiT blocks exclusively on the final VLM output, effectively repeating it across all multimodal self-attention layers. Importantly, we keep the total number of multimodal self-attention layers fixed, ensuring identical parameter counts and fair comparison. As shown in Figure 5, the 0% Depth optionadopted in our LIGHTBAGEL modelconsistently outperforms shallow or early-layer fusion for both text-to-image and image-editing tasks. We attribute this advantage to the fact that the final VLM representations encode high-level semantics more suitable for next-token prediction rather than multimodal alignment. Visual Tokenizer Choices. VAE and ViT encoders provide complementary visual representations: VAEs emphasize low-level details, while ViTs capture high-level semantic information. To assess which type of information is more suitable for image editing, we conduct an ablation study on visual tokenizer choice, with results reported in Table 6a. The findings indicate that both sources of information are essentialcombining low-level details with high-level semantics leads to more consistent and accurate image editing outcomes. Training Timestep Shift. Shifting timesteps during inference has been shown to improve generation quality in state-of-the-art models (Labs, 2024; Wan et al., 2025). In our preliminary experiments, we observe that the strong base DiT pathway already demonstrates robust denoising capabilities. Building on this, and following the inference-time timestep shifting strategy (Labs, 2024; Wan et al., 2025), we increase the training diffusion timestep range from 1.0 to 4.0 to achieve more noisy corrupted samples. As reported in Table 6b, timestep shift larger than 1 consistently lead to better results. 9 Preprint. Work In Progress. Table 6: Ablation study on visual tokenizer and timestep shift choices. Encoder ViT VAE ViT + VAE GEdit-EN ImgEdit 3.91 4.93 5.61 2.65 3.38 3.57 (a) Evaluation of different visual tokenizer choices. Timestep Shift 1 2 4 DPG-Bench 76.67 78.84 81.77 ImgEdit 3.07 3.36 3. (b) Evaluation of different timestep shift choices."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work introduces LIGHTBAGEL, unified multimodal model for both understanding and generation that achieves state-of-the-art performance across diverse tasks while requiring substantially fewer training tokens and compute than prior leading UMMs. These advantages are largely benefited by our Double Fusion design, which enables effective cross-modal feature interactions and naturally integrates high-level semantics with low-level visual details. By releasing the entire suite of code, models, and data, we hope to support reproducibility and accelerate progress in unified multimodal modeling."
        },
        {
            "title": "REFERENCES",
            "content": "Civitai. 2024. URL https://civitai.com/. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pp. 7491. Springer, 2024a. Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024b. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 10 Preprint. Work In Progress. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Google. Gemini 2.0 flash. 2025. URL https://developers.googleblog.com/en/ the-next-chapter-of-the-gemini-era-for-developers/. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Cihang Xie, and Yuyin Zhou. HQ-edit: high-quality dataset for instruction-based image editing. In The Thirteenth International Conference on Learning Representations, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Google Imagen 3 Team. Imagen 3. 2024. URL https://arxiv.org/pdf/2408.07009. Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pp. 150168. Springer, 2024. Black Forest Labs. Flux. 2024. URL https://github.com/black-forest-labs/flux. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2251122521, 2023. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 42964304, 2024. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. 11 Preprint. Work In Progress. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 23492359, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 65276536, 2024. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. 2023. Bingda Tang, Boyang Zheng, Sayak Paul, and Saining Xie. Exploring the deep fusion of large language models and diffusion transformers for text-to-image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2858628595, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025a. Peiyu Wang, Yi Peng, Yimeng Gan, Liang Hu, Tianyidan Xie, Xiaokun Wang, Yichen Wei, Chuanxin Tang, Bo Zhu, Changshi Li, et al. Skywork unipic: Unified autoregressive modeling for visual understanding and generation. arXiv preprint arXiv:2508.03320, 2025b. Preprint. Work In Progress. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gptimage-edit-1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025c. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: In The Thirteenth Building image editing generalist models through specialist supervision. International Conference on Learning Representations, 2024. Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023b. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 13 Preprint. Work In Progress. Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pp. 195211. Springer, 2024."
        }
    ],
    "affiliations": [
        "ByteDance Seed Project",
        "Monash University",
        "Tsinghua University",
        "UC Santa Cruz"
    ]
}