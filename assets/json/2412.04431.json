{
    "paper_title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
    "authors": [
        "Jian Han",
        "Jinlai Liu",
        "Yi Jiang",
        "Bin Yan",
        "Yuqi Zhang",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Xiaobing Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 1 3 4 4 0 . 2 1 4 2 : r Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis Jian Han, Jinlai Liu, Yi Jiang, Bin Yan Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu ByteDance {hanjian.thu123,liujinlai.licio,jiangyi.enjoy,yanbin.master}@bytedance.com, {zhangyuqi.hi,yuanzehuan,bingyue.peng,will.liu}@bytedance.com, Codes and models: https://github.com/FoundationVision/Infinity Figure 1: High-resolution image synthesis results from Infinity, showcasing its capabilities in precise prompt following, spatial reasoning, text rendering, and aesthetics across different styles and aspect ratios."
        },
        {
            "title": "Abstract",
            "content": "We present Infinity, Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving win rate of 66%. Without extra optimization, Infinity generates high-quality 10241024 image in 0.8 seconds, making it 2.6 faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling. Equal contribution. Corresponding author: yuanzehuan@bytedance.com Preprint. Under review. Figure 2: Visual tokenizer quantizes continuous features and then gets index labels. Conventional classifier (left) predicts 2d indices. Infinite-Vocabulary Classifier (right) predicts bits instead. Slight perturbations to near-zero values in continuous features cause complete change of index labels. Bit labels (i.e. quantized features) change subtly and still provide steady supervision. Besides, parameters of conventional classifiers grow exponentially as increases, while IVC grows linearly. If = 32 and = 2048, the conventional classifier requires 8.8 trillion parameters, exceeding current compute limits. By contrast, IVC only requires 0.13M parameters."
        },
        {
            "title": "Introduction",
            "content": "The visual generation[27, 52, 20, 48, 42] has recently witnessed rapid advancements, enabling highquality, high-resolution images and video synthesis[8, 21]. Text-to-image generation[50, 46, 45, 7, 43, 21] is one of the most challenging tasks due to its need for complex language adherence and intricate scene creation. Currently, visual generation is primarily divided into two main approaches: Diffusion models and AutoRegressive models. Diffusion models[27, 52, 20, 43, 42, 21], trained to invert the forward paths of data towards random noise, effectively generate images through continuous denoising process. AutoRegressive models[15, 22, 73, 61], on the other hand, harness the scalability and generalizability of language models[16, 2, 28, 62, 63, 68, 57, 3, 60] by employing visual tokenizer[64, 47, 72] to convert images into discrete tokens and optimize these tokens causally, allowing image generation through next-token prediction or next-scale prediction. AutoRegressive models encounter significant challenges in highresolution text-to-image synthesis[73, 66]. They exhibit inferior reconstruction quality when utilizing discrete tokens as opposed to continuous tokens. Additionally, their generated visual contents are less detailed than those by diffusion models. The inefficiency and latency in visual generation, stemming from the raster-scan method of next-token prediction, further exacerbates these issues. Recently, Visual AutoRegressive modeling (VAR)[61] redefined autoregressive learning on images as coarse-to-fine next-scale prediction. It demonstrates superior generalization and scaling capabilities compared to diffusion transformers while requiring fewer steps. VAR leverages the powerful scaling properties of LLMs [31, 25] and can simultaneously refine previous scale steps, benefiting from the strengths of diffusion models as well. However, the index-wise discrete tokenizer[64, 22, 77, 61, 37] employed in AutoRegressive or Visual AutoRegressive models faces significant quantization errors with limited vocabulary size resulting in difficulties in reconstructing fine-grained details especially in high-resolution images. In the generation stage, index-wise tokens suffer from fuzzy supervision leading to visual detail loss and local distortions. Moreover, train-test discrepancies from teacherforcing training, inherent to LLMs, amplify cumulative errors in visual details. These challenges make index-wise tokens significant bottleneck for AutoRegressive models. We propose novel approach called bitwise modeling, which substitutes index-wise tokens with bitwise tokens throughout the process. Our bitwise modeling framework consists of three primary modules: bitwise visual tokenizer, bitwise infinite-vocabulary classifier, and bitwise self-correction. Inspired by the success and widespread adoption of binary vector quantization[74, 79], we scaled up the tokenizer vocabulary to 264, significantly surpassing all previous AutoRegressive model vocabularies[75, 55]. This expansion allows for reconstruction quality that far exceeds previous 2 discrete tokenizers, achieving results comparable to continuous VAEs[48], with scores improving from 0.87 to 0.33 on ImageNet-256 benchmark[19]. In Fig.2, we transform the conventional token prediction from large integer into binary bits in bitwise infinite-vocabulary classifier to address optimization and computation challenges, enabling the learning of massive vocabularies in Visual AutoRegressive models. Additionally, we incorporated bitwise self-correction during training by randomly flipping some bits to simulate prediction mistakes and re-quantizing the residual features, thus endowing the system with self-correcting capabilities. Our method, Infinity: Bitwise Visual AutoRegressive Modeling, maintains the scaling and speed advantages of Visual AutoRegressive modeling while achieving detailed reconstruction and generation quality comparable to that of continuous Diffusion models. Infinity sets new record for AutoRegressive models, and also surpasses leading diffusion models including SDXL[43], PixArt-Sigma[12],DALL-E3[7] and Stable-Diffusion 3[21] on several challenging text-to-image benchmarks. Notably, Infinity surpasses SD3 by improving the GenEval benchmark score from 0.62 to 0.73, ImageReward benchmark score from 0.87 to 0.96, HPSv2.1 benchmark score from 30.9 to 32.3, achieving win rate of 66% for human evaluation and 2.6 reduction in inference latency with the same model size. Specifically, Infinity shows powerful scaling laws for image generation capabilities by scaling up the image tokenizer vocabulary size and the corresponding transformer size. As the image tokenizer and transformer sizes increase, the content and details of high-quality image generation show significant improvement. In summary, the contributions of our work are as follows: 1. We propose Infinity, an autoregressive model with Bitwise Modeling, which significantly improves the scaling and visual detail representation capabilities of discrete generative models. We believe this framework opens up new possibilities of infinity for the discrete generation community. 2. Infinity demonstrates the potential of scaling tokenizers and transformers by achieving near-continuous tokenizer performance with its image tokenizer and surpassing diffusion models in high-quality text-to-image generation. 3. Infinity enables discrete autoregressive text-to-image model to achieve exceptionally strong prompt adherence and superior image generation quality, while also delivering the fastest inference speed."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 AutoRegressive Models AutoRegressive models, leveraging the powerful scaling capabilities of LLMs[44, 9, 16, 62, 63], use discrete image tokenizers[64, 47, 22] in conjunction with transformers to generate images based on next-token prediction. VQ-based methods [64, 47, 22, 35, 55] employ vector quantization to convert image patches into index-wise tokens and use decoder-only transformer to predict the next token index. However, these methods are limited by the lack of scaled-up transformers and the quantization error inherent in VQ-VAE[64], preventing them from achieving performance on par with diffusion models. Parti [73], Emu3 [66], chameleon[59], loong[67] and VideoPoet[32] scaled up autoregressive models in text-to-image or video synthesis. Inspired by the global structure of visual information, Visual AutoRegressive modeling(VAR) redefines the autoregressive modeling on images as next-scale prediction framework, significantly improving generation quality and sampling speed. HART[58] adopted hybrid tokenizers based on VAR. Fluid[23] proposed random-order models and employed continuous tokenizer rather than discrete tokenizer. 2.2 Diffusion Models. Diffusion models have seen rapid advancements in various directions. Denoising learning mechanisms [27, 41] and sampling efficiency [53, 52, 38, 39, 4] have been continuously optimized to generate high-quality images. Latent diffusion models [48] is the first to propose modeling in the latent space rather than the pixel space for diffusion[50]. Recently, latent diffusion models[18, 21] have also adopted scaling up VAE to improve the representation in the latent space. DiT [42] and U-Vit[5] employ more scalable transformer to model diffusion, achieving superior results. Consequently, mainstream text-to-image diffusion models[21, 7, 14] have adopted the DiT architecture. DiT also inspire the text-to-video diffusion models[6, 8] 3 Figure 3: Framework of Infinity. Infinity introduces bitwise modeling, which incorporates bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier (IVC), and Bitwise Self-Correction. When predicting Rk, the sequence (R1, R2, ..., Rk1) serves as the prefixed context and the text condition guides the prediction through cross attention mechanism. Different from VAR, Infinity performs next-scale prediction with bit labels. 2.3 Scaling models Scaling laws in autoregressive language models reveal power-law relationship between model size, dataset size, and compute with test set cross-entropy loss [31, 25, 1]. These laws help predict larger model performance, leading to efficient resource allocation and ongoing improvements without saturation [9, 62, 63, 78, 68, 28]. This has inspired research into scaling in visual generation [56, 76, 61, 21, 8]"
        },
        {
            "title": "Infinity Architecture",
            "content": "3.1 Visual AutoRegressive Modeling Infinity incorporates visual tokenizer and transformer for image synthesis. During the training stage, sample consists of text prompt and ground truth image I. The proposed visual tokenizer first encodes the image into feature map Rhwd with stride and then quantize the feature map into multi-scale residual maps (R1, R2, ..., RK). The resolution of Rk is hk wk and it grows larger gradually from = 1 K. Based on this sequence of residuals, we can gradually approximate the continuous feature as in Eq.1 Fk = (cid:88) i=1 up(Ri, (h, w)) (1) Here up() means bilinear upsampling and Fk is the cumulative sum of the upsampled Rk. Subsequently, transformer learns to predict residuals of the next scale conditioned on previous predictions and the text input in an autoregressive manner. Formally, the autoregressive likelihood can be formulated as: p(R1, ..., RK) = (cid:89) k= p(Rk R1, ..., Rk1, Ψ(t)), (2) where Ψ(t) is the text embeddings from Flan-T5 [17] model. (R1, ..., Rk1, Ψ(t)) serves as the prefixed context When predicting Rk. Besides, the text embeddings Ψ(t) further guide the prediction through cross attention mechanism. In particular, as shown in Fig. 3, the text embeddings 4 Ψ(t) RLC is projected into SOS R11h as the input of the first scale, where is the hidden dimension of transformer. The transformer is required to predict R1 based on SOS in the first scale. In the latter k-th scale, to match the spatial size of the input and the output label Rk, we take the downsampled feature (cid:101)Fk1 from the last scale 1 as the input to predict Rk in parallel. (cid:101)Fk1 = down(Fk1, (hk, wk)), (3) where down() is bilinear downsampling and the spatial size of both (cid:101)Fk1 and Rk is (hk, wk). Refer to Alg.1 for detailed procedure to obtain binary quantization results and transformers inputs. In previous index-wise [61] representations, the shape of prediction is (hk, wk, Vd). Vd is the vocabulary size of the visual tokenizer. For binary quantization [74, 79] with code embedding dimension d, Vd = 2d. When is large, the required computational resource grows unaffordable. The transformer consists of stack of repeated blocks, where each block includes RoPE2d [26], SelfAttention, Cross Attention, and FFN layers. The text embeddings Ψ(t) provide effective guidance for image synthesis in each cross-attention layer. During the training stage, we exploit block-wise causal attention mask to ensure that the transformer can only attend to its prefixed context, i.e., (SOS, (cid:101)F1, ..., (cid:101)Fk1), when predicting Fk. During the inference stage, we perform KV-Caching to speed up inference and theres no need for masking. 3.2 Visual Tokenizer Increasing the vocabulary size has significant potential for improving reconstruction and generation quality. However, directly enlarging the vocabulary in existing tokenizers[75, 61] leads to substantial increase in memory consumption and computational burden. To address these challenges and fully exploit the potential of discrete tokenizers, this paper proposes new bitwise multi-scale residual quantizer, which significantly reduces memory usage, enabling the training of extremely large vocabulary, e.g. 264. Bitwise Multi-scale Residual Quantizer. We replace the original vector quantizer of VAR [61] with dimension-independent bitwise quantizer. In this paper, we consider two candidates, LFQ [75] and BSQ[79]. Given scales in the multi-scale quantizer, on the k-th scale, the input continuous residual vector zk Rd are quantized into binary output qk as shown below. qk = Q(zk) = (cid:40) sign(zk) 1 sign( zk zk ) if LFQ if BSQ (4) To encourage codebook utilization, an entropy penalty Lentropy = E[H(q(z))] H[E(q(z))] [30] is adopted, where H() represents the entropy operation. To obtain the distribution of q(z), we need to compute the similarities between the input and the whole codebook when using LFQ. However, this leads to unaffordable space and time complexity of O(2d). When the codebook dimension becomes large, e.g. 20, an out-of-memory (OOM) issue is faced as shown in Tab. 3. By contrast, since both input and output in BSQ are unit vectors, BSQ[79] proposes an approximation formula for the entropy penalty, reducing the computational complexity to O(d). As shown in Tab 3, there is no obvious increase in memory consumption for BSQ even when codebook size is 264. Unless otherwise stated, we adopt BSQ by default. 3.3 Infinite-Vocabulary Classifier The visual tokenizer obtains discrete labels by quantizing residual features. Consequently, the transformer predicts next-scale residual features labels yk [0, Vd)hkwk and optimizes the target through the cross-entropy loss. Previous works [61, 74] directly predict these index labels using classifier of Vd classes. However, it suffers from two drawbacks, huge computational costs and fuzzy supervision. As illustrated in Section 3.2, we exploit bitwise VQ-VAE as the visual tokenizer, where the vocabulary size Vd is extremely large. For example, if Vd = 232 and = 2048, conventional classifier would require weight matrix RhVd of 8.8 trillion parameters, which exceeds the limits of current computational resources. Moreover, VQ-VAE exploits the sign function during quantization as in Eq.4. After that, the positive elements are multiplied with the corresponding base and summed to get the index label yk(m, n) as in Eq.5, where [0, hk) and [0, wk). 5 yk(m, n) = d1 (cid:88) p=0 1Rk(m,n,p)>0 2p (5) Owing to the merits of the quantization method, slight perturbations to those near-zero features cause significant change in the label. As result, the conventional index-wise classifier [61, 11, 75] is difficult to optimize. To address the problems in computation and optimization, we propose Infinite-Vocabulary Classifier (IVC). As shown in Fig.2, instead of using conventional classifier with Vd classes, we use binary classifiers in parallel to predict if the next-scale residual Rk(m, n, p) is positive or negative, where = log2(Vd). The proposed Infinite-Vocabulary Classifier is much more efficient in memory and parameters compared to the conventional classifier. When Vd = 216 and = 2048, it saves 99.95% parameters and GPU memory. Besides, when there exist near-zero values that confuse the model in some dimensions, the supervision in other dimensions is still clear. Therefore, compared with conventional index-wise classifiers, the proposed Infinite-Vocabulary Classifier is easier to optimize. 3.4 Bitwise Self-Correction Weakness of teacher-forcing training. VAR [61] inherits the teacher-forcing training from LLMs. However, next-scale prediction in vision is quite different from next-token prediction in language. Specifically, we cannot decode the complete image until residuals Rk from all scales are obtained. We find that the teacher-forcing training brings about severe train-test discrepancy for visual generation. In particular, the teacher-forcing training makes the transformer only refine features in each scale without the ability to recognize and correct mistakes. Mistakes made in former scales will be propagated and amplified in latter scales, finally messing up generated images (left images in Fig.12). In this work, we propose Bitwise Self-Correction (BSC) to address this issue. In particular, we obtain Rf lip via randomly flipping the bits in Rk with probability uniformly sampled from [0, p], imitating different strengths of errors made in the prediction of the k-th scale. Here comes the key component of bitwise self-correction. Rf lip contains errors while Rk doesnt. After replacing Rk with Rf lip as predictions on the k-th scale, we recompute the transformer input (cid:101)Fk. Besides, re-quantization is performed to get new target Rk+1. The whole process of bitwise self-correction is illustrated in Alg.2. We also provide simplified illustration in Fig.3 (right) for better understanding. Notably, BSC is accomplished by revising the inputs and labels of the transformer. It neither adds extra computational cost nor disrupts the original parallel training characteristics. k Each scale undergoes the same process of random-flipping and re-quantization. The transformer takes partially randomly flipped features as inputs, taking the prediction errors into consideration. The re-quantized bit labels enable the transformer to autocorrect errors made in former predictions. In such way, we address the train-test discrepancy caused by teacher-forcing training and empower Infinity to have the self-correction ability. Algorithm 1 Visual Tokenizer Encoding Algorithm 2 Encoding with BSC Input: raw feature , scale schedule 1), ..., (hr K, wr 1, wr {(hr K)} Rqueue = [] (cid:101)Fqueue = [] for = 1, 2, , do multi-scale bit labels inputs for transformer Rk = Q(down(F Fk1, (hk, wk)) Queue_Push(Rqueue, Rk) Fk = (cid:80)k i=1 up(Ri, (h, w)) (cid:101)Fk = down(Fk, (hk+1, wk+1)) Queue_Push( (cid:101)Fqueue, (cid:101)Fk) end for Output: Rqueue, (cid:101)Fqueue Input: raw feature , random flip ratio p, scale 1, wr 1), ..., (hr K, wr K)}, schedule {(hr Rqueue = [], (cid:101)Fqueue = [] for = 1, 2, , do k1 , (hk, wk))) Rk=Q(down(F lip Queue_Push(Rqueue, Rk) Rf lip = Random_Flip(Rk, p) = (cid:80)k lip i=1 up(Rf lip (cid:101)Fk = down(F lip Queue_Push( (cid:101)Fqueue, (cid:101)Fk) , (h, w)) , (hk+1, wk+1)) end for Output: Rqueue, (cid:101)Fqueue 6 Table 1: Evaluation on the GenEval [24] and DPG [29] benchmark. result is with prompt rewriting. Methods # Params GenEval DPG Two Obj. Position Color Attri. Overall Global Relation Overall Diffusion Models LDM [49] SDv1.5 [49] PixArt-alpha [13] SDv2.1 [49] DALL-E 2 [45] DALL-E 3 [7] SDXL [43] PixArt-Sigma [12] SD3 (d=24) [21] SD3 (d=38) [21] 1.4B 0.9B 0.6B 0.9B 6.5B - 2.6B 0.6B 2B 8B AutoRegressive Models LlamaGen [55] Chameleon [59] HART [58] Show-o [70] Emu3 [66] Infinity 0.8B 7B 732M 1.3B 8.5B 2B 0.29 0.38 0.50 0.51 0.66 - 0.74 0.62 0.74 0.89 0.34 - - 0.80 0.81 0.85 0.02 0.04 0.08 0.07 0.10 - 0.15 0.14 0.34 0.34 0.07 - - 0.31 0.49 0.49 0.05 0.06 0.07 0.17 0.19 - 0.23 0.27 0.36 0.47 0.04 - - 0.50 0.45 0.57 0.37 0.43 0.48 0.50 0.52 0.67 0.55 0.55 0.62 0.71 0.32 0.39 0.56 0.68 0.66 0.73 - 74.63 74.97 77.67 - 90.97 83.27 86.89 - - - - - - 93.11 - 73.49 82.57 80.72 - 90.58 86.76 86.59 - - - - - - 90.76 - 63.18 71.11 68.09 - 83.50 74.65 80.54 84.08 - 65.16 - 80.89 67.48 81.60 83. 3.5 Dynamic Aspect Ratios and Position Encoding Infinity can generate photo-realistic images with various aspect ratios, which is significantly different from VAR [61] that can only generate square images. The main obstacles of generating various aspect ratio images lie in two folds. The first is to define the height hk and width wk of Rk based on varying aspect ratios. In the supplementary material, we pre-define list of scales, also called scale schedule, as {(hr K)} for each aspect ratio. We ensure that the aspect ratio of each tuple (hr k) is approximately equal to r, especially in the latter prediction scales. Additionally, for different aspect ratios at the same scale k, we keep the area of hr to be roughly equal, ensuring that the training sequence lengths are roughly the same. 1), ..., (hr wr K, wr k, wr 1, wr Secondly, we need to carefully design resolution-aware positional encoding method to handle features of various scales and aspect ratios. This issue poses significant challenge, as the existing solutions [65, 61, 54, 26, 40] exhibit substantial limitations under such conditions. In this paper, we apply RoPE2d [26] on features of each scale to preserve the intrinsic 2D structure of images. Additionally, we exploit learnable scale embeddings to avoid confusion between features of different scales. Compared to learnable APE element-wisely applied on features, learnable embeddings applied on scales bring fewer parameters, can adapt to varying sequence lengths, and are easier to optimize."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Dataset Data Curation. We curated large-scale dataset from open-source academic data and high-quality internally collected data. The pre-training dataset is constructed by collecting and cleaning opensource academic datasets such as LAION [51], COYO [10], OpenImages [33]. We exploit an OCR model and watermark detection model to filter undesired images with too many texts or watermarks. Additionally, we employ Aesthetic-V2 to filter out images with low aesthetic scores. 4.2 Implementation Infinity redefines text-to-image as coarse-to-fine, next-scale prediction task. In line with its architecture, we propose to train Infinity in progressive strategy. Specifically, we first train Infinity of 2B parameters on the pre-training dataset with 256 resolution for 150k iterations using batch size of 4096 and learning rate of 6e-5. Then we switch to 512 resolution and train 110k iterations using the same hyper-parameters. Next, we fine-tune Infinity at 1024 resolution with smaller, high-quality dataset. In this stage, we train Infinity for 60k iterations using batch size of 2048 and learning rate of 2e-5. All training stages use images with varying aspect ratios. Figure 4: Qualitative results from Infinity. 8 As for evaluation, we report results on popular text-to-image benchmarks like GenEval [24] and DPG [29]. We also measure our method on two human preference evaluation benchmarks, i.e., ImageReward [71] and HPSv2.1 [69]. These two benchmarks have trained models to predict human preference scores by learning from abundant human-ranked text-image pairs. We also build validation set consisting of 40K text-image pairs to measure FID. 4.3 Text-to-Image Generation 4.3.1 Qualitative Results Overall Results. Fig.1 and Fig.4 present generated images from our Infinity-2B model, showcasing Infinitys strong capabilities in generating high-fidelity images from various categories following user prompts. Qualitative comparison results among Infinity and other top-tier models can be found in the appendix. Prompt-Following. Fig.6 presents three examples demonstrating the superior prompt-following ability of Infinity. As highlighted in red, Infinity consistently adheres to user prompts, whether they are short or extremely long texts. We attribute these improvements to the bitwise token prediction and scaling autoregressive modeling. Text Rendering. As illustrated in Fig.7, Infinity can render text according to user prompts across diverse categories. Despite diverse backgrounds and subjects, Infinity accurately renders corresponding texts according to user requirements, such as fonts, styles, colors, and more. Benchmark. As in Tab 1, on GenEval[24], our model with re-writer achieves the best overall score of 0.73. Besides, Infinity also reaches the highest position reasoning score of 0.49. On DPG [29]. Our model reaches an overall score of 83.46, surpassing SDXL [43], Playground v2.5 [36], and DALLE 3 [7]. Whats more, Infinity achieves the best relation score of 90.76 among all open-source T2I models, demonstrating its stronger ability to generate spatially consistent images based on user prompts. Figure 5: Human Preference Evaluation. We ask users to select the better one in side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models. Human Preference Evaluation. We conduct human preference evaluation in both human studies and benchmarks. As in Fig.5, the generation results of Infinity are more frequently selected by humans in terms of overall quality, prompt following, and visual aesthetics in contrast to other open-sourced T2I models. Please refer to the appendix for more details. Tab.2 lists the results of two human preference benchmarks, i.e., ImageReward [71] and HPSv2.1 [69]. Infinity reaches the highest ImageReward and HPSv2.1, indicating our method could generate images that are more appealing to humans. Figure 6: Prompt-following qualitative comparison. We highlight text in red that Infinity-2B consistently adheres to while the other four models fail to follow. Zoom in for better comparison. Figure 7: Text rendering results from our Infinity-2B model. Infinity-2B could generate text-consistent images following user prompts across diverse categories. 10 Inference Latency. As shown in Tab. 2, Infinity demonstrates significant advantage in generation speed compared to diffusion models at around 2 billion parameters. Furthermore, our tests reveal that the speed advantage of Infinity becomes more substantial as the model size increases. Infinity achieves 7 faster inference latency compared to SD3.5 [21] at the same 8 billion parameters. Table 2: Human Preference Metrics and Inference Latency. We compared our method with SoTA open-source models. Infinity achieved the best human preference results with the fastest speed."
        },
        {
            "title": "Methods",
            "content": "# Params ImageReward HPSv2.1 Latency"
        },
        {
            "title": "Score Rank Score Rank Time",
            "content": "SD-XL [43] SD3-Medium [21] PixArt Sigma [12] Infinity 2.6B 2B 630M 2B 4 3 2 1 0.600 0.871 0.872 0.962 4 3 2 1 30.06 30.91 31.47 32. 4 3 2 1 2.7s 2.1s 1.1s 0.8s Table 3: Comparison of memory consumption (GB) between different quantizers during training. As codebook dimension increases, MSR-BSQ shows significant advantages over MSR-LFQ, enabling nearly infinite vocabulary size of 264. Quantizer = 16 = = 20 = 32 = 64 LFQ BSQ 37.6 32.4 53.7 32. OOM OOM OOM 32.4 32.4 32.4 Table 4: By scaling up visual tokenizers vocabulary, discrete tokenizer surpasses continuous VAE of SD [48] on ImageNet-rFID. VAE (stride=16) Vd = 216 Vd = 224 Vd = 232 Vd = 264 SD VAE [49] TYPE IN-256 rFID IN-512 rFID Discrete Discrete Discrete Discrete Contiguous 1.22 0.75 0.61 0.33 0.87 0.31 0.30 0.23 0.15 N/A Table 5: IVC saves 99.95% params and gets better performance to conventional classifier (Vd = 216) Classifier # Params vRAM Recons. Loss FID ImageReward HPSv2.1 Convention IVC 124M 0.65M 2GB 10MB 0.184 0.180 4.49 3.83 0.79 0.91 31.95 32. Table 6: Model architectures for scaling visual autoregressive modeling. Note that GFLOPs are rough values since they are affected by the length of the text prompt. # Params GFLOPs Hidden Dimension Heads Layers 125M 361M 940M 2.2B 4.7B 30 440 780 1500 2600 768 1152 1536 2080 2688 8 12 16 20 12 16 24 32 40 4.4 Scaling Visual Tokenizers Vocabulary Scaling Up the Vocabulary Benefits Reconstruction. Restricted by the vocabulary size, discrete VQ-VAEs have always lagged behind continuous ones, hindering the performance of AR-based T2I models. In this work, we successfully train discrete VQ-VAE matching its continuous counterparts by scaling up the vocabulary size. As in Tab. 4, we observe consistent rFID improvements as scaling up the vocabulary size from 216 to 264. Its noteworthy that our discrete tokenizer achieves rFID of 0.61 on ImageNet 256256 when Vd = 232, outperforming the continuous VAE of SD [49]. 11 Figure 8: Impact of Infinite-Vocabulary Classifier. Predicting bitwise labels with the InfiniteVocabulary Classifier (Right) generates images with richer details compared to predicting index-wise labels using conventional classifier (Left). Figure 9: Effects of Scaling Up the Vocabulary. We analyze the impact of scaling the vocabulary size under consistent training hyperparameters throughout. Vocabulary size Vd = 216 converges faster and achieves better results for small models (125M and 361M parameters). As we scale up the model size to 2.2B, Infinity with vocabulary size Vd = 232 beats that one with Vd = 216. Experiment with 5M high-quality image-text pair data under 256 256 resolution. 12 Figure 10: Effects of Scaling Visual AutoRegressive Modeling. We analyze the impact of scaling model size under consistent training hyperparameters throughout (Experiment with 10M pre-training data and 256 256 resolution). Validation loss smoothly decreases as function of the model size and training iterations. Besides, Validation loss is strong predictor of overall model performance. There is strong correlation between validation loss and holistic image evaluation metrics. Infinite Vocabulary Classifier Benefits Generation. We compare predicting bit labels with IVC to predicting index labels using conventional classifier under the vocabulary size of 216, since larger vocabulary causes OOM for the conventional classifier. We use the reconstruction loss on Rk, FID on the validation set and ImageReward for comprehensive evaluation. As shown in Tab.5, IVC achieves lower reconstruction loss and FID, suggesting IVC has better fitting capabilities. Beyond the quantitative results, training Infinity with IVC yields images with richer details as in Fig.8, which is consistent with higher ImageReward. 4.5 Scaling Bitwise AutoRegressive Modeling Scaling Up the Vocabulary Benefits Generation. We then scale up the vocabulary size to 232 during training the T2I model, which exceeds the range of the Int32 data type and can be considered infinitely large. In Fig.9, we illustrate the effect of scaling up the vocabulary from 216 to 232 for image generation. For small models (125M and 361M), the vocabulary size of 216 converges faster and achieves better results. However, as we scaled up the transformer to 2.2B, the vocabulary size of 232 beats 216 after 40K iterations. Therefore, its worthwhile to scale up the vocabulary along with scaling up the transformer. As illustrated in Tab.1,2, with infinite vocabulary and IVC, Infinity achieves superior performance among various benchmarks, elevating the ceiling of AR visual generation. Scaling Up Transformer Benefits Generation. In Fig.10, we depict the validation loss against the total training iterations and computational FLOPs for various model sizes of Infinity. The detailed model architectures for different sizes can be found in Tab.6. We consistently notice reduction in validation loss with an increase in training steps and computational FLOPs. Nevertheless, the advantages gained from training smaller models for extended periods lag behind those obtained from training larger models for shorter durations. This trend aligns with findings in language models, emphasizing the promising outlook for increasing model sizes with appropriate training. In Fig.10, we plot GenEval, ImageReward, and HPSv2 scores against validation loss for different model sizes ranging from 125M to 4.7B. We observe strong correlation between validation loss and evaluation metrics. To further quantify their correlation, we calculate the Pearson correlation coefficients through linear regression. The correlation coefficients for GenEval, ImageReward, and HPSv2 are -0.983, -0.981, and -0.979, respectively. These results demonstrate nearly linear correlation between validation loss and the evaluation metrics when scaling up model sizes from 13 Figure 11: Semantics and visual quality improve consistently with scaling up model size and training compute. Zoom in for better comparison. 125M to 4.7B. This promising phenomenon encourages us to scale up Infinity to achieve better performance. Visualization of Scaling Effects. To delve deeper into the scaling effect of Infinity, we compare set of generated 256256 images of three model sizes (125M, 940M, 4.7B) across three distinct training schedules (10K, 40K, 90K iterations) as illustrated in Fig.11. The semantics and visual quality of generated images improve steadily when scaling up model size and training compute, which is consistent with the scaling behaviors of Infinity. 4.6 Bitwise Self-Correction In Tab.7 and Fig.12, we list the evaluation metrics and present images generated by models trained using teacher-forcing and bitwise self-correction methods. Substantial advantages are observed after applying bitwise self-correction. Furthermore, we prove that the significant advantages are primarily driven by the self-correction mechanism rather than applying flipping. As shown in Tab.7, simply random flipping Rk doesnt bring improvements. Self-Correction imitates prediction errors and applies re-quantification to correct them. We emphasize that Self-Correction is essential for AR-based T2I models since it empowers models to correct errors automatically, significantly mitigating the train-test discrepancy. 14 Figure 12: Impact of Self-Correction. Teacher-forcing training introduces great train-test discrepancy which degrades performance during inference (left). Bitwise Self-Correction auto-corrects mistakes and thus generates better results (right). Decoding with τ = 1 and cf = 3. Table 7: Bitwise Self-Correction makes significant improvements. Experiment with 5M high-quality data and 512 512 resolution. FID is measured on the validation set with 40K images. Decoding with τ = 1 and cf = 3. Method FID ImageReward HPSv2.1 Baseline Baseline + Random Flip Baseline + Bitwise Self-Correction 9.76 9.69 3.48 0.52 0.52 0.76 29.53 29.20 30.71 4.7 Ablation Studies Optimal Strength for Bitwise Self-Correction. Bitwise Self-Correction mitigates the train-test discrepancy caused by teacher-forcing training. Here we delve into the optimal strength for applying bitwise self-correction in Tab.8. We empirically find that mistake imitation that is too weak (10% and 20%) fails to fully leverage the potential of Bitwise Self-Correction. Random flipping 30% bits yields the best results. Table 8: Comparison between different strengths of Bitwise Self-Correction. Experiment with 5M high-quality data and 512 512 resolution. Decoding with τ = 1 and cf = 3. Method FID ImageReward HPSv2.1 w/o Bitwise Self-Correction Bitwise Self-Correction (p = 10%) Bitwise Self-Correction (p = 20%) Bitwise Self-Correction (p = 30%) 9.76 3.45 3.48 3.33 0.515 0.751 0.763 0.775 29. 30.47 30.71 31.05 Figure 13: Comparison between learnable APE and our positional embeddings. Our method, i.e., applying RoPE2d along with learnable scale embeddings on features of each scale, converges faster and reaches higher training accuracy. Positional Embedding. Learnable APE adopted in VAR [61] brings too many parameters and gets confused when the sequence length varies. However, the sequence length changes frequently when training with various aspect ratios. Simply applying RoPE2d [26] or normalized RoPE2d [40] can 15 not distinguish features from different resolutions. In this work, we apply RoPE2d and learnable scale embeddings on features of each scale. RoPE2d preserves the intrinsic 2D structure of images. Learnable scale embeddings avoid confusion between features of different scales. To verify the effectiveness, we compare it with the learnable APE in Fig.13. Its obvious that applying RoPE2d along with learnable scale embeddings on features of each scale converges faster and reaches higher training accuracy. Decoding. Decoding is crucial for improving generation quality. VAR adopts the pyramid ClassiferFree Guidance (CFG) on predicted logits. That is, the strength of CFG increases linearly as the scale goes from 1 to K. Such pyramid scheme is used to tackle the issue of the model collapsing frequently when applying large CFG at early scales. We found that Infinity supports large CFG values even in very early scales equipped with Bitwise Self-Correction. Since Infinity is more robust to sampling, we revisit different decoding methods and find the best as illustrated in Tab.9. We visualize the comparison results of different decoding methods in Fig.14. We achieve the best generation results. Table 9: Comparison between different decoding methods. Method Param FID ImageReward HPSv2.1 Greedy Sampling Normal Sampling Pyramid CFG Pyramid CFG CFG on features CFG on logits CFG on logits (Ours) τ = 0.01, cf = 1 τ = 1.00, cf = 1 τ = 1.00, cf = 1 3 τ = 1.00, cf = 1 5 τ = 1.00, cf = 3 τ = 1.00, cf = 3 τ = 1.00, cf = 4 9.97 4.84 3.48 2.98 3.00 2.91 2.82 0.397 0.706 0.872 0.929 0.953 0.952 0.962 30.98 31.59 32.48 32.32 32.13 32.31 32.25 Figure 14: Comparison of different sampling methods. In contrast to Greedy Sample, Normal Sample and Pyramid Sample, our method could generate images with richer details and higher text-image alignments."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Infinity, bitwise visual autoregressive model to perform Text-to-Image generation. Infinity is pioneering framework for bitwise token modeling with the IVC and self-correction innovation. Extensive qualitative and quantitative results demonstrate Infinity significantly raised the upper limit for Autogressive Text-To-Image generative models, matching or surpassing leading diffusion models. We believe our framework, Infinity, will substantially promote the development of autoregressive visual modeling and inspire the community for faster and more realistic generation models."
        },
        {
            "title": "6 Acknowledges",
            "content": "Many colleagues from ByteDance supported this work. We are grateful to Guanyang Deng for his efforts in data processing. We also thank Chongxi Wang and Taekmin Kim for their contributions to model deployment. Special thanks to Xiaoxiao Qin for her work in human preference evaluation. Additionally, we are thankful to Hui Wu, Fu Li, Xing Wang, Hongxiang Hao, and Chuan Li for their contributions to infrastructure."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022. [5] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. [6] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [8] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. OpenAI, 2024. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. [11] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [12] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [13] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [14] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [15] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. [16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [18] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [20] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [23] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens, 2024. [24] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [25] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. [26] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2025. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [28] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [29] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [30] Aren Jansen, Daniel PW Ellis, Shawn Hershey, Channing Moore, Manoj Plakal, Ashok Popat, and Rif Saurous. Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 121125. IEEE, 2020. [31] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [32] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation, 2024. [33] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. [34] Black Forest Labs. Flux. https://blackforestlabs.ai/announcing-black-forest-labs/, 2024. [35] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [36] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [37] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. [38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [40] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-to-image generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. [41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [46] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 88218831. PMLR, 2021. [47] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [51] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [53] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [54] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [55] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [56] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. [57] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137, 2021. [58] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. [59] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [61] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [63] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 20 [64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [66] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [67] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [68] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. [69] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [70] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [71] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. [72] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [73] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [74] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [75] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [76] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023. [77] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. [78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [79] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024."
        },
        {
            "title": "A Predefined Scale Schedules",
            "content": "1, wr for each aspect ratio r, we predefine specific scale schedule As listed in Tab.10, {(hr K)}. We ensure that the aspect ratio of each tuple (hr K, wr 1), ..., (hr k) is approximately equal to r, especially in the latter scales. Additionally, for different aspect ratios at the same scale k, we keep the area of hr to be roughly equal, ensuring that the training sequence lengths are roughly the same. We adopt buckets to support training various aspect ratios at the same time. The consistent sequence lengths of different aspect ratios improve training efficiency. During the inference stage, Infinity could generate photo-realistic images covering common aspect ratios (1:1, 16:9, 4:3, etc.) as well as special aspect ratios (1:3, 3:1, etc.) following the predefined scale schedules. wr k, wr Table 10: Predefined scale schedules {(hr 1), ..., (hr K)} for different aspect ratios. Following the text guided next-scale prediction scheme, Infinity takes K=13 scales to generate 1024 1024 (or other aspect ratio) image. K, wr 1, wr Aspect Ratio Resolution Scale Schedule 1.000 (1:1) 0.800 (4:5) 1.250 (5:4) 0.750 (3:4) 1.333 (4:3) 0.666 (2:3) 1.500 (3:2) 0.571 (4:7) 1.750 (7:4) 0.500 (1:2) 2.000 (2:1) 0.400 (2:5) 2.500 (5:2) 0.333 (1:3) 3.000 (3:1) 10241024 (1,1) 8961120 (1,1) 1120896 (1,1) 8641152 (1,1) 1152864 (1,1) 8321248 (1,1) 1248832 (1,1) 7681344 (1,1) 1344768 (1,1) 7201440 (1,1) 1440720 (1,1) 6401600 (1,1) 1600640 (1,1) 5921776 (1,1) 1776592 (1,1) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (2,2) (4,4) (3,3) (3,3) (3,4) (4,3) (2,3) (3,2) (3,3) (3,3) (2,4) (4,2) (2,5) (5,2) (2,6) (6,2) (8,8) (6,6) (8,10) (4,5) (10,8) (5,4) (9,12) (6,8) (12,9) (8,6) (6,9) (4,6) (9,6) (6,4) (6,11) (4,7) (11,6) (7,4) (5,10) (3,6) (6,3) (10,5) (4,10) (6,15) (10,4) (15,6) (5,15) (3,9) (15,5) (9,3) (12,12) (12,15) (15,12) (12,16) (16,12) (10,15) (15,10) (8,14) (14,8) (8,16) (16,8) (8,20) (20,8) (7,21) (21,7) (16,16) (20,20) (16,20) (20,25) (20,16) (25,20) (15,20) (18,24) (20,15) (24,18) (14,21) (18,27) (21,14) (27,18) (12,21) (16,28) (21,12) (28,16) (11,22) (15,30) (22,11) (30,15) (10,25) (12,30) (25,10) (30,12) (12,36) (9,27) (36,12) (27,9) (24,24) (24,30) (30,24) (21,28) (28,21) (22,33) (33,22) (20,35) (35,20) (19,38) (38,19) (16,40) (40,16) (15,45) (45,15) (32,32) (40,40) (28,35) (36,45) (35,28) (45,36) (27,36) (36,48) (36,27) (48,36) (26,39) (32,48) (39,26) (48,32) (24,42) (32,56) (42,24) (56,32) (23,46) (30,60) (46,23) (60,30) (20,50) (26,65) (50,20) (65,26) (18,54) (24,72) (54,18) (72,24) (48,48) (44,55) (55,44) (45,60) (60,45) (42,63) (63,42) (40,70) (70,40) (37,74) (74,37) (32,80) (80,32) (30,90) (90,30) (64,64) (56,70) (70,56) (54,72) (72,54) (52,78) (78,52) (48,84) (84,48) (45,90) (90,45) (40,100) (100,40) (37,111) (111,37) Figure 15: Distribution of Prompt Categories"
        },
        {
            "title": "B Human Preference Evaluation",
            "content": "In order to measure the overall performance, we have conducted human preference evaluation. We build website and recruit volunteers to rank the generated images from different T2I models. Prompts. We have collected 360 prompts in total, including prompts randomly sampled from Parti [73] and other human-written prompts. As illustrated in Fig.15, these prompts are divided into nine categories, such as human (28%), animal (15%), products/artifacts (12%), landscape (9%), 1 Figure 16: Distribution of Prompts Challenges foods, indoor scene, architecture, plants, and text rendering. It is worth noting that we incorporate variety of human-related prompts, such as faces, bodies, and movements, in the human category as supplement to the Parti prompts. In Fig.16, we also list the challenges of these prompts, which includes simple prompts, complex prompts, quantity, positioning & perspective, painting style, detail, semantic understanding, color, and imagination. These statistics demonstrate that the prompts used for evaluation are balanced, covering various categories and challenges well. Generated Images. We compare Infinity with four open-source models: PixArt-Sigma [12], SD3Medium [21], SDXL [43], and HART [58]. The images of other models are generated by running their official inference code. No cherry-picking for any models. Human Evaluation. For the human evaluation process, we build website which presents two images from two anonymous models at the same time. There is one image generated by Infinity while the other is from other four models. Volunteers are required to pick better one from two images in terms of overall quality, prompt following, and visual aesthetics, respectively. Besides the aforementioned criterion, we make sure each side-by-side comparison is evaluated by at least two volunteers to reduce human bias. We filter out pairs with opposite results evaluated by two volunteers. These contradictory pairs are sent to third volunteer to assess. Then we take the consensus from three as the final results. Note that the whole process of human evaluation is completely double-blind. That is, volunteer doesnt know which model it is, as well as other volunteers results when performing side-by-side comparison. Results. As in Fig.6 of the submitted manuscript, we observe remarkable human preference for Infinity over the other four open-source models. Especially for the comparison with HART [58] (another SOTA AR-based model), Infinity earns 90.0%, 83.9%, and 93.2% win rate in terms of overall quality, prompt following, and visual aesthetics, respectively. As for the diffusion family, Infinity earns 76.0%, 79.0%, 66.0% win rate to PixArt-Sigma, SDXL and SD3-Medium, respectively. Whats more, Infinity reaches 71.1% win rate towards SD3-Medium regarding visual aesthetics. These results reveal that Infinity is more capable of generating visually appealing images. We attribute these great advantages to the proposed bitwise modeling, which has lifted the upper limits of AR models by large margins."
        },
        {
            "title": "C More Qualitative Results",
            "content": "Fig.17 shows the qualitative comparison results among Infinity and other top-tier models. The images of other models are obtained either by querying their open-source demo website (HART [58]) or running their official inference code locally (Flux-Schnell [34], SD3-Medium [21], and PixArt Sigma [12]). Whether thumbnail or zoom-in image, we observe significant differences among the generated images from different models. In particular, the AR model like HART generates images 2 Figure 17: T2I qualitative comparison among our Infinity-2B model and the other four open-source models. Here we select three diffusion models (Flux Schnell, SD3-Medium and PixArt Sigma), one AR model (HART) for comparison. Zoom in for better comparsion. with fewer details, blurred human faces and texture-less background compared to diffusion models. In contrast, Infinity overcomes those shortcomings of AR models and generates comparable or better images when compared to diffusion models like Flux-Schnell, SD3-Medium, and PixArt Sigma. For the first and second examples, Infinity adheres to the text prompts better than SD3-Medium, HART, and PixArt-Sigma. For the third and fourth examples, Infinity performs better in human hands and legs. For the last example, Infinity and PixArt Sigma have successfully generated images in an oil painting style while the other three failed. Flux Schnell performs worst in this example."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}