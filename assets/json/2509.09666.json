{
    "paper_title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
    "authors": [
        "Zhiyuan Yan",
        "Kaiqing Lin",
        "Zongjian Li",
        "Junyan Ye",
        "Hui Han",
        "Zhendong Wang",
        "Hao Liu",
        "Bin Lin",
        "Hao Li",
        "Xue Xu",
        "Xinyan Xiao",
        "Jingdong Wang",
        "Haifeng Wang",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 6 6 9 0 . 9 0 5 2 : r Can Understanding and Generation Truly Benefit Together or Just Coexist? Zhiyuan Yan1, 2*, Kaiqing Lin1*, Zongjian Li1, 3*, Junyan Ye4*, Hui Han1, Zhendong Wang5, Hao Liu2, Bin Lin1, 3, Hao Li1, Xue Xu2, Xinyan Xiao2 , Jingdong Wang2, Haifeng Wang2, Li Yuan1 Equal Contributors, Corresponding Authors 1PKU, 2Baidu ERNIE, 3Rabbitpre AI, 4SYSU, 5USTC {zhiyuanyan@stu., yuanli-ece@}pku.edu.cn https://github.com/PKU-YuanGroup/UAE"
        },
        {
            "title": "Abstract",
            "content": "The pursuit of unified multimodal models (UMMs) has long been hindered by fundamental schism between understanding and generation. Current approaches typically treat them as separate endeavors with disjoint objectives, leading to missing cross-task mutual benefits. We argue that true unification requires more than just merging two tasks; it requires unified, foundational objective that intrinsically links them. In this paper, we introduce an insightful paradigm through the Auto-Encoder lensunderstanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding (encoding) and generation (decoding) processes, bringing mutual gains. To implement this, we propose UAE, novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose UnifiedGRPO via reinforcement learning (RL), which covers three stages: (1) cold-start phase to gently initialize both encoder and decoder with semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoders reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. surprising aha moment arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity. This co-evolution provides compelling evidence of breakthrough toward genuine multimodal unification and intelligence."
        },
        {
            "title": "Introduction",
            "content": "Unifying multimodal models (UMMs) that support both generation and understanding remains an urgent, unresolved challenge [33, 5, 35, 39, 25, 14, 45, 41, 9, 11]. But many key questions still exist: How should gains in understanding translate into better generationand vice versa? Should both be trained under joint objective rather than separate losses? And if that, how can that objective be optimized in complementary, mutually reinforcing wayan area still underexplored? Figure 1: Illustration of the key insight of our UAE, an Auto-Encoder inspired design, for unified multimodal understanding and generation. The encoder converts an input image into detailed caption (I2T), and the decoder reconstructs the image conditioned on that caption (T2I). We treat reconstruction similarity as the unified scorequantified by Unified-Bench and further optimized by Unified-GRPO, which uses the semantic similarity as the RL reward to reinforce coherent and bidirectional information flow. We observe that, as the unified score increases, the intermediate text output by the understanding model becomes progressively longer, reflecting multimodal aha moments and more unified understanding and generation system. Most existing works on UMMs [35, 25, 4] frequently report that optimizing diffusion-based generative objectives negatively degrade the understanding capability and learned representations (and conversely), making joint training brittle. Consequently, many works decouple the UMM problem, training understanding and generation modules separately, and foregoing potential cross-task benefits. These design choices and empirical observations have dampened confidence in truly unified systems: absent demonstrable mutual gains, unification collapses into training two large components side by side. genuinely unified approach, however, should deliver explicit, bidirectional gainsleveraging each task to strengthen the other, rather than merely bridging them as independent parts. In this paper, we argue that the Auto-Encoder view is central to unifying multimodal understanding and generation. We adopt simple reconstruction loop: starting from an input image, the understanding module produces textual description; the generation module then reconstructs the image given that text. Training the system so that the reconstruction matches the inputassessed by semantic similaritybinds both sides under single objective. In this view  (Fig. 1)  , the tasks are symmetric and complementary: the encoder compresses visual content into semantically rich caption (I2T), and the decoder expands that caption back to pixels (T2I). Successful reconstruction signals coherent bidirectional information flow and improved visuallanguage alignment. As unification strengthens, the caption is compelled to carry all necessary details, becoming more descriptive, while reconstructions become correspondingly more accurate. To operationalize this, we introduce UAE, new framework for unified multimodal learning, as illustrated in Fig. 2. We first pre-train the decoder on 700K image-caption pairs with long-context captions and 1024 resolution to capture fine-grained semantics and compositional logic. We then propose Unified-GRPO, the first reinforcement learning (RL) approach to benefit both understanding and generation modules in complementary manner. Our Unified-GRPO covers three stages: (1) Figure 2: The overall workflow of our Unified-GRPO, consisting of three stages: cold-start reconstruction, generation for understanding, and understanding for generation. Cold-start reconstruction phase to gently initialize both the encoder and decoder. (2) Generation for Understandingthe encoder is trained to produce informative captions that maximize the decoders reconstruction quality, thereby strengthening visual understanding; (3) Understanding for Generationthe decoder is refined to reconstruct from the text, forcing it to leverage every detail and improving long-context instruction following and generation fidelity. We further explore using the image embedding from the visual encoder instead of the output caption from the understanding model, as we believe the image embedding can be viewed as the most informative text. We fine-tune the decoder in progressive manner, from the shorter caption to the longer caption, and even the longest caption (targeted image) for generation. Surprisingly, we observe an emergent aha moment in multimodal learning: as RL progresses, the encoder autonomously generates longer, richer captions, while the decoder concurrently improves its ability to interpret them, yielding reconstructions of striking fidelity, as demonstrated in Fig. 1. This co-evolution offers compelling evidence of progress toward genuine multimodal unification. In summary, our work makes the following key contributions: We propose UAE, the first work based on an Auto-Encoder unification principle, casting understanding as the encoder (I2T) and generation as the decoder (T2I), with reconstruction as measurable signal of cross-modal information coherence. This resolves the longstanding schism between understanding and generation and provides an actionable, verifiable objective for unified multimodal models (UMMs). We develop Unified-GRPO, the first reinforcement learning (RL) scheme that jointly and mutually improves both understanding and generation: (1) Generation for Understanding trains the encoder to produce informative captions that maximize reconstruction quality; (2) Understanding for Generation refines the decoder to reconstruct from these captions, forcing it to leverage every detail. This bidirectional optimization forms positive feedback loop toward genuine unification. We report an emergent aha moment in multimodal learning: as RL progresses, the encoder autonomously emits longer, more descriptive captions while the decoder simultaneously achieves strikingly faithful reconstructions. This co-evolution offers compelling empirical evidence for unified multimodal intelligence. We release Unified-Bench, to the best of our knowledge, the first benchmark explicitly designed to measure the degree of unification in UMMs, rather than individually evaluating the generation or understanding capabilities. 3 Figure 3: Visualization results of UAE at 10241024 resolution. 4 Figure 4: The detailed illustration of our framework design. Our framework employs an autoregressive LVLM to process the input image embedding derived from the original image. The model generates text caption, which is then fed into the autoregressive LLM. From this, we extract the final hidden state and project it through connector into the decoders feature space, where it serves as the condition for image generation."
        },
        {
            "title": "2 UAE Methodology",
            "content": "2.1 Architecture Overview. Our system follows compact encodeprojectdecode AE-based design that couples Large VisionLanguage Model (LVLM) for multimodal understanding with strong diffusion transformer for image synthesis, as illustrated in Fig. 4. The LVLM converts the input (image and optional prompt) into rich semantic representation; lightweight projector then maps this representation to the decoders conditioning space; finally, the diffusion model expands this condition into pixels. This separation keeps the interface minimal, preserves the strengths of each component, and makes the system modular and scalable. Encoder. We adopt Qwen-2.5-VL 3B [3] as the base LVLM encoder. It consists of visual encoder paired with an autoregressive language model capable of processing visionlanguage inputs. For generation, the LVLM autoregressively processes the prompt and multimodal context to produce high-dimensional, context-rich representation. Rather than passing raw text to the decoder, we extract the last hidden state from the LLM and feed it to small MLP projector. The projected embedding serves as the decoders conditioning signal, providing compact semantic summary grounded in the LVLMs learned world knowledge. Decoder. For the visual decoder, we use well-pretrained diffusion model to reconstruct image pixels from the LVLMs semantic representation. Concretely, we employ SD3.5-large [10] and add minimal projector head (two linear layers) to match the LVLM embedding dimension to the conditioning channels expected by SD3.5. During synthesis, the diffusion decoder takes the projected semantic condition as input and generates images in the pixel space. This setup leverages the LVLM for high-level semantics while relying on the diffusion backbone for photorealistic, high-fidelity rendering. LoRA-Adaptation. Following previous works [21, 40], we apply LoRA [15] adaptation for both the encoder and decoder for RL post-training, as it can help preserve the rich semantic knowledge learned from pre-training while efficiently and effectively learning novel knowledge from the new task. We maintain the same settings of LoRA with Flow-GRPO [21]. 2.2 Data SFT (long-context T2I). We construct 700K textimage corpus to warm up the decoder for long-context generation. Each sample pairs 1024-resolution image with detailed caption above 250 English words. Captions are produced by InternVL-78B [6] over our private image collection, emphasizing objects, attributes, spatial relations, and scene composition. We show two examples of our proposed dataset (see Fig. 8 and Fig. 9). During pretraining, the caption is used as the input prompt and the diffusion transformer is trained to synthesize the corresponding image from this text. GPT-4o distillation (50K). To further strengthen caption quality and stylistic consistency, we use the auto-script from GPT-ImgEval [41] to distill an additional 50K high-resolution samples from GPT-4o, yielding high-fidelity, long-form (around 300 words) descriptions. These distilled pairs are included in the pretraining mix to anchor dense semantics and improve instruction following. RL stage data (1K). For the reinforcement learning phase, we carefully curate 1K high-quality real-photography images and supplement them with subset of Echo-4o data [42]. These images serve as targets for reconstruction-driven optimization, where the model is encouraged to close the loopcaption then reconstructso that improvements in understanding (captioning) translate into more faithful generation. 2.3 Unified-GRPO Pipeline of Image Reconstruction. In our method, Large Vision-Language Model (LVLM; e.g., Qwen-2.5-VL [3]) as an autoregressive encoder for understanding, and Diffusion Transformer (DiT; e.g., SD-3.5-large [10]) as decoder for generation. Given image x, the LVLM autoregressively produces collection of tokens y1:T with hidden states {ht}T t=1. Subsequently, we take the last hidden state hT and map it through an MLP projector g() to obtain the condition vector ctext = g(hT ) Rd. As an alternative branch, we use visual-only condition cimg = g(v), where = fvis(x) is the image embedding from the LVLMs vision encoder (or dedicated image encoder). The decoder is an SD3-style diffusion transformer (DiT); it generates pθ( c) with {ctext, cimg}. Finally, Algorithm of Group Relative Policy Optimization (GRPO). We adopt the GRPO algorithm [30] to alternately optimize the understanding (i.e., Encoder) and generation (i.e., Decoder) of UAE, aiming to leverage the synergistic benefits between understanding and generation. In GRPO, for given input, policy model π generates set of trajectories, denoted {oi}G i=1. In our application, each trajectory oi corresponds to the generation process of single reconstructed image xi, resulting in group of images {xi}G i=1. For this task, the reward signal is provided by reconstruction reward, which we define using frozen CLIP image encoder fI () as: The estimation of an advantage Ai for each trajectory are shown following: Ri(x, xi) = cos fI (x), fI (xi) . Ai = Ri mean({Rk}G std({Rk}G k=1) k=1) . (1) The policys parameters θ are then updated by maximizing the GRPO objective function: (θ) = cC, {oi}G i=1πθold (c) (cid:34) 1 (cid:88) i=1 1 Ti Ti1 (cid:88) (cid:16) t=0 min (cid:0)ri t(θ) Ai, clip(ri t(θ), 1 ε, 1 + ε) Ai (cid:17) (cid:1) β DKL(πθ πθref ) (cid:35) , (2) where ri t(θ) is the probability ratio between the current and old policies, and Ti is the length of i-th trajectory. In Stage-2 and 3, we introduce the specific instantiations of the policy and the trajectory. Stage-1: Cold-start reconstruction. To ensure the basic alignment, we jointly initialize the LVLM and the DiT only with semantic reconstruction loss that closes the imageLVLMDiT loop. For each x, form condition (ctext), generate pθ( c), and minimize LStage1(ϕ, θ) = ExD (cid:2) 1 cos fI (x), fI (xi) (cid:3). (3) 6 Method CLIP [28] LongCLIP [44] DINO-v2 [24] DINO-v3 [31] Overall GPT-4o-Image [23] BAGEL [8] BLIP-3o [4] Janus-Pro [5] OmniGen2 [36] Show-o [38] UniWorld-V1 [20] UAE 90.42 88.97 84.84 88.72 88.36 80.18 85.49 90. 94.37 93.35 90.24 93.45 93.11 86.75 91.53 94.35 81.74 78.55 68.31 78.30 77.70 58.20 72.12 81.98 77.27 73.05 62.86 70.61 74.07 51.51 66.83 77. 85.95 83.48 76.56 82.77 83.31 69.16 78.99 86.09 Table 1: Comparisons of unified score on Unified-Bench, the first benchmark specifically for evaluating the unification between understanding and generation models in the unified multimodal systems. Bold indicates the best result, and underlined denotes the second best. Here, there is neither an autoregressive loss for the LVLM (the I2T loss) nor denoising loss for the DiT (the T2I loss). This stage can ensure an effective collaboration between LVLM and DiT for image reconstruction. In this stage, the LVLM πϕ serves as the policy, while Stage-2: Generation for Understanding. the DiT pθ is frozen and functions as part of the reward evaluation environment alongside the CLIP encoder. For each input image x, we sample group of caption sequences {y(i)}G i=1 from the old policy πϕold( x). From each sequence y(i), we extract the last hidden state h(i) to form condition c(i) = g(h(i) ), which is subsequently used to synthesize an image x(i) pθ( c(i)). The LVLMs parameters ϕ are then updated by maximizing the GRPO objective in Equation (2). In this context, each trajectory oi corresponds to sampled caption sequence y(i) = (y(i) (i)). The probability ratio is thus defined as ri . This stage trains the LVLM to emit 1 , . . . , y(i) t(ϕ) = πϕ(y(i) πϕold (y(i) x,y(i) <t) x,y(i) <t) last-hidden representations that maximize the decoders reconstruction quality. Stage-3: Understanding for Generation. The roles are now reversed: the image generation model pθ (e.g., DiT) acts as policy, while the LVLM is frozen, serving to provide conditions = {ctext, cimg} for generation. Note that cimg is an alternative option, as we find that it produces very similar results to those using only the LVLM output caption. We argue that the semantic similarity between the ultra-descriptive caption and original image embedding can be high, resulting in similar condition for decoder for generation (see Sec. 5 for detailed discussion). We optimize pθ using the GRPO by sampling reverse-time generation trajectories. For given condition c, the policy pθ generates group of images {xi 0}G i=1. In this context, each trajectory oi corresponds to full reverse-time 1, . . . , xi sequence (xi 0), representing the denoising process from an initial noise sample xi , xi to the final image xi 0. The parameters θ of the generation model are then updated by maximizing the GRPO objective in Equation (2). For this stage, the per-step likelihood ratio is given by: ri t(θ) = pθ(xi pθold(xi 1 xi 1 xi , c) , c) . (4) The stochasticity arises from the SDE sampling of the reverse process."
        },
        {
            "title": "3 Unified-Bench: A Benchmark tailored for Evaluating the Unified Models",
            "content": "Motivation. As illustrated in Fig. 1, we view understanding (IT) and generation (TI) as closed loop whose two halves should mutually enhance each other. Judging image realism alone or caption fidelity alone cannot reveal whether system is truly unified. We therefore adopt reconstruction-based similarityour unified-scoreto directly test whether the semantics distilled during understanding are sufficient for faithful regeneration, and whether regeneration in turn validates the completeness of the understanding. Unified-Bench protocol. We construct Unified-Bench to measure this mutuality: starting from 100 diverse source images, the model first produces detailed caption using the prompt, You are 7 Method Single object Two object Counting Colors Position Color attribution Overall SDv2.1 [29] SDXL [26] IF-XL [7] Lumina-Next [46] SD3-medium [1] SD3.5-large [10] FLUX.1-dev [18] NOVA [9] OmniGen [37] TokenFlow-XL [27] Janus [35] Janus Pro [5] Emu3-Gen [33] Show-o [38] MetaQuery-XL [25] BLIP3-o 8B [4] UniWorld-V1 [20] UniWorld-V1 [20] OmniGen2 [36] BAGEL [8] BAGEL [8] UAE UAE 0.98 0.98 0.97 0.92 0.99 0.98 0.99 0.99 0.98 0.95 0.97 0.99 0.98 0.98 - - 0.99 0.98 1 0.99 0.98 1 1 Dedicated T2I 0.44 0.39 0.66 0.48 0.72 0.73 0.79 0.62 0.66 Unified Model 0.41 0.30 0.59 0.34 0.66 - - 0.79 0.81 0.64 0.81 0.84 0.84 0.82 0.5 0.74 0.74 0.46 0.94 0.89 0.81 0.91 0.84 0.60 0.68 0.89 0.71 0.80 - - 0.93 0.93 0.95 0.94 0.95 0.89 0.97 0.85 0.85 0.81 0.70 0.89 0.83 0.74 0.85 0.74 0.81 0.84 0.90 0.81 0.84 - - 0.89 0.89 0.88 0.88 0.95 0.90 0.95 0.07 0.15 0.13 0.09 0.33 0.34 0.20 0.33 0. 0.16 0.46 0.79 0.17 0.31 - - 0.49 0.74 0.55 0.64 0.78 0.71 0.73 0.17 0.23 0.35 0.13 0.60 0.47 0.47 0.56 0.43 0.24 0.42 0.66 0.21 0.50 - - 0.70 0.71 0.76 0.63 0.77 0.79 0.84 0.50 0.55 0.61 0.46 0.74 0.71 0.67 0.71 0.68 0.55 0.61 0.80 0.54 0.68 0.80 0.84 0.80 0.84 0.80 0.82 0.88 0.86 0.89 Table 2: Comparisons of text-to-image generation capability on GenEval [13] benchmark. refers to the methods using LLM rewriter. Bold indicates the best result, and underlined denotes the second best. helpful assistant. Your task is to provide an accurate description based on the content of the image, helping the generation model to create an image based on your description. Ensure that your description is clear and comprehensive, so that the generation model can produce an image that closely matches the content of the image based on your input. The same model then synthesizes an image from its own caption. We compute unified-scores between the reconstruction and the source using four widely adopted vision backbonesCLIP, LongCLIP, DINO-v2, and DINO-v3and report per-backbone similarities and an overall summary."
        },
        {
            "title": "4 Results",
            "content": "4.1 Unified Evaluation We assess the unified degree with the proposed Unified-Bench. Tab. 1 shows that our UAE achieves the best Overall unified score (86.09), surpassing GPT-4o-Image (85.95). Specifically, UAE obtains the top results on CLIP (90.50), DINO-v2 (81.98), and DINO-v3 (77.54), and statistical parity on LongCLIP (94.35 vs. 94.37). These consistent gains across contrastive (CLIP-family) and self-supervised (DINO-family) features suggest that our UAE framework can preserve layoutand texture-level semantics that translate into more faithful reconstructions. The next tier includes BAGEL (83.48), OmniGen2 (83.31), and Janus-Pro (82.77), while BLIP-3o (76.56) and Show-o (69.16) lag notablyhighlighting that strong performance in individual the understanding or generation task does not necessarily yield higher unified score (strong mutual enhancement in the ITI loop.) 4.2 Text-to-Image Generation We evaluate UAE on three benchmarksGenEval, GenEval++, and DPG-Benchthat probe compositional understanding and instruction-following in increasingly challenging settings. 8 Method Color Count Color/Count Color/Pos Pos/Count Pos/Size Multi-Count Overall Dedicated T2I 0.000 0.325 SDv2.1 [29] 0.050 0.375 SDXL [26] SD3-medium [1] 0.550 0.500 FLUX.1-Kontext [18] 0.425 0.500 0.350 0.625 FLUX.1-dev [18] 0.025 0.000 0.125 0.200 0.150 0.000 0.000 0.350 0.250 0.275 Janus-Pro [5] T2I-R1 [17] BLIP3-o 4B [4] BLIP3-o 8B [4] OmniGen2 [36] Bagel [8] UAE Unified MLLM 0.450 0.300 0.675 0.325 0.125 0.225 0.250 0.250 0.550 0.425 0.325 0.600 0.550 0.525 0.125 0.200 0.100 0.125 0.200 0.250 0.550 0.300 0.350 0.450 0.600 0.275 0.325 0.550 0.000 0.000 0.175 0.300 0.200 0.075 0.075 0.125 0.125 0.125 0.250 0. 0.025 0.000 0.150 0.400 0.375 0.350 0.250 0.550 0.575 0.250 0.475 0.400 0.075 0.000 0.225 0.325 0.225 0.125 0.300 0.225 0.225 0.450 0.375 0.400 0.064 0.061 0.296 0.343 0.314 0.246 0.311 0.257 0.307 0.325 0.371 0. Table 3: Comparisons of instruction following generation ability on Geneval++ [13]. Bold indicates the best result, and underlined denotes the second best. Method Global Entity Attribute Relation Other Overall SDXL [26] Hunyuan-DiT [19] DALLE3 [22] SD3-medium [1] FLUX.1-dev [18] OmniGen [37] Show-o [38] EMU3 [33] TokenFlow-XL [27] Janus Pro [5] BLIP3-o 4B [4] BLIP3-o 8B [4] UniWorld-V1 [20] OmniGen2 [36] BAGEL [8] UAE 83.27 84.59 90.97 87.90 82.1 87.90 79.33 85.21 78.72 86.90 - - 83.64 88.81 88.94 83. Dedicated T2I 82.43 80.59 89.61 91.01 89.5 88.97 80.91 88.01 88.39 88.83 88.7 88.47 Unified Model 75.44 86.68 79.22 88.90 - - 88.39 88.83 90.37 91.43 78.02 86.84 81.29 89.40 - - 88.44 90.18 91.29 91. 86.76 74.36 90.58 80.70 91.1 87.95 84.45 90.22 85.22 89.32 - - 89.27 89.37 90.82 92.07 80.41 86.41 89.83 88.68 89.4 83.56 60.80 83.15 71.20 89.48 - - 87.22 90.27 88.67 84.32 74.65 78.87 83.50 84.08 84.0 81.16 67.27 80.60 73.38 84.19 79.36 81.60 81.38 83.57 85.07 84. Table 4: Comparisons of text-to-image generation ability on DPG-Bench [16] benchmark. Bold indicates the best result, and underlined denotes the second best. GenEval. As shown in Tab. 2, without considering LLM rewriting, our UAE attains the best Overall score among unified models (0.86). It leads on Counting (0.84) and Color attribution (0.79; +16 points vs. Bagels 0.63 and +3 vs. OmniGen2s 0.76), co-leads on Colors (0.90), is second-best on Position (0.71), and reaches 0.89 on Two object (below the strongest 0.940.95). When considering LLM rewriting, e.g., using the same rewritten prompts with Bagel, our UAE achieves an overall score of 0.89 on average, demonstrating the SOTA performance in the image generation task. GenEval++ (harder compositional control). GenEval++ [42] extends GenEval to prompts with three or more objects, each bearing distinct attributes and spatial relations, demanding comprehensive, multi-constraint satisfaction. In Tab. 3, UAE achieves the best Overall score (0.475), leading on 9 Method CLIP [28] LongCLIP [44] DINO-v2 [24] DINO-v3 [31] Overall Qwen-2.5-VL-3B [3] Qwen-2.5-VL-7B [3] UAE 88.34 88.26 90. 92.62 92.89 94.35 73.91 76.12 81.98 70.02 70.96 77.54 80.85 81.92 86.09 Table 5: Comparisons of captions produced by the understanding model for better text-to-image generation. Bold indicates the best result, and underlined denotes the second best. Color/Count (0.550) and Pos/Count (0.450), with runner-up performance on Color/Pos (0.550) and Multi-Count (0.400). Qualitative visualizations in Fig. 3 further show accurate attribute binding, disambiguation across multiple entities, and robust positioncount consistency under long, constraintheavy prompts. DPG-Bench. On DPG-Bench (Tab. 4), UAE achieves the top scores on Entity (91.43), Attribute (91.49), and Relation (92.07), and ranks second overall with 84.74, closely trailing Bagel (85.07). The sub-score pattern suggests UAEs advantages come from faithful entity grounding and relation handling under long prompts, translating into competitive end-to-end generation quality within unified architecture. 4.3 Improving Understanding Model as Better Captioner Core idea and training. Fig. 1 demonstrates that generation can benefit understanding. We turn the generator together with reconstructionsimilarity model (our unified-score) into reward model for training the understanding module as stronger captioner. Instead of optimizing only for readability, the captioner is directly optimized for reconstructability: it must describe the input image comprehensively and precisely so that the generator, conditioned on this caption, can reproduce all semantics of the original image. better captioner for generation. Under the Unified-Bench \"captiongeneratecompare\" protocol, captions produced by our trained understanding model yield the highest reconstruction similarity across all four backbones (Tab. 5): 90.50 (CLIP), 94.35 (LongCLIP), 81.98 (DINOv2), 77.54 (DINO-v3), with 86.09 Overall. Relative to Qwen-2.5-VL-7B (Overall 81.92) and 3B (80.85), our gains are especially pronounced on the DINO family. These results indicate that the caption generated by our understanding model is more suitable for generation, showing the stronger preservation of object structure and fine-grained, layout-aware semantics that matter for faithful reconstruction. Caption quality by commercial LLM judges. We further assess caption quality via pairwise comparisons against diverse opponents using four commercial LLM judges (Claude-4.1, GPT-4o, Grok-4, o4-mini). The prompt used for calling these LLMs is shown in Fig. 10. As shown in Tab. 6, our understanding model (using Qwen-2.5-VL-3B as the baseline) attains high average win rates: 94.7 vs. Show-o, 71.4 vs. OmniGen2, 64.3 vs. Bagel, and 76.3/71.5 vs. Qwen-2.5-VL (3B/7B), while remaining competitive with GPT-4o (47.2). The cross-judge agreement suggests our captions improve along multiple axescompleteness, attribute binding, relational and spatial fidelityprecisely the properties rewarded by the reconstruction-driven training signal. 4.4 Qualitative Analysis GenEval++ visualizations. Fig. 5 presents six representative prompts from GenEval++ [42], where each instruction contains three or more entities with distinct attributes and spatial relations. Across these examples, UAE shows three recurring strengths. First, it preserves attribute binding under multi-entity scenes: for three purple hair dryers and one pink surfboard, UAE attaches colors to the correct categories without leakage, whereas baselines often color surfboard purple or mix pink/purple across objects. Second, UAE is more reliable on discrete counts while respecting co-occurring constraints: for three beds on the above and three parking meters on the below, UAE maintains the 3+3 cardinality and the vertical arrangement; competing models tend to be off-by-one or satisfy the layout but drop meter/bed. Third, UAE handles left/right and grouping more faithfully: for an orange laptop on the left and purple knife on the right, our outputs keep the polarity and 10 Opponent GPT-4o [23] Bagel [8] OmniGen2 [36] Show-o [38] Qwen-2.5-VL-3B [3] Qwen-2.5-VL-7B [3] # Param Claude-4.1 GPT-4o Grok-4 o4-mini - 7B 3B 1.3B 3B 7B 47.4 57.7 67.9 97.8 76.3 68.8 89.4 92.9 97.6 100.0 99.0 99.0 30.6 58.3 63.5 89. 67.0 62.0 21.2 48.2 56.5 91.0 63.0 56.0 Avg 47.2 64.3 71.4 94.7 76.3 71. Table 6: Pairwise winning rate (%) of our trained understanding model (3B) against different opponents on Unified-Bench, evaluated by four judge models (using official commercial API). The Avg column reports the mean score across judges. avoid colorobject swaps that are common failure modes. Similar advantages emerge in the two cows, two books, and one donut and six vases prompts: UAE balances global composition with local details, maintaining counts while rendering plausible object geometry and material. These observations align with Tab. 3: UAE leads on Color/Count and Pos/Count, and is competitive on Color/Pos and Multi-Count, reflecting robust satisfaction of joint constraints rather than excelling on single dimension. An aha moment in Multimodal from RL. Fig. 1 and Fig. 6 illustrate how reinforcement learning with generator + reconstruction-similarity reward tightens the understandinggeneration loop. Visually, as training proceeds from left to right, captions produced by the understanding model become more complete and specific: early captions tend to state category and few salient attributes; mid-training captions begin to enumerate counts, colors, and spatial relations; later captions systematically cover accessories, materials, occlusions, backgrounds, and lighting (e.g., yellow knitted beanie, navy blue knitted turtleneck sweater, black-framed glasses, ears are not visible, blurred background, park-like setting). In lockstep, reconstructions grow closer to the source image. The bottom trend lines show two correlated signals: caption length increases (a proxy for semantic coverage) and the unified reward rises, with noticeable jumps whenever the captioner starts capturing previously omitted constraints (e.g., adding left/right or exact cardinalities). While longer captions are not inherently better, here the content added through RL is the kind that most directly benefits reconstruction, which is precisely what the reward encourages. Unified-Bench case study. Fig. 7 contrasts captions used for reconstruction on challenging example (small black dog wearing yellow beanie and glasses). Baselines reveal three typical errors. (i) Category drift: some misidentify the subject as monkey, causing the generator to synthesize an incorrect species. (ii) Attribute omissions or swaps: descriptions drop key items (beanie, glasses) or mismatch apparel colors, leading to reconstructions that caricature the outfit. (iii) Under-specified scenes: vague backgrounds and missing lighting cues prevent consistent photographic style at inference. UAEs caption, in contrast, enumerates the full set of semanticsspecies, apparel type and color, eyewear, pose, occlusions (ears are not visible), background style (blurred, park-like), and lightingproducing reconstruction that preserves identity, attire, and overall aesthetic. This example typifies the mechanism by which better understanding (denser, better-bound descriptions) yields better generation, echoing our Unified-Bench gains in Tab. 5."
        },
        {
            "title": "5 Discussion",
            "content": "In Stage-3  (Fig. 2)  , we explore an alternative branch On image-to-image (I2I) reconstruction. that replaces the long-caption embedding with dense image embedding from the visual encoder (similar to [32]). Although an image embedding is, in principle, richer than text, our experiments show that after long-text RL in Stage-3, subsequent I2I RL brings only marginal gains. We therefore view the two routeslong, highly descriptive captions vs. image embeddingsas near-equivalent in practice: both are produced in the same output space (Qwens conditioning embedding) and carry comparable semantic information for reconstruction. In short, once model has been trained to produce sufficiently descriptive long text, switching to I2I reconstruction could deliver little additional benefit, suggesting functional equivalence between the two Stage-3 alternatives. 11 Figure 5: Qualitative analysis of the results from GenEval++, where our UAE demonstrates visually consistent results aligned with the input captions, and performs reasonable composition for each element. On image editing. We regard editing as natural extension of I2I reconstruction with an added requirement: pixel-level preservation outside edited regions. Operationally, editing still ingests an image (plus brief instruction), obtains an output embedding via the LVLM, and renders with the generator; however, the objective differsbeyond semantic fidelity, one must minimize deviation (e.g., MSE) on untouched pixels. This favors injecting VAE image embeddings that encode low-level texture, geometry, and lighting, which recent strong editors (e.g., Qwen-Image [34], ImgEdit-E1 [43]) explicitly exploit. By contrast, systems that appear to rely primarily on semantic conditioning (e.g., without explicit VAE latents) tend to reconstruct global semantics yet struggle to perfectly preserve non-edited regions. Extending UAE to editing is therefore straightforward conceptually: augment the conditioning with VAE latents and train with reconstruction objective that jointly optimizes semantic compliance and pixel preservation within masked loss. On text rendering. Text rendering is current limitation. Our training data contains few high-quality, text-rich imagecaption pairs at 1024 resolution, and we did not perform targeted textspecific RL. Prior work (e.g., X-Omni [12]) shows that OCR-based rewards during RL substantially improve typography fidelity; recent results (e.g., Qwen-Image [34]) further underscore the importance of such supervision. Because our goal here is to present unified objective and training paradigm rather than feature-complete product, we focus RL on reconstruction similarity rather than OCR rewards. natural next step is Unified-GRPO for text, where the understanding module must Figure 6: Illustration of the generated results via our unified-GRPO, with the RL steps increasing, the understanding model (encoder) achieves better caption capability to produce longer, detailed, yet accurate caption to reconstruct the original image comprehensively; while the generation model (decoder) can take the detailed caption as input for better generation. capture glyph content and layout, and the generator is rewarded for reconstructing the original text. This finer-grained objective is likely to strengthen both understanding and generation in scenes containing signs, documents, and UI elements. On architectural evolution. In the first version of this work, we intentionally adopt minimal encoderconnectordecoder design (in the spirit of UniWorld-V1 [20]) to present the auto-encoder perspective as transparently as possible. Looking ahead, we plan two upgrades. First, refine the connector to better align the LVLMs output space with the diffusion decoders conditioning space (e.g., structured adapters, multi-scale cross-attention, or loss-aware projection). Second, push the diffusion component toward pure decoder and further decouple understanding and generation: the encoder focuses solely on semantic compression (IT or IE), the decoder on semantic decompression (T/EI), and unification arises from single reconstruction objective that drives mutual benefit. This tighter adherence to the auto-encoder principle should yield cleaner interfaces, more stable RL, and clearer pathways for plugging in specialized capabilities (e.g., editing via VAE latents, OCR-aware text rendering) without entangling the two halves of the system. On long-text supervision and conditioning. We argue that long text is foundational across all tasksunderstanding, generation, I2I, and editingbecause it provides higher-bandwidth alignment signal between vision and language [2]. From an alignment perspective, longer captions encode more complete semantics (entities, attributes, relations, counts, occlusions, background, lighting, style), reducing ambiguity in the conditioning and tightening the IT mapping. For understanding, captioner trained to produce very detailed descriptions must extract and organize nearly all recoverable image evidence; this improves coverage and attribute binding, andunder our unified rewarddirectly optimizes for reconstructability rather than mere fluency. For generation, decoder that can consume very long prompts can, in principle, render every clause of sufficiently descriptive caption (e.g., turning draw tangyuan into Chinese dessert. . . white, glutinous exterior with black sesame filling, . . . ), thereby collapsing instruction ambiguity and improving semantic fidelity. The same logic extends to I2I: dense visual embedding can be viewed as an extreme case of very long textual embedding that covers the entire scene; empirically, once Stage-3 long-text RL is in place, further I2I RL yields limited gains, supporting the practical nearequivalence between long-text conditioning and image-embedding conditioning in our framework. For editing, long text specifies what to change and what to preserve; combined with VAE latents for low-level pixel retention, long prompts help constrain edits to semantically precise, spatially localized operations. 13 Figure 7: Case study of the results from the proposed Unified-Bench, we see that our UAE enables to produce more detailed, accurate, comprehensive description based on the input image, and reconstructs similar result to the original image, showcasing the improved understanding and generation capabilities, and the better unification of the system. We also observe that GPT-4o-Image accepts very long inputs and reproduces their semantics with high fidelity, suggesting it has likely been fine-tuned on long-text supervision to internalize such granular constraints. However, despite its importance, the community still lacks truly largescale, high-resolution long-text corpus. This motivates our curated 700K long-text imagecaption set (above 250 English words, 1024-px images): it supplies the missing bandwidth for training captioners that cover all salient details and generators that can condition on them. Practically, long-text training introduces computational and modeling challenges (context length, positional extrapolation, redundancy control). Our design addresses these via lightweight projector/connector, reconstruction-based rewards that favor salient details over verbosity, and RL signals that penalize omissions and contradictionsturning long text from liability into precise, high-information interface that consistently improves unified performance."
        },
        {
            "title": "6 Conclusion\nWe show that an auto-encoder is a viable core for unifying multimodal understanding and generation.\nBuilding on this idea, we introduce UAE, which warms up the decoder on long-context captions. We\nthen propose Unified-GRPO, a three-stage RL procedure—cold-start-reconstruction, Generation-for-\nUnderstanding and Understanding-for-Generation—that jointly optimizes caption informativeness\nand reconstruction fidelity. To quantify progress toward unification, we present Unified-Bench, the\nfirst evaluation tailored to the bidirectional nature of UMMs. During training, we observe an \"aha\nmoment\": captions become longer and more precise while reconstructions sharpen, evidencing\ncoherent, bidirectional information flow. Together, these components offer a clear recipe and\nmeasurement protocol for building truly unified multimodal models.",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Sd3-medium. https://stability.ai/news/stable-diffusion-3-medium, 2024. [2] Hyojin Bahng, Caroline Chan, Fredo Durand, and Phillip Isola. Cycle consistency as reward: Learning image-text alignment without human preferences. arXiv preprint arXiv:2506.02095, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [5] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024. [7] DeepFloyd. Github link: https://github.com/deep-floyd/if, 2023. URL https://github.com/ deep-floyd/IF. [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [9] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [11] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation, 2024. URL https://arxiv.org/abs/2404.14396. [12] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [14] Agrim Gupta, Linxi Fan, Surya Ganguli, and Li Fei-Fei. Metamorph: Learning universal controllers with transformers. arXiv preprint arXiv:2203.11931, 2022. [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [16] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [17] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [18] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [19] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 15 [20] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [21] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [22] OpenAI. Dalle 3. https://openai.com/index/dall-e-3, 2024. [23] OpenAI. Gpt-4o. https://openai.com/index/introducing-4o-image-generation, 2025. [24] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [25] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [27] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25452555, 2025. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. [32] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024. [33] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [34] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [35] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 12966 12977, 2025. [36] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [37] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. 16 [38] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [39] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One Single Transformer to Unify Multimodal Understanding and Generation, October 2024. [40] Zhiyuan Yan, Jiangming Wang, Zhendong Wang, Peng Jin, Ke-Yue Zhang, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, and Li Yuan. Effort: Efficient orthogonal modeling for generalizable ai-generated image detection. arXiv preprint arXiv:2411.15633, 2, 2024. [41] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [42] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. [43] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [44] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pp. 310325. Springer, 2024. [45] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [46] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems, 37:131278131315, 2024. 17 Figure 8: Visual example of the proposed 700K long-context text-to-image dataset. Figure 9: Visual example of the proposed 700K long-context text-to-image dataset. 19 Figure 10: Original Prompt for official LLMs to judge the two captions outputted by the understanding models."
        }
    ],
    "affiliations": [
        "Baidu ERNIE",
        "PKU",
        "Rabbitpre AI",
        "SYSU",
        "USTC"
    ]
}