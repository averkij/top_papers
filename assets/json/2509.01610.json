{
    "paper_title": "Improving Large Vision and Language Models by Learning from a Panel of Peers",
    "authors": [
        "Jefferson Hernandez",
        "Jing Shi",
        "Simon Jenni",
        "Vicente Ordonez",
        "Kushal Kafle"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%"
        },
        {
            "title": "Start",
            "content": "Jefferson Hernandez1*, Jing Shi2, Simon Jenni2, Vicente Ordonez1, Kushal Kafle2 1Rice University, 2Adobe Research jefehern@rice.edu, {jingshi, jenni, kkafle}@adobe.com, vicenteor@rice.edu 5 2 0 2 1 ] . [ 1 0 1 6 1 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating peer review system, our models generate, assess, and refine outputs in response to curated set of prompts, mimicking classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%. 1. Introduction Large Vision and Language Models (LVLMs) have demonstrated impressive capabilities that require diverse set of skills such as compositional reasoning (e.g. [32, 34]), use of general knowledge (e.g. [2, 54]), pictorial reasoning about abstract figures (e.g. [48]), and character recognition (e.g. [73]). Learning generalist model that can tackle all these tasks at once has been challenge, as different LVLMs might feature complementary strengths depending on the richness of their training datasets. successful approach for improving both Large Language Models (LLMs) and LVLMs has been to further refine them by relying on human preference data to ensure that their outputs are more aligned with the expectations of human users [47, 49]. *Work done while interning at Adobe Research. Figure 1. Panel-of-Peers (PoP) generates candidate responses from multiple LVLMs. The panels scoring of these responses is used to build preference set, which is used to tune one or all the members of the panel, improving their accuracy individually. PoP significantly outperforms other forms of reaching consensus across many benchmarks. The training process for LVLMs has largely converged into three stages: 1) unimodal pretraining, where the vision encoder [45, 48, 76] and the LLM [3, 9, 22] are independently trained on large corpus of data; 2) multimodal pretraining, where the unimodal models are combined and trained on large corpus of image-text data [46, 51], sometimes augmented with extra knowledge (e.g., OCR, or grounding data); and 3) supervised fine-tuning (SFT) [32, 33] stage, where the model is trained on domain-specific data to enhance its performance on various downstream tasks. Despite extensive research into this three-stage pipeline, LVLMs continue to face challenges, including hallucination issues, misalignment between visual and textual representations, and persistent knowledge gaps. These limitations underscore the need to further refine our approach to enhance 1 Figure 2. Illustration of the overall learning from feedback from peers approach. Our post-alignment strategy involves rejection sampling, supervised finetuning, and preference optimization methods to learn from peers. See text for details. the alignment and reliability of LVLMs. Collecting high-quality multimodal data can be one of the most straightforward solutions but doing so on large scale is often expensive, and therefore most methods use low-quality, large-scale captioned image-text pairs, followed by fine-tuning with small-scale, higher-quality, supervised data. Many recent studies rely on learning from machinegenerated data from foundation models (e.g. GPT-4V [2], Gemini [54]). Still, the performance of the resulting models is limited by the performance of the foundation models [57, 77] and fails to significantly reduce the cost [57, 64]. In this work, we describe method for LVLMs to continue learning from peer-to-peer feedback, which we denote Panelof-Peers (PoP) that uses list of tasks/questions with no answers. This method takes inspiration from how students learn in classroom environment [43]. They are taught the basics in chapter and are asked to complete series of exercises that pose challenging questions without answers. By going through the exercises and discussing with their peers, they further solidify their understanding and emerge with an improved ability to complete the related tasks. Our peer-to-peer training methodology is summarized in Figure 2. We begin by constructing panel of peers (i.e., models of roughly the same capacity, trained on the same training set), but take advantage of the fact that some models might be naturally better for some tasks than their peers. Then, each model in the panel produces candidate answers for dataset of new prompts (without ground truth answers) and also evaluates the correctness of each others answers. These evaluations are then combined into final reward score using reward ensembling methods [12]. The resulting preference data is then used to fine-tune all the models in the panel (See Figure 1). This process results in the same number of models as in the original panel, all of which can be used for individual inference. Our extensive experiments show that such feedback from peers can augment the performance of the entire panel and individual LVLM members within the panel. Additionally, it can teach an individual panel member previously unseen task, provided that the other members possess that knowledge. Our main contributions are as follows: We introduce Learning from Panel-of-Peers (PoP), novel self-improvement paradigm for bootstrapping the capabilities of set of peer LVLMs. We show that PoP increases the average score on 15 selected benchmarks from 48 to 57 (9 absolute points). We demonstrate that PoP can enable knowledge transfer from panel members possessing certain abilities (e.g., OCR) to members that do not have that ability. We conduct extensive ablations regarding the choice of reward ensembling methods, the size/ability of panel members, allowing each member to produce one or multiple candidate answers, and the choice of alignment objectives to clearly show the effects of our modeling choices. 2. Related Work Our work is related to self-improving methods in LLMs, preference alignment in LVLMs, and the use of models as judges for the output of other models. LVLMs-as-a-judge. Strong LVLMs like GPT-4V [2] have been widely used to evaluate vision-language tasks [35, 72] through both pointwise [34, 53] and paired evaluations [39, 71]. Typically, this involves crafting scoring prompt to guide the evaluator in scoring, often referencing golden an2 swer. Our work diverges by scoring answers solely based on the models internal knowledge, foregoing reference answers. This aligns with efforts in golden-answer-free evaluation, such as Self-Taught Evaluators [59] and others [68, 74], which teach scoring through dedicated datasets. Additionally, the use of panel of models for evaluation has shown promise. For example, PoLL [56] demonstrated that panel of weaker models can produce human-aligned scores comparable to stronger models. Research suggests models excel at evaluation over generation [11, 19, 68], leveraging verification to self-improve. Prometheus-Vision [25] and LLaVACritic [67] further advanced evaluation by training models with curated datasets or user-defined scoring criteria. In contrast, we show that with robust evaluation prompts, models require no additional training to perform evaluations. Furthermore, we unify answer generation and evaluation within the same model, updating it through iterative self-improvement rather than keeping evaluation models fixed. Self-Improvement in Large Language Models. Various recent works have investigated self-improvement or selfplay strategies. Self-Rewarding Language Models [74] enable an LLM to act as both reward model and an answer generator, creating and ranking self-instruction data. SelfImproving Robust Preference Optimization [10] refines an LLMs answers through in-context learning with loss function akin to direct preference optimization [49]. Self-play Fine-Tuning [7] frames self-improvement as two-player game: one player generates answers indistinguishable from human-annotated data, while the other attempts to identify machine-generated answers. Self-Play Preference Optimization [65] similarly applies two-player game where the LLM interacts with its prior version, using an exponential weight update to solve the Nash equilibrium and ensure convergence. Unlike these LLM-focused methods, our Panel-of-Peers approach directly addresses LVLM modality misalignment by integrating peer feedback with supervised fine-tuning for more effective alignment. Alignment in Large Vision Language models. LLMs require alignment to ensure behavior matches human preferences [47, 49, 82]. In LVLMs, alignment methods focus on reducing hallucinations and enhancing modality alignment. For instance, LLaVA-RLFH [53] collected dataset of 10K human interactions to reduce hallucinations using RLFH on the LLaVA-1.0-7b model. POVID [79] and SeVa [81] simulate hallucinations by injecting errors, then aligning via DPO. Li et al. [30] collects 5k image-question pairs and obtain four responses per pair, which are scored by foundation model [54] using the prompt from [61]; alignment is performed via DPO. SIMA [60] creates two answers using different decoding algorithms and uses the models own selfcritic capabilities to create preference data for preference optimization. STIC [16] combines augmentations and adverse prompts to induce dispreferred data for DPO-based alignment, while CSR [80] uses beam search and CLIPScore to align responses by ranking preferred answers with DPO loss. VILA2 [19] takes task-specialized approach, constructing versions of the original model tailored to captioning, OCR, and general knowledge. Although these methods enhance alignment, they are resource-intensive, rely on human annotations, and may introduce biases. Notably, CSR [80] and VILA2 [19] are closest to our work: CSR uses calibrated rewards based on answer-image CLIPScore, which may suffice for captions but is limited in tasks like OCR and VQA. In contrast, PoP leverages knowledge gaps within peer models to achieve alignment across tasks. Unlike VILA2s multiple teachers-single student approaches, which require teacher re-training, PoP employs multiple students-to-multiple students setup, where models iteratively learn from each others knowledge gaps within the panel. 3. Learning with Panel-of-Peers We focus on autoregressive LVLMs where image tokens are projected into the embedding space of textual information and concatenated, method popularized by LLaVA [32, 34]. We start with models that have undergone the standard threestage training (pretrain-align-SFT) in all our experiments. We begin by constructing panel of models with similar performance (which we call peers) and devise novel learning algorithm where all members of the panel self-improve their capabilities in self-bootstrapping loop. Our method is called Panel-of-Peers (PoP). An overview of the stages of PoP is shown in Figure 2. Our method consists of two stages (1) candidate response generation and (2) data creation and fine-tuning, which take place alternately and can be used over multiple iterations. In the candidate response generation stage, we prompt the panel of peers with the image and question from selected dataset designed for the models to learn diverse set of tasks (captioning, OCR, general knowledge, among others). In the preference data creation and fine-tuning stage, each model performs peer-to-peer evaluations of the responses of the other models in the panel. reward score is created by combining the evaluations from all the models in the panel, which is then used alongside rejection sampling to create preference dataset. The final stage uses these data to fine-tune each model in the panel using preference alignment algorithm; we perform preference fine-tuning on the synthetic data, which has been found to work better than supervised fine-tuning techniques. This process is repeated iteratively; each time, the panel generates, evaluates, and learns from its answers. 3.1. Reward Modeling Inspired by consensus methods for multiple LLM judges [56, 68] and other works in machine learning that show the benefit of model ensembles, we propose peer-to-peer evaluation approach, where an ensemble of models scores each others output. When acting as judge, the model πi scores the output yj, i.e. The car is parked in spot 31, from one of its peer models πj(yjx). The peer model is tasked with answering query x, i.e. Where is the car parked? We create prompt that evaluates answers along five axes: Helpfulness, Correctness, Verbosity, Coherence, and Complexity [61, 62], to create pseudo-rewards for learning. Each score is graded on Likert scale from 1 to 5, summed, and then divided by 25 to normalize it to the 0-1 range, resulting in the final reward score. Following [56], we experiment with singleand relative-point scoring, where the judge is tasked with rating the quality of single model output independently or in comparison with other outputs. The judge receives natural language instructions on how the grading should be performed, detailed in full in the appendix, to create pseudorewards for learning. The rating is based solely on the judges internal knowledge of what constitutes quality output. We construct panel of peers. Each model independently assigns reward value between 0 and 1 to given peer output, resulting in reward Ri(yj) = πi(p yj), where is the answer from the model, is the original question, and is the evaluation prompt. To combine individual reward scores from the panel, we use mean voting Rµ(yj) to average the scores from all peers. 3.2. Candidate Response Generation Similar to how we use panel of peers to generate rewards and evaluate the output of an answer, our objective is to use the same panel to generate responses to build preference data. Each model in the panel receives the same input image and query pair and produces candidate response. Then, for each response y, we calculate its reward score Rµ(y). We apply an additional rejection sampling step before constructing the preference dataset, retaining only chosen samples with quality reward score of at least 0.85. We can also augment the responses generated using Best-of-N sampling, which is an inference-only alignment algorithm that works as follows. Let YN = {y(n)}N n=1 be the multi-set containing i.i.d. samples from πi(yix) for query x. Then, BoN algorithm returns y, where = argmax y(n)YN (cid:16) y(n)(cid:17) . (1) This method has been shown to be win-rate optimal and KL optimal asymptotically [4] to the preference alignment problem. Finally, after sampling answers using sentencelevel beam search from each of the models in the panel, we end up with candidate responses. These two components, (1) candidate response generation and (2) reward modeling, constitute the PoP algorithm. We also explore using single sample from each model, and this formulation is denoted as st-PoP. 3.3. Preference Curation and Iterative Training After generating candidate responses with their reward scores, our next step is to curate preference dataset. Here, for each input prompt, we select the responses with the highest and lowest cumulative reward scores as the preferred and dispreferred responses, respectively, to construct the preference dataset for fine-tuning. For each iteration t, we denote the constructed preference data as Dt = {(I, x, y(n), y(1))}D i=1, where represents the image, the text prompt, and y(n), y(1) are the highest and lowest ranked answers by the panel of peers after filtering. After obtaining the preference data, we fine-tune the whole panel of Large Vision-Language Models (LVLM) using SimPO. We choose SimPO [41] because it employs an implicit reward formulation that directly aligns with the generation metric, eliminating the need for reference model. Additionally, it introduces target reward margin γ to help separate the winning and losing responses. LSimPO-PoP(πθt+1, πθt) = (cid:20) log σ EDt (cid:18) β y(n) πθt(y(n)) β y(1) (cid:19)(cid:21) πθt(y(1)) γ (2) Where πθt+1 and πθt represent the models from the next and previous iterations, respectively. This optimization is repeated for each member of the panel. For the next iteration, the entire process of (1) candidate response generation and (2) data creation and fine-tuning are repeated, with the panel always initialized from the checkpoints of the previous iteration. To evaluate the effect of the alignment objective on performance, we compare SimPO with Supervised FineTuning (SFT) using the same curated preference dataset. 4. Experiment Settings Implementation Details. We use the original LLaVA-1.5 recipe of two stages: (1) multimodal pre-training and (2) supervised fine-tuning. Unless otherwise specified, rewards and responses are generated using the same visual backbone and the following LLMs: Mistral7B [22], Llama3-8B [3], and Vicuna-7B [9], resulting in three models for the panel of peers. These are trained on the BLIP-LAION-CC-SBU-558k dataset [34] for the stage 1, and the open source version of LLaVA-Instruct-mix665k dataset [32] for stage 2. The images and prompts used to construct the data are randomly sampled from the Cambrian-7M dataset [55], keeping the original proportions of Language: 21.00%, General Knowledge: 34.52%, OCR: 27.22%, Counting: 8.71%, Math: 7.20%, Code: 0.87%, and Scientific Knowledge: 0.88%. We sample 15 responses from each member of the panel (PoP) and compare against sampling single answer (st-PoP) as baseline. We reject samples with reward of less than 4 0.85 and maintain margin of 0.75 between preferred and dispreferred answers, as we found that this helps preference optimization. This process creates dataset with total of 300K samples per self-improvement iteration out of starting random sample of 1M images from the Cambrian-7M. It is worth noting that in each iteration, we start with more than the specified number of samples to ensure the required number is obtained after rejection sampling. Additionally, each iteration might include different samples. Overall, the iterative training is conducted over three iterations, using full fine-tuning on 8 A100 80GB GPUs. It takes roughly 80 hours to collect the preference data and 10 hours per model for the self-improvement step. For more detailed information on training hyperparameters and training data, please refer to the appendix. Evaluation Benchmarks. We conducted evaluations using VLMEvalKit1 select into the following catethe following datasets split gories: Chart&OCR: ChartQA [40], OCR-Bench [36], OCRVQA [42], TextVQA [52]; General VQA: MMBench [35], MM-Vet [72], SEED-Bench [26]; Knowlegde: [6], MathAI2D [23], MMMU(val) [37]; Hallucination: Vista(val) HallucinationBench [20], POPE [31]; Vision Centric: RealWorldQA [66]. More detailed descriptions of each dataset are discussed in the appendix. Baselines. We compare Panel-of-Peers Learning with the following preference learning approaches Silkie [29], LLaVA-RLHF [53], RLHF-V [70], POVID [79], SelfRewarding [80], CSR [80], SeVa [81], STIC [16], SIMA [60], and LLaVA-Critic [67] as well as, state-of-theart methods taken from the OpenVLM Leaderboard (see appendix).2. 5. Results and Ablations [38], ScienceQAIMG [18]. Specifically, we [75], MMStar We present three significant results of the PoP methodology, as well as various ablations studying its parameters. The three results comprise (1) comparison with other state-ofthe-art preference optimization methods for LVLMs, (2) the use of PoP as zero-shot evaluator, and (3) the use of the PoP methodology as self-improvement algorithm. 5.1. Main Results Our main result evaluates the efficacy of PoP learning across selected benchmarking metrics, showcasing its superior performance against competitive set of preference alignment models. For fairness, we keep the original LLaVA-1.5 configuration using the Vicuna-7B language model and CLIP/L14 vision model, as well as, comparing only to PoP performed for only one iteration. That way, we ensure that the 1https://github.com/open-compass/VLMEvalKit. Commit f547007 2https://huggingface.co/spaces/opencompass/open_ vlm_leaderboard. Accessed Oct 30, 2024. Model Data MMB SEED-B MM-Vet SQA POPE 64.3 - LLaVA-1.5-7B [32] 64.0 80k +VLFeedback [29] 10k 63.4 +RLHF [53] 1.4k 63.6 +RLHF-V [70] 64.9 17k +POVID [79] 64.5 17k +Self-Reward [80] 65.4 17k +CSR [80] 65.6 8k +SeVa [81] 65.3 6k +STIC [16] +SIMA [60] 64.9 17k +LLaVA-Critic [67] 113k 64.1 58.6 59.3 58.1 60.1 60.2 60.0 60.3 65.8 66.2 60.6 60.0 Preference Optimization (iteration 1) +st-PoP-iter1 (ours) 300k 65.5 300k 68.7 +PoP-iter1 (ours) 61.6 67. Preference Optimization (iteration 3) +st-PoP-iter3 (ours) 900k 67.4 900k 72.5 +PoP-iter3 (ours) 67.9 68.8 30.5 31.2 31.1 30.9 31.8 31.4 33.9 37.2 32.6 31.6 32.2 66.8 85.9 66.2 83.7 65.8 81.5 67.1 86.2 68.8 86.9 69.6 86.9 70.7 87.0 67.5 86.7 67.4 85.8 69.1 68.4 85.8 - 31.9 34. 67.1 86.8 71.2 87.0 32.5 35.0 74.0 86.3 86.4 87.0 Table 1. Performance of PoP using LLaVA-1.5. The evaluation benchmarks span several comprehensive evaluations, as well as general knowledge and hallucination evaluations. We mark the best performance in bold, and the second-best is underlined. data used stays in the same order of magnitude as previous methods. As depicted in Table 1, our method consistently outperforms other state-of-the-art preference alignment methods on these benchmarks. Notably, PoP-iter1 achieves score of 68.7% on the MMbench, 67.9% on the SEED-Bench, and 35.6% on the MM-Vet, demonstrating its robust capabilities in complex multi-modal scenarios. Furthermore, its score of 71.2% on ScienceQA highlights the ability of our method to improve performance on scientific question-answering tasks. These results show the effectiveness of our strategy as simple yet powerful way to create data for self-improvement. 5.2. Panel-of-Peers as Zero-Shot Evaluator In this experiment, we evaluate the PoP algorithms capacity as zero-shot evaluator by creating panels at different model scales and by sampling 15 candidate responses from each panel member and harnessing the ensembles collective judgment to select the best answer. We constructed panels at four distinct scales by selecting different sets of models based on their parameter count: > 3B, > 7B, > 10B, and > 30B. At each scale, the selected models not only generate answers but also evaluate their own responses and those from the rest of the panel. As shown in Figure 3, the st-PoP and PoP frameworks consistently outperform the average singlemodel approach (Avg-Single). This baseline represents the average score across all individual models within panel, serving as point of comparison. Our results demonstrate that the ensemble scores for PoP and st-PoP outperform the baseline at every model scale. This suggests that peerCapability Benchmark Iteration 0 Iteration 1 Iteration Iteration 3 GeneralVQA Knowledge Chart&OCR MMBench [35] MM-Vet [72] SEED-Bench [26] AI2D [23] MMMU [75] MMStar [6] MathVista [38] ScienceQA [37] ChartQA [40] TextVQA [52] OCR-Bench [36] OCRVQA [42] Hallucination POPE [31] HallusionBench [20] 62.4 21.1 64.6 62.0 32.7 36.4 30.3 58.0 39.6 44.9 33.6 59. 87.0 30.4 66.5 65.6 32.9 26.2 65.8 61.6 55.5 61.1 35.7 33.6 33.1 38.6 25.6 30.3 66.8 71.2 31.9 40.4 45.5 44.9 31.8 33.9 60.6 57.7 86.1 84.8 27.6 32.4 70.1 26.6 69. 74.8 38.3 40.8 42.4 68.5 46.4 51.4 44.8 64.4 87.6 30.9 68.7 73.8 34.1 32.9 67.9 65.9 67.0 73.8 41.9 39.4 37.1 43.2 35.8 42.4 71.2 84.1 37.4 47.4 52.1 51.4 42.5 45.2 65.4 62. 87.0 85.5 28.1 33.0 68.2 31.1 66.1 68.9 34.3 45.7 46.2 83.8 49.9 53.9 48.7 62.8 87.1 34.4 71.5 74.1 32.6 33.0 66.2 67. 70.1 74.3 39.6 39.8 44.9 48.6 44.9 44.9 83.3 85.2 51.3 51.8 54.0 53.4 45.5 46.7 63.6 64.3 86.4 86.4 32.9 35.0 69.1 32.3 68.7 70.8 35 46.2 52.3 86.9 51.2 54.6 50.2 64. 87.7 33.7 72.5 75.1 35.0 35.4 68.8 71.1 72.0 76.3 40.4 40.5 45.4 50.2 50.8 50.7 86.4 88.4 52.6 53.1 54.7 54.1 46.9 48.1 65.1 65.8 87.0 87.0 35.3 37.6 Vision Centric RWQA [66] 53.1 54.8 48.9 58.0 59.8 53.4 53.9 51.5 54. 55.5 53.0 56.0 Average 47.7 48.0 48.7 54. 53.1 55.6 55.7 55.9 57.3 56.4 56.7 58.2 Table 2. Evaluation on 15 vision-language benchmarks. We compare the performance of the regular Panel-of-PeersPoP. We have separated = PoP-LLaMA3. the benchmarks into five categories. Columns show three training iterations for indicates that the training set has been observed in our data mixture. For single-try Panel-of-Peers (st-PoP) see the appendix. = PoP-Vicuna, and = PoP-Mistral, assessment process enhances answer quality by aggregating judgments across diverse models, effectively creating an ensemble that is more accurate than any individual contributor. Details of the specific models used for this experiment can be found in the appendix. 5.3. Self-Improvement from Panel-of-Peers As illustrated in Figure 4, our method, PoP, consistently improves its performance over multiple iterations of selfimprovement, achieving higher average scores on the 15 selected benchmarks compared to CSR [80] and STIC [16]. For fairness, we keep the original LLaVA-1.5 configuration using the Vicuna-7B language model and CLIP/L-14 vision model. PoP exhibits steady upward trajectory, outperforming the other methods as the number of iterations increases, but seems to plateau after the third iteration. We report the per-iteration performance of each student in the PoP, on 15 vision-language benchmarks grouped into five categories (General VQA, Knowledge, Chart & OCR, Hallucination, and Vision Centric). See Table B.1. Each member in the panel is initialized with the model at iteration 0. The table illustrates how iterative self-improvement consistently boosts performance across all benchmarks, highlighting the benefits of our peer-feedback approach. 5.4. Learning an Ability from Peers. In this experiment, we test whether peer-to-peer learning can unlock new ability, not just improve an existing ability. To do this, our panel includes an additional model, which we designed to have minimal OCR capabilities. We refer to this model as OCR-Dumb. With this new four-member panel, we implemented self-improvement loop to assess whether the student models could elevate the OCR-Dumb model performance to match its peers. Furthermore, we investigated how this improvement impacted other tasks such as general knowledge, math, hallucination detection, and vision-centric. We initiate the OCR-Dumb model with progressively greater amounts of OCR knowledge, simulated by using 0%, 25%, 50%, 75%, and finally 100% of available OCR data in training. Each level of OCR knowledge was evaluated across five general categories: Chart & OCR, General VQA, Knowledge, Hallucination, and Vision-Centric tasks, as shown in Figure 5. As the OCR-Dumb model improved its reading capabilities, we observed steady increase in performance across all categories. This indicates that OCR knowledge is crucial component for reading-specific tasks and for tasks requiring an understanding of structured visual data or answering knowledge-based questions involving text. Interestingly, the models improvement in hallucination detection 6 Figure 3. Panel-of-Peers as reward model. Average scores of the Panel-ofPeers used as reward model on 15 selected benchmarks Figure 4. Self-improvement iterations. Average scores of PoP learning at different iterations of self-improvement over 15 selected benchmarks Figure 5. Learning new skill from peers We start with model with limited knowledge of OCR (0% - 100%) and use PoP to teach the model OCR knowledge. and vision-centric tasks suggests that enhanced OCR capabilities contribute to better general alignment in multimodal understanding. This experiment shows how Peer-to-Peer Learning, with varying levels of specialized knowledge, can iteratively improve models core abilities, even in areas where it initially struggles. 5.5. Ablations What makes good panel? good panel in PoP Learning combines diversity, unbiased evaluation, and iterative feedback. In the standard PoP setup, each model in the panel assesses the responses of others, promoting collaborative, classroom-like environment where models learn from their peers. Excluding self-evaluation (PoP - No Self Eval) has minimal impact, likely because the evaluations are blind and do not introduce self-bias. In the Senior Eval setup, however, smaller models (e.g., PoP-Vicuna, PoP-Mistral, PoPLLaMA3) generate answers while larger models (LLaVANeXT-34B [33], InternVL2-40B [8]) grade them. This setup boosts overall scores by leveraging senior models as reviewers but lacks the iterative learning benefits of having all models serve as both generators and evaluators, which enables continuous refinement across iterations. Finally, in the Best-of-15 configuration, single model (PoP-Vicuna) generates multiple responses, and the panel (PoP-Vicuna, PoP-Mistral, PoP-LLaMA3) evaluates and selects the best answer. This setup underperforms compared to diverse panels, likely because it reduces exposure to varied perspectives, focusing on optimizing single models responses rather than leveraging the strengths of multiple models. These results can be seen in detail in Figure 6a. Aligment objective. We examine the effect of the alignment objective on performance by comparing two strategies: Supervised Fine-Tuning (SFT) and Simple Preference Optimization (SimPO) [41]. For this experiment, we construct an SFT dataset by selecting the best from the panel. This allows us to create data = {(I, x, y(m))}D i=1 with only positive feedback from the panel. As shown in Figure 6b, our results indicate that using SFT with the Panel-of-Peers framework is match with SimPO. When we use single try per model, the reward margin in this setup is relatively limited, providing less separation for preference-based optimization to exploit. SFTs structured approach to fine-tuning appears more effective in this scenario, likely due to training directly on answers. Interestingly, when we allow multiple attempts per model, the performance gap between SFT and SimPO diminishes. With PoP, models are given up to 15 attempts to respond, increasing the likelihood of larger margin between the best and worst responses. This bigger margin allows SimPO to benefit more from the distinct variations in quality, thereby improving its ability to optimize based on preference data. Finally, SFT, while on average matched with SimPO, essentially reduces to knowledge distillation with rejection sampling [30]. SFT does not lead to reductions in hallucinations or improvements in visioncentric tasks; we believe that the rejected answers help steer the model away from hallucinationsa benefit lost when training solely on the preferred answer. Relative vs absolute scoring. The Relative Scoring approach allows each model to evaluate all responses simultaneously, assigning either relative score or ranking to each, which can then be aggregated by averaging or by majority vote. This method is computationally efficient as it enables faster evaluations. However, the Absolute Scoring approach, where each model evaluates responses individually without comparing them to others, demonstrated better performance. (a) Types of Panel-of-Peers. (b) Alignment objective. (c) Reward aggregation methods. (d) Prompt scoring methods Figure 6. Peer-to-Peer Learning ablation experiments We use the LLaVA-1.5, configuration using the Vicuna-7B language model and CLIP/L-14 vision model except for the first ablation which is evaluation only. We evaluate on 15 selected benchmarks. Absolute scoring can be aggregated in similar way. As seen in the figure, Absolute scoring with averaging produced the highest average score, followed closely by Absolute scoring with ranking, indicating that, despite being slower, absolute scoring yields more reliable evaluations than relative scoring due to its focused assessment of each response. Though costlier, it can be parallelized and improved with advances in model inference. In PoP (three models generating 45 answers), relative scoring prompts become excessively long, causing loss of context. Reward modeling. We evaluated three strategies for aggregating scores from the panel members. PoP-MEAN, involves straightforward averaging of scores across panel members, offering simplicity and ease of implementation. PoP-MIN adopts conservative approach by selecting the minimum score, ensuring that at least one model perceives the response as sufficiently accurate to pass. Finally, PoPUW (Uncertainty-Weighted), incorporates an uncertaintyaware mechanism that weights scores based on inter-model variance, adjusting for potential inconsistencies in panel evaluations. As shown in the figure, the uncertainty-weighted approach achieves the highest average score, followed closely by averaging, while the conservative approach lags slightly behind. However, due to its practical simplicity, we opted for averaging as the primary method for reward aggregation. Does PoP use extra data? The 900K images and queries from Cambrian-7M used to generate preference data were not part of the initial training data. However, it is important to emphasize that we only used the queries and images, excluding the ground truth answers, during the self-improvement process. We hypothesized that panel of peers can iteratively improve their collective understanding through feedback, similar to students completing end-of-chapter exercises. This hypothesis was validated by our results. For completeness, we performed supervised fine-tuning on the full 900K ground truth samples (corresponding to three iterations of self-improvement). The LLaVA-Vicuna model fine-tuned on this ground truth data achieved an average score of 54.0, 3% below the 57.0 achieved by PoP-Vicuna. These findings reinforce the effectiveness of PoP, demonstrating that using peer-generated feedback can produce high-quality synthetic data, which in turn facilitates learning that surpasses what is achieved with static, short, and noisy annotated datasets. 6. Discussion and Conclusion We introduced Panel-of-Peers (PoP) learning, new selfimprovement approach for enhancing LVLMs, where we showed that panel of models with similar initial capabilities can be used as both candidate response generator and evaluators to synthesize high-quality and diverse data, which enables us to iteratively and independently enhance the performance of individual panel members over multiple rounds. Our results from an extensive set of benchmarks and ablations clearly show that an iterative improvement can be achieved by simply working on dataset of queries (e.g. PoP-LLaMA3 model surpasses their LLaVA counterparts by 9 absolute points), which can reduce or eliminate the need to collect expensive human annotations. The PoP approach can also address specific weaknesses by leveraging complementary strengths in the panel. For example, model with limited OCR abilities can benefit from peer models with stronger OCR capabilities, fostering crossmodel knowledge transfer. The PoP framework imposes no limitations on model size or capacity as long as they are similar performing models, allowing PoP to scale seamlessly with future frontier models and making it versatile tool for the rapid evolution of LVLMs. Due to its design, PoP is also easily extensible. Better contrastive alignment methods, alternate sampling methods such as beam search variants, and other advancements in LVLMs can be independently useful alongside our PoP method. Acknowledgments. V. Ordonez is funded by an NSF CAREER Award #2201710 and the Ken Kennedy Institute."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 17 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2 [3] AI@Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 4, 13, 17 [4] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879, 2024. 4 [5] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 17 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. 5, 6, 14, [7] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 3 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 7, 15, 17 [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 1, 4, 13 [10] Eugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, and Mohammad Gheshlaghi Azar. Selfimproving robust preference optimization. arXiv preprint arXiv:2406.01660, 2024. 3 [11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training arXiv preprint verifiers to solve math word problems. arXiv:2110.14168, 2021. 3 [12] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. In The Twelfth International Conference on Learning Representations, 2024. [13] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. 15 [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 13 [15] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 15 [16] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. arXiv preprint arXiv:2405.19716, 2024. 3, 5, 6 [17] Alexey Dosovitskiy. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. [18] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for arXiv preprint evaluating large multi-modality models. arXiv:2407.11691, 2024. 5, 13, 15 [19] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. 3 [20] Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. arxiv. 10.48550. arXiv preprint arXiv.2310.14566, 2023. 5, 6, 14, 17 [21] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. 17 [22] AQ Jiang, Sablayrolles, Mensch, Bamford, DS Chaplot, de las Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023. 1, 4, 13 [23] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. 5, 6, 14, [24] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 17 [25] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheusvision: Vision-language model as judge for fine-grained evaluation. arXiv preprint arXiv:2401.06591, 2024. 3 [26] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of 9 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 5, 6, 14, 17 vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 15 [28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. 13 [29] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 5 [30] Shengzhi Li, Rongyu Lin, and Shichao Pei. Multi-modal preference alignment remedies degradation of visual instruction tuning on language models. arXiv preprint arXiv:2402.10884, 2024. 3, 7 [31] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large visionIn The 2023 Conference on Empirical language models. Methods in Natural Language Processing, 2023. 5, 6, 14, 17 [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 3, 4, 5, 13 [33] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 7, 15, [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 3, 4, 13 [35] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 2, 5, 6, 14, 17 [36] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 5, 6, 14, 17 [37] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 5, 6, 14, 17 [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 5, 6, 14, 17 [40] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, 2022. 5, 6, 14, [41] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. 4, 7 [42] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. 5, 6, 14, 17 [43] Angela Odonnell and Alison King. Cognitive perspectives on peer learning. Routledge, 2014. 2 [44] OpenAI. Gpt-4o system card, 2024. Accessed: 2024-09-30. [45] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1 [46] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 1, 13 [47] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 1, 3 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 13 [49] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [50] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [39] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating [52] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 10 Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 5, 6, 14, 17 [53] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, YuXiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 2, 3, [54] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2, 3, 15 [55] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 4, 13, 17 [56] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with panel of diverse models. arXiv preprint arXiv:2404.18796, 2024. 3, 4 [57] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 2 [58] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 15 [59] Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024. 3 [60] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visuallanguage modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024. 3, [61] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024. 3, 4 [62] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 33713384, 2024. 4 11 [63] Ross Wightman. https : / / github . com / rwightman / pytorch - image - models, 2019. 13 Pytorch image models. [64] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is human-aligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2222722238, 2024. 2 [65] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. 3 [66] x.ai Team. Grok-1.5 vision preview, 2024. 5, 6, 14, 17 [67] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llavacritic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. 3, 5 [68] Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, et al. The perfect blend: arXiv preprint Redefining rlhf with mixture of judges. arXiv:2409.20370, 2024. 3 [69] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 17 [70] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. 5 [71] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 2 [72] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Forty-first International Conference on Machine Learning, 2024. 2, 5, 6, 14, [73] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 1 [74] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. 3 [75] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 5, 6, 14, 17 [76] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1 [77] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. 2 [78] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 13 [79] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. 3, 5 [80] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 3, 5, [81] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. SelfarXiv preprint supervised visual preference alignment. arXiv:2404.10501, 2024. 3, 5 [82] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "Prompt Template for Generating Responses from the Panel of Peers",
            "content": "[System Prompt] You are an expert evaluation model. You are asked to evaluate the AI assistants response to users question based on an image. You will see the users question, the related image, and the AIs response. [Evaluation Criteria] Please rate the response using 5-point Likert scale across the following dimensions: Helpfulness, Correctness, Coherence, Complexity, and Verbosity. [Rating Guidelines] - Helpfulness: Rate from 1 (not useful at all) to 5 (extremely helpful). - Correctness: Score from 1 (completely incorrect) to 5 (fully correct and accurate). - Coherence: Evaluate from 1 (completely incoherent) to 5 (perfectly coherent and clear). - Complexity: Assess from 1 (basic, understandable by children) to 5 (expert level, specialized vocabulary). - Verbosity: Judge from 1 (very concise) to 5 (highly detailed and verbose). [Brief Definitions] - Helpfulness relates to the utility of the response in addressing the users need. - Correctness ensures the response is factual and free from errors. - Coherence checks for logical flow and consistency in the response. - Complexity reflects the sophistication of language and concepts used. - Verbosity measures the brevity or expansiveness of the response. Here is the question and the assistant response: [Question] {question} [Assistant Response] {response} [JSON Output] Your answer should look like this. Only output result in the following JSON schema format: {Helpfulness: (int), Correctness: (int), Coherence: (int), Complexity: (int), Verbosity: (int) } Figure A.1. Evaluating Synthetic Responses. We use the following prompt template, which is used to evaluate responses from the Panel-of-Peers. A. Implementation Detatils A.1. Training Hyperparameters In Table A.1, we list the detailed training dataset usage and hyperparameters. The training data are constructed based on the following datasets: BLIP-LAION-CC-SBU [34], which contains 558K image-text pairs from BLIP-captioned CC3M [28], SBU [46], and LAION400M [50] filtered by LLaVA; LLaVA-Instruct-mix665k [32], which contains 665k visual instruction-following data constructed to train the LLaVA family of models; and synthetic data created using images and questions from the Cambrian-7M dataset [55]. Unless otherwise specified, we randomly sample the indicated number of instances from each dataset during the training process. During training, we use Flash Attention [14], bfloat16, and PyTorch FSDP [78] to accelerate training efficiency. A.2. Panel-of-Peers Models Image Processing and Visual Representations We implement all image processing logic using the default image transforms provided by torchvision and the TIMM library [63]. We normalize pixel values using the default ImageNet normalization values. The default backbone employed by all visual representations that we evaluate in this work is Vision Transformer [17] trained with the CLIP objective [48]; we extract patch features from the penultimate layer, following LLaVA [34]. Vision-Language Projector We use simple 2-layer GELU MLP as the projector, which projects each patch independently into the embedding space of the language model. Language Model We choose three models to create the Panel-of-Peers: Vicuna-7B [9], Mistral-7B [22], and -8B [3]. In order to combine the projected visual patch embeddings, we perform simple sequence-wise concatenation by placing the patch embeddings before the text embeddings. A.3. Evaluation benchmarks Systemic evaluations of the Panel-of-Peers regarding General VQA, knowledge, Chart&OCR, Hallucination, and Vision-Centric capabilities have been conducted. The benchmarks and datasets used are listed in Table A.2. During the evaluation, we use VLMEvaKit [18] as our primary evaluation toolkit. A.4. Prompt Template To evaluate model-generated responses within our Panelof-Peers (PoP) framework, we designed detailed prompt Config Optimizer Learning Rate Weight Decay Training Epochs Warmup Ratio Learning Rate Scheduler Batch Size Per GPU Maximum Token Length Unfreeze LLM Stage Alignment AdamW 2e-3 0.0 1 0.003 Cosine 16 2048 Stage II SFT Stage III PoP Training Hyper-Parameters AdamW 2e-5 0.0 1 0.003 Cosine 8 2048 Training Data AdamW 6e-5 0.0 2 0.003 Cosine 8 2048 Dataset Data Size Data Type BLIP-LAION-CC-SBU LLaVA-Instruct-mix665k Sampled from Cambrian-7M 558K Pair 665K Instruction 3 300K Synthetic GPU Device Training Time 8NVIDIA A100-80GB 6h 8NVIDIA A100-80GB 10h 8NVIDIA A100-80GB 90h Training Cost Table A.1. Training recipes for PoP. The three training stages are introduced in Section 3. Stage I: Alignment training, Stage II: Instruction Tuning, Stage III: Panel-of-Peers Learning."
        },
        {
            "title": "General VQA",
            "content": "MM-Vet [72] MMBench [35] SEED-Bench [26] Multi-disciplinary QA Multi-disciplinary QA Multi-disciplinary QA - dev -"
        },
        {
            "title": "Knowledge",
            "content": "Chart&OCR AI2D [23] MMMU [75] MMStar [6] MathVista [38] ScienceQA [37] ChartQA [40] TextVQA [52] OCR-Bench [36] OCRVQA [42] test Science Diagrams College-level Multi-disciplinary val Misc Multi-disciplinary General Math Understanding High-school Science - min val Chart Understanding OCR; Reasoning OCR; Multi-disciplinary Document OCR test val Relaxed Accuracy VQAScore - Acc TESTCORE Acc GPT-4 Eval [72] GPT-3.5 Eval [35] Multi-choice Acc Multi-choice Acc Multi-choice Acc Multi-choice Acc GPT-4 Eval Multi-choice Acc"
        },
        {
            "title": "Hallucination",
            "content": "POPE [31] HallusionBench [20] Visual Hallucination Yes/No Hallucinations - - Acc, F1-score Acc, F1-score Vision Centric RWQA [66] Real-world QA dev Multi-choice Acc Table A.2. Overall descriptions of the evaluation benchmarks for evaluating capabilities, including GeneralVQA, Knowledge, Chart&OCR, Hallucination and Vision Centric Benchmarks. template to guide models in rating responses. This prompt was central to generating pseudo-rewards, which serve as feedback signals to enable self-improvement iterations. Each model evaluated the outputs of its peers based on set of predefined criteria and aggregated their results using an ensemble strategy to achieve consensus. The prompt comprises three main components: System Prompt, Evaluation Criteria, and Rating Guidelines. It is structured as follows: System Prompt: The model is instructed to act as an expert evaluator tasked with assessing the quality of Figure A.2. Learning New Skill from Peers (OCR). We start with model with very limited OCR knowledge ( 0%) and use PoP to iteratively teach OCR skills. The performance is evaluated across multiple categories, including Chart & OCR, General Knowledge, Math & Science, Hallucination, and Vision-Centric tasks. response provided to users question. Both the question and its related image are provided for context. Evaluation Criteria: Responses are evaluated across five dimensions on an ordinal Likert scale: 1. Helpfulness: Utility of the response in addressing the users query (1 to 5 scale). 2. Correctness: Accuracy and factuality of the response (1 to 5 scale). 3. Coherence: Logical consistency and clarity of the response (1 to 5 scale). 4. Complexity: Level of language sophistication, ranging from simple to expert-level (1 to 5 scale). 5. Verbosity: Appropriateness of detail and conciseness (1 to 5 scale). Rating Guidelines: Models receive detailed explanations for scoring each dimension. For instance, rating of 5 in Helpfulness indicates complete alignment with the users intent, while 1 represents failure to address the query effectively. Similarly, Coherence is rated based on logical flow, with 1 indicating substantial contradictions or redundancy. Output Format: To standardize results, models are instructed to provide evaluations in strict JSON schema format, including scores for each criterion. This prompt enabled consistent and systematic evaluation of the model-generated responses, ensuring that pseudorewards were aligned with the evaluation objectives outlined in our PoP framework. B. Additional Experiments B.1. Comparison with State of the Art We compare against the top 49 models on the OpenVLM leaderboard, highlighting the performance of our models using PoP. Our models include PoP-Vicuna, PoP-Mistral, PoP-LLaMA3, and their single-try counterparts, which are evaluated in 15 benchmarks against broad spectrum of state-of-the-art methods. Our best-performing models, PoP-LLaMA3 and mt-PoPLLaMA3, achieve an average score of 56.3% and 59.7%, starting from score of 48.9%. Compared to proprietary models like GPT4-o [44] and Gemini-1.5 [54], our models lag behind approximately 20 percentage points in performance. similar gap is observed when compared with opensource state-of-the-art models, such as Qwen2-VL-72B [58], InternVL2-Llama3-76B [8], and NVLM-D-72B [13]. Compared to models of the same size category but trained on significantly more data and higher-resolution inputs, our bestperforming models lag behind the recently released Qwen2VL-7B [58], the LLaVA-OneVision family [27], and the Molmo family [15] by approximately 10 percentage points. Compared to models of the same size category trained on similar budgets, our best-performing model surpasses all the LLaVA-NeXT family [33] except for models larger than 30B by approximately 5 percentage points. We remark that our models use 224x224 pixels as the input resolution compared to 768x768 pixels of the NeXT family. These results demonstrate the efficacy of our approach in using peer evaluations to improve model performance, effectively increasing the average score by approximately 12% compared to the original LLaVA-1.5-7b model. Figure B.1 illustrates comparative analysis of the top 49 models on the OpenVLM leaderboard [18], highlighting the performance of our models using peer-to-peer learning. B.2. Extra Results on Learning and Ability from"
        },
        {
            "title": "Scratch",
            "content": "In addition to the ablation study presented in the main manuscript, where we evaluated the ability of the Panel-of15 Figure B.1. Evaluation results of our approach on 15 selected benchmarks in the OpenVLM Leaderboard. The figure displays 49 selected LVLMs (until 2024.10.30) in descending order of average score. When calculating the average score, the scores of each benchmark are normalized to the range of 0 to 100. Peers (PoP) framework to teach model OCR capabilities, we expanded the analysis to include the performance of the OCR-Dumb model across other benchmark categories. Figure A.2 provides comprehensive view of the models iterative performance improvement across five categories: Chart&OCR, General Knowledge, Math and Science, Hallucination, and Vision-Centric tasks. The experiment began with an OCR-Dumb model trained with varying proportions of OCR knowledge (0%, 25%, 50%, 75%, and 100%). Interestingly, the results demonstrate that as OCR knowledge increases, the models performance steadily improves not only in OCR-related tasks but also in other categories. Notable observations include: Chart and OCR: Performance rises sharply with increased OCR knowledge, validating the importance of reading capabilities for interpreting structured visual data. General Knowledge: Gains in this category suggest that improved text recognition contributes to better multimodal understanding and reasoning. Math and Science: Enhanced OCR capabilities positively impact tasks involving numerical and scientific reasoning, where understanding text is critical. Hallucination: Improvements here indicate that OCR knowledge helps reduce misalignments and inconsistencies in model outputs at the beginning. However, this improvement plateaus if the model starts with more OCR knowledge. Vision-Centric: Even tasks not directly reliant on OCR knowledge show gradual improvement, though to lesser extent, with more OCR knowledge. This emphasizes the holistic impact of PoP training. These results show the applicability of Peer-to-Peer Learning, demonstrating its ability to transfer knowledge, including OCR, while simultaneously increasing performance in various multimodal tasks. This highlights the effectiveness of PoP as self-improvement mechanism, enabling models to iteratively learn new capabilities and address their initial weaknesses. 16 Capability Benchmark Iteration 0 Iteration 1 Iteration 2 Iteration 3 GeneralVQA Knowledge Chart&OCR MMBench [35] MM-Vet [72] SEED-Bench [26] AI2D [23] MMMU [75] MMStar [6] MathVista [38] ScienceQA [37] ChartQA [40] TextVQA [52] OCR-Bench [36] OCRVQA [42] Hallucination POPE [31] HallusionBench [20] 62.4 21.1 64.6 62.0 32.7 36.4 30.3 58.0 39.6 44.9 33.6 59.7 87.0 30.4 66.5 65.6 32.9 26.2 65.8 61. 55.5 61.1 35.7 33.6 33.1 38.6 25.6 30.3 66.8 71.2 31.9 40.4 45.5 44.9 31.8 33.9 60.6 57.7 86.1 84.8 27.6 32.4 65.3 24.5 68.7 66.0 35.5 37.4 33.1 62.4 42.4 48.4 34.7 62. 85.1 34.7 65.5 68.7 31.9 29.5 61.6 65.5 59.1 65.1 38.8 36.6 34.0 39.6 31.2 33.1 67.1 73.1 42.7 43.3 49.0 48.4 33.8 35.0 63.6 60.6 86.8 83.0 32.6 37.1 65.7 29.5 65. 65.8 39.1 40.8 34.9 66.1 46.3 50.2 39.5 60.9 86.2 30.7 66.1 69.9 31.6 31.9 66.2 66.9 60.2 66.1 36.4 36.9 39.6 40.2 33.8 35.5 71.9 75.4 45.1 45.7 50.3 49.3 38.7 38.3 62.4 61. 86.4 84.1 31.7 30.7 67.0 30.5 66.8 64.6 39.9 41.5 35.0 68.0 48.4 52.2 41.3 61.4 86.1 28.2 67.4 71.3 32.5 33.0 67.9 68. 62.9 71.4 37.1 37.6 40.9 45.3 34.9 37.7 74.0 77.6 47.1 47.8 52.3 51.2 41.6 44.5 62.9 62.2 86.3 85.0 31.8 36.5 Vision Centric RWQA [66] 53.1 54.8 48. 54.6 53.2 50.3 53.0 49.6 52.9 53.4 50.0 53. Average 47.7 48.0 48.7 50.4 50.1 51.2 51. 51.3 52.4 51.2 51.6 53.7 Table B.1. Evaluation on 15 vision-language benchmarks. We compare the performance of the single-try Panel-of-Peers (st-PoP). We have separated the benchmarks into five categories. Columns show three training iterations for = PoP-LLaMA3. indicates that the training set has been observed in our data mixture. = PoP-Vicuna, and = PoP-Mistral, B.3. Extra Details on the Panel-of-Peers Ensemble as Zero-Shot Evaluator We present more details on the experiments in Section 5.2. For models with more than 3B parameters, we included Phi-3-Vision [1], BLIP3 [69], and Paligemma [5]. In the more than 7B range, we selected LLaVA-NeXTLlama3, LLaVA-NeXT-Mistral, LLaVA-NeXT-Vicuna [33], and Idefics2 [24]. For models exceeding 10B parameters, we picked CogVLM2-Chat [21], LLaVA-NeXT-Vicuna13B [33], and Llama-3.2-Vision [3]. For models with more than 30B parameters, we incorporated InternVL2-26B, InternVL 1.5-26B [8], Cambrian-34B [55], and LLaVA-NeXTYi [33]. Each panel performed response regeneration and evaluations. However, this is an evaluation-only method, enabling the creation of an ensemble using their consensus. B.4. Extra Results of Our Trained Models We present the specific scores of each of the members of the panel of peers, outlined in Table 2 of the main manuscript, where we presented the average scores of the whole panel."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Rice University"
    ]
}