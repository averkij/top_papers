{
    "paper_title": "SCI-Verifier: Scientific Verifier with Thinking",
    "authors": [
        "Shenghe Zheng",
        "Chenyu Huang",
        "Fangchen Yu",
        "Junchi Yao",
        "Jingqi Ye",
        "Tao Chen",
        "Yun Luo",
        "Ning Ding",
        "LEI BAI",
        "Ganqu Cui",
        "Peng Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains."
        },
        {
            "title": "Start",
            "content": "SCI-VERIFIER: SCIENTIFIC VERIFIER WITH THINKING Shenghe Zheng1,2, Chenyu Huang3, Fangchen Yu1,6, Junchi Yao1,7, Jingqi Ye1,8, Tao Chen3, Yun Luo1, Ning Ding1,5, Lei Bai1, Ganqu Cui1, Peng Ye1,4 1 Shanghai AI Laboratory 4 CUHK 5 Tsinghua University 2 Harbin Institute of Technology 6 CUHK-Shenzhen 7 UESTC 8 USTC 3 Fudan University 5 2 0 S 9 2 ] . [ 1 5 8 2 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains."
        },
        {
            "title": "INTRODUCTION",
            "content": "As large language models (LLMs) become increasingly prevalent in scientific reasoning (Yang et al., 2025; Ren et al., 2025; Liu et al., 2024a; Bai et al., 2025), ensuring the reliability of their outputs has emerged as critical challenge (Chang et al., 2024; Liu et al., 2025). Scientific reasoning often involves intricate multi-step processes and wide range of equivalent answer formulations (Chen et al., 2025b), posing substantial difficulties for answer verification. The essence of verification lies in accurately determining whether an output of LLM is equivalent to the reference answer, task that serves both as the foundation for evaluating capabilities of LLMs and as key bottleneck to further advancement (Zhang et al., 2025a; Chen et al., 2025a; Zhang et al., 2025b). Despite recent progress, verification research in scientific domains still faces two major challenges. First, high-quality and systematic benchmarks are lacking. Existing benchmarks cover only narrow range of scientific disciplines and fail to account for discipline-specific equivalence forms (Liu et al., 2024b; Li et al., 2025; Yan et al., 2025; Chen et al., 2025a), making it difficult to comprehensively evaluate verification capabilities. Second, current methods are limited in complex reasoning scenarios and lack cross-disciplinary applicability. Rule-based approaches rely on manually crafted templates and heuristics, which are labor-intensive and insufficient for complexity (Hynek Kydlíˇcek, 2024; Gao et al., 2021). More recently, general LLMs or specialized verifiers leverage LLM generalization to achieve promising results (Li et al., 2023; Liu et al., 2025; Zhang et al., 2025b; Luo et al., 2023; Liu et al., 2023a), but they require extensive prompt engineering, produce unstable outputs, and still struggle with complex reasoning and cross-disciplinary tasks. To overcome these limitations, we Corresponding Author Equal Contribution. Project: SCI-Verifier"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 1: (a) and (b) show the performance gains with reasoning enabled on VerifierBench and VerifyBench-hard, respectively; (c) presents case where reasoning leads to the correct judgment. propose solution from both the data and model perspectives, with the dual goals of establishing systematic evaluation framework and designing robust verifier tailored to scientific reasoning. On the data side, we introduce SCI-VerifyBench, cross-disciplinary and highly targeted scientific verification benchmark. We first collected over 100K responses from LLMs across mathematics, physics, biology, chemistry, and general scientific question-answering tasks, ensuring broad coverage of domains, question types, and answer formats. To increase both difficulty and specificity, we apply domain-specific equivalence transformation to some samples, enriching the test set with challenging cases. These transformations include, but are not limited to, formula rewrites, and logical equivalence substitutions, simulating error patterns that verifiers are likely to encounter in real-world scientific scenarios. Finally, by combining model-generated labels with expert human annotations, we ensured the quality and diversity of the data, making SCI-VerifyBench benchmark that is both cross-disciplinary and rigorous for evaluating verification capabilities. In benchmark evaluations, we are surprised to find that reasoning abilities, which are often overlooked by many current specific verifiers, can significantly enhance the models performance on scientific verification tasks. At the model level, we propose SCI-Verifier, reasoning-augmented verifier tailored for scientific domains. In Fig. 1, we highlight the critical role of reasoning in verification. Enabling Chain-of-Thought (CoT) across models consistently boosts judgment accuracy. Motivated by this, SCI-Verifier employs two-stage post-training pipeline combining supervised fine-tuning and reinforcement learning to integrate logical reasoning into scientific verification, enabling it to handle complex equivalence judgments and multi-step reasoning frequently encountered in scientific tasks. It assesses equivalence from multiple perspectives while maintaining concise and stable outputs for practical deployment. Experimental results show that SCI-Verifier substantially improves accuracy on challenging and easily confusable samples compared to current verifiers and exhibits stronger cross-disciplinary generalization. Notably, the 8B version of SCI-Verifier achieves verification performance on par with the current state-of-the-art closed-source model GPT-5 (OpenAI, 2025b). In summary, this work makes three key contributions as following: We propose cross-disciplinary, high-challenge benchmark for scientific verification, SCIVerifyBench, which covers mathematics, physics, biology, chemistry and general question-answer fields. Using real LLM responses and domain-specific equivalence transformations, it evaluates verification performance in complex scenarios and sets unified standard for LLM assessment. We design reasoning-enhanced high-performance scientific verifier, SCI-Verifier. By integrating logical reasoning via supervised post-training, SCI-Verifier gains the capability to perform complex equivalence judgments and conduct multi-step scientific reasoning, thereby significantly outperforming existing verification models across multiple domains. Extensive experiments show that SCI-VerifyBench and SCI-Verifier together provide precise evaluation framework and practical guidance for improving LLM capabilities, reliability, and reasoning in scientific domain, setting new standard for cross-disciplinary verification research."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Verification Benchmark. The unstructured LLM outputs makes the verification of the answers challenging, motivating efforts to construct benchmarks for evaluating the verifiers. VAR (Chen et al.,"
        },
        {
            "title": "Preprint Version",
            "content": "2025a) evaluates 19 LLMs on 24 datasets to train and assess xVerify. VerifyBench (Li et al., 2025) covers general, logical, mathematical reasoning, and VerifyBench (Yan et al., 2025) has 4,000 expertlevel questions across STEM domains. VerifierBench (Liu et al., 2025) aggregates model outputs with manual meta-error analysis across math, science, knowledge, and general reasoning tasks. Existing works face two main issues: (1) pointless samples, such as multiple-choice questions that require no specially designed verifiers, and (2) limited disciplinary coverage that restricts generalization assessment of scientific domain. To address these problems, we introduce SCI-VerifyBench, spanning mathematics, physics, chemistry, biology, and general QA, with filtered tasks and expert annotations to enable rigorous evaluation of scientific verification. Verification Models. The verifier compensates for gaps in rule-based answer evaluation. xVerify (Chen et al., 2025a) is efficient but lacks reasoning, limiting performance; General-Verifier (Ma et al., 2025) has partial reasoning capabilities for cross-domain equivalence assessment. CompassVerifier (Liu et al., 2025) aims to provide efficient, high-performance, and robust answer verification using carefully designed error templates. Existing verifiers, constrained by limited reasoning capabilities, are inadequate for complex scientific reasoning, while using general models as verifiers requires careful prompt design with unstable outputs. Then, we propose SCI-Verifier, reasoning-augmented scientific verifier offering strong reasoning with concise, stable outputs. Reward Models. Reward models differ from verifiers in that they rank response quality, while verifiers assess correctness. Prior work primarily follows discriminative paradigm, outputting scalar score directly (Ouyang et al., 2022; Snell et al., 2025). More recent approaches leverage reasoning capabilities to enhance reward model performance. For example, J1 (Whitehouse et al., 2025) proposes an RL framework for training Thinking-LLM-as-a-Judge models; Think-J (Huang et al., 2025) introduces offline and online RL-based methods for judgment thinking optimization; Compass-Judger2 (Zhang et al., 2025b) uses verifiable rewards and rejection sampling to guide critical reasoning, improving robustness and generalization. Despite these advances, reward models aim to rank response quality rather than verify correctness, and this difference in objective introduces new challenges for data construction and training strategies."
        },
        {
            "title": "3 SCI-VERIFYBENCH",
            "content": "Current research on scientific verification faces major bottleneck in the lack of comprehensive and rigorous benchmarks, which creates blind spots in evaluating LLMs scientific reasoning capabilities and guiding their training. To address this problem, we construct SCI-VerifyBench, systematic crossdisciplinary benchmark covering mathematics, physics, chemistry, biology, and general scientific QA, designed to comprehensively evaluate the verification abilities of verifiers. We first present the characteristics of SCI-VerifyBench in Sec. 3.1, followed by the construction pipeline in Sec. 3.2. Table 1: Comparison of verification benchmarks. Difficulty Control Equivalence Transformation VerifyBench VerifyBench-hard VerifierBench Scale Domains SCI-VerifyBench 2000 1000 2817 2500 3 3 4 3.1 DATA OVERVIEW Table 2: Benchmark Statistic. We construct SCI-VerifyBench to assess verifiers scientific verification capabilities. In this part, we compare it with existing benchmarks and present static data analyses. Tab. 1 highlights the key differences, showing that SCIVerifyBench spans wider range of domains, incorporates more challenging yet commonly encountered equivalence transformations, and applies difficulty control mechanisms to better reflect realistic scientific reasoning while increasing overall task complexity. Tab. 2 presents the static analysis results, with further details provided in Appendix A. Total Data (each domain) Real QA (each domain) Synthetic QA (each domain) Average Answer Tokens Average Response Tokens 24.98 2980. 500 350 150 Number Statistic 3.2 DATA COLLECTION QuestionAnswer Data Generation. Comprehensive scientific verification requires coverage of diverse question areas, answer types, and response formats. To achieve this, we collect over"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 2: SCI-VerifyBench construction pipeline (left) and cases of Equivalent Representation (right). 15k question-answer pairs across mathematics (Gao et al., 2025), physics (Zheng et al., 2025), chemistry (Alampara et al., 2024), biology (Jiang et al., 2025), and general QA (Wang et al., 2024; Wei et al., 2024), and generate 100K+ responses using eight models of varying scales, while controlling response length. This design allows the verifier to adapt to different question and answer styles. The prompts used for data generation are provided in the Appendix A.1. Synthetic Data Generation. Current verifiers perform well on responses identical to reference answers. However, even carefully trained specialized verifiers can fail when confronted with complex, domain-specific equivalence transformations, which are common in real-world scientific reasoning. Fig. 2 illustrates typical equivalence transformations across mathematics, physics, chemistry, biology, and general QA. To address this, we select 500 representative questions from each domain that allow for answer equivalence transformations and generate five equivalent answers for each using the methodology described in Appendix A.1. During this process, five LLMs assist in assessing the quality of generated equivalences. If the equivalence is clearly invalid and multiple models agree, the sample is discarded and regenerated. This approach increases the challenge for verifiers and closely simulates the diverse answer forms encountered in realistic scientific reasoning scenarios. Data Annotation. The previous two pathways produced both real and synthetic question-answer data. To ensure annotation quality, we adopt hybrid approach combining LLMs and human experts. First, five LLMs evaluate the accuracy of generated or synthetic answers against the reference answers as shown in Appendix A.2. To reduce human effort and maintain dataset difficulty, we retain only samples where the five LLMs disagree. From the data in the two methods described above, we select 2,500 samples with the highest model disagreement (500 per domain) for human annotation. Each sample is assessed by at least two experts with bachelors degree or higher, and answers are considered equivalent if they can be transformed into each other. In cases of disagreement, third expert is consulted, ensuring labels reflect true equivalence while maintaining diversity. Data Filter. Using the above procedure, we obtained dataset comprising 5,000 human-annotated samples and large collection of LLM-annotated samples. We partitioned the data into training set and test set, with the latter corresponding to SCI-VerifyBench. For the test set, we sampled 350 real LLM responses and 150 equivalence-based synthetic responses per domain. The selection criterion required full agreement among human experts, while samples with disagreement between human experts and LLMs are preferentially included to increase difficulty. This process results in test set of 2,500 samples in total. The remaining non-overlapping data formed the training set, where only portion was human-annotated and the majority relied on LLM annotations. Samples with substantial disagreement among LLMs were filtered out to ensure label reliability, yielding training set of 14K samples. Both the training and test sets can be expressed in the following format: = {(qi, ai, ri, li)}N where qi denotes the question, ai denotes the reference answer, ri denotes the response whose correctness needs to be evaluated, and li denotes the label, which can be either true or false. i=1, (1)"
        },
        {
            "title": "4 SCI-VERIFIER",
            "content": "In this section, we develop reasoning-augmented verifier for scientific verification. First, Sec. 4.1 presents the motivation and necessity for incorporating reasoning capabilities into scientific verification. Then, Sec. 4.2 describes the approaches for integrating reasoning through supervised fine-tuning (SFT) and reinforcement learning (RL). This design emulates human step-by-step reasoning and improves verification reliability and robustness."
        },
        {
            "title": "4.1 MOTIVATION",
            "content": "We begin by motivating the introduction of reasoning capabilities into scientific verification. While Chain-of-Thought (CoT) has been widely recognized for enhancing model performance across various domains, most existing verifier studies have overlooked this aspect. As shown in Fig. 1, we evaluate models of different scales under two conditions: outputting only the final answer versus producing intermediate reasoning before the answer. The results demonstrate that reasoning brings substantial gains in scientific verification, largely because responses of scientific questions are inherently complex and often involve multiple equivalent forms. Reasoning is thus essential for assessing equivalence from different perspectives. Building on this insight, we argue that optimizing reasoning for scientific verification can significantly boost verifier performance. Next, we introduce SCI-Verifier, unified verifier designed to deliver concise yet powerful reasoning capabilities. 4.2 POST-TRAINING In this section, we present our approach to enhancing the reasoning ability of models for scientific verification. While reasoning is essential, the nature of verification requires reasoning paths to be as concise as possible to minimize resource consumption. Accordingly, we aim for lightweight verifier with short and stable outputs. Based on these characteristics, we utilize two-stage post-training paradigm that combines supervised fine-tuning (SFT) with reinforcement learning (RL). Supervised Fine-Tuning (SFT). In this stage, we employ large models with rejection sampling to generate diverse set of reasoning paths in structured format as shown in Appendix A.2. We then perform strict filtering to retain only valuable and concise traces. For reasoning models, we keep only the conclusive summary, while for non-reasoning models, we discard overly long or unstructured responses. The filtered reasoning paths are used to fine-tune smaller model, effectively injecting the essential reasoning ability with minimal overhead. The training objective is defined as follows: LSFT(θ) = E(x,y)DSFT log πθ(y x) , (2) (cid:104) (cid:105) where DSFT denotes the curated training dataset of high-quality reasoning traces. Unlike SFT in other domains like mathematics or physics where the focus is mostly on output formatting, verification SFT centers on transferring domain-specific verification knowledge to small models, which is essential for developing concise and useful reasoning for verification. Reinforcement Learning (RL). After SFT, the model acquires basic verification capability and the ability to generate outputs in fixed format. However, it inevitably faces the challenges of overfitting. To improve generalization, we adopt DAPO (Yu et al., 2025), which is refined GRPO (Shao et al., 2024) method. Within this framework, training samples are dynamically filtered. Overly simple samples fail to provide meaningful learning signals, while overly difficult samples may destabilize training. To further encourage concise reasoning, we incorporate length penalty into the reward function. The overall objective function for training is to maximize the following expression: JDAPO(θ) = (q,a)D, {oi}G i=1πθold (q) (cid:34) 1 {oi} (cid:88) oi (cid:88) i=1 t=1 min (cid:16) ri,t(θ) ˆAi,t, clip(cid:0)ri,t(θ), 1 ϵlow, 1 + ϵhigh (cid:1) ˆAi,t (cid:35) (cid:17) The advantage function ˆAi,t is calculated from the final reward Ri: ˆAi,t = Ri mean({Rj}G std({Rj}G j=1) j=1) 5 (3) (4)"
        },
        {
            "title": "Preprint Version",
            "content": "Table 3: Performance of different verifiers on SCI-VerifyBench. Specialized verifiers use default prompts, while all other models are allowed to use reasoning. Models Math Physics Chemistry Biology QA Total Avg. Token GPT-5 (OpenAI, 2025b) Gemini-2.5-Flash (Comanici et al., 2025) o4-mini (OpenAI, 2025a) Closed-source Models 82.60 80.40 78.40 74.60 68.80 69.80 88.00 84.00 83.20 Open-source Instruct models Qwen2.5-72B-Instruct (Qwen et al., 2025) Qwen3-30B-A3B-Instruct-2507 (Qwen et al., 2025) LLaMa-3.3-70B-Instruct (Grattafiori et al., 2024) 75.60 75.40 76.60 62.40 63.60 67.40 76.80 79.80 78.80 Open-source Reasoning Models Qwen3-4B (Yang et al., 2025) Qwen3-8b (Yang et al., 2025) GPT-oss-20b (OpenAI et al., 2025) Qwen3-30B-A3B-Thinking-2507 (Yang et al., 2025) GPT-oss-120B (OpenAI et al., 2025) Qwen3-235B-A22B (Yang et al., 2025) xVerify-8B (Chen et al., 2025a) CompassVerifier-3B (Liu et al., 2025) CompassVerifier-7B (Liu et al., 2025) CompassVerifier-32B (Liu et al., 2025) SCI-Verifier-4B SCI-Verifier-8B 59.80 61.60 54.20 64.40 67.80 61.00 73.20 75.00 71.00 76.20 78.80 76.40 Specific Verifiers 72.80 75.00 76.00 77.40 55.60 66.40 65.00 67.60 Ours 86.20 87. 77.40 79.60 79.40 79.20 80.40 83.00 84.20 81.80 80.80 82.00 80.40 81.60 83.20 80.60 89.00 88.80 88.00 80.80 89.80 84. 81.00 81.80 80.40 82.40 86.00 85.20 83.60 84.20 81.00 80.60 90.80 94.40 90.40 89.00 89.20 79.40 82.20 85.80 78.80 75.00 65.60 78.80 89.00 73. 83.00 77.00 79.00 82.80 84.92 82.20 81.72 75.00 78.16 78.64 74.44 74.52 70.32 76.96 81.16 75.64 75.16 76.92 76.28 78.00 89.40 89. 85.40 86.28 384.59 478.48 437.27 400.40 684.58 364.36 1466.92 1033.07 522.55 1714.66 110.21 4601.16 1.00 192.00 162.34 212.04 485.13 490. The final reward Ri is sum of the alignment reward Ralign,i and overlong penalty Poverlong,i: where the overlong penalty is defined as: Ri = Ralign,i + Poverlong,i Poverlong,i = 0 oiLmax Lbuffer λpenalty if oi Lmax if Lmax < oi Lmax + Lbuffer if oi > Lmax + Lbuffer (5) (6) Here, oi is the length of the response, Lmax is the maximum allowed length, Lbuffer is the overlong buffer length, and λpenalty is the penalty weight. Since verification is binary classification task, imbalanced data may lead the model to rely on label priors instead of reasoning. To address this, we rebalance the dataset during RL training to ensure equal positive and negative examples. Through this two-stage post-training paradigm, we obtain verification model with concise reasoning ability, applicable across domains for scientific answer validation. This approach not only improves the reliability of model capability evaluation, but also provides reward function with clear semantics, thereby facilitating the training of stronger reasoning-oriented language models."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 BASELINES AND SETUP We conduct systematic evaluation of SCI-VerifyBench on SCI-Verifier-4B and 8B, which are trained from Qwen3-4B-Base (Yang et al., 2025) and Qwen3-8B-Base (Yang et al., 2025), respectively. In addition, we benchmark on two established datasets: VerifierBench (Liu et al., 2025) and VerifyBenchhard (Yan et al., 2025). The baselines cover four categories: (1) closed-source models, (2) open-source instruct models, (3) open-source reasoning models, and (4) specialized verifiers. Details are provided in Appendix A.2. For evaluation, we report Accuracy on SCI-VerifyBench, since positive and negative samples are balanced by construction. On VerifierBench and VerifyBench-hard, we additionally report F1 score alongside Accuracy. In all cases, higher values indicate stronger verification performance. 5.2 EVALUATION AND ANALYSIS OF SCI-VERIFYBENCH In this part, we present and analyze the evaluation results on SCI-VerifyBench. Tab. 3 reports the performance of both closed-source and open-source models on SCI-VerifyBench. This comprehensive evaluation enables us to compare the verification capabilities of LLMs of different types and scales under the same settings. We then provide detailed analysis of the experimental results."
        },
        {
            "title": "Preprint Version",
            "content": "Table 4: Performance of different verifiers on VerifierBench and VerifyBench-Hard. Specialized verifiers use default prompts, while all other models are allowed to use reasoning. Models VerifierBench VerifyBench-Hard Avg. Token Acc. Avg. Token GPT-5 (OpenAI, 2025b) Gemini-2.5-Flash (Comanici et al., 2025) Qwen2.5-72B-Instruct (Qwen et al., 2025) Qwen3-30B-A3B-Instruct-2507 (Qwen et al., 2025) LLaMa-3.3-70B-Instruct (Grattafiori et al., 2024) Qwen3-4B (Yang et al., 2025) Qwen3-8b (Yang et al., 2025) GPT-oss-20B (OpenAI et al., 2025) Qwen3-30B-A3B-Thinking-2507 (Yang et al., 2025) Qwen3-235B-A22B (Yang et al., 2025) xVerify-8B (Chen et al., 2025a) CompassVerifier-3B (Liu et al., 2025) CompassVerifier-7B (Liu et al., 2025) CompassVerifier-32B (Liu et al., 2025) SCI-Verifier-4B SCI-Verifier-8B 81.67 88.88 79.00 Acc. F1 Closed-source Models 90.48 91.80 87.63 87.56 Open-source Instruct models 82.61 88.78 79.84 Open-source Reasoning Models 84.42 85.55 83.36 90.42 88.36 Specific Verifiers 78.03 82.39 85.56 89.88 Ours 92.37 93.01 84.54 85.56 83.73 90.05 88.01 75.53 83.37 84.83 88.91 92.01 93.06 203.45 265. 90.40 87.70 85.34 83.65 550.73 972.30 398.99 2119.87 1857.95 523.10 2438.52 5044.43 1.00 1.00 1.00 1.00 85.20 88.70 85. 83.40 84.40 85.90 88.60 86.80 83.20 86.60 87.50 88.30 81.31 85.03 81.10 78.80 79.58 80.36 84.92 82.26 79.60 84.16 84.13 85.86 245.64 302. 381.27 810.24 382.04 1755.61 1588.45 328.50 2226.46 4690.13 1.00 1.00 1.00 1.00 703.47 636.53 88.90 90.30 85.98 87. 470.26 393.61 Open-source models are gradually closing the gap with proprietary models, yet noticeable performance gap remains. On the verification task, many open-source models have approached the performance of closed-source models, including specialized verifiers, but proprietary models still maintain an edge. For instance, GPT-5 outperforms current open-source models by more than 5%. Notably, our proposed SCI-Verifier achieves performance comparable to GPT-5 on the scientific verification task, which confirms the effectiveness of the proposed verifier. Reasoning models and chat models do not exhibit significant differences on this task. On this task, reasoning models show no clear advantage over chat models. We attribute this to the fact that, unlike challenging problems such as IMO-level mathematics, scientific verification tasks are straightforward, requiring domain-specific knowledge and only brief reasoning. Since both model types share similar priors and lack reasoning-specific optimization, performance gains are limited. This observation underscores the need for reasoning tailored to the unique characteristics of verification tasks. Equivalence-based answers poses significant challenges for current LLMs. As shown in Fig. 3, on our equivalence-augmented test set derived from SCIVerifyBench, even state-of-the-art GPT-5 models perform poorly, with scores dropping below 50% in mathematics and physics. This highlights clear deficiency in handling complex equivalence transformations. Remarkably, our SCI-Verifier, in both its 4B and 8B configurations, achieves substantially higher performance on the same tests, owing to targeted optimization for this challenge. These results provide strong evidence for the effectiveness of integrating reasoning capabilities specifically tailored for equivalence verification. Model scale does not have decisive impact on results. Experiments across model scales show that scaling up the model does not consistently improve performance as shown in Fig. 4(a). We hypothesize this is because the verification primarily depends on prior knowledge to assess answer equivalence. Since current models are not optimized for this task, improvements in model capacity do not translate into enhanced verification performance. Figure 3: Evaluation on Equivalent Answer."
        },
        {
            "title": "Preprint Version",
            "content": "Figure 4: (a) Performance on SCI-VerifyBench versus model size. (b) Difficulty comparison across domains in SCI-VerifyBench. (c) Ablation study of training methods. Task characteristics across domains lead to domain-dependent performance differences. As shown in Tab. 3 and Fig. 4(b), performance varies across disciplines and exhibits consistent trends across different models. Scores in mathematics and physics are lower than in other subjects, mainly due to the complex transformations required in these domains, such as factorization, and Taylor expansions, which introduce greater task subtlety. In contrast, judgments in other disciplines are more straightforward once prerequisite knowledge is available. These results highlight the need for verifiers tailored to each disciplines characteristics. 5.3 EVALUATION AND ANALYSIS OF SCI-VERIFIER Generalization of SCI-Verifier. We conduct experiments on our SCI-VerifyBench and two existing verification benchmarks, VerifierBench and VerifyBench-Hard as shown in Tab. 4. The results demonstrate that, both sizes of SCI-Verifier achieve strong performance even at small sizes, reaching levels comparable to the state-of-the-art closed-source model GPT-5. Meanwhile, Fig. 3 demonstrates the strong capability of SCI-Verifier in judging equivalence transformations. The consistent advantage of SCI-Verifier across all three benchmarks indicates its strong verification ability and generalization capability across tasks. Notably, on SCI-VerifyBench, SCI-Verifier outperforms current open-source models in all disciplines, further validating its cross-disciplinary generalization in verification. our other other Models SCI-VerifyBench VerifyBench-Hard our Table 5: Comparison of model robustness across different prompts. our: default prompt; other: modified prompt. Qwen3-30B-A3B-Instruct-2507 GPT-oss-20b Qwen3-235B-A22B CompassVerifier-3B CompassVerifier-7B CompassVerifier-32B Prompt Robustness of SCIVerifier. We investigate the robustness of SCI-Verifier to prompt variations, property that is crucial for real-world applications where prompts must often be adapted to user requirements (Liu et al., 2023b). We evaluate multiple models on three benchmarks using both our proposed CoT prompt and the xVerify prompt (modified to allow reasoning for alignment purposes). Details are provided in Appendix A.2, and the results are summarized in Tab. 5. From these results, we draw two key conclusions: (1) SCI-Verifier exhibits strong robustness to prompt modifications, maintaining competitive performance even when the prompt differs from those seen during training; and (2) general models are considerably more sensitive to prompt variations in verification tasks, largely because they lack an intrinsic notion of answer equivalence and must instead rely on contextual cues. Notably, this sensitivity tends to diminish as model size increases. 76.72 78.08 77.00 74.52 76.44 81.00 88.70 85.90 86.80 86.60 87.50 88.30 75.40 79.50 81.00 79.30 80.30 84.70 78.16 70.32 75.64 76.92 76.28 78.00 SCI-Verifier-4B SCI-Verifier-8B 84.90 85. 88.30 89.70 85.40 86.28 88.90 90.30 5.4 ABLATION STUDY Training Methods. In this section, we analyze the contribution of each component in our two-stage training framework. We conduct experiments on both 4B and 8B models, with results shown in Fig. 5(a). We observe that applying SFT on the base model alone already yields relatively strong performance on verification tasks. Starting RL from reasoning model also achieves competitive"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 5: (a) Ablation study of training data impact. (b) Ablation study of SFT distillation methods. (c) Ablation study of training with CoT in scientific verification. results, whereas directly applying RL on the Base model performs poorly. This may be due to the absence of SFT warm-up, where the Base model requires large amount of training data to acquire targeted reasoning abilities. By contrast, combining SFT with RL leads to consistently superior performance, particularly in terms of generalization across different datasets. These findings highlight that both stages of the proposed training framework are indispensable. Training Data. In this part, we evaluate the quality of our constructed training dataset by comparing it with commonly used dataset (RM) (Zhao et al., 2025) in the Reward Model domain. Using Qwen3-4B-Base as the initial model, we conduct experiments with both datasets under SFT and SFT+RL settings, and the detailed results are presented in Fig. 5(b). The results show that our dataset consistently enables the model to achieve strong performance across three benchmarks, whether used for SFT or RL. This demonstrates the high quality of our data, from which the model can learn richer distributional information about the verification task. The RM dataset also yields reasonable performance under SFT, mainly because of its large scale with more than 180K samples. However, its effectiveness under RL is limited since the heterogeneous quality within such large dataset slows down model improvement, which makes data filtering necessary in practice. These findings confirm that our constructed training dataset, like our test data, is of high quality and reliability. Distillation Data. We investigate the effectiveness of our proposed short CoT distillation. Specifically, we compare the outcomes of distilling complete CoT versus short CoT, with results presented in Fig. 5(b). The findings reveal that distilling complete CoT not only fails to improve performance but also substantially increases output length, rendering it impractical. We attribute this to the nature of the verification task, which is relatively simple and does not require long reasoning chains. Instead, concise reasoning from fixed perspectives is sufficient to achieve strong performance. Therefore, distilling short reasoning traces during the SFT stage is both reasonable and efficient choice. Inference Mode. In this part, we investigate the impact of incorporating reasoning capabilities on model performance. We compare models trained with and without reasoning modes using the same training data, with results shown in Fig. 5(c). We find that omitting chain-of-thought leads to more efficient inference but results in substantial performance drop. This clearly demonstrates the importance of incorporating reasoning abilities for verification tasks in scientific domains."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We highlight verification as critical step toward advancing the scientific reasoning capabilities of LLMs. To this end, we introduce SCI-VerifyBench, high-quality and diverse benchmark spanning mathematics, physics, chemistry, biology, and commonsense scientific QA tasks, designed to rigorously and systematically assess models cross-disciplinary scientific verification capabilities. Our study further demonstrates that chain-of-thought reasoning is essential for scientific verification, particularly when answers are complex or admit multiple equivalent forms. Building on this insight, we develop SCI-Verifier, verifier endowed with concise reasoning abilities specifically tailored for verification tasks. Together, SCI-VerifyBench and SCI-Verifier provide both comprehensive evaluation framework and practical solution for scientific verification, offering strong potential to guide the continued advancement and reliability of LLMs in scientific reasoning."
        },
        {
            "title": "REFERENCES",
            "content": "Nawaf Alampara, Mara Schilling-Wilhelmi, Martiño Ríos-García, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, N. M. Anoop Krishnan, and Kevin Maik Jablonka. Probing the limitations of multimodal language models for chemistry and materials research. arXiv preprint arXiv: 2411.16955, 2024. Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025a. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-ofthought for reasoning large language models, 2025b. URL https://arxiv.org/abs/2503. 09567. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, runxin xu, Zhengyang Tang, Wang Benyou, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Omni-math: universal olympiad level mathematic benchmark for Baobao Chang. In Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu (eds.), Inlarge language models. ternational Conference on Representation Learning, volume 2025, pp. 100540100569, 2025. URL https://proceedings.iclr.cc/paper_files/paper/2025/file/ f9e1e8b56c7e363985ebeb0e9dd1a85c-Paper-Conference.pdf. Leo Gao, Jonathan Tow, Stella Biderman, Shawn Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jasmine Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Version v0. 0.1. Sept, 10:89, 2021. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and Kadian. The llama 3 herd of models, November 2024. Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, and Jiaheng Liu. Think-j: Learning to think for generative llm-as-a-judge, 2025. URL https: //arxiv.org/abs/2505.14268. Greg Gandenberger Hynek Kydlíˇcek. GitHub - huggingface/Math-Verify: robust mathematical expression evaluation system designed for assessing Large Language Model outputs in mathematical tasks., 2024. URL https://github.com/huggingface/Math-Verify. Jiyue Jiang, Pengan Chen, Jiuming Wang, Dongchen He, Ziqin Wei, Liang Hong, Licheng Zong, Sheng Wang, Qinze Yu, Zixian Ma, et al. Benchmarking large language models on multiple tasks in bioinformatics nlp with prompting. arXiv preprint arXiv:2503.04013, 2025. Xuzhao Li, Xuchen Li, Shiyu Hu, Yongzhen Guo, and Wentao Zhang. Verifybench: systematic benchmark for evaluating reasoning verifiers across domains, 2025. URL https://arxiv. org/abs/2507.09884."
        },
        {
            "title": "Preprint Version",
            "content": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.291. URL https://aclanthology.org/2023.acl-long.291/. Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023a. Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024a. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM computing surveys, 55(9):135, 2023b. Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek Wong, Songyang Zhang, et al. Compassverifier: unified and robust verifier for llms evaluation and outcome reward. arXiv preprint arXiv:2508.03686, 2025. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024b. Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as factual inconsistency evaluator for text summarization. arXiv preprint arXiv:2303.15621, 2023. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains, 2025. URL https://arxiv.org/abs/2505. 14652. OpenAI. Introducing OpenAI o3 and o4-mini, April 2025a. URL https://openai.com/ index/introducing-o3-and-o4-mini. OpenAI. Introducing gpt-5, August 2025b. URL https://openai.com/index/ introducing-gpt-5/. OpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, Che Chang, Kai Chen, Mark Chen, Enoch Cheung, Aidan Clark, Dan Cook, Marat Dukhan, Casey Dvorak, Kevin Fives, Vlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun Gogineni, Adam Goucher, Lukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke, Alec Helyar, Haitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman, Dominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume Leclerc, James Park Lennon, Scott Lessans, Mario Lezcano-Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss, Lily, Liu, Jiancheng Liu, Kevin Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum, Josh McGrath, Scott McKinney, Aidan McLaughlin, Song Mei, Steve Mostovoy, Tong Mu, Gideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino, Dana Palmie, Ashley Pantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic Peran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo Raso, Hongyu Ren, Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh Sanjeev, Max Schwarzer, D. Sculley, Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song, Dane Stuckey, Zhiqing Sun, Philippe Tillet, Sam Toizer, Foivos Tsimpourlas, Nikhil Vyas, Eric Wallace, Xin Wang, Miles Wang, Olivia Watkins, Kevin Weil, Amy Wendling, Kevin Whinnery, Cedric Whitney, Hannah Wong, Lin Yang, Yu Yang, Michihiro Yasunaga, Kristen Ying, Wojciech Zaremba, Wenting Zhan, Cyril Zhang, Brian Zhang, Eddie Zhang, and Shengjia Zhao. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow"
        },
        {
            "title": "Preprint Version",
            "content": "instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, January 2025. Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, and Jiajun Zhang. Towards scientific intelligence: survey of llm-based scientific agents, 2025. URL https://arxiv.org/abs/ 2503.24047. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37: 9526695290, 2024. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning, 2025. URL Saha. https://arxiv.org/abs/2505.10320. Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, and Yueting Zhuang. Verifybench: Benchmarking referencebased reward systems for large language models, 2025. URL https://arxiv.org/abs/ 2505.15801. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. Reasoning models know when theyre right: Probing hidden states for self-verification. arXiv preprint arXiv:2504.05419, 2025a. Taolin Zhang, Maosong Cao, Alexander Lam, Songyang Zhang, and Kai Chen. Compassjudger-2: Towards generalist judge model via verifiable rewards, 2025b. URL https://arxiv.org/ abs/2507.09104. Yulai Zhao, Haolin Liu, Dian Yu, SY Kung, Haitao Mi, and Dong Yu. One token to fool llm-as-ajudge. arXiv preprint arXiv:2507.08794, 2025. Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, et al. Scaling physical reasoning with the physics dataset. arXiv preprint arXiv:2506.00022, 2025."
        },
        {
            "title": "Preprint Version",
            "content": "Appendix for SCI-Verifier DETAILS OF SCI-VERIFYBENCH In this section, we introduce details of the process of constructing SCI-VerifyBench, including the prompts and models used for data generation described in Sec. A.1, the model annotation process and the prompts and parameters designed for practical use described in Sec. A.2, as well as the data details and sample cases in SCI-VerifyBench described in Sec. A.3. A.1 DATA GENERATION The data generation involves two parts. The first part uses LLMs to generate answers for existing questions, and correctness is determined by comparing the generated answers with the reference answers. The second part generates equivalent answers based on the characteristics of different subjects, testing whether the model can correctly recognize these equivalent forms. For both parts, multiple LLMs are used to generate candidate answers, including Qwen3-32B, Qwen3-30B-A3BThinking-2507, Qwen3-30B-A3B-Instruct-2507, LLaMa3.3-70B-Instruct, GPT-oss-20B, Qwen2.532B-Instruct, Gemma-3-27b-it, and Qwen3-8B. The prompt used in the first part is shown in Box. A.1. For the second part, different prompts are used for each subject according to the corresponding task. The Math prompts refer to Box. A.2 to Box. A.4. The Physics prompts refer to Box. A.5 to Box. A.7. The Chemistry prompts refer to Box. A.8 to Box. A.16. The Biology prompts refer to Box. A.17 to Box. A.21. The QA prompts refer to Box. A.22. Box A.1: Prompt for generating LLM response Please answer the problem adhering to the following rules: 1. Please use LaTeX format to represent the variables and formulas used in the solution process and results. 2. Please put the final answer(s) in boxed{}, note that the unit of the answer should not be included in boxed{}. 3. If the problem requires multiple answers, list them in order, each in separate boxed{}. Problem:{answer} Box A.2: Prompt for generating equivalent answers to mathematical interval problems You are given mathematical interval answer. Generate 10 different equivalent forms of this interval, using transformations such as: Rewriting as inequalities: [0, 1] 0 1 Rewriting as set operations:[0, 2] [1, 3] = [0, 3] Open/closed interval limit definitions: (a, b) = limϵ0+ [a + ϵ, ϵ] Converting to numeric sets: [0, 2] {0, 1, 2} (for integer endpoints) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: [0, 1] Output: 0 1 [0, 1) 1 (0 0.0001, 1 + 0.0001) 0 1 0, 0.25, 0.5, 0.75, 1 Only output the converted results, do not output the conversion process! Now, process the following answer: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.3: Prompt for generating equivalent answers to mathematical expression problems You are given mathematical expression or numeric answer. Generate 10 different equivalent forms of this expression, using transformations such as: Factoring or expansion: x2 + 2x + 1 (x + 1)2 Fraction simplification: (x2 1)/(x + 1) 1 Leaving fraction unsimplified: (x2 1)/(x + 1) (unchanged) Partial fraction decomposition: 1/(x(x + 1)) 1/x 1/(x + 1) Fraction to decimal conversion: 1/2 0.5 Trigonometric identities: sin2 + cos2 = 1 Trigonometric transformations: sin 2x = 2 sin cos Taylor expansion: sin x3/3! Exponential/logarithm rules: ln(ab) = lna + lnb Substitution: let y=x+1, then x2 + 2x + 1 = y2 Approximating special constants: π 3.14159, 2.718 Angle-radian conversion: π/6 = 30 Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: (x + 1)2 Output: (x + 1)(x + 1) x2 + 2x + 1 (x + 1)3 + Let = + 1, ; y2 (x + 1)2 1 + 2x; for small Only output the converted results, do not output the conversion process! Now, process the following answer: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.4: Prompt for generating equivalent answers to mathematical equation problems You are given mathematical equation. Generate 10 different equivalent forms of this equation, using transformations such as: Moving terms: + 3 = 5 = 2 Multiplying/dividing both sides: 2x = 4 = 2 Factoring: x2 1 = 0 (x 1)(x + 1) = 0 Completing the square: x2 + 6x + 5 = 0 (x + 3)2 4 = 0 Root extraction: x2 = 4 = 2 Substitution in equations:x2 + 1 = 0 letx = i, theni2 + 1 = 0 Trigonometric transformations: sin2 = 1 cos2 Taylor expansion for approximate solution: sin 0 Domain restrictions: 1 = 3 requires 1 Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: x2 1 = 0 Output: x2 = 1 (x 1)(x + 1) = 0 = 1 (x 0)2 1 = 0 cos2 θ sin2 θ = 0 (substitution = cos θ) Only output the converted results, do not output the conversion process! Now, process the following answer: {answer}, Box A.5: Prompt for generating equivalent answers to physics numerical problems Given the following question and an answer in expression form, generate 10 different equivalent forms of this interval, using transformations such as: Arithmetic operations or evaluation (e.g., 5 + 3 8) Substitution of variable values (e.g., 2x + 3,x = 2 7) Scientific notation, fractions, or decimals (e.g., 0.00012 1.2 104) Unit conversion or dimensional adjustment (e.g., 1km 1000m) Dimensional conversion (e.g., 1N 1kg m/s2) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: [0,1] Output: 0 [0, 1) 1 (0 0.0001, 1 + 0.0001) 0 1 0, 0.25, 0.5, 0.75, 1 Only output the converted results, do not output the conversion process! Now, process the following answer: Question: {question}. Answer (Numeric): {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.6: Prompt for generating equivalent answers to physics expression problems Given the following question and an answer in expression form, generate 10 different equivalent forms of this interval, using transformations such as: Factoring or expansion: x2 + 2x + 1 (x + 1)2 Fraction simplification: (x2 1)/(x + 1) 1 Leaving fraction unsimplified: (x2 1)/(x + 1) (unchanged) Partial fraction decomposition: 1/(x(x + 1)) 1/x 1/(x + 1) Fraction to decimal conversion:1/2 0.5 Trigonometric identities: sin2 + cos2 = 1 Trigonometric transformations: sin 2x = 2 sin cos Taylor expansion: sinx x3/3! Exponential/logarithm rules: ln(ab) = lna + lnb Substitution: let = + 1, then x2 + 2x + 1 = y2 Approximating special constants: π 3.14159, 2.718 Angle-radian conversion: π/6 = 30 Dimensional conversion (e.g., 1N 1kg m/s2) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: (x + 1)2 Output: (x + 1)(x + 1) x2 + 2x + 1 (x + 1)3 + 1 Let = + 1; y2 (x + 1)2 1 + 2x; for small Only output the converted results, do not output the conversion process! Now, process the following answer: Question: {question}. Answer (expression): {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.7: Prompt for generating equivalent answers to physics equation problems Given the following question and an answer in expression form, generate 10 different equivalent forms of this interval, using transformations such as: Moving terms: + 3 = 5 = 2 Multiplying/dividing both sides: 2x = 4 = 2 Factoring: x2 1 = 0 (x 1)(x + 1) = 0 Completing the square: x2 + 6x + 5 = 0 (x + 3)2 4 = 0 Root extraction: x2 = 4 = 2 Substitution in equations: x2 + 1 = 0 letx = i, theni2 + 1 = 0 Trigonometric transformations: sin2 = 1 cos2 Taylor expansion for approximate solution: sin 0 Domain restrictions: (cid:112)(x 1) = 3 requires 1 Dimensional conversion (e.g., = ma, = 1000g, = 2m/s2 = 2N ) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: x2 1 = 0 Output: x2 = 1 (x 1)(x + 1) = 0 = 1 (x 0)2 1 = cos2 θ sin2 θ = 0 (substitution = cos θ) Only output the converted results, do not output the conversion process! Now, process the following answer: Question: {question}. Answer (equation): {answer}"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.8: Prompt for generating equivalent answers to chemical solvent prediction problems Given chemical reaction and predicted solvent, generate 10 equivalent forms of the solvent answer, considering: Different chemical names (IUPAC, common, or trivial) Abbreviations (e.g., EtOH) Molecular formulas (e.g., C2H5OH) SMILES representations Solvent class equivalence (e.g., polar protic, polar aprotic) Mixture equivalences (e.g., EtOH:H2O 1:1 H2O:EtOH 1:1) Format: Output exactly 10 forms, each wrapped in LaTeX boxed{...} and separated by newline. Example: Input: Ethanol Output: Ethanol EtOH C2H5OH CCO alcohol polar protic solvent EtOH:H2O 1:1 H2O:EtOH 1:1 ethyl alcohol C-C-O Only output the converted results, do not output the conversion process! Now process: Reaction: {question} Predicted solvent: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.9: Prompt for generating equivalent answers to chemical property prediction problems Given molecule and predicted property value, generate 10 equivalent forms, using: Unit conversions (e.g., Celsius Kelvin Fahrenheit) Ranges vs single values Approximate vs exact Different notations (e.g., logP, Kow) Descriptive labels (e.g., high solubility) Format: 10 forms wrapped in LaTeX boxed{...}. Example: Input: Water boiling point = 100 Output: 100 373 212F 100 101 high boiling point approx. 100C 373.15K 0.1 103 100 Celsius water boils at 100C Only output the converted results, do not output the conversion process! Now process: Molecule: {question} Property: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.10: Prompt for generating equivalent answers to chemical yield prediction problems Given chemical reaction and predicted yield, generate 10 equivalent forms: Percentage decimal Ranges single value Descriptive labels (e.g., high yield, trace) Approximate expressions Format: 10 forms, LaTeX boxed{...}, newline separated. Example: Input: 85% Output: 85% 0.85 high yield 80 90% approx. 85% = 0.85 yield > 80% quantitative yield major product trace product Only output the converted results, do not output the conversion process! Now process: Reaction: {question} Predicted yield: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.11: Prompt for generating equivalent answers to chemical retrosynthesis problems Given target molecule, generate 10 equivalent retrosynthesis answers: Different valid disconnections Different reagents or reducing / oxidizing agents Alternative synthetic routes (single-step or multi-step) Protecting group alternatives Functional group or stereochemical equivalence Format: 10 forms wrapped in LaTeX boxed{...}. Example: Input: Target = Benzyl alcohol Output: Benzaldehyde + NaBH4 Benzyl alcohol Benzyl chloride + NaOH Benzyl alcohol Benzaldehyde reduced by LiAlH4 Benzyl bromide + KOH Benzyl alcohol Benzaldehyde H2 + Pd/C Benzyl alcohol C6H5CH2Cl + NaOH C6H5CH2OH Alternative protecting group: Boc route Multi-step oxidation-reduction route Reductive amination route to same alcohol Electrochemical reduction route Only output the converted results, do not output the conversion process! Now process: Target molecule: {question} Retrosynthesis answer: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.12: Prompt for generating equivalent answers to chemical temperature prediction problems Given reaction and predicted temperature, generate 10 equivalent forms: Different units (C, K, F) Ranges vs single value Descriptive labels (low, room temp, high) Common lab descriptions (ice bath, reflux) Format: LaTeX boxed{...}, 10 forms. Example: Input: 80C Output: 80 353 176 70 90 approx. 80C reflux room temperature ice bath moderate heating high temperature Only output the converted results, do not output the conversion process! Now process: Reaction: {question} Predicted temperature: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.13: Prompt for generating equivalent answers to chemical product prediction problems Given reactants and reaction conditions, generate 10 equivalent forms of the predicted product: Different structure representations: SMILES, InChI, molecular formula IUPAC name, common name, trivial name Stereoisomers (R/S) Tautomers, salts, hydrates Alternative valid representations (chair/boat conformers) Format: 10 LaTeX boxed{...} outputs. Example: Input: Nitrobenzene Output: C6H5NO2 c1ccc(cc1)[N+](=O)[O-] Nitrobenzene Benzene nitro compound C6H5-NO2 Nitrobenzol C6H5NO2 H2O aromatic nitro compound benzene derivative Racemic mixture if applicable Only output the converted results, do not output the conversion process! Now process: Reactants: {question} Predicted product: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.14: Prompt for generating equivalent answers to chemical mol2caption problems Given molecular structure, generate 10 equivalent textual descriptions: IUPAC name Common/trivial names Chinese name Functional description (solvent, reagent, drug) Usage or property description Format: LaTeX boxed{...}, 10 forms. Example: Input: C2H5OH Output:"
        },
        {
            "title": "Ethanol\nEtOH\nalcohol\nethyl alcohol",
            "content": "common solvent disinfectant flammable liquid reagent in reactions soluble in water Only output the converted results, do not output the conversion process! Now process: Molecule: {question} Mol2caption answer: {answer}, Box A.15: Prompt for generating equivalent answers to chemical caption2mol problems Given textual description of molecule, generate 10 equivalent molecular representations: Different SMILES strings (canonical and non-canonical) InChI Molecular formula Graphical structure representation (if feasible) Different naming conventions resolved to the same structure Format: LaTeX boxed{...}, 10 forms. Example: Output: Input: \"alcohol used for disinfection\" C2H5OH CCO OCC InChI=1S/C2H6O/c1-2-3/h3H,2H2,1H3 ethanol EtOH ethyl alcohol common lab alcohol solvent for reactions Only output the converted results, do not output the conversion process! Now process: Caption: question Predicted molecule: answer,"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.16: Prompt for generating equivalent answers to chemical name conversion problems Given molecule name in one format, generate 10 equivalent forms: IUPAC trivial/common name English Chinese name SMILES InChI Hydrate/salt forms if applicable Synonyms and international spelling variants Format: LaTeX boxed{...}, 10 outputs. Example: Input: Acetic acid Output: Acetic acid ethanoic acid CH3COOH C2H4O2 Vinegar acid Acetic acid, glacial Acetic acid aqueous solution Sodium acetate form Acid CH3COOH Only output the converted results, do not output the conversion process! Now process: Name: {question} Predicted conversion: {answer}"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.17: Prompt for generating equivalent answers to biological protein inverse folding problems Given the following question and an answer in protein sequence form, generate 10 different equivalent forms of this sequence, using transformations such as: Synonymous sequences folding into the same structure - Conservative amino acid substitutions (e.g., Lys Arg, Asp Glu) FASTA format plain sequence string One-letter code three-letter code Sequence with small tolerated mutations that preserve structure Adding/removing header lines in FASTA Using lowercase vs uppercase amino acid letters Introducing gap symbols to indicate alignment but preserving structure Representing sequence in JSON or array form Grouping amino acids by domains/regions but preserving the overall sequence Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Example: Input: MKAILVVLLYTAA Output: MKAILVVLLYTAA Met-Lys-Ala-Ile-Leu-Val-Val-Leu-Leu-Tyr-Thr-Ala-Ala >seq1nMKAILVVLLYTAA mkailvvll ytaa MKAILVVLYTAA [M,K,A,I,L,V,V,L,L,Y,T,A,A] MKAILVVLLY(S/T)AA MKAILVVLLY RAA (Arg substitution) [Domain1: MKAILV, Domain2: VLLYTAA] MKAILVVLLYTAA (unchanged) Only output the converted results in LaTeX boxed..., do not output the conversion process! Now, process the following answer: Question: {question} Answer: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.18: Prompt for generating equivalent answers to biological protein structure prediction problems Given the following question and an answer in protein structure form, generate 10 different equivalent forms of this structure, using transformations such as: PDB format mmCIF format 3D atomic coordinates Cα-only coordinates 3D structure contact map distance matrix Cartesian coordinates internal torsion angles (ϕ, ψ, χ) Secondary structure representation (helix/sheet/loop) instead of full 3D Alternative but equivalent chain numbering or atom ordering Superimposed structures with RMSD < threshold JSON or graph-based adjacency representation of the structure Coarse-grained models (e.g., backbone only) Visual encodings (e.g., dot-bracket for secondary structure, schematic diagrams) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Example: Input: PDB file with Cα coordinates of helix Output: ATOM 1 CA ALA 1 0.000 0.000 0.000 loop helix helix sheet contact map: (1,5),(2,6),(3,7) distance matrix: [[0,1.2,...],[1.2,0,...],...] torsion angles ϕ = 60, ψ = 45 mmCIF equivalent entry JSON: \"atoms\":[\"res\":\"ALA\",\"atom\":\"CA\",\"x\":0,\"y\":0,\"z\":0] graph: nodes=7, edges=[(1,5),(2,6),(3,7)] backbone-only representation cartoon schematic: helix symbol Only output the converted results in LaTeX boxed{...}, do not output the conversion process! Now, process the following answer: Question: {question} Answer: {answer},"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.19: Prompt for generating equivalent answers to biological transformed agentclinic problems Given the following question and an answer in agent action/trajectory form, generate 10 different equivalent forms of this solution, using transformations such as: Different but equivalent action sequences leading to same outcome Merging multiple actions into macro-actions Splitting macro-actions into finer-grained steps JSON tabular natural language action logs Reordering independent actions without changing outcome Adding no-op (null) actions that do not affect outcome Representing state transitions instead of actions Graph or tree representation of the policy trace Summarizing actions at high-level clinical outcome description Encoding with symbolic tokens instead of natural language Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Example: Input: Give drug Measure blood pressure Stop treatment Output: Give drug Measure BP Stop treatment Action1, Action2, Action3 [Give drug A, Measure blood pressure, Stop treatment] MacroAction: Treat+Monitor State transitions: S0 S1 S2 Add no-op: Give drug Wait Measure BP Stop JSON: actions\":[Give drug A\",Measure BP\",Stop\"] Table: 1. Give drug 2. Measure BP 3. Stop Graph representation: nodes=states, edges=actions Outcome: Patient stabilized after drug Only output the converted results in LaTeX boxed{...}, do not output the conversion process! Now, process the following answer: Question: question Answer: answer\"\"\","
        },
        {
            "title": "Preprint Version",
            "content": "Box A.20: Prompt for generating equivalent answers to biological rna structure prediction problems Given the following question and an answer in RNA structure form, generate 10 different equivalent forms of this structure, using transformations such as: Dot-bracket notation CT format BPseq format Base-pairing list adjacency matrix representation 2D secondary structure 3D atomic coordinates Minimal free energy structure near-optimal structure Adding base-pair probabilities annotation Grouping stems/loops/bulges into modular blocks JSON array representation of base pairs ASCII art of RNA fold representation Graph representation of paired/unpaired nodes - Annotated sequence with paired regions in brackets Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Example: Input: GCGCUUAGC, Structure: (((...))) Output: (((...))) dot-bracket: (((...))) CT: 1 9, 2 8, 3 7 BPseq: 1 9, 2 8, 3 7 pairs=[(1,9),(2,8),(3,7)] matrix: [[0,0,1,...],...] near-optimal structure (((..).)) ASCII: stem-loop diagram JSON: pairs\":[[1,9],[2,8],[3,7]] annotated sequence: GCG(CUU)AGC Only output the converted results in LaTeX boxed{...}, do not output the conversion process! Now, process the following answer: Question: {question} Answer: {answer}"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.21: Prompt for generating equivalent answers to biological rna inverse folding problems Given the following question and an answer in RNA sequence form, generate 10 different equivalent forms of this sequence, using transformations such as: Different sequences folding into the same target structure replacement (RNA vs DNA notation) FASTA format plain string sequence Lowercase vs uppercase bases Adding alignment gaps without changing fold JSON or array encoding of sequence Annotating sequence with regions (stem, loop, bulge) Replacing synonymous positions with alternative nucleotides that preserve folding Annotated sequence with secondary structure alignment marks Splitting long sequence into chunks and recombining Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Example: Output: Input: AUGCGAU AUGCGAU augcgau ATGCGAT >seq1nAUGCGAU AU-GCGAU [A,U,G,C,G,A,U] stem(AUG) loop(CGAU) AUGCGAU (unchanged) JSON: \"sequence\":\"AUGCGAU\" chunks: AU GC GA Only output the converted results in LaTeX boxed{...}, do not output the conversion process! Now, process the following answer: Question: {question} Answer: {answer}"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.22: Prompt for generating equivalent answers to QA problems You are given an answer to general question. Generate 10 different equivalent forms of this answer, using transformations such as: Paraphrasing the text while keeping the meaning the same. Changing the sentence structure or word order. Using synonyms or alternative expressions. Expressing the answer as list, table, or bullet points if applicable. Rewriting as formal statements, equations, or logical expressions if relevant. Approximating special constants: π 3.14159, 2.718 Angle-radian conversion: π/6 = 30 Dimensional conversion (e.g., 1N 1kg m/s2) Format: Output exactly 10 forms. Each form must be wrapped in LaTeX boxed{...}. Separate each answer with newline (n). Example: Input: \"Water boils at 100 degrees Celsius at sea level.\" Output: Water reaches its boiling point at 100C at sea level. At sea level, the boiling temperature of water is 100 degrees Celsius. 100C is the temperature at which water boils at sea level. Only output the converted results, do not output the conversion process! Now, process the following answer: Question: {question} Answer (expression): {answer} A.2 DATA ANNOTATION AND EVALUATION Next, we describe the configurations and prompts used during data annotation and the actual evaluation. In this process, the inputs are the question, the reference answer, and the answer to be evaluated, and the output is the correctness of the answer being evaluated. To ensure stable outputs during the experiments, temperature of 0 is used. The prompts with CoT are shown in Box. A.23, the prompts without CoT are shown in Box. A.24, and the prompts used in the main experiments to measure prompt stability are shown in Box. A.25. The LLMs used during the data annotation process were Qwen3-30B-A3B-Instruct-2507, GPT-oss-20B, Qwen2.5-72B-Instruct, LLaMa3.3-Instruct, and CompassVerifier-32B."
        },
        {
            "title": "Preprint Version",
            "content": "Box A.23: Prompt of Inference with Thinking As grading reward model, your task is to evaluate whether the candidates final answer matches the provided standard answer. You must first output detailed step-by-step analysis, then give final structured judgment. Do not regenerate or improve answers, only compare. Evaluation Protocol: 1. Reference Standard: The standard (gold) answer is definitive and always correct. The question is always valid never challenge it. Do not regenerate answers; only compare candidates answer with the gold answer. 2. Comparison Method: Analyze the questions requirements and the gold answers structure. Determine if the question requires exact matching or allows equivalence. Compare ONLY the candidates final answer. Ignore reasoning errors. Ignore differences in formatting or style. For math expressions: check algebraic equivalence step by step; if uncertain, test numerically at multiple points. For multiple-choice: only compare the final choice and its content. 3. Multi-part Answers: All parts must match the gold answer exactly. Partial matches are incorrect. If not specified, order may vary. For example, 27 4. Validity Check: 7 , 8 7 and 7 , 27 7 are equivalent. If incomplete (cut off, unfinished sentence) Label as INCOMPLETE. If repetitive (looping words/phrases) Label as REPETITIVE. If explicit refusal (e.g., \"I cannot answer...\") Label as REFUSAL. Gives an answer but then negates it at the end Label as REFUSAL. Any of the above classify as with the correct error type. Grading Scale: - CORRECT: Matches gold exactly or equivalent (including algebraic/numeric equivalence). For numerical values: equivalent if equal under rounding tolerance. Semantic equivalence allowed. - INCORRECT: Any deviation from gold. Partial matches for multi-part answers. - INCOMPLETE/REPETITIVE/REFUSAL: Invalid answers (must specify error type). Execution Steps and Output: Analysis step by step: 1. First check validity (INCOMPLETE, REPETITIVE, REFUSAL). 2. Compare candidates final answer vs. gold answer in detail. Identify strict requirements (e.g., exact match, order, completeness). Allow tolerances (format differences, equivalent math forms, unsimplified fractions, provide the full answer for completion-type questions). Check for equivalences, e.g., Consider: 2x7 (x+1)(x2) and 3 x+1 1 x2 are equivalent. Factoring/expansion: x2 + 2x + 1 (x + 1)2 Fraction simplification: x21 Partial fraction decomposition, decimals, trig identities, substitutions, x+1 1 etc. For multiple-choice, answer must exactly match gold or be fully equivalent. 3. Provide thorough reasoning chain, highlighting subtle equivalences or deviations. Final Judgment: A/B/C Here is your task: <Original Question Begin>{question} <Original Question End> <Standard Answer Begin>{gold answer} <Standard Answer End> <Candidates Answer Begin>{llm response} <Candidates Answer End> Analysis step by step (not to try solving the problem yourself) and Final Judgment:"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.24: Prompt of Inference without Thinking As grading reward model, your task is to evaluate whether the candidates final answer matches the provided standard answer. You must only give final structured judgment. Do not regenerate or improve answers, only compare. Evaluation Protocol: 1. Reference Standard: The standard (gold) answer is definitive and always correct. The question is always valid never challenge it. Do not regenerate answers; only compare candidates answer with the gold answer. 2. Comparison Method: Analyze the questions requirements and the gold answers structure. Determine if the question requires exact matching or allows equivalence. Compare ONLY the candidates final answer. Ignore reasoning errors. Ignore differences in formatting or style. For math expressions: check algebraic equivalence step by step; if uncertain, test numerically at multiple points. For multiple-choice: only compare the final choice and its content. 3. Multi-part Answers: All parts must match the gold answer exactly. Partial matches are incorrect. If not specified, order may vary. For example, 27 4. Validity Check: 7 , 8 7 and 7 , 27 7 are equivalent. If incomplete (cut off, unfinished sentence) Label as INCOMPLETE. If repetitive (looping words/phrases) Label as REPETITIVE. If explicit refusal (e.g., \"I cannot answer...\") Label as REFUSAL. Gives an answer but then negates it at the end Label as REFUSAL. Any of the above classify as with the correct error type. Grading Scale: - CORRECT: Matches gold exactly or equivalent (including algebraic/numeric equivalence). For numerical values: equivalent if equal under rounding tolerance. Semantic equivalence allowed. - INCORRECT: Any deviation from gold. Partial matches for multi-part answers. - INCOMPLETE/REPETITIVE/REFUSAL: Invalid answers (must specify error type). Annotation Criteria: 1. First check validity (INCOMPLETE, REPETITIVE, REFUSAL). 2. Compare candidates final answer vs. gold answer in detail. Identify strict requirements (e.g., exact match, order, completeness). Allow tolerances (format differences, equivalent math forms, unsimplified fractions, provide the full answer for completion-type questions). Check for equivalences, e.g., Consider: 2x7 (x+1)(x2) and 3 x+1 1 x2 are equivalent. Factoring/expansion: x2 + 2x + 1 (x + 1)2 Fraction simplification: x21 Partial fraction decomposition, decimals, trig identities, substitutions, x+1 1 etc. For multiple-choice, answer must exactly match gold or be fully equivalent. 3. Only give the final judgment. Final Judgment: A/B/C Here is your task: <Original Question Begin>{question} <Original Question End> <Standard Answer Begin>{gold answer} <Standard Answer End> <Candidates Answer Begin>{llm response} <Candidates Answer End> Final Judgment:"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.25: Prompt of Inference for the evaluation of robustness You are diligent and precise assistant tasked with evaluating the correctness of responses. You will receive question, an output sentence, and the correct answer. Your task is to determine if the output sentence accurately answers the question based on the provided correct answer. Respond with either [Correct] or [Incorrect]. - Special considerations: 1. **Multiple Answers**: If the output contains multiple answers, evaluate whether later answers modify or correct earlier ones. In such cases, compare the final answer with the correct answer. If the final answer is unclear or incorrect, respond with [Incorrect]. 2. **Mathematical Problems**: If the formats differ but the answers are mathematically equivalent, respond with [Correct]. 3. **Explicit Options**: If the question provides explicit candidate answers, the output will be considered correct if it clearly indicates the correct options code or the correct options content. 4. **No Explicit Options**: If the question does not provide explicit options, the output must align with the correct answer in content and meaning to be considered [Correct]. - Question: {question} Output sentence: {llm response} Correct answer: {gold answer} Judgement: A.3 DATA DETAILS In this section, we present several data cases, focusing primarily on examples of the equivalent forms we generated. Box. A.26 shows an equivalent example in mathematics, where the answers in the Outputs will undergo both LLM annotation and human annotation. Box. A.27 presents an equivalent example in physics, Box. A.28 in chemistry, Box. A.29 in biology, and Box. A.30 provides an example for QA questions. Box A.26: sample for generating equivalent answers to mathematical problems Question: Find all functions : Q+ (cid:55) Q+ such that: (x) + (y) + 2xyf (xy) = (xy) (x+y) . Gold Ans: 1 x2 Outputs: x2 1 (cid:18) 1 (cid:19)2 1 x2 for = 0 1 x2 = x"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.27: sample for generating equivalent answers to physical problems Question: Set crystal prism with an apex angle of 60, and the optical axis is perpendicular to the main cross-section of the prism. The sodium yellow light is refracted in the prism in the direction of the minimum deviation angle and focused using lens with focal length of 1 m. What is the separation between the o-ray and e-ray spectral lines? Gold Ans: 7.16 mm Outputs: 7.16 mm 7.16 mm 0.00716 7160 µm 7.16 103 Box A.28: sample for generating equivalent answers to chemical problems Question: Im looking for the SMILES version of this IUPAC chemical name. N4-(3,4-difluorophenyl)-2-methyl-N6-prop-2-enylpyrimidine-4,6-diamine Gold Ans: CC1=NC(=CC(=N1)NC2=CC(=C(C=C2)F)F)NCC=C Outputs: N4-(3,4-difluorophenyl)-2-methyl-N6-prop-2-enylpyrimidine-4,6-diamine 2-methyl-N6-(prop-2-en-1-yl)-N4-(3,4-difluorophenyl)pyrimidine-4,6-diamine CC1=NC(=CC(=N1)NC2=CC(=C(C=C2)F)F)NCC=C C12N(C=C(N1)C)C=CC(NC3=CC=C(C=C3)F)C=C2F"
        },
        {
            "title": "Preprint Version",
            "content": "Box A.29: sample for generating equivalent answers to biological problems Question: Please predict the dot-bracket notation of the secondary structure directly from the RNA sequence: UCUGAUCCAUGAUGACUUCCAAACACAUGUGGACUGUUGGAUGAAGCAUGCGGAUAUGGGACUGAGAUCAGAC Gold Ans: .((((..............................................................)))).. Outputs: .((((..............................................................)))).. CT: 2 30, 3 29, 4 28, 5 27, 6 26, 7 25, 8 24, 9 23, 10 22, 11 21, 12 20, 13 19, 14 18, 15 BPseq: 2 30, 3 29, 4 28, 5 27, 6 26, 7 25, 8 24, 9 23, 10 22, 11 21, 12 20, 13 19, 14 18, 15 17 pairs= [(2,30),(3,29),(4,28),(5,27),(6,26),(7,25),(8,24), (9,23),(10,22),(11,21),(12,20),(13,19),(14,18),(15,17)] matrix: [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1], ..., [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]] Box A.30: sample for generating equivalent answers to QA problems Question: Whats the name of the active president of the United States in Season 1 of Motherland: Fort Salem? Gold Ans: President Kelly Wade Outputs: The active president of the United States in Season 1 of Motherland: Fort Salem is named Kelly Wade. Kelly Wade is U.S. President in Season 1 of Motherland: Fort Salem. In the first season of Motherland: Fort Salem, the president is Kelly Wade. Kelly Wade is the incumbent U.S. leader in Season 1 of Motherland: Fort Salem. In Season 1 of Motherland: Fort Salem, Kelly Wade serves as the U.S. president."
        },
        {
            "title": "B DETAILS OF TRAINING",
            "content": "In this section, we present the parameter configurations used during the training process. The training mainly involves two parts: SFT and RL. For SFT, we adopt full fine-tuning, and the detailed parameter configurations are shown in Tab. 6. For RL, modified version of GRPO is used, with detailed parameters also provided in Tab. 7."
        },
        {
            "title": "C LIMITATIONS AND FUTURE WORK",
            "content": "In this section, we discuss the limitations of our work and outline directions for future research. We propose verifier for scientific verification tasks that demonstrates strong reasoning capabilities, achieving high performance with concise and interpretable reasoning outputs. However, some scenarios demand both high accuracy and extreme efficiency. In future work, we plan to leverage the models explicit reasoning abilities to further enhance its implicit reasoning, allowing it to maintain strong performance even without explicitly generating detailed reasoning steps. This approach could provide significant efficiency gains while preserving the models reliability and robustness across wider range of scientific verification tasks. Table 6: SFT Configuations for SCI-Verifier. Parameter Value BF16 Gradient Checkpointing Learning Rate LR Scheduler Type Minimum LR Rate Packing Maximum Sequence Length Maximum Steps Number of Training Epochs Per Device Train Batch Size Per Device Eval Batch Size GPUs Per Node Number of Nodes Seed Use Liger Kernel Warmup Ratio True False 5 105 cosine_with_min_lr 0.1 False 1024 -1 2 2 16 4 1 42 True 0. Table 7: RL Configuations for SCI-Verifier. Parameter BF16 Temperature Top Clip Ratio Low Clip Ratio High Max Response Length Overlong Buffer Len Learning Rate Number of Training Epochs GPUs Per Node Number of Nodes Seed Value True 1.0 1.0 0.2 0.28 2048 1024 1 106 20 4"
        },
        {
            "title": "D THE USE OF LARGE LANGUAGE MODELS",
            "content": "In our paper, LLMs are first used to polish the writing, improving the clarity and readability of the manuscript. At the same time, as mentioned multiple times in the main text and Appendix, we employ LLMs to generate and annotate training and test data. Since LLM outputs can sometimes be unreliable, as noted in the text, all selected data are subsequently manually re-annotated by human experts and carefully filtered."
        }
    ],
    "affiliations": [
        "CUHK",
        "CUHK-Shenzhen",
        "Fudan University",
        "Harbin Institute of Technology",
        "Shanghai AI Laboratory",
        "Tsinghua University",
        "UESTC",
        "USTC"
    ]
}