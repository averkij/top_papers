{
    "paper_title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "authors": [
        "Yifan Shen",
        "Yuanzhe Liu",
        "Jingyuan Zhu",
        "Xu Cao",
        "Xiaofeng Zhang",
        "Yixiao He",
        "Wenming Ye",
        "James Matthew Rehg",
        "Ismini Lourentzou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks."
        },
        {
            "title": "Start",
            "content": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs Yifan Shen1, Yuanzhe Liu2, Jingyuan Zhu2, Xu Cao1, Xiaofeng Zhang3, Yixiao He1, Wenming Ye4, James Matthew Rehg1, Ismini Lourentzou1 {yifan26,lourent2}@illinois.edu 1University of Illinois Urbana-Champaign 2University of Pennsylvania 3Shanghai Jiao Tong University 4Google 5 2 0 2 6 2 ] . [ 1 6 5 6 1 2 . 6 0 5 2 : r Abstract. Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (f DPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that DPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with DPO, sets new SoTA on SpatialRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks. (cid:128) https://plan-lab.github.io/spatialreasoner/ 1. Introduction Vision-Language Models (VLMs) have demonstrated significant advancements in multimodal understanding tasks, such as image captioning, visual question answering, object detection, and video interpretation [2, 31, 53, 61, 93]. However, their ability to perform spatial reasoning remains limited, especially in scenarios involving complex object arrangements, occlusions, and precise spatial relationships [7, 12, 17, 79]. This gap poses significant challenge for applications such as robotics, autonomous driving, and augmented reality, where robust spatial understanding is essential for effective decision-making [50]. Historically, early VLMs predominantly employed direct-response paradigms [2, 53], i.e., producing immediate answers without explicit reasoning, which often leads to shallow understanding. Recent advances in Chain-of-Thought (CoT) prompting have introduced step-by-step reasoning [70], but standard CoT traces are often too brief or abstract to capture fine-grained spatial logic. In contrast, Long Chain-of-Thought (LongCoT) prompting produces richer, more interpretable reasoning paths that better support comprehension [9, 43, 71]. Still, such prompting must go beyond simple depth estimation, as accurate spatial reasoning requires understanding occlusions, relative orientations, and positional ambiguity, all of which are difficult to capture without structured, fine-grained supervision. To address these challenges, we introduce SpatialReasoner-R1, novel VLM designed to perform spatial reasoning directly from 2D images. SpatialReasoner-R1 employs structured, interpretable LongCoT reasoning to systematically parse and solve spatial queries without relying on additional modalities or external sensor data. To optimize the training process for multi-step reasoning, we introduce new fine-grained Direct Preference Optimization (f DPO) method that applies differentiated learning updates tailored to two semantically distinct components, descriptive grounding and logical reasoning. Unlike traditional DPO, DPO introduces segment-specific preference granularity, allowing SpatialReasoner-R1 to adjust its optimization for each generation phase, emphasizing spatial localization during descriptive grounding and enhancing multi-step logical inferences during reasoning. To curate diverse high-quality spatial reasoning data for training, we propose Multi-Model Monte Carlo Tree Search (M3CTS) that generates high-quality LongCOT responses by leveraging collaborative exploration across multiple VLMs, and fine-grained spatial reward mechanism that evaluates candidate responses across three dimensions: descriptive accuracy, spatial grounding precision, and logical coherence, which are then used to construct positive and negative sample pairs for DPO and DPO training. Empirical results across several challenging spatial reasoning tasks demonstrate that SpatialReasoner-R1 achieves state-of-the-art performance, significantly outperforming existing VLMs and CoT-based methods, particularly on complex, multi-step spatial reasoning. Specifically, SpatialReasoner-R1 surpasses the best baseline by 9.8% in average accuracy on spatial understanding. Our DPO improves by 4.1% and 9.0% on average over standard DPO on spatial quality tasks, and spatial quantity tasks, respectively. The contributions of our work are as follows: (1) We introduce SpatialReasoner-R1, LongCoT spatial reasoning VLM that effectively generates interpretable, step-by-step explanations directly from 2D images. SpatialReasoner-R1 establishes new SoTA in spatial understanding, while maintaining robust performance on general vision-language benchmarks. (2) To enhance training stability and precision, we propose new fine-grained Direct Preference Optimization (f DPO) method that employs segment-specific learning updates tailored explicitly for descriptive grounding and logical reasoning. (3) To address the scarcity of high-quality spatial reasoning data, we introduce data generation pipeline that combines Multi-Model Monte Carlo Tree Search (M3CTS) with fine-grained spatial rewards, enabling the creation of diverse, logically consistent LongCoT trajectories for fine-grained preference training. 2 Figure 1: Method Overview including SpatialReasoner-R1 model architecture (left) and training pipeline (right). SpatialReasoner-R1 is VLM that takes as input text instruction, visual prompts, and an image, and generates LongCoT reasoning responses. To train SpatialReasoner-R1, we (1) generate reasoning paths using M3CTS, (2) construct fine-grained preference pairs via reward-based selection, and (3) train with fine-grained DPO (f DPO) to optimize descriptive and logical reasoning separately. 2. Related Work Recent advances in VLMs have significantly improved their capabilities in visual understanding tasks [31, 39, 40, 93]. However, these models still face notable challenges in accurately reasoning about spatial relationships and complex 3D configurations [12]. Previous work has explored various strategies including dedicated spatial fine-tuning [7, 12], zero-shot frameworks leveraging external geometric priors [44], and region-aware grounding techniques [23, 85, 89]. Preference optimization methods like DPO have also been adapted to enhance multimodal alignment, focusing on reducing hallucinations and improving visual grounding [68, 86]. Nevertheless, existing methods generally lack fine-grained, semantically-targeted optimization required for structured spatial reasoning. Our work addresses these limitations by introducing fine-grained preference optimization strategy and multi-level reward mechanisms, enabling verifiable and interpretable spatial reasoning. more comprehensive related work can be found in Appendix A. 3. Method 3.1. Spatial Reasoning from Images Spatial reasoning is core vision-language challenge, requiring models to understand visual layouts and perform logical inference over spatial relationships. We define spatial reasoning as multimodal understanding problem where the goal is to generate accurate reasoning paths based on visual and textual inputs. Formally, spatial reasoning instance can be represented as tuple = (I, Q, P) πθ where represents the input image containing the visual content, is the textual query specifying the spatial reasoning task, denotes the visual prompt tokens pointing to specific object or region in the image, and the textual response, providing the answer or step-by-step reasoning path. The primary objective of spatial reasoning model, denoted as πθ , is to map the multimodal input to logically sound and spatially grounded response R. Unlike typical direct-response VQA tasks, our model SpatialReasoner-R1 is designed to output LongCoT reasoning traces that decompose spatial reasoning into clear, verifiable steps. To train the model, we introduce fine-grained preference objective that optimizes descriptive and reasoning responses separately (3.2) and spatial reward mechanism that evaluates candidate reasoning paths based on spatial and logical understanding (3.3). Finally, to address the lack of LongCoT supervision for spatial reasoning, we propose multi-model collaborative tree search method that generates diverse, reward-aligned reasoning trajectories to enable preference-based training (3.4). An overview of the SpatialReasoner-R1 architecture and the proposed training framework is depicted in Figure 1. 3.2. Fine-grained Direct Preference Optimization (f DPO) We propose DPO as novel fine-grained off-policy preference learning algorithm to optimize LongCoT spatial reasoning. Traditional DPO methods apply single global trade-off parameter β uniformly across all reasoning steps [66, 72, 83], implicitly treating all response segments as equally learnable. However, this can lead to degenerate solutions, as the model may overfit to simpler descriptive responses while under-optimizing the more complex reasoning paths. This observation motivates the design of our fine-grained preference mechanism, which introduces segment-level preference granularity. To facilitate fine-grained preference optimization, we first segment each LongCoT response into its constituent description Rdesc and reasoning Rreason components, represented as = [Rdesc, Rreason]. We then quantify the preference signal for each segment by calculating the score difference between the corresponding segments derived from the positive Rp and negative responses Rl, yielding segment-wise preference differentials: Rdesc = score(Rp desc ) score(Rl desc ), Rreason = score(Rp reason ) score(Rl reason ), (1) where the differentials Rdesc and Rreason quantify the preference margin for description and reasoning segments based on the preference pair, and score() composite scores are introduced in Section 3.3. The design of DPO is guided by two key principles: Principle 1: Preference optimization strength should be dynamically balanced according to the intrinsic complexity and quality disparity between description and reasoning components. Our analysis of the fine-grained reward signals reveals that descriptive segments ( Rdesc) are easier to optimize while models struggle with reasoning segments ( Rreason) that are typically longer and require multi-hop logic. Thus, unified optimization parameter β may lead to reasoning under-optimization. To address this, DPO introduces separate, adaptively-tuned trade-off parameters, βdesc and βreason, which dynamically control the learning signals for each segment independently, to allow the model to prioritize deeper logical inference while maintaining visual and attribute accuracy. Principle 2: The choice of segment-specific optimization parameters (βdesc and βreason) should prioritize the component that exhibits larger preference differential, such that learning focuses on harder-to-learn segments. 4 Figure 2: Fine-Grained Spatial Rewards. Candidate reasoning paths are decomposed into three aspects, descriptive, spatial, and reasoning, scored separately; the higher value in each row is marked by and the lower by . Explanation of Scoring: Descriptive: Negative response omits the two bar-stools and uses generic modern kitchen wording, whereas the positive response lists every salient object; Spatial: Negative response wrongly claims the island is lower than the rear counter and ignores the 20cm offset revealed by the stool reference, whereas the positive response provides its estimate to the 75cm stool height plus that offset; Reasoning: Negative response uses an illogical half-height heuristic 90cm 45cm without intermediate computation, whereas the positive response explicitly adds reference height and gap (75cm+20cm=95cm). These per-category deficits yield lower composite reward, designating the upper response as negative sample. We further empirically observe that the preference score differential for descriptive components Rdesc is consistently smaller than Rreason. To account for this, DPO computes dynamic segment weights wdesc and wreason to adaptively adjust the learning signals for the description and reasoning components, respectively: ws = exp(λ Rdesc exp(λ Rs ) ) + exp(λ Rreason) , {desc, reason}, (2) where Rs is the preference score differential for segment (either description or reasoning), λ > 0 controls the sensitivity of weights, and {wdesc, wreason} reflect the relative importance of each segment. These weights are then mapped to adjustment factors centered around 1 and applied to the base optimization parameter β to yield segment-specific trade-off parameters βdesc and βreason for description and reasoning: βs = β (ws) = β[1 + α (2ws 1)], {desc, reason}. (3) where ws [0, 1] is the respective segment-specific weight (either description or reasoning), α is hyperparameter that controls the maximum scaling amplitude, and β is the base hyperparameter value. This design 5 implements dynamic learning strategy: segments with larger preference differentials (higher relative importance ws) receive higher effective βs, amplifying the learning signal and prioritizing those components. Conversely, smaller preference differentials yield lower βs, enabling finer-grained updates. This adaptive mechanism allows DPO to balance optimization based on segment-specific learning difficulty, promoting better alignment for complex reasoning steps while preserving descriptive accuracy. The final optimization objective for each segment is defined as follows: Fs(T , Rp, Rl) = log πθ πref (Rp (Rp x) x) log πθ πref (Rl (Rl x) x) , {desc, reason}. (4) is the models learned policy, Here, represents the multimodal input (image, text query, visual prompt), πθ and Fs measures the segment-specific preference margin in log-likelihood ratios relative to reference policy πref. Building upon the standard DPO objective, we define overall optimization objective for DPO as: Lf DPO(θ) = (T ,Rp,Rl )D [ ( log σ βdesc Fdesc(T , Rp, Rl) + βreason Freason(T , Rp, Rl) ] ) , (5) (l,i) where = {(x(i), desc, σ() is sigmoid activation function. (p,i) reason, (p,i) desc , (l,i) reason)}N i=1 denotes the dataset of segment-level preference pairs and 3.3. Fine-Grained Spatial Rewards To optimize spatial reasoning paths effectively, we introduce fine-grained reward mechanism that evaluates candidate reasoning paths across visual, spatial, and logical dimensions. Rewards capture alignment with image content, spatial relationships, and logical inference. Figure 2 illustrates the proposed Fine-Grained Spatial Rewards for DPO. Specifically, we define four scalar rewards; details are provided in Appendix B. Visual Consistency Reward (Rvc) evaluates the description Rdesc to ensure spatial grounding and fidelity. The reward verifies key aspects of the quality and alignment of the description with the visual scene, such as whether all referenced objects are present and identifiable, whether the stated properties (such as color, size, and shape) match the visual content, whether the description includes all necessary details prompted by the query, and whether it remains contextually appropriate and free from extraneous information. Depth-Guided Spatial Reward (Rsp) measures fine-grained spatial understanding by leveraging depth information. This reward is independently computed for the description Rdesc and reasoning Rreason components, with two adaptive weighting mechanisms: an uncertainty weight that adjusts the score for spatial expressions with qualifiers (e.g., \"approximately\",\"possibly\") to account for reduced confidence, and context-aware weight that emphasizes spatial relations directly relevant to the query. The final spatial rewards are computed as the uncertainty and context-aware weighted correctness scores across all spatial assertions in the corresponding description and reasoning components, validated against both the RGB image and its corresponding depth map. This ensures that more confident and contextually aligned relations have stronger influence on the reward. Logical Coherence Reward (Rlc) evaluates the reasoning block Rreason of the LongCoT path for structural integrity and logical correctness. This reward captures multi-hop inference and factual alignment by verifying that premises are consistent with the image, depth map, and preceding descriptions, reasoning steps maintain spatial and causal logic, the application of physical, spatial, and logical principles remains coherent throughout, and the conclusion is fully supported by the reasoning chain. 6 The preference differentials in DPO are computed from composite rewards that aggregate fine-grained evaluations across segments: score(Rdesc) = Rvc + Rsp,desc and score(Rreason) = Rlc + Rsp,reason. 3.4. Multi-Model MCTS (M3CTS) We introduce Multi-Model Monte Carlo Tree Search (M3CTS) framework for generating high-quality LongCoT data = {(T , Rp, Rl)} tailored to spatial reasoning. Inspired by DeepSeek-R1-Zero [15] and prior multimodal MCTS methods [84], M3CTS explores diverse reasoning trajectories across multiple large language models (LLMs) to effectively search for logical, spatially-consistent explanations that satisfy the query. Formally, the reasoning process is defined as sequence of reasoning states: = {s0, . . . , st, . . . , sT}, where st represents partial reasoning state, s0 is the initial state derived from , and sT is terminal state corresponding to fully reasoned path. M3CTS operates through four key stages: Expand, Simulate, Backpropagate, and Select. We describe each stage below. Expand. At each step t, M3CTS expands the current state st by generating diverse candidate reasoning states Sc using multiple VLMs {πk}K concurrently, i.e., k=1 Sc = ( πk k=1 st , Parent(st) ) , (6) where πk consistency, we enforce structured output format across all VLMs (Appendix C). is the k-th VLM, multimodal input, and Parent(st) ancestor reasoning states of st. To ensure Simulate. Each candidate sk,t Sc generated during expansion is evaluated based on three distinct criteria: (i) visual description accuracy against the original image, (ii) spatial correctness of inferred spatial relationships utilizing both original and depth-derived images, and (iii) logical coherence of the textual reasoning steps. The evaluation score R(sk,t) is computed as: R(sk,t) = 1 M m=1 [ visual(sk,t) + I(m) I(m) spatial(sk,t) + I(m) logical(sk,t) ] , where is the number of evaluation models, and each I(m) eval(sk,t) indicator function defined as: I(m) eval(sk,t) = {+1 (fully accurate), 0 (neutral), 1 (inaccurate)}. (7) (8) We preserve high-quality paths by pruning the candidate set according to the evaluation score, i.e., {sk,t R(sk,t) 0}. Appendix provides detailed descriptions of the evaluation. Backprop. To perform credit assignment, scores from the simulation phase are recursively propagated upwards through the search tree. The objective is to update the value estimates V(sk,t) and visit counts based on the performance of its children N(sk,t) for each parent node sk,t = Child(sk,t), = V(sk,t) N(sk,t)V(sk,t) + N(sk,t) + scS scS N(sc)R(sc) N(sc) , N(sk,t) N(sk,t) + scS N(sc). (9) Select. This phase is responsible for choosing the most promising candidate state for further exploration in the next iteration of tree expansion. We use the Upper Confidence Bound (UCB) strategy to select the next 7 k,t+1 state to traverse, based on updated values and visitation statistics. UCB ensures that high-value paths are prioritized, while also exploring less-visited nodes to discover new reasoning trajectories. The candidate selected maximizes the UCB objective, i.e., [ k,t+1 = arg max scS V(sc) + α log N(sk,t) 1 + N(sc) ] , (10) where V(sc) is the value estimate of the candidate state sc, N(sc) its visit count, and α > 0 is hyperparameter that balances exploration vs. exploitation. 4. Experiments 4.1. Experimental Setup We evaluate SpatialReasoner-R1 across diverse spatial reasoning and general vision-language established benchmarks to assess the models fine-grained spatial understanding and logical reasoning capabilities. Training setup, including hyperparameters and implementation specifics, is provided in Appendix E. Spatial Reasoning Benchmarks. Our primary benchmark is SpatialRGPT-Bench [12], comprising imagebased spatial reasoning questions and their corresponding ground truth answers. Detailed descriptions of benchmarks and evaluation protocols are provided in Appendix F. General Vision-Language Benchmarks. To validate the robustness of SpatialReasoner-R1 beyond purely spatial tasks, we evaluate on broader vision-language datasets such as MME, POPE, SEED-Bench, AI2D, SQA-test, MMMUv, MMStar, and HallusionBench [8, 22, 26, 29, 34, 35, 42, 90]. These datasets cover fundamental vision-language tasks such as object grounding, hierarchical scene parsing, multimodal understanding, and multi-turn reasoning in diverse multimodal contexts. Baselines. We benchmark SpatialReasoner-R1 against two categories of baseline models: General Large VLMs. This includes powerful, widely-accessible models such as Gemini 2.0 Flash [20], Llama 4 Maverick [47], Gemini 1.5 Pro [62], and ChatGPT-4o [49]. These are evaluated in zero-shot/fewshot settings as reference of standard VLM capabilities w/o task-specific fine-tuning. Specialized VLMs. This baseline set comprises models specifically developed, adapted, or fine-tuned for spatial understanding tasks, allowing us to assess our contributions relative to other specialized approaches. The models included are: SpatialBot-3B [6], SpaceThinker Qwen2.5VL-3B [4], InternVL2.5-78B [11], Sa2VA (4B, 8B) [87], and SpatialRGPT-8B [12]. SpatialReasoner-R1 VLMs. We also include SpatialReasoner-R1 4B and 8B variants with different training strategies such as SpatialReasoner-R1 SFT, SpatialReasoner-R1 DPO, trained with standard DPO, and SpatialReasoner-R1 DPO trained with the proposed fine-grained DPO method. 4.2. Experimental Results Spatial Reasoning. As shown in Table 1, SpatialReasoner-R1 models achieve substantial improvements over both general-purpose and spatial-specialized VLMs across all spatial tasks. Notably, SpatialReasoner-R1 DPO 8B sets new benchmark for average accuracy with 2.9% and 15.8% gains over SpatialRGPT-8B on spatial quality and quantity tasks, respectively. Our parameter-efficient SpatialReasoner-R1 DPO 4B outperforms larger models like InternVL2.5-78B, highlighting the effectiveness of our fine-tuning strategy. Finally, when compared to its predecessor DPO 8B, our optimized variant DPO 8B boosts average accuracy by 4.1% across quality tasks and by 9.0% in quantity tasks. 8 Table 1: Spatial Reasoning Success Rates () on SpatialRGPT-Bench. Classification (top) and numeric distance/direction (bottom). are SpatialReasonerare General Large VLMs, R1 variants. / indicates that the model refuses to provide response for that metric. are Customized VLMs, Gemini 2.0 Flash [20] Llama 4 Maverick [47] Gemini 1.5 Pro [62] ChatGPT-4o [49] SpatialBot-3B [6] SpaceThinker Qwen2.5VL-3B [4] InternVL2.5-78B [11] Sa2VA 4B [87] Sa2VA 8B [87] SpatialRGPT-8B [12] SpatialReasoner-R1 SFT 4B SpatialReasoner-R1 SFT 8B SpatialReasoner-R1 DPO 4B SpatialReasoner-R1 DPO 8B SpatialReasoner-R1 DPO 4B SpatialReasoner-R1 DPO 8B Below/ Above 58.33 54.17 85.83 87.50 52.50 89.16 94.16 22.50 50.00 99.17 79.16 81.66 91.66 94.16 95.83 98.33 Left/ Right 68.57 61.90 56.19 80. 62.86 63.81 94.28 25.71 39.04 100.00 78.09 81.90 91.42 93.33 93.33 98.10 Big/ Small 16.98 33.02 58.49 53.77 57.54 76.41 64.15 25.47 45.28 84.90 55.66 75.47 69.81 89.62 83.96 95. Tall/ Short 50.00 50.89 71.42 63.39 49.11 56.25 65.17 16.07 26.78 89.28 66.96 75.89 65.17 90.18 74.10 96.43 Wide/ Thin Behind/ Front 15.38 25.96 55.76 51.92 49.04 56.73 55.76 27.88 45.19 91.34 59.61 79.80 71.15 88.64 87.50 91.34 53.63 55.45 60.00 60.90 62.73 70.91 58.18 30.91 53.63 90.90 75.45 83.63 85.45 92.27 89.09 93. Direct Distance Horizontal Distance Vertical Distance Width Height Direction Gemini 2.0 Flash [20] Llama 4 Maverick [47] Gemini 1.5 Pro [62] ChatGPT-4o [49] SpatialBot-3B [6] SpaceThinker Qwen2.5VL-3B [4] InternVL2.5-78B [11] Sa2VA 4B [87] Sa2VA 8B [87] SpatialRGPT-8B [12] SpatialReasoner-R1 SFT 4B SpatialReasoner-R1 SFT 8B SpatialReasoner-R1 DPO 4B SpatialReasoner-R1 DPO 8B SpatialReasoner-R1 DPO 4B SpatialReasoner-R1 DPO 8B 9.45 24.48 14.18 / 6.00 24.32 27.70 13.51 14.18 45.90 22.29 28.43 47.97 62.83 60.13 70. 10.65 28.68 17.21 / 15.51 17.21 22.13 15.57 14.75 68.00 27.86 20.49 46.72 56.55 59.01 72.13 26.41 34.28 14.15 / 8.00 59.43 41.50 19.81 9.43 56.60 31.13 44.05 60.37 60.37 71.70 74. 10.52 35.71 19.54 / 10.52 23.27 29.32 13.53 14.28 48.90 25.56 33.59 45.11 70.45 65.41 80.45 30.82 44.61 36.09 / 18.75 23.62 34.58 12.03 19.54 61.70 33.80 51.63 55.63 68.42 57.89 74. 54.20 58.09 30.84 60.75 39.00 32.35 62.61 10.28 14.18 95.30 47.66 46.72 91.58 93.45 92.52 94.39 Qual Acc 44.29 47.18 65.14 66.67 55.56 69.25 72.29 24.65 43.37 92. 69.41 79.75 79.29 91.48 87.37 95.59 Quan Acc 22.43 36.72 21.90 / 15.62 28.97 35.25 14.02 14.55 61.42 30.71 37.12 56.61 68.22 66.76 77.30 General Vision-Language Understanding. Beyond achieving state-of-the-art performance in spatial reasoning tasks, our SpatialReasoner-R1 DPO 8B also demonstrates significant gains in general vision-language benchmarks compared to SpatialRGPT-8B, as presented in Table 2. Table 2: General Vision-Language Understanding Benchmark Results. Best performance in bold. Models MME POPE SEED-Bench AI2D SQAtest MMMUv MMStar HallusionBench SpatialRGPT-8b [12] SpatialReasoner-R1 DPO 8B 1667/348 1667/503 85.50 89.71 67.00 76.21 67.42 78.85 81.81 93. 41.40 48.11 43.98 55.43 40.80 51.10 9 Figure 3: Qualitative Examples of Spatial Reasoning Across Models. SpatialReasoner-R1 demonstrates coherent, multi-step logical chain that closely matches the ground truth, while other models like InternVL2.578B, Gemini 1.5 Pro, and SpatialRGPT-8B exhibit less precise or less interpretable reasoning paths. 4.3. Qualitative Examples 2 + 0.2 (gap) + 1.4 Figure 3 provides qualitative examples that demonstrate SpatialReasoner-R1s advanced capability for coherent, multi-step spatial reasoning. SpatialReasoner-R1 first estimates the fireplace and TV-stand widths at 1.2m and 1.4m, then computes 1.2 2 = 1.5m, value that nearly matches the reference while transparently tying each term to an observed feature. In contrast, InternVL2.5-78B adopts similar width guesses (1.2m, 1.5m) but assumes the distance between the fireplace and the TV stand seems to be about the width of one object. This assumption is inconsistent with what is shown in the image. Gemini 1.5 Pro aligns the fireplaces right edge with the TVs left edge, assigns both objects 1m width, and combines only half fireplace width (0.5m) with one quarter of the stand width (0.25m). These two estimates are not accurate and ignore the gap between the two regions. SpatialRGPT-8B yields more accurate estimate than Gemini 1.5 Pro and InternVL2.5-78B. However, since it is not designed as reasoning model, it cannot generate step-by-step reasoning traces, i.e., does not explicitly reveal the logical chain of spatial deductions or intermediate calculations leading to that estimate. These qualitative examples show that SpatialReasoner-R1 has more accurate spatial awareness. Additional qualitative examples can be found in Appendix G. 4.4. Ablations Table 3 illustrates the impact of varying the Alpha (α) parameter, which modulates the magnitude of segmentspecific learning adjustments during DPO optimization. When α is set too high, the model may overly focus on the reasoning part at the expense of the other, introducing instability and degraded performance, as observed when α reaches 40%. Conversely, if α is too low, both description and reasoning segments are optimized equally. moderate value of α = 30% allows the model to effectively amplify learning signals for 10 Table 3: Effect of Alpha (α). Metric 10% 20% 30% 40% Direct Dist. Horiz. Dist. Vert. Dist. Width Height Direction 53.38 52.46 65.09 51.88 56.39 91.58 56.76 55.74 67.92 57.89 57.14 92.23 60.13 59.01 71.75 65.41 57.89 92.52 58.11 56.55 69.81 63.16 58.64 94.39 Table 4: Effect of Lambda (λ) at α=30%. Metric λ=0.2 λ=0.4 λ=0.6 λ=0.8 Direct Dist. Horiz. Dist. Vert. Dist. Width Height Direction 54.05 53.27 65.09 53.38 56.39 91. 57.38 57.37 68.86 60.15 57.14 91.58 60.13 59.01 71.75 65.41 57.89 92.53 59.45 58.19 69.81 54.67 57.89 93.45 fine-grained spatial distinctions, leading to substantial improvements across all spatial metrics. Furthermore, Table 4 presents the impact of varying the Lambda (λ) parameter, which modulates the sensitivity of segmentspecific weights to preference differentials, controlling how responsively the model shifts learning focus based on the observed preference margins. As λ increases, the model becomes more sensitive to segment-specific preference differences, leading to noticeable changes in performance across spatial metrics. We observe that while moderate value of λ = 0.6 achieves the best overall results, setting λ too high can introduce slight performance degradation in some spatial metrics, likely due to overly aggressive re-weighting. 5. Conclusion In this work, we introduce SpatialReasoner-R1, novel VLM with state-of-the-art spatial reasoning capabilities, trained with proposed fine-grained DPO (f DPO) method that decomposes LongCoT paths into description and reasoning components, allowing for targeted preference-based learning and enhanced logical reasoning. DPO is guided by set of comprehensive rewards that evaluate reasoning paths across visual consistency, spatial alignment, logical coherence, and depth-based verification. Additionally, we propose Multi-Model Monte Carlo Tree Search (M3CTS) strategy that leverages multiple LLMs to generate highquality, diverse LongCoT data. Our comprehensive evaluations demonstrate SpatialReasoner-R1 achieves state-of-the-art performance, outperforming significantly larger models. Moving forward, we plan to evaluate DPO on additional VLM tasks such as GUI navigation and reasoning segmentation."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015. [3] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. 2024. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind 11 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In International Conference on Robotics and Automation (ICRA), 2024. [7] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In Advances in Neural Information Processing Systems (NeurIPS), 2024. [9] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [10] Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [12] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, and Tat-Seng Chua. Fine-grained verifiers: Preference modeling as next-token prediction in vision-language alignment. In International Conference on Learning Representations (ICLR), 2024. [14] Dai, Li, Li, AMH Tiong, Zhao, Wang, Li, Fung, and Hoi. Instructblip: towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [15] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe 12 Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv 2501.12948, 2025. [16] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [17] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [18] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In International Conference on Machine Learning (ICML), 2024. [19] Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Xirong Li. Mitigating hallucination in multimodal large language model via hallucination-targeted direct preference optimization. In Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), 2024. [20] Google Deepmind. Gemini 2.0 is now available to everyone. https://blog.google/technology/ google-deepmind/gemini-model-updates-february-2025/, 2024. [21] Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. [22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [23] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 13 [25] Amazon Artificial General Intelligence. The amazon nova family of models: Technical report and model card. 2024. [26] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision (ECCV), 2016. [27] Yangzhe Kong, Daeun Song, Jing Liang, Dinesh Manocha, Ziyu Yao, and Xuesu Xiao. Autospatial: Visual-language reasoning for social robot navigation through efficient spatial reasoning learning. arXiv preprint arXiv:2503.07557, 2025. [28] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [30] Fangjun Li, David Hogg, and Anthony Cohn. Reframing spatial reasoning evaluation in language models: real-world simulation benchmark for qualitative reasoning. In International Joint Conference on Artificial Intelligence (IJCAI), 2024. [31] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. In International Conference on Learning Representations (ICLR), 2025. [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), 2023. [33] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [35] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In International Conference on Computer, Artificial Intelligence and Control Engineering, 2024. [36] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. [38] Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, et al. Tis-dpo: Token-level importance sampling for direct preference optimization with estimated weights. In International Conference on Learning Representations (ICLR), 2024. 14 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [41] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. arXiv preprint arXiv:2501.10074, 2025. [42] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [43] Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Ada-r1: Hybrid-cot via bi-level adaptive reasoning optimization. arXiv preprint arXiv:2504.21659, 2025. [44] Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [45] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [46] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv preprint arXiv:2504.20024, 2025. [47] Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https: //ai.meta.com/blog/llama-4-multimodal-intelligence/, April 2025. [48] Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Côme Fiegel, et al. Nash learning from human feedback. In International Conference on Machine Learning (ICML), 2023. [49] OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2025. [50] Zhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse. arXiv preprint arXiv:2503.18470, 2025. [51] Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [52] Sungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. Ensembling large language models with process reward-guided tree search for better complex reasoning. In Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), 2025. [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [54] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [55] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [56] Ruichen Shao, Bei Li, Gangao Liu, Yang Chen, Xiang Zhou, Jingang Wang, Xunliang Cai, and Peng Li. Earlier tokens contribute more: Learning direct preference optimization from temporal decay perspective. In International Conference on Learning Representations (ICLR), 2025. [57] Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo: Advancing multilingual reasoning through multilingual-alignment-as-preference optimization. In Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [58] Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. Direct multi-turn preference optimization for language agents. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [59] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Explorationbased trajectory optimization of llm agents. In Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [60] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023. [61] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [62] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [64] Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, et al. Longwriter-v: Enabling ultra-long and high-fidelity generation in vision-language models. arXiv preprint arXiv:2502.14834, 2025. [65] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [66] Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. 16 [67] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities. In International Conference on Learning Representations (ICLR), 2025. [68] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [69] Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong Yu. Towards self-improvement of llms via mcts: Leveraging stepwise knowledge with curriculum preference learning. arXiv preprint arXiv:2410.06508, 2024. [70] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [71] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [72] Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. beta-dpo: Direct preference optimization with dynamic beta. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [73] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [74] Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Multimodal large language models make text-to-image generative models align better. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [75] Yuxi Xie, Guanzhen Li, Xiao Xu, and Min-Yen Kan. V-dpo: Mitigating hallucination in large vision language models via vision-guided direct preference optimization. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [76] Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, and Zhengzhong Tu. Re-align: Aligning vision language models via retrieval-augmented direct preference optimization. arXiv preprint arXiv:2502.13146, 2025. [77] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and In IEEE/CVF Conference on Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. Computer Vision and Pattern Recognition (CVPR), 2025. [78] Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multi-turn iterative preference learning. In International Conference on Learning Representations (ICLR), 2025. [79] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 17 [80] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 89418951, 2024. [81] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [82] Sen Yang, Yafu Li, Wai Lam, and Yu Cheng. Multi-llm collaborative search for complex problem solving. arXiv preprint arXiv:2502.18873, 2025. [83] Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucinations in large vision-language models via dpo: On-policy data hold the key. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [84] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [85] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In International Conference on Learning Representations (ICLR), 2023. [86] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, HaiTao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [87] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [88] Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [89] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [90] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [91] Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [92] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization. In International Conference on Machine Learning (ICML), 2024. [93] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mmllms: Recent advances in multimodal large language models. In Annual Meeting of the Association for Computational Linguistics (ACL), 2024. 18 [94] Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [95] Qi Zhao, Haotian Fu, Chen Sun, and George Konidaris. Epo: Hierarchical llm agents with environment preference optimization. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [96] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. [97] Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment, 2024. [98] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 19 A. Related Work A.1. Vision Language Models and Spatial Reasoning Recent advances in VLMs have significantly enhanced the ability of multimodal models to understand and generate descriptive text grounded in visual contexts [31, 39, 40, 93]. Models such as Flamingo [1], BLIP-2 [32], and Qwen-VL [39] use high-capacity vision encoders [53] paired with LLMs [5, 63] to achieve state-of-the-art performance in various multimodal tasks, such as visual question answering, image captioning, and instruction following [2, 14, 37, 64, 74, 98]. Current trends involve scaling models to improve general understanding [11, 24, 61] and using large-scale instruction tuning datasets [40, 55, 91]. Both proprietary [20, 25, 49] and open-source VLMs [11, 16, 87] have shown impressive results. However, while VLMs show promise in general visual understanding, accurately perceiving and reasoning about spatial arrangements, especially metric relationships and complex 3D configurations, remains significant challenge [12]. Recent efforts to enhance spatial understanding have explored various strategies. Some approaches involve fine-tuning VLMs on dedicated spatial VQA datasets [7, 12, 27, 41, 73], while others propose zero-shot frameworks that interact with external 3D foundation models to incorporate geometric priors [44]. Concurrently, region-aware models have been developed to improve grounding and enable finer-grained spatial queries [23, 85, 89]. These advancements demonstrate growing capabilities in complex scenarios, including video understanding [79] and 3D generation [46, 50]. To measure progress, specialized benchmarks such as Q-Spatial Bench [36], SpatialRGPT-Bench [12], VSI-Bench [79], 3DSRBench [45], and others [30] have been established to evaluate these quantitative spatial skills. Collectively, however, these studies highlight that current models still lack the capability to perform complex, multi-step spatial reasoning. SpatialReasoner-R1 addresses this gap by introducing fine-grained preference optimization strategy and multi-level reward mechanisms to generate verifiable, interpretable spatial reasoning responses. A.2. Aligning VLMs using Preference Optimization Preference-based learning methods, particularly DPO [54], have become standard techniques for aligning models with human intentions. These methods bypass the need for explicit reward model training and have often demonstrated strong performance compared to earlier Reinforcement Learning with Human Feedback (RLHF) approaches [3, 18, 48, 96]. In the multimodal domain, DPO and its variants have been adapted to address specific challenges such as reducing hallucinations and improving visual grounding [68, 76, 86]. The adaptability of DPO is further highlighted by its recent application in aligning generative models beyond language, such as text-to-image diffusion models [21, 33, 65, 80, 88]. Adaptation methods often involve constructing preference pairs based on human corrections, AI feedback, or contrasting inputs to guide the model towards desired behaviors [10, 13, 17, 19, 60, 66, 72, 75, 77, 83]. Standard DPO methods treat the CoT reasoning process as monolithic structure, applying uniform optimization across response segments. To mitigate this issue, preference granularity in DPO has been explored at token-level [38, 56, 92, 95, 97], step-level [28, 94], sentence-level [51, 54, 57], or turn-level [58, 59, 78]. These granularities have proven effective for certain domains but do not consider the semantic role of different segments in LongCoT, where descriptive grounding and logical reasoning require distinct optimization focus. In contrast, our proposed DPO introduces functional-level preference granularity that decomposes LongCoT paths into two distinct semantic roles: description and reasoning, and applies segment-specific adaptive preference parameters for more targeted structured reasoning learning. 20 A.3. Multi-LLM Guided Reasoning Recent work has explored leveraging multiple LLMs to collaboratively solve complex reasoning tasks, often integrated with Monte Carlo Tree Search (MCTS). Methods such as MoA [67], MoSA [82], AlphaLLM-CPL [69], and LE-MCTS [52] enhance multi-agent text-based reasoning using ensemble methods and stepwise search. CoMCTS (Mulberry) [84] extends multi-LLM MCTS to multimodal reasoning. However, CoMCTS primarily targets collaborative reflection and error correction, lacking fine-grained grounding and spatial alignment in its reasoning paths. In contrast, our method, M3CTS, addresses the challenge of spatial reasoning in Vision-Language Models (VLMs), introducing fine-grained preference learning and multi-level spatial rewards that allow for coherent, visually-grounded reasoning paths across multimodal data. B. Fine-Grained Spatial Reward Details This appendix details the calculation and rationale behind the hyperparameter choices for the Fine-Grained Spatial Rewards introduced in Section 3.3. The prompt template for estimating rewards is shown in Figure 8. B.1. Visual Consistency Reward (Rvc) The Visual Consistency Reward quantifies alignment between the generated description and the visual scene across four continuous criteria: Existence, Attribute accuracy, Completeness, and Appropriateness. Each component yields score in the range [0.0, 1.0], where its continuous range enables fine-grained assessment, permitting fractional scores when descriptions partially satisfy evaluation criteria. Scores near 0.0 indicate misalignment, while scores near 1.0 denote perfect alignment. Intermediate values reflect varying degrees of partial correctness or uncertainty. The total reward, Rvc [0, 4.0], distinguishes varying degrees of alignment across responses. B.2. Depth-Guided Spatial Reward (Rsp) We introduce depth-guided reward to evaluate the spatial accuracy of model outputs using groundtruth depth maps. The reward is computed independently for the description Rdesc and reasoning Rreason components, yielding two sub-scores: Rsp,desc and Rsp,reason, each ranging from 0 to 4. These scores capture the alignment of spatial expressions with geometric cues in the image. The final spatial reward is given by Rsp = Rsp,desc + Rsp,reason. Uncertainty Weight (Wu). Spatial expressions in model outputs often include uncertain qualifiers. Wu ranges from 0.8 to 1.0, with 1.0 indicating complete certainty in spatial assertions, and the lower bound of 0.8 representing cautious but plausible uncertainty. Setting the lower bound at 0.8 balances cautious language (e.g., approximately, possibly) without overly penalizing reasonable uncertainty. Lower values (below 0.8) would overly penalize reasonable, conservative predictions and discourage the model from producing cautious but informative reasoning. Context-aware Weight (Wc). The context-aware weight Wc [0.8, 1.0] reflects the relevance of spatial statements to the question. Explicitly asked spatial relationships are assigned Wc = 1.0, while auxiliary or indirect spatial references are assigned Wc = 0.8. This distinction prioritizes primary spatial relations explicitly required by the query, ensuring the model emphasizes essential spatial assertions more significantly. Scores below 0.8 would disproportionately underemphasize auxiliary information, degrading the models ability to handle broader contextually relevant details. 21 Given response, we extract all spatial relationship statements from the description and reasoning response sections. Each statement is then evaluated using GPT-4o by comparing the original image and its corresponding depth image, which is generated using Depth Anything [81], to obtain correctness score ri [0, 1]. (i) (i) and Every statement is also assigned an associated . The spatial reward scores are computed as Rsp,desc = 1 i=1 (i) (i) W ri, Rsp,reason = 1 i= (i) (i) ri, (11) where and denote the number of spatial statements in the description and reasoning components, respectively, and ri represents the correctness of each spatial relationship, validated against both the RGB image and its corresponding depth map. ) B.3. Logical Coherence Reward (Rlc This reward quantifies the logical robustness of response by aggregating four components: Factual Consistency, Logical Coherence, Correct Rule Application, and Conclusion Validity. Each component is scored in the range [0.0, 1.0], with fractional values capturing partial correctness, e.g., from minor gaps in logical sequences to partial inaccuracies in applying physical, spatial, or logical rules. Scores of 0.0 and 1.0 indicate complete logical coherence failure or perfect logical chains, respectively. The final reward, Rlc [0, 4.0], reflects the overall logical quality of the reasoning. C. Structured Output Format Specification for M3CTS To enable reliable parsing and downstream analysis, M3CTS requires reasoning paths from LLMs to follow standardized structured format. This format uses Markdown-style headings to clearly segment key components of the reasoning trace. Each section begins with line prefixed by ###, followed by descriptive heading. The defined sections are: ### Description: Details the input context, such as the visual scene or scenario description. ### Rationale: Summarizes the overall reasoning strategy or justification. ### Lets think step by step: An optional phrase before the detailed step-by-step breakdown. ### Step (e.g., ### Step 1, ### Step 2, ...): Enumerates the sequential steps involved in the reasoning procedure. Multiple steps are typically present. ### In Conclusion: States the final derived conclusion of the reasoning process. Figure 4 shows the reasoning tree example produced by M3CTS. D. Node Evaluation Protocol for M3CTS within the M3CTS framework, To ensure the semantic and visual quality of each candidate reasoning step sk,t we employ structured multi-criteria evaluation. Individual steps that form reasoning path are segmented by ###. Each candidate sk,t Sc is independently evaluated by two multimodal models, Gemini 1.5 Pro and Qwen2.5VL-72B, along key distinct dimensions: Visual Description Accuracy: Assesses whether the entities, attributes, and contextual cues described correctly reflect the visual content of the input image. This includes references to objects, colors, in sk,t spatial layouts, and contextual cues. 22 Figure 4: Example Reasoning Tree from the M3CTS Data Generation Pipeline. Diverse candidate reasoning paths are sampled from multiple models. Each path follows structured LongCoT format with markdown-style section headers that decompose the answer into interpretable reasoning stages. Spatial Consistency: Evaluates whether the spatial relations expressed in sk,t (e.g., above, to the left of, behind) are consistent with both the RGB image and depth map generated via the Depth Anything model [81]. Errors such as inversion of relations (e.g., stating behind instead of in front) are penalized. Logical Reasoning Coherence: For steps within the think step-by-step chain-of-thought reasoning block, this component checks whether the logical flow of inferences is coherent and justified. This includes identifying unsupported jumps in logic or contradictions. Each criterion I(m) eval(sk,t) is rated as follows: I(m) eval(sk,t) = +1, if the content is entirely accurate according to model m; 0, 1, if the content is ambiguous or partially accurate; if there is any clear inaccuracy. We preserve high-quality paths by pruning the candidate set, i.e., we retain any node sk,t score across all evaluators and criteria is non-negative: empirically to balance filtering out incorrect steps while maintaining adequate reasoning diversity. whose aggregated = {sk,t R(sk,t) 0}. This threshold is chosen E. Training Details E.1. Implementation Details We train the 8B-parameter model in two stages on two NVIDIA H100 GPUs, each stage taking approximately 2.5 days. For supervised fine-tuning, we employ AdamW optimizer with learning rate of 4 105, weight 23 decay of 0.05, and 5% linear warm-up schedule, using batch size of 2 per device with gradient accumulation over 4 steps. For Direct Preference Optimization, we similarly use AdamW with learning rate of 1 107, weight decay of 0.05, and 5% warm-up, training with batch size of 1 per device. E.2. Training Data For SFT, we convert samples from the Open Spatialdataset [12] to reasoning chains using the M3CTS pipeline. While the original Open Spatial dataset provides single-sentence answers, we transform 400K samples, grounded in distinct images, into structured LongCoT reasoning chains, where examples are used to teach the model to generate high-quality, step-by-step spatial reasoning responses. For Direct Preference Optimization (DPO) training, the goal is to train the model to distinguish high-quality spatial reasoning from suboptimal or subtly flawed alternatives. To this end, we utilize our Open Spatial Reasoning dataset, described below, that consists of spatial reasoning preference pairs. An additional set of 100K challenging negative pairs is meticulously crafted by perturbing only the conclusion keywords of high-quality positive samples. Each original response represents coherent and accurate reasoning path with factually correct outcome. To create the corresponding negative sample, we retain the exact description and reasoning segments and alter only the final conclusion value. This yields tightly controlled preference pairs that isolate correctness at the conclusion level. For example, positive sample may assert The distance between region1 and region2 is 11 meters., while its negative perturbed counterpart is The distance between region1 and region2 is 10 meters. Our method adopts data-centric strategy that emphasizes high-quality supervision and reasoning diversity. Instead of collecting large volumes of weakly aligned or noisy data, we curate training examples using the M3CTS sampling strategy guided by structured reward evaluations. By applying reward-based filtering, we reduce noise and enforce consistent output structure. In parallel, using multiple LLMs during generation introduces variation in reasoning styles, improving coverage of diverse spatial patterns and edge cases. The effectiveness of this approach is evident in the substantial performance gains of DPO-trained models over their simpler SFT counterparts  (Table 1)  and the reasoning improvements and diversity depicted in Figure 4. E.3. Open Spatial Reasoning Dataset We curate the Open Spatial Reasoning dataset, collection of 400K Vision Question Answering (VQA) preference pairs (yp, yl), to support training of preference-based spatial reasoning models. This dataset is derived from the Open Spatial dataset [12], which provides image-based spatial questions paired with ground-truth answers and offers 10 question variations per image-grounding scenario. To construct each preference pair, we randomly sample question instance from the source dataset, and generate diverse pool of eight candidate answers using four distinct sources: our M3CTS pipeline, Gemini 1.5 Pro, GPT-4o, and our SpatialReasoner-R1 Supervised Fine-Tuned (SFT) model, with each method contributing two response variants. All eight candidate responses are independently evaluated by our fine-grained spatial reward mechanism (Appendix B). The highest-scoring response is selected as the preferred answer (yp), while the response with the lowest score is designated as the less-preferred (yl), ensuring that each preference pair is anchored in meaningful fine-grained spatial reasoning quality. Figure 5 shows dataset examples. Figure 5: Example DPO Pairs of our Open Spatial Reasoning Dataset, constructed from M3CTSgenerated reasoning trajectories. Each pair consists of preferred and rejected response to the same spatial question. The examples highlight differences in descriptive accuracy, spatial alignment, and reasoning coherence, which guide preference optimization during training. F. Evaluation Details We evaluate on SpatialRGPT-Bench [12], benchmark specifically designed to assess the 3D spatial reasoning abilities of VLMs, featuring 657 qualitative and 749 quantitative VQA pairs, covering 88 object classes across diverse environments. We employ the same GPT-4 evaluation proposed in SpatialRGPT-Bench [12] for evaluating the free-form responses generated by the models. For qualitative questions, GPT-4o assesses the semantic alignment between the models response and the ground-truth answer, assigning binary score (1 for correct, 0 for incorrect). For quantitative questions (e.g., distance, size), GPT-4o first extracts numerical values from both the prediction and the ground truth, standardizing them to common unit (meters). We then compute accuracy (e.g., success rate defined as predictions within 25% of the ground truth). 25 Figure 6: Qualitative Examples of Spatial Reasoning Across Models. SpatialReasoner-R1 demonstrates coherent, step-by-step spatial reasoning that closely aligns with ground truth estimates. In contrast, baseline models produce less precise or partially incorrect reasoning steps, often neglecting key visual cues or misestimating spatial references. We also evaluate on several general vision-language benchmarks to provide comprehensive assessment of SpatialReasoner-R1s capabilities. Specifically, we use MME [34] to assess multimodal models on perception and cognition tasks across wide range of domains. POPE [34] is employed to evaluate object hallucination in testing the ability of VLMs to ground responses to visual content, while SEED-Bench [29] offers multi-dimensional evaluation, covering aspects from image understanding to complex reasoning across various modalities and tasks. We further utilize AI2D [26], benchmark focusing on diagram understanding and reasoning, which requires parsing visual elements and their relationships within schematic representations. SQA [42] is used to measure the models ability to answer science-related questions based on visual context, often requiring domain-specific knowledge and reasoning. MMMU [90] evaluates massive multi-disciplinary multimodal understanding and reasoning across diverse college-level subjects. Moreover, MMStar [8] provides challenging benchmark with meticulously curated, multimodal instances that require advanced reasoning, low hallucination, and resistance to leading questions. Finally. HallusionBench [22] is specifically designed to quantitatively measure and analyze the hallucination phenomena in VLMs, probing for both object-level and attribute-level inconsistencies. G. Qualitative Experiment Examples In this section, we provide additional qualitative experiment examples. Figure 6 shows question that requires estimation of the horizontal distance between truck and pedestrian. SpatialReasoner-R1 demonstrates clear advantage by decomposing the scene into semantically meaningful components, explicitly reasoning over the widths of multiple traffic lanes, the roadside, and the sidewalk. This results in an estimated distance 26 Figure 7: Qualitative Examples of Spatial Reasoning Across Models. SpatialReasoner-R1 correctly recognizes Region2 as computer tower and compares it clearly with the nearby monitor, reaching an accurate conclusion. InternVL2.5-78B relies on general object size knowledge but provides incorrect reasoning, Gemini1.5Pro fails to identify Region2 clearly and draws incorrect visual conclusions, while SpatialRGPT-8B directly provides wrong answer. that closely matches the ground truth and provides full transparency into the models stepwise deductions. In contrast, InternVL2.5-78B bases its answer primarily on the width of the trucks and the space between them, omitting the crucial step of accounting for the distance from the pedestrian to the roadway, which leads to significant underestimation. Gemini1.5Pro correctly recognizes that the separation includes the truck, traffic lane, and sidewalk, but substantially underestimates the width of the sidewalk, causing notable error in its final answer. Meanwhile, SpatialRGPT-8B provides more accurate estimate than Gemini or InternVL2.5-78B, but still has gap compared to the ground truth. Most importantly, it cannot generate step-by-step reasoning traces. Figure 7 presents another illustrative example evaluating spatial reasoning capabilities of various models, specifically focusing on size comparison between two highlighted image regions. The question is whether Region 1 (a computer monitor) appears smaller than Region 2 (a computer tower). SpatialReasoner-R1 accurately identifies Region 2 as computer tower and explicitly reasons by comparing Region 1 with the closest computer tower positioned adjacent to the monitor. This systematic visual grounding and clear comparative reasoning enable SpatialReasoner-R1 to correctly conclude that the monitor is indeed smaller than the tower. By contrast, the baseline models exhibit varying degrees of errors and reasoning inadequacies. InternVL2.5-78B relies significantly on prior general knowledge about typical object dimensions and incorrectly concludes the monitor is not smaller, without effectively validating this against the visual evidence provided. textbfGemini1.5Pro fails entirely to recognize what object Region 2 represents, causing it to inaccurately rely purely on the objects visual proximity and perspective, leading to an incorrect conclusion. 27 Lastly, the SpatialRGPT-8B model directly presents an incorrect judgment (Region1 is not smaller) without providing any interpretable reasoning steps or visual grounding. H. Broader Impacts This work aims to improve the spatial reasoning capabilities of vision-language models through fine-grained preference optimization. Accurate spatial understanding is critical for downstream applications such as robotics, autonomous navigation, assistive technologies, and visual analytics. By introducing more interpretable and structured reasoning mechanisms, our method can contribute to building AI systems that are safer, more transparent, and more aligned with human expectations in spatially grounded tasks. However, as with other vision-language systems, potential risks remain. If deployed in safety-critical domains, incorrect spatial inferences, especially in edge cases, could lead to unintended consequences. Additionally, reward scoring and generation rely on foundation models that may encode hidden biases, which can propagate through the training pipeline. Although we attempt to mitigate these risks via multi-source sampling and structured evaluation, future work should explore robustness to distribution shifts, adversarial spatial prompts, and the inclusion of human-in-the-loop verification for high-stakes use cases. I. Limitations While our work demonstrates strong improvements in spatial reasoning, limitation of our approach is its reliance on explicit region representations provided as input to disambiguate object references within the spatial queries. Enabling the model to implicitly ground entities solely based on natural language descriptions remains an avenue for future investigation, which would enhance the models flexibility in real-world scenarios. Future work could focus on integrating implicit linguistic context understanding to alleviate this constraint. Finally, our focus is limited to 2D spatial reasoning; extending this framework to 3D or embodied contexts would require structural adjustments left for future work. 28 System Prompt for LongCOT Reward Evaluation The following is spatial reasoning task, and this is the question: question and the ground truth is: ground_truth. The response is divided into different sections. There are 4 dimensions to evaluate, and will provide you with the corresponding image and text for reference. You will need to evaluate the response based on the following criteria: The first task: Descriptive Scoring (Total 04.0 points) Evaluate the \"Description\" section based on: Existence: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Does the description correctly identify objects that actually appear in the image? Attribute Accuracy: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Are the objects attributes (color, shape, size, etc.) described accurately? Completeness: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Does the description include all key objects and necessary details relevant to the question? Appropriateness: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Does the description focus on the core aspects of the question? Clearly state the score for each sub-category and sum them to obtain the final descriptive score. You need to give the score with the following format: {\"task1_score\": your score} The second task: Depth-Guided Spatial Relationship Scoring Description (Total 04.0 points) Evaluate all spatial statements within the \"Description\" section using the provided depth image as ground truth. For each spatial claim in the description: Correctness score: Assign 1 if the spatial claim is correct based on the depth image, and 0 if not correct. Uncertainty score: For claims expressed with uncertainty (using words like \"approximately\", \"roughly\", \"possibly\"), assign score from 0.8 to 1.0, where 1.0 means the statement is expressed with high certainty. Relationship score: Assign weight from 0.8 to 1.0 based on whether the relationship is explicitly emphasized by the question (1.0) or is extra/irrelevant information (0.8). Provide detailed breakdown for each spatial claim. Calculate the final score as: (Sum of (Correctness score Uncertainty score Relationship score)) / (Number of claims), then scale to 4.0. You need to give the score with the following format: {\"task2_claim_score\": [Correctness score, Uncertainty score, Relationship score]} The third task: Depth-Guided Spatial Relationship Scoring Reasoning (Total 04.0 points) Apply the same evaluation method as in Task 2 to the spatial statements within the \"Reasoning\" section. For each spatial claim in the reasoning: Correctness score: Assign 1 if the spatial claim is correct based on the depth image, and 0 if not correct. Uncertainty score: For claims expressed with uncertainty, assign score from 0.8 to 1.0, where 1.0 means the statement is expressed with high certainty. Relationship score: Assign weight from 0.8 to 1.0 based on whether the relationship is explicitly emphasized by the question (1.0) or is extra/irrelevant information (0.8). Provide detailed breakdown for each spatial claim. Calculate the final score as: (Sum of (Correctness score Uncertainty score Relationship score)) / (Number of claims), then scale to 4.0. You need to give the score with the following format: {\"task3_claim_score\": [Correctness score, Uncertainty score, Relationship score]} The fourth task: Reasoning Scoring (Total 04.0 points) Evaluate the \"Reasoning\" section (the chain-of-thought) based on: Factual Consistency: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Are the claims consistent with the image, depth image, and the earlier description? Logical Coherence: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Do the reasoning steps flow logically without gaps or contradictions? Correct Application of Rules: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and means mostly confidently incorrect. Are physical, spatial, and logical rules applied correctly? Conclusion Validity: Assign scores from 0 to 1.0, where 1.0 means mostly confidently correct and 0 means mostly confidently incorrect. Does the reasoning properly support the final answer? Clearly state the score for each sub-category and sum them to obtain the final reasoning score. You need to give the score with the following format: {\"task4_score\": your score} Figure 8: System Prompt for Evaluating LongCoT Spatial Reasoning w.r.t. descriptive accuracy, spatial alignment, and logical consistency of reasoning steps."
        }
    ],
    "affiliations": [
        "Google",
        "Shanghai Jiao Tong University",
        "University of Illinois Urbana-Champaign",
        "University of Pennsylvania"
    ]
}