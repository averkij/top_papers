{
    "paper_title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
    "authors": [
        "Ahmed Lekssays",
        "Utsav Shukla",
        "Husrev Taha Sencar",
        "Md Rizwan Parvez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains. We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques. Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 8 9 1 1 . 5 0 5 2 : r TECHNIQUERAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text Ahmed Lekssays1, Utsav Shukla2, Husrev Taha Sencar1, Md Rizwan Parvez1 1Qatar Computing Research Institute, Doha, Qatar, 2Independent Researcher alekssays@hbku.edu.qa, utsavshuk@gmail.com, hsencar@hbku.edu.qa, mparvez@hbku.edu.qa"
        },
        {
            "title": "Abstract",
            "content": "Example of text to (sub-)techniques annotated pairs Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizationssuch as custom hard-negative mining and denoisingresources rarely available in specialized domains. We propose TECHNIQUERAG, domain-specific retrievalaugmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal texttechnique pairs. First, our approach mitigates data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing resource-intensive retrieval training. Second, although conventional RAG mitigates hallucination by coupling retrieval and generation, its dependence on generic retrievers often introduces noisy candidates, thereby limiting domain-specific precision. To address, we enhance the retrieval quality and domain specificity through zero-shot LLM re-ranking that explicitly aligns retrieved candidates with adversarial techniques. Experiments on multiple security benchmarks demonstrate that TECHNIQUERAG achieves state-of-the-art performances without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights."
        },
        {
            "title": "Introduction",
            "content": "Uncovering new adversarial behaviors is critical for strengthening defenses against rapidly evolving cyber threats. These behaviors, defined by the tools, techniques, and procedures used by attackers, reveal how adversaries plan and execute attacks and impact systems and data. By identifying and analyzing the traces or artifacts left behind, security analysts can map low-level actions to higherlevel concepts, such as tactics (i.e., strategic obscripts Monero miner are downloaded from TeamTNTs server and piped to bash using SSH session as the root user with private key from /tmp/TeamTNT. 1. T1098.004: SSH Authorized Keys 2. T1195: Supply Chain Compromise 3. T1059.004: Unix Shell 4. T1021.004: Remote Services: SSH 5. T1552.004: Private Keys Figure 1: Example of MITRE ATT&CK techniques and sub-techniques highlighted in text with corresponding colored (implicit) indicators. IDs with \".\" denote subtechniques (e.g., T1098.004). jectives like \"lateral movement\"), techniques or fine-grained sub-techniques (i.e., tactical methods like \"Debugger Evasion\"), and procedures (i.e., implementation details like \"using PowerShell for credential dumping\"). The findings are shared with other experts through public channels and threat intelligence services via security reports and detailed descriptions, to strengthen defenses, anticipate possible threats, and improve incident response. The MITRE ATT&CK framework has established itself as the industry standard for categorizing and mapping adversarial behaviors (Corporation, 2022). This framework provides comprehensive knowledge base of adversarial tactics, techniques, and procedures (TTPs), built from realworld threat intelligence and incident response data. It uses hierarchical matrix structure to systematically organize and classify adversary behaviors. The broad adoption of the ATT&CK framework presents significant operational challenge: security analysts must manually map ambiguous threat descriptions from incident reports (such as the example shown in Fig 1) to standardized ATT&CK (sub)-techniquesa time-intensive process that demands expert knowledge. This manual task has motivated research into automated adversarial technique identification, which aims to systematically label text segments with their corresponding ATT&CK technique and sub-technique IDs. Prior approaches for (sub-)technique annotation adopt two primary paradigms: (1) Multi-class classification that directly map text to (sub-)technique IDs (You et al., 2022; for Threat-Informed Defense, 2023), which, while straightforward to implement, struggle with class imbalance and require extensive labeled training data; and (2) Retrieval/ranking approaches that evaluate semantic similarity between the text and (sub-)techniques. Early methods like Ladder (Alam et al., 2023) & AttackKG (Li et al., 2022a) introduce basic similarity-based ranking. Text2TTP (Kumarasinghe et al., 2024) advanced this through hierarchical re-ranking with fine-tuned embeddings, while NCE (Nguyen et al., 2024) improved similarity learning using dual-encoder architectures. Recently, IntelEX (Xu et al., 2024) employed LLMs in both retrieval and zero-shot learning settings to assess (sub-)technique relevance. Although promising, the current methods are constrained by critical trade-off: they either rely on general-purpose models lacking domain expertise or require large-scale labeled datasets and computationally intensive training pipelines. Retrieval approaches, for instance, require extensive hardnegative mining to distinguish fine-grained (sub- )techniques while classification models demand curated, balanced training setsresources rarely available in this specialized domain. Compounding this issue, despite MITRE ATT&CK framework defines over 550 adversarial (sub-)techniques, only approximately 10,000 annotated examples are publicly available (Kumarasinghe et al., 2024; Nguyen et al., 2024), severely limiting generalization. To address these dual challenges, we propose TECHNIQUERAG, domain-specific retrievalaugmented generation (RAG) framework for (sub- )techniques annotation task that bridges generic and specialized models while eliminating dependency on resource-intensive pipelines or large labeled data. Unlike conventional approaches, TECHNIQUERAG integrates three key components: (a) off-the-shelf retrievers for candidate extraction (b) instruction-tuned LLMs to re-rank candidate (sub- )techniques (c) minimal text-(sub-)technique pairs used exclusively for fine-tuning the generator. Our approach mitigates data scarcity by fine-tuning only the generation component on limited in-domain examples while leveraging novel re-ranking framework that uses generic off-the-shelf LLMs without fine-tuning to explicitly align retrieved candidates Figure 2: Overview of TECHNIQUERAG. with adversarial (sub-)techniques, thereby enhancing domain specificity. Fig 2 shows an overview. While LLMs offer promising capabilities for ranking adversarial (sub-)techniques, this task poses challenges beyond their standard pre-training and alignment objectives. Unlike traditional ranking taskssuch as those encountered in questionansweringsecurity technique ranking requires distinguishing among subtly different ATT&CK (sub-)technique that may co-occur in text without explicit indicators (see Fig 1). Consequently, conventional re-ranking frameworks like RankGPT (Sun et al., 2023), though effective for general search, struggle with the nuanced demands of security-specific ranking. To address, our framework prompts LLMs to engage in explicit, stepby-step reasoning about (sub-)technique relevance, considering both high-level techniques and finegrained sub-techniques. This structured decomposition not only enables more precise ranking but also captures hierarchical relationships among (sub- )techniques as in the ATT&CK framework. In experiments, we evaluate our framework on three benchmarks, addressing both single-label and multi-label prediction tasks for (sub-)techniques. Results demonstrate that TECHNIQUERAG significantly outperforms various baseline approaches, including classification-based, retrieval/rankingit based, and hybrid methods. achieves comparable or superior performance to vanilla RAG approaches, even when utilizing powerful LLMs like GPT-4o. Furthermore,"
        },
        {
            "title": "2 Related Work",
            "content": "Table 1 presents comparison of the methods proposed for the (sub-)technique ID annotation task. These methods can be categorized into three groups based on their characteristics Text-based Feature Extraction Initial ap-"
        },
        {
            "title": "Key Features",
            "content": "(for Threat-"
        },
        {
            "title": "Classification",
            "content": "(Husari et al., Matching/Ranking TRAM Informed Defense, 2023) TTPDrill 2017a) AttacKG (Li et al., 2022b) TIM (You et al., 2022) LADDER (Alam et al., 2023) NCE (Nguyen et al., 2024) Matching/Ranking Matching/Ranking Text2TTP (Kumarasinghe et al., 2024) IntexEX (Xu et al., 2024) Matching/Ranking Classification Matching/Ranking Hybrid (Retrieval and Evaluation) Utilizes n-gram frequency features Employs TF-IDF and BM25 for text retrieval"
        },
        {
            "title": "Leverages knowledge graph representations\nIncorporates textual and lexical features\nUses sentence encoder embeddings",
            "content": "Applies task-adapted dual-encoder embeddings Enhances dual-encoder retrievals with cross-encoder embedding filter model Combines sentence/entity-based search with LLM-based evaluation of candidate outputs TechniqueRAG (Ours) Retrieval Augmented Generation (RAG) Integrates any retriever with LLM-based re-ranking and fine-tuned generation Table 1: Overview of methods proposed for automatically mapping security text to Mitre ATT&CK (sub-)technique.. Our proposed TECHNIQUERAG leverages flexible RAG framework by combining off-the-shelf retrievers, novel LLM-based re-ranking mechanism, and fine-tuned generator, distinguishing it from prior approaches. proaches to (sub-)technique identification utilized classical text representations: bag-of-words models utilizing TF-IDF (Legoy et al., 2020; Tsai et al., 2020), n-gram frequencies (Legoy et al., 2020; for Threat-Informed Defense, 2023), and word embeddings (Legoy et al., 2020) as features for multiclass & multi-label classifiers. Later works enhanced representation through syntactic parsing to extract (subject, verb, object) patterns (Husari et al., 2017b) & knowledge graph alignment (Li et al., 2022b) to capture contextual relationships in threat behaviors. Neural Text Embedding Approaches Transformer-based language models (Reimers and Gurevych, 2019) enabled semantic similaritybased technique identification through neural embeddings. Early approaches used pre-trained encoders to embed threat behaviors, either handling multi-sentence descriptions (You et al., 2022) or specific attack patterns (Alam et al., 2023), evaluating relevance through embedding similarity. (Kumarasinghe et al., 2024) advanced this through multi-stage architecture combining fine-tuned cross-encoder and dual-encoder models to balance effectiveness and efficiency. (Nguyen et al., 2024) further developed this approach using dual-encoder architecture with alignment components, leveraging both scratch-trained embeddings and domain-specific models. LLM Applications The application of LLMs to technique identification has yielded important insights. (Kumarasinghe et al., 2024) found that Normally, LLMs perform poorly compared to finetuned smaller models due to hallucination issues. To address, (Xu et al., 2024) introduced hybrid approach combining zero-shot classification, retrieval, and LLM-based validation while we propose RAG to enhance reliability and reduce errors."
        },
        {
            "title": "3 Method",
            "content": "We present TECHNIQUERAG, domain-specific retrieval-augmented generation (RAG) framework for adversarial technique (or sub-technique) annotation. Unlike conventional approaches that rely on task-specific optimizations and extensive labeled data, TECHNIQUERAG effectively integrates retrievers, instruction-tuned LLMs and small-scale text-techniques paired data. We first provide an overview of our approach, followed by details on the key components: (i) retriever and (ii) LLMbased re-ranking (ii) generator fine-tuning. Fig 2 presents an overview of our system."
        },
        {
            "title": "3.1 Problem Formulation and Overview",
            "content": "Let = {x1, x2, . . . , xn} denote set of security texts (e.g., attack behaviors) and = {y1, y2, . . . , ym} denote the set of adversarial (sub- )techniques (e.g., MITRE ATT&CK IDs). For given security text, its annotation is represented as sequence = (y1, y2, . . . , yl), where each yi and m. We define as the set of all finite sequences over , so that . We assume access to small paired dataset = {(x1, Y1), (x2, Y2), . . . , (xn, Yn)} of threat descriptions and the corresponding set of groundtruth (sub-)techniques. Given an input text X, the task is to predict the corresponding (sub- )techniques Yx . Our framework, TECHNIQUERAG, comprises three modules: (1) retriever R, (2) an LLM-based re-ranker R, and (3) generator LLM G. Given an input xq, the retriever first retrieves the topK relevant pairs Rx from the dataset based on their similarity to the query text. The re-ranker processes the retrieved pair of annotated text, Rx, to produce an ordered sequence Rx, which is then augmented with the input sequence to form the generator context Cx = Rx where denotes concatenation. To conform to the context length of the generator G, the user may select reranked pairs for augmentation. These augmented pairs serve as exemplars that guide the generation process and help to reduce hallucination. Finally, the generator produces the target output Yx from the augmented input Cx (See Fig 2). In the following subsections, we provide detailed descriptions of the retriever R, the LLM-based re-ranker R, and the generator used in TECHNIQUERAG."
        },
        {
            "title": "3.2 Retriever R",
            "content": "The retriever module processes query security text xq by leveraging retrieval corpus DR to fetch most relevant candidate pairs. In our setting, due to the lack of additional data, we employ the paired dataset both as the retrieval corpus DR and as the training data for the generator G. To prevent data leakage during its training, we explicitly exclude xq from DR, defining it as: DR = {(xi, Yi) (xi, Yi) xi = xq} . 1 , YR K, YR 1 ), (xR ) sim(xq, xR The retriever returns the top-K pairs Rx = 2 ), . . . , (xR 2 , YR {(xR K)} DR, where each pair (xR , YR ) corresponds to security text and its associated (sub-)techniques from DR along with their (lexical/semantic) similarity sim(xq, xR ) > i. Any offthe-shelf retriever (e.g., sparse: BM25 or dense: pre-trained sentence embedding model) can be employed as R. While our approach is generic and further domain adaptation of may improve performance, it is important to note that only has behavior description and (sub-)technique annotation pairings (xi, Yi) without specifying the absolute relevance of xi with any of the individual (sub- )techniques within the sequence of ground truth technique annotations Yi. As result, training solely with heuristic losses, such as in-batch negatives, leads to sub-optimal adaptation. Furthermore, no hard negatives or denoising data are available. Therefore, instead of fine-tuning retriever, we employ an off-the-shelf retriever as and enhance it through re-ranking, as detailed below."
        },
        {
            "title": "3.3 Re-Ranker R",
            "content": "To address data scarcity and enhance domainspecific precision, our re-ranker refines the candidate set retrieved by using an instruction-tuned large language model. Unlike generic prompting frameworks for ranking (e.g., RankGPT (Sun et al., 2023)), which lack domain-specific knowledge, our re-ranker employs novel prompting framework specifically designed for adversarial technique annotation that addresses three key challenges described below. Explicit Reasoning for Implicit Mapping: Security texts rarely provide explicit rationales for technique mappings. For example, the text malware connects to C2 using custom encoding implies both command-and-control (TA0011) and defense evasion (TA0005) tactics, but doesnt directly state this relationship. instructs the LLM to decompose such implicit connections through structured reasoning: Prompt and Response: Break Down the Query # Decompose the given security query into distinct attack steps or phases. # Identify any implied or explicitly mentioned behaviors that indicate specific adversarial (sub-)techniques. Query: malware connects to C2 .. custom encoding Step 1: Identify core behavior C2 communication with encoding Step 2: Map to tactics Command and Control + Defense Evasion Step 3: Link to techniques T1071 (C2 Protocol) + T1027 (Obfuscation) Balanced Consideration of Multiple Techniques: Security activities often involve multiple techniques simultaneously. prompt ensures comprehensive coverage through parallel evaluation by instructing to explore each possible technique. Prompt and Response: Multiple Techniques # Consider that the query may involve multiple (sub- )techniques. (both direct and implied). Query: malware connects to C2 .. custom encoding Primary Technique: T1071 (Application Layer Protocol) Rationale: Direct C2 communication behavior Secondary Technique: T1027 (Obfuscated Files or Information) Rationale: Custom encoding for evasion Fine-Grained Relevance w.r.t Sub-Techniques: Certain techniques have multiple sub-techniques and some do not. Our framework evaluates subtle distinctions among them to inform the overall ranking of different techniques (with and without sub-techniques). For instance: Prompt and Response: Fine-grained Relevance # Map each identified step or behavior to the most appropriate technique or sub-technique (if available for each corresponding technique). For each matching technique, explain the connection between the query and the corresponding adversarial behavior. Query: PowerShell script encoded in base64 downloads malware from remote server Technique Analysis: 1. T1059 (Command and Scripting Interpreter) - T1059.001 (PowerShell): Direct match for script execution - Confidence: High due to explicit PowerShell usage 2. T1027 (Obfuscated Files) - No sub-techniques apply to basic encoding - Confidence: Med. as common obfuscation method Final Ranking: T1059.001 > T1027 Rationale: Sub-technique analysis reveals credential access as primary intent with process injection as supporting mechanism The complete system prompt to guide the LLM through this structured analysis is provided in Appendix E. This hierarchical, reasoning-based approach enables to reorder candidates while maintaining alignment with ATT&CKs taxonomy, addressing ambiguities in initial retrieval. To address the challenge of processing large candidate sets within the LLMs context window, we utilize the sliding window mechanism as in Sun et al. (2023)."
        },
        {
            "title": "3.4 Generator G",
            "content": "To adapt the LLM generator that produces the final annotations of the adversarial technique Yx from an augmented input Cx, we fine-tune it using as the training set. As discussed in Section 3.1, the augmentation process concatenates the original query with the re-ranked candidate pairs Rx (i.e., Rx), specifically as following: Cx = [text] xR 1 [technique] YR 2 [technique] YR xR 2 . . . [text] [technique] YR xR , 1 [text] where [] denotes separator tokens, xj and Yj are parallel data (e.g., xj is the security text and Yj = (y1,j, y2,j, . . . ym,j) is the corresponding (sub-)technique sequence. We train the generator model minimizing the negative log-likelihood of the ground-truth (sub- )technique annotations Yx = (y1,x, y2,x, . . . , yl,x) conditioned on the augmented input Cx: = (cid:88) (cid:88) (x,Yx)D i= log PG(yi,x Cx). To mitigate hallucination, is constrained to generate outputs from the re-ranked candidate set Cx. This design ensures that the final predicted (sub- )techniques are both contextually grounded in and consistent with the adversarial taxonomy provided by the exemplars in Cx."
        },
        {
            "title": "4.1 Data and Implementation",
            "content": "We assess the capability of TECHNIQUERAG to accurately map threat behaviors to (sub-)technique IDs. We consider both single-label (l = 1) and multi-label (l > 1) prediction setups. Following previous works, we evaluate on three publicly available benchmark datasets: Tram (for ThreatInformed Defense, 2023) as single-label dataset, and the Procedures and Expert datasets (Nguyen et al., 2024) representing single-label and multilabel settings, respectively. We report performances on the test sets of these datasets, training our model on the combined training sets. Rather than developing separate models for technique and sub-technique prediction, we train single model for sub-technique prediction. This is motivated by the fact that sub-technique annotations provide more fine-grained representation that inherently includes the broader technique identifier (e.g., in T1050.001, T1059 is the technique and 001 is the sub-technique). When evaluating for technique prediction, we simply truncate the sub-technique component. As the retriever R, we use BM25 with = 40 and = 3. For the frozen re-ranker R, we employ DeepSeek v3 (Liu et al., 2024) (with temperature set to 0) , processing retrieved candidates in batches of 40 with an overlap of 20. The trainable generator is implemented using an 8B Ministral Instruct model (MistralAI, 2024). Finetuning is performed with LLaMa-Factory using LoRA (Hu et al., 2021), with learning rate of 104, LoRA rank = 8, and α = 4. For generation, we use sampling temperature of 0.7, top-p value of 0.1, and context length of 2048 tokens. Our source code, datasets, and models are publicly available on GitHub1. 1https://github.com/qcri/TechniqueRAG Model Tram (Single-label) Procedures (Single-label) Expert (Multi-label) Prec. Rec. F1 Prec. Rec. Prec. Rec. F1 Retrieval-based Methods NCE Text2TTP BM25 RankGPT Our Re-Ranker Generative Models GPT-4o w/ CoT+Ref DeepSeek v3 w/ CoT+Ref Ministral 8B w/ CoT+Ref IntelEx RAG Models GPT-4 (RAG) DeepSeek v3 (RAG) Ministral 8B (RAG) TECHNIQUERAG 90.30 51.59 67.86 61.93 64.69 38.28 43.52 43.74 43.68 7.68 14.94 60.67 55.50 54.59 51.88 76. 78.90 21.36 64.74 58.56 61.43 49.98 67.20 65.69 66.36 31.71 26.21 70.71 70.64 77.36 57.61 72.14 84.22 30.22 66.26 60.20 63.02 43.35 52.83 52.51 52.68 12.36 19.03 65.31 62.16 64.01 54.60 74. 84.10 74.76 32.54 59.40 85.46 51.42 51.84 50.87 51.55 7.07 16.58 61.13 71.34 66.43 61.40 91.11 80.60 74.65 32.48 59.33 85.29 64.04 78.47 78.13 75.86 30.79 29.29 75.07 88.06 91.57 69.81 91. 82.31 74.70 32.51 59.37 85.37 57.04 62.43 61.62 61.39 11.50 21.17 67.39 78.82 77.00 65.34 91.09 N/A 32.96 48.67 46.89 49.25 19.63 17.17 41.88 55.76 60.86 35.23 37. 25.59 42.80 43.27 41.85 11.84 17.02 44.74 51.30 48.95 38.81 50.19 20.91 38.19 40.17 36.39 8.47 16.88 48.03 47.49 40.94 43.21 75.16 Table 2: Results in technique prediction. CoT+Ref: Chain-of-Thought w/ Reflection. The num of predicted labels are fixed for ranking models while generative models determines at runtime, hence compared in Section 5.3."
        },
        {
            "title": "4.2 Baselines and Evaluation Metrics",
            "content": "Retrieval/Ranking-only Methods These include state-of-the-art approaches that rely solely on retrieval and re-ranking w/o using generative models. We compare w/ NCE (Nguyen et al., 2024) for contrastive domain-specific learning, Text2TTP (Kumarasinghe et al., 2024), which combines bi-encoder semantic search w/ cross-encoder re-ranking, underlying BM25 retriever baseline, and RankGPT (Sun et al., 2023) re-ranking framework that uses same BM25 retrievals. As NCE is not released we report from (Nguyen et al., 2024). Generation-based Methods Direct Generation: We evaluate against powerful LLMs including GPT4, DeepSeek V3, and Ministral 8B. For each model, we implement both direct prompting and chainof-thought approaches with self-reflection (Shinn et al., 2024). Retrieval-Augmented Generation: We compare against IntelEX (Xu et al., 2024), hybrid retrieval and LLM-judge approach. Additionally, we implement retrieval-augmented versions of the above LLMs using text and identical exemplars from our retrieved and re-ranked pairs (Cx). Evaluation Metrics Following previous works, we evaluate performance on two settings: (i) for single-label technique and sub-technique prediction task, we use standard Precision, Recall, and F1 scores; (ii) for multi-label tasks, we adopt differentiated evaluation protocol. Our evaluation consists of: (1) End-to-End Evaluation: comparing our models adaptive label predictions with generative baselines, as both can dynamically determine the optimal number of labelsa capability that retriever-only methods lack; and (2) Ranking Analysis: evaluating our re-ranker against all retriever or ranking methods using standard ranking metrics (Precision, Recall, and F1) at k={1,3}."
        },
        {
            "title": "5.1 Technique-Level Performance",
            "content": "Table 2 reports the performance of various models on the technique prediction task across three datasets with increasing diversity: Tram (singlelabel with 198 unique), Procedures (single-label with 488 unique), and Expert (multi-label with 290 unique) techniques and sub-techniques. Among retrieval-based methods, NCE achieves the highest F1 score on Tram (84.22%), reflecting its strength in constrained label space. However, as the diversity increases, NCEs performance drops markedlyfor example, on Procedures it only reaches an F1 of 82.31% In contrast, our proposed TECHNIQUERAG excels consistently. On Procedures, TECHNIQUERAG attains an F1 score of 91.09%, demonstrating its superior ability to generalize in highdiversity, single-label setting. Although on the Expert dataset proprietary model GPT-4o (RAG) achieves marginally higher F1 (51.30% vs. TECHNIQUERAGs 50.19%), the overall perforModel Tram (Single-label) Procedures (Single-label) Expert (Multi-label) Prec. Rec. F1 Prec. Rec. Prec. Rec. F1 Retrieval-based Models NCE Text2TTP BM25 RankGPT Our Re-Ranker Generative Models GPT4o w/ CoT+Ref DeepSeek v3 w/ CoT+Ref Ministral 8B w/ CoT+Ref IntelEx RAG Models GPT4o (RAG) DeepSeek v3 (RAG) Ministral 8B (RAG) TECHNIQUERAG 77.00 42.62 48.41 43.03 50.76 27.62 32.33 30.97 33.28 3.72 10.96 53.09 39.29 39.31 34.94 72. 65.80 40.41 46.56 40.97 48.45 36.34 49.91 47.07 51.47 21.99 21.34 63.33 52.84 58.54 40.86 68.74 70.96 41.49 47.47 41.97 49.58 31.38 39.24 37.36 40.42 6.37 14.48 57.76 45.07 47.04 37.67 70. 75.70 71.08 24.17 51.19 84.21 42.42 43.90 40.98 41.47 3.25 7.17 53.07 64.11 59.72 53.41 91.11 71.88 70.94 24.17 51.12 83.98 55.04 68.33 64.23 61.63 17.32 13.53 67.77 81.63 86.47 63.75 88. 73.74 71.01 24.17 51.16 84.10 47.91 53.46 50.04 49.58 5.47 9.37 59.53 71.82 70.65 58.12 88.11 N/A 24.26 37.75 35.87 37.14 14.72 11.60 33.52 45.87 48.06 28.24 30. 19.77 34.78 35.22 33.67 9.17 12.14 37.88 43.73 41.11 30.39 42.22 16.69 32.24 34.60 30.79 6.65 12.74 43.55 41.77 35.91 32.90 70.06 Table 3: Performance Comparison for sub-technique prediction task (in percentage). Note: CoT+Ref: Chain-ofThought with Reflection. Retrieval-based methods are not applicable for the multi-label Expert dataset. mance averaged across the three datasets signifies the effectiveness of our open-source framework. When we compute the average F1 score, TECHNIQUERAG achieves approximately 80.76%, compared to only about 58.11% for GPT-4o (RAG). This substantial improvement underscores that our model is more robust, particularly in handling diverse and complex adversarial scenarios."
        },
        {
            "title": "5.2 Sub-Technique-Level Performance",
            "content": "Table 3 presents the results at the sub-technique level. similar trend is observed. At finer granularity, our method maintains dominance on Procedures (our F1 88.11 vs NCEs 73.74) while matching Trams performance gap (F1 70.66 vs NCEs 70.96). This again posits our effectiveness for complex and robust threat annotation. The performance gap between our model and other generative and RAG baselines widens further at the sub-technique level. While GPT-4o (RAG) achieves slightly higher F1 score on the Expert dataset (43.73 vs our 42.22), the overall results across all datasets demonstrate that our approach generalizes more effectively to complex, high-diversity environments."
        },
        {
            "title": "5.3 Single-Label versus Multi-Label Settings",
            "content": "Our experiments in Table 2 and 3 reveal that multilabel prediction poses significant challenges in compare to single-label. For example GPT-4o achieves F1 score of 76.75 in Procedure while only 19.77 in Expert). While retrieval augmented generation enhances all generative models, gains in open-source LLMs remain low such as using Ministral RAG without our finetuning scores F1 of 30.39 in Expert. Adapting to the domain TECHNIQUERAG boosts it up to 42.22 tailing the RAG score of GPT-4os 43.73. Furthermore, in Table 4 we compare all the ranking based models with our re-ranker frameworkshowing large margin gains over all. These significant F1 improvements both in single and multi-label setup confirm the effectiveness of our model in real-world scenarios."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Enhancement with Our Re-Ranker Our comprehensive evaluation in Table 2, 3 and 4 clearly indicate that our re-ranker not only outperforms all the ranking based methods but also enhance the overall end-to-end performances. In addition to our model, all the generative models (e.g., DeepSeek) in RAG setups using our re-ranked exemplars achieves notable gains over their direct or CoT+Ref inferences. Gains over Other Fine-Tuning Methods We also validate the effectiveness of our RAG-based domain adaptation methods over zero-shot and CoT+Ref based methods. For zero-shot, we finetune our same Ministral model on the same training data but without exemplars and for CoT+Ref based methods we followed the Alpaca approach (Taori et al., 2023) where for our same train data using DeepSeek v3 as the teacher model with the"
        },
        {
            "title": "Model",
            "content": "NCE Text2TTP BM25 RankGPT Our Re-Ranker"
        },
        {
            "title": "Technique Level",
            "content": "Sub-Technique Level @1 23.6 26.1 21.4 25.3 35.3 74.5 53.5 51.6 56.7 71. F1 35.9 37.4 35.1 35.5 30.2 37.4 34.9 44.6 47.2 @3 P 48.3 42.4 49.1 37.8 40.4 41.5 46.6 51.1 59.9 73.1 49.0 45.9 49.7 66.9 @1 18.2 21.3 15.6 19.8 29. F1 29.1 34.4 30.2 31.0 23.3 34.8 28.4 47.1 40.5 @3 39.9 36.8 39.7 30.5 29.9 36.3 37.8 50.4 54.2 Table 4: Performance of Ranking Methods on Expert Dataset (Multi-Label). - refers to results not reported. CoT+Ref prompt we synthesis new traindata and then fine-tune the Ministral model. Results in Fig 3 shows our approach achieves the highest gain in both target tasks in all benchmark datasets."
        },
        {
            "title": "5.5 Qualitative Analysis",
            "content": "Running Example. We provide in Appendix concrete example from Expert dataset that shows the predictions of our Re-Ranker and how TECHNIQUERAG generator improved it. We also provide examples of the prompts in RankGPT and ours with detailed responses with our re-ranker LLM DeepSeek V3 in Appendix E. Error Analysis Analysis reveals few challenges: Under-prediction. The model often captures primary techniques while missing related techniques in the same attack pattern (e.g., identifying T1055 but missing associated techniques like T1106) Contextual errors. (i) Confusion between similar techniques within the same tactic family specially for Command and Scripting Interpreter techniques (T1059.*) (ii) Missing implicit or contextual techniques not explicitly stated (iii) Difficulty capturing logical relationships between techniques Hierarchical issues. Struggles with parent-child technique relationships and sometimes generates invalid sub-technique IDs Re-ranker limitations. Missed techniques due to ambiguous queries and compound statements, affecting the Generator through propagation Technique similarity. Challenges in distinguishing between techniques with overlapping descriptions and keywords (e.g., phishing-related techniques T1598.003, T1566.002, T1204.001) Class Imbalance Effects. The severe data imbalance fundamentally impacts model performance - only 47 out of 203 techniques (23.2%) have more than 50 training samples. Techniques with abundant data show high precision and recall, while rare techniques suffer from both misclassification and under-prediction. We present detailed analysis in Appendix . (a) Performance Comparison (F1) - Technique Level (b) Performance Comparison (F1) - Sub-Technique Level Figure 3: F1 scores for different fine-tuning methods."
        },
        {
            "title": "6 Conclusion",
            "content": "Annotating threat intelligence texts with adversarial techniques from the MITRE ATT&CK framework is manual and time-intensive task that security analysts must perform daily. Its automation requires methods capable of accurately identifying techniques and sub-techniques across hundreds of possibilities while handling complex security terminology, diverse text formats, and limited labeled data. We introduce TECHNIQUERAG retrievalaugmented fine-tuning approach designed to tackle these challenges effectively. Our comparative analysis demonstrates that TECHNIQUERAG establishes new state-of-the-art, outperforming both semantic ranking models and other LLM-based methods in adversarial technique annotation."
        },
        {
            "title": "References",
            "content": "Obtaining large, balanced parallel datasets of threat descriptions and ground truth technique annotations remains significant challenge due to the reliance on domain expertise for accurate annotation. Although our approach mitigates data scarcity, two key limitations may impact performance: 1. Limited Technique Coverage: Coverage of techniques is often insufficient. Even the MITRE ATT&CK knowledge base lacks procedural examples for many techniques and sub-techniques. 2. Sparse Technique Annotations: Existing datasets typically contain very few technique annotations per example, with many instances in our data having only single technique label. During fine-tuning, this bias toward minimal technique labeling limited our methods ability to generalize effectively. To mitigate this, we oversampled examples with multilabel technique annotations. However, our method rarely assigned more than two technique labels per input query, leading to low recall, particularly on the Expert dataset, which consists almost exclusively of multi-label examples. 3. Annotation Inconsistencies Some model predictions marked as errors represent valid technical interpretations not included in gold standard annotations. For example, the following sentence: SMOKEDHAM was observed using UltraVNC to establish connection to the IP address and port pair 81.91.177[.]54[:]7234 that has been observed in past UNC2465 intrusions. had T1571: Non-Standard Port as the only ground truth label. However, if we analyze it carefully, we see that the threat actor used UltraVNC, so T1021.005: Remote Services - VNC exists in the given description. Our model correctly predicted it, but missed the T1571: NonStandard Port. This highlights challenges in maintaining consistent annotation standards for complex attack patterns."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors declare the use of AI Assistants in editing the text, fixing the typos, and generating the prompts and figures. Md Tanvirul Alam, Dipkamal Bhusal, Youngja Park, and Nidhi Rastogi. 2023. Looking beyond iocs: Automatically extracting attack patterns from external cti. Preprint, arXiv:2211.01753. MITRE Corporation. 2022. Enterprise Matrix. org/versions/v12/matrices/enterprise/. Accessed: Jan 2, 2024. MITRE ATT&CK https://attack.mitre. Center for Threat-Informed Defense. 2023. Tram: Threat-risk assessment model. Accessed: 2025-0102. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Ghaith Husari, Ehab Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. 2017a. Ttpdrill: Automatic and accurate extraction of threat actions from unstructured text of cti sources. In Proceedings of the 33rd annual computer security applications conference, pages 103115. Ghaith Husari, Ehab Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. 2017b. Ttpdrill: Automatic and accurate extraction of threat actions from unstructured text of cti sources. In Proceedings of the 33rd Annual Computer Security Applications Conference, ACSAC 17, page 103115, New York, NY, USA. Association for Computing Machinery. Udesh Kumarasinghe, Ahmed Lekssays, Husrev Taha Sencar, Sabri Boughorbel, Charitha Elvitigala, and Preslav Nakov. 2024. Semantic ranking for automated adversarial technique annotation in security In Proceedings of the 19th ACM Asia Context. ference on Computer and Communications Security, pages 4962. Valentine Legoy, Marco Caselli, Christin Seifert, and Andreas Peter. 2020. Automated retrieval of att&ck tactics and techniques for cyber threat reports. arXiv preprint arXiv:2004.14322. Zhenyuan Li, Jun Zeng, Yan Chen, and Zhenkai Liang. 2022a. Attackg: Constructing technique knowledge graph from cyber threat intelligence reports. In Computer SecurityESORICS 2022: 27th European Symposium on Research in Computer Security, Copenhagen, Denmark, September 2630, 2022, Proceedings, Part I, pages 589609. Springer. Zhenyuan Li, Jun Zeng, Yan Chen, and Zhenkai Liang. 2022b. Attackg: Constructing technique knowledge graph from cyber threat intelligence reports. In Computer Security ESORICS 2022: 27th European Symposium on Research in Computer Security, Copenhagen, Denmark, September 2630, 2022, Proceedings, Part I, page 589609, Berlin, Heidelberg. Springer-Verlag. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. MistralAI. 2024. Ministral-8b-instruct-2410. Accessed: 2025-02-15. Tu Nguyen, Nedim Šrndic, and Alexander Neth. 2024. Noise contrastive estimation-based matching framework for low-resource security attack pattern recognition. In Findings of the Association for Computational Linguistics: EACL 2024, pages 355373, St. Julians, Malta. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. CoRR, abs/1908.10084. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1491814937, Singapore. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Chia-En Tsai, Cheng-Lin Yang, and Chong-Kuan Chen. 2020. Cti ant: Hunting for chinese threat intelligence. In 2020 IEEE International Conference on Big Data (Big Data), pages 18471852. IEEE. Ming Xu, Hongtai Wang, Jiahao Liu, Yun Lin, Chenyang Xu Yingshi Liu, Hoon Wei Lim, and Jin Song Dong. 2024. Intelex: llm-driven attacklevel threat intelligence extraction framework. arXiv preprint arXiv:2412.10872. Yizhe You, Jun Jiang, Zhengwei Jiang, Peian Yang, Baoxu Liu, Huamin Feng, Xuren Wang, and Ning Li. 2022. Tim: threat context-enhanced ttp intelligence mining on unstructured threat data. Cybersecurity, 5(1):3."
        },
        {
            "title": "A Running Example",
            "content": "In Figure 4, the given text describes various execution techniques used by attackers, such as launching executables and DLLs in memory, leveraging schtasks.exe to modify task schedules, and executing PowerShell commands. The ground truth labels identify the relevant MITRE ATT&CK techniques: T1059.001 (PowerShell) is explicitly mentioned in The ability to launch custom PowerShell commands, T1053 (Scheduled Task/Job) and its sub-technique T1053.005 (Scheduled Task) are referenced in The ability to leverage schtasks.exe to add or modify task schedules. The RAG output provided partial alignment with the ground truth labels but included T1218.011 (Rundll32), which is not explicitly mentioned. However, this retrieval step helped the LLM Output by reinforcing correct predictions ensuring T1059.001 and T1053.005 were present while also introducing T1053. Additionally, the LLM expanded the predictions to include T1071.001 (Web Protocols), which does not appear relevant in this case."
        },
        {
            "title": "Example of TECHNIQUERAG Output",
            "content": "The ability to launch .exe or .dll file in memory. The ability to leverage schtasks.exe to add or modify task schedules. The ability to launch custom PowerShell commands. The ability to leverage standalone executable to load the DLL if the attacker otherwise has no way of doing so. Ground Truth Labels: 1. T1059.001: Command and Scripting Interpreter: PowerShell 2. T1053: Scheduled Task/Job 3. T1053.005: Scheduled Task/Job: Scheduled Task RAG Output (Our Re-ranker): 1. T1218.011: System Binary Proxy Execution: Rundll32 2. T1059.001: Command and Scripting Interpreter: PowerShell 3. T1053.005: Scheduled Task/Job: Scheduled Task LLM Output (TECHNIQUERAG): 1. T1059.001: Command and Scripting Interpreter: PowerShell 2. T1053: Scheduled Task/Job 3. T1053.005: Scheduled Task/Job: Scheduled Task 4. T1071.001: Application Layer Protocol: Web Protocols Figure 4: Example of MITRE ATT&CK techniques and sub-techniques highlighted in text with corresponding colored (implicit) indicators. IDs with \".\" denote subtechniques (e.g., T1059.001). Greyed-out IDs indicate incorrect predictions. Data Statistics Tables 5 and 6 shows the details of the employed datasets. The Expert split consists of actual sentences from full reports published by threat intelligence vendors. These sentences are multiilar techniques within the same tactic family, particularly between various Command and Scripting Interpreter techniques (T1059.*). The model also demonstrates tendency to miss contextual techniques that are implied but not explicitly stated in the text, such as failing to identify T1082: System Information Discovery when enumeration of system resources is described as part of larger operation. Additionally, there is notable pattern of missing data staging and encoding techniques (T1074, T1132) when they are described as part of exfiltration workflows. C.1 Contextual Inference Failures. The model demonstrates limitations in capturing implicit relationships, often missing techniques that are logical precursors or consequences of explicitly described actions. It frequently identifies primary techniques while missing related concurrent techniques within the same attack pattern. Class Imbalance Effects. The severe data imbalance fundamentally impacts model performance - only 47 out of 203 techniques (23.2%) have more than 50 training samples. Techniques with abundant data show high precision and recall, while rare techniques suffer from both misclassification and under-prediction. C.2 Dependency on Re-ranker In several cases, some (sub-)techniques are omitted, likely due to ambiguous language in the query or an overemphasis on the most actionable part of compound query for example. This, error further propagate to our Generator, which uses output from Re-ranker as few-shot examples. C.3 Similar Techniques Several techniques within the MITRE ATT&CK framework share significant similarities and often use overlapping keywords, which can influence our initial BM25 rankings. For instance, T1598.003, T1566.002, and T1204.001 are all phishing-related techniques that have similar descriptions with minor distinctions."
        },
        {
            "title": "D Different Domain Adaptation Methods",
            "content": "label, meaning they can be associated with multiple MITRE ATT&CK techniques. In contrast, the Tram split contains incomplete sentences, such as opens cmd.exe on the victim, searches for specified files, or icacls . /grant Everyone:F /T /C /Q, often presenting isolated technique references without sufficient context. Tram is single-label, meaning each sentence corresponds to only one technique. The Procedures split, extracted from the MITRE knowledge base, consists of complete sentences that summarize single technique mentioned in report. These sentences provide structured descriptions of attack techniques but are also single-label. In total, the training splits contain 499 unique techniques, covering approximately 78% of the 637 techniques available in the MITRE ATT&CK Enterprise Framework. Table 5: Dataset Statistics"
        },
        {
            "title": "Train\nTest",
            "content": "38.00 71.42 13.36 13.43 2.94 21.22 472 158 10,999 1768 3469 Table 6: Dataset statistics. S+T denotes the joint count of techniques and sub-techniques. Dataset Texts S+T TechAvg. # Avg. # niques Labels Tokens TRAM Procedures Expert 4797 11723 193 488 290 132 180 151 1.16 1.00 1.84 23"
        },
        {
            "title": "C Error Analysis",
            "content": "Common Errors. Analysis of the prediction errors reveals several systematic patterns in MITRE ATT&CK technique classification. The most frequent error type involves under-prediction, where the model identifies only the most prominent technique while missing other techniques that are part of the same attack pattern. For example, when analyzing process injection scenarios, the model often identifies the primary technique (T1055: Process Injection) but fails to capture associated techniques like T1106: Native API or specific sub-techniques like T1055.001: Dynamic-link Library Injection. Another common pattern involves confusing simTable 7: Performance Comparison of TECHNIQUERAG Across Domain Adaptation techniques Levels (Percentage Scores) Model Tram Procedures Expert Technique Sub-Technique Technique Sub-Technique Technique Sub-Technique 65.7 Zero-shot + CoT+Ref 64.1 TECHNIQUERAG 76.0 74.9 66.7 72.1 F1 69.9 65.4 74. 60.0 52.0 72.7 69.2 54.3 68.7 F1 64.3 53.1 70. 85.8 77.4 91.1 87.4 84.3 91.1 F1 86.6 80.7 91. 85.8 69.1 91.1 F1 87.4 76.9 88.09 86.6 72.8 88. 41.3 48.6 75.2 48.3 41.7 37.7 F1 44.5 44.9 50. 35.1 42.4 70.1 37.6 33.5 30.2 F1 36.3 37.4 42."
        },
        {
            "title": "Vanilla RankGPT Prompt and Output",
            "content": "# System Prompt: You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query. ## Objectives: will provide you with {num} passages, each indicated by number identifier [ ]. Rank the passages based on their relevance to the query. ## Given Passages: {Passage 1: Description} {Passage 2: Description} ... {Passage n: Description} ## Query: Monero miner scripts are downloaded from TeamTNTs server and piped to bash using an SSH session on the underlying host as the root user by supplying the private key from /tmp/TeamTNT. Later, the private key /tmp/TeamTNT is removed as well. ## Output T1552.004 > T1098.004 > T1563.001 > T1021 > T1555.002 > T1573.002 > T1546.004 > T1496 > T1059.004 > Figure 5: Example RankGPT prompt and its corresponding output. Green colored IDs are the correct ones. Re-ranker Prompt # System Prompt Act as an expert security analyst specializing in ranking given list of MITRE ATT&CK techniques by their relevance to security query. ## Objectives: 1. Determine if the given query describes an adversarial or attack behavior. 2. If it does, identify and rank the most relevant techniques and sub-techniques from the provided list. ## Instructions for Ranking: 1. Break Down the Query: - Decompose the security query into distinct attack steps. - Identify explicit and implied adversarial behaviors. 2. Match Techniques: - Map each behavior to relevant techniques/sub-techniques. - Consider multiple relevant techniques. 3. Provide Explanation: - Explain the connection between the query and the matched techniques. ## Final Output Format: After reasoning, output the final ranking as: [Technique A] > [Technique B] > [Technique C] > ... No variations or extra formatting allowed. ## Given Techniques: {Technique 1: Description} {Technique 2: Description} ... {Technique n: Description} ## Query: Monero miner scripts are downloaded from TeamTNTs server and piped to bash using an SSH session on the underlying host as the root user by supplying the private key from /tmp/TeamTNT. Later, the private key /tmp/TeamTNT is removed. ## LLM Output The query describes an attack involving the following key steps and techniques: Initial Access: The attacker gains access to the host via SSH using private key stored in /tmp/TeamTNT. This aligns with T1552.004 (Unsecured Credentials: Private Keys) and T1021.004 (Remote Services: SSH). Execution: The attacker downloads Monero miner scripts and pipes them to bash. This involves T1059.004 (Command and Scripting Interpreter: Unix Shell). Persistence: The attacker uses SSH with private key, which could imply persistence via T1098.004 (Account Manipulation: SSH Authorized Keys). Resource Hijacking: The Monero miner script indicates T1588.001 (Obtain Capabilities: Malware) for downloading and executing the miner. Key Techniques Identified: T1552.004: Exploitation of private keys for SSH access. T1021.004: Use of SSH for remote access. T1059.004: Execution of scripts via Unix shell (bash). T1098.004: Potential persistence via SSH authorized keys. T1588.001: Downloading and executing Monero miner scripts (malware). Ranking Explanation: Passages [1], [2], [6], [7], [8], [40] are highly relevant as they directly address the techniques used in the query. Passages [3], [5], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39] are less relevant as they describe techniques not directly involved in the query. Final Ranking: T1552.004 > T1098.004 > T1021.004 > T1059.004 > T1588.001 > T1496 > T1563.001 > T1546.004 > T1611 > T1140 Figure 6: Example re-ranker prompt and its corresponding output. Green colored IDs are the correct ones. Self-Relflection Prompt Your task is to analyze given text describing malware behavior, extract the associated MITRE ATT&CK techniques to this text, explain their relevance to it. ## Context: The MITRE ATT&CK framework is globally-accessible knowledge base of adversary tactics and techniques based on real-world observations. Its used by cybersecurity professionals to better understand and defend against cyber threats. ## Your Task: Analyze the following text. Extract all the associated MITRE ATT&CK technique and provide detailed explanation of why each technique is relevant to the text. ## Instructions: Follow these steps to provide your analysis: 1. <thinking>: Explain your thought process as you analyze the given cyber threat description. Identify key actions, tools, or methods mentioned that could correspond to MITRE ATT&CK techniques in 30 words. 2. <reflection>: Reflect on your initial analysis. Consider if youve missed any potential techniques or if any of your initial thoughts need revision. Think about the confidence level of your associations in 30 words. 3. <output>: Based on your thinking and reflection, output the final list of MITRE ATT&CK techniques as technique IDs and their names. For example: <output> - T1221: Template Injection - T1205.001: Traffic Signaling - Port Knocking </output> Ensure you use these exact tags (<thinking>, <reflection>, and <output>) in your response. ## Output Format: <thinking> Based on the given cyber threat description, can identify several key actions and tools that correspond to MITRE ATT&CK techniques: 1. [Insert relevant observations from the text] 2. [Continue with more observations] These observations suggest the following potential MITRE ATT&CK techniques: - [List potential techniques with brief explanations] </thinking> <reflection> Upon reflection, should consider the following: 1. Are there any subtle indicators in the text that might have overlooked: [Your answer in 20 words or less for question 1] 2. Have considered the full context of the attack, including potential preliminary or subsequent steps not explicitly mentioned? [Your answer in 20 words or less for question 2] 3. Are there any techniques Ive identified that might not be fully supported by the given information? [Your answer in 20 words or less for question 3] [Add any additional reflections or revisions to the initial analysis] Confidence level: [State the confidence level in the identified techniques] </reflection> <output> [List the final list of the extracted MITRE ATT&CK techniques as technique IDs and their names.] </output> Figure 7: The employed prompt in self-reflection."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Qatar Computing Research Institute, Doha, Qatar"
    ]
}