{
    "paper_title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture",
    "authors": [
        "Yutian Chen",
        "Shi Guo",
        "Tianshuo Yang",
        "Lihe Ding",
        "Xiuyuan Yu",
        "Jinwei Gu",
        "Tianfan Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 6 1 5 0 . 7 0 5 2 : r 4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture Yutian Chen1,2 Shi Guo1 Tianshuo Yang3 Lihe Ding2 Xiuyuan Yu2 Jinwei Gu Tianfan Xue2,1 1 Shanghai AI Laboratory; 2 The Chinese University of Hong Kong; 3 The University of Hong Kong; 4 NVIDIA; yutianchen@link.cuhk.edu.hk, guoshi@pjlab.org.cn, tfxue@ie.cuhk.edu.hk Figure 1. Our 4D Reconstruction Results of the real-captured scene. We propose an asynchronous capture scheme, which increases the effective capture frame rate by staggering the start times of cameras without any additional cost. We further leverage video diffusion priors to enhance the reconstruction results. The results show that our method can reconstruct high speed and complex motion with high quality."
        },
        {
            "title": "Abstract",
            "content": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100200 FPS without requiring specialized high-speed cameras. On processing side, we also propose novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances highspeed 4D reconstruction compared to synchronous capture. https://openimaginglab.github.io/4DSloMo/. 1. Introduction Fast-dynamic scenes reconstruction from multi-view videos is fundamental challenge in 3D vision with broad applications. In sports and biomechanics, high-speed 4D reconstruction enables precise motion capture for athlete performance evaluation and injury prevention. In autonomous driving and robotics, accurately reconstructing 3D models of fast1 moving objects, such as pedestrians and vehicles, is essential for perception and decision-making. Additionally, in VR/AR content production, reconstructing high-fidelity human performances, such as dance and martial arts, is crucial for creating realistic digital avatars. Still, 4D reconstruction of fast-moving objects remains challenging, as the majority of 4D-capturing camera arrays operate at no more than 30 FPS (frames per second). For instance, DNA-Rendering [2] operates at 15 FPS, while ENeRF-Outdoor [10] and Neural3DV [8] achieve 30 FPS. In contrast, many activities, such as cloth movement shown in Fig. 1, require higher frame rate; for example, standard high-speed 2D photography typically operates at 120 FPS or higher to capture them. Extending frame rate of existing camera arrays is hard, as that requires more expensive and dedicated hardware, and greatly increases data transmission bandwidth requirements. Another way to capture high-speed 4D motion without hardware changes is to increase the frame rate at the reconstruction stage. Recently, gaussian-splatting-based 4D reconstruction methods [8, 11, 23, 24, 26, 31] have significantly improved novel view synthesis for dynamic scenes. For simple motion, it can reconstruct continuous frames from sparse temporal inputs, effectively increasing the frame rate. However, it still fails to handle complex nonlinear motion, such as cloth movement, resulting in obvious artifacts, as shown in the top right of Fig. 1. Thus, this raises an interesting question: is it possible to recover intermediate frames of high-speed motion using regular video cameras at 30 FPS? To achieve that, we propose novel asynchronous capture scheme to increase frame rate. We deliberately add delay to the starting time between different cameras, so different cameras can capture different timestamps, effectively boosting the frame rate. An example is shown in Fig. 1. Two synchronized cameras can only achieve 25 FPS (top row) capturing. However, in our setup, by differing the capture time of Cam2, the capture interval reduces to 0.02s, achieving 50FPS capturing. In practice, we use eight 25FPS cameras, and divide them into 4 or 8 groups to effectively increase the perceived temporal resolution to 100 or 200 FPS, respectively. By capturing temporally denser frames, we can more accurately model the intermediate motion information, particularly for complex motion. As shown in Fig. 1, compared to the commonly used synchronized capture setting, our proposed asynchronous capture scheme reconstructs more accurate intermediate motion information in fast-moving scenes. key challenge introduced by such asynchronous capturing is the limited number of available views at each timestamp, which increases viewpoint sparsity and results in visible artifacts in the reconstruction, as illustrated in Fig. 1. Recent 3D sparse view reconstruction methods [4, 28, 29, 36] utilized the pre-trained image diffusion model to prove the reconstruction results. However, we experimentally find that utilizing image diffusion model to perform artifacts-fix cause temporal inconsistency for sparse 4D reconstruction problem, due to frame-wise independent refine. Thus we propose to train the video-diffusion based artifact-fix model for 4D reconstruction. To train this model, we construct dataset by temporally sub-sampling 4D sequences to simulate large motion and our proposed asynchronous capturing pattern. The sparsely sampled sequences are used to train 4D Gaussian Splatting model, whose rendered outputs naturally exhibit reconstruction artifacts. These degraded outputs are then paired with the original ground-truth sequences to form training data. Despite the limited size of available 4D datasets, our artifact-fix video diffusion model demonstrates strong generalization across multiple scenes, benefiting from the powerful spatiotemporal priors inherent in video diffusion model. Notably, with only 750 training pairs, our method effectively removes reconstruction artifacts and significantly improves visual quality, as shown in Fig. 1. We compare our method with several state-of-the-art approaches [19, 25, 35] using the DNA-Rendering dataset [1] and the Neural4DV dataset [8]. To simulate fast motion and asynchronous capture, we apply temporal subsampling. To further validate our approach on real-world asynchronous multi-view video data, we capture 12 sequences of fast and complex dynamic scenes using an asynchronous capture setup. Experimental results demonstrate that our method improves high-speed 4D reconstruction compared to synchronized capture. Our key contributions are summarized as follows: Hardware solution: We propose novel asynchronous capture scheme to increase the effective frame rate without any additional cost by staggering the start times of camera. Software solution: We train an artifact-fix video diffusion model to refine the 4D reconstruction results, significantly improving the visual quality of the rendered images. Dataset: We develop the first dataset containing 12 sequences of asynchronous multi-view videos to validate our approach, demonstrating the practical feasibility and effectiveness of our method. 2. Related Work 2.1. Dynamic 3D Reconstruction Reconstruction and novel view synthesis of dynamic 3D scenes remain challenging. Many existing methods require multiple synchronized videos captured from different viewpoints as input [10, 11, 19, 25, 30, 35]. These works tend to use radiance field models such as NeRF [15] or 3DGS [7] as the underlying static 3D representations, and model motion patterns using various types of the deformation field, for example, MLPs [14, 18, 34], spatial-temporal planes [19, 25], polynomial functions [9] and Fourier series [6]. These methFigure 2. The overall pipeline of our model. Given several asynchronous multi-view videos, we first initialize 4D Gaussian model for specific iteration. We then employ an artifact-fix video diffusion model to refine the input videos. The refined videos are subsequently used to update the 4D Gaussian model. ods learn motion patterns from independent time input in vanilla way and not adept at frames interpolation in the temporal dimension. However, due to constraints in data transmission bandwidth and camera performance, existing camera arrays for 4D scene capture typically operate at relatively low frame rates. For instance, DNA-Rendering [1] captures at 15 FPS, while ENeRF-Outdoor [10] and Neural3DV [8] achieve 30 FPS. This frame rate limitation poses challenges for reconstructing dynmaic scene of high temporal resolution. There are also few attempts to enhance the 4D models ability of modeling the temporal relationship. For example, TimeFormer [5] uses transformer module to enhance current deformable 3D Gaussians reconstruction methods in plugand-play manner. However, this form of enhancement does not provide the model with additional information sufficient to reconstruct complex and rapid linear motions. [12] proposed pixel-wise coded exposure method to capture high-speed information from single image. However, this approach provides only single view and is therefore not suitable for capturing 4D dynamic scenes. Therefore, we propose novel asynchronous capture method to improve the temporal resolution of the camera system, enabling us to reconstruct more complex and rapid motions without any additional cost. 2.2. Sparse View Scene Reconstruction As the asynchronous capture method reduces the number of available views at each timestamp, our work is closely related to the field of sparse view scene reconstruction. Sparse view reconstruction is challenging problem due to the limited availability of input views, which can lead to incomplete or ambiguous scene representations. To address this challenge, several recent works have proposed innovative solutions. For example, Freenerf [33] introduced method that incorporates depth regularization to improve the reconstruction quality from sparse views. Similarly, Nerf in 3d vision [3] and Regnerf [16] explored frequency regularization techniques to enhance the robustness and accuracy of scene reconstruction when only few input views are available. In addition to regularization-based methods, several recent works have leveraged extra supervision signals to guide the reconstruction process. For instance, SPARF [21] utilized optical flow estimated from pre-trained models to provide additional constraints during optimization. Monosdf [37] integrates both depth and normal maps as supplementary supervision signals. This combination helps refine the surface details of the reconstructed scene, enhancing its geometric accuracy. Darf [20] further explored the use of depth maps as an auxiliary signal in their method. Collectively, these works demonstrate the potential of combining various forms of supervision and regularization to enhance sparse view reconstruction. 2.3. 3D/4D Reconstruction with Diffusion Model The recent progress in diffusion models has driven significant advancements in 3D and 4D applications. Pioneer work ReconFusion [28] trains novel view synthesis diffusion model conditioning on sparse input views and then adopts SDS (Score Distillation Sampling) [17] style optimization strategy for novel view supervision. Unlike the SDS loss, ReconFusion directly predicts the pseudo ground truth of novel views by sampling from the trained diffusion model at 3 each optimization step, which is then used to compute the reconstruction loss. Follow-up works [4, 29, 36] also integrate diffusion models with NeRF or 3DGS. To further improve the optimization efficiency, Deceptive-NeRF [13] and 3D-GS Enhancer first render pseudo images from the sparseview-reconstructed 3D representation and use diffusion model to enhance these pseudo views to obtain high-quality novel view supervision without querying the diffusion model at every optimization step. Inspired by these few-shot 3D reconstruction methods, we utilize diffusion model to remove the artifacts from the 4D model rendered images. 3. Method To achieve high-quality 4D reconstruction for scenes with large and complex motion, we first revisit the data acquisition process and propose novel asynchronous capture scheme (Sec. 3.2) that surpasses the conventional 30 FPS (frame-per-second) limitation. Although the captured data can be leveraged by state-of-the-art method, i.e., GS4D [35] (revisited in Sec. 3.1), to achieve temporally dense 4D reconstruction, the asynchronous capture scheme inherently introduces challenges of sparse viewpoints and temporal inconsistency. To address the artifacts caused by sparse views, artifact-fix video diffusion model is introduced in Sec. 3.3, followed by the 4D reconstruction process with diffusion priors in Sec. 3.4. The overall framework of our approach is illustrated in Fig. 2. 3.1. Preliminary: 4D Gaussian Splatting We choose 4D Gaussian Splatting (GS4D) [35] as the 4D representation for reconstruction. In GS4D, dynamic 3D scenes are represented by introducing an additional time dimension into anisotropic 3D Gaussians [7]. Formally, 4D Gaussian is defined by its mean vector µ R4 and covariance matrix Σ R44: p(xµ, Σ) = exp (cid:18) 1 2 (x µ)T Σ1(x µ) (cid:19) , (1) where = (x, y, z, t), with (x, y, z) represents the spatial coordinates, and denotes the temporal coordinate. The covariance matrix Σ is factorized into scaling matrix and rotation matrix R, similar to 3D Gaussian splatting: Σ = RSST RT , where = diag(sx, sy, sz, st) defines the anisotropic scaling of the Gaussian, and is 4D rotation matrix parameterized by two quaternions ql, qr, ensuring valid rotation in the 4D space: = L(ql)R(qr). The rendering process follows the standard differentiable rasterization used in Gaussian splatting. Given pixel (u, v) at time t, the final rendered image xr is computed by blending the visible 3D Gaussians: xr(u, v, t) = (cid:88) i=1 pi(u, v, t)αici(d, t) i1 (cid:89) (1pj(u, v, t)αj), j=1 (2) where pi(u, v, t) is the probability density of the i-th Gaussian at pixel (u, v, t), αi represents its opacity, and ci(d, t) is the time-dependent color modeled by 4D spherical harmonics (4DSH) [35]. 3.2. Asynchronous Capture Scheme To capture 4D scenes, given set of cameras at different viewpoints {Pi}N i=1, the previous methods [2, 8] would synchronize the capture timing of cameras to capture images simultaneously, as shown in Fig. 1 (a). However, for high-speed motion, none of this cameras capture the information between two neighboring consecutive frames. The normal frame rate of cameras is less than or equal to 30FPS and 4D reconstruction may fail for large and complex motion that requires high frame rate [1, 8, 10]. To capture temporally denser information, we propose novel asynchronous capture scheme that enables cameras to start capturing at different time instants, as shown in Fig. 1 (b). Specifically, we put cameras into group (for example, = 2 in Fig. 1 (b)), starting at different times for the capture: the i-th camera starts capturing an image at the timestamp ti = (τ /K) + τ , where {1, 2, . . . , K} denotes the index of the camera within the group, and represents the frame index in the video sequence recorded at frame rate of 25 FPS. This design temporally staggers the capture timing of different cameras within the 1/25 second exposure intervals, effectively increases the frame rate of the camera system by factor of without introducing additional hardware costs. In the experiment, we choose = 4, which effectively pushes the frame rate to 100 FPS. 3.3. Artifact-fix Video Diffusion Model Although the asynchronous capture introduced in the previous section increase the effective frame rate by times to better capture highspeed movement, it also introduce additional artifacts. Particularly, at each time stamp, asynchronous capture reduces the number of viewpoints by times, resulting in floater artifacts in the 4D reconstruction as shown in Fig. 1 (c) due to the sparse view reconstruction challenges. Although there are some recent sparse view 3D reconstruction methods [4, 2729, 36] can leverage image diffusion priors to enhance reconstruction quality, directly applying them to 4D scenes still causes temporal inconsistency due to frame-wise independent processing. Therefore, we propose novel artifact-fix video diffusion model to solve this challenge in 4D sparse reconstruction. Particularly, we take pretrained video diffusion model and fine-tune on 4D reconstruction data to 4 3.4. 4D Reconstruction with Diffusion Prior With the artifact-fix video diffusion model M, we can improve the quality of 4D Gaussian model. As shown in Fig. 2, we first reconstruct an initial 4D Gaussian models from all the captured images. Then, for each training view, we render high-frame-rate video render, covering all timestamps observed by any of the cameras. Because the initial 4D Gaussian is reconstructed from sparse views, these videos render contain floater artifacts, but can provide the diffusion model with essential spatial viewpoint information and temporal object motion information. To remove those artifact, we then obtain the latent representation zrender Rcthw by applying Wan-VAE to render, and concatenate it with pure noise of the same shape along the channel dimension. This combined representation is then used to generate the refined video ˆV , which is clearer and sharper. Finally, the refined video ˆV is used to supervise the rendering process through the following loss function: Ldif = render ˆV 1 + Lp(V render, ˆV ) (4) where Lp is the perceptual distance LPIPS [39]. The effectiveness of this approach is illustrated in Fig. 1(c). The proposed asynchronous capture can recover fast-dynamic motion compared to conventional synchronous capture. However, due to the inherently sparse views at each time step, the learned Gaussian representation introduces noticeable artifacts. By applying our per-scene artifact-fix diffusion model to refine the Gaussian representation, these artifacts can be effectively suppressed, leading to improved final results. 4. Experiments 4.1. Implementation Details Our framework is illustrated in Fig. 2. We adopt 4D Gaussian Splatting (GS4D) [35] as the underlying 4D representation. To enhance visual quality, we build an artifact-correction video diffusion model based on Wan-I2V-14B [22], finetuning its DiT backbone with injected LoRA layers. The model is trained on 750 noisy-clean video pairs from the DNA-Rendering dataset [1], using learning rate of 104 and LoRA rank of 16. Next, we train the 4D Gaussian representation. The 4DGS model undergoes an initial optimization phase for 7k iterations. However, the output of this stage still contains noticeable artifacts. To mitigate these artifacts, we apply the trained artifact-fix diffusion model. In the subsequent 7k iterations, the optimization incorporates only with the refined videos produced by the diffusion model. Figure 3. Illustration of artifact-fix video diffusion model setup. Our model freezes all parameters in the network, except for the LoRA weights, to fine-tune video diffusion model. Precisely, we integrate Lora parameters into the DiT model. With LoRA rank designated as 16, this integration takes place in each transformer block. tailor for our tasks. The model takes the rendered video render RCT HW containing floater artifacts as input and generates temporally coherent, sharp, and outputs clean video ˆV RCT HW . This clean video will later be used to refined 4D Gaussian model, which we will discuss in Sec. 3.4. Below we introduce details of our model. Data curation. First, to train the model, we constructed dataset consisting of pairs of videos with artifacts and corresponding clean version as ground truth. Specifically, we asynchronously sub-sample the multi-view video along the temporal dimension, as shown in Fig. 2(a), to train 4D Gaussian Splatting (GS4D) model. The trained GS4D is then used to render videos at the original frame rate, producing noisy outputs containing reconstruction artifacts. These rendered videos are paired with the corresponding clean videos to form the training data for diffusion model fine-tuning. Fine-tuning. We build our artifact-fix model on top of the video diffusion model Wan2.1 [22] as shown in Fig. 3. To be specific, we use Wan-VAE to compress rendered video render and target video target into the latent space zrender, ztarget Rcthw. The noise latent ztarget and condition latent zrender are concatenated along the channel axis and then passed through the DiT model of Wan2.1. To repurpose the original image-to-video generation setting in Wan2.1 for video enhancement, we use all frames of zrender as conditioning to guide artifact repair across the entire sequence, as described in [22]. During fine-tuning, we freeze the encoder and decoder, and apply LoRA-based fine-tuning only to the DiT component. The training loss is defined as: Ltune = Eztarget,t,ϵ,zrender [(ϵθ(ztarget , t, zrender, ctex)ϵ)2 2] where ctex denotes an object-specific language prompt. (3) Figure 4. Qualitative result on the Neural3DV dataset [8]. Figure 5. Qualitative result on the DNA-Rendering dataset [1]. 4.2. Datasets We evaluate our method on multiple widely used multi-view datasets, including DNA-Rendering [2] and Neural3DV [8]. DNA-Rendering captures 10-second clips of dynamic human subjects and objects at 15 FPS using combination of 4K and 2K cameras. To induce large motions, we temporally subsample the dataset to one-fourth of its original frame rate. The test set consists of all frames from held-out view. Neural3DV captures multi-view videos at 30 FPS, each lasting ten seconds. We subsample the videos to one-twelfth of their original frame rate. For each scene, one view is held out for testing, while the remaining views are used for training. By applying temporal downsampling to existing 4D datasets, we effectively simulate large inter-frame motion and generate both synchronous and asynchronous capture settings within the same scene. This setup enables direct comparison between the two acquisition strategies under large-motion conditions. It is worth noting that the artifact-fix video diffusion model is trained on the DNA-Rendering dataset. We construct total of 750 noisy-clean video pairs for training, each with resolution of 1024 1024 and length of 25 frames. Capturing setup: Since this is the first work to employ an asynchronous capture strategy, no existing real-world 4D dataset has been recorded using such setup. To evaluate our method on real asynchronously captured scenes, we build custom multi-view camera array for 4D data acquisition. As illustrated in Fig. 6, the setup consists of 12 cameras operating at 25 FPS, all capable of hardware-synchronized triggering. The cameras are arranged at three different vertical levels, with four cameras evenly spaced at each level, approximately 22.5 degrees apart. Although all 12 cameras support synchronous triggering, we manually introduce varying trigger delays across cameras to enable asynchronous capture. By dividing the camera array into four or eight temporal groups, we effectively achieve capture rate of 100 FPS or 200 FPS, respectively, as discussed in Sec. 3.2. Benchmark for Asynchronous 4D Reconstruction Using the above camera array setup, we captured variety of 6 Table 3. Ablation study on DNA-Rendering dataset [2]. Higher values indicate better performance for metrics marked with (), while lower values are preferable for metrics marked with (). Capture Artifact-fix Sync. Sync. Async. Async. PSNR 24.75 24.15 26.23 26. SSIM LPIPS 0.797 0.800 0.831 0.845 0.337 0.331 0.315 0.293 4D reconstruction in large-motion scenarios. We also present quantitative comparisons on Neural3DV dataset [8] and DNA-Rendering dataset [2] in Fig. 4 and Fig. 5 respectively. The results show that our methods can produce higher fidelity renderings using the same size of input data. It becomes apparent that, due to insufficient input information in the temporal dimension, all comparison methods have generated severe artifacts and motion distortions, especially in regions of rapid motion. Specifically, methods like 4DGS and K-Planes, which estimate deformation field on top of static canonical representation, tend to become stuck at certain poses when lacking temporal supervision. In contrast, methods like GS4D, which treat temporal and spatial dimensions as an integrated whole, are prone to generating numerous artifacts in the absence of temporal supervision. Supporting this, the experimental results also prove that existing 4D representations are unable to automatically interpolate the temporal dimension to recover plausible motion information. Benefiting from the proposed asynchronous capture, our system hardware-wise obtains sufficient temporal supervision, enabling the reconstruction of plausible motion even in large-motion scenarios, as shown in Fig. 4 and Fig. 5. Furthermore, leveraging the intrinsic modeling capabilities of the 4D representation in the spatial domain and the diffusion prior, our method effectively mitigates artifacts caused by sparse views, achieving improved temporal-spatial consistency. The qualitative results on our real-captured dataset is shown in Fig. 7. Since we are unable to capture both synchronous and asynchronous videos from the same perspective at the same moment, we captured two separate sequences while maintain the same motion patterns to ensure fair comparison. The results show that, whether using 4DGS or GS4D, the asynchronous capture method helps the 4D reconstruction model recover more accurate geometry compared to the synchronous method. With the addition of the artifactfix model, our approach is further capable of reconstructing fast-moving regions (e.g., arms), non-linear motions, and complex texture areas (e.g., skirts) with high quality. Figure 6. Illustration of the capture system. Table 1. Quantitative comparison on the DNA-Rendering dataset [2]. Higher values indicate better performance for metrics marked with (), while lower values are preferable for metrics marked with (). Method K-Planes 4DGS [25] GS4D [35] Ours PSNR 22.74 23.09 24.75 26.76 SSIM LPIPS 0.750 0.772 0.797 0. 0.443 0.351 0.337 0.293 Table 2. Quantitative comparison on the Neural3DV dataset [8]. Higher values indicate better performance for metrics marked with (), while lower values are preferable for metrics marked with (). Method K-Planes 4DGS [25] GS4D [35] Ours PSNR 28.31 29.99 30.54 33.48 SSIM LPIPS 0.875 0.928 0.917 0. 0.211 0.252 0.178 0.134 high-speed motion scenes, including dancing, sports activities, and rapid object interactions such as waving chess piece. In total, we construct dataset consisting of 12 sequences of asynchronously recorded multi-view videos, focusing on non-linear and large-motion scenarios. Each video has resolution of 2048 2248 pixels. 4.3. Comparison Experiments For assessment, we use three metrics, encompassing peaksignal-to-noise ratio (PSNR), structural similarity index (SSIM) and perceptual quality measure (LPIPS). For baselines, we choose three state-of-the-art 4D reconstruction methods, including K-Planes [19], 4DGS [25] and GS4D [35]. We first conduct quantitative evaluate on two existing datasets, DNA-Rendering and Neural3DV, as shown in Table 1 and Table 2. Our method achieves the best results across fidelity metrics (PSNR and SSIM) as well as perceptual metrics (LPIPS). This demonstrates the effectiveness of asynchronous capture and artifact-fix model for high-frame-rate Figure 7. Quality result on our real-capture dataset. Figure 8. Ablation study on asynchronous capture and diffusion model (AF represents artifact-fix video diffusion model). 8 5. Discussion 5.1. Effect of Asynchronous Capture To evaluate the effectiveness of asynchronous capture, we compare it with synchronous capture on the DNA-Rendering dataset. Quantitative results are shown in Table 3, and qualitative comparisons are provided in Fig. 8. Under the original synchronous capture setup, the camera systems frame rate is too low to handle fast human motion, resulting in large displacements between adjacent frames. Moreover, the 4D representation is unable to interpolate along the temporal dimension to recover plausible motion. As result, the images rendered by the 4D Gaussian model exhibit poor visual quality (Fig.8(b)), and even the artifact-fix video diffusion model fails to generate satisfactory outputs (Fig.8(c)). In contrast, with asynchronous capture (Fig. 8(d)), although some artifacts remain due to sparse views, the reconstructed motion is more accurate. The results are clearly superior to those from synchronous capture, further demonstrating the effectiveness of asynchronous acquisition for high-frame-rate 4D reconstruction in large-motion scenarios. 5.2. Artifact-fix Video Diffusion Model Effectiveness of the artifact-fix video diffusion model. To demonstrate the effectiveness of our artifact-fix model, we compare rendering results with and without it. The comparison is presented in Table 3 and Fig.8(d)(e). Leveraging the strong spatiotemporal priors of the video diffusion model, our approach effectively removes floater artifacts caused by sparse views in asynchronous capture, restores fine textures, and eliminates temporal discontinuities in the rendered video. By optimizing with the diffusion loss (Eqn.4), we distill the diffusion prior into the 4D reconstruction pipeline, ultimately producing visually compelling results, as shown in Fig. 8(e). Comparing video diffusion with image diffusion. To verify that using video diffusion model as the backbone for the 4D artifact-fix task outperforms image-based diffusion, we train Stable Diffusion 1.5 model with the ControlNet Tile module [38] on the same dataset, following [32, 36]. The outputs of the two diffusion models are shown in Fig. 9. We observe that, beyond differences in facial generation quality due to the base models capabilities, the video diffusion model produces temporally consistent textures around the collarbone across adjacent frames. In contrast, the image diffusion model introduces noticeable texture variation in this region due to its frame-wise randomness. This temporal inconsistency poses major challenge to maintaining smooth motion in 4D reconstruction. Figure 9. Ablation study on video diffusion model and image diffusion model. video diffusion model. When the input to the artifact-fix video model is complexsuch as scenes in Fig. 10(b) where some regions lack texture while others contain dense artifactsthe model may struggle to distinguish between noise that should be removed and texture details that should be preserved. Consequently, its performance may become suboptimal. To better incorporate scene-specific information, we explore the idea of per-scene finetune the artifact-fix model. Specifically, we adopt leave-one-out strategy (similar to [32]) to construct noisy-clean video pairs from the input video, and then finetune the artifact-fix model on these pairs for small number of iterations. The results in Fig. 10(c)(d) demonstrates that per-scene fine-tuning enables the model to recover finer dress details, as the diffusion model leverages multi-view and temporal cues to learn 4D scene distributions. While this strategy improves performance in complex scenes, the leave-one-out construction introduces significant time overhead. Therefore, we do not adopt it as the default setting in this work. Nevertheless, we recognize it as promising direction for high-quality reconstruction and include it in our discussion to inspire future research. 5.4. Limitation Due to the inherent fidelity limitations of pre-trained diffusion models, some fine textures, despite the per-scene optimization, may still exhibit reduced fidelity after artifact removal. Future advancements in foundational models could further enhance the reconstruction accuracy of asynchronous capture. 5.3. Per-scene Finetuning Artifact-Fix Model 6. Conclusion To further improve visual quality, we also explored the idea of using per-scene optimization strategy for our artifact-fix In conclusion, we have presented novel asynchronous capture scheme and reconstruction pipeline to address the chal9 [3] Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, and Jonathan Li. Nerf: Neural radiance field in 3d vision, comprehensive review. arXiv preprint arXiv:2210.00379, 2022. [4] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 2, 4 [5] DaDong Jiang, Zhihui Ke, Xiaobo Zhou, Zhi Hou, Xianghui Yang, Wenbo Hu, Tie Qiu, and Chunchao Guo. Timeformer: Capturing temporal relationships of deformable 3d gaussians for robust reconstruction. arXiv preprint arXiv:2411.11941, 2024. 3 [6] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. compact dynamic 3d gaussian representation for real-time dynamic view synthesis. In European Conference on Computer Vision, pages 394412. Springer, 2024. 2 [7] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 4 [8] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55215531, 2022. 2, 3, 4, 6, 7 [9] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 2 [10] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 2, 3, 4 [11] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussianflow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2113621145, 2024. 2 [12] Dengyu Liu, Jinwei Gu, Yasunobu Hitomi, Mohit Gupta, Tomoo Mitsunaga, and Shree Nayar. Efficient space-time sampling with pixel-wise coded exposure for high-speed imaging. IEEE transactions on pattern analysis and machine intelligence, 36(2):248260, 2013. [13] Xinhang Liu, Jiaben Chen, Shiu-hong Kao, Yu-Wing Tai, and Chi-Keung Tang. Deceptive-nerf: Enhancing nerf reconstruction using pseudo-observations from diffusion models. 2023. 4 [14] Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, and Yuchao Dai. 3d geometry-aware deformable gaussian splatting for dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89008910, 2024. 2 [15] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 Figure 10. Ablation study on per-scene finetuning. lenge of fast-dynamic scene reconstruction. By introducing deliberate delays between cameras, our method effectively increases the frame rate, enabling more accurate modeling of intermediate motion information in fast-moving scenes. To address the viewpoint sparsity and reconstruction artifacts introduced by asynchronous capturing, we train videodiffusion-based artifact-fix model that significantly reduces these artifacts and enhances visual quality. Compared to previous sparse 3D reconstruction methods that employ image diffusion for refinement, our video diffusionbased approach effectively resolves the temporal inconsistency issues that arise in 4D reconstruction by leveraging spatiotemporal priors from video diffusion. Our method achieves superior performance in both real and synthetic scenarios, demonstrating its robustness across diverse conditions. Furthermore, we provide the first real-world asynchronous multi-view video dataset to advance research in high-speed 4D reconstruction. Our approach provides promising joint hardware-software solution to improve the accuracy of 4D reconstruction for fast-motion scenes, with potential applications in sports, autonomous driving, robotics, and VR/AR content creation."
        },
        {
            "title": "References",
            "content": "[1] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1998219993, 2023. 2, 3, 4, 5, 6 [2] Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu Chen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Bo Dai, and Kwan-Yee Lin. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. arXiv preprint, arXiv:2307.10173, 2023. 2, 4, 6, 7 10 [16] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54805490, 2022. 3 [17] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [18] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1031810327, 2021. 2 [19] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. 2, [20] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, MinSeop Kwak, Sungjin Cho, and Seungryong Kim. Darf: Boosting radiance fields from sparse input views with monocular depth adaptation. Advances in Neural Information Processing Systems, 36:6845868470, 2023. 3 [21] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2023. 3 [22] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 5 [23] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. 2 [24] Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. Advances in Neural Information Processing Systems, 37: 131316131343, 2025. 2 [25] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2031020320, 2024. 2, [26] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. 2 [27] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. arXiv preprint arXiv: 2503.01774, 2025. 4 [28] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reIn Proceedings of the construction with diffusion priors. IEEE/CVF conference on computer vision and pattern recognition, pages 2155121561, 2024. 2, 3 [29] Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf: Regularizing neural radiance fields with denoising diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 41804189, 2023. 2, 4 [30] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: RealIn CVPR, 2024. time 4d view synthesis at 4k resolution. 2 [31] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Realtime 4d view synthesis at 4k resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2002920040, 2024. 2 [32] Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Gaussianobject: Just taking four images to get high-quality 3d object with gaussian splatting. arXiv e-prints, pages arXiv2402, 2024. 9 [33] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82548263, 2023. [34] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033120341, 2024. 2 [35] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In International Conference on Learning Representations (ICLR), 2024. 2, 4, 5, 7 [36] Hanyang Yu, Xiaoxiao Long, and Ping Tan. Lm-gaussian: Boost sparse-view 3d gaussian splatting with large model priors. arXiv preprint arXiv:2409.03456, 2024. 2, 4, 9 [37] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural information processing systems, 35:2501825032, 2022. 3 [38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. 11 Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [39] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}