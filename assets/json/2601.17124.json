{
    "paper_title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
    "authors": [
        "Bin Lin",
        "Zongjian Li",
        "Yuwei Niu",
        "Kaixiong Gong",
        "Yunyang Ge",
        "Yunlong Lin",
        "Mingzhe Zheng",
        "JianWei Zhang",
        "Miles Yang",
        "Zhao Zhong",
        "Liefeng Bo",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 1 4 2 1 7 1 . 1 0 6 2 : r 2026-01-27 iFSQ: Improving FSQ for Image Generation with 1 Line of Code Bin Lin1,2, Zongjian Li1, Yuwei Niu1, Kaixiong Gong2, Yunyang Ge1,2, Yunlong Lin2, Mingzhe Zheng2, JianWei Zhang2, Miles Yang2, Zhao Zhong2, Liefeng Bo2, Li Yuan1, 1Peking University 2Tencent Hunyuan"
        },
        {
            "title": "Abstract",
            "content": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers theoretical bridge, yet vanilla FSQ suffers from critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with distribution-matching mapping to enforce uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ"
        },
        {
            "title": "Introduction",
            "content": "Image generation is currently bifurcated into two distinct paradigms: Autoregressive (AR) models that predict discrete image tokens (e.g., LlamaGen Sun et al. (2024)) and diffusion models that denoise continuous latent representations (e.g., DiT Peebles & Xie (2023)). This divide is deeply rooted in their tokenizers: AR relies on Vector Quantized-VAEs (VQ-VAE) Van Den Oord et al. (2017) for discrete codes, while diffusion relies on Variational Autoencoders (VAE) Kingma & Welling (2013) for continuous distributions. This fragmentation creates significant barrier to unified modeling and fair benchmarking. Specifically, it is difficult to disentangle whether performance differences stem from the generative models themselves (AR vs. Diffusion) or the distinct properties of their underlying tokenizers (VQ-VAE vs. VAE). To bridge this gap, we need unified tokenizer capable of producing both high-quality discrete tokens and continuous latents. Finite Scalar Quantization (FSQ) Mentzer et al. (2023) emerges as promising candidate. By replacing the complex learnable codebook of VQ-VAE with simple rounding operations, FSQ theoretically bridges the two worlds: its quantized values serve as continuous latents, while its rounded indices serve as discrete tokens. However, we identify critical flaw when applying vanilla FSQ to visual generation: mismatch between its equal-interval quantization and the non-uniform distribution of neural activations. As illustrated in fig. 1, this mismatch forces trade-off between reconstruction fidelity and information efficiency: High Fidelity, Low Efficiency (fig. 1 (a)): Vanilla FSQ uses equal-interval bins. Since neural activations typically follow bell-shaped (Gaussian-like) distribution, most data points crowd into the few central bins. While this dense sampling in the center yields low reconstruction error (MSE: 0.1678), lower reconstruction error is crucial for generating sharp images. However, it leaves edge bins severely underutilized. This activation collapse (Utilization: 83.3%) limits the effective size, hindering to learn diverse patterns. High Efficiency, Low Fidelity (fig. 1 (b)): Conversely, enforcing equal-probability for all bins maximizes information entropy (3.17 bits) and bin utilization (100%). However, to accommodate the gaussian tails, the outer bins must be excessively wide. This coarse quantization at the edges leads to significant precision loss (MSE rises to 0.1812), degrading the visual quality of reconstructed images. In this work, we propose iFSQ, tokenizer that unlocks the full potential of FSQ for generative modeling. Our key insight is simple yet effective: we replace the tanh function in original FSQ with distribution-matching activation that maps the unbounded gaussian latent space to bounded uniform distribution. Specifically, the tanh activation in original FSQ is replaced with = 2.0 σ(1.6x) 1, which is implemented in 1 line of code. This activation ensures that the quantization bins are utilized with equal probability (high information efficiency) while Work done during internship at Tencent Hunyuan. Corresponding Authors. 1 Figure 1: Empirical analysis of equal-probability and equal-interval quantization. Since neural network activations naturally follow gaussian-like distribution Lee et al. (2017), we begin our experiments under this setting. For panels (a)(f), we quantize the original data into 9 levels, which corresponds to an information entropy of 3.17 bits. We clip the original data to the range [3, 3] for visualization. Panel (a) shows equal-interval quantization of standard normal distribution. Panel (b) shows equal-probability quantization of standard normal distribution. Panel (c) shows equal-interval quantization of uniform distribution, which is also equal-probability quantization. For panels (a)(c), the area of each bin represents probability density. In panels (b) and (c), all bins have equal probability, while in panel (a), bins near the mean (0) have higher probability density. Panels (d)(f) plot the quantization error for each original value (x-axis), where denser regions use larger markers. maintaining equal intervals (high reconstruction fidelity). As shown in fig. 1 (c), this elegant property guarantees optimal efficiency (100% utilization) while simultaneously achieving superior fidelity (MSE: 0.1669). Crucially, iFSQ establishes fair and controlled benchmark for generative modeling. By using the exact same pretrained tokenizer for both paradigms, we eliminate the confounding variables introduced by distinct discretization methods (e.g., VQ-VAE vs. VAE). Through this unified lens, we uncover two insights regarding the scaling properties of visual generation. First, we identify that the trade-off between discreteness and continuity has an optimal equilibrium appearing at approximately 4 bits per dimension. Second, when comparing paradigms under identical reconstruction constraints, we observe distinct crossover: while AR models exhibit rapid convergence in early training, Diffusion models achieve superior performance ceiling with sufficient compute. This suggests that the strict sequential inductive bias of AR may limit the upper bounds of generation quality compared to the holistic refinement of diffusion. Beyond benchmarking, we adapt Representation Alignment (REPA) Yu et al. (2024) to autoregressive models to further enhance their performance, yielding LlamaGen-REPA. We observe that aligning the 8-th layer of LlamaGen with visual features yields the best results, consistent with findings in diffusion models. Notably, AR models require significantly stronger alignment regularization (λ = 2.0) compared to the standard λ = 0.5 used in diffusion. Our contributions can be summarized in three aspects as follows: Methodology: We propose distribution-aware improvement to FSQ. By transforming gaussian latents into uniform prior (implemented in 1 line of code), we resolve the conflict between information efficiency and reconstruction fidelity with simple, plug-and-play activation function. Benchmarking: We introduce iFSQ as unified tokenizer to benchmark AR against diffusion models. Our controlled experiments reveal that while AR excels in efficiency, diffusion dominates in peak generation quality, offering new guidance for model selection. Analysis & Extension: We conduct comprehensive study on the quantization spectrum, identifying that 4-bit representation serves as the sweet spot that balances the precision of continuous features with the compactness of discrete tokens. Furthermore, we successfully develop LlamaGen-REPA, identifying the critical role of alignment depth and loss weighting in balancing semantic alignment."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Visual Tokenization: Continuous vs. Discrete Image generation faces fundamental paradigm challenge, primarily bifurcated into diffusion models and autoregressive models. The core divergence lies in their choice of tokenizer: discrete versus continuous. Furthermore, in broader efforts to unify multi-modal architectures Han et al. (2025); Niu et al. (2025); Lin et al. (2025); Li et al. (2025c), the design of the tokenizer remains unconverged. (1) Continuous: Variational Autoencoders (VAEs) Kingma & Welling (2013) impose gaussian prior on the latent space to support the probabilistic requirements of diffusion models. To ensure high reconstruction fidelity, recent works Chen et al. (2025); Yao et al. (2025); Li et al. (2025b) incorporate adversarial losses (GAN) Goodfellow et al. (2020), perceptual losses (LPIPS) Zhang et al. (2018), and even discriminative feature supervision (e.g., DINO series Kang et al. (2023); Caron et al. (2021); Oquab et al. (2023); Simeoni et al. (2025)) to enhance semantic alignment. (2) Discrete and explicit codebook: AR models require discrete tokens, typically obtained via VQ-VAE Van Den Oord et al. (2017). VQ-VAE quantizes latents by looking up the nearest neighbor in learnable codebook. Despite its success, VQ-VAE suffers from codebook collapse and relies on the straight-through estimator for gradient approximation. While MAGVIT-v2 Yu et al. (2023) mitigates collapse via entropy regularization, the codebook lookup remains memory-intensive. Jukebox Dhariwal et al. (2020) re-initializes underutilized codes by monitoring code usage statistics. In addition, pretrained codebook initialization is widely adopted, such as applying k-means clustering to encoder features Zhu et al. (2024) or initializing codebook from LLM embeddings Han et al. (2025). (3) Discrete but implicit codebook: To circumvent the complexity and instability of learnable codebooks, Finite Scalar Quantization (FSQ) Mentzer et al. (2023) explores scalar quantization strategies, which project latent representations directly onto fixed, bounded grid via element-wise rounding. By eliminating the codebook lookup, FSQ simplifies the training dynamics and avoids codebook collapse. Moreover, FSQ is widely used for audio reconstruction (e.g., CoDiCodec Pasini et al. (2025), TTAE Parker et al. (2024) and Cosyvoice 2 Du et al. (2024)), image reconstruction (e.g., MANZANO Li et al. (2025a) and AToken Lu et al. (2025)) and video reconstruction (e.g., CS-FSQ Argaw et al. (2025), Videoworld Ren et al. (2025), ViDTok Tang et al. (2024) and Cosmos-discrete Agarwal et al. (2025); Liao et al. (2025)). 2.2 Neural Network Quantization Quantization is widely adopted to reduce the memory footprint and computational cost of large models. In the realm of Large Language Models (LLMs) Ahmed et al. (2025); Liu et al. (2024a); Achiam et al. (2023), Weight-only quantization methods like GPTQ Frantar et al. (2022) and AWQ Lin et al. (2024) are prevalent for accelerating inference. Other approaches, such as QAT Liu et al. (2024b), extend this to activation values. core theoretical challenge in this field is Rate-Distortion Optimization (RDO) Sullivan & Wiegand (2002), which seeks to minimize reconstruction error (Distortion) while minimizing the bit-rate (Rate) required for representation. Difference: However, these methods typically treat quantization as post-training compression step or an inference optimization technique. In contrast, our work integrates quantization directly into the tokenizer training phase, utilizing it as fundamental mechanism to modulate the discreteness of the latent representation rather than merely for hardware acceleration. 3 iFSQ section 3.1 outlines the FSQ pipeline, with additional context on autoregressive and diffusion tokenizers detailed in section A. Subsequently, section 3.2 presents our approach to enhance FSQ via one-line code change, validated by the qualitative and quantitative analyses in fig. 2. 3.1 Background: Latent Quantization via FSQ We adopt quantization workflow based on FSQ Mentzer et al. (2023). Given latent representation RNd, we first apply bounding function : [1, 1] (typically tanh) to constrain the value range. We define the quantization resolution with = 2K + 1 levels per channel, where the +1 term ensures the existence of an exact zero-center. The continuous latent is mapped to vector of discrete integer indices {0, . . . , 1}d through element-wise rounding. The quantization operation for the j-th dimension is formulated as: qj = round (cid:18) 1 2 (cid:19) ( (zj) + 1) (1) Figure 2: Empirical numerical study of 2.0 sigmoid(αx) 1.0. Sample 500k points from the standard normal distribution and compute the transformed distribution for several values of α. Panel (a) shows the probability density of 2.0 sigmoid(αx) 1.0 under different α values, and the panel (b) reports the similarity to uniform distribution measured by KS and RMSE as α varies. Notably, the case with tanh(α = 2.0) corresponds to the original FSQ. This maps the range [1, 1] to the integer set {0, . . . , 1}. For Latent Diffusion Models (Continuous): These models utilize the quantized feature vector zquant directly as the input. We map the indices back to the continuous value space [1, 1] via: zquant,j = (qj 1 2 ) 2 1 (2) This operation acts as lossy compression where zquant z. For Autoregressive Models (Discrete): These models require flat token representation. Unlike VQ-VAE which requires learnable codebook lookup, FSQ projects the vector of indices to single scalar index via bijective base-I expansion: = j= qj Ldj (3) Example: Consider 4-dimensional vector with = 1 (implying = 3). If the quantization yields = [2, 2, 1, 0], the unique codebook index is: = 2 33 + 2 32 + 1 31 + 0 30 = 75 The implicit codebook size is = Ld = (2K + 1)d. 3.2 Distribution Analysis and Optimization for iFSQ Clearly, for both diffusion and autoregressive models, the input feature distribution is intrinsically linked to the activation function (z). Typically, model features follow normal distribution. However, passing normal distribution through the tanh activation (employed in the original FSQ) yields non-uniform, bimodal distribution, as illustrated by the green curve in fig. 2 (a). Furthermore, we investigate the general form of the sigmoid function: s(x) = σ(αx) + (4) where setting = 2.0, α = 2.0, and = 1 renders it equivalent to tanh. Consequently, we sweep the parameter α to determine if specific value transforms the standard normal distribution s(x) into an approximate uniform distribution. Qualitative Analysis: As shown in fig. 2 (a), we visualize the probability density functions (PDF) for α {1.0, 1.3, 1.6, 2.0, 2.4}. As α transitions from 1.0 to 1.6, the distribution shifts from symmetric unimodal shape to uniform distribution. Conversely, as α increases from 1.6 to 2.4, the distribution becomes concave, forming bimodal structure. We observe that α = 1.6 (dark green solid line) most closely approximates the uniform distribution (grey dashed line). 4 Algorithm 1 Pseudocode of iFSQ in PyTorch-like style def iFSQ(z, levels): ''' z: visual feature map (B*H*W, D) levels: list or tensor defining levels per dim (L) ''' # 1. Bound input to [-1, 1] # We replace the tanh to achieve more uniform distribution. - = tanh(z) + = 2 * sigmoid(1.6 * z) - 1 # 2. Scale to the grid defined by levels # half_width corresponds to (L-1)/2 in Eq. (3) half_width = (levels - 1) / 2 z_scaled = * half_width # 3. Quantization with Straight-Through Estimator z_rounded = round(z_scaled) z_hat = z_rounded - z_scaled.detach() + z_scaled # 4. Normalization for diffusion in Eq. (4) z_q = z_hat / half_width # 5. Compute Indices for AR in Eq. (5) # basis: [Lˆ(d-1), ..., Lˆ0] z_ind = z_rounded + half_width basis = compute_basis(levels) indices = sum(z_ind * basis, dim=-1).long() return z_q, indices sigmoid: logistic sigmoid function; round: element-wise rounding. Quantitative Analysis: As depicted in fig. 2 (b), we employ the Root Mean Square Error (RMSE) Gauss (1877) and the Kolmogorov-Smirnov (KS) An (1933) statistic to quantify the proximity of the post-activation distribution to uniform distribution. RMSE: This metric measures the deviation between the quantized and target values. RMSE = (cid:118) (cid:117) (cid:117) (cid:116) 1 i=1 (xi ˆxi) (5) where is the number of samples, xi denotes the target value (ideal uniform), and ˆxi denotes the activated data point. KS: This statistic measures the maximum divergence between two probability distributions (e.g., the empirical distribution and the target distribution Q). It is defined as the supremum of the absolute difference between their Cumulative Distribution Functions (CDFs): DKS = sup FP(x) FQ(x) (6) Corroborating our qualitative findings, the quantitative metrics reveal clear trend: both RMSE and KS statistics reach their minima at α = 1.6, significantly outperforming the standard tanh baseline (α = 2.0). This confirms that α = 1.6 effectively transforms the gaussian input into near-uniform distribution within the bounded range. Theoretically, uniform distribution is optimal for the fixed-interval quantization employed by FSQ. It ensures that the static quantization bins are utilized with equal probability, thereby maximizing information entropy and mitigating the activation collapse observed in the original design. We designate this optimized formulation as iFSQ and empirically demonstrate in section 4.1.1 that this distributional alignment translates directly to superior image reconstruction quality. algorithm 1 presents the pseudo-code for iFSQ. Our method is remarkably straightforward, requiring only minimal modification to the activation function within the standard FSQ framework. Specifically, we replace the original bounding function, equivalent to 2σ(2z) 1 (i.e., tanh(z)), with the optimized form 2σ(1.6z) 1. This adjustment introduces no additional parameters or inference latency, serving as computationally free, plug-and-play module compatible with existing architectures. 5 Figure 3: Effect of α for iFSQ. In (a)(c), the x-axis denotes α. The primary y-axes show PSNR ( better), SSIM ( better), and LPIPS ( better), respectively. The secondary y-axis shows distribution metrics RMSE and KS (both better). The optimal choice at α = 1.6 is highlighted, and tanh performance at α = 2.0 is marked, which corresponds to the original FSQ."
        },
        {
            "title": "4 Experiments",
            "content": "We validate the effectiveness of iFSQ through extensive experiments in section 4.1. Additionally, we extend REPA to LlamaGen in section 4.2. 4.1 Experiments for iFSQ Specifically, we investigate the following questions for iFSQ and show the setup of tokenzier in section D: Methodology: For the image reconstruction and generation performance, does the latent representation of images encoded by iFSQ outperform discrete VQ-VAE, FSQ and continuous AE? (table 1, table 2) Benchmarking: Which performs better for image generation on unified tokenizer benchmark: autoregressive models or diffusion models?  (fig. 4)  Analysis: Can iFSQ achieve better balance between discrete and continuous reconstruction performance? (table 4, fig. 5, fig. 10). 4.1.1 iFSQ for Image Reconstruction Comparison between FSQ and iFSQ. In fig. 3, plot performance curves for α = {1.0, 1.2, 1.6, 1.8, 2.0, 2.4} with KS and RMSE overlaid. Notably, the case with α = 2.0 corresponds to the original FSQ. As α increases from 1.0 to 1.6, PSNR and SSIM increase while KS and RMSE decrease. We observe that iFSQ (α = 1.6) consistently outperforms the original FSQ (α = 2.0) across PSNR, SSIM, and LPIPS. As α increases from 1.6 to 2.4, KS and RMSE increase while PSNR and SSIM decrease. This pattern matches the analysis in the section 3.2: at α = 1.6, KS and RMSE reach minima and theoretical EfficiencyFidelity trade-offs are optimal. LPIPS attains its best at α = 2.4, but considering PSNR and SSIM, choose α = 1.6 as the optimal setting. Moreover, although training is conducted only on ImageNet, similar trends are observed on the COCO validation set, demonstrating the strong scalability and robustness of iFSQ. 4.1.2 iFSQ for Diffusion Image Generation Comparison between AE, FSQ and iFSQ. As shown in table 1, we do not use REPA for training DiT. Using iFSQ as the tokenizer yields better gFID (12.76) than AE (13.78), while iFSQ achieves 3 higher compression rate (96 vs. 24). The same trend appears in REPA, where iFSQ at 4 bits already reaches generation performance comparable to AE. Comparison of different bits within iFSQ. In table 1, iFSQ at 2 bits yields substantially worse gFID than AE (18.52 vs. 13.78 without REPA; 14.97 vs. 10.67 with REPA). As quantization level increases to 4 bits, iFSQ performance becomes comparable to AE. For bit > 4, iFSQ shows no consistent improvement and instead fluctuates. iFSQ at 58 bits does not differ markedly from AE. This pattern suggests 4-bit latent already captures most features, and enlarging the latent space does not necessarily yield faster convergence. 4.1.3 iFSQ for Auto-regressive Image Generation Comparison between VQ, FSQ and iFSQ. To accelerate experiments, we use LlamaGen-REPA for both iFSQ and VQ in table 2 with 256 spatial compression (introduced the background of the compression ratio in section B.) following the original LlamaGen. At the same latent dimension, autoregressive generation trained on VQ underperforms iFSQ, while iFSQ operates at lower bit rate. 6 Table 1: FID comparison on DiT-Large without CFG. All metrics evaluate on the ImageNet validation set. For AE, 16 bit denotes using 16-bit precision for inference. indicates lower is better. Iter. denotes training iterations. Numbers in () indicate with REPA performance. CR denotes the compression ratio. Table 2: FID comparison on LlamaGen-Large without CFG.. All metrics evaluate on the ImageNet validation set and we report gFID with LlamaGen-REPA. indicates lower is better. Iter. denotes training iterations. The 14 bit VQ donotes the LlamaGen-Large trains with 16,384 codebook size. Tokenizer Model CR Bit Iter. gFID Tokenizer Model Dim Bit Iter. gFID AE FSQ iFSQ iFSQ DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 DiT-L/2 24 96 96 192 128 96 76 64 54 48 16 4 4 2 3 4 5 6 7 100k 100k 100k 100k 100k 100k 100k 100k 100k 100k 13.78 (10.67) 13.38 (11.04) 12.76 (10.48) 18.52 (14.97) 14.51 (11.74) 12.76 (10.48) 14.35 (10.77) 15.02 (10.74) 12.80 (10.51) 14.06 (10.54) VQ FSQ iFSQ VQ iFSQ iFSQ iFSQ iFSQ iFSQ LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L LlamaGen-L 4 4 4 8 8 4 4 4 4 14 2 2 14 2 3 4 5 500k 500k 500k 500k 500k 500k 500k 500k 500k 33.90 32.48 31.09 29.91 26.02 29.02 28.07 29.12 32.60 Comparison of different bits within iFSQ. Larger bits (and thus larger codebooks) do not necessarily yield better results and performance peaks at 4 bits. Conjecture that as the codebook grows, the corresponding autoregressive model must also scale to provide sufficient capacity to predict such large codebook. 4.1.4 Training Efficiency Comparison As demonstrated in section 4.1.2 and section 4.1.3, iFSQ achieves performance comparable to AE and VQ-VAE in diffusion and autoregressive generation, respectively. Since iFSQ as both continuous latent and discrete index, it establishes fair platform for comparing diffusion and AR models by the same decoder reconstruction performance. As illustrated in fig. 4, we plot the performance scaling against computational resources for both models using the same tokenizer to investigate their training efficiency. For LlamaGen, the FLOPs for attention computation and value weighting are calculated as half of full attention. We observe that while diffusion models exhibit slower initial convergence compared to the rapid convergence of AR models, they eventually reach crossover point. Beyond this, diffusion models continue to achieve superior performance, whereas AR models show limited gains. This suggests that the strong sequential constraint is suboptimal for image generation. 4.1.5 Scaling Behavior of iFSQ Figure 4: Training Efficiency Comparison: DiT vs LlamaGen (FID vs Compute). At 256 resolution, DiT-Large and LlamaGen-L exhibit approximately 161.04G and 169.65G FLOPs, respectively. Both models employ optimal training configurations derived from ablation studies while sharing the same iFSQ. As shown in fig. 5, iFSQ achieves favorable trade-off across quantization levels: at high quantization levels performance approaches AE, while at low quantization levels performance remains substantially stronger than VQ. PSNR, SSIM, LPIPS, and FID are reported on ImageNet and COCO. All metrics exhibit similar trends: (1) at the same latent dimension, iFSQ performance improves as quantization level increases. iFSQ approaches AE around 4 bits, is nearly identical to AE at 78 bits, and iFSQ with 16-dim attains PSNR and SSIM that exceed AE. (2) at the same quantization level, larger latent dimensions improve performance for both iFSQ and AE. Notably, VQ-8dim performs worse than iFSQ-4dim or AE-4dim, suggesting that learning quantization scalar is easier than learning quantization embeddings. We also observe that at 2 bits, iFSQ with twice the latent dimension (iFSQ-2xdim) already surpasses AE at x-dim. This trend holds across quantization levels and latent dimensions, indicating strong scalability of iFSQ. Next, we analyze scalability further using compression ratio in section C. 7 Figure 5: Performance across quantization levels. We plot the performance of iFSQ and AE under different quantization levels, using larger markers to denote models with higher latent dimensionality. Each performance point (including AE) uses spatial compression factor of 64. The performance of AE is indicated by horizontal dashed lines, which train under mixed precision and use 16-bit precision to inference. 4.2 Experiments for LlamaGen-REPA Specifically, we investigate the internal dynamics of autoregressive models and the optimal configuration for representation alignment. We address the following questions: Quantitative Analysis: How do feature representations evolve layer-by-layer in autoregressive models? Do they exhibit clear transition from self-encoding to next-token prediction?  (fig. 6)  Semantic Acceleration: Can explicit feature alignment (REPA) effectively guide the model to acquire high-level semantics at earlier layers?  (fig. 7)  Ablation & Scaling: What are the optimal hyperparameters (target representation, alignment depth, and loss coefficient) for LlamaGen-REPA? Does the optimal alignment depth generalize across different model scales? (table 3, fig. 8, fig. 9) 4.2.1 Quantitative Analysis To elucidate the internal dynamics of autoregressive image generation models, we introduce three metrics to quantify layer-wise feature evolution. First, Self-Token Similarity (STS) measures the retention of input encoding by calculating the spatially aligned cosine similarity between the feature at layer l, denoted as hl, and the final output embedding hL: STSl = 1 N i=1 cos(h (i) , (i) ), (7) where represents the total number of tokens and h(i) denotes the feature at spatial position i. Second, to identify the transition to prediction state, we define Next-Token Similarity (NTS). The core mechanism of autoregressive modeling dictates that as the network deepens, the feature at the current position must evolve to predict the content of the next position + 1. Therefore, unlike STS which compares aligned positions, NTS measures the shifted similarity between the current layers features (from index 1 to 1) and the final output features (from index 2 to N): N1 i=1 rise in NTS indicates that the layer has shifted focus from encoding the current patch to anticipating the subsequent token, signaling the onset of the generation mode. 1 1 NTSl = (i+1) cos(h (i) , (8) ). Figure 6: Layer-wise analysis of LlamaGen models. The top row shows the evolution of STS, NTS, and CKNNA across normalized layer indices. The bottom row demonstrates the strong correlation between NTS and CKNNA, particularly at higher resolutions and model scales. The shaded regions indicate the 90% confidence intervals. Figure 7: Layer-wise CKNNA scores under different REPA alignment depths (1, 4, 8, and 12). The vertical dashed lines denote the alignment layers, and bubble sizes correspond to the training coefficient λ. The peak semantic alignment consistently synchronizes with the target alignment depth. Third, CKNNA assesses the alignment between the global semantics of the current layer and pre-trained DINOv2 features. Following the protocol in REPA Yu et al. (2024), we utilize CKNNA score to quantify this semantic correspondence without defining new formula. fig. 6 illustrates the qualitative and quantitative analysis of these metrics across different model scales (Large, XXLarge) and resolutions (256, 384). The top row reveals distinct trends: STS decreases as network depth increases, indicating gradual departure from the initial self-encoding state. Conversely, both NTS and CKNNA exhibit sharp increase in the middle-to-late layers. Notably, the layer index where NTS surgessignaling the onset of the prediction modesynchronizes highly with the rise in CKNNA. To rigorously validate this relationship, the bottom row of fig. 6 presents the linear fit between NTS and CKNNA. We observe strong positive correlation, with Pearson coefficients of = 0.47 (Large@256), = 0.79 (Large@384), and = 0.72 (XXLarge@384). This correlation becomes more pronounced as model scale and resolution increase, suggesting that the emergence of the next-token prediction capability is intrinsically linked to the acquisition of high-level semantic representations. These observations imply that autoregressive models undergo mode switch from self-encoding to next-prediction. Since the prediction phase aligns with high-level semantics, we identify clear motivation to accelerate this transition. Consequently, we adopt the REPA strategy to explicitly align intermediate model layers with pre-trained DINOv2 features. By leveraging the robust visual representations of DINOv2, we guide the model to attain the prediction-ready state earlier in the network, thereby enhancing training efficiency and generation quality. 9 Figure 8: Ablation study on alignment depth across different architectures and scales. The left panel shows LlamaGen (AR) results, and the right panel shows DiT (Diffusion) results. The annotations indicate the specific alignment layer relative to the total network depth (layer/total). The optimal alignment depth consistently scales to approximately 1/3 of the total network depth (highlighted in the optimal region), rather than remaining at fixed layer index. 4.2.2 Impact of Alignment Depth on Semantic Evolution To investigate the influence of the alignment depth on feature evolution, fig. 7 visualizes the CKNNA scores across layers for models aligned at depths 1, 4, 8, and 12. The vertical dashed lines indicate the specific layers where the REPA alignment is applied, while the marker size represents the magnitude of the alignment loss coefficient λ. We compute the scores at every second layer to track the progression of semantic similarity. Qualitatively, the results demonstrate that the REPA mechanism effectively controls the semantic trajectory of the model. We observe that the layer exhibiting the highest similarity to DINOv2 features consistently shifts to coincide with the enforced alignment depth. For instance, as shown in the third subplot, when alignment is applied at Layer 8, the feature representation achieves its maximal semantic overlap with DINOv2 precisely at this layer, regardless of the loss coefficient value. This indicates that the model successfully learns to accelerate the formation of high-level semantics to match the target depth. To complement this analysis, we provide quantitative evaluation of the performance improvements in section 4.2.3. 4.2.3 Ablation Studies Target representation for LlamaGen-REPA. As shown in table 3, we empirically apply the optimal REPA parameters on DiT to LlamaGen. We observe that aligning only the 8-th layer of LlamaGen-Large to the final-layer features of DINOv2-Base enables the optimal diffusion-model configuration to also accelerate convergence in the autoregressive image generation model. However, the autoregressive model must predict the next token for each token, which differs from the diffusion model that always encodes itself. Therefore, we next conduct an ablation study on which layer of the model should align with visual features. Alignment depth for LlamaGen-REPA. In table 3, we fix the coefficient of the alignment loss and perform ablations at layers 3, 4, 8, 12, 16, and 20 (out of 24 layers). We observe that performance improves with depth and peaks at layer 8. Beyond layer 8, performance gradually degrades, which we attribute to the model shifting toward next-token prediction, while per-token alignment with DINOv2 introduces mismatch. Table 3: Component analysis of LlamaGen-REPA. All models train for 500k iterations on LlamaGenLarge. All metrics evaluate on the ImageNet validation set. indicates lower is better. Iter. denotes training iterations. Coeff. denotes the loss coefficient of the feature-alignment branch. Model Target Repr. Depth Coeff. Iter. gFID LlamaGen-L LlamaGen-L DINOv2-B - LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B LlamaGen-L DINOv2-B - 8 3 4 8 12 16 20 8 8 8 - 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 2.0 3.0 500k 20.14 500k 17.17 500k 18.61 500k 18.57 500k 17.17 500k 18.32 500k 18.74 500k 19.85 500k 17.17 500k 16.49 500k 16.17 500k 17. Additionally, to address the ambiguity regarding how REPA alignment depth generalizes across model scales, we extend the experimental validation to both autoregressive and diffusion architectures of varying sizes. fig. 8 presents the best FID scores for LlamaGen (left) and DiT (right) on Large, XLarge, and XXLarge configurations. 10 Contrary to the assumption of fixed optimal layer, our results reveal proportional scaling law. While the Large model (24 layers) performs best when aligned at layer 8, this absolute value does not transfer to deeper networks. Instead, we observe that the optimal alignment depth consistently corresponds to approximately one-third of the total layers (e.g., 12/36 for XLarge and 16/48 for XXLarge). This pattern holds true for both autoregressive and diffusion paradigms, suggesting generalized heuristic for applying REPA to larger models: alignment should be performed at the 1/3 depth mark. Effect of λ for LlamaGen-REPA. As shown in table 3, we also test whether feature alignment in the AR model is sensitive to the alignment coefficient. Results show that LlamaGen-REPA benefits from larger λ (e.g., 2.0), which differs from DiT-REPA. We attribute this to the strong inductive bias introduced by the teacherforcing training scheme in autoregressive models. Additionally, we analyze the performance of different λ across layers in fig. 9. Outliers typically correspond to the first checkpoint, after which the model gradually converges. Lower boxes indicate better performance, while flatter boxes indicate faster convergence. We find that λ = 2.0 also achieves the best performance when the alignment depth reaches its optimum (8-th layer). Since this experiment uses large model, the eighth layer corresponds to one-third of the total depth (24 layers), which is consistent with our earlier findings."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 9: Extended Experiments on λ. During training, we record all evaluation FID scores and plot them as boxplots. In this work, we unify the disparate paradigms of Diffusion and Autoregressive models through the lens of activation quantization. By introducing tailored activation function, we successfully transform latent distributions into Uniform prior, mathematically resolving the inherent trade-off between reconstruction fidelity and information efficiency. This formulation yields the iFSQ tokenizer, which demonstrates that the optimal equilibrium between discrete and continuous modalities resides at approximately 4 bits. Leveraging iFSQ as controlled benchmark, we reveal distinct scaling behaviors: while Autoregressive models offer rapid initial convergence, Diffusion models demonstrate superior performance ceiling. These findings suggest that the strict sequential constraints of autoregression limit ultimate generation quality."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Imtiaz Ahmed, Sadman Islam, Partha Protim Datta, Imran Kabir, Naseef Ur Rahman Chowdhury, and Ahshanul Haque. Qwen 2.5: comprehensive review of the leading resource-efficient llm with potentioal to surpass all competitors. Authorea Preprints, 2025. Kolmogorov An. Sulla determinazione empirica di una legge didistribuzione. Giorn Dellinst Ital Degli Att, 4: 8991, 1933. Dawit Mureja Argaw, Xian Liu, Joon Son Chung, Ming-Yu Liu, and Fitsum Reda. Mambavideo for discrete video tokenization with channel-split quantization. arXiv preprint arXiv:2507.04559, 2025. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In Forty-second International Conference on Machine Learning, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Carl Friedrich Gauss. Theoria motus corporum coelestium in sectionibus conicis solem ambientium, volume 7. FA Perthes, 1877. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as dialect: Unifying visual understanding and generation via text-aligned representations. arXiv preprint arXiv:2506.18898, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Bernd Jahne. Digital image processing. Springer, 2005. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1012410134, 2023. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017. Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, et al. Manzano: simple and scalable unified multimodal model with hybrid vision tokenizer. arXiv preprint arXiv:2509.16197, 2025a. Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1777817788, 2025b. Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Feize Wu, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, et al. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025c. Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, et al. Langbridge: Interpreting image as combination of language embeddings. arXiv preprint arXiv:2503.19404, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. 12 Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 467484, 2024b. Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, and Yinfei Yang. Atoken: unified tokenizer for vision. arXiv preprint arXiv:2509.14476, 2025. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. Yuwei Niu, Weiyang Jin, Jiaqi Liao, Chaoran Feng, Peng Jin, Bin Lin, Zongjian Li, Bin Zhu, Weihao Yu, and Li Yuan. Does understanding inform generation in unified multimodal models? from analysis to path forward. arXiv preprint arXiv:2511.20561, 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Julian Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, and Xubo Liu. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. Marco Pasini, Stefan Lattner, and George Fazekas. Codicodec: Unifying continuous and discrete compressed representations of audio. arXiv preprint arXiv:2509.09836, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. Videoworld: Exploring knowledge learning from unlabeled videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2902929039, 2025. Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Gary Sullivan and Thomas Wiegand. Rate-distortion optimization for video compression. IEEE signal processing magazine, 15(6):7490, 2002. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1570315712, 2025. Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. Advances in Neural Information Processing Systems, 37:1261212635, 2024. 13 Background: Tokenizer for Generation Tokenizer Architecture. The tokenizer comprises an encoder and decoder. The encoder compresses images into latent space, while the decoder reconstructs the latent representation back to the pixel space. Given an input image RHW3, the encoder maps it to latent representation Rhwd typically via 8 or 16 downsampling. Subsequently, the decoder upsamples to reconstruct the image ˆx. Tokenizer for Diffusion Models. Modern diffusion models predominantly perform forward noising and reverse denoising within the latent space. Consider latent distribution pdata(x) and noise ϵ (0, 1). By sampling timestep [0, 1], the intermediate state is obtained via linear interpolation as zt = tx + (1 t)ϵ. The diffusion model predicts the velocity = ϵ by minimizing the following loss function: = Et,x,ϵ (cid:104) vθ(zt, t) v2(cid:105) (9) where vθ denotes the neural network parameterized by θ. Tokenizer for Autoregressive Models. As autoregressive models require discrete indices, quantization layer follows the encoder of tokenizer. This layer quantizes the latent representation Rhwd into sequence of indices NN, where = w. The autoregressive model predicts the subsequent token conditioned on the preceding tokens by minimizing the cross-entropy loss: = EI (cid:34) k=1 log pθ(ik i1, . . . , ik1) (cid:35) (10) where pθ represents the probability distribution predicted by the neural network parameters θ, and ik denotes the k-th token in the index sequence. Background: Compression Ratio Analysis To theoretically quantify the efficiency of different tokenizers, we analyze their Compression Ratio (CR). We define CR as the ratio of the raw image bit-rate to the latent representation bit-rate. Consider an input image RHW3 stored in 8-bit RGB format. The total input size in bits is Sinput = 3 8 = 24HW. We assume spatial downsampling factor (typically = 8 or 16), resulting in latent spatial resolution of w, where = H/ and = W/ . The channel dimension is denoted by d. Continuous VAE: Standard VAEs represent latents as continuous floating-point vectors. Assuming standard 16-bit floating-point precision (FP16 or BF16), the latent size is SVAE = 16. The compression ratio is: CRVAE = 24HW 16 = 24 2 16d = 3 2 2d (11) Due to the high bit-depth of floating-point numbers, VAEs typically exhibit lower compression ratio, prioritizing reconstruction fidelity over storage efficiency. VQ-VAE: VQ-VAEs quantize the latent vector into discrete indices from learnable codebook of size (e.g., 1024 or 8192). Each spatial location is represented by single index, requiring log2(C) bits. The latent size is SVQ = log2(C). The compression ratio is: CRVQ = 24HW log2(C) = 24 2 log2(C) (12) VQ-VAEs achieve significantly higher compression ratios than VAEs, making them suitable for modeling long sequences in AR tasks. iFSQ (FSQ): FSQ does not rely on fixed-size explicit codebook. Instead, it employs an implicit codebook defined by the number of levels per dimension d. The equivalent codebook size is = Ld. The total bits required to represent the scalar index is log2(Ld) = log2(L). The compression ratio is formulated as: CRiFSQ = 24HW log2(L) = 24 2 log2(L) (13) Comparison: iFSQ (FSQ) offers flexible trade-off. By adjusting and d, iFSQ (FSQ) can match the high compression ratio of VQ-VAE (e.g., setting log2 log2 CVQ). Crucially, unlike VAEs that require 16 bits per channel, iFSQ (FSQ) typically requires only few bits (e.g., = 5 = 2.3 bits) per channel, yet maintains the structural properties of continuous space before rounding. 14 Figure 10: Scalability of iFSQ. We plot the scaling law of performance with respect to the compression ratio. Each performance point (including AE) uses spatial compression factor of 256. The performance of AE is indicated by , which train under mixed precision and use 16-bit precision to inference. The compression ratio (x-axis) is on logarithmic scale and compression ratio of VQ ( plotted in figure) is about 438."
        },
        {
            "title": "C Scaling of Compression Ratio with iFSQ",
            "content": "In fig. 10, we retrain iFSQ and AE under 256 spatial compression setting to match the standard VQ configuration (16 compression in height and 16 in width). We observe that on log-scale compression ratio, all models exhibit approximately linear performance growth or decay as compression ratio changes. clear optimal knee point emerges around 48 compression (4 bits). We also plot VQ data points () on the figure and find that VQ lies almost exactly on the same scaling trend, providing strong evidence that iFSQ serves as compromise between discrete and continuous representations."
        },
        {
            "title": "D Tokenizer Setup",
            "content": "Tokenizer Dim PSNR SSIM LPIPS rFID Table 4: Performance comparison of tokenizer baselines. All metrics evaluate on the ImageNet validation set. indicates lower is better. indicates higher is better. Implementation details: For (V)AE and VQ-VAE, we follow the latent diffusion architecture. To ensure fair comparison, all tokenizers are trained for 25 epochs on ImageNet 256256 Deng et al. (2009). LPIPS loss Zhang et al. (2018) coefficient is set to 0.1. We use the Adam optimizer Kingma (2014) with constant learning rate of 0.001. For the model configurations of the diffusion and autoregressive models, we strictly adhere to the setup in DiT Peebles & Xie (2023) and LlamaGen Sun et al. (2024). To accelerate the diffusion model experiments, we use the ablation parameters from LightingDiT Yao et al. (2025), with all diffusion models running at batch size of 1024 and 100k iterations. The autoregressive models retain the original settings and are trained for batch size of 256 and 500k iterations. To ensure the validity of the conclusions, all experiments are conducted on large models, such as DiT-Large or LlamaGen-Large. VAE-f8 AE-f8 AE-f8 27.459 27.934 30.437 0.799 0.813 0. 1.998 1.733 1.661 0.106 0.103 0.065 VQ-f16 VQ-f16 9.626 16.928 22.903 21.955 26.704 26. VQ-f8 VQ-f8 0.111 0.118 0.613 0.564 2.277 2.084 0.770 0.780 0.206 0. 4 4 8 8 4 8 4 Evaluation: For the tokenizer reconstruction results, we report PSNR Jahne (2005), SSIM Wang et al. (2004), LPIPS Zhang et al. (2018), and Frechet Inception Distance (rFID for reconstruction) Heusel et al. (2017). To verify potential overfitting on ImageNet Deng et al. (2009), we additionally report reconstruction performance on COCO2017 Lin et al. (2014) in fig. 3, fig. 5 and fig. 10. For generated results, we report gFID (FID for generation). For diffusion models, we always use the Euler method for image generation, following LightingDiT, with the default number of function evaluations set to 250. For autoregressive models, we follow the inference settings of LlamaGen. Since discrete tokenizers typically impose no additional distributional constraints on the latent space, whereas 15 continuous tokenizers usually apply KL divergence regularization, we further conduct an ablation on whether to impose distributional constraints for continuous tokenizers. Tokenizer Baselines: As shown in table 4, we present the performance of all tokenizers under the same settings. We observe that the continuous VAE reconstruction performance is inferior to that of AE. Considering that MAETok Chen et al. (2025) and VA-VAE Yao et al. (2025) demonstrate that the convergence of diffusion models does not depend on the constraint that the latent space maintains standard normal distribution, we use the most standard continuous AE (without KL loss constraint) as the tokenizer for the diffusion model."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tencent Hunyuan"
    ]
}