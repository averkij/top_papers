{
    "paper_title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
    "authors": [
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Honglin Lin",
        "Xin Gao",
        "Yu Li",
        "Zinan Tang",
        "Conghui He",
        "Rui Yan",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 7 0 1 2 . 9 0 5 2 : r ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Qizhi Pei1,2,, Zhuoshi Pan2,3,, Honglin Lin2 Xin Gao2 Yu Li2 Zinan Tang2 Conghui He2, Rui Yan2,4, Lijun Wu2, 1Gaoling School of Artificial Intelligence, Renmin University of China, 2OpenDataLab, Shanghai Artificial Intelligence Laboratory, 3Tsinghua University, 4School of Artificial Intelligence, Wuhan University Large Reasoning Models (LRMs) have shown impressive capabilities in complex problemsolving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between Thinking and NoThinking modes. We then train specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields substantial performance increase of 11.3% compared to the original dataset and achieves 65.9% average accuracy on AIME24, AIME25, HMMT-Feb25, BRUMO25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the costefficient Qwen3-8B model as teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. We open-source the ScaleDiff-Math dataset, the fine-tuned ScaleDiff model, and the implementation code to facilitate further research and ensure reproducibility. Date: September 26, 2025 Correspondence: Conghui He (heconghui@pjlab.org.cn), Rui Yan (ruiyan@ruc.edu.cn), and Lijun Wu (wulijun@pjlab.org.cn) Model & Data: ScaleDiff HuggingFace Collection Code: ScaleDiff Github Repository"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements in Large Reasoning Models (LRMs) such as OpenAI-o1 [1] and DeepSeek-R1 [2] have demonstrated remarkable progress in tackling complex reasoning problems. These models exhibit the ability to perform trial-and-error, self-reflection, and iterative refinement within long *Corresponding Authors. Equal Contribution. Work during internship at Shanghai Artificial Intelligence Laboratory. 1 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Figure 1: Performance of ScaleDiff on AIME24, AIME25, and HMMT-Feb25, compared to other SFT-based LRM baselines. Chains of Thought (CoT), leading to enhanced problem-solving capabilities. To replicate this success, various efforts have been made, employing techniques like Supervised Fine-Tuning (SFT) on distilled data [3, 4, 5, 6], Reinforcement Learning (RL) with verifiable rewards [7, 8, 9, 10], or more complex training pipelines based on SFT and RL [11, 12, 13, 14, 15]. common strategy among these approaches is to identify challenging mathematical problems from existing datasets for training [4, 12, 14]. The rationale behind this is that difficult problems typically necessitate intricate reasoning processes, thereby stimulating more sophisticated model behaviors, whereas simpler problems often yield limited benefits. However, creating such difficult mathematical problemsparticularly those at the competition or olympiad levelis often costly because they are primarily handcrafted by human experts [16, 17, 18]. Recent research has explored the automated synthesis of mathematical data by prompting proprietary models as well as large-scale open-source counterparts, either from seed data [19, 20, 21, 22] or from inherent mathematical concepts [23, 24, 25, 26, 27]. However, scaling these approaches remains challenging due to their substantial computational costs, complex prompting design, and relatively limited difficulty of the generated problems. To further investigate the impact of difficult problems on enhancing complex reasoning abilities of LRMs, we propose ScaleDiff, simple yet effective pipeline that scales the creation of difficult problems to improve models complex reasoning capabilities. We begin by leveraging an existing adaptive thinking model [28], which can automatically switch between the Thinking and NoThinking modes depending on the difficulty of given problem, thereby serving as difficult problem identifier to detect difficult problems within existing datasets. This identification process requires only single forward pass, making it more efficient than commonly used approaches such as fail rate and LLMas-a-judge. Subsequently, to enable the generation of an arbitrary number of difficult problems, we train problem generator (denoted as DiffGen-8B) on these identified difficult problems. We then utilize DiffGen-8B to generate large-scale new difficult problems, eliminating the need for complicated prompting design, per-instance shot selection, and the substantial computational costs required by traditional methods. For each generated problem, we distill its long CoT solution using Qwen3-8B [29] in Thinking mode. This comparatively small model provides solutions in cost-efficient manner and offers favorable alternative to widely used larger models such as DeepSeek-R1 or QwQ-32B. We also apply both rule and model filtration to these solutions. The final ScaleDiff-Math dataset is composed of these difficult problem-solution pairs and the original dataset. Further SFT of Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset demonstrates promising performance. Our ScaleDiff consistently outperforms recent strong LRMs such as OpenThinker3 [6] 2 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning and AceReason-Nemotron [13] across AIME24, AIME25, HMMT-Feb25, BRUMO25, and MATH500 on average. It also improves upon AM-Qwen3-Distilled-7B by enhancing both the difficulty and scale of the training data, resulting in relative performance gain of 11.3%. These results highlighting the effectiveness of our approach in strengthing models complex reasoning abilities. Moreover, by varying the size of the augmenting dataset, we observe clear scaling phenomenon in model performance on AIME24 and AIME25, with accuracy improving as the number of difficult problems increased. This scaling behavior further highlights the potential of our method to drive continued gains as larger and more challenging datasets become available."
        },
        {
            "title": "2 Method",
            "content": "In this section, we first introduce our identification of difficult problems in Section 2.1. We then detail our approach for generating large-scale set of new challenging problems in Section 2.2. Finally, in Section 2.3, we describe our process for distilling and filtering high-quality solutions to these generated problems, which forms the basis of our training dataset. The overview of our ScaleDiff pipeline is shown in Figure 2."
        },
        {
            "title": "2.1 Difficult Problem Identification",
            "content": "Assessing the difficulty of mathematical problems primarily relies on two existing methods: fail rate [30] and LLM-as-a-judge [31]. Specifically, for fail rate, proxy mathematical model is used to solve given problem multiple times, and the proportion of incorrect responses determines its fail rate. For LLM-as-a-judge, more powerful LLM is prompted with the mathematical problem, its reference solution (if available), and predefined criteria for difficulty assessment. However, both methods have their limitations: the fail rate is computationally inefficient as it necessitates multiple solution attempts by the proxy mathematical model; LLM-as-a-judge is highly sensitive to the specific rules and criteria predefined within the input prompt. Different from existing methods, we seek to leverage AdaptThink* [28] as our difficult problem identifier. AdaptThink algorithm is designed to teach models to adaptively choose between time-consuming Thinking process for complex problems and direct NoThinking response for simpler ones through RL. This adaptive mechanism inherently reflects the models perceived difficulty of problem. The primary objective of AdaptThink is constrained optimization: max (x,)D,yπθ (x) I(y1 = </think>), s.t. (x,)D,yπθ (x)R(x, y) (x,)D,yπθre (x)R(x, y), (1) where denotes the problem-solution dataset, and we let = {x (x, y) D} denote its problem set. I(y1 = </think>) is an indicator function for the first generated token being </think>. R(x, y) is the reward function representing the accuracy of the models response for problem (returning 1 for correct solution and 0 for an incorrect one). This objective aims to maximize the probability of generating NoThinking responses, subject to constraint: the current models expected accuracy the reward for correct solution R(x, y)must be maintained at or above that of fixed reference model. This design compels AdaptThink to opt for the efficient NoThinking mode only when it does not compromise accuracy. Conversely, for problems where NoThinking would lead to significant performance drop, AdaptThink is driven to engage its Thinking mode to satisfy the accuracy constraint. This adaptive behavior effectively transforms AdaptThink into binary classifier for problem difficulty. We define problem as simple if AdaptThink produces NoThinking response, and difficult otherwise. Formally, our problem identification criteria are based on the first generated token (y1) of *https://huggingface.co/THU-KEG/AdaptThink-7B-delta0.05 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Figure 2: Overview of ScaleDiff pipeline. AdaptThinks response: Difficulty(x) = (cid:40)"
        },
        {
            "title": "Simple\nDifficult",
            "content": "if y1 = </think> if y1 = </think> (2) Notably, determining whether problem is simple or difficult requires only the models output of first token (one forward pass), making the identification process highly efficient than fail rate and LLMas-a-judge. By applying AdaptThink as such identifier, we efficiently identify and extract challenging problem-solution pairs from existing datasets, forming curated subset denoted as DDiff."
        },
        {
            "title": "2.1.1 Effectiveness of Difficulty",
            "content": "To evaluate the validity of using AdaptThink as difficult problem identifier and the effectiveness of difficult problems as training data, we conduct SFT on Qwen2.5-Math-7B-Instruct [32] with the full dataset D, as well as its two subsets: the identified difficult subset DDiff and simple subset DSimp. To further control for data size, we downsample DSimp to match the size of DDiff and additionally construct random subset DRand by sampling 192K problems from D. We use the AM-Qwen3-Distilled dataset as our initial D, with more experimental details provided in Section 3.1."
        },
        {
            "title": "D\nDSimp\nDSimp\nDRand\nDDi f f",
            "content": "558K 366K 192K 192K 192K AIME24 AIME25 HMMT-Feb25 BRUMO25 MATH500 avg@10 63.03.5 43.75.3 40.74.9 54.35.0 62.35.0 avg@10 51.75.6 38.74.5 33.72.8 42.05.2 44.37.6 avg@10 33.35.8 27.05.3 24.03.6 31.34.8 36.05.7 avg@10 60.77.7 53.76.0 48.37.8 57.04.6 59.06.3 avg@3 94.60.4 91.30.4 90.40.7 93.20.4 93.91."
        },
        {
            "title": "AVG",
            "content": "59.2 48.9 45.1 53.3 56.6 Table 1: Effect of problem difficulty on SFT performance across three mathematical benchmarks. The corresponding data size and results on three mathematical benchmarks are presented in Table 1. It can be observed that training on the difficult subset DDiff (192K) outperforms training on the simple subset DSimp (56.6 vs. 45.1 on average) and randomly sampled subset DRand (56.6 vs. 53.3) of the same size , highlighting that difficult problems provide more effective training signals for enhancing 4 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning reasoning ability. Even when comparing DDiff (192K) against the much larger DSimp (366K), the difficult subset still yields clear advantage (56.6 vs. 48.9). Moreover, while training on the full dataset (558K) achieves the strongest results overall (59.2), this improvement stems primarily from its larger scale. Notably, the performance gap between DDiff (192K) and the full dataset is only 2.6 points (56.6 vs. 59.2), despite the latter being nearly three times larger. In contrast, the gap between DSimp (192K) and the full dataset is approximately 14 points (45.1 vs. 59.2), underscoring that simple problems contribute far less effectively to improving model performance compared to difficult ones. These findings confirm that AdaptThink serves as an effective identifier for identifying high-value difficult problems, and that such problems are significantly more beneficial than simple ones in improving model performance."
        },
        {
            "title": "2.2 Difficult Problem Generator",
            "content": "Building upon the identified difficult problem set PDiff from DDiff in Section 2.1, we train dedicated difficult problem generator, denoted as DiffGen-8B, following similar methodology to ScaleQuest [33]. The rationale for training exclusively on difficult problem sets derives from Section 2.1.1, which demonstrates that difficult problems are more effective than simple ones in improving model performance. For each problem = (x1, x2, . . . , xn) in PDiff, where xi denotes the ith token of the problem, we construct the input by prepending standard instruction prefix = <im start>usern. Distinct from conventional instruction tuningwhere the loss is often computed over solution tokensour training loss is only applied to the problem tokens xi without solution as input. The training objective for DiffGen-8B is the standard cross-entropy loss for language modeling: LCE(θ) = 1 i=1 log P(xiI, x1, . . . , xi1). (3) This design encourages DiffGen-8B to capture the distributional patterns inherent in challenging mathematical problems. Importantly, the goal is not to optimize for problem solving, but rather to enable the model to generate new problems of comparable complexity. Given the instruction prefix I, the trained generator DiffGen-8B can produce large number of new difficult problems by adjusting decoding parameters such as temperature and top-p. The resulting collection of generated problems constitutes our problem set, denoted as PDiffGen."
        },
        {
            "title": "2.3 Solution Distillation and Filtration",
            "content": "After generating the problem set PDiffGen, we re-evaluate their difficulty using the methodology introduced in Section 2.1. The validation shows that about 88% of the generated problems are classified as difficult, suggesting that DiffGen-8B effectively captures the distributional characteristics of challenging problems. Since assessing the mathematical correctness and solvability of generated problems remains highly non-trivial task, we leave this aspect as future work and focus instead on ensuring the quality of distilled solutions. For each problem in PDiffGen, we utilize strong teacher model to distill its corresponding solution, resulting in ˆDDiffGen. Upon obtaining these solutions, we perform two-stage filtration process: rule and model filtration. The initial rule filtering stage removes solutions with common undesirable traits. This includes cases with extensive repetition or overly verbose reasoning that prevents the final answer from being clearly encapsulated within boxed{}. The model filtering further refines the dataset by discarding problems that our base model can already solve reliably. Specifically, if the base models predicted answer matches the teacher-provided answer, the problem is considered redundant and removed from the dataset. In total, we filter out approximately 43% of the initial samples and obtain the final problem-solution dataset DDiffGen. 5 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning"
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Initial Dataset D. We utilize the mathematical domain subset of the AM-Qwen3-Distilled dataset as our initial dataset D, which is well-regarded for its high quality and has demonstrated effectiveness in training mathematical reasoning models. Its problem set is compilation of several prominent sub-datasets, including DeepMath-103K [10], OpenR1-Math-220K [11], OpenMathReasoning [4], and NuminaMath [34], among others. Subsequently, undergoes rigorous deduplication, rule filtering, and decontamination concerning downstream tasks. The original solutions in are distilled from Qwen3-235B-A22B [29]. This distillation process is iteratively repeated until correct solution is obtained. Further filtering is also conducted based on metrics such as perplexity (PPL) and Ngram scores [3]. This multi-stage processing results in final curated dataset comprising 558K data instances. Training of DiffGen-8B. Following the identification process described in Section 2.1, 192K problems from are classified as difficult, denoted as PDiff. We train DiffGen-8B based on the Qwen3-8BBase [29] model. The training configuration consists of batch size of 128, maximum sequence length of 1024 tokens, learning rate of 5e-5, and total of 1 epoch. We employ 10% warmup steps with linear decay learning rate schedule. We use LLaMA-Factory [35] as our training framework. Construction of DDiffGen. Upon completion of training, we use DiffGen-8B to generate the PDiffGen. Generation parameters are set to temperature of 1.0, top-p value of 0.95, and top-k value of 20. We utilize the Qwen3-8B model [29] as teacher model to generate long CoT solutions for the problems within PDiffGen in Thinking mode, resulting in ˆDDiffGen. These generated solutions then undergo the filtration process detailed in Section 2.3. For the model filtering stage, we specifically employ the Qwen2.5-Math-7B-Instruct [44] model, given that this model also serves as the base model for our ScaleDiff. This comprehensive process yields our generated dataset, denoted as DDiffGen, comprising 1.15M problem-solution pairs. By augmenting with DDiffGen, we get the final DFinal (ScaleDiff-Math) dataset, comprising 1.7M problem-solution pairs. Training of ScaleDiff. As described above, in SFT, ScaleDiff model is initialized from Qwen2.5Math-7B-Instruct [44] model and trained on ScaleDiff-Math dataset. The batch size is set to 32, the maximum sequence length is 32,768 tokens, the training epoch is set to 3, with other training settings consistent with those employed for training DiffGen-8B. Due to the native context length limitation of the Qwen2.5-Math-7B-Instruct model to 4,096 tokens, we modify the rope theta parameter from 10K to 300K to enable support for maximum context length of 32,768 tokens, following the practice of OpenR1 [11]. The data template used for fine-tuning follows the default format of Qwen series. Evaluation. To ensure robust and reproducible results, our evaluation adheres to the standardized framework and best practices outlined in [45]. We assess the performance of our ScaleDiff model against relevant baselines on comprehensive set of widely recognized mathematical reasoning benchmarks: AIME24 [16], AIME25 [17], HMMT Feb25 [46], BRUMO [47], and MATH500 [48]. Performance is primarily measured using the standard Pass@1 metric. To account for potential variability, especially on smaller benchmarks, all evaluation results are averaged over multiple random seeds. Specifically, we use 10 random seeds for AIME24, AIME25, HMMT-Feb25, BRUMO25, and 3 random seeds for MATH500. The maximum number of new tokens, temperature, and top-p are set to 32,768, 0.6, and 0.95, respectively. All evaluations are conducted using the LightEval framework [49] with vLLM backend [50] and default chat template. All training and evaluation detailed above were conducted on NVIDIA A800 GPUs. Baselines. We mainly compare ScaleDiff with Qwen2.5-7B model series, including Qwen2.5-7BInstruct [36], Qwen2.5-Math-7B-Instruct [32], DeepSeek-R1-Distill-Qwen-7B [2], as well as LRMs that have undergone further SFT or RL based on Qwen2.5-7B model series. https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled 6 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning AIME24 AIME25 HMMT-Feb25 BRUMO25 MATH500 avg@10 11.35.4 11.32.7 53.05.3 75.04.5 avg@10 2.72.0 2.01.6 25.03.7 44.04. avg@10 22.33.7 18.06.0 54.77.2 68.02.7 avg@3 77.51.0 82.70.2 93.70.4 96.80.3 Model Qwen2.5-7B-Instruct [36] Qwen2.5-Math-7B-Instruct [32] DeepSeek-R1-Distill-Qwen-7B [37] Qwen3-8B [29] LIMR-7B [38] Oat-Zero-7B [39] Open-Reasoner-Zero-7B [40] AReaL-boba-RL-7B [41] Skywork-OR1-Math-7B [42] Skywork-OR1-7B [42] MiMo-7B-RL [43] OpenThinker-7B [5] OpenR1-Qwen-7B [11] OpenThinker2-7B [5] Light-R1-7B-DS [12] MiMo-7B-SFT [43] AceReason-Nemotron-7B [13] AM-Qwen3-Distilled-7B [3] AM-Thinking-v1-Distilled-7B [3] OpenThinker3-7B [6] OpenMath-Nemotron-7B [4] ScaleDiff-7B 33.34.5 30.04.5 17.03.5 58.04.8 59.73.8 61.54.2 68.34.3 28.04.3 50.75.1 54.77.6 55.35.4 60.36.0 64.32.6 63.03.5 62.05.8 66.34.3 73.74.1 73.05.0 avg@10 11.05.2 11.33.1 41.76.5 64.76.4 RL 7.33.3 11.04.0 17.03.1 43.04.8 49.75.0 50.35.5 59.05.0 SFT 25.74.7 36.33.5 38.05.6 41.32.7 44.36.7 50.32.8 51.75.6 50.03.3 57.35.5 60.74.7 58.78.2 AVG 22.6 22.8 51.6 68.9 24.4 26.2 27.6 53.1 57.7 58.3 64. 37.0 49.7 50.4 52.4 53.2 59.2 59.2 60.3 63.4 66.9 65.9 0.71.3 4.02.9 3.02.3 25.34.0 30.34.8 28.05.0 38.34.8 18.05.8 25.73.0 23.04.1 26.73.7 25.74.5 30.33.5 33.35.8 42.34.0 36.03.9 43.05.5 43.34.2 20.34.3 22.03.7 29.32.9 56.35.8 61.74.5 63.76.0 64.32.6 36.74.7 55.76.2 54.74.3 56.04.9 50.78.1 63.76.0 60.77.7 62.73.9 67.73.0 68.06.2 66.72.7 77.40.6 79.40.3 82.41.3 93.20.6 95.30.1 95.90.2 95.60. 87.90.4 93.40.7 93.90.4 94.00.3 93.60.2 96.10.4 94.60.4 94.90.7 95.80.4 95.20.3 95.20.3 Table 2: Pass@1 accuracy (mean std) comparison of different LRMs on AIME24, AIME25, HMMTFeb25, BRUMO25, and MATH500 benchmarks with multiple runs. The baseline results are sorted by the average performance. denotes results from our evaluation of the Qwen2.5-Math-7B-Instruct model trained by us on the corresponding dataset. The rows highlighted in gray correspond to the source data used for the ScaleDiff augmentation."
        },
        {
            "title": "3.2 Main Results",
            "content": "Our ScaleDiff demonstrates strong performance on both relatively simple benchmark MATH500 and more challenging benchmarks including AIME, HMMT-Feb25, and BRUMO, achieving average accuracies that surpass many RLor SFT-based strong LRMs, such as MiMo-7B-RL [43], Light-R17B-DS [12], AceReason-Nemotron-7B [13], and the recent OpenThinker3-7B [6]. Unlike most of these baseline methods, which rely on rejection sampling during solution distillationsampling multiple candidate solutions and retaining only those matching the ground-truth answerour approach samples single response per problem. This eliminates the need for repeated sampling until the correct solution is found, resulting in significantly lower data generation cost. Although the training data may contain incorrect answers, the diverse reasoning traces they provide can still contribute to enhancing the models reasoning ability. This observation is consistent with prior findings reported in [22, 51]. Comparison with AM-Qwen3-Distilled-7B. ScaleDiff achieves substantial improvements (11.3%) over AM-Qwen3-Distilled-7B [3], as ScaleDiff-7B can be viewed as hiking version of AM-Qwen3Distilled-7B. Here hiking refers to increasing both the overall difficulty and volume of the dataset through the ScaleDiff pipeline. We believe that such difficulty hiking is generally applicable when the original dataset maintains balanced difficulty distribution. 7 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Comparison with teacher model Qwen3-8B. Qwen3-8B is the teacher model for ScaleDiff-7B. From the results in Table 2, ScaleDiff-7B achieves 65.9% average accuracy, which closely approaches Qwen3-8Bs 68.9%. The gap between the two models is thus relatively small overall, indicating that the distillation and difficulty-hiking pipeline successfully transfers much of the teachers reasoning ability into the student model."
        },
        {
            "title": "3.3 Ablation Study",
            "content": "We further conduct an ablation study to investigate the contributions of different components in the ScaleDiff pipeline. Specifically, we focus on two key modules: (1) difficult problem identification and (2) response filtration. To verify the effect of difficult problem identification, we remove both the identification and the subsequent filtration steps, and instead train the question generator directly on the original problem set P. We then generate new problems from this generator, distill responses from the same teacher model, and fine-tune the same target model. To assess the effect of response filtration, we keep the difficult problem identification step but remove the filtration. For fair comparison, the total fine-tuning data size is fixed to 192K samples across all experiments. The results are summarized in Table 3, from which we can observe: (1) Removing response filtration degrades performance (58.4 55.3 on Model ScaleDiff w/o Filtration w/o Filtration & Difficult Size 192K 192K 192K AIME24 AIME25 HMMT-Feb25 BRUMO25 MATH500 avg@10 61.05.2 59.07.5 47.75.8 avg@10 57.75.0 56.74.9 47.03.5 avg@10 33.03.5 29.37.9 25.03.4 avg@10 52.05.0 46.77.3 45.06.2 avg@3 94.70.1 93.30.5 92.50.9 AVG 58.4 55.3 50.4 Table 3: Ablation Study on the effects of difficult problem selection and response filtration. average), showing that both rule and model filtering are important to eliminate noisy, repetitive, or low-value samples. This ensures the fine-tuning dataset remains both high-quality and challenging. (2) Removing difficult problem identification further causes notable drop in performance (55.3 50.4), confirming that pre-filtering challenging problems before generator training yields more effective data for enhancing reasoning capabilities. Without this step, the generated dataset may contain higher proportion of trivial problems, limiting SFT gains."
        },
        {
            "title": "4 Analysis",
            "content": "In this section, we present series of analyses to investigate the impact of data scaling (Section 4.1), the effect of teacher model (Section 4.2), and the difficulty of generated problems (Section 4.3). Unless otherwise specified, all experiments are conducted on unfiltered solutions. 4."
        },
        {
            "title": "Impact of Data Scaling",
            "content": "To assess the impact of augmentation scale on downstream performance, we vary the size of the generated dataset and evaluate the model across 3 benchmarks. Figure 3 illustrates the effect of augmentation dataset size on model performance across three benchmarks: AIME24, AIME25, and MATH500. The yellow dashed line denotes the baseline results of AM-Qwen3-Distilled-7B without augmentation. From the figure, we observe consistent performance improvement on the more challenging AIME24 and AIME25 benchmarks as the augmentation dataset size increases. Notably, even when the augmentation size reaches twice that of the original dataset, performance gains remain unsaturated, indicating the continued benefit of scaling difficult problems for enhancing complex reasoning. In contrast, for the relatively easier MATH500 benchmark, the augmentation provides 8 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Figure 3: Accuracy scaling with the size of augmented data on AIME24, AIME25, and MATH500. The amount of augmented data is 1/2, 1, and 2 times the size of the original dataset. no improvements, suggesting that additional difficult data contributes more significantly when the evaluation tasks themselves demand complex reasoning."
        },
        {
            "title": "4.2 Effect of Teacher Model",
            "content": "The performance of different teacher models may vary, and consequently, the quality of their distilled responses can influence downstream results. In this section, we investigate the effect of the teacher model. In our pipeline, the original solutions for DDiff are distilled from Qwen3-235B-A22B. We further distill three responses for each problem in PDiff using Qwen3-8B. We then compare the results obtained from the two teacher models on this controlled dataset. As shown in Table 4, we observe that using"
        },
        {
            "title": "Size",
            "content": "Qwen3-235B-A22B 192K 192K Qwen3-8B AIME24 AIME25 HMMT-Feb25 BRUMO25 MATH500 avg@10 62.35.0 57.35.3 avg@10 36.05.7 26.76.3 avg@10 59.06.3 56.36.6 avg@10 44.37.6 50.06.7 avg@3 93.91.2 93.51."
        },
        {
            "title": "AVG",
            "content": "56.6 55.6 Table 4: The effect of teacher model for solution distillation. Qwen3-235B-A22B as the teacher model yields slightly better performance than Qwen3-8B, though the difference is not substantial. This finding partially aligns with prior observations in [6, 52], which suggest that stronger-performing models are not necessarily better teachers because noticeable gap often exists between large teacher models and smaller student models. These results corroborate our decision to adopt the smaller Qwen3-8B as teacher model, demonstrating it to be more cost-efficient choice."
        },
        {
            "title": "4.3 Difficulty of Generated Problems",
            "content": "As described in Section 2.3, approximately 88% of the problems generated by DiffGen-8B are verified as difficult. To further investigate the characteristics of these problems, we analyze the distribution of response lengths across different datasets, namely DSimp, DDiff, and ˆDDiffGen, as well as across different teacher models, Qwen3-235B-A22B and Qwen3-8B (use superscript and to represent them, respectively). The results are illustrated in Figure 4, from which several findings emerge. (1) Comparing the distribution of DL Simp (blue curve) with the others, we observe that the difficulty levels identified by AdaptThink strongly correlate with response length: simple problems exhibit sharp density peak at very short token lengths, reflecting their requirement for only brief solution traces, while difficult problems shift the distribution toward longer token lengths, consistent with the need 9 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Figure 4: Distribution of solution lengths across datasets and teacher models. The superscript denotes the use of the large-sized Qwen3-235B-A22B as the teacher model, whereas indicates the use of the small-sized Qwen3-8B. Di and ˆDS Di and DS for more elaborate reasoning chains. (2) Comparing the distribution of DL Di (orange and green curves), we find that the choice of teacher model from the same family (Qwen3) has little impact on the response length distribution for difficult problems, as both yield similar patterns. (3) Comparing the distribution of DL Di Gen with rule and model filtration (orange and red curves), it becomes evident that generated problems tend to induce longer responses than original difficult problems, indicating higher intrinsic complexity. This observation is further corroborated by downstream results in Table 3 and 4: SFT on the 192K DL Di dataset yields an average performance of 56.6, whereas training on an equal amount of ˆDS Di Gen with rule and model filtration achieves 58.4. (4) Finally, comparing the distribution of ˆDS Di Gen with rule and model filtration (purple and red curves) shows that model filtration further refines the dataset by removing relatively easier problems, thereby retaining subset of problems with greater difficulty and longer reasoning traces. Di Gen with rule filtration and ˆDS"
        },
        {
            "title": "5.1 Mathematical Data for LRMs",
            "content": "Numerous datasets have been proposed to enhance the mathematical reasoning capabilities of LRMs through SFT. Prevalent strategies [10, 34, 53] involve selecting data from existing sources such as textbooks, examinations and websites. Beyond simple selection, some research focuses on data augmentation of these existing resources. Answer augmentation methods [4, 22, 30, 54, 55, 56, 57] use teacher model to synthesize novel and diverse solutions for existing problems, aiming to boost the student models performance. These methods are often referred to as data distillation. In contrast, Question augmentation methods [19, 20, 22, 58, 59, 60, 61, 62] involve synthesizing novel problems and their corresponding solutions. This method can expand topical coverage, introduce more diverse problem structures, though it requires rigorous validation to ensure the correctness of synthesized questions and solutions [21, 63]. To further enhance the diversity of synthetic data, Persona-based augmentation technique [64, 65, 66, 67] has emerged. By incorporating role-playing into prompts, LLMs can generate diverse, role-specific mathematical problems. However, while some efforts have emerged to synthesize new questions, they do not explicitly control the data difficulty. Consequently, the generated problems often lack sufficient challenge for current top-tier LRMs, leading to limited improvements. 10 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning"
        },
        {
            "title": "5.2 Difficulty-aware Data Selection and Synthesis",
            "content": "Data difficulty is crucial metric for assessing data quality, significantly impacting the training effectiveness [68]. Previous research has explored difficulty-aware question selection. For example, S1 [69] and Light-R1 [70] filter out simple problems that small models can easily solve, retaining difficult ones for SFT. AceReason [13] further incorporates difficulty-based filtering into RL training. DeepMath103K [10] proposes new dataset with higher proportion of challenging problems. However, these methods are limited to selecting from existing data and cannot generate new, challenging examples. Furthermore, most of these techniques assess difficulty by fail rate, model-specific metric, which may restrict their generalizability across different models. Another line of research focuses on synthesizing new data with varying difficulty. In the mathematical domain, DART-Math [30] synthesizes more solutions for difficult problems, enhancing response diversity for challenging questions. MATH2 [71] extracts core skills from existing math datasets and employs them as the basis for generating novel and difficult questions by prompting LLMs. DAST [72] proposes difficulty-matching few-shot prompting method, presenting longer, more detailed examples for harder questions. ScaleQuest [73] introduces Question Preference Optimization (QPO), which optimizes generated mathematical problems based on solvability and difficulty. The optimized questions then serve as positive samples for preference optimization of the question generator. MathSmith [27] generates math problems from scratch using conceptexplanation pairs, achieving superior performance on olympiad-level benchmarks. However, most of these methods are not specifically designed for synthesizing difficult math problems. The difficulty of their generated questions remains limited, leading to restricted performance improvements."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce ScaleDiff, simple yet effective pipeline for scaling the construction of difficult mathematical problems to enhance the complex reasoning abilities of LRMs. By leveraging AdaptThink as an efficient difficult problem identifier, training dedicated generator (DiffGen-8B) to produce new challenging problems, and applying both rule and model filtering to distill highquality solutions, we construct the ScaleDiff-Math dataset. Extensive experiments demonstrate that fine-tuning on this dataset yields substantial improvements over both strong SFTand RL-based baselines across multiple mathematical reasoning benchmarks, including AIME24, AIME25, HMMTFeb25, BRUMO25, and MATH500. Moreover, we observe clear phenomenon that augmenting training data with increasing quantities of difficult problems consistently improves performance on challenging benchmarks, underscoring the value of difficulty-aware augmentation for advancing reasoning capabilities. We believe that ScaleDiff offers practical and generalizable strategy for the community to strengthen LRMs, particularly in domains where complex reasoning is essential but difficult training data is scarce. References [1] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li. Not all correct answers are equal: Why your distillation source matters. arXiv preprint arXiv:2505.14464, 2025. ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning [4] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. [5] Team. Open Thoughts. https://open-thoughts.ai, January 2025. [6] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. [7] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [8] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild. arXiv preprint arXiv:2503.18892, 2025. [9] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL, 2025. Notion Blog. [10] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. [11] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. [12] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [13] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. [14] Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron 1.1: Advancing math and code reasoning through sft and rl synergy. arXiv preprint arXiv:2506.13284, 2025. [15] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. [16] AI-MO. AIMO Validation AIME Dataset. [17] Yen-Ting Lin. Aime 2025 dataset, 2025. URL https://huggingface.co/datasets/yentinglin/aime 2025. Accessed: 2025-03-29. [18] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [19] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [20] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large In The Twelfth International Conference on Learning Representations, 2024. URL https: language models. //openreview.net/forum?id=N8N0hgNDRt. 12 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning [21] Ping Yu, Jack Lanchantin, Tianlu Wang, Weizhe Yuan, Olga Golovneva, Ilia Kulikov, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. Cot-self-instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks. arXiv preprint arXiv:2507.23751, 2025. [22] Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations. [23] Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-pointdriven data synthesis with its enhancement on mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2417624184, 2025. [24] Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In ICML. OpenReview.net, 2024. [25] Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. Promptcot: Synthesizing olympiad-level problems for mathematical reasoning in large language models. arXiv preprint arXiv:2503.02324, 2025. [26] Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, and Emad Barsoum. Sandmath: Using llms to generate novel, difficult and useful mathematics questions and answers. arXiv preprint arXiv:2507.20527, 2025. [27] Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, and Fei Tang. Mathsmith: Towards extremely hard mathematical reasoning by forging synthetic problems with reinforced policy. arXiv preprint arXiv:2508.05592, 2025. [28] Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025. [29] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [30] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. In NeurIPS, 2024. [31] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Chenghao Ma, Shanghaoran Quan, Liang Chen, Qingxiu Dong, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations. [32] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [33] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024. [34] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [35] Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan YeYanhan, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, 2024. [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [37] DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. URL https://arxiv.org/abs/2501.12948. [38] Xuefeng Li, Haoyang Zou, and Pengfei Liu. LIMR: Less is More for RL Scaling. arXiv preprint arXiv:2502.11886, 2025. [39] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. 13 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning [40] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [41] Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. [42] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. [43] LLM Xiaomi, Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, et al. Mimo: Unlocking the reasoning potential of language modelfrom pretraining to posttraining. arXiv preprint arXiv:2505.07608, 2025. [44] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [45] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. [46] HMMT. Hmmt 2025, 2025. URL https://www.hmmt.org/. [47] BRUMO. Brown university math olympiad 2025, 2025. URL https://www.brumo.org/. [48] HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/HuggingFaceH4/MATH-500/blob/ main/README.md, 2024. Accessed: 2025-03-29. [49] Clementine Fourrier, Nathan Habib, Hynek Kydlıˇcek, Thomas Wolf, and Lewis Tunstall. LightEval: lightweight framework for LLM evaluation, 2023. URL https://github.com/huggingface/lighteval. [50] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [51] Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization. arXiv preprint arXiv:2508.07629, 2025. [52] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. [53] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23572367, 2019. [54] Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, Vicky Zhao, Conghui He, and Lijun Wu. Lemma: Learning from errors for mathematical advancement in llms. arXiv preprint arXiv:2503.17439, 2025. [55] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774, 2024. [56] Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai, Conghui He, and Lijun Wu. Metaladder: Ascending mathematical solution quality via analogical-problem reasoning transfer. arXiv preprint arXiv:2503.14891, 2025. [57] Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703, 2025. 14 ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning [58] Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In ACL (1), pages 1023010258. Association for Computational Linguistics, 2024. [59] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27322747, 2024. [60] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. [61] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. Mathfusion: Enhancing mathematical problem-solving of llm through instruction fusion. arXiv preprint arXiv:2503.16212, 2025. [62] Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, Vicky Zhao, Conghui He, and Lijun Wu. Rest: Stress testing large reasoning models by asking multiple problems at once. arXiv preprint arXiv:2507.10541, 2025. [63] Xuefeng Li, Yanheng He, and Pengfei Liu. Synthesizing verified mathematical problems. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [64] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. [65] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [66] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [67] Jing Luo, Longze Chen, Run Luo, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, et al. Personamath: Boosting mathematical reasoning via persona-driven data augmentation. arXiv preprint arXiv:2410.01504, 2024. [68] Zui Chen, Tianqiao Liu, Mi Tian, Weiqi Luo, Zitao Liu, et al. Advancing mathematical reasoning in language models: The impact of problem-solving data, data synthesis methods, and training stages. In The Thirteenth International Conference on Learning Representations. [69] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [70] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [71] Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Jiatong Yu, Yinghui He, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, et al. Ai-assisted generation of difficult math questions. arXiv preprint arXiv:2407.21009, 2024. [72] Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, et al. Dast: Difficulty-aware self-training on large language models. arXiv preprint arXiv:2503.09029, 2025. [73] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "OpenDataLab, Shanghai Artificial Intelligence Laboratory",
        "School of Artificial Intelligence, Wuhan University",
        "Tsinghua University"
    ]
}