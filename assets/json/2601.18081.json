{
    "paper_title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
    "authors": [
        "Peixuan Han",
        "Yingjie Yu",
        "Jingjun Xu",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 1 8 0 8 1 . 1 0 6 2 : r DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You University of Illinois Urbana-Champaign {ph16,yyu69,jingjunx,jiaxuan}@illinois.edu"
        },
        {
            "title": "Abstract",
            "content": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-theshelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in more complex multiround setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ ulab-uiuc/DRPG-RebuttalAgent."
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of large language models (LLMs), AI agents have become increasingly integrated into the human research workflow. In particular, they have begun to assist researchers across multiple stages of scientific discovery, including idea generation (Lu et al., 2024a), paper writing (Aydin et al., 2025), and peer review (Chang 1 et al., 2025; Yu et al., 2024). Despite these advances, AI for academic rebuttalthe process in which authors and reviewers exchange feedback on paperremains largely underexplored. As critical stage of the research lifecycle, rebuttal plays an essential role in ensuring fair and objective evaluation of submissions. Moreover, as the research community continues to expand, particularly in fast-growing fields such as computer science, preparing thoughtful rebuttals has become increasingly time-consuming for conscientious authors. For instance, major conferences such as NeurIPS and ICLR received over 25,000 submissions in 2025, creating an urgent need for more efficient mechanisms to facilitate communication between authors and reviewers. Consequently, automated or assistive rebuttal agents can potentially reduce researchers workload substantially, allowing them to focus more on innovative research. Despite its potential benefits, automating academic rebuttal with LLMs is challenging task. Rebuttal represents unique adversarial, multi-agent scenario that requires diverse skills, including precise comprehension, persuasive argumentation, and domain-specific expertise. Prior work typically relies on off-the-shelf LLMs or ad-hoc pipelines to generate responses (Kirtani et al., 2025; Jin et al., 2024b); however, such approaches often yield suboptimal results for two main reasons. Firstly, academic papers are usually lengthy and informationdense. As revealed by the Lost in the Middle phenomenon (Liu et al., 2024), LLMs struggle to identify and extract the most relevant evidence from long contexts when responding to specific reviewer concerns. Secondly, effective rebuttals require well-structured and convincing arguments tailored to reviewers critiques. Since LLMs are not explicitly trained for persuasion, they tend to produce responses that are overly generic, excessively conciliatory or defensive, failing to directly and convincingly address reviewers key concerns. To overcome these limitations, we propose DRPG(Decompose, Retrieve, Plan, Generate), four-stage agentic framework designed to automatically generate high-quality academic rebuttals. To mitigate long-context challenges, DRPG first employs Decomposer to break review into several points where each point is an atomic concern or confusion that needs to be addressed. For each point, Retriever then selects the most relevant paragraphs from the paper, reducing the input length by over 75% while preserving critical evidence needed for rebuttal. To further enhance argument quality, we introduce Planner that explicitly formulates rebuttal strategies before response generation. Inspired by planning techniques in structured debates (Wang et al., 2025a; Han et al., 2025), the Planner proposes multiple rebuttal perspectives and identifies the most promising one that is best supported by the paper content. Trained with compelling human rebuttal data, the planner can effectively identify the most supported perspective with an accuracy of over 98%. Experiments conducted on data from top-tier conferences demonstrate that DRPG can effectively address reviewers questions, outperforming existing rebuttal pipelines with around 40 points higher Elo score, which implies consistently higher win rates. In addition, DRPG surpasses average human performance using only an 8B model. Further analyses highlight the successful design of the Planner module, and show that it provides multi-perspective and explainable signal that substantially improves rebuttal quality. Finally, we showed the impressive performance of DRPG on multi-round discussions and conducted human study to validate our evaluation metrics. Overall, DRPG represents promising exploration of integrating LLM agents into the peer-review process, with the potential to reshape how authors and reviewers communicate at scale."
        },
        {
            "title": "2 Related Work",
            "content": "LLM for academic rebuttal. Recent advancements in AI have significantly impacted various stages of scientific discovery, including idea generation, experimentation, and paper writing (Luo et al., 2025; Lu et al., 2024a; Schmidgall et al., 2025; Yuan et al., 2025). Among these stages, peer review plays key role in ensuring the quality and credibility of research papers, and has been receiving increasing attention within the AI research community (Zhuang et al., 2025; Liang et al., 2024; Wei et al., 2025). Existing work in this domain has focused primarily on simulating the review process (Yu et al., 2024; Bougie and Watanabe, 2024) and training more effective reviewers (Chang et al., 2025; Kirtani et al., 2025; Jin et al., 2024b). However, the rebuttal phase, which is vital for facilitating communication between authors and reviewers, has received relatively little attention. few recent studies explore this area by collecting real-world rebuttal datasets (Zhang et al., 2025a; Kennard et al., 2022), using zero-shot LLMs to generate preliminary rebuttals (Kirtani et al., 2025; Jin et al., 2024b), and training rebuttal agents by designing pre-defined templates to address different questions (Purkayastha et al., 2023; Orbach et al., 2019). Based on these studies, we propose more systematic and effective rebuttal agent in this work. LLM for debate and persuasion. Similar to debate and persuasion (Rogiers et al., 2024), the objective of the rebuttal is to convince the reviewer to change their opinion. Researchers have utilized human strategies(Wang et al., 2019; Yang et al., 2019) and high-quality dialogues(Singh et al., 2024; Stengel-Eskin et al., 2024; Jin et al., 2024a; Furumai et al., 2024) to equip LLMs with strong persuasion capabilities. Recently, advanced agentic pipelines (Wang et al., 2025a; Hu et al., 2024; Wu et al., 2025) and Reinforcement learning algorithms (Cheng and You, 2025; Han et al., 2025) on debate has also been proposed. In the long term, we believe academic rebuttal is promising domain in debate research, since rebuttal is fact-oriented, grounded in high-quality papers, and has substantial public data available. Planning in high-stakes decision making. In academic rebuttal, the author must form responses thoughtfully, as the rebuttal quality may affect the result of the submission, with few opportunities to make changes. Planning and selecting argument directions are crucial in such high-stakes settings. There are two prevalent ways of pruning ideas. Firstly, researchers simulate the consequences of adopting each idea, method originating from Monte Carlo search trees (Coulom, 2006; Silver et al., 2016). This method is most commonly used in scenarios involving multi-agent interactions or external environments (Lu et al., 2024b; Shi et al., 2024; Weng et al., 2024; He et al., 2025). Secondly, researchers train selector or verifier networks to figure out the best candidate through supervised learning (Han et al., 2025; Wang et al., 2025b; Singh and 2 Figure 1: Overview of DRPG. Bali, 2024; Li et al., 2024; White et al., 2021; Lee et al., 2018) or reinforcement learning (Zhang et al., 2025b; Chen et al., 2025) when ground-truth can be obtained at scale. Following this line of work, we train Planner to select rebuttal perspectives when designing DRPG."
        },
        {
            "title": "3 Method",
            "content": "This section presents an overview of DRPG, an endto-end framework designed to generate coherent and professional rebuttals based on full-length, conference-level paper and its reviews. DRPG is composed of four core componentsDecomposer, Retriever, Planner, and Executoreach of which is introduced in detail in the following subsections. The overall workflow of the framework is illustrated in Figure 1."
        },
        {
            "title": "3.1 Decomposer",
            "content": "Reviews of academic papers typically involve multiple aspects and viewpoints. To transform such multifaceted and complex feedback into manageable points (refer to Table 5 for an example) for downstream processing, we employ Decomposer implemented using large language model (LLM). The Decomposer identifies the weaknesses and questions raised by the reviewer, which are key elements that must be addressed in the rebuttal. As result, the Decomposer divides the review into set of independent, fine-grained points that will be addressed in the following modules."
        },
        {
            "title": "3.2 Retriever",
            "content": "Due to the substantial length of academic papers, the performance of rebuttal agents may be adversely affected by the Lost in the Middle phenomenon (Liu et al., 2024). To address this issue, we divide the paper into paragraphs and employ Retriever to identify the most relevant paragraphs corresponding to each atomic point generated by the Decomposer. The Retriever is implemented using dense retrieval techniques, where text encoder is used to embed both review points and paper paragraphs, and cosine similarity is applied to measure their relevance. Only the most relevant paragraphs are passed to the executor, thereby increasing information density while reducing content length for around 75% in practice."
        },
        {
            "title": "3.3 Planner",
            "content": "In academic rebuttal scenarios, identifying an appropriate perspective from which to defend the authors work is crucial. However, large language models (LLMs) arent trained to conduct such deliberate planning, leading them to simply state specific details of the paper while overlooking the reviewers underlying reasoning and value judgments. To address this limitation, we introduce two-step Planner that guides LLMs to explicitly plan how to address review questions, making the communication between authors and reviewers more effective. In the first step, an idea proposer generates several candidate perspectives based on review point. The proposer is instructed to consider two high-level strategies: clarification, which identifies potential misunderstandings in the reviewers comments, and justification, which argues that the reviewers concern does not invalidate the papers core contributions. Note that the paper content is intentionally withheld from the idea proposer to encourage creative and diverse perspective generation. As result, some proposed perspectives may 3 be infeasible or unsupported, which will be filtered out in the subsequent step. In the second step, the Planner selects the most suitable perspective by evaluating its supportive score with respect to the papers content. Concretely, the Planner is implemented using text encoder (the same encoder as used in the Retriever) followed by multi-layer perceptron (MLP). We first obtain vector representations for each candidate rebuttal perspective and each relevant paragraph of the paper using the encoder. These vectors are then concatenated and fed into the MLP to compute score for each perspectiveparagraph pair. The final score of perspective is obtained by averaging its scores across all relevant paragraphs. Given perspective pers and set of paragraphs p1..K from the paper (where denotes the number of relevant paragraphs), the supportive score s(pers, p) is defined as1: s(pers, p) = 1 K (cid:88) j=1 M(cid:0)E(pers)E(pj)(cid:1), (1) where denotes the text encoder and represents the MLP module in the Planner. During training, we select rebuttals that lead to an increase in review scores. For each review point, we construct candidate set consisting of five synthetic perspectives generated by the idea proposer and one ground-truth perspective extracted from the actual content. The Planner is optimized using cross-entropy loss. Let the set of candidate perspectives be I1..N , and gt denote the index of the ground-truth, the training loss is then defined as: L(gt) = log exp (cid:0)s(Igt, p)(cid:1) i=1 exp (cid:0)s(Ii, p)(cid:1) . (cid:80)N (2) During inference, we design self-confidence mechanism to ensure the reliability of the selected perspective. perspective is passed to the Executor only if its confidence score exceeds predefined threshold ; otherwise, DRPG falls back to the setting without the Planner. The selected perspective and its confidence are computed as: ans = ArgmaxN i=1 s(Ii, p), conf(ans) = exp (cid:0)s(Ians, p)(cid:1) i=1 exp (cid:0)s(Ii, p)(cid:1) . 1The operator means vector concatenation. (cid:80)N (3) (4)"
        },
        {
            "title": "3.4 Executor",
            "content": "The Executor serves as the final stage of the rebuttal pipeline. Given the structured information produced by the preceding modules, the Executor generates coherent and persuasive rebuttal paragraph for each individual review point. The Executor can be instantiated using either generalpurpose LLM or model specialized for rebuttal generation. In summary, DRPG is an agentic workflow designed to automatically generate high-quality rebuttals. By integrating four specialized components, DRPG addresses two key limitations of using single LLM for rebuttal writing: the difficulty of effectively processing lengthy paper and review texts, and the tendency to produce generic, insufficiently targeted responses. The overall workflow of DRPG is illustrated in Algorithm 1. Algorithm 1 The procedure of DRPG2. Require: Paper , Review R, Decomposer D, Encoder E, Retrieved count K, Idea proposer I, Planner P, Threshold , Executor 1: r[1..Nr] D(R) 2: Vp[1..Np] E(P [1..Np]) 3: Vr[1..Nr] E(r[1..Nr]) 4: for = 1 to Nr do 5: 6: 7: end for 8: for = 1 to Nr do 9: 10: sim[i, 1..Np] Vr[i]Vp[j], = 1..Np p[i, 1..K] TopK(sim[i], K) PLAN candidate ideas supportive scores I[i, 1..NI ] I(r[i]) s[i, 1..NI ] P(I[i], p[i]) DECOMPOSE RETRIEVE means relevant paragraphs (Equation (1)) j=1 exp(s[i, j]) id arg maxj s[i, j]) conf exp(s[i, id]) / (cid:80)NI if conf then Iselect[i] I[i, id] Iselect[i] ϵ else 11: 12: 13: 14: 15: 16: 17: 18: end for 19: for = 1 to Nr do 20: 21: end for 22: RES (cid:13) (cid:13) 23: return RES end if Nr i=1 res[i] res[i] X(r[i], p[i], Iselect[i]) fallback when conf is low GENERATE concatenate all responses"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Dataset. We conduct experiments on Re2 (Zhang et al., 2025a), large-scale dataset consisting of over 17k academic papers and approximately 60k corresponding reviews and rebuttals collected from 2In Algorithm 1, boldface symbols represent LLMs or networks, and normal symbols represent data variables. Variables starting with are array sizes induced from LLM outputs. Table 1: Performance of rebuttal agents, which shows DRPG generates the most effective rebuttals across all settings. The last 5 columns in the pairwise comparison section correspond to the results of comparing the different structures of the same base model. Elo scores are calculated within each base model."
        },
        {
            "title": "Setting",
            "content": "REAL Qwen3-8B (Direct) Decomp DRG Jiu-Jitsu DRPG GPT-oss-20B (Direct) Decomp DRG Jiu-Jitsu DRPG Mixtral-8x7B (Direct) Decomp DRG Jiu-Jitsu DRPG LLaMa3.3-70B (Direct) Decomp DRG Jiu-Jitsu DRPG"
        },
        {
            "title": "Direct",
            "content": "- 47.81 - - - 60.65 40.28 - - - 75.53 49.04 - - - 51.42 50.09 - - - 65.44 - - 59.61 62.32 57.52 65.53 - 72.86 75.61 78.61 77.98 - 82.29 87.94 87.85 86.62 - 89.90 81.73 88.51 90.93 Win Rate Against (%) Decomp"
        },
        {
            "title": "DRG",
            "content": "Jiu-Jitsu DRPG"
        },
        {
            "title": "Judge Score",
            "content": "- 40.59 - 53.23 55.89 56.63 27.14 - 55.97 52.11 57.43 17.71 - 68.69 71.20 72.18 10.10 - 57.22 53.26 59.39 - 37.68 46.77 - 54.49 58.09 24.39 44.03 - 44.47 51.88 12.06 31.31 - 50.35 55.33 18.27 42.78 - 49.12 54.09 - 42.48 44.11 45.51 - 58.21 21.39 47.89 55.53 - 56.79 12.15 28.80 49.65 - 55.59 11.49 46.74 50.88 - 58.72 - 34.47 43.37 41.91 41.79 - 22.02 42.57 48.02 43.21 - 13.18 27.82 44.67 44.41 - 9.07 40.61 45.91 41.28 - - 936 991 1004 1013 1054 837 1012 1054 1029 1067 738 959 1088 1093 1119 725 1040 1065 1058 1109 5.72 5.63 5.75 5.75 5.72 5.78 5.73 5.80 5.75 5.72 5.88 5.63 5.68 5.66 5.65 5.68 5.60 5.67 5.68 5.67 5. 45 top-tier computer science conferences over 8 years, including ACL, ICLR, and NeurIPS. Data statistics are reported in Section B.1. Models. We evaluate our method on four base LLMs spanning different families and sizes: Qwen3-8B (Yang et al., 2025), GPT-oss-20B (Agarwal et al., 2025), Mixtral-8x7B (Jiang et al., 2024), and LLaMa3.3-70B (Dubey et al., 2024). Among all settings, the Retriever is always implemented as BGE-M3 (Chen et al., 2024). Baselines. We compare DRPG against both human-written rebuttals from the dataset (denoted as REAL) and four agentic baselines. As summarized in Table 2, the first three baselines, Direct, Decomp, and DRG, correspond to ablated versions of DRPG with specific components removed. In addition, Jiu-Jitsu (Purkayastha et al., 2023) serves as strong baseline that generates perspectives using predefined templates based on question types, replacing the Planner module in our pipeline. Metrics. We employ two LLM-based evaluation metrics to assess rebuttal quality3. First, we use GPT-4o as pairwise comparator to rank rebuttals 3Traditional n-gram-based metrics such as ROUGE and BLEU are not well-suited for evaluating the reasoning quality and coherence of rebuttals, and are therefore omitted. Table 2: Components of different baselines. Setting Direct Decomp DRG Jiu-Jitsu DRPG Decomposer Retriever Planner Executor and compute an Elo score for each method, following standard practice in open-ended generation tasks (Chiang et al., 2024; Boubdir et al., 2024). Elo scores are estimated by maximum likelihood under standard BradleyTerry model, with base rating of 1000. To validate the reliability of such comparison, we conduct human study, the results of which are reported in Section 5.5. Second, we use reinforcement learning to train judge model based on Qwen3-4B to simulate the reviewers evaluation and scoring process after reading the rebuttal. The judge model gives judge score exactly identical with human reviewers on 71% of the test data. Additional details are provided in Section B.3. Training and Inference Details. Detailed training configurations for the Planner are described in Section B.2. For retrieval, we set the number of retrieved paragraphs per review point to = 15. 5 During inference, the Planner applies confidence threshold of = 0.8. Under this setting, approximately 62% of review points are assigned valid perspective. The remaining cases typically fall into two categories. The review point is either straightforward and thus doesnt require an explicit perspective, or its heavily dependent on specific paper content, making it difficult to propose valid perspective for the Planner."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 clearly shows that an agentic workflow is crucial for producing high-quality rebuttals: Directly prompting an LLM to respond to an entire review (Direct) consistently yields inferior performance. All agent-based variants achieve substantially higher scores than Direct, demonstrating the effectiveness of structured processing. Among all methods, DRPG consistently outperforms the other variants in pairwise comparisons and achieves the highest post-rebuttal scores in most settings. This finding suggests that each component of DRPG plays an important role in mitigating the inherent limitations of single-LLM approach. Firstly, the Decomposer and Retriever break down complex reviews into atomic, focused points that can be easily handled, avoiding the shortcomings of excessively long contexts. Secondly, the Planner proposes and identifies an appropriate response direction for each review question. Building on these outputs, the Executor can generate high-quality, tailored responses. We show qualitative cases of DEPGs benefit in Section D.2. Although Jiu-Jitsu adopts pipeline structure similar to DRPG, its performance is consistently lower due to the limitations of its Planner. Specifically, the Jiu-Jitsu Planner selects from fixed set of canonical rebuttal templates, which often results in generic or impractical perspectives4. In contrast, DRPG employs content-aware Planner that selects perspectives based on the papers content, leading to more specific and persuasive rebuttals."
        },
        {
            "title": "5.1 Ablation Study on Planner Design",
            "content": "The Planner is built around an MLP-based scoring function that selects an effective rebuttal perspective by modeling the supportive relationship 4Typical examples include statements such as We agree some observations have been made in previous work, but there are critical differences or We will gladly provide the trained networks on request. Table 3: Comparison of different planner designs. Setting no-paper full-paper encoder Our Planner Train loss 0.5881 0.2393 N/A 0.0914 Test acc (%) 61.55 86.41 45.44 98. between candidate perspectives and paper paragraphs that are relevant to the review point. In this section, we further analyze its design by comparing it with three alternative variants: 1) no-paper, where the MLP scores each perspective independently without taking any paper content as input; 2) full-paper, where all paragraphs of the paper are used instead of the relevant paragraphs selected by the Retriever; and 3) encoder, training-free setting which uses vector similarity scores between perspectives and paragraphs as scores. Table 3 reports the training loss and test accuracy of different planner designs. Our Planner successfully identifies the perspective adopted in successful human rebuttal with an accuracy of 98.64%, substantially outperforming all alternatives. These results lead to the following observations: Incorporating paper content is essential for effective planning. Scoring perspectives in isolation (no-paper) results in poor performance. Including the Retriever as preprocessing step significantly improves performance. Compared to full-paper, using only relevant paragraphs makes the Planner focus on content directly related to the review point, preventing irrelevant paragraphs from dominating the aggregation in Equation (1). Simply relying on encoder similarity without learning (encoder) is insufficient. This suggests that the relationship between rebuttal perspective and its supporting evidence is more nuanced than surface-level relevance, and requires learned module to capture."
        },
        {
            "title": "Perspectives",
            "content": "As introduced in Section 3.3, the Planner considers two types of rebuttal perspectives: Clarification and Justification. Clarification aims to correct factual inaccuracies or misunderstandings in the review, whereas Justification seeks to defend the papers methodology or contributions when the reviewers comments are factually correct but potentially based on debatable evaluation criteria. Table 5 presents an illustrative example of these two 6 Table 4: Performance of DRPG with restricted perspective types (Clarification or Justification). We use LLaMa3.370B as the base model in this experiment."
        },
        {
            "title": "DRG",
            "content": "- DRG DRPG-C 50.38 35.04 DRPG-J 54.09 DRPG Win Rate Against (%) DRPG-C DRPG-J DRPG 45.91 64.96 41.49 65.95 34.02 - - 65.98 49.62 - 34.05 58."
        },
        {
            "title": "Judge Score Ratio of Clarification",
            "content": "1018 1014 915 1051 5.75 5.72 5.65 5.78 - 100% 0% 66.26% perspective types for the same review point. 5."
        },
        {
            "title": "Interpreting Planner Scores",
            "content": "To analyze the effect of perspective choice, we conduct an ablation study that restricts the Planner to single perspective type. Specifically, instead of allowing the Planner to select the most supported perspective, we force the Executor to respond using only Clarification or only Justification. We denote these two variants as DRPG-C and DRPG-J, respectively. These settings represent two extreme rebuttal strategies: one that focuses exclusively on factual correctness, and the other that emphasizes significance and contribution. Table 5: An example of candidate perspectives generated by the Planner. Example Atomic Point in Real-world Review: The proposed method performs much worse than HiNet in terms of the extraction accuracy of the secret-in-image hiding. Perspective by DRPG PSNR may not be the most suitable metric for evaluating the extraction accuracy in the image hiding task. Our performance with obfuscating is actually better than HiNets. Our method works well on secret-innetwork hiding, task much more challenging than image hiding. Type Justification Clarification Justification As shown in Table 4, both variants underperform the full DRPG, and even lag behind the DRG baseline, which does not even include Planner. This result highlights that relying solely on single perspective type weakens rebuttal quality. Effective academic rebuttals require balanced use of both clarification and justification. DRPG adapts between these two strategies depending on the review context: it applies clarification when addressing technical misunderstandings, and justification when responding to critiques based on subjective or questionable evaluation standards. This section presents an interpretability approach that reveals the explainability advantages of DRPG in the Planners decision-making. By examining the Planners scores for each perspectiveparagraph pair individually, we can gain insight into why particular perspective is selected. Figure 2: An example to illustrate how the planner evaluates different perspectives. Scores presented are normalized using sigmoid function, and only scores 0.2 are displayed. Figure 2 illustrates the Planners scores for the example shown in Table 5. Through supervised training, the Planner learns to capture claimevidence relationships between candidate perspectives and paper content, rather than relying solely on surface-level semantic similarity. For example, perspective 3 is most strongly supported by paragraph 3, which explicitly states that SinGAN performs better on challenging tasks. Paragraph 4 also receives relatively high score, as it discusses embedding richer information, which can serve as auxiliary evidence when constructing the rebuttal from the angle of hard tasks. Such fine-grained score analysis not only improves the transparency of the Planners decision-making process but also provides useful structural guide for human authors when composing or refining rebuttals."
        },
        {
            "title": "5.5 Human Study on Pairwise Comparison",
            "content": "Previous experiments focus on single-round rebuttals, however, in real conference review processes, authors and reviewers sometimes engage in multiple rounds of discussion, during which evaluations of the work become more informed and objective. To better reflect this setting, we design an experiment that simulates multi-round reviewerauthor interactions and evaluates the rebuttal agents performance accordingly. The interaction proceeds in round-by-round manner, alternating between the DRPG and the judge model trained via reinforcement learning (see Section B.3). In each round, the judge model first summarizes and evaluates the current rebuttal in its chain-of-thought (CoT), and then outputs final score. We extract this CoT content as proxy for the reviewers follow-up feedback and treat it as the new review for the next round. The DRPG then generates subsequent rebuttal in response. Repeating this process enables us to simulate multiround discussions between reviewers and authors. Figure 3: Performance of different rebuttal agents in multi-round discussions. DRPG addresses follow-up questions better and delivers greater gains compared with the baselines. Figure 3 illustrates the performance of different workflows over 3 rounds of discussion. As the number of interaction turns increases, the advantage of DRPG becomes increasingly pronounced. While the judge scores of baseline methods quickly plateau after the first round, DRPG continues to achieve consistent improvements in subsequent rounds. This trend suggests that DRPG is better equipped to incorporate feedback from earlier interactions and to respond effectively to follow-up questions raised by the reviewer, enabling reviewers to develop more complete understanding of the papers technical contributions. 8 In this section, we conduct human study to validate the LLM pairwise comparison in Section 4. Specifically, we sample 20 reviews5 and their corresponding rebuttals from DRG and DRPG (the base model is LLaMa-3.3-70B in both settings). The reviews are selected so that exactly 10 DRG rebuttals are preferred by gpt-4o, and 10 DRPG rebuttals are preferred. 3 experts in computer science then independently judge which rebuttal is more effective. Table 6: Human study results. Setting Among Human Annotators Human Majority v.s. LLM Align Rate (%) 70 75 κ Score Fleisss κ = 0.598 Cohens κ = 0.500 In Table 6, we report the agreement among human annotators, as well as the alignment between human judgments and GPT-4o. The results show that the three human experts exhibit highly consistent preferences, and that their evaluations demonstrate substantial level of agreement with the LLM. Moreover, qualitative analysis indicates that GPT-4o relies on evaluation criteria similar to those used by human reviewers, further supporting its validity as high-quality proxy for assessing rebuttal quality. Detailed results for 20 reviewed cases are provided in Section D.1, and examples are provided in Section D.2."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigate the largely overlooked problem of academic rebuttal automation and present DRPG, an agentic framework designed to generate grounded, coherent, and convincing rebuttal responses. DRPG consists of four components: Decomposer, Retriever, Planner, and Executor. By decomposing reviewer feedback, retrieving targeted evidence from long papers, and explicitly planning rebuttal strategies, DRPG addresses key challenges that limit the effectiveness of off-theshelf LLMs in this setting. Experimental results on top-tier conference data demonstrate that our approach consistently outperforms existing rebuttal methods and achieves strong performance even with compact model. Beyond empirical gains, our analysis also shows that structured planning provides an interpretable and multi-perspective signal 5Due to the substantial length of each review and rebuttal, we limit the human evaluation to 20 samples to ensure highquality expert judgments. that meaningfully improves rebuttal quality. As the research community continues to grow, we believe agentic systems like DRPG have the potential to help improve the quality and efficiency of scholarly discussions, thereby supporting the continued development of the academic community. Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, and Ngai Wong. 2025. Treereview: dynamic tree of questions framework for deep and efficient llm-based scientific peer review. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1566215693."
        },
        {
            "title": "Limitation",
            "content": "This work aims to design an academic rebuttal agent that generates fluent, grounded, and convincing rebuttal arguments. While DRPG shows strong performance, it primarily focuses on clarifying paper content and defending existing contributions, and isnt capable of conducting new experiments. In practice, additional experimental results can sometimes help address reviewers concerns during rebuttal. An interesting future direction is to integrate DRPG with AI Scientist systems to support experimental supplementation and achieve complete automation of rebuttal process."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work focuses on automating the academic rebuttal process. While language agents have the potential to significantly assist authors during rebuttal, they also entail inherent risks, particularly those related to hallucinations. Therefore, outputs generated by DRPG (as well as other rebuttal agents) should be carefully reviewed and verified by the authors before submission or release."
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, and 1 others. 2025. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925. Omer Aydin, Enis Karaarslan, Fatih Safa Erenay, and Nebojsa Bacanin. 2025. Generative ai in academic writing: comparison of deepseek, qwen, chatgpt, gemini, llama, mistral, and gemma. arXiv, abs/2503.04765. [Online; accessed 11-February2025]. Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. 2024. Elo uncovered: Robustness and best practices in language model evaluation. Advances in Neural Information Processing Systems, 37:106135106161. Nicolas Bougie and Narimasa Watanabe. 2024. Generative adversarial reviews: When llms become the critic. arXiv preprint arXiv:2412.10415. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, and 1 others. 2025. Rm-r1: arXiv preprint Reward modeling as reasoning. arXiv:2505.02387. Zirui Cheng and Jiaxuan You. 2025. Towards strategic persuasion with language models. arXiv preprint arXiv:2509.22989. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, and 1 others. 2024. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning. Rémi Coulom. 2006. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 7283. Springer. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Kazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yudai Yamazaki, Yasutaka Nishimura, Sina Semnani, Kazushi Ikeda, Weiyan Shi, and Monica Lam. 2024. Zero-shot persuasive chatbots with llm-generated strategies and information retrieval. arXiv preprint arXiv:2407.03585. Peixuan Han, Zijia Liu, and Jiaxuan You. 2025. Tomap: Training opponent-aware llm persuaders with theory of mind. arXiv preprint arXiv:2505.22961. Haorui He, Yupeng Li, Dacheng Wen, Yang Chen, Reynold Cheng, Donglong Chen, and Francis Lau. 2025. Debating truth: Debate-driven claim verification with multiple large language model agents. arXiv preprint arXiv:2507.19090. Zhe Hu, Hou Pong Chan, Jing Li, and Yu Yin. 2024. Debate-to-write: persona-driven multi-agent framework for diverse argument generation. In International Conference on Computational Linguistics. 9 Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Chuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang, Ruihua Song, and Huan Chen. 2024a. Persuading across diverse domains: dataset and persuasion large language model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 16781706. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. 2024b. Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708. Neha Nayak Kennard, Tim OGorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, and Andrew McCallum. 2022. Disapere: dataset for discourse structure in peer review discussions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 12341249. Chavvi Kirtani, Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Murari Mandal, and Dhruv Kumar. 2025. Revieweval: An evaluation framework for ai-generated reviews. arXiv e-prints, pages arXiv 2502. Sang-Woo Lee, Yu-Jung Heo, and Byoung-Tak Zhang. 2018. Answerer in questioners mind: Information theoretic approach to goal-oriented visual dialog. Advances in neural information processing systems, 31. Zixuan Li, Lizi Liao, and Tat-Seng Chua. 2024. Learning to ask critical questions for assisting product search. arXiv preprint arXiv:2403.02754. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, and 1 others. 2024. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024a. The ai scientist: Towards fully automated open-ended scientific discovery. Preprint, arXiv:2408.06292. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, ChanHung Yu, Hung-yi Lee, and Shao-Hua Sun. 2024b. Llm discussion: Enhancing the creativity of large language models via discussion framework and roleplay. arXiv preprint arXiv:2405.06373. Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. 2025. Llm4sr: survey on large language models for scientific research. arXiv preprint arXiv:2501.04306. Matan Orbach, Yonatan Bilu, Ariel Gera, Yoav Kantor, Lena Dankin, Tamar Lavee, Lili Kotlerman, Shachar Mirkin, Michal Jacovi, Ranit Aharonov, and 1 others. 2019. dataset of general-purpose rebuttal. arXiv preprint arXiv:1909.00393. Sukannya Purkayastha, Anne Lauscher, and Iryna Gurevych. 2023. Exploring jiu-jitsu argumentation for writing peer review rebuttals. arXiv preprint arXiv:2311.03998. Alexander Rogiers, Sander Noels, Maarten Buyl, and Tijl De Bie. 2024. Persuasion with large language models: survey. arXiv preprint arXiv:2411.06837. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, and Matthew Lease. 2024. Argumentative experience: Reducing confirmation bias on controversial issues through llmgenerated multi-persona debates. arXiv preprint arXiv:2412.04629. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, and 1 others. 2016. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489. Gaurav Singh and Kavitesh Kumar Bali. 2024. Enhancing decision-making in optimization through llmassisted inference: neural networks perspective. In 2024 International Joint Conference on Neural Networks (IJCNN), pages 17. IEEE. Somesh Singh, Yaman Singla, Harini SI, and Balaji Krishnamurthy. 2024. Measuring and improving persuasiveness of large language models. arXiv preprint arXiv:2410.02653. Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. to balance resistarXiv preprint 2024. ing and accepting persuasion. arXiv:2410.14596."
        },
        {
            "title": "Teaching models",
            "content": "Danqing Wang, Zhuorui Ye, Xinran Zhao, Fei Fang, and Lei Li. 2025a. Strategic planning and rationalizing on trees make llms better debaters. arXiv preprint arXiv:2505.14886. 10 Fuyu Wang, Jiangtong Li, Kun Zhu, and Changjun Jiang. 2025b. Inspiredebate: Multi-dimensional subjectiveobjective evaluation-guided reasoning and optimization for debating. arXiv preprint arXiv:2506.18102. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Persuasion for good: Towards personalized persuasive dialogue system for social good. arXiv preprint arXiv:1906.06725. Naifan Zhang, Ruihan Sun, Ruixi Su, Shiqi Ma, Shiya Zhang, Xianna Weng, Xiaofan Zhang, Yuhan Zhan, Yuyang Xu, Zhaohan Chen, and 1 others. 2025b. arXiv preprint Echo-n1: Affective rl frontier. arXiv:2512.00344. Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, and Jialiang Lin. 2025. Large language models for automated scholarly paper review: survey. Information Fusion, page 103332. Qiyao Wei, Samuel Holt, Jing Yang, Markus Wulfmeier, and Mihaela van der Schaar. 2025. The ai imperative: Scaling high-quality peer review in machine learning. arXiv preprint arXiv:2506.08134. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. Improving automated 2024. arXiv preprint research via automated review. arXiv:2411.00816. Cycleresearcher: Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh, and Noah Goodman. 2021. Open-domain clarification question generation without question examples. arXiv preprint arXiv:2110.09779. Jibang Wu, Chenghao Yang, Simon Mahns, Yi Wu, Chaoqi Wang, Hao Zhu, Fei Fang, and Haifeng Xu. 2025. Ai realtor: Towards grounded persuasive language generation for automated copywriting. In unknown. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, and Eduard Hovy. 2019. Lets make your request more persuasive: Modeling persuasive strategies via semisupervised neural nets on crowdfunding platforms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 36203630. Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, and Researchtown: Simulator Jiaxuan You. 2024. arXiv preprint of human research community. arXiv:2412.17767. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and Bowen Zhou. 2025. Dolphin: Closed-loop openended auto-research through thinking, practice, and feedback. arXiv preprint arXiv:2501.03916. Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, and Yang Yang. 2025a. Re2: consistency-ensured dataset for fullstage peer review and multi-turn rebuttal discussions. arXiv preprint arXiv:2505.07920."
        },
        {
            "title": "A Prompts",
            "content": "This section shows LLM prompts in the paper. Prompts used in the DRPG pipeline are shown in Figures 5 to 8. Note that Figure 7 is used in the Executor in Direct setting, and Figure 7 is used in other settings. After the Executor responds to each individual point, theyre merged together to form complete rebuttal. Refer to Section D.2 for illustrative examples. Prompts used in the evaluation are shown in Figures 9 and 10. To avoid the position bias, the order of the two rebuttals is randomly swapped during the comparison."
        },
        {
            "title": "B Data and Training Details",
            "content": "B.1 Data Statistics 50,000 review points to form the training set and an additional 5,000 review points for evaluation. The Planner is implemented as an MLP with three hidden layers of sizes 2048, 1024, and 512, respectively. The input layer takes the concatenated vectors of perspectiveparagraph pair with size of 2048, and the output layer produces scalar supportive score. Training is conducted for 3 epochs using batch size of 32 and learning rate of 5 105. Due to computational constraints, we freeze the parameters of the BGE-M3 encoder during training and update only the MLP module. B."
        },
        {
            "title": "Judge Model Training",
            "content": "Figure 4: GRPO training configuration for the judge model. Table 7: Dataset statistics. (a) Size of the dataset. Dataset Papers Reviews Rebuttals # Train # Eval 17,814 600 62,211 2,097 34,024 1,092 (b) Score changes after rebuttal. Dataset # Train # Eval Score Increased Score Decreased Score Unchanged 23,642 745 207 38,362 1,347 We summarize the data statistics in Table 7a. Note that only subset of official reviews are responded by the authors. Each official review includes score reflecting paper quality, and the scores may be changed during the rebuttal process. However, some initial scores are missing in the dataset. To reconstruct the full rebuttal trajectory, we utilize GPT-oss-120B (Agarwal et al., 2025) with the prompt in Figure 11 to predict initial scores from the rebuttal text and the final scores. Summary statistics of the rebuttal scores are shown in Table 7b. The resulting score distributions meet our expectations, and we further validated the predictions through human analysis of several randomly sampled examples. B.2 Planner Training As described in Section 3.3, we construct the Planners training data by combining five candidate perspectives generated by the idea proposer with one ground-truth perspective extracted from humanauthored rebuttals. Using this strategy, we collect Hyperparameter Learning rate Rollout temperature KL Coefficient Train batch size Mini batch size Micro batch size Training steps Number of Rollouts Value 1 106 1.0 0.001 64 32 16 200 4 The judge model is trained with Group Relative Policy Optimization (GRPO) (Shao et al., 2024), and the base model is Qwen3-4B. As shown in Section A, the judge model is expected to take careful thinking before generating final score. The reward is calculated as = 0.25ssg, where is the judge models answer, and sg is the actual score. This means the model will receive full reward when the predicted score matches exactly with the ground truth, and the reward decreases exponentially as the prediction gap increases. Figure 4 shows the hyperparameters during training. The Jiu-Jitsu Baseline We include Jiu-Jitsu (Purkayastha et al., 2023) as planning baseline. Different from our Planner, the Jiu-Jitsu Planner generates perspectives through selecting canonical rebuttal template. Given an atomic review point ri, Jiu-Jitsu maps the concern to canonical rebuttal template via retrieve-and-rank procedure. First, it converts ri into generated natural-language description di of the reviewer concern using fine-tuned sequence-to-sequence language model, which abstracts away surface wording differences and summarizes the underlying issue. Second, it assigns di to the closest attitude roottheme cluster zi (where the root corresponds to reviewing aspect such as Clarity and the theme corresponds to target paper section such as Experiments), which is associated with cluster description dzi. Third, it uses the rebuttal action label ai provided in the Jiu-Jitsu resources for each (di, zi) to retrieve the relevant candidate rebuttals for ranking. Finally, Jiu-Jitsu scores each candidate R(zi) using the cluster description dzi and action ai, then selects the top-ranked candidate as the canonical rebuttal, which serves as the rebuttal perspective in our pipeline. Figure 12 shows an example of the procedure of Jiu-Jitsu baseline and the selected canonical rebuttal."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Details for human study Our human study is conducted through web interface for human annotators to interact with, and the results are automatically collected. Figure 13 shows screenshot of the webpage. We also provide the results for all 20 reviews in Table 8. In the human study, we shuffled the order of 20 reviews for each human annotator. D.2 Case Study Figures 14 and 15 shows two examples comparing Decomp, DRG and DRPG. We also provide the comments from GPT-4o during the pairwise comparison, from which we can observe the value of the Retriever and Planner in providing more concrete rebuttals. 13 You are an experienced researcher in computer science. You have written conference paper in the field of computer science or AI and received review. You need to analyse the reviewers comments. Specifically, identify and list all the weakness points or confusions raised by the reviewer. - You may omit minor issues such as typos, but major comments should all be mentioned. - Preferably, extract sentences or words directly from the review. Do not oversimplify the comments. Below is an example of the expected output format: [ \"The paper introduced two modules, but lacks ablation study which includes only one of them.\", \"What does the author mean by PPO? Further explain will be helpful.\", \"The experimental results are only shown on 1 newly created environment.\" ] Figure 5: System Prompt for the Decomposer You are an experienced researcher in computer science. You have received review on research paper. Your task is to propose up to 5 perspectives to address this point in the rebuttal. - The perspective should either show the reviewers point wrong, or show that the work is valuable even though the review is correct. Specifically, You MUST consider the following two types of perspectives: - Clarification: The reviewer may have factual errors or misunderstood in the paper. For example, they may say something is missing when its actually present in the paper, or say the methodology is wrong because of misunderstanding. - Justification: Defend your choices and explain why the comment doesnt undermine your paper. For example, they may require an experiment which is unfeasible or unnecessary, or require empirical results for theoretical paper. - DO NOT propose suggestions or promises for future revision or future work. - DO NOT mention specific locations in the paper since you wont be able to access it (e.g. \"in section 3.2\"). Below is an example of the expected output format: Input: \"The paper introduced two modules, but lacks ablation study which includes only one of them.\" Output: [ \"Clarification: we have actually included such experiment in the paper.\", \"Clarification: the two modules are dependent on each other and therefore cannot be separated.\", \"Justification: the ablation study is not necessary as each module has been individually validated in prior work.\" ] Figure 6: System Prompt for the Perspective Generator You are an experienced researcher in computer science. You have written conference paper in the field of computer science or AI and received review. You need to write rebuttal to address the reviewers comments and convince them to increase their score. Guidelines: 1. Be polite, concise, and professional. Make sure all responses are factual, respectful, and persuasive. 2. Address each comment point-by-point. Its recommended to format the main part of the rebuttal as: \"Question: ...Response: ...\". For each point: 3. For each point, you should respond with clear reasoning, and evidence from the original paper, and your professional knowledge. - If the comment has misunderstood the paper or missed some content, clarify the point. If not, defend your choices and explain why this comment doesnt undermine your paper. - DO NOT propose suggestions or promises for future revision or future work. 4. Be confident with your paper. Try your best to explain and validate your work, and rebut the concerns raised by the reviewer. 5. Your rebuttal should be concise and no more than 1000 words. You should directly generate passage without additional comments or thoughts. Figure 7: System Prompt for the Executor for Whole Review 14 You are an experienced researcher in computer science. You have written conference paper in the field of computer science or AI and received review. You need to write rebuttal to address the reviewers comment and convince them to increase their score. Guidelines: 1. Make sure your response is factual, respectful, and persuasive. 2. You should respond with clear reasoning, and evidence from the original paper, and your professional knowledge. - If the comment has misunderstood the paper or missed some content, clarify the point. If not, defend your choices and explain why this comment doesnt undermine your paper. - DO NOT propose suggestions or promises for future revision or future work. 3. Be confident with your paper. Try your best to explain and validate you work, and rebute the concerns raised by the reviewer. 4. Your rebuttal should be concise and no more than 200 words. You should directly generate paragraph without additional comments or thoughts. Figure 8: System Prompt for the Executor for Individual Review Points You are an experienced academic paper reviewer. You will receive response from the authors addressing your review comments. Your task is to evaluate the response and decide whether to adjust your original score for the paper. The scoring rubric is from 1 - 10 scale. Certain scores correspond to the following meanings: - 1: The paper has serious flaws, lacks novelty, or is clearly unsuitable for acceptance. - 3: The paper has significant weaknesses or insufficient contributions. - 6: Top 25% of all submissions. The paper is slightly above the acceptance threshold, with generally solid work, but some limitations. - 8: Top 10% of all submissions. The paper has good-quality paper with clear contributions and well-supported results. - 10: Top 5% of all submissions. The paper makes exceptional contributions and is recommended for spotlight or oral presentation. You should focus on the following criteria when assessing the authors response: - 1. Does the authors response validates their work with clear arguments and coherent logic? - 2. Does the author provide sufficient evidence or reasoning to support their claims? - 3. Is the authors response consistent with the content of the original paper? In addition, please keep in mind that the goal of the response is to CONVINCE the reviewer about the paper, instead of SUGGESTIONS for future work or ADMITTING weakness. - DO NOT consider suggestions, promises, or impacts for future work and revisions when evaluating the responses. Focus on this paper alone. - DO NOT consider tones or emotional appeals, as long as the content is professional. Focus on the logic and reasoning. Then, you should decide whether to change your score based on the authors response. - You should be confident with your original review in most cases. You may increase your score only if the author provides sufficient reasoning that addresses your comments. - Do not increase your score based on minor corrections (e.g. typos) or promises on future revisions. - If the original score is low, you should be more lenient in increasing the score. If the original score is high, you should hold higher standard. - In most cases, the score change will be small. Large changes, like 2 points, should be rare and well-justified. As conclusion, output \"My final score is X\" where is your final score (an integer between 1 and 10). Figure 9: System Prompt for the Rebuttal Judge 15 You are an experienced academic paper reviewer. You will receive review of an academic paper in computer science, and two responses from the authors. Your task is to evaluate the responses and decide which response is better. The response may address the reviewers several comments. You should compare the responses to each comment individually. When comparing the responses, you can refer to the following criteria: - 1. Does the authors response validate their work with clear arguments and coherent logic? - 2. Does the author provide sufficient evidence or reasoning to support their claims? - 3. Is the authors response consistent with the content of the original paper? In addition, please keep in mind that the author isnt allowed to revise the paper afterwards. That is, the goal of the response is to CONVINCE the reviewer about the paper, instead of SUGGESTIONS for future work or ADMITTING weakness. - DO NOT consider suggestions, promises, or impacts for future work and revisions when evaluating the responses. Focus on this paper alone. - DO NOT consider tones or emotional appeals, as long as the content is professional. Focus on the logic and reasoning. Please give concrete evidence while being concise. DO NOT repeat or simply summarize the responses content or similarities; focus on their differences and YOUR ANALYSIS. Output \"I think response (1 or 2) is better\" or \"I think two responses are similar in quality\" at the end of your answer. Figure 10: System Prompt for Comparing Two Rebuttals System: You will be given reviewerauthor discussion text and the papers final score. Based only on the discussion text and the final score, predict the papers initial (pre-discussion) review score. Strictly output single valid JSON object and nothing else. The JSON must contain only these two fields: { \"opinion\": \"In 26 concise sentences, explain your analysis and list the main evidence/signals that support your prediction. If information is insufficient or contradictory, note that uncertainty here\", \"initial_score\": \"initial score as an integer from 1 to 10\" } Hard rules (must follow): 1. **Output only the JSON object** no extra commentary, no code fences outside the JSON, no explanations. 2. initial_score must be an integer between 1 and 10. 3. opinion must mention 24 clear signals or events from the discussion and explain how they affect the score estimate. 4. Do not invent facts outside the provided discussion text. Avoid hallucination. 5. If the discussion is ambiguous or contradictory, state that in opinion and then give the most likely integer prediction. Usually, the reviewer is confident with their review, which means they only raise or decrease scores where there is sufficient evidence. User: Discussion text: {discussion_text } Final score: {final_score } / Figure 11: System Prompt to Predict the Initial Review Score Review point ri: The experimental setup is unclear. Please specify the hyperparameters and training details. Generated description di: Missing experimental details and unclear description of training settings. Predicted roottheme cluster zi: Root=Clarity, Theme=Experiments Action label ai: rebuttal_concede-criticism Selected canonical rebuttal template ci (perspective): We apologize for the unclear description of the experimental settings. We will revise the paper to include the missing hyperparameters and training details. Figure 12: An example of Jiu-Jitsu procedure to generate rebuttal perspective for review point. 16 Figure 13: Illustration of the webpage used for human annotation. Review Point: Adding factual data in fact may not always lead to clear improvements, while in general most improvement appears to be driven by the heuristic ATE weighting. Decomp Response: We appreciate the reviewers comment and would like to clarify that the addition of factual data does indeed contribute to the overall improvement, even if its individual contribution may seem marginal in some cases. The comparison between the purple and green bars in Figure 2 and Figure 3, as well as Table 2, shows that the DT policy trained on both factual and counterfactual simulations (purple bar) outperforms the DT policy trained on counterfactual simulations only (green bar). This suggests that the factual data provides some benefit, even if the majority of the improvement comes from the counterfactual simulations and the ATE weighting scheme. Furthermore, the factual data helps to anchor the DT policy in the source environment, making the transfer to the target environment more effective. Therefore, while the ATE weighting scheme is key component of our approach, the inclusion of factual data is also an important factor in achieving the best results. DRPG Response: We appreciate the reviewers observation that the addition of factual data may not always lead to clear visual improvements, as seen in the comparison between the purple and green bars or in Table 2. However, we would like to clarify that the inclusion of factual data is intended to provide more comprehensive understanding of the results, rather than solely driving improvements. The ATE weighting scheme is indeed key factor in enhancing the generalization ability of the DT agents, as demonstrated in Figures 2 and 3. The factual data serves as foundation, allowing the counterfactual simulations to build upon and improve the overall performance. While the improvements may not always be drastic, the combination of factual and counterfactual data with ATE weighting consistently yields better results, as evident in the overall trend of our experimental findings. This suggests that the addition of factual data, although sometimes subtle in its impact, contributes to more robust and effective training scheme for DT agents. Comment: Both responses provide coherent arguments and reasoning, but Response 2 has slight edge due to its emphasis on the foundational nature of the work and its detailed explanation on the role of factual data and ATE weighting. Figure 14: case study comparing Decomp amd DRPG. Review Point: The dataset contains only 20 images. DRG Response: We acknowledge the reviewers concern regarding the size of the dataset used in our experiments. However, we would like to clarify that the dataset size was intentionally kept small to demonstrate the feasibility and effectiveness of the proposed SinGAN approach in hiding images in deep probabilistic models. With only 20 test image pairs, we were able to achieve promising results in terms of extraction accuracy and model security, as shown in Tables 1 and 2. Furthermore, the use of small dataset allowed us to focus on the quality and diversity of the generated cover images, as well as the weight distribution similarity between the original and stego SinGANs. While larger dataset may be desirable for more comprehensive evaluation, our current results provide solid foundation for future work and demonstrate the potential of the proposed framework. DRPG Response: We appreciate the reviewers comment regarding the dataset size. Although our experiments involve dataset of 20 images, we would like to emphasize that this is sufficient for proof-of-concept demonstration of our probabilistic image hiding framework. The primary goal of our work is to showcase the feasibility and effectiveness of hiding images in deep probabilistic models, particularly using SinGANs. With smaller dataset, we are able to establish baseline for the performance of our approach, as evidenced by the extraction accuracy results in Table 1 and the security evaluation in Table 2. Moreover, our methods ability to hide multiple images for different receivers, as shown in Table 3, further demonstrates its potential. While we acknowledge that larger datasets may be necessary for more comprehensive evaluations, our current results provide solid foundation for future work, which can indeed focus on scaling up to larger datasets and exploring more complex scenarios. Comment: Both responses argue that the small dataset size is adequate for proof-of-concept demonstration. However, Response 2 provides more detailed explanation by referencing specific tables in the paper that demonstrate the methods effectiveness, thus providing more evidence to support their claim. Figure 15: case study comparing DRG amd DRPG. 18 Table 8: Details for human study. Rebuttal 1 refers to the rebuttal generated by the DRG pipeline, and Rebuttal 2 refers to the rebuttal generated by DRPG. Number Paper Source Human Annotator 1 Human Annotator 2 Human Annotator 3 GPT-4o 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 EMNLP2020 ICML2020 KDD2023 ICML2020 CoRL2023 AAAI2022 ICLR2023 ACL2020 ICLR2022 NeurIPS2022 ICLR2022 ECCV2022 NeurIPS2022 ACL2020 ICLR2022 AAAI2022 SIGKDD2022 CoRL2023 ICLR2023 ICML Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 2 Rebuttal 1 Rebuttal 2 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 1 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal 2 Rebuttal"
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}