{
    "paper_title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving",
    "authors": [
        "Vincenzo Colle",
        "Mohamed Sana",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Fadhel Ayed",
        "Merouane Debbah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research."
        },
        {
            "title": "Start",
            "content": "TeleMath: Benchmark for Large Language Models in Telecom Mathematical Problem Solving Vincenzo Colle, Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Fadhel Ayed, Merouane Debbah Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France Universit`a degli Studi di Cassino del Lazio Meridionale, Cassino, Italy Khalifa University of Science and Technology, Abu Dhabi, UAE 5 2 0 2 2 1 ] A . [ 1 4 7 6 0 1 . 6 0 5 2 : r AbstractThe increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from selected seed of problems crafted by Subject Matter Experts. The evaluation of wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with large number of parameters, often struggle with these challenges. We have released the dataset1 and the evaluation code to ease result reproducibility and support future research. I. INTRODUCTION As the telecom industry advances toward next-generation networks, with 5G and the upcoming 6G, Artificial Intelligence (AI) and Machine Learning (ML) are expected to play an increasingly significant role. Within this evolving landscape, Large Language Models (LLMs) have emerged as powerful tools for assisting and automating complex tasks in diverse technical fields thanks to their semantic and reasoning abilities. Importantly, LLMs have improved significantly in areas such as arithmetic, algebra, and more broadly, mathematical reasoning, largely driven by the scale and diversity of their training data. Moreover, advancements in prompting techniques [1] [2] and reasoning strategies via Reinforcement Learning (RL) have further enhanced their capabilities, enabling them to tackle increasingly complex and abstract challenges [3]. In the telecom-domain, LLMs are explored for automatic generation of code, protocols [4] [5] and network configurations [6] [7]. Furthermore, researchers are investigating whether LLMs are also effective in sophisticated optimization [8] and forecasting problems [9]. To realize these tasks, LLMs must possess deep understanding of the underlying mathematical principles that govern them. 1https://huggingface.co/datasets/netop/TeleMath Despite recent efforts in evaluating LLMs on broad-view mathematical problems see MATH [10] and GSM8K [11] and telecom-related tasks, such as protocol summarization [12], standard document classification [13] and general telecom knowledge [14], comprehensive assessment of the LLMs mathematical capabilities within the telecomdomain, which often require not only numerical precision but also domain-specific knowledge, remains less understood. Although recent work has explored the LLM abilities in problem modeling and equation completion for the telecom domain [15], the challenging skill of solving mathematical problems, has not received any attention yet. This paper addresses this specific need by introducing TeleMath, the first benchmark dataset designed to evaluate LLM capabilities in solving mathematical problems in the telecommunications domain. The contributions of this paper are as follows: 1) We introduce novel framework to generate synthetic question-answer (QnA) pairs on different categories of mathematical problems. Starting from an initial set of mathematical problems seed dataset generated by Subject Matter Experts (SMEs), our framework first extracts reusable problem blueprints from each element of the seed dataset; then, it uses the blueprints to produce large set of synthetic QnAs. This approach facilitates the scalability of the entire process and makes our framework suitable to design synthetic dataset for different categories and domains. 2) Building upon the data augmentation framework, we curate and publicly release TeleMath, dataset consisting of 500 QnAs to assess LLM ability to solve mathematical problems in the telecoms domain. 3) Finally, we benchmark leading open-source LLMs on TeleMath. This evaluation shows that recent reasoningoriented models, explicitly designed to think step by step and explore multiple approaches, perform significantly better than general-purpose models. II. DATASET SOURCES AND CHARACTERISTICS This section introduces the seed dataset, which forms the backbone for building TeleMath. Next, we highlight the characteristics of TeleMath and we present the rationale behind our design choices. 2 Question: question designed to test specific concept within the telecommunications domain, framed so that the answer will always be numerical value. Answer: The correct numerical answer to the question, represented either as decimal or integer number, depending on the required quantity. The unit of the answer is either explicitly stated in the question or clearly implied by context, thus avoiding the need for unit conversion during validation, reducing the risk of errors. Category: label classifying the QnA within specific domain of telecommunications. Figure 1 provides an overview of TeleMath category distribution. Tags: Keywords that further categorize the QnA, allowing for granular classification. For example, question about SNR might be tagged with SNR, signal processing, and noise. Difficulty: The difficulty level of the question, classified as either Basic or Advanced. III. TELEMATH DATASET CREATION The full generation of hundreds of problems by dedicated SMEs with step-by-step solutions and formula derivations would be prohibitively expensive and time-consuming. To address this challenge, we develop framework composed of two synthetic data generation pipelines that expand small seed dataset of problems generated by SMEs. These two pipelines serve complementary roles: one is dedicated to generating blueprints for problems with numerical answers, while the other handles problems whose solutions are equations. Together, they enable the generation of large volumes of highquality data in scalable manner. Figure 2 shows the overall dataset creation framework composed by four key blocks, namely: problem decomposition, blueprint generation, synthetic data generation, and postprocessing. The following subsections elaborate on each of these blocks. A. Problem Decomposition In this block, we employ Qwen2.5-72B-Instruct, chosen to break for its strong instruction adherence capabilities, down complex SME problems into more granular units, i.e., subproblems. We prompt the LLM with the original problem, its solution, and the complete, step-by-step explanation. The problem decomposition is made of the two following phases: 1) Subproblems Derivation: In this task, the LLM analyzes the problems provided by the SMEs and identifies individual steps, or short sequences of logically connected steps, to develop self-contained subproblems. The SMEs manually numbered each step in the solutions of their problems, aiding the LLM to fulfill this task. For each identified subproblem, the model is also instructed to derive the corresponding solution, inferring it from the original problem. 2) Subproblems Review: In this task the LLM verifies that each derived subproblem is well-posed, unambiguous, and self-contained. Importantly, with this process, we confirm that each subproblem does not require additional information from the original SME problem. Figure 1: Distribution of TeleMath QnAs across their categories. A. TeleMath Seed Dataset We generate and curate the seed dataset with the help of 10 SMEs, who designed an initial set of 50 problems covering diverse areas, from fundamental concepts to more advanced topics in the telecommunications field. These problems span various difficulty levels to ensure that the dataset is comprehensive and reflects the variety of telecommunications domains. In addition, for each problem, the SMEs provided, together with the solution, detailed step-by-step explanation, in symbolic or textual form, to enable effective generation of the TeleMath dataset. B. Dataset Characteristics TeleMath adopts QnA format where answers are strictly numerical quantities. Although telecommunications problems often involve symbolic expressions, we prioritize numerical answers for two reasons: Validation Robustness: Numerical answers enable simple and clear evaluation. Answers containing formulas introduce variability e.g., due to the generation of expressions equivalent to the ground truth but different from syntactical point of view, which may trigger false negatives during automated grading, even when employing Computer Algebra Systems such as Mathematica and SymPy. (e.g., Domain-Specific Practicality: In telecommunications engineering, numerical signal-to-noise results plus interference ratios) are often the primary input for real-world decision-making (e.g., antenna placement, network optimization, and protocol configuration). By focusing on numerical answers, the dataset mirrors the types of calculations engineers regularly perform, ensuring that models are trained to generate practical, actionable results. Dataset Format. To maintain consistency across the dataset, we standardize the format for each QnA in JSON, with the following fields: 3 Figure 2: high-level overview of the pipelines for Q&A generation, with the symbolic math-driven path highlighted. B. Blueprint Generation"
        },
        {
            "title": "Original SME Problem",
            "content": "The core of the question generation methodology lies in creating versatile blueprints, either in the form of executable code or symbolic mathematical formulas, based on the derived subproblem. These blueprints can be instantiated to produce diverse and large set of QnAs. We define two approaches to derive blueprints from subproblems derived in the problem decomposition phase: 1) Code-Driven Blueprint Generation: This approach converts subproblems for which the expected answer is numerical value into executable Python codes. For this task, we select Qwen2.5-Coder-32B-Instruct, for its strong performance on code generation benchmarks. The model is provided with the subproblem along with the full step-by-step explanation, which serve as context to generate the corresponding Python code, i.e., the codedriven blueprint. The code is then executed with the original values of the subproblem parameters, and its output is compared with the expected result. If there is mismatch, the generated code blueprint is discarded. Figure 3 shows an example of generation of Python blueprint, including the original problem and the derived subproblem. 2) Symbolic Math-Driven Blueprint Generation: For subproblems where the solution involves deriving an equation, we extract the underlying symbolic expression and into reusable blueprints for generating transform it numerical QnAs. Given that symbolic expressions require rigorous structural consistency for reliable parsing, the process begins with standardizing the relevant portion of the solution from the original problem. This is handled by Qwen2.5-72B-Instruct, which reformats the mathematical expressions into consistent LaTeX representation. The LaTeX code is then parsed using SymPy, Python library for symbolic mathematics, which converts it into structured algebraic form with free parameters, i.e., the symbolic blueprint. C. Synthetic Data Generation From the generated blueprints, new QnAs can be created by defining new input parameters. To do so, for each subproblem, we instruct an LLM, Qwen2.5-72B-Instruct, to generate plausible inputs for the generated blueprints using the original problem and step-by-step solution as context. This contextual information provides strong reference to the LLM for proposing realistic and well-defined input parameters. For example, if subproblem involves calculating signal loss for 5G small cell at 3.5 GHz over certain distance, Question: wireless transmitter operating at carrier frequency of = 3.5 GHz communicates with receiver located at distance of = 0.2 km, with channel bandwidth of = 1 MHz. Both the transmit and receive antennas have gains of Gt = Gr = 2 dBd, and the receiver has noise figure of = 5 dB. a) Calculate the free-space path loss, PL. b) Determine the minimum transmit power, Pt required to achieve SNR of 15 dB at the receiver. c) Assess whether the transmitter complies with EIRP limit of 23 dBm. Step-by-Step Explanation: 1) Convert antenna gains from dBd to dBi. 2) Compute free-space path loss. 3) Calculate receiver noise power. 4) Calculate required received power. 5) Find minimum required transmit power. 6) Verify EIRP limit compliance. Solutions: a) PL = 89.35 dB b) Pt = 12.9 dBm c) EIRP = 8.8 dBm < 23 dBm = Compliant."
        },
        {
            "title": "Derived Subproblem",
            "content": "Question: Compute the free-space path loss at 3.5 GHz over 0.2 km. Solution: 89.35 dB"
        },
        {
            "title": "Generated Blueprint",
            "content": "import math def path_loss(freq_in_GHz, dist_in_km): return 20*math.log10(dist_in_km) + 20*math.log10(freq_in_GHz) + 92.45 Figure 3: Example of an original problem, together with one of the derived subproblems and the associated blueprint. the model, drawing on its vast training data, will propose other realistic frequencies and distances suitable for similar small cell scenarios, rather than arbitrary numbers. This ensures that the new parameters are contextually appropriate for the problem domain. Then, the blueprint is instantiated with the"
        },
        {
            "title": "Generation of a QnA from a subproblem with\nsymbolic solution",
            "content": "Question: Calculate the Shannon channel capacity for wireless communication system with bandwidth and signal-to-noise ratio SNRlinear. Symbolic Solution: = log2(1 + SNRlinear) Generated Blueprint: = B*log(1 + SNR_linear, 2) Generated Parameters: (B): 5 MHz, (SNRlinear): 100 SymPy Code Execution: = 5e6*log(1 + 100, 2) = 33.29e Question: Calculate the Shannon channel capacity, in Mbps, for wireless communication system with bandwidth of 5 MHz and linear SNR of 100. Numerical Solution: 33.29 Mbps Figure 4: Generation of QnA from subproblem with symbolic solution. proposed parameters by executing the generated Python code for code-driven blueprints and the Sympy code for symbolic blueprints, respectively. D. Post-Processing Following the data generation step, the process transitions into the post-processing stage. This phase ensures that the generated data is both usable and consistent with the original problem structure, which is realized by three functions: 1) Filtering: This step eliminates any generated outputs that fall outside acceptable bounds. For example, excessively large or small values for distance, frequency, or power are often indicative of invalid input parameters. By enforcing domain-specific thresholds, it is ensured that only realistic and physically plausible problem instances are retained. 2) Question Editing: The numerical parameters defined in the data generation step are injected into the original question. This editing is performed by prompting Qwen2.5-72B-Instruct with both the original question and dictionary of the generated parameter names and their corresponding values. The model is instructed to substitute the symbols and the values in the original question with the provided values, preserving the original structure of the text. Figure 4 shows an example of the generation of new question from subproblem with symbolic solution. 3) Semantic Validation: dedicated validator powered by Qwen2.5-72B-Instruct is employed to compare each rewritten question against its original counterpart. The goal is to ensure semantic fidelity by verifying: Structural Equivalence: The underlying problem structure and required solution steps remain unchanged. 4 Dimensional Consistency: All physical units are preserved and logically consistent. After processing, each QnA is annotated with auxiliary metadata category, tags, and difficulty assigned heuristically by Qwen2.5-Math-7B-Instruct. To do so, we prompt the model with the problem statement, instructing it to select category and to propose tags that capture finer-grained concepts. The output categories are further clustered and each cluster is renamed to avoid an excessive number of categories in TeleMath. The QnA difficulty is determined by the abulity of Qwen2.5-Math-7B-Instruct to correctly answer it. The average token count of these solutions is used as proxy for complexity. The distribution of the average token counts shows bimodal pattern, naturally separating TeleMath problems into two groups: those with shorter solutions (fewer tokens, indicating lower complexity) are labeled as Basic, and those with longer solutions (more tokens, indicating higher complexity) are labeled as Advanced. IV. PERFORMANCE EVALUATION In this section, we present the performance of popular opensource language models on TeleMath. In our experiments, we generate = 16 independent responses for each question and model pair. This generation process uses sampling temperature of 0.6, top-p value of 0.90 and maximum generation length of 16,384 tokens for all models, except for Qwen2.5-Math-72B-Instruct and Qwen2.5-Math-7B-Instruct, which are limited to maximum generation length of 4,096 tokens. From the generated responses, we evaluate the model performance on TeleMath using two metrics: pass@1 (pass at 1) and cons@16 (consensus at 16): pass@1: This metric assesses the ability of the model to generate correct answer within single attempt, averaged over multiple sample answers. cons@16: This metric evaluates performance based on majority voting over the 16 generated answers. The problem is considered correctly solved if the most frequently occurring answer among the generated ones is correct. A. Models Performance Analysis Table summarizes the performance of various reasoning and non-reasoning models on TeleMath. Specifically, this table presents the pass@1 and cons@16 accuracies achieved by each model across the different QnA categories of TeleMath. Reasoning Models. Our benchmarking shows that Qwen332B outperforms all other models achieving an average pass@1 of 69.51% and cons@16 of 76%. Following Qwen3-32B, DeepSeek-R1-Distill-Llama-70B (pass@1 53.21%, cons@16 60.80%) and Phi-4-reasoning+ (pass@1 53.56%, cons@16 58.40%) form distinct second tier. It is worth to notice the performance of the smallest reasoning model, Qwen3-4B, which surpasses that of substantially larger non-reasoning models. This finding underscores the impact of architecture and training methodologies optimized for reasoning tasks. Such optimization allows even more compact models to outperform larger, general-purpose counterparts in these Model Category Computer Networking [CN] Electrical Engineering [EE] Information Theory [IT] Operations Research [OS] Probability & Statistics [PS] Signal Processing [SP] Telecom. Engineering [TE] Overall Performance Accuracy Metric pass@1 cons@16 pass@1 cons@ pass@1 cons@16 pass@1 cons@16 pass@1 cons@16 pass@1 cons@16 pass@1 cons@16 55.99 66. 72.92 77.78 76.99 81.82 70.63 72.04 77.47 80.73 71.05 77.94 62.25 73. pass@1 cons@16 69.51 76.00 Top Domain pass@1 PS Reasoning Non-reasoning Qwen3 32B DeepSeek-R1 Distill-Llama-70B Phi-4 Reasoning+ Qwen3 4B Qwen2.5 Math-72B* Llama-3.3 70B* Qwen2.5 Math-7B* Llama-3.1 8B* 5 47.66 54.17 72.92 77. 62.07 75.00 55.98 64.52 70.81 75.23 43.93 55.88 40.28 46.41 53.21 60. EE 30.99 29.17 66.67 77.78 70.74 79.55 54.17 56.99 67.60 71. 52.39 63.24 41.54 45.10 53.56 58.40 IT 14.32 12.50 65.28 66. 64.77 72.73 49.40 53.76 59.58 63.30 41.18 50.00 33.62 36.60 45.62 50. EE 26.61 32.26 55.80 64.29 39.70 46.48 52.39 58.26 49.49 52. 36.11 45.56 30.50 38.05 39.99 46.48 EE 26.30 29.17 63.19 66. 38.21 36.36 40.99 50.54 49.77 56.88 32.17 39.71 24.88 23.53 36.23 40. EE 6.51 12.50 27.78 33.33 27.98 31.82 27.22 26.88 34.52 39. 21.14 27.94 11.89 16.99 22.38 26.60 PS 4.95 0.00 34.03 55. 13.64 22.73 14.45 22.58 16.40 22.02 15.63 25.00 10.21 15.03 13.56 20. EE Table I: Performance comparison of pass@1 and cons@16 accuracy. Asterisks (*) denote Instruct variants (e.g., Qwen2.5-72B-Math* stands for Qwen2.572B-Math-Instruct). specialized evaluations. For instance, in Information Theory, Qwen3-4B clearly outperforms Llama-3.3-70B-Instruct and Qwen2.5-Math-72B-Instruct, despite being approximately 17 times smaller. Non-Reasoning Models. On the other hand, Non-reasoning models generally lag behind reasoning models. Within the former group, Qwen2.5-Math-72B-Instruct leads with an overall pass@1 of 39.99% and cons@16 of 46.48%, followed by Llama-3.3-70B-Instruct at 36.23% (cons@16 40.20%). It is noteworthy that the Math specialized models (Qwen2.5-Math-72B-Instruct and Qwen2.5-Math-7B-Instruct) operate with constrained maximum completion length of 4096 tokens. Despite this limitation, Qwen2.5-Math-72BInstruct emerges as the best non-reasoning model. This suggests its specialized mathematical training can yield acceptable results even within such constraints. However, small non-reasoning models like Qwen2.5-Math-7B-Instruct (pass@1 22.38%, cons@16 26.60%) and Llama-3.1-8BInstruct (pass@1 13.56%, cons@16 20%) exhibit considerably lower performance across the board. Model Scale. The influence of model scale is consistently evident within model families. For instance, Qwen3-32B significantly outperforms Qwen3-4B. Similar trends are observed for the Qwen2.5-Math (72B vs. 7B) and Llama-3.x (70B vs. 8B) series. This observation reinforces the general principle that larger models within the same architectural family tend to yield better results. Domain Performance. Domain-wise, Electrical Engineering emerges as field where several models achieve strong scores. This the non-reasoning Llama-3.3-70BInstruct (pass@1 63.19%, cons@16 66.67%), which is more includes than 25 percentage points higher than the overall average. Such performance is likely due to greater representation of Electrical Engineering content in the training data, compared to more specialized fields. In contrast, Computer Networking and Telecommunications Engineering proved to be more challenging for most models. This is especially true for smaller non-reasoning models, where scores often fall below 20%. In summary, the analysis highlights the current superiority of reasoning-focused architectures. Among the evaluated models, Qwen3-32B consistently achieves the highest average performance in each category. The results also suggests that for complex technical domains requiring nuanced understanding and problem-solving, models designed and trained for reasoning offer clear advantage. This holds true even when comparing smaller reasoning models to larger non-reasoning counterparts. Although large general-purpose instruct models and specialized mathematical models show promise and utility, they generally do not reach the same level of consistent high performance as the top-tier reasoning models. V. LIMITATIONS TeleMath represents significant step forward in evaluating LLMs on telecommunications-specific mathematical problems. However, despite our efforts to ensure broad coverage, the seed dataset (50 problems) does not fully represent the entire telecommunications domain. Moreover, the current distribution of problem categories (as shown in Fig. 1) skews toward traditional telecommunications engineering (30.6%). Although this provides depth in core area, it may also imply that other important categories within the telecommunications landscape are under-represented. 6 [15] X. Li et al., WirelessMathBench: Mathematical Modeling Benchmark for LLMs in Wireless Communications, in Findings of the Association for Computational Linguistics: ACL 2025, 2025. Additionally, using Qwen family models for SME problem decomposition, blueprint generation, and review may introduce some bias in the TeleMath dataset, favoring higher scores for these models. However, we have limited this bias through the semantic validation step, which aim at ensuring semantic fidelity of the generated QnAs with respect to the original problems. VI. CONCLUSIONS This paper introduces TeleMath, novel benchmark dataset specifically developed to assess the mathematical problemsolving abilities of LLMs within the telecommunications domain. Recognizing gap in evaluating LLMs on specialized, quantitative tasks, TeleMath offers 500 curated questionanswer pairs sourcing from expert knowledge augmented through robust synthetic generation pipelines. Our evaluation of diverse open-source LLMs on this dataset reveals key insight: models explicitly designed for mathematical or logical reasoning consistently achieve superior performance compared to even larger, general-purpose counterparts. By making TeleMath publicly available, we provide resource for the research community to rigorously benchmark, understand, and ultimately enhance LLM capabilities for the complex mathematical demands inherent in the telecommunications sector and similar technical fields."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, in Advances in Neural Information Processing Systems (NeurIPS), vol. 35, 2022, pp. 24 82424 837. [2] D. Zhou et al., Least-to-Most Prompting Enables Complex Reasoning in Large Language Models, in International Conference on Learning Representations (ICLR), May 2023. [3] D. Guo et al., DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, arXiv preprint arXiv:2501.12948, 2025. [4] F. Ayed et al., Hermes: Large Language Model Framework on the Journey to Autonomous Networks, arXiv preprint arXiv:2411.06490, 2024. [5] Z. liu et al., LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols, arXiv preprint arXiv:2505.16821, 2025. [6] C. Wang et al., NetConfEval: Can LLMs Facilitate Network Configuration? Proc. ACM Netw., vol. 2, no. CoNEXT2, Jun. 2024. [Online]. Available: https://doi.org/10.1145/3656296 [7] R. Mondal et al., What do LLMs need to Synthesize Correct Router Configurations? in Proceedings of the 22nd ACM Workshop on Hot Topics in Networks, ser. HotNets 23, 2023, p. 189195. [8] C. Yang et al., Large Language Models as Optimizers, in International Conference on Learning Representations (ICLR), May 2024. [9] T. Cui et al., TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation, arXiv preprint arXiv:2504.04222, 2025. [10] D. Hendrycks et al., Measuring Mathematical Problem Solving With the MATH Dataset, in Proceedings of the Neural Information Processing Systems (NeurIPS), vol. 1, 2021. [11] K. Cobbe et al., Training Verifiers to Solve Math Word Problems, arXiv preprint arXiv:2110.14168, 2021. [12] I. Karim et al., SPEC5G: Dataset for 5G Cellular Network Protocol Analysis, in Findings of the Association for Computational Linguistics: IJCNLP-AACL, 2023, pp. 2038. [13] H. Zou et al., TelecomGPT: Framework to Build Telecom-Specfic Large Language Models, arXiv preprint arXiv:2407.09424, 2024. [14] A. Maatouk et al., TeleQnA: Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge, IEEE Network, pp. 11, 2025."
        }
    ],
    "affiliations": [
        "Khalifa University of Science and Technology, Abu Dhabi, UAE",
        "Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France",
        "Università degli Studi di Cassino del Lazio Meridionale, Cassino, Italy"
    ]
}