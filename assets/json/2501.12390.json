{
    "paper_title": "GPS as a Control Signal for Image Generation",
    "authors": [
        "Chao Feng",
        "Ziyang Chen",
        "Aleksander Holynski",
        "Alexei A. Efros",
        "Andrew Owens"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 9 3 2 1 . 1 0 5 2 : r a"
        },
        {
            "title": "GPS as a Control Signal for Image Generation",
            "content": "Chao Feng1 Ziyang Chen1 Aleksander Hołynski2 Alexei A. Efros2 Andrew Owens1 2UC Berkeley 1University of Michigan https://cfeng16.github.io/gps-gen/ Figure 1. What can we do with GPS-conditioned image generation model? We train GPS-to-image models and use them for tasks that require fine-grained understanding of how images vary within city. For example, model trained on densely sampled geotagged photos from Manhattan can generate images that match neighborhoods general appearance and capture key landmarks like museums and parks. We show images sampled from variety of GPS locations and text prompts. For example, an image with the text prompt bagel results in modern-style sculpture when conditioned on the Museum of Modern Art and an impressionist-style painting when conditioned on the Metropolitan Museum of Art. We also lift 3D NeRF of the Statue of Liberty from landmark-specific 2D GPS-to-image model using score distillation sampling. Please see the project webpage and Sec. A.1.1 for more examples."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We show that the GPS tags contained in photo metadata provide useful control signal for image generation. We train GPS-to-image models and use them for tasks that require fine-grained understanding of how images vary within city. In particular, we train diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure. Each time tourist snaps photo, they capture tiny sliver of the world. Research on geotagged photo collections has shown that these images, when analyzed collectively, can reveal surprising amounts of information, including the landmarks that people visit [17], the 3D structure of the buildings they see [78], and geographic variations in architecture and fashion [20, 53]. In this paper, we show that GPS conditioning is useful and abundantly available control signal for image generation, which complements other common forms of conditioning like text. We train diffusion models to map GPS coordinates from particular city (or from more spatially localized region) to images. To solve this problem, the model needs to capture fine-grained distinctions in how images change their appearance over space. Such model, for example, needs to know the locations of museums and parks, 1 the subtle differences between building facades in different neighborhoods, and how landmarks change their appearance from different perspectives. Consequently, these models convey information that would be difficult to obtain from image or language supervision alone. We demonstrate the utility of this location-based control signal in variety of ways. First, we train diffusion models on both GPS coordinates and text (obtained from captioning), allowing us to generate images that appear as though they were shot in given location while capturing particular text prompt to allow for additional control  (Fig. 1)  . The resulting model exhibits the ability to perform compositional generation and the ability to closely follow location conditioning. For example, the prompt aerial view produces plausible overhead image of Central Parks Bethesda Fountain. The prompt superman results in statue or painting when conditioned on locations within the New York Museum of Modern Art, while it generates photo of costumed human in Times Square. Second, we show that 3D geometry can be lifted from 2D GPS-to-image models (Fig. 1, Statue of Liberty). We exploit the fact that GPS conditioning tells us how landmark should appear from different viewing positions. Given GPS-to-photo model trained on specific landmark, we extract NeRF using score distillation sampling [64, 91], using the learned conditional distribution to ensure that the estimated NeRF is consistent with the visual appearance of photos from every viewing direction. This 3D reconstruction by 3D generation approach does not require explicit camera pose estimation, matching, or triangulation. Instead, it obtains its signal from cross-modal association between GPS and images. Our evaluations suggest that GPS conditioning provides useful control signal for generating images and extracting structure from geotagged image collections. These experiments suggest: GPS-conditioned image generation models can capture subtle variations between locations within city. GPS conditioning complements language-based conditioning for image generation and 3D generation. 3D reconstructions can be extracted from 2D GPS-toimage models without explicit camera pose estimation. 2. Related Work Exploring geotagged image collections. GPS tags associated with images have been used in many different ways. Some researchers use GPS coordinates as complementary signals for image classification [85] and remote sensing [16]. Some works predict geolocation from images [27, 29, 83, 94, 102] or retrieve GPS or address from images in CLIP style [90, 96]. Other work has applied GPS data for different applications, such as city mapping and landmark identification [17], architectural styles [20], scene chronology [57], and fashion trends [53]. Mall et al. [54] creates underground map of cities by analyzing fashion styles in public social media photos to reveal unique neighborhood information. Snavely et al. [78] used geo-tagged, unordered photos to reconstruct 3D models of tourist sites and formulated it as photo tourism problem. Shrivastava et al. [76] introduced the painting2GPS task, which estimates the GPS coordinates of painting by matching it to collection of real geo-tagged photos. In contrast to previous works, we leverage GPS tags as additional conditional signals in generative models, enabling GPS-guided generation of tourist images and providing free supervisory signals for 3D reconstruction. Conditioning diffusion models. Diffusion models [9, 19, 34, 63, 66, 67, 69, 7982] are designed to learn how to restore data that has been deliberately corrupted by adding Gaussian noise. Specifically, the forward process gradually adds noise to data over several time steps, transforming it into pure noise. The reverse process then learns to denoise the data step by step, reconstructing the original data from the noise. Prior work has used many various conditions to guide diffusion models for image/video/3D/4D synthesis, including text [3, 9, 10, 13, 45, 64, 66, 67, 69, 86, 87], depth [11, 101], audio [8, 26, 86, 87], camera poses [4, 41, 42, 48, 71, 74], motion [5, 23, 43, 75, 93, 100], tactile signals [98, 99], segmentation mask [2, 6, 101], and many other conditions. Siglidis et al. [77] utilizes conditional diffusion models as data mining tools. Recently, Deng et al. [18] uses street maps, height maps, and text as conditions for video generative models to synthesize streetscapes. Khanna et al. [39] generate satellite images from GPS coordinates but are limited to satellite imagery and require specialized, calibrated training data.l In contrast, our work learns from in the wild geotagged photos using the GPS tags taken from EXIF metadata, much more diverse and abundantly available data source, and we use our models for variety of downstream tasks that were not considered in prior work, such as 3D model extraction. 3D reconstruction and generation. Reconstructing 3D models from multiple images [28] is longstanding problem. The traditional pipeline involves matching and verifying features [52, 60], estimating camera pose and sparse 3D geometry with structure from motion (SfM) [73, 88, 95], and generating dense reconstructions using multi-view stereo [22, 38] or neural fields [58]. State-of-the-art methods for unordered photo collections use specialized matching, filtering, and bundle adjustment to handle all-pairs matching and dense structure estimation [1, 51, 60, 73, 78]. While these approaches have been successful, they remain brittle, as each step can introduce unrecoverable errors. Recent works have generated 3D models zero shot solely from models trained solely on 2D images [12, 37, 64, 70, 91]. Poole et al. [64] and Wang et al. [91] used the score 2 function of text-to-image generation model, an approach that they called score distillation sampling [64] or score Jacobian chaining [91]. Other work extends this approach with 3D synthetic data for fine-tuning [48, 74] and improved optimization [45, 92]. However, these models still face issues like the Janus problem due to difficulties in assigning pose [64]. We extend this framework to generate NeRFs that are assigned probability under GPS-to-image model: we seek NeRF for which every viewpoint has high probability under the conditional distribution, avoiding the need for explicit camera pose estimation or feature matching. Compositional generation. notable characteristic of diffusion models is the ease with which they allow concept composition through the simple addition of noise estimates. This can be interpreted by treating these noise estimates as gradients of conditional data distribution [81, 82], where their sum points in direction that jointly enhances multiple conditional likelihoods. This technique has been used to enable compositions of text prompts both globally [47] and spatially [6, 21], of various image transformations [24] and individual image components [25]. It has also been used for diffusion models originating from two distinct modalities (sight and sound) [15]. Another line of work like ControlNet [101] shows the composition of multiple conditions (e.g., text prompt and pose&depth). In this paper, we demonstrate that: 1) two conditions of GPS tags and text prompts can successfully generate images using single noise estimate; 2) our GPS-to-image diffusion models can obtain images representative of given concept over large geographic area by averaging noise estimates. 3. Method We propose GPS-to-image diffusion model to synthesize tourist image by conditioning on GPS coordinates and text. We then show that variation of the model can be trained on single tourist site and can be used to lift 3D models. 3.1. GPS-to-image diffusion We train model to generate images conditioned on GPS coordinates from given city, challenging case that requires capturing fine-grained distinctions between the appearance of different locations. This model is further conditioned on text prompts to improve control of the model. Preliminaries. Diffusion models [19, 34, 7982] iteratively denoise the Gaussian noise xT to generate the image x0 of distribution, which is reverse of forward process. In the forward process, clean image x0 is gradually transitioned into random noise xT by adding Gaussian noise. At each time step, the noisy latent zt+1 can be expressed: zt+1 = αtzt + βtϵt, where zt is noisy latent of previous timestep and ϵt is standard Gaussian noise. αt and βt are predefined coefficients, so zt+1 is also the function of x0. In DDPM [34], the training objective of diffusion models is simplified to: (ϕ) = Et,x0,ϵt (cid:2)ω (t) ϵt ϵϕ (zt, y, t) 2 (cid:3) , (1) where ω (t) is weighting function of timestep (usually set to 1), ϵϕ is the denoiser, and is the condition such as text prompt. For inference, DDIM [80] and classifier-free guidance (CFG) are usually employed: ˆϵϕ (zt; y, t) = (1 + ω)ϵϕ (zt; y, t) ωϵϕ (zt; , t) , (2) where ω is the guidance weight and ˆϵϕ (zt; y, t) is the predicted noise for denoising. Training GPS-conditioned diffusion. Given collection of geotagged tourist photos over certain area (like city), we want to learn diffusion model [67, 79] that can synthesize tourist images controllably, when conditioned on the photos GPS position. For randomly taken tourist image x, we use the GPS coordinates (x, y) to represent its position, where and are the longitude and latitude respectively. We use the (x, y) as an extra condition for diffusion models to make them aware of position geospatially. For instance, tourist photos taken at the Louvre Museum usually contain the Louvre Pyramid but not the Arc de Triomphe. In this way, we can endow models with the capability to provide reliable tour guides for walking through Paris by controllably synthesizing tourist photos. We build our model on top of pretrained text-to-image latent diffusion model [67]. Since our off-the-shelf model accepts text prompt, we provide the text caption generated by BLIP-3 [97] on our collected datasets, encoded as CLIP [65] text embedding RLD, where is the number of text tokens and is the token feature dimension. We then learn GPS-pose embedding = [f (x), (y)] R2D, and append it to text tokens to establish GPS CLIP text condition, as shown in Fig. 2. This input representation ensures that the model starts from an initialization that closely resembles what it was trained on. We finetune the model using the text embedding and GPS embedding g. Specifically, given tourist photo dataset , for training samples {x, p, g}, we optimize the diffusion loss: Lrecon = Ex,g,ϵt,t (cid:2) ϵt ϵϕ(zt; p, g, t) 2 2 (cid:3) , (3) where zt is the noisy latent of image at timestep t. Inference. During inference, we use the classifier-free guidance strategy from InstructPix2Pix [10] for two conditions (text prompt and GPS tag). Our score estimate is as follows: ϵϕ (zt; p, g, t) =ϵϕ (zt; , , t) + ωp (ϵϕ (zt; p, , t) ϵϕ (zt; , , t)) + ωg (ϵϕ (zt; p, g, t) ϵϕ (zt; p, , t)) , (4) 3 (a) GPS-to-image generation (b) GPS-to-3D reconstruction Figure 2. Method. (a) After downloading geotagged photos, we train GPS-to-image generation model conditioned on GPS tags and text prompts. The trained generative model can produce images using both conditioning signals in compositional manner. (b) We can also extract 3D models from landmark-specific GPS-to-image model using score distillation sampling. This diffusion model parameterizes the GPS location by the azimuth with respect to given landmarks center. + means we concatenate GPS embeddings and text embeddings. where ωp and ωg are gudiance weights. 3.2. GPS-guided 3D reconstruction Recent work has shown that 3D models can be extracted from 2D text-to-image diffusion models. We build on this idea to obtain 3D reconstructions of specific locations using GPS-to-image models. Preliminary. Prior work [45, 64, 74, 91, 92] leverages pretrained 2D text-to-image diffusion models like Imagen [69] to synthesize 3D contents by textual descriptions. During optimization, Gaussian noise would be added to each NeRF rendering = hθ (q), where is the camera pose. Then noisy rendering would be fed into pretrained denoiser ϵϕ and score distillation sampling (SDS) loss provides gradient to guide NeRF [7, 58] training: θLSDS (ϕ, = hθ (q)) Et,ϵ (cid:20) ω (t) (ˆϵϕ (zt; y, t) ϵ) (cid:21) . θ (5) The text prompt is appended by view-dependent texts of front view, side view, or back view based on the randomly sampled camera poses q, which can benefit 3D generation results. However, in some cases, 2D text-to-image diffusion models struggle to control viewpoints accurately, causing multi-face Janus issue [64]. Angle-to-image diffusion. GPS signals provide useful information, e.g., viewpoint details, for 3D landmark reconstruction from tourism photos. Hence, we train diffusion model to transform these implicit signals into score function for supervision. For an image taken from landmark at GPS coordinate (x, y) R2, we parameterize the pose using the azimuth angle α, with respect to the center of the . Here we use α landmark (xo, yo): α = arctan (cid:17) (cid:16) xxo yyo instead of (x, y) as an extra condition for diffusion models, which means that we would replace in Eq. (3) with = (α) R1D. This representation makes it straightforward to combine the approach with DreamFusion [64], which can be easily extended to accept angular conditioning. Additionally, we fix the text prompt to photo of {landmark name} for each landmark. We refer to the diffusion model conditioned on GPS coordinates as the GPS-to-image diffusion, and the model conditioned on the angle from GPS tags as the angle-to-image diffusion. We found that when the focused area is small, by directly finetuning the diffusion model with only few thousand self-collected tourist photos, the model adapts to the distribution very easily and sometimes loses the generative diversity of the original model. To avoid this issue, we follow [68] and use prior preservation loss to maintain the prior knowledge and regularize the pose-conditioned training. More details are presented in Appendix A.2.1. We combine both losses and optimize the final objective: = Lrecon + λLpreservation, (6) where λ is the weight to balance the reconstruction and preservation loss. GPS-guided score distillation sampling. Using the pose information from GPS, our angle-to-image diffusion model generates photos of monuments from various viewpoints by conditioning on the GPS poses. Our angle conditioning is analogous to view-specific prompting in Poole et al. [64] while providing better view prior, as shown in Fig. 3. We use score distillation sampling (SDS) to extract 3D model of the landmark from our diffusion model [64] (Fig. 2(b)). We parameterize this 3D model as NeRF hθ () with pa4 (a) SfM (b) DreamFusion (c) Ours Figure 3. 3D Setup Comparison. We extract 3D models from 2D GPS-to-image models. (a) Traditional approaches require running SfM to estimate camera pose, followed by dense geometry estimation. Since they are based on triangulation, they are susceptible to catastrophic errors due to incorrect pose; (b) DreamFusion [64] samples images from different poses within scene using viewdependent prompting. However, text has limited ability to precisely control the position of the camera. (c) Our method extends DreamFusion with GPS conditioning, reducing pose uncertainty. rameters θ. We optimize the parameters of NeRF [58] using gradient descent, such that every rendered viewpoint has high probability under the Angle-conditioned image model. During each iteration of the optimization process, we sample random virtual camera poses. For each one, we transform the pose to azimuth α, and create conditioning embedding (Fig. 2 (b)). We can then render corresponding image = hθ (q) from the NeRF. We use the SDS loss to obtain gradient from the angleto-image diffusion model to supervise NeRF [59]. Following recent approaches [36, 74], we gradually decrease both the maximum and minimum of the time step sampling interval for the SDS loss during the optimization process. We apply classifier-free guidance (CFG) [33] to the SDS loss. For the unconditional version of the model, we simultaneously zeroed out both text and GPS conditions. This results in noise estimator: ˆϵϕ (zt; p, g, t) = (1+ω)ϵϕ (zt; p, g, t)ωϵϕ (zt; , , t) , (7) where zt is the noisy latent of image rendered by NeRF and ω is the CFG guidance weight. The equation of gradient is presented in Appendix A.2.2. 4. Experiments We evaluate our model using variety of quantitative and qualitative metrics. We first evaluate our GPS-to-image diffusion model, measuring its ability to successfully generate images that convey GPS and semantics of text prompts. We then evaluate our models ability to obtain 3D reconstructions for landmarks guided by GPS. 4.1. Implementation details Tourist photo collection. To train GPS-to-image diffusion models, we obtained two city photo collections with GPS tags by querying from Flickr: 1) New York City (Manhattan, 501,592 photos); 2) Paris (315,306 photos). For the landmark reconstruction task, we gather 6 sets of landmark photos following similar approach. The number of evaluated landmarks aligns with previous work in the field [56]. Please see Appendix A.3 for dataset details. GPS-to-image diffusion. We use positional encoding with frequencies of 10 [58] and two-layer MLP to encode the GPS conditions. For each city, we normalize (x, y) to the range [1, 1]. We finetune Stable Diffusion-v1.4 [67] on Flickr images, at resolution of 512 512 for 15k steps. We use the AdamW [50] optimizer with learning rate of 1 104. We use batch size of 512 on 8 NVIDIA L40S GPUs. During training, we randomly drop text and GPS conditions, ensuring 5% text-only, 5% GPS-only, and 5% unconditional generation. We caption images using BLIP3 [97]. See Appendix A.1.5 for details. Angle-to-image diffusion. We train individual Angle-todiffusion models for each landmark. We calculate the azimuth angle α from GPS using α = arctan and map it to the nearest 10 angle bin. We use the normalized bin value with positional encoding as conditional input. We fix text prompts with the template photo of {landmark name}. The weight of preservation loss λ in Eq. (6) is set to 1.0. Please refer to Appendix A.2.1 for more details. (cid:16) xxo yyo (cid:17) 3D reconstruction. We apply different guidance weights of classifier-free guidance (CFG) in Eq. (7) for score distillation sampling for each landmark. We turn on shading after 1000 steps and use orientation and opacity regularization loss, following [64]. We use the Adam optimizer [40] with learning rate of 0.01 for 10k training steps. The time step sampling interval is gradually reduced from [0.98, 0.98] to Figure 4. Qualitative results for Paris. We show images that have been sampled from our GPS-to-image diffusion model for various locations and prompts within Paris. 5 Figure 5. Qualitative comparison for GPS-to-image diffusion. We compare the qualitative results of our method against baselines using specific pairs of text prompts and GPS tags. Each column shows text prompt and GPS tag at the top. Text-address-to-image diffusion model is conditioned on combination of the text prompt and the address name derived from the GPS tag. We also perform nearest neighbor in the training set based on GPS tags. Our GPS-to-image diffusion model uses text prompt and raw GPS tag as conditioning. Google Street View images are sampled for reference of geolocation. Our method achieves better compositionality and visual quality. Table 1. Evaluation of GPS-to-image diffusion. We compare our method with several baselines in terms of CLIP Score and GPS Score. NN represents the nearest neighbor and SD is for stable diffusion. The best results are in bold, and the second bests are colored in blue. Method CLIP Score () GPS Score () Avg () GPS NN SD (Text&address) SD (Text) Ours Ours (w/o text) 18.77 26.65 29.13 27.88 13.66 4.25 1.21 8.15 13.71 16.22 15.45 15.17 18.02 [0.02, 0.50]. To match the characteristics of tourism photo distribution, we restrict the elevation angle of sampled virtual camera views to below 0, while the azimuth angle is sampled across the full range. 4.2. Evaluation of GPS-to-image generation We first evaluate our GPS-to-image diffusion models in generating photos conditioned on GPS tags. Evaluation metrics. To evaluate our model and baselines, we create 1,292 random GPS-text pairs as conditions for generation. We report CLIP score (CS) [65] to measure the alignment between generated image and text prompts. Analogously, we train GPS-CLIP model on paired GPSimage data with contrastive loss [14, 30, 61, 65] and report GPS score (GS) which measures cosine similarity between image and GPS embeddings. See Appendix A.1.4 for details. Baselines. Since we are not aware of any prior work on GPS-to-image generation, we include two baselines: 1) Stable Diffusion (SD)v1.4 [67]; 2) GPS Nearest Neighbor. For SD, we consider two variations: the first accepts concatenation of the text prompt and address name1, while the second is conditioned only on the text prompt. Results. As shown in Tab. 1 and Fig. 5, our method achieves the best performance in terms of the average CLIP score and GPS score. Additionally, it demonstrates better compositionality, indicating that our method can successfully generate images from text prompts and GPS tags. Our method, without the text prompt, achieves better GPS score than the Nearest Neighbor, indicating our model better captures image distributions in geospatial context. More qualitative results are presented in Fig. 1 and Fig. 4. For instance, our method can successfully generate images under different weather [44] and lighting [46] conditions given certain GPS location. 4.3. Average images Inspired by work that computes average images [20, 89], we apply our GPS-to-image models to the problem of obtaining images that are representative of given concept over 1The address name is obtained through geocoding using GeoPy. (a) Paris (b) New York City Figure 6. Average images. We select five areas for Paris and New York City respectively. Using our GPS-to-image models, we obtain representative images of the concept of building within these geographic regions to observe architectural styles. More examples can be found on project webpage for different locations and concepts. large geographic area. Specifically, we generate single image that has high probability under all GPS locations within user-specified area, as measured by our diffusion model. To do this, we follow work on compositional generation [47] and simultaneously estimate noise using large number of GPS locations and average together the noise vectors during each step of the reverse diffusion process (see Appendix A.1.3 for details). In Fig. 6, we show images generated for the text prompt building over variety of streets in neighborhoods in Paris and New York. The resulting images capture the distinctive architectural styles of buildings in the specified areas. More examples can be found on the project webpage, showcasing different locations and concepts. 4.4. Evaluation of angle-to-image generation We also evaluate how well our angle-to-image diffusion models can generate photos of monuments conditioned on desired viewpoints (angles) derived from GPS tags. Evaluation metric. We train classifier on each landmark dataset individually to predict the discretized angle bins derived from GPS tags. For each angle bin of 10, we ask generative models to synthesize 10 images and pair them up with input angle bins as ground truth. Then we apply the trained angle classifier to evaluate these images using accuracy as the metric. The evaluation method for diffusion models we adopt is similar to CLIP score [65]. We use this classifier trained on our training dataset to testify whether the finetuned diffusion model has successfully fit the training distribution. It should be noted that the main goal of finetuning diffusion model is to facilitate the end goal of 3D landmark reconstructionthe reconstruction quality of the reconstructed 3D landmarks should serve as an evaluation that our model has successfully learned the data distribution and angle conditioning signal from GPS. Results. We compare our method with two baselines: 1) Stable Diffusion (SD) v1.4 [67]; 2) Random chance. The results are reported in Tab. 2. Our angle-to-image diffusion model significantly outperforms both random chance Table 2. Evaluation of angle-to-image diffusion. We evaluate the accuracy of our model in generating images with the correct azimuth, as determined by an image-to-azimuth model. Method Random chance Stable Diffusion [67] Ours Angle acc (%) 2.78 3.06 22.36 Table 3. Quantitative comparison for 3D. We report results: CLIP Score (CS) [65], Perceptual Quality (PC), and Tourist Score (TS). It shows that our method achieves the highest quality. Method CS [65] () PQ () TS () NeRF [84] Dreamfusion [64] Ours 20.57 29.49 31. 1.32 2.21 3.31 1.36 2.09 3.45 and text-to-image Stable Diffusion [67] by relatively large margin, demonstrating that it effectively learns viewpoint signals from image-GPS pairs. Some generated images from the model are provided in Appendix A.2.1. 4.5. Evaluation of 3D landmark reconstruction We evaluate how good our reconstructed landmarks are based on our angle-to-image diffusion model. Evaluation metrics. The generated 3D landmarks are evaluated both qualitatively and quantitatively. We calculate CLIP Score (CS) [65] on RGB renderings of the landmarks with their corresponding text prompts of their names. The final CLIP score is presented as an average calculated from 30 randomly selected views. Following prior work [35], we also conducted user study for evaluation. We asked 36 participants to score the Perceptual Quality (PQ) and Tourist Score (TS) of landmark reconstructions on scale of 1 to 5, where 5 is the best. We define the perceptual quality metric to evaluate the quality of generated 3D assets, which do not necessarily match ground truth. The tourist score evaluates the 3D landmark reconstruction compared with real tourist photos by human preference. We evaluate 6 landmarks as mentioned in Sec. 4.1. 7 Figure 7. Qualitative comparison for 3D monument reconstruction. We show qualitative results of DreamFusion [64] and our method on two monuments: 1) Leaning Tower of Pisa; 2) Arc de Triomphe. Our reconstructed 3D monuments have better visual quality and more accurate 3D structure. We use rendered depth to make the background of RGB rendering white. Please see Appendix A.2.2 and project webpage for more examples. Baselines. We compare our method to DreamFusion [64] and COLMAP [72] followed by NeRF [58]. For the latter step, we train Nerf in the wild (NeRF-W) [55] 2 and For DreamFusion [64], Nerfacto [84] for each scene. we use Stable Diffusion [67] to ensure fair comparison with our model. We consider two types of text prompts: 1) photo of {landmark name}; 2) {landmark name}, both of them are with view-dependent conditioning (text prompt is appended with front/back/side view). We pick the best one for evaluation. Results. As shown in Tab. 3, our method outperforms two baselines in terms of three evaluation metrics. Our qualitative results in Fig. 7 show that renderings from our models have better visual quality and more accurate 3D structure than those from DreamFusion [64]. This suggests that our GPS-conditioned diffusion model can provide better pose prior than the text-to-image diffusion [67] with view-related prompts. As expected, we found the SfMbased baseline to be all or nothing, either providing very high-quality reconstructions or catastrophically failing (as shown in Appendix A.2.3). For example, COLMAP [72] successfully reconstructs camera poses and sparse point clouds for 3 of the 6 scenes, and fails on three. NeRFW [55] estimation completely fails on 6 landmarks and Nerfacto [84] fails on 5. This also reveals one shortcoming: 2We use this popular reimplementation since NeRF-W is not publicly available. (a) Representation of geolocation (b) 3D reconstruction Figure 8. Ablation. We conducted ablation studies to analyze the effectiveness of different modules in our method for GPS-to-image generation and 3D landmark reconstruction. if COLMAP [72] cannot reconstruct the poses of the input images, NeRF [58] optimization is not possible. In contrast, our method addresses this and is able to reconstruct scenes that COLMAP [72] cannot reconstruct. 4.6. Ablation Study Attention map visualization. We visualize attention maps [32] for the text and GPS conditions to examine what signals the model focuses on. We show two examples in Fig. 9. We can see that the text prompt effectively controls the semantics of objects in the synthesized image, while the GPS tag (latitude and longitude) significantly influences the background. For instance, in the tourist example, we can observe the shape of the Oculus Center in the attention maps corresponding to the GPS tag. Representation of geolocation. Geolocation can be represented in two variations: 1) continuous GPS tag; 2) ad8 tion that is difficult to fully disentangle. Acknowledgements. We thank David Fouhey, David Crandall, Ayush Shrivastava, Chenhao Zheng, Daniel Geng, and Jeongsoo Park for the helpful discussions. We thank Yiming Dou for helping set up NeRF baselines. Chao especially thanks Xinyu Zhang for her help in this project. This work was supported in part by Cisco Systems, NSF CAREER #2339071, and DARPA Contract No. HR001120C0123."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54(10):105112, 2011. 2 [2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, and Andrea Tagliasacchi. Cc3d: Layout-conditioned generaIn Proceedings of the tion of compositional 3d scenes. IEEE/CVF International Conference on Computer Vision, pages 71717181, 2023. 2 [3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hyIn Proceedings of the brid score distillation sampling. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 2 [4] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. 2 [5] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision, pages 5372. Springer, 2025. [6] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 2, 3 [7] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 4 [8] Burak Can Biner, Farrin Marouf Sofian, Umur Berkay Karakas, Duygu Ceylan, Erkut Erdem, and Aykut Erdem. Sonicdiffusion: Audio-driven image generation and editing with pretrained diffusion models. arXiv preprint arXiv:2405.00878, 2024. 2 [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 Figure 9. Attention visualization. We visualize attention maps for text and GPS tokens. dress name in text geodecoded from GPS tag. We finetune stable diffusion [67] on these two variations and results are presented in Fig. 8. As shown in Fig. 8, though CLIP Scores are comparable, our method based on continuous GPS tag outperforms the text-based method by significant margin for GPS Score. This suggests that using continuous GPS tag as conditioning input better controls the geospatial aspects of image generation. 3D reconstruction. We conduct experiments to evaluate the importance of prior preservation loss and GPS conditioning for 3D landmark reconstruction. We train our angleto-image diffusion models without prior preservation loss and also perform experiments where we remove angle conditioning during training  (Fig. 8)  . Our method outperforms these baselines by large margin, suggesting that GPS is valuable conditioning signal for reconstruction. 5. Conclusion Our work suggests that GPS coordinates are useful signal for controllable image generation. We have proposed method to generate images conditioned on GPS tag and text prompt in compositional manner, which successfully learns the cross-modal association between GPS tags and images. It can achieve compositional generation for tasks that require fine-grained understanding of how images vary within city. We also find that GPS conditioning enables us to reconstruct 3D landmarks by score distillation sampling without explicit camera pose estimation. Our work opens two future directions. The first is to develop models that use GPS-to-image generation methods to analyze geotagged photo collections in additional ways. The second is to develop new generative models that can extract more information from GPS conditioning. Limitations. Our approach may not be well-suited for cases where few photos have GPS available. Score distillation sampling is known to produce saturated images. In some scenarios, GPS tags carry certain semantic informa9 [10] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instrucIn Proceedings of the IEEE/CVF Conference on tions. Computer Vision and Pattern Recognition, pages 18392 18402, 2023. 2, 3 [11] Shengqu Cai, Duygu Ceylan, Matheus Gadelha, ChunHao Paul Huang, Tuanfeng Yang Wang, and Gordon Wetzstein. Generative rendering: Controllable 4d-guided video In Proceedings of generation with 2d diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76117620, 2024. [12] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16123 16133, 2022. 2 [13] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2 [14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning In International conference on of visual representations. machine learning, pages 15971607. PMLR, 2020. 6 [15] Ziyang Chen, Daniel Geng, and Andrew Owens. Images that sound: Composing images and sounds on single canvas. arXiv preprint arXiv:2405.12221, 2024. 3 [16] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61726180, 2018. [17] David Crandall, Lars Backstrom, Daniel Huttenlocher, and Jon Kleinberg. Mapping the worlds photos. In Proceedings of the 18th international conference on World wide web, pages 761770, 2009. 1, 2 Snavely, [18] Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas and Gordon Wetzstein. Guibas, Noah Streetscapes: Large-scale consistent street view genIn ACM eration using autoregressive video diffusion. SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2, 3 [20] Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei Efros. What makes paris look like paris? ACM Transactions on Graphics, 31(4), 2012. 1, 2, 6 [21] Yilun Du, Conor Durkan, Robin Strudel, Joshua Tenenbaum, Sander Dieleman, Rob Fergus, Jascha SohlDickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pages 84898510. PMLR, 2023. 3 [22] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):13621376, 2009. [23] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024. 2 [24] Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24154 24163, 2024. 3 [25] Daniel Geng, Inbum Park, and Andrew Owens. Factorized diffusion: Perceptual illusions by noise decomposition. In European Conference on Computer Vision, pages 366384. Springer, 2025. 3 [26] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 2 [27] Lukas Haas, Michal Skreta, Silas Alberti, and Chelsea Finn. Pigeon: Predicting image geolocations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1289312902, 2024. 2 [28] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 2 [29] James Hays and Alexei Efros. Im2gps: estimating geographic information from single image. In 2008 ieee conference on computer vision and pattern recognition, pages 18. IEEE, 2008. [30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 6 [31] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 15 [32] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 8 [33] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [35] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. International Conference on Computer Vision (ICCV), 2023. [36] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dreamtime: An improved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422, 2023. 5 10 [37] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 867876, 2022. 2 [38] Takeo Kanade and Masatoshi Okutomi. stereo matching algorithm with an adaptive window: Theory and experiment. IEEE transactions on pattern analysis and machine intelligence, 16(9):920932, 1994. 2 [39] Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David Lobell, and Stefano Ermon. Diffusionsat: generative foundation model arXiv preprint arXiv:2312.03606, for satellite imagery. 2023. 2 [40] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [41] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multiarXiv preprint video generation with camera control. arXiv:2405.17414, 2024. 2 [42] Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, and Jun-Yan Zhu. Customizing text-to-image diffusion with camera viewpoint control. arXiv preprint arXiv:2404.12333, 2024. 2 [43] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dragapart: Learning part-level motion prior for articulated objects. arXiv preprint arXiv:2403.15382, 2024. 2 [44] Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, and Shenlong Wang. Climatenerf: Extreme weather synthesis in neural radiance field. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3227 3238, 2023. 6 [45] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: HighIn Proceedings of resolution text-to-3d content creation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 2, 3, 4 [46] Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei Efros, and Noah Snavely. Learning to factorize and relight city. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 544561. Springer, 2020. [47] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation In European Conferwith composable diffusion models. ence on Computer Vision, pages 423439. Springer, 2022. 3, 7 [48] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In Proceedings of the Zero-shot one image to 3d object. IEEE/CVF International Conference on Computer Vision, pages 92989309, 2023. 2, 3 [49] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983, 2016. 15 arXiv preprint [50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5, 16 [51] Yin Lou, Noah Snavely, and Johannes Gehrke. Matchminer: Efficient spanning structure mining in large image collections. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12, 2012. 2 [52] David Lowe. Distinctive image features from scaleinvariant keypoints. International journal of computer vision, 60:91110, 2004. 2 [53] Utkarsh Mall, Kevin Matzen, Bharath Hariharan, Noah Snavely, and Kavita Bala. Geostyle: Discovering fashion In Proceedings of the IEEE/CVF intrends and events. ternational conference on computer vision, pages 411420, 2019. 1, 2 [54] Utkarsh Mall, Kavita Bala, Tamara Berg, and Kristen Grauman. Discovering underground maps from fashion. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 31143123, 2022. 2 [55] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72107219, 2021. 8, 16 [56] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR, 2021. 5, 15, 16, [57] Kevin Matzen and Noah Snavely. Scene chronology. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13, pages 615630. Springer, 2014. 2 [58] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 4, 5, 8 [59] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. 5 [60] David Nister and Henrik Stewenius. Scalable recognition In 2006 IEEE Computer Society with vocabulary tree. Conference on Computer Vision and Pattern Recognition (CVPR06), pages 21612168. Ieee, 2006. 2 [61] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [62] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 15 11 [63] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2 [64] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. ICLR, 2023. 2, 3, 4, 5, 7, 8, 15, 16 [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 3, 6, 7 [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 2 Hierarchical [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 5, 6, 7, 8, 9, 16 [68] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 4 [69] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2, 4 [70] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1860318613, 2022. [71] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single real image. arXiv preprint arXiv:2310.17994, 2023. 2 [72] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. 8, 16, 17 [73] Johannes Schonberger Jan-Michael Frahm. and In Proceedings of Structure-from-motion revisited. the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. 2, 15, 16 [74] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2, 3, 4, [75] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interacIn Proceedings of the tive point-based image editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. 2 [76] Abhinav Shrivastava, Tomasz Malisiewicz, Abhinav Gupta, and Alexei Efros. Data-driven visual similarity for crossdomain image matching. ACM Trans. Graph., 30(6):154, 2011. 2 [77] Ioannis Siglidis, Aleksander Holynski, Alexei Efros, Mathieu Aubry, and Shiry Ginosar. Diffusion models as data mining tools. arXiv preprint arXiv:2408.02752, 2024. 2 [78] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo In ACM sigtourism: exploring photo collections in 3d. graph 2006 papers, pages 835846. 2006. 1, 2 [79] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. 2, [80] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [81] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 3 [82] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, 3 [83] Nicholas Collin Suwono, Justin Chih-Yao Chen, Tun Min Hung, Ting-Hao Kenneth Huang, I-Bin Liao, Yung-Hui Li, Lun-Wei Ku, and Shao-Hua Sun. Location-aware visual question generation with lightweight models. arXiv preprint arXiv:2310.15129, 2023. 2 [84] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 Conference Proceedings, pages 112, 2023. 7, 8, 15, 16, 17 [85] Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, and Lubomir Bourdev. Improving image classification with loIn Proceedings of the IEEE international cation context. conference on computer vision, pages 10081016, 2015. 2 [86] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context interleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2742527434, 2024. 2 [87] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36, 2024. 12 control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [101] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3 [102] Xiaohan Zhang, Xingyu Li, Waqas Sultani, Chen Chen, and Safwan Wshah. Geodtr+: Toward generic crossview geolocalization via geometric disentanglement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [88] Carlo Tomasi and Takeo Kanade. Shape and motion from image streams under orthography: factorization method. International journal of computer vision, 9:137154, 1992. 2 [89] Antonio Torralba and Aude Oliva. Statistics of natural image categories. Network: computation in neural systems, 2003. 6 [90] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geolocalization. Advances in Neural Information Processing Systems, 36, 2024. [91] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1261912629, 2023. 2, 3, 4 [92] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 3, 4 [93] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for In ACM SIGGRAPH 2024 Conference video generation. Papers, pages 111, 2024. 2 [94] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planetphoto geolocation with convolutional neural networks. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 3755. Springer, 2016. 2 [95] Changchang Wu, Sameer Agarwal, Brian Curless, and In CVPR Steven Seitz. Multicore bundle adjustment. 2011, pages 30573064. IEEE, 2011. 2 [96] Shixiong Xu, Chenghao Zhang, Lubin Fan, Gaofeng Meng, Shiming Xiang, and Jieping Ye. Addressclip: Empowering vision-language models for city-wide image address localization. arXiv preprint arXiv:2407.08156, 2024. [97] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 3, 5, 15 [98] Fengyu Yang, Jiacheng Zhang, and Andrew Owens. GenIn Proceedings of the erating visual scenes from touch. IEEE/CVF International Conference on Computer Vision, pages 2207022080, 2023. 2 [99] Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, et al. Binding touch to everything: Learning unified multimodal tactile represenIn Proceedings of the IEEE/CVF Conference on tations. Computer Vision and Pattern Recognition, pages 26340 26353, 2024. 2 [100] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained 13 (a) New York City (b) Paris Figure 10. More qualitative results for GPS-to-image generation. We present more qualitative results of GPS-to-image generation for New York City and Paris. Images are sampled from variety of GPS locations and text prompts. A.1. GPS-to-image generation A.1.1. More qualitative results We present more qualitative results for our GPS-to-image generation in Fig. 10. As shown in Fig. 10, our method can successfully generate images conditioned on GPS tag and text prompt in compositional manner. For instance, in New York City (a): 1) text prompt tiger along with GPS location of the Charging Bull statue generates an image of tiger in similar pose to the Charging Bull, with an appropriately matching background; 2) given text prompt spiderman and batman, we can generate an image of either an oil painting of spiderman or stele of batman, depending on the location within The Metropolitan Museum of Art; 3) when conditioned on the GPS location of Madison Square Garden and the text prompt apple event, our model generates an image that appears to have been taken at Madison Square Garden (see ceiling in the image) during real Apple event (see Apple logo in the image). In Paris (b): 1) with text prompt spiderman and GPS location of Rodin Museum, the GPS-to-image diffusion model can generate an image of spiderman statue posed similarly to The Thinker; 2) text prompt batman and GPS location of Louvre Museums statue gallery can result in an image of statue of batman, while the model can generate an image of painting about Eiffel Tower when GPS location is the painting gallery of Louvre and text prompt is eiffel tower; 3) given the text prompt musicals and the GPS location of Palais Garnier, an image depicting musical performance at Palais Garnier is generated; 4) when conditioned on text prompt breakfast and GPS location of Orsay Museum, our GPS-to-image diffusion model can generate an image of oil painting of breakfast; 5) using the text prompt car along with the GPS location of the ChampsElysees, an image of the car is generated with background filled with buildings in the Haussmannian architectural style. Additionally, we present randomly sampled images from generation results of our GPS-to-image diffusion models in Fig. 11. Some images have visible artifacts, and this may be due to the limited availability of photos with GPS tags in that area. A.1.2. Evaluation set We create test set for New York City and Paris, comtext prompts and GPS tags prised of 1292 pairs of in total. We attach two files nyc-eval.json and paris-eval.json to show the lists we use for each city. A.1.3. Average images we have area, selected locations of set Within sampled coordinates with {(x0, y0) , (x1, y1) , ..., (xM 1, yM 1)}, then we could get their corresponding GPS embeddings {g0, ..., gM 1}. For the concept like text prompt building, we obtain fixed text embedding for {g0, ..., gM 1}. The noise estimate is as follows:"
        },
        {
            "title": "GPS",
            "content": "ϵϕ (zt; p, g, t) = ϵϕ (zt; , , t) + ωp (ϵϕ (zt; p, , t) ϵϕ (zt; , , t)) + ωg (cid:32) (cid:80)M 1 i=0 ϵϕ (zt; p, gi, t) (cid:33) ϵϕ (zt; p, , t) , (8) where ωp and ωg are guidance weights also used in Eq. (4), and ϵϕ is the denoiser of our trained GPS-to-image diffusion 14 Figure 11. Random sampling. We show some randomly sampled images from generation results of our GPS-to-image diffusion models conditioned on text prompts and GPS tags. These sampled results were used in the quantitative evaluation."
        },
        {
            "title": "It is worth noting that all average images shown",
            "content": "model. in Fig. 6 share the same initial random noise. A.1.4. GPS-CLIP As mentioned in Sec. 4, we use GPS score which measures cosine similarity between image and GPS embeddings as one of evaluation metrics. Specifically, we use pretrained frozen DINOv2 (ViT-B/14) [62] as image encoder. We add single projection layer to the image encoder. For the GPS encoder, we use shared-weight 6-layer MLP for latitude and longitude. The resulting embeddings are concatenated and passed through single layer to produce the final GPS embedding, which has the same dimensionality as DINOv2. We use GELU [31] as activation function for GPS encoder. The batch size is 512, and temperature is 0.07, learning rate is 1 104 with warmup and cosine learning rate decay [49]. We train GPS-CLIP on single NVIDIA L40S. The pseudocode for training process is presented in Algorithm 1. A.1.5. Implementation details We use xgen-mm-phi3-mini-instruct-r-v1 of BLIP-3 [97] as our captioning model for collected datasets. For classifierfree guidance (CFG), we set ωp to 3.5 and ωg to 7.5 in Eq. (4) for GPS-to-image diffusion. Algorithm 1 Pseudocode of training GPS-CLIP. # x, y: batch of longitudes and latitudes # imgs: batch of images # f_gps: shared-weight encoder for longitude and latitude # f_v: vision encoder of DINOv2 # p: projection layer for f_gps # q: projection layer for f_v # t: temperature for imgs, x, in loader: # load minibatch x_f = f_gps.forward(x) y_f = f_gps.forward(y) gps_e = p.forward(cat([x_f, y_f], dim=1)) # GPS embedding img_f = f_v.forward(imgs) img_e = q.forward(img_f) # image embedding gps_e = gps_e / norm(gps_e) # embedding img_e = img_e / norm(img_e) # embedding normalization normalization logits = mm(img_e.view(1, C), gps_e.view(1, C).T)/t labels = torch.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_g = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_g)/2 loss.backward() mm: matrix multiplication. A.2. GPS-guided 3D reconstruction More 3D qualitative comparisons between our method and DreamFusion [64] are presented in Fig. 12 and project webpage. Please refer to Sec. A.2.2 for more details. Qualitative results regarding SfM [73], Nerfacto [84], and NeRF-W [56] are shown in Fig. 13. Please refer 15 Figure 12. More qualitative comparison for 3D monument reconstruction. We show qualitative results of DreamFusion [64] and our method on Stonehenge. Our reconstructed 3D monuments have better visual quality and more accurate 3D structure. We use rendered depth to make the background of RGB rendering white. Please see project webpage for more examples. Qualitative results. We show more qualitative comparison between our method and DreamFusion [64] in Fig. 12 and project webpage. It should be noted that for all videos, we directly use raw renderings and do not use rendered depth to make the background of RGB rendering white. A.2.3. Baseline of COLMAP with NeRF We present qualitative results of COLMAP [73], NeRFW [56], and Nerfacto [84] in Fig. 13. Since NeRF-Ws [56] official code is not available, we evaluate the popular reimplementation. As shown in Fig. 13, COLMAP [72] successfully reconstructs camera poses and sparse point clouds for 3 of the 6 scenes, and fails on 3. NeRF-W [55] estimation completely fails on 6 landmarks and Nerfacto [84] fails on 5. A.3. Datasets Tourist photo collection. By querying Flickr, we obtain photo collections for 2 cities: 1) New York City (Manhattan, 501,592 photos); 2) Paris (315,306 photos) and 6 landmarks: 1) Leaning Tower of Pisa (2,967 photos); 2) Arc de Triomphe (2,377 photos); 3) Washington Monument (2,563 photos); 4) Statue of Liberty (1,174 photos); 5) Stonehenge (2,486 photos); 6) Space Needle (1,800 photos). The number of evaluated landmarks is in-line with prior work [56] in the field. It is worth noting that we focus primarily on Manhattan for New York City due to resource constraints. Some examples sampled from datasets are shown in Fig. 15. It should be noted that 2 cities are collected for GPS-toimage generation and 6 landmarks are for angle-to-image generation and 3D landmark reconstruction. As mentioned in Sec. 3.2, the angle of capture is necessary so we use bespoke dataset for each landmark. to Sec. A.2.3 for more details. Some qualitative results of angle-to-image diffusion are presented in Fig. 14, please refer to Sec. A.2.1 for more details. A.2.1. Angle-to-image diffusion Prior preservation loss. As mentioned in Sec. 3.2, for angle-to-image model training, we utilize prior preservation loss. To be specific, with synthesized images from original stable diffusion model [67] and text condition p, we optimize the preservation loss: Lpreservation = Ex,ϵ,t (cid:2) ϵt ϵϕ(z ; p, , t) 2 2 (cid:3) , (9) where represents that we zero out the angle condition for these training examples. Implementation details. For each landmark, we finetune Stable Diffusion-v1.4 [67] on collected Flickr images, at resolution of 256256 for 800 steps. After angle discretization, we normalize the angle to the range of [1, 1]. We use positional encoding and two-layer MLP to encode the angle condition. For the positional encoding, we use 10 frequencies. We use the AdamW [50] optimizer with constant learning rate of 5 106 and gradient accumulation without warm-up. We use global batch size of 256 on 4 NVIDIA A40 GPUs. Qualitative results. Some generated images from our angle-to-image diffusion model are presented in Fig. 14. A.2.2. GPS-guided score distillation sampling Gradient. The gradient in Sec. 3.2 we use to supervise NeRF is as follows: θLSDS (ϕ, = hθ (q)) (cid:21) (cid:20) ω (t) (ˆϵϕ (zt; p, g, t) ϵt) θ , (10) Eg,ϵt,t where ω (t) is weighting function, which we set to ω (t) = σ2 following [64]. 16 Figure 13. SfM/NeRF baselines. We present SfM reconstructions from COLMAP [72], Nerfacto [84] rendering results, and NeRF-W [56] rendering results for 6 evaluated landmarks. SfM reconstruction fails on (a), (b), and (c). Nerfacto [84] only succeeds on (f). NeRF-W [56] completely fails on 6 scenes. Figure 14. Qualitative results for angle-to-image generation. We show generated images of our angle-to-image diffusion model for the Arc de Triomphe, Statue of Liberty, and Leaning Tower of Pisa. Images are sampled conditioned on different angles estimated by GPS tags. 17 Figure 15. Data samples. We show some random photos with their GPS tags from our collected datasets."
        }
    ],
    "affiliations": [
        "UC Berkeley",
        "University of Michigan"
    ]
}