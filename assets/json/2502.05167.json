{
    "paper_title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "authors": [
        "Ali Modarressi",
        "Hanieh Deilamsalehy",
        "Franck Dernoncourt",
        "Trung Bui",
        "Ryan A. Rossi",
        "Seunghyun Yoon",
        "Hinrich Sch√ºtze"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information."
        },
        {
            "title": "Start",
            "content": "NOLIMA: Long-Context Evaluation Beyond Literal Matching Ali Modarressi 1 2 * Hanieh Deilamsalehy 3 Franck Dernoncourt 3 Trung Bui 3 Ryan Rossi 3 Seunghyun Yoon 3 Hinrich Sch utze 1 2 Abstract Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving needle (relevant information) from haystack (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NOLIMA, benchmark extending NIAH with carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT4o, one of the top-performing exceptions, experiences reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. 5 2 0 2 7 ] . [ 1 7 6 1 5 0 . 2 0 5 2 : r 1. Introduction In recent years, large language models (LLMs) have made remarkable advancements in handling long-context inputs (Chen et al., 2023; Xiong et al., 2024; Peng et al., 2024). This capability has unlocked new possibilities in various NLP tasks that require understanding or generating content *Work done during an internship at Adobe Research. 1Center for Information and Language Processing, LMU Munich, Germany 2Munich Center for Machine Learning (MCML) 3Adobe Research. Correspondence to: Ali Modarressi <amodaresi@cis.lmu.de>. 1 over extended documents. Examples include longor multidocument question answering (QA), summarization, and many-shot in-context learning (Lee et al., 2024; Chang et al., 2024; Agarwal et al., 2024). To evaluate these models effectiveness in handling long contexts, several benchmarks have been developed. One prominent benchmark is Needle-in-aHaystack (NIAH), which tests models ability to search for and retrieve specific fact (the needle) hidden within irrelevant information (the haystack)(Kamradt, 2023; Mohtashami & Jaggi, 2023). While the baseline NIAH task assesses surface-level retrieval capabilities, recent adaptations have increased its complexity. These enhancements include introducing multiple needles, incorporating additional distractor material, and interconnecting facts to necessitate in-context reasoning (e.g., fact-chaining) (Hsieh et al., 2024; Levy et al., 2024; Kuratov et al., 2024). Other benchmarks, such as long-, multi-document QA, and long conversation understanding, have also been proposed to evaluate longcontext comprehension in more downstream task manner (Liu et al., 2024; Yen et al., 2024; Zhang et al., 2024; Dong et al., 2024; Wang et al., 2024; Maharana et al., 2024). Arguably, these tasks share common foundation: the ability to recall previously seen information (Goldman et al., 2024). This broader category, termed association recall tasks, has been extensively studied in machine learning (Graves et al., 2014; Ba et al., 2016). key argument is that the attention mechanism, which is the underlying foundation of many LLMs, is inherently adept at identifying and recalling associations present in the input (Olsson et al., 2022; Arora et al., 2024). However, this raises an important question: Long-context benchmarks feature tasks where the queried input (e.g., question or task) has literal matches with the provided context. Do such literal matches make it easier for language models to locate relevant information and output correct answers? We argue that many existing long-context benchmarks either explicitly (e.g., synthetic tasks or NIAH-based) or implicitly (e.g., multi-document or long-document QA) contain such literal matches. To address this, we introduce NOLIMA, benchmark designed to minimize literal overlap between questions and their corresponding needles. In NOLIMA, questions and needles contain keywords that are related through associative links, such as real-world knowledge NOLIMA: Long-Context Evaluation Beyond Literal Matching or commonsense facts. By embedding these needles in haystack, NOLIMA challenges models to leverage latent associative reasoning capabilities rather than relying on surface-level matching. We evaluate NOLIMA over 12 state-of-the-art language models, all claiming to support token lengths of at least 128K, including GPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B (Hurst et al., 2024; Team et al., 2024a; Meta, 2024). Unlike NIAH-based evaluations, which contain literal matches and exhibit near-saturated performance, NOLIMA presents more demanding challenge that highlights the limitations of these models. We evaluate 12 state-of-the-art language models, all of which claim to support token lengths of at least 128K, including GPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B (Hurst et al., 2024; Team et al., 2024a; Meta, 2024). Their performance declines noticeably as context length increases, with considerable drops even at 2K8K tokens. For instance, at 32K tokens, 10 out of 12 models achieve only half of their short-context performance. We conduct extensive analyses using NOLIMA, yielding the following insights: Impact of Latent Hops and Fact Direction: We demonstrate how the number of associative reasoning steps (latent hops) and the ordering of elements within fact statements influence task performance. Context Length vs. Needle Position: Our aligneddepth analysis shows that as latent reasoning complexity grows, performance depends more on context length than needle position. Without surface-level cues, longer contexts overwhelm the attention mechanism. Ablation Tests: We confirm that the presence of literal matches significantly simplifies the task, enabling models to achieve high accuracy in answering questions. In contrast, when literal matches serve as distractors, they severely impair accuracy. Chain-of-Thought (CoT) Prompting and Reasoningbased Models: While CoT prompting or reasoningbased models such as GPT-o1 (OpenAI et al., 2024) improve performance by encouraging step-by-step reasoning, they fail to fully mitigate the challenge, particularly in contexts exceeding 16K tokens. Through NOLIMA, we reveal the limitation of literal matching in long-context benchmarks and introduce novel approach for evaluating models latent reasoning in longer contexts. 2. Related Work With the increasing popularity of long-context language modeling, numerous benchmarks have been introduced to R-1 R-2 R-L Long-document QA Bench QA (Zhang et al., 2024) Bench MC (Zhang et al., 2024) 0.966 0.545 0.960 0.946 0.506 0.932 RAG-style (Multi-doc) QA RULER QA (Hsieh et al., 2024) HELMET (RAG) (Yen et al., 2024) 0.809 0.437 0.693 0.689 0.304 0.555 Recall-based 0.905 0.789 0.855 Vanilla NIAH (Kamradt, 2023) 0.571 0.461 0.500 RULER S-NIAH (Hsieh et al., 2024) BABILong (0K) (Kuratov et al., 2024) 0.553 0.238 0."
        },
        {
            "title": "NOLIMA",
            "content": "0.069 0.002 0.067 Table 1. ROUGE precision scores between the input document and the question: higher ROUGE scores indicate greater literal matches between the question and the relevant context. evaluate this capability. Needle-in-a-Haystack (NIAH) is the most well-known and widely used benchmark (Mohtashami & Jaggi, 2023; Kamradt, 2023). However, due to performance saturation, various extensions have been proposed. These include increasing complexity by adding more needles, chaining needles to require inter-needle reasoning (fact-chaining), or incorporating arithmetic or code reasoning (Kamradt, 2023; Hsieh et al., 2024; Levy et al., 2024; Kuratov et al., 2024; Hengle et al., 2024; Zhang et al., 2024). Some tasks increase the complexity to such an extent that they become overly difficult even in short-context scenarios. For instance, BABILong includes tasks that perform poorly (e.g. the counting task achieves 28% accuracy) even without any irrelevant background text (0K) (Kuratov et al., 2024). Similarly, the Ancestral Tree Challenge (ATC) employs extensive fact-chaining, resulting in tasks that are overly complex even for short contexts (<1K) (Li et al., 2024). While such tasks challenge language models in long contexts, they raise the question of whether the tasks are inherently too complex for models to handle, regardless of context length. Literal Matching in Long-Context Benchmarks. Another frequent pattern in many long-context benchmarks is the presence of literal matches between the facts required to answer question and the question itself. This fact is not limited to synthetic recall-based tasks (e.g., vanilla NIAH, RULER retrieval-based sets) but also affects downstreamlike QA-based benchmarks (Hsieh et al., 2024; Liu et al., 2024; Zhang et al., 2024; Bai et al., 2024; Yen et al., 2024), which often implicitly include literal matches between the relevant document and the question. Although many of these studies introduce complexity by adding similar documents as distractors, literal matches can still provide cues. These cues may help models focus on potential relevant facts based on matches, as attention mechanisms excel at recalling repetitive patterns (Olsson et al., 2022; Arora et al., 2 NOLIMA: Long-Context Evaluation Beyond Literal Matching"
        },
        {
            "title": "Keyword Types",
            "content": "Which character has been to Wq? Def. Inv. Actually, [CHAR] lives next to the Wn. Wn is next to where [CHAR] lives. Wn Buildings & Landmarks Wq Countries, cities, states Table 2. An example template of the proposed needle set in NOLIMA (all templates are available in Appendix A.) The placeholders [CHAR], Wq, and Wn represent the randomly selected character (also the answer), the query keyword, and the needle keyword, respectively. Def.: default order. Inv.: inverted order. 2024). We later demonstrate to what extent literal matches simplify recall-based questions (cf. 4.4.4). To quantify the prevalence of these matches in popular benchmarks, we compute ROUGE (R-1, R-2, and R-L) precision scores1 (Lin, 2004) between the needle (in recall-based tasks), the relevant document (in multi-document setups), or the full document (in long-document QA) and the corresponding question. This analysis measures the degree of literal overlap between the question and the context. Based on the scores in Table 1, NOLIMA demonstrates significantly less literal overlap compared to other datasets. 3. NOLIMA The goal of NOLIMA is to design task that is inherently simple to solve through associative reasoning, but for which surface-level matching has zero utility. As result, NOLIMA allows us to cleanly investigate associative reasoning in long-context scenarios without confounding from surfacelevel effects. The main elements of NOLIMA are similar to vanilla NIAH. needle single key piece of information is placed within haystack, i.e., long irrelevant text (in our case, snippets from books). Given question, the model is then tested on its ability to find the needle. The needle is designed to be clearly relevant answer to the question. In contrast to existing NIAH tasks, we impose the condition that the question have minimal literal match with the needle. To achieve this, we design set of needles and corresponding questions, collectively referred to as needle set. Table 2 presents one of the constructed needle set templates (see Appendix for the full list). Each needle consists of unique character and specific information about them. Example: The Semper Opera House is located in Dresden. Thus, the model should be able to identify the latent association link between Wq (Dresden) in the question and Wn (Semper Opera House) in the needle. Since there is no literal overlap between needle and question, the model must rely on this latent association link to retrieve Yuki, the correct answer. For some of our needles, the association involves commonsense reasoning instead of world knowledge. Example: Then Yuki mentioned that he has been vegan for years. Which character cannot eat fish-based meals? To push the limits of the models ability to identify hidden associations, we include questions that require two hops to connect Wq with Wn, for example: Which character has been to the state of Saxony? Here, the model should tap into its knowledge that Dresden (and hence the Semper Opera) is located in the state of Saxony. This two-hop setup further increases the difficulty of identifying the latent association of Wq with Wn. To make NOLIMA an effective benchmark for evaluating LLM long-context abilities, we impose several constraints on the needle set. (i) We select keywords that ensure simplicity so that, without irrelevant context, the associations are clear and the model can identify the correct answer. (ii) We randomize the assignment of character names from diverse pool to minimize sensitivity to tokenization problems and mitigate ethnic bias (Navigli et al., 2023; Jiang et al., 2024). Names already occurring in the haystacks are excluded. (iii) We ensure Wn is uniquely associated with Wq, avoid language-based cues and employ preface phrases to isolate needles from preceding context. See Appendix for details. Actually, Yuki lives next to the Semper Opera House. 3.1. Haystack Filtering Pipeline The needle contains keyword (Wn, here Semper Opera House) that serves as the critical link between needle and question. The question is designed to retrieve this information by asking which character possesses specific attribute Wq, Dresden in the example: Which character has been to Dresden? To ensure that the haystack does not contain: (1) Any distracting words that have extreme literal or high semantic similarities with the key points mentioned in the question (2) Any information that explicitly or in an inferrable case be potential false answer to the question, we devise filtering process. 1We use precision as our metric to measure how many of the questions tokens exist in the relevant context, rather than the reverse. Distractor Filtering. For this step, we use an embedding function, Contriever (Izacard et al., 2022), to find similar words in the haystack to the keywords of the questions. 3 NOLIMA: Long-Context Evaluation Beyond Literal Matching Figure 1. Haystack conflicting information filtering pipeline First we gather all words in the haystack and compute their respective embedding. Then using dot-product similarity we compute their similarity to the question keywords. We manually inspect the top-20 similar words per each Wq and flag those with high semantic or substring similarity for removal. In the removal process those sentences that contain flagged words are removed from the haystack. This initial filtering step helps to avoid an uncontrolled set of superficial distractors that could undesirably disrupt the experimental results. We will discuss the impact of distractors on the model performance in our analysis (Section 4.4.4). Conflicting Information Filtering. In this step, we implement semi-automatic redaction process to detect and remove such conflicting information. As shown in Figure 1, this process takes the haystack textalready filtered for distractorsalong with questions from our needle set as input. Assuming the model should infer cases within short contexts, we scan the input texts in smaller chunks.2 To identify potential answers within chunk, we pair each question with the chunk and input them into an instructiontuned language model, along with short instruction and few-shot examples. The model responds with either N/A (indicating no relevant information was found) or an explanation identifying possible conflict. Flagged examples are manually reviewed3 to determine whether the identified information should be removed. If no conflicts are found, the text remains unchanged. This process is repeated across all selected haystacks until no further removals are necessary. 2With an 800-character stride and 1000-character chunk size (250 tokens). 3All manual reviewsin both filtering stepswere conducted by one of the authors. 4. Experiments 4.1. Dataset Configuration In NOLIMA, we use 5 groups of needles, each with two fact-order variations: default and inverted. In the default order, the answer character always precedes the needle keyword Wn. In the inverted order, the fact is conveyed with the character name placed after Wn. Each group includes 26 keyword sets, with some sets containing multiple Wq items to produce both one-hop and two-hop examples. This setup results in 58 question-needle pairs in total. To generate the haystacks, we select 10 open-licensed books, ensuring each covers at least 50K tokens. Using the filtering mechanism described in Section 3.1, we process the text to prepare it for haystack construction. To mitigate potential memorization issuessince these books are publicly availablewe construct haystacks by concatenating short snippets. Specifically, we iteratively and randomly select book, extract continuous snippet (under 250 tokens), and append it to the haystack until it exceeds 2K lines, resulting in haystacks exceeding 60K tokens. In all experiments, each needle is placed 26 times at equal intervals across the evaluated context length. With 5 randomly generated haystacks, 58 question-needle pairs, and 26 placements per context length, this setup results in 7,540 tests per context length experiment. 4.2. Models For the filtering process, we opted using Llama 3.3 70b instruction tuned model (Meta, 2024). As control test, for each question, we place its needle in 100 randomly selected chunks to verify whether the model (1) understands the filtering task and (2) is familiar with the facts and capable of inferring the answer. The model achieves score of 4 NOLIMA: Long-Context Evaluation Beyond Literal Matching"
        },
        {
            "title": "Models",
            "content": "GPT-4o Llama 3.3 70B Llama 3.1 405B Llama 3.1 70B Gemini 1.5 Pro Jamba 1.5 Mini Command R+ Mistral Large 2 Claude 3.5 Sonnet Gemini 1.5 Flash GPT-4o mini Llama 3.1 8B"
        },
        {
            "title": "Claimed Effective\nLength\nLength",
            "content": "Base Score (0.85: Thr.) 128K 128K 128K 128K 2M 256K 128K 128K 200K 1M 128K 128K 8K 2K 2K 2K 2K <1K <1K 2K 4K <1K <1K 1K 99.3 (84.4) 97.3 (82.7) 94.7 (80.5) 94.5 (80.3) 92.6 (78.7) 92.4 (78.6) 90.9 (77.3) 87.9 (74.7) 87.6 (74.4) 84.7 (72.0) 84.9 (72.2) 76.7 (65.2) 1K 2K 4K 8K 16K 32K 98.1 94.2 89.0 91.0 86.4 76.3 77.0 86.1 85.4 68.6 67.7 65.7 98.0 87.4 85.0 81.8 82.7 74.1 73.5 85.5 84.0 61.6 58.2 54.4 95.7 81.5 74.5 71.2 75.4 70.8 66.3 73.3 77.6 51.0 44.1 44.1 89.2 72.1 60.1 62.7 63.9 62.2 39.5 51.5 61.7 44.4 32.6 31. 81.6 59.5 48.4 51.8 55.5 52.7 21.3 32.6 45.7 35.5 20.6 22.6 69.7 42.7 38.0 43.2 48.2 43.6 7.4 18.7 29.8 28.6 13.7 14.2 Table 3. NOLIMA benchmark results on the selected models. Following Hsieh et al. (2024), we report the effective length alongside the claimed supported context length for each model. However, we define the effective length as the maximum length at which the score remains above threshold set at 85% of the models base score (shown in parentheses). Scores exceeding this threshold are underlined. Scores that are below 50% of the base score are shaded in red . 99.8% in this test, indicating its ability to effectively flag conflicting information from the haystacks. For the evaluation process, we select five closed-source models: GPT-4o, GPT-4o Mini (Hurst et al., 2024), Gemini 1.5 Pro, Flash (Team et al., 2024a), and Claude 3.5 Sonnet (Anthropic, 2024), along with seven open-weight Llama models: The Llama 3.x model family (3.1 8B, 70B, 405B, and 3.3 70B) (Dubey et al., 2024; Meta, 2024), Mistral Large (Mistral, 2024), Command R+ (Cohere For AI, 2024), and Jamba 1.5 Mini (Team et al., 2024b). All these models are well-known and widely used in long-context setups. In our analysis on reasoning-based prompting and models, we evaluate GPT-o1, GPT-o3 Mini (OpenAI et al., 2024; OpenAI, 2025), and DeepSeek-R1 Distill-Llama-70B (DeepSeek-AI et al., 2025). More details regarding model versions and deployment details are described in Appendix B. 4.3. Evaluation Setup & Metric During inference, we use task template (see Appendix C) that instructs the model to answer the question based on the provided text. Since all questions seek the name of the character mentioned in the needle, any returned answer containing the correct name is considered accurate. Accuracy is reported as the proportion of tests with correct answers. Models are evaluated on all tasks over context lengths of 250, 500, 1K, 2K, 4K, 8K, 16K, and 32K. To take into account how models would perform on NOLIMA regardless of long-context scenario, we control the difficulty of the task by by reporting base score. Evaluations at context lengths of 250, 500, and 1K are used to compute the base score. These three are the shortest contexts. If model can solve the task at these lengths, then any deterioration of its performance at greater lengths is expected to be solely due to its difficulties with generalizing over long contexts. For each question-needle example, we compute the average score over 5 haystacks, then take the maximum score of that example across the 250, 500, and 1K tests. The final base score is obtained by averaging these maximum scores across all question-needle examples. Inspired by Hsieh et al. (2024), we also report the effective length of each model. While they use the performance of Llama 2 model at 4K length as threshold (85.6%), we define the threshold as 85% of the base score. Thus, the effective length of model is the largest tested length that exceeds this threshold. Additionally, some plots show the normalized score, calculated by dividing the accuracy score by the base score. 4.4. Results Table 3 presents the performance results of all NOLIMA tests on the selected models. Most models achieve high base scores, indicating that the designed needle set is relatively simple to answer in shorter contexts. Even models with base scores exceeding 90.0% exhibit significantly shorter effective length than their claimed lengths, generally limited to 2K tokens, with GPT-4o being an exception. While GPT-4o demonstrates strong overall performance, it fails to generalize effectively beyond 8K tokens. Out of the 12 models, 10 exhibit performance at 32K lengths that is half or less of their base scores. For comparison, in other benchmarks with similar settings, such as BABILong (QA1) (Kuratov et al., 2024) and RULER (Hsieh et al., 2024), Llama 3.1 70B achieves effective lengths of 16K4 and 32K, respectively. However, in NOLIMA, Llama 3.1 70B has an effective length of only 2K and shows significant drop in performance at 32K lengths (42.7% vs. 94.3% base score). 4In BABILong, the effective length is also based on 85% of the 0K base performance threshold 5 NOLIMA: Long-Context Evaluation Beyond Literal Matching (a) Full Sweep (One-hop) (b) Full Sweep (Two-hop) (c) Last 2K (One-hop) (d) Last 2K (Two-hop) Figure 2. The full sweep plots (a & b) illustrate performance across the entire context window. The plots for the last 2K tokens (c & d) depict performance when needle placements are aligned within the final 2K tokens for various context lengths. The color shading of each plot line represents the tested context length. To minimize noise and highlight trends more clearly, we increased the number of placements from 26 to 51 and applied moving average with window size of 12. Models such as Claude 3.5 Sonnet, Gemini 1.5 Flash, GPT4o mini, and Llama 3.1 8B may have weaker base scores, but their effective lengths are calculated relative to these scores. This reveals an interesting observation: model like Claude 3.5 Sonnet, despite having lower base score, may underperform in shorter contexts but demonstrate better length generalization than models with higher base scores, such as Llama 3.1 70B and Llama 3.3 70B. In fact, Sonnet even achieves higher raw scores in 4K-token experiments compared to some higher-base-score models. Model scaling generally improves performance, as seen in the progression from Llama 3.1 8B to 70B, Gemini 1.5 Flash to Pro, or GPT-4o mini to GPT-4o. However, the benefits of scaling diminish at larger scales; for example, the performance gap between Llama 3.1 70B and 405B is smaller (and sometimes worse) than that between 8B and 70B. In general, lite models such as Gemini 1.5 Flash, GPT-4o mini, and Llama 3.1 8B perform well in shorter contexts (<1K tokens) but fail to generalize effectively in longer contexts. 4.4.1. LATENT HOPS & INVERSION As discussed in Section 3, our needle set also includes examples requiring two-hop associative linking from the question keyword to the needle keyword. To evaluate the impact on length generalization, Figure 3(a) presents the normalized performance of two top-performing models on one-hop and two-hop tasks. It is evident that, for the same context lengths, questions involving two-hop latent reasoning steps are more challenging than those requiring one-hop reasoning. Notably, the performance gap between one-hop and two-hop tasks widens with increasing context lengths. GPT4o demonstrates impressive generalization performance, handling both types of examples effectively even at context lengths up to 4K. 6 Each group of needles includes both default and an inverted template and Figure 3(b) shows that inverted examples are more challenging to answer. We argue this difficulty arises from the models causal attention mechanism, particularly in longer contexts where attention signals weaken. In the default template, the question or particularly Wq, can link directly to Wn, which could contain information about the characters name since the name appears earlier in the sequence. This allows the model to backtrace effectively from Wq through Wn to the character. In the inverted template, Wq may still attend to Wn, but since the fact is incomplete (the character hasnt been stated yet), the model cannot use that attention to resolve the question. Instead, it must rely on weaker signals encoded in the characters name to establish the link, which becomes harder with longer contexts due to diminishing attention strength. While these findings shed light on the challenge, deeper mechanistic analysis is beyond the scope of this paper and requires further study. (a) One-hop vs. Two-hop (b) Default vs. Inverted Figure 3. Impact of (a) number of hops and (b) inversion on normalized performance across GPT-4o and Llama 3.3 70B models. The red dotted line indicates the 0.85 effective threshold. NOLIMA: Long-Context Evaluation Beyond Literal Matching 4.4.2. NEEDLE PLACEMENT DEPTH ANALYSIS common evaluation across NIAH-based benchmarks (Kamradt, 2023) examines the impact of needle placement within the context window. In Figure 2(a), we observe lost-in-the-middle effect (Liu et al., 2024) in 32K, where model performance dips when the needle appears in the middle of longer contexts. Additionally, Figure 2(b) reveals key phenomenon: longer contexts in more complex (two-hop) examples dampens the performance distribution over the full sweep depending on their length. In vanilla multi-document or NIAH-based benchmarks (Kamradt, 2023; Liu et al., 2024), models perform consistently well when the needle (or gold document) appears at the very beginning or end of the context window, with minimal impact from context length. However, in NOLIMA, as task complexity increases in two-hop scenarios, larger context sizes shift the entire trendline downward toward zero, with performance declining even at the edges of the context window. To further investigate this issue, we devise an alternative setup that focuses on analyzing the last 2K tokens instead of sweeping across the full context. Therefore, we align the placement positions in the last 2K tokens for all context lengths (see Figure 4). This makes that for certain token depth the only changing factor in each plotline would be the context length, which in turn means that the model has more tokens that can be attended to. Based on the final 2K results in Figure 2(c), the one-hop setup confirms our earlier observations from the full-sweep plots. The lost-in-the-middle phenomenonwhere performance dips toward the center of the contextprimarily appears in simpler tasks. Each plotline drops as it moves toward the center, reflecting its dependence on placement position and the way the model encodes positional information. In contrast, the two-hop scenario appears to be Figure 4. Needle placements in full sweep (top) vs. last 2K tokens sweep (bottom): In the last 2K setup, placement positions are aligned in different context lengths, unlike the proportion-based positioning in full sweep. 7 4K 8K 16K 32K One-hop 56.2 - w/o CoT - w/ CoT 60.6 Increase rate 5.9% 8.3% 12.8% 7.8% 90.3 95.6 84.1 91.1 73.2 82.6 Two-hop 25.9 - w/o CoT - w/ CoT 34.3 Increase rate 16.5% 22.1% 32.7% 32.4% 70.7 82. 57.4 70.1 42.7 56.7 Table 4. Comparison of Chain-of-Thought (CoT) improvements in performance for Llama 3.3 70B, evaluated on both one-hop and two-hop tests. influenced more by attention limitations than by position encoding alone. Figure 2(d) reveals that, rather than depth exacerbating performance drops, the plot lines remain relatively stable over the last 2K positions. However, context length significantly reduces the overall performance trends observed in this range. Llama 3.x models, like many other recent language models, features rotary position embeddings (RoPE) which is relative PE (Su et al., 2024). For each token depth in Figure 2(d), as the relative distance between question and fact remains the same regardless of context length, position encoding does not explain the perInstead, the main limiting factor is the formance drop. increased context length: as the number of tokens grows, the attention mechanism struggles to process information effectively. In the absence of strong surface-level cues (e.g., literal matches), locating relevant facts becomes challenging for the model, regardless of their position within long contexts. 4.4.3. COT PROMPTING Since NOLIMA examples require an associative reasoning between the needle and question keywords to retrieve the correct answer, in this part we evaluate when the model is prompted to reason in Chain-of-Thought (CoT) style (Wei et al., 2022) before returning final answer (see Appendix for more details). In Table 4, we present the results when asked for CoT compared to asking directly for the final answer. CoT prompting shows improvements over longcontext tests and it shows higher rate of improvement in two-shot. Despite the improvements, the tasks seem to remain challenging. For example, two-hop examples with CoT prompting barely achieve the scores of one-hop examples without CoT and continue to perform poorly on texts 16K tokens or longer. The challenge with CoT prompting is that the questions in NOLIMA are straightforward. They are mentioning singular clue to the answer, meaning they cannot be further decomposed into simpler steps. This limits the benefits of CoT prompting. However, the difficulty lies in reasoning through the association between the question NOLIMA: Long-Context Evaluation Beyond Literal Matching"
        },
        {
            "title": "Base\nScore",
            "content": "4K 8K 16K 32K Llama 3.3 70b 55.5 37.2 16.7 8.9 73.0 51.2 31.8 10.1 Reasoning models 92.0 78.0 60.1 31.1 52.8 36.9 25.5 18.9 91.4 75.5 49.4 20.7 98.3 97.1 99.9 98.8 99.9 - w/o CoT - w/ CoT GPT-o1 GPT-o3 Mini DeepSeek R1-DL-70b Table 5. Evaluation results of NOLIMA-Hard: Scores falling below 50% of the base score are highlighted in red . and the needle, which remains significant challenge for the model. To assess the performance of reasoning-based models (e.g., GPT-o1) on NOLIMA, we selected the 10 most challenging needle-question pairs from the 58 available, based on the results summarized in Table 3. We refer to this subset as NOLIMA-Hard and present the evaluation results in Table 5. While reasoning-based models outperform CoT prompting on Llama 3.3, they still fail to achieve full-length generalization on this subset. Across all models, performance drops below the 50% mark at 32K context length. Notably, base scores are nearly perfect, demonstrating the simplicity of the taskeven within this designated hard subset. This means that even with intermediate reasoning steps, models still struggle to link the needle to the question in long contexts without surface-level cues. 4.4.4. ABLATION STUDY: LITERAL MATCH EFFECT To examine the simplifying impact of literal matches on results, we define two new sets of tests: (1) Direct: questions that explicitly ask about the fact stated in the needle by stating Wn in the question, resembling vanilla NIAH evaluation (Kamradt, 2023). (2) Multiple Choice (MC): questions that maintain the required latent associative reasoning while incorporating literal matches. In this setup, the question includes four character names as answer optionsthree from the haystack and one correct answer from the needle. As expected, Table 6 shows that direct examples with high degree of literal overlap between the question and the needle are straightforward for the model to answer, even in long contexts, consistent with prior findings in RULER (Hsieh et al., 2024). Additionally, literal matches significantly aid the model when the questions remain unchanged, and only the multiple-choice format is introduced. The inclusion of literal matches in the multiple-choice setup provides significant guidance to the model. By offering the character names as answer options, including the correct name from the needle, the model can focus its search within smaller Direct One-hop - w/ Literal Match (MC) Two-hop - w/ Literal Match (MC) 8K 16K 32K 98. 98.5 98.5 84.1 98.7 57.4 96.3 73.2 97.4 42.7 94. 56.2 93.1 25.9 87.2 Table 6. Results in two literal match setups: direct and multiple choice (MC) questions. Model: Llama 3.3 70B scope. This dramatically simplifies the task of identifying the correct answer, as the literal match serves as direct hint, reducing ambiguity in the reasoning process. Distracting Literal Matches. While literal matches could serve as cues if they are part of the relevant fact, they can also act as distractors if they are irrelevant to the answer. In Section 2, we noted that some related benchmarks include similar documents in the context as distractors to test the models ability to discern the correct answer from irrelevant ones. This setup creates matches between the query and both relevant and irrelevant documents or facts. In contrast, NOLIMA allows us to explore different scenario: when the context contains distracting words overlapping with the question, while the relevant fact has minimal overlap with the query. We insert distractor sentence into the haystack (details in Appendix D) containing Wq but entirely irrelevant to both the needle and the questions intent. This setup poses significant challenge, requiring the model to disregard irrelevant literal overlaps while identifying relevant fact with no meaningful overlap with the query. As shown in Figure 5, such distractors have substantial impact on degrading length generalization. GPT-4o now demonstrates an effective length of just 1K, while Llama 3.3 70B performs even worse. While adding distractors slightly lowers base scores (GPT-4o: 93.8, Llama 3.3 70B: 84.4), the normalized plots still clearly illustrate performance Figure 5. Normalized performance comparison across GPT-4o and Llama 3.3 70B models, with and without distractors. The red dotted line marks the 0.85 effective threshold. 8 NOLIMA: Long-Context Evaluation Beyond Literal Matching drop at longer lengths. These results highlight the challenge of resolving queries in contexts where irrelevant overlaps mislead the model, and the relevant fact shares no overlap with the question. 5. Conclusion NOLIMA provides challenging benchmark for evaluating the reasoning capabilities of large language models in longcontext settings. By removing literal overlaps between questions and relevant information, the benchmark tests models ability to infer and link information within extensive irrelevant content. Our findings show that even state-of-the-art models struggle, especially as context length increases, revealing serious limits in their attention mechanism. While causal attention should theoretically access all previous tokens, models often rely on surface-level cues in longer contexts. This vulnerability becomes more pronounced when the context contains literal matches that fail to connect with the truly relevant fact, causing models to overlook the correct information and focus instead on superficial signals. We believe our findings with NOLIMA are likely to extend to downstream applications. For instance, in search engines or RAG systems, relevant document containing the correct answer may have lexical gap with the query. So, even if such document is retrieved alongside others that likely have higher lexical similarity, language models may struggle to extract the correct answer, as they can become distracted by the lexical overlaps in those other documents. This work highlights the need for benchmarks that go beyond surfacelevel retrieval to assess deeper reasoning. NOLIMA sets new standard for evaluating long-context comprehension and emphasizes the importance of developing approaches capable of handling complex reasoning in long contexts."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work aimed at advancing the field of long-context language modeling by evaluating and analyzing the most commonly used LLMs. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Abdullatif Koksal, Leonie Weissweiler, and Amir Hossein Kargaran for their valuable feedback and support, particularly in the early stages of this project."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Rosias, L., Chan, S. C., Zhang, B., Faust, A., and Larochelle, In ICML 2024 H. Many-shot in-context learning. Workshop on In-Context Learning, 2024. URL https: //openreview.net/forum?id=goi7DFHlqS. Anthropic, A. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3, 2024. Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and Re, C. Zoology: Measuring and improving recall in efficient language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=LY3ukUANko. Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu, C. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL https: //aclanthology.org/2024.acl-long.172/. Chang, Y., Lo, K., Goyal, T., and Iyyer, M. Booookscore: systematic exploration of book-length summarization in the era of LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=7Ttk3RzDeu. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Cohere For AI. c4ai-command-r-plus-08-2024, 2024. URL https://huggingface.co/CohereForAI/ c4ai-command-r-plus-08-2024. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dong, Z., Tang, T., Li, J., Zhao, W. X., and Wen, J.-R. BAMBOO: comprehensive benchmark for evaluating long text modeling capacities of large language models. In Calzolari, N., Kan, M.-Y., Hoste, V., Lenci, A., Sakti, S., and Xue, N. (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 20862099, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/ 2024.lrec-main.188/. 9 NOLIMA: Long-Context Evaluation Beyond Literal Matching Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. if all you need is retrieval? Goldman, O., Jacovi, A., Slobodkin, A., Maimon, A., Is it really long conDagan, I., and Tsarfaty, R. towards gentext uinely difficult long context NLP. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1657616586, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 924. URL https://aclanthology.org/2024. emnlp-main.924/. Graves, A., Wayne, G., and Danihelka, I. Neural turing machines, 2014. URL https://arxiv.org/abs/ 1410.5401. Hengle, A., Bajpai, P., Dan, S., and Chakraborty, T. Multilingual needle in haystack: Investigating long-context behavior of multilingual large language models. arXiv preprint arXiv:2408.10151, 2024. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., and Ginsburg, B. RULER: Whats the real context In First size of your long-context language models? Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=kIoBbc76Sy. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. ISSN 28358856. URL https://openreview.net/forum? id=jKN1pXi7b0. Jiang, B., Xie, Y., Hao, Z., Wang, X., Mallick, T., Su, W. J., Taylor, C. J., and Roth, D. peek into token bias: Large language models are not yet genuine reasoners. In AlOnaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 47224756, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 272. URL https://aclanthology.org/2024. emnlp-main.272/. Kamradt, G. Needle in haystack-pressure testing llms. Github Repository, pp. 28, 2023. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=e2TBb5y0yFf. Kuratov, Y., Bulatov, A., Anokhin, P., Rodkin, I., Sorokin, D. I., Sorokin, A., and Burtsev, M. BABILong: Testing the limits of LLMs with long context reasoning-ina-haystack. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview. net/forum?id=u7m2CG84BQ. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lee, J., Chen, A., Dai, Z., Dua, D., Sachan, D. S., Boratko, M., Luan, Y., Arnold, S. M. R., Perot, V., Dalmia, S., Hu, H., Lin, X., Pasupat, P., Amini, A., Cole, J. R., Riedel, S., Naim, I., Chang, M.-W., and Guu, K. Can long-context language models subsume retrieval, rag, sql, and more?, 2024. URL https://arxiv.org/abs/ 2406.13121. Levy, M., Jacoby, A., and Goldberg, Y. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1533915353, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 818. URL https://aclanthology.org/2024. acl-long.818/. Li, M., Zhang, S., Liu, Y., and Chen, K. Needlebench: Can llms do retrieval and reasoning in 1 million context window?, 2024. URL https://arxiv.org/abs/ 2407.11963. Lin, C.-Y. ROUGE: package for automatic evaluaIn Text Summarization Branches tion of summaries. Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013/. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157 173, 2024. doi: 10.1162/tacl 00638. URL https: //aclanthology.org/2024.tacl-1.9/. 10 NOLIMA: Long-Context Evaluation Beyond Literal Matching Maharana, A., Lee, D.-H., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. Evaluating very long-term conversational memory of LLM agents. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1385113870, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 747. URL https://aclanthology.org/2024. acl-long.747/. Meta, A. Llama 3.3 model card. 2024."
        },
        {
            "title": "URL",
            "content": "https://github.com/meta-llama/ llama-models/blob/main/models/llama3_ 3/MODEL_CARD.md. Mistral, A. Mistral large 2. Mistral Large 2 Blogpost, 2024. URL https://mistral.ai/news/ mistral-large-2407/. Mohtashami, A. and Jaggi, M. Random-access infinite context length for transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=7eHn64wOVy. Navigli, R., Conia, S., and Ross, B. Biases in large language models: Origins, inventory, and discussion. J. Data and Information Quality, 15(2), June 2023. ISSN 1936-1955. doi: 10.1145/3597307. URL https://doi.org/10. 1145/3597307. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads, 2022. URL https:// arxiv.org/abs/2209.11895. Openai o3-mini OpenAI. URL o3-mini-system-card/. 2025. system card. https://openai.com/index/ OpenAI, : Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=wHBfxhZu1u. 11 Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Team, J., Lenz, B., Arazi, A., Bergman, A., Manevich, A., Peleg, B., Aviram, B., Almagor, C., Fridman, C., Padnos, D., et al. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv preprint arXiv:2408.12570, 2024b. Wang, M., Chen, L., Cheng, F., Liao, S., Zhang, X., Wu, B., Yu, H., Xu, N., Zhang, L., Luo, R., Li, Y., Yang, M., Huang, F., and Li, Y. Leave no document behind: Benchmarking long-context LLMs with extended multidoc QA. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 5627 5646, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.322. URL https://aclanthology. org/2024.emnlp-main.322/. Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In Liu, Q. and Schlangen, D. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https:// aclanthology.org/2020.emnlp-demos.6/. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for NOLIMA: Long-Context Evaluation Beyond Literal Matching Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long. 260. URL https://aclanthology.org/2024. naacl-long.260/. Yen, H., Gao, T., Hou, M., Ding, K., Fleischer, D., Izsak, P., Wasserblat, M., and Chen, D. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K., Han, X., Thai, Z. L., Wang, S., Liu, Z., and Sun, M. bench: Extending long context evaluation beyond 100k tokens, 2024. URL https://arxiv.org/abs/ 2402.13718. 12 NOLIMA: Long-Context Evaluation Beyond Literal Matching A. Needle Set Design & Considerations In Table 7, we demonstrate the full needle set that we use in NOLIMA. In designing the needle templates, there are multiple considerations involved. First, all templates in the needle set begin with small introductory phrase or at least one word (e.g., Actually, In 2013,) to distinguish themselves from the preceding context. This ensures that the needles keyword or character is not inadvertently linked to the prior context. Since newline is appended at the end of each needle, this issue is mitigated if the keyword or character appears at the end of the needle."
        },
        {
            "title": "Needles",
            "content": "Which character has been to Wq? Which character has been to Wq? Which character has been to Wq? Which character cannot drink Wq? Which character cannot eat Wq? Def. Inv. Def. Inv. Def. Inv. Def. Inv. Def. Inv. There was [CHAR] who was an engineer living in Wn. There was an engineer living in Wn, named [CHAR]. Actually, [CHAR] lives next to the Wn. Wn is next to where [CHAR] lives. In 2013, after waiting in line for hours, [CHAR] finally saw the original Wn painting up close. In 2013, the original Wn painting was seen up close by [CHAR], finally, after waiting in line for hours. message came in from [CHAR] saying, Im Wn and nothing more. message came in saying, Im Wn, from [CHAR]."
        },
        {
            "title": "Keyword Types",
            "content": "Wn Countries, cities, states Wq Countries, cities, states Wn Buildings & Landmarks Wq Countries, cities, states Wn Buildings & Landmarks Wq Countries, cities, states"
        },
        {
            "title": "Wn Dietary",
            "content": "restriction (e.g., lactose intolerant) Wq Drinks & Beverages Then [CHAR] mentioned that he has been Wn for years. There was Wn guest, named [CHAR]."
        },
        {
            "title": "Wn Dietary",
            "content": "restriction (e.g., vegan)"
        },
        {
            "title": "Wq Foods",
            "content": "Table 7. Our proposed needle set templates in NOLIMA. The placeholders [CHAR], Wq, and Wn represent the randomly selected character (also the answer), the query keyword, and the needle keyword, respectively. Def.: default order. Inv.: inverted order. Another consideration is that the needle keyword should be uniquely associated with the query keyword. For instance, in the following sentence: There was an engineer living in Cambridge, named Yuki. Although the term Cambridge is commonly associated with the United Kingdom, it is not uniquely so; it could also refer to cities in the United States, Canada, or other countries. Additionally, we aim to avoid relying on language-specific markers. Many cities have distinctive elements in their names, such as orthographic features, morphological structures, or cultural naming conventions, that hint at their linguistic or geographic origins. By minimizing the influence of such markers, the needle design ensures more rigorous evaluation of the models ability to make meaningful connections based on learned knowledge rather than surface-level linguistic cues. For each template, we manually curated 2-6 keyword pairs, resulting in total of 28 keyword pairs. Taking into account the order of fact statements, this generates 58 needle-question pairs. B. Models In Table 8, we list all the models selected for evaluation. Models that are open weights were deployed using the vLLM library (Kwon et al., 2023), with weights obtained from HuggingFace (Wolf et al., 2020). C. Task Prompt Templates & Inference Settings In Table 9, we present the task prompts used across all evaluations. While we do not employ the commonly used Lets think step by step prompt in the Chain-of-Thought (CoT) setup (Kojima et al., 2022), our prompt encourages the model to elaborate and expand its reasoning sufficiently before producing final answer. To manage the extensive testing scope7,540 tests per context lengthwe limit reasoning to three sentences or maximum of 192 generated tokens. In the CoT setup, test is considered successful if the final answer (on the newline) includes the correct answer. This differs with the non-CoT setup, where success is determined based on whether the correct answer is present within the"
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o mini Llama 3.3 70B Llama 3.1 405B Llama 3.1 70B Llama 3.1 8B Gemini 1.5 Pro Gemini 1.5 Flash Claude 3.5 Sonnet Jamba 1.5 Mini Command R+ Mistral Large 2 GPT-o1 GPT-o3 Mini DeepSeek R1-DL-70b NOLIMA: Long-Context Evaluation Beyond Literal Matching Context Length Open Weights? Model Revision 128K 128K 128K 128K 128K 128K 2M 1M 200K 256K 128K 128K 128K 128K 128K No No Yes Yes Yes Yes No No No Yes Yes Yes gpt-4o-2024-11-20 gpt-4o-mini-20240718 meta-llama/Llama-3.3-70B-Instruct meta-llama/Llama-3.1-405B-Instruct meta-llama/Llama-3.1-70B-Instruct meta-llama/Llama-3.1-8B-Instruct gemini-1.5-pro-002 gemini-1.5-flash-002 anthropic.claude-3-5-sonnet-20241022-v2 ai21labs/AI21-Jamba-1.5-Mini CohereForAI/c4ai-command-r-plus-08-2024 mistralai/Mistral-Large-InstructReasoning-based models No No Yes gpt-o1-2024-12-17 gpt-o3-mini-2025-01-31 deepseek-ai/DeepSeek-R1-Distill-Llama-70B Table 8. Details of the selected models used for evaluation. generated output. For all standard instruction-tuned models, we use greedy decoding during generation. For reasoning-based models, we utilize the default sampling decoding mechanism for GPT-o1 and GPT-o3 Mini, while R1-based models employ top-P sampling with = 0.95 and temperature of 0.6. In addition, we cap the maximum number of generated tokens in reasoning-based models at 1536 tokens, including both reasoning and output tokens. In all models, we apply each models instruction-tuned chat templates."
        },
        {
            "title": "Prompt Template",
            "content": "You will answer question based on the following book snippet: {haystack w/ needle} w/o CoT Use the information provided in the book snippet to answer the question. Your answer should be short and based on either explicitly stated facts or strong, logical inferences. Question: {question} Return only the final answer with no additional explanation or reasoning. You will answer question based on the following book snippet: {haystack w/ needle} w/ CoT Use the information provided in the book snippet to answer the question. Be aware that some details may not be stated directly, and you may need to INFER the answer based on the given information. Begin with brief explanation of your reasoning in NO MORE THAN THREE (3) sentences. Then, return the final answer on new line. Question: {question} Table 9. Details of prompt templates utilized in our evaluation. D. Distractor Design To construct and integrate the distractor sentences mentioned in Section 4.4.4, we devised two templates, applied uniformly across all needle-question pairs. Depending on the Wq, we use one of the following templates: 14 NOLIMA: Long-Context Evaluation Beyond Literal Matching There was an article about Wq in the daily newspaper. or There was photo of Wq in the daily newspaper. Some instances of Wq may naturally include an article (e.g., or an), making them better suited for the second template, while others fit the first. Regardless of the choice, the templates are designed to remain neutral and unrelated to the intent of the question or the fact stated by any needle. To minimize interference with the needle, we randomly place the distractor sentence while ensuring token distance of at least 20% of the context length. For example, in 1K-token test, the distractor must be at least 200 tokens away from the needle. Additionally, to avoid any advantage from proximity to the beginning or end of the context (which may gain extra attention), we restrict placement to between the 20% and 80% marks of the context length. Together, these two constraints leave span of 40%-60% of the context length available for random placement of the distractor sentence."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Center for Information and Language Processing"
    ]
}