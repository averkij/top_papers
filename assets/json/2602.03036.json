{
    "paper_title": "LatentMem: Customizing Latent Memory for Multi-Agent Systems",
    "authors": [
        "Muxin Fu",
        "Guibin Zhang",
        "Xiangyuan Xue",
        "Yafu Li",
        "Zefeng He",
        "Siyuan Huang",
        "Xiaoye Qu",
        "Yu Cheng",
        "Yang Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks."
        },
        {
            "title": "Start",
            "content": "2026-2-1 LatentMem: Customizing Latent Memory for Multi-Agent Systems Muxin Fu1,2,, Guibin Zhang3,, Xiangyuan Xue4, Yafu Li4, Zefeng He5, Siyuan Huang6, Xiaoye Qu2, Yu Cheng4, Yang Yang6 1Tongji University 2Shanghai AI Laboratory 4The Chinese University of Hong Kong 3National University of Singapore 6Shanghai Jiao Tong University 5Nanjing University 6 2 0 F 3 ] . [ 1 6 3 0 3 0 . 2 0 6 2 : r # boonkana10@gmail.com, guibinz@outlook.com Equal Contribution https://github.com/KANABOON1/LatentMem Corresponding Author. Abstract Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, learnable multi-agent memory framework designed to customize agent-specific memories in token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in lightweight form, and memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves performance gain of up to 19.36% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks. 1. Introduction Large Language Model (LLM)-powered multiagent systems (MAS), have emerged as powerful framework for solving complex tasks by allowing agents to collaborate [Ye et al., 2025, Yue et al., 2025, Zhang et al., 2024a] or compete [Yang et al., 2025b, Zhang et al., 2024c] beyond the capabilities of individual LLM agents. Pivotal to this success is the concept of multiagent memory [Hu et al., 2025, Wu et al., 2025b], which enables agents to accumulate, retain, and reuse experiences through interactions with both other agents and the environment, thereby supporting more coherent coordination and continual adaptation. Figure 1 The paradigm comparison between existing multi-agent memory and LatentMem. Instead of relying on handcrafted memory units, LatentMem extracts agent-specific memories from the latent space by combining raw trajectories with agent profiles. Building on this memory foundation, recent studies have increasingly explored multi-granularity memory repositories that capture experiences at different levels of abstraction, including (i) MAS trajectories [Qian et al., 2024b, Wang and Chen, 2025], (ii) distilled semantic insights [Liu et al., 2025, Zhu et al., 2025], and (iii) orchestrable skill schemas [Han et al., 2025, Zhang et al., 2025d]. These designs endow memory systems with the ability to capture diverse memory patterns, such as LatentMem: Customizing Latent Memory for Multi-Agent Systems trajectory summarization and high-level insight extraction, enabling MAS to adaptively integrate past experiences and jointly refine decision-making strategies [Tomilin et al., 2025, Zheng et al., 2026]. However, despite the growing sophistication of existing memory systems, they remain constrained by two key limitations: (i) Memory homogenization: Most methods adopt one-size-fits-all strategy, ignoring the functional heterogeneity of agents, which undermines role adherence and amplifies correlated errors [Cemri et al., 2025], weakening system robustness and hindering long-term adaptation. (ii) Information overload: MAS inherently involves long interaction contexts [Zhang et al., 2024a], and multi-granularity memory designs further amplify this burden by introducing large volumes of stored entries [Wang and Chen, 2025, Zhang et al., 2025a], ultimately overwhelming agents and obscuring critical decision signals. Given the aforementioned challenges, natural question arises: Given long and complex contexts in MAS, can we design learnable memory that is both role-aware and token-efficient, without extensive manual engineering? To address these challenges, we propose LatentMem, latent multi-agent memory framework that materializes agent-aware memory customization via token-efficient latent memory generation. Specifically, LatentMem consists of two components: lightweight experience bank for storing and retrieving raw MAS trajectories, and memory composer that leverages agent profiles to distill raw trajectories into role-aware, compact latent memories and integrate them into the agents reasoning process. To encourage the memory composer to distill transferable, high-utility latent representations from raw trajectories, we propose Latent Memory Policy Optimization (LMPO), which computes advantages from relative rewards within multi-agent rollouts, optimizes token-level objectives, and exploits latent memory differentiability to enable gradient backpropagation through the memory composer. As novel attempt in latent MAS memory, LatentMem offers three principal advantages: (I) It conditions the memory composer on agent role profiles to customize role-aware latent memories, thereby mitigating memory homogenization; (II) It encodes multi-agent memory as fixed-length latent representations rather than unbounded discrete textual traces, thereby mitigating information overload; (III) It exploits LMPO and latent memory differentiability to enable autonomous memory internalization and reconstruction, thereby avoiding language constraints and obviating the need for meticulously engineered memory architectures. Extensive experiments across six benchmarks and four mainstream MAS frameworks demonstrate that LatentMem achieves: (I) high performance, improving state-of-the-art MAS by up to 16.20% and 18.45% in knowledge QA and code generation tasks, respectively; (II) high efficiency, using 50% fewer tokens and reducing inference time to 2/3 compared to mainstream memory designs; and (III) strong generalization, with out-of-domain datasets such as PDDL showing 7.10% improvement, and unseen MAS such as CAMEL exhibiting 7.90% gain compared to the vanilla setting. These results establish LatentMem as novel and effective framework for MAS memory. 2. Related Works LLM-Based Multi-Agent Systems. MAS is framework in which multiple agents collaborate by assuming distinct roles to achieve shared objectives [Guo et al., 2024, Li et al., 2024, Tran et al., 2025]. Our work focuses on leveraging MAS for task-specific problem solving. Early task-solving MAS frameworks [Du et al., 2023, Hong et al., 2023, Li et al., 2023, Liang et al., 2023, Wang et al., 2024, Wu et al., 2024, Zhang et al., 2024d] typically rely on predefined, static workflows, while more recent studies [Yang et al., 2026, 2025a, Yue et al., 2025, Zhang et al., 2024b, 2025c, Zhou et al., 2025a,b, Zhuge et al., 2024] have enabled MAS to dynamically reconfigure their organizational structures, improving adaptability to diverse and complex tasks while reducing computational costs. However, 2 LatentMem: Customizing Latent Memory for Multi-Agent Systems Figure 2 Overview of LatentMem. The framework proceeds as follows: (1) retrieve relevant trajectories from the experience bank; (2) compress them with agent role profiles into latent memories via the LMPO-trained memory composer; (3) inject these memories into agent reasoning processes without altering the agent architectures; and (4) store new trajectories for continual improvement. these methods typically require extensive searches over the design space, resulting in considerable computational and token overhead. Consequently, developing lightweight mechanism for flexible MAS adaptation remains an open and challenging research problem. Memory in Multi-Agent Systems. Memory enables agents to accumulate experience through interactions, thereby supporting coherent coordination and continual adaptation [Hu et al., 2025, Xu et al., 2025a]. It plays crucial role in task-solving and social simulation; our focus lies primarily on the former. Early memory designs in MAS typically rely on simple, within-trial mechanisms coupled to the system itself, such as shared pool storing raw trajectories [Chen et al., 2023, Hong et al., 2023, Qian et al., 2023, 2024a, Rezazadeh et al., 2025, Yin et al., 2023]. Modern memories, by contrast, have shifted towards more intricate and flexible structures. Representative examples include OAgents, which employs multi-granularity memory [Zhu et al., 2025]; EvolveR [Wu et al., 2025a] and Agent KB [Tang et al., 2025], which compress raw trajectories into high-level semantic units; and MIRIX [Wang and Chen, 2025], which transforms user goals into orchestrable procedural memories. However, these approaches overlook heterogeneous, role-aware memory design. LatentMem addresses this limitation by equipping each agent with compact, role-aware latent memory, thereby reinforcing role compliance, enhancing coordination, and improving continual adaptation. 3. Preliminary Notations. Consider multi-agent system containing ğ‘ agents = {ğ‘1, ğ‘2, . . . , ğ‘ğ‘ }, and equipped with global memory module that stores and retrieves shared information among agents. Formally, 3 LatentMem: Customizing Latent Memory for Multi-Agent Systems the system can be represented as the tuple: = (A, G, M) (1) parameterized by ğœƒğ‘˜. During Each agent ğ‘ğ‘˜ = (ğ›¾ğ‘˜, ğœ‹ğœƒğ‘˜) is defined by role profile ğ›¾ğ‘˜ and policy ğœ‹ğœƒğ‘˜ execution, an agent receives an input prompt ğ‘ and retrieved memory ğ‘š from M, and produces response ğ‘œ, denoted as ğ‘œ = ğ‘ğ‘˜ ( ğ‘, ğ‘š). The execution graph governs the topological order in which agents operate. Depending on the system architecture, can be instantiated as either static predefined topology [Qian et al., 2024b] or centralized dynamic regulation mechanism [Wang et al., 2025]. Problem Formulation. Our objective is to find the memory module that maximizes the expected performance of MAS X, which is formally defined as: max ğ”¼ğ‘D,ğœX (ğ‘) [ğ‘…(ğœ)] (2) where denotes the dataset and ğ‘ is query sampled from it. The system processes the query ğ‘ to produce reasoning trajectory ğœ, and the reward function ğ‘… extracts the final answer from ğœ and evaluates its correctness. This formulation is agnostic to specific memory architectures, ranging from hand-crafted symbolic systems to learnable, parameterized counterparts. Conventional memory systems often rely on predefined patterns to accumulate experiences, while our approach adopts learnable memory module that generates compact, role-aware latent representations for dynamic integration into each agents reasoning. 4. Methodology In this section, we first present the overall pipeline of our proposed LatentMem framework ( Section 4.1). Then we detail each module within the framework, including the experience bank ( Section 4.2) and the memory composer ( Section 4.3). Finally, we introduce Latent Memory Policy Optimization (LMPO), which enables end-to-end optimization of the memory composer through task-level feedback and reinforcement learning algorithm ( Section 4.4). 4.1. Overall Pipeline The overall pipeline of our proposed LatentMem framework is illustrated in Figure 2. It consists of two core components: lightweight experience bank for storing and retrieving historical trajectories, and learnable memory composer that transforms the retrieved relevant trajectories into compact, role-aware latent memories. Upon receiving new query, LatentMem first retrieves subset of relevant trajectories from the experience bank. These trajectories capture the interactions and behaviors of agents in previous MAS executions, forming historical context that can guide reasoning. The retrieved trajectories, together with each agents role profile, are then processed by the memory composer, which distills them into compact latent memories tailored to the corresponding agents. During each agents reasoning process, these latent memories are appended to the token embeddings as additional latent tokens, forming memory-augmented representation that incentivizes the agent to leverage prior experience and generate improved outputs. After the MAS completes task, the newly generated trajectory is appended to the experience bank, enabling incremental accumulation of experiences. This procedure forms self-improving loop, allowing LatentMem to continuously refine agent reasoning, support long-horizon coordination, and enhance continual adaptation. Moreover, the injection of 4 LatentMem: Customizing Latent Memory for Multi-Agent Systems latent memories maintains end-to-end differentiability of the entire forward process, facilitating efficient RL-based post-training [Qu et al., 2025] without incurring the heavy computation of retraining foundation models. 4.2. Experience Bank To accurately record historical MAS trajectories for future reuse, we construct an extremely lightweight experience bank B. In line with the principle that scalable systems should rely on general learning mechanisms rather than hand-crafted knowledge [Sutton, 2019], this bank stores and retrieves only raw trajectories, without introducing any human priors such as trajectory condensation [Wang and Chen, 2025] or insight extraction [Zhao et al., 2024], Initialization. We populate the experience bank with wide-ranging collection of trajectories covering multiple domains and MAS frameworks to enable the memory composer to learn generalizable memory patterns across diverse domains and agent coordination patterns. The resulting initialized bank is denoted as {ğœğ‘–}ğ¶ , where ğ¶ specifies its initial capacity. Each trajectory ğœ = {(ğ›¼ ğ‘—, ğ‘ ğ‘—, ğ‘œ ğ‘—)}ğ» ğ‘—=1 records, at each step, the index of the active agent ğ›¼ ğ‘— along with its input prompt ğ‘ ğ‘— and corresponding output ğ‘œ ğ‘—, where ğ» denotes the trajectory horizon. ğ‘–=1 Retrieval. Upon receiving new user query ğ‘, LatentMem performs similarity-based retrieval over to obtain subset of ğ¾ relevant trajectories Tğ‘: Tğ‘ = top-ğ¾ğœğ‘– (sim(v(ğ‘), v(ğœğ‘–))) = {ğœğ‘–}ğ¾ where v() maps queries or trajectories into latent embedding space, e.g., using MiniLM [Wang et al., 2020a], and sim(, ) denotes the cosine similarity. The retrieved trajectories will be subsequently processed by the memory composer, which distills them into latent memories to guide subsequent MAS reasoning tasks. ğ‘–=1, (3) Update. Once task is completed, the new trajectory ğœnew is appended to the experience bank for future reuse: {ğœnew}. This streamlined update mechanism allows LatentMem to incrementally accumulate experiences online during inference, facilitating continual adaptation and cross-task coordination without the need for retraining. (4) However, directly feeding the retrieved raw trajectories to agents is suboptimal, as it can overwhelm LLMs with excessive context [Cemri et al., 2025] and fails to capture role-specific representations in heterogeneous MAS [Subramaniam et al., 2025]. To address these limitations, we introduce the memory composer C, which effectively transforms low-level raw trajectories into compact, high-level, role-aware latent memories. 4.3. Memory Composer After identifying the relevant raw trajectories Tğ‘, we introduce the memory composer C, which provides each agent with generalizable memories. Formally, is instantiated as deep neural network ğœğœ™ parameterized by ğœ™. At each reasoning step ğ‘—, ğœğœ™ takes as input the retrieved trajectories Tğ‘ and the role profile ğ›¾ğ›¼ ğ‘— , producing fixed-length, agent-aware latent memory matrix: of the active agent ğ‘ğ›¼ ğ‘— ğ‘š ğ‘— = ğœğœ™(ğ›¾ğ›¼ ğ‘— , Tğ‘) â„ğ¿ ğ·, (5) where ğ¿ is fixed length of the latent memory and ğ· denotes the hidden dimension of the foundation model. 5 LatentMem: Customizing Latent Memory for Multi-Agent Systems To conduct reasoning, the active agent ğ‘ğ›¼ ğ‘— state vectors â„ ğ‘— = (â„(1) form an extended input shaped â„(ğ¿+ğ¿ ) ğ·, resulting in memory-augmented policy: first encodes its input prompt ğ‘ ğ‘— into sequence of hidden ) â„ğ¿ğ·. The agents latent memory ğ‘š ğ‘— is then concatenated to â„ ğ‘— to , . . . , â„(ğ¿) ğ‘— ğ‘— ğœ‹ğœƒğ›¼ ğ‘— ( ğ‘ ğ‘—, ğ‘š ğ‘—) = ğœ‹ğœƒğ›¼ ğ‘— (concat(â„ ğ‘—, ğ‘š ğ‘—)), (6) is wrapped version of ğœ‹ğœƒğ›¼ ğ‘— where ğœ‹ğœƒğ›¼ ğ‘— that seamlessly incorporates memory injection at the model level, remaining transparent to the agent layer and requiring no modifications to the system architecture. 4.4. Latent Memory Policy Optimization (LMPO) To enable end-to-end optimization of LatentMem while preserving strong generalization across diverse domains and MAS frameworks, we propose Latent Memory Policy Optimization (LMPO), variant of GRPO [Shao et al., 2024], which encourages the memory composer to generate transferable, high-utility latent representations. Parametric Dependency. We first describe the gradient flow during LMPO, in which the learning signal propagates through the latent memories to optimize the memory composer C, while keeping the agent backbones {ğœƒğ‘˜}ğ‘ ğ‘˜=1 frozen. Formally, given query ğ‘ and the retrieved trajectories Tğ‘ from the experience bank B, the generation of new trajectory ğœ = {(ğ›¼ ğ‘—, ğ‘ ğ‘—, ğ‘œ ğ‘—)}ğ» ğ‘—=1 is factorized sequentially as: ğ» (cid:214) â„™(ğœ ğ‘, Tğ‘; ğœ™, {ğœƒğ‘˜}ğ‘ ğ‘˜=1) = â„™(ğ‘œ ğ‘— ğ‘ ğ‘—, ğ‘š ğ‘—; ğœƒğ›¼ ğ‘—). (7) ğ‘—=1 Crucially, the latent memory ğ‘š ğ‘— = ğœğœ™(Tğ‘, ğ›¾ğ›¼ ğ‘—), as defined in Equation (5), serves as differentiable interface through which ğœ™ influences the autoregressively generated output ğ‘œ ğ‘— of the active agent ğ‘ğ›¼ ğ‘— at reasoning step ğ‘—: â„™(ğ‘œ ğ‘— ğ‘ ğ‘—, ğ‘š ğ‘—; ğœƒğ›¼ ğ‘—) = ğ‘‡ (cid:214) ğ‘¡=1 ğœ‹ğœƒğ›¼ ğ‘— (ğ‘œ(ğ‘¡) ğ‘— ğ‘ ğ‘—, ğ‘œ(<ğ‘¡) ğ‘— , ğ‘š ğ‘—). (8) Since the composite policy ğœ‹ğœƒğ›¼ ğ‘— is conditioned on ğ‘š ğ‘—, the gradient of any task-level objective can be backpropagated through the agents forward pass to refine ğœ™. This dependency ensures that the memory composer can be optimized end-to-end to produce high-quality latent memories, forming the basis of our policy optimization strategy. Policy Optimization. Building on the differentiable path above, LMPO leverages task-level feedback through latent memories as bridge to directly optimize memory composer attached to the MAS, encouraging it to distill high-utility, agent-specific memories from retrieved raw trajectories and thereby enhance reasoning quality and overall performance. Formally, given query ğ‘ and its retrieved relevant trajectories Tğ‘, we sample group of ğº trajectories: ğ‘–=1 â„™( ğ‘, Tğ‘; ğœ™, {ğœƒğ‘˜}ğ‘ Each trajectory is evaluated using reward ğ‘…(Ë†ğœğ‘–), and its relative quality is captured by the group-based advantage: {Ë†ğœğ‘–}ğº ğ‘˜=1). (9) Ë†ğ´ğ‘– = ğ‘…(Ë†ğœğ‘–) mean({ğ‘…(Ë†ğœğ‘–)}ğº ğ‘–=1) + ğœ– std({ğ‘…(Ë†ğœğ‘–)}ğº ğ‘–=1) . (10) While standard reinforcement learning [Zhang et al., 2025f] often employs trajectory-level objectives, such approaches treat all sequences equally, causing tokens in longer MAS interactions to contribute 6 LatentMem: Customizing Latent Memory for Multi-Agent Systems disproportionately less to the gradient [Yu et al., 2025]. This makes it difficult for the memory composer to capture critical coordination patterns within long-horizon tasks. Instead, we adopt token-level surrogate objective: JLMPO(ğœ™) = ğ”¼ğ‘D,{Ë†ğœğ‘– }ğº ğ‘–=1â„™( ğ‘,Tğ‘ ) (cid:34) 1 {Ë†ğœğ‘–}ğº ğ‘–=1 ğ‘–, ğ‘—,ğ‘¡ (cid:35) Lğ‘–, ğ‘—,ğ‘¡ (ğœ™) , (11) where {Ë†ğœğ‘–}ğº defined as: ğ‘–=1 is the total number of generated tokens within the trajectory group and Lğ‘–, ğ‘—,ğ‘¡ (ğœ™) is Lğ‘–, ğ‘—,ğ‘¡ (ğœ™) = min (cid:16) ğ‘Ÿğ‘–, ğ‘—,ğ‘¡ (ğœ™) Ë†ğ´ğ‘–, clip(cid:0)ğ‘Ÿğ‘–, ğ‘—,ğ‘¡ (ğœ™), 1 ğœ€, 1 + ğœ€(cid:1) Ë†ğ´ğ‘– (cid:17) , and the token-level importance sampling ratio ğ‘Ÿğ‘–, ğ‘—,ğ‘¡ (ğœ™) = ğœ‹ğœƒ(ğ‘œ(ğ‘¡) ğ‘–, ğ‘— ğœ‹ğœƒ(ğ‘œ(ğ‘¡) ğ‘–, ğ‘— ğ‘ğ‘–, ğ‘—, ğ‘œ(<ğ‘¡) ğ‘–, ğ‘— ğ‘ğ‘–, ğ‘—, ğ‘œ(<ğ‘¡) ğ‘–, ğ‘— , ğœğœ™(ğ›¾ğ›¼ğ‘–, ğ‘— , Tğ‘)) , ğœğœ™old (ğ›¾ğ›¼ğ‘–, ğ‘— , Tğ‘)) (12) (13) measures how the policy of agent ğ‘ğ›¼ğ‘–, ğ‘— memory. at reasoning step ğ‘— and token ğ‘¡ is modulated by the updated 5. Experiments 5.1. Experimental Setup Datasets and Benchmarks. Our evaluation covers six benchmarks across four domains: (1) Knowledge-intensive QA: TriviaQA [Joshi et al., 2017] and PopQA [Mallen et al., 2023]; (2) Coding: KodCode [Xu et al., 2025c] and BigCodeBench [Jain et al., 2024]; (3) Reasoning QA: StrategyQA [Geva et al., 2021]; and (4) Symbolic Planning: PDDL [Silver et al., 2024]. Detailed information for these benchmarks are provided in Appendix B.1. Baselines. Besides memory-free methods, we select three representative single-agent memory baselines, including Voyager [Wang et al., 2023], Generative [Park et al., 2023], and JoyAgent [Liu et al., 2025], as well as four multi-agent memory baselines adopted from mainstream MAS frameworks: MetaGPT [Hong et al., 2023], ChatDev [Qian et al., 2024a], OAgents [Zhu et al., 2025], JoyAgent Liu et al. [2025], and MAS-specific G-Memory [Zhang et al., 2025a]. Additional details are provided in Appendix B.2. MAS and LLM Backbones. Four representative multi-agent frameworks are adopted to integrate with LatentMem and the baselines, including AutoGen [Wu et al., 2024], MacNet [Qian et al., 2024b], CAMEL [Li et al., 2023] and DyLAN [Liu et al., 2024]. More details on the MAS setups are placed in Appendix B.3. To instantiate these MAS frameworks, we adopt two common LLMs with different sizes, i.e., Qwen/Qwen3-4B-Instruct-2507 and meta-llama/Llama-3.1-8B-Instruct. Training Configurations. We implement the embedding function v() mentioned in Equation (3) with the all-MiniLM-L6-v2 model Wang et al. [2020b]. The memory composer is realized as lightweight transformer, with its parameters initialized from the backbone LLM and trained using LoRA Hu et al. [2022]. We set ğ¾ = 1 in Equation (3) and fix the latent memory sequence length to ğ¿ = 8. The ablation study on hyper-parameter settings is reported in Section 5.6. Detailed training setups and parameter configurations are listed in Appendix B.4. 7 LatentMem: Customizing Latent Memory for Multi-Agent Systems Table 1 Performance comparison across diverse memory frameworks on six benchmarks, using Qwen3-4B-Instruct-2507 as the backbone. The best and second best results are highlighted. TriviaQA, KodCode, StrategyQA, and PopQA are in-domain benchmarks, while BigCodeBench and PDDL are out-of-domain. AutoGen and MacNet are in-distribution MAS frameworks, whereas CAMEL and DyLAN are unseen frameworks introducing new agent roles and collaboration patterns. Additional details are provided in Appendix B.4. TriviaQA KodCode StrategyQA PopQA BigCodeBench PDDL Held-in Held-out 60.31 57.342.97 60.350.04 59.650.66 57.502.81 60.560.25 59.440.87 59.850.46 76.5116.20 53.77 57.293.52 61.167.39 59.896.12 58.194.42 62.438.66 61.337.56 60.636.86 65.9812.21 56.96 57.550.59 59.062.10 57.630.67 56.570.39 59.202.24 58.101.14 58.331.37 68.7411.78 53.87 56.282.41 61.787.91 60.396.52 59.065.19 61.237.36 59.966.09 60.987.11 65.5511.68 68.40 68.550.15 70.051.65 70.902.50 68.950.55 71.403.00 70.902.50 70.802.40 76.808.40 70.40 70.500.10 71.501.10 71.150.75 69.800.60 72.502.10 70.800.40 71.300.90 78.908. 70.70 68.202.50 69.900.80 70.650.05 69.850.85 70.400.30 70.200.50 71.400.70 77.757.05 69.25 68.550.70 71.502.25 71.702.45 70.050.80 71.352.10 71.352.10 71.151.90 78.809.55 58.25 59.180.93 60.922.67 62.664.41 60.482.23 63.895.64 62.334.08 62.454.20 65.487.23 56.44 60.223.78 61.144.70 61.795.35 61.354.91 62.205.76 62.345.90 61.795.35 64.468.02 58.04 59.041.00 59.381.34 61.203.16 58.270.23 60.122.08 61.343.30 60.092.05 64.206.16 57.44 60.232.79 62.234.79 60.923.48 61.794.35 63.766.32 62.124.68 61.353.91 65.227. 38.78 33.245.54 33.804.98 40.371.59 33.565.22 42.673.89 41.893.11 40.701.92 52.7013.92 24.89 35.3310.44 33.989.09 43.3918.50 35.3810.49 43.8818.99 43.2218.33 41.9017.01 44.1419.25 32.38 36.784.40 37.475.09 35.322.94 36.253.87 38.045.66 37.505.12 31.990.39 47.2314.85 24.89 39.2714.38 37.0012.11 43.5718.68 40.4015.51 42.2817.39 41.6716.78 42.3617.47 44.2519.36 79.53 79.150.38 80.400.87 81.141.61 79.400.13 80.961.43 80.520.99 81.321.79 81.491.96 78.15 79.171.02 78.330.18 79.471.32 80.532.38 80.442.29 79.801.65 78.860.71 81.493. 79.14 78.121.02 80.531.39 80.351.21 79.910.77 79.280.14 79.450.31 79.280.14 80.701.56 79.81 79.820.01 78.251.56 78.860.95 80.000.19 80.350.54 79.890.08 78.771.04 81.401.59 16.39 15.221.17 11.954.44 13.942.45 13.622.77 17.060.67 14.262.13 16.700.31 23.497.10 20.73 15.854.88 17.812.92 16.813.92 14.755.98 21.821.09 21.200.47 22.832.10 25.134.40 22.10 18.583.52 22.550.45 17.294.81 23.651.55 24.562.46 20.651.45 13.898.21 28.126.02 23.81 16.557.26 19.194.62 18.585.23 26.372.56 17.945.87 25.812.00 19.754.06 29.085. Average 53.61 52.111.50 52.910.70 54.781.17 52.251.36 56.092.48 54.891.28 55.301.69 62.759.14 50.73 53.062.33 53.993.26 55.424.69 53.332.60 57.216.48 56.455.72 56.225.49 60.029.29 53.22 53.040.18 54.821.60 53.740.52 54.080.86 55.272.05 54.541.32 52.500.72 61.127.90 51.51 53.451.94 54.993.48 55.674.16 56.284.77 56.154.64 56.805.29 55.734.21 60.729.21 Method No-memory ChatDev MetaGPT Generative Voyager G-Memory JoyAgent OAgents LatentMem No-memory ChatDev MetaGPT Generative Voyager G-Memory JoyAgent OAgents LatentMem No-memory ChatDev MetaGPT Generative Voyager G-Memory JoyAgent OAgents LatentMem No-memory ChatDev MetaGPT Generative Voyager G-Memory JoyAgent OAgents LatentMem - H - H o t a L C y 5.2. Main Results LatentMem Delivers High-Performance Memory Across Domains and MAS Frameworks. As shown in Table 1, when integrated with in-domain MAS frameworks such as AutoGen and MacNet that are powered by Qwen3-4B-Instruct-2507, LatentMem outperforms the state-of-the-art singleand multi-agent memory baselines by an average of 7.86% and 6.66%, respectively. Notably, it yields 16.20% improvement for AutoGen on the TriviaQA benchmark. Furthermore, LatentMem exhibits strong scalability with the model size increases. As shown in Appendix 4, it elevates MacNets performance on KodCode from 48.50% to 65.50% using Llama-3.1-8B-Instruct. LatentMem Exhibits Strong Generalization Capability. On out-of-domain benchmarks, most MAS memory methods fail to generalize. As shown in Table 1, LatentMem improves AutoGen on PDDL LatentMem: Customizing Latent Memory for Multi-Agent Systems by 7.10%, while MetaGPT and Voyager drop by up to 4.44% and 2.77%, respectively. Similarly, on previously unseen MAS frameworks, LatentMem boosts CAMEL on KodCode by 7.05%, whereas nearly all baselines decline. We attribute these gaps to the rigid and homogeneous memory designs of existing methods, which limit adaptability and representational capacity. These results demonstrate LatentMem robustness across domains, agent roles, and collaboration patterns, highlighting the importance of role-aware memory for generalizable MAS. 5.3. Cost Analysis As shown in Figure 3, LatentMem achieves the largest performance gains among memorybased baselines while incurring minimal time and token costs. It delivers the greatest improvement on TriviaQA for DyLAN (+11.68% over No-memory) with substantially lower time overhead (e.g., cutting inference time by factor of 2.16 relative to OAgents), and achieves the highest gain on KodCode for AutoGen (+8.40%) while using even fewer tokens than No-Memory (0.01M tokens less). In contrast, JoyAgent consumes 1.87M additional tokens for only 2.50% gain, highlighting the superior efficiency of LatentMem. 5.4. Comparison with Multi-Agent FineTuning To assess the effectiveness of LatentMem under comparable training conditions, we compare it with representative multi-agent fine-tuning baseline, MARTI [Zhang et al., 2025e]. MARTI is trained with GRPO under exactly the same computational budget as LatentMem, where all agents share single LLM backbone and are trained on the same datasets (TriviaQA, KodCode, StrategyQA, and PopQA), ensuring fully fair and controlled comparison. Figure 3 Time and token consumption of LatentMem. Each panel shows the trade-off between performance and resource cost under different memory architectures: the top row plots performance versus time, the bottom row plots performance versus token cost. Circle area reflects relative resource consumption. Table 2 Performance comparison between the multi-agent fine-tuning method MARTI and LatentMem on KodCode and TriviaQA across two MAS frameworks, AutoGen and MacNet. As shown in Table 2, LatentMem consistently outperforms direct agent backbone fine-tuning across all settings. Notably, on the TriviaQA dataset with the AutoGen framework, LatentMem achieves substantial improvement of 11.73%. Moreover, on more complex MAS settings such as MacNet, MARTI experiences 1.10% performance drop on KodCode compared to AutoGen, whereas LatentMem instead surpasses its AutoGen counterpart by 2.10%. These results indicate that LatentMem better exploits the structural advantages of complex MAS, leading to stronger performance gains than direct backbone fine-tuning. 64.78 76.5111.73 62.31 65.983.67 74.20 76.802.60 73.10 78.905.80 MARTI LatentMem MARTI LatentMem KodCode TriviaQA AutoGen Method MacNet MAS 9 LatentMem: Customizing Latent Memory for Multi-Agent Systems 5.5. Framework Analysis LatentMem Consistently Delivers Role-Aware Memory. As shown in Figure 4, LatentMem consistently generates role-specific latent memories across both in-domain and out-of-domain datasets, as well as seen and unseen MAS. In the left panel (in-domain KodCode, seen MAS AutoGen), user-proxy and assistant memories form two clearly separated clusters. In the right panel (out-of-domain BigCodeBench, unseen MAS CAMEL), the role-specific memories remain well separated, demonstrating LatentMem ability to avoid homogeneous memory even in entirely novel task domains, agent roles, and collaboration patterns. LatentMem Scales Efficiently as Task Horizon Expands. We visualize the cumulative gains of different memory systems as tasks progress, specifically by tracking their impact on cumulative accuracy. As shown in Figure 5, LatentMem steadily improves as more experiences are collected, surpassing all baselines that rely on complex, multi-granularity memory. Although early performance exhibits higher variance due to limited samples, LatentMem quickly stabilizes and continues to improve, demonstrating its ability to efficiently distill high-utility, transferable knowledge from past interaction trajectories, which can then be leveraged to guide the reasoning process of MAS. 5.6. Sensitivity & Ablation Study. Sensitivity Analysis. We analyze the sensitivity of LatentMem to two key hyperparameters: the latent memory length ğ¿ and the number of relevant trajectories ğ¾. As shown in Figure 6 (Left), performance generally improves with larger ğ¿, but with diminishing returns; balancing accuracy and computational cost, we set ğ¿ = 8. The effect of ğ¾ is detailed in Appendix C.3: while baselines such as G-Memory (see Appendix B.2 for more details) degrades when ğ¾ > 3 due to information overload, LatentMem consistently improves on both TriviaQA and KodCode, demonstrating its ability to distill useful information from redundant trajectories via latent memory. Component Ablation. We present ablation Figure 4 t-SNE visualization of latent memories generated by LatentMem across different datasets and MAS frameworks. Figure 5 Evolution of cumulative accuracy (reward) across question indices. The cumulative accuracy at index ğ‘– is defined as the average accuracy (reward) over the first ğ‘– questions. Figure 6 (Left) Sensitivity of model performance to the latent memory length ğ¿. (Right) Ablation results highlighting the impact of the memory composer and the experience bank. 10 LatentMem: Customizing Latent Memory for Multi-Agent Systems Figure 7 Case study of LatentMem. By leveraging role-aware and compact latent memory, LatentMem prevents common MAS issues such as step repetition and blindly following retrieved trajectories, while enabling role-aware coordination and self-correction. studies of LatentMem in Figure 6 (Right), where we introduce two variants: without role and without experience, corresponding to the removal of agent profile guidance (ğ›¾ in eq. (5)) and the disabling of real-time updates in the experience bank (as in eq. (4)), respectively. When the memory composer no longer receives agent profiles, resulting in identical latent memories across agents, performance drops slightly for simple MAS such as AutoGen (2.30% on KodCode) and more substantially for complex MAS like MacNet (6.45%), highlighting the importance of agent-aware memory. Disabling real-time updates in the experience bank leads to minor performance degradation on KodCode (3.60% on MacNet) but larger drop on PDDL (7.63%), demonstrating its crucial role in adapting to complex task distributions. These results underscore the contributions of both components to the overall effectiveness of LatentMem. 5.7. Case Study Figure 7 shows that LatentMem, by providing role-aware memory, can prevent or promptly correct common error patterns in MAS. Vanilla MacNet often suffers from step repetition, while MacNet with OAgents blindly follow the retrieved trajectories, violating task specifications. In contrast, LatentMem high-level, role-aware latent memory enables agents to reinforce role compliance and coordinate effectively, allowing the MAS to self-correct short-term errors and complete tasks successfully. Detailed trajectories and error analyses are in Appendix C.4. 6. Conclusion In this work, we present LatentMem, latent memory framework for multi-agent systems that enables role-aware and token-efficient memory customization. By leveraging lightweight experience bank and learnable memory composer, each agent receives its latent memories distilled from raw trajectories, naturally reinforcing role compliance and enhancing coordination. We further introduce Latent Memory Policy Optimization, which encourages the composer to produce transferable, high-utility latent representations, enhancing generalization across diverse task domains and MAS frameworks. Extensive experiments on six benchmarks and four MAS frameworks demonstrate that LatentMem achieves substantial performance gains, robust generalization, and high efficiency, while effectively mitigating memory homogenization and information overload. 11 LatentMem: Customizing Latent Memory for Multi-Agent Systems"
        },
        {
            "title": "Impact Statement",
            "content": "Ethical Considerations. This study focuses on developing and evaluating multi-agent memory mechanisms using publicly accessible benchmarks and datasets. It does not involve the collection, processing, or deployment of private, personal, or sensitive user information, and all experiments are carried out in controlled, offline research environments. Consequently, we do not foresee any major ethical risks associated with this work. Societal Implications. The methods proposed in this paper seek to enhance the robustness and reliability of LLM-based multi-agent systems, with potential benefits for applications including assistive robotics, information organization, and long-term decision-making. However, more powerful memory capabilities may also increase the potential for misuse if such systems are deployed without proper safeguards. As such, we position this work primarily as research contribution and stress that real-world deployment should be accompanied by appropriate oversight, safety assessments, and compliance with legal and ethical standards."
        },
        {
            "title": "References",
            "content": "Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Ma Chang, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llm agents. Advances in neural information processing systems, 37:7432574362, 2024. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2023. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023. arXiv preprint arXiv:2305.14325, 2023. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor RÃ¼hle, and Saravan Rajmohan. Legomem: Modular procedural memory for multi-agent llm systems for workflow automation. arXiv preprint arXiv:2510.04851, 2025. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multiagent collaborative framework. In The twelfth international conference on learning representations, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1:3, 2022. LatentMem: Customizing Latent Memory for Multi-Agent Systems Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, et al. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, and Zilong Zheng. Seek in the dark: Reasoning via test-time instancelevel policy gradient in latent space, 2026. URL https://arxiv.org/abs/2505.13308. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 2024. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. Jiarun Liu, Shiyue Xu, Shangkun Liu, Yang Li, Wen Liu, Min Liu, Xiaoqing Zhou, Hanmin Wang, Shilin Jia, Shaohua Tian, et al. Joyagent-jdgenie: Technical report on the gaia. arXiv preprint arXiv:2510.00510, 2025. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. dynamic llm-powered agent network for task-oriented agent collaboration. In First Conference on Language Modeling, 2024. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):25792605, 2008. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, 2023. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6(3):1, 2023. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, In Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. 13 LatentMem: Customizing Latent Memory for Multi-Agent Systems Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186, 2024a. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155, 2024b. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, and Yujia Bao. Collaborative memory: Multi-user memory sharing in llm agents with dynamic access control. arXiv preprint arXiv:2505.18279, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Tom Silver, Soham Dan, Kavitha Srinivas, Joshua Tenenbaum, Leslie Kaelbling, and Michael Katz. Generalized planning in pddl domains with pretrained large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 2025620264, 2024. Vighnesh Subramaniam, Yilun Du, Joshua Tenenbaum, Antonio Torralba, Shuang Li, and Igor Mordatch. Multiagent finetuning: Self improvement with diverse reasoning chains. arXiv preprint arXiv:2501.05707, 2025. Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint arXiv:2507.06229, 2025. Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Yali Du, and Mykola Pechenizkiy. Meal: benchmark for continual multi-agent reinforcement learning. arXiv preprint arXiv:2506.14990, 2025. Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang Nguyen. Multi-agent collaboration mechanisms: survey of llms. arXiv preprint arXiv:2501.06322, 2025. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Junlin Wang, Jue Wang, Ben Athiwaratkun, et al. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1): 396413, 2020a. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:57765788, 2020b. 14 LatentMem: Customizing Latent Memory for Multi-Agent Systems Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957, 2025. Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, and Shingo Takamatsu. Talk structurally, act hierarchically: collaborative framework for llm multi-agent systems. arXiv preprint arXiv:2502.11098, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. Rong Wu, Xiaoman Wang, Jianbiao Mei, Pinlong Cai, Daocheng Fu, Cheng Yang, Licheng Wen, Xuemeng Yang, Yufan Shen, Yuxin Wang, et al. Evolver: Self-evolving llm agents through an experience-driven lifecycle. arXiv preprint arXiv:2510.16079, 2025a. Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu. From human memory to ai memory: survey on memory mechanisms in the era of llms. arXiv preprint arXiv:2504.15965, 2025b. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025a. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms, 2025b. URL https://arxiv.org/abs/2502.12134. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025c. Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, et al. Toward efficient agents: Memory, tool learning, and planning. arXiv preprint arXiv:2601.14192, 2026. Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, and Weinan Zhang. Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems. arXiv preprint arXiv:2504.00587, 2025a. Yongjin Yang, Euiin Yi, Jongwoo Ko, Kimin Lee, Zhijing Jin, and Se-Young Yun. Revisiting multiagent debate as test-time scaling: systematic study of conditional effectiveness. arXiv preprint arXiv:2505.22960, 2025b. Rui Ye, Xiangrui Liu, Qimin Wu, Xianghe Pang, Zhenfei Yin, Lei Bai, and Siheng Chen. X-mas: Towards building multi-agent systems with heterogeneous llms. arXiv preprint arXiv:2505.16997, 2025. Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuan-Jing Huang, and Xipeng Qiu. Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1513515153, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. Masrouter: Learning to route llms for multi-agent systems. arXiv preprint arXiv:2502.11133, 2025. 15 LatentMem: Customizing Latent Memory for Multi-Agent Systems Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506, 2024a. Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024b. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025a. Guibin Zhang, Muxin Fu, and Shuicheng Yan. Memgen: Weaving generative latent memory for self-evolving agents, 2025b. URL https://arxiv.org/abs/2509.24704. Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture search via agentic supernet. arXiv preprint arXiv:2502.04180, 2025c. Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, and Shuicheng Yan. Memevolve: Meta-evolution of agent memory systems. arXiv preprint arXiv:2512.18746, 2025d. Kaiyan Zhang, Runze Liu, Xuekai Zhu, Kai Tian, Sihang Zeng, Guoli Jia, Yuchen Fan, Xingtai Lv, Yuxin Zuo, Che Jiang, Ziyang Liu, Jianyu Wang, Yuru Wang, Ruotong Zhao, Ermo Hua, Yibo Wang, Shijie Wang, Junqi Gao, Xinwei Long, Youbang Sun, Zhiyuan Ma, Ganqu Cui, Lei Bai, Ning Ding, Biqing Qi, and Bowen Zhou. Marti: framework for multi-agent llm systems reinforced training and inference, 2025e. URL https://github.com/TsinghuaC3I/MARTI. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025f. Yiqun Zhang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, and Kaisong Song. Can llms beat humans in debating? dynamic multi-agent framework for competitive debate. arXiv preprint arXiv:2408.04472, 2024c. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents: Large language models collaborating on long-context tasks. Advances in Neural Information Processing Systems, 37:132208132237, 2024d. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1963219642, 2024. Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, and Qianli Ma. Lifelong learning of large language model based agents: roadmap. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2026. Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan VuliÄ‡, Anna Korhonen, and Sercan Ã– ArÄ±k. Multi-agent design: Optimizing agents with better prompts and topologies. arXiv preprint arXiv:2502.02533, 2025a. Heng Zhou, Hejia Geng, Xiangyuan Xue, Li Kang, Yiran Qin, Zhiyong Wang, Zhenfei Yin, and Lei Bai. Reso: reward-driven self-organizing llm-based multi-agent system for reasoning tasks. arXiv preprint arXiv:2503.02390, 2025b. 16 LatentMem: Customizing Latent Memory for Multi-Agent Systems He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, et al. Oagents: An empirical study of building effective agents. arXiv preprint arXiv:2506.15741, 2025. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and JÃ¼rgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024. Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, and Ling Yang. Latent collaboration in multi-agent systems, 2025. URL https://arxiv.org/abs/2511.20639. 17 LatentMem: Customizing Latent Memory for Multi-Agent Systems A. More Related Work This work is closely related to prior studies on latent reasoning, including but not limited to MemGen [Zhang et al., 2025b], SoftCoT [Xu et al., 2025b], and LatentSeek [Li et al., 2026]. While these works demonstrate that latent-encoded memory can be an efficient carrier of reasoning and experience, their applicability to multi-agent memory systems remains largely unexplored. Our study addresses this gap by extending latent reasoning into the multi-agent setting and opening new direction for structured, collaborative memory evolution. LatentMAS [Zou et al., 2025] is also relevant work, though it concentrates on latent communication. B. Experimental Details B.1. Dataset Descriptions In this section, we describe the datasets used in our experiments: TriviaQA [Joshi et al., 2017] is large-scale question answering dataset comprising challenging, naturally occurring trivia questions paired with evidence documents, designed to evaluate reading comprehension and information retrieval over long contexts. KodCode Xu et al. [2025c] is fully synthetic coding dataset with diverse tasks, verifiable solutions, and tests, supporting both supervised fine-tuning and reinforcement learning. StrategyQA [Geva et al., 2021] is QA benchmark of yes/no questions requiring multi-step reasoning, with annotated reasoning steps and supporting evidence to evaluate implicit reasoning. PopQA [Mallen et al., 2023] is large-scale open-domain question answering dataset of entitycentric QA pairs generated from Wikidata knowledge tuples with annotated subject, relation, object, and Wikipedia page view metadata, designed to support factual QA evaluation. BigCodeBench [Jain et al., 2024] is programming benchmark dataset of diverse Python coding tasks with structured prompts, canonical solutions, and unit tests, designed to evaluate and compare code generation capabilities of large language models. PDDL Silver et al. [2024] is game dataset from AgentBoard [Chang et al., 2024], comprising variety of strategic games where agents use PDDL expressions to complete complex tasks. B.2. Baseline Setup In this section, we provide detailed descriptions of each baseline used in our comparison: Voyager: The Voyager memory originates from the Voyager agent [Wang et al., 2023], where an embodied agent continuously explores the Minecraft environment and generates new artifacts, with memory acting as the central driver of its development. Since Voyagers memory is originally designed for single-agent setup, we extend it to the multi-agent setting by retrieving agentspecific histories based on each agents visible dialogue context. Other single-agent memory architectures are adapted similarly. Generative: This memory baseline follows Generative agent [Park et al., 2023] and incorporates both raw observational memory and high-level reflective memory. The reflective component encodes abstract insights produced by the agent via reflection, offering more organized and conceptual representation of its experiences. JoyAgent: Adapted from JoyAgent [Liu et al., 2025], this hierarchical memory comprises working, semantic, and procedural layers. Working memory maintains the execution context, semantic memory distills task trajectories and lessons for retrieval, and procedural memory encodes dynamic system prompts for strategic control. This structure ensures long-horizon continuity 18 LatentMem: Customizing Latent Memory for Multi-Agent Systems and adaptive behavior in single-agent task execution. MetaGPT: This memory design is derived from MetaGPT [Hong et al., 2023], incorporating shared message pool that enables agents to publish and subscribe to structured information throughout the software development lifecycle. This system facilitates inside-trial coordination by allowing agents to retrieve historical context for self-correction, while also supporting cross-trial improvement through long-term memory layer that stores feedback to refine agent constraints over time. ChatDev: This memory design is adapted from ChatDev [Qian et al., 2024a], focusing solely on inside-trial memoryinformation stored internally during the resolution of single task by multiple agents. OAgents: Based on OAgents framework [Zhu et al., 2025], this design uses hierarchical memory system: short-term for real-time tasks and long-term for fused historical insights. It employs memory summarization and vectorized retrieval to manage information density and extract relevant context. This multi-layered approach ensures long-horizon reasoning while minimizing redundancy during complex tasks. G-Memory: G-Memory [Zhang et al., 2025a] operates through structured lifecycle of retrieval, traversal, and updating to manage the extensive interaction histories of multi-agent systems. When new query arrives, the system first performs coarse-grained similarity search on the Query Graph to identify relevant historical tasks, followed by bi-directional traversal: an upward traversal to the Insight Graph to extract high-level strategic guidance and downward traversal to the Interaction Graph to retrieve condensed, fine-grained collaboration trajectories. These filtered memories are then customized for each agent based on their specific roles to provide actionable context without overwhelming the context window. Once the task is executed, GMemory performs an agentic update where new collaborative logs are stored, successful or failed experiences are distilled into new nodes in the Insight Graph, and semantic edges are reinforced across the three-tier hierarchy to facilitate continuous self-evolution. B.3. Multi-agent System Setup In this section, we detail the setups of our four adopted MAS frameworks, AutoGen, MacNet, CAMEL and DyLAN. B.3.1. AutoGen AutoGen [Wu et al., 2024] is widely adopted framework for orchestrating multi-agent collaboration. In this work, we adopt its A2 setting, which consists of Retrieval-augmented User Proxy agent and Retrieval-augmented Assistant agent, both extended from AutoGens built-in agents. The User Proxy agent incorporates vector database retriever to provide relevant external context, while the Assistant agent utilizes the retrieved information to generate accurate responses. This design enables interactive retrieval and supports effective knowledge grounding for both question answering and code generation tasks. B.3.2. MacNet MacNet [Qian et al., 2024b] is representative work that explores decentralized and scalable multiagent systems. Its key feature lies in the absence of central agent; instead, it introduces edge agents, which are invoked between agent interactions to provide actionable instructions to the next agent based on the previous agents outputs. In our implementation, we adopt the random graph topology from MacNet, shown to be robust across diverse scenarios, and employ total of five agents, including the edge agents. 19 LatentMem: Customizing Latent Memory for Multi-Agent Systems B.3.3. CAMEL CAMEL [Li et al., 2023] is role-playingbased communication agent framework designed to guide agents to autonomously collaborate on complex tasks with minimal human intervention via inception prompting. The framework introduces task specifier agent that concretizes vague ideas and achieves task objectives through multi-turn, instruction-following dialogues. In our implementation, we employ four agents: three participating in the debate and one dedicated to summarization. B.3.4. DyLAN DyLAN [Liu et al., 2024] is debate-style framework similar to LLM-Debate, but incorporates more efficient agent-wise early stopping mechanism during multi-turn interactions. DyLAN utilizes an agent selection algorithm based on an unsupervised metric, namely the Agent Importance Score, which identifies the most contributive agents through preliminary trial tailored to the specific task. In our implementation of DyLAN, four agents engage in the debate, while an additional ranker agent evaluates their relative importance. B.4. Training Details B.4.1. Evaluation of Generalization To evaluate the generalization capabilities of different methods, we consider two dimensions: (1) whether the method can generalize to domains unseen during training, and (2) whether it can generalize to multi-agent systems unseen during training. For the in-domain and out-of-domain evaluation, we use TriviaQA, KodCode, StrategyQA, and PopQA as in-domain datasets, and BigCodeBench and PDDL as out-of-domain datasets. For MAS generalization, AutoGen and MacNet are considered as MAS encountered during training, while CAMEL and DyLAN are treated as MAS not seen during training. B.4.2. Training Trajectories Collection and Usage For all baselines and LatentMem, we first collect data on all in-domain datasets using the training splits and in-distribution MAS. Specifically, we gather training data by running AutoGen and MacNet on TriviaQA, KodCode, StrategyQA, and PopQA. In total, we obtain 40,580 trajectories. For nonparametric baselines, such as MetaGPT [Hong et al., 2023] and G-Memory [Zhang et al., 2025a], the training data is processed and stored directly in their memory modules. For parametric baselines, such as MARTI [Zhang et al., 2025e], the training data is used to optimize model parameters. For LatentMem, we first follow the same procedure as non-parametric baselines: the MAS performs standard reasoning on the training data, and the resulting trajectories are stored in the experience bank (without using the memory composer at this stage). We then train the memory composer via LMPO on the same training data, enabling it to effectively transform retrieved trajectories from the experience bank into latent memories that can assist MAS reasoning. B.4.3. Parameter Configurations Here, we provide full details of the parameter settings used to implement LatentMem. In practice, the memory composer is instantiated as pretrained LLM (e.g., Qwen3-4B-Instruct-2507 or Llama-3.1-8B-Instruct). Prior to training, we collect diverse datasets and raw MAS trajectories on AutoGen and MacNet using the official training splits of in-domain datasets (TriviaQA, KodCode, StrategyQA, PopQA). Training then proceeds following the same protocol applied to all LatentMem: Customizing Latent Memory for Multi-Agent Systems baseline methods. Detailed hyperparameter configurations are reported in Table 3. Table 3 The detailed parameter settings when implementing LatentMem."
        },
        {
            "title": "Parameter\nToken limit for prompts\nToken limit for responses\nTraining temperature\nEvaluation temperature\nDiscount factor\nClipping epsilon\nWeight of KL penalty\nNumber of training epochs\nNumber of prompt reuse\nMacro training batch size\nMicro training batch size\nMacro rollout batch size\nMicro rollout batch size\nOptimizer name\nLearning rate\nWarmup ratio\nLora r\nLora alpha\nLora target modules\nLora dropout\nLora bias\nLora task type\nGradient norm\nGradient clipping\nGradient checkpoint\nFlash Attention\nMixed precision\nEnable vLLM\nEnable DeepSpeed",
            "content": "Setting 10240 4096 1.0 0.0 1.0 0.2 0.0 1 1 32 8 32 8 AdamW 1e-5 0.1 16 32 [q_proj, v_proj] 0.1 None CAUSAL_LM 1.0 True True True True True True C. Extra Results C.1. Cost Analysis Figure 8 provides additional comparisons of token and time costs across various benchmarks and MAS frameworks with different memory architectures. Overall, LatentMem leverages its latent memory design to achieve the fastest inference and lowest token usage among all memory baselines, while consistently delivering the most substantial improvements. C.2. Role-aware Memory Analysis This section outlines the approach used to examine the geometric organization of latent memories (illustrated in Figures 4 and 9) and to discover semantically meaningful clusters. The workflow consists of two main steps: first, deriving fixed-size vector representation for each memory sequence, 21 LatentMem: Customizing Latent Memory for Multi-Agent Systems Figure 8 Time and token consumption of LatentMem. Each panel shows the trade-off between performance and resource cost under different memory frameworks: the top row plots performance versus time, the bottom row plots performance versus token cost. Circle area reflects relative resource consumption. + indicates MAS framework evaluated on dataset B. and second, applying dimensionality reduction and clustering techniques. Consider {ğ‘š1, ğ‘š2, . . . , ğ‘šğµ} as collection of ğµ latent memory sequences obtained from LatentMems inference, where each sequence ğ‘šğ‘– = (ğ‘šğ‘–,1, . . . , ğ‘šğ‘–,ğ¿) â„ğ¿ ğ· contains ğ¿ token embeddings of dimensionality ğ·. To generate single, compact representation for each sequence, we compute its mean embedding ğ‘šğ‘– â„ğ· as follows: ğ‘šğ‘– = 1 ğ¿ ğ¿ ğ‘™=1 ğ‘šğ‘–,ğ‘™. (14) The resulting set of high-dimensional vectors, { ğ‘š1, ğ‘š2, . . . , ğ‘šğµ}, is then used as input for downstream analysis. For visualization purposes, we apply t-SNE [Maaten and Hinton, 2008] to map the vectors { ğ‘š1, ğ‘š2, . . . , ğ‘šğµ} into two-dimensional space. Formally, this transformation is defined as These 2D embeddings are subsequently employed to produce the scatter plots. ğ‘¦ğ‘– â„2 = ğ‘“t-SNE( ğ‘šğ‘–). (15) LatentMem: Customizing Latent Memory for Multi-Agent Systems Figure 9 t-SNE visualization of latent memories generated by LatentMem across different datasets and MAS frameworks. Figure 10 Comparison between LatentMem and G-Memory under varying top-ğ¾ settings on KodCode and TriviaQA with AutoGen. While LatentMem consistently benefits from more trajectories as ğ¾ increases, G-Memorys performance declines when ğ¾ > 3. C.3. Sensitivity Analysis As supplement to Section 5.6, we conduct sensitivity analysis of LatentMem with respect to the top-ğ¾ raw trajectories retrieved from the experience bank, as shown in Figure 10. It can be observed that G-Memory initially benefits from increasing ğ¾, but its performance starts to decline when ğ¾ > 3; for example, on KodCode, accuracy drops from 72.95% at ğ¾ = 3 to 70.25% at ğ¾ = 5. In contrast, LatentMem continues to improve as ğ¾ increases, highlighting its ability to leverage fixed-length, role-aware latent memory to distill useful information even from redundant trajectories. LatentMem: Customizing Latent Memory for Multi-Agent Systems Table 4 Performance comparison with diverse memory frameworks on six benchmarks. The underlying LLM backbone is Llama-3.1-8B-Instruct. We highlight the best and second best results. TriviaQA, KodCode, StrategyQA, and PopQA are in-domain benchmarks, while BigCodeBench and PDDL are out-of-domain; AutoGen and MacNet are in-distribution MAS frameworks, whereas CAMEL and DyLAN are unseen frameworks. Additional details are provided in Appendix B.4. TriviaQA KodCode StrategyQA PopQA BigCodeBench PDDL Held-in Held-out 72.030.00 74.102.07 73.771.74 74.532.50 74.602.57 73.121.09 74.922.89 71.620.00 72.570.95 72.831.21 72.170.55 71.790.17 72.200.58 74.452. 72.860.00 72.990.13 72.360.50 72.570.29 73.690.83 73.520.66 74.001.14 72.620.00 73.871.25 73.220.60 73.691.07 74.001.38 73.490.87 74.101.48 47.450.00 50.002.55 49.401.95 46.451.00 50.202.75 49.552.10 65.9018.45 48.500.00 50.301.80 49.851.35 44.803.70 49.200.70 48.500.00 65.5017.00 48.950.00 51.702.75 49.800.85 48.950.00 49.500.55 49.900.95 63.8514.90 47.550.00 49.852.30 47.100.45 44.802.75 48.801.25 48.250.70 64.2516. 61.500.00 58.732.77 63.972.47 61.280.22 64.202.70 65.944.44 67.896.39 62.940.00 62.660.28 60.802.14 61.351.59 62.230.71 63.760.82 66.423.48 60.250.00 62.662.41 60.260.01 61.571.32 61.140.89 61.481.23 65.285.03 60.670.00 62.882.21 64.854.18 61.350.68 61.350.68 63.252.58 66.836.16 41.200.00 47.416.21 46.004.80 46.755.55 47.246.04 47.256.05 49.408.20 40.590.00 48.427.83 47.326.73 47.116.52 48.968.37 48.307.71 50.169. 40.440.00 47.687.24 49.098.65 48.688.24 48.808.36 47.907.46 49.909.46 40.500.00 47.927.42 48.558.05 48.888.38 47.326.82 47.907.40 49.348.84 80.960.00 82.111.15 82.111.15 81.490.53 82.671.71 82.111.15 83.842.88 79.280.00 81.282.00 81.111.83 81.672.39 81.462.18 82.283.00 83.844.56 80.280.00 81.931.65 80.280.00 81.751.47 81.931.65 81.491.21 82.752.47 80.260.00 81.281.02 82.191.93 81.581.32 82.282.02 81.301.04 83.252. 14.030.00 25.6111.58 20.636.60 21.657.62 24.3110.28 16.992.96 28.9614.93 17.330.00 21.474.14 24.807.47 18.971.64 19.101.77 16.710.62 25.618.28 17.450.00 23.245.79 23.976.52 15.791.66 20.693.24 17.120.33 24.467.01 15.200.00 18.453.25 17.692.49 14.330.87 19.644.44 16.751.55 22.627.42 Average 52.860.00 56.333.47 55.983.12 55.362.50 57.204.34 55.832.97 61.828. 53.380.00 56.122.74 56.122.74 54.350.97 55.462.08 55.291.91 61.007.62 53.370.00 56.703.33 55.962.59 54.881.51 55.962.59 55.231.86 60.046.67 52.800.00 55.712.91 55.602.80 54.101.30 55.562.76 55.162.36 60.067.26 Method No-memory MetaGPT Generative Voyager G-Memory OAgent LatentMem No-memory MetaGPT Generative Voyager G-Memory OAgent LatentMem No-memory MetaGPT Generative Voyager G-Memory OAgent LatentMem No-memory MetaGPT Generative Voyager G-Memory OAgent LatentMem - H - H o t a M A D C.4. Case Study Figure 7 illustrates how LatentMem, by providing role-aware memory, can prevent or promptly correct common error patterns in MAS. For example, when solving the PDDL task: Satisfy the following conditions: ball1 is at room b, ball2 is at room a, ball3 is at room b, ball4 is at room b, ball5 is at room a, ball6 is at room a, the vanilla MacNet repeatedly moves ball2 between room and room while ignoring the tasks termination conditions, resulting in loop, which is called Step Repetition error. MacNet with OAgents, although able to retrieve similar historical trajectories, is influenced by excessively long histories and fails to account for differences between the current task and the retrieved trajectories. This leads to blindly following the retrieved steps without satisfying the actual task requirements, which is called Disobey Task Specification error. Moreover, both methods suffer from significant Reasoning-Action Mismatch, where the agents planned reasoning steps are correct, but its executed actions deviate from the intended plan. In contrast, LatentMem provides high-level, role-aware latent memory to each agent at every reasoning step. By leveraging personalized high-level historical experience, each agent naturally reinforces its role compliance, thereby enabling the actor-critic style coordination mechanism in MacNet to function effectively. Even when short-term error occurs in the trajectory (e.g., generating an incorrect action \"pick ball5 rooma right\"), the MAS can immediately self-correct in the next reasoning step using 24 LatentMem: Customizing Latent Memory for Multi-Agent Systems check valid answer procedure, ultimately completing the task successfully. D. Prompt Templates In this section, we provide the prompt templates used in our framework. Note that these templates may vary slightly across different task domains. As an example, we present the prompt template of CAMEL and AutoGen for code tasks. CAMELs Prompt Template for Strategy Agent You are strategy-generation agent. Your task is to read given coding problem and provide **detailed implementation strategy**, but **do not write any code**. # Objectives - Understand the problem requirements. - Describe the algorithm, data structures, and step-by-step approach. - Ensure the strategy is clear enough for developer tao implement directly. # Output Guidelines - Focus on logic and process; avoid including actual code or irrelevant explanations. - You should keep your response concise, no more than 3 sentences. CAMELs Prompt Template for Code Agent You are Code Implementation agent. You will be provided with problem and an analysis of that problem from user agent. Your task is to produce complete and correct code implementations based on coding problems. # Objectives - Write clear, well-structured, and correct Python code. - Do not include any explanations or comments outside the code. # Output Guidelines - Wrap the entire Python code inside code block using triple backticks: python # your code here CAMELs Prompt Template for Test Agent You are code evaluator. Your task is to review the current coding problem and the code written by the actor agent for that problem. - If the code is correct, reply only with: \"Agree\". - If the code has issues, give brief and concise feedback only(Keep your response short and within 3 sentences). CAMELs Prompt Template for Summarizer Agent You are summarization and final-code-generation agent. Your task is to read the previous actor code implementations and the corresponding critic improvement suggestions, and then produce the final, corrected, and consolidated code solution for the current task. # Objectives - Carefully examine the actors code solutions. - Incorporate the critics improvement suggestions when necessary. - Produce clean, complete, and correct final code implementation. - Do not include explanations, comments, or any text outside the code block. 25 LatentMem: Customizing Latent Memory for Multi-Agent Systems # Output Format - Wrap the entire final Python code inside triple backticks: python # final code here AutoGens Prompt Template for Assistant Agent You are strategy-generation agent. Your task is to read given coding problem and provide **detailed implementation strategy**, but **do not write any code**. # Objectives - Understand the problem requirements. - Describe the algorithm, data structures, and step-by-step approach. - Ensure the strategy is clear enough for developer tao implement directly. # Output Guidelines - Focus on logic and process; avoid including actual code or irrelevant explanations. - You should keep your response concise, no more than 3 sentences. AutoGens Prompt Template for User Proxy Agent You are Code Implementation agent. Your task is to read the implementation strategy provided by the Assistant agent and produce **complete, executable Python code** that follows the strategy exactly. # Objectives - Implement the solution according to the detailed strategy from the Assistant. - Write clear, well-structured, and correct Python code. - Make sure the code covers all steps and handles edge cases mentioned in the strategy. - Do not include any explanations or comments outside the code. # Output Guidelines - Wrap the entire Python code inside code block using triple backticks: python # your code here E. Example In this section, we present example trajectories generated by LatentMem-enhanced AutoGen on Knowledge QA (TriviaQA) and Game (PDDL) tasks to demonstrate the effectiveness of LatentMem."
        },
        {
            "title": "Example trajectory on PDDL",
            "content": "<im_start>user ## Successful Examples (Reference Cases) Below are some examples of similar tasks that were successfully completed. Please use these as references to guide your thinking and approach to the current task: <im_start>user <observation>Ball1 is ball. Ball1 is at rooma. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at rooma. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at rooma. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>Pick up ball1 at rooma with arm right.</action><im_end> <im_start>user 26 LatentMem: Customizing Latent Memory for Multi-Agent Systems <observation>Ball1 is ball. Ball1 is carrying right. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at rooma. Left is gripper. Left is free. Right is gripper. Robby is at rooma. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>Pick up ball2 at rooma with arm left.</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is carrying right. Ball2 is ball. Ball2 is carrying left. Ball3 is ball. Ball3 is at rooma. Left is gripper. Right is gripper. Robby is at rooma. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>Move from rooma to roomb</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is carrying right. Ball2 is ball. Ball2 is carrying left. Ball3 is ball. Ball3 is at rooma. Left is gripper. Right is gripper. Robby is at roomb. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>drop ball1 at rooma with arm right</action><im_end> <im_start>user <observation>The action is not valid and therefore takes no effect. Please remember to satisfy the restriction of actions. You can also check valid actions.</observation><im_end> <im_start>assistant <action>check valid actions</action><im_end> <im_start>user <observation>Valid actions are: drop ball1 roomb right, drop ball2 roomb left, Move from roomb to rooma. , check valid actions</observation><im_end> <im_start>assistant <action>drop ball1 roomb right</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is carrying left. Ball3 is ball. Ball3 is at rooma. Left is gripper. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>drop ball2 roomb left</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at rooma. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>move from roomb to rooma</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at rooma.Left is gripper. Left is free. Right is gripper. Right is free. Robby is at rooma. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>Pick up ball3 at rooma with arm right.</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is carrying right. Left is gripper. Left is free. Right is gripper. Robby is at rooma. Room rooma Room roomb</observation><im_end> <im_start>assistant <action>Move from rooma to roomb</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is carrying right. Ball4 is ball. Ball4 is at rooma. Left is gripper. Left is free. Right is gripper. Robby is at roomb. Room rooma Room roomb</observation>< im_end> <im_start>assistant <action>drop ball3 rooms right</action><im_end> <im_start>user <observation>Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at roomb. Ball4 is ball. Ball4 is at rooma. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb. The task is completed.</observation><im_end> --- Use the above examples and insights as foundation, and now work on the following task: You are robot with gripper that can move objects between different rooms. Your name is Robby. 27 LatentMem: Customizing Latent Memory for Multi-Agent Systems ## There are three actions defined in this domain: - think xxx: - Format: think: need to move block first because... - Any other format is invalid and will not be recognized by the system. - move <room1> <room2>: This action allows the robot to move from one room to another.The action has single precondition, which is that the robot is currently in room. The effect of this action is to move the robot to another room and to remove the fact that it is in the original room. - pick <obj> <room> <gripper>: This action allows the robot to pick up an object using the gripper. The action has three preconditions: (1) the object is located in room (2) the robot is currently in the same room and (3) the gripper is free (i.e., not holding any object). The effect of this action is to update the state of the world to show that the robot is carrying the object using the gripper, the object is no longer in the room, and the gripper is no longer free. - drop <obj> <room> <gripper>: This action allows the robot to drop an object that it is carrying. The action has two preconditions: (1) the robot is currently carrying the object using the gripper, and (2) the robot is currently in room. The effect of this action is to update the state of the world to show that the robot is no longer carrying the object using the gripper, the object is now located in the room, and the gripper is now free. ## NOTE: - You should strictly follow these actions, no any other actions are allowed! - Before you respond, you must ensure that your output actions strictly comply with the rules described above! Here is your initial observation: Ball1 is ball. Ball1 is at rooma. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at roomb. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at rooma. Room rooma Room roomb **Here is your task: The goal is to satisfy the following conditions: ball1 is at roomb. , ball2 is at roomb. , ball3 is at rooma. , ball4 is at rooma.<im_end> <im_start>assistant <action>pick ball1 rooma right</action><im_end> <im_start>user Ball1 is ball. Ball1 is carrying right. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at roomb. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Robby is at rooma. Room rooma Room roomb<im_end> <im_start>assistant <action>move from rooma to roomb</action><im_end> <im_start>user Ball1 is ball. Ball1 is carrying right. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at roomb. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant <action>drop ball1 roomb right</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at roomb. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant <action>pick ball2 rooma left</action><im_end> <im_start>user The action is not valid and therefore takes no effect. You should use check valid actions. command to get some clues!<im_end> <im_start>assistant <action>check valid actions</action><im_end> <im_start>user Valid actions are: Pick up ball1 at roomb with arm left. , Pick up ball3 at roomb with arm left. , Move from roomb to rooma. , Pick up ball3 at roomb with arm right. , Pick up ball4 at roomb with arm left. , Pick up ball1 at roomb with arm right. , Pick up ball4 at roomb with arm right. , check valid actions<im_end> <im_start>assistant <action>pick ball3 roomb left</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is carrying left. Ball4 is ball. Ball4 is at roomb. Left is gripper. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant LatentMem: Customizing Latent Memory for Multi-Agent Systems <action>move from roomb to rooma</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is carrying left. Ball4 is ball. Ball4 is at roomb. Left is gripper. Right is gripper. Right is free. Robby is at rooma. Room rooma Room roomb<im_end> <im_start>assistant <action>drop ball3 rooma left</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at rooma. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at rooma. Room rooma Room roomb<im_end> <im_start>assistant <action>pick ball4 roomb right</action><im_end> <im_start>user The action is not valid and therefore takes no effect. You should use check valid actions. command to get some clues!<im_end> <im_start>assistant <action>check valid actions</action><im_end> <im_start>user Valid actions are: Pick up ball2 at rooma with arm right. , Pick up ball3 at rooma with arm right. , Pick up ball3 at rooma with arm left. , Pick up ball2 at rooma with arm left. , Move from rooma to roomb. , check valid actions<im_end> <im_start>assistant <action>pick ball2 rooma right</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is carrying right. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Robby is at rooma. Room rooma Room roomb<im_end> <im_start>assistant <action>move from rooma to roomb</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is carrying right. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant <action>drop ball2 roomb right</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is at roomb. Left is gripper. Left is free. Right is gripper. Right is free. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant <action>pick ball4 roomb right</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is carrying right. Left is gripper. Left is free. Right is gripper. Robby is at roomb. Room rooma Room roomb<im_end> <im_start>assistant <action>move from roomb to rooma</action><im_end> <im_start>user Ball1 is ball. Ball1 is at roomb. Ball2 is ball. Ball2 is at roomb. Ball3 is ball. Ball3 is at rooma. Ball4 is ball. Ball4 is carrying right. Left is gripper. Left is free. Right is gripper. Robby is at rooma. Room rooma Room roomb<im_end> <im_start>assistant <action>drop ball4 rooma right</action><im_end>"
        },
        {
            "title": "Example trajectory on TriviaQA",
            "content": "<im_start>user In 2006, who was the first football player in World Cup history to win the \"Best Young Player Award\"?<im_end> <im_start>assistant <search> first football player to win Best Young Player Award at World Cup 2006 </search>< im_end> <im_start>user Doc 1(Title: \"FIFA World Cup awards\") specific award for their position, they are still eligible for the Golden Ball as well, as when Oliver Kahn was awarded in 2002. In the 29 LatentMem: Customizing Latent Memory for Multi-Agent Systems event of tie, the Golden Glove Award goes to the goalkeeper who progressed furthest in the competition. The next tiebreakers are saves made, then minutes played. Although the Golden Glove Award was first awarded in 1994, every All-Star Team in World Cups prior to 1998 except 1990 included only one goalkeeper. The \"\"Best Young Player\"\" award was awarded for the first time at the 2006 World Cup in Germany and given to Germanys Lukas Doc 2(Title: \"2006 Ballon dO\"\") Netherlands, New Zealand, Norway, Poland, Portugal, Republic of Ireland, Romania, Russia, San Marino, Scotland, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine and Wales. Each picked first (5pts), second (4pts), third (3pts), fourth (2pts) and fifth choice (1pt). Cannavaro was the fifth Italian player to win the award after Omar Sivori (1961), Gianni Rivera (1969), Paolo Rossi (1982) and Roberto Baggio (1993). Gianluigi Buffon (Italy) was the top ranked goalkeeper in the list, in second place; Ronaldinho (Brazil) was the top ranked midfielder, in fourth place, while Thierry Henry (France) was the top-ranked forward, in third place. The following Doc 3(Title: \"2006 FIFA World Cup\") goals were scored by 110 players, with four of them credited as own goals. FIFAs Technical Study Group (TSG) also granted Man of the Match award to one player in each match. Italys Andrea Pirlo won the most Man of the Match awards, with three in total. Miroslav Klose, Agustin Delgado, Arjen Robben, Ze Roberto, Alexander Frei, Michael Ballack, and Patrick Vieira each received two awards. The All-star team is squad consisting of the 23 most impressive players at the 2006 World Cup, as selected by FIFAs Technical Study Group. The team was chosen from shortlist of<im_end > <im_start>assistant <search>first football player to win Best Young Player Award at 2006 World Cup</search>< im_end> <im_start>user Doc 1(Title: \"FIFA World Cup awards\") specific award for their position, they are still eligible for the Golden Ball as well, as when Oliver Kahn was awarded in 2002. In the event of tie, the Golden Glove Award goes to the goalkeeper who progressed furthest in the competition. The next tiebreakers are saves made, then minutes played. Although the Golden Glove Award was first awarded in 1994, every All-Star Team in World Cups prior to 1998 except 1990 included only one goalkeeper. The \"\"Best Young Player\"\" award was awarded for the first time at the 2006 World Cup in Germany and given to Germanys Lukas Doc 2(Title: \"2006 Ballon dOr\") Netherlands, New Zealand, Norway, Poland, Portugal, Republic of Ireland, Romania, Russia, San Marino, Scotland, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine and Wales. Each picked first (5pts), second (4pts), third (3pts), fourth (2pts) and fifth choice (1pt). Cannavaro was the fifth Italian player to win the award after Omar Sivori (1961), Gianni Rivera (1969), Paolo Rossi (1982) and Roberto Baggio (1993). Gianluigi Buffon (Italy) was the top ranked goalkeeper in the list, in second place; Ronaldinho (Brazil) was the top ranked midfielder, in fourth place, while Thierry Henry (France) was the top-ranked forward, in third place. The following Doc 3(Title: \"2006 FIFA World Cup\") goals were scored by 110 players, with four of them credited as own goals. FIFAs Technical Study Group (TSG) also granted Man of the Match award to one player in each match. Italys Andrea Pirlo won the most Man of the Match awards, with three in total. Miroslav Klose, Agustin Delgado, Arjen Robben, Ze Roberto, Alexander Frei, Michael Ballack, and Patrick Vieira each received two awards. The All-star team is squad consisting of the 23 most impressive players at the 2006 World Cup, as selected by FIFAs Technical Study Group. The team was chosen from shortlist of<im_end > <im_start>assistant <answer>Lukas Podolski</answer><im_end>"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "National University of Singapore",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tongji University"
    ]
}