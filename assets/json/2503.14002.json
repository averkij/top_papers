{
    "paper_title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling",
    "authors": [
        "Damian Boborzi",
        "Phillip Mueller",
        "Jonas Emrich",
        "Dominik Schmid",
        "Sebastian Mueller",
        "Lars Mikelsons"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is a promising perspective for making these models available in these fields. Creating high-quality, domain-specific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains a significant bottleneck. We present MeshFleet, a filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes a pipeline for automated data filtering based on a quality classifier. This classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through a comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 0 0 4 1 . 3 0 5 2 : r MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling Damian Boborzi University of Augsburg damian.boborzi@uni-a.de"
        },
        {
            "title": "Dominik Schmid\nUniversity of Augsburg\nWork done at BMW Group",
            "content": "Phillip Mueller BMW Group University of Augsburg phillip.mueller@bmw.de"
        },
        {
            "title": "Jonas Emrich\nTU Darmstadt\nWork done at BMW Group",
            "content": "Lars Mikelsons University of Augsburg lars.mikelsons@uni-a.de"
        },
        {
            "title": "Abstract",
            "content": "Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is promising perspective for making these models available in these fields. Creating high-quality, domainspecific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains significant bottleneck. We present MeshFleet, filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes pipeline for automated data filtering based on quality classifier. This classifier is trained on manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling. 1. Introduction The early-stage engineering design process is complex and iterative endeavour, characterized by rapid iteration, ideation, feasibility studies, and simulation-driven evaluation. specific example is automotive design, where engineers and designers rely on digital 3D design representations to explore concepts, evaluate proportions, and en1 sure compatibility with mechanical and aerodynamic constraints. High-quality 3D models are central to this process and accelerate decision-making, allowing teams to visualize and refine designs before committing to physical prototypes. Generating such structured 3D assets remains challenge, as manual modeling is both time-intensive and resource-demanding. Recent advances in 3D generative modeling have shown remarkable potential for creating realistic and diverse 3D content from text or reference images[14, 24, 28, 29, 32, 37, 39]. Moving towards such generative models could revolutionize the whole design process in many areas. However, 3D generative models have yet to be widely adopted in industrial design applications as they fall short of generating design representations that exhibit symmetry, geometric consistency, and high levels of detail [1, 21, 25]. Fine-tuning 3d generative foundation models for engineering design data promises to increase their industrial applicability. Although large-scale 3D datasets like Objaverse-XL [7] offer an unprecedented data volume for such endeavors, the availability of datasets in specialized domains is often limited. The performance of these models is highly dependent on the quality and relevance of fine-tuning data [27, 33], and large-scale public datasets contain significant proportion of noisy or irrelevant samples, necessitating careful filtering and annotation [32, 37]. The general amount of vehicles and vehicle-like objects in Objaverse and Objaverse-XL [6, 7] is high, with an estimate of more than 20,000 captions from Cap3D [16, 17] and TRELLIS500K[37] depicting cars. However, the actual number of high-quality vehicles which satisfy the requirements for fine-tuning model for design applications is much lower, the exact number of these objects being unclear. The manual curation of such datasets is timeconsuming and expensive. This not only hinders progress in training or fine-tuning 3D generative models for specific domains but also restricts the development of custom methods for conditional control. Here, an analogy to image generation is apparent, where mechanisms such as ControlNet [41] or Readout Guidance [15] require training additional adapters on task-specific datasets. To address these challenges, we propose MeshFleet1, curated dataset of high-quality 3D vehicle models derived from Objaverse-XL. We define high-quality vehicle as single 3D object that is well-defined car with recognizable make and model, exhibiting detailed contours and representative features. The MeshFleet dataset was constructed in two phases. First, we created manually labeled subset of Objaverse [6] by automatically identifying potential vehicles through image-based object detection. Each candidate object was then manually annotated with quality label that reflected its suitability to fine-tune generative model to produce high-quality vehicle representations. Second, leveraging this manually labeled data, we trained quality classifier based on DINOv2 [20] features and SigLIP [40] embeddings. This classifier was designed to automatically identify and filter out low-quality or nonvehicle objects from the larger Objaverse-XL collection. The initial classifier training was followed by an iterative refinement process. This process involved analyzing the description of objects from CAP3D [16] and TRELLIS500K [37] to identify and correct misclassifications, with the corrected samples added to the training data. Furthermore, we incorporated Monte Carlo dropout [11] to estimate the uncertainty of the model, enabling active learning by prioritizing objects with high output entropy for manual review and potential inclusion in the training set [23]. Our final training data for the classifier consists of 6200 labeled objects from different categories and with varying quality. Only small subset of these objects belongs to high-quality vehicles. The classifier trained on this data, achieves 95% agreement with manual labels on held-out test set. After finalizing the classifier training, we applied our automated pipeline to process and classify the remaining objects in Objaverse-XL. Specifically, we processed over one million (1,059,727) objects from the Objaverse Alignment [7] and TRELLIS500K [37] subsets, both of which contain diverse collection of high-quality 3D objects. From this processing, we identified and selected 1620 high-quality vehicle models for inclusion in the MeshFleet dataset. final manual inspection of these filtered vehicles ensured that all included objects met our quality criteria. Beyond the 3D models themselves, MeshFleet includes generated captions and size estimates for each vehicle, providing additional metadata for downstream tasks. We validate the effectiveness of our labeling and filter1Code is available at: https : / / github . com / FeMa42 / MeshFleet ing method by fine-tuning SV3D [32], multiview generative model, on the filtered datasets and comparing the results with those obtained using other filtering strategies. The results demonstrate that finetuning on high-quality objects results in increased quality and multi-view-consistency of the domain-specific generated objects compared to finetuning on more, but less relevant data. This underlines that data quality is preferable over data quantity. Finetuning SV3D on significantly fewer, high-quality objects yields better model performance than finetuning on more objects with lower quality. Our contributions can be summarized as follows: filtered and annotated vehicle dataset extracted from Objaverse-XL, along with embeddings, text descriptions, and vehicle sizes. pipeline for the automated creation of high-quality, domain-specific 3D datasets. Together with qualitylabeled dataset of rendered objects. comprehensive evaluation demonstrating the superiority of our approach over existing filtering techniques, highlighting the importance of targeted data selection for 3D generative modeling. 2. Related Work Existing 3D datasets vary considerably in scale, quality, and the richness of their annotations. ShapeNet [2], while foundational resource, is limited by its relatively lowresolution models and simplistic textures. More recent datasets, such as ABO [4], GSO [8], and OmniObject3D [36], offer improved texture quality but are considerably smaller in scale. Objaverse [6] and Objaverse-XL [7] provide unprecedented scale and diversity; however, their heterogeneous quality necessitates effective filtering techniques to extract high-quality subsets [32, 37]. Manual annotation is inherently expensive and does not scale to the size of these large datasets. Automated annotation methods, such as GeoBiked for images [19] and CAP3D for 3D objects [16], address this scalability challenge by leveraging pre-trained models to generate descriptive text. Other approaches, such as UniG3D [26] combine different data sets into uniform multi-modal data representation, including images, text, and mesh information. Quality assessment of 3D models typically involves evaluating geometric validity, texture fidelity, and semantic consistency. Recent work has explored the use of large pre-trained models for this task, with promising results in the image domain using CLIP embeddings [12] and large vision-language models [35]. For 3D quality perception, CLIP embeddings of rendered images have also shown utility. For instance, Xiang et al. [37] employed CLIP embeddings of rendered views to estimate aesthetic scores and filter for objects with high aesthetic quality. Combining image-based assessments with 3D representations has been 2 shown to further enhance the performance of quality assessment methods [42]. Within the domain of 3D vehicles, several specialized datasets have emerged. While both ShapeNet [2] and Objaverse [6] include annotated cars, ShapeNets models often suffer from low resolution and simplistic textures, and the number of high-quality, annotated cars in Objaverse is limited. Chen et al. [3] introduce synthetic data generation pipeline for creating 3D car assets under various lighting conditions. DrivAerNet++ [10] is multimodal dataset specifically designed for aerodynamic car design, providing 3D meshes, parametric models, aerodynamic coefficients, flow and surface field data, segmented parts, and point clouds for 8,000 car configurations. While DrivAerNet++ offers high-quality data, its primary focus is on engineering and aerodynamic simulation rather than high-fidelity visual representation. In contrast, the 3DRealCar dataset [9] provides scans of 2,500 real-world cars, representing the first large-scale 3D dataset of real car scans with accompanying images and point clouds captured in diverse real-world settings. 3DRealCar offers the advantages and inherent challenges associated with real-world data. It is ideally suited for tasks requiring photorealism or those that must account for the complexities of real-world measurements, including sensor noise, occlusions, and varying visibility conditions. However, for engineering tasks demanding precise shape information, and editable 3D Models, the inherent measurement errors and imprecisions of real-world scanned data may be less desirable than clean, synthetic data. Our proposed MeshFleet dataset complements these existing resources by providing collection of curated, highquality, synthetic vehicle CAD models extracted and filtered from the large-scale Objaverse-XL dataset [7]. Unlike 3DRealCars focus on real-world scans, MeshFleet offers readily editable and customizable 3D models. To the best of our knowledge, MeshFleet is the first dataset to compile such collection of high-quality, synthetic car models with detailed textures at this scale. We believe that 3DRealCar, DrivAerNet++, and MeshFleet offer complementary resources. 3DRealCars real-world scans, DrivAerNet++s parametric car models, and MeshFleets curated CAD models, each provide unique capabilities for domain-specific research in 3D modeling, generation, and design. The combination of these datasets holds significant potential for enabling experiments in fine-grained 3D generation with domain-specific control and guidance, facilitating research that bridges the gap between synthetic CAD models and real-world car scans. 3. Data Labeling and Processing Our proposed pipeline comprises four primary stages: (1) manual quality labeling to create the 3D-Car-Quality Dataset, (2) training of classifier on 3D-Car-Quality Dataset Figure 1. Simplyfied overview of the quality assessment process to generate the MeshFleet Dataset. We render 4 views of each object from high-quality objaverse-XL subsets. We use object detection, clustering and text based filtering to generate subset of vehicle candidate objects which are subsequently labeled. We then train the High-Quality Car Classifier using the labeled 3D-Car-Quality Dataset. After training we used the trained classifiert to automatically generate the High-Quality Car Dataset which is finally manually reviwed and annotated. (3) automated quality filtering using the classifier, and (4) iterative refinement of the 3D-Car-Quality Dataset based on textual descriptions and model uncertainty. The automated pipeline operates on subset of the Objaverse-XL dataset [7], specifically targeting approximately 1,200,000 3D objects from the Alignment Dataset and the remaining objects within TRELLIS500K [37]. For each object, we generate four rendered views using Blender [5]. These renders are produced at resolution of 500x500 pixels, capturing the object from equidistant viewpoints along an orbital trajectory at consistent height and distance after object normalization. From these rendered views, we extract both DINOv2 and SigLIP feature embeddings. We perform dimensionality reduction via Principal Component Analysis (PCA) for the DINOv2 embeddings. These processed embeddings serve as input to the trained quality classifier, which estimates the objects quality label. 3.1. Manual Quality Labeling To facilitate the training of 3D vehicle quality classifier, we created the 3D-Car-Quality Dataset. This dataset was constructed in several stages. Initially, we manually labeled subset of approximately 4,000 objects from the Objaverse dataset [6]. This initial set comprised objects previously categorized as cars based on LVIS categories, supplemented by additional objects identified as potential vehicles using YOLOv10 [34], thereby accelerating the data collection process. We then performed iterative refinement, as detailed in Section 3.5, incorporating information from image captions and model uncertainty estimates to enhance both the quality and diversity of the labeled data. This iterative process resulted in final dataset of 6,200 objects 3 from Objaverse-XL [7]. Each data point consists of four rendered images of 3D object, along with quality label ranging from 1 to 5, reflecting the objects suitability for fine-tuning 3D generative model specializing in vehicles (see Figure 4 for examples): 1. Unsuitable: Not vehicle, extremely low quality, or exhibiting significant rendering artifacts. 2. Low-Quality Vehicle: Recognizable as car, but with substantial flaws, missing parts, or representing fictional or highly stylized vehicles. 3. Average-Quality Vehicle: recognizable car, but lacking fine details, overall refinement, and potentially exhibiting minor geometric inaccuracies. May represent specialized vehicle types (e.g., police cars). 4. High-Quality Vehicle: well-defined car with an identifiable make and model, exhibiting detailed contours and accurately representing characteristic features. 5. Very High-Quality Vehicle: Exhibiting comprehensive detail, accurate shape representation, high-fidelity texture detail, and complete and accurate feature representation from all viewpoints. To promote open science and ensure reproducibility, we will publicly release the 3D-Car-Quality Dataset, including both the rendered images and their corresponding quality labels. Providing the rendered images, in addition to the object identifiers and quality scores, is crucial for reproducibility, as the perceived quality can be influenced by the 3D object loading and rendering pipeline. 3.2. Verification of the Manual Quality Labeling To validate the effectiveness of our manual quality labeling scheme, we conducted series of fine-tuning experiments using Stable Video 3D (SV3D) [32], multi-view diffusion model. SV3D was selected as the foundation model for these experiments due to several key characteristics: its open-source availability, its strong baseline generation capabilities, its moderate computational requirements for finetuning, its architecture as an adapted latent diffusion model (which are known to be effective for domain-specific finetuning [22]), and, crucially, its multi-view output representation, which facilitates efficient analysis of object quality and view consistency in image space. We fine-tuned the UNet component of SV3D using fullparameter optimization, leveraging 21 rendered views per object. Since our focus is vehicle generation, we held out test set of 12 high-quality car instances for evaluation. The training datasets were constructed by filtering the manually labeled vehicle data according to the quality label. Each training subset included all objects with given quality label and higher. For instance, the Label 3 subset comprised objects with labels 3, 4, and 5, while the Label 4 subset contained objects with labels 4 and 5. hyperparameter search was performed for each label category to identify optimal learning rate schedules (including scheduler-specific parameters) and the number of optimization steps. Table 1 reports the results for the best-performing run in each category, detailing the training subset size (number of objects), training steps, and the resulting evaluation metrics: CLIPScore (CLIP-S) [12] and mean squared error (MSE) of the generated multi view images using the test set. Table 1. Fine-tuning subsets with the amount of objects, training steps, epochs, and evaluation metrics. The best scores are bold. Method Objects Steps Epochs MSE CLIP-S SV3D Label 5 Label 4 Label 3 Label 2 0 32 380 1265 2474 0 4000 8000 8000 12000 0 500 86 26 20 0.0527 0.0359 0.0218 0.0228 0.0268 0.897 0.892 0.923 0.913 0. Although fine-tuning solely on Label 5 data did not improve performance, likely due to the limited number of training instances, the combination of Label 4 and 5 data yielded the best overall fine-tuning results  (Table 1)  . Surprisingly, incorporating Label 3 objects decreased performance, despite increasing the training set size from 380 (Label 4+5) to 1265 objects (Label 3+4+5) more than threefold increase. While most objects with Label 3 are still recognizable as cars (see Figure 4), they exhibit lower fidelity, including reduced geometric detail or representation of specialized vehicle types (e.g., police cars). Further experiments with both extended (1200 steps) and reduced (400 steps) training durations on the Label 3 subset also resulted in lower evaluation metrics. Figure 2 presents example generations from the models fine-tuned on each data subset (more examples are in 11). Qualitative assessment further underlines the quantitative metrics: the model fine-tuned on Label 4 data produces, in our judgment, the visually superior results. While all fine-tuning runs show improved visual quality compared to the base model, generations from the Label 3 and Label 2 fine-tuned models exhibit less detail and more artifacts compared to those from the Label 4 fine-tuned model. These findings underscore the critical importance of selective filtering approach that prioritizes data relevance over sheer quantity for optimal fine-tuning. Simply increasing the dataset size with lower-quality or less relevant data proves detrimental. 3.3. Comparison of Manual Quality Labels and Aesthetic Scores We further investigate the relationship between our manual quality labels and the aesthetic scores provided in the TRELLIS500K dataset [37]. To isolate car-related objects within TRELLIS500K, we employ text classification approach using BART-based large language model 4 3.4. Car Quality Classification To automate quality assessment for the Objaverse-XL dataset, we developed binary classification model trained on the 3D-Car-Quality dataset with manually labeled 3D objects. Motivated by the superior fine-tuning performance of SV3D models achieved using high-quality data (Label 4 and 5, as demonstrated in Section 3.2), our classifier aims to identify objects comparable to these high-quality instances. Therefore, we constructed binary classification dataset from the manually labeled car subset, assigning label of 1 to high-quality cars (original Label 4 and 5) and 0 to all other instances (original Label 1, 2, and 3). We represent each 3D object using four rendered views captured from equidistant viewpoints along an orbital trajectory. From these renderings, we extract image features using both SigLIP [40] and DINOv2 [20]. SigLIP provides sequence of four 768-dimensional feature vectors per object. Recognizing the inherent redundancy in multiview renderings and the limited size of our manually labeled dataset, we employ Principal Component Analysis (PCA) to reduce the dimensionality of the DINOv2 features. Specifically, we concatenate the DINOv2 features from all four views (initial dimensions: 4 257 768) into single vector. We then apply PCA, on the entire labeled dataset, to reduce the dimensionality of the feature vectors to 3072. This dimensionality was chosen to match the aggregated SigLIP feature dimension, while achieving an explained variance ratio of 0.92. We investigated several classifier architectures for our binary quality prediction task. simple multi-layer perceptron (MLP) operating on concatenated SigLIP features achieved reasonable performance. However, we hypothesized that exploiting the sequential nature of the multiview features would improve both parameter efficiency and model robustness. Therefore, we compared two sequence processing architectures: Transformer encoders [31] and MLPMixer networks [30]. For the MLPMixer, we adopted the architecture presented in [30], which consists of two types of MLP layers: one applied independently to each feature patch, and another applied across patches. Instead of image patches, we treated the PCA-reduced DINOv2 features as four patches of 768 dimensions each. These were concatenated with the SigLIP feature sequence (4 768) along the patch dimension, resulting in combined input feature sequence of shape 8768. After the MLPMixer layers, the features are average-pooled, and final MLP layer predicts the objects quality label. We performed hyperparameter optimization for each classifier architecture using an 80/20 train/test split of the manually labeled data. We also investigated the influence of the number of rendered views on classifier performance. While using only single view per object significantly degraded performance, increasing the number of views beyond four provided only Figure 2. Example views of two vehicles from the Validation set. With the original render (top), SV3D without fine-tuning generated (2nd row), SV3D with Label 4 fine-tuning (3rd row), Label 3 fine-tuning (4th row), Label 2 fine-tuning (5th row), and MeshFleet fine-tuning (6th row). Figure 3. Comparison of the labels from the manual quality labeling (from label 1 to label 5) to the Aesthetic Scores from TRELLIS500K [37]. The plot shows the frequency of aesthetic scores at the different quality labels. We only include data which are described as car based on the caption from TRELLIS500K. [13] to identify captions describing cars. This filtering process yields 15, 820 car-related objects from TRELLIS500K (see Section 9 for more details on the text-based filtering), of which 2, 833 overlapped with our manually annotated dataset. Figure 3 presents comparative analysis of aesthetic score distributions across different quality labels within this overlapping subset. The analysis reveals weak correlation between aesthetic scores and our assigned quality labels. This discrepancy likely arises because our definition of high quality emphasizes domain-specific characteristics such as geometric detail, shape accuracy, and the absence of extraneous objects, criteria not directly captured by general aesthetic score. Coupled with the fine-tuning results, this observation suggests that filtering solely based on textual descriptions and generic aesthetic scores may be insufficient for specialized fine-tuning tasks requiring high fidelity 3D models. 5 marginal gains in accuracy, even with significantly more views (e.g., over 16). Given the increased computational cost associated with rendering and processing larger number of views, we opted to use four views per object as compromise between accuracy and computational efficiency. Furthermore, we evaluated the impact of different feature embeddings on the classification accuracy. Individually, both SigLIP and DINOv2 features yielded strong results. However, the combination of both feature sets produced the highest accuracy. Our final binary car quality classifier, utilizing the combined features and the MLP-Mixer architecture, achieved validation accuracy of 95.0%. 3.5. Refinement using Captions and Uncertainty Estimation To improve the accuracy of our initial classification, we incorporated refinement process utilizing object descriptions from CAP3D and TRELLIS500K. We leveraged textbased classification [13] (as in Section 3.2 for the car classification) to identify potential misclassifications by comparing the predicted quality label with the objects described content. For instance, consistent misclassifications of planes and chairs as high-quality cars were identified and rectified by adding these objects to the training set with label of 0. Furthermore, we employed Monte Carlo dropout [11] to estimate model uncertainty on unseen objects to improve the efficiency of data collection cycles based on active learning techniques[23]. By activating the dropout layers during inference and performing multiple classifications (we used 500) for each object, we obtained distribution of predictions. The entropy of this distribution served as measure of model uncertainty, where high entropy indicates high uncertainty. Objects exhibiting high uncertainty, indicative of ambiguous classifications and the potential to add information to the training set[23], were flagged for manual review and, incorporated into the training data. This iterative refinement process, involving four cycles of textbased misclassification detection and uncertainty-guided manual review, resulted in final labeled dataset of 6200 objects from Objaverse-XL [7]. The class distribution of this final dataset is illustrated in Figure 4. 4. Evaluation and Dataset Validation We rendered and processed over one million 3D objects from the Objaverse-XL dataset. Applying the binary classifier described in Section 3.4, we initially identified 1, 814 objects as high-quality vehicles. Subsequent manual verification of these classified objects resulted in the selection of 1, 620 objects for inclusion in the final MeshFleet dataset. Representative examples of MeshFleet objects are Figure 4. Relative amount of objects in each label categorie for the final dataset we used for training and testing the vehicle classification model. The total amount of objects in the dataset is 6200. Example objects for each label are shown inside each corresponding section. presented in Appendix 72. 4.1. Verification using Fine-tuning Experiments The MeshFleet dataset, containing 1620 high-quality vehicle models, is over four times larger than the initial manually labeled subset of Label 4 and 5 objects used for training the classifier. This increase in size is expected to improve the performance of 3D generative models fine-tuned on the domain-specific data. To validate the effectiveness of our automated filtering approach and the quality of the MeshFleet dataset, we conducted series of fine-tuning experiments using Stable Video 3D (SV3D) [32]. These experiments mirror the methodology described in Section 3.2, but utilize the MeshFleet dataset as the training data. To evaluate the effectiveness of our specialized quality filtering, we conducted comparative analysis against filtering approach relying solely on textual descriptions and aesthetic scores. Specifically, we created two subsets from Objaverse-XL, utilizing object descriptions and aesthetic scores from the TRELLIS500K dataset [37]. The first subset, designated CarCaption3K, included all objects filtered based on captions indicative of high-quality, realistic cars (detailed in Section 9). The second subset, CarCaption800, further restricted CarCaption3K by including only objects with an aesthetic score of 6.5 or higher. As with the MeshFleet and manually labeled experiments, we performed hyperparameter search for each fine-tuning subset. All experiments utilized the same held-out test set of 2Renderings of all MeshFleet objects are available at https:// anonymous.4open.science/r/meshfleet-render-6A4A 6 12 high-quality car instances, ensuring fair comparison by excluding these instances from all training subsets. Table 2 presents the results of this comparative analysis. Fine-tuning SV3D with the MeshFleet dataset yielded the highest CLIP-S of 0.925, surpassing even the 0.923 CLIP-S achieved with the Label 4 subset  (Table 1)  . While the MSE for the MeshFleet fine-tuned model was slightly higher than that of the model trained on the Label 4 subset, the superior CLIP-S indicates improved overall perceptual quality. Finetuning with CarCaption3K also improved upon the baseline SV3D performance, but resulted in lower overall metrics compared to fine-tuning with either the manually labeled subsets or MeshFleet. Notably, fine-tuning with the more restrictive CarCaption800 dataset decreased model performance. Manual inspection of objects with high aesthetic scores but low predicted quality revealed prevalence of toys, fictional vehicles, or partial car models. Although we used more restrictive caption-based filtering, the CarCaption3K dataset still includes these undesirable objects. While we optimized the prompts for the selection of the vehicles in the CarCaption3K dataset, we acknowledge that we did not perform exhaustive prompt optimization or finetuning of vision-language models (VLMs) for dirct filtering or generating object descriptions optimized for filtering. Leveraging the labeled data from the 3D-Car-Quality Dataset to perform such optimization represents promising avenue for future research. Table 2. Fine-tuning result with the MeshFleet and the caption filtered Dataset. Includes the training steps, epochs, and evaluation metrics. The best scores are bold. Baseline SV3D metrics are in Tabel 1. Method Objects Steps Epochs MSE CLIP-S MeshFleet 1620 CarCaption3K 3326 CarCaption800 834 12000 20000 8000 30 25 39 0.0229 0.0318 0.1390 0.925 0.902 0. 4.2. Dataset Statistics The MeshFleet dataset comprises 1620 high-quality 3D vehicle models: 1046 sourced from Sketchfab and 574 from GitHub. The majority of the models (1001) are licensed under Creative Commons Attribution license. In total, 1112 objects are available under licenses similar to Creative Commons, including variants with non-commercial restrictions, while 503 models have no explicitly specified license. To enhance the datasets utility for text-driven tasks, we generated descriptive captions for all vehicles using specialized prompt with GPT-4o-mini. Although many objects already possess captions from CAP3D or TRELLIS500K, we posit that consistent set of captions specifically focused on vehicle characteristics (e.g., body style, make, Figure 5. Distribution of vehicle categories within the MeshFleet dataset. The bar chart displays the frequency of each vehicle category (e.g., Sports Car, Coupe, SUV, Sedan) present in the dataset. The x-axis labels indicate the category, and the y-axis represents the number of 3D models belonging to that category. and model) will be more beneficial for downstream applications, such as text-to-3D vehicle generation. In addition to the 3D models and captions, we extracted vehicle-specific metadata for each object, including its category (e.g., sports car, coupe, SUV) and physical dimensions. Figure 5 presents the distribution of vehicle categories within the MeshFleet dataset. To facilitate comparisons of vehicle proportions, we derived the length, width, height, and wheelbase of each vehicle from its normalized 3D model. Figure 6 presents pairplot visualizing the relationships between these normalized dimensions. We anticipate that this detailed metadata will enable further research in 3D generative modeling, facilitating tasks such as shape generation with explicit control over vehicle dimensions (e.g., similar to VehicleSDF [18]) and domain-specific conditioning analogous to approaches like ControlNet [41] or Readout Guidance [15] in the image domain. 4.3. Additional Data and Preprocessing Details To facilitate reproducibility and further research, we will publicly release the code and data used for processing and filtering the Objaverse-XL dataset. While the MeshFleet dataset focuses on high-quality vehicles, reflecting our belief that this focus addresses critical need in current 3D generative model research, we recognize that the optimal balance between data quality and quantity may vary depending on the specific application. Therefore, alongside MeshFleet, we will release comprehensive collection of intermediate data and processing results. This supplementary data includes: rendered images, generated embeddings (SigLIP and PCA-compressed DINOv2), YOLOv10 detections, image captions, classifier outputs (with the uncertainty estimates), and the text classifications for all processed objects from Objaverse-XL. We 7 sults underscore the importance of specialized, task-specific quality assessment for 3D assets, particularly when downstream applications demand high fidelity and geometric accuracy. The strong performance of MeshFleet suggests our approachs suitability for generating domain-specific data tailored to 3D generative fine-tuning. Despite the promising results, we acknowledge several limitations of our current approach and highlight potential avenues for future research. primary limitation is the initial dependence on manually labeled data for training the quality classifier. Although general-purpose visionlanguage models (VLMs) showed promise and substantially reduced our initial manual effort, they did not eliminate the need for further manual refinement. We further mitigated the manual annotation burden through an iterative refinement process incorporating active learning principles (e.g., uncertainty sampling). However, achieving fully automated, high-quality 3D data curation without any manual labeling remains an open challenge. Another avenue for improvement involves the representation used for quality assessment. Our current pipeline leverages features extracted from 2D renderings (using DINOv2 and SigLIP) from four viewpoints. While this approach represents an effective balance between computational demands and classification accuracy, it may not fully capture all aspects of 3D mesh quality. Incorporating methods that directly analyze the 3D structure, could provide more comprehensive assessment of geometric and topological quality. Although prior work, such as MM-PCQA [42], has demonstrated the benefits of combining image features and point cloud data for 3D quality assessment, our preliminary experiments did not reveal significant improvement in classification accuracy when incorporating point clouds alongside DINOv2 and SigLIP features. Alternatively, 3Dnative embedding models, such as the structured latents from TRELLIS [37], may offer improved performance for 3D similarity and quality analysis due to their direct training on 3D data. However, the computational cost associated with these models must be considered; for instance, the structured latents in TRELLIS are derived from DINOv2 embeddings of 150 renderings, significantly increasing the processing requirements compared to our four-view approach. 6. Conclusion We have presented novel pipeline for the automated creation of MeshFleet, high-quality, filtered, and annotated 3D vehicle dataset derived from Objaverse-XL. Our approach combines quality classifier, trained on relatively small set of manually labeled data, with iterative refinement leveraging textual descriptions and model uncertainty. Comparative analysis against other existing filtering techniques, and fine-tuning experiments with SV3D, demonFigure 6. Pairplot visualizing the relationships between normalized vehicle dimensions in the MeshFleet dataset. The diagonal subplots show the distribution of each dimension (length, width, height, and wheelbase). Off-diagonal subplots show scatter plots (upper triangle) and kernel density estimates (lower triangle) for each pair of dimensions, revealing correlations within the dataset. anticipate that this resource will enable researchers to efficiently filter for domain-specific subsets tailored to their individual needs. For example, by adjusting thresholds on classifier scores and uncertainty estimates, researchers can easily create larger or smaller car-centric datasets with varying quality levels. Many of the processed objects already have associated descriptions from CAP3D [16] or TRELLIS500K [37]. For objects lacking pre-existing captions, we generated descriptive captions using the large version of Florence-2 [38], utilizing the <DETAILED CAPTION> prompt. The provided embeddings, including SigLIP embeddings (useful for rapid clustering and similarity analysis) and the compressed DINOv2 embeddings (used in our classification pipeline), will further expedite downstream tasks. 5. Discussion Our experiments demonstrate the effectiveness of our proposed automated filtering pipeline in creating high-quality, domain-specific 3D datasets. The MeshFleet dataset, generated by this pipeline, significantly outperforms datasets filtered using simpler methods, such as those relying solely on textual descriptions and aesthetic scores, when used for fine-tuning 3D generative model (SV3D). These re8 strate the superior performance of our method for creating domain-specific 3D datasets tailored to generative modeling. Notably, models fine-tuned on MeshFleet exhibit improved generation quality within the target domain compared to those trained on datasets filtered solely through automated methods. With the public release of MeshFleet and the 3D-Car-Quality Dataset, we provide valuable resource to the community and contribute to the advancement of research in 3D generative modeling and large-scale 3D data processing."
        },
        {
            "title": "References",
            "content": "[1] Md Ferdous Alam, Austin Lentsch, Nomi Yu, Sylvia Barmack, Suhin Kim, Daron Acemoglu, John Hart, Simon Johnson, and Faez Ahmed. From Automation to Augmentation: Redefining Engineering Design and Manufacturing in the Age of NextGen-AI. An MIT Exploration of Generative AI, 2024. 1 [2] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University Princeton University Toyota Technological Institute at Chicago, 2015. 2, 3 [3] Xiaoxue Chen, Jv Zheng, Hao Huang, Haoran Xu, Weihao Gu, Kangliang Chen, He xiang, Huan ang Gao, Hao Zhao, Guyue Zhou, and Yaqin Zhang. Rgm: Reconstructing highfidelity 3d car assets with relightable 3d-gs generative model from single image, 2024. 3 [4] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022. 2 [5] Blender Online Community. Blender - 3d modelling and rendering package, 2018. [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. 1, 2, 3 [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 1, 2, 3, 4, 6 [8] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: higharXiv quality dataset of 3d scanned household items. preprint arXiv:2204.11918, 2022. 2 [9] Xiaobiao Du, Haiyang Sun, Shuyun Wang, Zhuojie Wu, Hongwei Sheng, Jiaying Ying, Ming Lu, Tianqing Zhu, 3drealcar: An in-the-wild rgbKun Zhan, and Xin Yu. arXiv preprint car dataset with 360-degree views. arXiv:2406.04875, 2024. 3 [10] Mohamed Elrefaie, Florin Morar, Angela Dai, and Faez Ahmed. Drivaernet++: large-scale multimodal car dataset with computational fluid dynamics simulations and deep In Advances in Neural Information learning benchmarks. Processing Systems, pages 499536. Curran Associates, Inc., 2024. 3 [11] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine Learning, pages 10501059, New York, New York, USA, 2016. PMLR. 2, [12] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. 2, 4 [13] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. CoRR, abs/1910.13461, 2019. 5, 6, 1, 2 [14] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. 1 [15] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features, 2024. 2, 7 [16] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. arXiv preprint arXiv:2306.07279, 2023. 1, 2, 8 [17] Tiange Luo, Justin Johnson, and Honglak Lee. View selection for 3d captioning via diffusion ranking. arXiv preprint arXiv:2404.07984, 2024. [18] Hayata Morita, Kohei Shintani, Chenyang Yuan, and Frank Permenter. VehicleSDF: 3d generative model for constrained engineering design via surrogate modeling. In NeurIPS 2024 Workshop on Data-driven and Differentiable Simulations, Surrogates, and Solvers, 2024. 7 [19] Phillip Mueller, Sebastian Mueller, and Lars Mikelsons. Geobiked: dataset with geometric features and automated labeling techniques to enable deep generative models in engineering design, 2024. 2 [20] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification. 2, 5 [21] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali 9 [35] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, Q-align: Teaching lmms for visual and Weisi Lin. arXiv preprint scoring via discrete text-defined levels. arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. 2 [36] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and In IEEE/CVF Conference on Computer Vision generation. and Pattern Recognition (CVPR), 2023. [37] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 1, 2, 3, 4, 5, 6, 8 [38] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. arXiv preprint arXiv:2311.06242, 2023. 8, 2 [39] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 1 [40] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2, 5 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2, 7 [42] Zicheng Zhang, Wei Sun, Xiongkuo Min, Quan Zhou, Jun He, Qiyuan Wang, and Guangtao Zhai. Mm-pcqa: Multimodal learning for no-reference point cloud quality assessment. IJCAI, 2023. 3, Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Bjorn Ommer, Christian Theobalt, Peter Wonka, and Gordon Wetzstein. State of the Art on Diffusion Models for Visual Computing, 2023. 1 [22] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2022. 4 [23] Burr Settles. Active learning literature survey, 2009. 2, 6 [24] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. 1 [25] Sarah Steininger, H. Camci, and J. Fottner. Current state, potentials and challenges for the use of artificial intelligence in the early phase of product development: survey. pages 551555, 2024. 1 [26] Qinghong Sun, Yangguang Li, ZeXiang Liu, Xiaoshui Huang, Fenggang Liu, Xihui Liu, Wanli Ouyang, and Jing Shao. Unig3d: unified 3d object generation dataset, 2023. 2 [27] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation, 2024. [28] Tencent Hunyuan3D Team. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation, 2024. 1 [29] Tencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. 1 [30] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An allMLP architecture for vision. In Advances in Neural Information Processing Systems, 2021. 5 [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 5 [32] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multi-view synthesis and 3D generation from single image using latent video diffusion. In European Conference on Computer Vision (ECCV), 2024. 1, 2, 4, 6 [33] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 82288238, 2024. [34] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time endto-end object detection. arXiv preprint arXiv:2405.14458, 2024. 3, 2 10 MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling 7. MeshFleet Further Examples"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Comparison of predicted quality class labels and aesthetic scores from TRELLIS500K. The plot displays the distribution of aesthetic scores for objects classified as high-quality (suitable for fine-tuning) and low-quality (unsuitable for fine-tuning) by our classifier. Only objects identified as cars based on their TRELLIS500K captions are included. Figure 7 shows several views of example objects from the MeshFleet dataset. We selected variety of vehicles from different categories to show the diversity of the vehicles. Figure 8 compares the predicted class labels from our trained quality classifier (high-quality vs. low-quality) with the aesthetic scores from TRELLIS500K [37], focusing on objects within the CarCaption3K subset. The figure reveals weak correlation between the predicted quality class and the TRELLIS500K aesthetic scores. Although the mean aesthetic score is slightly higher for objects classified as high-quality, the overall distributions exhibit significant overlap. This indicates that high aesthetic score in TRELLIS500K does not reliably predict suitability for fine-tuning generative model on realistic, high-quality car models. 8. Vehicle Property Extraction For each vehicle in the MeshFleet dataset, we generated descriptive captions using GPT-4o-mini, employing prompt engineered to generate descriptions detailing the vehicle type, its key characteristics, color, and an assessment of the 3D models quality. Beyond the captions, we extracted metadata for each vehicle, including its category and physical dimensions. Vehicle categories were determined by processing the generated captions with BART-based large language model [13]. This model was used to classify captions into the following categories: SUV, Sedan, Hatchback, Pickup, Truck, Minivan, MPV, Coupe, Convertible, sports car, Lorry, race Figure 7. Example renderings from the high-quality vehicle models of the MeshFleet Dataset. 1 car, police car, and bus. Physical dimensions (length, width, height, and wheelbase) were derived from the normalized 3D models. Normalization involved scaling each model to fit within unit cube. Subsequently, 32 views of each normalized vehicle were rendered, and precise 2D bounding boxes were computed for the object in each rendered image, utilizing the transparent background. The side, front, and back views were then identified from these bounding boxes on the basis of their relative dimensions. The wheelbase estimation was performed using Florence-2 [38] with the <OD> prompt to detect tires within the identified side views. All dimensions are reported relative to the normalized vehicle height (a value between 0 and 1), but the original unnormalized height is also provided. 9. Automated Object Filtering Figure 9. Examples of zero shot image classification of high quality cars using SigLIP. To evaluate the MeshFleet dataset, we required method to automatically identify car-related objects within the larger Objaverse-XL collection. We investigated several approaches, including embedding-based clustering, object de2 tection, and zero-shot image and text classification. Object detection using YOLOv10 [34] and zero-shot image classification using SigLIP [40] embeddings produced significant number of false positives. This is likely due to two factors: (1) many Objaverse objects represent entire scenes containing multiple objects, rather than isolated instances, and (2) both YOLOv10 and SigLIP, like most foundational computer vision models, are primarily trained on photorealistic images, not rendered objects. Figure 9 presents examples of objects classified as high-quality cars by SigLIP. This initial classification was performed as two-step process. First, we determined whether the object is car at all. However, many of these initial detections can also contain simplistic or unrealistic car models. Therefore, in second step, we attempted to classify the quality of the detected cars, using the following classification categories: Detailed car model, simplistic car model, partial car model Realistic car model, toy car, fictional car High quality car, Low quality car In contrast to image-based methods, text classification based on TRELLIS500K object captions proved to be more promising. We employed BART-based large language model [13] to identify captions describing cars. Filtering based on this criterion yielded 15,820 objects from TRELLIS500K and 20,352 from the combined TRELLIS500K and CAP3D datasets. Given the greater detail in TRELLIS500K captions compared to CAP3D, we focused on the TRELLIS500K captions for subsequent filtering. To further refine this selection and identify realistic and detailed car models, we applied the same categories used in the two-step image classification (detailed above). Filtering for objects labeled as Realistic car model, Detailed car model, and with score of at least 0.8 for High quality car resulted in subset of 3,326 objects. Although manual review of these objects (see Figure 12 for examples) confirmed that most correctly depicted cars generally fitting the desired categories, non-negligible number of partial, simplistic, or non-realistic car models remained. 10. Additional Dataset Assessments 10.1. ShapeNet Car Evaluation ShapeNet [2] contains core collection of labeled objects, including category of approximately 3400 cars. To improve the generalizability of our quality assessment approach, we initially considered incorporating these ShapeNet car models into the 3D-Car-Quality Dataset. We manually labeled subset of these ShapeNet cars according to the same quality criteria described in Section 3.1. However, during preliminary experiments with the quality classifier, we observed that including ShapeNet vehicles degraded overall classification performance. We hypothethe full ShapeNet car set. We further evaluated the trained quality classifiers ability to generalize to unseen data by applying it to the entire labeled ShapeNet car subset. This evaluation can help to assess potential overfitting to the characteristics of the Objaverse-XL objects used in the primary training set. While the classifier had been exposed to the high-quality ShapeNet examples during training, it had not seen the lower-quality ShapeNet instances. Despite this, the classifier achieved 90% agreement with the manual labels on the complete ShapeNet car subset, using the same configuration as for the Objaverse-XL classification. This result suggests reasonable degree of generalization beyond the primary training data source, although some domain adaptation may still be beneficial. Figure 11. Examples of Shapenet cars classified as high quality vehicles. 10.2. 3DRealCar Evaluation To further assess the generalizability of our trained quality classifier and its applicability to real-world car scans, we evaluated it on subset of the 3DRealCar dataset [9]. We selected 400 vehicles from 3DRealCar and generated rendered images using the same procedure as with ObjaverseXL. As noted previously, Objaverse-XL does contain some real-world scanned car models. However, during the manual labeling process for the 3D-Car-Quality Dataset, we generally classified these scanned models as not meeting the criteria for high-quality vehicles as defined in Section 3.1. This definition prioritizes detailed, well-defined CAD models, whereas scanned models often exhibit noise, artifacts, or incomplete geometry. Consistent with this prior assessment, the classifier assigned quality label of 0 (lowquality) to all 400 tested objects from 3DRealCar. This result highlights the classifiers specialization for identifying Figure 10. Examples from the dataset generated by filtering Objaverse-XL using object captions from TRELLIS500K. size that this performance drop is attributable to the relatively homogeneous and simplistic texture representation of ShapeNet models, which typically exhibit matte appearance. To mitigate this issue, we experimented with including only the high-quality ShapeNet car models (as determined by our manual labeling) in the 3D-Car-Quality Dataset. This resulted in subset of 111 high-quality ShapeNet car instances. Incorporating this restricted subset improved the classifiers performance compared to using Figure 12. Examples of Shapenet cars classified as low quality vehicles. Figure 14. Example view generations of SV3D after finetuning with the Label 3 subset. Generated with single view from the objects of the validation set. Figure 13. Example view generations of SV3D after finetuning with the Label 2 subset. Generated with single view from the objects of the validation set. high-quality synthetic car models and its intended distinction from datasets of real-world car scans. 11. Additional Example Generations after Fine-tuning Figures 13 to 16 show additional examples generated with SV3D after fine-tuning with the respective subsets. All multi-view generations were done with single view from the objects of the validation set. Figure 15. Example view generations of SV3D after finetuning with the Label 4 subset. Generated with single view from the objects of the validation set. 4 Figure 16. Example view generations of SV3D after finetuning with the MeshFleet dataset. Generated with single view from the objects of the validation set."
        }
    ],
    "affiliations": [
        "BMW Group",
        "University of Augsburg"
    ]
}