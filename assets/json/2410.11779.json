{
    "paper_title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
    "authors": [
        "Chenxi Wang",
        "Xiang Chen",
        "Ningyu Zhang",
        "Bozhong Tian",
        "Haoming Xu",
        "Shumin Deng",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 9 7 7 1 1 . 0 1 4 2 : r Preprint. MLLM CAN SEE? DYNAMIC CORRECTION DECODING FOR HALLUCINATION MITIGATION Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen Zhejiang University National University of Singapore, NUS-NCS Joint Lab, Singapore {sunnywcx,xiang chen,zhangningyu}@zju.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by large margin compared to baselines, highlighting its potential to mitigate hallucinations1. The first principle is that you must not fool yourselfand you are the easiest person to fool. Richard Feynman"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, the rapid development of Multimodal Large Language Models (MLLMs) has demonstrated potential pathway towards achieving Artificial General Intelligence (AGI) (Wang et al., 2024; Yao et al., 2024; Lu et al., 2024a; Team, 2024; OpenAI, 2023; Liu et al., 2023b; Chern et al., 2024). However, in practice, the development of MLLMs is hindered by the phenomenon of hallucination, which typically results in the model generating statements about non-existent images while neglecting to mention certain visible objects, effectively causing it to fool itself (Bai et al., 2024; Liu et al., 2024a; Li et al., 2023b; Liu et al., 2023a; Rawte et al., 2023). This issue poses significant risks in high-stakes fields such as medical imaging (Chen et al., 2024a; Hu et al., 2023; Wang et al., 2023b), autonomous driving (Cui et al., 2024; Wang et al., 2023c), and human-computer interaction systems (Brie et al., 2023), where such errors could result in irreparable consequences. The reasons behind hallucinations in MLLMs are complex. Unlike analyses focused on unimodal LLMs (Chuang et al., 2024; Chen et al., 2024c; Orgad et al., 2024; Chen et al., 2024d; Lu et al., 2024b), many current works assume that MLLM may indeed see visual information. However, due to factors such as excessive model depth (Chen et al., 2024b; Zhang et al., 2024a), aggregation patterns (Huang et al., 2024), or priors knowledge inherent in the MLLMs (Leng et al., 2023; Zhang et al., 2024b), these models ultimately still experience hallucinations. Concretely, our understanding of the underlying mechanisms of hallucinations in MLLMs remains limited. It is still uncertain whether the visual information is never correctly recognized or if it is recognized but subsequently suppressed by later information streams. Hallucinated MLLM can see (to some extent). Inspired by the aforementioned works, we conduct an empirical analysis and find that MLLMs are not blind; they can recognize objects in the preceding Equal Contribution. Corresponding Author. 1Code is available in https://github.com/zjunlp/DeCo. 1 Preprint. (a) Object probing results. (b) Different resolution results. Figure 1: Overall results of the probing experiment with MLLMs, indicating that they possess certain level of awareness regarding the presence of visual objects (Figure 1(a)), with prediction accuracy being higher in the preceding layers (Figure 1(b)) but gradually decline afterward. layers, but this recognition is suppressed in later layers, leading to hallucinations. Specifically, we focus on object hallucinations2 and conduct experiments with MLLMs, demonstrating that they know to some extent whether an object exists (as shown in Figure 1 and Section 2.1). We further observe that the confidence of generated tokens is influenced by the knowledge priors of MLLMs (Section 2.2), leading to reduction in the probability of ground truth tokens in the deeper layers. Dynamic correction decoding with preceding-layer knowledge. Based on those findings, we propose Dynamic Correction Decoding with preCeding-Layer Knowledge (DeCo) to mitigate hallucinations for MLLMs. Our core hypothesis is that preceding layers exhibit higher confidence for ground truth tokens, and the logits for these tokens should rank prominently at the last layers outputs. To enhance the logits of ground truth tokens, DeCo dynamically selects preceding layer and utilizes its prior knowledge to correct the final output logits. Additionally, we introduce dynamic soft modulation to preserve the original style of the generated responses. DeCo is training-free and can be integrated with any popular decoding strategies, such as greedy search, nucleus sampling as well as beam search, and can seamlessly incorporate into any MLLMs for hallucination mitigation. Contributions. Our primary contribution lies in exploring the internal mechanisms of hallucinations in MLLMs. We find that the confidence of generated tokens is influenced by the knowledge priors of MLLMs, leading to reduction in the probability of ground truth tokens in the deeper layers. We further propose DeCo, dynamic correction decoding method guided by preceding-layer knowledge. DeCo is integrated with InstructBLIP, MiniGPT-4, LLaVA, and Qwen-VL using three popular decoding strategies: greedy search, nucleus sampling, and beam search. Experimental results show that DeCo achieves an average hallucination suppression rate of 10.8% in image captioning dataset, demonstrating superior suppression effectiveness. Additionally, DeCo outperforms baselines on visual question answering datasets including POPE, and MME. Additionally, we analyze the latency and throughput, showing that DeCo introduces an approximate 1.2x increase in latency compared to the basic decoding process, much faster than previous baselines such as VCD and OPERA."
        },
        {
            "title": "2 WHY DO MLLMS GENERATE NON-EXIST OBJECTS?",
            "content": "In this section, we conduct series of empirical analysis to investigate the internal mechanisms of MLLM and elucidate the underlying reasons for its generation of non-existent objects. To strike balance between the realism and complexity of the experiments, we primarily focus on the generation of objects in image description scenarios (image caption tasks). Preliminaries of MLLM generation. MLLMs typically concatenate visual tokens, processed by the visual encoder and projection layer, with embedded textual tokens before feeding them into an autoregressive language model. We denote the visual tokens as XV = {xv1, xv2 , . . . , xvP } and textual tokens as XC = {xc1, xc2 , . . . , xcQ}. Here and are the lengths of the visual tokens and textual tokens respectively. Finally, the input is = concat{XV , XC}. Then would be passed into MLLM with stacked transformer layer. The intermediate variable generated by the 2This approach is applicable to other types of hallucinations as well. 2 Preprint. Figure 2: Illustration of token probabilities across transformer layers, which reveals distinct trends for target hallucinated (orange) and non-hallucinated (green) tokens. In the preceding layers, nonhallucinated tokens exhibit higher probability. In the final layers, hallucinated tokens demonstrate increased probabilities, while the probability of non-hallucinated tokens drops sharply. i-th layer is called hidden states, denoted as hi = {hi 1}, where = + Q. During the generation phase, we use the hidden state at the last position in the final layer, which is mapped to the vocabulary dimension through an affine layer ϕ(), to predict the probability of the next token. Formally, we have: 1, . . . , hi 0, hi p(xT x<T ) = softmax(ϕ(hN 1))xT , xT (1) where we use x<T to simplify the sequence {xi}T 1 i=0 and refers to the whole vocabulary set. 2.1 FINDING 1: MLLM KNOWS TO SOME EXTENT WHETHER AN OBJECT EXISTS Inspired by (Ye et al., 2024), we explore how MLLMs comprehend objects in the image captioning task. For simplicity, we abstract this process into function called isexist(obj), which determines whether an object is present in an image. To examine the application of this function within the MLLMs image captioning workflow, we conduct probing experiments at the conclusion of object descriptions in each layer of the MLLMs language model component, which consists of 32 transformer layers in 7-billion-parameter model (Detailed setup in Appendix A.1). We employ the prompt template, USER: <image>Describe the image. ASSISTANT: The image contains obj. Both the training and testing datasets are formatted accordingly before being input into MLLMs. We train probe classifier at the final position of the hidden state outputs for each transformer layer, resulting in total of 32 classifiers. (For details on the subset division, OOD and in-distribution splits, and prompt templates, please refer to Appendix A.1.) The model is evaluated using the test set, as shown in Figure 1(a) (left). Further experiments are conducted on three splits of the evaluation dataset proposed by POPE, with results reported in Figure 1(a) (right). These evaluations provide comprehensive understanding of the models object recognition capabilities across diverse scenarios. We select the best-performing probe classifier from the 32 classifiers to compare accuracy across all objects, existing objects, and non-existing objects. Our results show that the MLLM achieves high accuracy for correctly generated objects in image captions. Despite generating many non-existent objects, the MLLM still maintains around 80% accuracy in our probing experiments. This suggests that MLLMs possess certain level of understanding regarding object existence in images. Additionally, our probing experiments reveal higher accuracy in the preceding layers, as illustrated in Figure 1(b), which aligns with previous findings (Zhang et al., 2024b; Leng et al., 2023). Furthermore, we show that increasing the resolution of the visual encoder (from 224px to 336px) enhances accuracy for non-existing objects, indicating that token information at the last position in the preceding layers better represents visual information. (For detailed explanation of the different 3 Preprint. visual resolutions, please refer to Appendix A.1) These findings can deepen our understanding of the object comprehension processes in MLLMs and highlight potential avenues for improvement."
        },
        {
            "title": "2.2 FINDING 2: LANGUAGE MODEL PRIORS SUPPRESS THE VISUAL INFORMATION THAT",
            "content": "MLLM ALREADY SEE. We hypothesize that the representations in the preceding layers effectively capture (to some extent) visual information. However, the prior knowledge embedded in the MLLM reduces the probabilities of ground truth tokens in deeper layers. Figure 2 illustrates this hypothesis with running examples. We analyze the Top-4 tokens ranked by probability in the final layers output. Non-hallucinated tokens like people, left, blue, and umbrella exhibit high probabilities from the 18th layer. In contrast, hallucinated tokens like bird and green only show comparatively high probabilities around the 30-th layer. Interestingly, the probabilities of ground truth tokens umbrella and blue sharply decline from the 30-th layer onwards, eventually falling below the hallucinated tokens probabilities in the final layer. To further investigate this phenomenon, we conduct an early exit experiment (Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022) to analyze the evolution of the MLLMs internal representations across transformer layers. We randomly select 500 images from the MSOCO dataset and use random prompts to elicit raw responses from LLaVA1.5-7b. We then extract all non-existent objects along with their corresponding preceding text and input this data into the MLLM. We observe the probabilities of the next token across the transformer layers to gain insights into the models behavior (see Appendix A.2 for detailed experimental setup). The output of the i-th layer is denoted as hi, and the probability distribution of the next token is represented as p(x<s)i = softmax(ϕ(hi s1)). To reduce the observation tokens and simulate the real sampling process, we truncate the vocabulary, similar to Top-p sampling, and obtain the candidate tokens, denoted as Vcandidate with default threshold of 0.9. We then label the tokens in Vcandidate. Specifically, we filter out data where Vcandidate contains at least one ground truth token and observe whether an activated ground truth token exists among the candidate tokens, formally expressed as: Figure 3: Distribution of activated ground-truth tokens across layers. xa Vcandidate (0, ], p(xax<s)i p(xhx<s)i threshold, where xa is the activated ground truth token, xh is the token with the highest probability of being hallucinated token in the probability distribution of the final layer and threshold (0, 1). Based on the experimental setup described above, we conducted the following investigation: (2) What suppresses the expression of visual facts? We analyze the occurrence of xa at each decoding layer, as shown in Figure 3. The results reveal that the activated ground truth tokens are primarily present between layers 20 and 28, indicating that MLLMs accurately recognize the image content in the latter layers. However, the activated ground truth tokens are suppressed in the final output layer. This suppression may stem from the guidance of the input image or the inherent knowledge bias of the MLLM. To investigate this, we generate candidate tokens candidate in the absence of an input image, representing tokens based on the MLLMs inherent knowledge. We calculate that the overlap rate of xh existing in candidate reaches 91.05%, suggesting that even without expressing image information, MLLMs still tend to generate the original hallucination tokens. This finding reveals that the inherent knowledge in MLLMs may diminish the probability of the ground truth token in the deeper layers."
        },
        {
            "title": "3 PROPOSED APPROACH: DYNAMIC CORRECTION DECODING WITH",
            "content": "PRECEDING-LAYER KNOWLEDGE After investigating the reasons why MLLMs generate non-existent objects, inspired by (Chuang et al., 2024), we introduce Dynamic Correction Decoding with preCeding-Layer Knowledge (DeCo), which can alleviate hallucinations during inference. The overall framework of Deco is 4 Preprint. Figure 4: Framework of DeCo. DeCo first dynamically selects an appropriate anchor layer from the preceding layers and then correct the knowledge in the final layer with dynamic coefficient. illustrated in Figure 4, consisting of dynamic preceding layer selection (Section 3.1) and decoding correction with preceding-layer knowledge (Section 3.2). 3.1 DYNAMIC PRECEDING-LAYER SELECTION Candidate token acquisition. Due to the vast vocabulary space, we track only the changes in the top-ranked tokens as candidate tokens across different layers for computational convenience. This is based on the hypothesis that ground tokens usually appear in the top position of the MLLMs last layer output logits. Inspired by (Li et al., 2023a), we use truncation strategy to select the candidate tokens, with the default truncation strategy being top-p truncation, formally: Vcandidate (xT x<T ) = xT : (cid:88) vVp Pτ (xT = vx0, x1, . . . , xT 1) (3) where is the whole vocabulary, and refers to the parameter used in top-p. Table 1: Hit Rate of layers across different intervals. Preceding-layer selection. Our findings in Section 2 demonstrate that activated ground truth tokens typically exhibit higher probabilities in preceding layers compared to hallucinated tokens. Based on this observation, we hypothesize that selecting the token xth, where xth Vcandidate, with the highest probability from the interval layers corresponds to the ground truth token. We compute the accuracy of xth as the ground truth token and denote this metric as the hit rate, as shown in Table 1. The results indicate that within specific range of layers (e.g., 15-28), xth indeed has high universal probability of representing the ground truth token. Intuitively, we track Layer Range Hit Rate (%) 2015-28 71.14 61.69 5 Preprint. candidate tokens and dynamically choose the layer in which the token with the highest probability among the preceding layers resides to calibrate the final logit distribution of the MLLM. The selected preceding layer is referred to as the anchor layer, formally defined as: = argmaxi (cid:8)xT Vcandidate : softmax(ϕ(hi 1))xT , [a, b](cid:9), (4) where b, a, [1, ], and [a, b] represents the layer interval for MLLMs. Expanding the range of layers can improve the hit rate. To avoid increased search computation time, we assign default values of = 20 and = 28 for our subsequent experiments."
        },
        {
            "title": "3.2 DECODING CORRECTION WITH PRECEDING-LAYER KNOWLEDGE",
            "content": "Dynamic soft modulation. We introduce dynamic modulation coefficient, defaulting to the maximum probability. Formally, we have: max prob = max(softmax(ϕ(hA 1))). (5) This coefficient can help prevent hard changes in logits, particularly when the probability differences between candidate tokens in preceding layers are insignificant. From the example in Figure 4, we can observe that the absence of the dynamic modulation coefficient may lead to semantic incoherence or even more severe hallucinations. Preceding-layer knowledge guided decoding. Given the selected preceding layers, we integrate information from these layers into the final layer to correct the logit distribution. We utilze hyperparameter, α, to control the proportion of early-layer information incorporated. Additionally, dynamic soft modulation is employed to preserve the generative style of the original model. By utilizing the correction of preceding-layer representations, the probability of predicting the next token and the logits are updated as follows: ˆp(xT x<T ) = softmax(cid:0)logits(cid:1) logits = ϕ(hN , xT 1) + α max prob ϕ(hA 1), (6) (7) where is the last layer of MLLM and is the selected preceding layer. Comparison of previous methods. Our work shares similar assumption with OPERA (Huang et al., 2024) and VCD (Leng et al., 2023), positing that the knowledge priors inherent in MLLMs may suppress the models ability to comprehend visual information. However, our approach is comparatively simpler than that of OPERA (Huang et al., 2024) and VCD (Leng et al., 2023). Additionally, our work differs from the assumption in unimodal LLMs, where the semantic information present in the shallow layers interferes with factual recall in the final layer (Chuang et al., 2024; Chen et al., 2024c). However, our method is actually parallel to previous approaches and can be combined to achieve better results."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 SETUP Baselines. We integrate DeCo with various decoding methods, including greedy decoding, nucleus sampling, and beam search, and compare it against several baselines for mitigating hallucinations, as outlined below: Dola (Chuang et al., 2024) is specifically designed for alleviating hallucinations in factual tasks for LLMs by reducing shallow semantic influences to improve the factuality of the final layers output. VCD (Leng et al., 2023) mitigates the influence of language models priors in MLLMs by generating representations that enhance visual information through the subtraction of interfering knowledge prior during each sampling step. OPERA (Huang et al., 2024) dynamically penalizes overconfident tokens based on the emergence of aggregation patterns, while proposing retrospective allocation strategy to avoid cases where hallucinations have already occurred. For all the baselines, we use the default hyperparameters from the source code for fair comparsion. 6 Preprint. Table 2: CHAIR hallucination evaluation results. Lower scores indicate fewer hallucinations. OPERA utilizes beam search, VCD applies nucleus sampling, and DeCo is the proposed method compatible with various decoding approaches. Decoding Method Greedy Beam Search Nucleus Vanilla DoLa DeCo (Ours) Vanilla OPERA DeCo (Ours) Vanilla VCD DeCo (Ours) InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI 58.8 48.4 41.2 17. 46.0 46.8 42.2 3.8 45.0 47.8 37.8 7.2 31.8 32.2 27.0 4.8 14.7 13.8 11.1 3.6 12.5 12.9 10.7 1.8 23.7 15.9 14.4 9. 9.9 10.0 8.8 1.1 55.6 46.4 43.8 11.8 54.6 58.0 43.6 11.0 15.8 14.2 12.7 3.1 24.8 17.0 12.9 11.9 30.6 26.2 24.8 5. 32.6 33.8 30.8 1.8 9.5 9.5 7.5 2.0 10.7 11.1 9.5 1.2 48.8 44.6 33.0 15.8 48.8 54.0 42.8 6.0 13.9 12.8 9.7 4. 14.2 16.0 13.2 1.0 41.8 34.6 32.0 9.8 49.2 46.4 43.8 5.4 10.8 9.5 8.7 2.1 13.1 11.9 11.8 1.3 Table 3: POPE hallucination evaluation results. The best results are in bold. Decoding Method InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL F1 F1 F1 F1 Greedy Beam Search Nucleus Vanilla DoLa DeCo (Ours) Vanilla OPERA DeCo (Ours) Vanilla VCD DeCo (Ours) 80.0 83.4 84.9 4.9 84.4 84.8 84.9 0.5 79.8 79.9 81.8 2.0 58.5 72.8 77.4 18.9 70.3 73.3 77.9 7.6 52.8 56.0 63.8 11. 82.2 83.2 86.7 4.5 84.9 85.4 86.7 1.8 83.1 83.1 85.4 2.3 85.2 85.8 86.3 1.1 85.3 86.1 86.4 1.1 84.5 84.7 85.2 0. Model. We select four of the most representative MLLM models for evaluation, including InstructBLIP (Dai et al., 2023), MiniGPT-4 (Zhu et al., 2024), LLaVA-1.5 (Liu et al., 2023b) and Qwen-VL (Bai et al., 2023). All the MLLMs we used have language model size of 7 billion parameters (7B). Implementation Details. To select the appropriate preceding layers for hallucination mitigation, we conduct ablation experiments, details of which can be found in the Section 4.4. For 7B-sized, 32-layer decoder-only architecture language model, we choose layers 20-28 as candidates for the preceding layers (according to the findings in Section 2.1). For the image captioning and VQA tasks, α is set within the range of 0.1 to 0.6. In all experiments, we conduct inference on single A800 GPU. The inference of 500 image-caption pairs take approximately 40 minutes. 4.2 BENCHMARK AND METRICS CHAIR. Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2018) metric, widely used in image captioning, identifies hallucinated objects by comparing the extracted objects with ground truth labels and evaluates both at the instance level (CHAIRI) and sentence level (CHAIRS), as shown in Eq. 8. Following (Huang et al., 2024), we conduct experiments using the same settings, including the consistent 500 images from the MSCOCO 2014 validation dataset and the identical prompt, Please help me describe the image in detail.. CHAIRI = {hallucinated objects} all mentioned objects , CHAIRS = {captions with hallucinated objects} all captions . (8) POPE. The Polling-based Object Probing Evaluation (POPE) (Li et al., 2023b) is VQA-based metric for assessing object hallucination in MLLMs. It evaluates hallucinations by asking questions such as Is there <object> in the image? where <object> is derived from three types of splits: random (randomly selected objects), popular (frequently occurring objects), and adversarial (objects closely related to those in the image). The evaluation includes 500 MSCOCO images, with six questions per image for each split. We use F1 score for performance evaluation. MME. The comprehensive MLLM Evaluation benchmark (MME) (Fu et al., 2023) assesses the perceptual and cognitive abilities of MLLMs across total of 14 subtasks, including tasks such as OCR, visual knowledge, attribute relationships, and object recognition. 7 Preprint. GPT-4o assisted evaluation. To further assess the models performance in image captioning, we extend beyond the CHAIR metric, which targets object hallucination. Following prior studies (Huang et al., 2024; Leng et al., 2023), an open evaluation is conducted using GPT-4o on 100 randomly sampled COCO images. GPT-4o assesses two assistants descriptions in terms of Accuracy (C) and Detailedness (D). We introduce the prompt used in the experiments in Table 8."
        },
        {
            "title": "4.3 EXPERIMENTAL RESULTS",
            "content": "Results of hallucination in image captioning. Note that we use the baselines original decoding settings for fair comparison and run DeCo under the same settings. From Table 2, we notice that DeCo consistently outperforms other approaches in mitigating hallucinations across four MLLMsInstructBLIP, MiniGPT-4, LLaVA-1.5, and Qwen-VLusing three decoding strategies: greedy search, beam search, and nucleus sampling. We find that DeCo slightly outperforms OPERA, while our method demonstrates higher efficiency and simplicity in inference (see Section 4.4). Additionally, VCD does not perform as well, likely due to producing an increased number of hallucinated descriptions during the generation process. In conclusion, the proposed approach DeCo effectively reduces hallucinations in visual description tasks solely through dynamic decoding correction, achieving an average suppression rate of approximately 10.8% on image captioning datasets. Additionally, we further evaluate the performance of DeCo on the AMBER image caption dataset, as detailed in Table 7 of the Appendix. Results of hallucination in VQA. In contrast to image captioning, POPE employs simple polling approach to assess hallucination levels in MLLMs with respect to object recognition. As shown in Table 3, DeCo demonstrates superior performance across all settings, further validating the effectiveness of the proposed approach. Additionally, Figure 5 reveals that DeCo also achieves better results on MME, which evaluates the multifaceted VQA capabilities of LLaVA-1.5. These findings suggest that the underlying mechanism we identified not only applies to object recognition but also extends to attribute-related tasks and more complex reasoning tasks. Figure 5: DeCo generally improves the MLLMs performance. Results of GPT-4os assistance. Following (Huang et al., 2024; Leng et al., 2023), we further use GPT-4o to evaluate our method against greedy decoding across four distinct models. From Table 4, we notice that our approach consistently outperform greedy decoding in terms of accuracy, demonstrating its efficacy in hallucination suppression. The impact of decoding intervention is evident in the level of detail produced: for some models, our method yield only marginally higher or, in certain cases, slightly lower levels of detail compared to greedy decoding. Nonetheless, our method exhibit clear advantage in mitigating hallucinations across all evaluated models. Table 4: GPT-4o assisted hallucination evaluation results on MSCOCO. Two aspects are verified, correctness (C) and detailedness (D). Method InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL D D Greedy Search 4.92 5.65 6.25 5.77 DeCo (Ours) 5.71 6.20 5.21 6.31 5.56 6.62 6.33 6.08 7.42 6.25 7.81 6.70 4.4 ANALYSIS Latency and throughput analysis. To evaluate the efficiency of DeCo, we compare its latency and throughput with several baselines, including DoLa, OPERA, and VCD based on Greedy, Beam Search, and Nucleus Sampling, respectively. Figure 6 illustrates the results of this comparison. The findings indicate that DeCo operates within an acceptable efficiency cost, striking bal8 Figure 6: Comparison of latency and throughput across different baselines. Preprint. ance between effectiveness and computational overhead. Compared to the basic decoding process, the latency increase introduced by our method is approximately 1.2 times. In contrast, the latency increases for VCD and OPERA are 1.8 and 5.1 times, respectively. While both VCD and OPERA demonstrate comparable efficacy in mitigating hallucinations, their computational overheads remain relatively high. This highlights the practical value of DeCo, as it can be integrated into real-world applications without significantly compromising efficiency. Perturbation in the selected preceding-layer. To evaluate the effectiveness of the dynamic layer selection method, we introduce random perturbation strategy. Specifically, for the predetermined preceding layers, we add random values ranging from -5 to 5 to modify the selection of layers. We randomly select 200 images from the MSCOCO dataset and prompt MLLMs to generate descriptions. The results after incorporating the perturbations are presented in Table 5. Notably, the perturbed results demonstrate significant degradation in performance, further validating the effectiveness of our proposed method. Table 5: Comparison of results between DeCo and perturbed DeCo in image captioning tasks Method InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI DeCo DeCo + ϵ 39.3 45.6 6.3 12.6 14.3 1.7 32.4 33.3 0.9 9.6 10.1 0. 38.8 42.2 2.4 11.1 11.3 0.2 44.5 47.0 2.5 11.1 12.8 1.7 Hyperparameter analysis. Our method incorporates two primary hyperparameters: α and the selection of interval layers. In the experiments, we employ DeCo based on greedy decoding. On the one hand, the hyperparameter α regulates the intensity of early information enhancement. Figure 7(a) illustrates the performance across various α values. We observe that hallucination suppression is most effective when α approximates 0.6. As α increases, the efficacy of DeCo in mitigating hallucinations improves. However, it is crucial to note that excessively high α values may lead to the generation of atypical image descriptions, characterized by repetitive word usage. On the other hand, the layer interval hyperparameter [a, b] determines the candidate layers for inclusion in the enhancement process. We conduct experiments using intervals of four layers, with results presented in Figure 7(b). Our analysis reveals that hallucination suppression for MLLM is negligible in layers 1-16, while layers 20-28 demonstrate substantial mitigation of hallucinations. Notably, layers 29-32 exhibit minimal hallucination suppression, aligning with our findings discussed in Section 2.2. (a) Ablation study of α. (b) Ablation study of interval layers. Figure 7: Ablation experiment results for hyperparameter α and different interval layers. Mitigating snowballing hallucinations. Snowballing hallucinations are prevalent issue in the responses generated by MLLMs. This phenomenon occurs when an initial hallucination triggers sequence of subsequent errors, leading to compounding effect that significantly degrades the quality and coherence of the generated text. Figure 8 illustrates typical example of snowballing hallucinations, where an initial misinterpretation of the visual input propagates through the decoding process, resulting in highly inconsistent and erroneous output. Our approach can reduce the accumulation of errors and improves the overall consistency and accuracy of the generated responses. The effectiveness of DeCo is further demonstrated through additional cases based on diverse MLLMs, which can be found in Figures 9, 10, 11, and 12 in Appendix C. 9 Preprint."
        },
        {
            "title": "5.1 MLLM HALLUCINATION MECHANISM",
            "content": "Hallucination in MLLMs, characterized by contradictions between image input and textual output, has been prevalent issue (Liu et al., 2024a; Chen et al., 2024f). Current research on the mechanism of hallucination in MLLMs focuses on two key aspects: the interaction between images and text at different layers, and the prior bias of the LLM during decoding. Several studies have investigated the role of image-text interaction at different layers in MLLMs. Grad-CAM (Zhang et al., 2024a) visualizations reveal that image-text interaction exists in the preceding layers (1-11) but not in the deep layers. OPERA (Huang et al., 2024) further proposes that the Aggregation Pattern leads to hallucination, where visual information from preceding layers is gradually aggregated to anchor tokens, and focusing solely on these tokens during prediction while ignoring visual information leads to high probability of hallucination in the generated sequence. However, other studies have revealed that MLLMs exhibit biases towards LLM priors, even in the presence of noisy or absent visual information. VCD (Leng et al., 2023) discovers that MLLMs generate high-confidence answers even when the image is noisy or absent, indicating bias towards LLM priors. Similarly, PAI (Liu et al., 2024b) describes this phenomenon as Text Inertia and posits that it stems from existing paradigms that map visual representations onto the text representations as tokens. This leads to an inference process that fails to adequately account for image tokens, resulting in hallucinations. 5.2 HALLUCINATION MITIGATION FOR MLLMS One straightforward approach to mitigate hallucination is to reduce the knowledge gaps and data bias between vision and language during model training. Finetuning-based methods have been explored, focusing on crafting specific datasets (You et al., 2024; Gunjal et al., 2024; Chen et al., 2024e) and alignment training (Sun et al., 2023; Yu et al., 2023; Chen et al., 2023; Li et al., 2023c) to achieve better knowledge alignment between images and text. While these methods have shown promising results, they often require expensive annotated paired data and substantial computational resources. Hallucination can also be mitigated by post-processing methods, which usually involve using additional tools or self-reflection strategies to revise the response. For instance, LURE (Zhou et al., 2024) detects hallucinations using manually-crafted features and revises the generated text accordingly. Woodpecker (Yin et al., 2023) combines MLLM outputs with an expert VQA model to post-edit hallucinations. VOLCANO (Lee et al., 2023) trains MLLMs to provide self-feedback and reflect on the original generated text. However, these methods incur additional inference costs and delays, and require task-specific procedures and prompts to be designed (Xu et al., 2024). Training-free decoding methods have been explored to mitigate hallucination. OPERA (Huang et al., 2024) identifies an abnormal attention pattern that often accompanies hallucinated descriptions and proposes the mitigation method based on this pattern. VCD (Leng et al., 2023) introduces the notion that visual uncertainty increases hallucination and proposes contrast decoding method to alleviate the issue. VDD (Zhang et al., 2024b) proposes Post-Hoc debias approach that ensures uniform scores for each answer in the absence of an image to mitigate the influence of LLM priors."
        },
        {
            "title": "6 CONCLUSION AND LIMITATIONS",
            "content": "In this paper, we demonstrate that MLLMs exhibit an awareness of hallucinated objects, with earlier layers showing higher confidence, while tokens shaped by prior knowledge diminish the likelihood of true tokens in the final layers. Based on this insight, we introduce DeCo, dynamic correction decoding with preceding-layer knowledge to mitigate hallucinations. Extensive experiments demonstrate the efficacy of our approach, which also shows advantages in latency and throughput. Limitations. (1) Lack of generalized research. Due to the GPU cost consideration, we conduct experiments solely on limited MLLMs, without exploring additional MLLMs or those with larger parameter sizes. (2) No free lunch. The results shown in Table 4 indicate that our method has little negative impact on the level of detailedness metric. In future work, we aim to integrate DeCo with other strategies and explore approaches that can effectively balance truthfulness and diversity. 10 Preprint."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have submitted the relevant code in the supplementary materials. The names of the experimental benchmarks, the prompt templates used, and the models hyperparameter settings can all be found in Section 4. The Appendix A.1 and A.2 provides detailed description of the experimental setup for the mechanism experiments."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. CoRR, abs/2404.18930, 2024. doi: 10.48550/ARXIV.2404.18930. URL https://doi.org/10.48550/arXiv. 2404.18930. Paul Brie, Nicolas Burny, Arthur Sluyters, and Jean Vanderdonckt. Evaluating large language model on searching for GUI layouts. Proc. ACM Hum. Comput. Interact., 7(EICS):137, 2023. doi: 10.1145/3593230. URL https://doi.org/10.1145/3593230. Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou Wang. Huatuogptvision, towards injecting medical visual knowledge into multimodal llms at scale. CoRR, abs/2406.19280, 2024a. doi: 10.48550/ARXIV.2406.19280. URL https://doi.org/10. 48550/arXiv.2406.19280. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. CoRR, abs/2403.06764, 2024b. doi: 10.48550/ARXIV.2403.06764. URL https://doi.org/10.48550/arXiv.2403.06764. Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian He. In-context sharpness as alerts: An inner representation perspective for hallucination mitigation. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024c. URL https://openreview.net/forum?id= s3e8poX3kb. Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, and Nenghai Yu. Llama slayer 8b: Shallow layers hold the key to knowledge injection, 2024d. URL https: //arxiv.org/abs/2410.02330. Xiang Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei Huang, Chengfei Lyu, Dan Zhang, and Huajun Chen. Factchd: Benchmarking fact-conflicting In Kate Larson (ed.), Proceedings of the Thirty-Third International hallucination detection. Joint Conference on Artificial Intelligence, IJCAI-24, pp. 62166224. International Joint Conferences on Artificial Intelligence Organization, 8 2024e. doi: 10.24963/ijcai.2024/687. URL https://doi.org/10.24963/ijcai.2024/687. Main Track. Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. Unified hallucination detection for multimodal large language In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd models. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 32353252. Association for Computational Linguistics, 2024f. doi: 10.18653/V1/2024.ACL-LONG.178. URL https://doi.org/10. 18653/v1/2024.acl-long.178. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CoRR, 11 Preprint. abs/2312.14238, 2023. doi: 10.48550/ARXIV.2312.14238. URL https://doi.org/10. 48550/arXiv.2312.14238. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. ANOLE: an open, autoregressive, native large multimodal models for interleaved image-text generation. CoRR, abs/2407.06135, 2024. doi: 10. 48550/ARXIV.2407.06135. URL https://doi.org/10.48550/arXiv.2407.06135. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Th6NyL07na. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, and Chao Zheng. survey on mulIn IEEE/CVF Winter Conference on timodal large language models for autonomous driving. Applications of Computer Vision Workshops, WACVW 2024 - Workshops, Waikoloa, HI, USA, January 1-6, 2024, pp. 958979. IEEE, 2024. doi: 10.1109/WACVW60836.2024.00106. URL https://doi.org/10.1109/WACVW60836.2024.00106. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards generalWang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. purpose vision-language models with instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= SJg7KhVKPH. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 1813518143. AAAI Press, 2024. doi: 10.1609/AAAI.V38I16.29771. URL https://doi.org/10.1609/aaai.v38i16.29771. Mingzhe Hu, Shaoyan Pan, Yuheng Li, and Xiaofeng Yang. Advancing medical imaging with language models: journey from n-grams to chatgpt. CoRR, abs/2304.04920, 2023. doi: 10.48550/ ARXIV.2304.04920. URL https://doi.org/10.48550/arXiv.2304.04920. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. OPERA: alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. CVPR, abs/2311.17911, 2024. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= 6FXtu8clyp. Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. CoRR, abs/2311.07362, 2023. doi: 10.48550/ ARXIV.2311.07362. URL https://doi.org/10.48550/arXiv.2311.07362. 12 Preprint. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. CoRR, abs/2311.16922, 2023. doi: 10.48550/ARXIV.2311.16922. URL https://doi.org/10.48550/arXiv.2311.16922. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as opIn Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedtimization. ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1228612312. Association for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.ACL-LONG.687. URL https://doi.org/10.18653/v1/2023.acl-long.687. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 292305. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.EMNLP-MAIN.20. URL https://doi.org/10.18653/v1/2023.emnlp-main.20. Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multimodal models. CoRR, abs/2311.06607, 2023c. doi: 10.48550/ARXIV.2311.06607. URL https: //doi.org/10.48550/arXiv.2311.06607. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. CoRR, abs/2306.14565, 2023a. doi: 10.48550/ ARXIV.2306.14565. URL https://doi.org/10.48550/arXiv.2306.14565. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. CoRR, abs/2402.00253, 2024a. doi: 10.48550/ARXIV.2402.00253. URL https://doi.org/10. 48550/arXiv.2402.00253. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023b. Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms, 2024b. URL https://arxiv.org/abs/2407.21771. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding. CoRR, abs/2403.05525, 2024a. doi: 10.48550/ARXIV.2403.05525. URL https://doi.org/10.48550/arXiv.2403. 05525. Taiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, and Daniel Khashabi. Insights into llm longcontext failures: When transformers know but dont tell, 2024b. URL https://arxiv.org/ abs/2406.14673. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. Llms know more than they show: On the intrinsic representation of llm hallucinations, 2024. URL https://arxiv.org/abs/2410.02707. Vipula Rawte, Amit P. Sheth, and Amitava Das. survey of hallucination in large foundation models. CoRR, abs/2309.05922, 2023. doi: 10.48550/ARXIV.2309.05922. URL https:// doi.org/10.48550/arXiv.2309.05922. 13 Preprint. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Jan 2018. doi: 10.18653/v1/d18-1437. URL http://dx. doi.org/10.18653/v1/d18-1437. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 6fac9e316a4ae75ea244ddcef1982c71-Abstract-Conference.html. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. CoRR, abs/2309.14525, 2023. doi: 10. 48550/ARXIV.2309.14525. URL https://doi.org/10.48550/arXiv.2309.14525. Chameleon Team. CoRR, abs/2405.09818, 2024. doi: 10.48550/ARXIV.2405.09818. URL https://doi.org/10. 48550/arXiv.2405.09818. Chameleon: Mixed-modal early-fusion foundation models. Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early In 23rd International Conference on Pattern Recognition, exiting from deep neural networks. ICPR 2016, Cancun, Mexico, December 4-8, 2016, pp. 24642469. IEEE, 2016. doi: 10.1109/ ICPR.2016.7900006. URL https://doi.org/10.1109/ICPR.2016.7900006. Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. CoRR, abs/2311.07397, 2023a. doi: 10.48550/ARXIV.2311.07397. URL https://doi.org/ 10.48550/arXiv.2311.07397. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409. 12191. Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large language models. CoRR, abs/2302.07257, 2023b. doi: 10.48550/ARXIV.2302.07257. URL https://doi.org/10.48550/arXiv. 2302.07257. Wenhai Wang, Jiangwei Xie, Chuanyang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, Hao Tian, Lewei Lu, Xizhou Zhu, Xiaogang Wang, Yu Qiao, and Jifeng Dai. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. CoRR, abs/2312.09245, 2023c. doi: 10.48550/ARXIV.2312. 09245. URL https://doi.org/10.48550/arXiv.2312.09245. Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. CoRR, abs/2401.11817, 2024. doi: 10.48550/ARXIV.2401.11817. URL https://doi.org/10.48550/arXiv.2401.11817. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. CoRR, abs/2407.20311, 2024. doi: 10. 48550/ARXIV.2407.20311. URL https://doi.org/10.48550/arXiv.2407.20311. Preprint. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. CoRR, abs/2310.16045, 2023. doi: 10.48550/ARXIV.2310.16045. URL https:// doi.org/10.48550/arXiv.2310.16045. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=2msbbX3ydD. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. RLHF-V: towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. CoRR, abs/2312.00849, 2023. doi: 10.48550/ARXIV.2312.00849. URL https://doi.org/10.48550/arXiv.2312. 00849. Xiaofeng Zhang, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, and Jieping Ye. From redundancy to relevance: Enhancing explainability in multimodal large language models. CoRR, abs/2406.06579, 2024a. doi: 10.48550/ARXIV.2406.06579. URL https://doi.org/10.48550/arXiv.2406.06579. Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Debiasing multimodal large language models. CoRR, abs/2403.05262, 2024b. doi: 10. 48550/ARXIV.2403.05262. URL https://doi.org/10.48550/arXiv.2403.05262. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=oZDJKTlOUe. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing In The Twelfth Internavision-language understanding with advanced large language models. tional Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=1tZbq88f27. 15 Preprint."
        },
        {
            "title": "A DETAILED EXPERIMENTAL SETUP",
            "content": "A.1 DETAILED SETTINGS FOR FINDINGS 1 In the probing experiment, we utilize the pipeline proposed in the POPE (Li et al., 2023b) to construct 1,200 balanced positive and negative sample pairs from the MSCOCO dataset as training data for the probe classifier, where each sample consists of an object accompanied by label indicating its existence or non-existence. (Note: There is no overlap between the training data and the evaluation data for object hallucination proposed by the POPE). We select the AMBER dataset (Wang et al., 2023a), which has different distribution from the MSCOCO dataset, to test whether our conclusions can generalize. The AMBER dataset contains 1,004 carefully annotated images, each labeled with existent objects as well as non-existent objects. We use the prompt Describe the image. to generate raw responses from LLaVA-1.5 on the images and then extract all object category tokens and label them with whether they exist. Given that the training set contains only 80 object categories, we denote the object tokens in test data belonging to these 80 categories as in-distribution (in-dist), while the remaining tokens are categorized as out-of-distribution (OOD). Previous work (Karamcheti et al., 2024) has demonstrated that increasing the resolution of the vision encoder enhances the visual comprehension capabilities of MLLMs. In our study, we compare LLaVA trained with resolution of 224px against the original LLaVA with resolution of 336px in probing experiments. Notably, the language models weights differ between the two MLLMs, although both initial models are based on Vicuna-1.5-7b. Our results, as illustrated in the Figure 1(b), further affirm the scaling law associated with visual resolution, while also providing indirect validation of the reliability of the probing experiments. A.2 DETAILED SETTINGS FOR FINDINGS 2 In the early exit experiment, we randomly select 500 images from MSOCO and use random prompts (shown in Table 6) to elicit raw responses from LLaVA-1.5-7b. We then extract all non-existent objects along with their corresponding preceding text. Specifically, for the sentence Additionally, there is car., we extract the hallucinated object token car and the preceding text Additionally, there is a. We re-input the preceding text into the MLLM and observe the changes in its internal state when predicting the next token. We denote that total of preceding texts are selected, with the j-th preceding text denoted as sj. Table 6: Randomly prompts. Prompts Describe the image. Please describe this image in detail. Generate caption for this image. 16 Preprint."
        },
        {
            "title": "B EVALUATION RESULTS IN AMBER",
            "content": "The AMBER image caption dataset consists of 1,004 images, each accompanied by meticulously annotated labels. These annotations include all objects present in the images, as well as some potential hallucinated objects. AMBER employs four evaluation metrics: CHAIR (the proportion of generated hallucinated objects among all objects), Cover (the coverage of generated objects against all ground truth objects), Hal (the proportion of hallucinations among all generated captions), and Cog (the overlap ratio with potential hallucinated objects). Lower values of CHAIR, Hal, and Cog indicate higher truthfulness for the MLLMs, while higher Cover value signifies better diversity. We compare Deco with the baselines on the LLaVA-1.5-7b. The results are as shown in Table 7. The results reveal that Deco demonstrates significant advantage in truthfulness, although its diversity is somewhat lacking, yet remains within an acceptable range. Table 7: Results of using DeCo on the AMBER image caption dataset with LLaVA-1.5-7b. Decoding Method LLaVA-1.5 CHAIR Cover Hal Cog Greedy Beam Search Nucleus Vanilla DoLa DeCo (Ours) Vanilla OPERA DeCo (Ours) Vanilla VCD DeCo (Ours) 8.2 8.0 6.6 1.6 7.1 6.4 6.3 0.8 10.2 9.0 8.3 1.9 48.9 50.8 47.5 1.4 50.7 49.0 46.8 3. 50.2 51.7 48.0 2.2 34.3 37.5 28.1 6.2 32.4 27.5 25.1 7.3 43.3 40.2 37.5 5.8 4.0 4.3 2.8 1.2 3.8 2.9 2.4 1. 4.5 4.4 3.4 1.1 17 Preprint."
        },
        {
            "title": "C CASE ANALYSIS ACROSS DIVERSE MLLMS",
            "content": "Figure 8: The case of mitigating snowballing hallucination with DeCo. 18 Preprint. Figure 9: DeCos performance in reducing hallucinations of InstructBlip-7B on three basic decoing methods. 19 Preprint. Figure 10: DeCos performance in reducing hallucinations of LLaVA-1.5-7B on three basic decoing methods. 20 Preprint. Figure 11: DeCos performance in reducing hallucinations of Qwen-VL-7B on three basic decoing methods. 21 Preprint. Figure 12: DeCos performance in reducing hallucinations of MiniGPT4-7B on three basic decoing methods. 22 Preprint. Table 8: The prompt used for GPT-4o evaluation adopted from Leng et al. (2023); Huang et al. (2024); Liu et al. (2024b) GPT-4o Prompt You are required to score the performance of two AI assistants in describing given image. You should pay extra attention to the hallucination, which refers to the part of descriptions that are inconsistent with the image content, such as claiming the existence of something not present in the image or describing incorrectly in terms of the counts, positions, or colors of objects in the image. Please rate the responses of the assistants on scale of 1 to 10, where higher score indicates better performance, according to the following criteria: 1: Accuracy: whether the response is accurate with respect to the image content. Responses with fewer hallucinations should be given higher scores. 2: Detailedness: whether the response is rich in necessary details. Note that hallucinated descriptions should not count as necessary details. Please output the scores for each criterion, containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by space. Following the scores, please provide an explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. [Assistant 1] {Response of Assistant 1} [End of Assistant 1] [Assistant 2] {Response of Assistant 2} [End of Assistant 2] Output format: Accuracy: <Scores of the two answers> Reason: Detailedness: <Scores of the two answers> Reason:"
        }
    ],
    "affiliations": [
        "National University of Singapore, NUS-NCS Joint Lab, Singapore",
        "Zhejiang University"
    ]
}