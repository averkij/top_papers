{
    "paper_title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation",
    "authors": [
        "Dongjie Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 7 9 7 7 0 . 2 1 4 2 : r Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation Dongjie Fu Mogo AI mirecofu@163.com Figure 1. Mogo is novel GPT-type model for high-quality text-to-motion generation, capable of producing longer sequences and handling open-vocabulary prompts. Given inputs e.g. person is dancing ballet gracefully, person walks like zombie. Mogo generates vivid, lifelike 3D human motions at once."
        },
        {
            "title": "Abstract",
            "content": "In the field of text-to-motion generation, Bert-type Masked Models (e.g. MoMask, MMM) currently produce higherquality outputs compared to GPT-type autoregressive models (e.g. T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging GPT-type structurewithout adding extra refinement models that complicate scaling data, we propose novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves Frechet Inception Distance (FID) score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation. 1. Introduction In recent years, the study of generating 3D human motion from textual descriptions has gained increasing attention in fields such as video games, VR/AR, animations, and humanoid robotics, leading to extensive research efforts [1, 16, 29, 39, 4345]. Among these studies, models using transformer [36] algorithms have demonstrated significant advantages in both generation quality and real-time performance [16, 29, 43, 46]. Typically, transformer-based motion generation models consist of two components: 1) 1 vector quantized variational autoencoder (VQ-VAE) to discretize continuous 3D human motion into quantized tokens, and 2) transformer-based model trained to generate these discrete motion tokens through conditional reasoning. Despite notable improvements over previous approaches, these methods still exhibit inherent limitations: VQ quantization inevitably introduces some errors due to encoding granularity, impacting the overall generation quality. Moreover, autoregressive transformer models suffer from long-range attention loss when generating long sequences. To address these limitations, AttT2M [46] incorporated body-part attention-based spatio-temporal feature extraction method(BPST), to more finely train the VQ quantization model, while MoMask adopts RVQ-VAE, residual vector quantization autoencoder, to achieve higher-quality motion quantization. To overcome the limitations of unidirectional autoregressive transformer models in generating high-quality motion, models like MoMask [16] and MMM [29] employ masked transformer encoder architecture to deliver superior generation results. However, these impressive results still leave certain issues unresolved. Fine-grained VQ quantization models [46] require detailed motion joints splitting during the encoder training phase, but it did not significantly improve the generation quality of T2M-GPT (FID increased from 0.116 to 0.112). Bert-type models lose the token-by-token streaming output capability required for low-latency frame-by-frame generation in video games and multimedia environments, and they often exhibit higher risk of overfitting along with reduced out-of-distribution generation capability. Models using RVQ-VAE as the encoder require two models to be trained during the generation phase [5, 16]: one model (M-Transformer) generates base motion sequences based on the codebooks base layer, while another model (RTransformer) generates the motion residuals from the remaining codebook layers. This design introduces additional model training overhead when scaling data. Given the success of GPT-type generative models in various domains, as well as research indicating their superiority in few-shot learning, scaling effects, long-sequence generation, and multimodal applications [2, 19, 37, 40], we hypothesize that GPT-type models are well-suited for motion generation, which inherently requires long-sequence generation and multi-style adaptation. In light of these insights, we propose an innovative architecture named Mogo. By employing RVQ-VAE as highquality motion tokenizer and utilizing single hierarchical causal transformer model for inference, Mogo achieves superior generation quality and generalization capability over BERT-type models. Our contributions are as follows: 1. We designed simple model structure that requires only hierarchical GPT-type model (without the refine model used in [16]) to apply the codebook information generated by the RVQ-VAE encoder across all layers. This design significantly facilitates future efforts toward streaming output and scaling data for the model. 2. Mogo significantly enhances the generation quality The metof GPT-type motion generation models. rics achieve state-of-the-art (SOTA) performance among GPT-type models on the HumanML3D [13] and KITML [30] test sets. On the CMP[42] test set zero-shot inference evaluation, Mogo reaches SOTA level for all transformer-based models. 3. The generation length of Mogo breaks through the limitations of the training datasets motion sequence length. For instance, the maximum motion data sequence length in HumanML3D [13] is 10 seconds (196 frames), while our trained model can generate continuous and cyclic motion sequences of up to 13 seconds(260 frames). 4. We optimized the input prompts during inference through the LLM model to enhance Mogos generation performance in zero-shot and few-shot scenarios. 2. Related work Human Motion Generation. Human motion generation based on different conditions, such as text, audio, music, or image inputs, has made significant progress in recent years [47]. Early works[1, 10] commonly model motion generation deterministically. However, this type of model often results in vague and uncertain motions during generation. Stochastic models can effectively address this issue. [4, 39] employed GAN models to generate motion sequences based on conditions. [15, 28] used temporal VAE and transformer architecture to model and infer motions. In the field of text-based motion generation, [13] used temporal VAE to model the probabilistic mapping between text and motions. With the widespread application of diffusion [33] and transformer [36] architecture in text and image domains, their potential in motion generation has gradually been explored. Works such as [7, 20, 21, 34, 44, 45] utilized diffusion architecture to train models for motion generation. [16, 29] adopted Bert-type structure in their model designs. [18, 43, 46] used GPT-type autoregressive transformer models for text-to-motion generation. Among them, [18] achieved multimodal input-output for text-to-motion and motion-to-text by fine-tuning language model. GPT-type Models. We categorize motion generation models with decoder-only transformers as GPT-type models, and those using masked token bidirectional attention as BERT-type models. Current research indicates that while BERT-type models slightly outperform GPT-type models on generation quality metrics, GPT-type models have shown distinct advantages in language modeling, excelling in few-shot learning, scalability, and long-sequence generation [2, 19, 37]. As motion generation datasets grow and application scopes broaden, GPT-type models are expected 2 Figure 2. The architecture of the model:(a) RVQ-VAE is hierarchical residual vector quantization variational autoencoder that discretizes continuous motion sequences with high precision; (b) Rq Hierarchical Causal Transformer generates base motion sequences autoregressively while inferring residuals across layers. to reveal greater potential. Experiments by [43] confirm that GPT-type models continue to benefit from scaling effects in motion generation. AttT2M [46] addresses accuracy limitations of GPT models, reducing the FID from 0.116 in T2M-GPT [43] to 0.112 on the HumanML3D test set [13] by refining body joints segmentation encoding. MotionGPT [18], trained with large language model [32], supports bidirectional input and output of motion and text descriptions but achieves an FID of 0.232. Hierarchical Transformer Model. In solving complex problems, hierarchical transformer models have been widely applied in natural language processing [26, 27], image generation [9, 26], and computer vision [6, 23], demonstrating significant advantages. Unlike traditional flat structures, hierarchical structures allow the model to capture fine-grained features at different levels, thus more efficiently handling high-dimensional and more complex tasks. For example, [26, 27] significantly enhanced the models learning and reasoning capabilities for long sequences using hierarchical structures. [6] successfully expanded the processing resolution of computer vision to 4K levels using hierarchical transformer models, showcasing their strong capabilities in handling large-scale visual data. Similarly, [9] achieved state-of-the-art performance in image generation using hierarchical transformer models, proving the effectiveness of this structure in complex generation tasks. Motion Tokenizer. TM2T [14] first introduced VQ-VAE into the field of motion generation, mapping continuous human motions to discrete tokens. T2M-GPT [43] further optimized the performance of VQ-VAE through EMA and codebook reset techniques. However, quantization errors still exist during motion reconstruction. AttT2M [46] reduced quantization errors during the quantization phase by performing fine-grained segmentation of human body motions. MoMask [16] not only generates base motion sequence tokens using RVQ-VAE but also captures tokens that represent residual information, significantly improving the reconstruction accuracy of discrete motion sequences. 3. Methods Our goal is to generate high-quality motion sequence m1:N of length from textual description c, where mi RD and represents the dimensionality of the motion pose. As illustrated in Fig. 2, our model architecture consists of two core components: residual quantizationbased encoder for discretizing the motion sequence into multiple layers of motion tokens (Sec. 3.1), and Hierarchical Causal Transformer model for inferring and generating these multi-layer motion tokens in single pass (Sec. 3.2),The prompt engineering for our inference process is presented in Sec. 3.3. 3.1. Training: Motion Residual VQ-VAE We largely base our RVQ-VAE design on MoMask [16], but we introduce some modifications and optimizations in the sampling strategy. The Residual Quantizer (RQ) expresses Figure 3. The architecture of prompt engineering for enhanced model inference motion latent sequence as an ordered sequence of + 1 codes using + 1 quantization layers. We denote this as: RQ(b1:n) = [bv 1:n Rnd represents the code sequence at the v-th quantization layer. The residual value at layer 0 is r0 = b, and the sequences bv and rv+1 at subsequent layers are calculated as: v=0. where bv 1:n]V bv = Q(rv), rv+1 = rv bv, (1) After RQ computation, the final approximation of the latent sequence is the sum of all quantized sequences across layers: (cid:80)V v=0 bv, which serves as input to the decoder for motion reconstruction. Training Loss Function. The loss function for training the RVQ-VAE model is defined as follows: Lrvq = ˆm1 + β (cid:88) v=1 rv sg[bv]2 2, (2) ]V where sg[] denotes the stop-gradient operation, and β is hyperparameter controlling the commitment loss. This loss is optimized using the straight-through gradient estimator [35], and as in [16, 43], we employ codebook resetting and exponential moving average to update the codebook. After training, each motion sequence can be represented by + 1 discrete token sequences = [t1:n v=0, where each {1, . . . , Cv}n is an ordered codetoken sequence t1:n book index of the quantized embeddings b1:n , such that bi = Cvti for [1, n]. Among these + 1 sequences, the first (base) sequence contains the most significant information, while the subsequent layers gradually contribute less. Quantization Optimization Strategy. Although our RVQVAE adopts similar quantization approach as [16, 43], Inwe make improvements in the sampling strategy. stead of using convolutions with stride of 2 and nearestneighbor interpolation for downsampling and upsampling, as in [16, 43], we adjust the convolution stride to 1. This enhances the precision and expressiveness of motion reconstruction, enabling finer feature extraction and smoother reconstruction processes. 3.2. Training: Hierarchical Causal Transformer Hierarchical Architecture Design. After encoding the motion sequences using RQ-VAE, we obtain + 1 discrete motion token sequences for each motion sequence. To handle these layers in unified manner, we designed hierarchical Transformer model capable of processing the features from each layer simultaneously. For each layer of the Transformer model, we construct the input sv, which includes the text embedding, the quantization layer embedding, and the summed representations of the motion embeddings from the previous layers: sv = [p + qemb, t1:n ] (3) Here, represents the text prompt sentence embedding obtained from the pretrained CLIP model [31], capturing the global relationship between the sentence and the motion. qemb is the embedding of the quantization layer index v, computed as: qemb = Qone-hot, where Rmn is the weight matrix representing the linear transformation of the quantization embedding, and Qone-hot = one-hot(qid) is the one-hot representation of the layer index qid. We sum and qemb to obtain the mixed motion generation condition (PnQ condition), which serves as the prefix condition for the input sequence at each transformer layer. = (cid:80)v t1:n {1, 2, . . . , } represents the features of the current layer v, obtained by summing the embedded features of the ordered codebook index sequences t1:n from all previous layers up to layer v. This design alv lows for cumulative construction, ensuring that the features of the current layer are built upon the accumulated information from all previous layers and the layer-wise sequential information. i=0 t1:n , We also tested input sequences without mixed conditions: sv = [p, qemb, t1:n ], with results detailed in Sec. 4.6. Relative Positional Encodings. As described in Sec. 3.1, we adjusted the RQ-VAE convolution stride from 2 to 1 [16, 43], resulting in motion sequence length that is four times longer after encoding. To address the challenges posed by this increase in sequence length, we were inspired by Transformer-XL [8] and incorporated the relative positional encoding attention architecture introduced in [8]. Original attention mechanisms using absolute positional encoding often suffer from performance degradation when handling longer sequences, especially when the sequence length during inference differs from that during training [8]. By adopting the relative positional encoding, we not only enhanced the models ability to capture long-range attention dependencies but also improved the coherence and naturalness of the generated motion sequences. The attention computation with relative positional encoding is defined as follows:"
        },
        {
            "title": "Arel",
            "content": "i,j = xi (cid:124) W Wk,EExj (cid:125) + xi (cid:124) (cid:123)(cid:122) (a) + uWk,EExj (cid:123)(cid:122) (cid:125) (c) (cid:124) (cid:123)(cid:122) (b) + vWk,RRij (cid:123)(cid:122) (cid:125) (d) (cid:124) Wk,RRij (cid:125) (4) W In this formulation, (a) xi Wk,EExj represents the linear transformation of the embedding Exj at position via the embedding vector Exi, query weights Wq, and key weights Wk,E, capturing the direct influence of position on position j. (b) Wk,RRij calculates the relaxi tionship between position and the relative position Rij, emphasizing their distance. (c) uWk,EExj applies linear transformation to Exj with parameters and weights Wk,E, supplementing the information associated with position j. Finally, (d) vWk,RRij transforms the relative position Rij via parameters and weights Wk,R, reflecting the positional relationship between and j. Thus, in each layer of the Transformer, the autoregressive attention computation is expressed as: = Masked-Softmax(An an )Vn (5) where an denotes the attention output at the n-th layer is the attention of the Transformer model for layer v, An matrix with relative positional encoding. Loss Function. The goal of the model is to autoregressively generate motion tokens based on the text prompt input c, aiming to make the generated sequence as close as possible to the real motion sequence. We use maximum likelihood estimation function to compute the models loss value, optimizing the model parameters by minimizing the log-likelihood difference between each generated token and the target token: Lce = 1 (cid:88) Ti(cid:88) (cid:88) i=1 j=1 v=1 log pθ(tj Ov i,j1, c) (6) where denotes the batch size, Ti is the sequence length of the i-th sample, and is the number of quantization layers. Ov i,j1 represents the output at the v-th layer for the i-th 5 sample when generating the j-th token, conditioned on the previous 1 generated tokens and the text prompt c. This design allows the model to better integrate outputs from different levels, enhancing the quality of the generation. Data Processing. Following T2M-GPT [43], we used the same data augmentation strategy: during training, τ 100% of real code indices are replaced with random ones, where τ is either hyperparameter or randomly sampled from [0, 1]. In our case, τ is set to 0.5. This method improves the models generalization capability. 3.3. Inference Capability Optimization During user feedback experiments, we found that most participants provided text descriptions not present in the dataset, such as zombie, warrior, or ninja motions. While OMG [22] improves the Zero/Few-shot generation capability of diffusion models through motion ControlNet, recent studies on prompt engineering for large language models [3, 41] highlight the generative strengths of GPT in similar scenarios. Consequently, we developed prompt engineering framework  (Fig. 3)  using GLM-4 [11] to evaluate and optimize user prompts based on HumanML3D text labels. By utilizing labeled examples, GLM-4 learns descriptive styles and rewrites user prompts to better align the generated output with user expectations. 4. Experiments This section presents the experimental process and evaluation of Mogo. Sec. 4.1 outlines the datasets and metrics, while Sec. 4.2 details the training parameters. We discuss optimizing Zero/Few-shot generation via prompt engineering in Sec. 4.3, compare our models results with SOTA motion generation models in Sec. 4.4, detail the user study on perceived realism in Sec. 4.5, and explore parameter impacts through ablation experiments in Sec. 4.6. 4.1. Datasets and Evaluation Metrics Datasets. Our model was trained and tested on the HumanML3D [13] and KIT-ML [30] datasets. The HumanML3D dataset includes 14, 616 motion entries from the Amass [25] and HumanAct12 [12] datasets, with total of 44, 970 textual descriptions (three per entry). The KIT-ML dataset contains 3, 911 motion entries and 6, 278 textual descriptions. We followed the data processing approach of T2M [13], splitting the dataset into training, testing, and validation sets in ratio of 0.8 : 0.15 : 0.05. CombatMotionProcessed (CMP) dataset [42] is curated collection of 8,700 high-quality motions, all showcasing intense fighting-style motions, with many involving various weapon-based actions sourced from video games. For textual annotations, CMP provides three levels of description for each motion: concise summary, sensoryenriched brief, and an extensive, detailed account. As Datasets Methods HumanML3D KIT-ML CMP (zero-shot) MotionDiffuse [44] T2M-GPT [43] Fg-T2M[38] AttT2M [46] MotionGPT [18] MoMask[16] MMM [29] Mogo MotionDiffuse[44] T2M-GPT [43] Fg-T2M[38] AttT2M [46] MotionGPT [18] MoMask[16] MMM [29] Mogo T2M-GPT [43] AttT2M [46] MotionGPT [18] MoMask[16] MMM [29] Mogo Top 1 0.4910.001 0.4910.003 0.4920.002 0.4990.005 0.4920.003 0.5210.002 0.5040.002 0.5050.003 0.4170.004 0.4160.006 0.4180.005 0.4130.006 0.3660.005 0.4330.007 0.3810.005 0.4200.007 0.0610.003 0.0650.004 0.0500.002 0.0620.003 0.0670.004 0.0690.003 Precision Top 2 0.6810.001 0.6800.002 0.6830.003 0.6900.006 0.6810.003 0.7130.002 0.6960.003 0.6930.003 0.6210.004 0.6270.006 0.6260.004 0.6320.006 0.5580.004 0.6560.005 0.5900.006 0.6340.007 0.1030.005 0.1090.008 0.0940.002 0.1080.005 0.1160.008 0.1190.004 Top 3 0.7820.001 0.7750.002 0.7830.003 0.7860.004 0.7780.002 0.8070.002 0.7940.004 0.7990.003 0.7390.004 0.7450.006 0.7450.004 0.7510.006 0.6800.005 0.7810.005 0.7180.005 0.7540.007 0.1470.006 0.1470.008 0.1330.003 0.1500.004 0.1540.008 0.1660.004 FID MultiModal Dist MultiModality 0.6300.001 0.1160.004 0.2430.019 0.1120.004 0.2320.008 0.0450.002 0.0800.004 0.0790.002 1.9540.062 0.5140.029 0.5710.047 0.8700.039 0.5100.004 0.2040.011 0.4290.019 0.3130.016 16.0920.099 18.4030.071 10.6540.183 24.3510.205 17.0870.313 14.7240. 3.1130.001 3.1180.011 3.1090.007 3.0380.016 3.0960.009 2.9580.008 2.9980.007 3.0020.008 2.9580.005 3.0070.023 3.1140.015 3.0390.016 3.5270.021 2.7790.022 3.1460.019 2.9570.029 4.1790.049 4.0480.017 4.4310.021 4.8170.022 4.3600.017 4.0220.029 1.5530.042 1.8560.011 1.6140.049 2.4520.043 2.0080.084 1.2410.040 1.2260.035 2.0790.070 0.7300.013 1.5700.039 1.0190.029 2.2810.043 2.3280.117 1.1310.043 1.1050.026 2.0630.066 2.1180.033 2.2080.019 5.5350.259 1.6510.050 2.8020.011 3.1170.066 Table 1. Comparison with the GPT-type Models of text-conditional motion synthesis on the HumanML3D and KIT-ML test set. indicates 95% confidence interval. Bold face indicates the best result, while underscore refers to the second best. denotes GPT-type model. CMP exclusively features non-daily motion types, we leverage its test set to rigorously evaluate our models out-ofdistribution generation capabilities. This setup offers precise assessment of generalization performance, underscoring the models robustness and adaptability to unfamiliar motion types. For fairness in evaluating, we do not apply prompt engineering when evaluating Mogo on this dataset. Evaluation Metrics. We adopted the evaluation framework from T2M[13], using metrics such as Frechet Inception Distance (FID) for measuring distributional differences between generated and real motions[17]. R-Precision and Multimodal Distance (MM-Dist) assess the consistency of generated motions with input text, where R-Precision includes Top-1, Top-2, and Top-3 matching rates. MultiModality (MModality) evaluates the variance in distances between multiple motions generated from the same description. We primarily focus on FID to highlight the models motion quality advantage. 4.2. Implementation Details Our model is implemented using PyTorch. For RVQ-VAE, we use codebook size of 8192 128, with convolution stride of 1 and dropout rate of 0.2. It is trained on HumanML3D[13] and KIT-ML[30] datasets with 6 quantization layers, using AdamW optimizer and learning rate of 2 104 over 2000 iterations, with batch size of 512, trained on an NVIDIA 4090 GPU. For Hierarchical Causal Transformer, we design 6 sub-Transformers corresponding to RVQ-VAE layers with head counts of [16, 12, 6, 2, 2, 2], layer counts of [18, 16, 8, 4, 2, 2], setting model dimensions to 1024. We use AdamW optimizer with CosineAnnealingLR schedule [24]. For HumanML3D, the learning rate decays from 2.5 105 to 3 106 over 800000 iterations on an NVIDIA A800-80G GPU with batch size 32 for 1500 epochs. For KIT-ML, it decays from 3105 to 3106 on an NVIDIA V100-32G GPU with batch size 48 for 1500 epochs. 4.3. Inference We use GLM-4[11] for prompt preprocessing. As shown in Fig. 3: first, GLM-4 assesses whether the users prompt requires rewriting by comparing it to text annotations from the training set to determine if it aligns with typical human motions and descriptions. If necessary, GLM-4 reformulates the prompt to match the style of HumanML3D[13].The detailed prompt design is shown in the appendix. On an NVIDIA RTX 4090, the average inference and decoding time per frame is 0.002 seconds. Although the hierarchical structure adds some inference overhead, it still meets the requirements for real-time streaming output in multimedia environments. 4.4. Comparison to state-of-the-art approaches We compare our models performance with state-of-the-art motion generation models through quantitative evaluation and user feedback. Fig. 4 illustrates the visual quality com6 Figure 4. Comparison of the generation quality between our model and the latest SOTA motion generation models[16, 18]. Dataset HumanML3D KIT-ML Methods TM2T[14] M2DM[21] T2M-GPT[43] MoMask[16] MMM[29] Mogo M2DM[21] T2M-GPT[43] MoMask[16] MMM[29] Mogo FID 0.3070.002 0.0630.001 0.0700.001 0.0190.001 0.0750.001 0.0160.001 0.4130.009 0.4720.011 0.1120.002 0.6410.014 0.0420.001 Table 2. Comparison of the Reconstruction of our VAE Design vs. Motion VAEs from previous works. parison between our model and the latest SOTA models. Quantitative results. In Tab. 6, we compare the reconstruction quality of our RVQ-VAE with existing SOTA motion generation models, showing that our model significantly outperforms others in motion reconstruction accuracy. As presented in Tab. 1, our method achieves SOTA performance among all GPT-type models. Across all model types, our model ranks second only to MoMask[16]. In zero-shot evaluation on the CMP dataset [42], our model achieves SOTA performance, demonstrating superior out-of-distribution generation capabilities. Notably, MoMask[16] exhibits significant drop in all metricsespecially FIDcompared to MMM[29] on this dataset, highlighting its susceptibility to overfitting. 4.5. User Study Unlike previous studies relying on double-blind user feedback for generation preferences, we conducted user feedback study using scoring system focused on Prompt Figure 5. HumanML3D User Study alignment and Generation quality. For Prompt alignment, full motion alignment with the prompt scores 5 points; partial alignment results in deduction of 1 to 3 points based on motion proportion and order, and no alignment leads to full 5-point deduction. For Generation quality, fluent, natural motion earns 5 points, with 1 to 2 points deducted for minor issues and 3 to 5 points for major quality problems. We evaluated 25 samples generated by Mogo, MoMask[16], T2M-GPT[43], and MMM[29] on HumanML3D. Fig. 5 shows Mogo was significantly preferred by 20 users in double-blind test. 4.6. Ablation Study Codebook Size. As shown in Tab. 3, we conducted ablation experiments on the models reconstruction and generation capabilities on the HumanML3D dataset [13], focusing on different codebook sizes. During the experiments, the number of heads for each layer of the Transformer was set to [16, 12, 6, 2, 2, 2], and the number of layers was set"
        },
        {
            "title": "Codebook size",
            "content": "512 512 1024 1024 2048 512 4096 256 8192 128 FID 0.0220.001 0.0150.001 0.0170.001 0.0190.001 0.0160.001 Reconstruction Top1 0.5080.003 0.5110.002 0.5110.003 0.5100.002 0.5100.002 MM-Dist 2.9970.007 2.9840.010 2.9800.007 2.9890.006 2.9890.007 FID 0.2030.009 0.1840.008 0.0920.005 0.0900.005 0.0790.002 Generation Top1 0.4690.002 0.4670.003 0.4880.002 0.4910.002 0.5050. MM-Dist 3.1380.006 3.1140.009 3.0970.008 3.0010.008 3.0020.008 Table 3. Study on the number of code in codebook on HumanML3D[13] test set.Bold face indicates the best FID result to [18, 16, 8, 4, 2, 2]. We used the FID of the generated results as the core reference metric, ultimately selecting codebook size of 8192 128. Numbers of RVQ-VAE Layers. In the context of codebook size of 8192 128, we investigated the impact of varying the number of layers in the RVQ-VAE on the reconstruction quality. We provide detailed experimental data in the appendix. Impact of dataset size. As shown in Tab. 4, we further analyze our models scaling data capability. We mixed HumanML3D[13] dataset with different proportions of CMP[42] dataset to evaluate the impact of scaling data on model FID performance without retraining the RVQ-VAE, the models generation quality improves significantly as the data volume increases. Epochs w/ 0% CMP w/ 50% CMP w/ 100% CMP 1 50 80 100 24.064 0.671 0.437 0. 22.993 0.601 0.379 0.263 18.783 0.501 0.284 0.205 Table 4. The impact of varying dataset sizes on FID performance. Input Condition. We compared the performance of input conditions by either adding the prompt token and layer token as sequence prefix or not. As shown in Tab. 5, our experiments reveal that when without PnQ (adding prompt token to quantization layer token) is applied, the correlation between generated motion sequences and text (measured by R-Precision and MM Distance) decreases significantly. This may result from an additional layer token between action sequence tokens and prompt tokens during training, leading to some attention loss. (a) w/o PnQ condition (b) w/ PnQ condition Figure 6. Demonstration of Transformer attention masks with different condition structures. Layers and Attention Heads on Model Performance. 8 Codebook size 2048 512 4096 256 8192 128 Codebook size 2048 512 4096 256 8192 128 FID 0.0920.005 0.0900.005 0.0790.002 FID 0.0910.005 0.0880.005 0.0830.002 w/ PnQ Top1 0.4880.002 0.4910.002 0.5050.003 w/o PnQ Top1 0.4620.002 0.4690.002 0.4720.003 MM-Dist 3.0970.008 3.0010.008 3.0020.008 MM-Dist 3.2930.009 3.1410.007 3.1270. Table 5. Ablation study on the use of the PnQ condition. We tested various configurations of layers and heads for each Transformer in the model on the HumanML3D dataset [13]. Results in Appendix indicate that increasing layers and heads enhances generation quality. 5. Limitations and Discussion Motion Edit. Our model is based on GPT-type architecture with unidirectional masked autoregressive attention mechanism, lacks the inherent support for temporal completion model editing found in Bert-type models [16, 29]. This limitation is typical for all GPT-type models. Length Limitation of Generation. Although relative positional encoding enhances Mogos inference capabilities, allowing it to generate up to 260 frames in continuous, cyclic motion sequences, it is still unable to produce more than 196 frames for non-continuous motions. The lack of inherent correlations in motion data restricts the application of concatenation and the Transformer-XL segment memory mechanism [8]. The primary solution to this limitation is to extend the motion sequence lengths in the dataset. 6. Conclusion To leverage GPT-type models strengths in few-shot learning and streaming output while achieving generation quality on par with or exceeding Bert-type models, we developed Mogo, text-to-3D motion model using residual quantizer and hierarchical autoregressive Transformer. Mogo attains SOTA performance across multiple metrics, offering high-quality, extended sequence generation and superior out-of-distribution capabilities."
        },
        {
            "title": "References",
            "content": "[1] C. Ahuja and L. Morency. Language2pose: Natural language In 2019 International Confergrounded pose forecasting. ence on 3D Vision (3DV), pages 719728, Los Alamitos, CA, USA, 2019. IEEE Computer Society. 1, 2 [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 2 [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Ma teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. 5 [4] Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung Tang. Deep video generation, prediction and completion of In Proceedings of the European human action sequences. Conference on Computer Vision (ECCV), 2018. 2 [5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin P. Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. 2023. 2 [6] Richard J. Chen, Chengkuan Chen, Yicong Li, Tiffany Y. Chen, Andrew D. Trister, Rahul G. Krishnan, and Faisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1614416155, 2022. 3 [7] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. [8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, Florence, Italy, 2019. Association for Computational Linguistics. 4, 5, 8 [9] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hi9 erarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 3 [10] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional In Proceedings of animations from textual descriptions. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 13961406, 2021. 2 [11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 5, [12] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. Proceedings of the 28th ACM International Conference on Multimedia, 2020. 5 [13] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 2, 3, 5, 6, 7, 8 [14] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generIn ECCV, 2022. 3, ation of 3d human motions and texts. 7 [15] Chuan Guo, Xinxin Zuo, Sen Wang, Xinshuang Liu, Shihao Zou, Minglun Gong, and Li Cheng. Action2video: Generating videos of human 3d actions. International Journal of Computer Vision, 130(2):285315, 2022. 2 [16] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. 2023. 1, 2, 3, 4, 6, 7, 8 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 66296640, Red Hook, NY, USA, 2017. Curran Associates Inc. 6 [18] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6, [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. 2 [20] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Freeform language-based motion synthesis & editing. preprint arXiv:2209.00349, 2022. 2 arXiv [21] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space, 2023. 2, 7 [22] Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, and Lan Xu. Omg: Towards open-vocabulary motion generation via mixIn Proceedings of the IEEE/CVF Conture of controllers. ference on Computer Vision and Pattern Recognition, pages 482493, 2024. [23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 3 [24] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. 6 [25] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5441 5450, 2019. 5 [26] Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 15591571, Seattle, United States, 2022. Association for Computational Linguistics. 3 [27] R. Pappagari, Piotr Zelasko, Jesus Villalba, Yishay Carmiel, and Najim Dehak. Hierarchical transformers for long document classification. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 838844, 2019. 3 [28] Mathis Petrovich, Michael J. Black, and Gul Varol. Actionconditioned 3D human motion synthesis with transformer In International Conference on Computer Vision VAE. (ICCV), 2021. [29] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model, 2024. 1, 2, 6, 7, 8 [30] Matthias Plappert, Christian Mandery, and Tamim Asfour. The KIT motion-language dataset. Big Data, 4(4):236252, 2016. 2, 5, 6 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 4 [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. 3 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 2 [34] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. [35] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 63096318, Red Hook, NY, USA, 2017. Curran Associates Inc. 4 [36] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 1, 2 [37] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Lian zi Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. 2024. 2 [38] Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via diffusion model, 2023. 6 [39] Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, and Changyou Chen. Learning diverse stochastic human-action generators by learning smooth latent transitions, 2019. 1, 2 [40] Peng Xu, Xiatian Zhu, and David A. Clifton. Multimodal IEEE Transactions learning with transformers: survey. on Pattern Analysis and Machine Intelligence, 45:12113 12132, 2022. 2 [41] Qinyuan Ye, Mohamed Ahmed, Reid Pryzant, and Fereshte In FindKhani. Prompt engineering prompt engineer. ings of the Association for Computational Linguistics ACL 2024, pages 355385, Bangkok, Thailand and virtual meeting, 2024. Association for Computational Linguistics. 5 [42] Ziming Cheng Yihao Liao, Yiyu Fu and Jiangfeiyang Wang. Animationgpt:an aigc tool for generating game combat motion assets. https://github.com/fyyakaxyy/ AnimationGPT, 2024. 2, 5, 7, [43] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual deIn Proceedings of scriptions with discrete representations. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 4, 5, 6, 7 [44] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2, 6 [45] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. arXiv preprint arXiv:2304.01116, 2023. 1, 2 10 [46] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision (ICCV), pages 509519, 2023. 1, 2, 3, 6 [47] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: survey, 2023. 2 Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Overview In this Appendix, we present: Section B: Effects of varying layer numbers and head counts in Transformer layers on HumanML3D generation. Section C: Illustration of the statistical approach applied in the user study. Section D: Training loss and FID evolution. Section E: Prompt engineering details. Section F: Length restriction. Section G: More visual results of model generations. Section H: Model architecture code. Additionally, we clarify that, to ensure fairness when comparing with other approaches, all evaluation results in the main text are reported without applying prompt engineering optimizations. This decision is based on feedback from real-world applications, where users often provide natural and intuitive prompts such as walk like monkey or wizard casting spell. Consequently, we treat prompt engineering as qualitative tool for practical usage rather than quantitative evaluation strategy. B. Ablation study of Mogo architecture As shown in Tab. 6, We evaluated the impact of different quantization layers in RVQ-VAE on reconstruction quality. Layers 1 3 6 FID 0.0700.001 0.0210.001 0.0160.001 Top 1 0.502 0.001 0.508 0.001 0.510 0.001 MM-Dist 2.999 0.006 2.992 0.008 2.989 0.007 Table 6. The impact of different quantization layers on model reconstruction quality when the codebook size is 8192 128. Bold face indicates the best result. As shown in Tab. 7, we tested the impact of different layer counts and head counts of each Transformer layer in the model on the generation quality using the HumanML3D dataset.the codebook size is 8192 128. Parameter represents heads: [12, 6, 4, 2, 2, 2], layers: [16, 8, 6, 4, 2, 2]. Parameter represents heads: [16, 8, 4, 2, 2, 2], layers: [18, 10, 6, 4, 2, 2]. Parameter represents heads: [16, 12, 4, 2, 2, 2], layers: [18, 16, 6, 4, 2, 2]. Parameter represents heads: [16, 12, 6, 2, 2, 2], layers: [18, 16, 8, 4, 2, 2]. Layers FID Parameter 0.0940.003 Parameter 0.0900.003 Parameter 0.0820.004 Parameter 0.0790.002 Top 1 0.473 0.003 0.487 0.002 0.493 0.003 0.505 0.003 MM-Dist 3.260 0.010 3.168 0.009 3.063 0.010 3.0020.008 Table 7. the impact of different layer counts and head counts on the generation results using the HumanML3D dataset. Bold face indicates the best result. C. User Study Eval Metrics Fig. 7 illustrate the statistical results of our user study, classified according to the dimensions of experimental evaluation. Figure 7. User study eval metrics D. Training Loss and FID Fig. 8a and Fig. 8b illustrate the effects of different codebook sizes on the loss and generation FID of the Hierarchical Causal Transformer model during training on the HumanML3D dataset(the first 1000 epochs.). The x-axis of both figures represents iterations. During the training process, we evaluate the generation performance on the validation set every 15 epochs. E. Prompt Engineering In this section, will introduce our prompt engineering cues. The system prompt for the determine phase is: Determine Prompt: You are now an expert in human motion machine learning. Your task is to determine prompts for model trained on the HumanML3D dataset, which generates motion sequences from text. will provide you with some training set prompt 1 contains only everyday actions, boxing actions, and street dance types. Examples from the training set: person walking with their arms swinging back to front and walking in general circle person is standing and then makes stomping gesture the figure bends down on its hands and knees and then crawls forward person jumps and then side steps to the left person casually walks forward The person takes 4 steps backwards. The person was pushed but did not fall. This person kicks with his right leg then jabs several times. person lifting both arms together in front of them and then lifts them back down Note: Do not write specific characters; the action subject should be man or person since there are no specific characters in the training set, such as knights, wizards, soldiers, etc. You should describe their figure through limb movement as much as possible. Do not include objects being held, as the training set does not have specific objects like swords, knives, or guns. Describe their figure through limb movement instead. Your description should use simple and clear language, avoiding complex vocabulary. Try to mimic the wording style of the prompt examples provided as much as possible. Examples: Input: person anxiously paces after getting up, feeling restless. Output: man rises from the ground, walks in circle, and sits back down on the ground. Input: medieval knight is fighting. Output: person stands firmly, raising sword high, then lunges forward, swinging the sword from right to left while shifting weight onto his front foot. Input: man walks in figure 8 Output: man walks in figure 8 Input: man crawls forward Output: man crawls forward Input: person walks in circle Output: person walks in circle Input: man is battling Output: man is boxing and bouncing around (a) Training Process Loss (b) Training Process FID Figure 8. Impact of different codebook sizes on training results in the HumanML3D dataset. examples. Please use these examples to determine whether the users input needs to be rephrased to better match the datasets description style. Simply respond with yes or no. Examples: person walking with their arms swinging back to front and walking in general circle person is standing and then makes stomping gesture the figure bends down on its hands and knees and then crawls forward person jumps and then side steps to the left person casually walks forward The person takes 4 steps backwards. The person was pushed but did not fall. This person kicks with his right leg then jabs several times. person lifting both arms together in front of them and then lifts them back down Rewrite Prompt: You are now an expert in human behavior machine learning. You need to write prompts for model trained on the HumanML3D dataset that generates motion sequences from text. You need to describe abstract actions directly in English as concrete movements, specifying detailed limb movements and directions. Please output the detailed description directly, limited to one sentence and within 25 words. Do not include interactions with specific objects, only describe human movements. If the input prompt is already concrete motion description and in English, please return the original input prompt without modification. As reference, the original dataset 2 F. Length Restriction We did not adopt the length prediction mechanism of GPTtype models, which involves appending an [END] token after each data entry in the dataset to instruct the model when to stop generating. This is because, if we implemented this, the models generation length would be limited to 196 frames. Instead, we used length restriction approach, allowing users to input their desired generation length to determine when to stop the generation. G. Visualizations of Mogos Generation Fig. 9 illustrates further exemplary generative capabilities of our model, showcasing its performance in handling complex scenarios, diverse open-vocabulary prompts, and longsequence generation. The results highlight the models robustness in maintaining coherent motion patterns and precise alignment with textual inputs, even in challenging conditions. H. Code The core training code structure of our motion generation model is illustrated below. The variable abbreviations used in the code are defined as follows: Algorithm 1 Motion Generation Algorithm 1: Input: texts, ids, len, lbls 2: bs, nt, shape(m ids) 3: logits EncodeText(p texts) 4: logits CondEmb(p logits) 5: ids mask motion token(m ids[:, :-1, :]) 6: tks TokEmb(m ids) 7: for = 0 to - 1 do 8: tks Reduce(tks, i) tks st[i] tks 9: 10: end for 11: all layer out [] 12: for = 0 to - 1 do ids Fill(bs, i) 13: oh EncodeQuant(q ids) 14: tks QuantEmb(q oh) st tks Concatenate(p logits + tks, tks st[i]) ret, att Transformer(st tks, lbls) layer out Head(att) all layer out.append(layer out) 15: 16: 17: 18: 19: 20: end for 21: Output: out Stack(all layer out) 22: ce loss, pred id, acc CalcPerf(out, lbls, len) 23: return ce loss, acc, pred id, out 3 4 Figure 9. Demonstration of generative capabilities for open vocabulary and ultra-long sequences."
        }
    ],
    "affiliations": [
        "Mogo AI"
    ]
}