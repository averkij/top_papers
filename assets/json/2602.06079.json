{
    "paper_title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
    "authors": [
        "Liangyu Wang",
        "Siqi Zhang",
        "Junjie Wang",
        "Yiming Dong",
        "Bo Zheng",
        "Zihan Qiu",
        "Shengkun Tang",
        "Di Wang",
        "Rui Men",
        "Dayiheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline."
        },
        {
            "title": "Start",
            "content": "Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Liangyu Wang * 1 Siqi Zhang * 2 Junjie Wang 3 Yiming Dong 2 Bo Zheng 2 Zihan Qiu 2 Shengkun Tang 4 Di Wang 1 Rui Men 2 Dayiheng Liu 2 Abstract The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an α-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving 1.57 speedup in end-to-end iteration time and reducing optimizer step latency by 5.8 compared to the baseline. 6 2 0 2 4 ] . [ 1 9 7 0 6 0 . 2 0 6 2 : r 1. Introduction The relentless scaling of Large Language Models (LLMs) (Qwen et al., 2025; Yang et al., 2025; Grattafiori et al., 2024; Guo et al., 2025; Kaplan et al., 2020; Hoffmann et al., 2022) has driven continuous search for more effi1KAUST 2Alibaba Group 3Peking University 4MBZUAI. Correspondence to: Dayiheng Liu <liudayiheng.ldyh@alibabainc.com>, Rui Men <menrui.mr@alibaba-inc.com>. Preprint. February 9, 2026. Work done when Liangyu Wang, Junjie Wang, and Shengkun Tang were interns at Alibaba Group. 1 cient training algorithms. While element-wise optimizers like AdamW (Loshchilov & Hutter, 2017) and SGD (Ruder, 2016) remain the de facto standard, recent advances (Liu et al., 2025; Anil et al., 2020; Osawa et al., 2019; Wen et al., 2025) have demonstrated the superior convergence efficiency of Matrix-based Optimizers, such as Shampoo (Gupta et al., 2018), SOAP (Vyas et al., 2024), and Muon (Jordan et al.). Unlike element-wise optimizers that update parameters independently, these algorithms leverage second-order information or structural properties (e.g., via SVD (Golub & Van Loan, 2013) or Newton-Schulz iterations (Higham, 1997)) to accelerate training. However, deploying matrix-based optimizers in modern massive-scale training stacks presents fundamental systemalgorithm conflict. To minimize memory footprints, stateof-the-art frameworks like Megatron (Shoeybi et al., 2019) employ aggressive sharding strategies: ZeRO-1 (Rajbhandari et al., 2020; Zhao et al., 2023) partitions optimizer states across Data Parallelism (DP) ranks, while Tensor Parallelism (TP) splits weight matrices across devices (Shoeybi et al., 2019). This sharding creates strict Atomicity Constraint: matrix-based optimizers mathematically require access to the full tensor dimensions to perform holistic updates, but the distributed system physically fragments these tensors across disparate ranks. The details are illustrated in Appendix B. Existing approaches struggle to reconcile this conflict without severe compromises. Naive strategies often resort to Synchronous Compute (SC) (see details in Section 3.1), where ranks perform redundant operations or block globally to preserve atomicity, significantly hindering scalability. Alternatively, solutions like layer-wise partitioning attempt to distribute the workload but fundamentally violate the geometric layout required by ZeRO primitives (see detailed analysis in Section 3.1 and Appendix D.2). By misaligning the global load-balancing with the physical bucket-based parameter partition, they lose the capability for efficient, bucketbased Reduce-Scatter, sacrificing system throughput for algorithmic correctness. To bridge this gap, we propose Unified, Asynchronous, and Load-Balanced framework designed to support generic Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Figure 1. Comparison of Data Parallelism (DP) Partitioning Strategies. (Left) DP-SC: One way that can be directly used by those optimizers is DDP, which replicates optimizer states on all ranks, resulting in Redundant Compute where every rank performs identical matrix-based operations (synchronous). (Right) DP-ASC: Eliminates redundancy by partitioning states. Equal Chunk (Standard ZeRO-1, Gray Arrow and Line): Standard partitioning (e.g., for AdamW) slices the buffer into uniform shards (B/R). This arbitrary slicing (dashed lines) violates the atomicity required by matrix-based optimizers. Ours (Static Partitioning, Orange / Green Arrow): We enforce parameter atomicity by respecting tensor boundaries. Load Imbalance (Orange Arrow, Line, and Box): naive atomic assignment leads to significant computational stragglers and communication bubbles (dashed box) due to varying parameter costs. Load Balance (Green Arrow, Line, and Box): Our α-Balanced algorithm optimizes the static layout, redistributing whole parameters to equalize the workload across ranks. Note: In this figure, the blocks labeled represent the optimizer states and the associated update computation for those parameters. The parameters themselves remain replicated across ranks during the forward and backward passes (following the ZeRO-1 protocol). matrix-based optimizers while preserving the efficiency of established parallel architectures. Our method decouples the logical assignment of optimizer tasks from the physical distribution of parameters. For DP, we introduce Static Partitioning strategy that respects the Atomicity Constraint by assigning whole parameters to ranks, thereby enabling zerocommunication local updates (Figure 1, Ours). For TP, we design an Asynchronous Compute pipeline that batches fragmented tensor updates into Micro-Groups, effectively hiding the communication overhead of reconstruction (Figure 2, Right). Crucially, our design respects both the Atomicity Constraint and the ZeRO-1 Geometric Constraints (see details in Section 3.1), allowing us to fully inherit the efficient, coalesced communication overlapping capabilities of the standard Forward-Backward pass. significant challenge introduced by this atomic assignment is load imbalance. Naive assignment leads to severe stragglers and computational bubbles, as depicted in Section 3.1 and 4.1. To address this, we propose Canzona, unified, asynchronous, and load-Balanced framework for Distributed matrix-based optimizer. We formulate the partitioning as load-balancing optimization problem. We propose an α-Balanced Greedy LPT (Graham, 1969) algorithm for DP and Micro-Group Scheduling algorithm for TP to equalize execution times across heterogeneous workloads. Our contributions are summarized as follows: Unified and Decoupled System Architecture: We 2 propose unified distributed framework, Canzona, that reconciles the Atomicity Constraint of matrixbased optimizers with established ZeRO and TP paradigms. By fundamentally decoupling the logical optimizer assignment from the physical parameter distribution, our Static Partitioning (for DP) enables zero-communication during optimizer updates and preserves ZeRO-style geometric alignment between bucketed parameters and optimizer ownership, while our Asynchronous Micro-Group Pipeline (for TP) performs communication-efficient optimizer update via fused All-to-All. Crucially, Canzona provides unified, optimizer-agnostic abstraction that assigns each optimizer task designated host rank and executes these tasks asynchronously in parallel across ranks, enabling computecompute overlap that substantially reduces optimizer-step makespan. Heterogeneity-Aware Load Balancing Algorithms: We identify that the non-linear computational complexity (e.g., cubic scaling) of matrix-based operators introduces severe workload heterogeneity, which naive partitioning fails to handle. We formulate this as static scheduling problem and propose two dedicated offline the α-Balanced Greedy LPT algorithm algorithms: for DP, and Micro-Group Balanced Scheduling algorithm with greedy rollback for TP. These strategies effectively neutralize computational stragglers and elimiCanzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers nate pipeline bubbles. Large-Scale Verification and Efficiency: We evaluate our framework on cluster of 256 GPUs training the Qwen3 model family (ranging from 1.7B to 32B parameters). Extensive experiments demonstrate that our strategy outperforms layerwise optimizer, achieving 1.57x speedup in end-to-end iteration time and reducing the specific optimizer step latency by 5.8x. We further validate the generality of our approach across multiple emerging optimizers, including Muon, Shampoo, and SOAP. 2. Preliminary Modern LLM training relies on Data Parallelism (DP) to scale training, yet its vanilla implementation, Distributed Data Parallelism (DDP), necessitates redundant state replication (Figure 1, Left). To improve memory efficiency, frameworks like Megatron adopt the ZeRO-1 strategy, utilizing contiguous param and grad buffer organized into logical Buckets to pipeline communication (ReduceScatter/All-Gather) with computation. Complementing this, Tensor Parallelism (TP) is employed orthogonal to DP to uniformly split weight matrices across devices. However, critical system-algorithm conflict arises: standard sharding strategies, specifically the Equal Chunk ZeRO-1 partition shown in Figure 1 and the uniform splitting in TP, arbitrarily fragment tensors to balance memory and computation without regard for parameter boundaries. This violation of the Atomicity Constraint precludes the use of Matrix-based Optimizers (e.g., Muon and Shampoo), which mathematically require holistic tensor access for matrix-based operations, thereby preventing efficient local updates. We provide comprehensive review of these distributed training infrastructures and matrix-based optimizations in Appendix 3. Load-Balanced Asynchronous Compute for Data Parallelism Matrix-based Optimizer To satisfy the atomicity requirement of matrix-based optimizers at scale, we propose decoupled execution framework that separates logical optimizer-task ownership from physical parameter placement. Under modern parallel training, atomicity-induced communication arises in two planes: (i) the DP plane, where parameters and optimizer states are sharded across data-parallel ranks (e.g., ZeRO-1-style bucketed layouts), and (ii) the TP plane, where individual tensors are partitioned across tensor-parallel ranks and must be reconstructed for holistic matrix updates. We address these planes with complementary mechanisms. (1) DP (this section): we introduce Static Partitioning that respects both the Atomicity Constraint and the ZeRO-1 geometric bucket layout, so that each matrix update is executed locally by its designated owner rank without introducing additional collectives during the optimizer step, while retaining the training frameworks bucketed communication structure. (2) TP (Section 4): we assign each tensor-level optimizer task to host rank and execute tasks asynchronously and in parallel across ranks (with load-balanced scheduling), enabling scalable distributed matrix computation during the optimizer step. 3.1. Design Paradigm Analysis: Why Static Layout? While standard ZeRO-1 (Equal Chunk in Figure 1) eliminates redundancy by creating uniform shards (B/R), this arbitrary slicing violates the Atomic Constraint (Appendix B). As illustrated by the gray dashed lines, update responsibilities for tensors (e.g., P2, P8) are fragmented across ranks, rendering local holistic operations like SVD impossible without expensive reconstruction. To address this, we must adopt strategy that respects parameter boundaries. We analyze three paradigms below: Paradigm 1: Synchronous/Redundant Compute (The SC Baseline) straightforward way to preserve atomicity is to revert to DDP-style approach. This strategy replicates optimizer states across ranks, leading to severe computational redundancy where every device performs identical matrixbased operations (Figure 1 Left). While mathematically correct, this redundancy severely limits scalability. Paradigm 2: Layerwise Partitioning (Layer-based Schedule). recent proposal, such as NVIDIAs layerwise optimizer1 , attempts to avoid tensor sharding by assigning optimizer states at the granularity of whole layers. However, this approach introduces fundamental Geometric Incompatibility between the logical task assignment (determined by global load balancing) and the physical parameter partition (determined by ZeRO-1 geometry). As detailed in Appendix D.2 and visualized in Figure 15, if we preserve Megatrons param and grad buffer and bucket-level coalesced communication, this ZeRO Geometric Incompatibility structurally precludes the use of the efficient, bucket-based Reduce-Scatter pipeline. Consequently, it forces the system to fallback to suboptimal communication paths: (1) It typically necessitates All-Reduce for gradient synchronization, which incurs 2 the communication volume compared to the Reduce-Scatter used in ZeRO-1 and significantly increases bandwidth requirements; and (2) because the updated parameters are not geometrically aligned with their owners, it requires an additional expensive All-Gather or Broadcast during the optimizer step to redistribute weights, introducing further runtime overhead. Paradigm 3: Static Partitioning (Ours). To elimi1https://github.com/NVIDIA/Megatron-LM/pull/ 3 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers nate the aforementioned communication overheads, we adopt Static Layout strategy corresponding to the Ours paradigm in Figure 1. Instead of ZeRO-1s equal chunk, we enforce an Atomic Ownership Rule under ZeRO-1 Geometric Constraints: Given stride size = B/R (total buffer size divided by ranks), parameter is assigned exclusively to rank based on its starting position in the flattened buffer. Specifically, rank owns the optimizer states of if and only if: (r 1) Start Index(p) < (1) Crucially, by anchoring the ownership directly to the parameters physical Start Index, this strategy automatically satisfies the ZeRO-1 Geometric Constraints (See details in Appendix D.2). It ensures that every parameters optimizer states reside fully on single device (e.g., Rank 1 owns optimizer states of P1, P2 entirely as shown in Figure 1), respecting Atomicity Constraint, without scrambling the sequential rank ordering. This geometric alignment allows the system to fully inherit the efficient, coalesced Reduce-Scatter communication primitive. Given the prohibitive overheads associated with Paradigms 1 and 2, we adopted the Static-Layout Partitioning strategy as our implementation. The Challenge: Load Imbalance. While preserving atomicity is necessary, naive static assignment leads to the Load Imbalance scenario depicted in Figure 1. Because matrix-based optimizers have non-linear computational costs, rank assigned heavy parameter (e.g., the large block on Rank 1 in the figure) becomes computational straggler. This creates pipeline bubbles (highlighted by the dashed box in Figure 1) on other ranks, forcing them to wait and severely degrading global throughput, and introduce higher peak memory usage. (We also note that the variable chunk sizes of each bucket (labeled as Bucket in Figure 1) of the param and grad buffer introduce Reduce-Scatter and All-Gather communication imbalance, but this is secondary concern as it can be effectively hidden by overlapping communication with forward and backward computation. (Appendix C.5)) Our goal is to transform this state into the Load Balance state shown in the bottom right of Figure 1, where parameters are redistributed to equalize the workload. We formulate the problem as load-balanced optimization problem below. 3.2. Load-Balance Optimization Consider distributed training setting with ranks and param and grad buffer chunked into logical buckets = {B1, . . . , BN }. Each bucket Bi consists of an ordered sequence of non-splitable parameters Pi. We assign function W(p) (Typically W(p) = numel(p)) to each parameter p, representing its primary resource cost (e.g., memory usage, computation, or communication). Note that while we assume linear costs here for simplicity, we provide generalized explanation for non-linear optimizer costs in Appendix D.5. Algorithm 1 α-Balanced Greedy LPT Partitioning ( D.3) Require: Buckets B, Ranks R, Balance Factor α Require: Load function W() = numel(p) Ensure: Partition vectors {si} for all 1: Define: Let = (cid:80) pBi W(p) be the total load of bucket i. 2: Define: Let Φi(u) = (cid:80) tive load up to cut point u. pBi[0:u] W(p) be the cumula3: Initialize: Global load vector 0R 4: Target: µ ((cid:80) 5: Sort: Virtual reorder such that π(1) π(2) π(N ) (LPT based on Load, Virtual Inter-Bucket reorder) i)/R r=1; Dtotal (cid:80) 6: for = 1 to do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Let current bucket index = π(t) Step (1): Calculate Deficits (in Load domain) [max(0, µ Lr)]R Step (2): Define Basis Vectors veven [1/R, . . . , 1/R] vfill d/Dtotal if Dtotal > 0 else veven Step (3): Blended Target Allocation (1 α)veven + αvfill target alloc v {Target load for ranks} Step (4): Discretization (Project Load to Valid Cuts, following Atomicity Constraint) Init sk,0 0; 0 for = 1 to 1 do + target alloc[r] {Find cut where cumulative load Φk(u) is closest to target C} sk,r arg minuUk Φk(u) {Update global load with actual load of the slice} Lr Lr + (Φk(sk,r) Φk(sk,r1)) 17: 18: 19: 20: 21: end for sk,R Size(Bk) LR LR + (Φk(sk,R) Φk(sk,R1)) 22: 23: 24: 25: 26: end for 27: Return {si} sorted by original indices The goal is to determine slicing vectors si for each bucket Bi. The cumulative load assigned to rank for bucket Bi is Li,r = (cid:80) prange(si,r1,si,r) W(p). We aim to optimize two objectives: (1) Global DP Balance (Minimize Stragglers): Minimize the maximum deviation from the ideal mean load µload: JDP = max (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) i=1 Li,r µload (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (2) 4 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers (2) Fwd-Bwd Bucket Communication Balance (Minimize Idle Time): Ensure data volume within each bucket is distributed evenly: JComm = (cid:88) (cid:88) i=1 r= (cid:12) (cid:12) (cid:12) (cid:12) Si,r Bi (cid:12) (cid:12) (cid:12) (cid:12) (3) Crucially, those optimization objectives respect the ZeRO-1 Geometric Constraints (Appendix D.2) by shifting the slice boundaries si,r within the bucket. Because the parameters are never physically reordered, the sequential, monotonic physical ordering is preserved. This boundary-shifting mechanism serves as the logical bridge that allows us to neutralize load imbalance while retaining the capability to launch optimal, coalesced Reduce-Scatter primitives. Optimizing this discrete partitioning problem under strict atomicity constraints is NP-hard. We propose heuristic approach named α-Balanced Greedy LPT (Algorithm 1). By processing buckets in Longest Processing Time (LPT) order, we ensure that heavy parameter blocks are handled early when the flexible deficits space is largest. The algorithm employs control parameter α [0, 1] to interpolate between two objectives: (1) Global Load Balancing (α 1): Prioritizes filling the accumulated deficits of straggler ranks to minimize the global makespan and optimizer state memory usage. (2) Fwd-Bwd Bucket Communication Balance (α 0): Ignores historical accumulation and partitions the current bucket uniformly (approximating standard ZeRO-1 behavior). 3.3. System Workflow: Static-Layout Enforcement The execution workflow consists of two stages: 1. Offline Planning (Setup Phase). Before training, the α-Balanced Greedy LPT algorithm computes Global Partition Map Π. For each rank and bucket Bi, Π defines the memory interval [si,r1, si,r) owned by that rank. Crucially, because we enforce the Atomicity Constraint, these intervals strictly align with parameter boundaries. Based on Π, we override the standard shard registration mechanism. Instead of allocating uniform chunks, each rank allocates memory strictly proportional to its assigned load volume Si,r, ensuring the physical buffer layout matches the logical partition. 2. Runtime Execution with Overlapping. During training, the data flow respects the static layout while preserving Megatrons efficient communication overlapping: (1) Backward (Variable-Size Reduce-Scatter): As gradients accumulate in bucket Bi, we trigger non-uniform Reduce-Scatter operation. Unlike the standard implementation, which assumes equal split sizes, our communication primitive handles the variable shard sizes Si,r defined by Π. Importantly, since Megatron overlaps this communication with the backward computation of the subsequent bucket (Bi1), the slight latency variance caused by the nonuniform data sizes is effectively hidden, maintaining high training throughput. (2) Optimizer Step (Load-Balanced Asynchronous Computation and Zero-Communication): Rank updates the parameters strictly within its assigned interval. Since the partition respects atomic boundaries, every parameter (with its gradient and optimizer state) is fully available locally. The matrix-based optimizer performs operations without any additional communication. (3) Forward (Variable-Size All-Gather): Before the forward pass, non-uniform All-Gather reconstructs the full bucket parameters. Similar to the backward pass, this operation is overlapped with the forward computation of the previous bucket, minimizing the impact of size imbalance. 4. Tensor Parallelism with Load-Balanced"
        },
        {
            "title": "Asynchronous Compute",
            "content": "While our DP strategy (Section 3) strictly enforces zerocommunication layout during optimizer step to protect the global network, Intra-node Tensor Parallelism (TP) presents different challenge. Figure 2 compares the workflow of standard TP and our proposed strategy. In the standard TP Synchronous Compute (TP-SC) (Figure 2 Left, NVIDIAs layerwise optimizer implementation), every rank must communicate and compute the full tensor for every parameter. This results in the Redundant Compute highlighted in the figure, where all devices perform identical operations (e.g., Compute with Full G1) regardless of the tensor ownership. In contrast, our TP Asynchronous Compute (TP-ASC) (Figure 2 Right) eliminates this redundancy through statically planned asynchronous computation pipeline. As shown in the Micro Gradient Group block, we batch multiple tensors (e.g., G1, G2, G3) into single fused communication group. Crucially, our offline algorithm (Algorithm 2) assigns these tensors to specific Host Ranks to equalize the load W. We emphasize that this reconstruction communication is viable specifically because TP typically operates within the high-bandwidth Intra-node domain (e.g., via NVLink). This stands in sharp contrast to DP, which spans bandwidth-constrained Inter-node connections, thereby necessitating the strict zero-communication strategy we enforced in Section 3. 4.1. Task Abstraction: The Asynchronous Compute Unit To enable fine-grained scheduling, we abstract the update of each TP-split parameter as an atomic Compute Task assigned to specific Host Rank r. Since the scheduling decision is static (determined during the initialization phase), we can enforce strict locality for optimizer states. SpecifiCanzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Figure 2. Optimizer Update Workflow Comparison of Tensor Parallelism (TP) Strategies. (Left) TP-SC: An intuitive approach that relies on synchronous collective communication (All-Gather) and redundant computation (performing the same tensor operations on all TP ranks), which limits scalability and efficiency. (Right) TP-ASC: Our proposed strategy utilizing Micro-Group Scheduling. Micro Gradient Group: Gradients are aggregated into micro groups (labeled as in the figure) to saturate the All-to-All (dashed box) communication bandwidth, replacing the inefficient small-kernel calls. Load Balancing: Instead of fixed assignments, these groups are dynamically scheduled to Host Ranks. The distinct block lengths in the Compute phase illustrate our algorithms ability to handle varying computational costs, minimizing the overall execution makespan. cally, the full optimizer states (e.g., momentum) will be initialized directly on their designated Host Ranks. Consequently, these states never require transmission throughout the training process. Algorithm 2 Micro-Group Construction for TP load balance with Greedy Rollback (See D.3 for detailed walkthrough) 1: Input: Parameters P, Cost Model W(), Ranks R, Cap Cmax {Min-Heap Solver to find optimal Host Ranks} Lmax MinHeapBalance(Mtest, R) if Lmax Cmax then 2: Output: Sequence of Micro Groups 3: {Phase 1: Deterministic Global LPT Sort} 4: Psorted Sort(P, key = (W(p), IDp), descending) 5: Mcurr ; 6: {Phase 2: Partitioning with Rollback} 7: for in Psorted do 8: Mtest Mcurr {p} 9: 10: 11: 12: 13: 14: 15: 16: end if 17: 18: end for 19: M.append(Mcurr) 20: return {Constraint Violated: Rollback & Finalize} M.append(Mcurr) Mcurr {p} {Start new micro group with p} Mcurr Mtest {Valid: Accept p} else the Micro Gradient Group panel of Figure 2: (1) All-to-All for gathering: Since full optimizer states are already resident on Host Ranks, we only aggregate the gradients G. We fuse the asynchronous gather operations of multiple tensors (e.g., Micro Group = {G1, G2, G3} in Figure 2) into single All-to-All collective. By batching parameters into Micro Groups, we ensure the data volume is sufficient to saturate the All-to-All bandwidth, avoiding the overhead of small messages while preventing the prohibitive memory peak caused by fusing all gradients globally. This efficiently routes the gradient shards to their designated Host Ranks where the corresponding optimizer states await. (2) Asynchronous Computation: Host Ranks execute the matrix-based operations using the gathered gradients and the locally resident optimizer states (e.g., = Muonstep(G, Stateslocal)). This phase balances the workload by assigning heavy computations (like G2 in Figure 2) to dedicated ranks, while packing lighter tasks (G1, G3) together. (3) All-to-All for scattering: Once the update tensors (W ) are computed, the Host Ranks slice them into shards. These update shards are then scattered back to the original owners of the parameters via fused All-to-All operation. (4) Local Update: Finally, as shown in the Update Shard block of Figure 2, each rank applies the received update shards to its local parameter shards (e.g., Pi,local Pi,local + Wi,local). This guarantees mathematical correctness while avoiding the transmission of both model weights and optimizer states. Combined with the property that optimizer steps derive updates solely from gradients and states without requiring the weight parameters (P ) immediately, we can optimize the data flow to strictly minimal communication. The lifecycle of task group follows the execution stages visualized in 4.2. Workload Scheduling: Hierarchical Partitioning While Figure 2 illustrates the example ideal execution flow for single instance (Micro Group j), the system-level challenge lies in generalizing this balance across the entire 6 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers model. We must rigorously partition the full parameter set into sequence of such Micro Groups = {M1, . . . , MK} and determine the optimal Host Rank assignments to minimize the overall execution time. We formulate this as hierarchical partitioning problem with Lexicographical Objectives: Priority 1: Minimize Computational Imbalance. Within each micro group Mk, we minimize the makespan across the ranks. Let Lk,r be the load on rank for group Mk: min Φ1(Mk) = max (Lk,r) min (Lk,r) r Priority 2: Maximize Group Saturation. To reduce the total number of communication steps, we pack as much load as possible into each micro group without violating the memory/buffer capacity Cmax: max Φ2(Mk) = (cid:88) r=1 Lk,r, s.t. max (Lk,r) Cmax Algorithm: Global LPT with Greedy Rollback. We propose deterministic heuristic (Algorithm 2) to solve this optimization problem offline. First, we use Global Sort based on computational cost (LPT rule) to ensure determinism across ranks and prioritize heavy tasks (which are harder to balance). Second, we employ Greedy Packing with Rollback strategy. We iteratively add parameters to current micro group and simulate load distribution using Min-Heap. If adding parameter violates the Cmax constraint (indicating the makespan or memory has exceeded limits), we trigger Rollback: the current group is finalized, and the parameter initiates new group. Note that, similar to Section 3.2, while we assume simplified linear cost model W(p) = numel(p) in this section for clarity, our framework formulation fully supports the generalized non-linear complexity (detailed in Appendix D.5) inherent to optimizers like Muon and Shampoo. Integration with Runtime Workflow. The sequence generated by Algorithm 2 serves as the static execution plan. Effectively, this algorithm automatically chunks the continuous stream of gradients into discrete, load-balanced unitsone of which corresponds to the example Micro Gradient Group visualized in Figure 2. During training, the runtime system simply iterates through this precomputed sequence M. For each group Mk, it triggers the exact Asynchronous Compute Unit lifecycle described in Section 4.1, ensuring that the complex load balancing decisions made offline are executed efficiently with minimal runtime overhead. 4.3. Canzona: Framework for Unified, Asynchronous, and Load-Balanced Matrix-based Optimizer We conclude our methodology by highlighting the cohesive design philosophy that spans both parallelism dimensions. Our framework establishes strictly Unified architecture: neither the DP partitioner (Section 3.2) nor the TP scheduler (Section 4.2) requires modification to the optimizers internal mathematics. Instead, we treat tensor updates as generic computational tasks defined solely by their cost metrics, making the system optimizer-agnostic and compatible with any current or future matrix-based optimizers. Furthermore, both DP and TP strategies converge on fully Asynchronous execution model, ensuring that expensive TensorOps are asynchronously executed. Finally, by enforcing Load-Balanced static planning at both DP and TP levels, we effectively neutralize the cost loads that previously hindered the scaling of matrix-based optimizers. 5. Experiment We evaluate our framework on cluster with up to 512 GPUs, covering experimental setup (Section 5.1 and Appendix C.1), efficiency comparison (Section 5.2), and precision verification (Section 5.3). We provide extended analyses in Appendix C: (1) Full Performance Comparison against NVIDIAs layerwise optimizer (Appendix C.2); (2) Scalability Analysis across DP/TP and model sizes (Appendix C.3); (3) Generalization to Shampoo and SOAP (Appendix C.4); and (4) Ablation Studies on hyper-parameters (Appendix C.5&C.6). 5.1. Experiment Setup We evaluate the Qwen3 family (1.7B32B) using the Muon optimizer (sequence length = 4096, batch size per-DPrank=1) by default, with verification on Shampoo and SOAP. The number of GPUs varies by experiment and is specified in each subsection. We evaluate Canzona via its core strategy, LB-ASC (Load-Balanced Asynchronous Compute), against: (1) SC (Synchronous Compute (DDP/TP)); (2) NVlayerwise (NVIDIAs layerwise optimizer); and (3) our ablation ASC (Asynchronous Compute (ZeRO-1/TP) (Decoupled architecture without load balancing). 5.2. Main Results All experiments in this section are conducted on largescale cluster of 256 GPUs. The distributed topology is configured with DP size of 32 and TP size of 8. Effectiveness of Load Balancing Strategies (Figure 3). We analyze how our proposed strategies mitigate the severe load imbalance inherent in matrix-based optimizers. For Data Parallelism (Figure 3c), standard static partitioning Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers (a) Efficiency Comparison (b) Tensor Parallelism Load Balancing Analysis (c) Data Parallelism Load Balancing Analysis Figure 3. Main Results: (a) Efficiency Comparison: Our LB-ASC strategy outperforms baselines by effectively eliminating computational bubbles and maximizing device utilization. (b) & (c) Load Balancing Analysis: The visualized load distributions demonstrate that our proposed scheduling algorithms successfully flatten the workload variance for both Tensor Parallelism (b) and Data Parallelism (c), significantly mitigating the straggler problem compared to naive partitioning. (Without DP Load Balancing) results in extreme stragglers, exhibiting FLOPs imbalance ratio (Max/Avg) of 3.24 and Memory imbalance ratio of 2.46. In contrast, our α-Balanced Partitioning drastically reduces these ratios to 1.43 and 1.11, effectively flattening the workload distribution. For Tensor Parallelism (Figure 3b), the naive baseline similarly suffers from high variance (FLOPs Ratio: 3.24). By introducing our Micro-Group Scheduling, we improve the FLOPs ratio to 2.46 and the Memory ratio to 1.16. Crucially, as shown in the Makespan comparison (Figure 3a), this improved balance directly translates to efficiency: LB-ASC achieves the lowest maximum step time (1.05 TFLOPS equivalent) compared to SC and ASC, effectively eliminating the computation bubbles. Comparison with layerwise optimizer (Figure 4). We compare the end-to-end training iteration time against NVIDIAs layerwise optimizer (NV-layerwise), the current best practice for preserving atomicity. As shown in Figure 4, our approach achieves 1.57 speedup in total iteration time (0.877s vs. 1.381s). This performance gain stems from two critical sources: (1) Drastic Reduction in Optimizer Latency (5.8 Speedup): Our optimizer step time drops from 0.383s to 0.066s. While NV-layerwise necessitates extra runtime Broadcast or All-Gather to redistribute updated parameters, our zerocommunication DP strategy and asynchronous TP pipeline effectively hide the optimization overhead. (2) Improved Fwd-Bwd Efficiency (1.23 Speedup): Our method also reduces the Forward-Backward pass time from 0.998s to 0.811s. This highlights structural advantage: NVlayerwise suffers from the geometric conflict described in Appendix D.2, which structurally forces it to rely on gradient All-Reduce (incurring 2 communication volume), whereas our approach utilizes the more efficient, bucketoverlapped Reduce-Scatter. 5.3. Precision Verification To ensure algorithmic correctness, we trained Qwen31.7B model on 400B tokens using the Muon optimizer (DP=8, TP=4). As shown in Figure 5, the training loss trajectory of our strategy LB-ASC is indistinguishable from the standard synchronous baseline (SC). This confirms that our decoupled partitioning and scheduling strategies act as 8 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers gies. By introducing the α-Balanced Static Partitioning for DP and the Asynchronous Micro-Group Scheduling for TP, we successfully reconciled strict atomicity requirements with massive parallelism. Our extensive experiments on the Qwen3 model family demonstrate that our approach not only eliminates the heavy communication overhead of existing solutions but also effectively neutralizes the load imbalance inherent in second-order optimization, achieving 1.57 end-to-end speedup."
        },
        {
            "title": "References",
            "content": "Ahn, K. and Xu, B. Dion: communication-efficient optimizer for large models. arXiv e-prints, pp. arXiv 2504, 2025. Anil, R., Gupta, V., Koren, T., Regan, K., and Singer, Y. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020. Golub, G. H. and Van Loan, C. F. Matrix computations. JHU press, 2013. Gong, W., Scetbon, M., Ma, C., and Meeds, E. Towards efficient optimizer design for llm via structured fisher approximation with low-rank extension. arXiv preprint arXiv:2502.07752, 2025. Graham, R. L. Bounds on multiprocessing timing anomalies. SIAM journal on Applied Mathematics, 17(2):416429, 1969. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Gupta, V., Koren, T., and Singer, Y. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pp. 18421850. PMLR, 2018. He, W., Han, K., Zhou, H., Chen, H., Liu, Z., Chen, X., and Wang, Y. Root: Robust orthogonalized optimizer for neural network training. arXiv preprint arXiv:2511.20626, 2025. Higham, N. J. Stable iterations for the matrix square root. Numerical Algorithms, 15(2):227242, 1997. Figure 4. End-to-End Iteration Time Comparison: Our framework significantly outperforms layerwise optimizer. The performance advantage is driven by two factors: (1) the elimination of runtime communication during the optimizer step via our decoupled design, and (2) the preservation of the ZeRO-1 Geometric Constraint mentioned in Appendix D.2. Figure 5. Precision Verification: SC Baseline & LB-ASC (Ours) purely system-level optimizations, preserving the exact convergence behavior of the original matrix-based optimizer. More precision verification experiments can be found in Appendix C.4 6. Related Work The deployment of matrix-based optimizers in distributed settings presents unique system-algorithm conflict. While early works like Distributed K-FAC (Osawa et al., 2019) utilized approximate curvature to mitigate communication costs, modern exact optimizers (e.g., Muon, Shampoo) require strict atomicity that conflicts with standard sharding strategies like ZeRO (Rajbhandari et al., 2020). To resolve atomicity, approaches like NVIDIAs layerwise optimizer assign states by layer but disregard ZeRO geometric constraints. As detailed in Appendix D.2, this Geometric Incompatibility precludes efficient bucket-based Reduce-Scatter, forcing fallback to costly All-Reduce (2 volume). Conversely, our framework harmonizes atomicity with geometric partitioning, fully preserving the optimal communication primitives. Please refer to Appendix for comprehensive review. 7. Conclusion In this paper, we presented unified distributed optimization framework that resolves the fundamental conflict between matrix-based optimizers and modern parallel training strateHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal 9 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers large language models. arXiv preprint arXiv:2203.15556, 2022. Jordan, K., Jin, Y., Boza, V., Jiacheng, Y., Cecista, F., Newhouse, L., and Bernstein, J. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan. github. io/posts/muon, 6. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Khaled, A., Ozkara, K., Yu, T., Hong, M., and Park, Y. Muonbp: Faster muon via block-periodic orthogonalization. arXiv preprint arXiv:2510.16981, 2025. Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Ruder, S. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li, T., Paszke, A., Smith, J., Vaughan, B., Damania, P., et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704, 2020. Shi, H.-J. M., Lee, T.-H., Iwasaki, S., Gallego-Posada, J., Li, Z., Rangadurai, K., Mudigere, D., and Rabbat, M. distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale. arXiv preprint arXiv:2309.06497, 2023. Li, X.-L. Preconditioned stochastic gradient descent. IEEE transactions on neural networks and learning systems, 29 (5):14541466, 2017. Liu, H., Li, Z., Hall, D., Liang, P., and Ma, T. Sophia: scalable stochastic second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023. Liu, J., Su, J., Yao, X., Jiang, Z., Lai, G., Du, Y., Qin, Y., Xu, W., Lu, E., Yan, J., et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 24082417. PMLR, 2015. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Osawa, K., Tsuji, Y., Ueno, Y., Naruse, A., Yokota, R., and Matsuoka, S. Large-scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1235912367, 2019. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., 10 Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Vyas, N., Morwani, D., Zhao, R., Kwun, M., Shapira, I., Brandfonbrener, D., Janson, L., and Kakade, S. Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. Wang, J., Zhou, P., Dong, Y., Li, H., Li, J., Zhou, X., Lao, Q., Fang, C., and Lin, Z. Conda: Column-normalized adam for training large language models faster. arXiv preprint arXiv:2509.24218, 2025. Wen, K., Hall, D., Ma, T., and Liang, P. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046, 2025. Xie, T., Luo, H., Tang, H., Hu, Y., Liu, J. K., Ren, Q., Wang, Y., Zhao, W. X., Yan, R., Su, B., et al. Controlled llm training on spectral sphere. arXiv preprint arXiv:2601.08393, 2026. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers A. Summary of Notations Symbol Description Table 1. Summary of Notations System Settings & Preliminaries"
        },
        {
            "title": "R\nB\nBi\nPi\nS\nIr\nop",
            "content": "Total number of ranks (DP ranks (Section 3) or TP ranks (Section 4)) The set of logical buckets {B1, B2, . . . , BN } in the param and grad buffer The i-th logical bucket consisting of ordered parameters The sequence of non-splitable parameters (pi,1, . . . , pi,Mi) in Bi Uniform shard size (B/R) in standard ZeRO partition The memory interval [(r 1)S, rS) assigned to rank in ZeRO The starting memory offset (index) of parameter in the flattened buffer"
        },
        {
            "title": "Load Cost Metrics",
            "content": "Computational cost of parameter (FLOPs) Cmem(p) Memory usage cost of parameter Ccomp(p) Ccomm(p) Communication cost of parameter (parameter size, = numel(p)) Wload(p) Wsize(p) W(p) Primary metric for load balancing (Cmem or Ccomp) Secondary metric for communication grouping (usually Ccomm) general cost metric assigned to parameter for load balancing Problem Formulation & Load-Balance Algorithm for DP LB-ASC Slicing vector [si,0, . . . , si,R] containing cut points for bucket Bi Set of feasible (atomic) cut points for bucket Bi The cumulative load assigned to rank in bucket Bi The data size assigned to rank in bucket Bi. Here, Si,r = si,r si,r1 Control parameter [0, 1] trading off between JDP and JComm Vector tracking the global accumulated load for each rank Basis vector for perfectly even distribution Basis vector for deficit-filling distribution Blended target allocation vector for the current bucket Cumulative load function of bucket Bi up to cut point Problem Formulation & Load-Balance Algorithm for TP LB-ASC The sequence of Micro Groups {M1, ..., MK} generated by the scheduler The maximum capacity constraint (memory or FLOPs) for single Micro Group si Ui Li,r Si,r α veven vfill Φi(u) Cmax B. Extended Preliminary B.1. Data Parallelism and ZeRO Data Parallelism (DP) is the dominant paradigm for training Large Language Models (LLMs). In standard Distributed Data Parallelism (DDP) (Li et al., 2020) (Figure 1), the model parameters and optimizer states are replicated across all parallel data ranks. While this enables parallel gradient computation, the redundancy of optimizer states consumes significant portion of GPU memoryoften 23 the model size for mixed-precision training (e.g., FP32 master weights (Micikevicius et al., 2017) and momentum). 11 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Zero Redundancy Optimizer (ZeRO-1) (Rajbhandari et al., 2020) addresses this redundancy by partitioning the model states across data-parallel ranks. Specifically, ZeRO-1 (Optimizer State Partitioning) shards the optimizer states such that each rank is responsible for updating only fraction (1/R) of the total parameters. This linear reduction in memory footprint is crucial for scaling typically massive models. Frameworks like Megatron (Shoeybi et al., 2019) implement ZeRO-1 through its DistributedOptimizer, which relies on unified memory management structure known as the param and grad buffer (Flat and Bucket boxes in Figure 1). (Because ZeRO-1 has become default choice in Megatron, we will refer to ZeRO-1 as DP in the following text.) Continuous Memory Buffer and Bucketing of param and grad buffer. Instead of managing memory for each parameter tensor individually, Megatron flattens all model parameters (and their gradients) into several continuous memory buffers. To facilitate communication overlapping of forward and backward, each continuous buffer is logically divided into buckets. As parameters are registered, they are packed into these buckets sequentially. This contiguous layout is essential for high-performance collective communications, allowing single large operation (e.g., Reduce-Scatter) to process multiple parameters simultaneously rather than launching many small kernels. Uniform Partition Rule and ZeRO-1 Geometric Constraint. Crucially, Megatron imposes strict geometric partition rule. For bucket of size B, the buffer is rigidly divided into equal contiguous segments of size = B/R, entirely agnostic to the boundaries of individual parameter tensors. Consequently, rank is hard-wired to process the r-th segment. This rigid positional mapping forms fundamental ZeRO-1 Geometric Constraint that is essential for efficient communication primitives like Reduce-Scatter. The typical training workflow with this buffer structure proceeds as follows: Backward Pass: As the backward pass proceeds, gradients are computed and accumulated into the buckets. Once the gradients for all parameters in bucket are ready, Reduce-Scatter operation is triggered across all DP ranks. This operation aggregates the gradients and shards them, such that each rank receives only its assigned continuous segment S. Importantly, this communication is overlapped with the backward computation of the subsequent bucket. Optimizer Step: Rank performs the optimizer step (updating parameters) strictly on its assigned memory range [(r 1)S, rS). Forward Pass: Before the forward computation for bucket can begin, the updated parameter shards must be gathered from all ranks. An All-Gather operation is triggered in the first micro-step to reconstruct the full parameters for that bucket. Similar to the backward pass, this communication is overlapped with the forward computation of the previous bucket. B.2. Tensor Parallelism While Data Parallelism distributes the batch dimension, Tensor Parallelism (TP) reduces the memory footprint of activation and parameters by partitioning the model execution itself across multiple devices. Megatron introduces an efficient TP scheme tailored for Transformer architectures, splitting individual weight matrices across multiple GPUs. Specifically, Megatron utilizes two splitting strategies: Column Parallelism and Row Parallelism. In Multi-Head Attention (MHA) or Feed-Forward Network (FFN) block, the first linear layer is typically column-parallelized (splitting the output dimension), while the second linear layer is row-parallelized (splitting the input dimension). This arrangement allows the output of the first layer to be fed directly into the second layer without synchronization, requiring only single All-Reduce communication operation in the forward pass (and one in the backward pass) per block. TP is orthogonal to DP; typically, DP is applied across nodes while TP is applied within node to minimize the latency of the frequent All-Reduce operations. B.3. Matrix-based Optimizers Compatibility with Element-wise Optimizers. This rigid partition scheme works seamlessly for standard optimizers like AdamW (Loshchilov & Hutter, 2017) or SGD (Ruder, 2016). These optimizers are element-wise: the update for parameter scalar depends solely on its gradient and its historical states (e.g., momentums m, v). t+1 = Update(w(i) w(i) , g(i) , m(i) , v(i) ) (4) Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Since the operations on the i-th element are independent of the j-th element, cutting parameter tensor at an arbitrary index does not affect mathematical correctness. The DistributedOptimizer can essentially treat the entire model as one giant 1D vector without awareness of tensor boundaries. Matrix-based Optimizers. Recent advances in optimization have introduced algorithms that move beyond element-wise updates to utilize second-order information or structural properties of the weight matrices. We refer to these as Matrix-based Optimizers. Examples include Muon (Jordan et al.), which applies Newton-Schulz iterations to orthogonalize weight matrices, and Shampoo (Gupta et al., 2018), which utilizes preconditioning matrices derived from the Kronecker product of gradients. Unlike element-wise, these optimizers operate at tensor granularity. For parameter Rdindout, the update rule typically involves MatrixOp like Singular Value Decomposition (SVD) or Matrix Multiplication: Wt+1 = Wt η MatrixOp(Wt, Wt) (5) This imposes an Atomicity Constraint: the optimizer requires access to the complete dimensions of the tensor to perform the update. The arbitrary slicing performed by the standard Megatron param and grad bufferwhich might cut single parameter across two different ranksrenders the local computation of MatrixOp impossible without expensive on-the-fly communication to reconstruct the tensor. Consequently, adapting DP or TP for matrix-based optimizers requires partition strategy that respects parameter boundaries. C. More Experiments C.1. Full Experiment Setup Models and Optimizers. We evaluate the scalability and efficiency of our framework using the Qwen3 (Yang et al., 2025) model family, scaling from 1.7B to 32B parameters. To verify the generality of our approach across different matrix-based algorithms, we conduct experiments with three representative second-order optimizers: Muon (Jordan et al.), Shampoo (Gupta et al., 2018), and SOAP (Vyas et al., 2024). Most of the experiments primarily report the detailed performance analysis using Qwen3-32B trained with the Muon optimizer, unless otherwise specified. The training configuration utilizes batch-size per DP rank of 1 and sequence length of 4096. Baselines and Methods. We compare three distinct optimization strategies to demonstrate the effectiveness of our decoupled load-balancing framework: SC (Synchronous Compute, Baseline, Paradigm 1 in Section 3.1): Represents the naive practice (e.g., DDP for DP and All-Gather for TP). It employs naive non-partitioning (DP-SC) and synchronous collective communication (TP-SC), resulting in redundant computation and blocking synchronization. NV-layerwise (Baseline, Paradigm 2 in Section 3.1): NVIDIAs layerwise optimizer implementation. It assigns optimizer states at the granularity of whole layers to respect tensor boundaries. However, as analyzed in Section 3 and Appendix D.2, this approach introduces Geometric Incompatibility between task assignment and parameter partition. This mismatch precludes efficient bucket-based Reduce-Scatter overlapping and necessitating expensive All-Reduce operations. ASC (Asynchronous Compute, Paradigm 3 in Section 3.1): Adopts our proposed decoupled architecture to enable asynchronous execution (DP-ASC + TP-ASC) but utilizes naive partitioning without load-aware scheduling. LB-ASC (Load-Balance Asynchronous Compute, Our Frameworks Core Strategy): The complete implementation of our proposed strategies (DP-LB-ASC + TP-LB-ASC), which integrates the α-Balanced Static Partitioning for DP and the Greedy LPT scheduling for TP to minimize stragglers. Evaluation Metrics. We analyze system performance using two primary metrics: 1. Runtime Breakdown: We report the wall-clock time for different training phases. To provide comprehensive performance context, we include the Fwd-Bwd time (Forward and Backward pass) and standard AdamW optimizer time 13 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Figure 6. End-to-End Performance Comparison with NVIDIA layerwise optimizer. We compare the breakdown of step latency between layerwise optimizer (NV-layerwise) and our proposed framework across the Qwen3 model family (ranging from 1.7B to 32B parameters) under various parallelism configurations. (Top Row) Results for smaller models (1.7B, 4B) showing consistent latency reduction. (Bottom Row) Results for larger models (14B, 32B) where the computational complexity of the optimizer increases. Our method demonstrates robust scalability, with the performance gap widening as the model size and optimizer complexity grow, primarily due to the significant reduction in Optimizer Time (Purple bar). 14 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers as references. The primary metric for comparison is the Target Optimizer time (e.g., Muon step latency). All reported timings are averaged over 10 runs and 10 steps within each run to ensure stability. 2. Load-Balance Ratio: To quantify the severity of the long-tail straggler problem, we define the Load-Balance Ratio (RLB) for both memory footprint and computational FLOPs: RLB = maxr(r) avgr(r) (6) where represents the metric value (Peak Memory or FLOPs) on rank r. An ideal balanced system yields RLB 1.0, while higher ratio indicates severe imbalance. C.2. Full Performance Comparison with layerwise optimizer To rigorously evaluate the superiority of our proposed framework over the current state-of-the-art solution for atomicity preservation, we conduct comprehensive comparison against NVIDIAs layerwise optimizer on cluster with up to 256 GPUs. Figure 6 presents the step latency breakdown across the entire Qwen3 model family (1.7B, 4B, 8B, 14B, and 32B) under various Data Parallel (DP) and Tensor Parallel (TP) configurations. Consistent Superiority. The results demonstrate that our proposed framework consistently outperforms NV-layerwise in every tested configuration. The total iteration time reduction ranges from significant to dramatic, driven primarily by the optimization of the optimizer step (Purple bar). For instance, in the Qwen3-32B (DP16-TP8) setting, our method reduces the specific optimizer latency by approximately 8.3x compared to NV-layerwise. Robustness to Model Size. key observation is that the performance gap widens as the model size increases. For smaller models like Qwen3-1.7B, the absolute time savings are noticeable but bounded by the relatively low computational cost of the optimizer. However, as we scale to Qwen3-32B, the matrix-based operations become computationally dominant. NV-layerwise, constrained by expensive collective All-Reduce and extra All-gather, exposes this latency fully. In contrast, our asynchronous pipeline effectively hides these expensive communication operators, resulting in more pronounced speedup for larger models. Robustness to Parallelism Strategies. We further observe that our advantage remains robust regardless of the specific parallelism split (e.g., comparing DP16-TP8 vs. DP32-TP4). While NV-layerwise shows sensitivity to the communication patterns induced by different TP sizes, our static partitioning and micro-group scheduling adapt to the changing topology, maintaining high throughput. This confirms that our load-balancing algorithms are agnostic to the specific dimension of parallelism employed. Figure 7. Verification of Communication Efficiency in Forward-Backward Pass. We compare the Fwd-Bwd step latency of our framework and the NV-layerwise against two controlled baselines: AdamW with All-Reduce (simulating the heavy communication volume necessitated by layer-wise partitioning) and Reduce-Scatter (representing the optimal ZeRO-1 communication volume). The results show that the NV-layerwise (blue) consistently aligns with the AdamW All-Reduce baseline (teal solid), confirming that its performance is bottlenecked by the 2x communication volume of All-Reduce operations despite overlapping. Conversely, our method (orange) closely tracks the AdamW Reduce-Scatter baseline (teal hatched), confirming that our static partitioning strategy successfully preserves the efficient communication capabilities of the underlying Megatron framework. 15 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Supplementary Experiment of Fwd-Bwd Communication Efficiency To quantitatively verify the effectiveness of communication overlap during the Forward-Backward (Fwd-Bwd) pass, we designed comparative experiment. Since isolating precise communication and computation times within the complex pipelining of Fwd-Bwd is challenging, we established two reference baselines using the standard AdamW optimizer training in Megatron: (1) AdamW (All-Reduce, DDP), where gradients are collected via overlapping All-Reduce, representing the latency upper bound; and (2) AdamW (Reduce-Scatter, ZeRO-1), where Reduce-Scatter communication is optimally overlapped with computation (standard ZeRO-1 behavior), representing the theoretical lower bound. Figure 7 demonstrates that our proposed framework successfully preserves the efficient bucket-based communication mechanism, achieving Fwd-Bwd latencies that closely track the AdamW (Reduce-Scatter) baseline. In contrast, the layerwise optimizer exhibits latencies matching the AdamW (All-Reduce) baseline, confirming that its violation of ZeRO-1 geometric constraints prevents effective hiding of communication overhead. We note that in certain configurations, our Fwd-Bwd time is marginally higher than the ideal AdamW (Reduce-Scatter) setting. This minor discrepancy arises because our load-balancing strategy necessitates variable-sized parameter chunks (Section 3.3), introducing slight communication imbalances compared to AdamWs perfectly equal chunks. However, this impact is negligible compared to the significant performance penalty of the inefficient All-Reduce primitive. C.3. Scaling Analysis (a) Data Parallelism Size Scaling Analysis (Fix TP=4) (b) Tensor Parallelism Size Scaling Analysis (Fix PP=4 and DP=4) Figure 8. Parallelism Scaling Analysis. We evaluate the scalability of our load balancing algorithms as the degree of parallelism increases. (a) Data Parallelism Scaling: We fix the model (Qwen3-32B, Muon) and scale DP size from 16 to 128. While our proposed ASC suffers from increasing load imbalance (rising load-balance ratio) and slower convergence, our α-Balanced strategy (DP LB-ASC) maintains near-optimal load-balance ratio ( 1.0) and stable optimizer time. (b) Tensor Parallelism Scaling: Similarly, scaling TP size from 2 to 8 reveals that our Micro-Group Scheduling (TP LB-ASC) effectively neutralizes the straggler effect that plagues the non-load-balanced ASC. We investigate the scalability of our approach on large-scale cluster of up to 512 GPUs from two dimensions: system size (Parallelism Scaling) and workload complexity (Model Size Scaling). Parallelism Scaling (Figure 8). We fix the model size (Qwen3-32B) and scale the number of GPUs. 16 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers (a) Model Size Scaling with DP-load-balance Analysis (b) Model Size Scaling with TP-load-balance Analysis Figure 9. Model Size Scaling Analysis. We evaluate the robustness of our load balancing algorithms across different model sizes (1.7B to 32B). (a) Data Parallelism: Larger models introduce greater parameter heterogeneity, causing the load-balance ratio of the baseline (ASC) to degrade significantly. Our LB-ASC strategy maintains consistent balance. (b) Tensor Parallelism: The load imbalance fluctuates based on the specific hidden dimension alignment, yet our Greedy Scheduling consistently optimizes the packing. Data Parallelism (Figure 8a): With TP fixed at 4, as we scale the DP size from 16 to 128, the statistical variance in parameter sizes naturally leads to severe load imbalance in naive strategies. The ASC baseline exhibits linear degradation in the load-balance ratio for both memory and FLOPs, leading to increased optimizer time due to stragglers. Conversely, our DP LB-ASC (α-Balanced) strategy maintains load-balance ratio close to the ideal 1.0, effectively neutralizing the straggler effect even at large scale (128 DP ranks). Tensor Parallelism (Figure 8b): Similarly, with PP fixed at 4 and DP fixed at 4, increasing TP size typically exacerbates the fragmentation of weight matrices. The baseline shows sharp increase in computational imbalance. Our MicroGroup Scheduling (TP LB-ASC) successfully mitigates this, keeping the FLOPs load-balance ratio significantly lower than the baseline, thereby preserving low optimizer latency as the system scales. Model Size Scaling (Figure 9). We analyze how parameter heterogeneity affects load balancing as models grow from 1.7B to 32B. For these experiments, we fix the parallelism configuration to DP=16 and TP=4. DP Load Balance (Figure 9a): Larger models typically contain wider variance of tensor shapes (e.g., larger embedding layers vs. smaller projection heads). This heterogeneity causes the naive partitionings load-balance ratio to increase significantly with model size. Our LB-ASC strategy adapts to this heterogeneity, maintaining flat LB profile. TP Load Balance (Figure 9b): Interestingly, for TP, the baseline imbalance does not strictly increase with model size but fluctuates based on the specific architecture (e.g., hidden dimension size). However, our greedy scheduling algorithm consistently finds near-optimal packing, ensuring that our performance advantage is sustained across all model sizes. 17 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers (a) Efficiency Validation with Qwen3-14B (b) Precision Validation with Qwen3-1.7B Figure 10. Generality Validation with Shampoo Optimizer. (a) Efficiency: Comparison of step time for Qwen3-14B using the Shampoo optimizer. The results demonstrate that our proposed LB-ASC strategy is equally effective for Shampoo, achieving drastic speedup compared to both the synchronous (SC) and asynchronous (ASC) methods. (b) Precision: Training loss curves for Qwen3-1.7B confirm that our system optimizations do not alter the convergence trajectory compared to the synchronous baseline. (a) Efficiency Validation with Qwen3-14B (b) Precision Validation with Qwen3-1.7B Figure 11. Generality Validation with SOAP Optimizer. (a) Efficiency: Step time comparison for Qwen3-14B trained with the SOAP optimizer. Similar to the Shampoo results, our framework generalizes well to SOAP, significantly reducing step latency. (b) Precision: The loss curve for Qwen3-1.7B confirms that our implementation introduces no algorithmic deviation, ensuring mathematical equivalence with standard synchronous training. C.4. Generalization to Other Optimizers (Shampoo & SOAP) To verify the generality of our framework beyond Muon, we evaluate its performance on Shampoo and SOAP, two other representative matrix-based optimizers. These algorithms involve distinct cubic-complexity operations (SVD and Eigendecomposition, respectively), serving as robust test cases for our systems adaptability. Efficiency (Figures 10a & 11a). These experiments are conducted on 256 GPUs. Due to the significant memory overhead of Shampoo and SOAP preconditioners, we utilize Qwen3-14B with configuration of PP=2, DP=32, and TP=4 to accommodate the state requirements. The matrix operations in Shampoo and SOAP incur high computational costs, as evidenced by the high latency in the SC baseline (e.g., Shampoo step takes 3.313s). The experimental results confirm that our proposed framework remains highly effective in this context. By applying our unified partitioning and scheduling strategy, we reduce the optimizer step time to 0.110s for Shampoo and similarly for SOAP. This represents speedup of over 30x, demonstrating that our framework can successfully handle diverse matrix-based workloads without requiring algorithm-specific tuning. Precision (Figures 10b & 11b). We validate correctness by training Qwen3-1.7B model for 400B tokens using DP=8, TP=4 configuration. The loss curves for both Shampoo and SOAP under our framework overlap perfectly with the standard 18 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers synchronous baseline. It is important to emphasize that LB-ASC strategy is purely system-level optimization. Unlike methods that approximate curvature or skip updates to save time, our approach strictly adheres to the original optimizers mathematical definition. Consequently, there is zero mathematical precision loss, and the convergence behavior is identical to that of the baseline. Load Balance (Figure 12). The load distribution analysis for Shampoo / SOAP further corroborates the effectiveness of our proposed LB-ASC strategy. Despite the different computational characteristics of Shampoo compared to Muon, our scheduler successfully flattens the workload variance (reducing the FLOPs load-balance ratio from > 2.0 to 1.05), ensuring efficient hardware utilization across all ranks. (a) Data Parallelism Load Balance Ratio Figure 12. Load Balance Analysis for Shampoo/SOAP. Visualization of the load distribution (Memory and FLOPs) across ranks for the Shampoo optimizer. Similar to the Muon results, the naive partitioning (Left) leads to significant computational bubbles and memory peaks, while our proposed load-balancing strategy (Right) effectively flattens the distribution, achieving near-perfect load-balance ratio. (b) Tensor Parallelism Load Balance Ratio C.5. DP Load Balance α Analysis We conduct an ablation study on 128 GPUs (configured as PP=8, DP=16) to determine the optimal value of the control parameter α in our α-Balanced Greedy LPT algorithm (Section 3.2). Recall that α 0 prioritizes uniform communication sizes (matching standard ZeRO), while α 1 prioritizes balanced optimizer computational load. As shown in Figure 13, as α increases from 0.0 to 1.0, the Muon time (Purple bar) decreases monotonically. This confirms that computational stragglers are the primary bottleneck during the optimizer step. Crucially, while increasing α introduces slight non-uniformity in the communication bucket sizes (potentially affecting Fwd-Bwd time), the Fwd-Bwd time (Green bar) remains relatively stable. This stability suggests that the efficient communication overlapping in Megatron effectively masks the minor communication imbalances introduced by our sizing strategy. Consequently, setting α = 1.0 yields the best end-to-end performance, Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Figure 13. Sensitivity to DP Load-Balance Factor α. We analyze the impact of the control parameter α (ranging from 0.0 to 1.0) on the total step latency for Qwen3-32B. α = 0 prioritizes communication size uniformity, while α = 1 prioritizes optimizer computational load balance. The results indicate that setting α = 1.0 yields the lowest total time, suggesting that computational stragglers in the optimizer step are the dominant bottleneck, while communication imbalance in the Fwd-Bwd pass is effectively hidden by overlap. justifying our design choice to prioritize computational load balancing in the final implementation. C.6. TP Communication Fusion Analysis We analyze the impact of the Micro-Group Fusion strategy for Tensor Parallelism (Section 4.1) using 128 GPUs with DP=16 and TP=8. We vary the maximum capacity constraint (Cmax) of the micro-groups, which dictates the buffer size for the fused All-to-All communication. Figure 14 illustrates the Optimizer Time as function of Cmax. The No-Fuse baseline, which performs communication for each tensor individually, suffers from high latency ( 0.11s) due to the overhead of launching many small kernels and poor bandwidth utilization. Enabling fusion immediately drops the latency to 0.073s. We observe that performance improves slightly as Cmax increases, stabilizing around 512MB. Beyond this point (1024MB, 2048MB), the latency plateaus, indicating that the All-to-All bandwidth is fully saturated. This result confirms that our fusion strategy is essential for efficiency and that the system is robust to the specific choice of Cmax provided it is sufficiently large to saturate the interconnect. Figure 14. TP Micro-Group Fusion. We evaluate the impact of fusing tensor updates into Micro Groups with varying capacity constraints (Cmax). No-Fuse represents treating each tensor individually. The results demonstrate that fusing communication significantly reduces optimizer time by saturating All-to-All bandwidth. Performance plateaus once Cmax exceeds 1024MB, indicating the optimal granularity for overlap. 20 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers D. Supplementary Explanation D.1. Load-Balance Algorithm Latency It is worth noting that the load-balancing algorithms described in Section 3.2 and Section 4.2 are executed strictly as one-time offline planning step during model initialization. Consequently, they introduce almost zero overhead to the subsequent training iterations. Furthermore, the computational cost of this initialization phase itself is negligible: both algorithms are designed as efficient greedy heuristics with time complexity dominated by sorting (O(N log )), where is the number of parameter tensors or buckets (typically in the scale of thousands). In our large-scale experiments with Qwen3-32B on 256 GPUs, the offline planning phase completes in milliseconds. This latency is orders of magnitude smaller than the heavy system initialization procedures, such as CUDA context creation, memory allocation, and NCCL communicator setup (which typically span tens of seconds to minutes). Consequently, the scheduling overhead is effectively imperceptible and cannot be distinguished from system noise during the model initialization stage. Given that this overhead is effectively imperceptible and indistinguishable from system noise, we omit specific latency experiments for the load-balancing phase in our reported results. D.2. The Geometric Incompatibility of Layer-wise Partitioning Figure 15. Visualization of Geometric Incompatibility. (Left) Ours: Our approach respects both Atomicity and the strict Geometric Constraints of ZeRO-1 primitives. By optimizing slice sizes (shifting boundaries) inside bucket without scrambling the physical parameter sequence, we preserve the contiguous bucket abstraction. This alignment enables the use of the optimal Reduce-Scatter primitive. (Right) Layer-wise: Strategies relying on Global Load Balancing (e.g., Global LPT) assign tasks purely by weight (e.g., assigning heavy parameter P3 to DP1) regardless of their physical location. This Data-Task Mismatch (visualized by the interleaved DP1/DP2 assignments in the bucket view) breaks bucket coalescing. Consequently, under Megatron ZeRO-1s fixed geometric slicing and bucket coalescing assumption, the system must abandon bucket-based Reduce-Scatter in favor of bucket-based All-Reduce or perparameter-based Reduce-Scatter, incurring 2x communication volume and higher latency. (Note: The visualization of the third bucket (P6, P7, P8, P9) in the right sub-figure is omitted here because its load-based assignment (P6 DP1, {P7, P8, P9} DP2) coincidentally aligns with the geometric rank order (monotonic), thus not violating the ZeRO-1 Geometric Constraint.) Mechanism (The Hybrid ZeRO-1). Existing layer-wise strategies (e.g., NVIDIAs layerwise optimizer) operate as hybrid system to reconcile atomicity with distribution. During the Forward and Backward passes, they function in DDP mode and utilize standard All-Reduce (incurring 2 communication volume compared to ZeRO-1s ReduceScatter) to synchronize gradients. However, for the Optimizer Step, they shard the optimizer states and employ Global Load Balancing strategy (Global LPT) to assign the ownership of optimizer states. This assignment is based strictly on computational weights (e.g., placing heavy layers on specific ranks) to equalize execution time, disregarding the physical memory layout of the parameters. The Conflict (ZeRO Geometric Incompatibility in Megatron). This design introduces fundamental conflict between the logical task assignment and the physical communication primitives used in modern training stacks like ZeRO-1. 21 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers ZeRO-1 Primitive (Position-Based): The efficient Reduce-Scatter primitive (Appendix B) relies on strict Geometric Partitioning logic (Figure 15 (Right)). For contiguous bucket of size across ranks, the primitive rigidly dictates that the r-th geometric slice (interval [ r1 B, B)) is sent to Rank r. The destination is determined solely by the datas physical position in the buffer. Layer-wise Logic (Weight-Based): In contrast, the layer-wise allocator assigns parameter update tasks based on load. As illustrated in Figure 15 (Right), heavy parameter such as P3 might be assigned to DP1 to balance the global workload, even though its physical position in the buffer falls within region that geometrically belongs to or is interleaved with DP2. This creates clear Data-Task Mismatch. The Consequence: The Lose-Lose Dilemma. Due to this data-task mismatch, layer-wise approaches are forced to choose between two suboptimal communication paths: Option A: The Bandwidth Penalty (Forced All-Reduce). This is the default choice for current layerwise optimizer implementations. Since the system cannot coalesce the geometrically-fragmented task assignments into single geometry-based Reduce-Scatter, it defaults to All-Reduce. This broadcasts full gradients to all ranks, ensuring that every rank receives the necessary data regardless of its assignment. However, this incurs 2x the communication volume compared to ZeRO-1s Reduce-Scatter, significantly increasing the bandwidth requirement. Option B: The Latency Penalty (Forced Reduce-Scatter). Alternatively, one might attempt to enforce Reduce-Scatter (sending only the specific parameter data to its assigned rank). However, this fundamentally breaks the Bucket Coalescing abstraction. Because the destination ranks are no longer geometrically alignedvisualized in Figure 15 as the interleaved assignment sequence P1(DP1) P2(DP2) P3(DP1)the system cannot launch single monolithic kernel for the entire bucket. Instead, it must dismantle the bucket and trigger discrete, per-parameter communication kernels to route specific tensors to their arbitrary destinations. This introduces massive kernel launch overhead and prevents the system from saturating the interconnect bandwidth, shifting the bottleneck from bandwidth to latency. Compounded Penalty on Parameter Update. Crucially, this geometric violation extends beyond gradient synchronization to parameter reconstruction. Under ZeRO-1 geometric constraints, the updated parameters are gathered via bucket-based All-Gather that is efficiently overlapped with the Forward pass computation. However, because layer-wise partitioning misaligns the ownership of the updated parameters with their geometric shards, standard coalesced All-Gather is impossible. Consequently, implementations are forced to either: (1) perform an explicit All-Gather or Broadcast within the Optimizer Step to redistribute weights (adding significant exposed latency, as observed in our baseline experiments); or (2) resort to inefficient per-parameter All-Gather operations during the Forward pass, which destroy the overlap efficiency. In summary, our framework outperforms layer-wise approaches because our α-Balanced Partitioning respects not only the Atomicity Constraint but also the Geometric Constraints of ZeRO-1 primitives. By optimizing slice sizes without breaking the sequential rank ordering, we retain the efficient, coalesced Reduce-Scatter capability in backward, and the corresponding overlapped, coalesced All-Gather in forward, that layer-wise strategies are structurally forced to abandon. D.3. Detailed Walkthrough of Load-Balancing Algorithms In this section, we provide rigorous walkthrough of the two core scheduling algorithms proposed in the paper. We explain the mathematical intuition behind the variable definitions and the algorithmic decisions. Algorithm 1: α-Balanced Static Partitioning (DP) The core challenge in DP is determining the cut points si,r for bucket Bi such that we balance two objectives: DP Computational Load vs. Fwd-Bwd Bucket Communication Uniformity. Intuition of the Control Parameter α: The algorithm constructs Target Allocation Vector for the current bucket by blending two basis vectors: 22 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers else item Imeta[idx] if Lmax Cmax then {Simulation Step: Try to solve partition for current candidate list} {Valid: Item fits. Continue accumulating.} idx idx + 1 Algorithm 3 Micro-Group Construction with Greedy Rollback 1: Input: Parameters P, Shapes S, Capacity Cmax, Ranks 2: Output: List of Micro Groups Mopt where each Mopt contains assignments {(p, rank)} 3: {Phase 1: Deterministic Global Sort} 4: Imeta [(Cost(p), p, s) for p, in zip(P, S)] 5: Sort Imeta descending by Cost 6: Mopt [] 7: Mcurr [] {Current candidate items} 8: idx 0 9: while idx < Length(Imeta) do 10: 11: Mcurr.append(item) 12: 13: Assignments, Lmax MINHEAPSOLVER(Mcurr, R) 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end while 31: {Handle remaining items} 32: if Mcurr is not empty then 33: 34: Mopt.append(F inalAlloc) 35: end if 36: Return Mopt end if {Finalize the previous valid group} inalAlloc, MINHEAPSOLVER(Mcurr, R) Mopt.append(F inalAlloc) {Start new group with the current item} Mcurr [] {Do not increment idx; retry item in next iteration} {Constraint Violated: Trigger Rollback} Mcurr.pop() {Remove the item that caused overflow} if Mcurr is empty then inalAlloc, MINHEAPSOLVER(Mcurr, R) Error: Single item exceeds Cmax. end if vf ill (The History Basis): Represents compensatory strategy. It assigns more work to ranks that are currently underloaded (have load deficit) based on previous allocations. veven (The Geometry Basis): Represents perfectly uniform partition based on data size (1/R). This corresponds to the standard ZeRO-1 strategy. The parameter α [0, 1] controls this blend. If α = 1, the algorithm aggressively fills deficits (prioritizing compute balance). If α = 0, it ignores history (prioritizing communication uniformity). Step-by-Step Execution: 1. Global Sorting (LPT Rule): We process buckets in descending order of their total cost W. This Longest Processing Time (LPT) rule is crucial because large rocks (heavy buckets) are harder to balance and should be placed when the deficit voids are most flexible. 2. Deficit Calculation: At step t, we calculate the current load vector L. The mean load µ represents the ideal average. Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Algorithm 4 Subroutine: MinHeapSolver (LPT) 1: Input: Items I, Ranks 2: Output: Allocation A, MaxLoad Lmax 3: Sort descending by Cost {Local LPT Sort} 4: PriorityQueue [(0, r) for 0..R 1] {Min-Heap of (load, rank)} 5: [[] for 0..R 1] 6: for each item (c, p, s) in do 7: min load, rbest heapq.heappop(P Q) 8: A[rbest].append((p, s)) 9: 10: 11: end for 12: Lmax max(l,r)P Q(l) 13: Return A, Lmax new load min load + heapq.heappush(P Q, (new load, rbest)) The deficit vector = max(0, µ Lr) identifies which ranks are lagging behind the ideal average. 3. Target Construction: We calculate fill proportion vector vf ill = d/ (cid:80) d, which dictates how much of the current bucket should be given to each rank to help them catch up. The final target proportion is = (1 α)veven + αvf ill. 4. Atomic Discretization: The target vector gives us continuous ideal load. We iterate through the parameter list and place the cut point strictly at parameter boundaries to minimize the distance to the target cumulative sum, thereby enforcing the Atomicity Constraint. Algorithm 2: Micro-Group Scheduling with Greedy Rollback (TP) For TP, we must partition stream of tensor updates into discrete Micro Groups (for fused communication) and assign them to Host Ranks (for computation). This is modeled as Bin Packing Problem with an embedded Multiprocessor Scheduling Problem. Based on our implementation, we define Two-Level Greedy Strategy detailed in Algorithm 3 and Algorithm 4. Algorithmic Logic Explanation: 1. Global vs. Local Sorting: We perform Global Sort (Line 5, Alg 3) of all parameters first. This ensures that we attempt to pack the largest, most difficult tensors into groups first, preventing straggler tensors from accumulating at the end. Inside the MINHEAPSOLVER (Line 3, Alg 4), we perform Local Sort as part of the standard Longest Processing Time (LPT) heuristic, which provides reliable approximation for makespan minimization. 2. The Simulation & Rollback Mechanism: The core innovation is the feedback loop between the Bin Packing (Outer loop) and the Scheduling (Inner solver). Instead of estimating the cost of group by simple summation ((cid:80) Cost/R), we run the actual scheduling simulation (MINHEAPSOLVER) at every step (Line 13, Alg 3). This calculates the exact makespan Lmax given the current items. If adding new item causes Lmax to exceed Cmax (due to fragmentation or imbalance), we strictly enforce the limit by triggering Rollback (Lines 18-26). The current group is finalized, and the overflow item becomes the seed for the next group. 3. Efficiency: Although simulating the solver at every step seems computationally intensive, the MINHEAPSOLVER operates in O(K log R) for items. Since the number of items per Micro Group (K) is small (typically < 50), this overhead is negligible during the offline planning phase. D.4. Discussion on the Choice of Training Framework In the realm of large-scale distributed training, Megatron (Shoeybi et al., 2019) and PyTorch FSDP (Fully Sharded Data Parallel) (Zhao et al., 2023) represent the two dominant frameworks. In this work, we chose to implement and evaluate our strategies primarily within Megatron for two key reasons: (1) it remains the de facto standard for training massivescale foundational models (e.g., GPT-3, Qwen) due to its highly optimized tensor parallelism and pipelining capabilities; and (2) its reliance on strict geometric partitioning provided the ideal testbed to demonstrate how our Static Partitioning 24 Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers respects ZeRO-1 Geometric Constraints to preserve coalesced communication overlapping, feature structurally abandoned by other atomicity-preserving methods. However, the principles of our proposed framework, particularly the Load-Balanced Asynchronous Compute (LB-ASC), are not limited to Megatron and can be generalized to FSDP. From high-level system abstraction, the optimizer step in FSDP shares fundamental structural similarities with the Tensor Parallelism (TP) challenge we address in Section 4. Specifically, both FSDP and TP fragment parameters across devices. To perform matrix-based optimizer update (which requires strict atomicity), both paradigms necessitates an All-Gather operation to reconstruct the full parameter, followed by the computation, and shard update. Therefore, our Micro-Group Scheduling and Asynchronous Compute pipeline are directly transferable to FSDP. By treating FSDP shards as logical units similar to TP splits, our load-balancing algorithm can similarly effectively neutralize stragglers and maximize throughput in an FSDP-based training stack. It is crucial to note that adapting our TP LB-ASC strategy to FSDP requires distinct handling depending on the specific ZeRO stage employed. FSDP-ZeRO1 Scenario: In this configuration, gradients are typically not sharded (they are replicated across ranks within the sharding group). Since our TP LB-ASC primarily serves as gradient reconstruction mechanism, the All-to-All communication depicted in Figure 2 can be skipped. FSDP-ZeRO2/3 Scenario: Here, gradients are sharded across devices. Consequently, the full execution flow of our TP LB-ASC (including the fused All-to-All reconstruction) is necessary within the optimizer step to assemble the required data. Communication Overhead and Engineering Complexity in Hybrid Parallelism: Our choice is driven by the distinct communication characteristics of Tensor Parallelism (TP) versus Data Parallelism (DP). TP typically leverages highbandwidth intra-node interconnects (e.g., NVLink); thus, while parameter sharding is unavoidable, performing reconstruction communication within the TP group during the optimizer step is acceptable. In contrast, DP communication usually spans lower-bandwidth inter-node networks, making any synchronization within the DP group prohibitively expensive. (Notably, this bottleneck constitutes the primary drawback of the layerwise optimizer baseline, which necessitates an expensive All-Reduce and an additional All-Gather operation over these slow links during the step.) significant challenge arises when superimposing TP onto FSDP-ZeRO2/3: this hybrid setting necessitates coupled 2D (DP+TP) communication mesh to reconstruct tensors fragmented across both dimensions. This not only forces expensive synchronization over the slower inter-node DP links but also introduces substantial engineering complexity to manage the hybrid topology. Megatrons architecture avoids this by inherently decoupling DP and TP, enabling us to isolate synchronization strictly within the high-speed TP domain while keeping the DP dimension communication-free (via ZeRO-1). D.5. Discussion about Load-Balance Generalization with Non-linear Cost While our framework mathematically supports arbitrary cost functions W(p) to handle the cubic complexity of matrix-based optimizers, in our practical implementation and experiments, we adopted the simplified linear metric W(p) = numel(p) primarily to enforce the Unified design philosophy of our framework.We justify this choice based on three key rationales: Optimizer-Agnostic Universality: core contribution of this work is decoupling the system architecture from the specific optimization algorithm. By using numel as the universal cost metric, our partitioner remains strictly agnostic to the optimizers internal complexity (whether O(N ) or O(N 3)), ensuring that the same infrastructure seamlessly supports Muon, Shampoo, SOAP, and future algorithms without modification. Shape-Cost Correlation: For standard Transformer architectures (e.g., Qwen), parameter tensors typically fall into grouped shapes where computational cost correlates strongly with parameter count, making numel an effective proxy. 25 Figure 16. Cost Metric Ablation. Comparison of Muon step latency using Numel vs. exact FLOPs cost function. The difference is negligible ( 104s). Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Implementation Robustness: This simplification avoids the engineering fragility of maintaining distinct cost models for every new optimizer, reducing system complexity while retaining high loadbalancing efficiency as demonstrated in our experiments. To quantitatively validate the effectiveness of this simplification, we conducted controlled experiment using the Qwen3-32B model with parallelism configuration of DP=16 and TP=8. We compared the execution makespan of the optimizer step when scheduling with the exact computational complexity (W(p) = FLOPs) versus the simplified parameter count (W(p) = numel(p)). As illustrated in Figure 16, the performance difference is negligible: using FLOPs yields latency of 0.0717s, while using Numel yields 0.0718s. The discrepancy is approximately 104s. In the context of our broader results in Section C, where the performance gaps between different methods typically range from 102s to 101s, this minute deviation confirms that the shape-based cost metric (numel) serves as highly accurate proxy for load balancing. It captures the dominant workload characteristics of Transformer blocks without introducing the engineering overhead of calculating exact FLOPs for every tensor. E. Extended Related Work To situate our contributions, we classify existing distributed optimization strategies into three categories: (1) System-Level Compromises (e.g., Layer-wise Partitioning), which preserve mathematical exactness but sacrifice pipeline efficiency; (2) Algorithmic Approximations (e.g., Blocking/Shard-local), which sacrifice mathematical fidelity to fit system constraints; and (3) our proposed Canzona, which reconciles both without compromise. E.1. The Fundamental Conflict: Matrix-based Optimizers vs. Distributed Atomicity Modern large-scale training relies on Geometric Sharding (e.g., ZeRO-1, Tensor Parallelism ( B)) to distribute memory. Frameworks like Megatron flatten parameters into contiguous param and grad buffer and slice them uniformly (e.g., into 1/N shards) regardless of tensor boundaries. The Conflict: Matrix-based optimizers (e.g., Muon (Jordan et al.), Shampoo (Gupta et al., 2018), SOAP (Vyas et al., 2024), Conda (Wang et al., 2025), ROOT (He et al., 2025), PSGD (Li, 2017), Sophia (Liu et al., 2023), K-FAC (Martens & Grosse, 2015; Osawa et al., 2019), and (Xie et al., 2026)) are inherently non-separable. They require holistic access to the full weight matrix Rdindout to perform operations like SVD or Newton-Schulz iterations. The Gap: Standard geometric sharding violates this Atomicity Constraint. single weight matrix is often fragmented across multiple ranks, making local execution of these mathematical operations impossible without costly reconstruction. E.2. System-Level Attempts: Exactness at the Cost of Efficiency To preserve the mathematical exactness of the optimizer, recent industrial implementations have adopted coarser-grained partitioning. Layer-wise Partitioning (e.g., NVIDIAs layerwise.optimizer, Distributed Shampoo (Shi et al., 2023)). This approach assigns optimizer states at the granularity of whole layers to ensure atomicity. While mathematically correct, it introduces severe system-level bottlenecks driven by the Geometric Incompatibility (detailed in D.2): 1. Structural Fallback to All-Reduce: Because the logical assignment conflicts with the geometric slicing of ZeRO primitives, the system cannot coalesce parameters into single Reduce-Scatter kernel. It is structurally forced to fallback to All-Reduce operations (2 communication volume) and an additional Broadcast/All-Gather step during the optimizer update to synchronize the new parameters across ranks (as shown in Figure 4 and 6). E.3. Algorithmic Attempts: Efficiency at the Cost of Fidelity To avoid the heavy communication of exact reconstruction, another line of research modifies the optimizer algorithm itself, introducing approximations. Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Block-Diagonal Approximations (Small-Block Methods). Methods like Distributed Shampoo (Anil et al., 2020) and K-FAC (Osawa et al., 2019) approximate the full preconditioner with block-diagonal matrix (e.g., approximating 4096 4096 correlation with smaller blocks). Limitation: This ignores off-diagonal correlations. While effective for smaller models or specific architectures (e.g., CNNs), this Small-Block assumption may degrade convergence speed or solution quality for Large Language Models where dense correlations matter. Shard-Local Orthogonalization (e.g., MuonBP). Recent proposals like MuonBP (Khaled et al., 2025) attempt to bypass global communication by performing orthogonalization strictly on the local shard of the weight matrix. Limitation: This constitutes Mathematical Approximation. The Local Newton-Schulz update differs from the Global Newton-Schulz update. This creates Directional Drift between the true gradient geometry and the applied update, which can lead to training instability or require frequent, expensive synchronization steps to correct. Low-Rank and Subspace Approximations. To circumvent the prohibitive cost of full-rank matrix operations, recent wave of optimizers proposes constraining the update trajectory to lower-dimensional manifolds to reduce communication and memory overheads. Dion (Ahn & Xu, 2025) introduces distributed orthonormalization technique that replaces global Newton-Schulz iterations with decoupled momentum buffers and low-rank approximations, allowing states to evolve independently on shards without expensive synchronization. Similarly, methods utilizing dynamic subspace selection, such as (Gong et al., 2025), project second-order information onto predefined orthogonal bases of low-rank structures. Limitation: While these approaches drastically improve efficiency, they impose strong structural assumptionseffectively assuming weight updates lie strictly in low-rank subspace. This constitutes fundamental fidelity trade-off : unlike system-level attempts, which support the exact holistic update required by the original algorithm, these methods alter the optimization path, potentially degrading convergence quality in regimes where full-rank curvature information is critical. Overall Limitation: Beyond the potential degradation in convergence fidelity and the need for further verification at scale, these algorithmic approximations fundamentally lack Unified design philosophy. Instead of offering general system solution, they often resort to ad-hoc, algorithm-specific modificationssuch as tailoring strategies strictly for Muon (e.g., MuonBP)which restricts their extensibility to other emerging or future matrix-based optimizers. E.4. Our Solution: Canzona (System-Level Exact, Efficient, and Unified Optimization) Our framework, Canzona, distinguishes itself by simultaneously achieving Zero-Fidelity-Loss, high system throughput, and broad universality. Unlike prior works that compromise on one or more of these axes, our framework delivers comprehensive solution: System-Level Exactness (vs. Algorithmic Approximations): We strictly adhere to the original mathematical definition of the optimizer. By handling the atomicity constraint purely through memory layout and scheduling, rather than altering the update rule (e.g., via blocking or shard-local approximations), we guarantee Zero-Fidelity-Loss, ensuring convergence behaviors identical to single-device baselines. Straggler-Free Efficiency (vs. Layer-wise Partitioning): We address the system bottlenecks across both parallelism dimensions. For Data Parallelism, our α-Balanced Static Partitioning preserves the computation balance during the optimizer update step. By successfully reconciling the Atomicity Constraint with ZeRO-1 Geometric Constraints, it allows us to fully inherit the efficient, coalesced communication overlapping of the standard ZeRO-1 pipeline and zero communication during optimizer step. Simultaneously, for Tensor Parallelism, our Asynchronous Micro-Group Scheduling eliminates the redundancy of synchronous execution. By batching fragmented updates and balancing computational loads, we effectively hide the expensive reconstruction overhead that limits existing layer-wise approaches. Unified and Agnostic Framework (vs. Specific Hacks): Crucially, our framework is designed as Unified framework decoupled from specific optimizer logic. We treat tensor updates as generic computational tasks defined solely by cost metrics (e.g., numel). This makes our system Optimizer-Agnostic, allowing it to support not only Muon but also Shampoo, SOAP, and future matrix-based algorithms without requiring algorithm-specific hacks or re-engineering."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "KAUST",
        "MBZUAI",
        "Peking University"
    ]
}