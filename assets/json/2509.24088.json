{
    "paper_title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems",
    "authors": [
        "Yifan Yu",
        "Moyan Li",
        "Shaoyuan Xu",
        "Jinmiao Fu",
        "Xinhai Hou",
        "Fan Lai",
        "Bryan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 8 8 0 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "CORRECT",
            "content": "CORRECT: CONDENSED ERROR RECOGNITION VIA KNOWLEDGE TRANSFER IN MULTI-AGENT SYSTEMS Yifan Yu University of Illinois UrbanaChampaign Moyan Li Amazon Shaoyuan Xu Amazon Jinmiao Fu Amazon Xinhai Hou University of Michigan Fan Lai University of Illinois UrbanaChampaign Bryan Wang Amazon"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-agent systems (MAS) are increasingly capable of tackling complex realworld tasks, yet their reliance on inter-agent coordination, tool use, and longhorizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, large-scale dataset of over 2,000 annotated trajectories collected through novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multi-agent systems (MAS) have emerged as powerful paradigm for solving complex tasks that require diverse capabilities and collaborative problem-solving, with demonstrated success in various domains, including software development (Qian et al., 2023; Hong et al., 2023; Zhang et al., 2024), scientific research (Lu et al., 2024), web navigation (Zhou et al., 2023), and general-purpose task automation (Wu et al., 2024; Wang et al., 2025). By orchestrating multiple specialized agents, MAS can tackle challenges beyond the reach of single-agent systems (SAS) that rely on single LLMs for task solving, achieving human-level performance on benchmarks like GAIA (Mialon et al., 2023) and SWE-bench (Jimenez et al., 2023). However, as MAS increasingly scale in both complexity (e.g., sophisticated interactions with other agents and tools) and deployments, decisive error recognition in MAS deployments becomes exceptionally challenging. Unlike SAS where failures can be traced to single faulty output, MAS failures often emerge from cascading effects across agents: single error-prone step by one agent can propagate through downstream interactions, ultimately causing task failure (Gao et al., 2025). We refer to this fundamental challenge as decisive error recognition: pinpointing the precise agent and step that first triggered the failure (Zhang et al., 2025). Efficient decisive error recognition is essential for reliable MAS deployments, sustaining service quality, and guiding operational management such as safe agent upgrades, targeted restarts, and service monitoring (Epperson et al., 2025). Unfortunately, decisive error recognition in MAS remains open-ended due to three fundamental obstacles: (1) Generality: MAS span wide spectrum of applications, and even within an application,"
        },
        {
            "title": "CORRECT",
            "content": "requests can exhibit drastically different error patterns, making it difficult to design methods that generalize. Existing advances (Zhang et al., 2025) often resort to LLM-as-a-judge methods for generality, yet achieve 10% accuracy in pinpointing the exact error step, barely above the accuracy of random guessing (3%). Recent efforts (Ge et al., 2025) to improve accuracy through fine-tuning not only hurt generalization, but fall short in (2) Data Efficiency: obtaining labeled data in error recognition is notoriously expensive and inherently ambiguous. For example, in the WHO&WHEN dataset (Zhang et al., 2025), annotators spent over 30 expert hours labeling fewer than 200 trajectories, yet disagreement rates exceeded 50%. This makes any training-based approaches ineffective without voluminous data; and (3) Computation Efficiency: even with sufficient data, tuning LLMs for error recognition may be impractical to catch up with deployments where new error types arise continuously and sporadically, e.g., new attacks in cloud AIOps (Wang et al., 2025). In this paper, we first notice that failures in MAS tend to recur with similar structures across requests (2.2). Because MAS application often relies on the same role specifications, orchestration rules, tool APIs, and verification policies, diverse requests often funnel into common decision skeletons. Our real-world analysis of WHO&WHEN dataset proposed in Zhang et al. (2025) shows that over 80% of failure trajectories have at least one counterpart with 0.8 semantic similarity in error logs. This suggests that error knowledge can be systematically distilled, cached, and reused. However, naive approaches, such as in-context learning (ICL) that insert prior trajectories as in-context exemplars, quickly break down: logs can exceed 32,000 tokens (Yang et al., 2025; Dubey et al., 2024), contain low-entropy noise, and even underperform zero-shot baselines (2.2). These challenges demand new approach: one that is general, data-efficient, and lightweight for practical deployments. In this paper, we propose CORRECT (COndensed eRror RECognition via knowledge Transfer), novel framework that formalizes decisive error recognition as first-class problem for reliable MAS, and automatically distills prior error patterns into compact, reusable schemas that capture their essential signatures, triggering contexts, and propagation patterns. During inference, CORRECT identifies relevant schemas and applies them to the new request, enabling accurate, lightweight, and training-free recognition in dynamic environments. Our contributions are summarized as follows: CORRECT: the first schema-guided detector with broad generality. We introduce CORRECT, the first framework that distills recurrent MAS failures into compact error schemata and reuses them for decisive error recognition. Unlike LLM-as-a-judge approaches that trade accuracy for generality, or costly fine-tuning approaches, CORRECT leverages schemata to transfer knowledge across requests in real time. This design improves step-level localization accuracy up to 20 points over state-of-the-art designs (Zhang et al., 2025), while remaining training-free and computationally efficient, making it practical for deployment in dynamic MAS environments. CORRECT-Error: versatile, large-scale, and high-fidelity dataset for MAS error recognition. To close the benchmarking gap, we construct CORRECT-Error, large-scale collection of over 2,000 multi-agent trajectories with fine-grained, step-level error annotations. Unlike prior datasets that are small and noisy, CORRECT-Error is built using novel error-injection pipeline guided by real-world failure distributions, yielding trajectories that balance controlled coverage with the realism of natural MAS failures. We conduct extensive human validation, confirming strong alignment between synthetic annotations and expert judgment. CORRECT-Error not only enables rigorous evaluation of CORRECT, but also establishes reusable, extensible benchmark for the community to advance the development of MAS error recognition methods. Together, these contributions advance the state of the art in MAS error recognition and establish new foundation and datasets for making MAS more reliable, interpretable, and deployable at scale."
        },
        {
            "title": "2 BACKGROUND AND MOTIVATION",
            "content": "2.1 DECISIVE ERROR IN MULTI-AGENT SYSTEMS Task failures in MAS often arise from specific decisive errors that, once committed, render successful task completion impossible. We formalize this notion following prior advances (Zhang et al., 2025). Consider MAS executing trajectory τ = {(a1, s1), (a2, s2), . . . , (aT , sT )}, where agent ai performs step si. The outcome of the trajectory is denoted by R(τ ) {0, 1}, with 1 for success and 0 for failure. step (ak, sk) in failed trajectory τ is decisive error if replacing it with"
        },
        {
            "title": "CORRECT",
            "content": "Figure 1: MAS failure traces are complex and long, often exceeding the model capacity. Figure 2: Failure trajectories exhibit high semantic similarities (Who and When dataset). Figure 3: Performance of naive ICL and our method (Who and When dataset). correct alternative sk would change the outcome to success. Formally, the earliest decisive error is: (a, s) = minkD(τ ) k, where D(τ ) = { : R(τ ) = 0 R(τ[sksk]) = 1 }, and τ[sksk] denotes the modified trajectory in which step sk is replaced by sk. Intuitively, decisive error is the earliest step whose correction flips the trajectory outcome from failure to success. Identifying decisive errors is fundamental to reliable MAS. Unlike coarse-grained error recognition, which only flags failed trajectories, decisive error recognition pinpoints the exact agent and step responsible for initiating failure. This precision enables targeted interventions such as tuning role specifications, refining orchestration logic, or upgrading individual agents, without costly overhauls of the entire system. 2.2 MOTIVATIONS OF CORRECT Automated decisive error recognition in MAS has primarily followed two directions: LLM-as-ajudge approaches (Zheng et al., 2023; Zhang et al., 2025) and fine-tuning specialized LLMs (Cemri et al., 2025). Both exhibit fundamental limitations in accuracy, generality, and efficiency. Limitations of Existing Advances. LLM-as-a-judge methods were initially designed to rate the quality of LLM outputs, achieving up to 80% agreement with human preferences (Zheng et al., 2023). Zhang et al. (2025) extended this paradigm to MAS error attribution, introducing three variants: (i) all-at-once, which provides the full error log to the LLM and asks it to identify the responsible agent and error step; (ii) step-by-step, which incrementally reveals the trajectory and checks errors at each step; and (iii) binary search, which recursively partitions the log to localize the error. However, as MAS becomes more complex, which elongates the failure trajectory, these methods lose diagnostic precision (Figure 1): on the WHO&WHEN dataset (Zhang et al., 2025), Qwen-2.5-7B achieves only 3.5% step-level accuracy, far below practical deployment requirements. Fine-tuning (FT)-based approaches, whether supervised or reinforcement learningbased, face substantial efficiency and expense challenges. Their success hinges on large-scale, high-quality labeled datasets. Yet annotating MAS failures is prohibitively expensive: annotators must disentangle long, interdependent interactions across agents and tools, taking 30 expert hours for annotating fewer than 200 trajectories. Worse, error trajectories vary across applications and requests, making FT-trained detectors brittle and undermining generalization for diverse deployments at scale. Pervasive Error Similarity Yet Hard to Reuse. Despite these challenges, our analysis of the WHO&WHEN dataset (Figure 2) reveals that more than 80% of failed requests share semantic (cosine) similarity above 0.8, measured via BERT-based embeddings. This reveals an underexploited opportunity: historical failures could be reused as exemplars for new requests. natural attempt is to adopt in-context learning (ICL), retrieving and appending similar trajectories to guide error recognition. However, our experiments (Figure 3) show that such strawman approach even degrades recognition accuracy, due to two primary limitations: (i) extreme trajectory length: 17% of trajectories exceed 32K tokens, surpassing the context length of most LLMs (e.g., Qwen3 (Yang et al., 2025)); and (ii) low signal-to-noise ratio: execution trajectories interleave request-specific details and tool outputs with the true error-inducing steps, diluting critical information."
        },
        {
            "title": "TRANSFER",
            "content": "Our observations call for novel approach that can systematically reuse structural error knowledge without overwhelming context or succumbing to noise, while remaining general, data-efficient, and lightweight for real-time MAS deployment. To these ends, we introduce CORRECT (COndensed eRror RECognition via knowledge Transfer), the first framework that distills past errors into compact error schemas and adaptively applies them for accurate decisive error recognition without any training, enabling efficient adaptation to diverse tasks and errors. As summarized in Algorithm 1, CORRECT combines three interconnected phases: (1) Offline schema extraction, (2) Online schema-guided error recognition, and (3) Dynamic schema management."
        },
        {
            "title": "3.1 ERROR SCHEMA EXTRACTION",
            "content": "Given an annotated error trajectory = {(ai, si, ri)}n i=1, where ai denotes the agent at step i, si represents the step content, and ri is the corresponding result, along with the identified error at step se and error reason re, CORRECT generates an error schema capturing (Figure 4): (1) Error Signatures Σ: Characteristic patterns such as agent actions, interaction sequences, and key behavioral markers; (2) Error Context Analysis C: Detailed analysis of the conditions that led to the error, including agent states, task progress, and environmental factors; and (3) Detection Heuristics H: Actionable rules and guidelines for identifying similar errors in new contexts. To minimize human efforts, CORRECT leverages capable LLMs (e.g., GPT-5) to generate error schemas. We discuss how to ensure the quality of the schema in Section 3.2. Schema Extraction. Clustered Even with LLMs, generating schema for every trajectory is still costly due to voluminous requests in practical MAS deployments. In fact, doing so is unnecessary because schema reuse often follows longtailed distribution: small number of schemas are frequently reused, while most are rarely applied (5). To exploit this, CORRECT performs (1) Trajectory Clustering: Failure trajectories are embedded the following offline procedure: semantically and clustered to group similar error patterns, and then (2) Cluster-level Schema Generation: One representative schema is generated per cluster, capturing the common error structure without redundant costs. Figure 4: Example of an error schema generated on the WHO&WHEN dataset. 3.2 SCHEMA-GUIDED ERROR RECOGNITION ONLINE Once failure request requires diagnosis, we start with its trajectory Ttarget and retrieve the top-k relevant schemas from the cache via semantic similarity search (e.g., cosine similarity of embeddings): sim(Ttarget, Si) = cos(embed(Ttarget), embed(Si)) With retrieved schemas, we prompt the LLM to perform error recognition by instantiating the schemas in the context of the target trajectory. These schemas, together with the trajectory and lightweight adaptation instructions, are passed to an LLM for diagnosis. Formally, the LLM receives (Ttarget, {Sj}k j=1, promptdetect) as input prompt and produces the error recognition result: result = LLMdetect (cid:0)Ttarget, {Sj}k j=1, promptdetect (cid:1). By leveraging schemas as condensed expert knowledge, this process guides the LLM to focus on salient failure patterns without the overhead of processing entire historical trajectories."
        },
        {
            "title": "CORRECT",
            "content": "Algorithm 1: CORRECT Framework Input: Annotated trajectories {(T , se, re)} for schema extraction; target trajectory Ttarget for diagnosis Output: Error recognition result (a, s, c) Offline Schema Extraction Cluster trajectories by semantic similarity and generate one representative schema per cluster; foreach annotated trajectory (T , se, re) do Generate schema: LLMextract(T , se, re) ; Apply filtering/distillation to enforce compactness, token filtering, and consistency; Insert into cache: C.put(S) ; Online Schema-Guided Error Recognition Embed target trajectory: embed(Ttarget) ; Retrieve top-k schemas: {Sj}k Update access statistics: : C.update access(Sj) ; Diagnose decisive error: (a, s, c) LLMdetect(Ttarget, {Sj}, promptdetect) ; Dynamic Schema Management if user feedback confirms successful recognition then j=1 C.search top k(e) ; if sim(Ttarget, Si) < δ for all Si then Extract ground truth label from data; Distill new schema Snew from Ttarget with ground truth; C.put(Snew); if C.access count(Sj) > θhot then i}m Generate candidates: {S Evaluate by replaying on prior trajectories; Select best schema: arg maxS Replace old schema: C.replace(Sj, ) ; i=1 LLMm extract(Tj, se,j, re,j) ; accuracy(S i) ; return (a, s, c) Adaptation with Schema Expansion and Distillation. CORRECT maintains an effective schema cache through two complementary mechanisms. First, schema expansion: when user feedback confirms successful recognition, CORRECT leverages the ground truth label from the user to generate and cache new error schema following 3.1. Priority is given to trajectories with low similarity to existing schemas (i.e., sim(Tnew, Si) < δ for all cached schemas), ensuring the cache captures diverse error patterns rather than redundant ones. Second, schema distillation: expansion alone may yield suboptimal quality, and frequently accessed error schemas (cache hits > θhot) may benefit from further refinement. In such cases, CORRECT generates multiple candidate schemas and replays them against prior trajectories to select the discriminative one with the highest accuracy. Together, expansion ensures coverage for novel errors online, while distillation preserves cache efficiency by retaining only high-quality, discriminative schemas."
        },
        {
            "title": "4 CORRECT -ERROR: A LARGE-SCALE ERROR DETECTION BENCHMARK",
            "content": "Existing efforts for trajectory-level error analysis are limited in both scale and diversity, and human annotation is costly and difficult to scale (2.2). To bridge this gap and evaluate the effectiveness of CORRECT (3), we introduce CORRECT-Error, large-scale benchmark that faithfully reflects the distribution of natural errors encountered in real-world MAS. 4.1 BOOTSTRAP ERROR SYNTHESIS PIPELINE In building CORRECT-Error, we develop bootstrap methodology that uses small set of humanannotated error trajectories as seeds for scalable error generation, blending realism with controllability. It follows three-stage pipeline that distills human expertise into scalable synthetic data while preserving the structural and semantic integrity of real-world error patterns."
        },
        {
            "title": "CORRECT",
            "content": "Stage 1: Diverse Trajectory Collection. We begin by generating large pool of successful multiagent trajectories spanning heterogeneous tasks and domains. Alongside, we curate smaller but high-quality set of human-annotated error trajectories. These serve as reference exemplars of realistic failure dynamics, capturing both localized mistakes and their downstream propagation. Stage 2: Semantic Error Schema Matching. Each successful trajectory is paired with its closest human-labeled error trajectory using semantic similarity measures that account for both high-level task goals and fine-grained agent interactions. This alignment ensures that the selected error schema is contextually aligned with the target trajectory, avoiding unrealistic mismatches. Then we use GPT5 to devise an error injection strategy that specifies (i) where in the target trajectory to introduce the error, and (ii) how to adapt the error pattern while preserving its core semantics. Stage 3: Contextual Error Injection. Following the injection strategy, we prompt GPT-5 that generated the original successful trajectory to introduce an erroneous action at the designated point. This guarantees consistency in linguistic and behavioral style while embedding realistic failure."
        },
        {
            "title": "4.2 HUMAN-ALIGNMENT ANALYSIS",
            "content": "Following our bootstrap pipeline (4.1), we synthesized over 2,000 trajectories across seven datasets (Figure 5), yielding 12.3 more data than WHO&WHEN with cost over 3 billion tokens using GPT-5 series models and GPT-4o series models based on Magnetic-One (Fourney et al., 2024) and AutoGen (Wu et al., 2024). The resulting benchmark spans diverse tasks, including multi-hop QA, common planning, mathematical reasoning, and scientific problem-solving. By leveraging limited human annotations as seeds, our novel pipeline generates diverse error scenarios at scale. To rigorously assess this authenticity, we conducted human evaluation study involving three independent expert labelers over 120 hours. Each labeler was shown an equal mix of synthetic and human-labeled error trajectories, without disclosure of their origin. As shown in Figure 5, labelers struggled to distinguish between the two: 47.1% of synthetic trajectories were misclassified as human-labeled, while 42.3% of genuine trajectories were correctly identified. This near-random classification accuracy (close to 50% in both cases) indicates that our synthetic errors are effectively indistinguishable from real ones. Moreover, we notice that high inter-annotator agreement on authenticity: 94.4% of synthetic trajectories were judged as genuine by at least two out of three labelers, with 52.9% receiving unanimous consensus. We add detailed human-alignment analysis in Appendix A.2. Figure 5: CORRECT-Error includes diverse tasks. The synthesized data preserves high realism, where human labelers frequently misclassified synthetic errors as genuine ones. Together, these results validate that our error injection pipeline faithfully captures the nuanced characteristics of real-world MAS failures. We emphasize that this methodology enables data generation that is cheap (automated and low-cost), abundant (scalable to millions of examples), precise (with unambiguous ground-truth labels), and realistic (closely matching natural error distributions derived from human annotations), building the foundation for training effective recognition models."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We demonstrate that CORRECT significantly enhances error recognition, achieving improvements of up to 20% and an average gain of 28.7% across MAS tasks (5.1), all at zero training costs. CORRECT remains robust under distribution shifts arising from model updates, dataset variations, schema cache size, and the number of stored schemata (5.2). Models and Tasks. We evaluate CORRECT on both the human-annotated WHO&WHEN benchmark and our high-quality benchmark, CORRECT-ERROR, spans diverse tasks including multi-hop QA (HotpotQA (Yang et al., 2018), Musique (Trivedi et al., 2022), WikiMQA (Ho et al., 2020)), sci-"
        },
        {
            "title": "CORRECT",
            "content": "entific reasoning (ARC (Clark et al., 2018), MMLU-Pro (Wang et al., 2024)), mathematical reasoning (Math500 (Lightman et al., 2023)), and general agentic tasks including planning (GAIA (Mialon et al., 2023)). WHO&WHEN consists of two subsets: Human-Crafted subset and an AlgorithmGenerated subset. Experiments are conducted on both openand closed-source models, including the Qwen (Yang et al., 2024; 2025), Llama (Dubey et al., 2024), GPT (Hurst et al., 2024; OpenAI, 2025), DeepSeek-R1 (Guo et al., 2025), and Gemini series (Comanici et al., 2025). We mask each trajectory itself and avoid receiving its own error schema for preventing data leakage. Additional experimental details are provided in Appendix A.1. Baselines. We compare CORRECT against three state-of-the-art approaches: LLM-as-a-Judge, zero-shot prompting strategy where an LLM directly inspects trajectories without auxiliary guidance (Zhang et al., 2025; Peng et al., 2023a); Fine-tuning, where an LLM is trained on the full trajectory dataset with cross-entropy loss to encode domain-specific failure patterns (Chen et al., 2025; Fu et al., 2025); and Naive In-Context Learning, which inserts complete error trajectories as few-shot exemplars, which may be constrained by limited context windows (Dong et al., 2022; Yu et al., 2025). Metrics. Following existing advances (Zhang et al., 2025), we report step-level accuracy, which provides actionable debugging signals. To account for the ambiguity of error attribution, we additionally report accuracy@k, where predictions within steps of the ground truth are treated as correct, e.g., Acc@0 requires identifying the exact erroneous step, while Acc@1 tolerates an offset of one step. This better reflects practical debugging scenarios, where approximate localization is often sufficient. We report the average performance over five independent runs. 5.1 END-TO-END PERFORMANCE Method Model Human-Crafted Algorithm-Generated Acc@0 Acc@1 Acc@0 Acc@ LLM-as-a-Judge Qwen-2.5-7b Qwen-3-30b Qwen-3-80b Llama-8b DeepSeek-R1 Gemini-2.5-flash Gemini-2.5-pro GPT-4o-mini GPT-4o GPT-5-nano GPT-5 Fine-tuned LLM Qwen-2.5-7b Qwen-2.5-7b Naive ICL 3.5 1.7 6.9 1.7 3.5 5.2 3.5 3.5 3.5 1.7 8.6 3.5 5.2 8.6 5.2 8.6 3.5 17.2 13.8 8.6 12.5 10.3 5.2 24. 11.9 10.3 19.1 15.1 21.4 3.2 23.8 31.8 25.4 12.7 18.3 19.1 18.3 18.9 15.9 42.9 42.9 47.6 15.9 54.0 56.0 50.0 39.7 50.0 41.3 56.4 42.9 40.5 CORRECT Qwen-2.5-7b Gemini-2.5-flash Gemini-2.5-pro GPT-5-nano GPT-5 12.1 (+8.6) 10.3 (+5.1) 5.2 (+1.7) 6.9 (+5.2) 17.2 (+8.6) 15.5 (+6.9) 20.7 (+6.9) 15.5 (+6.9) 17.2 (+12.0) 37.8 (+13.7) 19.8 (+0.7) 38.9 (+7.1) 24.6 (-0.8) 24.6 (+5.5) 38.1 (+19.8) 46.8 (+3.9) 55.2 (-0.8) 52.4 (+2.4) 44.4 (+3.1) 58.8 (+2.4) Table 1: CORRECT achieves higher error recognition accuracy over existing advances on WHO&WHEN dataset. Acc@0 denotes exact-step accuracy (the model must pinpoint the precise error step). Acc@k denotes tolerant accuracy (a prediction is correct if it falls within steps of the ground truth). For rows corresponding to CORRECT, numbers in parentheses indicate absolute improvements over the LLM-as-a-Judge baseline. CORRECT achieves significant gains in error recognition accuracy (WHO&WHEN dataset). Table 1 shows that CORRECT consistently surpasses existing advances across both human-crafted and algorithm-generated subsets. On human-crafted data, CORRECT raises Qwen-2.5-7Bs exactstep accuracy from 3.5% to 12.1% (a 3.5 improvement), and improves GPT-5 from 8.6% to 17.2%."
        },
        {
            "title": "CORRECT",
            "content": "Method Tolerance Synthesized by GPT-4o-mini LLM-as-a-Judge CORRECT Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@ Synthesized by GPT-5-Nano Baseline CORRECT Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@5 Dataset Gaia HotpotQA Musique WikiMQA Arc Math500 MMLU-Pro Avg. Improv. 28.6 42.9 50.0 28.6 50.0 64.3 16.7 27.8 38.9 30.6 44.4 52.8 34.8 59.4 63.8 60.9 94.2 95.7 14.7 48.9 65.8 35.8 72.7 84.3 27.8 77.8 77.8 38.9 88.9 88.9 11.9 43.9 58.8 32.7 61.2 77. 14.7 55.9 64.7 44.1 88.2 94.1 6.44 35.6 56.1 16.9 49.9 69.0 64.0 75.0 78.0 80.4 88.2 91.2 62.8 69.6 71.1 80.4 88.2 91.2 10.2 23.4 35.6 57.1 87.8 95.9 41.8 57.1 64.3 57.1 87.8 95. 58.3 62.5 66.7 69.1 88.2 94.1 50.0 64.7 70.59 69.1 88.2 94.1 - - - +20.1 +27.6 +28.7 - - - +16.8 +20.1 +18.9 Table 2: Performance comparison across multiple datasets. All numbers are accuracy (%). These gains extend to tolerant metrics as well, with GPT-5 + CORRECT achieving 37.8% at Acc@1 versus 24.1% for the baseline. By contrast, fine-tuning (3.5%) and naive ICL (5.2%) offer only marginal improvements, suggesting that standard supervised learning and raw in-context trajectories fail to capture complex error patterns. On algorithm-generated data, CORRECT maintains clear advantages, with Gemini-2.5-Flash improving from 31.8% to 38.9% (+7.1 points) and GPT-5 exhibiting the largest gain (+19.8 points). These consistent improvements across model families (Qwen, Gemini, GPT) and scales (7B to GPT-5) highlight the effectiveness of condensed error schemas. CORRECT delivers 1728% average improvements (CORRECT-ERROR benchmark). Table 2 highlights CORRECTs strong generalization across seven datasets. For GPT-4o-mini subset, CORRECT improves average accuracy by +20.1%, +27.6%, and +28.7% at Acc@1, Acc@3, and Acc@5, respectively using Qwen-2.5-7b. Gains are especially pronounced on knowledge-intensive tasks: HotpotQA (+26.1 points), WikiMQA (+29.4 points), and Math500 (+46.9 points). At higher tolerances, performance gaps widen further: CORRECT reaches 94.2%, 91.2%, and 95.9% at Acc@5, compared to baseline scores of 63.8%, 78.0%, and 35.6%. CORRECT exhibits similar trend in GPT-5-nano subset, with average improvements of +16.8%, +20.1%, and +18.9% across tolerance levels using Qwen-2.5-7b. Even on the challenging GAIA benchmark, CORRECT raises accuracy from 28.6% to 30.6%, while achieving near-perfect scores on several datasets at Acc@5. Strong Schema Transferability across Datasets and Models. Figure 6 shows that schemas distilled from human-crafted trajectories transfer effectively to algorithm-generated data. Across GPT5-nano, Gemini-2.5-Flash, and Qwen-7B, CORRECT with transferred schemas consistently outperforms baselines, with Gemini-2.5-Flash improving from 31.8% to 36.5%. This cross-domain transferability indicates that distilled schemas capture fundamental error patterns. Moreover, Figure 7 demonstrates that CORRECT benefits directly from model upgrades: using GPT5 instead of Qwen-72B as the schema generator raises detection accuracy from 8.6% to 12.2% for GPT-5-nano and from 10.3% to 12.2% for Qwen-2.5-7B. These adaptive gains confirm that better models yield higher-quality schemas that immediately enhance downstream performance. Together, these results establish CORRECT as flexible, upgrade-compatible framework that leverages both existing schema libraries and future model advances without architectural changes. 5.2 ABLATION STUDIES Impact of Schema Repository Size. Figure 8 shows CORRECTs robustness to cold-start scenarios and varying cache sizes. On CORRECT-ERROR, even with only 10% of the schema library, CORRECT achieves 69.1% and 35.4% Acc@1 on MMLU pro and HotpotQA, substantially outperforming baselines. Performance improves steadily with larger caches but plateaus beyond 50% (tens of schemas), suggesting that relatively small set of diverse schemas captures most common error patterns. This logarithmic growth pattern validates our clustering-based extraction strategy: error patterns are finite and reusable across trajectories. Importantly, CORRECT retains 90% of peak performance with only 30% of the cache, highlighting efficiency for practical deployments."
        },
        {
            "title": "CORRECT",
            "content": "improvements Figure 6: CORRECT delivers on consistent Algorithm-Generated, with schemata generated and transferred on Hand-Crafted dataset. Figure 7: CORRECT can adaptively upgrade its performance on the Hand-Crafted dataset with model upgrade. Figure 8: Correct deliver robust improvements under different cache sizes. Figure 9: Performance of Correct with different number of error schemata on Handcrafted subset of Who&When. Figure 10: Performance of Correct and Correct with the variation using oracle error schema on CORRECT-Error. Figure 11: LLMs have low performance in recognizing the errors when they encounter them. Impact of Number of Schemas in Online Error Recognition. Figure 9 demonstrates that retrieving and using single schema already greatly improves baseline Acc@1 (12.1% vs. 8.6%). Accuracy increases as more schemas are added (13.8% with 5 schemas, 15.5% with 10), though gains diminish (15 adds +1.7%; 510 adds only +1.7%). At higher tolerances (Acc@5), settings converge to 48%. These results show that small set of well-matched schemas already efficiently captures most critical patterns, while additional schemas offer limited incremental value. Comparison with Oracle Error Schema. To estimate the upper bound of our framework, we compare CORRECT against an oracle configuration where each trajectory uses its own ground-truth schema. As shown in Figure 10, the oracle achieves superior step-level accuracy, while CORRECT with 5 retrieved schemas reaches 71.5%104% of oracle performance. This narrow gap shows that our semantic retrieval strategy effectively identifies schemas encoding near-equivalent knowledge to trajectory-specific patterns. The convergence toward oracle performance validates that diverse MAS error patterns often share structural regularities, which can be captured via finite schema library. LLMs cant Recognize Errors During Execution Figure 11 shows that LLMs have limited metacognitive ability to detect their own errors during execution. When asked to identify injected errors at the exact step, they achieve only 21% accuracy on flipped trajectories (where errors alter the final answer) and 1718% on non-flipped trajectories (where errors do not affect the outcome). This reveals fundamental limitation: agents lack the self-awareness to recognize their own mistakes, regardless of downstream task success. These findings underscore the necessity of external error-detection mechanisms like CORRECT, as relying on self-monitoring leaves MAS vulnerable to undetected and compounding failures."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced CORRECT, the first schema-guided framework that distills recurrent MAS failures into compact, reusable schemata, enabling accurate, lightweight, and training-free identification of decisive errors in new runs. Complementing this, we release CORRECT-ERROR, large-scale, high-fidelity benchmark capturing realistic error patterns. Across diverse tasks, models, and deployment scenarios, CORRECT significantly outperforms existing advances, offering practical, generalizable path toward reliable, interpretable, and scalable MAS deployment."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, and Xiao Wang. Step-wise adaptive integration of supervised fine-tuning and reinforcement learning for task-specific llms. arXiv preprint arXiv:2505.13026, 2025. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and In ProceedSaleema Amershi. ings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 115, 2025. Interactive debugging and steering of multi-agent ai systems. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, et al. Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468, 2024. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. Mingyan Gao, Yanzi Li, Banruo Liu, Yifan Yu, Phillip Wang, Ching-Yu Lin, and Fan Lai. Singleagent or multi-agent systems? why not both? arXiv preprint arXiv:2505.18286, 2025. Yu Ge, Linna Xie, Zhong Li, Yu Pei, and Tian Zhang. Who is introducing the failure? automatically attributing failures of multi-agent systems via spectrum analysis. arXiv preprint arXiv:2509.13782, 2025."
        },
        {
            "title": "CORRECT",
            "content": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multiIn Proceedings of the 28th hop QA dataset for comprehensive evaluation of reasoning steps. International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. URL https: //www.aclweb.org/anthology/2020.coling-main.580. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for In The Twelfth International Conference on Learning multi-agent collaborative framework. Representations, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: In The Twelfth International Conference on Learning benchmark for general ai assistants. Representations, 2023. OpenAI. Gpt-5. https://openai.com, 2025. Large language model, accessed via ChatGPT. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023a. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023b. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024."
        },
        {
            "title": "CORRECT",
            "content": "Zhaodong Wang, Samuel Lin, Guanqing Yan, Soudeh Ghorbani, Minlan Yu, Jiawei Zhou, Nathan Intent-driven Hu, Lopa Baruah, Sam Peters, Srikanth Kamath, Jerry Yang, and Ying Zhang. network management with multi-agent llms: The confucius framework. In SIGCOMM, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. URL https://api. semanticscholar.org/CorpusID:274859421. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question In Conference on Empirical Methods in Natural Language Processing (EMNLP), answering. 2018. Yifan Yu, Yu Gan, Lillian Tsai, Nikhil Sarda, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry Levy, and David Culler. Echolm: Accelerating llm serving with real-time knowledge distillation. arXiv preprint arXiv:2501.12689, 2025. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, et al. Which agent causes task failures and when? on automated failure attribution of llm multi-agent systems. arXiv preprint arXiv:2505.00212, 2025. Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh Patel. Reactable: enhancing react for table question answering. Proceedings of the VLDB Endowment, 17(8):19811994, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 SPECIFICS OF EXPERIMENTAL SETTINGS We implement our evaluation pipeline based on Zhang et al. (2025). We host open-source models using vLLM(Kwon et al., 2023) and access GPT-series models(Achiam et al., 2023) via the OpenAI API. To handle long contexts exceeding standard model limits for Qwen models(Yang et al., 2025), we employ RoPE(Su et al., 2024) scaling with 4 length extension using the yarn(Peng et al., 2023b) scaling type. To simulate realistic deployment scenarios where ground truth is unknown, we exclude the correct answer from evaluation prompts. For our method, we first generate all the error schemata using GPT-5 model. We then derive similarity mapping to assign error schemata based on the semantic embedding decoded by BAAI-BPE-M3 model(Chen et al., 2024). To avoid the data leakage, we mask each trajectory itself and avoid receiving its own error schema. We decide the number of error schemata from the experiments using Qwen-2.5-7b models on Hand-Crafted dataset, Algorithm-Generated dataset, and HotpotQA dataset of CORRECT-Error. We use the same number of error schemata across all models and all datasets in CORRECT-Error. Specifically, we use 1 error schema for all experiments on the AlgorithmGenerated dataset, 10 error schemata for all experiments on the Hand-Crafted dataset, and 5 error schemata for all experiments on CORRECT-Error. Figure 12: Percentage of human labelers to believe the trajectory is not synthesized. A.2 MORE DETAILS OF THE CORRECT-ERROR We implemented variant of Magentic-One (Fourney et al., 2024) using selector group based workflow control using AutoGen (Wu et al., 2024) to generate CORRECT-Error. Apart from the figures we showed in section 4, we observed strong inter-annotator consensus: 94.4% of synthetic trajectories fooled at least two labelers, while 52.9% were unanimously mistaken for genuine errors. We show the distribution in Figure A.3 PROMPTS FOR OFFLINE ERROR SCHEMA GENERATION We show the prompts we used for offline schema generation in Fig A.3 A.4 PROMPTS FOR ONLINE SCHEMA-GUIDED GENERATION We show the prompts we used for online schema-guided generation in Fig A."
        },
        {
            "title": "CORRECT",
            "content": "Error Schema Generation Given an error analysis from multi-agent conversation, create an error schema to help identify similar errors in the future. Context: Question: {question} Ground Truth: {ground truth} Error Agent: {mistake agent} Error Step: {mistake step} Error Reason: {mistake reason} Conversation History: {chat content} Based on this error case, please create error schema that will help IDENTIFY similar errors in future conversations. Focus primarily on recognition patterns rather than mitigation strategies. The schema should include: 1. Error Signatures: - What distinctive patterns or signals indicate this type of error is occurring? - What are the telltale signs in the agents behavior or responses? 2. Error Context Analysis: - What contextual conditions typically surround this type of error? - What sequence of interactions tends to precede this error? 3. Detection Heuristics: - What specific questions can be asked to determine if this error is present? - What analytical framework can help identify this error pattern? - What key phrases or conversation patterns serve as reliable indicators? Please format your response as structured schema that focuses specifically on ERROR IDENTIFICATION, not on how to improve agent behavior. Provide concise, actionable schema in the following format: Agent Name: {mistake agent} Step Number: {mistake step} Reason for Mistake: [Your analysis of why this specific error occurred and how to identify similar patterns] Schema-guided generation HOW TO USE THIS REFERENCE EXAMPLE: This template demonstrates one type of error pattern for reference. To apply it to your analysis: 1. Study the ERROR PATTERN shown: What type of mistake does this example identify? 2. Use this as reference to analyze YOUR conversation: Read through your conversation systematically (Step 0, Step 1, Step 2...) At each step, ask: Is there an error here, and does it match this pattern or different one? The error in your case may follow the same pattern or be completely different 3. Remember this is just reference example: Your error may occur at any step number Your error may be different type entirely Use this template to help you recognize what errors look like, not to assume your error matches"
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Illinois Urbana-Champaign",
        "University of Michigan"
    ]
}