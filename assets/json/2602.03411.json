{
    "paper_title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training",
    "authors": [
        "Huatong Song",
        "Lisheng Huang",
        "Shuang Sun",
        "Jinhao Jiang",
        "Ran Le",
        "Daixuan Cheng",
        "Guoxin Chen",
        "Yiwen Hu",
        "Zongchao Chen",
        "Wayne Xin Zhao",
        "Yang Song",
        "Tao Zhang",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master."
        },
        {
            "title": "Start",
            "content": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training Huatong Song1, Lisheng Huang1, Shuang Sun1, Jinhao Jiang1, Ran Le2, Daixuan Cheng1, Guoxin Chen1, Yiwen Hu1, Zongchao Chen2, Wayne Xin Zhao1, Yang Song2, Tao Zhang2, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2BOSS Zhipin, Beijing, China. songhuatong123@ruc.edu.cn, batmanfly@gmail.com, songyang@kanzhun.com Abstract In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves resolve rate of 61.4% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling (TTS) with LLM-based environment feedback, SWE-Master reaches 70.8% at TTS@8, demonstrating strong performance potential. SWE-Master provides practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master. 6 2 0 2 3 ] . [ 1 1 1 4 3 0 . 2 0 6 2 : r Figure 1: Performance overview and scaling analysis of SWE-Master. Left: Comparasion of the perference of various open-source foundational models and SWE agents on SWE-bench Verified. Right: Performance of SWE-Master across different training stages and evaluation metrics. Equal contribution. Correspondence to Wayne Xin Zhao and Yang Song."
        },
        {
            "title": "Introduction",
            "content": "Large language model based software engineering agents, also referred to as SWE agents [1], have recently emerged as powerful paradigm for automating complex software development tasks [2, 3]. Unlike traditional code generation models that focus on short snippets or isolated functions [4, 5], modern SWE agents are expected to understand natural language requirements, navigate large codebases, modify multiple files, execute tests, and iteratively refine solutions until task is successfully completed [6]. By operating at the level of end-to-end autonomous workflows, SWE agents have the potential to significantly reduce human engineering effort and accelerate software development and maintenance in real-world settings. Recent progress in SWE agents has been driven by coordinated advances across the entire pipeline, spanning data construction, training with environment feedback, and inference-time scaffold. On the training side, mainstream approaches construct executable task instances from real-world GitHub issues and train models as agents that interact with environments over multiple stepsexploring codebases, modifying files, executing commands, and iteratively refining solutions until final patch is validated by unit tests, with execution feedback obtained from containerized execution environments (i.e., Docker) providing supervision signals [7, 8, 9, 10]. On the inference side, existing methods typically adopt standardized scaffolds with basic capability workflows, such as OpenHands [6]. Some studies further augment these frameworks with additional tools to support extended capabilities, including long-context management [11, 12, 13]. Through systematic training optimization combined with well-designed inference frameworks, recent systems developed by organizations such as OpenAI and Anthropic have achieved strong performance on challenging real-world software engineering benchmarks [14, 15]. Despite the rapid progress of software engineering agents, existing approaches remain fundamentally limited by the lack of transparency and reproducibility across training data construction and optimization procedures. In practice, the closed nature of many state-of-the-art systems obscures several critical challenges that are essential for building effective SWE agents. On the training data side, key difficulty lies in efficiently constructing high-quality teacher trajectories that capture long-horizon reasoning and realistic environment interactions. On the optimization side, agent training typically follows two-stage paradigm: SFT and RL. The former requires careful data filtering and mixture design to balance correctness, diversity, and task difficulty, while the latter demands delicate algorithms tuning and reward formulations to encourage sufficient exploration and stable learning, without suffering from issues such as entropy collapse or reward hacking. In addition, on the inference side, existing approaches are largely constrained by basic agent frameworks, with limited exploration of advanced tools and system designs, particularly in terms of execution efficiency and long-context management. Together, these opaque and interdependent components form high barrier to entry, hindering reproducible research and limiting the accessibility of SWE agent development for the broader academic community. To address these challenges, we introduce SWE-Master, an open-source software engineering agent framework that fully exposes the post-training pipeline in transparent and reproducible manner. Rather than treating agent performance as the outcome of isolated design choices, SWE-Master systematically studies how software engineering capabilities emerge from the interaction between data construction, optimization strategies, and inference-time behaviors, even when starting from an open-source model with limited initial SWE task performance (e.g., below 10 points on SWE-bench Verified benchmarks using Qwen2.5-Coder-32B model) [4]. In particular, we analyze the impact of different teacher models and data filtering strategies during trajectory synthesis, and show that controlling the difficulty distribution of training data plays crucial role in shaping the interaction depth and decision-making behavior of models after SFT. Building on this foundation, we further investigate RL in real execution environments by exploring combinations of optimization algorithms and reward designs, enabling efficient exploration and effective learning while mitigating common failure modes such as reward hacking and unstable behaviors. Together, SWE-Master provides comprehensive, open, and empirically grounded framework for understanding and advancing the post-training of software engineering agents. Building on the previously discussed limitations of existing inference frameworks, we further investigate the impact of equipping advanced capabilities at inference time. Motivated by the observation that many software engineering failures stem from insufficient understanding of large codebases rather than code generation errors, we focus on enhancing agents code interaction and navigation abilities. In particular, we study the transition from simple text-based search to structured code navigation based on language server protocols, and analyze its impact on reasoning and decision making in large repositories. Through systematic empirical analysis, we find that tools grounded in the Language Server Protocol (LSP) constitute new foundational paradigm for SWE agents. This approach empowers agents with IDE-grade code comprehension, thereby facilitating precise inspection and modification of complex file systems within realistic software engineering scenarios. To validate the effectiveness of the proposed approach, we conduct extensive experiments on SWEbench Verified [16], widely used benchmark for evaluating realistic software engineering agents. Under identical experimental settings, including the same base model, training data sources, and inference configurations, our long-horizon SFT strategy significantly outperforms existing opensource methods, achieving resolve rate of 57.8%. These results indicate that careful data curation and trajectory-level supervision alone can substantially improve performance on real-world software engineering tasks. Building on this strong SFT baseline, we further apply RL with real execution environments, which consistently extends model capabilities and enables the agent to solve more challenging instances, pushing the performance to 61.4%. Furthermore, inspired by prior studies that leverage LLMs to simulate real execution feedback [17, 18], we adopt test-time scaling (TTS) strategy [19] powered by LLM-based environment feedback. This approach enables the agent to explore and rank multiple candidate solutions without incurring the overhead of physical execution. By selecting the most promising candidate, our method achieves score of 70.8% under the TTS@8 setting. This strategy avoids direct execution in real environments, which is particularly valuable in scenarios where environment interactions are costly, irreversible, or unsafe. Finally, by integrating an LSP-based code navigation framework at inference time, SWE-Master improves agent efficiency with minimal impact on task success rates, achieving practical balance between effectiveness and efficiency. Based on our experiments, our major contritions are summarized below: We release the first fully open-source, end-to-end training pipeline for software engineering agents, covering data processing, SFT, RL infrastructure and strategies, and inference-time agent frameworks. We introduce IDE-level capabilities based on LSPdriven code navigation, enabling more efficient and structured repository understanding, and significantly improving agent efficiency without sacrificing performance. We significantly advance open-source model performance on SWE-bench Verified, achieving 61.4% accuracy with Qwen2.5-Coder-32B, improving to 70.8% with test-time scaling and 76.2% under Pass@8, demonstrating strong perpormance potential."
        },
        {
            "title": "2.1 Problem Formulation: The SWE Task",
            "content": "We define the software engineering task as an automated program repair or feature implementation problem. Formally, let = {(Ii, Ci, Ui)}N i=1 represent dataset of software engineering problems. For specific instance, the input consists of: An issue description I, which describes the bug report or the feature request. codebase C, representing the initial state of the code repository (i.e., the file system structure). The ground truth typically includes golden patch and unit test suite comprising series of test cases. The goal of the model is to generate patch ˆp (a set of diffs) such that applying the patch to the codebase resolves the issue I, defined as effectively passing all unit tests. Let fapply(C, p) be function that applies patch to the codebase. The modified codebase is denoted as = fapply(C, ˆp)."
        },
        {
            "title": "2.2 Agent-Based Environment Interaction",
            "content": "We formulate the problem solving process as sequential decision-making process within an interactive environment. The environment state at step is denoted as sk, which includes the current file contents, the command line history, and the previous execution outputs. The agent functions as policy πθ(akhk), where θ represents the model parameters and hk = s0, a0, o0, s1, . . . , ok1 is the interaction history. The agent generates an action ak consisting of 3 reasoning trace (Thought) and tool invocation (Action). The action space typically includes: = Anav Aedit Aexec (1) where Anav contains navigation commands (e.g., ls, cd), Aedit contains file manipulation commands (e.g., view, create, str_replace), and Aexec contains execution commands (e.g., pytest). Upon executing action ak, the environment returns an observation ok (e.g., standard output, error logs, or file content) and transitions to new state sk+1. The agent then proceeds with subsequent interactions.This process continues until the agent issues termination action (e.g., submit) or reaches maximum step limit Kmax. The trajectory is defined as τ = I, a0, o0, a1, o1, . . . , aK."
        },
        {
            "title": "2.3 Evaluation Protocol",
            "content": "Evaluation is strictly execution-based within isolated Docker containers. For each issue, the model interacts with the codebase to implement solution, which is subsequently captured as patch via git diff. This patch is then applied to the original repository for verification. The validity of generated patch ˆp is determined by the unit test suite U. The test suite consists of two subsets: = Ufail Upass. Specifically, Ufail denotes the set of fail-to-pass (F2P) tests designed to reproduce the bug, whereas Upass comprises pass-to-pass (P2P) tests intended to ensure no regression in existing functionality. Let (C, uj) be the verification reward function for unit test uj U, defined such that (C, uj) = 1 if uj passes on codebase C, and 0 otherwise. software engineering task is considered resolved if and only if the modified codebase successfully passes the entire test suite U, where is obtained by applying the predicted patch ˆp to the original codebase C. Formally, the resolution status Resolved(ˆp) is defined as: Resolved(ˆp) = (cid:88) (C, uj) = (2) uj where I[] denotes the indicator function. Consequently, the task-level reward is unity if and only if the verification reward is 1 for every individual test case uj U; otherwise, the reward remains 0."
        },
        {
            "title": "3.1 Training Framework and Environments",
            "content": "Training effective issue-solving code agents requires environments that closely reflect real-world Software Engineering workflows. Unlike static benchmarks (e.g., code genreation [20], websearch [21]), such tasks demand interactive execution environments with terminal access, persistent file systems, and package management support, allowing agents to compile, run, and debug code under realistic conditions. To enable reliable trajectory collection and maintain stability for SFT and RL, we apply robust and systematic framework for environment interaction. The overall inference pipeline is based on R2E-Gym framework [18], which is lightweight scaffold adapted from OpenHands and follows standard ReAct-style interaction loop [22]. To support this interaction logic, we adopt decoupled DockerServer architecture, where execution environments are deployed on dedicated CPU nodes, thereby remaining physically separated from model inference servers. This design enables the on-demand creation of lightweight and isolated coding environments while ensuring stable and uninterrupted inference. Each container provides the essential components required for agent training, including terminal interface, file system, and the corresponding code repositories. Network access is preserved to support standard package installation and dependency management. Our framework integrates several widely used open-source SWE Python datasets that rely on Docker, including SWE-Gym [23], R2E-Gym [18], SWE-smith [24], and SWE-rebench [25]. All unit tests are built offline and preloaded into their respective Docker images before evaluation. Given the large number of Docker images involved (approximately 13,000), we distribute them across multiple CPU nodes. During inference, requests are routed according to the associated issue identifier to locate the appropriate node and initialize the required environment. For each issue, the agent interacts with the environment through set of tools: bash_execute, file_editor, and submit. These tools provide the functionality necessary for resolving software 4 Table 1: Statistics and distribution of open-source SWE data. The right section details the yield from the rollout process and the final number of trajectories selected after filtering for use in SFT. Dataset Source Dataset Statistics Generation & Filtering # Samples # Images # Repos Res. Inst. Res. Trajs. Final Trajs. SWE-Gym SWE-rebench Real Real R2E-Gym Synthetic Synthetic SWE-smith 2,438 6,542 4,578 14,103 2,438 6,542 4,578 114 11 1,429 10 114 1,068 4,268 3,234 6,353 5,685 10,861 18,398 17, 2,948 7,157 2,462 - issues. In addition, we support higher-level tools built on the Language Server Protocol (LSP) [26], which are described in Section 5. To preserve evaluation integrity and mitigate the risk of git hacking [27], we enforce strict security constraints within the execution environments. In particular, the potentially exploitable git-related commands (i.e., git log and git show) are disabled to prevent the agent from accessing remote repositories or retrieving ground-truth solutions, thereby reducing the risk of data leakage. By combining physical isolation of Dockers with decoupled server design, the system sustains efficient policy inference under high concurrency. This infrastructure supports large-scale parallel data collection and stable RL training, making it well suited for scalable code agent development."
        },
        {
            "title": "3.2 Trajectory Synthesis and Data Curation",
            "content": "This section outlines the trajectory generation and fine-grained filtering pipeline for the SWE dataset. Table 1 summarizes the statistics of candidate datasets and rollouts, while Figure 3 illustrates the data distribution following the applied filtration strategies."
        },
        {
            "title": "3.2.1 Agent-Based Trajectory Rollout",
            "content": "As described in Section 3.1, we adopt multiple established software engineering datasets that are packaged with Docker environments, including SWE-Gym, SWE-rebench, R2E-Gym, and SWEsmith. We use MiniMax-M2 [28] and GLM-4.6 [29] as teacher models to generate trajectories, using the inference pipeline based on R2E-Gym framework. The rollout process follows an agent-based paradigm, in which the model interacts directly with realistic execution environment. Specifically, the agent is able to explore the target code repository, modify source files, write and execute unit tests to validate proposed fixes, and iteratively revise previous changes based on test outcomes. Throughout this process, we record both the models internal reasoning traces and its function call sequences, yielding complete interaction trajectories paired with corresponding reward signals. To assess the difficulty of individual issues, we conduct rollouts (with [3, 12]) for each issue and generate trajectories, that form the foundation for subsequent data filtering and training stages. Table 1 summarizes the composition and generation metrics of our rollout data corpus, which integrates hybrid of real-world and synthetic data sources. The collection demonstrates high structural diversity; notably, SWE-rebench provides extensive repository coverage with over 1,400 unique repos, while SWE-smith contributes significant volume of samples. Meanwhile, Figure 2 illustrates the correlation between interaction turns and model performance across four datasets. consistent inverse correlation is observed: resolve rates progressively decline as the number of turns increases, indicating that extended interaction budgets fail to guarantee success in more complex issue-solving problem. This persistence of failure stems from both the intrinsic difficulty of the tasks and the accumulation of noise inherent in long-horizon interactions. The distribution of solved samples is predominantly concentrated between 20 and 60 interaction turns, suggesting that the majority of instances are relatively simple and can be successfully resolved with limited number of interactions."
        },
        {
            "title": "3.2.2 Data Filtering",
            "content": "Format-Based Filter. We apply rigorous quality filtration protocol to the raw generated trajectories. First, we eliminate unsuccessful attempts by discarding trajectories with reward of zero. Second, 5 Figure 2: Distribution of resolve rates and resolved sample counts across interaction turns for SWEGym, SWE-smith, SWE-rebench, and R2E-Gym. to ensure computational stability, we prune instances exceeding context length of 80K tokens or 100 turns as we find that these outliers constitute merely 5% of the resolved instances yet pose disproportionate risk of out-of-memory (OOM) errors. Finally, we filter out trajectories containing syntactically invalid actions, specifically defining these as unparsable function calls or erroneous multiple invocations. Difficulty-Based Filter. Existing open-source SWE datasets lack explicit annotations for issue difficulty. As described in Section 3.2.1, we conduct rollouts (with [3, 12]) for each issue and compute the average resolve rate, which serves as proxy for issue difficulty. As illustrated in Figure 4, the distribution exhibits bimodal pattern where the majority of issues are either consistently solved (trivial) or consistently failed (intractable). Consequently, we exclude these polar extremes from the candidate pool, selectively retaining only those issues that yield mixture of successful and failed trajectories to ensure the training set focuses on samples with learnable difficulty. Figure 3 shows the evolution of the trajectory length distribution across the format-based and difficulty-based filtering stages. The results show that the filtering pipeline effectively removes outliers, particularly long-tail failure cases present in the initial distribution, leading to substantially smoother and more stable distribution of trajectory lengths in the final SFT dataset. Figure 3: Evolution of interaction turn distributions through sequential filtering stages. From left to right: (1) the initial distribution of successful and failed trajectories; (2) the distribution after applying format and reward constraints; and (3) the final refined distribution following difficulty-based filtering."
        },
        {
            "title": "3.3 Long-Horizon Supervised Fine-Tuning",
            "content": "Based on the filtered dataset and the corresponding trajectories, we perform multi-turn SFT on the Qwen2.5-Coder-32B-Instruct [4] and Qwen3-4B-Instruct-2507 models [30]. We apply YaRN [31] to extend the maximum context length from 32K to 80K tokens, enabling effective modeling of long multi-turn trajectories. During training, we adopt multi-turn masking strategy that excludes environment feedback obtained from Docker-based execution from the loss computation, ensuring 6 Figure 4: Difficulty distribution across SWE datasets estimated via best-of-n performance. The datasets include SWE-Gym, SWE-smith, SWE-rebench, and R2E-Gym. that the model focuses on learning reasoning and action generation rather than fitting execution outputs. After training on approximately 60K instances, we obtain SWE-Master-SFT, which serves as the initialization for the subsequent RL training. detailed analysis of data scaling is provided in Section 6.1, while the impact of data filtering is discussed in Section 6.2."
        },
        {
            "title": "3.4 Reinforcement Learning with Real Environments",
            "content": "This section provides an overview of the RL stage built upon SWE-Master. The data distribution used for RL is aligned with that of the SFT dataset, ensuring consistency between the two training phases. All interactions during training are conducted within real Docker-based execution environments, which guarantees the reliability and fidelity of environmental feedback."
        },
        {
            "title": "3.4.1 Policy Optimization Algorithm",
            "content": "We adopt Reinforcement Learning with Verifiable Reward (RLVR) as our foundational paradigm, employing the Group Relative Policy Optimization (GRPO) algorithm [32] to optimize SWE-MasterSFT directly. Building upon the established efficacy of prior reinforcement learning methodologies [33, 34, 35, 36], we incorporate several empirical optimizations to enhance training stability and performance: Leave-One-Out Advantage Estimation. To reduce the variance of the policy gradient estimate without introducing bias, we compute the advantage for each sample by normalizing its reward against the average reward of the other samples in the group, excluding the sample itself. Mitigation of Inherent Bias. To prevent biased optimization, we modify two standard normalization terms. We replace the dynamic division by the trajectory length 1/oi (where oi represents the i-th generated trajectory for prompt) with fixed constant scaling, as formulated in Eq. 3. This modification prevents the policy from favoring brevity in correct answers or verbosity in incorrect ones. Additionally, we omit the standard deviation normalization from the advantage calculation to avoid biasing updates based on task difficulty variance. Clip-Higher. To counter the phenomenon of entropy collapse and sustain exploration, we adopt the clip-higher strategy. This modification relaxes the upper clipping bound, addressing the limitation in standard GRPO where the probability growth of low-likelihood exploration tokens is disproportionately constrained compared to high-likelihood exploitation tokens. Removal of KL Divergence. We eliminate the KL divergence penalty from the objective function. This unbinds the policy from the trust region of the initial SFT reference model, granting the model the flexibility to optimize more aggressively towards the verifiable reward signal. Incorporating these modifications, the final objective function is defined as follows: (θ) = qP (Q),{oi}G i=1πold 1 (cid:88) i=1 1 Lmax oi (cid:88) t= (cid:16) ρi,t(θ) ˆAi, clip (ρi,t(θ), βlow, βhigh) ˆAi (cid:17) min (3) where (Q) denotes the distribution of input prompts, and the clipping bounds are defined as βlow = 1 εlow and βhigh = 1 + εhigh. The term Lmax represents fixed constant for length normalization. The probability ratio ρi,t(θ) and the group-relative advantage ˆAi are given by: ρi,t(θ) = πθ(oi,tq, oi,<t) πold(oi,tq, oi,<t) , ˆAi = Ri 1 (cid:88) Rj. j=1,j=i"
        },
        {
            "title": "3.4.2 Reward Design",
            "content": "We employ standard binary outcome-based reward function aligned with the evaluation protocols of SWE-bench. For given issue, if the patch submitted by the policy model successfully passes all F2P and P2P unit tests, the reward is = 1; otherwise, the reward is = 0. Although SWE-Master is fine-tuned on long-context trajectories (up to 80K tokens) during the SFT phase, extending this capability to RL presents significant challenges. We observe that during the initial stages of RL, the policy model sometimes exhibits uncertainty, engaging in repetitive cycles of unit test generation and modification without issuing final submission. This behavior leads to high rate of trajectory truncation (approximately 20%) due to exhaustion of token or turn budgets. Consequently, the model receives zero rewards despite making partial progress, which fails to reinforce effective reasoning behaviors and causes the average training reward to degrade (see Figure 5), eventually leading to training collapse. However, offline evaluation of the patches generated during the RL process reveals that approximately 24.3% of the truncated trajectoriesthose without final submissionsuccessfully resolve the current issue. This failure to submit primarily stems from model underconfidence, which drives the agent into redundant verification loops, or from the generation of excessively rigorous unit tests that create self-imposed barriers to submission. To address this pathology and stabilize training, we implement forced submission mechanism, coupled with reward shaping strategy. Specifically, if trajectory terminates due to budget exhaustion (e.g., timeout or maximum turns) rather than voluntary submission, we enforce patch submission based on the current state of the repository to evaluate its correctness. This mechanism ensures that each trajectory yields concrete submission. We then apply stop-reason-dependent modulation to the final reward. Furthermore, the training process relies on launching and executing containerized environments. Due to hardware or storage-related instability on machines hosting Docker images, container startup failures may occasionally occur. Such failures are unrelated to the policy models behavior and can introduce spurious noise into training if not properly handled. To preserve training stability and integrity, we apply container error masking strategy: all tasks affected by container startup failures are masked and excluded from reward computation and policy updates. This design prevents infrastructure-level errors from negatively influencing the learning process. Based on the design principles discussed above, our reward design is formulated as follows: = routcome α routcome 0 if = DONE if {TIMEOUT, MAX_STEPS, MAX_TOKENS} if {CONTAINER_FAILED} = (cid:26) 0 1 if {CONTAINER_FAILED, SERVER_ERROR} otherwise (4) (5) where and denote the final shaped reward and the loss mask, respectively. Here, routcome {0, 1} represents the binary evaluation result of the patch, and signifies the trajectory termination reason. The parameter α (0, 1) is penalty coefficient for forced submissions, set to 0.5 in our experiments. The specific categories of termination conditions are defined as follows: 8 DONE represents the agent voluntarily submit patch before reaching resource limits. TIMEOUT, MAX_STEPS, and MAX_TOKENS denote forced terminations due to budget exhaustion (e.g., reaching temporal, token, or interaction step limits). CONTAINER_FAILED refers to unforeseen infrastructure-level failures (e.g., hardware issues or environment crashes). This design serves multiple strategic objectives. Primarily, it ensures training integrity by masking loss of failed trajectories stemming from container runtime errors, guaranteeing that policy updates are driven exclusively by valid agent-environment interactions. Meanwhile, the modulated reward coefficient α calibrates the trade-off between exploration and efficiency; it encourages the model to undertake the extended reasoning necessary for complex tasks while implicitly penalizing redundant behaviors that lead to timeout. Finally, the forced submission mechanism mitigates reward sparsity by validating potential solutions even upon budget exhaustion, thereby preventing the training collapse associated with vanishing signalsa benefit further substantiated in Section 6.3."
        },
        {
            "title": "3.4.3 Training Tricks",
            "content": "Budget Awareness. Prior studies [37] demonstrate that explicitly providing agents with budgetrelated signals during long-horizon interactions enables more rational behavior planning and more effective allocation of limited interaction steps. Motivated by these findings, we incorporate budget awareness into the agentenvironment interaction process. Concretely, at the end of each interaction turn, the environment returns not only the standard feedback but also the remaining budget, defined as the number of turns left before termination. This design allows the policy model to explicitly condition its decisions on the remaining interaction budget, thereby encouraging more deliberate planning and earlier convergence toward viable solution. The corresponding environment additional repsonse is as follows:"
        },
        {
            "title": "Environment Response for Budget Awareness",
            "content": "This is step STEP_INDEX of maximum of MAX_STEPS. Steps Remaining: STEP_REMAINING. Restriction of Git Commands. Consistent with the trajectory distillation stage in SFT, we strictly restrict the use of git-related commands during reinforcement learning. In particular, when the model attempts to invoke commands such as git log or git show, which may reveal solutionrelevant information without genuine reasoning, the environment immediately returns warning message. The model is explicitly informed that such behavior is prohibited and is encouraged to rely on its own analysis and code modifications instead. This constraint is designed to prevent shortcut exploitation and to ensure that the learned policy reflects authentic problem-solving capabilities. The corresponding environment repsonse is as follows:"
        },
        {
            "title": "Environment Response for Git Command",
            "content": "Bash command git show and git log is not allowed. Please use different command or tool. Environmental Response Masking. During training, we implement environment response masking to exclude environment feedback from both loss and advantage calculations. This strategy ensures that only the tokens generated by the model itself contribute to the optimization process, thereby enhancing training stability and preserving the structural integrity of the agents reasoning sequences."
        },
        {
            "title": "3.4.4 Dynamics of Policy Learning in RL",
            "content": "Figure 5 illustrates the RL training dynamics of the policy model, where clear synergy between performance gains and behavioral adaptation emerges. The reward exhibits consistent upward trajectory from an initial value of 0.35 toward stabilized peak, demonstrating that the model effectively learns to navigate repository environments to secure verifiable rewards. This performance improvement is accompanied by gradual increase in interaction turns, suggesting that as the agent matures, it learns to utilize larger interaction budgetlikely for more exhaustive exploration and rigorous self-verificationto tackle complex software issues. Simultaneously, the entropy follows 9 Figure 5: The training dynamics of interaction turns, reward and entropy for SWE-Master RL training. steady decline without exhibiting the phenomenon of entropy collapse. Together, these metrics signify stable and effective reinforcement learning process."
        },
        {
            "title": "3.5 Test-Time Scaling",
            "content": "Test-Time scaling (TTS) significantly enhances the performance of SWE agents by leveraging increased inference-time compute to navigate complex task spaces [38, 39]. Typically, TTS is implemented via two primary paradigms [40]: sequential scaling, which involves increasing the allowance of interaction turns; and parallel scaling, which generates multiple trajectories and corresponding patches, followed by employing specific verifier to select the optimal candidate for submission. In this section, we investigate the performance of SWE-Master under both scaling strategies."
        },
        {
            "title": "3.5.1 Sequential Scaling",
            "content": "Figure 6: Performance scaling of SWE-Master via test-time scaling. Left: Sequential scaling illustrates the impact of increasing interaction turn limits. Right: Parallel scaling demonstrates the performance gains achieved through increased rollout breadth and candidate verification. Sequential scaling is implemented by extending the maximum limits on generated tokens and interaction turns. With this expanded computational budget, the model is empowered to explore the repository structure more comprehensively and generate more unit tests to validate its proposed modifications, thereby enhancing the accuracy of the final submitted patch. Given the significant heterogeneity in file counts and repository content across different problem instances, which leads to substantial variance in the token consumption for environment feedback and code modifications, we adopt the number of interaction turns as the primary constraint metric rather than token consumption. We fix the maximum context length at 128K to isolate the impact of interaction depth, and systematically scale the maximum interaction limit from 25 to 150. As illustrated in Figure 6, increasing the interaction budget from 25 to 150 turns yields significant performance gains for both models. The SWE-Master-RL model demonstrates superior scalability, effectively leveraging the extended turn limit to 61.4% resolve rate, consistently outperforming the SWE-Master-SFT model. While average turn usage (dashed lines) indicates active utilization of the 10 available budget, the resolve rates for both models begin to plateau beyond 125 turns, suggesting diminishing marginal returns at higher limits."
        },
        {
            "title": "3.5.2 Parallel Scaling",
            "content": "Parallel scaling enhances problem-solving performance by generating multiple candidate trajectories for single issue and employing selection mechanism to identify the most viable patch. The efficacy of this approach hinges critically on the ability to distinguish correct solutions from incorrect ones within pool of candidates. Existing approaches [17, 23] typically rely on training verifier to predict scalar probability (i.e., binary Yes/No classification) based on the generation trajectory or the final patch. However, these black-box verifiers often lack interpretability and struggle to accurately assess correctness when verifying long, noisy context windows characteristic of repository-level software engineering. To address these limitations, we use the SWE-World model [19], specialized model designed to perform simulated evaluation rather than simple probability estimation. Unlike traditional verifiers, SWE-World aligns the selection process with the formal execution environment found in SWE-bench tasks. Upon the completion of candidate trajectory, SWE-World extracts the context relevant to the evaluation, including the modified files, test cases, and the patch itself. It then processes this input to generate simulated test report and predicted reward. This approach provides granular, interpretable rationale for selection, mimicking the feedback of real compiler and test runner. We construct the training data for SWE-World through an offline rollout process. For every patch applied during the rollout, we capture the relevant execution context alongside the ground-truth test report and the actual reward obtained from the environment. To ensure high data quality, we employ Qwen3-235B-A22B-Thinking-2507 to perform reverse reasoning [41]. This teacher model analyzes the causal relationships between the input context and the structured evaluation feedback to reconstruct reasoning chains, thereby creating logically coherent dataset for SFT. To implement parallel scaling, we generate independent rollout trajectories for given issue using the policy model, yielding candidate trajectory set = {τ1, τ2, . . . , τn} and corresponding patch set = {p1, p2, . . . , pN }. Subsequently, to robustly estimate the correctness of each solution, we perform stochastic reward simulations for each trajectory τi using the reward model (setting = 3 in our experiments). Formally, the optimal trajectory τ is identified by maximizing the expected reward, which is approximated by the arithmetic mean of the sampled rewards: τ = argmax τiT"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) j=1 ˆri,j , (6) where ˆri,j denotes the reward obtained from the j-th simulation iteration for candidate τi. In cases where multiple candidates achieve the maximal score, ties are broken via random selection. Finally, the patch associated with the selected trajectory τ is submitted to the environment as the definitive solution for the current issue. As illustrated in Figure 6, increasing the number of rollouts yields consistent improvement in the SWE-bench Verified resolve rate for both the SFT and RL models. Pass@K (dashed lines) represents the theoretical upper bound where an oracle selects the correct patch, while TTS@K (solid lines) reflects the actual performance using the SWE-World. We observe that the RL model consistently outperforms the SFT baseline, starting at 61.4% and surpassing 70.8% at TTS@8. Crucially,TTS@K performance closely tracks the theoretical optimal curves (Pass@K), indicating that our selection mechanism is highly effective at identifying the correct solution within the generated candidate pool. This strong alignment validates that the simulated evaluation approach successfully converts the increased computational budget into tangible performance gains, minimizing the gap between potential and realized accuracy. To evaluate the fidelity of the reward predictions, we assess the alignment between the simulated rewards and the ground-truth environmental feedback across the evaluated trajectories. As presented in Table 2, our trained SWE-World achieves an accuracy of 77.59%, with recall of 71.40% and precision of 71.64%. This performance confirms that the SWE-World effectively functions as high-fidelity simulated execution environment, enabling it to serve as dependable surrogate for the actual sandbox during candidate patch selection. 11 Table 2: Comparison of Precision, Recall, and Accuracy for trajectory reward prediction. Real-Docker serves as the ground truth baseline for evaluating SWE-World. Method Precision Real-Docker SWE-World 100.00 77.59 Recall 100.00 71.40 Accuracy 100.00 71."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Evaluation Datasets. We primarily evaluate our method on the SWE-bench Verified [16] dataset, which consists of 500 solvable instances curated from the original SWE-bench benchmark [3]. The dataset is constructed from real-world GitHub issues spanning 12 open-source python repositories and is designed to assess the end-to-end issue resolution capabilities of LLMs. The verified split provides more reliable evaluation setting by restricting instances to those with confirmed valid solutions. Evaluation Metrics. The models performance is quantified by the resolve rate, representing the percentage of tasks successfully addressed, following the protocol detailed in Section 2.3. Baselines. We employ Qwen2.5-32B-Coder-Instruct [4] and Qwen3-4B-Instruct-2507 [30] as backbone models. To evaluate SWE-Master, we compare its performance against leading open-source code agents [4, 30, 23, 18, 42, 24, 8, 43, 35, 44, 45, 7, 11, 40, 46, 47, 48, 49], and frontier opensource foundations [50, 51, 52, 53, 27, 54]. Given that the design of an agents scaffold significantly influences final resolve rates, we directly cite the metrics reported in the original publications to ensure fair comparison under each models intended optimal configuration. Implementation Details. We utilize R2E-Gym framework, streamlined adaptation of the OpenHands, for trajectory generation, while leveraging OpenRLHF and RLLM for SFT and RL, respectively. During the SFT phase, models are trained over 5 epochs using maximum context length of 80K tokens and global batch size of 256. We employ cosine learning rate scheduler, decaying from peak of 5 105 to 5 106, with warmup ratio of 0.1. Subsequently, in the RL stage, we utilize constant learning rate of 1 106 and batch size of 32 problems, with each problem generating 4 parallel rollouts at sampling temperature of 1.0. The exploration is constrained by per-trajectory timeout of 5,400 seconds, maximum interaction turn of 150 turns, and context window of 108K tokens. For final evaluation, inference is performed with temperature of 0.7, maximum context capacity of 128K tokens, and the maximum interaction turn is 150."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table3 shows the results of SWE-Master and the baselines on SWE-bench verified. We can obtain the following observations: Achieving Frontier Performance among Open-Source Code Agents. SWE-Master demonstrates superior performance compared to existing open-source code agents. Specifically, our SWE-Master32B-RL model achieves resolve rate of 61.4% at Pass@1, significantly outperforming strong baselines such as daVinci-Dev (58.5%) and SWE-SWE-Compressor (57.6%). This indicates that our framework effectively unleashes the potential of the 32B parameter models, achieving high efficacy without relying on excessive model scaling. Demonstrating the Efficacy of Reinforcement Learning across Scales. The transition from SFT to RL consistently yields notable performance gains across different model scales, validating the robustness of our RL framework. For the smaller Qwen3-4B-2507 backbone, RL training boosts the resolve rate from 27.6% to 33.4% (an absolute improvement of 5.8%). Similarly, on the larger Qwen2.5-Coder-32B backbone, the performance increases from 57.8% to 61.4%. These results confirm that our RL strategy effectively guides models to explore and internalize complex verification strategies beyond simple behavior cloning. 12 Model/Method BackBone Scaffold Training Resolve Rate (%) Minimax-M2.1 GLM-4.7 DeepSeek-V3.2 GPT-OSS-20B GPT-OSS-120B Qwen2.5-Coder-32B Qwen3-Coder-30B-A3B SWE-Gym-32B R2E-Gym-32B + TTS@16 Skywork-SWE-32B + TTS@8 SWE-Fixer-72B SWE-agent-LM-32B SA-SWE-32B SWE-Swiss-32B DeepSWE-32B-Preview + TTS@16 Seed-OSS-36B SWE-Mirror-LM-32B Kimi-Dev-72B + TTS@40 FrogBoss-32B SWE-Compressor SWE-Lego-Qwen3-32B + TTS@16 daVinci-Dev-32B daVinci-Dev-72B SWE-Master-4B-SFT SWE-Master-4B-RL SWE-Master-32B-SFT + TTS@8 SWE-Master-32B-RL + TTS@ Open-Source Foundation Models - - - - - Internal Internal Internal Internal Internal - - - - - Open-Source Code Agents - - Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B-Inst Qwen2.5-72B-Base Qwen2.5-Coder-32B-Inst Qwen3-32B Qwen2.5-32B-Inst Qwen3-32B Qwen3-32B - Qwen2.5-32B-Inst Qwen2.5-72B-Base Qwen2.5-72B-Base Qwen3-32B Qwen2.5-Coder-32B-Inst Qwen3-32B Qwen3-32B Qwen2.5-32B-Base Qwen2.5-72B-Base Qwen3-4B-Inst-2507 Qwen3-4B-Inst-2507 Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B Qwen2.5-Coder-32B-Inst Qwen2.5-Coder-32B OpenHands OpenHands OpenHands R2E-Gym R2E-Gym OpenHands OpenHands Agentless SWE-agent OpenHands Agentless OpenHands OpenHands OpenHands MOpenHands SWE-Agent Agentless SWE-Agent OpenHands OpenHands OpenHands SWE-Agent MT+SFT SWE-Agent MT+SFT - - SFT SFT SFT SFT SFT SFT SFT RL SFT+RL RL RL - SFT SFT+RL SFT+RL SFT+RL SFT SFT SFT R2E-Gym R2E-Gym R2E-Gym R2E-Gym R2E-Gym R2E-Gym SFT SFT+RL SFT SFT+RL SFT+RL SFT+RL 74.0 73.8 73.1 60.7 62. 6.2 51.6 20.6 34.4 49.4 38.0 47.0 32.8 40.2 39.4 58.0 42.2 59.0 56.0 52.2 48.6 60.4 54.6 57.6 52.6 58.8 56.1 58.5 27.6 33.4 57.8 70.2 61.4 70.8 Table 3: Performance comparisons between SWE-Master and the baselines on SWE-bench Verified. The baseline results are referenced from their respective papers. The best and second best Pass@1 results are highlighted in bold and underlined, respectively. Unlocking Peak Performance via Test-Time Scaling. Incorporating Test-Time Scaling via the simulated verification and ranking mechanism using SWE-World, yields substantial improvements in resolve rates. With moderate compute budget of 8 rollouts (TTS@8), SWE-Master-32B-SFT improves by 12.4% (from 57.8% to 70.2%), and SWE-Master-32B-RL improves by 9.4% (from 61.4% to 70.8%). Notably, our TTS@8 approach is significantly more efficient and effective than competitors utilizing larger budgets, such as DeepSWE-32B + TTS@16 (59.0%) and R2E-Gym-32B + TTS@16 (49.4%). These results demonstrate the effectiveness of combining strong policy with robust selection mechanism."
        },
        {
            "title": "4.3 Observations on Model Behavior during Evaluation",
            "content": "In this section, we analyze the behavior exhibited by SWE-Master during the evaluation. As illustrated in Figure 7, we observe general inverse correlation between trajectory length and resolve rate for both models, confirming that instances requiring extended reasoning chains are inherently more prone to failure due to the higher intrinsic complexity of these tasks. Despite this difficulty, the RL model exhibits pronounced distributional shift toward the upper limits of the turn budget (peaking at 120140 turns), contrasting with the SFT models tendency to submit earlier. This varying interaction 13 Figure 7: Resolve rates and sample distributions of SWE-Master with respect to the number of interaction turns, based on the corresponding evaluation results. Figure 8: Distribution of tool usage in the evaluation results of SWE-Master. turn is intrinsically linked to the tool usage patterns shown in Figure 8, where the frequency of execute_bash and file_editor_replace operations increases significantly in the RL model. This suggests that the reinforcement learning process encourages the agent to engage in more active iterative debuggingleveraging the expanded budget to repeatedly execute tests and modify code. IDE-Level Code Capacity This section introduces the integration of lsp_tool, which empowers the code agent with holistic understanding of the codebase and facilitates precise navigation of complex repository structures. We systematically evaluate its impact across training and inference phases, showcasing how this design bridges the gap between basic bash-level execution behavior and senior-level capability."
        },
        {
            "title": "5.1 From Grep to LSP-Based Code Navigation",
            "content": "Current frontier software agent frameworks, such as OpenHands [6] and SWE-agent [2] encounter significant bottlenecks when addressing non-crashing defects where the context is ambiguous. Predominantly, these systems rely on Linux CLI-based lexical stream retrieval tools (e.g., grep, find) for code context localization. While effective for simple pattern matching, these tools lack semantic understanding. When identifying bugs involving heavily overloaded function names or multi-level function calls across files and repositories, such methods [6, 2, 18] are not only inefficient and unreliable but often retrieves irrelevant contexts that obscure true semantic correlations and call relationships, severely hindering LLMs from achieving robust defect repair. While some studies have recognized these limitations and incorporated Abstract Syntax Tree (AST) for localization [55, 56, 57], they lack standardized interfaces, cross-language scalability, and rigorous verification on benchmarks like SWE-bench Verified. To bridge this semantic gap, we propose novel approach inspired by modern integrated development environments (IDEs). We introduce the first unified, language-agnostic code navigation tool for agents based on the Language Server Protocol (LSP). In contrast to previous ad-hoc approaches, our lsp_tool implementation provides standardized interface that enables advanced semantic featuresincluding precise go to definition and find referencesconsistent with modern IDEs. We 14 integrated this tool into the framework described in Section 3.1 and initially validated its effectiveness using powerful open-source foundation models, specifically MiniMax-M2.1 and GLM-4.7, confirming that lsp_tool significantly enhances repository-level understanding. Building on this validation, we employ distillation to endow our SWE-Master model with this advanced IDE-level code navigation capability, enabling it to master complex repository exploration with the efficiency required for real-world deployment."
        },
        {
            "title": "5.2 Overview of LSP Tools",
            "content": "In this section, we introduce the foundational technology behind our approach, detail the implementation of the lsp_tool specifically designed for code agents, and illustrate how this tool is integrated into the agentic workflow."
        },
        {
            "title": "5.2.1 The Language Server Protocol",
            "content": "The Language Server Protocol [26], originally introduced by Microsoft, is pivotal standard designed to unify the interaction between development tools (clients) and language-specific intelligence providers (servers). It defines standardized communication protocol based on JSON-RPC that decouples the IDE from the underlying language compilers or analysis engines. This decoupling allows text editors and IDEs to interact with different programming languages through uniform interface, eliminating the need for -to-N point-to-point integrations, as depicted in Figure 9a. Appendix A.1 shows more details about this. (a) Language Server Protocol (b) Illustration of LLM navigating through code repository with LSP tools. Figure 9: Language Server Protocol from supporting integrated development environments to supporting code agents navigating through code repository. Taking the popular IDEVisual Studio Code (VSCode) as prime example, LSP provides unified interface that facilitates seamless communication between the editor and language servers. This integration empowers the editor with access to deep static analysis, effectively transforming standard text editor into powerful, semantics-aware development environment (more details in Appendix A.1). It powers essential features such as semantic symbol resolution, cross-file reference tracking, and automated refactoring. For instance, when developer hovers over function name, LSP provides signature help and documentation; clicking on symbol triggers precise jump to its definition or lists all its references across the workspace. These capabilities allow developers to bypass manual text searching, offering real-time, accurate code insights and error detection. By adopting LSP, the software ecosystem gains high scalability, where complex, language-specific logic is implemented once in Language Server and reused across various editing environments."
        },
        {
            "title": "5.2.2 LSP Tools Implementation for Code Agents",
            "content": "Inspired by the workflow of human developers who rely on intelligent IDEs for efficient debugging and code navigation, we designed and implemented unified lsp_tool interface tailored for code agents. Unlike human-facing GUIs, this tool adapts the language server protocol into function-calling format optimized for LLMs, bridging the gap between raw protocol data and model comprehension. Our implementation encapsulates comprehensive suite of features defined in the LSP specification. As presented in Table 4, we categorize these supported capabilities into four distinct groups: repo navigation, dependency analysis, code understanding and workspace search. These features are exposed to the agent via unified API, allowing it to perform structural exploration of the codebase. 15 Category Feature Name Description & Utility for Agents Repo Navigation go to definition go to declaration go to type definition go to implementation Locate the exact definition of symbol (function, class, variable). Jump to the declaration of symbol. Navigate to the definition of variables type. Find concrete implementations of an interface or abstract method. Dependency Analysis prepare call hierarchy incoming calls outgoing calls Initialize the call hierarchy for selected function. Identify all functions that call the target function (Callers). Identify all functions called by the target function (Callees). Code Understanding signature help document symbols document color Provide parameter details and return types for function calls. Extract structural outline (tree) of the current file. Identify color representations within the code (if applicable). Workspace Search workspace symbols find references Search for symbols globally across the entire project. List all usages of specific symbol across the workspace. Table 4: Overview of LSP features integrated into the code agent. To facilitate better model interaction, we implemented parser between the agent and the Language Server, avoiding the direct exposure of complex raw JSON-RPC methods. This parser perform pre-processing and post-processing on the interactions between the model and the Language Server, enabling the agent to interact with the language server losslessly without grappling with the protocols underlying complexity. The specific detail are provided in appendix A.2."
        },
        {
            "title": "5.2.3 Workflow Integration",
            "content": "We integrate the lsp_tool into the code agents software engineering task-solving loop, as illustrated in Figure 9b. In this workflow, the agent operates as central decision-maker. Upon receiving task (e.g., GitHub issue), the agent analyzes the repository. Instead of merely adopting brute-force find commands, the agent utilizes lsp_tool to traverse the codebase structurallyjumping to definitions to understand logic, querying references to assess impact, and analyzing call hierarchies to trace bug propagation. This structured observation is then fed back into the models context, allowing it to reason about the bugs root cause with high fidelity before generating patch. The integration of the lsp_tool helps transform the agents operating paradigm from blackbox-testing to white-box-analysis. Previously, agents largely relied on trial-and-error approachhypothesizing fix, running scripts, and analyzing error logsa process prone to redundant, ineffective debugging loops. By empowering the agent to read the code structure directly (e.g., via call hierarchies and definition jumps), our approach allows for precise localization of logic defects without the need for constant execution. This shift not only significantly enhances exploration efficiency by reducing the number of invalid interaction turns but also mitigates the risk of hallucinated modifications, ensuring that the agent fully comprehends the code context before attempting fix."
        },
        {
            "title": "5.3.1 Verification in Open-Source Foundation Models",
            "content": "To verify the effectiveness of our proposed toolchain, we conduct experiments on the SWE-bench Verified dataset and select pyright as the underlying language server engine because the dataset is python-only. It is important to note that our system design is modular and language-agnostic; switching to other programming languages (e.g., Java or C++) is plug-and-play process that requires only replacing the corresponding LSP binary within the Docker container and updating the startup command. In all evaluations involving the lsp_tool, we enabled the full suite of features listed in Table 10, with the exception of get_declaration and get_implementation. These two features were omitted solely because the pyright language server does not currently support them for Pythons dynamic typing system. We validate the efficacy of the lsp_tool on MiniMax-M2.1 and GLM-4.7. As shown in Table 5, integrating LSP capabilities yields consistent performance gains across both models. For MiniMax-M2.1, the inclusion of LSP improves the resolution accuracy from 68.4% to 70.4%, while simultaneously reducing the average interaction turns from 82.0 to 77.0. Similarly, GLM-4.7 achieves higher 16 accuracy of 67.6% with reduced interaction turns. These results demonstrate that LSP tools effectively lower the cognitive burden on the model: by providing deterministic semantic context instead of noisy keyword search results, the agents can navigate to the bug location more directly, thereby solving tasks more efficiently. Model Scaffold w/o LSP w. LSP Acc. (%) Avg. Turns Acc. (%) Avg. Turns M2.1 R2E-Gym GLM-4.7 R2E-Gym 68.4 66. 82.0 97.3 70.4 67.6 77.0 94.4 Table 5: Performance comparison of GLM-4.7 and Minimax-M2.1 with and without LSP tools."
        },
        {
            "title": "5.3.2 Continual Training of SWE-Master",
            "content": "Motivated by these results on powerful teacher models, we distill the high-level repository navigation skills into SWE-Master, to enable similar IDE-level behaviors. Using the integration method from Section 3.2.1, we incorporate LSP tools and leverage GLM-4.6 and Minimax-M2 as teachers to generate trajectories. rule-based filter combined with an LLM judge ensures only high-quality trajectories demonstrating correct LSP use and problem solving are selected. We then perform SFT starting from the SWE-Master-RL checkpoint, mixing new LSP trajectories with original SFT data to avoid overfitting. Detailed filtering prompts are in Appendix B.1. As shown in Table 6, the distilled model achieves significant efficiency gains while maintaining resolve rate comparable to the strong RL baseline (61.0% vs. 61.4%). The integration of LSP capabilities reduces input and output token consumption by 23.7% and 16.3%, respectively, and shortens the average trajectory by 17.5%. This efficiency stems from the agents shift from verbose, trial-and-error lexical searches to precise semantic queries, achieving equivalent proficiency with substantially lower computational costs. It should be noted, however, that the efficiency gains are not solely attributable to LSP; they also reflect behavioral shifts induced by the continual training of SWE-Master-RL. Model Setting Input Tok. Output Tok. Avg. Turns Resolve Rate RL RL + Cont. SFT w. LSP. w/o LSP. (k) 5549.6 4232.1 (k) 32.5 27.2 (k) 111.2 91.7 (%) 61.4 61.0 Table 6: Comparison of token efficiency and performance metrics of SWE-Master-RL before and after continual training with LSP tools."
        },
        {
            "title": "5.3.3 Case Study on LSP Tool Use",
            "content": "To explicitly demonstrate how lsp_tool empower the model to transcend the limitations of lexical search, we present comparative case study on pydata_xarray-6812 from SWE-bench Verified. The task requires fixing subtle bug where .swap_dims() incorrectly modifies the original dataset object in-place due to improper reference handling. Figure 10 visualizes the contrasting trajectories from current frontier framework OpenHands and our enhanced framework integrated with lsp_tool. The Struggle of Lexical Retrieval (Baseline). As illustrated on the left side of Figure 10, the OpenHands agent relies on lexical stream retrieval tools, such as grep and find, for bug localization. In Step 5, it attempts to search for the .swap_dims() function definition via text matching. However, since grep lacks semantic awareness of function boundaries, the agent is forced to scroll through the code by repeatedly increasing line counts, wasting 4 steps just to barely piece together code snippet. This fragmented reading induces Cognitive Friction, impeding the agents ability to comprehend dependence relationships between objects, and to construct mental model of cross-file dependencies. Lacking mechanism to jump to definitions, the agent falls into Information Underload, still searching for information even in step 40, which means it consumes tokens reading code without grasping the 17 Figure 10: Example of trajectory generated using LSP tools. structural relationships. Consequently, the agent falls into black-box trial-and-error loop (Steps 54-81), repeatedly running tests without pinpointing the logic error, eventually failing after 91 steps. The Success of IDE-level Navigation (Ours). In contrast, our enhanced framework (right side of Figure 10), equipped with the lsp_tool suite, demonstrates IDE-level intelligent navigation patterns. Instead of guessing file paths, Global Symbol Resolution (Step 4): the agent utilizes get_workspace_symbols to bypass massive textual noise. It retrieves all occurrences of swap_dims across the codebase, thereby constructing mental model of cross-file dependencies. Hierarchical Understanding (Step 8): By calling get_document_symbols, the agent quickly grasps the architecture of the file, specifically the class structure of Variable. It identifies relevant methods like to_index_variable without reading the entire file, achieving efficient code exploration. Deterministic Trace (Step 22): The crucial breakthrough occurs when the agent uses get_definition to trace the _replace_with_new_dims method. Unlike the baseline which implies relationships from text, LSP provides deterministic link to the underlying logic. This allows the agent to identify that IndexVariable.to_index_variable returns self (a shared reference) rather than copy, effectively exposing the root cause of the mutability bug. This IDE-level code navigation capability enables SWE-Master to solve the task in just 57 stepsa 37% reduction in trajectory lengthproving that the distilled model has successfully learned to read code structure rather than just search text."
        },
        {
            "title": "6.1 Data Scaling for SFT",
            "content": "To investigate the data scaling laws within the multi-turn SFT phase, we evaluate the models performance and behavioral evolution as the training corpus expands from 0 to 60K samples. As illustrated in Figure 11 (Left), the resolve rate exhibits robust logarithmic growth, surging from baseline of 6.2% to 57.8%; however, the marginal utility of additional data begins to plateau beyond 48K, suggesting saturation point for supervised behavior cloning. Crucially, this performance 18 improvement is accompanied by marked increase in inference efficiency, as shown in Figure 11 (Right). Both the average interaction turns and the thinking token consumption demonstrate consistent downward trend, decreasing from approximately 115 to 94 turns and 14.8k to 12.7k tokens, respectively. This inverse correlation implies that as the model absorbs more expert demonstrations, it internalizes more efficient problem-solving heuristics, thereby reducing redundant exploration and ineffective reasoning loops while achieving higher accuracy with fewer computational resources. Figure 11: Impact of SFT data scaling on performance and reasoning efficiency."
        },
        {
            "title": "6.2 Data Filtering for SFT",
            "content": "To assess the impact of data quality control, we examine the performance differential between models trained with and without difficulty-based filtering as shown in Section 3.2.2. Specifically, w. difficultybased filtering excludes problems with invariant outcomes in which all corresponding trajectories either consistently pass or consistently fail, whereas w/o difficulty-based filtering involves random sampling directly from the pool of all correct trajectories. As presented in Table 7, applying the filtering mechanism yields distinct improvement in the resolve rate, suggesting that theverified, high-quality trajectories is essential for model training. Notably, this performance gain is achieved with negligible variations in average interaction turns and thinking token consumption. It refines the correctness of the agents reasoning, enabling it to resolve issues more reliably without altering the fundamental behavioral complexity or computational cost. Table 7: Ablation study on the effectiveness of difficulty-based filtering."
        },
        {
            "title": "Method",
            "content": "Resolve Rate (%) Avg. Turns Avg. Think Tokens w/o difficulty-based Filtering w. difficulty-based Filtering 54.2 57.8 94.57 93.58 12,850 12,"
        },
        {
            "title": "6.3 Loss Masking and Reward Design in RL",
            "content": "DeepSWE [35] suggests that setting the reward to zero and masking the loss for trajectories truncated by environmental constraints (e.g., maximum context, timeouts, or interaction turn limits) enhances stability during the RL phase. However, contrary to these findings, our experiments indicate that applying this masking strategy to SWE-Master precipitates continuous degradation in reward and eventual training collapse, rather than fostering efficient reasoning. We attribute this discrepancy to fundamental differences in policy model initialization and data complexity: whereas DeepSWE trains from scratch on the relatively simpler R2E-Gym dataset (as detailed in Section 3.2.1 and Figure 4), SWE-Master undergoes cold-start stage through sufficient SFT on long-horizon heterogeneous data, resulting in distinct optimization landscape. This divergence is empirically illustrated in Figure 12, which compares the training dynamics of the DeepSWEs masking strategy (blue line) against our reward shaping mechanism (orange line). While the masking approach leads to instability, our method ensures stable reward growth and facilitates deeper interaction, as evidenced by the increasing trend in trajectory length. Although 19 this encouragement of longer reasoning chains imposes slight penalty on training throughput due to increased computational overhead, it proves indispensable for achieving robust convergence on complex software engineering tasks. Figure 12: Comparison of interaction turns, reward, and entropy between DeepSWE and our proposed loss masking and reward design strategies based on the SWE-Master-SFT model."
        },
        {
            "title": "6.4 Git Hacking",
            "content": "Figure 13: Comparison between the models autonomous git hack attempts (top), such as git log and git show, and the corresponding environmental intervention (bottom). Consistent with observations in MiMo-v2-Flash [27], we found that in SWE scenarios, the model autonomously attempts to exploit commands such as git show and git log to retrieve golden patches directly from the network, as shown in Figure 13. To address this, as detailed in Section 3.1, the environment is configured to intercept such attempts and return prohibition warning whenever the model seeks to extract solutions via unauthorized network access. This constraint was strictly enforced during both the SFT and RL stages. We conduct an ablation study on SWE-Master to evaluate the impact of this mandatory anti-hacking mechanism, with results presented in Table 8. Interestingly, removing the restriction on Git commands results in slight degradation in performance. We attribute this phenomenon to the models lack of exposure to such exploitation patterns during the training phase; consequently, the trained model 20 lacks proficiency in utilizing these hacking tools effectively, rendering these attempts less successful than standard problem-solving approaches. These findings further validate the effectiveness of our tool-level strategy in preventing data leakage and ensuring robust evaluation. Table 8: Performance comparison with and without Git tools, along with focused analysis of identified git hacking behaviors."
        },
        {
            "title": "Method",
            "content": "w/o Git Tool Resolve Rate (%) w/ Git Tool Resolve Rate (%)"
        },
        {
            "title": "Hacking Analysis",
            "content": "# Samples Acc. (%)"
        },
        {
            "title": "SFT\nRL",
            "content": "57.8 61.4 57.0 58.4 51 28 56.8 57."
        },
        {
            "title": "6.5 Summary-Based Context Manager for SWE Tasks",
            "content": "The SWE tasks require LLMs to continuously interpret environment feedback, execute actions, and revise strategies over numerous interaction rounds [58]. This imposes significant demands on context management. Currently, leading code agents and frameworks [53, 59], predominantly adopt an Append-Only strategy. In this approach, all historical interaction information is sequentially concatenated to the message history without compression. Conversely, in web search scenarios, approaches like the Condenser, described as Discard-All [53], have achieved frontier performance on BrowseComp [21] benchmarks. This method automatically discards all tool response from previous turns once specific window size is exceeded. While effective for search, we argue that this approach is detrimental in coding scenarios. Because coding tasks are heavily state-dependent, discarding observations (e.g., file contents read in previous turns, error logs) strips the model of critical environmental context, forcing it to redundantly re-read files or hallucinate code structures. To bridge this gap, we propose summary-based context manager specifically optimized for SWE scenarios inspired by summary methods [60] adopted in search scenarios. As shown in Figure 14, instead of discarding history, we compress past interactions into natural language summaries while maintaining high-fidelity sliding window for recent turns. Formally, let be the summarization interval (i.e., summary is triggered every turns) and be the minimum size of the recent context window reserved for raw interactions. At any given turn t, the number of completed summary blocks is = (t k)/m. The context τt is constructed as: ) τt = (I, S1, S2, . . . , SL, (atwt, otwt), . . . , (at1, ot1) (cid:125) (cid:124) (cid:123)(cid:122) Recent Sliding Window wt (7) where wt = represents the raw context window, dynamically varying between and + 1. The summarization process is triggered only when the window size wt reaches + m. At that point, the oldest turns in the window are condensed into new summary SL+1 by summarizer model πsummary: SL+1 = πsummary (cid:16) {(aj, oj)}(L+1)m1 j=Lm (cid:17) (8) This hybrid structure ensures that the model retains long-term memory of high-level logic while possessing precise, token-level access to the immediate context required for debugging and editing. We integrate this strategy into our framework and evaluate it on the SWE-bench Verified dataset using MiniMax-M2.1 and GLM-4.7, with each model serving as its own summary model. As presented in Table 9, our context manager significantly enhances efficiency without compromising capabilities. For the M2.1 model, enabling the context manager yields 42% saving in total input tokens (from 2.8M to 1.6M) and reduces the average peak token usage per trajectory by nearly 50% (from 55.4k to 28.5k).This optimization effectively enables the model to transcend its inherent context length limitations. Crucially, this aggressive compression does not degrade performance, likely because the cleaner context helps the model focus on relevant information. similar trend is observed with GLM-4.7, where token consumption drops by 36% while maintaining competitive resolve rate. 21 Figure 14: Comparison of different context management strategies. The increase in average turns suggests that the reduced context overhead allows the agent to explore deeper and attempt more correction steps before hitting context limits. However, when extending this mechanism to SWE-Master, we observed no measurable improvements in either efficiency or efficacy. We attribute this to the models smaller parameter scale, which limits its ability to effectively utilize the managed context. Model Setting Input Tok Output Tok Avg Peak Tok Avg Turns Resolve Rate (k) (k) (k) (%) M2.1 GLM-4.7 w/o Mem. w. Mem. w/o Mem. w. Mem. 2821.9 1636.6 3540.4 2259.8 18.7 24.9 20.9 28.5 55.4 28.5 63.3 32. 82.0 92.9 97.3 120.0 68.4 69.8 66.2 65.8 Table 9: Comparison of token efficiency and performance metrics based on Minimax-M2.1 and GLM-4.7. Input Tok and Output Tok refer to the average all input and output all tokens across all evaluation trajectories, respectively. Avg Peak Tok refers to the average maximum single-turn context length per trajectory."
        },
        {
            "title": "7 Related Work",
            "content": "Datasets for Software Engineering Tasks. The evaluation paradigm for code models has witnessed significant shift, transitioning from standalone algorithmic code generation [61, 20] to complex, repository-level software engineering tasks. SWE-bench [3] represents pioneering milestone in this domain, serving as the first comprehensive benchmark to assess the capability of LLM-based code agents in resolving real-world GitHub issues. Building upon this foundation, SWE-bench Verified [16] is subsequently introduced, providing refined subset that rigorously ensures issue solvability. In parallel, to enhance the proficiency of code agents in repository-level issue resolution, recent research has focused on the construction of high-quality training datasets and execution environments. Works such as SWE-Gym [23], R2E-Gym [18], SWE-smith [24], and SWE-rebench [25] have established scalable pipelines for harvesting, constructing, and filtering data from GitHub, thereby creating robust environments for code agent training. Furthermore, the landscape of agent evaluation is rapidly expanding to cover broader dimensions. Recent initiatives[62], including Multi-SWE-bench [63] and SWE-bench Pro[64] extend the scope to diverse programming languages and introduce heightened difficulty issues, while SWE-bench Multimodal [65] extends the issue-solving paradigm to the multimodal domain. Furthermore, Terminal-Bench [66] introduces diverse task types, facilitating more comprehensive and multi-faceted evaluation of code agents capabilities. 22 Software Engineering LLMs and Agents. To enhance the capabilities of autonomous code agents, researchers have pursued several distinct paradigms. Early efforts primarily focused on the development of agentic frameworks. Representative systems such as SWE-agent [2] and OpenHands [6] utilize powerful foundational models [53, 67] within real-world environment, demonstrating strong performance in issue-solving tasks. Meanwhile, recent studies have shifted toward enhancing the underlying models specifically for software engineering tasks. For instance, daVinci-Dev [46] employs data synthesis and mid-training to instill agentic reasoning into base models. Similarly, SWE-Mirror [44] and SWE-Lego [40] leverage agentic supervised fine-tuning [68] by generating and sampling high-quality trajectories from proprietary LLMs within grounded execution environments. To further bridge the gap between static training and dynamic interaction, DeepSWE [35] and SkyRL [8] incorporate agentic reinforcement learning [69, 10]. These approaches optimize agent policies by allowing them to learn from trial-and-error feedback within sandboxed container environments. In contrast to the end-to-end agentic loop, parallel line of research explores agentless pipelines [70, 45]. Instead of maintaining continuous autonomous state, agentless pipelines decompose the issue-solving task into three discrete, predefined stages: fault localization, code repair, and patch verification [49, 43], and each stage is optimized independently."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we present SWE-Master, an open-source software engineering agents capable of resolving complex, repository-level issues. By building robust infrastructure with decoupled Docker execution environments and adopting carefully designed data curation pipelineincluding agent-based trajectory rollout, format-based filtering, and difficulty-based selectionwe construct high-quality dataset that substantially improves training effectiveness for both SFT and RL. On top of this foundation, we propose reinforcement learning framework tailored to long-horizon software engineering tasks. In addition, we reduce the semantic gap in code navigation by introducing novel, language-agnostic lsp_tool, which provides IDE-level structural information to the agent. The evaluations on SWE-bench Verified show that SWE-Master achieves advanced performance among open-source code agents. These results demonstrate the effectiveness of our end-to-end approach, spanning environment design, data engineering, and advanced reinforcement learning techniques, in advancing autonomous software engineering agents."
        },
        {
            "title": "References",
            "content": "[1] Caihua Li, Lianghong Guo, Yanlin Wang, Daya Guo, Wei Tao, Zhenyu Shan, Mingwei Liu, Jiachi Chen, Haoyu Song, Duyu Tang, et al. Advances and frontiers of llm-based issue resolution in software engineering: comprehensive survey. arXiv preprint arXiv:2601.11655, 2026. [2] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [3] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [4] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [5] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [6] Xingyao Wang, Simon Rosenberg, Juan Michelini, Calvin Smith, Hoang Tran, Engel Nyst, Rohit Malhotra, Xuhui Zhou, Valerie Chen, Robert Brennan, et al. The openhands software agent sdk: composable and extensible foundation for production agents. arXiv preprint arXiv:2511.03690, 2025. [7] Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre Côté, et al. Bugpilot: Complex bug generation for efficient learning of swe skills. arXiv preprint arXiv:2510.19898, 2025. [8] Shiyi Cao, Dacheng Li, Fangzhou Zhao, Shuo Yuan, Sumanth Hegde, Connor Chen, Charlie Ruan, Tyler Griggs, Shu Liu, Eric Tang, et al. Skyrl-agent: Efficient rl training for multi-turn llm agent. arXiv preprint arXiv:2511.16108, 2025. [9] Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, and Furu Wei. Llm-in-sandbox elicits general agentic intelligence. arXiv preprint arXiv:2601.16206, 2026. [10] Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, et al. Training long-context, multi-turn software engineering agents with reinforcement learning. arXiv preprint arXiv:2508.03501, 2025. [11] Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, and Bryan Dai. Context as tool: Context management for long-horizon swe-agents. arXiv preprint arXiv:2512.22087, 2025. [12] Yuhang Wang, Yuling Shi, Mo Yang, Rongrui Zhang, Shilin He, Heng Lian, Yuting Chen, Siyu Ye, Kai Cai, and Xiaodong Gu. Swe-pruner: Self-adaptive context pruning for coding agents. arXiv preprint arXiv:2601.16746, 2026. [13] Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, and Jiecao Chen. Scaling long-horizon llm agent via context-folding. arXiv preprint arXiv:2510.11967, 2025. [14] OpenAI. Building more with gpt-5.1-codex-max. https://openai.com/index/ gpt-5-1-codex-max/, 2025. 2025-11-19. [15] Anthropic. Introducing claude sonnet 4.5. https://www.anthropic.com/news/ claude-sonnet-4-5, 2025. 2025-09-30. [16] Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos E. Jimenez, John Yang, Leyton Ho, Tejal Patwardhan, Kevin Liu, and Aleksander Madry. Introducing swe-bench verified, August 2024. [17] KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He, et al. Swe-rm: Execution-free feedback for software engineering agents. arXiv preprint arXiv:2512.21919, 2025. [18] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. [19] Shuang Sun, Huatong Song, Lisheng Huang, Jinhao Jiang, Ran Le, Zhihao Lv, Zongchao Chen, Yiwen Hu, Wenyang Luo, Wayne Xin Zhao, Yang Song, Tao Zhang, and Ji-Rong Wen. Swe-world: Building software engineering agents in docker-free environments. https: //github.com/RUCAIBox/SWE-World, 2026. 2026-02-03. [20] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [21] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [23] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. [24] John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. 24 [25] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swerebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents, 2025. [26] Microsoft. language-server-protocol. https://microsoft.github.io/ language-server-protocol/overviews/lsp/overview. [27] Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, et al. Mimo-v2-flash technical report. arXiv preprint arXiv:2601.02780, 2026. [28] MiniMax. Minimax m2 & agent: Ingenious in simplicity. https://www.minimax.io/news/ minimax-m2, 2025. 2025-10-27. [29] Z.ai. Glm-4.6: Advanced agentic, reasoning and coding capabilities. https://z.ai/blog/ glm-4.6, 2025. 2025-09-30. [30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [31] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [33] Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for largescale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025. [34] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [35] Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Tarun Venkat, Shang Zhu, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, and Ion Stoica. Deepswe: Training fully open-sourced, state-of-the-art coding agent by scaling rl. https://www.together.ai/blog/deepswe, 7 2025. Blog post. [36] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [37] Tengxiao Liu, Zifeng Wang, Jin Miao, Hsu, Jun Yan, Jiefeng Chen, Rujun Han, Fangyuan Xu, Yanfei Chen, Ke Jiang, et al. Budget-aware tool-use enables effective agent scaling. arXiv preprint arXiv:2511.17006, 2025. [38] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [39] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [40] Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang, Ruoyu Wang, Xiaohui Li, Sidi Yang, Yiming Du, Jianbo Dai, et al. Swe-lego: Pushing the limits of supervised fine-tuning for software issue resolving. arXiv preprint arXiv:2601.01426, 2026. [41] Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, and Lijun Wu. Scaling code-assisted chain-of-thoughts and instructions for model reasoning. arXiv preprint arXiv:2510.04081, 2025. 25 [42] Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, et al. Skywork-swe: Unveiling data scaling laws for software engineering in llms. arXiv preprint arXiv:2506.19290, 2025. [43] Zhenyu He, Qingping Yang, Wei Sheng, Xiaojian Zhong, Kechi Zhang, Chenxin An, Wenlei Shi, Tianle Cai, Di He, Jiaze Chen, and Jingjing Xu. Swe-swiss: multi-task fine-tuning and rl recipe for high-performance issue resolution. https://github.com/zhenyuhe00/ SWE-Swiss, 2025. Notion Blog. [44] Junhao Wang, Daoguang Zan, Shulin Xin, Siyao Liu, Yurong Wu, and Kai Shen. Swemirror: Scaling issue-resolving datasets by mirroring issues across repositories. arXiv preprint arXiv:2509.08724, 2025. [45] Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, et al. Kimi-dev: Agentless training as skill prior for swe-agents. arXiv preprint arXiv:2509.23045, 2025. [46] Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, et al. davinci-dev: Agent-native mid-training for software engineering. arXiv preprint arXiv:2601.18418, 2026. [47] ByteDance Seed Team. Seed-oss open-source models. https://github.com/ ByteDance-Seed/seed-oss, 2025. [48] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. [49] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. [50] MiniMax. and with strong https://www.minimaxi.com/news/ m21-multilingual-and-multi-task-coding-with-strong-general, 2026. 2025-0104. M2.1: generalization."
        },
        {
            "title": "Multilingual",
            "content": "multi-task coding [51] Z.ai. Glm-4.7: Advancing the coding capability. https://z.ai/blog/glm-4.7, 2025. 2025-12-22. [52] Kimi Team. Kimi k2: Open agentic intelligence, 2025. [53] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. [54] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [55] Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, and Yunfang Wu. One tool is enough: Reinforcement learning for repository-level llm agents. arXiv preprint arXiv:2512.20957, 2025. [56] Qingao Dong, Mengfei Wang, Hengzhi Zhang, Zhichao Li, Yuan Yuan, Mu Li, Xiang Gao, Hailong Sun, Chunming Hu, and Weifeng Lv. Infcode-c++: Intent-guided semantic retrieval and ast-structured search for c++ issue resolution. arXiv preprint arXiv:2511.16005, 2025. [57] Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. Cosil: Software issue localization via llm-driven code repository graph searching. arXiv preprint arXiv:2503.22424, 2025. [58] Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, and Huacan Wang. Memgovern: Enhancing code agents through learning from governed human experiences, 2026. [59] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. 26 [60] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin Zhao. Manusearch: Democratizing deep search in large language models with transparent and open multi-agent framework. arXiv preprint arXiv:2505.18105, 2025. [61] Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [62] Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Mengtong Li, Mengfei Xie, Xiaojiang Zhang, Jinghui Wang, Wenhao Zhuang, Zheng Lin, Huiming Wang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, and Jiaheng Liu. Swe-compass: Towards unified evaluation of agentic coding abilities for large language models, 2025. [63] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. [64] Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, and Brad Kenstler. Swe-bench pro: Can AI agents solve long-horizon software engineering tasks? CoRR, abs/2509.16941, 2025. [65] John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. SWE-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2025. [66] Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbjörn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, and Ludwig Schmidt. Terminal-bench: Benchmarking agents on hard, realistic tasks in command line interfaces, 2026. [67] Sherman Wong, Zhenting Qi, Zhaodong Wang, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Wenlin Chen, Yilun Du, Minlan Yu, and Ying Zhang. Confucius code agent: Scalable agent scaffolding for real-world codebases, 2025. [68] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis, 2025. [69] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [70] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents, 2024. [71] Malintha Ranasinghe. Understanding the language server protocol. https://medium.com/ @malintha1996/understanding-the-language-server-protocol-5c0ba3ac83d2, 2021."
        },
        {
            "title": "A Detailed Implementation of LSP Tools",
            "content": "This section complements Section 5 with more fine-grained details of the implementation of LSP Tool, along with fuller characterization of the tool call format. A.1 Language Server Protocol As shown in Figure 15, the core architectural contribution of the Language Server Protocol is the mitigation of the -to-N point-to-point integration bottleneck. Historically, providing support for programming languages across different editors required unique implementations. By introducing standardized intermediate protocol, LSP transforms this requirement into more scalable + problem, where single language server can serve any compliant client, significantly lowering the barrier for language support in diverse development environments. There are three components of LSP, including Language Server (LS), Language Client (LC) and the protocol [71]. Language Server is process that provides language smarts by communicating with development tools. Language Client is code editor/development tool or an extension that can communicate with particular Language Server. To deliver intelligent language features, the Language Server functions essentially as persistent compiler frontend. Upon receiving source code updates from the client, the server performs lexical and syntactic analysis to construct an Abstract Syntax Tree (AST), hierarchical representation of the codes structure. Traversing this AST, the server builds and maintains Symbol Table, which resolves identifier scopes, types, and bindings. To support workspace-wide operations, the server further aggregates these symbols into Symbol Search Engine (or global index). It is through the synthesis of the AST, Symbol Table, and Search Engine that the server translates raw text positions from JSON-RPC requests into semantic responses. The protocol is analogous to HTTP, which consists of header part and content part. The header part consists of two header fields named content length and content type. Content length is number indicating the length of the content part in bytes. And the content type indicates the mime type of the content part which defaults to application/vscode-jsonrpc; charset=utf-8 which is used to encode the content part. More details in [26]. The lifecycle of an LSP session begins when the Language Client spawns the Language Server process and establishes communication channel, typically utilizing standard I/O or TCP sockets. The interaction formally commences with an initialization handshake, wherein the client transmits an initialize request detailing its specific capabilities and workspace context. In response, the server returns an InitializeResult to negotiate and declare its supported language features, such as incremental text synchronization or completion providers. Once the session is established, it enters an operational loop driven by user actions: the client sends JSON-RPC notifications (e.g., textDocument/didChange) to update the servers internal state without expecting reply, and requests (e.g., textDocument/definition) to query the servers static analysis engine. The server processes these queries and returns responses that the client renders in the UI. Finally, the session concludes with shutdown request to ensure graceful resource cleanup, followed by an exit notification to terminate the server process. It is precisely through this structured workflow that the Language Server Protocol establishes unified mechanism for interaction and code intelligence delivery between the server and the client. By enabling this seamless exchange of semantic data, LSP facilitates the significant leap from text editor to the Integrated Development Environment. A.2 LSP Features Integrated into the Code Agent In the pre-processing phase, we expose simplified, intuitive function-calling interface to the model and translate these calls into standard JSON-RPC requests. Notably, we aggregated the three distinct call hierarchy methods defined in the LSP specification into single, unified featureget_call_hierarchyto streamline the analysis workflow, thereby reducing token consumption and cognitive load. In the post-processing phase, we parse the servers raw JSON responses into format readable by the model. To maximize the validity and density of the returned information, we selectively augment certain outputs with necessary context (e.g., fetching the actual source code content rather than just location coordinates) while filtering out redundant noise. Table 10 details the specific input 28 Figure 15: The Language Server Protocol for use between integrated development environments (IDEs) and servers that provide language intelligence tools. parameters and output results for our processed tools, while the Appendix provides comparison with the standard raw JSON-RPC format. Category Tool Call Name Input Paras. Output Results Repo Navigation get_definition get_declaration get_type_definition get_implementation fp, l, fp, l, fp, l, fp, l, The symbols source code. The symbols source code. The symbols source code. The symbols source code. Dependency Ana. get_call_hierarchy fp, l, The functions call hierarchy. Code Understanding get_hover get_document_symbols get_document_highlights Workspace Search get_workspace_symbols get_references fp, l, fp fp, l, query fp, l, The function signature. The document outline. Highlight usages of the symbol. All symbol information globally. All symbol references globally. Table 10: Overview of LSP Features Integrated into the Code Agent. fp is an abbreviation of file_path. is an abbreviation of line, the correct line number of the selected symbol in the code file. is an abbreviation of symbol, the name of code entity, including identifiers for Classes, Functions, Methods, Variables, Fields, and Modules. A.3 Tool Call Format of LSP Tool We show eight of the ten features used in the evaluation of SWE-bench Verified, excluding get_declaration and get_implementation. These two features are not shown because the Pyright language server does not currently support them due to Pythons dynamic typing system, although they are available for languages like C++ or Java and other programming languages. Example 1: get_workspace_symbols Input: <function=lsp_tool> <parameter=command>get_workspace_symbols</parameter> <parameter=query>swap_dims</parameter> </function> Output: Execution output of [lsp_tool]: [status_code]: success [Result]: Found 6 workspace symbol(s): swap_dims (Method) [in DataArray] at /testbed/xarray/core/dataarray.py (line 2044) 29 swap_dims (Method) [in Dataset] at /testbed/xarray/core/dataset.py (line 3688) test_append_with_append_dim_not_set_raises (Method) [in ZarrBase] at /testbed/ xarray/tests/test_backends.py (line 2095) test_no_warning_from_open_emptydim_with_chunks (Method) [in ZarrBase] at / testbed/xarray/tests/test_backends.py (line 2231) test_swap_dims (Method) [in TestDataArray] at /testbed/xarray/tests/ test_dataarray.py (line 1771) test_swap_dims (Method) [in TestDataset] at /testbed/xarray/tests/test_dataset. py (line 2958) Example 2: get_definition Input: <function=lsp_tool> <parameter=command>get_definition</parameter> <parameter=file_path>/testbed/xarray/core/dataset.py</parameter> <parameter=symbol>_replace_with_new_dims</parameter> <parameter=line>1017</parameter> </function> Output: Execution output of [lsp_tool]: [status_code]: success [Result]: Found definition at: /testbed/xarray/core/dataset.py char 31) (start at line 1017 char 9, end at line 1017 --- SOURCE CODE START --- 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 def _replace_with_new_dims( self: T_Dataset, variables: dict[Hashable, Variable], coord_names: set None = None, attrs: dict[Hashable, Any] None Default = _default, indexes: dict[Hashable, Index] None = None, inplace: bool = False, ) -> T_Dataset: \"\"\"Replace variables with recalculated dimensions.\"\"\" dims = calculate_dimensions(variables) return self._replace( variables, coord_names, dims, attrs, indexes, inplace= inplace 1029 --- SOURCE CODE END --- ) Example 3: get_type_definition Input: <function=lsp_tool> <parameter=command>get_type_definition</parameter> <parameter=file_path>/R2E-Gym/src/r2egym/agenthub/runtime/docker.py</parameter> <parameter=symbol>logger</parameter> <parameter=line>342</parameter> </function> Output: [status_code]: 30 success [Result]: Found definition at: - /home/usr/anaconda3/lib/python3.12/site-packages/pyright/dist/dist/typeshedfallback/stdlib/logging/__init__.pyi line 114 char 13) (start at line 114 char 7, end at - /home/usr/anaconda3/lib/python3.12/logging/__init__.py (start at line 1482 char 7, end at line 1482 char 13) --- SOURCE CODE START --- 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class Logger(Filterer): # undocumented # undocumented name: str # undocumented level: int # undocumented parent: Logger None propagate: bool handlers: list[Handler] disabled: bool # undocumented root: ClassVar[RootLogger] manager: Manager # undocumented def __init__(self, name: str, level: _Level = 0) -> None: ... def setLevel(self, level: _Level) -> None: ... def isEnabledFor(self, level: int) -> bool: ... def getEffectiveLevel(self) -> int: ... def getChild(self, suffix: str) -> Self: ... # undocumented # see python/typing #980 128 129 ...... 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 if sys.version_info >= (3, 12): def getChildren(self) -> set[Logger]: ... def makeRecord( self, name: str, level: int, fn: str, lno: int, msg: object, args: _ArgsType, exc_info: _SysExcInfoType None, func: str None = None, extra: Mapping[str, object] None = None, sinfo: str None = None, ) -> LogRecord: ... def hasHandlers(self) -> bool: ... def callHandlers(self, record: LogRecord) -> None: ... # undocumented --- SOURCE CODE END --- Example 4: get_call_hierarchy Input: <function=lsp_tool> <parameter=command>get_call_hierarchy</parameter> <parameter=file_path>/testbed/sympy/combinatorics/perm_groups.py</parameter> <parameter=symbol>minimal_blocks</parameter> <parameter=line>4354</parameter> </function> Output: [status_code]: 31 success [Result]: Call Hierarchy Analysis for: minimal_blocks Location: file:///testbed/sympy/combinatorics/perm_groups.py === Incoming Calls (Who calls this?) === - Caller: sylow_subgroup (Function) Location: /testbed/sympy/combinatorics/perm_groups.py line - Caller: test_minimal_blocks (Function) Location: /testbed/sympy/combinatorics/tests/test_perm_groups.py line 514 === Outgoing Calls (Who does this call?) === - Callee: len (Function) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line 1572 - Callee: range (Class) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line 1334 - Callee: tuple (Class) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line 1043 - Callee: is_transitive (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line 2334 - Callee: schreier_vector (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line 3702 - Callee: append (Method) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line - Callee: random_stab (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line 3311 - Callee: PermutationGroup (Class) Location: /testbed/sympy/combinatorics/perm_groups.py line 24 - Callee: stabilizer (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line - Callee: orbits (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line 2850 - Callee: minimal_block (Method) Location: /testbed/sympy/combinatorics/perm_groups.py line 2481 - Callee: _number_blocks (Function) Location: /testbed/sympy/combinatorics/perm_groups.py line - Callee: degree (Property) Location: /testbed/sympy/combinatorics/perm_groups.py line 1292 - Callee: enumerate (Class) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line 1327 - Callee: issubset (Method) Location: /opt/miniconda3/envs/testbed/lib/python3.9/site-packages/pyright/ dist/dist/typeshed-fallback/stdlib/builtins.pyi line 1271 Example 5: get_hover Input: <function=lsp_tool> <parameter=command>get_hover</parameter> <parameter=file_path>/testbed/lib/matplotlib/dates.py </parameter> <parameter=line>779</parameter> </function> Output: Execution output of [lsp_tool]: 32 [status_code]: success [Result]: Hover Info: (variable) tickdate: NDArray[Any] Example 6: get_document_symbols Input: <function=lsp_tool> <parameter=command>get_document_symbols</parameter> <parameter=file_path>/testbed/sympy/sets/conditionset.py</parameter> </function> Output: [status_code]: success [Result]: Document Symbols: - ConditionSet (Class) at (line 22 to 224) - __new__ (Method) at (line 91 to 165) - sig (Function) at (line 146 to 147) - free_symbols (Method) at (line 171 to 174) - bound_symbols (Method) at (line 176 to 179) - _contains (Method) at (line 181 to 198) - ok_sig (Function) at (line 182 to 190) - as_relational (Method) at (line 200 to 206) - _eval_subs (Method) at (line 208 to 224) Example 7: get_document_highlights Input: <function=lsp_tool> <parameter=command>get_document_highlights</parameter> <parameter=file_path>/testbed/sympy/combinatorics/perm_groups.py</parameter> <parameter=symbol>minimal_blocks</parameter> <parameter=line>4354</parameter> </function> Output: [status_code]: success [Result]: Found 2 related highlight(s) in this document: - (Module) at (at line 2133) - (Module) at (at line 4354) Example 8: get_references Input: <function=lsp_tool> <parameter=command>get_references</parameter> <parameter=file_path>/testbed/sympy/combinatorics/perm_groups.py</parameter> <parameter=symbol>minimal_blocks</parameter> <parameter=line>4354</parameter> </function> Output: [status_code]: success [Result]: Found 5 reference(s) across 2 file(s): /testbed/sympy/combinatorics/perm_groups.py: - (at line 2133) - (at line 4354) /testbed/sympy/combinatorics/tests/test_perm_groups.py: - (at line 516) - (at line 519) - (at line 522)"
        },
        {
            "title": "B LLM Prompts",
            "content": "System Prompt for LSP-Augmented Trajectory Filtering You are an expert code analysis assistant tasked with evaluating the effectiveness of LSP (Language Server Protocol) tools in helping AI agents solve complex software engineering problems. This is the trajectory:{trajectory} ## Your Task Analyze the provided trajectory where an AI agent attempts to solve coding problem from the SWE-bench dataset. Your focus is to judge whether the lsp_tool really brings **positive contributions** to the problem-solving process or not. If there are any errors in the return result of lsp_tool, or if meaningless call is made (the return result does not contain useful information), please immediately output no (i.e. lsp_tool is not helpful for this trajectory) Please think carefully and maintain an objective, rational analysis. You will be punished if you are overly biased. ## Context: The lsp_tool The lsp_tool is Pyright-based code intelligence tool that provides: - **Semantic code understanding**: Goes beyond text search by analyzing AST and symbol tables - **Navigation capabilities**: get_definition, get_type_definition, get_references - **Code structure analysis**: get_document_symbols, get_workspace_symbols - **Call relationship analysis**: get_call_hierarchy, get_incoming_calls, get_outgoing_calls - **Contextual information**: get_hover for docstrings and type information This contrasts with simpler tools like grep, sed, or cat which only perform text-based operations. ## Evaluation Framework ### 1. Positive Indicators (Evidence that lsp_tool is helpful) Carefully identify instances where lsp_tool: **a) Efficient Navigation** - Successfully locates function/class definitions across multiple files 34 - Finds symbol references faster than grep-based approaches - Discovers code relationships that would be hard to find manually **b) Accurate Code Understanding** - Provides structural overview via get_document_symbols before diving into details - Uses get_workspace_symbols to quickly locate relevant code entities - Employs get_references to understand how function/class is used across the codebase **c) Strategic Analysis** - Uses get_call_hierarchy to understand function dependencies - Leverages semantic information to make informed decisions - Follows code flow using get_definition chains **d) Problem-Solving Efficiency** - Reduces the number of file reads needed - Avoids blind searching through irrelevant files - Quickly identifies the right files to modify ### 2. Negative Indicators (Evidence of ineffective usage) Also identify cases where: **a) Tool Misuse** - Uses lsp_tool when simple grep or cat would suffice - Makes redundant lsp_tool calls that dont add new information - Uses wrong commands (e.g., get_workspace_symbols with overly generic queries ) **b) Failed Tool Calls** - LSP commands fail and the agent doesnt adapt - Symbol not found errors that waste time - Incorrect parameters leading to errors **c) Ignored Results** - Agent calls lsp_tool but doesnt utilize the returned information - Makes decisions that contradict lsp_tool findings - Proceeds with manual file exploration after lsp_tool already provided the answer **d) Inefficiency** - Over-reliance on lsp_tool when the problem is straightforward - Using multiple lsp_tool calls where one would suffice - Mixing lsp_tool with redundant text-based searches ### 3. Comparative Analysis Compare scenarios: - **With lsp_tool**: How does the agent locate code? How many steps? - **Without lsp_tool (hypothetical)**: How would the agent likely proceed using only grep/cat? - Does lsp_tool provide **decisive advantage** in understanding code structure ? ## Output Format Provide your analysis in the following structured format: ### Executive Summary (2-3 sentences) Provide clear verdict: Does lsp_tool bring positive contributions? Is it helpful, neutral, or counterproductive? ### Detailed Analysis 35 #### Section A: Positive Contributions If have, for each positive use case found: **Use Case #X: [Brief Title]** - **Step Number**: [which step in the trace] - **LSP Command**: [which command was used] - **What it found**: [summarize the output] - **How it helped**: [explain the benefit] - **Efficiency gain**: [compare with alternative approaches] #### Section B: Ineffective/Problematic Usage If have, for each problematic use case: **Issue #X: [Brief Title]** - **Step Number**: [which step] - **Problem Type**: [misuse/failed call/ignored result/inefficiency] - **What went wrong**: [describe the issue] - **Impact**: [how it affected problem-solving] #### Section C: Key Statistics Provide quantitative measures: - Total lsp_tool calls: - Successful calls: (Z%) - Calls that directly contributed to solution: - Failed/error calls: - Redundant/unnecessary calls: #### Section D: Critical Moments If have, identify 1-3 pivotal moments where lsp_tool either: - **Made breakthrough**: Enabled the agent to find the root cause - **Caused confusion**: Led the agent in the wrong direction - **Was underutilized**: Could have helped but wasnt used ### Overall Assessment **Answer**: boxed{{yes or no}} **Recommendations**: - What could be improved in how the agent uses lsp_tool? - Are there missed opportunities where lsp_tool should have been used? - Should any usage patterns be avoided? ## Important Guidelines 1. **Be Evidence-Based**: Every claim must reference specific step numbers and command outputs 2. **Be Objective**: Dont assume lsp_tool is good or bad; let the evidence speak 3. **Consider Context**: failed lsp_tool call isnt inherently bad if the agent adapts well 4. **Think Counterfactually**: Would the agent have struggled more without lsp_tool? 5. **Focus on Problem-Solving**: The ultimate question is: did it help solve the actual issue? 6. **Note Tool Synergy**: Sometimes lsp_tool + grep together are better than either alone ## Special Attention Areas - Look for cases where get_workspace_symbols quickly locates the right class/ function 36 - Check if get_document_symbols helps understand file structure before editing - Observe whether get_references reveals unexpected usage patterns - See if get_definition chains help trace code flow - Notice when the agent **should have used** lsp_tool but didnt Begin your analysis now."
        }
    ],
    "affiliations": [
        "BOSS Zhipin, Beijing, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}