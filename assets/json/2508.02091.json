{
    "paper_title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
    "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Chris Shum",
        "Jiwei Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN"
        },
        {
            "title": "Start",
            "content": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search Xiaoya Li,, Xiaofei Sun, Albert Wang, Chris Shum and Jiwei Li University of Washington, DeepReinforce Team github.com/deepreinforce-ai/crinn Abstract Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, new paradigm for ANNS algorithms. CRINN treats ANNS optimization as reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINNs effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINNs success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. (cid:66) 5 2 0 2 4 ] . [ 1 1 9 0 2 0 . 8 0 5 2 : r Figure 1: QPS versus recall curves for different models across six datasets. CRINN achieves achieves best-in-class performance on three out of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular) and matching state-of-the-art results on two (SIFT-128 and GloVe-25). (cid:66) Email: xxiaoya@uw.edu, {xiaofei_sun, albert_wang, chris_shum, jiwei_li}@deep-reinforce.com"
        },
        {
            "title": "Introduction",
            "content": "Approximate nearest-neighbor search (ANNS) [20, 21, 6, 7, 19, 12, 5] aims at finding data points that are closest to query point in high-dimensional spaces while trading off small amount of accuracy for significant speedup over exact search methods. ANNS is of growing importance due to the unprecedented popularity of retrieval-augmented generation (RAG) [16, 10, 23] and agent-based LLM applications [25], which require fast retrieval of relevant information from massive vector databases. Existing widely-used open-source projects use fundamental algorithms such as Vamana [12] and HNSW [20] as the backbone: DiskANN and ParlayANN [21] build upon Vamana, while FAISS [6], Vespa1, Weaviate2, Qdrant3, Milvus4, and GLASS [29]5 leverage HNSW, proposing different levels of optimization to cater for various scenarios. Existing optimizations for ANNS are mostly manual, where humans identify bottlenecks through profiling tools, analyze cache miss patterns, hand-tune parameters like graph degree and search beam width, carefully design data structures for memory locality, and iteratively experiment with different algorithmic variants based on hardware characteristics. This process requires deep expertise in computer architecture, parallel programming, and the mathematical properties of ANNS algorithms. With the increasing power of LLMs in code generation [9, 11], natural question arises: can LLMs facilitate optimization by automatically generating and testing algorithmic improvements, learning from the execution speeds of previous implementations to propose better solutions, and discovering novel optimization patterns that human engineers might overlook? In this paper, we propose CRINN, reinforcement learning-augmented LLM framework for automated ANNS optimization. The core of CRINN is contrastive RL model that performs comparative analysis of previously generated code variants alongside their execution metrics, enabling the model to learn by distinguishing between effective and ineffective optimization strategies and generate better solutions. Through this contrastive learning approach, CRINN develops an understanding of which code patterns lead to performance improvements and which cause degradation. The generated code variants are evaluated based on execution time, with faster implementations receiving higher rewards in the RL training process. This reward signal drives the LLM to iteratively generate progressively more efficient ANNS implementations. By learning from the performance outcomes of its own generated code, CRINN effectively transforms the manual optimization process into an automated search through the space of possible implementations. Our experimental evaluation demonstrates CRINNs effectiveness across six widely-used NNS benchmark datasets [1]. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular); tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). More importantly, the success of CRINN carries broader implications beyond ANNS optimization. It demonstrates that RL-augmented LLMs can serve as powerful tools for automating complex algorithmic optimizations that traditionally require deep domain expertise and extensive manual tuning. As the demand for efficient vector search continues to grow with the proliferation of RAG and agent-based LLM applications, automated optimization frameworks like CRINN will become increasingly valuable for maintaining competitive performance across evolving hardware architectures and application requirements."
        },
        {
            "title": "2 Background: HNSW",
            "content": "We use HNSW (Hierarchical Navigable Small World) [20] as the optimization backbone. HNSW is state-of-the-art graphbased ANNS algorithm that achieves high recall rates with logarithmic search complexity. Here we describe the core modules that most HNSW-based implementations adopt, on which our RL optimization is performed."
        },
        {
            "title": "2.1 Graph Construction",
            "content": "In HNSW, we first need to build graph for base vectors, where each vector is represented as node in the graph. As the search is performed through this graph for given query, graph construction is key step in HNSW. The graph construction module builds the graph through incremental insertion of vectors. HNSW constructs multi-layer graph where each element is assigned to multiple layers based on an exponentially decaying probability distribution. Each new base vector is assigned to layer 0 with probability 1, and to each subsequent layer with probability pl, where is typically set to 1/2 ln(2). This creates hierarchical structure similar to skip lists, enabling efficient navigation during search. 1https://docs.vespa.ai/en/nearest-neighbor-search.html 2https://weaviate.io/blog/vector-search-explained 3https://github.com/qdrant/qdrant 4https://github.com/milvus-io/milvus 5https://github.com/hhy3/pyglass 2 For each layer where vector is present, the algorithm performs greedy search from the layers entry point to find the nearest neighbors. The search parameter ef controls the number of candidates explored during this neighbor selection process. The algorithm maintains connections for upper layers and 2 connections for the bottom layer (layer 0) to ensure rich connectivity. It is worth noting that ef is crucial parameter in HNSW, controlling the tradeoff between recall and speed. Larger values of ef mean exploring more candidates during the search phase, thus achieving higher recall at the cost of reduced speed. HNSW employs heuristic pruning strategy to optimize the graph structure. This strategy maintains connectivity while promoting the small-world property by prioritizing diverse connections over purely nearest neighbors. Redundant edges that dont contribute to search efficiency are removed, resulting in graph that balances accuracy with traversal efficiency."
        },
        {
            "title": "2.2 Search",
            "content": "Given constructed graph and an input query, the search module performs k-nearest neighbor search through two-phase hierarchical process. The search begins with the upper layer, starting from the global entry point at the top layer. The algorithm greedily traverses each layer to find the single nearest neighbor, using this neighbor as the entry point for the next lower layer. This process continues until reaching the bottom layer (layer 0), effectively narrowing down the search space. At layer 0, the algorithm switches to more exhaustive exploration strategy. It initializes candidate set with the entry point from layer 1 and maintains priority queue of ef nearest candidates (where ef k). The search iteratively explores neighbors of the closest unexplored candidate, updating the result set whenever closer neighbors are discovered. The process terminates when no remaining candidate can improve the current results, ensuring that the nearest neighbors have been found with high probability."
        },
        {
            "title": "2.3 Refinement",
            "content": "Most HNSW-based algorithms incorporate refinement modules that enhance search efficiency through various optimization strategies. One common approach is quantized preliminary search, where vectors are compressed to int4 or int8 representations for rapid similarity estimation. Product quantization further improves this by dividing vectors into subspaces and quantizing each independently. Asymmetric distance computation keeps queries in full precision while using quantized database vectors, balancing speed and accuracy. Hierarchical pruning strategies form another class of refinements. These include layer-wise filtering that uses coarse distance estimates to prune entire graph regions early in the search process. Batch processing amortizes memory access costs by handling multiple queries simultaneously, while adaptive beam search dynamically adjusts the search width based on query difficulty."
        },
        {
            "title": "Improving ANNS with Contrastive Reinforcement Learning",
            "content": "In this section, we describe how we optimize an ANNS algorithm using contrastive RL [18] in detail."
        },
        {
            "title": "3.1 Overview",
            "content": "An ANSS algorithm usually contains multiple modules, e.g., graph construction, search, refinement as described in Section 2. We treat each module in ANSS as independent and optimize module by module sequentially using contrastive RL. The core idea of contrastive RL is to transform the traditional reinforcement learning paradigm by integrating both exemplar code snippets and their execution times directly into the LLM prompt. This enables the LLM to explicitly reason about why certain ANNS implementations outperform others, learning to identify performance-critical patterns through direct comparison. Given these speed-annotated examples, the LLM conducts comparative analysis to understand the factors driving efficiency differences, then synthesizes new module code that incorporate these insights. The generated code is evaluated based on execution time, which serves as the reward signal for updating the LLM parameters within the RL framework. This creates feedback loop where the model continuously improves its ability to both analyze performance characteristics and generate optimized implementations."
        },
        {
            "title": "3.2 Prompt Construction",
            "content": "Here we describe the construction of prompts provided to the LLM during contrastive-RL training. The prompt comprises the following structured components: 1. Task Description: detailed description of the current ANNS module to optimize, including input/output requirements, performance metrics, and optimization objectives with emphasis on execution speed."
        },
        {
            "title": "Prompt Template Used in CRINN",
            "content": "Task Description You are an approximate nearest neighbor search optimization expert specializing in high-performance similarity search algorithms. Given reference implementations for search, your objective is to create an accelerated version that maintains identical functionality. You will receive previous module implementations accompanied by their scores indicating the general speed. Higher scores indicate higher speed. Conduct comparative analysis of these implementations and use the insights to develop optimized graph construction code. Previous Implementations with Speed 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 // Implementation 1 (Score: 1.42) class Module_v1 { void build_index(const float* data, int n, int d) { ... } void search(const float* query, int k, int* indices, float* distances) { ... } }; // Implementation 2 (Score: 1.34) class Module_v2 { void build_index(const float* data, int n, int d) { ... } void search(const float* query, int k, int* indices, float* distances) { ... } }; Generation Protocol You MUST use exactly two hash symbols (##) at the beginning of each section. ## Performance Analysis: Compare ANNS implementations above and articulate on: 1. Which implementations achieve superior query throughput and what algorithmic factors contribute to their fast execution? 2. What indexing structures or search strategies demonstrate the best speed-accuracy tradeoffs? 3. What are the primary bottlenecks limiting query performance in slower implementations? 4. Which vectorization, parallelization, or caching techniques remain unexploited? ## Algorithm Design: Describe your optimization strategy as numbered points outlining key techniques and improvements for accelerating execution speed ## Code: Your code implementation Requirements and Constraints ## Critical Requirements: 1. Search quality must match the reference implementation exactly (same recall, precision). Failure to maintain search accuracy will result in score of 0. 2. The module must support the same interface: build_index() and search() methods with identical parameters. 3. Results must be deterministic and reproducible across runs. Table 1: Prompt template used in CRINN for RL training. 4 2. Previous Implementations with Speed: Previously generated ANNS implementations paired with their scores indicating execution speed. 3. Generation Protocol: Explicit instructions defining the required output format. 4. Critical Requirements: Explicit instructions for the required output content. The models response must contain three structured components: 1. Performance Analysis: comparative analysis identifying which previous ANNS module implementations achieved superior speed performance and the underlying algorithmic or implementation factors contributing to their fast execution. 2. Algorithm Design: high-level description of the proposed optimization strategy, outlining key techniques and improvements for accelerating execution speed as numbered points in natural language. 3. Code Implementation: The detailed code implementation. detailed example of the prompt structure is shown in Table 1. For contrastive exemplar selection, we adopted the similar strategy to [18, 26], where we maintain performance-indexed database of all successful code samples. We sample exemplars from the dataset using temperature-scaled softmax distribution: (Bi) = exp ((si µ)/τ ) exp ((sj µ)/τ ) (cid:80) (1) where si denotes the score of code sample in the database, µ denotes the mean score across all codes in the database, and τ is the temperature parameter governing the exploration-exploitation tradeoff."
        },
        {
            "title": "3.3 Speed Reward",
            "content": "Giving generated code proper reward, which must be scalar, is crucial in training our system. The reward serves two important purposes: (1) guiding parameter updates in reinforcement learning and (2) constructing prompts for contrastive analysis. Unfortunately, constructing reward that effectively captures general code speed performance in ANNS is not as straightforward as it seems. In the ANNS setting, performance is characterized by two interdependent metrics: queries per second (QPS) and recall. The only parameter we can adjust is ef , which controls the number of neighbors explored during search: higher values of ef lead to lower QPS and higher recall. However, this creates comparison problem: two implementations with identical ef can exhibit different combinations of QPS and recall. Simply using QPS as the reward would be unfair without accounting for recall differences. This challenge explains why the ANNS literature rarely reports single quantitative metrics (unlike traditional ML metrics such as accuracy or F1 score). Instead, researchers typically present QPS-recall curves (as in Figure 1) that visualize the entire performance tradeoff space. seemingly straightforward solution would be to fix one metric and optimize the othereither fix recall and maximize QPS, or fix QPS and maximize recall. Unfortunately, this approach is infeasible because ef takes discrete values. We cannot continuously tune ef to achieve specific target values of QPS or recall; instead, we can only obtain discrete set of (QPS, recall) points corresponding to different ef settings. This discretization prevents us from making fair comparisons at fixed performance levels. This discrete nature creates other evaluation challenges: some algorithms may excel in low-recall regions but perform poorly in high-recall regions, while others show the opposite pattern; some algorithms may not even produce data points in certain regions of the spectrum. For example, certain high-quality algorithms often cannot achieve low recall values regardless of ef values. This incomplete coverage of the performance spectrum adds significant complexity to reward estimation, where the RL framework requires the reward to be single scalar value. To address these challenges, we employ the following evaluation strategy: given module implementation, we first vary ef across different values to obtain comprehensive set of (QPS, recall) points. We then filter these points to retain only those within the recall range of [0.85, 0.95] and compute the area under the curve formed by these points. This area serves as our scalar reward. We choose the [0.85, 0.95] recall range for several reasons. First, we primarily care about algorithms that achieve reasonable recall levelsperformance at very low recall is generally not useful for practical applications. Second, in the high-recall region above 0.95, data points become increasingly sparse, and some algorithms may not even produce points in this range, leading to unstable reward estimation. The [0.85, 0.95] range thus represents sweet spot where most algorithms have sufficient data points and where performance differences are most meaningful for real-world deployment."
        },
        {
            "title": "3.4 RL Training",
            "content": "For reinforcement learning training, we employ the Group Relative Policy Optimization (GRPO) approach [27]. For each input prompt augmented with selected demonstrations, we generate code completions from the current policy πold, represented as {d1, d2, . . . , dG}. The corresponding reward scores are denoted by = (r1, r2, . . . , rG). To ensure training stability, rewards undergo smoothing following [18]. Following the GRPO methodology, we normalize rewards within each group: The GRPO training objective maximizes the following loss function: ˆri = ri mean(r) std(r) LGRPO(θ) = qP (q),{di}G i=1πθold (dq) 1 (cid:88) i= 1 di di (cid:88) (cid:18) t=1 min (cid:18) πθ(di,tq, di,<t) πθold(di,tq, di,<t) ˆri, clip (cid:18) πθ(di,tq, di,<t) πθold (di,tq, di,<t) (cid:19) (cid:19) (cid:19)(cid:21) , 1 ε, 1 + ε ˆri βDKL[πθπref ] Here: πθ represents the policy network under optimization πθold denotes the policy from the preceding training step ε controls the clipping range for policy updates β is regularization parameter balancing exploration and adherence to the reference policy DKL measures the Kullback-Leibler divergence between current and reference distributions (2) (3) For comprehensive details on GRPO, we direct readers to [27]. The models parameters are updated through this GRPO objective, utilizing contrastive prompts enriched with comparative exemplars.RetryClaude can make mistakes. Please doublecheck responses."
        },
        {
            "title": "3.5 Using Glass as the RL Starting Point",
            "content": "Instead of implementing an ANSS algorithm from scratch, we use an existing open-source ANSS algorithm as starting point and progressively optimize its constituent modules. We selected GLASS as our initial baseline due to its balanced performance across diverse evaluation datasets. GLASS is recent graph-based ANNS algorithm that achieves efficient graph construction while maintaining competitive search performance. It is important to note that CRINN is general optimization frameworkwhile we demonstrate its effectiveness using GLASS, it can take any existing open-source ANNS algorithm as starting point and progressively evolve its implementation for improved performance. The choice of GLASS simply provides strong foundation for showcasing CRINNs capabilities across different optimization scenarios. We will update the results based on another strong baseline ParlayANN in the upcoming version of this project. Using GLASS, we sequentially optimize the code across the three key modules in HNSW: graph construction, search, and refinement."
        },
        {
            "title": "4.1 Datasets and Baselines",
            "content": "We use the following six widely used datasets for evaluation: SIFT-128-Euclidean, GIST-960-Euclidean, MNIST-784-Euclidean, GloVe-100-Angular, GloVe-25-Angular, and NYTimes-256-Angular. SIFT-128-Euclidean consists of 128-dimensional SIFT features extracted from images. GIST-960-Euclidean contains 960-dimensional GIST global descriptors for images. MNIST784-Euclidean is composed of 784-dimensional vectors representing flattened 2828 pixel images of handwritten digits. GloVe-100-Angular and GloVe-25-Angular are datasets of word embeddings trained by the GloVe algorithm on large text corpus, with dimensions 100 and 25, respectively. NYTimes-256-Angular contains 256-dimensional bag-of-words vectors from New York Times articles. The statistics for these datasets are summarized in Table 2. To maintain single generalizable codebase, we train our reinforcement learning model exclusively based on rewards from the SIFT-128 dataset. The resulting implementation is then evaluated across all datasets without modification. It is worth noting that SIFT-128 uses Euclidean distance, which means our RL model is trained only on Euclidean rewards. This might cause problems for angular-distance datasets. However, as we will show in the experiments, the code trained on Euclidean distance 6 Dataset LID Base Vectors Query Vectors SIFT-128 GIST-960 MNIST-784 GloVe-25 GloVe-100 NYTimes-256 Euclidean Distance 9.3 20.5 14.1 1,000,000 1,000,000 60, Angular Distance 9.9 12.3 12.5 1,183,514 1,183,514 290,000 128 960 784 25 100 256 10,000 1,000 10, 10,000 10,000 10,000 Table 2: Statistics of the benchmark datasets. is the vector dimension, and LID is the average Local Intrinsic Dimension. generalizes well to angular-distance datasets, demonstrating the strong generalization capability of CRINN. Incorporating both Euclidean and angular distances as training rewards constitutes future work. We employ the following widely used open-source projects as baselines: Glass [29]: graph-based approximate nearest neighbor search library developed by Zilliz, utilizing hierarchical navigable small world (HNSW) graphs with optimizations for high-dimensional vector search and hardware acceleration. Glass serves as the starting point for our RL training process, making it natural baseline for performance comparison. ParlayANN [22]: parallel approximate nearest neighbor library that leverages multi-core parallelism and cache-efficient algorithms, offering implementations of graph-based search methods optimized for shared-memory multiprocessors. In the updated version of this project, we will include the experimental setup that uses ParlayANN as the reinforcement learning starting point. NNDescent [24]: An implementation of the NN-Descent algorithm that constructs approximate k-nearest neighbor graphs through iterative local search, efficiently handling both dense and sparse data with minimal memory overhead. PyNNDescent6: Python implementation of NN-Descent that provides fast approximate nearest neighbor queries and KNN graph construction, with support for wide variety of distance metrics and efficient handling of high-dimensional data. Vearch [17]7: distributed vector search system designed for large-scale similarity search, combining graph-based indexing with inverted file structures to support billion-scale vector retrieval in production environments. Voyager8: library developed by Spotify for approximate nearest neighbor search, implementing hierarchical navigable small world graphs with optimizations for music and recommendation system workloads."
        },
        {
            "title": "5.1 Major Results",
            "content": "Figure 1 presents the QPS versus recall curves for different models across six datasets. CRINN outperforms all baselines on three benchmarks: GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular. The improvement is particularly substantial for MNIST-784-Euclidean. On SIFT-128-Euclidean and GloVe-25-angular, CRINN achieves performance comparable to the best baselinematching ParlayANN on SIFT-128 and Vearch on GloVe-25. Among the six datasets, CRINN underperforms the best baseline only on NYTimes-256. This performance gap likely stems from the fundamental differences between distance metrics. Since the RL-optimized code was trained exclusively on SIFT-128 using Euclidean distance, it may not effectively capture the optimization patterns required for angular similarity computations. Notably, it is common for strong models to exhibit dataset-specific weaknesses. For instance, ParlayANN performs poorly on GloVe-25-angular despite its strong performance elsewhere. When comparing CRINN with GLASS, which serves as the RL starting point, CRINN demonstrates substantial improvements. This consistent performance gain indicates that contrastive RL can robustly and progressively optimize the code. 6https://github.com/lmcinnes/pynndescent 7https://github.com/vearch/vearch 8https://github.com/spotify/voyager 7 Dataset Recall CRINN Best Baseline Baseline QPS QPS Improvement SIFT-128 GIST-960 MNIST-784 GloVe-100 GloVeEuclidean Distance 36,876 27,499 13,014 5,158 4,288 2,925 1,149 24,826 22,008 17,457 10,600 ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN ParlayANN Angular Distance 5,947 3,024 37,474 28,909 13,574 4,588 Vearch ParlayANN Glass Glass Glass Glass 0.900 0.950 0.990 0.999 0.900 0.950 0.990 0.900 0.950 0.990 0.999 0.900 0.950 0.900 0.950 0.990 0.999 NYTimes-256 0. 1,623 ParlayANN 29,368 23,057 11,808 4,996 3,788 2,348 666 19,324 17,293 11,728 5,722 5,768 3, 31,611 21,899 11,804 4,549 9,459 +25.57% +19.26% +10.21% +3.25% +13.20% +24.59% +72.68% +28.47% +27.26% +48.85% +85.25% +3.09% -5.84% +18.55% +32.01% +14.99% +0.87% -82.85% Table 3: Performance comparison of CRINN against best baselines across different datasets and recall levels. QPS (Queries Per Second) measures throughput."
        },
        {
            "title": "5.2 QPS with Fixed Recall",
            "content": "To give quantitative comparison, Table 3 presents the QPS (Queries Per Second) performance of CRINN against the bestperforming baselines across six benchmark datasets at various recall levels (0.9, 0.99, 0.999). For cases where performances are absent for specific recall levels, it indicates that none of the tested methods could reach the target recall threshold. The results demonstrate that CRINN consistently outperforms state-of-the-art methods across most configurations, with improvements ranging from modest gains of 3.09% to substantial speedups of 85.25%. CRINN shows particularly strong performance on MNIST-784, achieving up to 85.25% improvement at 0.999 recall, and on GIST-960 at high recall levels, with 72.68% improvement at 0.99 recall. The SIFT-128 dataset, which was used for training the RL agent, shows consistent improvements across all recall levels, with gains decreasing as recall requirements become more stringent. Among angular distance datasets, GloVe-25 exhibits significant improvements of up to 32.01%, while GloVe-100 shows mixed results, including slight degradation of 5.84% at 0.95 recall. As mentioned above, CRINN achieves poor performance on NYTimes-256, where CRINN underperforms the best baseline by 82.85% for the 0.9 recall setup."
        },
        {
            "title": "5.3 Progressive Improvements for Different Modules",
            "content": "In CRINN, optimization proceeds sequentially through three modules: graph construction, search, and refinement. To evaluate the individual contribution of each module, we examine the progressive performance improvements at each optimization stage. We demonstrate these incremental gains by measuring the average QPS improvement across fixed recall values (0.90, 0.95, 0.99, and 0.999). Table 4 presents the result. As can be seen, CRINN demonstrates substantial gains through all its three optimization stages. The graph construction module yields the most significant individual improvements, averaging 22.11% across all recall values and datasets, with particularly impressive results on gist-960-euclidean (58.26%) and mnist-784-euclidean (45.85%). The search optimization module contributes an additional 18.30% on average, maintaining strong performance especially for high-dimensional datasets. The refinement module, showing more modest individual gains of 9.69%. The diminishing returns 8 across the three stages can be attributed to two primary factors. First, the fundamental importance hierarchy of these stages naturally leads to different improvement potentials. Graph construction is the foundation that determines the entire search space structure. In contrast, the refinement stage serves more specialized role of fine-tuning results, where the candidates are already of reasonable quality, thus offering less dramatic improvement potential. Second, the optimization order plays significant roleearlier stages have more optimization opportunities available, while later stages operate on an already-improved system. The graph construction stage works with the raw, unoptimized baseline, allowing techniques like adaptive search, multi-level prefetching, and multi-entry points to capture the \"low-hanging fruit\" of performance improvements. By the time the refinement stage is reached, many inefficiencies have already been addressed, leaving less room for dramatic improvement. The only outlier is nytimes-256-angular, which shows performance degradation across all stages, suggesting that the optimization techniques may need dataset-specific tuning for certain angular distance computations. Overall, the results validate the effectiveness of the progressive optimization strategy, with five out of six datasets showing substantial cumulative improvements ranging from 16% to 134%. Dataset Graph Construction Individual Cumulative Search Individual Cumulative Refinement Individual Cumulative sift-128-euclidean gist-960-euclidean mnist-784-euclidean glove-100-angular glove-25-angular nytimes-256-angular +30.12% +58.26% +45.85% +13.19% +6.94% -21.68% +30.12% +58.26% +45.85% +13.19% +6.94% -21.68% +25.86% +55.98% +46.43% +104.69% +90.34% +44.49% +32.22% +19.03% +13.46% +6.52% -54.22% -32.54% +11.19% +67.17% +29.63% +134.32% +18.30% +108.64% +38.08% +5.86% +16.16% +2.70% -63.78% -9.56% Overall Average +22.11% +22.11% +18.30% +40.41% +9.69% +50.10% Table 4: Average QPS improvement across different recall levels."
        },
        {
            "title": "6 Analysis",
            "content": "Here, we conduct detailed analysis of each of the three optimization modules, examining the specific changes introduced by contrastive RL and how they contribute to improved performance."
        },
        {
            "title": "6.1 Graph Construction",
            "content": "We identify the following three optimization strategies discovered by contrastive RL in the graph construction module. Adaptive Search with Dynamic EF Scaling , which adjusts search effort based on target recall requirements, replacing the fixed ef_construction parameter in the original code with an adaptive value. This strategy helps allocate computational resources proportionally to quality requirements. 1 2 3 4 5 6 7 8 // Old: Fixed search budget size_t ef = ef_construction; // Always constant // New: Adaptive search budget based on recall needs if (target_recall > critical_threshold) dynamic_ef = ef_search * (1.0 + recall_excess * 14.5); else dynamic_ef = ef_search; Zero-Overhead Multi-Level Prefetching , which replaces fixed prefetching with an adaptive multi-level strategy that considers neighbor density and search layer. It reduces memory access latency compared to the fixed prefetch window in the old implementation 1 2 3 4 5 6 // Old: Fixed prefetch window for (int = 0; < min(5, size); ++j) computer.prefetch(neighbors[j], 1); // New: Adaptive multi-level prefetching prefetch_depth = min(adaptive_depth, size); // 24-48 based on performance 9 7 8 9 10 for (int = 0; < prefetch_depth; ++j) computer.prefetch(neighbors[j], 3); // L1 cache if (high_recall_needed) // Additional L2 prefetch for more neighbors Multi-Entry Point Search Architecture , which maintains multiple diverse entry points for parallel exploration instead of single global entry point. This strategy improves recall for the same computational budget by exploring diverse graph regions simultaneously. 1 2 3 4 5 6 7 8 9 10 11 12 // Old: Single entry point start_node = enterpoint_node; results = search(start_node, query); // New: Multiple diverse entry points (up to 9) for (node : strategic_entrypoints) { if (distance_to_others(node) > threshold) entry_points.add(node); } // Search from multiple starting points for (ep : entry_points) results.merge(search(ep, query));"
        },
        {
            "title": "6.2 Search",
            "content": "For search, we identify the following three optimization strategies proposed by contrastive RL. Multi-Tier Entry Point Selection , which replaces single entry point initialization with sophisticated multi-tier system that selects from primary, secondary, and tertiary entry points based on search budget. This strategy improves search quality by starting from diverse, high-quality nodes. 1 2 3 4 5 6 7 8 9 // Old: Single entry point initialize_search(single_entry_point) // New: Multi-tier entry selection add_entry(primary_entry_point) if search_budget > threshold_1: add_entry(secondary_entry_point) if search_budget > threshold_2: add_entry(tertiary_entry_point) Batch Processing with Adaptive Prefetching , which optimizes neighbor processing by collecting edges into batches and using enhanced prefetch strategies. This reduces random memory access and improves cache utilization. 1 2 3 4 5 6 7 8 9 10 // Old: Fixed prefetching for in range(prefetch_count): prefetch(neighbor[i]) // New: Adaptive batch prefetching prefetch_size = prefetch_count * batch_factor for in range(adaptive_prefetch_size): prefetch(neighbor[i]) if processing_node[j]: prefetch(neighbor[j + prefetch_size]) // Look ahead Intelligent Early Termination with Convergence Detection , which monitors search progress and terminates early when convergence is detected, avoiding unnecessary exploration while maintaining quality. 1 2 3 4 5 6 7 // Old: Explore until pool exhausted while has_candidates(): process_neighbor() // New: Smart termination no_improvement_count = 0 while has_candidates(): 10 8 9 10 11 12 improvements = process_neighbor() if improvements == 0: no_improvement_count++ if check_convergence(no_improvement_count): // Early termination break"
        },
        {
            "title": "6.3 Refinement",
            "content": "For the refinement module, RL proposed the following two optimization strategies. Adaptive Memory Prefetching , which replaces basic hierarchical search with an intelligent prefetching system that adapts based on edge patterns and node characteristics. This strategy significantly reduces memory latency during the refinement process. 1 2 3 4 5 6 7 8 9 10 11 12 // Old: Basic traversal without prefetching for each edge in node_edges: if distance(v) < best_distance: update best_node // New: Adaptive prefetching with lookahead if should_prefetch: prefetch(edges[0]) for i, edge in node_edges: prefetch(edges[i + lookahead]) if distance(v) < best_distance: update best_node // Prefetch future edges Pre-computed Edge Metadata with Pattern Recognition , which enhances the refiner by pre-computing and storing edge counts for each node level. This eliminates redundant computations and enables pattern-based optimizations during refinement. 1 2 3 4 5 6 7 8 9 10 11 12 13 // Old: Runtime edge counting count = 0 for each edge in node: if edge != -1: count++ // New: Pre-computed metadata access metadata = get_precomputed_metadata(level, node) edge_count = metadata.count pattern_score = metadata.intelligence_score // Use metadata for optimization decisions if pattern_score > threshold: apply_pattern_optimization()"
        },
        {
            "title": "7 Related Work",
            "content": "The past year has witnessed surge of interest in leveraging LLMs and RL-augmented LLM models for code optimization. This includes significant advances in compiler optimization [4], assembly code optimization [30], and CUDA kernel optimization [13, 2, 18]. Reinforcement learning frameworks such as CodeRL [14] and PPOCoder [28] have emerged as powerful tools for enhancing LLM performance in code generation and optimization tasks. Notably, RLEF [8] demonstrates that end-to-end reinforcement learning can effectively train models to utilize execution feedback during code synthesis, achieving state-of-the-art performance on competitive programming benchmarks. In assembly code optimization, recent breakthroughs show that PPO-trained LLMs can achieve remarkable resultsreaching 96.0% test pass rates and delivering 1.47 speedups compared to the gcc -O3 baseline [30]. Similarly, Metas LLM Compiler [3] achieves 77% of the optimization potential of exhaustive autotuning searches, validating the effectiveness of LLMs for optimizing compiler intermediate representations. For GPU-accelerated computing, CUrator [15] introduces an efficient LLM execution engine that seamlessly integrates CUDA libraries such as cuBLAS and CUTLASS, optimizing performance for modern language models. Complementing this, ComputeEval provides an open-source benchmark framework specifically designed to evaluate LLM capabilities in CUDA programming tasks, establishing standardized metrics for this emerging field."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we presented CRINN, novel framework that employs contrastive reinforcement learning-augmented LLMs to automatically optimize approximate nearest-neighbor search algorithms. By treating ANNS optimization as reinforcement learning problem where execution speed serves as the reward signal, CRINN successfully transforms the traditionally manual and expertise-intensive optimization process into an automated search through the space of possible implementations. Our experimental results demonstrate CRINNs effectiveness across diverse benchmark datasets, achieving best-in-class performance on three out of six benchmarks and matching state-of-the-art results on two others. The success of CRINN carries broader implications beyond ANNS optimization. It demonstrates that RL-augmented LLMs can serve as powerful tools for automating complex algorithmic optimizations that traditionally require deep domain expertise and extensive manual tuning. As the demand for efficient vector search continues to grow with the proliferation of RAG and agent-based LLM applications, automated optimization frameworks like CRINN will become increasingly valuable for maintaining competitive performance across evolving hardware architectures and application requirements."
        },
        {
            "title": "References",
            "content": "[1] AUMÜLLER, M., BERNHARDSSON, E., AND FAITHFULL, A. J. Ann-benchmarks: benchmarking tool for approximate nearest neighbor algorithms. Inf. Syst. 87 (2018). [2] CHEN, W., ZHU, J., FAN, Q., MA, Y., AND ZOU, A. Cuda-llm: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092 (2025). [3] CUMMINS, C., SEEKER, V., GRUBISIC, D., ROZIERE, B., GEHRING, J., SYNNAEVE, G., AND LEATHER, H. Meta large language model compiler: Foundation models of compiler optimization. arXiv preprint arXiv:2407.02524 (2024). [4] CUMMINS, C., SEEKER, V., GRUBISIC, D., ROZIERE, B., GEHRING, J., SYNNAEVE, G., AND LEATHER, H. Llm compiler: Foundation language models for compiler optimization. In Proceedings of the 34th ACM SIGPLAN International Conference on Compiler Construction (2025), pp. 141153. [5] DONG, W., MOSES, C., AND LI, K. Efficient k-nearest neighbor graph construction for generic similarity measures. In Proceedings of the 20th international conference on World wide web (2011), pp. 577586. [6] DOUZE, M., GUZHVA, A., DENG, C., JOHNSON, J., SZILVASY, G., MAZARÉ, P.-E., LOMELI, M., HOSSEINI, L., AND JÉGOU, H. The faiss library. arXiv preprint arXiv:2401.08281 (2024). [7] FU, C., XIANG, C., WANG, C., AND CAI, D. Fast approximate nearest neighbor search with the navigating spreading-out graph. arXiv preprint arXiv:1707.00143 (2017). [8] GEHRING, J., ZHENG, K., COPET, J., MELLA, V., CARBONNEAUX, Q., COHEN, T., AND SYNNAEVE, G. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089 (2024). [9] GUO, D., ZHU, Q., YANG, D., XIE, Z., DONG, K., ZHANG, W., CHEN, G., BI, X., WU, Y., LI, Y. K., LUO, F., XIONG, Y., AND LIANG, W. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. ArXiv abs/2401.14196 (2024). [10] GUU, K., LEE, K., TUNG, Z., PASUPAT, P., AND CHANG, M.-W. Realm: Retrieval-augmented language model pre-training. ArXiv abs/2002.08909 (2020). [11] HUI, B., YANG, J., CUI, Z., YANG, J., LIU, D., ZHANG, L., LIU, T., ZHANG, J., YU, B., DANG, K., YANG, A., MEN, R., HUANG, F., QUAN, S., REN, X., REN, X., ZHOU, J., AND LIN, J. Qwen2.5-coder technical report. ArXiv abs/2409.12186 (2024). [12] JAYARAM SUBRAMANYA, S., DEVVRIT, F., SIMHADRI, H. V., KRISHNAWAMY, R., AND KADEKODI, R. Diskann: Fast accurate billion-point nearest neighbor search on single node. Advances in neural information processing Systems 32 (2019). [13] LANGE, R. T., PRASAD, A., SUN, Q., FALDOR, M., TANG, Y., AND HA, D. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. Tech. rep., 2025. [14] LE, H., WANG, Y., GOTMARE, A. D., SAVARESE, S., AND HOI, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. ArXiv abs/2207.01780 (2022). 12 [15] LEE, Y. N., YU, Y., AND PARK, Y. Curator: An efficient llm execution engine with optimized integration of cuda libraries. In Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization (2025), pp. 209224. [16] LEWIS, P., PEREZ, E., PIKTUS, A., PETRONI, F., KARPUKHIN, V., GOYAL, N., KUTTLER, H., LEWIS, M., TAU YIH, W., ROCKTÄSCHEL, T., RIEDEL, S., AND KIELA, D. Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv abs/2005.11401 (2020). [17] LI, J., LIU, H., GUI, C., CHEN, J., NI, Z., AND WANG, N. The design and implementation of real time visual search system on jd e-commerce platform, 2019. [18] LI, X., SUN, X., WANG, A., LI, J., AND SHUM, C. Cuda-l1: Improving cuda optimization via contrastive reinforcement learning. arXiv preprint arXiv:2507.14111 (2025). [19] LIN, J., MA, X., LIN, S.-C., YANG, J.-H., PRADEEP, R., AND NOGUEIRA, R. Pyserini: python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval (2021), pp. 23562362. [20] MALKOV, Y. A., AND YASHUNIN, D. A. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence 42, 4 (2018), 824836. [21] MANOHAR, M. D., SHEN, Z., BLELLOCH, G., DHULIPALA, L., GU, Y., SIMHADRI, H. V., AND SUN, Y. Parlayann: Scalable and deterministic parallel graph-based approximate nearest neighbor search algorithms. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (2024), pp. 270285. [22] MANOHAR, M. D., SHEN, Z., BLELLOCH, G. E., DHULIPALA, L., GU, Y., SIMHADRI, H. V., AND SUN, Y. Parlayann: Scalable and deterministic parallel graph-based approximate nearest neighbor search algorithms. Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (2023). [23] MENG, Y., LI, X., ZHENG, X., WU, F., SUN, X., ZHANG, T., AND LI, J. Fast nearest neighbor machine translation. In Findings (2021). [24] ONO, N., AND MATSUI, Y. Relative nn-descent: fast index construction for graph-based approximate nearest neighbor search. In Proceedings of the 31st ACM International Conference on Multimedia (2023), pp. 16591667. [25] PARK, J. S., OBRIEN, J. C., CAI, C. J., MORRIS, M. R., LIANG, P., AND BERNSTEIN, M. S. Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (2023). [26] ROMERA-PAREDES, B., BAREKATAIN, M., NOVIKOV, A., BALOG, M., KUMAR, M. P., DUPONT, E., RUIZ, F. J., ELLENBERG, J. S., WANG, P., FAWZI, O., ET AL. Mathematical discoveries from program search with large language models. Nature 625, 7995 (2024), 468475. [27] SHAO, Z., WANG, P., ZHU, Q., XU, R., SONG, J., BI, X., ZHANG, H., ZHANG, M., LI, Y., WU, Y., ET AL. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [28] SHOJAEE, P., JAIN, A., TIPIRNENI, S., AND REDDY, C. K. Execution-based code generation using deep reinforcement learning. ArXiv abs/2301.13816 (2023). [29] WANG, Z. Graph library for approximate similarity search, 4 2025. [30] WEI, A., SURESH, T., TAN, H., XU, Y., SINGH, G., WANG, K., AND AIKEN, A. Improving assembly code performance with large language models via reinforcement learning. arXiv preprint arXiv:2505.11480 (2025)."
        }
    ],
    "affiliations": [
        "DeepReinforce Team",
        "University of Washington"
    ]
}