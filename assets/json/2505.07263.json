{
    "paper_title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning",
    "authors": [
        "Xiaokun Wang",
        "Chris",
        "Jiangbo Pei",
        "Wei Shen",
        "Yi Peng",
        "Yunzhuo Hao",
        "Weijie Qiu",
        "Ai Jian",
        "Tianyidan Xie",
        "Xuchen Song",
        "Yang Liu",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 3 6 2 7 0 . 5 0 5 2 : r Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning Xiaokun Wang, Chris, Weijie Qiu, Ai Jian, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Tianyidan Xie, Xuchen Song, Yang Liu, Yahui Zhou Skywork AI, Kunlun Inc. xuchen.song@kunlun-inc.com"
        },
        {
            "title": "Abstract",
            "content": "We propose Skywork-VL Reward, multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct large-scale multimodal preference dataset that covers wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) and vision-language models (VLMs) have recently achieved remarkable progress [17], demonstrating impressive capabilities across wide range of tasks. Despite these advances, aligning their behavior with human preferences remains significant challenge [8, 9, 6]. Reward models (RMs) have become indispensable in tackling this issue, serving as key components in both the training and inference stages of LLMs and VLMs [1012]. While reward models for text-only LLMs have been extensively studied, the development of multimodal RMs remains in its early stages, with two major limitations: Existing multimodal RMs lack generalizability across diverse tasks and struggle to effectively evaluate advanced VLM reasoners with complex inference. Hence, there is pressing need for multimodal RMs capable of assessing outputs from both standard VLMs and advanced VLM-based reasoners across diverse domains and tasks. In this paper, we introduce Skywork-VL Reward, multimodal RM designed to serve as comprehensive and robust evaluator for VLM outputs. Our approach addresses previous limitations in domain Equal contribution Corresponding author https://huggingface.co/Skywork/Skywork-VL-Reward-7B Tech report. Preprint. coverage and reasoning capacity by incorporating two critical improvements: (i) creating carefully curated multimodal preference dataset derived from various sources, and (ii) developing strong base model and training paradigm to enable effective vision-language understanding and reasoning. Specifically, we compile high-quality preference pairs from both publicly available datasets and internal annotations, spanning tasks from basic image descriptions to intricate reasoning scenarios. The collected preference pair includes the image (when applicable), textual prompt, and candidate responses sourced from standard VLMs [13, 14] and advanced VLM reasoners [6]. Building on this dataset, we construct Skywork-VL Reward based on Qwen2.5-VL-7B-Instruct, with an integrated reward head designed to output scalar scores aligned with human preferences. The model is trained using two-stage training paradigm that combines both pure-text and multimodal data, which enhances its generalization and performance across wide range of multimodal scenarios. Experimental evaluations confirm that Skywork-VL Reward achieves state-of-the-art results on VL-RewardBench [15] while maintaining competitive performance in text-only scenarios. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective when used for training with Mixed Preference Optimization (MPO) [16], leading to significant improvements in multimodal reasoning capabilities. Our contributions are summarized as follows. First, we introduce Skywork-VL Reward, multimodal reward model capable of evaluating outputs from both standard VLMs and advanced VLM reasoners across diverse domains and tasks. Second, our model achieves state-of-the-art results on VLRewardBench, while maintaining competitive performance in text-only scenarios. Third, preference data generated using Skywork-VL Reward proves highly effective in MPO training, demonstrating the practical value of our model."
        },
        {
            "title": "2 Related Work",
            "content": "Reward Models for Text-only Large Language Models. Reward modeling has become cornerstone of aligning LLM behavior with human preferences [1722]. Generally, RMs are trained on comparisons of outputs (chosen vs. rejected responses) to predict which output is better, often using data collected from human raters or AI assistants. Existing RMs can be categorized along two axes: (1) the model form [23] (discriminative RMs vs. generative RMs vs. implicit RMs) and (2) the feedback target (outcome-based vs. process-based supervision). Discriminative RMs [24, 9, 25] treat preference prediction as binary (or scalar) regression problem: given an input and candidate response, the model directly outputs score (or probability of being the preferred response). By contrast, generative RMs use language-model head to generate an evaluation or verdict based on specific prompt [26, 23, 2], rather than directly outputting numeric score. third category, implicit RMs [27], effectively reparameterize preference learning within the model itself via Direct Preference Optimization (DPO) [28], enables construct preference pairs without requiring an explicit RM. The second axis pertains to the nature of the feedback signal. Outcome-based RMs [29] generate single scalar reward for the entire response, reflecting its overall quality or correctness. Process-based RMs [30] assess intermediate steps within response, producing sequence of rewards that reflect the quality of reasoning throughout the generation. Reward Models for Vision-Language Models. Motivated by the observed benefits of preferencebased alignment for VLMs [16], the extension of reward modeling to the multimodal domain is an active research area. Most studies focused on generative RMs [3133] based on open-source VLMs and explored data augmentation for response generation [34, 35]. For discriminative RMs, LLaVA-RLHF [36] pioneered the application of Reinforcement Learning from Human Feedback (RLHF) [37] to VLMs by leveraging human feedback to train multimodal RM. Building on this, they further proposed Fact-RLHF, which enhances reward signals by incorporating additional information (e.g., image captions). To address the challenge of data scarcity, recent efforts have focused on the synthesis of large-scale, multi-domain preference datasets using advanced models as proxies for human evaluation. For instance, IXC-2.5-Reward [12] leverages GPT-4o and verifier functions [23] to automatically generate preference labels, leading to significant performance gains. Most of the current approaches belong to ORM, providing reward signal for the final outputs of VLMs. More recently, Wang et al. introduced VisualPRM [38], model trained on multimodal process supervision data that provides fine-grained multimodal process reward signals and serves as an effective critic model for test-time scaling of MLLMs, thereby enhancing the reasoning capabilities of existing VLMs. 2 Figure 1: Distribution of Training Data from Open-Source Sources."
        },
        {
            "title": "3 Method",
            "content": "Our objective is to develop multimodal reward model named Skywork-VL Reward, which takes as input an optional image, textual prompt, and candidate response generated by either multimodal understanding model or reasoning model. The model outputs scalar reward score that reflects the quality or degree to which the response aligns with human preferences. We achieve this by fine-tuning pretrained VLM on curated set of preference comparison data. In this section, we describe our dataset construction pipeline, the model architecture and modifications for reward modeling, the loss function used for pairwise preference training, and the overall training strategy. 3.1 Dataset Construction Open Source Data. We construct comprehensive training dataset for Skywork-VL Reward by integrating multiple open-source preference datasets and additional in-house annotations. The dataset primarily includes three sources: (1) LLaVA-Critic-113k [33], (2) Skywork-Reward-Preference-80Kv0.2 [10], and (3) RLAIF-V-Dataset [32]. LLaVA-Critic-113k is an open-source dataset consisting of 113k multimodal instructionresponse examples. Each example contains an image, user query, and one or more model responses with associated quality judgments. This dataset uniquely provides both pointwise scores and pairwise rankings generated by GPT-4o, covering tasks from straightforward image descriptions to complex reasoning challenges. Each pair is often accompanied by explanatory annotations, enriching our understanding of judgment criteria. Skywork-Reward-Preference-80K-v0.2 is high-quality dataset comprising 80k pairs of human-preferred textual responses, covering diverse domains such as general Q&A and creative writing. By eliminating noisy and inconsistent judgments through careful filtering, this dataset significantly enhances the text comprehension and alignment capabilities of Skywork-VL Reward, enabling it to effectively handle purely textual inputs. RLAIF-V-Dataset is large-scale multimodal feedback dataset containing 83,132 preference pairs. The instructions in this dataset are sourced from diverse datasets. Incorporating this 3 Field Table 1: Distribution of In-house Training Data. Physics Mathematics Biology Chemistry Others Percentage (%) 35.4 24.6 14.7 20.2 5.1 Table 2: Percentage of Generation Approaches of In-house Training Data. Approach Direct generation Two-step generation Percentage (%) 47.4 52. dataset greatly enhances the general multimodal understanding abilities of Skywork-VL Reward, enabling robust performance across varied tasks and contexts. In-house Reasoning Data. We augment existing datasets with proprietary in-house dataset consisting of approximately 50,000 preference comparisons focused on complex reasoning tasks. The tasks primarily involved carefully curated multimodal problems spanning mathematics, physics, biology, and chemistry  (Table 1)  . These comparisons were collected through human annotation, where annotators assessed the correctness and reasoning quality of various VLM-generated reasoning-style responses. 3.2 Data Curation Procedure Our data curation involved three stages. Open-source datasets were utilized in the initial two stages. For the final stage, we employed reasoning dataset derived from internal human annotation resources. Stage 1: The first data curation stage involved deduplication and filtering of the aggregated dataset. Specifically, our data curation involved the following: Deduplication: Removal of identical pairs across different sources. Similarity Filtering: Elimination of highly similar samples based on semantic similarity. Judgment Filtering: Discarding pairs with ambiguous or low-confidence preference judgments (compared to GPT-4o), where \"equal quality\" was considered ambiguous. This process yielded approximately 200,000 distinct, high-confidence preference pairs. This refined dataset was subsequently used to train surrogate RM, which then scored the entire dataset. Stage 2: We further refined the collected dataset based on scores predicted by the surrogate RM in the second stage, with the following principles: Low-Scoring Chosen Responses: If the chosen response in preference pair received low score from the surrogate RM, we regenerated that response using GPT-4o, effectively replacing the original. Minimal Score Difference: For preference pairs where the score difference between the chosen and rejected responses was minimal, we also regenerated the chosen response with GPT-4o to enhance clarity. Following this refinement process, total of 150,000 data samples were retained. The distribution of this resulting training data is presented in Figure 1. Stage 3: percentage breakdown presented in Table 2: In this stage, reasoning-style responses were generated via two distinct methods, with the Direct generation: Using our vision-language reasoning model Skywork R1V to directly produce reasoning-style responses conditioned on the original multimodal input. 4 Two-step generation: First employ standard VLM model (we use the InternVL series [39]) to generate textual description of the image, which serves as substitute for the original visual input. Then, this textual description is fed into llm-reasoner (we use the Deepseek R1 series [40]) to generate the final reasoning-style response. After the above procedure, our final training dataset consists of approximately 190k comparison samples (each prompt with two responses and preference), of which about 70% include images. We believe this dataset is one of the largest and most diverse multimodal preference collections to date, and it is key enabler of Skywork-VL Rewards strong generalization. 3.3 Model Architecture Our RM is built upon the Qwen2.5-VL-7B-Instruct [13]. Qwen2.5-VL-7B-Instruct is 7-billionparameter VLM recently released by Alibaba Cloud, which extends the Qwen-7B LLM with visual understanding capabilities. The architecture consists of three main components: Vision Transformer (ViT) as the visual encoder, vision-language adapter/projector, and the Qwen language model decoder. The visual encoder processes an input image into sequence of patch features. These raw image features are then passed through small adapter module that projects them into compact set of visual tokens that the language model can attend to. The language model then takes these visual tokens, along with text tokens from the prompt, and generates response. For Skywork-VL Reward, we modify this architecture to output scalar reward instead of text sequence. We remove Qwens causal LM head used for token prediction and replace it with reward head that produces single score. Concretely, we attach fully-connected layer on top of the final hidden state to predict the reward.The reward head processes the final hidden state following the answers last token, producing raw score rθ. This score is used during training to calculate preference loss across various answers. At inference, the Skywork-VL Reward model evaluates given prompt and response by outputting quality score. 3.4 Reward Model Loss Function We train Skywork-VL Reward using standard pairwise preference loss [41] commonly used in the reward modeling stage of RLHF. For given comparison example, we have prompt (with optional image) x, preferred response y+ (the one chosen by the annotator or judge), and dispreferred response (the one that was rejected). The model generates scalar reward score for each response: s+ = rθ(x, y+) for the preferred answer and = rθ(x, y) for the dispreferred one, where rθ represents the reward models output. Our loss function aims to maximize the difference between s+ and s, which can be formulated as: (cid:16) LRM(θ) = log σ (cid:17) rθ(x, y+) rθ(x, y) , (1) where σ(z) = 1 1+ez is the sigmoid function. This formulation focuses solely on learning the relative ranking of responses, encouraging the model to assign higher scores to preferred answers without explicitly calibrating the absolute reward scores. Consequently, examples with equal or near-equal preferences are prone to introducing ambiguity and were therefore excluded from our training data."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Training Details Training Parameter. To efficiently fine-tune the model as reward scorer, we adopt partial parameter freezing strategy [42]. In particular, we freeze the entire visual encoder of Qwen2.5VL-7B-Instruct to preserve its visual abilities pretrained on massive image-text data. The trainable weights in Skywork-VL Reward are limited to the projector, language backbone, and the reward head. Two-Stage Fine-Tuning Procedure. We formulate preference learning as supervised learning task over the constructed preference dataset. The fine-tuning follows two-stage training strategy. In the first stage, the model is trained exclusively on multimodal preference data, allowing it to develop strong vision-language alignment capabilities. In the second stage, we additionally incorporate 5 Table 3: Evaluation Results on VL-RewardBench. Models Model Size General Hallucination Reasoning Overall Accuracy Macro Average Claude-3.5-Sonnet(2024-06-22) Gemini-1.5-Flash (2024-09-24) GPT-4o(2024-08-06) Gemini-1.5-Pro(2024-09-24) Gemini-2.0-flash-exp(2024-12) Qwen2-VL-7B-Instruct MAmmoTH-VL-8B Qwen2.5-VL-7B-Instruct InternVL3-8B IXC-2.5-Reward-7B Qwen2-VL-72B-Instruct Molmo-72B-0924 QVQ-72B-Preview Qwen2.5-VL-72B-Instruct InternVL3-78B Skywork-VL Reward (Ours) - - - - - 7B 8B 7B 8B 7B 72B 72B 72B 72B 78B 7B Proprietary Models 43.4 47.8 49.1 50.8 50.8 55.0 59.6 67.6 72.5 72.6 Open-Source Models 31.6 36.0 43.4 60.6 80.3 38.1 33.9 41.8 47.8 67. 66.0 19.1 40.0 42.0 44.0 65.3 32.8 42.3 46.2 46.8 52.5 80.0 62.3 58.4 70.5 64.2 70.1 51.1 52.0 63.0 62.3 60.4 58.0 54.9 51.2 63.5 64.5 61. 55.3 57.6 65.8 67.2 68.8 28.3 42.2 48.0 57.0 66.3 39.5 44.1 46.4 51.6 63.3 73.1 53.6 55.3 62.4 62.5 64.5 33.9 42.7 49.5 55.6 68.6 43.0 43.7 46.4 52.7 61.6 69. pure-text preference data to further improve the models generalization and reasoning abilities in text-only scenarios. We use AdamW [43] with moderate learning rate for optimization in the first stage (105) and lower learning rate in the second stage (106 ). And the model is fine-tuned for 2 epochs per stage, which we find sufficient for convergence. 4.2 Evaluation Benchmarks We evaluate Skywork-VL Reward on two benchmarks: VL-RewardBench [15] and RewardBench [23]. VL-RewardBench is designed to assess vision-language reward modeling. It contains 1,250 carefully curated examples spanning general multimodal queries, visual hallucination detection, and complex reasoning tasks involving images. RewardBench is pure-text benchmark targeting reward functions for language models. This dataset includes prompt-chosen-rejected triplets covering diverse range of topics within general chat, safety, and reasoning. We report the performance on both benchmarks, specifically focusing on their key evaluation dimensions and the aggregated overall accuracy. 4.3 Baselines For VL-RewardBench, we compare Skywork-VL Reward against broad range of RMs, including both cutting-edge proprietary models and leading open-source alternatives. The proprietary multimodal RMs (closed-source) in our evaluation include GPT-4o [1], Claude 3.5 [44] with Vision, and Google Gemini 1.5 [2]. These models represent top-performing industrial models and serve as upper-bound references for RM performance. The involved prominent open-source models include Qwen2-VL-7B-Instruct [45], MAmmoTH-VL-8B [46], Qwen2.5-VL-7B-Instruct [13], InternVL3-8B [47], Qwen2-VL-72B-Instruct [45], IXC-2.5-Reward-7B [12], Molmo-72B [48], QVQ-72B-Preview [49], Qwen2.5-VL-72B-Instruct [13], and InternVL3-78B [47]. For RewardBench, we evaluate several advanced language-only RMs, including InternLM2-7BReward [24], Skywork-Reward-Llama3.1-8B [10], Skywork-Reward-Llama3.1-8B-v0.2 [10], and QRM-Llama3.1-8B-v2 [50]. We also evaluate multimodal RMs that are comparable in size to our own models including Qwen2-VL-7B-Instruct, InternVL3-8B, IXC-2.5-Reward-7B, and Qwen2.5VL-7B-Instruct. In our experiments, Qwen2.5-VL-7B-Instruct, InternVL3-8B, Qwen2.5-VL-72B-Instruct, IXC-2.5Reward-7B, and InternVL3-78B were reproduced by ourselves, whereas the results for the remaining models were obtained from official reports. 6 Table 4: Evaluation Results on RewardBench. Models Chat Chat Hard Safety Reasoning Avg Score Language-Only Reward Models InternLM2-7B-Reward Skywork-Reward-Llama3.1-8B Skywork-Reward-Llama-3.1-8B-v0.2 QRM-Llama3.1-8B-v2 99.2 95.8 94.7 96.4 69.5 87.3 88.4 86.8 MultiModal Reward Models Qwen2-VL-7B-Instruct InternVL3-8B Qwen2.5-VL-7B-Instruct IXC-2.5-Reward-7B Skywork-VL Reward (Ours) 65.1 97.2 94.3 90.8 90.0 50.9 50.4 63.8 83.8 87.5 87.2 90.8 92.7 92. 55.8 83.6 84.1 87.8 91.1 94.5 96.2 96.7 96.8 68.3 83.9 86.2 90.0 91.8 87.6 92.5 93.1 93. 60.0 78.8 82.1 88.1 90.1 4.4 VL-RewardBench Evaluation Table 3 presents comparisons with both proprietary and open-source models on VL-RewardBench. In the general category, skywork-VL Reward achieves score of 66.0%, significantly outperforming even the strongest proprietary model, Gemini-2.0-flash-exp (50.8%). However, gap remains compared to IXC-2.5-Reward-7B (80.3%). In the hallucination category, our model achieves the best score (80.0%), surpassing both proprietary models (e.g., Gemini-2.0-flash-exp at 72.6%) and the top-performing open-source model, IXC-2.5-Reward-7B (65.3%). This result highlights our models strong capability in mitigating factual inconsistencies. Our model also demonstrates robust performance in the reasoning category. It achieves reasoning score of 61.0%, which is comparable to that of the much larger InternVL3-78B (64.5%), despite having 10 fewer parameters. Our model achieves an overall accuracy of 73.1% and Macro Average of 69.0%, demonstrating superior performance across diverse task types and surpassing the best proprietary model, Gemini2.0-flash-exp (68.8% overall accuracy and 64.5% Macro Average), and the second-best open-source model, IXC-2.5-Reward-7B (66.3% overall accuracy and 68.6% Macro Average). These results demonstrate the effectiveness of our method in providing reliable reward signals for multimodal tasks. 4.5 RewardBench Evaluation Table 4 reports the results on RewardBench, language-focused reward benchmark. Our model achieves an average score of 90.1% on RewardBench, achieve advanced performance among all open-source multimodal RMs of comparable scale and outperforming the second-best model, IXC-2.5-Reward-7B, by 2.0%. It also shows competitive performance against advanced language-specific RMs, such as QRM-Llama3.1-8B-v2 (93.1%). Compared to other multimodal RMs of similar size, our model leads in Chat Hard (87.5%), Safety (91.1%), and Reasoning (91.8%), outperforming the second-best models by 3.7%, 3.3%, and 1.8%, respectively. These results highlight the robustness and well-rounded performance of our model across challenging and safety tasks. Furthermore, the results validate that our model is not only effective in handling multimodal data but also exhibits strong capabilities on pure-text inputs. 4.6 Case Study We present two illustrative examples that highlight the efficacy of our Skywork-VL Reward across distinct reasoning scenarios. For each example, we supply multimodal prompt together with good and bad answer. For each given answer, Skywork-VL Reward produces scalar reward value upon querying. The first example (Figure 2) is geometry problem that asks for the area of circular sector. While both candidate answers arrive at the same numerical conclusion, the good answer showcases accurate reasoning in its derivation, unlike the bad answers rambling, self-corrective approach. This indicates that Skywork-VL Reward strongly favors the concise solution, demonstrating its sensitivity to the quality of reasoning rather than just the final correctness. 7 Figure 2: Evaluating Skywork R1V on Mathematical Problems. 8 Figure 3: Evaluating Skywork R1V on Chart Problems. Table 5: Performance Evaluation on the MathVista Benchmark Using MPO with Different Reward Models. Model Base Qwen2.5-VL-7B-Instruct InternVL3-8B Ours Performance (%) 69. 71.2 71.8 73.5 The second example (Figure 3), involving identifying the country with the longest bar in an extreme poverty rate chart, further illustrates this. The good answer concisely states the label and cites relevant percentages, while the bad answer redundantly lists the same numbers. Again, Skywork-VL Reward strongly favors the compact explanation, demonstrating robustness across different visual domain. These two cases, spanning distinct domains, highlight Skywork-VL Rewards consistent ability to differentiate well-structured reasoning from verbose or confused discourse. The significant reward gap observed in both settings suggests the model captures valuable alignment signal for downstream reinforcement learning. 4.7 Skywork-VL Reward for Mixed Preference Optimization We examine the effect of Skywork-VL Reward as reward signal for MPO [16], recent strategy to further improve model alignment. MPO refers to optimizing the behavior of model using mixture of preference signals rather than single RM. This approach was proposed to stabilize and enhance training, especially for complex reasoning tasks that benefit from diverse feedback. Moreover, preference data generated using our Skywork-VL reward demonstrates high effectiveness in training MPO, resulting in substantial gains in multimodal reasoning abilities. We leverage Skywork-VL Reward to generate preference data, which is subsequently used to fine-tune VLM reasoner (the base model of Skywork R1V2 [7]) via MPO. Performance is evaluated on MathVista [51], challenging benchmark for mathematical reasoning over visual content. As shown in Table 5, incorporating Skywork-VL Reward as an additional reward yields notable improvement: the models MathVista score increases from 69.2% to 73.5%. This improvement demonstrates the potential of Skywork-VL Reward as critical component in training VLMs capable of long-CoT reasoning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce Skywork-VL Reward, multimodal reward model for VLMs, aimed at addressing the critical need for reliable and general-purpose evaluators in multimodal understanding and reasoning tasks. Through the construction of large-scale, meticulously curated preference dataset encompassing various tasks and scenarios, coupled with two-stage training paradigm, our model is able to effectively assess responses generated by both standard VLMs and VLM reasoners. Empirical results demonstrate the state-of-the-art performance of Skywork-VL Reward on the VL-RewardBench benchmark and its competitive capabilities on the text-only RewardBench. Furthermore, integrating Skywork-VL Reward to provide supervised signals for MPO significantly enhances the multimodal reasoning abilities of VLMs, highlighting its practical value."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4 technical report, 2023. 1, 6 [2] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 6 [3] OpenAI. Gpt-4o system card, 2024. [4] Gemini Team. era. agentic google-gemini-ai-update-december-2024/#ceo-message, 2024. Introducing gemini the https://blog.google/technology/google-deepmind/ new ai model 2.0: our for 10 [5] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [6] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-ofthought. arXiv preprint arXiv:2504.05599, 2025. 1, 2 [7] Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, and Yahui Zhou. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning, 2025. 1, 10 [8] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024. 1 [9] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216, 2024. 1, [10] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms, 2024. 1, 3, 6 [11] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark, 2025. [12] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. 1, 2, 6 [13] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 5, 6 [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2 [15] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. Vlrewardbench: challenging benchmark for vision-language generative reward models, 2024. 2, [16] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization, 2025. 2, 10 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2 [18] Gemma Team. Gemma. 2024. [19] Donald Joseph Hejna III and Dorsa Sadigh. Few-shot preference learning for human-in-the-loop rl. In Conference on Robot Learning, pages 20142025. PMLR, 2023. [20] Ali Narin. Evolutionary reward design and optimization with multimodal large language models. In Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 202208, 2024. [21] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. 11 [22] Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. 2 [23] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 2, 6 [24] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, et al. Internlm2 technical report, 2024. 2, [25] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. In ACL, 2024. 2 [26] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 2 [27] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. 2 [28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 2 [29] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. 2 [30] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. [31] Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Clip-dpo: Visionlanguage models as source of preference for fixing hallucinations in lvlms, 2024. 2 [32] Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness, 2024. 3 [33] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models, 2025. 2, 3 [34] Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, and Yapeng Tian. Efficient self-improvement in multimodal large language models: model-level judgefree approach, 2024. [35] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension, 2024. 2 [36] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023. 2 [37] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. 2 12 [38] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. 2 [39] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [40] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 5 [41] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 5 [42] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms, 2024. [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 6 [44] Anthropic. Claude-3.5, 2024. 6 [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [46] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale, 2024. 6 [47] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 6 [48] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. [49] Qwen Team. Qvq: To see the world with wisdom, December 2024. 6 [50] Nicolai Dorka. Quantile regression for distributional reward models in rlhf. arXiv preprint arXiv:2409.10164, 2024. 6 [51] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023."
        }
    ],
    "affiliations": [
        "Skywork AI, Kunlun Inc."
    ]
}