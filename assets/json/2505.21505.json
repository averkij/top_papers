{
    "paper_title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective",
    "authors": [
        "Shimao Zhang",
        "Zhejian Lai",
        "Xiang Liu",
        "Shuaijie She",
        "Xiao Liu",
        "Yeyun Gong",
        "Shujian Huang",
        "Jiajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 0 5 1 2 . 5 0 5 2 : r How does Alignment Enhance LLMs Multilingual Capabilities? Language Neurons Perspective Shimao Zhang1* Zhejian Lai1* Xiang Liu1* Shuaijie She1 Xiao Liu2 Yeyun Gong2 Shujian Huang1 Jiajun Chen1 1 National Key Laboratory for Novel Software Technology, Nanjing University 2 Microsoft Research Asia {smzhang,laizj,liuxiang,shesj}@smail.nju.edu.cn {xiao.liu.msrasia,yegong}@microsoft.com {huangsj,chenjj}@nju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multilingual Alignment is an effective and representative paradigm to enhance LLMs multilingual capabilities, which transfers the capabilities from the highresource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides new perspective to analyze and understand LLMs mechanisms more specifically in multilingual scenarios. In this work, we propose new finer-grained neuron identification algorithm, which detects language neurons (including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with focus on different types of neurons. We also analyze the phenomenon of Spontaneous Multilingual Alignment. Overall, our work conducts comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs."
        },
        {
            "title": "Introduction",
            "content": "By training on the extensive corpus, large language models (LLMs) demonstrate outstanding language capabilities [Grattafiori et al., 2024, Yang et al., 2024, Liu et al., 2024, Zhang et al., 2025]. However, due to the unbalanced pretraining corpus across different languages, LLMs have very uneven performance on high-resource languages and low-resource languages [Huang et al., 2023, Zhu et al., 2023, Zhang et al., 2024]. Therefore, researchers have conducted comprehensive explorations to further enhance the multilingual performance of LLMs. straightforward approach is increasing the proportion of non-English texts during pretraining [Ni et al., 2021, Yang et al., 2024] or performing continual pretraining with multilingual texts [Liu et al., 2021, Ji et al., 2024]. But these approaches often entail high computational costs and substantial amounts of multilingual data. *Equal contribution. Work done during his internship at MSRA. Corresponding author. 1The project will be available at https://github.com/NJUNLP/Language-Neurons-Alignment. Preprint. Under review. Considering LLMs great performance on the high-resource languages, multilingual alignment has emerged as representative paradigm for enhancing multilingual capabilities by transferring knowledge from high-resource to low-resource languages [Zhao et al., 2024a, She et al., 2024]. representative example is MAPO She et al. [2024], which improves multilingual alignment by utilizing well-trained multilingual translation model to compute alignment scores based on the conditional generation probability of translating non-English responses into English. Many studies conduct systematic mechanism analyses of the multilingual alignment and LLMs multilingual capabilities. Zhao et al. [2024b] split the multilingual processing workflow into three parts: multilingual understanding, resolving tasks, and generating outputs in the target language. This three-stage inference workflow clearly demonstrates how LLMs leverage English as pivot language to handle multilingualism using unified pattern. Inspired by the neurobiological underpinnings of human language faculties, Tang et al. [2024] conducts fine-grained identification by detecting the language-specific neurons. Their results indicate that LLMs predominantly utilize small subset of neurons to process the particular target language. Furthermore, these language-specific neurons are primarily situated in the models top and bottom layers [Tang et al., 2024], which is consistent with the threestage multilingual workflow of Zhao et al. [2024b]. However, we notice key limitation in the existing language-specific neuron identification methodology: Some neurons are shared across multiple languages but are not entirely language-agnostic. Such neurons are incorrectly categorized as either language-specific or language-agnostic neurons under the existing framework. We present case study in Figure 1. Furthermore, we aim to systematically explore an important question: Can we better analyze and understand how multilingual alignment enhances the LLMs multilingual capabilities from the perspective of language neurons? Figure 1: neurons activation probability across different languages. This neuron that exhibits high activation probabilities across multiple but not all languages cant be correctly categorized under the existing methodology. In this work, we comprehensively investigate the multilingual alignment of LLMs from the perspective of language neurons, where we deploy MAPO as representative multilingual alignment algorithm. Considering the above limitation of the existing language-specific neuron identification methodology, we define language neurons as the union of language-specific neurons and language-related neurons, as opposed to language-agnostic neurons. We separately distinguish language-related neurons from both language-specific and language-agnostic neurons, which allows for more precise analyses. Furthermore, we propose new language neuron identification algorithm in our work, which is able to identify language-related neurons that are in fact shared across multiple languages. Then we analyze the models before and after alignment, focusing on the changes in different types of neurons. Based on the distributional characteristics of these neurons, we divide LLMs internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Our findings reveal that different parts exhibit distinct dependencies on different types of neurons, and that multilingual alignment significantly enhances the activation of the corresponding types of neurons across the relevant layers. Additionally, we analyze the important Spontaneous Multilingual Alignment [Zhang et al., 2024] phenomenon in LLMs, providing insights into the roles of languageagnostic neurons and language-related neurons shared across languages. For further analysis, we also provide observations about the uniqueness of English and the neuron distributions. Overall, based on different types of neurons, we present empirical results and valuable insights that contribute to deeper understanding of multilingual alignment and the multilingual capabilities of LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multilingual Alignment Conducting pretraining or continual pretraining on the multilingual corpus is straightforward and effective method to enhance LLMs multilingual capabilities [Ni et al., 2021, Ji et al., 2024]. However, these methods typically require substantial investments in time, data, and computational resources. Thus, many researchers perform multilingual alignment to improve LLMs multilingual performance by transferring the capabilities from high-resource languages to low-resource languages [Eronen et al., 2023, Zhao et al., 2024c,a, She et al., 2024], which efficiently and effectively improves the model performance in low-resource language scenarios. Furthermore, Zhang et al. [2024] first finds the Spontaneous Multilingual Alignment phenomenon in LLMs, which demonstrates that conducting multilingual alignment based on small number of languages effectively improves the alignment even between English and many languages unseen during alignment. 2.2 Mechanistic Interpretability In addition to enhancing LLMs multilingual performance, research on the underlying mechanisms of multilingual capabilities in LLMs is still ongoing. It is crucial for us to understand and explain the LLMs and related methods explicitly. Typically, the existing approaches primarily perform mechanistic interpretability analyses by observing the internal states of the model [Nostalgebraist, 2020, Zhang et al., 2024, Zhao et al., 2024b, Mousi et al., 2024]. Overall, neuron states and latent intermediate logits are both important objects of observation. For latent logits, Wendler et al. [2024] utilizes logit lens [Nostalgebraist, 2020] to directly project the logits in the intermediate layers to the vocabulary space, which reveals the latent participation of English in the intermediate layers. For neuron states, Hu et al. [2024] analyzes the neuron activation overlap to measure the extent of shared neuron activation across different languages. 2.3 Language-Specific Neurons Many studies have revealed the language-related and language-agnostic components in LLMs. At the layer level, the multilingual processing of LLMs is considered to involve three stages [Zhao et al., 2024b, Wendler et al., 2024]: converting multilingual inputs into shared semantic space, intermediate-layer reasoning, and outputting in the target language. The top and bottom layers of the model handle multilingual processing, while the intermediate layers perform inference in similar patterns across different languages. This demonstrates distinct division of labor within the model at the layer level regarding language specificity. Furthermore, many studies investigate the finer-grained methods for language-specific neuron identification [Kojima et al., 2024, Tang et al., 2024]. Tang et al. [2024] categorizes activated neurons into language-specific neurons and language-agnostic neurons. They detect language-specific neurons by calculating language activation probability entropy on massive text. However, we find that some neurons are activated by multiple languages (i.e., not language-specific), yet are not universally activated across all languages (i.e., not language-agnostic). Simply categorizing activated neurons into two classes blurs this distinction. Thus, we propose new method to identify neurons, which categorizes activated neurons into three types: language-specific, language-related, and language-agnostic."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our overall analysis pipeline in our work. First, we review the existing multilingual alignment algorithm and neuron analysis techniques as preliminary study in 3.1. Then we introduce the multilingual alignment algorithm we utilize in our work in 3.2. Finally, for mechanistic interpretability analysis, we introduce our new method for detecting language-specific, language-related, and language-agnostic neurons (3.3). 3.1 Preliminary Study Most LLMs are pretrained mainly on the high-resource language corpus, which leads to LLMs unstable and unbalanced performance in multilingual scenarios. As representative multilingual 3 alignment algorithm, Multilingual-Alignment-as-Preference Optimization (MAPO) [She et al., 2024] effectively and efficiently improves the LLMs multilingual performance. Additionally, it is also important for us to understand and analyze the mechanism of LLMs multilingual capabilities and multilingual alignment. Moreover, some studies on the identification of the language-specific and language-agnostic neurons in LLMs [Tang et al., 2024, Kojima et al., 2024]. It is found that LLMs capabilities of processing particular language mainly come from small subset of neurons [Tang et al., 2024]. However, there are still many important questions waiting for further investigation. On the one hand, many methods overlook neurons activated by multiple languages yet not language-agnostic, namely language-related neurons that lie between language-specific and language-agnostic categories. On the other hand, research from the perspective of language neurons on the underlying mechanisms of LLMs multilingual alignment and multilingual capabilities remains quite limited, which is essential for better understanding and improving the multilingual performance of LLMs. 3.2 Multilingual Alignment MAPO is typical multilingual alignment algorithm to align the reasoning capabilities of non-English language responses with those of English, which serves as the pivot language. Specifically, for given query in target (non-English) language and its corresponding English variant XEng, we collect their respective responses and YEng. An off-the-shelf translation model, parameterized by θ, is deployed to estimate the conditional generation probability (Y YEng; θ) by force-decoding conditioned on YEng. higher conditional probability is interpreted as stronger alignment between the target language response and its English counterpart. This probability is then used as an alignment score, denoted rθ(X, ). This alignment score can be integrated into preference optimization algorithms. For instance, in PPO [Schulman et al., 2017], rθ(X, ) can be directly employed as the reward score. In DPO [Rafailov et al., 2023], for each target language, distinct outputs are generated. Based (cid:1) preference pairs (Yw, Yl), where Yw is on the alignment score these outputs are used to form (cid:0)n deemed superior to Yl due to higher alignment score. The model is then optimized by Eq1. 2 LDP O(πθ; πref ) = E(X,Yw,Yl)D log σ β log (cid:20) (cid:18) πθ(YwX) πref (YwX) β log (cid:19)(cid:21) πθ(YlX) πref (YlX) (1) 3.3 Language Neurons Identification Following Tang et al. [2024], the neurons in our work are defined as linear transformation of single column in weighted matrix followed by non-linear activation, SiLU [Shazeer, 2020]. For the j-th neuron in the i-th layer, its activation probability when processing responses in language is computed as: i,j = (cid:0)I (cid:0)SiLU(xiW i)j > 0(cid:1) language k(cid:1) pk (2) We define language neurons as those exhibiting higher activation probabilities for some languages, while discriminating from others. To select neurons that, despite exhibiting relatively higher entropy, still demonstrate high activation probabilities for some languages, we additionally incorporate the maximum activation probability, as formulated in Eq 3: scorei,j = (cid:88) k=1 i,j log pk pk i,j λ max 1kl pk i,j, (3) where i,j represents the probability distribution pi,j after normalization and λ is balancing coefficient. Neurons with scores falling in the lowest percentile, specifically, the bottom 5% are selected. Furthermore, to identify how many languages each selected neuron is related to, we introduce threshold τ and compute: Ni,j = (cid:88) k=1 I(cid:0)pk i,j > τ (cid:1). 4 (4) neuron is considered as language-specific neuron if Ni,j = 1, and as language-related neuron if 1 < Ni,j < l. Both of them belong to language neuron. In contrast, neuron is considered language-agnostic neuron if it exhibits high activation probabilities across all languages. Finally, given our focus on multilingual reasoning tasks, we select neurons exclusively based on responses from multilingual reasoning datasets, rather than relying on multilingual plain text [Tang et al., 2024]."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Following She et al. [2024], we conduct our experiments and analyses on the mathematical reasoning tasks and different languages. In this section, we introduce our experimental settings in detail. Models We include two different models in our experiments and analyses. Following She et al. [2024], we conduct our experiments on MistralMathOctopus-7B 2 and MetaMathOctopus7B 3. MistralMathOctopus is obtained by fine-tuning MetaMath-Mistral [Yu et al., 2023] with MGSM8KInstruct [Chen et al., 2023]. MetaMathOctopus is obtained by fine-tuning MetaMath [Yu et al., 2023] with MGSM8KInstruct. Considering limited computational resources and reproducibility, we directly utilize the publicly released base models. Our analyses are mainly based on MistralMathOctopus in the main text and we report more results in the Appendix. Datasets We conduct experiments on two representative mathematical reasoning benchmarks, MGSM [Shi et al., 2022] and MSVAMP [Chen et al., 2023]. MGSM is widely used benchmark for multilingual mathematical reasoning evaluation. MSVAMP is an out-of-domain test set in contrast to MGSM, which evaluates robustness and generalization [Zhu et al., 2024, She et al., 2024]. Languages Following She et al. [2024], we choose the following 10 different languages for analysis in our work. As pivot language, English (en) is used as the alignment target. We also choose Chinese (zh), Russian (ru), German (de), French (fr), Spanish (es), Japanese (ja), Swahili (sw), Thai (th) and Bengali (bn) as 9 representative non-English languages. Implementations Due to limited computational resources, our exploration focuses on the most effective DPO variant of MAPO [She et al., 2024]. We select 1, 4, and 8 tasks from the NumGLUE [Mishra et al., 2022], an arithmetic reasoning benchmark, and translate questions into 9 languages, consistent with the MGSM, thereby creating multilingual seed dataset. To construct preference pairs, we sample responses using the corresponding base models and employ NLLB-200distilled-600M4 as the translation model to obtain alignment scores. Finally, for each model and each target language (excluding English), we gain 10,000 preference pairs. Training is conducted using LoRA [Hu et al., 2022]. During the neuron selection stage, we perform force-decoding on the responses of the MGSM or MSVAMP dataset to obtain the activation probabilities of neurons for each language. Based on empirical results on development sets, we set the balancing coefficient λ = 0.04 and the threshold τ = 0.5. Additional implementation details are provided in Appendix B. 4.2 Language Neurons Identification Based on the neuron identification algorithm introduced in 3.3, we identify the language-specific neurons, language-related neurons, and language-agnostic neurons in the model. To further validate the effectiveness of our algorithm, we follow Tang et al. [2024] by examining changes in the perplexity of LLMs by deactivating the identified language neurons across different languages. Experiments are conducted on both the base model and the aligned model, with results presented in Figure 2. We report the results of both language-specific neurons and language neurons. It can be found that whether deactivating language-specific neurons or all language neurons, the results consistently exhibit the same pattern: the diagonal elements in each row show the highest 2https://huggingface.co/kevinpro/MistralMathOctopus-7B 3https://huggingface.co/kevinpro/MetaMathOctopus-7B 4https://huggingface.co/facebook/nllb-200-distilled-600M 5 (a) Base - Language-Specific Neurons (b) Aligned - Language-Specific Neurons (c) Base - Language Neurons (d) Aligned - Language Neurons Figure 2: PPL changes of MistralMathOctopus on MGSM after deactivating language-specific neurons or language neurons. Base indicates the results of the base model. Aligned indicates the results of the aligned model. For comparison, the results of Tang et al. [2024] are provided in Appendix D. values. Notably, deactivating language neurons leads to more pronounced effect compared to deactivating only language-specific neurons. These observations support the following findings: (1) Our algorithm effectively identifies language-specific and language-related neurons; (2) For given language, in addition to its language-specific neurons, there are also substantial number of shared language-related neurons contributing to its performance; (3) Deactivating all the language-related neurons of one language doesnt cause significant impacts on the models performance in other languages. The above findings confirm the validity of the language neurons identified by our method and further provide insights into the characteristics of language neurons. 4.3 Layer-wise Functionality Analysis Based on the identified neurons, we perform layer-wise functional analyses of all layers in the LLMs. We begin by analyzing the distributions of different types of neurons in the base model. And we report the results in Figure 3. Through the analysis of the distribution of different types of neurons, we can further divide the LLMs internal process for multilingual inference into four parts rather than the three-stage division discussed in some works [Wendler et al., 2024, Zhao et al., 2024b]: 1. Multilingual Understanding: the number of language neurons (language-specific and language-related neurons) peaks, while the number of languageagnostic neurons is relatively low. The model maps multilingual inputs into unified semantic space at this stage. In the initial layers, 2. Shared Semantic Space Reasoning: In the intermediate layers, the model engages in reasoning within shared semantic space across different languages. During this stage, language neurons are largely absent, whereas language-agnostic neurons become dominant. 3. Multilingual Output Space Transformation: The model transfers features into the multilingual output space in this stage in preparation for generating the final output. In this part, 6 Figure 3: Layer-wise distribution of the different types of neurons of MistralMathOctopus on MGSM. the number of language neurons reaches peak again, while the number of language-agnostic neurons drops to the lowest point. 4. Vocabulary Space Outputting: In the last layer, the model maps vectors of different languages into shared vocabulary space to generate outputs. The number of both languagerelated and language-agnostic neurons rises sharply, while the language-specific neurons are fewer than those in several previous layers. Meanwhile, the distribution of different types of neurons aligns with the conclusions from existing studies mentioned above. Overall, we can find that the number of neurons varies correspondingly with the different inference stages of LLMs. 4.4 Layer-wise Neuron Changes Analysis In 4.3, we investigate the fundamental distribution of neurons and the basic partitioning of the model. We further analyze the changes in different types of neurons before and after multilingual alignment. Based on the four functional stages in LLMs, we quantify the layerwise changes () in the number of different types of neurons. Figure 4 presents the results for language-specific neurons, language-related neurons, and language-agnostic neurons. During the multilingual understanding stage, the number of language neurons increases, while language-agnostic neurons decrease. In the subsequent shared semantic space reasoning stage, language-agnostic neurons increase substantially, whereas language neurons remain stable and nearly absent. Then, in the third stage, as language-agnostic neurons decrease, language neurons increase overall. Additionally, we notice that the number of language-related neurons show an upward trend. Finally, in the last stage, the number of language-agnostic increases significantly in the aligned model, accompanied by reduction in language neurons. We also report the results of different checkpoints during the alignment process in Appendix F. Overall, we find that language neurons and language-agnostic neurons exhibit generally opposite trends across different layers, which Figure 4: Layer-wise changes in the number of different types of neurons of MistralMathOctopus on MGSM. 7 Figure 5: Changes in the number of neurons shared by languages after alignment. Table 1: Accuracy of the MistralMathOctopus base model and aligned model on MGSM. \"X/Y T\" indicates that languages and are used for multilingual alignment."
        },
        {
            "title": "MGSM",
            "content": "bn th sw ja zh ru de es fr en Avg. 43.6 53.2 50.4 55.6 59.6 59.2 61.2 62.8 56.8 75.6 57.8 base zh/de en 46.4 55.6 59.2 56.8 64.0 71.2 66.8 71.2 69.2 75.2 63.6 sw/th en 48.8 58.8 59.2 56.4 68.4 68.4 69.2 69.6 70.4 77.6 64.7 corresponds to the characteristics of LLMs at different stages of inference. Especially, at the last stage, language-agnostic neurons play an more important role than language neurons. Multilingual alignment facilitates more effective activation of the appropriate neurons at each stage, thereby improving the models capability to handle multilingual tasks. 4.5 Macroscopic Analysis of Different Types of Neurons We further conduct macroscopic analysis for different types of neurons. In our neuron identification algorithm introduced in 3.3, the number of languages that share specific neuron is an attribution characterizing all types of activated neurons. Since our study involves 10 languages, the valid range of is from 1 to 10. Among these, values of from 2 to 9 correspond to language-related neurons. As special cases in our work, = 1 represents language-specific neurons, while = 10 corresponds to language-agnostic neurons. We report the changes in the number of neurons after multilingual alignment for each value of ranging from 1 to 10 in Figure 5. The results show decrease in the number of language-specific neurons, while an increase in the number of language-related neurons, which are shared across multiple languages. This indicates that multilingual alignment encourages LLMs to develop and utilize more shared language-related neurons, rather than on language-specific neurons, which are applicable to only single language. Meanwhile, during the alignment process, the model improves its understanding of task-relevant common knowledge. Therefore, the overall number of languageagnostic neurons also increases significantly. 4.6 Spontaneous Multilingual Alignment Analysis The important Spontaneous Multilingual Alignment phenomenon is first revealed and discussed by Zhang et al. [2024], which means that conducting alignment in small number of languages significantly improves multilingual alignment even between English and many languages unseen during the alignment process. We further analyze this phenomenon in our experiments. As shown in Table 1, spontaneous multilingual alignment also emerges under the multilingual alignment strategy employed in our study. Except for the languages used for alignment, LLMs exhibit notable performance gain in other unaligned languages. Table 3: Average number of different types of neurons for English and non-English languages of MistralMathOctopus on MGSM. We round the results to the nearest integer. Language Language-Specific Language-Related English non-English 46 613 603 2006 Table 4: Overlap ratio of different types of neurons across different domains and before and after alignment. Following She et al. [2024], MSVAMP is regarded as an out-of-domain dataset. The results of MistralMathOctopus on MGSM are used as the fiducial value. Variable (%) Language-Specific Language-Related Domain Alignment 80.7 95.6 92.3 92.1 To understand how multilingual alignment generalizes to other languages, we analyze the changes in different types of neurons before and after multilingual alignment based on our method. Taking the case of zh/de en as representative example, we report the average results in Table 2. For the trained languages, the number of language-specific neurons decreases, while the number of language-related neurons increases. This indicates that the aligned languages tend to utilize more languagerelated neurons shared with other languages rather than exclusive language-specific neurons. Moreover, we extend this analysis to languages other than the trained languages and observe similar phenomenon. These findings indicate that multilingual alignment facilitates the use of language-related neurons while reducing the reliance on language-specific neurons in both trained and other unseen languages. We hypothesize that the new language-related neurons shared with trained languages contribute to the performance improvement on other unseen languages. Table 2: Average results of neuron count changes across multiple languages. Trained indicates the trained languages in the spontaneous multilingual alignment experiment. Others indicates other languages except the trained languages. We round the results to the nearest integer. Language Language-Specific Language-Related Trained Others +232 +205 -37 -36 4.7 Further Analysis Uniqueness of English Since current LLMs are primarily pretrained on English data, English is often regarded as playing special role within LLMs [Wendler et al., 2024]. In our experiments, we also observe that English exhibits markedly different characteristics compared to other non-English languages. Based on the identified neurons in our work, in Figure 2, we can find that deactivating the language neurons of English has negligible impact on the models performance in English, which is entirely different from the behavior observed in other languages. This is also consistent with the results of Tang et al. [2024]. Furthermore, based on this finding, we quantify the number of language neurons for English and non-English languages based on the MistralMathOctopus base model  (Table 3)  . Our analysis reveals that English has significantly fewer neurons than other languages, both in terms of language-specific and language-related neurons. We hypothesize that this is due to the fact that English actually possesses numerous language-related neurons. And since English serves as pivot language, these language-related neurons are likely shared with almost all other languages. It causes them to be confounded with language-agnostic neurons. Stability of Neuron Distributions We discuss the stability of neuron distributions across different data domains, as well as before and after alignment. To quantify the stability of neuron distributions, we compute the neuron overlap ratio in both settings, with the results summarized in Table 4. We can find that although the exact positions of few language neurons may vary across different settings, the positional distribution of most language neurons remains stable. This also indicates good reliability and generalization of the language neurons identified under fixed hyperparameters."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we systematically investigate the multilingual alignment from the perspective of language neurons. We propose new language neuron identification algorithm based on entropy and probability value, which detects the language-specific neurons, language-related neurons, and 9 language-agnostic neurons in LLMs. The validity of the identified neurons is confirmed through deactivation ablation experiments. Furthermore, we examine the multilingual alignment mechanism by analyzing the roles of different types of neurons. Based on their distributional characteristics, we categorize LLMs internal process into four functional parts. Our analysis reveals that multilingual alignment enhances the models utilization of the corresponding types of neurons across different functional parts. Meanwhile, we find that alignment promotes greater reliance on shared languagerelated neurons across languages, rather than on language-specific neurons. We also explore the phenomenon of spontaneous multilingual alignment. Additionally, we provide further analysis and more empirical results based on the preceding findings."
        },
        {
            "title": "References",
            "content": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, and Yeyun Gong. Process-based self-rewarding language models. arXiv preprint arXiv:2503.03746, 2025. Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in llms: Improving multilingual capability by crosslingual-thought prompting. arXiv preprint arXiv:2305.07004, 2023. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023. Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, and Shujian Huang. Getting more from less: Large language models are good spontaneous multilingual learners. arXiv preprint arXiv:2405.13816, 2024. Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang, and Nan Duan. M3p: Learning universal representations via multitask multilingual multimodal pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39773986, 2021. Zihan Liu, Genta Indra Winata, and Pascale Fung. Continual mixed-language pre-training for extremely low-resource neural machine translation. arXiv preprint arXiv:2105.03953, 2021. Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyán OBrien, Hengyu Luo, Hinrich Schütze, Jörg Tiedemann, et al. Emma-500: Enhancing massively multilingual adaptation of large language models. arXiv preprint arXiv:2409.17892, 2024. Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond english: An empirical study on language capability transfer. arXiv preprint arXiv:2401.01055, 2024a. Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization. arXiv preprint arXiv:2401.06838, 2024. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. How do large language models handle multilingualism? arXiv preprint arXiv:2402.18815, 2024b. 10 Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. Language-specific neurons: The key to multilingual capabilities in large language models. arXiv preprint arXiv:2402.16438, 2024. Juuso Eronen, Michal Ptaszynski, and Fumito Masui. Zero-shot cross-lingual transfer language selection using linguistic similarity. Information Processing & Management, 60(3):103250, 2023. Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, and Lidong Bing. Adamergex: Cross-lingual transfer with large language models via adaptive adapter merging. arXiv preprint arXiv:2402.18913, 2024c. Nostalgebraist. interpreting gpt: the logit lens. https://www.lesswrong.com/posts/ AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. Basel Mousi, Nadir Durrani, Fahim Dalvi, Majd Hawasly, and Ahmed Abdelali. Exploring alignIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar, ment in shared cross-lingual spaces. editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 63266348, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.344. URL https: //aclanthology.org/2024.acl-long.344/. Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. Do llamas work in english? on the latent language of multilingual transformers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1536615394, 2024. Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, and Shujian Huang. Large language models are cross-lingual knowledge-free reasoners. arXiv preprint arXiv:2406.16655, 2024. Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, and Yutaka Matsuo. On the multilingual ability of decoder-based pre-trained language models: Finding and controlling languagespecific neurons. arXiv preprint arXiv:2404.02431, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246, 2023. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question translation training for better multilingual reasoning. arXiv preprint arXiv:2401.07817, 2024. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: suite of fundamental yet challenging mathematical reasoning tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 35053523, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.246. URL https://aclanthology.org/2022.acl-long.246/. 11 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023."
        },
        {
            "title": "A Limitations",
            "content": "We provide insights and analysis results for multilingual alignment and multilingual capabilities of LLMs, which allows for better understanding of multilingualism. Despite we have conducted systematic investigation in our work, there are still some limitations waiting for research. Due to the limited resources, we only conduct experiments on two different models and two different datasets. We are willing to perform more comprehensive analysis on different scenarios if more resources are available in the future. Additionally, we dont perform finer-grained analysis of neurons within the same layer in this work. We would like to explore this in our future work."
        },
        {
            "title": "B Implementation Details",
            "content": "Our experiments are conducted on 4 NVIDIA RTX A6000 GPUs or 4 NVIDIA GeForce RTX 3090 GPUs. We use the TRL [von Werra et al., 2020] and DeepSpeed [Rasley et al., 2020] frameworks for preference alignment, and the vLLM engine Kwon et al. [2023] for inference. B.1 MAPO We employ the officially released scripts5 to generate the preference data. For the alignment process, we set the learning rate to 1e-6 and the batch size to 16. LoRA is utilized to fine-tune the model with LoRA rank of 64, LoRA alpha of 128, and LoRA dropout rate of 0.05. The total number of training steps is set to 1000. It takes 7.5 hours for one alignment. B.2 Language Neurons Identification We spend 25 minutes calculating the activation probability of each neuron with MGSM dataset on single NVIDIA GeForce RTX 3090 GPU. We spend 2 hours calculating the activation probability of each neuron with MSVAMP dataset on single NVIDIA GeForce RTX 3090 GPU. After obtaining the activation probability, it takes 3 minutes to identify how many languages each selected neuron is related to."
        },
        {
            "title": "C Licenses for Used Assets",
            "content": "We list the names of the licenses for each asset we utilized in our work: MGSM: CC-BY-SA-4.0 MSVAMP: Apache-2.0 GSM8KInstruct: Apache-2.0 NumGLUE: Apache-2.0 MistralMathOctopus: Apache-2.0 MetaMathOctopus: Apache-2.0 NLLB-200-distilled-600M: CC-BY-NC-4.0 TRL: Apache-2.0 DeepSpeed: Apache-2.0 vLLM: Apache-2."
        },
        {
            "title": "D Deactivation Ablation Experiments",
            "content": "For the MistralMathOctopus model, Figure 6 presents the results of the deactivation ablation experiments conducted on MGSM using the neuron identification algorithm proposed by Tang et al. [2024]. Additionally, Figure 7 presents the results of the deactivation ablation experiments on the out-of-domain test set, MSVAMP. To verify the generalization of our identification algorithm, we also deploy the MetaMathOctopus to conduct the deactivation ablation experiment, with the result in Figure 8. 5https://github.com/NJUNLP/MAPO 13 Layer-wise distribution of the different types of neurons In Figure 9a and 9b, we separately display the layer-wise distribution of the different types of neurons of MistarlMathOctopus on MSVAMP and MetaMathOctopus on MGSM to validate the generalization of our finding discussed in 4.3. Layer-wise Changes in the Number of Different Types of Neurons During"
        },
        {
            "title": "Alignment",
            "content": "Figure 10 shows the same pattern discussed in 4.4, thereby supporting its generalization. Furthermore, for each dataset, we examine the layer-wise changes in the number of different types of neurons in the process of alignment in Figure 11, 12 and 13. The notation ckpt-x denotes the model checkpoint obtained after training steps."
        },
        {
            "title": "G Spontaneous Multilingual Alignment",
            "content": "We report the accuracy of the base model on the MGSM and MSVAMP benchmarks in Table 5. The expected accuracy of random guessing is 50.0%. \"X/Y T\" indicates that languages and are used for alignment. The best results for each language are highlighted. It is evident that models trained on multilingual translation data substantially outperform the original model across wide range of languages, indicating that multilingual training significantly enhances the models multilingual capabilities. 14 (a) Base - Language-Specific Neurons (b) Aligned - Language-Specific Neurons Figure 6: PPL changes of MistralMathOctopus on MGSM when deploying the algorithm proposed by Tang et al. [2024]. (a) Base - Language-Specific Neurons (b) Aligned - Language-Specific Neurons (c) Base - Language Neurons (d) Aligned - Language Neurons Figure 7: PPL changes of MistralMathOctopus on MSVAMP after deactivating language-specific neurons or language neurons. Base indicates the results of the base model. Aligned indicates the results of the aligned model. 15 (a) Base - Language Neurons on MGSM (b) Aligned - Language Neurons on MGSM Figure 8: PPL changes of MetaMathOctopus on MGSM after deactivating language neurons. Base indicates the results of the base model. Aligned indicates the results of the aligned model. (a) MistralMathOctopus on MSVAMP. (b) MetaMathOctopus on MGSM. Figure 9: Layer-wise distribution of the different types of neurons. Figure 10: Layer-wise changes in the number of MetaMathOctopus (a) MGSM (b) MSVAMP Figure 11: Layer-wise changes in the number of language-specific neurons of MistralMathOctopus during the alignment. (a) MGSM (b) MSVAMP Figure 12: Layer-wise changes in the number of language-related neurons of MistralMathOctopus during the alignment. (a) MGSM (b) MSVAMP Figure 13: Layer-wise changes in the number of language-agnostic neurons of MistralMathOctopus during the alignment. 17 Tested on MGSM bn th sw ja zh ru de es fr en Avg. 43.6 53.2 50.4 55.6 59.6 59.2 61.2 62.8 56.8 75.6 57.8 base zh en 49.6 58.4 54.8 56.4 65.2 70.0 66.4 72.8 68.8 78.4 64.1 zh/de en 46.4 55.6 59.2 56.8 64.0 71.2 66.8 71.2 69.2 75.2 63.6 sw/th en 48.8 58.8 59.2 56.4 68.4 68.4 69.2 69.6 70.4 77.6 64.7 zh/es/ru en 46.0 56.4 58.8 54.8 63.2 70.8 68.8 71.6 69.6 76.8 63.7 zh/es/fr/ja/de/sw/ru/th/bn en 49.6 60.0 56.4 56.4 64.4 64.8 65.2 65.2 61.2 76.4 62.0 Tested on MSVAMP bn th sw ja zh ru de es fr en Avg. 49.3 62.5 60.6 60.9 67.4 64.9 66.5 67.6 67.2 77.0 64.4 base zh en 52.8 62.5 60.9 63.7 66.6 67.5 69.4 69.8 69.7 76.5 65.9 zh/de en 53.2 62.0 62.9 65.3 66.1 68.7 69.9 69.0 70.1 77.0 66.4 sw/th en 53.8 67.4 65.1 67.7 71.9 71.4 71.9 72.0 72.8 78.9 69.3 zh/es/ru en 52.9 63.2 61.5 63.0 67.8 68.1 69.8 68.7 70.6 78.3 66.4 zh/es/fr/ja/de/sw/ru/th/bn en 54.9 65.9 65.6 68.6 69.7 70.2 72.3 71.6 71.6 77.3 68.8 Table 5: Accuracy of the base model and aligned variants on benchmarks."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "National Key Laboratory for Novel Software Technology, Nanjing University"
    ]
}