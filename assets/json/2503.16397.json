{
    "paper_title": "Scale-wise Distillation of Diffusion Models",
    "authors": [
        "Nikita Starodubcev",
        "Denis Kuznedelev",
        "Artem Babenko",
        "Dmitry Baranchuk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies."
        },
        {
            "title": "Start",
            "content": "Scale-wise Distillation of Diffusion Models"
        },
        {
            "title": "Dmitry Baranchuk",
            "content": "Yandex Research https://yandex-research.github.io/swd 5 2 0 2 0 2 ] . [ 1 7 9 3 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present SWD, scale-wise distillation framework for diffusion models (DMs), which effectively employs nextscale prediction ideas for diffusion-based few-step generators. In more detail, SWD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SWD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SWD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies. 1. Introduction Diffusion models (DMs) are the leading paradigm for visual generative modeling [2, 4, 12, 13, 22, 2830, 46, 47, 49, 53, 56, 58]. They typically start with Gaussian noise at the target resolution and iteratively denoise it to produce clear data sample. However, generating high-resolution images or videos (e.g., 10241024 or larger) becomes computationally prohibitive when operating directly in pixel space. To mitigate this, state-of-the-art DMs [4, 44, 46, 47, 51, 55] leverage lower-resolution latent space provided by VAE [32], which significantly reduces computational overhead. Despite this, the VAEs used in latent DMs typically employ an 8 scaling factor, meaning the latent space still remains high-dimensional. Given the slow sequential diffusion process requiring 2050 steps, generation speed is significant bottleneck, especially for newer, large-scale models with 830 billion parameters [4, 47, 49, 55]. Previous works have made substantial efforts in DM acceleration from different perspectives [35, 39, 56, 71, 74]. One of the most successful directions is distilling DMs into few-step generators [31, 54, 60, 74], aiming to achieve the inference speeds comparable to single-step generative models, such as GANs [15]. This direction has procreated novel families of generative frameworks, combining DMs with adversarial training [54, 55], distribution matching [73, 74] and consistency modeling [41, 57, 60]. Notably, these approaches generally focus on reducing the number of sampling steps while freezing other promising degrees of freedom, such as model architectures or data dimensionality. Recently, there has been introduced highly promising next-scale prediction paradigm [63], i.e., generating images by gradually increasing the resolution at each step. This approach offers natural inductive bias for visual generation, resembling the coarse-to-fine process inherent in human perception and drawing. This paradigm has been shown to compete with state-of-the-art DMs [18, 62, 63, 68] while offering significantly faster inference. Interestingly, Dieleman [9], Rissanen et al. [50] have also noticed the coarse-to-fine nature of the image diffusion generative process, drawing parallels to the implicit form of spectral autoregression. Specifically, low frequency image information is modeled at high noise levels, while higher frequencies are progressively produced over the reverse diffusion process. This observation establishes connection to the next-scale prediction models, which predict higher frequency details at each step via upscaling. Nevertheless, DMs still operate within fixed dimensionality throughout the diffusion process, highlighting an underexplored direction for improving their efficiency. In this work, we propose to leverage this insight by generating images scale-by-scale across the diffusion process. To this end, we introduce Scale-wise Distillation of diffusion models (SWD), simple yet effective modification of state-of-the-art diffusion distillation methods. Unlike previous diffusion-based approaches, SWD gradually increases the spatial resolution during sampling using single model. Through this approach, we demonstrate that performing sampling steps at smaller scales, rather than omitting them entirely, holds significant promise for improving the effec1 tiveness of few-step generators. Below, we outline the contributions of this paper: We introduce SWD, novel scale-wise diffusion distillation method that progressively increases spatial resoluInspired by spectral analytion at each sampling step. sis of VAE latent spaces in existing text-to-image DMs, SWD avoids redundant computations at high noise levels, where high frequencies are largely suppressed by the noise at the corresponding diffusion timestep. We extend distribution matching diffusion distillation methods by proposing patch distribution matching loss, PDM, which minimizes the distance between spatial token distributions. Notably, PDM requires no additional models, making it computationally efficient and easy to integrate into existing distillation pipelines. Interestingly, PDM can also serve as standalone distillation objective, offering simple yet effective baseline. We apply SWD to state-of-the-art text-to-image diffusion models and show that SWD surpasses full-resolution distilled models under similar computational budget. Compared to state-of-the-art text-to-image diffusion and nextscale prediction models, SWD competes or even outperforms them while being 2.510 faster. 2. Related Work Diffusion distillation. Diffusion distillation methods [31, 4143, 54, 55, 60, 73, 74, 81] aim to reduce generation steps to 14 while maintaining teacher model performance. These methods can be largely grouped into two categories: teacher-following methods [25, 41, 43, 57, 60] and distribution matching [42, 54, 55, 73, 74, 80, 81]. Teacher-following methods approximate the teachers noise-to-data mapping by integrating the diffusion ODE in fewer steps than numerical solvers [39, 56]. They pioneered the diffusion distillation direction due to their simplicity but struggle to achieve the teacher performance in isolation."
        },
        {
            "title": "Distribution matching methods",
            "content": "relax the teacherfollowing constraint, focusing instead on aligning student and teacher distributions without requiring exact noiseto-data mapping. State-of-the-art approaches, such as DMD2 [73] and ADD [54, 55], demonstrate strong generative performance in 4 steps, sometimes surpassing the teacher in image fidelity thanks to adversarial training on external data. However, they still exhibit noticeable quality degradation at 12 step generation, leaving room for further improvement. In this work, we integrate SWD with DMD2 [73], leading method achieving state-of-the-art performance on largescale text-to-image DMs, such SDXL [46]. DMD optimizes the reverse KL-divergence between real and fake distributions using their score functions, with the teacher DM providing the real score and separate model trained for the fake score. Note that SWD is also compatible with other Figure 1. Spectral analysis of SDXL (Left) and SD3.5 (Right) VAE latents (128128) for different diffusion timesteps. Vertical lines mark frequency boundaries for lower resolutions; frequencies to the right are not present at lower scale latents. Noise masks high frequencies, suggesting that latent DMs can operate at lower latent resolutions for high noise levels. distribution matching approaches like ADD [54, 55]. Scale-wise generative models. The concept of progressively increasing resolution during generation was initially explored in VAEs [7, 40, 64, 78] and GANs [8, 26, 27, 77], driven by the intuition that stepwise learning simplifies the mapping from noise latents to high-resolution images. This idea has since been adopted in hierarchical or cascaded DMs [1, 17, 23, 30, 45, 48, 53], which are strong competitors to latent DMs [51] for high-resolution generation. Cascaded DMs consist of multiple DMs operating at different resolutions, where sampling is run in stages. At each stage, DM conditions on the previous lowerresolution sample and performs full diffusion sampling at higher resolution, typically with fewer steps. In contrast, SWD performs upsampling over the diffusion process with single model and using single step at each scale. Beyond diffusion models, next-scale prediction modeling [18, 62, 63, 68] has demonstrated significant potential, with scale-wise generators increasingly rivaling stateof-the-art DMs in generation quality. However, despite these models rely on discrete tokeniztheir efficiency, ers [11, 18, 65], which remain inferior to continuous VAEs in accuracy. Recent work [62] has addressed this by integrating DMs into next-scale prediction models to reduce dequantization errors in discrete latents before VAE decoding. In this context, SWD emerges as promising alternative, bypassing discrete modeling entirely by operating naturally in continuous space through diffusion process. Dynamic diffusion inference. Another line of research [5, 61, 69, 75, 79] focuses on reducing the number of tokens during generation with DMs. In general, these approaches employ trainable routers to dynamically select the most relevant tokens per layer and timestep, skipping less relevant ones for improved efficiency. In contrast, our method avoids additional trainable parameters or extra calculations for scale-wise generation. 2 3. Latent space spectral analysis This section provides the key intuition for why diffusion processes at high noise levels can be modeled in lower resolution spaces for text-conditional latent diffusion models. Rissanen et al. [50] and Dieleman [9] showed that diffusion models approximate spectral autoregression in the pixel space for natural images. Since current state-of-theart text-to-image diffusion models [4, 12, 46] are based on latent models [51], we first extend this analysis for various latent spaces and diffusion processes. Specifically, we analyze SDXL and SD3.5 models, encoding 310241024 images into C128128 latents, where C=4 for SDXL and C=16 for SD3.5. SDXL uses variance-preserving (VP) process [22, 59], while SD3.5 employs flow matching (FM) [38]. Figure 1 illustrates the radially averaged power spectral density (RAPSD) of Gaussian noise (blue), clean latents (purple) and noisy latents (orange) corrupted with different noising processes. Vertical lines indicate frequency boundaries for lower resolution latents (1616, 3232 and 9696). Frequencies to the right of each line are absent at the corresponding scale, while those to the left align with the full latent resolution (128128). Observations. First, we note that the latent frequency spectrum approximately follows power law, similar to natural images [66]. However, in contrast, high frequencies in latent space have greater magnitude, particularly evident in SDXL VAE latents. We attribute this to the KL term in VAEs, which enforces Gaussian latent distribution, causing clean latents to appear slightly noisy. Additionally, we observe that noise filters out high frequencies during the noising process, determining the safe downsampling range without information loss. For instance, Figure 1 (Left) shows that at t=750, noise masks high frequencies in latents above 3232 resolution, allowing safe 4 downsampling of 128128 latents (green area). On the other hand, 8 downsampling would impact the data signal (red area), as noise does not fully suppress it. Practical implication. Based on this analysis, we suppose that latent diffusion models may operate at lower resolution spaces at high noise levels without losing the data signal. In other words, modeling high frequencies at timestep is unnecessary if those frequencies are already masked at that noise level. We summarize this conclusion as follows: Diffusion process allows lower-resolution modeling at high noise levels. 4. Method This section introduces scale-wise distillation framework for text-conditional DMs. We begin by proposing scaleFigure 2. SWD training step. i) Sample training images and the pair of scales [si, si+1] from the scale schedule. ii) The images are downscaled to the si and si+1 scales. iii) The lower resolution version is upscaled and noised according to the forward diffusion process at the timestep ti. iv) Given the noised images, the model predicts clean images at the target scale si+1. v) Distribution matching loss is calculated between predicted and target images. Figure 3. SWD sampling. Starting from noise at the low scale s1, the model gradually increases resolution via multistep stochastic sampling. At each step, the previous prediction at the scale si1 is upscaled and noised according to the timestep schedule, ti. Then, the generator predicts clean image at the current scale si. wise distillation pipeline, highlighting its key features and challenges. Next, we examine specific instance of distribution matching-based distillation: DMD2, adapting it for scale-wise generation. Finally, we present our novel patch distribution matching objective. 4.1. Scale-wise Distillation of DMs Although SWD can be adapted to arbitrary DM architectures, we primarily focus on its application to latentand transformer-based diffusion models, particularly variants of the DiT architecture [44], which are widely used in stateof-the-art text-conditional models [4, 11, 47]. key characteristic of DiT-based models is their reliance on attention layers [67], which scale quadratically with spatial resolution. Additionally, these models maintain constant number of tokens across layers without downscaling, unlike the UNet-based DMs [46, 51]. These factors underscore the particular significance of SWD for such architectures. 3 Table 1. Comparison of noisy latent upscaling strategies (B, C) for 64 128 in terms of generation quality (FID-5K) against the real noisy latents (A). Upscaling xdown before noise injection (B) aligns better with full-resolution noisy latents. Configuration noise xt x0 xdown 0 xdown 0 upscale x0 noise xdown noise xt upscale xt = = 600 = 800 9.2 32.4 9.8 17. 12.9 13.0 129.7 235.0 340.2 Most distilled few-step generators operate as follows. First, they define time schedule, [t1, . . . , tN ], and train the model to predict real latent x0 at each timestep in the schedule. During inference, the model generates an initial prediction ˆx0 from noise. To transition to the next timestep, real noise is added to ˆx0 according to the forward process, and the model then predicts new ˆx0 from the less noisy latent ˆxti. This iterative procedure, known as stochastic multistep sampling [54, 60], typically achieves top performance with around 4 neural network evaluations in practice. In our approach, we additionally introduce scale schedule, [s1, . . . , sN ], that assigns latent resolution si to the corresponding diffusion timestep ti at which the model prediction is performed. During inference, we progressively increase the resolution, starting with standard Gaussian noise at the lowest scale, s1. The proposed sampling procedure is illustrated in Figure 3. Upsampling. Despite the apparent simplicity, this modification introduces significant practical challenges. Specifically, the key question is when and how to perform upsampling. naive approach would upscale the noisy latents, ˆxti, to transition to the next scale. However, we find it crucial to upscale ˆx0 and then apply noise to it rather than directly upscaling ˆxti. Noise injection mitigates the out-ofdistribution (OOD) nature of upscaled predictions, ensuring better alignment with the distribution of real noisy latents. Notably, noise injection works effectively even with simple bicubic interpolation. Thus, the stochastic multistep sampling appears highly suitable for scale-wise generation. To validate this intuition, we conduct simple experiment where we generate samples using pretrained diffusion model starting from different noisy latents. Specifically, we take full-resolution (128128) real image latent, x0, and its downscaled version (6464), xdown , and consider three settings. (A) we inject noise to the x0; (B) we first upscale xdown and then inject noise; (C) we first inject 0 noise to xdown and then upscale. The results are presented in Table 1. We can see that upscaling of x0 (B) largely outperforms another strategy (C). Moreover, for higher noise levels, e.g., = 800, upscaling artifacts are almost absent. Time schedule shifting. An important modification tailored to scale-wise generation involves shifting the time 0 0 schedule. Building on the intuition that noise injection helps to reduce upscaling artifacts, we further amplify this effect by shifting the timesteps to higher values. Training. However, noise injection alone is not sufficient to completely mitigate upscaling artifacts. Thus, we aim to train few-step generator that also serves as robust upscaler. So, key distinctive feature of our approach is: Scale-wise distilled models serve dual purposes: few-step generators and image upscalers. We train single model across multiple scales, iterating over scale pairs [si, si+1] from the scale schedule at each training step. Below, we describe training step in more detail and provide its schematic illustration in Figure 2. Recall that we deal with latent DMs in practice and hence train the generator in the VAE latent space. First, we sample batch of full-resolution training images with the corresponding prompts from the dataset, downscale them to the previous and target resolutions in pixel space, according to the si and si+1 scales, and then encode them into the VAE latent space. Notably, we find that downscaling in pixel space before the VAE encoding largely outperforms latent downscaling in our experiments. Next, we upscale the latents from si to si+1 using bicubic interpolation and add noise based on the time schedule, ti. The resulting noised latents are then fed into the scalewise generator, which predicts ˆx0 at the target scale si+1. Finally, we employ different distribution matching losses (e.g., adversarial [54] and reverse KL-divergence [74]) between the predicted and target latents at si+1. Training on synthetic data. We also emphasize the importance of training on synthetic data rather than real data, following LADD [55]. Prior to distillation, we prepare dataset of synthetic samples generated by the teacher model. We note that this step does not pose bottleneck for training, as the distillation process itself converges relatively fast (3 5K iterations) and requires significantly less data than the DM training. 4.2. Scale-wise DMD2 In our work, we integrate SWD into DMD2 [73], achieving state-of-the-art performance in diffusion distillation. Additionally, we adapt it for transformer-based text-to-image DMs, whereas the original implementation is designed for SDXL [46], which utilizes UNet architecture [52]. Specifically, the generator consists of the pretrained DM with trainable LoRA adapters [24]. The model is trained to minimize the reverse KL-divergence using the scores of the real and fake probability distributions. The real score is modeled using the pretrained DM, while the fake one is modeled by training separate fake DM on the generated 4 samples during distillation. The fake model is parameterized with separate LoRA adapters added to the teacher DM. In addition to the reverse KL, we include GAN component. The discriminator is small MLP [19], which operates on the intermediate features extracted from the middle transformer block of the fake DM. The LoRA adapters of the fake DM are also updated using the discriminator loss, following [73]. More details can be found in Appendix A. We distill the teacher into 46 step scale-wise models and perform stochastic multistep sampling based on the selected time and scale schedules at inference. We explore various schedule setups in Section 5. 4.3. Patch Distribution Matching In addition to adapting DMD2, we present novel technique called patch distribution matching (PDM), which minimizes Maximum Mean Discrepancy (MMD) [16, 36]: MMD2(P, Q) = ExP [ψ(x)] EyQ[ψ(y)]2 , (1) where ψ() is kernel function. While prior work has used MMD for GAN training [3, 70] or standalone generative models [36], SWD applies MMD to measure distances between patch distributions rather than entire images. Furthermore, we calculate the MMD on the intermediate features of the pretrained DM, leveraging their expressive power. Notably, PDM does not require training additional models, such as discriminator or fake DM. In more detail, we extract feature maps FRN LC from the middle transformer block of the teacher DM for generated and target images and denote them as Ffake and Freal, respectively. is batch size, is number of spatial tokens, and is hidden dimension of the transformer. Note that spatial tokens serve as patch representations in visual transformers [10]. For MMD computation, we consider two kernels: linear (ψ(x) = x) and radial basis function (RBF) [6]. The former aligns means of the patch distributions, while the latter also matches higher-order moments. In our experiments, both kernels perform similarly, so we simplify LPDM using the linear kernel. Specifically, we calculate the expectation as an average over spatial tokens per image: LPDM = (cid:88) n= (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) l="
        },
        {
            "title": "Freal",
            "content": "n,l,"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) l=1 Ffake n,l, (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) . (2) We use LPDM as an additional loss in the scale-wise DMD2. Interestingly, we observe that LPDM can be competitive standalone distillation loss despite its simplicity. Overall objective. Our distillation loss combines three key components: reverse KL-divergence, GAN loss, and patch distribution matching loss (LPDM). Figure 4. SD3.5 generates cropped images at low-resolutions (256256), while SDXL does not produce meaningful images at all. SWD is able to perform successful distillation for such cases and corrects these limitations. Table 2. Quantitative comparison between scale-wise and fullscale setups in terms of automated metrics within SD3.5 medium. Configuration # steps PS CS IR FID Scale-wise Scale-wise Full-scale Full-scale Full-scale Scale-wise Scale-wise Full-scale Full-scale Full-scale COCO 2014 6 4 6 4 6 4 6 4 2 0.230 0.230 0.228 0.228 0."
        },
        {
            "title": "MJHQ",
            "content": "0.221 0.220 0.218 0.219 0.217 0.355 0.356 0.353 0.356 0.357 0.363 0.363 0.356 0.361 0.363 1.11 1.12 1.08 1.09 1.03 1.11 1.13 1.09 1.10 1.04 23.0 23.3 20.4 21.2 20. 13.6 13.7 13.4 13.5 13.1 5. Experiments We validate our approach by distilling SD3.5 Medium, SD3.5 Large [12]. The models are distilled for 5K iterations using 500K synthetic samples generated by their respective teacher models on 8 A100 GPUs. The synthetic data generation settings for each model are in Appendix A. Additionally, we apply the full-resolution version of our approach to these models to fairly validate our scale-wise modifications. Configuration. We evaluate the distilled models for 2, 4, and 6 generation steps. The scale schedule begins at an image resolution of 256256 and progresses to 10241024. We chose 256256 as the starting point because lower resolutions provide only marginal speed improvements. Metrics. We assess the performance using automated metrics: FID [21], CLIP score [20], ImageReward (IR) [72], and PickScore (PS) [33] and GenEval [14]. For testing, we use 30K text prompts from the COCO2014 and MJHQ sets [34, 37]. Additionally, we conduct user preference 5 Table 3. Comparison of generation and training times between scale-wise and full-scale configurations within SD3.5 Medium and Large. The measurement setup is described in Appendix D. SD3.5 Medium SD3.5 Large Model Steps Latency, s/image Train time, s/iter Latency, s/image Train time, s/iter Scale-wise Scale-wise Full-scale Full-scale Full-scale Teacher 6 4 6 4 2 28 0.17 0.14 0.38 0.26 0.15 1.61 19 21 41 41 41 0.41 0.32 0.92 0.63 0.33 4. 17 18 27 27 27 Figure 5. Side-by-side comparison between scale-wise and fullscale settings. The numbers indicate the sampling steps. Figure 6. Few examples of image generations for scale-wise and full-scale approaches. SWD outperforms the 2-step configuration. study through side-by-side comparisons conducted by professional assessors. We select 128 text prompts from the PartiPrompts dataset [76], following [46, 55, 73], and generate 2 images per prompt. More details are in Appendix G. Baselines. Besides the full resolution versions of our approach, we also evaluate already existing fast text-to-image generative models. We consider the state-of-the-art distilled models (DMD2-SDXL [73], SDXL-Turbo [54]) and nextscale prediction models (Switti [68] and Infinity [18]). 5.1. Importance of scale-adapted teacher model Before conducting the main experiments, we address an important question: Does the teacher model need to be capable of generating images at low scales prior to scale-wise distillation? The teacher model may not inherently handle low scales effectively, making the scale-wise distillation more challenging task compared to the full-scale distillation. If this is the case, additional pretraining of the teacher on small scales might be required, which could compromise the efficiency of the proposed approach. To address this question, we evaluate the ability of SD3.5 Medium and SDXL to generate images at lower scales (256256). The results are presented in Figure 4. We find out that SD3.5 produces cropped and simplified images, but the overall quality remains acceptable due to its pretraining on 256256 resolution. SWD effectively distills this model, correcting the cropped images and enhancing their complexity. However, more crucially, SDXL is unable to generate plausible images at 256256 resolution. Interestingly, SWD can still produce plausible generator even with such poor starting point for distillation. 5.2. Scale-wise versus full-scale Next, we compare SWD with its full-scale counterpart in terms of quality and speed. Here, we focus on two settings: acceleration under the same number of generation steps and quality improvement under equivalent time constraints. To this end, we consider the following configurations: 6 steps time schedule: [1000, 945, 895, 790, 737, 602] scale schedule: [256, 384, 512, 640, 768, 1024] 4 steps; time schedule: [1000, 895, 737, 602] scale schedule: [256, 512, 768, 1024] We note that our configurations utilize the full scale only once, at the final timestep. For the full-scale version, we use the same time schedules but with the fixed resolution 1024. In addition, we consider 2-step setting for this version. Table 2 provides sample quality across various configurations using standard automated metrics. Additionally, Table 3 reports the time required to generate single image (latency), including VAE decoding and text encoding, as well as the average time for single training iteration. 6 Table 4. Quantitative comparison of SWD against other leading open-source models. Bold denotes the best performing configuration, while underline the 2nd one. gray highlights the results for SD3.5 Large, which stands out due to its model size. Model Latency, s/image Parameters count, PickScore CLIP IR FID PickScore CLIP IR FID GenEval SD3.5/ [12] SD3.5/ [12] SDXL [46] SDXL-Turbo [54] SD3.5-Turbo / [55] DMD2-SDXL [73] Switti [68] Infinity [18] DMD2+PDM/ DMD2+PDM/ SWD / SWD / 1.61 4.42 1.72 0.20 0.63 0.20 0.44 0.80 0.38 0.92 0.17 0.41 2.0 8.0 2.6 2.6 8.0 2.6 2.5 2.0 2.0 8. 2.0 8.0 COCO 30K MJHQ 30K 0.227 0.230 0.226 0.229 0.230 0.231 0.227 0.229 0.228 0.233 0.230 0.232 0.359 0.357 0.360 0.355 0.356 0.356 0.351 0.347 0.353 0. 0.355 0.353 1.00 1.06 0.77 0.83 0.94 0.87 0.91 0.96 1.08 1.17 1.11 1.17 16.4 16.3 14.4 17.6 22.5 14.3 18.7 28.9 20.4 23.8 23.0 22.9 0.219 0.221 0.217 0.216 0.219 0.219 0.217 0.217 0.218 0. 0.221 0.223 0.367 0.368 0.384 0.365 0.355 0.374 0.375 0.351 0.356 0.364 0.363 0.363 0.97 1.04 0.78 0.84 0.89 0.87 0.76 0.99 1.09 1.21 1.11 1.21 9.6 10.8 7.6 15.7 13.4 7.2 8.3 11.8 13.4 13. 13.6 14.3 0.69 0.70 0.55 0.55 0.71 0.58 0.61 0.69 0.69 0.69 0.70 0.71 Figure 7. Side-by-side comparisons of SWD and baseline models. Table 5. Ablation studies conducted on 30K MJHQ prompts. Configuration PS CS IR FID Scale-wise ablation SWD (basic configuration) SWD w/o time shift SWD constant scale schedule SWD w/o scale-wise training SWD using real data 0.221 0.213 0.219 0.216 0.208 0.363 0.363 0.357 0.354 0.321 1.11 0.89 1.09 0.93 0.55 13.6 21.5 14.7 17.2 28.7 PDM loss ablation SWD w/o PDM SWD PDM only SWD with RBF PDM 0.214 0.215 0.221 0.352 0.356 0.361 0.98 0.95 1.09 19.3 20.6 13.8 First, we compare quality and generation time for the same number of steps (4 vs 4, 6 vs 6). We do not observe noticeable quality degradation according to the metrics. Human evaluation (Figure 5) reveals slight degradations in image complexity and image aesthetics, though these are not significant. To qualitatively confirm this, we present some image examples in Figure 6 and Figure 11. In terms of computational efficiency, our approach achieves 2 speedup in both generation and training compared to the full-scale counterpart. Second, we align the generation times of the scale-wise and full-scale configurations to evaluate quality improvements (4 vs 2 and 6 vs 2). While automated metrics show minimal improvements in PS and IR, the human evaluation identifies significant gap in terms of defects and image complexity. To support this observation, Figure 6 provides example images illustrating the high defect rates in the fullscale configuration at 2 steps. 5.3. Comparison with state-of-the-art baselines We evaluate SWD against state-of-the-art baselines in terms of generation quality and speed. We conduct the main comparisons against the models of similar sizes. Table 4 and Figure 7 show the performance of our method compared to leading generative models. We find that SWD achieves the best performance in terms of PS, IR and GenEval. According to the human study, SWD demonstrates highly promising results. Specifically, SWD outperforms all baselines in image complexity and achieves surpasses most baselines in image aesthetics and text relevance. Only slight degradation in defects is observed when compared to DMD2-SDXL. Qualitative comparisons are presented in Figure 8, with additional examples provided in Appendix E. 5.4. Ablation studies In this section, we compare different design choices with our basic 6-step configuration presented in Section 5.2. Specifically, we investigate the impact of the time schedule 7 Figure 8. Qualitative comparison of SWD against the state-of-the-art baselines. shifting, PDM loss, scale schedule, and scale-wise training on overall performance. The results are in Table 5. First, we ablate different design choices for the scaleSpecifically, configuration demonwise framework. strates the influence of the time shifting, revealing notable degradation in PS, IR and FID compared to the basic setting. In B, we employ constant scale of 256 for all steps except the final one, where we use 1024. We notice degradation across all metrics. In C, we consider scale-wise generation but use model distilled in the full-scale regime to validate the importance of the scale-wise training. The results confirm the importance of the proposed training procedure. Finally, in D, we evaluate SWD using curated internal real data and observe significant decline in performance across all metrics compared to the training on the teacher synthetic data. Finally, we ablate the PDM loss. shows that removing PDM results in degradation across all metrics. Furthermore, confirms that PDM can function effectively as an independent distillation method. shows that using the RBF kernel does not yield noticeable quality boost. 6. Discussion This paper introduces scale-wise distillation (SWD) for diffusion models, novel approach motivated by the observation that DMs can perform safely in lower-resolution spaces at large diffusion timesteps. Potential future research could investigate adaptive or dynamic scale scheduling, where the resolution progression is optimized based on the complexity of the input or the target output. Moreover, SwD approach could be extended to video generation, where temporal coherence and resolution scaling are critical."
        },
        {
            "title": "References",
            "content": "[1] Yuval Atzmon, Maciej Bala, Yogesh Balaji, Tiffany Cai, Yin Cui, Jiaojiao Fan, Yunhao Ge, Siddharth Gururani, Jacob Huffman, Ronald Isaac, et al. Edify image: High-quality image generation with pixel space laplacian diffusion models. arXiv preprint arXiv:2411.07126, 2024. 2 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1 [3] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 5 [4] Black Forest Labs. Flux.1. https://huggingface. co/black-forest-labs/FLUX.1-dev, 2024. 1, 3 [5] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45994603, 2023. 2 [6] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing lowdegree polynomial data mappings via linear svm. Journal of Machine Learning Research, 11(4), 2010. 5 [7] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020. [8] Emily Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using laplacian pyramid of adversarial networks. Advances in neural information processing systems, 28, 2015. 2 [9] Sander Dieleman. Diffusion is spectral autoregression, 2024. 1, 3 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 5 [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2020. 2, 3 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. CoRR, abs/2403.03206, 2024. 1, 3, 5, 7 [13] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems, 2024. [14] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment, 2023. 5 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1 [16] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723 773, 2012. 5 [17] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2 [18] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis, 2024. 1, 2, 6, 7 [19] Simon Haykin. Neural networks: comprehensive foundation. Prentice Hall PTR, 1994. 5 [20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 5 [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 5 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 3 [23] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. arXiv preprint arXiv:2106.15282, 2021. 2 [24] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [25] Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge diffusion for distillation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [26] Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 50775086, 2017. 2 [27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. 2 [28] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. 1 [29] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proc. CVPR, 2024. [30] Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, and Valentin Khrulkov. Yaart: Yet another art rendering technology, 2024. 1, 2 [31] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of onestep generator from low-resolution diffusion teacher, 2024. 1, 2 [32] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 1 [33] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. 2023. 5 [34] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. [35] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedings Q-diffusion: Quantizing diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 1753517545, 2023. 1 [36] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International conference on machine learning, pages 17181727. PMLR, 2015. 5 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. 5 [38] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 3 [39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 1, 2 [40] Eric Luhman and Troy Luhman. image vaes for sample quality. chical arXiv:2210.10205, 2022. 2 Optimizing hierararXiv preprint [41] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference, 2023. 1, 2 [42] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [43] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. 10 On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 2 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 1, 3 [45] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen: An efficient architecture for large-scale text-to-image diffusion models, 2023. 2 [46] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 1, 2, 3, 4, 6, 7 [47] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. 1, [48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. 2 [49] Recraft AI. Recraft - ai. https://www.recraft.ai/ about, Accessed 2024. 1 [50] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In International Conference on Learning Representations (ICLR), 2023. 1, 3 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 1, 2, 3 [52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 4 [53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. 1, [54] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023. 1, 2, 4, 6, 7 [55] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation, 2024. 1, 2, 4, 6, 7 [56] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1, 2 [57] Yang Song and Prafulla Dhariwal. Improved techniques for In The Twelfth International training consistency models. Conference on Learning Representations, 2024. 1, 2 [58] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [59] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [60] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 1, 2, 4 [61] Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, and Nan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. arXiv preprint arXiv:2410.02098, 2024. 2 [62] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint, 2024. 1, 2 [63] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. 1, [64] Arash Vahdat and Jan Kautz. Nvae: deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:1966719679, 2020. 2 [65] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 2 [66] van der Arjen Schaaf and van Johannes Hateren. Modelling the power spectra of natural images: Statistics and information. Vision Research, 36:27592770, 1996. 3 [67] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [68] Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, and Dmitry Baranchuk. Switti: Designing scale-wise transformers for text-to-image synthesis. arXiv preprint arXiv:2412.01819, 2024. 1, 2, 6, 11 [69] Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, and Xiaodan Liang. Qihoo-t2x: An efficient proxy-tokenized diffusion transformer for text-to-any-task. arXiv preprint arXiv:2409.04005, 2024. 2 [70] Wei Wang, Yuan Sun, and Saman Halgamuge. Improving mmd-gan training with repulsive loss function. arXiv preprint arXiv:1812.09916, 2018. 5 [71] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 1 [72] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [73] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 1, 2, 4, 5, 6, 7 [74] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 1, 2, 4 [75] Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, et al. Layer-and timestep-adaptive differentiable token compression ratios for efficient diffusion transformers. arXiv preprint arXiv:2412.16822, 2024. 2 [76] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 6 [77] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 5907 5915, 2017. 2 [78] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from generative models. arXiv preprint arXiv:1702.08396, 2017. 2 [79] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and Yang You. Dynamic arXiv preprint arXiv:2410.03456, diffusion transformer. 2024. [80] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. ArXiv 2406.01561, 2024. 2 [81] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024. 2 Scale-wise Distillation of Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "(a) Scale-wise against full-scale. Figure 10. Visual examples for the SD3.5 Large models. ferent set of LoRA adapters that are added to the teacher model. In the reverse KL-divergence, we set the guidance for the real score to 4.5 and 0.0 for the fake one. To train the discriminator, we use 4-layer MLP head including layer normalization, GELU and linear layer. The MLP head processes features extracted from the 11-th layer of the fake diffusion model for SD3.5 Medium. We use synthetic data for the distillation process. Specifically, the samples are generated using the standard teacher configuration. For SD3.5 Medium, we employ 40 sampling steps with guidance scale of 4.5. For SD3.5 Large, we employ 28 sampling steps with guidance scale of 4.5. B. SD3.5 Large Following the same settings as for SD3.5 medium, we distill SD3.5 large. Here we present human evaluation and qualitative examples (Figure 9 and 10). According to the human evaluation, our approach outperforms the teacher and 2-step full-scale configuration. (b) SWD against its teacher (SD3.5 Large) and SD3.5 Large Turbo. Figure 9. Side-by-Side comparisons for SD3.5 Large. The numbers indicate the sampling steps. A. Implementation details We employ DMD2, which consists of three components: student model, fake diffusion model, and discriminator. The student model is trained on top of pretrained diffusion model using LoRA adapters. These adapters are integrated into both the attention and MLP layers, with rank of 64. The models are trained with learning rate of 4e6 and batch size of 80 for 35K iterations on single node with 8 A100 GPUs. For the fake diffusion model, we train dif12 alongside textual prompt and asked to choose the preferred one. For each pair, three independent responses are collected, and the final decision is determined through majority voting. The human evaluation is carried out by professional assessors who are formally hired, compensated with competitive salaries, and fully informed about potential risks. Each assessor undergoes detailed training and testing, including fine-grained instructions for every evaluation aspect, before participating in the main tasks. In our human preference study, we compare the models across four key criteria: relevance to the textual prompt, presence of defects, image aesthetics, and image complexity. Figures 13, 16, 14, 15 illustrate the interface used for each criterion. Note that the images displayed in the figures are randomly selected for demonstration purposes. Table 6. Comparison to the naive patch loss."
        },
        {
            "title": "Configuration",
            "content": "PS CS IR FID"
        },
        {
            "title": "PDM Linear\nNaive patch loss",
            "content": "0.224 0.209 0.348 1.02 0.306 0.51 21.7 52.4 Table 7. Ablation studies conducted on 30K COCO prompts. Configuration PS CS IR FID Scale-wise ablation SWD (basic configuration) SWD w/o time shift SWD constant scale schedule SWD w/o scale-wise training SWD using real data 0.230 0.223 0.229 0.227 0. 0.355 0.355 0.354 0.350 0.326 1.11 0.96 1.09 0.98 0.75 23.0 21.2 24.1 21.5 23.2 PDM loss ablation SWD w/o PDM SWD PDM only SWD with RBF PDM 0.226 0.224 0. 0.344 0.348 0.354 0.99 1.02 1.11 27.1 21.7 21.3 C. Naive patch loss Our PDM loss minimizes the distance between spatial token distributions. Additionally, we evaluate naive variant that minimizes element-wise distances:"
        },
        {
            "title": "LNaive",
            "content": "patch = (cid:88) n=1 (cid:13) (cid:13)Freal n,, Ffake n,, (cid:13) 2 (cid:13) . (3) Table 6 shows that it is crucial to align the token distributions rather than the token-to-token mapping. D. Runtime measurement setup In our experiments, we measure runtimes in half-precision (FP16), using torch.compile for all models: VAE decoders, text encoders, and generators. The measurements are conducted in an isolated environment on single A100 GPU. We use batch size of 8 for all runs, and each measurement is averaged over 100 independent runs. The latency is then obtained by dividing the average runtime by the batch size. E. Additional qualitative comparisons We present more qualitative comparisons between scalewise and full-scale configurations in Figure 11. We also present more results against the baselines in Figure 12. F. Ablation studies In Table 7, we conduct ablation studies on COCO prompts. The results validate all techniques introduced in the paper. G. Human evaluation The evaluation is conducted using Side-by-Side (SbS) comparisons, where assessors are presented with two images Figure 11. Qualitative examples of image generations for scale-wise and full scale approaches for different generation steps. 14 Figure 12. Qualitative comparisons against the baselines. 15 Figure 13. Human evaluation interface for aesthetics. Figure 15. Human evaluation interface for relevance. Figure 16. Human evaluation interface for complexity. Figure 14. Human evaluation interface for defects."
        }
    ],
    "affiliations": [
        "Yandex Research"
    ]
}