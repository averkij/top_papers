{
    "paper_title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "authors": [
        "Gabriele Sarti",
        "Vilém Zouhar",
        "Grzegorz Chrupała",
        "Ana Guerberof-Arenas",
        "Malvina Nissim",
        "Arianna Bisazza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows."
        },
        {
            "title": "Start",
            "content": "QE4PE: Word-level Quality Estimation for Human Post-Editing Gabriele Sarti1 Vilém Zouhar2 Grzegorz Chrupała3 Ana Guerberof-Arenas1 Malvina Nissim1 Arianna Bisazza1 1CLCG, University of Groningen 2ETH Zürich 3CSAI, Tilburg University {g.sarti, a.bisazza}@rug.nl"
        },
        {
            "title": "Abstract",
            "content": "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human postediting. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in realistic setting involving 42 professional post-editors across two translation directions. We compare four errorspan highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by wordand segment-level human annotation. We find that domain, language and editors speed are critical factors in determining highlights effectiveness, with modest differences between human-made and automated QE highlights underlining gap between accuracy and usability in professional workflows."
        },
        {
            "title": "Introduction",
            "content": "Recent years saw steady increase in the quality of machine translation (MT) systems and their widespread adoption in professional translation workflows (Kocmi et al., 2024a). Still, human post-editing of MT outputs remains fundamental step to ensure high-quality translations, particularly for challenging textual domains requiring native fluency and specialized terminology (Liu et al., 2024). Quality estimation (QE) techniques were introduced to reduce post-editing effort by automatically identifying problematic MT outputs without the need for human-written reference translations and were quickly integrated into industry platforms (Tamchyna, 2021). Segment-level QE models correlate well with human perception of quality (Freitag et al., 2024) and exceed the performance of reference-based metrics in specific settings (Rei et al., 2021; Amrhein et al., 2022, 2023). On the other hand, word-level QE methods for identifying error spans requiring revision have received less attention in the past due to their modest agreement with human annotations, despite their promise for more granular and interpretable quality assessment in line with modern MT practices (Zerva et al., 2024). In particular, while the accuracy of these approaches is regularly assessed in evaluation campaigns, research has rarely focused on assessing the impact of such techniques in realistic post-editing workflows, with notable exceptions suggesting limited benefits (Shenoy et al., 2021; Eo et al., 2022). This hinders current QE evaluation practices: by foregoing experimental evaluation with human editors, it is implicitly assumed that word-level QE will become helpful once sufficient accuracy is reached, without accounting for the additional challenges towards successful integration of these methods in post-editing workflows. In this study dubbed QE4PE (Quality Estimation for Post Editing), we address this gap by conducting large-scale study with 42 professional translators for the EnglishItalian and EnglishDutch directions to measure the impact of word-level QE on editing quality, productivity and usability. We aim for realistic and reproducible setup, employing the high-quality open-source NLLB 3.3B MT model (NLLB Team et al., 2024) to translate challenging documents from biomedical and social media domains. We then conduct controlled evaluation of post-editing with error spans using four highlight modalities: supervised state-of-theart QE model trained on human error annotations (XCOMET, Guerreiro et al., 2024a), an unsupervised method leveraging the uncertainty of the MT model during generation, oracle error spans ob5 2 0 2 ] . [ 1 4 4 0 3 0 . 3 0 5 2 : r error span highlighting techniques for other wordlevel QE methods and translation directions."
        },
        {
            "title": "2 Related Work",
            "content": "MT Post-editing Human post-editing of MT outputs is increasingly common in professional translator workflows, as it was shown to increase the productivity of translators while preserving translation quality across multiple domains (Liu et al., 2024). However, many factors were found to influence the variability of post-editing productivity across setups, including MT quality (Zouhar et al., 2021b), interface familiarity (Läubli et al., 2021), individual variability and source-target languages typological similarity (Sarti et al., 2022). Studies evaluating the post-editing process generally focus on productivity, i.e. number of processed words/characters per minute, and the temporal, technical and cognitive dimensions of post-editing effort, operationalized through behavioral metrics such as editing time, keystrokes and pauses (Krings, 2001; Sarti et al., 2022). We adopt these metrics for the QE4PE study and relate them to different highlight modalities. Quality estimation for MT The field of quality estimation was initially concerned with MT model uncertainty (Blatz et al., 2004; Specia et al., 2009), but in time began focusing on predicting translation quality even without using references (Turchi et al., 2013, 2014; Kepler et al., 2019; Thompson and Post, 2020 inter alia). Advances in segmentand word-level QE research are regularly assessed in annual WMT campaigns (Fomicheva et al., 2021; Zerva et al., 2022, 2024; Blain et al., 2023), where the best-performing QE systems are usually Transformer-based language models trained on human quality judgments, such as the popular COMET model suite (Rei et al., 2020, 2021, 2022). The widespread adoption of the fine-grained Multidimensional Quality Metrics scale (MQM, Lommel et al., 2014) prompted paradigm shift in MT evaluation, leading to new QE metrics predicting quality at various granularity levels (Kocmi and Federmann, 2023; Fernandes et al., 2023; Guerreiro et al., 2024a). Aside from supervised models, unsupervised methods exploiting model uncertainty and its internal mechanisms were proposed as efficient alternatives to identify potential error spans for MT (Fomicheva et al., 2020; Dale et al., 2023; Xu et al., 2023; Himmi et al., 2024, surveyed by Leiter et al. 1https://github.com/gsarti/qe4pe Figure 1: summary of the QE4PE study. Documents are translated by neural MT model and reviewed by professional editors across two translation directions and four highlight modalities. These include no highlight baseline and three settings using unsupervised, supervised, and oracle word-level QE methods. Editing effort, productivity and usability across modalities are estimated from editing logs and questionnaires. Finally, the quality of MT and edited outputs is assessed with MQM/ESA human annotations and automatic metrics. tained from the consensus of previous human posteditors, and no highlight baseline. The human post-editing is done in simple online interface built for this study, GROTE, supporting the realtime logging of granular editing data for quantitative assessment of editing effort and productivity across highlight modalities. We also survey professionals using an online questionnaire to collect qualitative feedback about the usability and quality of the MT model, as well as the interface and error span highlights. Finally, subset of the original MT outputs and their post-edited variants is annotated following the MQM and ESA protocols (Lommel et al., 2014; Kocmi et al., 2024b) to verify quality improvements after post-editing. See Figure 1 for an overview of the study. More generally, our work embraces recent calls for an evaluation of translation technologies that is centered on users experience (Guerberof-Arenas and Moorkens, 2023; Savoldi et al., 2025). We release all data, code and the GROTE editing interface to foster future studies on the usability of 2024) and other language generation tasks (Kuhn et al., 2023; Farquhar et al., 2024). In this work, we compare the downstream effectiveness of stateof-the-art supervised and unsupervised word-level QE metrics for post-editing settings. Human Assessment of Quality Estimation Automatic QE methods are widely used in the translation industry for triaging automatic translations (Tamchyna, 2021). While QE usage has been found helpful to increase the confidence and speed of human assessment (Mehandru et al., 2023; Zouhar et al., 2024b), an incautious usage of these techniques can lead to users being misled (Zouhar et al., 2021a). Interfaces supporting word-level error highlights were developed for studying MT post-editing (Coppers et al., 2018; Herbig et al., 2020) and code reviewing (Sun et al., 2022; Vasconcelos et al., 2024), with results suggesting that striking the right balance of user-provided information is fundamental to improve the editing experience and prevent cognitive overload. Most similar to our study, Shenoy et al. (2021) investigated the effect of synthetic word-level QE highlights for EnglishGerman post-editing on Wikipedia data, concluding that current word-level QE accuracy was still insufficient to produce tangible productivity benefits. In this work, we expand the scope of such evaluation by including two translation directions, two challenging real-world text domains and state-of-the-art MT and QE systems and methods."
        },
        {
            "title": "3.1 Structure of the Study",
            "content": "Our study is organized in five stages: 1) Oracle Post-editing As preliminary step, segments later used in the main assessment are post-edited by three professionals per direction using their preferred interface without logging. This allows us to obtain post-edits and produce oracle word-level spans based on the editing consensus of multiple human professionals. Translators involved in this stage are not involved further in the study. 2) Pretask (PRE) The pretask allows the core translators (12 per language direction, see Section 3.4) to familiarize themselves with the GROTE interface and text highlights. Before starting, all translators complete questionnaire to provide demographic and professional information about their In the pretask, all translators profile  (Table 7)  . work in an identical setup, post-editing small set of documents similar to those of the main task with supervised highlights. We rank the translators into three groups (faster, average, slower) based on their speed from editing logs. Individuals from each group are then assigned randomly to highlight modalities to ensure an equal representation of editing speeds in each highlight modality. This procedure is repeated independently for both translation directions to balance subjective variability in the main task. 3) Main Task (MAIN) This task, conducted in the two weeks following the pretask, covers the majority of the collected data and is the main object of study for the analyses of Section 4. In the main task, 24 core translators work on the same texts using the GROTE interface, but each with their assigned highlight modality (three translators per modality in each translation direction, as in Figure 1). After the main task, translators complete questionnaire on the quality and usability of the MT outputs, the interface and, where applicable, word highlights. 4) Post-Task (POST) After MAIN, core translators are asked to post-edit an additional small set of related documents with GROTE, but this time working all with the no highlight modality. This step lets us obtain baseline editing patterns for each translator to estimate individual speed and editing differences across highlight modalities without the confounder of interface proficiency of PRE. 5) Quality Assessment (QA) Finally, subset of 148 main task segments is selected randomly for manual annotation by six additional translators per direction (see Section 3.4). For each segment, the original MT output and all its post-edited versions are annotated with MQM error spans, including minor/major error severity and MQM error categories (Lommel et al., 2014). Moreover, the annotator proposes corrections for each error span, ultimately providing 0-100 quality score. We adopt this scoring system, which closely follows the ESA protocol (Kocmi et al., 2024b), following recent results showing its effectiveness and efficiency.2 In summary, for each translation direction we collect 3 full sets of oracle post-edits, 12 full sets of edits with behavioral logs for PRE, MAIN and POST task data, 12 preand post-task questionnaire responses, and 13 subsets of main task post-edits/MT 2See Figure 6 for an overview of setup and guidelines. outputs annotated with MQM error spans, corrections and segment-level ESA ratings."
        },
        {
            "title": "3.2 Highlight Modalities",
            "content": "We conduct our study on four highlight modalities across two severity levels (minor and major errors). Using multiple severity levels follows the current MT evaluation practices (Freitag et al., 2021, 2024), and previous results showing that users tend to prefer more informative word-level highlights (Shenoy et al., 2021; Vasconcelos et al., 2024). No Highlight The text is presented as-is, without any highlighted spans. This setting serves as baseline to estimate the default post-editing quality and productivity using our interface. Oracle Following the Oracle Post-editing phase, we produce oracle error spans from the editing consensus of human post-editors. We label text spans that were edited by two out of three translators as minor, and those edited by all three translators as major, following the intuition that more critical errors are more likely to be identified by several annotators, while minor changes will show more variance across subjects. This modality serves as best-case scenario, providing an upper bound for future improvements in word-level QE quality. Supervised In this setting, word-level error spans are obtained using XCOMET-XXL (Guerreiro et al., 2024a), which is multilingual Transformer encoder (Goyal et al., 2021) further trained for joint wordand sentence-level QE prediction. We select XCOMET-XXL in light of its broad adoption, open accessibility and state-of-the-art performance in QE across several translation directions (Zerva et al., 2024). For the severity levels, we use the default labels predicted by the model, ignoring confidence scores and mapping critical labels to the major level. Unsupervised In this modality, we exploit the access to the MT model producing the original translations to obtain uncertainty-based highlights. We assess the agreement of two unsupervised QE methods employing the MT model logprobabilities with human post-edits, following recent results showing these can act as significant predictors of human editing time, and hence of the presence of potential issues (Lim et al., 2024). Scores were collected for the EnglishItalian and EnglishDutch directions of QE4PE Oracle postedits and DivEMT (Sarti et al., 2022) to identify the best-performing method.3 Results show strong performance for the variance of nexttoken log-probabilities for 10 steps of Monte Carlo Dropout (Gal and Ghahramani, 2016), even surpassing the agreement of supervised metrics across both datasets. We ultimately select this method, which was previously shown to provide an efficient alternative to model ensembling for estimating epistemic uncertainty at the sentence level (Fomicheva et al., 2020)."
        },
        {
            "title": "3.3 Data and MT model",
            "content": "MT Model On the one hand, the MT model must achieve high translation quality in the selected languages to ensure our experimental setup applies to state-of-the-art proprietary systems. Still, the MT model should be open-source and have manageable size to ensure reproducible findings and enable the computation of uncertainty for the unsupervised setting. All considered, we use NLLB 3.3B (NLLB Team et al., 2024), widely-used MT model achieving industry-level performances across 200 languages (Moslem et al., 2023). Data selection We begin by selecting two translation directions, EnglishItalian and EnglishDutch. We intentionally focus on out-of-English translations as they are generally more challenging for modern MT models (Kocmi et al., 2023). We aim to identify documents that are manageable for professional translators without domain-specific expertise but still prove challenging for our MT model to ensure sufficient amount of error spans across modalities. We begin by translating 3672 multi-segment English documents from the WMT23 General and Biomedical MT shared tasks (Kocmi et al., 2023; Neves et al., 2023) and MT test suites to Dutch and Italian. Then, XCOMET-XXL is used to produce first set of segment-level QE scores and word-level error spans for all segments. To make the study tractable, we further narrow down the selection of documents according to several heuristics to ensure realistic editing experience and balanced occurrence of error spans (see Appendix for details and data statistics). This procedure yields 351 documents, from which we manually select subset of 64 documents (413 segments, 8744 source words 3Unsupervised evaluation results are shown in Table 13. Token-level highlights are aggregated at the word level to match the granularity of other modalities. per post-editor) across two domains: Social media posts, including Mastodon posts from the WMT23 General Task (Kocmi et al., 2023) EnglishGerman evaluation and Reddit comments from the Robustness Challenge Set for Machine Translation (RoCS-MT; Bawden and Sagot, 2023), displaying atypical language use, such as slang or acronymization. Biomedical abstracts extracted from PubMed from the WMT23 Biomedical Translation Task (Neves et al., 2023), including domainspecific terminology. While the presence of multiple domains in the same task can render our post-editing setup less realistic, we deem it essential to test the cross-domain validity of our findings. Critical Errors Before producing highlights, we manually introduce 13 critical errors in main task segments to assess post-editing thoroughness. Errors are produced, for example, by negating statements, inverting the polarity of adjectives, inverting numbers, and corrupting acronyms. We replicate the errors in both translation directions to enable direct comparison. Most of these errors were correctly identified across all three highlight modalities (examples in Table 4)."
        },
        {
            "title": "3.4 Participants",
            "content": "translation For both directions, professional providers Translated and Global Textware recruited three translators for the Oracle post-editing stage, 12 for PRE, MAIN and POST tasks, and finally six for the QA stage. All translators were freelancers with native proficiency in their target language and self-assessed proficiency of at least C1 in English. Almost all translators had more than two years of professional translation experience and regularly post-edited MT outputs (details in Table 7)."
        },
        {
            "title": "3.5 Editing Interface",
            "content": "We develop custom interface, which we name Groningen Translation Environment (GROTE, Figure 2), to support editing over texts with word-level highlights. While the MMPE tool used by Shenoy et al. (2021) provide extensive multimodal functionalities (Herbig et al., 2020), we aim for barebones setup to avoid confounders in the evaluation. GROTE is web interface based on Gradio (Abid et al., 2019) and hosted on the Hugging Face Spaces to enable multi-user data collection Figure 2: An example of the QE4PE GROTE setup for two segments in an EnglishItalian document. online. Upon loading document, source texts and MT outputs for all segments are presented in two columns following standard industry practices. For modalities with highlights, the interface provides an informative message and supports the removal of all highlights on segment via button, with highlights on words disappearing automatically upon editing, as in Shenoy et al. (2021). The interface supports real-time logging of user actions to analyze the editing process. In particular, we log the start and end times for each edited document, the accessing and exiting of segment textboxes, highlight removals, and individual keystrokes during editing. GROTE intentionally lacks standard features such as translation memories, glossaries, and spellchecking to ensure equal familiarity among translators, ultimately controlling for editor proficiency with these tools, as done in previous studies (Shenoy et al., 2021; Sarti et al., 2022). While most translators noted the lack of features in our usability assessment, the majority also found the interface easy to set up, access, and use  (Table 7)  ."
        },
        {
            "title": "4.1 Productivity",
            "content": "We obtain segmentand document-level edit times and compute editing productivity as the number of processed source characters per second over the sum of all document-level edit times. To account for potential breaks taken by post-editors during editing, we filter out pauses between logged actions longer than 5 minutes. We note that our findings times, possibly reflecting an increase in cognitive effort to process additional information. We find highlights to have significant impact on increasing the editing speed of EnglishItalian translators (p < 0.001), but minimal impact for EnglishDutch. Comparing the productivity of the same translator editing with and without highlights (MAIN vs POST), two-thirds of the translators editing with highlights were up to two times slower on biomedical texts. However, the same proportion of translators was up to three times faster on social media texts across both directions. In summary, we find that highlight modalities are not predictive of edit times on their own, but translation direction and domain play an important role in determining the effect of highlights on editing productivity. We attribute these results to two main factors, which will remain central in the analysis of the following sections: (1) the different propensity of translators to act upon highlighted issues in the two tested directions, and (2) the different nature of errors highlighted across domains."
        },
        {
            "title": "4.2 Highlights and Edits",
            "content": "We then examine how highlights are distributed across modalities and how they influence the editing choices of human post-editors. Agreement Across Modalities First, we quantify how different modalities agree in terms of highlights distribution and editing. We find that cross-modality highlight overlap ranges between 15% and 39%, with the highest overlap for EnglishItalian social media and EnglishDutch biomedical texts.4 Despite the relatively low highlight agreement, we find an average agreement of 73% for post-edited characters across modalities. This suggests that edits are generally uniform regardless of highlight modalities and are not necessarily restricted to highlighted spans.5 Do Highlights Accurately Identify Potential Issues? Table 1 (Base Freq.) shows raw highlight and edit frequencies across modalities. We observe different trends across the two language pairs: for EnglishItalian, post-editors working with highlights edit more than twice as much as translators with No Highlight, regardless of the highlight modality. On the contrary, for EnglishDutch they 4Scores are normalized to account for highlight frequencies across modalities. Highlight agreement shown in Table 8. 5Editing agreement is shown in Figure 8. Figure 3: Productivity of post-editors across QE4PE stages (PRE, MAIN, POST). The marks outliers and marks missing data. Each row corresponds to the same three translators across all stages. are not sensitive to this filtering. Do Highlights Make Post-editors Faster? Figure 3 shows translators productivity across stages, with every dot corresponding to the productivity of single individual. We observe that no highlight modality leads to systematically faster editing across all speed groups and that the ordering of PRE-task speed groups is maintained in the following stages despite the different highlight modalities. These results suggest that individual variability in editing speed is more critical than highlight modality in predicting editing speed. However, faster EnglishDutch translators achieve outstanding productivity (> 300 char/min) only for No Highlight, and, Oracle modalities, suggesting that lower-quality highlights hinder editing speed. We validate these observations by fitting negative binomial mixed-effect model on segment-level editing times (model details in Table 5). Excluding random factors such as translator and segment identity from the model produces significant drop in explained variance, confirming the inherent variability of editing times (R2 = 0.93 0.41). Model coefficients show that MT output length and the proportion of highlighted characters are the main factors driving an increase in editing Base Freq."
        },
        {
            "title": "Measured",
            "content": "P (H) (E) (EH) ΛE (HE) ΛH # (EH)"
        },
        {
            "title": "Projected",
            "content": "# ΛE # (HE) # ΛH - No High. Random 0.16 Oracle 0.15 Unsup. 0.16 Sup. 0.12 - No High. Random 0.17 Oracle 0.20 Unsup. 0.20 Sup. 0.12 0.05 - 0.12 0.13 0.16 0.14 - 0.10 0.11 0.09 - - 0.37 0.25 0.28 - - 0.26 0.20 0.24 - - 4.62 2.27 2. - - 4.33 2.50 3.43 EnglishItalian - - 0.45 0.21 0.22 - - 4.1 2.2 2.0 EnglishDutch - 0. - 1.20 - 0.06 0.180.19 6.001.38 0.550.10 4.230.14 0.110.14 2.750.48 0.370.16 2.470.26 0.140.14 3.501.50 0.350.13 3.181.18 - 1.20 - - 0.53 0.36 0.33 - 1.14 - 0. - - 3.12 0.280.02 2.551.78 0.400.13 2.350.77 2.00 0.220.02 1.830.67 0.310.05 1.720.28 3.30 0.280.04 2.331.10 0.240.09 2.400.90 - 0.19 - 1.19 Table 1: Highlighting (H) and editing (E) average statistics across directions and highlight modalities. Measured: actual edits performed in the specified modality. Projected: using modality highlights over No Highlight edits to account for editing biases (Section 4.2). Random highlights matching average word frequencies are used as Random baseline, and Projected increases / decreases compared to Measured counterparts are shown. Significant Oracle gains over all other modalities are underlined (p < 0.05 with Bonferroni correction). edit 33% less in the same setting. These results suggest different attitude towards acting upon highlighted potential issues across the two translation directions, with EnglishItalian translators appearing to be conditioned to edit more when highlights are present. We introduce four metrics to quantify highlights-edits overlap: (EH) and (HE), reflecting highlights precision and recall in predicting edits, respectively. ΛE def= (EH)/P (EH) shows how much more likely an edit is to fall within rather than outside highlighted characters. ΛH def= (HE)/P (HE) shows how much more likely it is for highlight to mark edited rather than unmodified spans. Intuitively, character-level recall (HE) should be more indicative of highlight quality compared to precision (EH), provided that word-level highlights can be useful even when not minimal.6 Table 1 (Measured) shows metric values across the three highlight modalities (breakdowns by domain and speed shown in Tables 10 and 11). As expected, Oracle highlights obtain the best performance in terms of precision and recall, with (HE), in particular, being significantly higher than the other two modalities across both directions. 6For example, if the gender of the highlighted word traduttore is changed to the feminine traduttrice, (HE) = 1 (edit correctly and fully predicted) but (EH) = 0.3 since word stem characters are left unchanged. Surprisingly, we find no significant precision and recall differences between Supervised and Unsupervised highlights despite the word-level QE training of XCOMET used in the former modality. Moreover, they support the potential of unsupervised, model internals-based techniques to complement or substitute more expensive supervised approaches. Still, likelihood ratios ΛE, ΛH 1 for all modalities and directions indicate that highlights are 2-4 times more likely to precisely and comprehensively encompass edits than nonhighlighted texts. This suggests that even imperfect highlights that do not reach Oracle-level quality might effectively direct editing efforts toward potential issues. We validate these observations by fitting zero-inflated negative binomial mixedeffects model to predict segment-level edit rates. Results confirm significantly higher edit rate for EnglishItalian highlighted modalities and the social media domain with < 0.001 (features and significances shown in Appendix Table 6). We find significant zero inflation associated with translator identity, suggesting the choice of leaving MT outputs unedited is highly subjective. Do Highlights Influence Editing Choices? Having observed clear influence of highlights on the editing rate of translators, we question whether the relatively high (EH) and (HE) values for some highlight modalities are not artificially inflated by the eagerness of translators to intervene on highlighted spans, rather than reflecting actual highlight quality. In other words, do highlights identify actual issues, or do they condition translators to edit when they otherwise would not? To answer this, we propose to project highlights from selected modalityin which highlights were shown during editingonto the edits performed by the No Highlight translators on the same segments. The resulting difference between measured and projected metrics can then be taken as an estimate for the impact of highlight presentation on their resulting accuracy. To further ensure the soundness of our analysis, we use set of projected Random highlights as lower bound for highlight performance. To make the comparison fair, Random highlights are created by randomly highlighting words in MT outputs matching the average word-level highlight frequency across all highlighted modalities given the current domain and translation direction. Table 1 (Projected) shows results for the three highlighted # (EH) are modalities. Projected precision scores influenced by edit frequency, and hence see major decrease for EnglishItalian, where the No Highlight edit rate (E) is much lower. However, the # ΛE across all EnglishItalian modaliincrease in ties confirms that, despite the lower edit proportion, highlighted texts remain notably more likely to be edited than non-highlighted ones. Conversely, # ΛE for EnglishDutch shows that edthe lower its become much less skewed towards highlighted spans in this direction when accounting for pre- # ΛH , which are not sentation bias. influenced by different edit rates, exhibit the same trend. Finally, projected metrics remain consistently above the Random baseline, suggesting higher-than-chance ability to identify errors even in the worst-performing settings. # (HE) and Overall, while the presence of highlights makes EnglishItalian translators more likely to intervene in MT outputs, their location in the MT output often pinpoints issues that would be edited regardless of highlighting. EnglishDutch translators, on the contrary, intervene at roughly the same rate regardless of highlights presence, but their edits are focused mainly on highlighted spans when they are present. This difference is consistent across all subjects in the two directions despite the identical setup and comparable MT and QE quality across languages. This suggests that cultural factors might play non-trivial role in determining the usability and influence of QE methods regardless of span accuracy, phenomenon previously observed in human-AI interaction studies (Ge et al., 2024)."
        },
        {
            "title": "4.3 Quality Assessment",
            "content": "We continue our assessment by inspecting the quality of MT and post-edited outputs along three dimensions. First, we use XCOMET segment-level QE ratings as an automatic approximation of quality and compare them to human-annotated quality scores collected in the last phase of our study. For efficiency, these are obtained for the 0-100 Direct Assessment scale commonly used in QE evaluation (Specia et al., 2020), but following an initial step of MQM error annotation to condition scoring on found errors, as prescribed by the ESA protocol (Kocmi et al., 2024b). Secondly, MQM error span annotations are used to analyze the distribution of error categories. Thirdly, we manually assess critical errors, which we inserted to quantify whether highlight modalities can help address unambiguous issues. Do Highlights Influence Post-Editing Quality? In this stage, we focus particularly on edited quality improvements, i.e. the quality difference between MT and post-edited versions of the same segment. Positive scores for this metric reflect an increase in quality after the post-editing stage and are bounded by the maximal achievable quality gain given the initial MT quality. Figure 4 shows median improvement values across quality bins defined from the distribution of initial MT quality scores (shown in histograms), in which all postedited versions of each MT output appear as separate observations. Positive median scores confirm that post-edits generally lead to quality improvements across all tested settings. However, we observe different trends across the two metrics: across both domains, XCOMET underestimates the human-assessed ESA quality improvement, especially for biomedical texts where it shows no improvements regardless of the initial MT quality. These results echo recent findings cautioning users against the poor performance of trained MT metrics for unseen domains and highquality translations (Agrawal et al., 2024; Zouhar et al., 2024a). Focusing on the more reliable ESA scores, we observe large quality improvements from post-editing, as shown by near-maximal quality gains across most bins and highlight modalities. While No Highlight seems to underperform other modalities in the social media domain, the lack Figure 4: Median quality improvement for post-edited segments at various initial MT quality levels across domains and highlight modalities. Quality scores are estimated using XCOMET segment-level QE (top) and professional ESA annotations (bottom). Histograms show example counts across quality bins for the two metrics. Dotted lines show upper bounds for quality improvements given starting MT quality. of more notable differences suggests that highlights quality impact might not be evident in terms of segment-level quality, motivating our next steps in the quality analysis. We also find no clear relationship between translator speed and edited quality improvements, suggesting that higher productivity does not come at cost for faster translators (Figure 10). This finding further confirms that neglecting errors is not the cause of the differences in editing patterns shown in previous sections. Which Error Types Do Highlights Identify? Figure 5 shows breakdown of MQM annotations across modalities, domains and directions for three error macro-categories.7 At this granularity, differences across modalities become visible, with overall error counts showing clear rela- # ΛE from Table 1 (Oracle being remarkably tion to better for EnglishItalian, with milder and more uniform trends in EnglishDutch). At least for EnglishItalian, these results confirm that an observable quality improvement from editing with highlights is present in the best-case Oracle sce7Detailed MQM error counts are shown in Table 9. Figure 5: Distribution of MQM error categories for MT and post-edits across highlight modalities. nario. We also note different distribution of Accuracy and Style errors, with the former being more common in biomedical texts while the latter appearing more often for translated social media posts. We posit that differences in error types across domains might be the culprit behind the opposite productivity trends observed in Section 4.1: highlighted accuracy errors might lead to timeconsuming terminology checks in biomedical texts. In contrast, style errors with simple fixes might be corrected more quickly in social media texts. Do Highlights Detect Critical Errors? We examine whether the critical errors we inserted were detected by different modalities, finding that while most modalities fare decently with more than 62% of critical errors highlighted, Unsupervised is the only setting for which all errors are correctly highlighted across both directions. Then, critical errors are manually verified in all outputs, finding that 16-20% more critical errors are edited in highlighted modalities compared to No Highlight (full results in Table 9). Hence, the presence of highlights might result in narrow but tangible quality improvements that remain undetected in coarser assessments, and finer-grained evaluations might be needed to quantify future improvements in word-level QE quality."
        },
        {
            "title": "4.4 Usability",
            "content": "In post-task questionnaire answers  (Table 2)  , most translators stated that MT outputs had average-tohigh quality and that provided texts were challenging to translate. Highlights were generally found decently accurate, but they were generally not found useful to improve either productivity or quality (including Oracle ones). Interestingly, despite the convincing gains for critical errors measured in the last section, most translators stated that highlights did not influence their editing and did not help them identify errors that would have otherwise been missed. Concretely, this suggests that the potential quality improvements might not be easily perceived by translators and might have secondary importance compared to the extra cognitive load elicited by highlighted spans. When asked to comment about highlights, several translators called them more of an eye distraction, as they often werent actual mistakes and not quite accurate enough to rely on them as suggestion. Some translators also stated that missed errors led them to disregarding the highlights to focus on checking each sentence. Despite their high quality, only one editor working with Oracle highlights found highlights helpful in making the editing process faster and somehow easier. Taken together, these comments convincingly point to negative perception of the quality and usefulness of highlights, suggesting that improvement"
        },
        {
            "title": "Question",
            "content": "EngIta EngNld MT outputs were generally of high quality. Provided texts were challenging to translate. Highlights ... ... were generally accurate in detecting potential issues. ... were generally useful during editing. ... improved my editing productivity. ... improved the quality of my translations. ... required additional editing effort on my part. ... influenced my editing choices. ... helped identify errors Id have otherwise missed. Table 2: Post-task questionnaire responses. Columns are averaged (1Strongly disagree to 5Strongly agree) across = 3 translators per language for No Highlight, Oracle, Unsupervised, and Supervised. in QE accuracy may not be sufficient to improve QE usefulness in editors eyes."
        },
        {
            "title": "5 Conclusion",
            "content": "This study evaluated the impact of various errorspan highlighting modalities, including automatic and human-made ones, on the productivity and quality of human post-editing in realistic professional setting. Our findings highlight the importance of domain, language and editors speed in determining highlights effect on productivity and quality, underscoring the need for broad evaluations encompassing diverse settings. The limited gains of human-made highlights over automatic QE and their indistinguishable perception from editors assessment indicate that further gains in the accuracy of these techniques might not be the determining factor in improving their integration into post-editing workflows. In particular, future work might explore other directions to further assess and improve the usability of word-level QE highlights, for example, studying their impact on non-professional translators and language learners or combining them with edit suggestions to justify the presence of error spans."
        },
        {
            "title": "Acknowledgements",
            "content": "Gabriele Sarti and Arianna Bisazza acknowledge the support of Imminent, which funded the data collection for this study with Language Technology Grant. The authors are grateful to Translated S.r.l. and Global Textware B.v. and all translators that collaborated in the data collection process. Gabriele Sarti, Grzegorz Chrupała and Arianna Bisazza also acknowledge the support of the Dutch Research Council (NWO) for the project InDeep (NWA.1292.19.399). Arianna Bisazza is further supported by the NWO Talent Programme (VI.Vidi.221C.009). Ana Guerberof Arenas is supported by the European Research Council (ERC CoG INCREC 101086819)."
        },
        {
            "title": "Broader Impact and Ethical\nConsiderations",
            "content": "Our study explicitly centers the experience of professional translators, responding to recent calls for user-centered evaluation of translation technologies. By prioritizing translators perspectives and productivity, we aim to contribute to methods that complement rather than replace human expertise. Our findings highlight gap between user perception and measured quality improvements, suggesting that future efforts should focus primarily on improving the usability of these methods in editing interfaces. In particular, new assistive approaches for post-editing should not only strive to increase productivity, but rather reduce the cognitive burden associated with post-editing work. This insight is crucial for designing more user-centered quality estimation tools that genuinely support human work. Finally, our results underscores the importance of culturally adaptive design for assessing the effectiveness of proposed methodologies across participants with different cultural backgrounds. All participants in this study were professional translators who provided informed consent. The research protocol ensured anonymity and voluntary participation, with translators recruited through professional translation providers. The studys open data release further promotes transparency, allowing other researchers to reproduce and extend our findings."
        },
        {
            "title": "References",
            "content": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019. Gradio: Hassle-free sharing and testing of ml models in the wild. Sweta Agrawal, António Farinhas, Ricardo Rei, and Andre Martins. 2024. Can automatic metIn Prorics assess high-quality translations? ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1449114502. Association for Computational Linguistics. Chantal Amrhein, Nikita Moghe, and Liane Guillou. 2022. ACES: Translation accuracy challenge sets for evaluating machine translation metrics. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 479513. Association for Computational Linguistics. Chantal Amrhein, Nikita Moghe, and Liane Guillou. 2023. ACES: Translation accuracy chalIn Proceedings of lenge sets at WMT 2023. the Eighth Conference on Machine Translation, pages 695712. Association for Computational Linguistics. Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. 2021. Deep learning through the lens of example difficulty. In Advances in Neural Information Processing Systems, volume 34, pages 1087610889. Curran Associates, Inc. Rachel Bawden and Benoît Sagot. 2023. RoCSMT: Robustness challenge set for machine translation. In Proceedings of the Eighth Conference on Machine Translation, pages 198216. Association for Computational Linguistics. Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023. Findings of the WMT 2023 shared task on quality estimation. In Proceedings of the Eighth Conference on Machine Translation, pages 629653. Association for Computational Linguistics. John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence In COLestimation for machine translation. ING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 315321. COLING. Sven Coppers, Jan Van den Bergh, Kris Luyten, Karin Coninx, Iulianna Van der Lek-Ciudin, Tom Vanallemeersch, and Vincent Vandeghinste. 2018. Intellingo: An intelligible translation environment. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 113. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460 1474. David Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence In Proceedings of the similarity Even better. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3650. Association for Computational Linguistics. Sugyeong Eo, Chanjun Park, Hyeonseok Moon, Jaehyung Seo, and Heuiseok Lim. 2022. Wordlevel quality estimation for korean-english neural machine translation. IEEE Access, 10:44964 44973. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting hallucinations in large language models using semantic entropy. Nature, 630:625630. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation, pages 10661083. Association for Computational Linguistics. Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estimation: Overview and results. In Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165 178. Association for Computational Linguistics. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Markus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and Alon Lavie. 2024. Are LLMs breaking MT metrics? results of the WMT24 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 4781. Association for Computational Linguistics. Yarin Gal and Zoubin Ghahramani. 2016. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 10501059, New York, New York, USA. PMLR. Xiao Ge, Chunchen Xu, Daigo Misaki, Hazel Rose Markus, and Jeanne Tsai. 2024. How culture shapes what people want from ai. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI 24, New York, NY, USA. Association for Computing Machinery. Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Largerscale transformers for multilingual masked language modeling. CoRR, abs/2105.00572. Ana Guerberof-Arenas and Joss Moorkens. 2023. Ethics and Machine Translation: The End User Perspective, pages 113133. Springer International Publishing, Cham. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024a. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024b. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Nico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, and Josef van Genabith. 2020. MMPE: MultiModal Interface for Post-Editing Machine Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 16911702. Association for Computational Linguistics. Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno Guerreiro. 2024. Enhanced hallucination detection in neural machine translation through simple detector aggregation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1857318583. Association for Computational Linguistics. Fabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, and André F. T. Martins. 2019. OpenKiwi: An open source framework for quality estimation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 117122. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, Mariya Shmatova, Steinthór Steingrímsson, and Vilém Zouhar. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 142. Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023. GEMBA-MQM: Detecting translation quality In Proceedings of error spans with GPT-4. the Eighth Conference on Machine Translation, pages 768775. Association for Computational Linguistics. Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popovic, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: balanced approach for human evaluation of machine translation. In Proceedings of the Ninth Conference on Machine Translation, pages 14401453. Association for Computational Linguistics. Hans Krings. 2001. Repairing texts: Empirical investigations of machine translation postediting processes, volume 5. Kent State University Press. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations. Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. Towards explainable evaluation metrics for machine translation. Journal of Machine Learning Research, 25(75):149. Zheng Wei Lim, Ekaterina Vylomova, Charles Kemp, and Trevor Cohn. 2024. Predicting human translation difficulty with neural machine translation. Transactions of the Association for Computational Linguistics, 12:14791496. Zhongtao Liu, Parker Riley, Daniel Deutsch, Alison Lui, Mengmeng Niu, Apurva Shah, and Markus Freitag. 2024. Beyond human-only: Evaluating human-machine collaboration for collecting high-quality translation data. In Proceedings of the Ninth Conference on Machine Translation, pages 10951106. Association for Computational Linguistics. Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional quality metrics (MQM): framework for declaring and describing translation quality metrics. Tradumàtica, pages 0455463. Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841846. Samuel Läubli, Patrick Simianer, Joern Wuebker, Geza Kovacs, Rico Sennrich, and Spence Green. 2021. The impact of text presentation on translator performance. Target. International Journal of Translation Studies, 34. Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong, Marine Carpuat, and Niloufar Salehi. 2023. Physician detection of clinical harm in machine translation: Quality estimation aids in reliance and backtranslation identifies critical errors. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1163311647. Association for Computational Linguistics. Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine translation with large language models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227237. European Association for Machine Translation. Mariana Neves, Antonio Jimeno Yepes, Aurélie Névéol, Rachel Bawden, Giorgio Maria Di Nunzio, Roland Roller, Philippe Thomas, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, Dina Wiemann, and Cristian Grozea. 2023. Findings of the WMT 2023 biomedical translation shared task: Evaluation of ChatGPT 3.5 as comparison system. In Proceedings of the Eighth Conference on Machine Translation, pages 4354. Association for Computational Linguistics. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: UnbabelIST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585. Association for Computational Linguistics. Ricardo Rei, Ana Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? unbabel-IST 2021 submission for the metrics shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 10301040. Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685 2702. Association for Computational Linguistics. Gabriele Sarti, Arianna Bisazza, Ana GuerberofArenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 77957816. Association for Computational Linguistics. Beatrice Savoldi, Alan Ramponi, Matteo Negri, and Luisa Bentivogli. 2025. Translation in the hands of many:centering lay users in machine translation interactions. Raksha Shenoy, Nico Herbig, Antonio Krüger, and Investigating the Josef van Genabith. 2021. helpfulness of word-level quality estimation for post-editing machine translation output. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1017310185. Association for Computational Linguistics. Lucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzmán, and André F. T. Martins. 2020. Findings of the WMT 2020 shared task on quality estimation. In Proceedings of the Fifth Conference on Machine Translation, pages 743764. Association for Computational Linguistics. Lucia Specia, Marco Turchi, Nicola Cancedda, Nello Cristianini, and Marc Dymetman. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation. European Association for Machine Translation. Jiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. 2022. Investigating the benefits of free-form rationales. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 58675882. Association for Computational Linguistics. Aleš Tamchyna. 2021. Deploying MT quality estimation on large scale: Lessons learned and In Proceedings of Machine open questions. Translation Summit XVIII: Users and Providers Track, pages 291305. Association for Machine Translation in the Americas. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 34503466. Association for Computational Linguistics. Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90121. Association for Computational Linguistics. Marco Turchi, Antonios Anastasopoulos, José G. C. de Souza, and Matteo Negri. 2014. Adaptive quality estimation for machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 710720. Association for Computational Linguistics. Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240251. Association for Computational Linguistics. Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2024. Generation probabilities are not enough: Uncertainty highlighting in ai code completions. ACM Trans. Comput.-Hum. Interact. Just Accepted. Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. 2023. Understanding and detecting hallucinations in neural machine translation via model introspection. Transactions of the Association for Computational Linguistics, 11:546564. Chrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality estimation shared task at WMT 2024: Are LLMs closing the gap in QE? In Proceedings of the Ninth Conference on Machine Translation, pages 82109. Association for Computational Linguistics. Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orasan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 6999. Association for Computational Linguistics. Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. 2024a. Fine-tuned machine translation metrics struggle in unseen domains. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 488500. Association for Computational Linguistics. Vilém Zouhar, Michal Novák, Matúš Žilinec, Ondˇrej Bojar, Mateo Obregón, Robin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, and Lisa Yankovskaya. 2021a. Backtranslation feedback improves user confidence in MT, not quality. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 151161. Association for Computational Linguistics. Vilém Zouhar, Martin Popel, Ondˇrej Bojar, and Aleš Tamchyna. 2021b. Neural machine translation quality and post-editing performance. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1020410214. Association for Computational Linguistics. Vilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2024b. AI-assisted human evaluation of machine translation."
        },
        {
            "title": "Domain",
            "content": "# Docs # Seg. # Words"
        },
        {
            "title": "POST",
            "content": "Social Biomed. Social Biomed. Social Biomed."
        },
        {
            "title": "Total",
            "content": "4 2 30 21 6 2 64 23 15 160 34 16 413 539 348 3375 3384 841 257 Table 3: Statistics for QE4PE data. Remove negation (13-6)"
        },
        {
            "title": "Dutch",
            "content": "English No significant differences were found with respect to principal diagnoses [...] Er werden geen significante verschillen significante verschillen gevonden met betrekking tot de belangrijkste diagnoses [...] Title literal translation (16-3)"
        },
        {
            "title": "Italian",
            "content": "The Last of Us is an easy and canonical example of dad-ification. [...] The Last of Us Lultimo di noi è un esempio facile canonico di dad-ification. [...] Wrong term (48-5)"
        },
        {
            "title": "English\nItalian",
            "content": "[...], , except for alkaline phosphatase. [...], ad eccezione della fosfatasi alcalina chinasi proteica. Table 4: Examples of original manually inserted critical errors with document-segment ID from Table 9."
        },
        {
            "title": "A Filtering Details and Statistics for",
            "content": "QE4PE Data 1. Documents should contain between 4 and 10 segments, each containing 10-100 words (959 docs). This ensures that all documents are roughly uniform in terms of size and complexity to maintain steady editing flow (Section 3.5). 2. The average segment-level QE score predicted by XCOMET-XXL is between 0.3 and 0.95, with no segment below 0.3 (429 docs). This forces segments to have decent but still imperfect quality, excluding fully wrong translations. 3. At least 3 and at most 20 errors spans per document, with no more than 30% of words in the document being highlighted (351 docs). This avoids overwhelming the editor with excessive highlighting, while still ensuring error presence. The same heuristics were applied to both translation directions, selecting only documents matching our criteria in both cases. Table 3 provide some statistics for the selected examples. Target: Seg. Edit Time, 5s bins from 0 to 600s Feature Coeff."
        },
        {
            "title": "Significance",
            "content": "(Intercept) MT Num. Chars Highlight Ratio % Target Lang.: ITA Text Domain: Social Oracle Highlight Sup. Highlight Unsup. Highlight MT XCOMET QE Score ITA:Oracle ITA:Sup. ITA:Unsup. Social:Oracle Social:Sup. Social:Unsup. Highlight Ratio:Oracle Highlight Ratio:Sup."
        },
        {
            "title": "Edit Order\nTranslator ID\nSegment ID",
            "content": "1.67 2.42 1.59 -0.34 0.31 -0.79 0.02 -0.07 0.01 0.91 1.18 0.48 -0.19 -0.34 -0.22 -0.83 -1.33 *** *** *** *** *** . *** *** *** *** ** *** *** * ***"
        },
        {
            "title": "Random Factors",
            "content": "Table 5: Details for the negative binomial mixed-effect model used for the productivity analysis of Section 4.1. Target: % of edited characters in segment (0-100)."
        },
        {
            "title": "Feature",
            "content": "Coeff."
        },
        {
            "title": "Significance",
            "content": "(Intercept) MT Num. Chars Highlight Ratio % Target Lang.: ITA Text Domain: Social Oracle Highlight Sup. Highlight Unsup. Highlight ITA:Oracle ITA:Sup. ITA:Unsup. Social:Oracle Social:Sup. Social:Unsup. Highlight Ratio:Oracle Highlight Ratio:Sup. 21.0 10.3 7.1 -9.9 10.9 -5.2 -4.7 -0.9 12.2 15.9 13.4 3.5 -0.4 2.1 -0.18 -1.78 *** *** *** *** *** *** *** *** *** ** ***"
        },
        {
            "title": "Edit Order\nTranslator ID\nSegment ID",
            "content": "MT Num. Chars Target Lang Text Domain Translator ID"
        },
        {
            "title": "Random Factors",
            "content": "Zero-Inflation Factors Table 6: Details for the zero-inflated negative binomial mixed-effect model used for the editing analysis of Section 4.2. The model achieves an RMSE of 0.11 and an R2 of 0.98. 9-1 9-2 9-3 952-1 52-2 Doc ID - Seg. ID"
        },
        {
            "title": "Score",
            "content": "Specifying periand postnatal factors in children born very preterm (VPT) that affect later outcome helps to improve long-term treatment. Specificare fattori perie postnatali nei bambini nati molto pretermine (VPT) che influenzano il risultato successivo aiuta migliorare il trattamento lungo termine. Specificare fattori perie postnatali nei bambini nati molto pretermine (VPT, Very Preterm) che influenzano il risultato successivo aiuta migliorare il trattamento lungo termine. When we have foreign acronym, the usual rule is to indicate also the whole term the first time it appears."
        },
        {
            "title": "Minor",
            "content": ""
        },
        {
            "title": "Severity",
            "content": "To enhance the predictability of 5-year cognitive outcome by perinatal, 2-year developmental and socio-economic data. Migliorare la prevedibilità del risultato cognitivo 5 anni mediante dati perinatali, di sviluppo socioeconomici 2 anni. 5-year infants born VPT were compared to 34 term controls. neonati di 5 anni nati VPT sono stati confrontati con 34 nati termine come controllo. neonati di 5 anni nati VPT sono stati confrontati con 34 controlli termine. The IQ of 5-year infants born VPT was 10 points lower than that of term controls and influenced independently by preterm birth and SES. Il QI dei bambini di 5 anni nati VPT era di 10 punti inferiore quello dei nati termine di controllo, influenzato indipendentemente dalla nascita pretermine dai dati SES. Il QI dei bambini di 5 anni nati VPT era di 10 punti inferiore quello dei nati termine influenzato indipendentemente dalla nascita pretermine dallo stato socioeconomico (SES)."
        },
        {
            "title": "Minor",
            "content": ""
        },
        {
            "title": "Minor",
            "content": "70 Il QI dei bambini di 5 anni nati VPT era di 10 punti inferiore quello dei nati termine di controllo, influenzato indipendentemente dalla nascita pretermine dai dati SES. Il QI dei bambini di 5 anni nati VPT era di 10 punti inferiore quello dei nati termine influenzato indipendentemente dalla nascita pretermine dallo stato socioeconomico (SES). Unexplained Nonacronym. expert people could have trouble understanding the meaning."
        },
        {
            "title": "Minor",
            "content": "But with less than 3 months to go for that, feel Im not ready yet, but having never taken it, have nothing to compare it to besides colleagues advice. Ma con meno di 3 mesi per farlo, sento di non essere ancora pronto, ma non lho mai preso, non ho nulla con cui confrontarlo oltre ai consigli dei colleghi. Ma con meno di 3 mesi per farlo, sento di non essere ancora pronto, non avendolo mai fatto, non ho nulla con cui confrontarlo oltre ai consigli dei colleghi. Without knowing what know, they cant know if Im actually ready yet, but many of them are pushing me to sign up for it. Senza sapere quello che so, non possono sapere se sono ancora pronta, ma molti di loro mi stanno spingendo iscrivermi. Se non hanno idea di quanto sappia, non possono sapere se sono davvero pronta, ma molti di loro mi stanno spingendo iscrivermi. Senza sapere quello che so, non possono sapere se sono ancora pronta, ma molti di loro mi stanno spingendo iscrivermi. Se non hanno idea di quanto sappia, non possono sapere se sono davvero pronta, ma molti di loro mi stanno spingendo iscrivermi. 52-3 Im close... just dont know if Im 2 months study close. Ci sono quasi... solo che non so se ce la farò in soli 2 mesi, ma penso di potercela fare. Ci sono quasi... solo che non so se ce la farò in soli 2 mesi."
        },
        {
            "title": "Major",
            "content": ""
        },
        {
            "title": "Minor",
            "content": ""
        },
        {
            "title": "Major",
            "content": "20 Accuracy Incorrect meaning has been transferred to the source text."
        },
        {
            "title": "Omission",
            "content": "Translation includes the information that is not present in the source and it changes or distorts the original message. Translation is missing the information that is present in the source, which is important to convey the message."
        },
        {
            "title": "Mistranslation",
            "content": "Translation does not accurately represent the source content meaning."
        },
        {
            "title": "Inconsistency",
            "content": "There are internal inconsistencies in the translation (for example, using different verb forms in the bullet list or in CTAs, calling the same UI element differently, terminology used inconsistently etc)."
        },
        {
            "title": "Untranslated",
            "content": "Content that should have been translated has been left untranslated. Linguistic Official linguistic reference sources such as grammar books."
        },
        {
            "title": "Grammar",
            "content": "Punctuation is used incorrectly (for the locale or style), including missing or extra white spaces and the incorrect use of space (non-breaking space). Violation of typographic conventions of the locale. Issues related to spelling of words, including typos, wrong word hyphenation, word breaks and capitalization. Issues related to the grammar or syntax of the text, other than spelling. Style Not suitable/native; awkward. too literal or"
        },
        {
            "title": "Inconsistent Style",
            "content": "Style is inconsistent within text."
        },
        {
            "title": "Readability",
            "content": "Translation does not read well (due to heavy sentence structure, frequent repetitions, unidiomatic)."
        },
        {
            "title": "Wrong Register",
            "content": "Inappropriate style for the specific subject field, the level of formality, and the mode of discourse (e.g., written text versus transcribed speech)."
        },
        {
            "title": "Minor",
            "content": "The Severity Level of an error that seriously affects the understandability, reliability, or usability of the content for its intended purpose or hinders the proper use of the product or service due to significant loss or change in meaning or because the error appears in highly visible or important part of the content. The Severity Level of an error that does not seriously impede the usability, understandability, or reliability of the content for its intended purpose, but has limited impact on, for example, accuracy, stylistic quality, consistency, fluency, clarity, or general appeal of the content."
        },
        {
            "title": "Neutral",
            "content": "The Severity Level of an error that differs from quality evaluators preferential translation or that is flagged for the translators attention but is an acceptable translation. Figure 6: Top:QA interface with cropped examples of biomedical and social media texts with error annotations (Biomedical: post-edited segments with No Highlight; Social media: MT outputs). Bottom: Annotation instructions for our MQM-inspired error taxonomy. . p o e d s t h h t s u e E . t l t d m i t e a v u p M . a d z i s T n e w i"
        },
        {
            "title": "T\nM\nd\na\nB",
            "content": "."
        },
        {
            "title": "E\nP\nd\nn\no\ny\ne\nb",
            "content": "e a e c t n o . t p n s h r o . n f a l : . w t p , v u P : G . l m , w t p , l , v u P : G . l m , w t p , l , i a : G r t , t o s , t c l : . l e w , r t : . a : G . l n s r s o a y i d n a n . l q r , r t , v e : . o i e : G e o l t d p i . v e : . l m , i c r : G . a p n o i f s t a c . r : . l m t , w t p , l , v u P : G . w l f i c r a e s s r I i s t t o t p r l s . v e o c t y i d r e o j b i w b , e g n e , v u p , o i e : G l m , v e , o i e : G v u p , w t p : G i a , l m : G . e i i a s n , v u p y t n o l s . t n n d e a a i a . t n r t t p A . i s t t s i r / u c / i d n c r o . v e s n e i s l b , o i e s . e i r o o n d , v u p d G . e s d t u v m . e i n i c w a m n r e . e y a a , v u p . e t c a t , d e . v m o f f G G G . n e r n s d E . t c . t c l l : B . w t p , v u P : G : B . w t p , v u P : G . t c e : . l m , v u P : G . l m , w t p , l , i c r : G . t c l : . o i e , v u P : G . w t p , i q , v e , v u P : G . w t p , v u P : G . o i t , v u p : G . t c l : . w t p , v u P : G . w t p , l , v u P : G . t c l : . w t p , i c r : G . v e : G m e s t S f t O w y l f e s A t O w y l w l f t y A y l f t O i o m m y A y l f % 0 0 % 0 2 % 0 4 % 0 6 % 0 2 % 0 8 % 0 % 0 6 % 0 8 % 0 4 % 0 0 1 % 0 2 % 0 % 0 4 % 0 0 1 % 0 8 % 0 4 % 0 6 % 0 % 0 6 % 0 4 % 0 6 % 0 6 % 0 8 5 - 2 < 5 - 2 5 - 2 0 1 - 5 0 1 - 5 2 < 5 - 2 0 1 - 5 5 - 2 0 1 - 5 5 - 2 0 1 > 5 - 2 0 1 - 5 0 1 - 5 5 - 2 5 - 2 5 - 5 - 2 5 - 2 0 1 - 5 0 1 0 1 - 5 5 - 0 1 > 0 1 > 0 1 - 5 0 1 - 5 5 - 2 2 < 0 1 > 0 1 - 5 0 1 > 0 1 > 0 1 > 0 1 > 0 1 > 0 1 > 0 1 - 5 0 1 > 0 1 > 0 1 - 0 1 - 5 0 1 > 0 1 > 0 1 > 0 1 - 5 1 1 2 2 2 2 1 1 1 1 1 1 1 2 2 1 2 2 2 2 2 2 2 1 ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r ) ( n e ) ( a r ) ( a r a - h - - e a - h - - w - h - - t - a - - w - a - - g - a - - t - n - - e a - n - - w - n - - t - - - g - - - w - - - t - h - - e a - h - - w - h - - t - a - - g - a - - w - a - - t - n - - e a - n - - w - n - - t - - - g - - - w - - - t m t - P : d / g M"
        },
        {
            "title": "E\no\nY\nT\nA\nC",
            "content": ". % e - P"
        },
        {
            "title": "E\no\nY",
            "content": "t - P"
        },
        {
            "title": "E\no\nY",
            "content": ". r L . b e fi e ? n r o c fl ? f ? l ? p - - - 4 1 2 3 4 2 1 - - - 1 3 1 3 3 1 4 - - - 1 4 2 1 4 2 4 - - - 1 1 2 2 2 1 2 - - - 1 1 2 1 3 1 4 - - - 1 1 1 3 2 3 2 - - - 1 1 3 2 3 3 1 - - - 2 1 1 4 3 5 2 - - - 2 1 3 3 2 1 1 - - - 2 1 3 3 4 1 3 - - - 5 1 3 2 2 1 2 - - - 2 1 1 3 4 1 3 1 4 5 5 5 5 5 4 2 2 4 5 5 2 4 4 4 4 5 5 5 2 1 4 2 4 3 3 2 1 4 4 3 3 3 4 2 1 3 2 - - - 4 2 2 3 2 1 2 - - - 1 1 2 3 4 2 3 - - - 4 2 3 3 2 2 2 - - - 2 3 3 4 2 2 4 8 0 . 4 0 . 8 . 8 0 . 6 0 . 6 . 6 0 . 6 0 . 6 . 4 0 . 4 0 . 6 . 4 0 . 6 0 . 4 . 6 0 . 6 0 . 4 . 8 0 . 6 0 . 4 . 4 0 . 6 0 . 8 0 . 6 0 . 8 0 . 4 0 . 4 0 . 6 0 . 8 0 . 6 0 . 4 0 . 4 0 . 4 0 . 6 0 . 2 0 . 4 0 . 2 0 . 6 0 . 8 0 . 6 0 . 6 0 . 6 0 . 4 0 . 6 0 . 4 0 . 8 0 . 4 3 3 5 3 3 3 3 3 3 3 2 2 3 3 3 3 1 3 3 e c , i m , l m , n n s , i d , t fl n s , l m y o r , t fl l l s s o , l m , t fl , n x o , l m , n x o , l m , n x o , l m , n x o , i m , l m , n t n n , i m , l m , n x o , i m , l m , n x o , i m , l m y o r , t fl , n t n n , n n s s s o , i d , l m e t t n n , i m , l m t o i , l m , n x o , i d , n l l g n e , n a s a - h - - g - h - - w - h - - t - a - - g - a - - w - a - - e a - n - - g - n - - w - n - - t - - - g - - - w - - - e a - h - - w - h - - t - a - - g - h - - w - a - - g - a - - e a - n - - w - n - - g - n - - t - - - w - - - g - - - e n t s i i u fi k r c f n f u . H r a . H r a y u fl t u s s . F fi n y o 1 u e a s - P . e p o e = . u r i o e s - p l S : t . u e n t q t - f p : T : 7 a . c r l r 5 e s d"
        },
        {
            "title": "Unsupervised and",
            "content": "EnglishItalian EnglishDutch"
        },
        {
            "title": "Social Both",
            "content": "Sup. Unsup. Oracle Unsup. Oracle Sup. 0.17 0.14 0.19 0.19 0.22 0. 0.32 0.30 0.31 0.33 0.32 0.37 0.25 0.20 0.26 0.25 0.27 0. 0.38 0.31 0.30 0.28 0.35 0.39 0.29 0.27 0.26 0.24 0.30 0. 0.34 0.28 0.29 0.25 0.33 0.33 0.26 0.22 0.24 0.24 0.28 0. 0.29 0.29 0.29 0.29 0.31 0.31 0.29 0.24 0.28 0.25 0.30 0. Table 8: Average highlight agreement proportion between different modalities across language pairs and domains (Section 4.2). Scores are normalized to account for the relative frequency of highlight modalities compared to the mean highlight frequency for the current language and domain combination. # Doc.-Seg. Error Type"
        },
        {
            "title": "Has Highlight",
            "content": "% Post-edited Oracle Unsup. Sup. No High. Oracle Unsup. Sup. 1-8 13-6 16-3 20-1 20-7 20-7 22-1 23-4 31-2 34-7 37-4 43-5 48-5 Wrong number Remove negation Title literal translation Wrong acronym Wrong acronym (1) Wrong acronym (2) Name literal translation Addition Wrong acronym Numbers swapped Verb polarity inverted Wrong name Wrong term"
        },
        {
            "title": "NLD\nITA\nBoth\nNLD\nNeither\nNLD\nBoth\nNLD\nNLD\nNLD\nBoth\nBoth\nNLD",
            "content": ""
        },
        {
            "title": "Both\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth\nBoth",
            "content": ""
        },
        {
            "title": "Both\nBoth\nBoth\nITA\nNeither\nITA\nBoth\nNeither\nNeither\nNLD\nBoth\nBoth\nNLD",
            "content": "62 67 50 83 0 0 0 50 100 17 17 67 50 67 44 83 33 100 33 58 58 50 100 33 50 83 83 50 63 83 33 100 33 50 50 83 83 17 33 67 67 60 83 50 100 33 25 25 67 50 33 67 83 83 83 60 Table 9: Highlighting and post-editing statistics for manual critical errors (Section 3.3). Labels in Has Highlight columns indicate whether the error was highlighted in Both, only one (ITAor NLD) or Neitherdirections. Total scores represent the percentage of detected errors (13 errors, 6 editors per highlight modality). Domain Modality P(H) P(E) P(EH) P(EH) ΛH(E) P(HE) P(HE) ΛE(H) F1H Biomed."
        },
        {
            "title": "Social",
            "content": "Biomed."
        },
        {
            "title": "Social",
            "content": "Random No High. Oracle Unsup. Sup. Random No High. Oracle Unsup. Sup. Random No High. Oracle Unsup. Sup. Random No High. Oracle Unsup. Sup. .12 - .08 .16 .11 .20 - .25 .17 . .17 - .21 .23 .12 .16 - .19 .15 .12 EnglishItalian - /.02 - .05/.02 .08/.02 .11/.02 - /.09 - .13/.04 .14/.07 .18/.06 - /1.0 - 5.2/4.0 2.2/3.0 1.6/2. - /1.0 - 3.2/5.7 2.5/2.7 2.1/3.8 EnglishDutch - /.10 - .05/.08 .07/.08 .06/.09 - /.19 - .07/.15 .11/.17 .08/.17 - /1.2 - 4.2/2.5 2.4/2.1 3.3/2.3 - /1.1 - 4.7/2.6 2.2/1.9 3.7/2. - /.02 - .26/.08 .18/.06 .18/.05 - /.09 - .42/.23 .35/.19 .38/.23 - /.12 - .21/.20 .17/.17 .20/.21 - /.22 - .33/.39 .25/.33 .30/.36 - /.11 - .30/.26 .29/.36 .16/.23 - /.21 - .52/.66 .33/.37 .27/. - /.19 - .52/.41 .43/.38 .30/.25 - /.19 - .54/.39 .30/.26 .36/.23 - .02 .07 .10 .12 - .09 .20 .18 .21 - .10 .08 .09 .08 - .19 .12 .13 . - /.13 - .06/.08 .14/.15 .10/.10 - /.20 - .18/.21 .14/.15 .11/.12 - /.17 - .18/.18 .21/.21 .11/.11 - /.16 - .15/.15 .13/.12 .10/.10 - /0.8 - 5.0/3.2 2.0/2.4 1.6/2.3 - /1.0 - 2.8/3.1 2.3/2.4 2.4/3. - /1.1 - 2.8/2.2 2.0/1.8 2.7/2.2 - /1.1 - 3.6/2.6 2.3/2.1 3.6/2.3 - /.03 - .28/.12 .22/.10 .17/.08 - /.13 - .46/.34 .34/.25 .32/.29 - /.15 - .30/.27 .24/.23 .24/.23 - /.17 - .41/.39 .27/.29 .33/. Table 10: Highlighting (H) and editing (E) statistics for each domain, modality and translation direction combination (n = 3 post-editors per combination). Values after slashes are adjusted by projecting highlights of the specified modality over edits from No Highlight translators to estimate highlight-induced editing biases (Section 4.2). Random baseline is added by projecting random highlights matching the average frequency over all modalities for specific domain and translation direction settings. Domain Speed P(H) P(E) P(EH) P(EH) ΛH(E) P(HE) P(HE) ΛE(H) F1H Biomed."
        },
        {
            "title": "Social",
            "content": "Biomed."
        },
        {
            "title": "Social",
            "content": "Fast Avg. Slow Fast Avg. Slow Fast Avg. Slow Fast Avg. Slow .04/.01 .10/.05 .09/.02 .11/.07 .23/.14 .17/. .03/.02 .11/.19 .12/.10 .06/.07 .17/.32 .18/.18 EnglishItalian .03/.01 .09/.04 .08/.01 .07/.04 .18/.10 .14/.03 4.0/2.0 3.0/3.0 2.6/4. 4.2/5.0 2.6/3.2 2.7/4.6 EnglishDutch .02/.01 .10/.17 .10/.07 .04/.04 .15/.29 .15/.14 5.5/5.0 2.0/1.7 2.6/3.2 4.7/5.2 2.1/1.6 2.5/2. .12/.02 .27/.12 .21/.04 .30/.20 .48/.32 .39/.14 .11/.05 .20/.30 .26/.23 .19/.21 .32/.48 .38/.40 .30/.27 .22/.30 .19/.26 .40/.52 .30/.42 .31/. .48/.61 .25/.29 .29/.42 .37/.47 .22/.23 .25/.34 .09 .14 .14 . .08/.11 .07/.11 .07/.11 .11/.16 .09/.15 .11/.17 .13/.18 .13/.16 .12/.16 .10/.13 .10/.12 .09/.11 3.7/2.4 3.1/2.7 2.7/2.3 3.6/3.2 3.3/2.8 2.8/3. 3.6/3.3 1.9/1.8 2.4/2.6 3.7/3.6 2.2/1.9 2.7/3.0 .17/.04 .24/.17 .20/.07 .34/.29 .37/.36 .35/.22 .18/.09 .22/.29 .27/.30 .25/.29 .26/.31 .30/. Table 11: Highlighting (H) and editing (E) statistics for each domain, and translation direction across translator speeds (n = 4 post-editors per combination, regardless of highlight modality). Values after slashes are adjusted by projecting highlights of the specified modality over edits from No Highlight translators to estimate highlight-induced editing biases (Section 4.2)."
        },
        {
            "title": "Language MQM Category",
            "content": "MT"
        },
        {
            "title": "Unsupervised Supervised",
            "content": "Maj. Min. Maj. Min. Maj. Min. Maj. Min. Maj. Min."
        },
        {
            "title": "Dutch",
            "content": "Accuracy - Addition Accuracy - Mistranslation Accuracy - Inconsistency Accuracy - Omission Accuracy - Untranslated Style - Inconsistent Style Style - Readability Style - Wrong Register Linguistic - Grammar Linguistic - Punctuation Linguistic - Spelling"
        },
        {
            "title": "Total",
            "content": "Accuracy - Addition Accuracy - Mistranslation Accuracy - Inconsistency Accuracy - Omission Accuracy - Untranslated Style - Inconsistent Style Style - Readability Style - Wrong Register Linguistic - Grammar Linguistic - Punctuation Linguistic - Spelling"
        },
        {
            "title": "Total",
            "content": "0 21 2 2 1 0 17 0 6 1 5 55 0 25 0 3 4 2 1 3 0 1 39 1 22 4 0 4 0 25 8 15 13 3 1 34 0 1 4 0 27 2 19 6 1 95 0 10 1 0 1 0 5 2 0 0 19 0 18 0 1 1 0 1 0 2 0 1 0 12 3 0 2 0 30 3 16 9 4 79 2 25 2 1 1 5 20 14 3 1 77 0 4 2 0 0 0 0 0 0 0 0 0 23 0 2 1 1 0 0 3 0 2 32 0 8 2 1 1 0 12 5 3 3 38 3 27 2 1 4 7 13 3 23 4 1 0 24 1 4 1 0 4 1 3 1 3 42 0 12 0 1 1 0 2 2 0 0 0 17 3 1 1 0 34 1 12 6 2 77 2 31 2 1 2 15 1 6 2 1 1 17 0 1 3 0 1 3 2 0 0 0 16 0 4 0 0 6 1 3 0 0 1 17 2 2 2 0 29 2 12 3 71 1 29 5 2 2 9 41 0 12 3 0 18 30 104 Table 12: MQM error counts averaged across = 3 translators per highlight modality for every translation direction. description of MQM categories is available in Figure 6."
        },
        {
            "title": "DivEMT",
            "content": "QE4PE EnIt EnNl EnIt EnNl AP AU AP AU AP AU AP AU LOGPROBS (Fomicheva et al., 2020) PREDICTION DEPTH (Baldock et al., 2021) LOGPROBS MCD VAR (Fomicheva et al., 2020, Unsup.) 0.18 0.26 0.41 0.18 0.50 0.41 0.19 0.26 0.42 0.19 0.51 0. 0.10 0.17 0.23 0.09 0.23 0.23 0.09 0.19 0.31 0.09 0.27 0.31 XCOMET-XXL (Guerreiro et al., 2024b, Sup.) 0. 0.23 0.19 0.28 AVG. Oracle SINGLE TRANSLATOR - - - - 0.53 0.73 0.55 0. Table 13: Average Precision (AP) and Area Under the Precision-Recall Curve (AU) between metrics and error spans derived from human post-editing. We use mBART 1-to-50 (Tang et al., 2021) and NLLB 3B (NLLB Team et al., 2024) respectively for DivEMT and QE4PE. For DivEMT, single post-editor is available for computing the agreement, while for QE4PE we use consensus-based Oracle highlights. For QE4PE, we report the average agreement between individual oracle post-editors and their consensus as an agreement upper bound. Figure 9: ESA ratings for MT outputs and post-edits across domains and translation directions, showing comparable quality distributions for MT and post-edits across domains and directions. Figure 7: Top: Post-editing rate across highlight modalities, domains and directions. Bottom: Proportion of edits in highlighted spans across highlight modalities. *** = < 0.001, ** = < 0.01, * = < 0.05, ns = not significant with Bonferroni correction. Figure 8: Post-editing agreement across various modalities (Section 4.2). Results are averaged across all translator pairs for the two modalities (n = 3 intra-modality, = 9 inter-modality for every language) and all segments. Figure 10: Median ESA quality improvement following post-editing for segments at various initial MT quality levels across translators speed groups, showing no clear quality trends across editors productivity levels. Pre-task"
        },
        {
            "title": "Main task",
            "content": "Post-task Figure 11: Segment-level post-editing time with respect to post-editor progression. Values are medians across all annotators. Light gray area is min-max values, dark gray represents 25%-75% quantiles. The annotators do not became considerably faster with the task progression, likely due to the simplicity of the task and the high post-editing proficiency of professional post-editors. The high variability in editing times motivates the careful group assignments performed using PRE task edit logs. Pre-task"
        },
        {
            "title": "Main task",
            "content": "Post-task Figure 12: Editing proportion, measured by word error rate between MT and post-edited texts, with respect to post-editor progression. Values are medians across all post-editors."
        }
    ],
    "affiliations": [
        "CLCG, University of Groningen",
        "CSAI, Tilburg University",
        "ETH Zürich"
    ]
}