{
    "paper_title": "Optimized Minimal 3D Gaussian Splatting",
    "authors": [
        "Joo Chan Lee",
        "Jong Hwan Ko",
        "Eunbyung Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/."
        },
        {
            "title": "Start",
            "content": "Optimized Minimal 3D Gaussian Splatting Joo Chan Lee1 Jong Hwan Ko1(cid:66) Eunbyung Park2(cid:66) 1Sungkyunkwan University, 2Yonsei University 5 2 0 2 M 1 2 ] . [ 1 4 2 9 6 1 . 3 0 5 2 : r Figure 1. Our approach focuses on minimizing storage requirements while using only minimal number of Gaussian primitives. By proposing an efficient attribute representation, including sub-vector quantization, we achieve scene representations under 5 MB with 600+ FPS rendering. We visualize qualitative examples (left) and the rate-distortion curve evaluated on the Mip-NeRF 360 dataset (right). All rendering speeds were measured on an NVIDIA RTX 3090 GPU, with values in parentheses in the left visualizations measured using an NVIDIA RTX 4090 GPU."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has emerged as powerful representation for real-time, high-performance rendering, enabling wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on relatively large number of Gaussians, focusing primarily on attribute compression. This is because smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians efIn this pafectively rather than only optimizing storage. per, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose sub-vector quantization technique for improved irregularity representation, maintaining fast training with negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-ofthe-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/. 1. Introduction 3D Gaussian Splatting (3DGS) [26] has gained popularity for fast and photorealistic 3D scene reconstruction and rendering, offering compelling alternative to conventional methods. By leveraging tile-based parallelism to approx1 imate NeRFs [37] volumetric rendering, 3DGS enables significantly accelerated rendering while maintaining high visual quality. This has facilitated wide range of applications, such as dynamic scene reconstruction [55, 58], photorealistic avatar generation [38, 45], generative models [10, 53], and city-scale rendering [27, 46], demonstrating its versatility across various domains. 3DGS adjusts the number of Gaussian primitives during training by iteratively cloning or splitting Gaussians with high positional gradients while removing low-opacity Gaussians. However, this optimization process introduces substantial number of redundant Gaussians (over 3 million per 360 scenes [2]), leading to excessive storage requirements and computational overhead. To address this issue, various approaches have been proposed, including pruning based on rendering loss [29, 33] or importance score [12, 43] and optimized densification strategies [36]. Notably, several methods [12, 13, 61] reduce the number of Gaussians to around 0.5 million, enabling real-time rendering even on low-capacity GPUs while preserving rendering quality. Despite these efforts, reducing the number of Gaussians alone does not sufficiently mitigate storage overhead. Each Gaussian is parameterized by 59 learnable parameters, so even with reduced number of primitives, storage consumption remains substantial (e.g., 133 MB in Fig. 1). To address this, many works have explored compressing Gaussian attributes by leveraging vector quantization [11, 41], neural fields [50], sorting mechanisms [39], and entropy optimization [7, 54], demonstrating considerable improvements in reducing storage consumption. However, the aforementioned compression methods typically rely on large number of Gaussians (over one million). This is due to two major challenges when the number of Gaussians is drastically reduced: 1) each Gaussian needs to represent larger portion of the scene, making it more susceptible to compression loss, and 2) the increased spacing between Gaussians disrupts spatial locality, leading to higher attribute irregularity and posing challenges for entropy minimization and efficient compression. Since the number of Gaussians directly impacts computational costs, including training time and rendering speed, it is crucial to develop approaches that effectively minimize the number of Gaussians while maintaining compressibility. In this paper, we propose Optimized Minimal Gaussian representation (OMG), an efficient compression framework that operates with minimal number of primitives. To address the irregularity of sparse Gaussians and maximize the compressibility, we employ per-Gaussian features in novel way. Although the reduced number of Gaussians leads to decrease in local continuity, we can still leverage the spatial correlation associated with each Gaussians position. Therefore, we introduce lightweight neural field model with negligible parameters to capture the coarse spatial feature. This feature is integrated with the per-Gaussian features to represent each attribute, as shown in Fig. 2. This approach requires fewer per-Gaussian parameters than directly learning the original attributes, enabling more compact representation. While the proposed OMG architecture effectively represents sparse Gaussians, the use of per-Gaussian features impacts storage efficiency. To mitigate this, we introduce sub-vector quantization (SVQ, Fig. 3(c)), which splits the input vector into multiple subvectors and applies vector quantization to each sub-vector. This approach alleviates the computational overhead associated with large vector quantization codebooks (Fig. 3(a)) and reduces the storage burden caused by the multiple indexing stages of residual vector quantization (Fig. 3(b)), while maintaining high-precision representation. Finally, to retain only the minimal number of Gaussians, we introduce novel importance metric that evaluates each Gaussians local distinctiveness relative to its neighbors, identifying the most informative Gaussians. This metric is used alongside existing importance scoring methods based on blending weights from training views [12, 43], further reducing the number of Gaussians while preserving scene fidelity. Extensive experimental results demonstrate that OMG achieves 49% reduction in storage compared to the previous state-of-the-art method [50], requiring only 4.1MB for the Mip-NeRF 360 dataset [2] while preserving comparable rendering quality. Additionally, OMG utilizes only 0.4 million Gaussians, enabling 600+ FPS rendering. These results underscore the effectiveness of OMG in both compression efficiency and computational performance, demonstrating it as highly promising approach for 3D Gaussian Splatting representation. 2. Related Work 2.1. Neural Radiance Fields Neural Radiance Fields (NeRF) [37] introduced pioneering approach for novel view synthesis by leveraging volumetric rendering in conjunction with Multilayer Perceptrons (MLPs) to model continuous 3D scenes. While NeRF achieves high-quality rendering, its reliance on MLP leads to inefficiencies, particularly in terms of slow training and inference times. To overcome these limitations, the following methods [14, 31] utilized explicit voxel-based representations, enabling significantly faster training compared to traditional MLP-based NeRF models. However, these approaches still suffer from slow inference speeds and impose substantial memory requirements, posing challenges for scalability and practical deployment in large-scale environments. Compact representation. To mitigate the memory overhead while maintaining rendering fidelity, various works 2 have been introduced, including grid factorization [4, 5, 15, 16, 20], hash grids [6, 40], grid quantization [49, 52], and pruning-based strategies [47]. Nevertheless, achieving realtime rendering for complex, large-scale scenes remains formidable challenge. The fundamental limitation of these approaches stems from the necessity of dense volumetric sampling, which, despite optimizations, continues to constrain training and inference speed. 2.2. 3D Gaussian Splatting Recently, 3D Gaussian Splatting (3DGS) [26] has emerged as paradigm-shifting technique for real-time neural rendering by representing scene with 3D Gaussian primitives. 3DGS leverages customized CUDA kernels and optimized algorithms to achieve unparalleled rendering speed while preserving high image quality. Unlike volumetric methods that require dense per-ray sampling, 3DGS projects Gaussians onto the image plane and rasterizes them tile-wise, significantly improving computational efficiency. Due to its versatility, 3DGS has become dominant paradigm in 3D representation, leading to advancements across various domains and applications, such as mesh extraction [19, 22], simultaneous localization and mapping (SLAM) [25], dynamic scene representation [35], multi-resolution rendering [59], and further improvements in rendering quality [12]. However, 3DGS requires substantial number of Gaussians to maintain high-quality rendering. Furthermore, each primitive is represented with multiple attributes, such as covariance matrices and spherical harmonics (SH) coefficients, requiring large number of learnable parameters. Consequently, 3DGS demands substantial memory and storage resources, often exceeding 1GB per scene in high-resolution environments. Reducing the number of Primitives. To alleviate the substantial computational and memory overhead of 3DGS, numerous methods have been proposed to reduce the number of Gaussians while preserving rendering quality. Several approaches follow 3DGS by pruning low-opacity Gaussians, incorporated with opacity regularization [41], anchored Gaussians [34], or hyperparameter search [39]. An alternative approach utilized binary masking techniques [7, 29, 51, 54, 56], where pruning decisions are directly learned based on rendering loss. To optimize the binary masks, Compact-3DGS [41] initially adopted STE [3], while subsequent works [33, 62] employed Gumbel-Softmax. Another direction focuses on importance-based metrics to identify and remove redundant Gaussians. These methods primarily leverage each Gaussians blending weight contribution to rendering training-view images as measure of importance [11, 12, 17, 36, 43]. LightGaussian [11] further incorporates Gaussian volume and opacity into the importance computation, while Taming 3DGS [36] integrates multiple information, including gradients, pixel saliency, and Gaussian attributes. Building upon these advancements, we introduce novel importance metric that incorporates color distinction among neighboring Gaussians, enabling more effective selection of essential primitives. Attribute compression. Earlier methods employed conventional compression techniques such as scalar and vector quantization (VQ) [11, 17, 39, 41, 42, 44, 57] and entropy coding [7, 8, 29, 39, 42] to reduce storage requirements. VQ-based representations have proven highly efficient by the fact that many Gaussian attributes are redundant across scene, allowing for compact encoding. However, large codebook leads to substantial computational overhead, increasing training time. While residual vector quantization (R-VQ) [29] can alleviate computational costs, it introduces additional storage inefficiencies due to the need for multiple code indices. Another line of work explored structured representations, incorporating anchor-based encoding [7, 32, 34, 56] and factorization techniques [51], integrated with grid representations. Scaffold-GS [34] first introduced an anchorbased approach, where attributes of grouped Gaussians are encoded using shared anchor features and MLP-based refinements. Building upon this, subsequent methods [7, 9, 32, 56] incorporated context modeling to further improve compression rates. While showing high compression performance, these approaches require per-view processing, involving multiple MLP forward passes, which results in significant rendering latency. Recent efforts have utilized neural field architectures to exploit the local continuity of neighboring Gaussians. Compact-3DGS [29] encodes view-dependent color, while LocoGS [50] represents all Gaussian attributes except for view-independent color. However, unlike NeRF-based representations, where exact spatial positions are used as inputs to neural fields, mapping Gaussian center points to their corresponding attributes remains challenging. This difficulty leads to the use of large neural field models to achieve accurate reconstruction. In this work, we propose novel approach that effectively captures both the continuity and irregularities across Gaussians, enabling more efficient and compact attribute representation. 3. Method Background. 3DGS represents scene using set of Gaussians, parameterized by their attributes: center position RN 3, opacity [0, 1]N , 3D scale RN 3 + , 3D rotation represented as quaternion RN 4, and view-dependent color modeled using spherical harmonics (SH) coefficients h(0) RN 3 (0 degree for static color), h(1,2,3) RN 45 (1 to 3 degrees for view-dependent color). The covariance matrix of each Gaussian Σn R33 is determined by scale sn and rotation rn attributes. To render an image, 3D Gaussians are projected into 2D 3 Figure 2. The overall architecture of our proposed OMG. OMG learns per-Gaussian geometric and appearance features, applying SubVector Quantization (SVQ) to all of them. The SVQ-applied geometric attributes are used for rendering, while the space feature based on the Gaussian center position is integrated into the appearance features to define the final appearance. space. Each pixel color in the image C() is then rendered through the alpha composition using colors cn (determined by spherical harmonics under the given view direction) and the final opacity in 2D space αn(), C(x) = (x) (cid:88) k=1 ckαk(x) k1 (cid:89) j= (1 αj(x)), (1) αn(x) = on exp (cid:18) 1 (x n)T Σ1 (x n) (cid:19) , (2) where denotes pixel coordinate and Σ are the projected Gaussian covariance and center position. (x) represents the number of Gaussians around x, where the Gaussians are depth-sorted based on the given viewing direction. n, 3.1. Overall Architecture OMG is designed to accurately and efficiently represent the attributes of the minimal Gaussian primitives. Existing approaches [29, 50] have leveraged neural fields to exploit the local continuity of Gaussian attributes. However, as Gaussians become sparser, local continuity between them decreases. Unlike in dense representations, where smooth transitions between Gaussians can maintain fidelity, such transitions become less feasible in sparse settings. Especially for geometry, each Gaussian covers larger spatial region, requiring more specific scale and rotation to accurately capture structural details. Therefore, we retain the per-Gaussian parameterization for scale RN 3 and rotation RN 4 as in 3DGS. + For appearance, local continuity can still be maintained even with increased sparsity. However, unlike NeRF, where the query input is direct spatial point, mapping Gaussian center points to corresponding appearances is inherently challenging. This requires larger neural field model to maintain high fidelity. Conversely, entirely disregarding local continuity leads to an inefficient representation, 4 limiting the ability to capture meaningful spatial relationships. OMG addresses these challenges by integrating perGaussian attributes with neural field structure, effectively leveraging both irregularity and continuity. To represent appearance, we learn the static feature RN 3 and view-dependent feature RN 3, as perGaussian attributes. As illustrated in Fig. 2, each feature is concatenated with the space feature Fn, derived from each Gaussians center position, to generate static and viewdependent color, and opacity. The space feature itself is efficiently parameterized using positional encoding and an MLP, ensuring highly compact representation. Formally, this process can be expressed as follows: h(0) = MLPt (cid:0)cat(Tn, Fn)), on = MLPo (cid:0)cat(Tn, Fn)), (3) h(1,2,3) = MLPv (cid:0)cat(Vn, Fn)(cid:1), Fn = MLPs (cid:0)γ(pn)(cid:1), (4) cat(, ) denotes where concatenation function, the γ() represents the positional encoding function, and MLPt(), MLPo(), MLPv(), MLPs() are the MLPs for static color, opacity, view-dependent color, and space feature, respectively. 3.2. Sub-Vector Quantization Vector quantization (VQ) [18] has shown high efficiency for representing Gaussian attributes, capitalizing on their inherently vectorized structure and strong global coherence across an entire scene. However, to maintain high fidelity, large codebook size is required, inevitably resulting in substantial computational overhead and increased training complexity [60] (Fig. 3(a)). To address these issues, Residual Vector Quantization (R-VQ) [60] has been used as hierarchical quantization strategy [29], progressively refining representations while reducing the size of each individual codebook. However, as shown in Fig. 3(b), multiple code Figure 3. Conceptual diagram of (a) vector quantization, (b) residual vector quantization, and (c) sub-vector quantization. + and denote the element-wise summation and the vector concatenation. indices per attribute result in increased storage overhead, illustrating tradeoff between reducing per-codebook complexity and increasing overall storage requirements. To navigate this tradeoff, we propose Sub-Vector Quantization (SVQ), which partitions the attribute vector into multiple sub-vectors and applies vector quantization separately to each component (Fig. 3(c)), motivated by Product Quantization [24]. By reducing the dimensionality of each quantized unit, SVQ allows for smaller codebooks and more efficient lookups, which can balance computational cost and storage efficiency while maintaining high fidelity. We can apply SVQ to an input vector RM L, where and represent the total number of sub-vectors (partitions) and the sub-vector length, respectively. Each partition {1, ..., } has an independent codebook (m) RBL, where denotes the number of codewords per codebook. The codeword selection is based on the nearest match from (m), with (m)[j] representing the j-th codeword corresponding to the m-th sub-vector. More formally, SVQ-applied vector ˆz can be formulated as follows, ˆz := q(z; ) = cat(C (1)[i1], (2)[i2], ..., (M )[iM ]), im = arg min zm (m)[j]2 2, {1, ..., }, (5) (6) where q(z; ) denotes applying SVQ with sub-vectors and im {1, . . . , B} is the selected index of m-th subvector. SVQ ensures significantly reduced computation with small codebooks compared to VQ. We apply SVQ to geometric attributes sn, rn, resulting in quantized vectors ˆsn, ˆrn, which are then used for 3DGS rendering. For the appearance features Tn, Vn, we first concatenate them and apply SVQ. The resulting quantized features are then split back into two components ˆTn, ˆVn, which replace Tn and Vn in Eqs. (3) and (4). Although the reduced codebook size significantly decreases computational overhead compared to VQ, the process of updating both the indices and codes at every training iteration increases training time. Moreover, we observe that as training converges, the selected codebook indices remain largely unchanged. Therefore, we adopt fine-tuning strategy in the final 1K iterations: after initializing with Kmeans, we freeze the indices and finetune only the codebook using the rendering loss, without introducing any additional losses. Since K-means initialization is completed within seconds due to the small codebooks, this approach adds minimal additional training time, unlike other methods that incur significant overhead. 3.3. Local Distinctiveness for Important Scoring OMG adopts importance scoring to identify essential Gaussians and retain minimal number of them. Existing scoring-based pruning methods typically determine the importance of each Gaussian based on its blending weights (the values multiplied by ck in Eq. (1)) across training-view renderings. We use two factors as our baseline metric: (1) whether it has been the most dominant contributor for at least one ray [12, 13] and (2) its total blending weight contribution across all training rays [17, 43]. Formally, we define the base importance score as: Ii = (cid:40)(cid:80)N ρ=1 wi,ρ, 0, if ρ {1, ..., R}wi,ρ = maxj wj,ρ, otherwise, (7) where wi,ρ represents the blending weight of Gaussian for ray ρ and is the number of total rays in training views. While this score captures global importance, it does not account for redundancy among closely-positioned GausIn cases where multiple Gaussians are located in sians. close proximity, their blending weights tend to be highly similar, thus naively thresholding them can lead to two potential issues: (1) abrupt performance degradation when all similar Gaussians are simultaneously removed, and (2) redundancy when multiple Gaussians with near-identical contributions are retained. 5 Table 1. Quantitative results of OMG evaluated on the Mip-NeRF 360 dataset. Baseline results are sourced from the LocoGS [50] paper, where the rendering results were obtained using an NVIDIA RTX 3090 GPU. Our rendering performance was measured using the same GPU, with the values in parentheses obtained from an NVIDIA RTX 4090 GPU. We highlight the results among compression methods by coloring the best , second-best , and third-best performances."
        },
        {
            "title": "Method",
            "content": "3DGS Scaffold-GS [34] CompGS [41] Compact-3DGS [29] C3DGS [42] LightGaussian [11] EAGLES [17] SOG [39] HAC [7] LocoGS-S [50] LocoGS-L [50] OMG-XS OMG-M OMG-XL Mip-NeRF 360 PSNR SSIM LPIPS Size(MB) FPS 27.44 27.66 27.04 26.95 27.09 26.90 27.10 27.01 27.49 27.04 27.33 27.06 27.21 27.34 0.813 0. 0.804 0.797 0.802 0.800 0.807 0.800 0.807 0.806 0.814 0.807 0.814 0.819 0.218 0.223 0.243 0.244 0.237 0.240 0.234 0.226 0.236 0.232 0.219 0.243 0.229 0.218 822.6 187. 22.93 26.31 29.98 53.96 59.49 43.77 16.95 7.90 13.89 4.06 5.31 6.82 127 122 236 143 134 244 155 134 110 310 270 350 (612) 298 (511) 251 (416) To mitigate these issues, we propose incorporating local distinctiveness metric into the importance computation. Specifically, we introduce an additional term that measures the appearance (static) feature similarity among neighboring Gaussians, ensuring that distinct Gaussians show high importance. The final importance score is defined as: Gaussians. We have conducted simple post-processings after training: Applying 16-bit quantization to the position and compressing with G-PCC [48]. Huffman encoding [23] to SVQ indices. Storing all the components into single file with Ii = Ii 1 (cid:88) jN Ti Tj1 , (8) λ LZMA [1] compression. where denotes the set of K-nearest neighbors of Gaussian and λ is scaling factor that adjusts the sensitivity to appearance variation. As computing exact K-nearest neighbors for every Gaussian is computationally expensive, we approximate neighbor selection by sorting Gaussians in Morton order and selecting Gaussians with adjacent indices as their local neighbors. We remove low-importance Gaussians using CDF-based thresholding [30] with threshold τ . 4. Experiment 4.1. Implementation Details Following the previous works, we evaluated our approach on three real-world datasets, Mip-NeRF 360 [2], Tanks&Temples [28], and Deep Blending [21]. Our model is implemented upon Mini-Splatting [12], one of the methods achieving high performance with small number of 6 We provide five OMG variants (XS, X, M, L, XL), adjusting storage requirements. The only factor controlling the storage is the CDF-based threshold value τ of Gaussian importance, which is set to 0.96, 0.98, 0.99, 0.999, and 0.9999 for each variant, respectively. Further implementation details are provided in the supplementary materials. 4.2. Performance Evaluation Compression performance. Tabs. 1 and 2 compare the performance of OMG against various baseline methods on the Mip-NeRF 360, Tanks & Temples, and DeepBlending datasets. OMG consistently shows the smallest storage requirements while maintaining high performance across all datasets, achieving state-of-the-art (SOTA) results. Notably, on the Mip-NeRF 360 dataset, OMG-XS achieves nearly 50% reduction in storage compared to the small variant of LocoGS [50], the previous SOTA compression method, while retaining PSNR and SSIM. With over 30% reduced storage, OMG-M outperforms LocoGS-S in all quality metrics. Moreover, OMG-XL surpasses LocoGS-L in all metrics, even though requiring less storage than LocoGS-S. Table 2. Quantitative results of OMG evaluated on the Tanks&Temples and Deep Blending datasets. Baseline results are sourced from the LocoGS [50] paper, where the rendering results were obtained using an NVIDIA RTX 3090 GPU. Our rendering performance was measured using the same GPU, with the values in parentheses obtained from an NVIDIA RTX 4090 GPU. Method 3DGS [26] Scaffold-GS [34] CompGS [41] Compact-3DGS [29] C3DGS [42] LightGaussian [11] EAGLES [17] SOG [39] HAC [7] LocoGS-S [50] OMG-M OMG-L Tank&Temples Deep Blending PSNR SSIM LPIPS Size FPS PSNR SSIM LPIPS Size FPS 23.67 24. 23.29 23.33 23.52 23.32 23.14 23.54 24.08 23.63 23.52 23.60 0.844 0.855 0.835 0.831 0.837 0.829 0.833 0.833 0.846 0.847 0.842 0.846 0.179 0. 0.201 0.202 0.188 0.204 0.203 0.188 0.186 0.169 0.189 0.181 452.4 154.3 14.23 18.97 18.58 29.94 30.18 24.42 8.42 6.59 3.22 3.93 175 329 199 166 379 244 222 129 333 555 (887) 478 (770) 29.48 30.28 29.89 29.71 29.53 29.12 29.72 29.21 29.99 30.06 29.77 29.88 0.900 0. 0.907 0.901 0.899 0.895 0.906 0.891 0.902 0.904 0.908 0.910 0.246 0.243 0.253 0.257 0.254 0.262 0.249 0.271 0.268 0.249 0.253 0.247 692.5 121. 15.15 21.75 24.96 45.25 54.45 19.32 4.51 7.64 4.34 5.21 134 194 301 184 143 287 137 224 235 334 524 (894) 479 (810) The qualitative results presented in Fig. 4 also demonstrate the strong performance of OMG. Despite achieving over 100 compression compared to 3DGS, OMG maintains comparable visual quality. Especially, in the bicycle scene, OMG-XS achieves over 300 compression relative to 3DGS while accurately reconstructing details that 3DGS fails to represent, resulting in blurry area (highlighted in red) in its rendering. This superiority can be attributed to the blur split technique of our baseline model, minisplatting [12]. Despite reducing the number of Gaussians by an additional 20% compared to mini-splatting, OMG-XS retains high visual fidelity, demonstrating its effectiveness in extreme compression scenarios. Computational efficiency. OMG achieves remarkable efficiency alongside high performance. As shown in Tab. 3, OMG shows superior scene fidelity with significantly fewer Gaussian primitives compared to LocoGS. This reduction results in substantial rendering speed improvements of 13%, 67%, and 57% for the Mip-NeRF 360, Tank&Temples, and Deep Blending datasets (Tabs. 1 and 2), respectively, compared to LocoGS, highlighting its potential for real-time rendering on low-capacity devices. Furthermore, OMG accelerates training speed. The substantial improvement over LocoGS can be attributed to two key factors: the reduced number of Gaussians and the absence of large neural field. By efficiently exploiting coarse spatial information through tiny MLP, OMG achieves high computational efficiency. Table 3. Efficiency comparison of OMG variants compared to LocoGS evaluated on the Mip-NeRF 360 dataset. We present training time, the number of Gaussians, and the storage requirement with rendering quality. Method Training #Gauss Size PSNR SSIM LPIPS LocoGS-S LocoGS-L OMG-XS OMG-S OMG-M OMG-L OMG-XL 1h 20m 15s 20m 57s 21m 10s 21m 32s 22m 26s 1.09M 1.32M 13.89 7. 427K 501K 563K 696K 727K 4.06 4.75 5.31 6.52 6.82 27.04 27.33 27.06 27.14 27.21 27.28 27.34 0.806 0.814 0.807 0.811 0.814 0.818 0. 0.232 0.219 0.243 0.235 0.229 0.220 0.218 Table 4. Ablation study of OMG using the Mip-NeRF 360 dataset. We evaluate the contribution of the space feature integration and local distinctiveness (LD) scoring. Method PSNR SSIM LPIPS #Gauss Size OMG-M w/o Space feature w/o LD scoring w/o Both OMG-XS w/o Space feature w/o LD scoring w/o Both 27.21 26.96 27.09 26.81 27.06 26.85 26.83 26.52 0.814 0.811 0.813 0. 0.807 0.804 0.804 0.798 0.229 0.232 0.230 0.234 0.243 0.246 0.246 0.252 0.56M 5.31 0.59M 5.58 0.57M 5.36 0.59M 5.59 0.43M 4.06 0.44M 4.17 0.43M 4.12 0.45M 4.24 4.3. Ablation Study OMG architecture. OMG leverages highly compact neural field to capture coarse spatial information while reducing the number of learnable parameters per Gaussian. Tab. 4 validates the contribution of this space feature. Although the total number of Gaussians slightly increases, performance significantly degrades. The absence of spatial information introduces instability in attribute learning, hindering effective importance scoring. This trend is con7 Figure 4. Qualitative results of OMG compared to 3DGS. We provide per-image rendering PSNR with storage requirements for each scene. Table 5. Ablation study on SVQ using the Mip-NeRF 360 dataset. We substitute SVQ to VQ. Method Training #Gauss Size PSNR SSIM LPIPS 20m 15s OMG-XS SVQ VQ 21m 22s 427K 426K 4.06 3.99 27.06 26.97 0.807 0.805 0.243 0.245 sistently observed in both our small and medium models, highlighting the effectiveness of the space feature despite its minimal parameter overhead. Local distinctiveness scoring. OMG improves Gaussian pruning by incorporating local distinctiveness (LD) scoring into the importance estimation. Tab. 3 evaluates the impact of LD scoring, showing that despite similar number of Gaussians, the inclusion of LD scoring leads to significant performance improvement. This effect becomes even more pronounced when the target Gaussian number is lower, demonstrating that LD scoring provides an effective approach for further reducing sparse set of Gaussians. Furthermore, when both the space feature and LD scoring are removed, the model experiences the most substantial performance drop. This indicates that the two contributions are orthogonal, independently contributing to model efficiency and performance. Sub-vector quantization. To assess the impact of SVQ, we replaced it with VQ while maintaining the proposed clever training strategy, where K-means is performed only once before the final 1K iterations, after which only the codebook is updated. Despite this adjustment, VQ leads to 5% increase in training time, entirely due to K-means initialization. This is because large codebooks are inevitably required for accurate representation (here, we use 214 codes for scale and rotation and 213 codes for each appearance feature). Nevertheless, VQ results in decline in rendering quality, as shown in Tab. 5. To improve rendering quality, VQ requires an enlarged codebook, which is constrained by significant computational and memory overhead. Moreover, the increase in codebook size reduces storage efficiency when the number of Gaussians is small. In contrast, SVQ offers scalability of representation by flexibly adjusting the bit budget with sub-vector length in regard to the capacity-performance trade-off. 5. Conclusion In this paper, we proposed Optimized Minimal Gaussians (OMG), novel framework that significantly reduces the number of Gaussian primitives while maximizing compressibility and maintaining high rendering quality. By effectively identifying and preserving locally distinct Gaussians, OMG minimizes the redundancy of Gaussians with minimal loss of visual fidelity. Furthermore, our compact and precise attribute representation, combined with sub-vector quantization, enables efficient exploitation of both continuity and irregularity, ensuring high efficiency. 8 Experimental results demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art method while allowing over 600 FPS rendering performance. OMG sets new benchmark for highly efficient 3D scene representations, paving the way for future advancements in real-time rendering on resourceconstrained devices."
        },
        {
            "title": "References",
            "content": "[1] Lzma compression algorithm. https://www.7-zip. org/sdk.html. 6, 12 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54705479, 2022. 2, 6, 12, 13 [3] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic arXiv preprint neurons for conditional computation. arXiv:1308.3432, 2013. 3 [4] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J. Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometryaware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1612316133, 2022. 3 [5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and In European Hao Su. Tensorf: Tensorial radiance fields. Conference on Computer Vision, 2022. 3 [6] Yihang Chen, Qianyi Wu, Mehrtash Harandi, and Jianfei Cai. How far can we compress instant-ngp-based nerf? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2032120330, 2024. 3 [7] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In European Conference on Computer Vision, pages 422438, 2024. 2, 3, 6, 7 [8] Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Fast feedforward 3d gaussian splatting compression. In International Conference on Learning Representations, 2025. 3 [9] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac++: Towards 100x compression of 3d gaussian splatting. arXiv preprint arXiv:2501.12255, 2025. 3 [10] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. In Proceedings of Text-to-3d using gaussian splatting. the IEEE/CVF conference on computer vision and pattern recognition, pages 2140121412, 2024. [11] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. In Advances in Neural Information Processing Systems, pages 140138140158, 2024. 2, 3, 6, 7 [12] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In European Conference on Computer Vision, pages 165181, 2024. 2, 3, 5, 6, 7, 12 [13] Guangchi Fang and Bing Wang. Mini-splatting2: Building 360 scenes within minutes via aggressive gaussian densification. arXiv preprint arXiv:2411.12788, 2024. 2, 5 [14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55015510, 2022. 2 [15] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 3 [16] Quankai Gao, Qiangeng Xu, Hao Su, Ulrich Neumann, and In Zexiang Xu. Strivec: Sparse tri-vector radiance fields. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1756917579, 2023. 3 [17] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision, pages 5471, 2024. 3, 5, 6, [18] Robert Gray. Vector quantization. IEEE Assp Magazine, 1 (2):429, 1984. 4 [19] Antoine Guedon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstrucIn Proceedings of tion and high-quality mesh rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. 3 [20] Kang Han and Wei Xiang. Multiscale tensor decomposition and rendering equation encoding for view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42324241, 2023. 3 [21] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 6, 12, [22] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH, 2024. 3 [23] David Huffman. method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9): 10981101, 1952. 6, 12 [24] Herve Jegou, Matthijs Douze, and Cordelia Schmid. ProdIEEE transuct quantization for nearest neighbor search. actions on pattern analysis and machine intelligence, 33(1): 117128, 2010. 5 [25] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2135721366, 2024. 3 [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time 9 radiance field rendering. ACM Transactions on Graphics (ToG), 42(4):114, 2023. 1, 3, [27] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics (TOG), 43(4), 2024. 2 [28] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36 (4):113, 2017. 6, 12, 14 [29] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2171921728, 2024. 2, 3, 4, 6, 7 [30] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetric radiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42224231, 2023. 6 [31] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, pages 15651 15663, 2020. 2 [32] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 29362944, 2024. [33] Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, and Xiao Sun. Maskgaussian: Adaptive 3d gaussian representation from probabilistic masks. arXiv preprint arXiv:2412.20522, 2024. 2, 3 [34] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 3, 6, 7 [35] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In International Conference on 3D Vision (3DV), pages 800809, 2024. 3 [36] Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Markus Steinberger, Francisco Vicente Carrasco, and Fernando De La Torre. Taming 3dgs: High-quality radiance fields with limited resources. In SIGGRAPH Asia, 2024. 2, 3 [37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, page 405421, 2020. 2 [38] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, and Eduardo Perez-Pellitero. Human gaussian splatting: Real-time rendering of animatable avatars. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 788798, 2024. [39] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. Compact 3d scene representation via selforganizing gaussian grids. In European Conference on Computer Vision, pages 1834, 2024. 2, 3, 6, 7 [40] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. Graph., 41(4), 2022. 3 [41] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compgs: Smaller and faster gaussian splatting with vector quantizaIn European Conference on Computer Vision, pages tion. 330349, 2024. 2, 3, 6, 7 [42] Simon Niedermayr, Josef Stumpfegger, and Rudiger Westermann. Compressed 3d gaussian splatting for accelerated In Proceedings of the IEEE/CVF novel view synthesis. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1034910358, 2024. 3, 6, 7 [43] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900+ fps. arXiv preprint arXiv:2403.13806, 2024. 2, 3, 5 [44] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. Reducing the memory footprint of 3d gaussian splatting. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 7(1), 2024. 3 [45] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars In Proceedings of via deformable 3d gaussian splatting. the IEEE/CVF conference on computer vision and pattern recognition, pages 50205030, 2024. [46] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 2 [47] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, and Eunbyung Park. Masked wavelet In Prorepresentation for compact neural radiance fields. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2068020690, 2023. 3 [48] Sebastian Schwarz, Marius Preda, Vittorio Baroncini, Madhukar Budagavi, Pablo Cesar, Philip Chou, Robert Cohen, Maja Krivokuca, Sebastien Lasserre, Zhu Li, et al. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 9(1):133148, 2018. 6, 12 [49] Seungjoo Shin and Jaesik Park. Binary radiance fields. In Advances in Neural Information Processing Systems, pages 5591955931, 2023. 3 [50] Seungjoo Shin, Jaesik Park, and Sunghyun Cho. Localityaware gaussian compression for fast and high-quality rendering. In International Conference on Learning Representations, 2025. 2, 3, 4, 6, 7 [51] Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, and Eunbyung Park. F-3dgs: Factorized coIn ordinates and representations for 3d gaussian splatting. Proceedings of the 32nd ACM International Conference on Multimedia, pages 79577965, 2024. 3 [52] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Muller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable bitrate neural fields. In ACM SIGGRAPH 2022 Conference Proceedings, 2022. 3 [53] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In International Conference on Learning Representations, 2024. 2 [54] Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, and Zhibo Chen. End-to-end ratedistortion optimized 3d gaussian representation. In European Conference on Computer Vision, pages 7692. Springer, 2024. 2, 3 [55] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. 2 [56] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex Kot, and Bihan Wen. Contextgs : Compact 3d gaussian splatting with anchor level context model. In Advances in Neural Information Processing Systems, pages 5153251551, 2024. [57] Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, and Zhi Wang. Mesongs: Posttraining compression of 3d gaussians via efficient attribute In European Conference on Computer Vitransformation. sion, pages 434452, 2024. 3 [58] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In The Twelfth International Conference on Learning Representations, 2024. 2 [59] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1944719456, 2024. 3 [60] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. 4 [61] Yangming Zhang, Wenqi Jia, Wei Niu, and Miao Yin. Gaussianspa: An optimizing-sparsifying simplification framework for compact and high-quality 3d gaussian splatting. arXiv preprint arXiv:2411.06019, 2024. 2 [62] Zhaoliang Zhang, Tianchen Song, Yongjae Lee, Li Yang, Cheng Peng, Rama Chellappa, and Deliang Fan. Lp-3dgs: Learning to prune 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:122434122457, 2025."
        },
        {
            "title": "Appendix",
            "content": "6. Implementation Details All experiments were conducted using an NVIDIA RTX 4090. Our method was implemented within the minisplatting [12] framework and trained for 30K iterations. At the 20K iteration simplification process, local distinctiveness scoring was incorporated. Scale and rotation were trained from the initial training, while appearance features were introduced at 15K iterations. At this stage, the static features were initialized using the spherical harmonics DC coefficients trained until 15K iterations, whereas view-dependent features were initialized as zero vectors. From 29K iterations (last 1K iterations), SVQ (SubVector Quantization) was applied to per-Gaussian features. As mentioned in the paper, to enhance training efficiency, K-means clustering was performed once. The assigned indices based on K-means were fixed, and only the codebooks were optimized for the remaining 1K iterations. For SVQ, different bit allocations were assigned. Scale: length 1, 26 codes for each sub-vector Rotation: length 2, 29 codes for each sub-vector Appearance features: length 2, 210 codes for each subvector The length 1 SVQ applied to scale can be interpreted as scalar quantization, dynamically learning the quantization range with the codebooks. All codes in the codebook are stored with 16-FP precision. This SVQ configuration was commonly applied across all variants from XS to XL. The model storage for each variant was determined only by the importance score threshold τ , which is used for simplification at the 20K iteration, set to 0.96, 0.98, 0.99, 0.999, and 0.999, respectively. 7. Effect of Post-Processings As mentioned in the main paper, we applied the following two post-processing methods: Compressing the 16-bit quantized position with GPCC [48]. Huffman encoding [23] to SVQ indices and compressing the results with LZMA [1]. Both methods are applied losslessly, and we report the resulting storage changes in Tab. 6. When applied independently, G-PCC and Huffman encoding consistently reduce the total storage by 26-27% and 4-5% across all storage budgets, respectively. Applying both methods together also results in the overall storage reduction remaining consistent at approximately 30-32%. Table 6. Ablation study on the post-processing methods applied in OMG. G-PCC Huffman Size (MB) - - - - - - - - - - OMG-XS OMG-S - - - - OMG-M - - OMG-L - - - - OMG-XL 5.82 4.30 5.58 4. 6.83 5.04 6.54 4.75 7.66 5.64 7.33 5.31 9.47 6.92 9.08 6.52 9.89 7.25 9.46 6.82 Table 7. The average storage allocation for each component across OMG variants. Actual size refers to the total size of single file containing all components. Attribute Position Scale Rotation Appearance MLPs Total Actual size XS 0.93 0.83 0.87 1.39 0.03 4. 4.06 1.08 0.97 1.02 1.63 0.03 4.73 4.75 1.20 1.09 1.15 1.82 0.03 5.29 5.31 1.43 1.33 1.40 2.22 0.03 6. 6.52 XL 1.52 1.41 1.49 2.35 0.03 6.80 6.82 Tab. 7. Across all variants, OMG allocates approximately 20-25% of the total storage to position, scale, and rotation, while around 35% is dedicated to representing appearance attributes, including static and view-dependent color as well as opacity. The four MLPs for representing local continuity and aggregating appearance attributes exhibit negligible storage requirements, even without extra compression. 8. Storage Analysis 9. Per-scene Results We conducted experiments to analyze the storage requirements of OMG for representing each attribute, as shown in We report per-scene results in Tab. 8 (Mip-NeRF 360 [2]) and Tab. 9 (T&T [28] and DB [21]). 12 Table 8. Per-scene results evaluated on the Mip-NeRF 360 [2] dataset. Method Metric bicycle bonsai counter flowers garden kitchen room stump treehill Avg. OMG-XS OMG-S OMG-M OMG-L OMG-XL PSNR SSIM LPIPS Train #Gauss Size FPS PSNR SSIM LPIPS Train #Gauss Size FPS PSNR SSIM LPIPS Train #Gauss Size FPS PSNR SSIM LPIPS Train #Gauss Size FPS PSNR SSIM LPIPS Train #Gauss Size FPS 24.95 0.743 0.276 18:03 480772 4.61 682 25.08 0.750 0.264 19:01 573126 5.46 601 25.14 0.756 0.256 18:58 646191 6.15 25.24 0.762 0.241 19:25 813561 7.69 476 25.22 0.764 0.239 20:43 864124 8.15 430 30.90 0.932 0.202 20:30 263892 2.53 648 31.05 0.936 0.195 21:09 310096 2.94 585 31.06 0.938 0.190 21:01 350999 3.33 536 31.47 0.941 0.183 21:16 463285 4.32 31.51 0.942 0.182 21:54 450246 4.22 465 28.40 0.899 0.206 24:44 310056 2.95 433 28.56 0.903 0.199 25:19 360930 3.41 401 28.62 0.905 0.195 25:44 400442 3.76 371 28.66 0.907 0.189 26:06 480133 4.48 332 28.78 0.908 0.187 26:21 507473 4.72 21.32 0.596 0.368 19:18 543034 5.24 616 21.18 0.602 0.358 20:13 633607 6.10 555 21.40 0.606 0.351 20:35 708074 6.79 510 21.45 0.613 0.338 20:50 859963 8.23 422 21.52 0.614 0.334 22:09 922061 8.81 379 26.42 0.818 0.190 18:02 607254 5.65 26.56 0.826 0.177 18:41 691441 6.43 556 26.71 0.832 0.169 18:51 772338 7.18 522 26.83 0.837 0.160 19:14 909961 8.42 422 26.93 0.839 0.157 20:23 953050 8.82 422 30.81 0.919 0.137 23:45 356752 3.33 498 30.89 0.921 0.132 24:12 412126 3.83 31.05 0.923 0.129 24:18 454908 4.21 440 31.03 0.924 0.126 24:20 524457 4.82 405 31.15 0.925 0.126 24:56 547636 5.02 397 31.09 0.918 0.208 20:30 281236 2.67 648 31.20 0.922 0.201 21:38 338884 3.19 620 31.30 0.923 0.198 22:14 375520 3.53 31.26 0.926 0.191 22:05 524457 4.82 539 31.25 0.926 0.191 22:37 493754 4.58 512 27.00 0.788 0.247 17:49 523821 4.95 708 27.08 0.792 0.239 18:29 619734 5.83 601 27.06 0.794 0.233 18:31 704907 6.61 566 27.05 0.795 0.226 19:22 869388 8.14 27.00 0.796 0.224 20:22 920589 8.59 435 22.60 0.647 0.357 19:40 479520 4.64 658 22.64 0.650 0.347 19:55 573425 5.54 588 22.55 0.652 0.339 20:22 649157 6.24 525 22.57 0.653 0.329 21:14 819435 7.81 414 22.69 0.655 0.324 22:33 885229 8.44 27.06 0.807 0.243 20:15 427371 4.06 612 27.14 0.811 0.235 20:57 501485 4.75 552 27.21 0.814 0.229 21:10 562504 5.31 511 27.28 0.818 0.220 21:32 696071 6.52 441 27.34 0.819 0.218 22:26 727129 6.82 416 Table 9. Per-scene results evaluated on the Tank&Temples [28] and Deep Blending [21] datasets. Method Metric Tank&Temples Deep Blending Train Truck Avg. drjohnson Playroom Avg. OMG-M OMG-L PSNR SSIM LPIPS Train #Gauss Size FPS PSNR SSIM LPIPS Train #Gauss Size FPS 21.78 0.806 0.233 12:12 303187 2.95 861 21.85 0.811 0.225 12:12 369440 3.58 760 25.25 0.878 0.144 11:30 257649 3.49 913 25.36 0.881 0.136 11:39 442359 4.28 23.52 0.842 0.189 11:51 330418 3.22 887 23.60 0.846 0.181 11:56 405900 3.93 770 29.37 0.905 0.253 17:18 520385 4.87 829 29.44 0.907 0.247 17:39 627868 5.86 745 30.18 0.910 0.253 14:51 404237 3.82 959 30.32 0.912 0.247 14:58 485329 4.55 29.77 0.908 0.253 16:05 462311 4.34 894 29.88 0.910 0.247 16:19 556599 5.21"
        }
    ],
    "affiliations": [
        "Sungkyunkwan University",
        "Yonsei University"
    ]
}