{
    "paper_title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "authors": [
        "Ziyang Wang",
        "Honglu Zhou",
        "Shijie Wang",
        "Junnan Li",
        "Caiming Xiong",
        "Silvio Savarese",
        "Mohit Bansal",
        "Michael S. Ryoo",
        "Juan Carlos Niebles"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 7 7 5 0 . 2 1 5 2 : r Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding Ziyang Wang1,2* Honglu Zhou1 Shijie Wang1 Junnan Li1 Caiming Xiong1 Silvio Savarese1 Mohit Bansal2 Michael S. Ryoo1 Juan Carlos Niebles1 1Salesforce AI Research 2University of North Carolina at Chapel Hill https://activevideoperception.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative planobservereflect process with MLLM agents. In each round, planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens. 1. Introduction From streaming platforms to TV programs, video has become primary medium for capturing and conveying information. However, long video understanding (LVU) remains challenging because it demands the ability to localize and *Work done during internship at Salesforce. Figure 1. Motivation of Active Video Perception. Prior methods follow passive perception paradigm which leverage queryagonistic captioner to perceive the video information, leading to low efficiency and imprecise visual grounding. Instead, we actively perceive query-relevant content by treating the long video as an interactive environment to be explored in goal-directed manner. integrate sparse, temporally dispersed cues across long time spans. Although recent multimodal large language models (MLLMs) [3, 2224, 35, 51, 53, 59] substantially improve visual recognition, naively applying them to densely sampled, full-length videos is both computationally costly and brittle for complex queries: most video tokens are redundant, while the brief, localized evidence that actually matters is diluted or overlooked in the long sequence. These limitations have motivated recent surge of agentic approaches for long video understanding [60, 63, 72, 75]. 1 Rather than treating the video as single monolithic input, these methods use LLMs to orchestrate perception and reasoning over the video through planning. However, recent leading methods [45, 79, 85] still rely on captioners to convert visual information into text space as the primary interface for LLM reasoning and tool calling. This caption-based framework leverages LLMs strengths in text processing but introduces two inherent limitations: 1. High Computational Cost: Query-agnostic captioning generates large amounts of irrelevant information, expending computation on unrelated content and resulting in low efficiency. 2. Imprecise Grounding via Captions: Existing approaches use captions to localize key events, which may discard fine-grained temporal and spatial cues and weaken causal tracing. These limitations underscore the need for an agentic framework that adaptively focuses on informative video regions, seeks query-related evidence directly over video pixels while maintaining high efficiency. We take inspiration from how humans inspect long videos: we do not need to watch every frame; instead, we plan our observation based on the query. For instance, given question about specific plot, we first skim the video for coarse cues (plot localization), then take targeted observation by focusing on the key video regions for detail clues. Active perception theory [1, 4, 5] formalizes this behavior: An agent is an active perceiver if it knows why it wishes to sense, and then chooses what to perceive, and determines how, when and where to achieve that perception. Even though active perception concept is mainly used in robotics domain [43, 48, 67], We argue that agentic LVU frameworks can similarly benefit from query-driven, temporally grounded observation that decides what, when, and where to look, while continually assessing whether the accumulated evidence is sufficient for the query or whether further observation is required. Building on this view, we propose Active Video Perception (AVP), an agentic evidence seeking framework for long video understanding. As shown in Fig. 1, rather than passively perceiving the video by captioning, AVP treats the video as an interactive environment and actively decides what/where/how to observe the video to acquire the queryrelated information. This targeted observation design allows AVP to focus on the key informative segments, avoid redundant processing over static or irrelevant content, ultimately improving both efficiency and reliability on complex longhorizon queries. Since complex queries often depend on sparse or ambiguous cues that cannot be resolved in single round of observation, AVP adopts an iterative planobservereflect process with MLLM agents. In each round, planner proposes targeted interactions with the video by deciding what to inspect, where to focus, and at what granularity. Then, observer executes these plans to extract compact, time-stamped evidence. Finally, reflector evaluates the query-sufficiency of the extracted evidence and decide whether additional round of observation is needed. If the extracted evidence is insufficient, it appends the current plan, evidence, and justification to the running history to guide the planner in deciding the next plan. This closed-loop process enables AVP to progressively refine its focus, revisit uncertain moments, and allocate computation adaptively, leading to more efficient processing and reliable reasoning on long, complex videos. We demonstrate the effectiveness and efficiency of AVP by evaluating it on five long video understanding benchmarks, including MINERVA [33], LVBench [68], VideoMME [15], MLVU [81] and LongVideoBench [66]. Compared to the existing agentic approaches, AVP attains higher accuracy while using substantially less compute by formulating LVU as goal-conditioned observations. Specifically, compared to the leading agentic method DeepVideoDiscovery (DVD) [79], AVP achieves an average accuracy gain of 5.7%. Whats more, on LVBench, AVP achieves better performance while only consuming 18.4% inference time and 12.4% input tokens compared to DVD, validating the efficiency of AVP. We further conduct extensive ablation studies that highlight and justify the key design choices of AVP. 2. Related Work Long Video Understanding The advancement in long video understanding (LVU) benchmarks [7, 15, 66, 68, 81] has extended video reasoning problem from short clips to realistic scenarios, involving multi-minute or hour-long videos. To address this, previous video-specific MLLMs [26, 41, 42, 54, 56, 80] mainly focus on the challenge of excessive token inputs by extending the context length [9, 78], reducing the video tokens [25, 44, 47, 61] or keyframe selection [2, 6, 49, 57, 70, 71, 73, 84]. Notably, VAP [29] also introduce the concept of action perception to LVU task, they treats key frame selection as data acquisition in active perception and leverages lightweight text-conditioned video generation model to represent prior world knowledge. Instead, AVP treats LVU as query-driven evidence seeking in video environments. As result, AVP tackles complex LVU task by focus perception in key regions, achieves significantly better efficiency. Recently, inspired by the great success of DeekSeekR1 [10], several works [14, 55, 62, 65] explore the Chainof-thoughts video reasoning model. Later works [16 18, 38, 50, 58, 74, 77] explore the idea of Thinking with Video, which incorporate visual CoT strategy to conduct coarse-to-fine video exploration. Compared to these methods, AVP has two clear advantages: (1) query-adaptive, previous work mainly follows coarse-to-fine schema with fixed 2 FPS/resolution setup, instead, AVP decides what/where/how to observe the video based on the query; (2) training-free, instead of generating large-scale training samples with reasoning trace, we directly employ an agentic approach and significantly reduce compute cost. Agentic Frameworks for Long Video Understanding To decouple the complex LVU task, early agentic frameworks [12, 19, 20, 30, 40, 63, 64, 75, 76] adopt captionerLLM design: video segments are converted into captions, which an LLM then uses the generated caption to answer the video query. Meanwhile, several works [13, 21, 27, 31, 32, 46, 83] utilize the idea of visual programming, decompose the complex query into multiple steps to leverage expert modules. Reflection-based frameworks [8, 82] add verification agent after the initial answering process to refine the reasoning. . Building on these works, recent studies [8, 11, 28, 39, 45, 69, 79, 85] aim to improve evidence retrieval and reasoning efficiency in text space. Notably, VGent [45] constructs caption-based graph to enable longrange retrieval and relational reasoning across segments. VideoLucy [85]introduces memory backtracking mechanism that allows the model to revisit earlier multi-scale text captions during multi-step reasoning. Deep Video Discovery [79] uses tool-based search to iteratively refine textual evidence over long videos. Instead of relying on captioners, AVP reasons directly over visual inputs through an iterative planobservereflect process, selectively watching only what the query requires and maintaining compact evidence record. This active, iterative video observation design preserves fine-grained grounding while avoiding the redundancy and overhead of caption-based LVU pipelines. 3. Method We present Active Video Perception (AVP), an iterative evidence seeking framework for agentic LVU. AVP is inspired by the concept of active perception [1, 4, 5], which argues complete artificial agent necessarily must include the ability of knowing why it wishes to sense, and then choosing what to perceive, and determining how, when and where to achieve that perception. Through the lens of active perception, we formulate LVU task as query-driven evidence seeking in video environments, where the LVU agent iteratively decides what, where and how to interact with the video to find the key evidence based on previous observation. Concretely, as shown in Fig. 2, given query and video , AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, planner first proposes observation plan by choosing what to inspect, where to focus, and how to sample. An observe agent executes that plan to extract compact, time-stamped evidence by observing the video purposefully. reflector verifies evidence against the query to estimate the confidence; if it exceeds the confidence Figure 2. Framework of Active Video Perception (AVP). AVP operates by an iterative plan-observe-reflect process with MLLM agents. At each round, the planner decide what/where/how to interact with the video, the observer extract structured query-related evidence by executing the plan and the reflector evaluates the extracted evidence to decide whether an additional round is need. threshold, AVP outputs the answer and stops, otherwise it returns justification to guide the next round of observation planning. We iterate this process until either sufficiently confident answer is obtained or the round limit is reached. We introduce each component in detail as follow. 3.1. Query-Conditioned Action Planning Inspired by active perception concept, instead of passively processing frames uniformly or converting into caption list, AVP first plans deciding what, where, and how to observe the long video to obtain the query-related evidence. Specifically, AVP leverage planner (PLANNER) to decide what to look for, where to look, and how to observe in order to solve the give query. Initial Plan. At round r=1, given query and video , the PLANNER instantiates concrete observation specification that states what to observe, the region to inspect, and how to sample. We initialize (1) PLANNER.INIT(Q), 3 and represent it as (1) = (cid:0)what(1), where(1), how(1)(cid:1). what is brief, query-conditioned instruction naming the key evidence to seek (e.g., locate the moment the coach enters, determine who hands over the box, verify the scoreboard change). For complex query which requires multi-step reasoning, we prompt the PLANNER to first plan the initial observation and leave the following steps in the next rounds. By decomposing the complex queries, AVP achieves better handling in multi-hop reasoning and temporally dispersed evidence seeking. where is targeted temporal region [ts, te]. It is seeded from: (i) explicit timestamps in (e.g., 1:001:30), (ii) soft textual cues (opening scene, final minutes). When no prior is available, we first sweep the entire video at low cost (low fps and spatial_res) to gather coarse evidence. Across rounds, this region can be tightened or shifted based on the Reflectors feedback, enabling coarseto-fine localization without dense scanning. how specifies sampling granularity for the long video observation, where how = (fps, spatial_res). The PLANNER determines the granularity of the targeted evidence and accordingly decides the sampling strategy. By default, it adopts coarse settings (lower fps and spatial_res) to perform low-cost exploration across the video and quickly identify potential evidence regions. When finer details are requiredsuch as subtle object interactions or small spatial cues, the PLANNER increases sampling density to ensure more precise perception. This adaptive design allows AVP to allocate computation efficiently across granularities while maintaining high fidelity. The resulting serves as compact, executable target observation that guides the Observer on what to looking for, which region of the video to inspect, and how to sample for efficient, query-focused observation. 3.2. Targeted Video Observation Once the plan is generated, the observer (OBSERVER, MLLM) executes the plan to gather detailed, time-stamped evidence from the video. Specifically, in round r, given the plan (r) = (what(r), where(r), how(r)), the OBSERVER inputs the the query and instruction in what(r), the video segment defined by the temporal where and uses the sampling strategy in how (fps and spatial resolution). Instead of generating free-form text responses, the OBSERVER is prompted to produce structured, timestamp-aware evidence text in the form of {([starti, endi], di)}N i=1, where each di is concise, query-conditioned description of the visual event within the time interval [starti, endi]. Specifically, we maintain an evidence list that accumulates evidence across rounds. At each round r, the OBSERVER generates new evidence Er and append it to the cumulative evidence list: Er = OBSERVER(V, Q, (r)), Er. This cumulative evidence list serves as the working memory of AVP, allowing the reflector to assess sufficiency based on all past evidence and guiding the PLANNERs subsequent updates. Compared with free-form captioning, this design yields more stable, query-relevant evidence and leads to better grounded reasoning over long videos. This targeted video observation design allows AVP to perceive only the most query-relevant portions of the video, keeping it efficient and avoiding redundant or irrelevant information. 3.3. Evidence Reflection and Re-Planning After each observation round, AVP employs reflector (REFLECTOR) to evaluate the sufficiency of the accumulated evidence and decide whether additional observation is required. The REFLECTOR verifies how well the collected evidence supports an answer, and when confidence is insufficient, it provides feedback for the next round of planning. Evidence Reflection. At round r, given the query and the current cumulative evidence list E, the REFLECTOR jointly produces query confidence score (r)and justification (r): (C (r), (r)) = REFLECTOR(Q, E), where (r) [0, 1] measures the confidence in evidence sufficiency to answer the given query, and (r) specifies which answer the current evidence supports or what information is still missing. If the confidence is higher than the confidence threshold τconf, the REFLECTOR directly extracts the final answer from (r); otherwise, the justification highlights missing or uncertain cues to guide the next round of planning step. History Update and Re-Planning. When confidence remains below the threshold, the Reflector appends the current observation and justification to the running history H. The history provides the PLANNER with concise summary of what has been inspected, verified, or left unresolved. The PLANNER then refines its next plan using this feedback: (r+1) = PLANNER.REPLAN(Q, H, (r)), shifting attention toward the regions, entities, or temporal spans identified as uncertain by the Reflector. By iteratively running the plan-observe-reflect process, AVP forms closed-loop perceptionreasoning cycle that 4 Algorithm 1: Active Video Perception(AVP) Inputs :Video , Query Q, Max Rounds Rmax, Confidence Threshold τconf Output :Answer A, Justification J, Evidence List E, History 1 (1) PLANNER.INIT(Q); [ ]; [ ] 2 for 1 to Rmax do 3 E(r) OBSERVER(V, Q, (r)) E(r) (C (r), (r)) REFLECTOR(Q, E) if (r) τconf then REFLECTOR.EXTRACTANSWER(J (r)) return A, (r), E, if = Rmax then REFLECTOR.FORCEANSWER(Q, E) return A, (r), E, {(P (r), E(r), (r))} (r+1) PLANNER.REPLAN(Q, H, (r)) 4 5 7 8 9 10 11 13 // Init plan, evidence list and history // accumulate this rounds evidence // evidence reflection: confidence & justification // answer is entailed by justification // force to give answer on final round // append plan & evidence & justification to history // re-plan for additional observation continuously refines its focus until the gathered evidence becomes sufficient. This iterative design allows the system to adaptively reason over long videos, reducing computation, and maintaining grounded, query-aligned understanding. We present full algorithm in Algorithm 1. 4. Experimental Setup 4.1. Datasets We evaluate AVP on five diverse long video understanding benchmarks: (1) MINERVA [33] is recent challenging video reasoning benchmark consisting of 1515 hand-crafted questions. The average video duration is 12 minutes. (2) LV-Bench [68] is benchmark specifically designed for long video understanding which includes 1549 multiplechoice questions across 103 hour-long videos. (2) MLVU [81] is multi-task Long Video Understanding Benchmark for the comprehensive and in-depth evaluation of LVU. We use the multiple-choice QA samples from the MLVU test split, containing 2175 video QA samples with more than 15 minutes average video duration. (4) Video MME [15] is comprehensive evaluation benchmark for video analysis from short to long videos (average min for long split video). We use the standard split of VideoMME, which contains 2700 samples designed for both perception and reasoning tasks (900 samples with 41min average duration for the long split). (5) LongVideoBench (LVB) [66] is video QA benchmark that highlights referred reasoning questions, which are dependent on long frame inputs. We test on the public validation split, which contains 1337 video reasoning questions (533 samples with 15-60 min video for long split). 4.2. Evaluation Metrics We evaluate AVP under the multiple-choice QA setting. We use standard accuracy metrics for all experiments. We do not include auxiliary subtitle for all benchmarks. 4.3. Implementation Details We adopt Gemini-2.5-Pro1 [51] as our default MLLM agent for all components. We also provide the results with lightweight Gemini-2.5-Flash model in Tab. 1 and Tab. 4. We provided more ablation with different backbone models (including open-source models) in appendix. For fair comparison, we fix the max input token as 128K. If the input video (region) exceeds this budget, we uniformly sample the max frames that within the token limit. For spatial token setup (spatial_res), we follow Geminis MediaResolution setup to have 2 scale (low, medium, high), while low and medium is 66 and 258 tokens per frame, respectively. We set the max rounds Rmax as 3 and confidence threshold τconf as 0.7. We provide additional analysis for the design choices in Sec. 5.2.2. We provided more implementation details (including detailed prompts) and analysis in appendix. 5. Results 5.1. Main Results on Long Video Benchmarks Tab. 1 presents comprehensive comparison of AVP against existing general-purpose MLLMs [34, 36, 51, 53], videospecific MLLMs [17, 44, 61, 65], and agentic video frameworks [8, 45, 60, 63, 76, 79, 85] across five video understanding benchmarks: MINERVA [33], LVBench [68], MLVU [81], Video-MME [15] and LongVideoBench [66]. 12025-06-17 version 5 Methods General-Purpose MLLMs Seed-1.5-VL [53] Qwen-3-VL [52] GPT-4o [34] GPT-4.1 [36] Gemini-2.5-Flash [51] Gemini-2.5-Pro [51] Video-Specific MLLMs LongVU [44] AdaReTaKe [61] Video-RTS [65] FrameMind [17] Agentic Video Frameworks VideoAgent [60] VideoTree [63] SiLVR [76] VideoLucy [85] Vgent [45] LVAgent [8] DeepVideoDiscovery (DVD) [79] Active Video Perception (Ours) AVP Gemini-2.5-Flash AVP Gemini-2.5-Pro MINERVA LVBench MLVU Video-MME LongVideoBench Overall Overall Test Overall Long Val Long - - 45.5 54.0 54.6 61.8 - - 37.8 - - 40.2 44.4 - - - - 64.6 67.7 48.9 63.4 56.7 67.4 - 53.3 43.2 - 29.3 28.8 - 58.8 - - 74. 82.1 84.3 54.9 - 72.4 79.6 65.4 78.1 - 48.6 64.4 60.4 45.2 76.1 72.1 83.9 - 77.9 79.2 71.9 72.0 74.2 82.4 60.6 73.5 63.0 60.9 - 60.6 74.1 72.5 68.9 81.7 - - - 65.3 - 69.1 77.6 59.5 65.0 54.1 57.5 46.4 54.2 77.7 66.8 - 74.3 67.3 74.4 - 66.7 - 66.2 69.8 - 67.0 56.6 - - - - - 59.7 80.0 71. - - 60.9 - 61.8 66.6 - - 52.2 - - - - - - - 68.6 56.9 (+2.3) 65.6 (+3.8) 63.8 (+7.1) 74.8 (+7.4) 74.1 (+1.7) 84.3 (+4.7) 81.2 (+7.0) 85.3 (+2.9) 76.7 (+7.6) 81.9 (+4.3) 70.2 (+4.0) 73.4 (+3.6) 65.5 (+3.7) 70.0 (+3.4) Table 1. Comparison with general-purpose MLLMs, Video-specific MLLMs, and agentic video frameworks on five long video understanding benchmarks (MINERVA, LVBench, MLVU, Video-MME, LongVideoBench). We bold the best and underline the second-best result in each column. Results shows that AVP achieves best performance on all datasets across different baselines, achieving significant improvements on its backbone model (in blue) across all benchmark. We gray out the results that use auxiliary subtitle information. Comparison with MLLMs. Among general-purpose multimodal LLMs, proprietary systems such as Gemini-2.5-Pro [51] and Seed-1.5-VL [53] achieve strong overall results but still fall short of our proposed AVP. In particular, AVP (w/ Gemini-2.5-Pro) surpasses the state-of-the-art Gemini-2.5Pro model [51] by 4.5% average accuracy over all benchmarks, demonstrating that direct inference over full length remains insufficient for complex, long-horizon queries that require targeted evidence seeking. AVP (w/ Gemini-2.5Flash) also outperforms its backbone by 4.4%, showing generalization ability of the proposed framework in weaker backbone MLLMs. Meanwhile, AVP significantly outperforms the video-specific MLLMs, including compression-based methods [44, 61] and (visual) Chain-of-Thoughts methods [17, 65]. This result highlights the active perception concept for long video understanding and encourages future research. SiLVR [76], VideoLucy [85], LVAgent [8] and DeepVideoDiscovery (DVD) [79]. We find that AVP achieves best performance against all baseline methods and significant improvement compared to the backbone model in all benchmarks. Comparing to the recent VideoLucy and DVD methods, AVP achieves 10.5% and 5.7% average improvements while both using strong LLM backbones (DeepSeekR1 [10] for VideoLucy, and OpenAI-o3 [37] for DVD). We also compared the efficiency in term of inference time with DVD in Tab. 2, showing AVP is not only more performant, but also significantly efficient. These results validate the effectiveness of active perception for long video understanding : rather than passively encoding frames, AVP plans what to observe, observes purposefully, and reflects adaptively, leading to higher accuracy and greater efficiency than both MLLMs and recent agentic frameworks. Comparison with Agentic Frameworks. Within the class of agentic video reasoning systems, AVP consistently achieves the best (or second-best) results across all benchmarks. We compare AVP with six recent agentic video frameworks, including VideoAgent [60], VideoTree [63], 5.2. Quantitative Analysis In this section, we analyze different aspect of AVP, including efficiency analysis, ablation study on different design choices. We provided more quantitative analysis in the appendix. 6 Method Avg. Inference Time (s) Avg. Input Tokens (K) Acc PLANNER OBSERVER REFLECTOR MINERVA LVBench DVD AVP (Ours) 790.5 145.3 1071.6 132.5 74.2 74. Table 2. Efficiency comparison on LVBench. We report average inference time in seconds, average input token count, and accuracy. By actively querying the video rather than passively captioning all clips, AVP achieves better overall efficiency and accuracy."
        },
        {
            "title": "MINERVA LVBench",
            "content": "Observer (Baseline) Planner + Observer Planner + Observer + Reflector (AVP) 60.8 63.9 65.6 67.4 72.6 74.8 Table 3. Component ablation of AVP. Adding the Planner and then the Reflector on top of the Observer baseline consistently improves MINERVA and LVBench accuracy, showing that queryconditioned planning and reflection are key to AVPs performance. 5.2.1. Efficiency Analysis As shown in Tab. 2, we evaluate inference efficiency on LVBench in terms of average runtime, average input token count, and accuracy. DVD [79] requires 790.5s per video and processes on average 1.07M tokens. Notably, finer breakdown shows that its captioning stage alone takes 637.2s and consumes roughly 0.9M tokens. In contrast, AVP eliminates this query-agnostic captioning stage and performs only targeted query reasoning, reducing inference time to 145.3s, achieving 5.44 faster (81.6% reduction). Meanwhile, AVP only consumes 12.4% of the input tokens compared to DVD while improving the LVBench accuracy. These results indicate that actively deciding what, where, and how to observe not only removes redundant caption processing but also strengthens reasoning by concentrating computation on query-relevant content. 5.2.2. Ablation Study AVP Components. We conduct step-wise ablation to assess the contribution of each component in AVP. As shown in Tab. 3, introducing the PLANNER notably improves both MINERVA and LVBench accuracy, demonstrating the benefit of query-conditioned multi-step exploration over static observation. The PLANNER guides the agent to allocate computation toward potentially informative regions rather than processing frames uniformly. Adding the REFLECTOR yields further performance gain, confirming that iterative process enhances reasoning trustworthy. Together, these results highlight that active perception, planning what to observe and reflecting on what has been seen substantially strengthens long video understanding. Model Selection. Table 4 examines the impact of varying the model selection across Planner, OBSERVER, and RE2.5-Flash 2.5-Pro 2.5-Flash 2.5-Pro 2.5-Flash 2.5-Flash 2.5-Pro 2.5-Pro 2.5-Flash 2.5-Pro 2.5-Flash 2.5-Pro 56.9 60.2 63.6 65.6 63.8 67.6 71.8 74.8 Table 4. Agent MLLM selection within AVP. We vary Gemini-2.5 Flash/Pro backbones for the PLANNER, OBSERVER, and REFLECTOR, stronger components consistently improve performance on both benchmarks. Max Rounds MINERVA LVBench 1 2 3 5 63.9 65.0 65.6 65.5 72.6 74.6 74.8 74.6 Table 5. Ablation on max round limit. Increasing the number of max round limit improves performance on both benchmarks and gets best results by three rounds, indicating that only few interaction steps are sufficient. FLECTOR within AVP under Gemini-2.5 [51] family (we add additional model ablation in supp.). We observe that both benchmarks benefit from stronger components, but their sensitivities differ. On MINERVA, which features complex, multi-hop reasoning queries, performance improves substantially with stronger Planner and Reflector models, indicating that strategic planning and reflective consolidation are crucial for handling compositional reasoning. In contrast, LVBench, characterized by extremely long videos, relies more heavily on robust Observer, the component directly responsible for navigating and gathering evidence efficiently from vast temporal spans. The best configuration employs powerful models across all three modules, confirming that AVPs active perception design yields synergistic gains in both reasoning depth and temporal scalability. Max Round Limit. Table 5 studies how the number of PlanObserveReflect rounds affects performance. Both MINERVA and LVBench show steady gains from one to three rounds, confirming that iterative reasoning enables AVP to progressively refine its evidence set and improve decision confidence. The improvement is more pronounced on MINERVA, where multi-hop reasoning benefits from repeated reflection and targeted re-observation. Beyond three rounds, performance saturates, suggesting that AVP has already acquired sufficient evidence and additional cycles bring limited benefit. This result validates the efficiency of our design, AVP achieves strong reasoning capability with only few lightweight interaction rounds. 7 Figure 3. Qualitative example of AVP. Given multiple-choice query about the Tombstone monuments first on-screen appearance, Round 1 performs coarse scan of the entire video (0.5 FPS, low resolution) and localizes candidate interval [1:00, 1:10], but the REFLECTOR judges the evidence insufficient. Round 2 re-plans targeted pass over this window (2 FPS, medium resolution), enabling the OBSERVER to localize the monument in the upper-left background and the REFLECTOR to confidently select the correct answer (option D) and halt. 5.3. Visualization the next round) and failure case in appendix. In Fig. 3, we illustrate how AVP acquires and verifies evidence through multi-round PlanObserveReflect loop on long video. Given the query, In the clip where the German woman is introduced by the narrator, where can the Tombstone monument be initially seen on screen?, Round 1 uses coarse, uniform sweep to localize candidate moments (0.5 FPS, low resolution). This pass narrows the search to the [1:00, 1:10] interval but the reflector flags the observations as insufficient due to lack of detail, prompting refined followup (Round 2). In Round 2, the planner schedules targeted revisit over [1:00, 1:10] at 2 FPS with medium resolution, and the observer extracts query-relevant cues: the Tombstone monument appears as small, conical structure on hill in the upper-left background while the German couple stands in the mid-ground. The evidence list is now sufficient for the reflector to stop and produce the final answer, demonstrating AVPs coarse-to-fine scheduling, evidence-grounded verification. We provided additional visualization samples with different scenario (start with grounded video region from query prior and refine the region based on the observation in 6. Conclusion Inspired by active perception theory, we present Active Video Perception (AVP), which handles long video understanding as an iterative, query-driven evidence seeking process. Rather than passively caption the video frames, AVP treats the video as an interactive environment and actively decides what to inspect, where to focus, and at what granularity in order to acquire compact, time-stamped evidence directly from pixels. Concretely, AVP runs an iterative planobservereflect process using MLLM agents. Empirically, AVP achieves best performance among agentic frameworks across five long video benchmarks, and surpasses the leading agentic method (DVD) by 5.7% in average accuracy while only requiring 18.4% inference time and 12.4% input tokens. Our ablation study shows that AVP achieves significant improvement under different MLLM backbones, validating the robustness. Looking ahead, an exciting direction is extending active video perception to embodied agents that must decide what and when to observe while acting under real-world physical constraints."
        },
        {
            "title": "References",
            "content": "[1] Yiannis Aloimonos. Active perception. Psychology Press, 2013. 2, 3 [2] Anurag Arnab, Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. Temporal chain of thought: Long-video understanding by thinking in frames, 2025. 2 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [4] Ruzena Bajcsy. Active perception. Proceedings of the IEEE, 76(8):9661005, 1988. 2, [5] Ruzena Bajcsy, Yiannis Aloimonos, and John K. Tsotsos. Revisiting active perception, 2016. 2, 3 [6] Shyamal Buch, Cristobal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the Video in Video-Language Understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [7] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour In NeurIPS Datasets and video-language understanding. Benchmarks Track, 2024. 2 [8] Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, and Yali Wang. Lvagent: Long video understanding by multi-round dynamical collaboration of mllm agents. arXiv preprint arXiv:2503.10200, 2025. 3, 5, 6 [9] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. 2 [10] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 2, 6 [11] Zixuan Dong, Baoyun Peng, Yufei Wang, Lin Liu, Xinxin Dong, Yunlong Cao, and Xiaodong Wang. See what you need: Query-aware visual intelligence through reasoning-perception loops, 2025. 3 [12] Sunqi Fan, Meng-Hao Guo, and Shuojin Yang. Agentic keyframe search for video question answering, 2025. 3 [13] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding, 2024. 3 [14] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 2 [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. 2, [16] Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, and Wei-Shi Zheng. Love-r1: Advancing long video understanding with an adaptive zoom-in mechanism via multistep reasoning, 2025. 2 [17] Haonan Ge, Yiwei Wang, Kai-Wei Chang, Hang Wu, and Yujun Cai. Framemind: Frame-interleaved video reasoning via reinforcement learning, 2025. 5, 6 [18] Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, and Yu Cheng. Framethinker: Learning to think with long videos via multi-turn frame spotlighting, 2025. 2 [19] Sullam Jeoung, Goeric Huybrechts, Bhavana Ganesh, Aram Galstyan, and Sravan Bodapati. Adaptive video understanding agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning, 2024. 3 [20] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long video understanding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 56275646, Vienna, Austria, 2025. Association for Computational Linguistics. 3 [21] Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, and Ehsan Adeli. Videomultiagents: multi-agent framework for video question answering, 2025. 3 [22] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model, 2025. [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, pages 1288812900. PMLR, 2022. [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 1 [25] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 2 [26] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2 [27] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025. 3 [28] Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, and Rongrong Ji. Video-rag: Visually-aligned retrieval-augmented long video comprehension, 2024. 3 [29] Martin Ma, Willis Guo, Aditya Agrawal, Ankit Gupta, Paul Pu Liang, Russ Salakhutdinov, and Louis-Philippe Morency. Video active perception: Efficient inference-time long-form video understanding with vision-language models. 2024. 2 [30] Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and Jianfei Cai. Drvideo: Document retrieval based long video understanding, 2024. 3 [31] Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, and Cordelia Schmid. Caviar: Criticaugmented video agentic reasoning, 2025. 3 [32] Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, and Cordelia Schmid. Morevqa: Exploring modular reasoning models for video question answering, 2025. 3 [33] Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, and Tobias Weyand. Minerva: Evaluating complex video reasoning, 2025. 2, 5, [34] OpenAI. Gpt-4o system card, 2024. 5, 6 [35] OpenAI. Gpt-5 system card. https://cdn.openai.com/ gpt-5-system-card.pdf, 2025. 1 [36] OpenAI. Introducing gpt-4.1 in the api. https://openai. com/index/gpt-4-1/, 2025. Accessed: 2025-11-10. 5, 6 [37] OpenAI. Openai o3 and o4-mini system card. tem Card v1, OpenAI, 2025. https://cdn.openai.com/pdf/2221c875-02dc-4789-800be7758f3722c1/o3-and-o4-mini-system-card.pdf. Accessed: 2025-11-10. SysPDF available at: [38] Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Conan: Progressive learning to reason like detective over multi-scale visual evidence, 2025. 2 [39] Ziqi Pang and Yu-Xiong Wang. Mr. video: \"mapreduce\" is the principle for long video understanding, 2025. 3 [40] Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryu, Donghyun Kim, and Michael S. Ryoo. Too many frames, not all useful: Efficient strategies for long-form video qa, 2025. 3 [41] Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, and Michael Ryoo. Understanding long videos in one multimodal language model pass. In International Conference on Learning Representations, 2025. [42] Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Jongwoo Park, Kanchana Ranasinghe, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles. xgen-mm-vid (blip-3-video): You only need 32 tokens to represent video even in vlms, 2025. 2 [43] Jinghuan Shang and Michael S. Ryoo. Active vision reinforcement learning under limited visual observability, 2023. 2 [44] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, 10 Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 2, 5, 6 [45] Xiaoqian Shen, Wenxuan Zhang, Jun Chen, and Mohamed Elhoseiny. Vgent: Graph-based retrieval-reasoning-augmented generation for long video understanding, 2025. 2, 3, 5, 6 [46] Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. Enhancing video-llm reasoning via agent-of-thoughts distillation, 2025. [47] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Videoxl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 2 [48] Venkatesh Sripada, Samuel Carter, Frank Guerin, and Amir Ghalamzan. Scene exploration by vision-language models, 2025. 2 [49] Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, and Qixiang Ye. Adaptive keyframe sampling for long video understanding, 2025. 2 [50] Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, and Xuming Hu. Moss-chatv: Reinforcement learning with process reasoning reward for video temporal reasoning, 2025. 2 [51] Gemini team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. 1, 5, 6, 7 [52] Qwen Team. Qwen3-vl: general vision-language model. https : / / qwenlm . github . io / blog / qwen3 - vl/, 2025. Model card and technical documentation. [53] Seed-VL team. Seed1.5-vl technical report, 2025. 1, 5, 6 [54] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models, 2024. 2 [55] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning, 2025. 2 [56] Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. Vamos: Versatile action models for video understanding, 2023. 2 [57] Shihao Wang, Guo Chen, De an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose M. Alvarez, Lei Zhang, and Zhiding Yu. Videoitg: Multimodal video understanding with instructed temporal grounding, 2025. 2 [58] Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, and Xuelian Cheng. Video-thinker: Sparking \"thinking with videos\" via reinforcement learning, 2025. 2 [59] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [60] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. Videoagent: Long-form video understanding with large language model as agent, 2024. 1, 5, 6 plorer: Think with videos for agentic long-video understanding, 2025. 2 [75] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering, 2023. 1, 3 [76] Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, and Gedas Bertasius. Silvr: simple language-based video reasoning framework, 2025. 3, 5, 6 [77] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal toolaugmented reinforcement learning for long video reasoning, 2025. 2 [78] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [79] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep video discovery: Agentic search with tool use for long-form video understanding. In Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. 2, 3, 5, 6, 7, 12 [80] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2 [81] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 5 [82] Yiyang Zhou, Yangfan He, Yaofeng Su, Siwei Han, Joel Jang, Gedas Bertasius, Mohit Bansal, and Huaxiu Yao. Reagent-v: reward-driven multi-agent framework for video understanding, 2025. 3 [83] Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, and Chunhua Shen. Active-o3: Empowering multimodal large language models with active perception via grpo, 2025. 3 [84] Yuanhao Zou, Shengji Jin, Andong Deng, Youpeng Zhao, Jun Wang, and Chen Chen. A.i.r.: Enabling adaptive, iterative, and reasoning-based frame selection for video question answering, 2025. [85] Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, and Changxin Gao. Videolucy: Deep memory backtracking for long video understanding. In Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. 2, 3, 5, 6 [61] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Adaretake: Adaptive redundancy reduction to perceive longer for video-language understanding, 2025. 2, 5, 6 [62] Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025. 2 [63] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. arXiv preprint arXiv:2405.19209, 2024. 1, 3, 5, 6 [64] Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, and Yali Wang. Videochat-a1: Thinking with long videos by chain-of-shot reasoning, 2025. 3 [65] Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul Islam, Gedas Bertasius, and Mohit Bansal. Video-RTS: Rethinking reinforcement learning and test-time scaling for efficient and enhanced video reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2811428128, Suzhou, China, 2025. Association for Computational Linguistics. 2, 5, 6 [66] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 2, 5 [67] Haoyu Xiong, Xiaomeng Xu, Jimmy Wu, Yifan Hou, Jeannette Bohg, and Shuran Song. Vision in action: Learning active perception from human demonstrations, 2025. 2 [68] Bowen Xu, Yifan Zhang, Yufei Zhao, Yizhou Wang, Yu Qiao, and Hongsheng Li. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, [69] Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, and Lili Qiu. Ava: Towards agentic video analytics with vision language models, 2025. 3 [70] Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long video understanding. arXiv preprint arXiv:2503.09146, 2025. 2 [71] Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, and Manling Li. Re-thinking temporal search for longform video understanding, 2025. 2 [72] Serena Yeung, Olga Russakovsky, Greg Mori, and Li FeiFei. End-to-end learning of action detection from frame glimpses in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 1 [73] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. In NeurIPS, 2023. 2 [74] Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, and Zhicheng Dou. Videoex-"
        },
        {
            "title": "Appendix",
            "content": "In appendix, we present the following: limitations (Sec. A), additional quantitative results and analysis (Sec. B), additional qualitative analysis (Sec. C), additional implementation details (Sec. D). A. Limitations While AVP achieves strong performance and efficiency gains across multiple LVU benchmarks, it also has several practical limitations that point to promising future work rather than fundamental constraints. First, we primarily evaluate AVP in the standard offline video QA setting, where the full video is available. An exciting direction for future work is to explore how the same active evidence-seeking framework operates in broader scenarios, such as embodied or online streaming environments where an agent must perceive and act in real time. Second, AVP currently uses prompting to drive planning and observation, learning policies that optimize long horizon sensing efficiency under resource and latency constraints (e.g., via reinforcement learning or differentiable planners) would be complementary direction that builds on the same architecture. B. Additional Quantitative Results and Analysis B.1. Reasoning Trace Analysis Proposed by MINERVA [33], the MiRA (MINERVA Reasoning Assessment) score is reference-based, LLM-as-a-judge metric for evaluating the quality of multimodal models stepby-step reasoning traces for video question answering. It assesses models generated reasoning against ground-truth trace using the four axes of the MINERVA rubric: Perceptual Correctness, Temporal Localization, Logical Reasoning, and Completeness. his normalized score helps analyze why models succeed or fail beyond just the final answers accuracy, specifically highlighting weaknesses in video-centric aspects like temporal grounding and perception. As shown in Tab. 6, AVP achieves the highest overall MiRA score, outperforming all baselines across key reasoning dimensions. Compared to single-pass MLLMs, AVP delivers substantially stronger temporal localization, logical reasoning, and correctness. These improvements indicate that actively collecting structured, query-conditioned evidence leads to higher-quality reasoning traces besides higher final accuracy. In particular, AVPs gains in temporal and completeness highlight the benefit of iterative planning and reflection for complex multi-hop queries. B.2. Full Results for LVBench As shown in Tab. 7, AVP achieves the best overall accuracy on LVBench, outperforming all prior systems including the"
        },
        {
            "title": "Method",
            "content": "Acc. % MiRA Score C"
        },
        {
            "title": "Total",
            "content": "OpenAI o1 GPT-4o Gemini 2.0 Flash Gemini 2.5 Pro AVP (Ours) 43.5 45.5 53.5 61.8 65.6 0.52 0.57 0.62 0.60 0.52 0.67 0.75 0. 0.86 0.77 0.83 0.97 0.88 0.79 0.82 0.78 0.69 0.70 0.75 0.74 0.62 0.82 0. 0.93 0.84 Table 6. Reasoning trace quality check on MINERVA. We report multiple-choice accuracy and MiRA scores normalized to be between 0 and 1. P: Perceptual Correctness, T: Temporal Localization: L: Logical Reasoning: C: Completeness. The best result is in bold, and the second best is in italic."
        },
        {
            "title": "Methods",
            "content": "ER"
        },
        {
            "title": "Sum Overall",
            "content": "GPT-4o OpenAI o3 AdaReTAKe VideoTree VideoAgent VCA MR. Video DVD 48.9 57.6 53.0 30.3 28.0 43.7 59.8 73.4 49.5 56.4 50.7 25.1 30.3 40.7 57.4 73.3 48.1 62.9 62.2 26.5 28.0 37.8 71.4 80.4 40.9 46.8 45.5 27.7 29.3 38.0 58.8 72.3 50.3 50.8 54.7 31.9 28.0 46.2 57.7 70. 50.0 67.2 37.9 25.5 36.4 27.3 50.0 74.1 AVP (Ours) 71.9 76.7 80.1 73. 67.7 75.9 48.9 57.1 53.3 28.8 29.3 41.3 60.8 74.2 74.8 Table 7. Results by question type on LVBench. We report performance across six official LVBench splits: Entity Recognition (ER), Event Understanding (EU), Key Information Retrieval (KIR), Temporal Grounding (TG), Reasoning (Rea), and Summarization (Sum). Accuracy (%) is computed as Correct / Total for each split. strongest agentic baseline DVD [79]. The gains are most pronounced on splits that require integrating information over long temporal ranges: AVP delivers the highest scores on Event Understanding, Temporal Grounding, and Summarization, indicating that its planobservereflect loop is effective at steering perception toward query-relevant moments and aggregating evidence across distant segments. On Key Information Retrieval, Entity Recognition, and Reasoning, AVP remains competitive with DVD, while still substantially outperforming powerful generic MLLMs across all question types. These results suggest that explicit active video perception is crucial for long video understanding. B.3. Additional Ablation Study More EfficiencyAccuracy Tradeoff Comparisons. As shown in Tab. 8, when all methods share the same OpenAIo3 backbone (which DVD employs), AVP achieves substantially better efficiencyaccuracy tradeoff than both the raw OpenAI-o3 baseline and DVD. Compared to DVD, AVP cuts inference time by over 80% while maintaining comparable performance on LV-Bench and improving accuracy 12 Method Avg. Inference Time (s) LV-Bench Video-MME-Long"
        },
        {
            "title": "MINERVA LVBench",
            "content": "OpenAI-o3 DVD AVP (Ours) 40.6 790.5 145.3 57.1 74.2 73. 64.7 67.3 76.8 Table 8. Comprehensive comparison with DVD using the same OpenAI-o3 MLLM on LV-Bench. We report average inference time in seconds, average input token count, and accuracy. By actively querying the video rather than passively captioning all clips, AVP achieves better overall efficiency and accuracy. Backbone MLLM MINERVA (Acc. %) Qwen3-VL-8B Gemini-2.5-Flash OpenAI-o3 Gemini-2.5-Pro 41.2 56.9 59.0 65. Table 9. Backbone MLLM selection within AVP. The performance of AVP on MINERVA scales steadily with the strength of the backbone MLLM. on Video-MME-Long. Relative to the raw o3 model, AVP attains large gains on both benchmarks with only moderate increase in runtime. These trends highlight that actively querying the video yields stronger long-video reasoning under same MLLM backbone. Different Backbone MLLM Selection within AVP. As shown in Tab. 9, the performance of AVP on MINERVA scales steadily with the strength of the backbone MLLM. Using the lightweight Qwen3-VL-8B yields 41.2% accuracy (2.0% improvements compared to the direct inference), while swapping in stronger general-purpose models such as Gemini-2.5-Flash and OpenAI-o3 improves accuracy to 56.9% and 59.0%, respectively. The best results are obtained with Gemini-2.5-Pro (65.6%), indicating that richer reasoning and instruction-following capabilities at the backbone level directly translate into better planning, evidence selection, and reflection for complex multi-hop queries. At the same time, AVP delivers consistent gains across wide spectrum of MLLMs, suggesting that our AVP framework is broadly applicable and can flexibly exploit future backbone improvements. Structured vs. Unstructured Evidence List. As shown in Tab. 10, replacing our structured, time-aligned evidence list with an unstructured flat list degrades performance on both benchmarks, indicating that temporally and semantically organized evidence is crucial for effective planning and reflection. Unstructured List Structured Evidence List (Ours) 63.2 65. 71.2 74.8 Table 10. Ablation on structured evidence list. Replacing our structured, time-aligned evidence list with an unstructured flat list hurts performance on both benchmarks, showing that organizing evidence by temporal and semantic grounding is important for effective planning and reflection."
        },
        {
            "title": "LVBench",
            "content": "0.5 0.7 0.9 64.2 65.6 65.4 73.2 74.8 74.8 Table 11. Ablation on confidence threshold. We vary the confidence threshold for halting, observing that different values trade off answer conservativeness and coverage on both benchmarks. Confidence Threshold Sensitivity Analysis. As shown in Tab. 11, moderate confidence threshold yields the strongest results on MINERVA and ties for best performance on LVBench. Lower thresholds lead to premature halting and reduced accuracy, while overly strict thresholds offer no additional gains. This suggests that AVP benefits from balanced stopping criterion, confident enough to avoid early termination, yet flexible enough to prevent unnecessary observation rounds. C. Additional Qualitative Results C.1. Additional Visualization As illustrated in Fig. 4, this example showcases how AVP leverages iterative planning to solve compositional, numerically precise queries that cannot be answered from single view of the video. In the first round, the agent executes narrowly targeted observation around the specified timestamp to read off the millimeter totals from the paper, but the reflector explicitly flags that the evidence is incomplete. The planner then revises its strategy, broadening the search space to coarse scan over the entire video to hunt for the missing semantic attribute (the average hatchling length), which the observer recovers from narration. Only after both local numeric measurements and global semantic context are available does the reflector combine them into the final answer. This visualization shows AVP could tackle complex, multi-hop video reasoning via its iterative design. C.2. Failure Case In Fig. 5, we analyze representative failure mode of AVP on fine-grained counting query. To save computation, the planner opts for coarse 0.5 FPS scan of the entire video and the observer only records two three-point plays before the 13 Figure 4. Qualitative example of multi-round active perception in AVP (MINERVA sample). Given the query, After adding up all the millimeter totals on the sheet of paper illustrated at 09:58, and then adding the average length of Louisiana Pine Snake hatchlings according to the video, how many total millimeters are there?, AVP first plans to focus on the local timestamped frame at 09:58 and extracts the seven millimeter totals from the handwritten measurement sheet (Round 1). The reflector correctly judges that this evidence is insufficient because the average hatchling length is still unknown, and triggers second round. In Round 2, the planner re-directs the observer to uniformly scan the full video at low FPS, locating narrated segment that states hatchlings usually range from 4 to 5 feet in length. By fusing the previous numeric evidence with this newly discovered range, the reflector computes the total millimeter interval and selects the correct option. second HawaiiUCSB clip. Since the missing shot at 00:20 is never observed, the reflector receives logically consistent but incomplete evidence list and confidently outputs the wrong answer. This case illustrates that, while our active perception pipeline is effective for locating dispersed, highlevel evidence, it might make mistakes on questions that hinge on short, local events and subtle broadcast cues (e.g., bar graphics and rapid scoring plays). D. Additional Implementation Detail D.1. Prompts User Query: {full_query} Video Information: {duration} seconds) duration in seconds (e.g., Duration: Options (optional): multiple-choice options attached to the query Planning framework. Produce observation with: What (Reasoning Objective): what the step tries to accomplish. Where: temporal span to examine, either uniform (entire video) or specific time range. How: fps and spatial_token_rate. Timestamp handling. First classify the query: Factual questions: e.g., what, how many, who, which, count, identify. Reasoning / explanation questions: e.g., why, how, explain, We provided the planner prompt, the observer prompt, and the reflector prompt as follow. reason, cause. Then apply: Rule 1 (Exact ranges). Planner prompt (initial planning) Function. get_planning_prompt(query, video_meta, options). Goal. You are an expert video analysis planner. Create concise, observation plan to answer the users query. Inputs. Factual: use the exact range, no padding (e.g., 07:1507:18 [435.0, 438.0]). Reasoning: add 1530 padding before and after (e.g., 07:1507:18 [420.0, 453.0]). Rule 2 (Single timestamp). Factual: 1 forward window from timestamp (e.g., at 02:15 [135.0, 136.0]). 14 Observer prompt (video inference / evidence extraction) Goal. Analyze specific video segment and extract precise, timestamped evidence relevant to the user query. Inputs. sub_query: the focused question for this round. original_query: the full user question (for multi-step agents). context: accumulated evidence from previous rounds. start_sec, end_sec: bounds of the segment to analyze. video_duration_sec: duration of the full video. is_region: whether this step analyzes specified region or uniform scan. regions: list of [start, end] spans if multiple clips are provided. Prompt structure. Primary task: describe visually relevant events in the analyzed video span. Provide: Detailed observations tied to the query. Key timestamp ranges (timestamp_start, timestamp_end) for each salient event. Reasoning connecting observations to the sub-query. Timestamp and evidence rules. Round timestamps to integer seconds: floor(start), ceil(end). List all relevant intervals for events that may match the query. Use context to avoid redundant descriptions. Multiple-clip handling. When inputs include several regions, each corresponds to its absolute time span in the original video. You may reference clips descriptively (e.g., Clip 1, Clip 2). Fallback rule (critical). If analyzing region and no relevant information is present: Explicitly state: No relevant information found in this time segment. Suggest expanding search to uniform scan or additional regions. Output format. Return JSON object: { } \"detailed_response\": \"...\", \"key_evidence\": [ { \"timestamp_start\": <number>, \"timestamp_end\": <number>, \"description\": \"...\" } ], \"reasoning\": \"...\" Example. [Few_examples] Reflector prompt (evidence sufficiency checker) Goal. Given the original query and cumulative evidence from all observation rounds, decide whether the current evidence is sufficient to answer the query, and produce justification that either (i) contains the final answer, or (ii) explains what is missing. Inputs. query: original user query (with options if MCQ). evidence_summary: aggregated evidence from all Observer steps. video_duration: total duration in seconds. options: optional list of MCQ options. Your task. 15 Figure 5. Failure Case of AVP (MINERVA sample). Given long broadcast basketball video, AVP must answer: How many three-pointers are made before the second clip of Hawaii versus UCSB? The planner chooses to scan the entire video at 0.5 FPS with low spatial resolution, the observer summarizes the retrieved segments into structured evidence list, and the reflector produces confident answer of two. However, the ground-truth reasoning (yellow box) shows that three-pointer at 00:20 is missed, so the correct count is three. Although the internal reasoning over the collected evidence is coherent, the initial coarse observation policy fails to capture short, local event, leading to an overconfident but incorrect prediction. Reasoning: add 1530 context (e.g., at 02:15 [120.0, 150.0]). Rule 3 (Approximate / vague timing). Use 15 window around the mentioned time (e.g., around 1:23 [68.0, 98.0]). Heuristics for unknown timing. opening / beginning [0, 30]. end / ending [max(0, duration - 30), duration]. No timing mentioned: use coarse uniform scan with fps in 0.251.0 and low/medium resolution. Step configuration guidelines. Uniform scan (timing unknown). load_mode = \"uniform\", fps in 0.251.0, spatial_token_rate {\"low\", \"medium\"}, regions = []. Region analysis (explicit timestamps). load_mode = \"region\", fps 2.0, spatial_token_rate {\"low\", \"medium\"}, regions = [[start, end]]. Few-shot examples. [Few_examples] Output format. Return single JSON object with: reasoning: natural language explanation of your planning. plans: what sub_query, where {\"uniform\", \"region\"}, how numeric fps (0.52.0), spatial_token_rate {\"low\", \"medium\"}, and regions (list of [start, end] in seconds; empty for uniform). Decide boolean sufficient indicating whether the evidence is enough to answer the query. If sufficient (true): the justification must give the direct answer. MCQ: state the option letter (A/B/C/...) and brief reason. Open-ended: clearly state the answer in natural language. If not sufficient (false): the justification must explain what information is missing or uncertain (e.g., which regions, entities, or temporal spans require additional observation). Always provide short reasoning paragraph that summarizes why the evidence is (not) sufficient. Required JSON output (LLM response). { } \"sufficient\": <true false>, \"justification\": \"...\", \"reasoning\": \"...\" Few-shot examples. [Few_examples]"
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "University of North Carolina at Chapel Hill"
    ]
}