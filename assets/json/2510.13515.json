{
    "paper_title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Kaichen Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Yueyi Zhang",
        "Weidong Cai",
        "Jiankang Deng",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 5 1 5 3 1 . 0 1 5 2 : r UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning Tiancheng Gu, *, Kaicheng Yang*, Kaichen Zhang, , Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing MiroMind AI The University of Sydney M.R.L. Team LMMs-Lab Team Imperial College London yueyi.zhang@miromind.ai, j.deng16@imperial.ac.uk Webpage: https://garygutc.github.io/UniME-v2 Github: https://github.com/GaryGuTC/UniME-v"
        },
        {
            "title": "Abstract",
            "content": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ inbatch negative mining by measuring the similarity of querycandidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, reranking model trained on our mined hard negatives through joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks. Introduction Multimodal embedding models aim to encode heterogeneous multimodal data into unified dense representation space, enabling wide range of downstream applications such as visual question answering (Dong et al. 2025; Hamza et al. 2025; Li et al. 2025) and multimodal retrieval (Zheng et al. 2025; * Equal contribution. Corresponding Author. Work done during T.G. and K.Z.s internship at MiroMind AI. Figure 1: Comparison between previous works and UniMEV2. UniME-V2 exploits the understanding capabilities of MLLMs for hard negatives mining and generates soft semantic matching score to supervise the model in learning the semantic difference among candidates. Gu et al. 2025b; Yang et al. 2025; Gu et al. 2024). With the increasing adoption of these models, multimodal representation learning has garnered significant research attention. Among these models, CLIP (Radford et al. 2021) stands out as pioneering approach, achieving remarkable performance in textimage retrieval by leveraging cross-modal contrastive learning on large-scale web-collected image-text pairs (Schuhmann et al. 2021). However, its effectiveness is hindered by three major limitations: (1) CLIP enforces strict text token limit of 77, which restricts its ability to process detailed or lengthy descriptions (Zhang et al. 2024a; Cao, Wei, and Ma 2024; Huang et al. 2024b); (2) Its dual-encoder design processes images and text independently, which reduces its effectiveness in handling complex tasks, such as instructionfollowing multimodal retrieval (Jiang et al. 2025; Liu et al. 2024; Gu et al. 2025a); and (3) CLIP exhibits limited proficiency in advanced language understanding, struggles with handling compositionality, and often demonstrates bag-ofwords behavior (Yuksekgonul et al. 2022; Tschannen et al. 2023; Hu et al. 2025). Recent advances in Large Language Models (LLMs) have achieved state-of-the-art performance on the MTEB benchmark (Muennighoff et al. 2022). Motivated by these developments (Lee et al. 2024; BehnamGhader et al. 2024), researchers are currently exploring how to utilize Multimodal Large Language Models (MLLMs) to learn universal multimodal representation. E5-V (Jiang et al. 2024) adopts unimodal contrastive learning approach, training the language component of MLLMs on sentence pairs to better align the cross-modal representation spaces. VLM2Vec (Jiang et al. 2025) introduces the Massive Multimodal Embedding Benchmark (MMEB), comprising 36 datasets across four metatasks, and proposes contrastive learning framework to repurpose pre-trained vision-language models as embedding models via training on the MMEB dataset. QQMM (Xue, Li, and Liu 2025a) provides an in-depth analysis of the gradients derived from the InfoNCE loss and proposes amplifying gradients associated with hard negative samples to encourage the model to learn more discriminative embeddings. UniME (Gu et al. 2025a) presents two-stage framework that leverages powerful LLM-based teacher model to improve the embedding capabilities of the language component in MLLMs. Furthermore, it incorporates hard negative sampling strategy that selects multiple challenging negatives per instance within each batch. Despite these advances, existing methods fail to fully exploit the semantic differences between candidates and are limited by the lack of diversity in negative samples. Moreover, the raw embeddings produced by these models are frequently inadequate for reliably discriminating between hard negatives and false negatives. In this paper, we propose novel Universal Multimodal Embedding (UniME-V2) model, which leverages the robust understanding capabilities of MLLMs to enhance representation learning. As shown in Fig. 1, we first construct potential hard negative set through global retrieval. Then, we introduce MLLM-as-a-Judge to assess the semantic alignment of query-candidate pairs, producing semantic matching scores. This score serves as the foundation for hard negative mining, effectively reducing interference from false negatives and enabling the identification of high-quality, diverse hard negatives. Additionally, we use the scores as soft labels to mitigate strict one-to-one mapping constraints. Aligning the similarity matrix with the semantic score matrix enables the model to capture semantic distinctions among candidates, significantly improving its discriminative ability. To further enhance performance, we introduce UniME-V2-Reranker, reranking model trained on our mined hard negatives through joint pairwise and listwise optimization approach. Extensive experiments on the MMEB benchmark and various retrieval tasks, including short/long caption retrieval and compositional retrieval, demonstrate that our method achieves state-of-the-art performance across all tasks. The main contributions of this paper are summarized as follows: We introduce an MLLM-as-a-Judge pipeline for hard negative mining that uses the advanced understanding capabilities of MLLM to assess the semantic alignment of each query-candidate pair within globally retrieved potential hard negative set. We present UniME-V2, novel universal multimodal embedding model trained with an MLLM judgment based distribution alignment framework. By leveraging semantic matching scores as soft labels, the model effectively captures semantic differences between candidates, significantly enhancing its discriminative capability. We propose UniME-V2-Reranker, reranking model trained on high-quality, diverse hard negatives through joint pairwise and listwise optimization approach. We conduct extensive experiments on the MMEB benchmark and various retrieval tasks, including short and long caption retrieval as well as compositional retrieval. The results demonstrate that our method achieves state-of-the-art performance on average across all tasks."
        },
        {
            "title": "Related Work",
            "content": "Multimodal Large Language Models Multimodal Large Language Models (MLLMs) extend traditional LLMs to process and integrate information across multiple modalities (Wang et al. 2025b,a; Xie et al. 2024; Bai et al. 2025; An et al. 2025a, 2024). As foundational contribution, LLaVA (Liu et al. 2023) leverages subset of the CC3M (Changpinyo et al. 2021) dataset to achieve more balanced conceptual coverage. In this approach, the visual encoder and language model remain frozen, while only the projection layer is trained to align visual features with language tokens. Subsequently, numerous MLLM variants (Peng et al. 2023; Lin et al. 2024a; Tang et al. 2025a,b; An et al. 2025b) have achieved remarkable results in multimodal understanding and reasoning tasks. For instance, CogVLM (Wang et al. 2023) incorporates trainable visual expert module into the attention and feed-forward layers of the language model, achieving significant improvements on 17 standard crossmodal benchmarks. Similarly, Qwen2-VL (Wang et al. 2024) introduces the Naive Dynamic Resolution mechanism and integrates M-RoPE to enhance positional information fusion, yielding competitive performance across diverse benchmarks. LLaVA-OneVision (Li et al. 2024) pushes the boundaries of open MLLMs by excelling in single-image, multi-image, and video tasks, showcasing robust video understanding through effective task transfer from image-based training. Although these advances have significantly improved the understanding capabilities of MLLMs, further research is needed to explore how MLLMs can effectively learn unified multimodal representations. Multimodal Representation Learning CLIP (Radford et al. 2021) demonstrates strong image-text retrieval performance through large-scale cross-modal contrastive learning but faces three key limitations: (1) 77token text truncation restricts fine-grained semantic alignment (Zhang et al. 2024a; Cao, Wei, and Ma 2024; Huang et al. 2024b); (2) Its dual-encoder architecture limits effective cross-modal fusion, particularly for instruction-sensitive tasks (Jiang et al. 2025; Liu et al. 2024; Gu et al. 2025a); and Figure 2: The MLLM-as-a-Judge pipeline for Hard Negatives Mining. We first utilize an existing multimodal embedding model for global retrieval to construct potential hard negative set. We then leverage the powerful understanding capabilities of MLLM to score query-candidate pairs based on their semantic alignment, enabling precise identification of hard negatives. (3) Simplistic language modeling results in bag-of-words representations (Yuksekgonul et al. 2022; Tschannen et al. 2023; Hu et al. 2025; Lei et al. 2024; Huang et al. 2024a; Andonian, Chen, and Hamid 2022). To address these issues, recent studies have incorporated MLLMs for enhanced multimodal representation learning. E5-V (Jiang et al. 2024) employs unimodal contrastive learning, training the language component of MLLMs on sentence pairs to reduce cross-modal representation gaps. VLM2Vec (Jiang et al. 2025) introduces the Massive Multimodal Embedding Benchmark (MMEB) and adapts state-of-the-art vision-language models into embedding models using contrastive framework trained on MMEB. QQMM (Xue, Li, and Liu 2025a) analyzes InfoNCE loss gradients and proposes enhancing gradients associated with hard negatives to improve embedding discrimination. UniME (Gu et al. 2025a) adopts two-stage framework with an LLM-based teacher model to refine language embeddings and employs hard negative sampling strategy, selecting multiple challenging negatives per batch. Despite these advancements, existing methods still under-utilize the semantic differences among candidates and struggle to effectively identify and leverage hard negatives during retrieval."
        },
        {
            "title": "Task Definition",
            "content": "Unlike CLIP, which employs separate encoders to generate embeddings for each modality, we investigate leveraging the unified architecture of MLLM to extract embeddings across multiple modalities and improve retrieval performance through reranking. Specifically, given query and set of candidates Ωc = {c1, c2, . . . , cn}, which may include images, text, and interleaved image-text data, the universal embedding model Φemb encodes the query and candidates, retrieving the top-k most relevant candidates Ωk = Φemb(q, Ωc). To further enhance retrieval performance, reranker model Φrank refines this subset through reranking process, producing the final ranked output ˆΩk = Φrank(q, Ωk). MLLM-as-a-Judge for Hard Negatives Mining Previous works (Jiang et al. 2025; Liu et al. 2024) primarily rely on in-batch hard negative mining, where query-candidate embedding similarities are computed to sample negatives. However, this method often suffers from limited negative sample diversity and insufficient embedding discriminative power to effectively distinguish false and hard negatives. To overcome these challenges, as shown in Fig. 2, we first construct potential hard negative set using global retrieval. After that, inspired by previous work (Zheng et al. 2023; Chen et al. 2024a), we leverage the robust understanding capabilities of MLLMs to assess the semantic alignment of each querycandidate pair and generate soft semantic matching score. This score guides hard negative mining, enabling the identification of diverse and high-quality hard negatives while reducing the impact of false negatives. Potential Hard Negative Set. To extract higher-quality hard negatives from global samples, we first use VLM2Vec to generate embeddings for both queries and candidates. We then retrieve the top 50 most relevant candidates for each query. To address false negatives and improve diversity, we apply similarity threshold (δ) based on the query-candidate similarity scores and select the top 50 candidates as the potential hard negative set (Ωp): Ωp = Rank50 ({x1, . . . , xn}) , where xi < δ, (1) where xi is the similarity score of the query and candidates ˆΩc calculated by the VLM2Vec model. Semantic Matching Score. After constructing the potential hard negative set (Ωp), we employ an MLLM as judge to compute semantic matching score for each query-candidate pair in Ωp, guided by the following instruction: will provide you with query and candidate. Please evaluate whether the candidate meets the requirements of the query. If it does, respond with Yes; if it doesnt, respond with No. Query:<Query>, Candidates:<Candidate>. embedding eq and candidate embeddings Ec as follows: P(eq, Ec) = exp(cos(eq, e+ )/τ ) + (cid:80)k )/τ ) exp(cos(eq, e+ i=1 exp(cos(eq, ec )/τ ) (2) Based on the semantic matching scores Sc = {sq,ct, sq,c1, ..., sq,ck }, we compute the semantic matching score matrix Q(Sc) derived from the MLLM judgment as follows: . Figure 3: The architecture of the MLLM Judgment Based Training Framework. UniME-V2 uses soft semantic matching scores as supervised signals to enhance semantic distinction learning between candidates. UniME-V2-Reranker employs joint pairwise and listwise optimization to enhance reranking performance. After that, we compute the semantic matching score = {s1, s2, . . . , sm} based on the logits of the Yes (ey) and ei , where Rnq50 and No (en) tokens, where si = y+ei ei nq denotes the number of queries. Leveraging the advanced understanding capabilities of MLLMs, the semantic matching score effectively captures the degree of semantic alignment between queries and candidates. Hard Negative Sampling. To enhance the quality of hard negatives, we refine candidates using the semantic matching score (S). False negatives are excluded if their score exceeds threshold α = σq,ct β, where ct denotes the positive sample and β is hyperparameter controlling the threshold margin as 0.01. To maintain diversity, we apply cyclical sampling strategy with five-step intervals. If the refined set contains fewer than ten candidates, we duplicate selections to ensure minimum of ten. In the rare case where no candidates meet the criteria (< 1%), we randomly select 10 candidates from the initial pool of fifty and assign each semantic matching score of 1.0. Finally, for each query q, we obtain the hard negative set Ωh = {c1, ..., ck} along with the corresponding semantic matching scores Sh = {sq,c1 , ..., sq,ck }."
        },
        {
            "title": "MLLM Judgment Based Training Framework",
            "content": "UniME-V2. Previous works (Jiang et al. 2025; Gu et al. 2025a) are limited by rigid one-to-one mapping, which restricts the ability to learn distinctions among diverse negative samples. To address this, as shown in Fig. 3, we propose an MLLM judgment based distribution alignment framework, leveraging soft semantic matching scores as supervised signals to improve representation performance. Specifically, given query and its candidate set Ωc = {ct, c1, ..., ck}, we input them into the MLLM and extract the last token as embeddings for the query eq and candidates Ec = {e+ is the embedding of target candidate and is the hard negative number for each query. We then compute the relation score matrix between the query }, where e+ , .., ck , c1 Q(Sc) = exp(sq,ct /τ ) exp(sq,ct /τ ) + (cid:80)k i=1 exp(sq,ci /τ ) . (3) To enhance learning robustness and ensure matrix symmetry, we employ JS-Divergence, symmetric alternative to KL-Divergence (Nielsen 2020). The final loss function is defined as: = 1 2 ("
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:88) i=1 KL(P(ei, Ec)Q(Sc))+ KL(Q(Sc)P(ei, Ec))). (4) UniME-V2-Reranker. Following previous works (Liu et al. 2024; Lin et al. 2024b), we train reranking model to enhance retrieval precision following initial embedding-based retrieval. Specifically, we train UniME-V2-Reranker using joint pairwise and listwise approaches to enhance its reranking capability (refer to Fig. 3). In pairwise training, we construct two pairs for each query by combining with the positive candidate ct and the hardest negatives ch. We then instruct UniME-V2-Reranker to output YES for the positive and NO for the negative. The pairwise loss Lpair is computed using the cross-entropy loss function as: Lpair = Lce(YES, η(q, ct)) + Lce(NO, η(q, ch)), (5) where η denotes the autoregressive output process of UniMEV2-Reranker. For listwise training, based on the semantic matching score, we choose top-x candidates ({c1, ...cx}) from the hard negative candidates, insert the target candidate ct at random position and get its index Ict. The UniMEV2-Reranker is then prompted to output the position of the ground truth, formulated as: Llist = Lce(Ict, η(q, ct, {c1, ...cx})). (6) The final loss function is defined as = Lpair + Llist. Detailed descriptions of the prompts used for pairwise and listwise training are provided in the supplementary material. Inference Pipeline After obtaining UniME-V2 and UniME-V2-Reranker, we integrate them during inference to improve retrieval performance. We initially use UniME-V2 embed query and candidate into features and utilize cosine similarity scores to retrieve the top-10 most relevant candidates. Subsequently, UniME-V2-Reranker reranks these candidates based on the following instruction: Models # of Datasets #Parameters Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall 10 10 12 20 16 36 CLIP (ViT-L)(Jiang et al. 2025) OpenCLIP (ViT-L)(Radford et al. 2021) Magiclens (ViT-L)(Zhang et al. 2024b) SigLIP (So/14)(Zhai et al. 2023) BLIP2 (ViT-L)(Li et al. 2023) CLIP (ViT-BigG/14)(Cherti et al. 2022) EVA-CLIP(Sun et al. 2023) E5-V (Phi3.5-V)(Jiang et al. 2024) E5-V (LLaVA-1.6)(Jiang et al. 2024) CLIP (ViT-L)(Jiang et al. 2025) VLM2Vec (Qwen2-VL)(Jiang et al. 2025) VLM2Vec (Qwen2-VL)(Jiang et al. 2025) LLaVE (LLaVA-OV)(Lan et al. 2025) QQMM (LLaVA-OV)(Xue, Li, and Liu 2025b) UniME (Qwen2-VL)(Gu et al. 2025a) UniME (Qwen2-VL)(Gu et al. 2025a) UniME (LLaVA-OV)(Gu et al. 2025a) UniME-V2(Qwen2-VL) UniME-V2(Qwen2-VL) UniME-V2(LLaVA-OV) 0.4B 0.4B 0.4B 0.9B 1.2B 2.5B 8B 4.2B 7B 0.4B 2B 7B 7B 7B 2B 7B 7B 2B 7B 7B Zero-shot on MMEB 42.8 41.5 38.8 40.3 27.0 52.3 56.0 39.1 39.7 9.1 6.9 8.3 8.4 4.2 14.0 10.4 9.6 10.8 Fine-tuning on MMEB 19.7 49.4 57.8 65.4 66.8 53.4 59.0 66. 55.2 59.0 62.6 65.7 66.8 59.0 64.7 66.8 62.1(+3.1) 64.0(-0.7) 65.3(-1.5) 53.0 44.6 35.4 31.6 33.9 50.5 49.2 38.0 39.4 53.2 65.4 69.9 70.9 70.5 64.9 71.6 70.5 51.8 53.5 26.0 59.5 47.0 60.3 58.9 57.6 60.2 62.2 73.4 81.7 91.9 90.4 69.6 82.7 90.9 37.1 32.8 31.0 32.3 25.3 38.9 38.1 33.1 34. 47.6 66.0 72.2 75.0 74.7 65.5 72.2 74.6 38.7 36.0 23.7 38.0 25.1 45.8 45.6 31.9 33.4 42.8 52.6 57.8 64.4 65.6 54.6 61.4 65.8 39.2 36.6 27.1 35.0 28.0 44.3 43.7 36.1 37.5 47.6 60.1 65.8 70.3 70.7 60.6 67.4 70.7 56.3(+2.9) 68.0(+3.1) 72.7(+3.1) 67.4(+1.9) 58.9(+4.3) 63.6(+3.0) 60.1(+1.1) 73.1(+1.5) 82.8(+0.1) 72.0(-0.2) 63.0(+1.6) 68.0(+0.6) 67.6(+1.0) 72.9(+2.4) 90.2(-0.7) 74.8(+0.2) 66.7(+0.9) 71.2(+0.5) Table 1: Results on the MMEB benchmark. IND: in-distribution, OOD: out-of-distribution. Scores are average Precision@1. Detailed results in the supplementary material. will provide you with query followed by multiple candidates in the format: (1) candidate1 (2) candidate2, etc. Each candidate is independent of the others. Evaluate each candidate against the query, and respond with the number corresponding to the candidate that best meets the requirements of the query. Query:<Query>, Candidates:<Candidate list>."
        },
        {
            "title": "Experiments and Results",
            "content": "Implementation We extract query and candidate embeddings using VLM2Vec (Qwen2-VL-7B) to construct potential hard negative set. We use the Qwen2.5VL-7B to generate the soft semantic matching score. We train UniME-V2 using two different multimodal large language models: Qwen2-VL (Wang et al. 2024) and LLaVA-OneVision (Li et al. 2024). To optimize GPU memory, we implement LoRA (rank=16) with DeepSpeed ZeRO stage-2 (Aminabadi et al. 2022). The training of UniME-V2 is conducted on 8NVIDIA A800 (80GB) GPUs to accommodate the substantial computational demands. We use 336336 resolution image inputs, set the accumulated batch size to 1024, with learning rates of 1e-4 (Qwen2-VL) and 2e-5 (LLaVA-OneVision). We set the temperature of the Symmetric KL loss τ = 0.02 and sample = 8 hard negatives, and train each model for 2,000 steps. Datasets and Evaluation Training Data. Following VLM2Vec (Jiang et al. 2025) and UniME (Gu et al. 2025a), we employ 20 in-distribution datasets from the MMEB benchmark, which cover four core multimodal tasks: classification, visual question answering, multimodal retrieval, and visual grounding. This comprehensive training corpus, incorporating both unimodal and multimodal input data, totals 662k carefully curated training pairs, ensuring robust model adaptation across diverse multimodal tasks. Evaluation. In this study, we evaluate UniME-V2 across both in-distribution (20 test sets) and out-of-distribution (16 test sets) benchmarks from MMEB (Jiang et al. 2025) to assess its multimodal embedding capabilities across diverse retrieval tasks. Following standard evaluation protocols (Liu et al. 2024; Jiang et al. 2025), we report Precision, measuring the proportion of correct matches among the top-ranked candidates for each dataset. To further examine the unimodal embedding performance of UniME-V2, we conduct experiments on multiple cross-modal retrieval tasks, including short-caption image-text retrieval on Flickr30K (Plummer et al. 2015) and COCO2014 (Lin et al. 2014), long-caption image-text retrieval on ShareGPT4V (Chen et al. 2024b) and Urban1K (Zhang et al. 2024a), and compositional retrieval on SugarCrepe (Hsieh et al. 2023). Consistent with the MMEB benchmark, we use Precision as the primary evaluation metric across all datasets."
        },
        {
            "title": "Main Results",
            "content": "Multi-Modal Retrieval. In Tab. 1, we present the performance of the proposed UniME-V2 compared to existing baseline models. Under identical training data and configurations, UniME-V2 consistently achieves notable performance improvements across various foundation models. Specifically, UniME-V2 outperforms VLM2Vec by 3.5% and 2.2% on the Qwen2-VL-2B and 7B models, respectively. When built on Models #Parameters Flickr30K COCO ShareGPT4V Urban1K SugarCrepe qi ct qt ci qi ct qt ci qi ct qt ci qi ct qt ci Replace Swap Add Short Caption Long Caption Compositional OpenCLIP (ViT-L) (Radford et al. 2021) CLIP (ViT-BigG/14) (Cherti et al. 2022) EVA-CLIP (Sun et al. 2023) E5-V (Phi3.5-V) (Jiang et al. 2024) E5-V (LLaVA-1.6) (Jiang et al. 2024) VLM2Vec (Qwen2-VL) (Jiang et al. 2025) VLM2Vec (Qwen2-VL) (Jiang et al. 2025) UniME (Qwen2-VL) (Gu et al. 2025a) UniME (Qwen2-VL) (Gu et al. 2025a) UniME (LLaVA-OV) (Gu et al. 2025a) UniME-V2 (Qwen2-VL) UniME-V2 (Qwen2-VL) UniME-V2 (LLaVA-OV) 0.4B 2.5B 8B 4.2B 7B 2B 7B 2B 7B 7B 2B 7B 7B 67.3 79.5 80.3 72.2 77.3 69.3 80.0 74.9 80.8 83.3 37.0 51.3 52.0 44.7 49.1 40.0 49.2 44.0 50.9 54. 87.2 92.9 94.5 79.6 85.7 89.6 94.2 90.6 92.7 94.4 84.0 93.6 91.2 88.5 82.1 88.2 90.4 88.6 93.8 89.3 79.8(+4.9) 89.9(-0.7) 53.7(+9.7) 65.1(+1.6) 91.6(+8.0) 94.2(+5.6) 95.6(+12.3) 92.2(+9.0) 70.9(+5.3) 51.2(+6.0) 70.2(+4.5) 84.6(+3.8) 93.5(+0.8) 57.3(+6.4) 70.3(+0.5) 94.3(+0.8) 95.2(+1.4) 97.2(+1.9) 96.3(+2.3) 77.8(+9.0) 62.2(+9.2) 79.0(+9.2) 85.5(+2.2) 93.7(-0.7) 60.9(+6.1) 74.1(+0.1) 95.1(+1.2) 94.1(+4.8) 96.3(+2.0) 96.7(+1.2) 88.6(+8.1) 73.7(+8.2) 90.5(+8.3) 47.0 77.8 80.4 83.8 88.9 78.7 94.0 83.3 95.3 94.3 58.1 67.3 70.1 53.4 57.6 62.5 68.5 63.5 69.8 74.0 81.8 90.1 93.1 86.0 85.1 78.1 78.5 83.6 86.5 93.9 79.5 86.5 85.9 88.2 86.3 67.2 70.0 65.6 68.8 80. 74.9 88.4 86.7 75.3 66.9 66.4 72.2 65.7 69.8 82.2 47.0 80.7 77.8 83.6 83.2 83.9 94.2 83.2 94.0 95.5 62.7 68.9 70.3 66.6 68.7 46.5 51.7 45.2 53.0 65.5 Table 2: Zero-shot text-image retrieval results on short caption (Flickr30K, MS-COCO), long caption (ShareGPT4V, Urban1K) and compositional (SugarCrepe) datasets. Scores are Recall@1. Embedding Model Reranker #Data MMEB RShort RLong RCompos LamRA(7B) 60.6 UniME(2B) 63.6 UniME-V2(2B) UniME-V2(2B) 1.1M 67.3 UniME-V2(2B) UniME-V2(7B) 0.6M 67.6 67.4 UniME(7B) 68.0 UniME-V2(7B) UniME-V2(7B) 1.1M 69.1 UniME-V2(7B) UniME-V2(7B) 0.6M 69.6 LamRA(7B) 68.3 72.1 76.4 76.4 73.6 76.4 78.3 78.7 84.7 93.4 96.4 96.9 92.4 95.8 97.2 97.5 58.8 64.1 87.4 94.8 63.9 73.0 87.4 94.8 Figure 4: Comparison of representation distributions between EVA-CLIP-8B and UniME-V2 (LLaVA-OneVision-7B). LLaVA-OneVision as the foundation, UniME-V2 achieves 0.5%-0.9% improvement over previous state-of-the-art models, including QQMM, LLaVE, and UniME. Furthermore, UniME-V2 attains score of 66.7 on out-of-distribution datasets, significantly exceeding all prior approaches, highlighting its robustness and superior transferability. Short & Long Caption Cross-Modal Retrieval. We evaluate UniME-V2 on zero-shot cross-modal retrieval tasks. For short-caption datasets, including Flickr30K and MS-COCO, UniME-V2 demonstrates 2.2%-9.7% performance improvement in image-to-text retrieval compared to UniME. In textto-image retrieval, its performance is comparable to UniME, primarily due to two factors: (1) the limited proportion of text-to-image data in the MMEB training set and (2) the insufficient semantic information in short captions. For longcaption cross-modal retrieval tasks, UniME-V2 achieves significant improvements on ShareGPT4V and Urban1K, benefitting from its enhanced discriminative capability and the richer semantic content provided by detailed captions. Notably, compared to EVA-CLIP-8B, UniME-V2 demonstrates more robust retrieval performance. This is primarily due to its universal multimodal embedding can significantly reduce the modality gap (as shown in Fig. 4). Compositional Cross-Modal Retrieval. We evaluate the capacity of the UniME-V2 model to discriminate hard negaTable 3: Comparison of reranking performance between LamRA and UniME-V2-Reranker using UniME-V2 (Qwen2VL-7B) and UniME-V2 (Qwen2-VL-2B). tive samples using the compositional benchmark SugarCrepe. As shown in Tab. 2, UniME-V2 consistently delivers superior performance across all evaluated metrics. Compared with UniME, our model achieves 5.3%, 6.0%, 4.5% performance improvement using Qwen2-VL-2B. After scaling the model from 2B to 7B, our model also achieves 9.0%, 9.2%, and 9.2% performance improvement. Additionally, UniME-V2 exhibits improvements of 2.7%, 3.4%, and 3.8% compared to EVA-CLIP-8B, underscoring its robust capability to discriminate against hard negative samples. Reranking Comparison. In Tab. 3, we compare the performance between LamRA and UniME-V2-Reranker using listwise reranking on the top-5 retrieval results. To ensure fairness, we use the same training parameters and base model (Qwen2.5-VL-7B) as LamRA. When UniME-V2 (Qwen2VL-2B) is used for retrieval, both LamRA and UniMEV2-Reranker improve performance across four downstream tasks, with UniME-V2-Reranker consistently achieving superior results while utilizing only half the data. Similarly, with UniME-V2 (Qwen2-VL-7B) for retrieval, UniME-V2Reranker outperforms LamRA, achieving performance gains of 0.5%, 0.4%, 0.3%, and 7.4% across the four tasks. Notably, UniME-V2-Reranker demonstrates significant advantage over LamRA in compositional understanding retrieval tasks, attributed to its use of MLLMs understanding capabilities to extract diverse and high-quality hard samples, which effectively enhance the models discriminative power. Hard Negatives Soft Score MMEB RShort RLong RCompos 60.1 61.6 63.6 63.4 68.9 72.1 82.2 89.8 93.4 60.0 63.7 64.1 Table 4: Ablation study on our proposed MLLM-as-a-Judge hard negatives mining method and MLLM judgment based training framework. Judge Model MMEB RShort RLong RCompos Qwen2.5VL-7B InternVL3-8B InternVL3-14B 63.6 58.5 63.2 72.1 70.2 72.9 93.4 91.3 93.1 64.1 64.1 63.2 Table 5: Ablation study on different MLLM-based judges. #Negatives MMEB RShort RLong RCompos 4 6 8 10 61.3 61.8 63.6 63.0 69.2 70.8 72.1 72. 91.0 91.7 93.4 93.4 62.4 61.2 64.1 63.4 Table 6: Ablation study on the number of hard negatives. Analysis Ablation on Different Components. We evaluate the effectiveness of UniME-V2 through ablation studies on the proposed MLLM-as-a-Judge hard negatives mining method and the MLLM judgment based training framework, utilizing Qwen2-VL-2B. As shown in Tab. 4, our proposed hard negatives mining method achieves performance improvements of 1.5%, 5.5%, 7.6%, and 3.7% over direct contrastive learning (e.g., VLM2Vec) on the MMEB, short-retrieval, longretrieval, and composed-retrieval tasks, respectively. Building on this, the introduction of the MLLM judgment based training framework further enhances the models discriminative ability by capturing finer semantic distinctions among candidate samples, leading to additional performance gains of 2.0%, 3.2%, 3.6%, and 0.4% for the corresponding tasks. Ablation on Different MLLM-based Judges. The comprehension ability of the MLLM acting as judge directly impacts the accuracy of the generated semantic matching scores, thereby influencing the final model performance. Therefore, based on Qwen2-VL-2B, we compare two influential MLLMs in the current open-source community: Qwen2.5-VL-7B, InternVL3-8B, and InternVL3-14B. As shown in Tab. 5, under the same inference settings, the quality of semantic matching scores produced by Qwen2.5-VL is significantly superior to that of InternVL3-8B, particularly on the MMEB (63.6 v.s. 58.5). When employing InternVL3-14B, there is notable enhancement in downstream performance compared to Intern3-8B, but it remains slightly inferior to Qwen2.5-7B. The primary reason can be attributed to differences in the distribution of instruction data used during their SFT phase. Ablation on the Number of Hard Negatives. Tab. 6 presents the impact of varying the number of hard negatives based on Qwen2-VL-2B. When the number of hard negative samples increases from 4 to 8, UniME-V2 demonstrates consistent improvements across all evaluation metrics: Figure 5: Qualitative examples. We present the retrieval and reranking results of our method across different tasks. +2.3% on MMEB, +2.9% on short retrieval, +2.4% on long retrieval, and +1.7% on composed retrieval. These gains can be attributed to the models enhanced ability to discriminate between candidates during training. However, further increasing to 10 introduces easier negatives, diminishing discriminative learning, and slightly reducing performance. Qualitative Results. Fig. 5 illustrates the qualitative results of our method across various tasks. Retrieval results from UniME-V2 are shown, with the top-1 candidate refined by UniME-V2-Reranker highlighted in red dashed boxes. UniME-V2 effectively retrieves query-relevant candidates, such as black bear and brown bear in the first example, while UniME-V2-Reranker further refines the ranking of retrieved results, prioritizing brown bear over black bear. Conclusion In this paper, we explore how to leverage the advanced understanding capabilities of MLLMs to enhance representation learning and propose novel Universal Multimodal Embedding model (UniME-V2). Specifically, we first construct potential hard negative set using global retrieval. We then introduce MLLM-as-a-Judge, which utilizes the robust semantic understanding of MLLMs to assess the alignment of query-candidate pairs and generate soft semantic matching scores. These scores guide hard negative mining by reducing false negative interference and identifying high-quality, diverse hard negatives. Additionally, the scores serve as soft labels, relaxing the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns finer-grained semantic distinctions among candidates, thereby enhancing its discriminative power. To further improve performance, we propose UniMEV2-Reranker, which incorporates joint pairwise and listwise reranking optimization based on the mined hard negatives. We conduct extensive experiments on the MMEB benchmark and various retrieval tasks and our method achieves state-of-the-art performance on average across all tasks. We hope our work provides insights into universal multimodal representation learning. References Aminabadi, R. Y.; Rajbhandari, S.; Awan, A. A.; Li, C.; Li, D.; Zheng, E.; Ruwase, O.; Smith, S.; Zhang, M.; Rasley, J.; et al. 2022. Deepspeed-inference: enabling efficient inference In SC22: of transformer models at unprecedented scale. International Conference for High Performance Computing, Networking, Storage and Analysis, 115. IEEE. An, R.; Yang, S.; Lu, M.; Zhang, R.; Zeng, K.; Luo, Y.; Cao, J.; Liang, H.; Chen, Y.; She, Q.; et al. 2024. Mc-llava: Multiconcept personalized vision-language model. arXiv preprint arXiv:2411.11706. An, R.; Yang, S.; Zhang, R.; Shen, Z.; Lu, M.; Dai, G.; Liang, H.; Guo, Z.; Yan, S.; Luo, Y.; et al. 2025a. UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens. arXiv preprint arXiv:2505.14671. An, X.; Xie, Y.; Yang, K.; Zhang, W.; Zhao, X.; Cheng, Z.; Wang, Y.; Xu, S.; Chen, C.; Wu, C.; et al. 2025b. LLaVAOneVision-1.5: Fully Open Framework for Democratized Multimodal Training. arXiv preprint arXiv:2509.23661. Andonian, A.; Chen, S.; and Hamid, R. 2022. Robust cross-modal representation learning with progressive selfdistillation. In CVPR. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. BehnamGhader, P.; Adlakha, V.; Mosbach, M.; Bahdanau, D.; Chapados, N.; and Reddy, S. 2024. Llm2vec: Large language models are secretly powerful text encoders. COLM. Cao, A.; Wei, X.; and Ma, Z. 2024. FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training. arXiv:2411.11927. Changpinyo, S.; Sharma, P.; Ding, N.; and Soricut, R. 2021. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 35583568. Chen, D.; Chen, R.; Zhang, S.; Wang, Y.; Liu, Y.; Zhou, H.; Zhang, Q.; Wan, Y.; Zhou, P.; and Sun, L. 2024a. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with visionlanguage benchmark. In ICML. Chen, L.; Li, J.; Dong, X.; Zhang, P.; He, C.; Wang, J.; Zhao, F.; and Lin, D. 2024b. Sharegpt4v: Improving large multimodal models with better captions. In ECCV. Cherti, M.; Beaumont, R.; Wightman, R.; Wortsman, M.; Ilharco, G.; Gordon, C.; Schuhmann, C.; Schmidt, L.; and Jitsev, J. 2022. Reproducible scaling laws for contrastive language-image learning. arXiv:2212.07143. Dong, G.; Song, X.; Zhu, Y.; Qiao, R.; Dou, Z.; and Wen, J.-R. 2025. Toward general instruction-following alignment for retrieval-augmented generation. In AAAI. Gao, L.; Zhang, Y.; Han, J.; and Callan, J. 2021. Scaling deep contrastive learning batch size under memory limited setup. arXiv preprint arXiv:2101.06983. Gu, T.; Yang, K.; An, X.; Feng, Z.; Liu, D.; Cai, W.; and Deng, J. 2024. Rwkv-clip: robust vision-language representation learner. In EMNLP. Gu, T.; Yang, K.; Feng, Z.; Wang, X.; Zhang, Y.; Long, D.; Chen, Y.; Cai, W.; and Deng, J. 2025a. Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs. In ACM MM. Gu, T.; Yang, K.; Zhang, C.; Xie, Y.; An, X.; Feng, Z.; Liu, D.; Cai, W.; and Deng, J. 2025b. RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm. In ACM MM. Hamza, A.; Ahn, Y. H.; Lee, S.; Kim, S. T.; et al. 2025. Llava needs more knowledge: Retrieval augmented natural language generation with knowledge graph for explaining thoracic pathologies. In AAAI. Hsieh, C.-Y.; Zhang, J.; Ma, Z.; Kembhavi, A.; and Krishna, R. 2023. Sugarcrepe: Fixing hackable benchmarks for visionlanguage compositionality. NeurIPS. Hu, X.; Yang, K.; Wang, J.; Xu, H.; Feng, Z.; and Wang, Y. 2025. Decoupled Global-Local Alignment for Improving Compositional Understanding. In ACM MM. Huang, H.; Nie, Z.; Wang, Z.; and Shang, Z. 2024a. Crossmodal and uni-modal soft-label alignment for image-text retrieval. In AAAI. Huang, W.; Wu, A.; Yang, Y.; Luo, X.; Yang, Y.; Hu, L.; Dai, Q.; Dai, X.; Chen, D.; Luo, C.; et al. 2024b. Llm2clip: Powerful language model unlock richer visual representation. arXiv:2411.04997. Jiang, T.; Song, M.; Zhang, Z.; Huang, H.; Deng, W.; Sun, F.; Zhang, Q.; Wang, D.; and Zhuang, F. 2024. E5-v: Universal embeddings with multimodal large language models. arXiv:2407.12580. Jiang, Z.; Meng, R.; Yang, X.; Yavuz, S.; Zhou, Y.; and Chen, W. 2025. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. ICLR. Lan, Z.; Niu, L.; Meng, F.; Zhou, J.; and Su, J. 2025. Llave: Large language and vision embedding models with arXiv preprint hardness-weighted contrastive learning. arXiv:2503.04812. Lee, C.; Roy, R.; Xu, M.; Raiman, J.; Shoeybi, M.; Catanzaro, B.; and Ping, W. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. ICLR. Lei, Y.; He, F.; Chen, C.; Mo, Y.; Li, S. J.; Xie, D.; and Lu, H. 2024. MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval. In NAACL. Li, B.; Zhang, Y.; Guo, D.; Zhang, R.; Li, F.; Zhang, H.; Zhang, K.; Zhang, P.; Li, Y.; Liu, Z.; et al. 2024. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML. Li, Y.; Cao, Y.; He, H.; Cheng, Q.; Fu, X.; Xiao, X.; Wang, T.; and Tang, R. 2025. M²IV: Towards Efficient and Finegrained Multimodal In-Context Learning via Representation Engineering. In Second Conference on Language Modeling. Lin, J.; Yin, H.; Ping, W.; Molchanov, P.; Shoeybi, M.; and Han, S. 2024a. Vila: On pre-training for visual language models. In CVPR. Wang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y.; Ji, J.; Yang, Z.; Zhao, L.; Song, X.; Xu, J.; Xu, B.; Li, J.; Dong, Y.; Ding, M.; and Tang, J. 2023. CogVLM: Visual Expert for Pretrained Language Models. arXiv:2311.03079. Xie, Y.; Yang, K.; Yang, N.; Deng, W.; Dai, X.; Gu, T.; Wang, Y.; An, X.; Zhao, Y.; Feng, Z.; et al. 2024. Croc: Pretraining large multimodal models with cross-modal comprehension. arXiv preprint arXiv:2410.14332. Xue, Y.; Li, D.; and Liu, G. 2025a. Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying. arXiv preprint arXiv:2506.02020. Xue, Y.; Li, D.; and Liu, G. 2025b. Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying. arXiv preprint arXiv:2506.02020. Yang, K.; Gu, T.; An, X.; Jiang, H.; Dai, X.; Feng, Z.; Cai, W.; and Deng, J. 2025. Clip-cid: Efficient clip distillation In AAAI, volume 39, via cluster-instance discrimination. 2197421982. Yuksekgonul, M.; Bianchi, F.; Kalluri, P.; Jurafsky, D.; and Zou, J. 2022. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv:2210.01936. Zhai, X.; Mustafa, B.; Kolesnikov, A.; and Beyer, L. 2023. Sigmoid loss for language image pre-training. In ICCV. Zhang, B.; Zhang, P.; Dong, X.; Zang, Y.; and Wang, J. 2024a. Long-clip: Unlocking the long-text capability of clip. In ECCV. Zhang, K.; Luan, Y.; Hu, H.; Lee, K.; Qiao, S.; Chen, W.; Su, Y.; and Chang, M.-W. 2024b. Magiclens: Selfsupervised image retrieval with open-ended instructions. arXiv:2403.19651. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. NeuriPS, 36: 4659546623. Zheng, T.; Zhang, Y.; An, X.; Feng, Z.; Yang, K.; and Ding, Q. 2025. Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval. In EMNLP. Lin, S.-C.; Lee, C.; Shoeybi, M.; Lin, J.; Catanzaro, B.; and Ping, W. 2024b. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual instruction tuning. NeurIPS. Liu, Y.; Chen, P.; Cai, J.; Jiang, X.; Hu, Y.; Yao, J.; Wang, Y.; and Xie, W. 2024. LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant. CVPR. Muennighoff, N.; Tazi, N.; Magne, L.; and Reimers, N. 2022. MTEB: Massive Text Embedding Benchmark. arXiv:2210.07316. Nielsen, F. 2020. On generalization of the JensenShannon divergence and the JensenShannon centroid. Entropy. Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma, S.; and Wei, F. 2023. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824. Plummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo, J. C.; Hockenmaier, J.; and Lazebnik, S. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In ICML. Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; and Komatsuzaki, A. 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114. Sun, Q.; Wang, J.; Yu, Q.; Cui, Y.; Zhang, F.; Zhang, X.; and Wang, X. 2023. EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters. arXiv:2402.04252. Tang, F.; Huang, Z.; Liu, C.; Sun, Q.; Yang, H.; and Lim, S.-N. 2025a. Intervening anchor token: Decoding strategy in alleviating hallucinations for MLLMs. In ICLR. Tang, F.; Liu, C.; Xu, Z.; Hu, M.; Huang, Z.; Xue, H.; Chen, Z.; Peng, Z.; Yang, Z.; Zhou, S.; et al. 2025b. Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding. In CVPR, 2614726159. Tschannen, M.; Kumar, M.; Steiner, A.; Zhai, X.; Houlsby, N.; and Beyer, L. 2023. Image captioners are scalable vision learners too. NeurIPS. Wang, H.; Li, L.; Qu, C.; Zhu, F.; Xu, W.; Chu, W.; and Lin, F. 2025a. To code or not to code? adaptive tool integration for math language models via expectation-maximization. arXiv preprint arXiv:2502.00691. Wang, H.; Qu, C.; Huang, Z.; Chu, W.; Lin, F.; and Chen, W. 2025b. Vl-rethinker: Incentivizing self-reflection of visionlanguage models with reinforcement learning. arXiv preprint arXiv:2504.08837. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. This supplementary material elaborates on our experimental setup, covering training configurations, instruction prompts for UniME-V2-Reranker and evaluation benchmarks for retrieval tasks. It also includes extended results such as an ablation study on temperature and detailed performance analysis on the MMEB benchmark. Additionally, we provide supplementary visualizations of training data samples, retrieval outputs, and reranking results."
        },
        {
            "title": "Training Details",
            "content": "We provide the training configurations of UniME-V2 in Tab. 7 and UniME-V2-Reranker in Tab. 8. UniME-V2: Following UniME (Gu et al. 2025a), we adopt identical experimental settings for training UniME-V2. We configure LoRA rank as 16 and employ GradCache (Gao et al. 2021) for efficient training over 2,000 steps using 8A800 GPUs (80GB memory each). The learning rate is set to 1104 for the Qwen series and 2105 for LLaVAOneVision. Due to memory constraints, the input resolution is fixed at 336 for LLaVA-OneVision and 672 for the Qwen series. Hyperparameter Qwen2-VL-2B/7B LLaVA-OV-7B Training samples Batch size Learning rate LoRA rank Training steps Optimizer Infra Max length temperature #Hard Negatives Image Resolution Precision GPU configuration Random Seed 662K 1024 110 2105 16 2000 AdamW GradCache 4096 0.02 8 672 336 BF16 8A800 42 Table 7: Training hyperparameters and computational requirements for UniME-V2 (Qwen2-VL-2B/7B) and UniME-V2 (LLaVA-OneVision-7B). UniME-V2-Reranker: Following LamRAs experimental setup (Liu et al. 2024), we adopt Qwen2.5-VL-7B as the backbone for UniME-V2-Reranker. The model is trained using LoRA with rank of 128 for 1 epoch almost 2,000 steps. All experiments are conducted using the lmms-finetune infrastructure with maximum sequence length of 4096 tokens. Detail Instruction Prompt for UniME-V2-Reranker The prompt template employed for pairwise training of UniME-V2-Reranker is presented below: Hyperparameter Qwen2.5-VL-7B"
        },
        {
            "title": "Training samples\nBatch size\nLearning rate\nLoRA rank\nTraining epochs\nOptimizer\nInfra\nMax length\nPrecision\nDeepSpeed Stage\nGPU configuration\nRandom Seed",
            "content": "662K 64 2105 128 1 AdamW lmms-finetune 4096 BF16 2 8A800 42 Table 8: Training hyperparameters and computational requirements for UniME-V2-Reranker (Qwen2.5-VL-7B). will provide you with query and candidate. Please evaluate whether the candidate meets the requirements of the query. If it does, respond with Yes; if it doesnt, respond with No. Query:<Query>, Candidate:<Candidate>. The prompt used for listwise training of UniME-V2Reranker is shown below: will provide you with query followed by multiple candidates in the format: (1) cand1 (2) cand2, etc. Each candidate is independent of the others. Evaluate each candidate against the query, and respond with the number corresponding to the candidate that best meets the requirements of the query. Query:<Query>, Candidates:<Candidate list>. Retrieval Task Evaluation Benchmarks We evaluate UniME-V2 and UniME-V2-Reranker on diverse retrieval benchmarks, including short-caption, long-caption, and compositional image-text tasks (Tab. 9). For each benchmark, we follow the standard evaluation protocol. In retrieval tasks, we primarily report Recall@1 as the evaluation metric, using the prompt Represent the image/text for both image and text instructions."
        },
        {
            "title": "Benchmark",
            "content": "Zero-shot #Queries #Cands Flickr30K (Plummer et al. 2015) COCO (Lin et al. 2014) ShareGPT4V (Chen et al. 2024b) Urban1K (Zhang et al. 2024a) SugarCrepe (Hsieh et al. 2023) 1K 5K 1K 1K 7.5K 5K 25K 1K 1K 2 Table 9: Summary of the evaluation benchmarks. # Queries represents the number of test queries, and # Cands denotes the number of test candidates per query. Figure 7: Qualitative examples. We present the additional retrieval and reranking results of our method across different tasks. Visualization of the Retrieval and Rerank Results Fig. 7 presents additional qualitative results demonstrating our methods performance across multiple tasks. The visualization reveals that while UniME-V2 successfully retrieves query-matched candidates, UniME-V2-Reranker further refines these results by selecting the optimally matched candidate as the final output. Temperature MMEB RShort RLong RCompos 0.03 0.02 0.01 61.9 63.6 62.1 70.6 72.1 70.1 92.0 93.4 91.1 65.8 64.1 66.5 Table 10: Ablation study on the temperature. We report the mean scores on the MMEB benchmark, short and long crossmodal retrieval, as well as compositional cross-modal retrieval. Figure 6: Qualitative examples. We present examples showing queries and their corresponding hard negative candidates processed after our hard negative mining pipeline."
        },
        {
            "title": "External Results",
            "content": "Ablation on the Temperature We conduct additional experiments with UniME-V2 (Qwen2VL-2B) to analyze the impact of temperature in the final loss function. As evidenced by Tab. 10, temperature value of 0.02 yields optimal performance across all evaluation metrics, including MMEB, short & long retrieval, and compositional retrieval tasks. Specific Results on the MMEB Benchmark Tab. 11 presents comprehensive results on the MMEB benchmark across eight models: BLIP2, MagicLens, EVA-CLIP, E5-V, VLM2Vec, UniME, UniME-V2, and UniME-V2. Results for BLIP2 through UniME are reproduced directly from UniME (Gu et al. 2025a). Our implementation details specify that UniME-V2 employs the Qwen2-VL-7B backbone, while UniME-V2 represents uses LLaVA-OneVision-7B as its backbone. Further Analysis Visualization of the Training Data Fig. 6 presents training examples from the MMEB dataset annotated with their semantic matching scores, which obtained after our proposed pipeline. The visualization demonstrates: (1) target candidates achieving the highest match scores (nearly 1.0), (2) partially relevant candidates with intermediate scores (between 0.0 and 1.0), and (3) irrelevant candidates receiving near-zero scores. BLIP2 MagicLens EVA-CLIP E5-V VLM2Vec UniME UniME-V2 UniME-V2 Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews t2i VisualNews i2t MSCOCO t2i MSCOCO i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding Final Score (36 tasks) All All IND All OOD 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27.0 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33. 28.9 47.4 59.5 52.0 47.0 28.0 25.3 25.1 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38.8 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8.3 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35.4 22.1 22.8 35.6 23.4 26. 27.1 31.0 23.7 75.0 33.8 49.3 44.3 62.7 38.7 54.8 95.4 67.8 38.7 56.0 9.9 2.8 7.4 6.0 1.5 2.2 14.1 4.3 44.7 10.8 10.4 20.4 36.0 82.4 88.2 65.3 67.2 0.2 70.9 16.1 46.7 1.8 95.6 49.2 35.8 59.9 70.0 70.2 58.9 43.7 38.1 45. 40.5 31.5 49.3 76.7 52.3 32.0 18.2 56.7 34.2 5.9 39.7 15.1 4.7 9.1 8.7 4.2 4.5 9.6 8.6 34.1 9.5 10.8 57.6 41.0 43.9 46.8 68.6 54.8 0.1 33.7 11.2 61.0 0.5 53.8 39.4 41.7 62.2 74.9 61.8 60.2 37.5 34.2 33.4 66.5 76.4 60.9 84.0 73.2 42.1 39.9 74.6 34.3 16.1 56. 66.5 54.9 64.4 34.8 33.1 49.8 37.3 39.9 57.3 65.7 50.4 75.3 51.3 70.7 75.2 69.9 67.7 63.3 83.6 15.2 63.4 49.6 73.7 63.3 77.0 85.9 83.8 83.6 82.6 63.3 64.9 53.9 71.3 79.5 64.6 90.4 75.9 45.6 45.5 78.4 36.4 18.7 60.6 68.3 58.7 67.6 37.0 33.4 51.7 40.5 42.7 63.6 65.2 52. 79.7 52.2 74.8 78.8 74.9 73.8 66.2 89.8 16.5 66.6 55.7 86.2 67.9 76.5 89.3 90.6 84.1 85.1 66.6 68.4 57.9 80.3 66.9 65.9 84.9 78.9 42.5 53.7 87.9 35.0 32.3 64.0 59.3 32.3 91.2 63.9 56.9 60.1 44.5 47.4 55.8 78.4 60.1 83.4 64.0 79.9 83.5 77.7 73.0 69.3 91.5 28.5 68.8 71.2 84.4 73. 69.3 88.4 89.7 78.7 82.8 68.0 72.0 63.0 78.8 66.6 65.3 92.0 78.7 42.9 48.0 89.3 73.1 19.8 65.3 71.9 71.4 92.6 63.5 55.8 62.5 54.0 53.7 69.5 84.5 67.6 84.2 65.5 77.3 79.2 79.1 75.2 68.1 90.6 26.4 71.2 68.0 88.2 72.9 78.2 94.6 91.4 93.8 90. 71.2 74.8 66.7 Table 11: The comprehensive evaluation results comparing baseline methods with our UniME-V2 on the MMEB benchmark, comprising 20 in-distribution and 16 out-of-distribution datasets (the OOD marked with yellow). The UniME-V2 uses Qwen2VL-7B as its backbone, and UniME-V2 denotes using LLaVA-OneVision-7B as its backbone."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "LMMs-Lab Team",
        "M.R.L. Team",
        "MiroMind AI",
        "The University of Sydney"
    ]
}