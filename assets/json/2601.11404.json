{
    "paper_title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "authors": [
        "Linqing Zhong",
        "Yi Liu",
        "Yifei Wei",
        "Ziyu Xiong",
        "Maoqing Yao",
        "Si Liu",
        "Guanghui Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively."
        },
        {
            "title": "Start",
            "content": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models Linqing Zhong1,2 Yi Liu2 Yifei Wei1,2 Ziyu Xiong2 Maoqing Yao2 Si Liu1 Guanghui Ren2* 1Beihang University 2AgiBot 6 2 0 2 6 1 ] . [ 1 4 0 4 1 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoningsuch as sub-task prediction (language) or goal image synthesis (vision)to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chainof-Thought (ACoT), paradigm where the reasoning process itself is formulated as structured sequence of coarse In this paper, action intents that guide the final policy. we propose ACoT-VLA, novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit actionlevel reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBEROPlus and VLABench, respectively. 1. Introduction To overcome the generalization limits of task-specific robot policies [11, 48, 61], recent work has converged on VisionLanguage-Action (VLA) models [5, 24, 33, 41], which always leverage pre-trained Vision-Language Model (VLM) [1, 2, 46] to encode visual and linguistic inputs into *Corresponding author Figure 1. Chain-of-Thought in different space. (a) Language CoT paradigm predicts sub-tasks as intermediate reasoning. (b) Visual CoT paradigm synthesizes goal image to provide guidance for action policy. (c) Our proposed Action CoT directly operates in action space and provides homogeneous action guidance. latent representation that conditions an action decoder. Recent advancements seek to improve the mapping from the input space to the action space by introducing the intermediate reasoning step by language generation, leading to more generalized and precise action outputs [22, 53], as visualized in Fig. 1(a). parallel thrust leverages world models [16, 50, 65] to simulate environmental dynamics, directly enhancing the efficacy and goal-oriented nature of the generated action sequences [58, 62], as shown in Fig. 1(b). Despite the promising trajectory set by these paradigms, critical challenge persists: existing generalist policies think predominantly in the vision-language (input) space, often failing to adequately address the inherent disparity between these rich, semantic representations and the requirements of precise, low-level action execution (output). Specifically, the knowledge encoded within the VLM backbone of VLA models is derived from pre-training on web1 scale datasets focused on semantic alignment and questionanswering, yielding representations optimized for linguistic understanding rather than physical dynamics. Similarly, while world models forecast future visual states conditioned on inputs, their guidance remains tethered to naturally visual representations. Crucially, both semantic and visual forms of reasoning only offer suboptimal, indirect guidance for generating the necessary action sequence. Consequently, these prevailing approaches rely on an inherently constrained information conduit, struggling to convey the full, granular knowledge of the action space essential for truly grounded and accurate robotic policy learning. The inherent semantic-kinematic gap in existing policies, i.e., fundamental disconnect between high-level, abstract inputs and low-level, executable motor commands, necessitates paradigm shift in how guidance is provided. We contend that to bridge this chasm, policies require guidance that is kinematically coherent, rather than purely semantic or visual. This core principle underpins our novel framework: Action Chain-of-Thought (ACoT) (Fig. 1(c)). We redefine the thought process not as sequence of linguistic tokens, but as structured chain of explicit, kinematically-grounded action intents. This approach furnishes the policy with direct motion cues, supplanting indirect representations. In manner analogous to learning from physical demonstration, this direct conditioning on action-space information enables substantially more efficient and veridically grounded policy learning process. This foundational shift, however, introduces critical and distinct research challenge: How can we robustly and efficiently synthesize the complex, high-dimensional motion cues required for ACoT reasoning from the raw, heterogeneous multimodal inputs? Action-related information manifests in two complementary forms, i.e., explicit or implicit. The explicit form corresponds to observable motion trajectories, such as those in human demonstrations, which directly encode executable patterns of behavior. In contrast, the implicit form resides in latent cues, e.g., linguistic expressions like reach out or grasp, as well as interaction intents embedded in visual contexts. Although these cues are not presented as explicit robotic trajectories, they implicitly define distributions over feasible actions within the action space. Building upon this insight, we introduce two synergistic mechanisms to generate both explicit and implicit guidance in the action space. We first propose the Explicit Action Reasoner (EAR), which is realized as light-weight transformer. Particularly, EAR synthesizes coarse-grained motion trajectories conditioned on multimodal observations, offering direct and executable guidance within the action space. Secondly, we devise the Implicit Action Reasoner (IAR), which infers latent action priors through applying cross-attention modeling between downsampled multimodal representations and learnable queries, thereby providing implicit behavioral priors. Note that these two mechanisms are inherently complementary to each other. Subsequently, through jointly leveraging both EAR and IAR, we develop ACoT-VLA, an integrated Action Chain-of-Thought framework that enables grounded generalist robot policy learning. Extensive experiments across both real-world settings and three simulation benchmarks consistently demonstrate the effectiveness and versatility of our ACoT-VLA. To summarize, our main contributions are as follows: Conceptually, we introduce Action Chain of Thought (ACoT), new paradigm for generalist robot policies. To the best of our knowledge, this is the first work to formulate the deliberative process as structured chain of explicit action-space intents, rather than abstract linguistic or visual sub-goals. We delve into essential action space guidance and propose the Explicit and Implicit Action Reasoners, which provide both explicit trajectory guidance and implicit behavioral inspiration for action prediction. Building upon these two modules, we further propose ACoT-VLA, unified framework for grounded generalist robot policy learning. Empirically, we validate our approach through extensive simulation and real-world experiments, achieving state-of-the-art performance on multiple benchmarks, i.e., 98.5%, 84.1% and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively. 2. Related Works Vision-Language-Action Models. VLA models [14, 18, 19, 27] incorporate pre-trained VLM models to predict language-driven robotic action sequences. Early works [24, 67] formulate robot control as an autoregressive sequence generation problem, discretizing continuous actions into bins. Inspired by generative modeling [31, 38, 66], increasing works [5, 20, 33] adopt diffusion-based action policies to synthesize smooth and high-quality action trajectories. Given that robotic manipulation inherently occurs in threedimensional space, line of studies [30, 52, 59] have sought to enhance the spatial reasoning capability of VLA models by integrating 3D priors. For instance, SpatialVLA [41] integrates spatial embeddings to endow model with 3D awareness, while 4D-VLA [55] incorporates both spatial and temporal information to enrich representations. Besides, due to the scarcity of large-scale real-world robot demonstrations, series of efforts [6, 10, 12, 23, 37, 57] focus on datacentric solutions, constructing large-scale robotic datasets through simulation or real-world collection to scale up policy learning. Moreover, recent large-scale co-training approaches, such as π0.5 [22], GenieReasoner [35] and Gemini Robotics [47], demonstrate the potential of unifying web-scale language understanding with action learning, en2 Figure 2. Architectural Overview of ACoT-VLA. The framework consists of three main components operating on features from shared VLM backbone. (a) The Explicit Action Reasoner (EAR) is Transformer-based module that synthesizes coarse reference trajectory, providing explicit action-space guidance. (b) The Implicit Action Reasoner (IAR) employs cross-attention mechanism with learnable queries to extract latent action priors from the VLMs internal representations. (c) The Action-Guided Prediction (AGP) head synergistically integrates both explicit and implicit guidances via cross-attention to condition the final denoising process, producing the executable action sequence. hancing the policys generalization ability while retaining the reasoning capability of pre-trained foundation models. World-Model-based Policies. Advances in world models have demonstrated remarkable capability in synthesizing high-fidelity images and temporally coherent videos. Building upon such progress, emerging researches [26, 29, 36, 56] exploit their predictive dynamics to implicitly guide action generation. Specifically, CoT-VLA [60] introduces visual chain-of-thought reasoning by forecasting sub-goal images, explicitly integrating visual reasoning into action prediction. WorldVLA [8] employs an autoregressive architecture that unifies perception and action generation within single framework. DreamVLA [58] extends beyond visual prediction and enriches world modeling with dynamic, depth, and semantic cues, improving the models physical consistency. Collectively, existing world-model-based approaches adopt knowledge-forecasting perspective, incorporating primarily visual guidance into action trajectories generation. In contrast to previous works focusing on visual or linguistic intermediaries for robotic policy learning, our key insight lies in investigating guidance directly within the action space, which intrinsically mitigates the heterogeneity between perception and action, enabling the model to effectively learn action-relevant priors. Sec. 3.1. The core of our method lies in two distinct action reasoners introduced in Sec. 3.2 and Sec. 3.3, which provide explicit and implicit guidance within the action space. We conclude by illustrating the policy prediction strategy that effectively integrates this action guidance during policy learning (Sec. 3.4). 3.1. Problem Formulation Given natural language instruction and current visual observation ot, the generalist robot policy πθ aims to predict action sequences at:t+H1 that accomplishes the specified task. The process can be formally expressed as: at:t+H1 = πθ(ot, l), (1) where represents the action horizon. Numerous works introduce additional guidance signals g, which encapsulates various forms of auxiliary information to enhance policys prediction ability. Specifically, these guidance signals can be broadly categorized into two types: language-level guidance glang and vision-level guidance gvis. The former is primarily adopted by VLM-based methods, e.g., leveraging LLMs reasoning capabilities to predict sub-tasks, while the latter is always employed by world-model-based approaches, such as simulating future observations. Such relationship can be formulated as: 3. Methodology In this section, we present detailed investigation into how to generate effective action space guidance and integrate it into robotic policy learning. We first define the robotic manipulation problem and formulate our proposed approach in πθ(at:t+H1, ot, l) = πθ(at:t+H1 ot, l, g)πθ(g ot, l), (2) where {glang, gvis}. Conversely, we shift the focus toward the action domain itself and investigate cues operating directly in the action space, symbolized as gaction. The above guidances are extended as {glang, gvis, gaction}."
        },
        {
            "title": "Such action guidance can intuitively be disentangled into",
            "content": "explicit and implicit forms. The explicit guidance gex action provides direct priors in the form of reference action sequences, whereas the implicit guidance gim action arises from contextual signals, e.g., action distribution inherently implied in linguistics. 3.2. Explicit Action Reasoner To incorporate explicit action trajectories into the thinking process of πθ to generate high-quality action predictions, we propose the Explicit Action Reasoner (EAR). We design mechanism that enables the model to autonomously synthesize reference action sequences as internal guidance for policy learning. Analogously, this formulation can be viewed as an action-space transfer of selfconditioning in generative models [9, 34], where incorporating prior estimates into the generation process has been shown to markedly improve sample quality. Building upon this principle, we instantiate EAR as light-weight transformer, as shown in Fig. 2 (a), generating kinematically plausible action reference as explicit action-space guidance gex action for downstream action policy. Formally, given visual observation ot and language instruction l, pre-trained VLM encodes them into contextual key-value cache: (K VLM 1:N , VLM 1:N ) = VLM(ot, l), (3) where represents the number of layers of VLM. Subsequently, the EAR, denoted as πref θ , takes noisy action sequence at:t+H ref 1 as input, where ref indicates the horizon of reference actions. The sequence is first embedded into an initial hidden representation href 0 , which serves as the input to EARs transformer layers. At each transformer layer i, we adopt self-attention, along with crossattention with the contextual key-value cache from the corresponding VLM layer: , VLM i1, VLM href = Self-Attn(href i1) + CrossAttn(href ), (4) where self-attention module captures temporal dependencies within the action sequence and cross-attention mechanism injects multimodal contextual priors from the VLM. Then, the intermediate representation href is processed by feed-forward network (FFN) in residual-parallel manner, updating the i-th EAR representation href : = href href i1 + FFN(href ). (5) Through training via flow matching, πref θ learns distribution over action trajectories, producing denoised action sequence: aref t:t+H ref 1 = πref θ (at:t+H ref 1, VLM 1:N , VLM 1:N ). (6) 4 The generated sequence is then encoded via MLP projector to obtain action embedding ex, which serves as explicit action-space guidance gex action for action policy learning. 3.3. Implicit Action Reasoner Beyond the explicit action trajectories, the multimodal latent space of VLM also encodes implicit motion cues [13, 40], e.g., visual affordances and action-related semantics. Effectively extracting these action-relevant representations potentially offers complementary guidance. To this end, we introduce an Implicit Action Reasoner (IAR), which directly operates on the VLMs keyvalue cache. Concretely, as presented in Fig. 2 (b), for each VLM layer [1, ], we initialize learnable matrix Qi RM d, where is hyperparameter and represents VLMs hidden dimension. Considering the information redundancy within VLMs keyvalue cache and computational efficiency, we first downsample the corresponding keyvalue pairs into lower-dimensional space, which is formulated as: = QiW (i) = VLM = VLM (i) , , i (i) , (7) are learnable linear prowhere (i) , (i) jectors and d. , (i) Rdd Later, cross-attention is applied to extract action-relevant and information from each . The resulting features are subsequently integrated via average pooling and transformed through MLP projector, as visualized in Fig. 2 (b), producing compact representations that capture the implicit action semantics zim embedded in VLMs i-th layer: = MLP(Pool(CrossAttn(Q zim i, i, ))). (8) Then, through aggregating these representations across layers, we obtain implicit action-related feature im, which serves as implicit action-space guidance gim action, complementing the explicit motion priors. 3.4. Action-Guided Prediction Building upon the explicit action embedding ex produced by EAR and implicit action-related feature im obtained in IAR, in this section, we introduce the Action-Guided Prediction (AGP) strategy to incorporate both action guidances into policy learning. As illustrated in Fig. 2 (c), given noisy action segment at:t+H1, we first encode it into noisy action embedding via MLP projector. Particularly, unlike previous approaches that directly feed this embedding into action head πhead , we treat it as action query, denoted as Qaction, which interacts with both ex and im to retrieve complementary priors for conditional prediction. θ Specifically, we perform dual cross-attention operations: Sex = CrossAttn(Qaction, ex, ex), (9)"
        },
        {
            "title": "Guidance",
            "content": "Diffusion Policy [11] Octo [48] CoT-VLA [60] WorldVLA [8](256*256) WorldVLA [8](512*512) DreamVLA [58] UniVLA [49] F1 [36] GE-Act [29] TraceVLA [64] OpenVLA [24] UniAct [63] SpatialVLA [41] ThinkAct [17] π0-FAST [39] FPC-VLA [51] SmolVLA [43] GR00T-N1 [4] π0 [5] DD-VLA [28] MemoryVLA [42] π0.5 [22] OpenVLA-OFT [25]"
        },
        {
            "title": "Ours",
            "content": ""
        },
        {
            "title": "Action",
            "content": "Spatial SR Rank 78.3 78.9 23 22 Object SR Rank 92.5 85.7 15 23 Goal SR Rank 68.3 84.6 24 Long SR Rank 50.5 51.1 24 23 Avg. SR Rank 72.4 75.1 24 22 87.5 85.6 87.6 97.5 95.4 98.2 98.2 84.6 84.7 77.0 88.2 88.3 96.4 87.0 93.0 94.4 96.8 97.2 98.4 98.8 97. 99.4 17 19 16 7 11 4 4 21 20 24 15 14 10 18 13 12 9 8 3 2 6 1 91.6 89.0 96.2 94.0 98.8 97.8 97.6 85.2 88.4 87.0 89.9 91.4 96.8 92.0 94.0 97.6 98.8 98.6 98.4 98.2 98. 99.6 17 20 12 13 2 8 9 24 21 22 19 18 11 16 13 9 2 4 5 7 5 1 87.6 82.6 83.4 89.5 93.6 95.4 95.8 75.1 79.2 77.0 78.6 87.1 88.6 86.2 91.0 93.0 95.8 97.4 96.4 98.0 97. 98.8 14 19 18 12 9 8 6 23 20 22 21 15 13 16 11 10 6 4 5 2 3 1 69.0 59.0 60.0 89.5 94.0 91.3 94.4 54.1 53.7 70.0 55.5 70.9 60.2 82.2 77.0 90.6 85.2 92.0 93.4 92.4 94. 96.0 16 19 18 10 4 8 3 21 22 15 20 14 17 12 13 9 11 7 5 6 2 1 81.1 79.1 81.8 92.6 95.5 95.7 96.5 74.8 76.5 77.8 78.1 84.4 85.5 86.9 88.8 93.9 94.1 96.3 96.7 96.9 97. 98.5 17 18 16 11 8 7 5 23 21 20 19 15 14 13 12 10 9 6 4 3 2 1 Table 1. Comparison on the LIBERO benchmark. The best results are highlighted in bold. All metrics are average success rates (%). Sim = CrossAttn(Qaction, im, im), (10) where Sex and Sim denote the attended representations guided by explicit and implicit priors, respectively. Note that although both encode action-relevant information, they may highlight different facets of the underlying motion. For instance, explicit priors provide kinematic cues, whereas implicit priors capture latent action tendencies. Hence, to effectively combine these complementary guidance, we concatenate the two attended features and process them through self-attention fusion block, which integrates the priors into unified representation h: = Self-Attn([Sex; Sim]). (11) Eventually, the aggregated representation is fed into πhead which predicts the denoised action sequence at:t+H1. Training Objectives. The entire framework is optimized under standard flow-matching mean-squared error (MSE) objective. The training losses consist of two parts, i.e., flowmatching MSE for both Explicit Action Reasoner πref θ and action head πhead , respectively. θ Hence, the overall objective is: , denoted as Lπref and Lπhead θ , θ θ Ltotal = λ1Lπref θ + λ2Lπhead θ , (12) where λ1 and λ2 are balance factors. Teacher Forcing Stabilization. During training, the outputs of πref can be unstable. To stabilize optimization, θ we compute ex directly from ground-truth reference trajectories instead of from πref θ predictions, preventing optimization interference to πhead . During inference, the model θ 5 switches to fully self-conditioned mode, where πref θ autonomously generates the reference actions to guide πhead θ in action prediction. 4. Experiments In this section, we first outline the experimental setup in Sec. 4.1. Then, in Sec. 4.2, we evaluate our approach on three simulation benchmarks, followed by comprehensive ablation studies in Sec. 4.3. Moreover, we present realworld deployment results in Sec. 4.4 to evaluate real-world applicability. 4.1. Experimental Setup Data Sources. For simulation experiments, we strictly follow the official training splits provided by the corresponding benchmark (LIBERO [32], LIBERO-Plus [15], and VLABench [57]), and train our models exclusively on their standard demonstration datasets without introducing any additional data. For the real-world setting, all demonstrations used for model training are collected on our own robotic platform. More details about data sources are introduced in Appendix A. Implementation Details. We implement our approach upon π0.5 [22]. Specifically, we adopt SigLIP [54] as the visual encoder, while the LLM backbone is instantiated as Gemma 2B architecture [3] with = 18 layers and hidden size = 2048. For frame processing, each input frame is resized to 224 224 prior to the visual encoder. Regarding"
        },
        {
            "title": "Methods",
            "content": "WorldVLA [8] OpenVLA [24] NORA [21] UniVLA [7] π0-Fast [39] RIPT-VLA [44] OpenVLA-OFT [25] OpenVLA-OFT+ [15] π 0 [5] π 0.5 [22] Ours"
        },
        {
            "title": "Layout",
            "content": "Avg."
        },
        {
            "title": "Action",
            "content": "0.1 0.8 2.2 1.8 65.1 55.2 56.4 92.8 79.6 70.3 91.2 27.9 3.5 37.0 46.2 21.6 31.2 31.9 30. 21.1 41.7 62.5 41.6 23.0 65.1 69.6 61.0 77.6 79.5 85.8 72.5 81.1 80. 43.7 8.1 45.7 69.0 73.2 88.4 88.7 94.9 84.7 97.3 95.1 17.1 34.8 58.6 81.0 73.2 91.6 93.3 93. 86.2 94.6 91.5 10.9 15.2 12.8 21.2 74.4 73.5 75.8 89.3 68.3 71.8 88. 38.0 28.5 62.1 31.9 68.8 74.2 74.2 77.6 69.4 84.9 84.9 25.0 15.6 39.0 42.9 61.6 68.4 69.6 79. 67.4 75.7 84.1 Table 2. Performance comparison on the LIBERO-Plus benchmark. Best results are highlighted in bold. An asterisk (*) denotes results reproduced by us for fair comparison. the EAR, we employ compact Transformer-based design composed of = 18 layers. Concerning the IAR, each learnable query matrix Qi is configured with row dimension of = 1. The reduced dimension in the downsampling strategy is set to = 128. In terms of model training, unless explicitly specified, the horizon of predicted reference actions ref and action policy output are fixed to 15 and 10, with action shift set to 2 and 1, respectively. To clarify, the action shift specifies the temporal interval relative to the expert demonstration. For instance, shift of 1 yields frame-aligned predictions, whereas shift of 2 skips one intermediate frame. We set the balance factors in training losses as λ1 = λ2 = 0.5. Training Configuration. We adopt unified set of training hyperparameters across all experiments unless explicitly specified. Concretely, the learning rate follows cosinedecay schedule with warm-up phase of 10K steps, peak learning rate of 5e5, and decay toward 5e5 over 10K steps. Optimization is performed with AdamW with gradient-norm clipping set to 1.0. An exponential moving average (EMA) of model parameters is maintained with decay rate of 0.999. Regarding hardware settings, model training is performed on single node equipped with 8 NVIDIA H100 GPUs using bfloat16 precision. And the inference is conducted on single NVIDIA RTX 4090. 4.2. Simulation Experiments In this section, we conduct the simulation evaluations across three benchmarks, i.e., LIBERO [32], LIBERO-Plus [15], and VLABench [57], to comprehensively evaluate our approachs performance and generalization capabilities under diverse task structures. LIBERO Benchmark. LIBERO is widely adopted simulation benchmark for evaluating generalist robotic policies. It consists of four task suites, i.e., LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. It is designed to probe different aspect of policy capability, including spatial awareness, object-centric manipulation, goal completion, and long-horizon reasoning. Each suite consists of 10 tasks and provides 50 human-teleoperated demonstrations per task for policy training. Following the official evaluation protocol, we evaluate our policy on all tasks within the benchmark. For each task, the policy is evaluated over 50 independent trials, resulting in 500 rollouts in total. As reported in Table 1, the quantitative evaluation results demonstrate that our proposed approach outperforms existing methods across all tracks. Compared to the previous state-of-the-art method π0.5, our approach achieves 1.6% absolute improvement in average success rate, highlighting the clear advantages of incorporating action-space guidance. Notably, we observe pronounced improvement on the LIBERO-Long suite, where tasks require long-horizon manipulation with strict error control. We attribute this advantage to the nature of our proposed ACoT. Particularly, unlike languageor vision-CoT, whose intermediate reasoning remains abstract or indirect with respect to action execution, our proposed ACoT naturally operates in precise representation. Through leveraging actions as intermediate reasoning tokens, the model feeds the following action head with structured and fine-grained guidance, which significantly enhances the robustness to error accumulation in long-horizon manipulation tasks. LIBERO-Plus Benchmark. Built upon LIBERO, LIBERO-Plus is an extended robustness-oriented benchmark, designed to systematically evaluate generalist robotic policies under controlled distribution shifts. Concretely, introduces 7 perturbation dimensions, LIBERO-Plus i.e., languagerobot-initial-states, variations, background-textures, sensor-noise and object-layout, which aim to expose hidden failure modes under standard evaluations. Notably, LIBERO-Plus consists of 10, 030 evaluation episodes, providing statistically reliable evaluation. lighting-conditions, camera-viewpoints, Following standard training configuration in LIBEROPlus [15] , we train our policy on the provided training set for total of 100K optimization steps. In terms of evaluation, we adhere to the protocol established in the bench-"
        },
        {
            "title": "Guidance",
            "content": "π0 [5] π0.5 [22]"
        },
        {
            "title": "Action",
            "content": "In-dist."
        },
        {
            "title": "Texture",
            "content": "Avg. IS 67.8 75.0 79.8 PS 62.7 60. 66.1 IS 44.0 49.6 54.1 PS 33.6 35. 38.9 IS 54.9 57.5 52.3 PS 43.0 41. 37.8 IS 58.0 57.1 56.8 PS 38.7 30. 39.6 IS 50.6 62.0 74.6 PS 42.5 47. 54.6 IS 55.0 60.2 63.5 PS 44.1 43. 47.4 Table 3. Comparison on the VLABench benchmark. IS and PS represent Intention score and Progress score, respectively. Name EAR IAR Spatial Object Goal Long Avg. Baseline #1 #2 # 98.8 99.0 99.2 99.4 98. 99.4 99.2 99.6 98.0 98.0 98.2 98.8 92.4 96.6 95.6 96.0 96. 98.3 98.1 98.5 Table 4. Module ablations. The performance is gradually improved with the continuous addition of proposed methods. Name Baseline +EAR Action shift Action horizon Equi. horizon Spatial Object Goal Long Avg. 1 1 2 1 2 2 3 10 5 30 15 30 30 10 10 10 30 30 60 90 98.6 99.4 99.6 99.2 99.0 99.4 98.8 99. 99.4 99.6 99.2 99.4 99.0 99.4 96.4 98.8 98.4 97.6 98.0 98.2 97.4 92.2 95.0 94.4 95.6 96.6 95.0 96.2 96. 98.2 98.0 97.9 98.3 97.9 98.0 mark, i.e., each episode is executed once without repeated rollouts. Note that the average success rate is computed over the entire evaluation set. As shown in Table 2, our method significantly boosts the policys performance, surpassing all previous methods by huge margin. In particular, our approach demonstrates pronounced robustness under challenging perturbations such as camera-viewpoint shifts (+11.6%), robot initial-state perturbations (+16.3%), and sensor noise (+12.5%), where existing language-guided or vision-guided policies exhibit significant degradation. These results highlight the effectiveness of our action-space guidance in improving generalization under diverse perturbation factors. VLABench Benchmark. VLABench is large-scale evaluation suite aimed at benchmarking both VLAs and VLMs on diverse robotic tasks. Built on ManiSkill3 [45], its manipulation benchmark consists of various tabletop scenarios, e.g., contact-rich interactions and articulated-object manipulation. The standard evaluation is organized into 5 public tracks, i.e., in-distribution, cross-category, commonsense, semantic-instruction, and unseen-texture, which respectively assess standard in-distribution performance, category-level generalization, commonsense reasoning, instruction understanding, and robustness to appearance variations. Particularly, VLABench proposes Intention Score (IS) and Progress Score (PS) to evaluate robot policies. In our context, we train π0, π0.5, along with our method in unified training setup. The model training is performed on VLABenchs official training data, with global batch size 128. All models are optimized for 60K steps. We present quantitative results in Table 3. Overall, our method achieves the best performance across both IS and PS. Notably, under the unseen-texture track, it delivers substantial gains, i.e., +12.6% in IS and +7.2% in PS, indicating strong robustness to distributional shifts. Together, these results further confirm the effectiveness of our proposed approach. Table 5. Reference action parameter ablation. We observe that different reference-action configurations within EAR generally lead to performance improvements. Methods Baseline Query Attention Pooling Downsample Spatial Object Goal Long 98.8 98.8 99.4 99.2 98.2 99.0 98.6 99.2 98. 97.2 98.2 98.2 92.4 92.8 92.8 95.6 Avg. 96.9 97.0 97.3 98. Table 6. Comparison of KV-cache interaction strategies in IAR. 4.3. Ablation Study We examine each components contribution via systematic ablation experiments on the LIBERO benchmark, which are shown in Table 4, Table 5, and Table 6. Note that we adopt π0.5 as the Baseline method. More ablations in different benchmarks are in Appendix C. EAR. As shown in Table 4, compared with the baseline, the experiment #1 introduces the Explicit Action Reasoner (EAR) module into policy learning, which lifts the average success rate from 96.9% to 98.3%, demonstrating that the explicit action-space guidance benefits the robotic action sequence prediction. plausible explanation is that EAR introduces an intermediate reference action sequence, which injects strong inductive bias on the behavior and thereby reduces ambiguity in mapping from observations to actions. IAR. Analogously, with the Implicit Action Reasoner (IAR) module added in #2, the average success rate increases from 96.9% to 98.1%. This gain suggests that exploiting the implicit action distribution encoded in visionlanguage representations can also provide effective guidance for policy learning. This performance gain can be partly attributed to the fact that IAR distills action-related clues implicitly encoded within the visionlanguage backbone, which potentially reflects the distribution of feasible actions. Such priors encourages the policy to remain closer to coherent, task-consistent behavioral patterns. EAR + IAR. In Table 4, experiment #3 incorporates both EAR and IAR, achieving the highest average success rate 7 Figure 3. Visualization of three manipulation tasks in real world. of 98.5%. The consistent improvements demonstrate that explicit action guidance and implicit action cues extracted from VLMs key-value cache are complementary, jointly providing stronger guidance for accurate action prediction. Reference Action Configurations. To further examine the effect of explicit action references in EAR, we investigate different settings of action shift and action horizon, as summarized in Table 5. We observe various parameter combinations consistently bring improvements over the baseline, indicating that providing action cues is broadly beneficial for policy learning. Besides, we find that shorter horizons combined with moderate shifts tend to produce relatively stronger gains. These observations offer further insight into how explicit action guidance influence policy learning. KV-cache Interaction Strategies. We compare three strategies for extracting action-relevant cues from VLMs key-value cache within IAR module, as presented in Table 6. Concretely, Query method uses learnable queries to attend to VLMs original key-value cache. Attention Pooling method forms pooled query by averaging key-value cache and then applies cross-attention operation. Downsample method first downsamples VLMs key-value cache and then aggregates them using learnable matrix. As shown in Table 6, all three variants outperform the baseline, indicating that extracting implicit action cues from VLM benefits policy learning. Notably, the Downsample strategy achieves the best performance, suggesting that VLMs features may contain noisy information for action prediction. This also highlights the importance of designing appropriate interaction mechanisms to align visionlanguage and action. 4.4. Real-World Deployment To further validate the effectiveness of our framework, we conduct extensive real-world experiments on the AgiBot G1 robot. We consider three manipulation tasks, i.e., Wipe Stain, Pour Water, and Open-set Pick, which respec8 Figure 4. Evaluation results of real-world experiments. tively assess contact-rich manipulation, fine-grained object handling, and instruction-following abilities. Specifically, as visualized in Fig. 3, the Wipe Stain task requires the robot to pick up sponge from the table and wipe away the stain until the surface is clean. The Pour Water task requires the robot to grasp the kettle by its handle, locate the target cup, pour water into it without causing overflow, and finally return the kettle to the table in stable manner. The Open-set Pick task instructs the robot to pick up the correct tabletop object according to given natural-language command. Additionally, to examine the cross-embodiment adaptability, we also perform the Openset Pick task on the AgileX robotic platform. Details about training and evaluation are provided in the Appendix B. As shown in Fig. 4, our approach achieves consistently higher average success rates than both π0.5 and π0, i.e., 66.7% against 61.0% and 33.8%. These results demonstrate that the proposed framework maintains effectiveness under real-world sensing conditions. Moreover, the aligned improvements observed on both Agibot G1 and AgileX also indicate that our method exhibits adaptability across different robotic embodiments. 5. Conclusion In this work, we addressed the fundamental semantickinematic gap in modern robotic policies by proposing new paradigm: Action Chain-of-Thought (ACoT). We argued that for physically grounded intelligence, deliberation should occur not in the abstract space of language or vision, but directly in the kinematically grounded space of actions. We materialized this concept in our ACoT-VLA framework, which leverages two synergistic modules, i.e., an Explicit Action Reasoner (EAR) and an Implicit Action Reasoner (IAR), to generate and fuse both explicit trajectory plans and implicit behavioral priors. This actioncentric guidance mechanism creates direct, informationrich conduit between high-level intent and low-level motor control. Our extensive experiments across multiple simulation and real-world benchmarks demonstrate that this approach yields state-of-the-art performance, significantly improving both task success and robustness. By shifting the locus of reasoning from perception to action, our work not only provides more effective and grounded method for robot policy learning but also opens new avenue for research into more structured, interpretable, and capable embodied agents. We believe that learning to think in the language of actions is critical step towards developing the next generation of generalist robots."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 5 [4] Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open arXiv foundation model for generalist humanoid robots. preprint arXiv:2503.14734, 2025. 5 [5] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: visionlanguage-action flow model for general robot control. corr, arXiv preprint abs/2410.24164, 2024. doi: 10.48550. ARXIV.2410.24164. 1, 2, 5, 6, 7 [6] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [7] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions, 2025. 6 [8] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 3, 5, 6 [9] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. 4 [10] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 2 [11] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 44 (10-11):16841704, 2025. 1, 5 [12] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, et al. Graspvla: grasping foundation model arXiv pre-trained on billion-scale synthetic action data. preprint arXiv:2505.03233, 2025. 2 [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. 4 [14] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulateanything: Automating real-world robots using visionlanguage models. arXiv preprint arXiv:2406.18915, 2024. [15] Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, et al. Libero-plus: In-depth robustness analysis of visionlanguage-action models. arXiv preprint arXiv:2510.13626, 2025. 5, 6, 12 [16] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. 1 [17] Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, YuChiang Frank Wang, and Fu-En Yang. Thinkact: Visionlanguage-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. 5 [18] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li. A3vlm: Actionable articulation-aware vision language model. arXiv preprint arXiv:2406.07549, 2024. 2 [19] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. [20] Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, and Heng Yang. Motvla: vision-language-action arXiv preprint model with unified fast-slow reasoning. arXiv:2510.18337, 2025. 2 [21] Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, Tan, Navonil Majumder, Soujanya Poria, et al. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. 6 [22] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 1, 2, 5, 6, 7, 13 [23] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea open-world dataset and g0 dualsystem vla model. arXiv preprint arXiv:2509.00576, 2025. 2 9 [24] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 2, 5, [25] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 5, 6 [26] Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, et al. Vla-rft: Vision-language-action reinforcement fine-tuning with verified rewards in world simulators. arXiv preprint arXiv:2510.00406, 2025. 3 [27] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1806118070, 2024. 2 [28] Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, and Ping Luo. Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. arXiv preprint arXiv:2508.20072, 2025. 5 [29] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025. 3, 5 [30] Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Yuxin Du, Jiting Liu, Encheng Gu, and Bo Zhao. Evo-0: Vision-languagearXiv action model with implicit spatial understanding. preprint arXiv:2507.00416, 2025. 2 [31] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [32] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. 5, 6, 12 [33] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 1, 2 [34] Yunzhe Liu, Rinon Gal, Amit Bermano, Baoquan Chen, Self-conditioned generative adarXiv preprint and Daniel Cohen-Or. versarial networks for image editing. arXiv:2202.04040, 2022. 4 [35] Yi Liu, Sukai Wang, Dafeng Wei, Xiaowei Cai, Linqing Zhong, Jiange Yang, Guanghui Ren, Jinyu Zhang, Maoqing Yao, Chuankang Li, et al. Unified embodied vlm reasoning with robotic action via autoregressive discretized pretraining. arXiv preprint arXiv:2512.24125, 2025. 2 [36] Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. F1: vision-language-action model bridg10 ing understanding and generation to actions. arXiv preprint arXiv:2509.06951, 2025. 3, 5 [37] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: In 2024 IEEE InterOpen x-embodiment collaboration 0. national Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. 2 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [39] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for visionlanguage-action models. arXiv preprint arXiv:2501.09747, 2025. 5, 6 [40] Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, and Li Erran Li. Affordancellm: Grounding affordance from vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75877597, 2024. [41] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial represenarXiv preprint tations for visual-language-action model. arXiv:2501.15830, 2025. 1, 2, 5 [42] Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang. Memoryvla: Perceptual-cognitive memory in vision-language-action models for robotic manipulation. arXiv preprint arXiv:2508.19236, 2025. 5 [43] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. 5 [44] Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp Interactive post-training for vision-languageKrahenbuhl. action models. arXiv preprint arXiv:2505.17016, 2025. [45] Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, et al. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425, 2024. 7 [46] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [47] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: arXiv preprint Bringing ai arXiv:2503.20020, 2025. 2 into the physical world. [60] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reaIn Proceedings soning for vision-language-action models. of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. 3, [61] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 1 [62] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. 1 [63] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, YaQin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2250822519, 2025. 5 [64] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. 5 [65] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d In Eurooccupancy world model for autonomous driving. pean conference on computer vision, pages 5572. Springer, 2024. 1 [66] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [67] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. 2 [48] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: arXiv preprint An open-source generalist robot policy. arXiv:2405.12213, 2024. 1, 5 [49] Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025. 5 [50] Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Haiyang Liu, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Renderworld: World Wang, Fabio Remondino, et al. In 2025 IEEE Intermodel with self-supervised 3d label. national Conference on Robotics and Automation (ICRA), pages 60636070. IEEE, 2025. 1 [51] Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, et al. Fpc-vla: vision-language-action framework with supervisor for failure prediction and correction. arXiv preprint arXiv:2509.04018, 2025. 5 [52] Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, and Hang Zhao. Depthvla: Enhancing vision-language-action models with depth-aware spatial reasoning. arXiv preprint arXiv:2510.13375, 2025. [53] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via arXiv preprint embodied chain-of-thought arXiv:2407.08693, 2024. 1 reasoning. [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 5 [55] Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, et al. 4d-vla: Spatiotemporal vision-language-action pretraining with cross-scene calibration. arXiv preprint arXiv:2506.22242, 2025. 2 [56] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025. 3 [57] Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, YuGang Jiang, et al. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with longIn Proceedings of the IEEE/CVF horizon reasoning tasks. International Conference on Computer Vision, pages 11142 11152, 2025. 2, 5, 6, [58] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, et al. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025. 1, 3, 5 [59] Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis EH Tay, Sijin Chen, Ziwei Liu, et al. From spatial to actions: Grounding vision-language-action model in spatial foundation priors. arXiv preprint arXiv:2510.17439, 2025. 2 11 A. Dataset Description In this section, we present comprehensive characterization of the benchmark datasets and the custom-collected data used for model training in our experiments. We systematically report key statistics, including the total number of episodes, frame counts, and other relevant properties, which is summarized in Table 7 below: Type Dataset Embodiment DoF Episodes Frames FPS Simulation Real-World LIBERO LIBERO-Plus VLABench Wipe Stain Pour Water Open-set Pick Open-set Pick Franka Franka Franka AgiBot G1 AgiBot G1 AgiBot G1 AgileX 7 7 7 22 22 22 1,693 14,347 4,713 177 1,821 1,936 962 273,465 2,238,036 528,398 356,316 5,062,506 219,824 251,283 10 20 10 30 30 30 Table 7. Dataset statistics. Simulation Benchmarks. We utilize three publicly released simulation datasets, i.e., LIBERO [32], LIBEROPlus [15], and VLABench [57]. Specifically, the LIBERO dataset contains 1, 693 episodes and 273, 465 frames, recorded at fixed 10 Hz. Its demonstrations exhibit relatively uniform trajectory lengths and smooth motion patterns, making it widely adopted benchmark in community. However, due to the increasing performance saturation observed on LIBERO, LIBERO-Plus is recently introduced to provide more challenging and diversified evaluation setting. LIBERO-Plus provides 14, 347 episodes and 2, 238, 036 frames, captured at 20 Hz. In contrast to the homogeneous trajectories in LIBERO, LIBERO-Plus explicitly emphasizes perturbation-oriented design. The demonstrations display substantially larger variations in motion magnitude and camerarobot viewpoint configuration. These characteristics make it more suitable benchmark for evaluating policy generalization under structured distribution shifts. Besides these two datasets, we further benchmark our method on VLABench, whose training set includes 4, 713 episodes and 528, 398 frames, recorded at 10 Hz, which requires higher level of visual and physical understanding from the policy. Real-World Experiment. For real-world deployment, we collect demonstrations across 3 tasks, i.e., Wipe Stain, Pour Water, and Open-set Pick, as shown in Table 7. The Wipe Stain dataset contains 177 episodes with 356, 316 frames, characterized by dense toolsurface contact and fine-grained force control. The Pour Water dataset includes 1, 821 episodes and 5, 062, 506 frames. Its large scale stems from the tasks long-horizon and multistage nature. Regarding the Open-set Pick task, the AgiBot G1 subset provides 1, 936 episodes with 219, 824 frames, while the AgileX subset offers 962 episodes with 251, 283 frames, both featuring diverse tabletop layouts and natural-language instructions. Task Action Space Action Horizon State Batch Size Training Step LIBERO LIBERO-Plus VLABench Wipe Stain Pour Water Open-set Pick Open-set Pick Delta EEF Delta EEF Abs EEF Abs Joint Abs Joint Abs Joint Abs Joint 10 10 30 30 30 30 128 128 128 128 128 128 128 40K 100K 60K 50K 240K 50K 50K Table 8. Training details. Note that the Open-set Pick task is performed on AgileX platform. B. Training & Evaluation Details Training Details. We describe the task-specific training configurations, e.g., action space and state usage, for better understanding. As presented in Table 8, for the LIBERO and LIBEROPlus suites, the policy is trained using delta end-effector control (Delta EEF) with an action horizon of 10 steps. In particular, no privileged state information is provided during training. We utilize global batch size of 128 and train the policies for 40K and 100K steps, respectively. Similarly, we train our models in VLABench for 60K steps, while adopting state input and absolute end-effector (Abs EEF) actions to align the benchmarks control convention. In terms of the real-world tasks, we utilize Abs Joint control with longer action horizon of 30. Unlike the simulator benchmarks, these tasks additionally provide structured robot state observations to improve robustness under realworld sensing and actuation noise. Our models are trained for 50K, 240K, and 50K steps, in Wipe Stain, Pour Water, and Open-set Pick tasks, respectively, with same batch size of 128. Evaluation Details. Next, we illustrate the evaluation protocols and success criteria for all real-world tasks. Each task is assessed using fixed and repeatable initializations to ensure reproducibility and reduce environmental variance. Concretely, in terms of the Wipe Stain task, we predefine three initial sponge poses. For each pose, the robot is required to clean stains placed at four distinct table locations. Every configuration is executed twice, resulting in 24 trials in total. trial is considered successful if the robot grasps the sponge and removes the stain from the specified location. As for the Pour Water, we standardize six predefined relative configurations between the bottle and the glass. Then, each configuration is executed two times. trial is counted as successful if the robot lifts the bottle, pours water into the cup, and places the bottle back onto the coaster. Note that minor spillage of water when pouring is allowed. Eventually, regarding the Open-set Pick task, we initialize ten object arrangements on the table, containing both in-distribution and out-of-distribution instances. In each arrangement, the robot is instructed to grasp specified target object using either its left or right arm, as indicated by the"
        },
        {
            "title": "Name",
            "content": "EAR IAR Camera Robot Language Light Background Noise Layout Avg."
        },
        {
            "title": "Baseline",
            "content": "#1 #2 #3 70.3 88.7 80.7 91. 41.7 63.5 48.7 62.5 81.1 80.4 82.6 80.3 97.3 94.0 97.7 95. 94.6 90.2 90.9 91.5 71.8 89.5 84.3 88.3 84.9 84.2 86.0 84. 75.7 83.7 80.4 84.1 Table 9. Module ablations on LIBERO-Plus benchmark. The performance is gradually improved with the addition of proposed methods. Name Action Head EAR LIBERO LIBERO-Plus Param. Denoise Param. Denoise Spatial Object Goal Long Avg. Camera Robot Language Light Background Noise Layout Avg. Baseline 300M #1 # #3 #4 #4 #5 #6 # 600M 600M 300M 300M 300M 300M 300M 300M 10 10 20 10 10 10 10 10 - - - 300M 300M 300M 150M 250M 500M - - - 10 10 10 10 10 98. 99.0 96.4 92.2 96.6 70.3 41.7 97.6 97. 98.6 99.0 99.0 99.2 99.0 98.4 98. 98.8 99.6 99.4 99.4 99.2 98.2 99. 97.8 96.4 97.6 98.0 95.2 97.5 97.8 95.4 97.9 98.0 96.6 98.3 98.0 96.6 98.3 97.8 94.2 97.6 98.6 94.2 97.5 96.6 94.2 97.0 68.7 70. 88.2 88.7 88.7 86.4 87.2 80.8 44. 44.8 62.4 63.5 63.5 54.3 59.7 57. 81.1 83.1 82.7 81.5 80.4 80. 81.7 81.1 84.1 97.3 96.4 97.6 95.0 94. 94.0 92.2 95.0 95.6 94.6 92. 93.1 91.5 90.2 90.2 91.4 93. 92.1 71.8 84.9 75.7 66.6 66. 88.6 89.5 89.5 89.1 87.4 79.8 84. 83.2 85.3 84.2 84.2 82.1 83.5 83. 74.9 75.1 83.9 83.7 83.7 81.7 83. 80.9 Table 10. Effects of parameters and denoise steps on policy performance. The best results are highlighted in bold, and the second-best results are underlined. Note that the IAR module is not added in this experiment. instruction. Each armobject pair is evaluated twice, resulting in 40 trials overall. trial is deemed successful if the robot grasps the instructed object with the correct arm. Across all tasks, evaluations are carried out by trained operators with substantial prior testing experience, and success rates are computed as the proportion of successful trials relative to the total number of executed attempts. C. More Experimental Results In this section, we provide additional quantitative experiments to substantiate the effectiveness of our proposed approach and to empirically uncover several insightful phenomena. Specifically, the experimental analyses comprise three parts: (1) ablation study conducted on the LIBEROPlus benchmark in Table 9, (2) an investigation of how the parameter sizes of the Action Head and Explicit Action Reasoner (EAR), as well as the number of denoising steps, influence policy performance in Table 10, and (3) comparative study examining the relationship among inference latency, model size, and performance in Table 11. Note that we adopt π0.5 [22] as the baseline method, denoted as Baseline. Module Ablation. As shown in Table 9, incorporating the proposed reasoning modules consistently improves policy performance on the LIBERO-Plus benchmark. Adding the EAR module, i.e., experiment #1, yields clear gain over the baseline, increasing the average success rate from 75.7% to 83.7%. This improvement can be attributed to EARs ability to generate an explicit reference action trajectory, which significantly reduces the ambiguity in mapping complex visual or linguistic observations to low-level actions, such as camera shifts and background changes. Meanwhile, incorporating only the IAR (#2) also improves the performance from 75.7% to 80.4%, indicating that decoding the latent action-related semantics within the visionlanguage backbone provides useful behavioral priors. Finally, combining EAR and IAR (#3) achieves the highest success rate of 84.1%, demonstrating their complementary effects, i.e., EAR provides explicit motion guidance, while IAR supplies dense representation-level priors. Effect of Model Scaling & Denoising Budget. Then, we analyze the superiority of our method by comparing settings with matched total model parameters and denoising steps. As shown in Table 10, firstly, we enlarge the model size of the action head and increase the number of denoising steps in experiments #1 and #2, to construct fair baselines for subsequent comparison. We observe preliminary observation, i.e., increasing the model size or denoising steps does not reliably enhance performance. Specifically, compared with the baseline, while #1 improves performance on the LIBERO benchmark, it simultaneously drops on LIBEROPlus. Next, comparing #1 and #2 reveals that further increasing denoising steps yields only negligible fluctuations. Subsequently, we incorporate the EAR module under fully matched overall parameterization and denoising budgets. Concretely, in both comparison pairs, #1 with #3 and #2 with #4, we consistently observe notable perfor13 tively modest compared to the performance gains, may pose challenges for deployment on resource-constrained robotic platforms. Besides, another limitation stems from the fact that the prevailing action representation in the community is implemented as action chunks, i.e., sequences of lowlevel control commands such as joint angles or end-effector poses. While such representations faithfully encode the executed motions, they lack explicit geometric structure that would facilitate higher-level spatial reasoning, such as object-centric coordination and contact geometry. Hence, the potential of ACoT reasoning may not be fully unleashed. Enriching action representations with spatially grounded information to enable ACoT to operate in geometrically interpretable 3D space, constitutes an interesting and promising avenue for future exploration. E. LLM Usage Statement In this paper, we employ Large Language Models (LLMs) solely for minor linguistic refinement during the manuscript preparation stage, such as correcting grammatical errors. None of the technical content, implementation details, or experimental results were generated by LLMs. Name EAR IAR Param. Latency LIBERO LIBERO-Plus Avg. SR Avg. SR Baseline #1 #2 #3 3.35B 3.80B 3.36B 3.81B 91ms 110ms 93ms 112ms 96.9 98.3 98.1 98.5 75.7 83.7 80.4 84.1 Table 11. Ablation experiment on model efficiency and performance. mance improvements on both benchmarks, once the EAR module is introduced. This indicates that the performance gains originate from our proposed action chain-of-thought. The proposed mechanism supplies explicit reference actions that effectively mitigate the intrinsic instability of action prediction, especially under challenging external perturbations, as shown in the LIBERO-Plus, enabling more reliable and grounded generalist robotic policy. Effect of EAR Scale. Moreover, we investigate how various scale of the EAR module influences action prediction fidelity. To isolate the effect of EAR, we keep the action head parameters and the denoising schedule strictly fixed, while scaling the EAR module to 150M, 250M, 300M, and 500M parameters via adjusting hidden size. As presented in Table 10, through the comparison across experiments #4, #5, #6, and #7, we find that although all EARequipped variants outperform non-EAR baselines on both benchmarks, the performance trend is non-monotonic. Applying moderate EAR scales, e.g., 300M, yields the greatest improvement. Particularly, as evidenced in #7 in Table 10, when the parameter of EAR module even exceeds that of the action head, we observe marked drop in performance. We attribute this degradation to the tendency of an over-parameterized EAR to overfit spurious correlations during training. Therefore, it generates reference action trajectories that are systematically biased, which ultimately misdirect the action head toward suboptimal predictions. Latency Analysis. In Table 11, we further examine the inference efficiency of our approach in terms of both parameter count and end-to-end latency. As additional reasoning modules are introduced, we observe slight increase in latency. Incorporating the EAR module raises latency from 91ms to 110ms, while adding the IAR module introduces only an additional 2ms. However, this marginal overhead is outweighed by the substantial improvement, which reflects favorable trade-off. D. Limitations & Future Works In this section, we discuss the limitations existing in our work and promising directions for future research. Although our proposed action chain-of-thought (ACoT) substantially boosts policy performance, our framework still exhibits several constraints. The reasoning modules introduce additional computational cost, which, while rela-"
        }
    ],
    "affiliations": [
        "AgiBot",
        "Beihang University"
    ]
}