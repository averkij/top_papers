{
    "paper_title": "PubMed-OCR: PMC Open Access OCR Annotations",
    "authors": [
        "Hunter Heidenreich",
        "Yosheb Getachew",
        "Olivia Dinica",
        "Ben Elliott"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 5 2 4 1 1 . 1 0 6 2 : r PUBMED-OCR: PMC OPEN ACCESS OCR ANNOTATIONS Hunter Heidenreich, Yosheb Getachew, Olivia Dinica, Ben Elliott Roots.ai ai-ml@roots.ai Dataset: huggingface.co/datasets/rootsautomation/pubmed-ocr ABSTRACT PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; 1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions."
        },
        {
            "title": "Introduction",
            "content": "PDFs and scanned documents are ubiquitous in business, government, education, research, and healthcare. To realize AI copilots that meaningfully reduce repetitive work, systems must robustly understand real-world documents [1]. Open data is central to this goal. By democratizing access and standardizing formats, open corpora enable better models and algorithms [2], support reproducibility [3], broaden participation, and yield stronger benchmarks. This dynamic is evident in broad-spectrum LLM training sets [4, 5, 6, 7, 8] and in large-scale multimodal resources [9, 10, 11]. In document processing, open datasets have repeatedly shaped progress, from IIT-CDIP [12, 13] and its derivatives [14, 15, 16, 17, 18, 19] to PubMed-derived resources that target complex scientific articles [20, 21, 22, 23]). pattern of co-reinforcement is evident: open data leads to stronger models, and stronger models catalyze the creation of further open data. While closed-source systems are often scaled further than open models, their outputs can be released to accelerate community progress, as seen with OCR-IDL [24]. Open models are also combined in ensembles to improve data quality for state-of-the-art systems. For example, DeepSeek-OCR [25] scaled supervision by leveraging PP-DocLayout [26], MinerU [27], GOT-OCR2.0 [28], and PaddleOCR [29]. These effects extend beyond document processing. Advances in OCR yield improved LLM training corpora [30], since OCR provides the translation layer from optical signals to discrete text. Without faithful translation, information remains effectively invisible to text-only models. This work. We introduce PubMed-OCR, built from the same open-access database used by prior PubMed resources. Unlike approaches that align text or regions mined from digital PDFs to JATS XML (a process prone to parser noise, heuristic dependencies, and missed text from scanned documents), we annotate page images directly with commercial OCR system (Google Vision OCR) to produce word-, line-, and paragraph-level supervision. We provide corpus statistics and qualitative examples, and release the resource to support model development, benchmark curation, and related research. PubMed-OCR"
        },
        {
            "title": "2 Related Work",
            "content": "2.1 PMCOA-Derived Layout and Table Datasets PubMed Central Open Access (PMCOA) has long served as substrate for document understanding research because it provides both PDFs and machine-readable Journal Article Tag Suite (JATS) XML. Most prior datasets leverage this pairing by aligning text and regions extracted from PDFs to the XML through heuristic or semi-supervised matching. GROTOAP [20] and GROTOAP2 [21] provided early large-scale ground truth for PMCOA. GROTOAP2 distributes hierarchical XML annotationspages decomposed into zones, lines, words, and characterswith two-point bounding boxes and 22 zone labels (e.g., title, abstract, body, references). PubLayNet [22] scales layout supervision by aligning PMCOA XML with PDFMiner [31] output to produce 3.5M region annotations for 360K pages. Regions are mapped to five canonical classes, and splits are constructed at the journal level with stricter selection for validation and test, including sampling rules to limit overrepresentation by any single journal. Complementary to layout segmentation, PubTables-1M [23] targets table understanding: 575K pages and 948K tables annotated at the table, row, column, and cell levels, with bounding boxes in both PDF and image coordinates and word boxes provided for downstream parsing. In contrast, our corpus is OCR-native: we bypass PMCOA XML entirely and derive word-, line-, and paragraph-level supervision directly from page images using high-quality OCR engine, thereby avoiding alignment errors inherited from PDF parsers and enabling OCR on non-digital pages (i.e., pages containing scans without text overlays within the document). 2.2 General-Domain OCR and Layout Resources IIT-CDIP [12, 13] aggregates 7M tobacco-litigation documents (TIFF scans + text) hosted by UCSF IDL, with substantial real-world noise (handwriting, stains, scanning artifacts). Crucially, this text is already linearized, lacking bounding boxes for words or lines. Subsequent work overlays OCR and structure on IIT-CDIP subsets. For example, DESSURT [32] released Tesseract [33] outputs (words/lines) plus block/paragraph regions derived via PubLayNet/PrimaNet [34]. Widely used benchmarks curated from this source include RVL-CDIP (document classification) [35], FUNSD (form understanding) [15], Tobacco-3482/Tobacco-800 (classification, page-stream segmentation) [16, 12, 17, 18], and DocVQA [19]. OCR-IDL [24] extends this lineage by providing large-scale OCR annotations over the UCSF Industry Documents Library using commercial engine (Amazon Textract). The release covers >26M pages (a sampled subset of library exceeding 70M documents), enabling evaluation of systems that depend on commercial-grade OCR without bundling proprietary models. TabMe++ [36] reprocesses the TabMe page-stream segmentation benchmark [37] with Azure OCR, replacing noisier Tesseract outputs. Although nested within the IIT-CDIP/IDL universe, TabMe++ illustrates the impact of higher-quality OCR on downstream segmentation and classification. Several resources target document layout but do not provide OCR. DocBank [38] aligns LATEX to PDF to yield 500K pages with token-level labels and PDF-derived word boxes. DocLayNet [39] contributes 81K manually labeled pages in the wild across eleven region classes; earlier datasets include Marmot [40] and the PRImA layout benchmark [41]. In the same parser-derived family, PDFA (PDF Association dataset) [42] is large-scale subset of the SafeDocs CC-MAIN-2021-31 crawl [43], providing digital-PDF words/lines, inferred reading order, and layout metadata at web-scale (millions of documents). Unlike OCR-first corpora, PDFA recovers text from PDF objects and thus largely excludes scanned/image-only pagesa side-effect inherited by any parser-based approach. General-domain corpora demonstrate the utility of large-scale grounded OCR and layout supervision for downstream tasks; however, scientific articles pose distinct challenges (dense mathematics, fine-grained references, heavy table/figure usage). PubMed-OCR addresses this gap with OCR-first supervision on PMCOA pages. 2.3 Text Recognition versus OCR We distinguish plain text recognitionwhich serializes page into single sequence in reading orderfrom grounded/structured OCR, which yields words, lines, and paragraphs with bounding boxes. The former is useful for ingesting documents into text corpora for LLM pre-training [30, 44], while grounded outputs preserve spatial provenance and enable layout-aware modeling and verifiable attribution. 2 PubMed-OCR Dataset Domain Engine Sizes Granularity (bboxes) Docs Pages Blocks Lines Words Chars IIT-CDIP UCSF IDL OCR-IDL TabMe++ PDFA GROTOAP2 PubTables-1M PubMed-OCR (Ours) Table 1: Comparison of text resources by size and annotation granularity. Commercial engines are marked with . UCSF IDL UCSF IDL CC PMCOA PMCOA PMCOA 209.5K 6.5M 35.5M 825.1K 26M 4.6M 44.8K 122.5K 2.2M 18M 13.2K 119.3K 575.3K 1.5M Tesseract Amazon Textract Azure OCR PDF Parsing (no OCR) PDF Parsing (no OCR) PDF Parsing (no OCR) Google Vision OCR Layout-aware models explicitly consume text and layout to improve document understanding, either with image encoders or by encoding layout tokens alongside text. Frequently, layout information is ingested in the form of bounding boxes from OCR. Representative approaches include LayoutLM [45, 46] and LiLT [47], and more recent LLM-centric methods such as LayoutLLM (layout instruction tuning) [48], DocLLM (LLM with layout tokens only) [49], and LayTextLLM (interleaving bounding-box tokens with text) [50], which demonstrate strong results without heavy vision backbones. Grounded outputs also support grounded response generation: answers are produced together with fine-grained evidence (citations and, when available, coordinates on the page). Recent work on attributed/grounded generation improves verifiability by learning to attach citations at span-level granularity [51] and by evaluating citation quality [52], with complementary advances in grounded reasoning that interleave text with bounding-box coordinates [53]. Because grounded OCR can be deterministically linearized when needed, it is the more verbose yet more flexible annotation. Our corpus therefore adopts the grounded setting with supervision at the word, line, and paragraph levels."
        },
        {
            "title": "3 PubMed-OCR Dataset",
            "content": "3.1 Data Collection We downloaded PMCOA PDFs via the official FTP/OAI endpoints and restricted redistribution to articles whose licenses permit sharing derivative artifacts. From 2M PDFs, 60% met this criterion (1.2M). We sample 209.5k documents uniformly at random and annotate each page with the Google Vision API (December 19, 2024 release), priced at $1.50 per 1000 pages. This amounts to cost of $2.3k (with the cost of full OCR at roughly 5x, or $12k). We include only articles whose PMCOA licenses permit redistribution. For each document we release OCR JSON (always) and, where permitted, the original PDF. The metadata CSV records the license (e.g., CC BY, CC BY-SA, CC BY-NC, CC BY-NC-SA), direct PMCID/PMID link, and allowed use (e.g., commercial use = true/false). OCR annotations are licensed under the same terms as the source article. 3.2 OCR Processing and Normalization We render each PDF page to an image at 150 DPI and run Google Cloud Visions document_text_detection on the image bytes. No manual deskewing is performed prior to calling the API with page images. From the resulting full_text_annotation, we traverse pages blocks paragraphs words, extracting each words text and its four-vertex polygon. Vertices are canonicalized to axis-aligned bounding boxes by the {top-left, bottom-right}. Paragraph text is formed by concatenating its words; the paragraph bounding box is the axis-aligned rectangle spanning all word vertices. Line reconstruction. The Google Vision API only returns bounding boxes for words and paragraphs. We derive lines by clustering words that are vertically aligned with coarse heuristic: 1. For each word w, let ymin(w) and ymax(w) be the minimum and maximum of its vertices, and xmin(w), xmax(w) the min/max x. 2. Maintain line groups with representative (ymin, ymax). word joins an existing group iff ymin(w) ymin 5 and ymax(w) ymax 5 pixels; otherwise start new group. 3 PubMed-OCR 3. To avoid cross-column or cross-paragraph merges, we split any group containing words from different paragraphs according to the paragraph indices returned in the original Google Vision OCR, so each line is contained within single paragraph. 4. Within each group, sort words by xmin(w) (left-to-right) and concatenate to form the line text. The line bounding box is (cid:2) minw xmin(w), minw ymin(w), maxw xmax(w), maxw ymax(w)(cid:3). Standardized output. For each page, we emit two standard artifacts: JSON and the raw PDF. Each page JSON contains text.words, text.lines, and text.paragraphs, where each items polygon is converted to an axisaligned box [X1, Y1, X3, Y3] (top-left, bottom-right). We also include basic image metadata (path, width, height, dpi) used to produce the OCR for reproducibility. 3.3 Data Statistics Documents Pages Paragraphs Lines Words Count 209.5K 1.5M 61.0M 164.3M 1.3B PubMed-OCR Per Doc Per Page Count OCR-IDL Per Doc Per Page - 7.4 291.3 784.9 6229. - - 39.5 106.3 844.0 4.6M 26M - 46M 166M - 6 - 101.3 360.8 - - - 17.5 62.5 Table 2: PubMed-OCR corpus statistics (left) versus reported statistics from OCR-IDL (right). We report OCR-IDL statistics as published, but note that the number of documents/pages and their per document/page statistics imply an order of magnitude more words and lines than the manuscript purports. Comparison with prior corpora. Table 1 situates PubMed-OCR among widely used document resources. IIT-CDIP is the largest in absolute size but, in its native form, lacks bounding boxes altogether; overlays such as the Tesseract pass add boxes only for 825K-page subset [32]. OCR-IDL and TabMe++ demonstrate the value of commercial OCR at scale in the UCSF IDL domain but omit paragraphor character-level boxes. Parser-derived PMCOA datasets (GROTOAP2, PubTables-1M, PDFA) recover text/regions from digital PDFs rather than page images, process prone to reduced recall for non-digital documents. In contrast, PubMed-OCR is OCR-first on PMCOA and provides paragraph-, line-, and word-level boxes, filling gap between parser-derived PMCOA resources and OCR-first corpora in other domains. Figure 1: Distribution of number of words (left), lines (middle), and paragraphs (right) per page. µ indicates the mean, indicates the median, and σ is the standard deviation. Each distribution is truncated at or below the 99.5th percentile to visualize the core probability mass instead of the long tail. Corpus summary (ours). As shown in Table 2, the release comprises 209.5K documents and 1.5M pages (mean 7.4 pages/doc). On average, each page contains 39.5 paragraphs, 106.3 lines, and 844 words, corresponding to 291.3 paragraphs, 784.9 lines, and 6,229.6 words per document. Comparing these statistics with the statistics reported by 4 PubMed-OCR OCR-IDL [24], we observe that despite having fewer documents and pages, PubMed-OCR has almost 4x the number of line annotations and 10x the number of word annotations. Figures 1 and 2 show that both per-page and per-document counts with right tails, reflecting the mix of short communications and long articles. This combination of scale and grounded granularity (paragraphs/lines/words with boxes) is designed to support layout-aware modeling, document QA with page coordinates, and robust evaluation across heterogeneous article lengths. Figure 2: Distribution of number of words (left), lines (middle), and paragraphs (right) per document. µ indicates the mean, indicates the median, and σ is the standard deviation. Each distribution is truncated at or below the 99th percentile to visualize the core probability mass instead of the long tail. Journal distribution. The PMCOA composition induces head of high-volume journals. The top three titles Journal of Cell Biology (9.7%), Journal of Experimental Medicine (9.4%), and Nucleic Acids Research (3.9%)account for roughly 23% of documents. Despite this skew, 2,478 journals are represented across our dataset. Singleton journals (journals represented with singular document) make up 637 of the 2,478 journals, roughly 25.7% of journals and 0.3% of documents. We show the top 20 journals by document count in Figure 3. 3.4 Qualitative Analysis To better understand the qualitative aspects of our dataset, we sample 40,000 pages uniformly at random and run them through pre-trained layout detection module. To do so, we use PP-DocLayout_plus-L, which tags regions of our pages into 20 different classes. Some of the more interesting features include formulas (present in 25% of pages), images (present in 22% of pages), and charts and tables (present in 16% and 18%, respectively). We present the breakdown of the 20 classes of layout features in Table 3. As qualitative examples, we show two pages in Figure 4. The first is an older document with stamped seal on its upper-right. It has mixture of dense text features and formulas embedded within that text. The second image shows tabular data, algorithmic definitions, among other standard text and title features. We provide handful of additional samples in Appendix A."
        },
        {
            "title": "4 Conclusion",
            "content": "We presented PubMed-OCR, an OCR-first corpus derived from the PubMed Central Open Access subset that exposes paragraph-, line-, and word-level bounding boxes directly from page images in compact, standardized JSON format. By bypassing fragile PDF/XML alignment, the resource complements parser-based PMCOA derivatives and enables layout-aware modeling, grounded question answering, and attributed generation on scientific literature. We also provide corpus-level statistics and distributions to characterize scale and diversity across journals and article lengths, making the dataset practical substrate for training and evaluation. While useful, the corpus has limitations. It currently relies on single OCR engine, and line annotations are reconstructed heuristically from word boxes, which may introduce biases in reading order and grouping. Character-level boxes and explicit representations of mathematical expressions or figure/table structure are not included, and coverage reflects PMCOAs license and journal distribution. These constraints should be considered when reporting results and designing experimental splits. 5 PubMed-OCR Figure 3: Top 20 journals represented in PubMed-OCR. The top 3 journals account for 23% of all documents included. Taken together, PubMed-OCR offers reproducible, openly accessible dataset for research that requires faithful textlayout grounding in scientific articles. We release the resource to support robust evaluation and to facilitate fair comparisons without dependence on proprietary pipelines, and we invite the community to audit, extend, and build upon it."
        },
        {
            "title": "References",
            "content": "[1] Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, and Daniel Dahlmeier. Document intelligence in the era of large language models: survey. arXiv preprint arXiv:2510.13366, 2025. [2] Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, et al. On the societal impact of open foundation models. CoRR, 2024. [3] Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence dAlché Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research (a report from the neurips 2019 reproducibility program). Journal of machine learning research, 22(164):120, 2021. [4] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [5] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:31809 31826, 2022. [6] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. [7] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: an open corpus of three trillion tokens for language 6 PubMed-OCR Layout Feature Number Detected % Pages with Feature number text header footer paragraph title figure title formula image reference content reference table chart doc title footnote abstract aside text formula number algorithm content seal 36,961 223,264 44,349 36,709 58,312 28,101 69,480 13,426 154,944 13,319 9,277 15,421 5,286 5,529 4,379 1,466 1,238 28 9 6 92.09 87.43 79.66 64.96 63.65 43.99 24.83 22.54 19.87 19.80 18.34 16.59 12.81 10.80 10.32 2.65 1.18 0.06 0.02 0.01 Table 3: The layout features detected across random 40k page sample from PubMed-OCR. Number detected indicates the number of each layout feature found across the entire dataset whereas the % pages with feature indicates the percentage of pages in our sample that had at least one instance of given layout feature. Layout features were detected using PP-DocLayout_plus-L, which predicts high prevalence of images, tables, charts, and formulas. Note that these results are model-dependent and should not be treated as gold labels. model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, 2024. [8] Maurice Weber, Dan Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, et al. Redpajama: an open dataset for training large language models. Advances in neural information processing systems, 37:116462116492, 2024. [9] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In NeurIPS Workshop Datacentric AI, number FZJ-2022-00923. Jülich Supercomputing Center, 2021. [10] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [11] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. [12] David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David Grossman, and Jefferson Heard. Building test collection for complex document information processing. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 665666, 2006. [13] Ian Soboroff. Complex document information processing (cdip) dataset. https://doi.org/10.18434/ mds2-2531, 2022. Accessed: 2025-10-28. [14] Adam Harley, Alex Ufkes, and Konstantinos Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In 2015 13th international conference on document analysis and recognition (ICDAR), pages 991995. IEEE, 2015. [15] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: dataset for form understanding in noisy scanned documents. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), volume 2, pages 16. IEEE, 2019. 7 PubMed-OCR Figure 4: Two example pages from PubMed-OCR, overlaid with layout detection classes predicted by PP-DocLayout. On the left, we have page with seal alongside high-density text (with formulas embedded within the text). On the right, we have page with many tabular outputs, code snippets, and other text. [16] Guangyu Zhu and David Doermann. Automatic document logo detection. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), volume 2, pages 864868. IEEE, 2007. [17] G. Agam, S. Argamon, O. Frieder, D. Grossman, and D. Lewis. The complex document image processing (cdip) test collection project. Technical report, Illinois Institute of Technology, 2006. [18] University of California, San Francisco. The legacy tobacco document library (ltdl), 2007. [19] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [20] Dominika Tkaczyk, Artur Czeczko, Krzysztof Rusek, Lukasz Bolikowski, and Roman Bogacewicz. Grotoap: ground truth for open access publications. In Proceedings of the 12th ACM/IEEE-CS joint conference on Digital Libraries, pages 381382, 2012. [21] Dominika Tkaczyk and Pawel Szostek. Grotoap2-the methodology of creating large ground truth dataset of scientific articles. D-Lib Magazine, 20(11/12), 2014. [22] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition (ICDAR), pages 10151022. IEEE, 2019. [23] Brandon Smock, Rohith Pesala, and Robin Abraham. Pubtables-1m: Towards comprehensive table extraction from unstructured documents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46344642, 2022. [24] Ali Furkan Biten, Ruben Tito, Lluis Gomez, Ernest Valveny, and Dimosthenis Karatzas. Ocr-idl: Ocr annotations for industry document library dataset. In European Conference on Computer Vision, pages 241252. Springer, 2022. [25] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. 8 PubMed-OCR [26] Ting Sun, Cheng Cui, Yuning Du, and Yi Liu. Pp-doclayout: unified document layout detection model to accelerate large-scale data construction. arXiv preprint arXiv:2503.17213, 2025. [27] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. CoRR, 2024. [28] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. [29] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. [30] Hynek Kydlíˇcek, Guilherme Penedo, and Leandro von Werra. Finepdfs. https://huggingface.co/datasets/ HuggingFaceFW/finepdfs, 2025. [31] pdfminer.six developers. pdfminer.six: python library for extracting information from pdf documents. https: //github.com/pdfminer/pdfminer.six, 2025. Version: 20250506. MIT License. [32] Brian Davis, Bryan Morse, Brian Price, Chris Tensmeyer, Curtis Wigington, and Vlad Morariu. End-to-end document recognition and understanding with dessurt. In European Conference on Computer Vision, pages 280296. Springer, 2022. [33] Anthony Kay. Tesseract: an open-source optical character recognition engine. Linux Journal, 2007(159):2, 2007. [34] Brian Davis. Tesseract ocr of iit-cdip dataset. https://doi.org/10.5281/zenodo.6540454, May 2022. [35] Adam Harley, Alex Ufkes, and Konstantinos Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In International Conference on Document Analysis and Recognition (ICDAR), 2015. [36] Hunter Heidenreich, Ratish Dalvi, Rohith Mukku, Nikhil Verma, and Neven Piˇculjan. Large Language Models for Page Stream Segmentation, August 2024. [37] Thisanaporn Mungmeeprued, Yuxin Ma, Nisarg Mehta, and Aldo Lipani. Tab this folder of documents: page stream segmentation of business documents. In Proceedings of the 22nd ACM Symposium on Document Engineering, pages 110, 2022. [38] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics, pages 949960, 2020. [39] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: large humanannotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 37433751, 2022. [40] Jing Fang, Xin Tao, Zhi Tang, Ruiheng Qiu, and Ying Liu. Dataset, ground-truth and performance metrics for table detection evaluation. In Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS), pages 445449, Gold Coast, QLD, Australia, March 2012. IEEE Computer Society. [41] Apostolos Antonacopoulos, David Bridson, Christos Papadopoulos, and Stefan Pletschacher. realistic dataset for performance evaluation of document layout analysis. In 2009 10th International Conference on Document Analysis and Recognition, pages 296300. IEEE, 2009. [42] Pablo Montalvo and Ross Wightman. PDF Association dataset (PDFA): English WebDataset Shards. https: //huggingface.co/datasets/pixparse/pdfa-eng-wds, 2024. Hugging Face dataset card. Accessed 202510-31. [43] Safedocs (cc-main-2021-31-pdf-untruncated). https://digitalcorpora.org/corpora/file-corpora/ cc-main-2021-31-pdf-untruncated/, 2023. Digital Corpora. Accessed 2025-10-31. [44] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models. [45] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 11921200, 2020. [46] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM international conference on multimedia, pages 40834091, 2022. 9 PubMed-OCR [47] Jiapeng Wang, Lianwen Jin, and Kai Ding. Lilt: simple yet effective language-independent layout transformer In Proceedings of the 60th Annual Meeting of the Association for for structured document understanding. Computational Linguistics (Volume 1: Long Papers), pages 77477757, 2022. [48] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout instruction tuning with large language models for document understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1563015640, 2024. [49] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. Docllm: layout-aware generative language model for multimodal document understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85298548, 2024. [50] Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, et al. bounding box is worth one token: Interleaving layout and text in large language model for document understanding. arXiv preprint arXiv:2407.01976, 2024. [51] Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, et al. Learning fine-grained grounded citations for attributed large language models. arXiv preprint arXiv:2408.04568, 2024. [52] Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, and Zhiguo Wang. Citeeval: Principle-driven citation evaluation for source attribution. arXiv preprint arXiv:2506.01829, 2025. [53] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025."
        },
        {
            "title": "A More Examples",
            "content": "We show handful of additional samples with layout detection annotations from PP-DocLayout in Figures 5 8. 10 PubMed-OCR Figure 5: sample page from PubMed-OCR exhibiting variety of features: aside text, charts, captions, and formulas. 11 PubMed-OCR Figure 6: sample with complex scientific table. second tabular section is mis-identified as an algorithm. Introducing PubMed-OCR with layout annotations could be valuable augmentation that would benefit current generation layout models. 12 PubMed-OCR Figure 7: sample page with dense text and table of contents as well as structured detection of paragraph and document titles. 13 PubMed-OCR Figure 8: sample page that is image-dense with structured captions about the details and intended interpretation."
        }
    ],
    "affiliations": [
        "Roots.ai"
    ]
}