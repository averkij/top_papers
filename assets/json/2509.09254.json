{
    "paper_title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis",
    "authors": [
        "Jing Hao",
        "Yuxuan Fan",
        "Yanpeng Sun",
        "Kaixin Guo",
        "Lizhuo Lin",
        "Jinrong Yang",
        "Qi Yong H. Ai",
        "Lun M. Wong",
        "Hao Tang",
        "Kuo Feng Hung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 5 2 9 0 . 9 0 5 2 : r Towards Better Dental AI: Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis Jing Hao1 Yuxuan Fan2 Yanpeng Sun3 Kaixin Guo1 Lizhuo Lin1 Jinrong Yang4,5 Qi Yong H. Ai6 Lun M. Wong7 Hao Tang8 Kuo Feng Hung1 1Faculty of Dentistry, The University of Hong Kong 2The Hong Kong University of Science and Technology (GZ) 4CVTE 5Sun Yat-sen University 3National University of Singapore 6Department of Diagnostic Radiology, The University of Hong Kong 7Imaging and Interventional Radiology, Faculty of Medicine, The Chinese University of Hong Kong 8School of Computer Science, Peking University"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates 24.73% improvement. Both MMOral and OralGPT hold significant potential as critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large vision-language models (LVLMs) [34, 41, 12, 8, 9, 45, 62, 55, 60, 20, 63, 77, 72, 68, 69, 43, 71, 18] have driven significant progress in open-world medical image understanding, supported by benchmarks such as GMAI-MMBench [67], RadBench [65], and OmniMedVQA [31], as well as models like LLaVA-Med [35], HuatuoGPT-Vision [15], MedDr [28], HealthGPT [38], and so on [47, 25, 46, 37]. These efforts focus on broad, general-purpose medical Equal Contribution. Project Leader. Corresponding Authors: hungkfg@hku.hk, haotang@pku.edu.cn Preprint. Under review. Figure 1: Overview of the MMOral. It consists of four sub-datasets: MMOral-Attribute, MMOralReport, MMOral-VQA, and MMOral-Chat. MMOral-Attribute contains total of 49 categories of anatomical structures within panoramic X-rays. MMOral-Report consists of two types of textual descriptions: the grounding caption and the medical report. MMOral-VQA includes closed-ended and open-ended QA pairs spanning five diagnostic dimensions. MMOral-Chat simulates the dialogue process between patients and radiology experts regarding the interpretation of panoramic X-rays. scenarios, aiming to evaluate and improve LVLMs across diverse modalities and tasks. However, these general-purpose benchmarks overlook the unique requirements of domain-specific medical fields. In particular, oral radiologya critical specialty relying on dental imaging for diagnosis and treatment planningremains largely absent from existing medical benchmarks. The panoramic X-ray is one of the most commonly used imaging modalities and has been widely accepted as primary source of information for assessing oral health [58, 17]. It provides comprehensive visualization of all teeth and surrounding structures in single image, enabling basic evaluation of dentition, periodontal bone loss, and lesions within the jawbones. The omission of this modality leaves significant gap: the lack of tailored evaluation and instruction resources hampers the adaptation of LVLMs to dentistry-specific tasks. Unlike other modalities, interpreting panoramic X-rays presents unique challenges, characterized by dense anatomical structures and fine-grained pathological cues. Addressing these challenges requires not only dental-specific instruction data but also specialized benchmark aligned with the clinical knowledge of dental practitioners. To bridge this gap, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray understanding. MMOral comprises 20,563 annotated panoramic X-rays paired with 1.3 million instruction-following instances, spanning multiple task formats including attribute extraction, report generation, visual question answering, and image-grounded dialogue. Complementing the dataset, MMOral-Bench offers curated evaluation suite covering five key diagnostic dimensions, including the condition of teeth, pathological findings, historical treatments, jawbone observations, and clinical summary & recommendations. This benchmark consists of 100 images, paired with 500 closed-ended and 600 open-ended questions. All cases in MMOral-Bench are manually chosen and checked from the MMOral to ensure their quality and reliability. Together, MMOral and MMOral-Bench lay critical foundation for advancing intelligent dentistry and enabling clinically meaningful multimodal AI. 1https://platform.stepfun.com/ 2https://www.volcengine.com/product/doubao/ 2 Figure 2: The MMOral dataset curation pipeline, which consists of four sequential steps. We assess 53 publicly available LVLMs (44 general-purpose and 9 medical-specific models) as well as 11 advanced proprietary LVLMs such as GPT-4o [32], GPT-4V [32], Claude-3-7-Sonnet [11], Gemini-2.0-Flash [53], Gemini-2.5-Flash [53], Qwen-Max-VL [12], Step-1o series 1, and Doubao1.5 series 2 on our Oral-Bench. We summarize five findings according to the evaluation outcomes: (1) MMOral-Bench is significant challenge for current LVLMs, even for GPT-4o, which achieves only 41.45% average score. (2) The performance of existing universal medical LVLMs is suboptimal in the field of dental applications. (3) Existing medical LVLMs show no clear advantage over generalpurpose LVLMs for dentistry tasks. (4) Nearly all LVLMs perform worse on open-ended questions compared to closed-ended questions in the MMOral-Bench. (5) LVLMs exhibit clear performance bias across five diagnostic dimensions (depending on the focus of anatomical structures), and they show relatively limited capability when it comes to fine-grained teeth-related questions. To further promote the progress of intelligent dentistry, we propose OralGPT, which conducts extensive supervised fine-tuning (SFT) experiments on the Qwen2.5-VL-7B model [12]. Experimental results show that the average score of OralGPT on MMOral-Bench could improve by 24.73% when conducting SFT utilizing MMOral instruction data (MMOral-Report, MMOral-VQA, MMOral-Chat) for one epoch. This obvious improvement highlights the value of the MMOral towards intelligent dentistry. To summarize, our contributions are threefold: (1) We introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray understanding. MMOralBench offers curated evaluation suite covering five key diagnostic dimensions, which could comprehensively reflect the capabilities of current LVLMs in the dental field. (2) 64 existing representative LVLMs are assessed on MMOral-Bench, including 11 proprietary models and 53 open-source models (44 general-purpose models and 9 medical-specific models). Evaluation results pave the way for the next optimization direction to enhance the interpretation of panoramic X-rays. (3) We propose OralGPT, which implements supervised fine-tuning using our MMOral instruction data to enhance the capability of panoramic X-ray analysis. Experiments demonstrate the average score of OralGPT on MMOral-Bench could improve by 24.73% when conducting SFT for only one epoch."
        },
        {
            "title": "2 MMOral Dataset Curation",
            "content": "The MMOral dataset curation pipeline consists of four sequential steps, which are shown in Figure 2. 2."
        },
        {
            "title": "Images and Visual Specialists Construction",
            "content": "We collect panoramic X-ray images from two publicly available datasets: the TED3 dataset [27] and the dataset proposed by Hoang Viet Do [19]. After filtering out duplicate images, total of 20,563 images is obtained. Subsequently, we build ten visual specialist models to simulate the interpretative process of oral radiology experts. These models are designed to recognize 49 categories of anatomical structures in radiographic images, encompassing visual elements such as tooth numbering (1 to 32 according to the FDI system [64]), four quadrants, pathological findings, historical treatments, bone loss, and visible bone structures. The category spaces of multiple visual specialist models overlap with each other, 3 ensuring the reliability of anatomical structure extraction. Additionally, we utilize an off-the-shelf OpenOCR model [21] to detect the acquisition time displayed within the images. More details of image source information, visual specialist model construction, and the specific category list of detected anatomical structures can be found in the supplementary materials."
        },
        {
            "title": "2.2 Anatomical Structure Extraction",
            "content": "After obtaining the extracted anatomical structures from all visual specialist models, we further integrate these outcomes for two primary considerations: (1) the necessity to deduplicate overlapping categories detected by multiple visual specialist models, and (2) the discrete nature of identified anatomical structures lacking interrelational information. First, we meticulously design the anatomical structures post-processing pipeline to remove redundant information. Second, since the associations between dental pathological findings, historical treatment, and their corresponding tooth numbering systems remain undefined, we establish these correlations through their spatial relationships. Third, domain-specific clinical knowledge is inserted. For instance, extraction of maxillary third molars (teeth #18/28) is recommended when they lack opposing mandibular counterparts (teeth #48/38). The developed anatomical structure post-processing and relationship matching pipeline comprises eight systematic steps, as detailed in the supplementary materials."
        },
        {
            "title": "2.3 Report Generation",
            "content": "We synthesize the discrete anatomical structure information into two coherent textual outputs: grounding caption and medical report. The grounding caption contains detailed coordinates, categories, and confidence scores of all anatomical structures, providing multi-dimensional spatial observation and identification for panoramic X-rays. All anatomical information is systematically organized into structured textual descriptions following manually designed rules. An illustrative example of the grounding caption is shown in the left panel of the MMOral-Report part in Figure 1. In contrast to grounding captions, medical reports focus on providing condensed summary of key anatomical structures, abnormal findings, and corresponding diagnostic recommendations. Through extensive consultations with senior dental specialists, we structure the medical report into three principal sections: Teeth-Specific Observations, Jaw-Specific Observations, and Clinical Summary & Recommendations. The medical report is derived from two-stage LLM-based generation. First, we prompt DeepSeek-R1-Distill-Llama-70B3 to generate medical reports based on grounding captions. Following that, we manually check the generated medical reports and summarize several common errors. According to these errors, we carefully prepare the prompt for report correction and instruct GPT-4-turbo [10] to simultaneously output both revised reports and corresponding revision logs. By examining these revision logs, we can efficiently identify modified sections of the reports, thereby facilitating quality verification of the revised content. To comprehensively evaluate the quality of the revised reports, two professional dentists are engaged to conduct the human evaluation, which will be discussed in section 3.2. The details of prompts in two-stage generation are provided in the supplementary materials. 2."
        },
        {
            "title": "Instruction Data Generation",
            "content": "Based on the generated grounding captions and medical reports, we construct two types of single-turn instruction-following QA pairsclosed-ended and open-endedusing template-based and LLMbased approaches. For grounding captions, we generate both closed-ended multiple-choice QA pairs and open-ended QA pairs using manually designed question templates. The incorrect choices in the closed-ended QA are created by introducing random perturbations to the ground truth for enhancing the models ability to recognize and understand subtle differences. For the medical report, we prompt GPT-4-turbo (see supplementary materials for prompt designs) to simultaneously generate both closed-ended and open-ended QA pairs. To systematically evaluate panoramic X-ray analysis capabilities, we establish taxonomy across five clinically grounded dimensions: condition of teeth (Teeth), pathological findings (Patho), historical treatments (HisT), jawbone observations (Jaw), and clinical summary & recommendation (SumRec). Each QA pair is mapped to one or more of these diagnostic categories based on its clinical intent, forming multi-dimensional analysis. 3https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B 4 Table 1: The brief description of four sub-datasets in MMOral and their corresponding data size. Dataset Sub-Dataset Description MMOral MMOral-Attribute The category, position and correlation of anatomical structures shown in the panoramic X-ray image. MMOral-Report Two types of textual description for each panoramic X-ray image: grounding caption and medical report. MMOral-VQA MMOral-Chat Two types of visual question answering: closed-ended QA and open-ended QA. The multi-turn conversation between the assistant and person asking questions about the panoramic X-ray. Size 904k 41k 965k 296k In addition to the single-turn QA pairs mentioned above, we also prompt GPT-4-turbo to generate multi-turn conversation between the assistant and person asking questions about the panoramic X-ray. The answers are in tone as if the assistant is seeing the panoramic radiograph and explaining the findings and summaries in patient-friendly manner. diverse set of questions is asked about the visual content of the image and the assistants explanations, and only questions that have definite answers are considered. Please see supplementary materials for the detailed prompt."
        },
        {
            "title": "3.1 Data Statistics",
            "content": "The MMOral comprises 20,563 images paired with 1.3 million instruction-following data instances, establishing comprehensive multimodal resource for human-AI interaction research in the digital dentistry field. It consists of four distinct sub-datasets: MMOral-Attribute, MMOral-Report, MMOralVQA, and MMOral-Chat. Each sub-dataset corresponds to specific tasks, including visual perception, report generation, visual question answering, and image-grounded dialogue. Table 1 provides detailed information on each component and its corresponding data size. Notably, single panoramic X-ray contains an average of 44 bounding boxes, reflecting its structural diversity and dense representations, making its interpretation highly complex. Figure 3 (a) presents the category distribution of MMOralAttribute, comprising 5 major categories and 49 subcategories of anatomical structures identified in panoramic X-rays, illustrating its diversity and comprehensive coverage. To the best of our knowledge, MMOral is the largest multimodal dataset for panoramic X-rays to date, forming robust foundation for the development and evaluation of LVLMs. Figure 3: The data statistic distribution and human evaluation results."
        },
        {
            "title": "3.2 Strategies for Ensuring Data Quality",
            "content": "We ensure the quality of the MMOral from the following three perspectives: (1) Collaborative validation of anatomical structures by multiple specialist models. The anatomical structures present in panoramic X-rays serve as metadata for subsequent report generation and instruction data construction. If these visual structures contain errors, such inaccuracies will propagate throughout all datasets. To ensure precise detection of anatomical structures, we construct ten visual specialist models with overlapping category spaces. For instance, ten structures (e.g., caries, periapical lesion, impacted teeth, missing teeth area, filling, implant, root canal treatment, crown, 5 mandibular canal, and maxillary sinus) are validated by two or more visual specialists, with the final results obtained through post-processing. This approach ensures the reliability of the extraction of anatomical structures, providing trustworthy metadata for subsequent medical report generation and instruction data construction. (2) Two-stage LLM-based report generation. Unlike most existing approaches that rely on single LLM to generate image captions, we adopt two-stage scheme of generation followed by correction to ensure report quality. Due to the complexity of generating structured medical reports based on discrete anatomical structures, LLMs inevitably introduce errors in areas such as structural organization, content description, and hallucinations. By manually reviewing the preliminary reports generated by DeepSeek-R1-Distill-Llama-70B, we identify common errors and summarize them into 8 rules for prompting GPT-4-turbo to revise the reports. Analysis of the revision logs revealed that 95.45% of the reports are successfully corrected, significantly improving their overall quality. (3) Professional dentist evaluation. We invite two professional dentists to evaluate the medical report and assess whether the associated panoramic X-ray image meets practical imaging quality standards. Both the image quality and the report quality are evaluated using scoring system with five levels, ranging from 1 to 5, representing progression from \"very dissatisfied\" to \"very satisfied.\" For the report evaluation, in addition to assigning an overall score, the evaluators are required to provide further scores in four specific aspects: correctness, completeness, relevance, and clarity. Figure 3 (b) illustrates the overall scoring results for both the image quality and the report quality, showing that the average scores for all aspects exceeded 3.5, highlighting the high quality of MMOral. The evaluation guidelines are detailed in the supplementary materials."
        },
        {
            "title": "4.1 Benchmark Construction",
            "content": "We construct MMOral-Bench by curating 500 closed-ended and 600 open-ended QA pairs with 100 images through significant manual selection and validation. To ensure image quality, we select images from the dataset proposed by Hoang Viet Do [19] because its acquisition process is clearer and more reliable. Moreover, we filter out QA pairs that could not be answered with the image, and incorrect answers are identified and re-annotated. MMOral-Bench covers five different clinically grounded dimensions (e.g., Teeth, Patho, HisT, Jaw, SumRec) and thus can comprehensively evaluate the ability of LVLMs to understand and interpret panoramic X-rays. Each QA pair is assigned to one or more diagnostic dimensions based on its clinical intent, enabling multi-dimensional analysis. The distribution of the diagnostic dimension on MMOral-Bench can be seen in Figure 3(c), and some QA examples are present in the MMOral-VQA part of Figure 1."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "We adopt two evaluation metrics tailored for closed-ended and open-ended questions, respectively. For closed-ended questions, we use accuracy as the evaluation metric. For open-ended questions, following previous works [73, 74], we construct few-shot prompt and leverage GPT-4-turbo to assist with the evaluation. The few-shot prompt incorporates nine in-context examples with open-ended answers, covering fully correct cases, partially correct cases, and incorrect cases. GPT-4-turbo assigns score ranging from 0 to 1 based on each samples input question, ground truth, and model output. We report the evaluation scores for each category as well as the overall performance. The full details of the few-shot prompt can be found in the supplementary materials. We integrate the evaluation of MMOral-Bench into the standard VLMEvalKit [22] framework, thereby facilitating subsequent capability assessments of newly developed LVLMs4."
        },
        {
            "title": "5.1 Experimental Setups",
            "content": "Benchmarked LVLMs. We conduct zero-shot evaluations across 64 LVLMs on our MMOral-bench, and we pick 36 out of 64 models for demonstration in the main text; additional results are provided 4https://anonymous.4open.science/r/MMOral-2459/README.md 6 Table 2: Results on MMOral-Bench for existing various LVLMs across both closed-ended and open-ended VQA tasks. The best-performing model in each category is highlighted in-bold, while the second-best is underlined. 36 out of 64 models for demonstration in the main text; additional results are provided in the supplementary material. Model Proprietary LVLMs GPT-4o-2024-11-20 [32] GPT-4V [32] Claude-3-7-Sonnet-20250219 [11] Gemini-2.5-Flash-preview-04-17 [53] Gemini-2.0-Flash [53] Qwen-Max-VL-2025-04-08 [12] Step-1o-turbo -1 Doubao-1-5-thinking-vision-pro-250428 0 Open-Source LVLMs Deepseek-VL-7b-chat [43] Emu3-chat [62] Qwen2.5-VL-72B [13] CogVLM2-19B [60] GLM-4V-9B [23] LLaVA-NeXT-13B-hf [41] LLaVA-OneVision [34] LLaMA-3.2-Vision-11B-Instruct [24] Cambrian-34B [57] Phi-4-multimodal-instruct [9] InternVL3-38B [16] Chameleon-7B [44] PaliGemma-3B [14] MiniCPM-O2.6 [66] Kosmos-2 [50] mPLUG-Owl3-7B [68] Gemma3-12B [54] XComposer2-VL-7B [76] Molmo-72B-0924 [18] Yi-VL-34B [71] Qwen-QVQ-72B [56] Ovis2-34B [45] Kimi-VL-A3B-Thinking [55] Medical Specific LVLMs LLaVA-Med [35] HuatuoGPT-V-34B [15] HealthGPT-XL32 [38] MedVLM-R1 [47] MedDr [28] Close-ended VQA Open-ended VQA Teeth Patho His Jaw Summ Overall Teeth Patho His Jaw Summ Report Overall 39.65 37.88 41.24 24.60 37.17 18.41 31.86 26.20 22.65 40.89 26.55 33.63 29.03 30.09 14.51 27.96 34.87 36.28 28.67 32.57 26.02 30.27 15.75 34.16 24.78 25.49 28.85 36.81 48.67 45.84 25.84 25.49 28.85 39.65 28.67 36.46 40.99 39.13 37.27 16.15 35.40 11.18 24.22 27.33 17.39 44.72 27.95 31.68 35.40 32.92 18.01 21.12 34.16 36.65 21.12 44.10 24.22 23.60 18.01 32.30 19.88 26.71 14.91 36.64 49.07 51.55 27. 26.71 14.91 44.10 31.68 36.02 46.71 48.50 44.31 27.55 44.31 27.55 38.92 23.35 28.74 37.73 26.35 34.13 41.32 30.54 35.33 40.72 44.31 49.10 25.75 37.13 42.52 24.55 28.14 36.53 31.74 21.56 29.94 43.11 59.28 53.89 25.75 21.56 29.94 51.50 37.72 41.92 55.81 51.69 39.70 20.60 46.82 32.96 55.81 19.85 59.93 60.67 22.47 38.95 62.55 38.20 42.70 50.94 70.04 60.30 39.33 29.59 47.57 36.33 10.11 71.91 34.08 13.86 26.59 41.20 74.53 79.40 29. 13.86 26.59 76.41 65.17 73.03 56.25 58.33 45.83 10.42 58.33 47.92 41.67 31.25 52.08 43.75 47.92 60.42 64.58 60.42 31.25 50.00 60.42 54.17 31.25 52.08 35.42 14.58 25.00 62.50 29.17 45.83 22.92 70.83 72.92 79.17 27.08 45.83 22.92 79.17 47.92 64.58 45.40 43.40 41.40 22.00 41.20 22.00 36.00 24.80 31.20 45.8 26.80 35.20 40.20 33.80 24.40 34.00 44.40 43.60 28.40 35.80 33.20 30.27 17.40 42.80 26.60 23.20 25.60 40.20 56.60 56.80 26. 23.20 25.60 52.00 38.60 46.00 31.48 27.76 36.93 35.99 37.27 10.22 33.02 34.38 12.75 18.02 13.05 26.11 17.85 14.48 22.68 24.81 33.10 25.52 33.69 6.02 8.20 29.20 13.58 12.50 25.21 6.52 9.25 24.97 20.37 32.48 52.53 23.23 32.62 29.80 22.58 27.50 26.05 13.47 26.65 22.76 26.05 7.30 21.56 25.45 8.16 7.02 18.44 17.09 8.01 10.28 13.48 20.71 21.42 20.14 22.41 6.10 9.65 17.38 10.71 8.44 20.00 11.01 6.31 23.40 12.28 24.33 37. 18.75 18.65 22.16 12.28 28.14 37.56 33.50 42.39 40.61 40.36 11.12 30.20 39.09 8.40 15.50 11.66 26.86 17.46 9.23 17.75 25.03 31.83 27.69 29.70 9.35 8.70 24.38 11.18 8.52 20.65 15.00 3.49 20.59 16.75 31.60 53.79 11.36 28.05 24.11 21.57 30.20 57.42 58.95 51.09 51.53 52.40 22.88 51.31 56.11 30.00 28.53 26.88 49.24 24.12 22.41 38.35 33.65 48.24 43.29 46.11 9.71 13.29 49.76 19.76 30.59 26.88 7.67 12.65 39.35 41.05 50.88 68. 32.82 53.12 47.82 40.61 49.17 30.37 30.84 28.04 32.71 35.05 6.86 31.31 39.72 13.14 12.44 7.44 18.14 15.93 14.30 18.72 17.56 13.60 13.84 20.23 5.35 6.16 15.93 8.49 3.26 22.33 2.10 5.00 15.23 22.90 21.05 50.93 26.28 18.60 24.77 21.96 26.17 42.50 45.00 50.00 45.50 49.00 27.00 45.00 49.00 9.10 9.60 11.50 24.50 19.40 21.30 11.20 19.50 16.00 12.80 42.90 8.40 0.60 27.90 3.40 13.90 33.20 8.53 9.20 9.90 24.5 31.70 61. 5.30 15.40 10.00 24.50 7.50 37.50 34.83 40.67 39.08 40.67 14.33 36.00 40.33 13.42 16.05 14.77 27.63 17.50 15.43 20.93 23.97 29.63 24.57 34.15 7.27 7.78 28.42 11.87 13.67 25.32 8.99 8.23 22.98 22.75 33.02 54.55 19.60 29.48 27.17 24.58 26.17 Avg. 41.45 39.12 41.04 30.54 40.94 18.15 36.00 32. 22.31 30.93 20.79 31.42 28.85 24.62 22.67 28.99 37.02 34.09 31.28 21.54 20.49 28.21 14.64 28.24 25.96 16.10 16.92 31.59 39.68 44.91 40.68 21.40 27.54 39.59 31.59 36.09 in the supplementary material. We evaluate 8 proprietary LVLMs via API: GPT-4o [32], GPT-4V [32], Claude-3-7-sonnet [11], Gemini-2.5-Flash [53], Gemini-2.0-Flash [53], Qwen-Max-VL [12], Step-1o-turbo3, and Doubao-1-5-thinking-vision-pro4. For medical-specific LVLMs, we test 5 powerful models, including LLaVA-Med [35], HuatuoGPT-V [15], MedVLM-R1 [47], MedDr [28], and HealthGPT [38]. We also evaluate 23 representative general-purpose LVLMs: Deepseek-VL7b [43], Emu3 [62], Qwen2.5-VL-72B [13], CogVLM2-19B [60], GLM-4V-9B [23], LLaVA-NeXT13B-hf [41], LLaVA-OneVision [34], LLaMA-3.2-Vision-11B [24], Cambrian-34B [57], Phi-4multimodal-instruct [9], InternVL3-38B [16], Chameleon-7B [44], PaliGemma-3B [14], MiniCPMO2.6 [66], Kosmos-2 [50], Kimi-VL-A3B-Thinking [55], Ovis2-34B [45], Qwen-QVQ-72B [56], Gemma3-12B [54], XComposer2-VL-7B [76], Molmo-72B [18], and Yi-VL-34B [71]. Supervised Fine-Tuning. We also implement supervised fine-tuning (SFT) on two popular LVLM models with different scales (Qwen-2.5-VL-7B [12] and LLaVA-Next-13B [41]) using our MMOral instruction data to verify its effectiveness. The extensive experiments are implemented through the LLaMA-Factory framework [78] while maintaining default hyperparameters, with all models being trained for single epoch. The results on Qwen-2.5-VL-7B will be discussed in Sec. 5.3, and the results on LLaVA-Next-13B can be found in supplementary materials. We refer to the LVLM obtained through supervised fine-tuning of Qwen-2.5-VL-7B as OralGPT, due to its outstanding performance in panoramic X-ray analysis."
        },
        {
            "title": "5.2 Evaluation Results",
            "content": "Following comprehensive review of the evaluation outcomes, which are shown in Table 8, we have identified 5 key findings regarding the performance of existing LVLMs in the dental domain: Finding 1. The MMOral-Bench poses significant challenges to ALL LVLMs. Even the most advanced model, GPT-4o, only achieves 41.45% overall performance, highlighting persistent challenges and fundamental limitations in current LVLMs capacity to interpret complex panoramic X-rays, which are characterized by anatomically dense structures and fine-grained pathological patterns. This critical performance gap reveals fundamental limitations of existing LVLMs capacity in dental-specific images, underscoring substantial room for improvement. 7 Figure 4: Performance comparison on both closed-ended and open-ended QA across multiple LVLMs. Table 3: The effectiveness verification of MMOral instruction data by supervised fine-tuning. Model Qwen2.5-VL-7B [13] OralGPT OralGPT OralGPT OralGPT SFT Close-ended VQA Open-ended VQA Report VQA Chat Teeth Patho 24.96 21.12 26.90 39.12 43.19 37.17 27.33 36.65 40.99 30.43 His Jaw Summ Overall Teeth Patho His Jaw Summ Report Overall 27.54 37.08 26.35 37.73 43.11 38. 45.32 62.92 63.60 52.81 35.42 37.50 43.75 37.50 45.83 27.00 31.00 43.60 46.20 39.60 17. 16.10 11.18 29.41 9.07 27.82 36.22 39.85 55.45 15.82 31.92 32.41 33. 25.92 32.49 35.20 45.74 63.76 78.47 78.06 74.47 22.33 40.93 36.98 45.17 8.20 38.00 4.30 36.80 50.50 15. 32.62 35.73 42.85 52.77 Avg. 21.46 31.81 39.67 44.53 46.19 Finding 2. The performance of existing universal medical LVLMs is suboptimal in the field of dental applications. Current universal medical LVLMs, which predominantly focus on enhancing capabilities for generalized clinical scenarios across diverse medical imaging modalities, have unsatisfactory performance when it comes to understanding panoramic X-raysa specialized, finegrained modality within dental imaging. The results reveal that general medical LVLMs achieve less than 40% average accuracy on MMOral-Bench, with HealthGPT-XL32 [38] attaining peak performance at 39.59%. This indicates that current universal medical LVLMs still require further exploration and improvement in their ability to interpret panoramic X-rays, which is an imaging modality characterized by complex and numerous anatomical structures. Finding 3. Existing medical LVLMs show no significant advantage over general LVLMs in the field of dentistry. Existing medical-specific LVLMs, including the LLaVA-Med series, HuatuoGPT series, MedVLM-R1, MedDr, and HealthGPT, fail to outperform general-purpose models in our MMOral-bench. This indicates that current medical LVLMs lack adequate understanding and analytical capabilities specific to the oral region. Among all medical LVLMs evaluated, HealthGPT demonstrates the best performance, achieving an average score of 39.59%. However, this score remains lower than that of general-purpose open-source models such as the Ovis2 series and commercial models like GPT-4o and Claude-3-7-Sonnet. These results highlight the need for further improvements in medical-specific AI models to enhance their understanding of the oral regionan area intrinsically linked to essential human functions such as eating and speaking. Finding 4. Nearly all LVLMs perform worse on open-ended questions compared to closed-ended questions in the MMOral-Bench. Figure 4 shows the performance comparison on both closed-ended and open-ended QA tasks. Although some LVLMs (e.g., LLaVA-NeXT-8B-hf, DeepSeek-VL-7B, Ovis2-34B) perform relatively well in closed-ended VQA tasks, they exhibit significant performance drop in open-ended VQA tasks. Moreover, the proportion of open-sourced models with an overall score below 25% in open-ended VQA tasks is as high as 62.3% (33 out of 53 models). This highlights the current limitations of LVLMs in handling open-ended answer generation for dentistry-related questions and the urgent need for targeted optimization. Finding 5. LVLMs demonstrate clear bias across various question categories, depending on the focus of anatomical structures involved. The questions are categorized into five dimensions based on their focus as mentioned in Sec. 2.4: Teeth, Patho, HisT, Jaw, SumRec. We observe that current LVLMs perform relatively well on the Jaw categoryboth in closed-ended and openended taskswhere the focus is on larger anatomical structures such as bone loss, mandibular canals, and maxillary sinuses. In contrast, the models generally exhibit poorer performance on categories that require fine-grained visual understanding, such as Teeth, HisT, and Patho. This suggests that current LVLMs still need significant improvement in their ability to perform fine-grained dental visual understanding and reasoning. 8 Figure 5: Three examples of case studies on closed-ended QA and open-ended QA, respectively. More examples can be found in the supplementary materials."
        },
        {
            "title": "5.3 Efficacy Validation of MMOral Instruction Data",
            "content": "We implement supervised fine-tuning (SFT) on the Qwen2.5-VL-7B model using our MMOral instruction data, and the results are presented in Table 9. When using the MMOral-report or MMOralVQA dataset individually for SFT, the average score on MMOral-Bench improved by 10.35% (from 21.46% to 31.81%) and 18.21% (from 21.46% to 39.67%), respectively. Furthermore, when both MMOral-report and MMOral-VQA are used together for SFT, the average score achieves more significant improvement, rising from 21.46% to 44.53%. Based on this, incorporating the MMOral-Chat into the SFT process results in an additional 1.66% improvement in the average score. Notably, OralGPT demonstrates significant improvements on open-ended QA tasks when MMOral-Chat is included in SFT, while exhibiting slight decline in performance on closed-ended QA tasks. We hypothesize that image-grounded conversation data can significantly enhance the models instruction-following ability for open-ended questions and improve overall user experience."
        },
        {
            "title": "5.4 Case Study",
            "content": "To provide more intuitive demonstration of the current capabilities of LVLMs in understanding panoramic X-rays, we conduct case study on high-performing proprietary model (GPT-4-Turbo) and medical-specific LVLM (MedDr [28]) by analyzing their performance on both closed-ended and open-ended question types. Figure 5 (a) illustrates closed-ended QA case, where the question requires the LVLM to simultaneously understand tooth numbering according to the FDI standard, detect caries, and identify periapical abscesses. As shown, GPT-4V could successfully answer the question, while MedDr provides an incorrect response. For the open-ended case, both GPT-4V and MedDr demonstrate some level of understanding of panoramic X-rays but fall short of providing comprehensive outcomes. For instance, as depicted in Figure 5 (b), when examining teeth #36 and #46, which both exhibit crown restoration and root canal treatment, GPT-4-Turbo is able to detect the dental crown but explicitly states that no root canal treatment is identified. Conversely, MedDr detects the root canal treatment but overlooks the clearly visible crown restoration. Despite being among the top-performing models on the MMOral-Bench benchmark, GPT-4-Turbo and MedDr still exhibit significant limitations, highlighting substantial room for improvement in their ability to accurately interpret panoramic X-rays. Figure 5 (c) shows the case that GPT-4V misrecognises the tooth #44 and #45, leading to completely incorrect answer. Additionally, we observe that some proprietary models, such as Qwen-Max-VL and Qwen-QVQ, commonly refuse to provide answers due to safety concerns stemming from commercial policies. For example, these models often respond with statements like, Input data may contain inappropriate content. This strict adherence to safety protocols and ethical standards significantly limits their response capabilities in the dental field."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose MMOral, the first large-scale multimodal instruction dataset and benchmark for panoramic X-ray understanding, and introduce OralGPT, powerful multimodal vision-language model specifically designed for panoramic X-ray analysis. The evaluation outcomes on MMOralBench reveal that existing universal medical-specific LVLMs have unsatisfactory performance when it comes to understanding panoramic X-rays characterized by dense anatomical structures and finegrained pathological cues. We hope that our instruction dataset and benchmark will serve as pivotal resource for advancing digital dentistry towards more sophisticated and intelligent multimodal AI in oral healthcare. Limitation and Future Work. MMOral exhibits limitations in imaging modality diversity compared to existing medical instruction datasets. However, panoramic X-rays hold significant clinical value owing to their comprehensive visualization of the entire oral anatomy. Automated interpretation of panoramic X-rays could substantially advance intelligent dental AI. Future efforts will aim to expand coverage to additional oral imaging modalities, including 2D modalities such as periapical X-rays, intraoral photographs and cephalometric radiographs, as well as 3D modalities like cone-beam computed tomography (CBCT) and magnetic resonance imaging (MRI)."
        },
        {
            "title": "References",
            "content": "[1] Dentistry computer vision project, 2023. URL https://universe.roboflow.com/ nanyang-technological-university-kdgtt/dentistry-vibir. [2] Dental caries detection computer vision project, 2023. URL https://universe.roboflow.com/ panoramic-xray-images/dental-caries-detection. [3] 2023. yaml. URL https://www.kaggle.com/datasets/reemsalahshehab/dental?select=data. [4] Dental 2024. diseases, dental-diseases/data. URL https://www.kaggle.com/datasets/ayaalialnozahyy/ [5] vzrad2 computer vision project, 2024. URL https://universe.roboflow.com/ arshs-workspace-radio/vzrad2. [6] 2024. URL https://universe.roboflow.com/arshs-workspace-radio/vzrad2. [7] 2024. URL https://www.kaggle.com/datasets/nadaaglan/dental-periapical-x-rayss. [8] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [9] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [10] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [11] Anthropic. Claude 3.7 sonnet system card. URL https://api.semanticscholar.org/CorpusID: 276612236. [12] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [13] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [14] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [15] J. Chen, C. Gui, R. Ouyang, A. Gao, S. Chen, G. H. Chen, X. Wang, R. Zhang, Z. Cai, K. Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. [16] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [17] T. S. de Oliveira Capote, M. de Almeida Gonçalves, A. Gonçalves, and M. Gonçalves. Panoramic radiographydiagnosis of relevant structures that might compromise oral and general health of the patient. In Emerging Trends in Oral Health Sciences and Dentistry. IntechOpen, 2015. [18] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [19] H. V. Do, T. N. N. Vo, P. T. Nguyen, T. H. L. Luong, N. G. Cu, and H. S. Le. dataset of apical periodontitis lesions in panoramic radiographs for deep-learning-based classification and detection. Data in Brief, 54:110486, 2024. [20] R. Dong, C. Han, Y. Peng, Z. Qi, Z. Ge, J. Yang, L. Zhao, J. Sun, H. Zhou, H. Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. International Conference on Learning Representations, 2024. [21] Y. Du, Z. Chen, H. Xie, C. Jia, and Y.-G. Jiang. Svtrv2: Ctc beats encoder-decoder models in scene text recognition. CoRR, abs/2411.15858, 2024. URL https://arxiv.org/abs/2411.15858. [22] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [23] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. [24] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [25] Y. Guo and W. Huang. Llava-next-med: Medical multimodal large language model. In 2025 Asia-Europe Conference on Cybersecurity, Internet of Things and Soft Computing (CITSC), pages 474477. IEEE, 2025. [26] I. E. Hamamci, S. Er, E. Simsar, A. E. Yuksel, S. Gultekin, S. D. Ozdemir, K. Yang, H. B. Li, S. Pati, B. Stadlinger, et al. Dentex: An abnormal tooth detection with dental enumeration and diagnosis benchmark for panoramic x-rays. arXiv preprint arXiv:2305.19112, 2023. [27] J. Hao, Y. Zhu, L. He, M. Liu, J. K. H. Tsoi, and K. F. Hung. T-mamba: unified framework with long-range dependency in dual-domain for 2d & 3d tooth segmentation. arXiv preprint arXiv:2404.01065, 2024. [28] S. He, Y. Nie, Z. Chen, Z. Cai, H. Wang, S. Yang, and H. Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv e-prints, pages arXiv2404, 2024. [29] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. [30] Z. He, Y. Wang, A. Yan, Y. Liu, E. Y. Chang, A. Gentili, J. McAuley, and C.-N. Hsu. Medeval: multilevel, multi-task, and multi-domain medical benchmark for language model evaluation. arXiv preprint arXiv:2310.14088, 2023. [31] Y. Hu, T. Li, Q. Lu, W. Shao, J. He, Y. Qiao, and P. Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2217022183, 2024. [32] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [33] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 11 [34] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [35] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. [36] S. Li, T. Lin, L. Lin, W. Zhang, J. Liu, X. Yang, J. Li, Y. He, X. Song, J. Xiao, et al. Eyecaregpt: Boosting comprehensive ophthalmology understanding with tailored dataset, benchmark and model. arXiv preprint arXiv:2504.13650, 2025. [37] T. Li, Y. Su, W. Li, B. Fu, Z. Chen, Z. Huang, G. Wang, C. Ma, Y. Chen, M. Hu, et al. Gmai-vl & gmai-vl-5.5 m: large vision-language model and comprehensive multimodal dataset towards general medical ai. arXiv preprint arXiv:2411.14522, 2024. [38] T. Lin, W. Zhang, S. Li, Y. Yuan, B. Yu, H. Li, W. He, H. Jiang, M. Li, X. Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. [39] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [40] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [41] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/. [42] M. Liu, W. Hu, J. Ding, J. Xu, X. Li, L. Zhu, Z. Bai, X. Shi, B. Wang, H. Song, et al. Medbench: comprehensive, standardized, and reliable benchmarking system for evaluating chinese medical large language models. Big Data Mining and Analytics, 7(4):11161128, 2024. [43] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [44] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36:4344743478, 2023. [45] S. Lu, Y. Li, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, and H.-J. Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. [46] M. Moor, Q. Huang, S. Wu, M. Yasunaga, Y. Dalmia, J. Leskovec, C. Zakka, E. P. Reis, and P. Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. [47] J. Pan, C. Liu, J. Wu, F. Liu, J. Zhu, H. B. Li, C. Chen, C. Ouyang, and D. Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. [48] K. Panetta, R. Rajendran, A. Ramesh, S. P. Rao, and S. Agaian. Tufts dental database: multimodal panoramic x-ray dataset for benchmarking diagnostic systems. IEEE journal of biomedical and health informatics, 26(4):16501659, 2021. [49] J. Park, S. Kim, B. Yoon, J. Hyun, and K. Choi. M4cxr: Exploring multi-task potentials of multi-modal large language models for chest x-ray interpretation. arXiv preprint arXiv:2408.16213, 2024. [50] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [51] Z. Qin, Y. Yin, D. Campbell, X. Wu, K. Zou, Y.-C. Tham, N. Liu, X. Zhang, and Q. Chen. Lmod: large multimodal ophthalmology dataset and benchmark for large vision-language models. arXiv preprint arXiv:2410.01620, 2024. [52] Y. Sun, H. Wu, C. Zhu, S. Zheng, Q. Chen, K. Zhang, Y. Zhang, D. Wan, X. Lan, M. Zheng, et al. Pathmmu: massive multimodal expert-level benchmark for understanding and reasoning in pathology. In European Conference on Computer Vision, pages 5673. Springer, 2024. 12 [53] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [54] G. Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [55] K. Team, A. Du, B. Yin, B. Xing, B. Qu, B. Wang, C. Chen, C. Zhang, C. Du, C. Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [56] Q. Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/ blog/qvq-72b-preview/. [57] P. Tong, E. Brown, P. Wu, S. Woo, A. J. V. IYER, S. C. Akula, S. Yang, J. Yang, M. Middepogu, Z. Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [58] N. Turosz, K. Checinska, M. Checinski, A. Brzozowska, Z. Nowak, and M. Sikora. Applications of artificial intelligence in the analysis of dental panoramic radiographs: an overview of systematic reviews. Dentomaxillofacial Radiology, 52(7):20230284, 2023. [59] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [60] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, S. XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37: 121475121499, 2024. [61] X. Wang, F. Wang, Y. Li, Q. Ma, S. Wang, B. Jiang, C. Li, and J. Tang. Cxpmrg-bench: Pretraining and benchmarking for x-ray medical report generation on chexpert plus dataset. arXiv preprint arXiv:2410.00379, 2024. [62] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [63] H. Wei, L. Kong, J. Chen, L. Zhao, Z. Ge, J. Yang, J. Sun, C. Han, and X. Zhang. Vary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pages 408424. Springer, 2024. [64] T. Wong and J.-L. Eiselé. Fdi world dental federation: Responding to new realities of oral health, 2015. [65] C. Wright and P. Reeves. Radbench: benchmarking image interpretation skills. Radiography, 22(2): e131e136, 2016. [66] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [67] J. Ye, G. Wang, Y. Li, Z. Deng, W. Li, T. Li, H. Duan, Z. Huang, Y. Su, B. Wang, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. Advances in Neural Information Processing Systems, 37:9432794427, 2024. [68] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. [69] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, and F. Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1304013051, 2024. [70] K. Ying, F. Meng, J. Wang, Z. Li, H. Lin, Y. Yang, H. Zhang, W. Zhang, Y. Lin, S. Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. [71] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, G. Wang, H. Li, J. Zhu, J. Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [72] E. Yu, L. Zhao, Y. Wei, J. Yang, D. Wu, L. Kong, H. Wei, T. Wang, Z. Ge, X. Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. In European Conference on Computer Vision, pages 425443. Springer, 2024. 13 [73] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [74] W. Yu, Z. Yang, L. Ren, L. Li, J. Wang, K. Lin, C.-C. Lin, Z. Liu, L. Wang, and X. Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. [75] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [76] P. Zhang, X. Dong, Y. Zang, Y. Cao, R. Qian, L. Chen, Q. Guo, H. Duan, B. Wang, L. Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [77] L. Zhao, E. Yu, Z. Ge, J. Yang, H. Wei, H. Zhou, J. Sun, Y. Peng, R. Dong, C. Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474, 2023. [78] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. 14 Towards Better Dental AI: Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis Supplementary Materials"
        },
        {
            "title": "B MMOral Curation Details",
            "content": "B.1 Image and Visual specialists construction . . . . . . . . . . . . . . . . . . . . . . B.2 Anatomical structure Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Report generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Instruction Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 LLM as the Evaluator for Open-ended Questions: Feasibility Analysis . . . . . . D.3 Evaluation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "H Case Study",
            "content": "16 16 16 17 17 20 20 20 23 25 26 28"
        },
        {
            "title": "A Related works",
            "content": "Over the past few years, the evaluation landscape for large vision-language models (LVLMs) has evolved significantly. Benchmarking plays crucial role in assessing model capabilities, identifying model deficiencies, and guiding future optimization directions. Within the medical domain, existing benchmarks can be classified into two primary categories based on their alignment with the imaging modality coverage: general-purpose benchmarks for broad applicability and specialized benchmarks for discipline-specific evaluation. There have been numerous efforts toward advancing general medical AI, such as LLaVA-Med [35], GMAI-VL [37], MedDr [28], HealthGPT [38], and HuatuoGPT [15]. Alongside these advancements, several general-purpose medical benchmarks spanning diverse imaging modalities and medical domains have been proposed, such as MMMU [75], OminimedVQA [31], MedEval [30], MedBench [42], MMT-Bench [70], and GMAI-Bench [67]. While these medical general-purpose benchmarks enable broader assessments across multiple medical fields, they inevitably fall short in their coverage of imaging modalities and specific medical domains. Therefore, they risk overlooking advancements in specific medical domains not encompassed by the general-purpose benchmarks. Conversely, specialized benchmarks are concentrated on particular imaging modality or medical discipline. For instance, PathVQA [29], PathMMU [52], and PathBench 1 focus on pathology analysis, while SLAKE [39], VQA-RAD [33], and RadBench [65] target radiology understanding. In addition, Eyecare-Bench [36] and LMOD [51] assess the overall performance of LVLMs on intelligent ophthalmic diagnosis tasks. CXPMRG-Bench [61] and M4CXR [49] provide an evaluation suite for chest X-ray interpretation tasks. These specialized benchmarks facilitate in-depth evaluations within their respective disciplines, which are better suited to advancing the development of specific medical fields. Notably, both current medical general-purpose benchmarks and specialized benchmarks overlook the evaluation of MLLMs within oral radiology, particularly in panoramic X-ray understandinga widely used imaging modality that serves as primary diagnostic source for assessing oral health. Consequently, there is an urgent demand for more comprehensive and robust benchmarks to address this gap and advance the development of LVLMs in oral healthcare."
        },
        {
            "title": "B MMOral Curation Details",
            "content": "B.1 Image and Visual specialists construction The method used to identify eligible panoramic X-ray datasets for image curation and visual specialist model construction was adapted from previous studies published in prestigious international peerreviewed journals [67, 31, 51, 42] and globally recognized preprint platforms [36, 30]. Specifically, we collect panoramic X-ray images from two publicly available datasets: the TED3 dataset [27] and the dataset proposed by Hoang Viet Do [19]. The TED3 dataset is large-scale semantic segmentation dataset constructed by aggregating 18 publicly available datasets from various public platforms, including Kaggle, Grand Challenge, and Tianchi. We filter out duplicate images according to the naming rules and ultimately obtain 16,639 unique images. The dataset proposed by Hoang Viet Do [19] is designed for detecting apical periodontitis lesions in panoramic radiographs. This dataset is obtained from the high-quality Dental Treatment Centre, School of Dentistry, Hanoi Medical University, and consists of total of 3,924 images. Therefore, the final curated dataset comprised 20,563 images, and it exhibits significant diversity across various dental conditions, such as dentate and edentulous dentitions, tooth misalignment, impacted teeth, dental caries, root canal treatment, apical lesions, periodontal bone loss, dental implants, and various types of metallic and non-metallic dental restorations. The TED3 dataset [27] and the dataset proposed by Hoang Viet Do [19] are licensed under the Apache License 2.0 and CC BY 4.0, respectively. Both licenses allow for the reproduction and distribution of copies of the original datasets with modifications. Therefore, we utilize these two publicly available datasets as the image sources for MMOral. 1https://smartlab.cse.ust.hk/showcase/PathBench/ 16 Subsequently, we leverage visual specialist models to simulate the interpretative process of oral radiology experts. This process aims to recognize as many anatomical structures as possible from the image, covering attributes from teeth to bone structures, historical treatments, and potential existing diseases. Owing to the inherent complexity of anatomical variations and fine-grained pathological cues observed in panoramic X-rays, we build total of ten specialized visual models that are capable of detecting total of 49 different anatomical structures. These models are fine-tuned on public datasets specifically related to the panoramic radiograph to extract various visual attributes from the images, and object detection and instance segmentation models are selected for training following the annotation protocols of these public datasets [26, 48, 1, 2, 4, 5, 3, 6, 7, 21]. The details of developed visual specialists and the corresponding category list of detected anatomical structures are listed in Table 4. Table 4: The details of visual specialists and the corresponding category list of detected anatomical and pathological structures. Dataset Source Task Type Category Space # Categories # Samples [26, 48, 1] Object Detection 1 to 32 tooth numbering following the FDI tooth numbering system Object Detection 4 Quadrants Object Detection Caries, Deep Caries, Periapical lesions, Impacted tooth Object Detection Caries, Filling Object Detection Caries, Crown, Filling, Implant, Malaligned, Mandibular canal, Missing teeth area, Periapical lesion, Retained root, Root canal treatment, Impacted tooth Object Detection Granuloma, Cyst, Abscess Instance Segmentation Caries, Filling Instance Segmentation Bone loss Instance Segmentation Mandibular canal, Maxillary sinus Instance Segmentation Caries, Crown, Root canal treatment, Badly Decayed, Restoration, Normal Object Detection Optical character recognition (OCR) N/A 32 4 4 2 3 2 1 2 6 634 705 448 9206 3924 7986 327 1899 N/A [48] [48] [2] [4] [19] [5] [3] [6] [7] [21] B.2 Anatomical structure Extraction We construct ten visual specialist models with overlapping category spaces to ensure precise detection of anatomical and pathological structures. For instance, ten structures (e.g., caries, periapical lesion, impacted teeth, missing teeth area, filling, implant, root canal treatment, crown, mandibular canal, and maxillary sinus) are validated by two or more visual specialists. To process the detected redundant visual elements, we meticulously design post-processing pipeline that integrates anatomical structures and establishes associations between dental pathological findings, historical treatments, and their corresponding tooth notations based on their spatial relationships. The designed anatomical structure integration and relationship generation pipeline comprises eight systematic steps, as detailed in Algorithm 1. B.3 Report generation Through extensive consultations with senior dental specialists, we structure the medical report of panoramic x-ray into three principal sections: Teeth-Specific Observations, Jaw-Specific Observations, and Clinical Summary & Recommendations. Each section is further subdivided into some subsections, and the specific content covered within each part is outlined in Table 5. 17 Algorithm 1: Anatomical Structure Integration and Relationship Generation Pipeline. Input: Panoramic X-ray images dataset = {I1, I2, . . . , In}, Expert models {M1, M2, . . . , M10} Output: Visual attributes and relationships = {A1, A2, . . . , An} for all images for each panoramic X-ray image Ii do // Step 1: Detect imaging timestamp Detect imaging timestamp in Ii (if present) and save as ti; // Step 2: Detect teeth locations and tooth notations Detect all teeth positions and their corresponding notations following FDI tooth numbering system Ti = (Pi, Ni); // Step 3: Divide the image into four quadrants Divide the panoramic X-ray image into four quadrants: Qi = {QU R, QU L, QLR, QLL}; // Step 4: Anatomical identification using specialist models Initialize bounding boxes Bi = , Li = , and Si = ; for each Mj {M3, ..., M10} do (Bj, Cj, Sj) Mj(Ii); Bi Bi Bj; Li Li Lj; Si Si Sj; // Step 5: Post-process bounding boxes Bi Filter(Bi, τ ), retaining boxes with sk τ ; Bi CategoryIntegration(Bi, Li); Bi NMS(Bi, Si); // Step 6: Assign tooth-related observations to the specific tooth Ri Assign(Ti, Bi, Li) // Step 7: Insert domain knowledge rules if #18 Ni and #48 / Ni or #28 Ni and #38 / Ni then Oi Comment(Consider extraction of tooth #18/#28); // Step 8: Generate final visual structures and relationships Ai {ti, Ti, Qi, Bi, Li, Si, Ri, Oi}; return = {A1, A2, . . . , An}; Table 5: The detailed construction of the medical report within three sections and their corresponding content."
        },
        {
            "title": "Content",
            "content": "Teeth-Specific Observations Jaw-Specific Observations"
        },
        {
            "title": "Visible Structures",
            "content": "Clinical Summary & Recommendations - treatments, Number of teeth, presence and number of wisdom teeth, and cases of impaction. Presence of cavities (caries, deep caries) and periapical lesions (e.g., granuloma, cyst, abscess). Past dental including fillings, crowns, root canal treatments, and implants. Assessment of periodontal bone loss. Observation of key anatomical features such as mandibular canals and maxillary sinuses. Summary of priority concerns, proposed preventive measures, and recommended follow-up protocols. Thanks to the robust text comprehension and instruction-following capabilities of LLMs, we prompt LLMs to automatically generate medical reports for panoramic X-ray images based on the grounding caption generated by human-designed templates. To ensure precise and structured output, we adopt two-stage LLM-based generation method to generate the medical report. First, the DeepSeek-R1Distill-Llama-70B model is selected for medical report generation due to its exceptional performance Figure 6: The prompt for DeepSeek-R1-Distill-Llama-70B to generate the medical report of panoramic X-ray images. on text understanding, logical reasoning, and instruction-following abilities. The entire generation process requires approximately 48 hours utilizing 4NVIDIA A100 80G GPUs. We meticulously craft system prompt and manually prepare an example for in-context learning to query the LLM. The details of the system prompt and manually prepared example are shown in the Figure 6 and Figure 7, respectively. During the inspection of generated reports, we identify and summarize several common issues. We hypothesize that these issues are inevitable due to the inherent complexity of the task of generating medical reports based on location captions. This specified task presents significant complexity as it requires LLMs to simultaneously perform multiple cognitive sub-tasks, including text comprehension, organization, classification, structuring, summarization, and extrapolation. As result, relying solely on the DeepSeek-R1-Distill-Llama-70B model cannot produce high-quality and entirely accurate 19 Figure 7: The manually designed in-context examples for medical report generation. Given grounding caption of panoramic X-rays (left side), the ideal medical report exemplar is shown on the right side. reports in single step, despite the models impressive natural language processing capabilities. To further enhance the quality of reports, we employ GPT-4-Turbo to refine reports generated by DeepSeek. Based on several common issues in the reports, we carefully design rules for validation and corrections and instruct GPT-4-Turbo to simultaneously output both revised reports and corresponding revision logs. By examining these revision logs, we can efficiently identify modified sections of the reports, thereby facilitating quality verification of the revised content. The details of the designed prompt for GPT-4o-Turbo are provided in Figure 8. B.4 Instruction Data Generation Based on the refined medical report, we prompt GPT-4-turbo to generate the instruction data, including the visual question-answering data (MMOral-VQA) and the image-grounded conversation data (MMOral-Chat). The designed prompts for these two sub-datasets are shown in Figure 9 and Figure 10, respectively."
        },
        {
            "title": "C MMOral Visualization",
            "content": "To provide an intuitive demonstration of the information conveyed in the textual description, we visualize the most frequently occurring words in the MMOral-Report, MMOral-VQA, and MMOralChat through word cloud maps, as demonstrated in Figure 11. Besides, Figures 14 - 17 show six examples in MMOral-Attribute and MMOral-Report sub-datasets."
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Evaluation Metrics In this section, we describe the evaluation process in detail. For closed-ended questions, we adopt the assessment pipeline designed by the CMMMU benchmark. Specifically, we use options as keywords to extract model responses through robust regular expressions, selecting the response with the highest number of options as the final answer. If there is no valid answer in the models response, random selection is performed for multiple-choice questions. We adopt accuracy as the evaluation metric. 20 Figure 8: The prompt for GPT-4-Turbo to revise the generated report. We manually check the generated medical reports from the DeepSeek-R1-Distill-Llama-70B and summarize several rules for validation and correction. We ask the GPT-4-Turbo simultaneously output both revised reports and corresponding revision logs for convenient human verification. For open-ended questions, following MM-Vet [73]and MM-Vetv2 [74], we construct few-shot prompt and leverage GPT-4-turbo to assist with the evaluation. The few-shot prompt is demonstrated in Figure 12. Specifically, our designed prompt includes nine in-context examples with open-ended answers. These examples encompass fully correct responses (i.e., 1.0), entirely incorrect responses (i.e., 0.0), and cases illustrating various types of \"partially correct\" answers. The LLM-based evaluator enables the evaluation of any style of models prediction using unified and consistent metric. By inputting the prompt, GPT-4 automatically generates scores for each sample based on the input question, ground truth, and models prediction. Each sample is assigned score ranging from 0 to 1. 21 Figure 9: The prompt for GPT-4-Turbo to generate both closed-ended and open-ended questionanswering pairs based on the medical report."
        },
        {
            "title": "The total scores are calculated by",
            "content": "S = (cid:80)N i=1 si 100% (1) where si is the score of sample i, and is the number of samples. The score regarding each sub-category can derived by = 100%, (2) (cid:80) si Nc where is the set of samples belonging to specific sub-category (e.g., Teeth, Patho, HisT, Jaw, SumRec, Report), and Nc is the number of samples in this set. The evaluation on MMOral-Bench is conducted using the standard VLMEvalKit framework. We have prepared the automatic evaluation script for MMOral-Bench2, and it will be supported by the VLMEvalKit framework in the future. 2https://github.com/isbrycee/OralGPT 22 Figure 10: The prompt for GPT-4-Turbo to generate multi-turn conversation between the assistant and person asking questions about the panoramic X-ray. D.2 LLM as the Evaluator for Open-ended Questions: Feasibility Analysis Effectiveness. To verify the effectiveness of LLM-based evaluation for open-ended tasks, we invite two professional dentists to objectively score the outputs of different LVLMs. We calculate the absolute difference between the evaluators scores and the human-annotated scores. Specifically, Figure 11: The word cloud maps for MMOral-Report, MMOral-VQA, and MMOral-Chat subdatasets. Figure 12: Few-shot prompt for evaluating model predictions using GPT-4-Turbo, where is the question, is the ground truth and is the models prediction for the question. The prompt demonstrates exemplar scoring criteria for diverse open-ended responses. Taking the prompt filled with Q,G,and P, GPT-4-Turbo will generate soft grading score from 0 to 1. the few-shot prompts designed for LLM-based evaluation are presented to the dentists to determine the evaluation criteria. The two dentists then independently scored the predictions of GPT-4o and HealthGPT-XL32 on 600 cases from the MMOral-Bench open-ended QA task based on these criteria. The absolute differences between human scores and evaluators scores are shown in Table 6, represented as . Overall, the absolute differences of the Overall metric given by dentists are approximately 1 point lower than those of the LLM-based evaluation for the predictions of both LVLMs (GPT-4o and HealthGPT-XL32). This indicates that human scoring preferences align with the trends of LLMbased evaluation. However, it also suggests that the dentists scoring is more stringent compared to the LLM-based evaluation, potentially due to subjective differences in their interpretation of the evaluation criteria provided in the few-shot prompts. 24 For each subcategory, Dentist shows smaller differences in scores compared to the LLM-based evaluation for questions in the Teeth, Patho, HisT, and SumRec categories, whereas the differences are larger for the Jaw and Report categories. Although Dentist exhibits slightly larger differences with LLM-based scoring across all subcategories, their overall score difference is only 0.37 points. This indicates that LLM-based scoring aligns well with human preferences in reflecting the overall performance of LVLMs on MMOral-Bench open-ended tasks. At the same time, we speculate that the score fluctuations in each subcategory are strongly associated with the subjective perceptions of human evaluators. Table 6: Average absolute differences () between the evaluation scores of the LLM-based evaluator and the dentist-annotated scores on the open-ended QA task in MMOral-Bench. Model Evaluators Teeth Patho HisT Jaw SumRec Report Overall GPT-4o [32] Dentist GPT-4-Turbo () Dentist HealthGPT-XL32 [38] GPT-4-Turbo () 30.16 31.48 -1. 26.51 29.80 -3.29 27.65 26.05 +1.6 29.26 22.16 +7.1 40.03 37.56 +2.47 21.66 24.11 -2.45 67.21 57.42 +9. 43.68 47.82 -4.14 33.30 30.37 +2.93 28.20 24.77 +3.43 33.35 42.50 -9.15 14.5 10.00 +4.50 35.43 37.50 -2. 26.80 27.17 -0.37 Stability. Since using LLMs as evaluators inevitably introduces randomness, even with the temperature hyperparameter set to 0, we conduct multiple repeated experiments to verify the stability of LLMs as evaluators. Specifically, we evaluate the prediction results of GPT-4o [32], HealthGPT-XL32 [38], Qwen2.5-VL-7B [12], and Ovis2-34B [45] on open-ended questions using GPT-4-Turbo [10] with the same prompt five times. The obtained mean, standard deviation, and coefficient of variation (CV) of the metric overall are shown in Table 7. For proprietary models, medical-specific models, and general-purpose LVLMs, the standard deviation of the metric \"overall\" is within 0.45 when evaluated 5 times using GPT-4-Turbo with our designed few-shot prompt. Specifically, for the prediction results of Ovis2-34B, the standard deviation of the scores is 0.434, while for Qwen2.5-VL-7B, the standard deviation is as low as 0.096. Meanwhile, CV (Coefficient of Variation), as standardized measure of dispersion of probability distribution, can be used to assess the stability of scores across multiple experiments. The CV values for the prediction results of these four models, after being scored 5 times, are all around 1%, which demonstrates the evaluation stability of using LLMs as evaluators. The detailed results across each specific category are demonstrated in Figure 13. Table 7: Stability verification of using LLMs as evaluators: Standard deviation and coefficient of variation (CV) are reported across four LVLMs from five repeated evaluations. Model GPT-4o [32] HealthGPT-XL32 [38] Qwen2.5-VL-7B [12] Ovis2-34B [45] Mean 37.567 27.284 15. 32.671 StdDev CV (Coefficient of Variation, %) 0.330 0.172 0. 0.434 0.879 0.631 0.607 1.329 D.3 Evaluation results We conduct zero-shot evaluations across 64 LVLMs on our MMOral-Bench, the results are provided in the Table 8. 3https://platform.stepfun.com/ 4https://www.volcengine.com/product/doubao/ 5https://github.com/google-health/medgemma/ 25 Figure 13: The means and standard deviations of each category on 5 repeated evaluations across four LVLMs predictions."
        },
        {
            "title": "E Efficacy Validation of MMOral Instruction Data",
            "content": "We implement supervised fine-tuning (SFT) on two LVLMs of different scales, Qwen2.5-VL-7B [12] and LLaVA-Next-13B-hf [41], using our MMOral instruction data, and the results are presented in Table 9. We use the LLaMA-Factory [78] framework to perform SFT for one epoch while maintaining other default hyperparameters. When fine-tuning Qwen2.5-VL-7B and LLaVA-Next-13B-hf with MMOral-Report, MMOral-VQA, and MMOral-Chat together, the average scores improve by 24.73% and 18.42%, respectively. This clearly demonstrates the effectiveness of the MMOral instruction data and its potential value in advancing digital AI applications in the field of oral healthcare."
        },
        {
            "title": "F Limitations",
            "content": "The ground truth reports generated in this project were based on the ground truth labels provided by previous studies published in esteemed international peer-reviewed journals [48, 19] and globally recognized preprint and dataset platforms [1, 2, 4, 5, 3, 6, 7, 26]. However, the potential inaccuracies in the provided ground truth labels cannot be entirely neglected, as their accuracy has not been validated by independent third-party organizations. Given the considerably large volume of annotated data used to construct the visual specialist model (e.g., 10 datasets comprising 28,375 images), it is not practical for single centre to manually verify the accuracy of these ground truth labels in short period of time. Nevertheless, we have utilized multiple visual specialist models with overlapping category spaces to identify the same anatomical and pathological structures, thereby minimizing the risk of potential inaccuracies in the generated ground truth reports. For instance, ten structures (e.g., caries, periapical lesion, impacted teeth, missing teeth area, filling, implant, root canal treatment, crown, mandibular canal, and maxillary sinus) are validated by two or more visual specialist models, with the final results 26 Table 8: Results on MMOral-Bench for existing various LVLMs across both closed-ended and open-ended VQA tasks. The best-performing model in each category is highlighted in-bold, while the second-best is underlined. Model Proprietary LVLMs GPT-4o-2024-11-20 [32] GPT-4V [32] Claude-3-7-Sonnet-20250219 [11] Gemini-2.5-Flash-preview-04-17 [53] Gemini-2.0-Flash [53] Qwen-Max-VL-2025-04-08 [12] Step-1o-vision-32k 3 Step-1o-turbo 4 doubao-1.5-vision-lite-250315 [] doubao-1.5-vision-pro-250328 [] Doubao-1-5-thinking-vision-pro-250428 5 Open-Source LVLMs Deepseek-VL-7b-chat [43] Emu3-chat [62] Qwen2-VL-7B [59] Qwen2.5-VL-7B [13] Qwen2.5-VL-32B [13] Qwen2.5-VL-72B [13] CogVLM-9B [60] CogVLM2-19B [60] GLM-4V-9B [23] LLaVA-v1.5-7B [40] LLaVA-v1.5-13B [40] LLaVA-NeXT-8B-hf [41] LLaVA-NeXT-13B-hf [41] LLaVA-OneVision [34] LLaMA-3.2-Vision-11B-Instruct [24] Cambrian-8B [57] Cambrian-13B [57] Cambrian-34B [57] Phi-3-Vision-128K-Instruct [8] Phi-3.5-Vision-Instruct [8] Phi-4-multimodal-instruct [9] InternVL2.5-8B [16] InternVL3-8B [16] InternVL3-14B [16] InternVL3-38B [16] Chameleon-7B [44] PaliGemma-3B [14] MiniCPM-V2.6 [66] MiniCPM-O2.6 [66] Kosmos-2 [50] Kimi-VL-A3B-Instruct [55] Ovis1.5-8B [45] Ovis2-8B [45] mPLUG-Owl2-7B [69] mPLUG-Owl3-7B [68] Gemma3-12B [54] XComposer2-VL-7B [76] Molmo-7B-O-0924 [18] Molmo-72B-0924 [18] Yi-VL-6B [71] Yi-VL-34B [71] Qwen-QVQ-72B [56] Ovis2-34B [45] Kimi-VL-A3B-Thinking [55] Medical Specific LVLMs LLaVA-Med [35] LLaVA-NeXT-Med [25] HuatuoGPT-V-7B [15] HuatuoGPT-V-34B [15] HealthGPT-M3 [38] HealthGPT-XL32 [38] MedGemma-4B [54] MedVLM-R1 [47] MedDr [28] Close-ended VQA Open-ended VQA Teeth Patho His Jaw Summ Overall Teeth Patho His Jaw Summ Report Overall 39.65 37.88 41.24 24.60 37.17 18.41 27.43 31.86 21.24 30.27 26.20 22.65 40.89 30.97 24.96 25.66 26.55 26.19 33.63 29.03 20.53 21.24 36.81 30.09 14.51 27.96 27.26 32.57 34.87 30.62 27.08 36.28 32.21 22.12 31.50 28.67 32.57 26.02 28.50 30.27 15.75 44.60 25.31 40.18 23.54 34.16 24.78 25.49 25.31 28.85 28.32 36.81 48.67 45.84 25.84 25.49 19.12 25.31 28.85 45.84 39.65 34.51 28.67 36.46 40.99 39.13 37.27 16.15 35.40 11.18 27.95 24.22 14.29 27.95 27.33 17.39 44.72 27.33 21.12 26.71 27.95 21.12 31.68 35.40 17.39 20.50 33.54 32.92 18.01 21.12 23.60 27.33 34.16 32.92 22.36 36.65 27.95 24.22 29.81 21.12 44.10 24.22 25.47 23.60 18.01 42.24 29.19 47.83 20.50 32.30 19.88 26.71 21.74 14.91 39.75 36.64 49.07 51.55 27. 26.71 22.36 21.74 14.91 44.10 44.10 29.81 31.68 36.02 46.71 48.50 44.31 27.55 44.31 27.55 24.55 38.92 28.14 26.35 23.35 28.74 37.73 26.95 27.54 23.35 26.35 34.73 34.13 41.32 28.14 30.54 41.32 30.54 35.33 40.72 44.31 43.11 44.31 40.72 40.72 49.10 24.55 21.56 26.35 25.75 37.13 42.52 29.94 24.55 28.14 38.32 37.13 44.91 43.11 36.53 31.74 21.56 26.35 29.94 40.11 43.11 59.28 53.89 25.75 21.56 28.74 26.35 29.94 46.71 51.50 43.11 37.72 41.92 55.81 51.69 39.70 20.60 46.82 32.96 25.47 55.81 27.34 29.96 19.85 59.93 60.67 35.58 37.08 22.10 22.47 32.58 38.95 62.55 26.22 31.46 51.31 38.20 42.70 50.94 64.04 70.41 70.04 44.57 38.95 60.30 34.83 35.96 35.58 39.33 29.59 47.57 34.08 36.33 10.11 70.79 70.79 75.66 44.57 71.91 34.08 13.86 25.09 26.59 58.80 41.20 74.53 79.40 29. 13.86 31.09 25.09 26.59 72.66 76.41 59.55 65.17 73.03 56.25 58.33 45.83 10.42 58.33 47.92 29.17 41.67 33.33 50.00 31.25 52.08 43.75 29.17 35.42 14.58 47.92 64.58 60.42 64.58 47.92 39.58 64.58 60.42 31.25 50.00 50.00 39.58 60.42 64.58 56.25 54.17 37.50 31.25 27.08 31.25 52.08 35.42 14.58 14.58 25.00 66.67 41.67 70.83 54.17 62.50 29.17 45.83 12.50 22.92 77.08 70.83 72.92 79.17 27.08 45.83 43.75 12.50 22.92 75.00 79.17 39.58 47.92 64.58 45.40 43.40 41.40 22.00 41.20 22.00 26.40 36.00 21.80 30.20 24.80 31.20 45.8 30.20 27.00 24.60 26.80 29.00 35.20 40.20 23.00 25.20 41.60 33.80 24.40 34.00 37.00 41.20 44.40 37.40 31.60 43.60 30.20 25.40 31.20 28.40 35.80 33.20 28.00 30.27 17.40 49.8 37.40 50.80 31.80 42.80 26.60 23.20 24.40 25.60 40.80 40.20 56.60 56.80 26. 23.20 24.80 24.40 25.60 52.80 52.00 40.00 38.60 46.00 31.48 27.76 36.93 35.99 37.27 10.22 27.93 33.02 32.00 33.45 34.38 12.75 18.02 18.73 17.01 16.97 13.05 25.05 26.11 17.85 12.57 9.71 15.87 14.48 22.68 24.81 25.96 24.29 33.10 20.18 28.84 25.52 27.85 27.89 26.22 33.69 6.02 8.20 27.38 29.20 13.58 28.20 31.19 28.70 11.82 12.50 25.21 6.52 11.49 9.25 17.43 24.97 20.37 32.48 52.53 23.23 2.51 0.00 32.62 30.39 29.80 29.69 22.58 27.50 26.05 13.47 26.65 22.76 26.05 7.30 17.07 21.56 12.87 17.07 25.45 8.16 7.02 9.57 16.10 10.92 18.44 14.96 17.09 8.01 13.19 12.13 5.39 10.28 13.48 20.71 10.78 12.55 21.42 17.80 25.25 20.14 20.50 14.47 17.59 22.41 6.10 9.65 17.38 17.38 10.71 17.94 26.52 26.17 16.81 8.44 20.00 11.01 6.74 6.31 13.62 23.40 12.28 24.33 37. 18.75 1.06 0.00 18.65 18.86 22.16 11.26 12.28 28.14 37.56 33.50 42.39 40.61 40.36 11.12 25.64 30.20 29.95 36.55 39.09 8.40 15.50 13.96 11.18 11.60 11.66 26.04 26.86 17.46 10.18 15.62 9.17 9.23 17.75 25.03 22.37 20.71 31.83 16.57 24.08 27.69 26.75 25.09 27.16 29.70 9.35 8.70 24.38 24.38 11.18 32.49 30.41 26.63 9.94 8.52 20.65 15.00 8.52 3.49 21.12 20.59 16.75 31.60 53.79 11.36 0.24 0.30 28.05 26.90 24.11 28.12 21.57 30.20 57.42 58.95 51.09 51.53 52.40 22.88 45.85 51.31 60.92 59.39 56.11 30.00 28.53 40.71 29.41 23.82 26.88 42.41 49.24 24.12 18.88 17.29 27.67 22.41 38.35 33.65 39.41 41.18 48.24 46.35 42.94 43.29 40.06 37.29 44.06 46.11 9.71 13.29 49.76 49.76 19.76 53.53 48.29 53.06 24.76 30.59 26.88 7.67 10.94 12.65 32.24 39.35 41.05 50.88 68. 32.82 0.94 0.29 53.12 46.07 47.82 41.88 40.61 49.17 30.37 30.84 28.04 32.71 35.05 6.86 31.31 31.31 28.97 30.37 39.72 13.14 12.44 4.65 9.07 9.53 7.44 13.95 18.14 15.93 17.09 13.60 14.42 14.30 18.72 17.56 20.58 17.21 13.60 20.35 24.42 13.84 22.33 18.14 22.91 20.23 5.35 6.16 15.93 15.93 8.49 19.95 19.06 20.81 18.72 3.26 22.33 2.10 3.95 5.00 16.74 15.23 22.90 21.05 50.93 26.28 2.21 0.00 18.60 21.03 24.77 22.62 21.96 26.17 42.50 45.00 50.00 45.50 49.00 27.00 26.00 45.00 48.00 47.50 49.00 9.10 9.60 15.10 8.20 13.50 11.50 14.50 24.50 19.40 11.50 11.40 24.50 21.30 11.20 19.50 19.50 14.70 16.00 8.60 9.50 12.80 25.00 23.80 36.40 42.90 8.40 0.60 27.90 27.90 3.40 25.60 19.80 30.72 14.50 13.90 33.20 8.53 6.00 9.20 8.60 9.90 24.5 31.70 61. 5.30 0 0.70 15.40 19.50 10.00 39.40 24.50 7.50 37.50 34.83 40.67 39.08 40.67 14.33 29.08 36.00 36.42 37.67 40.33 13.42 16.05 18.70 15.92 15.40 14.77 24.03 27.63 17.50 13.22 12.23 16.68 15.43 20.93 23.97 24.28 22.67 29.63 20.93 25.93 24.57 27.50 25.82 29.27 34.15 7.27 7.78 28.42 28.42 11.87 30.00 30.20 30.70 14.80 13.67 25.32 8.99 9.02 8.23 18.00 22.98 22.75 33.02 54.55 19.60 1.38 0.20 29.48 28.92 27.17 30.45 24.58 26.17 Avg. 41.45 39.12 41.04 30.54 40.94 18.15 27.74 36.00 29.11 33.94 32. 22.31 30.93 22.65 21.46 20.00 20.79 26.52 31.42 28.85 18.11 18.72 29.14 24.62 22.67 28.99 30.64 31.94 37.02 29.17 28.77 34.09 28.85 25.61 30.24 31.28 21.54 20.49 28.21 28.21 14.64 39.90 33.80 40.75 23.30 28.24 25.96 16.10 16.71 16.92 29.40 31.59 39.68 44.91 40.68 21.40 13.09 12.21 27.54 40.86 39.59 35.23 31.59 36.09 Table 9: The effectiveness verification of MMOral instruction data by supervised fine-tuning. Model Qwen2.5-VL-7B [13] Qwen2.5-VL-7B [13] Qwen2.5-VL-7B [13] Qwen2.5-VL-7B [13] Qwen2.5-VL-7B [13] LLaVA-NeXT-13B-hf [41] LLaVA-NeXT-13B-hf [41] LLaVA-NeXT-13B-hf [41] LLaVA-NeXT-13B-hf [41] LLaVA-NeXT-13B-hf [41] SFT Close-ended VQA Open-ended VQA Report VQA Chat Teeth Patho HisT Jaw SumRec Overall Teeth Patho HisT Jaw SumRec Report Overall 24.96 21. 27.54 37.08 26.90 39.12 43.19 37.17 27.33 36.65 40.99 30.43 26.35 37.73 43.11 38.32 45.32 62.92 63.60 52. 30.09 32.92 30.54 38.20 27.79 39.12 43.19 59.15 25.47 40.37 48.45 57. 31.74 31.14 38.92 37.13 57.68 48.69 45.32 32.21 35.42 37.50 43.75 37.50 45.83 60.42 62.50 29.17 35.42 35. 27.00 31.00 43.60 46.20 39.60 33.80 35.00 39.80 43.20 48.20 17.01 16. 11.18 29.41 27.82 36.22 39.85 55.45 15.82 31.92 32.41 33.40 25.92 32.49 35.20 45.74 14. 10.28 9.23 17.23 41.56 40.33 42.11 10.21 26.95 26.38 23.48 11.18 32.78 36.57 41.01 63.76 78.47 78.06 74. 22.41 25.41 81.94 76.76 88.47 9.07 22.33 40.93 36.98 45.17 14.30 16.51 37.21 36.98 33. 8.20 38.00 4.30 36.80 50.50 21.30 19.8 6.20 29.30 33.60 15.92 32.62 35.73 42.85 52. 15.43 17.07 37.98 41.10 44.18 Avg. 21.46 31.81 39.67 44.53 46.19 24. 26.03 38.89 42.15 46.19 obtained through post-processing. In addition, we have adopted two-stage LLM-based scheme of generation followed by correction to ensure the report quality. First, we utilize the DeepSeek-R1Distill-Llama-70B to generate preliminary reports. Subsequently, through manual review of these 27 preliminary reports, we identify common errors and summarize them into eight key rules (see in Figure 8) for prompting GPT-4-turbo to revise reports. An analysis of the revision logs shows that 95.45% of the reports are successfully corrected, leading to significant improvement in their overall quality. Future efforts should focus on third-party validation of ground truth accuracy in these public datasets to further ensure their reliability."
        },
        {
            "title": "G Experiments Compute Resources",
            "content": "The experimental section of this paper, involving the construction of the MMOral dataset and the evaluation of MMOral-Bench, requires the use of proprietary LLMs API. The total cost of the experiments is approximately 1000 USD, with around 600 USD spent on building the MMOral dataset and about 400 USD on evaluating existing LVLMs. Furthermore, the SFT experiments conducted in this paper are performed on 4NVIDIA A100 80G GPUs. Figure 14: An example of MMOral-Attribute and MMOral-Report. 28 Figure 15: An example of MMOral-Attribute and MMOral-Report."
        },
        {
            "title": "H Case Study",
            "content": "In this section, we provide additional examples of the performance of various models on closed-ended and open-ended QA tasks. Figures 18 - 23 show examples of closed-ended QA, while Figures 24 - 26 show examples of open-ended QA. 29 Figure 16: An example of MMOral-Attribute and MMOral-Report. 30 Figure 17: An example of MMOral-Attribute and MMOral-Report. Figure 18: closed-ended QA example. Red highlights the right answer. 32 Figure 19: closed-ended QA example. Red highlights the right answer. Blue highlights the wrong answer. 33 Figure 20: closed-ended QA example. Red highlights the right answer. Figure 21: closed-ended QA example. Red highlights the right answer. Blue highlights the wrong answer. 35 Figure 22: closed-ended QA example. Red highlights the right answer. 36 Figure 23: closed-ended QA example. Red highlights the right answer. Blue highlights the wrong answer. Figure 24: An open-ended QA example. Red highlights the right description. 38 Figure 25: An open-ended QA example. Red highlights the right description. 39 Figure 26: An open-ended QA example. Red highlights the right description."
        }
    ],
    "affiliations": [
        "CVTE",
        "Department of Diagnostic Radiology, The University of Hong Kong",
        "Faculty of Dentistry, The University of Hong Kong",
        "Imaging and Interventional Radiology, Faculty of Medicine, The Chinese University of Hong Kong",
        "National University of Singapore",
        "School of Computer Science, Peking University",
        "Sun Yat-sen University",
        "The Hong Kong University of Science and Technology (GZ)"
    ]
}