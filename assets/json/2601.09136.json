{
    "paper_title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "authors": [
        "Lijun Liu",
        "Linwei Chen",
        "Zhishou Zhang",
        "Meng Tian",
        "Hengfu Cui",
        "Ruiyang Li",
        "Zhaocheng Liu",
        "Qiang Ju",
        "Qianxi Li",
        "Hong-Yu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 6 3 1 9 0 . 1 0 6 2 : r SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL Lijun Liu*,1, Linwei Chen*,1,6,7, Zhishou Zhang1, Meng Tian1, Hengfu Cui1, Ruiyang Li1, Zhaocheng Liu1, Qiang Ju1, Qianxi Li,2,3,4,5, Hong-Yu Zhou,6 1Baichuan Inc. 2Department of Dermatology, Peking University First Hospital 3Beijing Key Laboratory of Molecular Diagnosis on Dermatoses 4National Clinical Research Center for Skin and Sexually Transmitted Diseases 5NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics 6School of Biomedical Engineering, Tsinghua University 7University of Hong Kong Correspondence: hongyu.zhou.ai@gmail.com; chancylee7@126.com"
        },
        {
            "title": "Abstract",
            "content": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\"the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within constrained semantic space. Furthermore, we propose clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes new state-of-the-art on the Fitzpatrick17k benchmark, achieving +12.06% gain in Top-1 accuracy and +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling."
        },
        {
            "title": "Introduction",
            "content": "Dermatological diagnosis is visually intensive medical field where diagnostic accuracy relies heavily on the precise identification and interpretation of fine-grained pathological features (Zhang et al., 2023; Badr et al., 2025). With the recent advancement of Large Vision-Language Models (LVLMs), there has been significant interest in developing automated systems to assist in skin disease identification(Moor et al., 2023; Zhou et al., 2023; Tu et al., 2024). However, despite the success of general-purpose models, their application to dermatology remains hindered by two primary challenges. First, general LVLMs often suffer from \"diffuse attention,\" where the model fails to distinguish between critical lesions and irrelevant background noise, leading to suboptimal information transmission. Second, conventional evaluation metrics, such as Top-1 accuracy or exact-match rates, adopt binary notion of correctness that is fundamentally misaligned with clinical reality. In practice, diagnosis that captures the correct pathological lineage is far more valuable than semantically distant misclassification, yet standard metrics treat them as equally incorrect. In this paper, we address these challenges by reframing dermatological diagnosis as an information transmission optimization problem. We conceptualize the model as an image compressiondecoding system where performance is strictly bounded by the efficiency of information flow: the encoder must compress raw pixels into high-capacity manifold, while the decoder must reconstruct this data within constrained diagnostic semantic space. We posit that the failure of existing models stems not from lack of reasoning power, but from an inability to maximize the \"recoverable information\" regarding subtle, non-describable pathological cues. To break the geometric limitations of standard vision backbones, we design the Dynamic Visual Encoding (DVE) module (Chen et al., 2025). As evidenced by our attention attribution analysis, DVE allows the model to \"unfold\" complex visual manifolds, adaptively suppressing background redundancy and amplifying the signal-to-noise ratio of diagnostic lesions. This architectural innovation enables transition from uncertain global scanning to high-confidence focal reasoning. To further maximize this transmission efficiency, we propose two-stage reinforcement learning framework that systematically decouples visual evidence into explicit and implicit streams: Stage (Semantic Alignment via Compression): We introduce medical captioning task that forces the model to compress visual information into linguistically interpretable features (describable components), ensuring the retention of explicit clinical signs. Stage II (Diagnostic Refinement via Decoding): Building on this aligned representation, the model is optimized to reconstruct implicit, non-describable pathological textures within diagnosis-specific output space, effectively bridging the gap between visual perception and clinical deduction. Furthermore, we challenge the utility of standard metrics in high-stakes medical settings. Moving beyond rigid label matching, we establish Clinically Grounded Evaluation Protocol inspired by the hierarchical taxonomy of skin diseases (Yan et al., 2025a). This framework prioritizes diagnostic safety and clinical actionability, rewarding therapeutically consistent \"near-misses\" while strictly penalizing errors that cross critical boundaries (e.g., malignancy). Empirical results validate our hypothesis that geometric efficiency trumps raw scale. Despite utilizing only 7 billion parameters, our model substantially outperforms general-purpose giants exceeding 200B parameters. On the rigorous Fitzpatrick17k benchmark, we establish new state-of-the-art, achieving +12.06% gain in Top-1 accuracy and massive +28.57% boost in Top-6 accuracy over the strongest open-source baseline (Qwen3VL235B), while also surpassing GPT-5.2. These findings suggest that optimizing the compression and decoding of visual information is more effective mechanism for medical precision than parameter scaling. On the internal dataset, our model consistently provides more reliable diagnostic candidate pool, outperforming all competitors in Top-2 through Top-6 metrics (e.g., reaching 79.21% Top-6 accuracy). These findings suggest that enhancing both the compression and decoding efficiency of visual information serves as fundamental mechanism for improving diagnostic performance and clinical reliability under limited decoding capacity. Moreover, Our framework naturally supports open-vocabulary dermatological diagnosis, moving beyond fixed-label classification to handle the diverse and long-tailed distribution of real-world skin conditions. Our main contributions are summarized as follows: We propose theoretical framework that treats dermatological VLM training as an optimization of information transmission efficiency, identifying the \"recoverable information\" bottleneck. We introduce the Dynamic Visual Encoding (DVE) module, which significantly enhances the visual signal-to-noise ratio by adaptively \"unfolding\" pathological manifolds. We implement two-stage RL training strategy that sequentially masters explicit medical descriptions and implicit diagnostic textures, ensuring robust feature alignment. We establish clinical-centric evaluation framework that accounts for disease hierarchies and diagnostic safety, providing rigorous standard for open-world medical AI."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Traditional Skin Disease Diagnosis Based on Dermoscopic Data Early studies on automated skin disease diagnosis mainly relied on dermoscopic images, which are captured by professional devices(Pham et al., 2021; Shahin et al., 2018; Salamaa and Aly, 2021; Ashraf et al., 2022). Pham et al (Pham et al., 2021) employed deep convolutional neural networks (CNNs) to achieve high-accuracy melanoma recognition from dermoscopic images. Shahin et al(Shahin et al., 2018) proposed an ensemble model by integrating ResNet-50 and Inception V3, which demonstrated strong performance in distinguishing malignant from benign lesions. Other works combined VGG16 with support vector machines (SVMs) for binary lesion classification(Salamaa and Aly, 2021). However, these methods are constrained by their reliance on specialized imaging equipment and the limited availability of annotated datasets, making them difficult to deploy in large-scale health monitoring or telemedicine applications. 2.2 Single-Modality Visual Recognition Methods Traditional deep learning approaches have primarily focused on single modality visual tasks such as lesion classification and segmentation(Ashraf et al., 2022; Srinivasan et al., 2025). For instance, U-Net and its variants (e.g., ResUNet++) have been widely used for automated lesion segmentation to assist clinicians in delineating lesion boundaries(Ashraf et al., 2022; Jojoa Acosta et al., 2021; Araújo et al., 2022). Capsule networks (CapsNet) have also been explored to enhance spatial hierarchical representation, enabling multi-class skin disease recognition and fine-grained classification(Srinivasan et al., 2025; Nawaz et al., 2025; Eskandari and Sharbatdar, 2024). Despite their success, these models typically output static classification results without natural language explanations or interactive reasoning. The lack of interpretability and user interaction makes such black-box systems less suitable for clinical practice, where transparent decision-making and physicianpatient communication are essential. 2.3 Multimodal Models in Medical Diagnosis With the rapid development of large-scale multimodal models, neural systems are now capable of jointly understanding visual and textual inputs, producing analysis results in natural language form(Guo et al., 2024; El Mir et al., 2024; Zhou et al., 2023; Yan et al., 2025b). LLaVA-Ultra(Guo et al., 2024) introduced fine-grained visuallanguage fusion for Chinese ultrasound image question answering, improving medical semantic comprehension. TinyLLaVAMed(El Mir et al., 2024) achieved efficient inference in low-resource medical settings through lightweight fine-tuning. SkinGPT-4(Zhou et al., 2023), one of the few multimodal models specifically designed for dermatology, adopted two-stage training process to enable both lesion description generation and diagnosis assistance. Although these methods reveal the promise of multimodal medical diagnosis, comprehensive and systematic research in this domain remains limited. 2.4 Emergence of Next-Generation Multimodal Foundation Models Recent multimodal foundation models, such as Qwen2.5-VL(Bai et al., 2025), Qwen3-VL(Team, 2025), and InternVL-3(Zhu et al., 2025), have demonstrated outstanding performance in vision language understanding and visual question answering tasks. Domain-specific medical multimodal models, such as Lingshu-32B(Xu et al., 2025), have also shown remarkable progress in medical image interpretation. Nevertheless, how to effectively integrate general-purpose multimodal foundation models into medical applicationsand optimize them for domain-specific semantics and diagnostic reasoningremains an open and underexplored problem. 2.5 Reinforcement Learning and Generalization While supervised fine-tuning (SFT) remains the mainstream post-training paradigm, its generalization ability is inherently limited. Recent studies suggest that SFT can be viewed as an implicit form of policy gradient with hidden reward bias, which may lead to overfitting on narrow data distributions(Wu et al., 2025). In contrast, reinforcement learning (RL) explicitly optimizes policies based on reward signals, enabling adaptive improvement in complex task spaces and yielding stronger robustness and generalization(Chu et al., 2025). However, RL in high-dimensional medical domains still faces challenges in exploration efficiency and training stability(Dulac-Arnold et al., 2019; Al-Hamadani et al., 2024). To address these issues, our work introduces two-stage reinforcement learning framework that integrates multimodal description learning with reward-driven diagnostic policy optimization, achieving improved accuracy while maintaining stability and scalability."
        },
        {
            "title": "3 Method",
            "content": "This section details the proposed two-stage training pipeline for dermatological diagnosis (Figure 1). We begin by formalizing our framework from an information transmission perspective, providing the theoretical rationale that unifies the staged design. Building upon this foundation, we introduce the Dynamic Vision Encoder (DVE) module, which addresses the visual representation bottlenecks in current multimodal LLMs and the RL algorithm used for optimization. Finally, we provide detailed implementation of Stage (caption learning) and Stage II (diagnosis learning), elaborating on the construction of dermatological datasets and the design of our clinically-grounded reward functions. 3.1 Overall Framework: An Information Transmission Perspective To optimize the diagnostic performance of our model, we formalize the training process as an image compressiondecoding task. As illustrated in our framework, the models efficacy is determined by the information transmission efficiency from the raw pixels to the final semantic diagnosis. We define the total visual information as comprising two distinct components: describable features Id (explicit medical signs) and non-describable features In (implicit pathological textures). The core rationale for our two-stage strategy is to maximize the recoverable information within constrained decoding space. Stage (Information Compression): By introducing the medical captioning task, we force the encoder to prioritize the compression of Id into linguistically interpretable representations. This stage establishes highcapacity channel for key diagnostic features. Stage II (Semantic Decoding): Upon the foundation of high-quality representations, the model is then fine-tuned to integrate In and decode the joint information into diagnosis-specific semantics. By optimizing these processes in tandem, the models information transmission approaches the theoretical upper bound of the diagnostic space, ensuring that the final output is grounded in both explicit clinical evidence and implicit visual cues. Figure 1: Two-stage reinforcement learning framework for dermatological diagnosis. In Stage 1, the model performs medical caption generation. The LLM scores each attribute field of the generated description, and these field-wise scores are integrated into caption reward to refine medical feature learning. In Stage 2, the model predicts disease categories based on learned representations. The LLM evaluates each prediction, and customized reward function converts these evaluations into final diagnostic reward that optimizes classification accuracy and ranking consistency. 3.2 Virtual-Width Dynamic Vision Encoder While existing Multimodal LLMs (MLLMs) achieve great succuss, stark asymmetry exists in current MLLMs: while the LLM backbones have scaled to billions of parameters to support advanced cognitive reasoning, the vision encoders remain disproportionately lightweight (e.g., vision encoder 0.6B vs. 7B LLM in Qwen2.5-VL). This imposes fundamental representation bottleneck: the model possesses powerful \"brain\" for semantic processing but is limited by \"retina\" with insufficient geometric capacity, restricting its ability to discern subtle pathological features essential for diagnosis. To address this asymmetry without incurring the massive computational cost of scaling the vision encoder, we introduce FDLinear (Frequency Dynamic Linear) (Chen et al., 2025), parameter-efficient dynamic operator. By replacing static linear layers with FDLinear in MLPs, we exponentially expand the effective geometric capacity of the vision encoder. 3.2.1 Theoretical Motivation: Escaping the Capacity Curse of Covers Theorem Standard Vision Transformers rely on static linear layers for feature mixing. Formally, given an input feature Rd, static layer computes = + b, where Rdd is fixed after training. According to Covers Theorem (Cover, 2006) on the geometrical separation of patterns, the probability (N, d) that random patterns are linearly separable in d-dimensional space is: (N, d) (cid:40) 1 0 if 2d if 2d (1) In complex dermatological diagnosis, the number of visual patterns (texture, erythema, scale, ulcers) can be very Figure 2: Illustration of frequency disjoint basis construction. The process transforms the weight matrix Wori Rdd from the spatial domain to the Fourier domain via the Discrete Fourier Transform (DFT). The frequency spectrum is then partitioned into disjoint groups based on frequency index (e.g., P1 corresponds to the central low-frequency components in blue, while 2 corresponds to the peripheral high-frequency components in orange). To generate specific spatial basis B1, we retain only the learnable parameters belonging to Group 1 (P1) and mask all other frequency indices to zero. Finally, an inverse DFT (iDFT) reconstructs the spatial basis matrix. This design ensures that each basis Bk specializes in distinct frequency band, minimizing spectral redundancy. large ( ), while the physical dimension of the vision encoder is fixed (e.g., = 1280). Consequently, static encoder suffers from Capacity Collapse, forcing the model to average out fine-grained details to satisfy the global optimization objective."
        },
        {
            "title": "3.2.2 FDLinear: Implicit High-Dimensional Mapping\nA straightforward approach to satisfy the geometric separability condition (N ≤ 2d) would be to physically expand\nthe channel dimension d. However, this strategy is computationally intractable. Increasing channel width leads to a\nquadratic growth in parameters (O(d2)). Furthermore, due to the inherent 2D spatial nature of images, increasing\nresolution to capture fine pathological details results in a quadratic explosion in the number of visual tokens.\nCoupling a massive physical dimension with this surge in token count would cause computational complexity\n(FLOPs) to skyrocket, rendering the model undeployable in real-world clinical settings.",
            "content": "We propose paradigm shift from physical dimension expansion to virtual dimension expansion. FDLinear circumvents the computational wall by decoupling the weight space into orthogonal spectral bases {B1, . . . , BK}. As illustrated in Figure 2, these bases are not randomly initialized but are constructed via Frequency Disjoint Partitioning. We partition the full frequency spectrum of the weight matrix into disjoint groups (e.g., concentric frequency bands). Each basis Bk is generated by retaining only the parameters in the k-th frequency group and applying an inverse Discrete Fourier Transform (iDFT). The dynamic weight (x) is constructed via context-aware linear combination: (cid:88) (x) = αk(x) Bk (2) k= Crucially, to maintain extreme parameter efficiency, the dynamic coefficient αk(x) is not dense matrix but is factorized into three orthogonal modulation vectors (corresponding to the input dimension, output dimension, and basis dimension K), all of which are predicted by single, highly lightweight fully-connected (FC) bottleneck layer. is image-level global average result. The Virtual Dimension Expansion Mechanism: Mathematically, the operation = (x)x can be viewed as two-step \"Expand-and-Collapse\" process. If we were to explicitly compute the projection of onto all bases, we would generate massive hidden representation RKd: = Concat(B1x, B2x, . . . , BKx) (3) Here, the channel dimension virtually explodes from to (e.g., 1280 81, 920 for = 64). In this hyperspace, the complex visual manifolds are \"unfolded\" and become linearly separable according to Covers Theorem (Cover, 2006). Computational Efficiency Implementation: Crucially, we never explicitly materialize this massive vector H. By exploiting the linearity of matrix operations, we fuse the aggregation step (the weighted sum) before the matrix multiplication: (cid:32) (cid:88) = (cid:33) αk(x) Bk k=1 (cid:124) (cid:125) (cid:123)(cid:122) Pre-computed (x)Rdd (4) Figure 3: Visualizing manifold unfolding and virtual capacity. We evaluate the geometric representation capability on four classic non-linearly separable datasets: Spirals, XOR, Circles, and Moons. (Left column of each sub-figure) Constrained by Covers Theorem, standard static layer (width = 2) is topologically restricted to single separating hyperplane, resulting in severe underfitting (accuracy 50%). (Middle column of each sub-figure) Without increasing the physical width (d = 2), FDLinear (K = 12) dynamically constructs high-order decision boundaries, successfully disentangling complex manifolds (e.g., the intertwined spirals). This empirically validates the \"Virtual Width Expansion\" hypothesis. (Right column of each sub-figure) The vector fields visualize the orientation of the generated weight matrix (x) across the input space. The rotating and radiating patterns (labeled \"Field Adaptation\") demonstrate that FDLinear acts as sample adaptive layer, modulating its projection direction based on local geometric curvature. Instead of performing calculations in the exploding dimension (which would require O(K d2) FLOPs per token), we essentially \"compress\" the weight basis into single dynamic matrix (x) of size d. This allows the model to enjoy the geometric capacity of massive width (K d) while maintaining the computational footprint of compact layer (d). 3.2.3 Visual Verification and Efficiency Analysis To empirically validate the \"virtual width expansion\" hypothesis, we visualized the decision boundaries of FDLinear on classic non-linearly separable manifolds (Figure 3). As predicted by Covers Theorem, the static linear baseline suffers from capacity collapse, failing to separate complex topologies like intertwined spirals or concentric circles. In contrast, FDLinear successfully \"unfolds\" these manifolds. Crucially, the dynamic weight directions (Figure 3, right column of each sub-figure) reveal non-uniform, rotating vector field. This confirms that the model is not merely memorizing data, but is actively modulating its \"gaze\" (projection direction) according to the local geometry of the input features. This geometric adaptability translates into paradigm shift in efficient scaling for medical AI: Geometric Capacity beyond Physical Limits: As evidenced by the \"spirals\" experiment, FDLinear enables compact layer to model decision boundaries that typically require vast parameter expansion. This effectively bridges the capacity gap between the 0.6B vision perceiver and the 7B language reasoner. Parameter-Efficient Storage: Despite the virtual width expansion, we store only the bases and lightweight coefficient predictor. The parameter overhead is negligible (< 5%) compared to the dense matrices required for physical scaling. Inference Speed: Since the dynamic composition (x) collapses into standard linear transformation at the operator level, the computational complexity remains comparable to standard linear layer, avoiding the high latency and memory fragmentation associated with MoE (Mixture of Experts) architectures. By \"virtualizing\" the width of the vision encoder, SkinFlow achieves high-fidelity diagnostic reasoning at fraction of the cost of training multi-billion parameter vision backbone. 3.3 Preliminary: RL algorithm In contrast to approaches that explicitly replicate intermediate reasoning steps, RLVR (Guo et al., 2025; Team et al., 2025) relies solely on outcome-driven feedback, facilitating scalable reinforcement learning across extensive task datasets. Group Relative Policy Optimization (GRPO) (Guo et al., 2025) is an efficient RL algorithm that eliminates the need for separate critic model. Given query q,We adopt Group Relative Policy Optimization (GRPO) (Guo et al., 2025) as our RL backbone. GRPO is sample-efficient variant of policy optimization that eliminates the need for separate critic by operating on groups of outputs. Given query q, GRPO samples group of candidate outputs o1, . . . , oG from the current policy πθold and evaluates them using task-specific reward function to yield rewards r1, . . . , rG. Advantages are computed by normalizing within the sampled group: Ai = ri mean rj = 1G) std(rj = 1G) . (5) The GRPO objective optimizes clipped importance-weighted surrogate with KL penalty to reference policy: GRPO(θ) = Eq D, oi = 1G πθold"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) (cid:16) t= min (cid:0)φi,t(θ)Ai,t, clip(φi,t(θ), 1 ϵ, 1 + ϵ)Ai,t (cid:1) β, DKL(πθπref ) (cid:17) , where φi,t(θ) = πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) tion of reward signals from diverse candidate outputs. . GRPOs group normalization stabilizes training and enables efficient utiliza3.4 Stage I: Learning Dermatological Image Descriptions The primary goal of this stage is to enable the model to produce accurate and clinically coherent descriptions for dermatological images. Compared with supervised fine-tuning (SFT), reinforcement learning mitigates the problem of entropy collapse and achieves better generalization, especially under limited annotation conditions. 3.4.1 Construction of Dermatological Description Samples We collected approximately 5,000 dermatological images from both domestic and international sources, each annotated with disease category labels. To obtain high-quality description samples (captions), we employed hybrid strategy combining large language model (LLM)-based generation and expert refinement. Among them, 4,000 captions were automatically annotated through the machine labeling process described below. Machine annotation process: 1. Use multimodal large language model (MLLM) to generate an initial caption. To facilitate the subsequent quantitative evaluation of caption quality, we guided the model to produce structured medical captions. The process for determining the structured fields is as follows: Generation of unstructured captions: The large language model (LLM) first generates rich, descriptive free-text captions of skin lesion appearances. Structuring of captions: The unstructured captions are then reformatted into structured form through LLM-based processing. Field frequency analysis: The frequency of each field is calculated and ranked. Manual refinement: Based on the ranked results, experts manually determine the final set of structured fields and corresponding annotation guidelines. 2. Input the generated caption into an LLM to infer the corresponding diagnosis. 3. Compare the inferred diagnosis with the ground-truth disease label. If consistent, the caption is accepted. If inconsistent, regenerate the caption and repeat Steps (2)(3) up to five times. 4. If consistent caption is not obtained after five iterations, the sample is passed to human experts for revision. Human annotation process: 1. Use an MLLM to generate an initial caption. 2. Medical experts manually revise the caption based on professional knowledge. 3. The revised caption is re-evaluated by an LLM for diagnostic consistency. 4. If the LLM-determined diagnostic accuracy meets the quality threshold, the caption is accepted; otherwise, the revision process is repeated. 3.4.2 Reward Function Design To ensure that the generated captions are both clinically valid and semantically complete, we designed multi-dimensional reward mechanism.For attributes involving continuous degrees of variation, such as lesion color and size, we formulated instruction-based rules to measure the correlation between the predicted and ground-truth descriptions. For attributes with well-defined medical categories, such as lesion type, we constrained the models outputs through instructional rules that require selection within predefined set of medically valid options. For each predefined attribute, the LLM assigns score ranging from 0 to 10, with scores 6 considered acceptable. The overall reward is computed as weighted average of individual attribute scores: (cid:88) = αi si (6) where ( si ) denotes the attribute score and ( αi ) represents its corresponding weight. To validate the reliability of the reward design, we calculated the correlation between the generated caption reward and the diagnostic accuracy obtained when using the caption as input for inference. strong positive correlation indicates that the designed reward provides an effective optimization signal. Table 1 presents the average rewards and diagnostic accuracies under the optimal scoring configuration. Determination of attribute weights The weight of each attribute (( αi )) is determined according to its frequency of use in the LLMs diagnostic reasoning process. Attributes that are more frequently referenced as diagnostic evidence are assigned higher weights accordingly. Table 1: Correlation between caption reward and diagnostic accuracy across different multimodal models. Model Name Caption Reward Diagnostic Accuracy Qwen2.5-VL-7B Qwen2.5-VL-72B Lingshu-32B InternVL3-78B Qwen3-VL-Instruct-235B-A22B 6.162 6.688 5.924 6.912 7.186 13.79% 23.00% 24.14% 26.44% 27.59% 3.5 Stage II: Dermatological Diagnosis Training In the second stage, the model aims to predict the top-K most probable diagnoses for given dermatological image. Training is again conducted under an RL framework rather than SFT, for two key reasons: 1. Terminological diversity: single disease often has multiple equivalent medical names.The rigid token-level matching of SFT is thus unsuitable for learning semantically equivalent expressions. 2. Efficiency in top-K learning: SFT requires explicit construction of all possible top-K labels, whereas RL flexibly assigns rewards without enumerating the entire label space. Reward Function Design The model outputs ranked list of top-K candidate diagnoses. Let the position weights be ( = [w1, w2, ..., wK]), and let the correctness list be ( = [l1, l2, ..., lK] ), where ( li 0, 1 ) is determined by an LLM-based verification process. If the correct diagnosis first appears at position ( ), the final reward is defined as the corresponding positional weight ( wi ): = min{k 1 K, lk = 1} = wi (7) (8) This reward design ensures that the model is rewarded for correctness while also being encouraged to rank the correct diagnosis as high as possible in its prediction list."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup 4.1.1 Implementation Details Model Architecture: We initialized our model based on Qwen2.5-VL-Instruct-7B (Bai et al., 2025). To construct the Virtual-Width Dynamic Vision Encoder, we replaced the static linear layers within the MLPs of the original Vision Transformer with our proposed FDLinear operators at layers 8, 16, 24 and 32. We set the number of spectral bases d/2, where is the input dimension. effectively expanding the virtual geometric width by large factor. Despite this substantial capacity boost, the additional parameter overhead for storing bases and coefficient predictors is less than 5% of the original vision encoder size. Training Protocol: The first-stage training was conducted based on the Qwen2.5-VL-Instruct-7B model(Bai et al., 2025). The AdamW optimizer was employed with learning rate of (1 106), and cosine warmup strategy was used for scheduling. The second-stage training continued from the first-stage checkpoint under reinforcement learning (RL) framework, with learning rate of (5 107). The entire RL pipeline was implemented using the VERL framework (Sheng et al., 2024). 4.1.2 Test Dataset To evaluate dermatological diagnostic performance under realistic clinical settings, we construct two evaluation benchmarks that do not assume closed-set disease taxonomy. Together, the two benchmarks cover approximately 200 distinct skin disease categories, reflecting the diversity and long-tailed nature of real-world dermatological practice. The first benchmark is derived from the publicly available Fitzpatrick17k dataset. From this dataset, we randomly sample 1,000 images spanning broad range of dermatological conditions. As widely used open-source benchmark, Fitzpatrick17k provides representative and diverse test bed for assessing generalization performance. The second benchmark is an internally curated dataset consisting of approximately 200 images. All samples in this internal dataset were independently reviewed and corrected by board-certified dermatologists from Class-III Grade-A hospitals, each with more than five years of clinical experience. This rigorous expert verification process ensures high diagnostic accuracy of the ground-truth labels and enhances the objectivity and reliability of the evaluation. Importantly, neither benchmark restricts the diagnosis space to fixed or exhaustive disease list. Instead, the evaluation setting allows for hierarchical and semantically related diagnoses, which more closely mirrors real-world clinical scenarios where disease boundaries may overlap and exact subtype distinctions are not always required for effective clinical decision-making. 4.1.3 Baselines We compared our proposed SkinFlow model with several representative multimodal large language models (MLLMs), including: General-purpose MLLMs: Qwen2.5-VL-Instruct-7B(Bai et al., 2025), InternVL3-78B(Zhu et al., 2025), Qwen3-VL-Instruct-235B-A22B(Team, 2025) and GPT-5.2(OpenAI, 2025). Medical-domain MLLM:Lingshu-32B (Xu et al., 2025),medgemma-27b-it (Sellergren et al., 2025) . 4.1.4 Evaluation Metrics Conventional evaluation metrics for classification tasks, such as accuracy or exact-match rate, adopt binary notion of correctness, where any prediction that does not exactly match the reference label is treated as equally incorrect. However, this evaluation paradigm is fundamentally misaligned with real-world clinical practice, particularly in dermatological diagnosis. In clinical settings, diagnostic predictions often exhibit varying degrees of semantic proximity and therapeutic relevance. diagnosis that is not strictly identical to the reference label may still provide substantial clinical value if it leads to an appropriate treatment plan, whereas semantically distant but technically distinct diagnosis may result in harmful clinical decisions. Recent studies have emphasized that medical AI systems should be evaluated based on their ability to support clinical reasoning and decision-making rather than mere label correspondence. For example, (Sokol et al., 2025) argue that AI systems should genuinely enhance clinical decision quality to bridge the translational gap between algorithmic performance and real-world utility. In the dermatology domain, Derm1M (Yan et al., 2025a) further highlights the strong hierarchical structure among skin diseases, where diagnoses along the same pathological lineage (e.g., parentchild relations) are often more clinically informative than unrelated categories. Motivated by these insights, we adopt clinically grounded evaluation protocol instead of conventional accuracy. Our evaluation explicitly accounts for the hierarchical relationships between dermatological diseases and their corresponding clinical implications, particularly treatment consistency and diagnostic safety. Specifically, model predictions are categorized according to the following criteria: True (Correct Diagnosis): The predicted diagnosis exactly matches the reference label or corresponds to medically accepted synonym, alias, or abbreviation (e.g., Herpes zoster Shingles). True (Subclass Match): The predicted diagnosis is clinically valid subclass of the reference diagnosis (e.g., Atopic dermatitis Eczema), reflecting higher diagnostic specificity while remaining therapeutically consistent. Parent-Class Predictions: True: The predicted diagnosis is closely related parent category that retains clear clinical value, where treatment strategies are consistent or differ only in lesion location. True (Coarse but Directionally Correct): The prediction captures the correct diagnostic direction but lacks specificity; nevertheless, it remains clinically actionable. False: The predicted parent category is overly broad and fails to provide meaningful guidance for clinical decision-making (e.g., Dermatitis, Skin cancer). False (Sibling-Class Confusion): The predicted diagnosis belongs to sibling category of the reference disease, representing common but clinically misleading misclassification. Table 2: Comparison of different models on Self-owned dataset and Fitzpatrick17k. Domain Model General Qwen2.5VL-7B-Instruct Qwen2.5VL-72B-Instruct Qwen3VL-32B-Instruct Qwen3VL-235B-A22B-Instruct Internvl3-78B GPT-5.2 Medical medgemma-27b-it Lingshu-32B Ours Param Size 7B 72B 32B 235B 78B / 27B 32B 7B Fitzpatrick17k Self-owned dataset TOP1 TOP2 TOP3 TOP4 TOP TOP6 TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 10.05 11.75 16.02 17.13 12.70 18.24 13.60 10.06 17.13 19.01 23.72 25.75 22.16 25.69 21.23 17.91 21.34 23.59 28.03 31.96 27.79 31. 26.13 23.28 24.59 27.41 32.33 35.57 32.76 36.49 30.72 26.63 27.66 31.04 36.74 39.78 35.82 39.16 33.95 29.69 31.00 33.62 40.34 42.59 38.30 42. 37.87 32.47 23.98 37.24 43.88 45.92 50.51 26.73 42.57 51.98 56.93 60.40 30.65 47.24 50.25 55.28 59.80 35.43 50.29 56.00 57.71 60.57 28.22 46.04 53.47 56.93 60.40 39.11 48.51 56.93 63.86 66.34 22.87 28.72 34.04 45.74 51.06 20.00 26.11 35.00 39.44 44.44 54.59 64.85 64.32 64.00 62.87 68.81 54.79 50.00 29. 46.12 55.38 62.26 67.72 71.16 36.63 50.99 59.90 68.81 73. 79.21 vs Qwen3VL-235B-A22B-instruct - +12.06 +20.37 +23.42 +26.69 +27.94 +28.57 +1.2 +0.7 +0.7 +11.1 +12.7 +15. False (Safety-Critical Errors): Predictions that cross critical clinical boundaries, such as benign vs. malignant or infectious vs. non-infectious diseases, are strictly penalized in accordance with the medical principle of First, do no harm. False (Invalid or Irrelevant Predictions): The predicted diagnosis is empty or exhibits no meaningful clinical relationship to the reference label. Under this evaluation framework, diagnostic predictions are judged by their clinical actionability and safety, rather than by rigid label equivalence. This design allows us to distinguish between clinically meaningful near-miss predictions and dangerous or non-informative errors, thereby providing more realistic and clinically relevant assessment of model performance in open-world dermatological diagnosis scenarios. All models were evaluated using identical prompts for fair comparison. the evaluation was conducted using Gemini-2.5-Pro. Each evaluation was repeated three times and the mean accuracy was reported to mitigate randomness. 4.2 Main Results Table 2 presents comprehensive quantitative comparison across Fitzpatrick17k and the Self-owned datasets. Our 7B model achieves state-of-the-art (SOTA) performance on nearly all benchmarks, consistently outperforming both massive general-purpose VLMs and specialized medical models. Superiority on Public Benchmarks. The most striking results are observed on the Fitzpatrick17k dataset. While general VLMs and specialized medical models struggle with the fine-grained diversity of skin diseases, our model achieves Top-1 accuracy of 29.19%, surpassing the strongest baseline (GPT-5.2) by 10.95% and the massive Qwen3VL-235B by 12.06%. Furthermore, our model exhibits exceptional retrieval capability in complex cases, reaching Top-6 accuracy of 71.16%, which represents +28.57% boost over the Qwen3VL-235B baseline. This substantial margin validates the effectiveness of our two-stage strategy in mastering intricate pathological patterns that general models fail to capture. Parameter Efficiency vs. Massive Models. Despite being orders of magnitude smaller (7B parameters), our model demonstrates superior performance and efficiency compared to \"super-large\" models. On the Self-owned dataset, although GPT-5.2 maintains slight edge in Top-1 accuracy (39.11% vs. 36.63%), our model surpasses it in all other ranking metrics (Top-2 to Top-6). Notably, our model achieves 79.21% Top-6 accuracy on the Self-owned set, significantly higher than GPT-5.2 (68.81%) and Qwen3VL-235B (64.00%). This suggests that while general models may identify the primary label in familiar contexts, our model provides much more robust and clinically relevant diagnostic \"candidate pool,\" which is critical for reducing omissions in real-world dermatology. Table 3: Ablation study of different components. Method Fitzpatrick17k Self-owned dataset TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 TOP1 TOP2 TOP3 TOP4 TOP5 TOP ours ours(w/o DVE) ours(w/o DVE & stage 1) 29.19 24.45 15.22 46.12 33.24 25.26 55.38 42.22 32.06 62.26 48.62 37.61 67.72 54.15 41. 71.16 57.69 45.36 36.63 35.64 27.46 50.99 51.49 45.08 59.90 60.89 52.33 68.81 66.83 59.07 73.27 70.79 64. 79.21 74.75 66.84 4.3 Ablation Studies and Analysis To investigate the individual contributions of the key components in our proposed method, specifically the Stage 1 captioning training task and the Dynamic Visual Encoding (DVE) module, we conducted series of ablation experiments. The results are summarized in Table 3. We compared three settings: (1) the baseline model without Figure 4: Effectiveness of Stage 1 Caption Training. (a) Training reward curves and (b) validation reward curves. The blue line represents the model trained directly on the general-purpose baseline, while the red line denotes the model further trained based on the caption-enhanced Stage model. both DVE and the Stage 1 caption task, (2) the model with Stage 1 training but utilizing static visual encoder (w/o DVE), and (3) our full model equipped with both components. Effectiveness of Stage 1 Caption Training As shown in Figure 4, the Stage2 model exhibited faster convergence and achieved higher validation rewards compared with the Base model. As shown in the last two rows of Table 3, the introduction of the captioning task in Stage 1 yields substantial performance improvement over the baseline. On the Self-owned dataset, the Top-1 accuracy surges from 27.46% to 35.64%, and on the Fitzpatrick17k dataset, it increases from 15.22% to 24.45%. This demonstrates that the caption generation task significantly enhances the models ability to align visual features with textual descriptions, establishing robust foundation for downstream disease identification. Impact of Dynamic Visual Encoding (DVE) The incorporation of the DVE module further boosts performance, particularly in terms of generalization and top-ranked accuracy. Comparing the full model (ours) with the variant without DVE, we observe consistent improvements. On the Self-owned dataset, the Top-1 accuracy rises to 36.63%, and the Top-6 accuracy reaches 79.21%. Notably, the efficacy of DVE is most pronounced on the challenging Fitzpatrick17k dataset, where the Top-1 accuracy improves by approximately 4.74% (from 24.45% to 29.19%), and the Top-6 accuracy sees remarkable increase of over 13% (from 57.69% to 71.16%). These results suggest that dynamic visual encoding allows the model to capture more fine-grained and adaptive visual features, which is critical for handling diverse skin conditions across different domains. 4.4 Qualitative Analysis: Visual Evidence Alignment To interpret the source of the proposed models diagnostic superiority, we visualize the cross-attention maps corresponding to the final diagnostic token (Figure 5). Three key phenomena observed in the heatmaps and histograms validate our architectural and training hypotheses: From global scanning to local fixation: General-purpose MLLMs (Qwen2.5-VL, Qwen3-VL) tend to distribute attention broadly across the entire anatomical region (e.g., the whole knee in Row 1), exhibiting \"semantic spread\" where the model fails to distinguish between the lesion and the surrounding healthy skin. In contrast, our method demonstrates distinct attention sparsity, concentrating its focus exclusively on the pathological lesions. This confirms that the model has learned to ignore irrelevant background noise and lock onto the discriminative visual features essential for diagnosis. Enhanced disentanglement via DVE and stage 1: Comparing the ablation variant (\"w/o stage1 & DVE\") with the full pipeline, we observe that the complete method eliminates background artifacts more effectively. For instance, in complex scenarios with multiple scattered lesions (Row 2 and Row 4), the full model distinctly highlights separate lesion boundaries, whereas the ablation model produces blurred or connected heatmap. This empirically supports our theoretical motivation: the Dynamic Vision Encoder (DVE), combined with the fine-grained visual guidance from stage 1, effectively \"unfolds\" the visual manifold. This allows the attention mechanism to linearly separate pathological textures from normal skin tones with high precision. High-confidence activation: The attention weight distribution histograms (Figure 5, right column) quantify this qualitative improvement. Baseline models follow heavy-tailed distribution towards low weights (0.00 0.02), indicating degree of uncertainty or \"hesitation\" in feature selection. Conversely, our method (yellow bars) exhibits pronounced peak in the high-weight region (> 0.06). This confidence shift suggests that the proposed model performs diagnosis based on strong, localized visual evidence rather than weak global context. To verify that the observed attention concentration is generalizable behavior rather than anecdotal evidence, we conducted quantitative statistical analysis over 500 randomly sampled test images (Figure 6). The distribution Figure 5: Visual attention attribution analysis. We visualize the cross-attention maps of the final diagnostic token relative to the input image across different models. The histograms (right) show the distribution of attention weights. (1) Attention concentration: While baselines (Qwen2.5/3-VL) exhibit diffuse attention, often distracted by healthy skin or background noise, Our method demonstrates precise lesion localization, sharply focusing on pathological features (e.g., papules, ulcers). (2) Effect of FDLinear and stage 1 training: Comparing \"w/o stage1 & DVE\" with \"Ours\", the introduction of the Dynamic Vision Encoder (DVE) and stage 1 training significantly improve the model to adaptively highlights important diagnostic details. (3) Confidence shift: The histograms reveal that our method (yellow) assigns higher attention weights (> 0.06) to key regions than the baselines, indicating shift from uncertain global scanning to confident diagnostic reasoning. reveals distinct \"Shift-to-Right\" phenomenon driven by our two-stage design: Baseline Uncertainty: Qwen2.5-VL (light green) and the model without Stage 1 (dark green) show heavy-tailed distribution dominated by the 0.00 0.01 range. This indicates \"diffuse scanning\" mode where probability mass is wasted on irrelevant background pixels. Semantic Alignment via Stage 1: Upon introducing caption learning (Orange, \"w/o DVE\"), we observe sharp drop in the low-weight bin and surge in the > 0.06 bin. This confirms that medical captioning acts as strong supervisor, teaching the model to distinguish pathological regions from healthy skin. Feature Purification via DVE: The full model (Yellow, \"Ours\") incorporating the Dynamic Vision Encoder achieves the most extreme distribution: it has the lowest noise floor (0.00 0.01) and the highest confidence peak (> 0.06). This quantitatively proves that FDLinear effectively suppresses background redundancy and amplifying diagnostic signal strength."
        },
        {
            "title": "5 Limitations",
            "content": "Model interpretability was not further evaluated in this study. After the second-stage training, we observed that the model tended to generate shorter captions as diagnostic evidence during end-to-end prediction. This behavior may affect the interpretability of its reasoning process. In future work, we plan to collaborate with dermatologists to design more systematic interpretability evaluation metrics and further refine the model accordingly. Figure 6: Quantitative distribution of attention weights across 500 test samples. The histogram statistics (scaled by 100) reveal progressive shift in attention mechanisms. (1) Noise Suppression: The full model (Ours, yellow) exhibits the lowest frequency in the background noise interval (0.00 0.01), substantially lower than the Qwen2.5-VL (light green). (2) Signal Amplification: In the high-confidence interval (> 0.06), the introduction of Stage 1 (orange) drastically increases the frequency of focused attention. The integration of DVE (yellow) further boosts this peak, demonstrating that the full pipeline maximizes the signal-to-noise ratio in visual reasoning. All images used in this study were captured under relatively simple background conditions. Therefore, the models diagnostic performance may degrade when applied to real-world scenarios with complex or cluttered backgrounds. Future work will involve expanding the dataset to include more diverse imaging environments to improve robustness and generalization."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we addressed the fundamental limitations of general-purpose LVLMs in the domain of dermatological diagnosis. By framing the diagnostic process as an optimization of an image compressiondecoding system, we demonstrated that the bottleneck in diagnostic performance stems from inefficient visual information transmission. Our proposed two-stage RL-based training strategy, coupled with the Dynamic Visual Encoding (DVE) module, effectively enables the model to disentangle critical pathological features from clinical background noise. Our extensive experiments on both public datasets (Fitzpatrick17k) and self-owned datasets yield several key insights. First, we show that domain-specialized 7B model can surpass massive models over 30 its size by enhancing visual-semantic alignment. Second, the proposed two-stage paradigm proves that explicit linguistic guidance (Stage I) and DVE module provides necessary foundation for the reconstruction of implicit diagnostic cues (Stage II). Third, our qualitative attention analysis confirms significant \"confidence shift\" from diffuse global scanning to localized diagnostic reasoning. Finally, by introducing clinically grounded evaluation framework, we bridge the gap between algorithmic accuracy and real-world clinical actionability. We believe that this study provides robust blueprint for developing high-efficiency, safety-aware medical AI. Future work will explore the generalization of this compression-decoding framework to other visually intensive medical specialties, such as pathology and radiology, to further validate the universality of our approach."
        },
        {
            "title": "References",
            "content": "Mokhaled NA Al-Hamadani, Mohammed Fadhel, Laith Alzubaidi, and Balazs Harangi. 2024. Reinforcement learning algorithms and applications in healthcare and robotics: comprehensive and systematic review. Sensors, 24(8):2461. Rafael Luz Araújo, Flávio HD de Araújo, and Romuere RV Silva. 2022. Automatic segmentation of melanoma skin cancer using transfer learning and fine-tuning. Multimedia Systems, 28(4):12391250. Hassan Ashraf, Asim Waris, Muhammad Fazeel Ghafoor, Syed Omer Gilani, and Imran Khan Niazi. 2022. Melanoma segmentation using deep learning with test-time augmentations and conditional random fields. Scientific Reports, 12(1):3948. Mohamed Badr, Abdullah Elkasaby, Mohammed Alrahmawy, and Sara El-Metwally. 2025. multi-model deep learning architecture for diagnosing multi-class skin diseases. Journal of Imaging Informatics in Medicine, 38(3):17761795. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Linwei Chen, Lin Gu, Liang Li, Chenggang Yan, and Ying Fu. 2025. Frequency dynamic convolution for dense image prediction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 30178 30188. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161. Thomas Cover. 2006. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3):326334. Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901. Aya El Mir, Lukelo Thadei Luoga, Boyuan Chen, Muhammad Abdullah Hanif, and Muhammad Shafique. 2024. Democratizing mllms in healthcare: Tinyllava-med for efficient healthcare diagnostics in resource-constrained settings. In 2024 IEEE International Conference on Image Processing Challenges and Workshops (ICIPCW), pages 41644170. IEEE. Arshia Eskandari and Mahkame Sharbatdar. 2024. Efficient diagnosis of psoriasis and lichen planus cutaneous diseases using deep learning approach. Scientific Reports, 14(1):9715. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Xuechen Guo, Wenhao Chai, Shi-Yan Li, and Gaoang Wang. 2024. Llava-ultra: Large chinese language and vision assistant for ultrasound. In Proceedings of the 32nd ACM international conference on multimedia, pages 88458854. Mario Fernando Jojoa Acosta, Liesle Yail Caballero Tovar, Maria Begonya Garcia-Zapirain, and Winston Spencer Percybrooks. 2021. Melanoma diagnosis using deep learning techniques on dermatoscopic images. BMC Medical Imaging, 21(1):6. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. 2023. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259 265. Khadija Nawaz, Atika Zanib, Iqra Shabir, Jianqiang Li, Yu Wang, Tariq Mahmood, and Amjad Rehman. 2025. Skin cancer detection using dermoscopic images with convolutional neural network. Scientific Reports, 15(1):7252. OpenAI. 2025. Gpt-5.2: The most advanced frontier model for professional work and long-running agents. https://openai.com/index/introducing-gpt-5-2/. Accessed: 2025-12-11. Tri-Cong Pham, Chi-Mai Luong, Van-Dung Hoang, and Antoine Doucet. 2021. Ai outperformed every dermatologist in dermoscopic melanoma diagnosis, using an optimized deep-cnn architecture with custom mini-batch logic and loss function. Scientific Reports, 11(1):17485. Wessam Salamaa and Moustafa Aly. 2021. Deep learning design for benign and malignant classification of skin lesions: new approach. Multimedia Tools and Applications, 80(17):2679526811. Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, and 1 others. 2025. Medgemma technical report. arXiv preprint arXiv:2507.05201. Ahmed Shahin, Ahmed Kamal, and Mustafa Elattar. 2018. Deep ensemble learning for skin lesion classification from dermoscopic images. In 2018 9th Cairo International Biomedical Engineering Conference (CIBEC), pages 150153. IEEE. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Kacper Sokol, James Fackler, and Julia Vogt. 2025. Artificial intelligence should genuinely support clinical reasoning and decision making to bridge the translational gap. npj Digital Medicine, 8(1):345. Nuthal Srinivasan, Mohamed Yacin Sikkandar, Maryam Alhashim, and Chinnadurai. 2025. Capsule network approach for monkeypox (capsmon) detection and subclassification in medical imaging system. Scientific Reports, 15(1):3296. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, and 1 others. 2024. Towards generalist biomedical ai. Nejm Ai, 1(3):AIoa2300138. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. 2025. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629. Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, and 1 others. 2025. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044. Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, and Zongyuan Ge. 2025a. Derm1m: million-scale vision-language dataset aligned with clinical ontology knowledge for dermatology. arXiv preprint arXiv:2503.14911. Siyuan Yan, Zhen Yu, Clare Primiero, Cristina Vico-Alonso, Zhonghua Wang, Litao Yang, Philipp Tschandl, Ming Hu, Lie Ju, Gin Tan, and 1 others. 2025b. multimodal vision foundation model for clinical dermatology. Nature Medicine, pages 112. Junpeng Zhang, Fan Zhong, Kaiqiao He, Mengqi Ji, Shuli Li, and Chunying Li. 2023. Recent advancements and perspectives in the diagnosis of skin diseases using machine learning and deep learning: review. Diagnostics, 13(23):3506. Juexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, and Xin Gao. 2023. Skingpt-4: an interactive dermatology diagnostic system with visual large language model. arXiv preprint arXiv:2304.10691. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, and 1 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479."
        },
        {
            "title": "A Predefined Caption Schema",
            "content": "{ } \"color\": \"\", // Detailed color-related information in the skin description; empty if none \"location\": \"\", // Description of all sites showing skin abnormality; empty if none \"shape\": \"\", // Information related to the described shape; empty if none \"lesion_type\": \"\", // Type of skin lesion, e.g., macule, papule, nodule, vesicle, pustule, wheal; scales, erosion, ulcer, crust, lichenification; empty if none \"number\": \"\", // Count of abnormalities; empty if none \"size\": \"\", // Description of the abnormal skin's size; empty if none \"texture\": \"\", // Texture of the abnormal skin, e.g., hardness; empty if none \"border_characteristics\": \"\", // Clarity of the abnormal skin's border; empty if none \"surface_characteristics\": \"\", // All surface features, including central features; empty if none \"distribution_characteristics\": \"\", // Distribution features, e.g., scattered, isolated, dense, confluent; empty if none \"surrounding_characteristics\": \"\", // All features around the abnormal skin; empty if none \"other\": \"\" // Any other appearance descriptors that cannot fit into the fields above; only used when absolutely necessary, otherwise empty"
        },
        {
            "title": "B Disease Diagnosis Prompt",
            "content": "[Character] As dermatologist, please provide the most likely set of skin diseases (Top-{k}) based on the provided photos of skin diseases and explain the reasons. [Output] First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags.like this: <answer>{ \"\"\" + f'\"top{flag[-1]}_diseases\": '+ \"\"\"[ { \"disease\": \"The name of the disease\", \"probability\": 0.80, // Between 0 and 1 \"key_matching_fields\": [\"color\", \"shape\"], // Hit field \"brief_reason\": \"A one-sentence reason \" }, ... ] }</answer>"
        },
        {
            "title": "C Examples of disease diagnosis and examples of captions",
            "content": "Figure 7: Examples of disease diagnosis and examples of captions."
        }
    ],
    "affiliations": [
        "Baichuan Inc.",
        "Beijing Key Laboratory of Molecular Diagnosis on Dermatoses",
        "Department of Dermatology, Peking University First Hospital",
        "NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics",
        "National Clinical Research Center for Skin and Sexually Transmitted Diseases",
        "School of Biomedical Engineering, Tsinghua University",
        "University of Hong Kong"
    ]
}