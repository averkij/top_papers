{
    "paper_title": "Dynamic Concepts Personalization from Single Videos",
    "authors": [
        "Rameen Abdal",
        "Or Patashnik",
        "Ivan Skorokhodov",
        "Willi Menapace",
        "Aliaksandr Siarohin",
        "Sergey Tulyakov",
        "Daniel Cohen-Or",
        "Kfir Aberman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts, i.e., entities defined not only by their appearance but also by their motion. In this paper, we introduce Set-and-Sequence, a novel framework for personalizing Diffusion Transformers (DiTs)-based generative video models with dynamic concepts. Our approach imposes a spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an identity LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the identity LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal weight space that effectively embeds dynamic concepts into the video model's output domain, enabling unprecedented editability and compositionality while setting a new benchmark for personalizing dynamic concepts."
        },
        {
            "title": "Start",
            "content": "DANIEL COHEN-OR"
        },
        {
            "title": "KFIR ABERMAN",
            "content": "SNAP RESEARCH https://snap-research.github.io/dynamic_concepts 5 2 0 2 0 2 ] . [ 1 4 4 8 4 1 . 2 0 5 2 : r Fig. 1. We personalize video model to capture dynamic concepts entities defined not only by their appearance but also by their unique motion patterns, such as the fluid motion of ocean waves or the flickering dynamics of bonfire (left). This enables high-fidelity generation, editing, and the composition of these dynamic elements into single video, where they interact naturally (right). Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts entities defined not only by their appearance but also by their motion. In this paper, we introduce Setand-Sequence, novel framework for personalizing Diffusion Transformers (DiTs)based generative video models with dynamic concepts. Our approach imposes spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an identity LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the identity LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework resulting in spatio-temporal weight space effectively embeds dynamic concepts into the video models output domain, enabling unprecedented editability and compositionality, and setting new benchmark for personalizing dynamic concepts."
        },
        {
            "title": "Introduction",
            "content": "The advent of generative models has revolutionized content creation, enabling the synthesis and manipulation of high-quality visual media with remarkable fidelity. Recent advances in text-toimage [45, 49, 54] and text-to-video [4] models have further expanded these capabilities, opening up unprecedented opportunities for creative expression and personalization. Personalization has been well-established area of research in the image domain, allowing models to learn user-specific concepts that can be customized, edited, and composed into diverse contexts [16, 51]. However, while video models have shown improvement in quality and capability, the task of personalizing these models remains an open problem. Unlike images, videos introduce an additional temporal dimension, making personalization significantly more challenging. In particular, video concepts are inherently dynamic, encompassing both appearance and motion, which must be learned and represented cohesively. In this work, we propose novel approach to personalizing generative text-to-video models, focusing on the idea of dynamic concepts objects or subjects characterized by their entangled appearance and motion. Describing dynamic concepts through text is challenging, hence, embedding them into the output domain of video model and representing them as tokens, facilitates wide range of editing and compositional tasks. Our work is built upon the state-of-the-art Diffusion Transformers (DiTs) architecture for video generation [36, 38], which processes spatial and temporal tokens simultaneously. Unlike video generators using factorized spatial and temporal modeling (UNet-based) [6] suffering from rigid artifacts, this joint spatio-temporal modeling is necessary for high-quality video generation [38]. While DiTs achieve superior quality, they lack the innate inductive bias to disentangle spatial and temporal features. This absence of built-in separation poses challenge for embedding dynamic concepts effectively in the models weight space. Furthermore, directly fine-tuning low-rank adapters (LoRAs) on single video often fails to capture both appearance and motion, resulting in non-reusable representation that fails to generalize across diverse contexts or support meaningful compositionality of dynamic concepts. To address these limitations, we introduce novel framework called Set-and-Sequence, designed to impose spatialtemporal structure in the weight space, enabling the representation of dynamic concepts in the weights space. , Vol. 1, No. 1, Article . Publication date: February 2025. 2 Abdal et al. Fig. 2. Set-and-Sequence framework operates in two stages: (i) Identity Basis: We train LoRA Set Encoding on unordered set of frames extracted from the video, focusing only on the appearance of the dynamic concept to achieve high fidelity without temporal distractions. (ii) Motion Residuals: The Basis of the Identity LoRAs is frozen and the coefficient part is augmented with coefficients of LoRA Sequence Encoding trained on the temporal sequence of full video clip, allowing the model to capture the motion dynamics of the concept. The proposed Set-and-Sequence framework operates in two stages: (i) Identity Basis: We train LoRAs on static unordered set of frames extracted from the video, focusing solely on the appearance of the dynamic concept to achieve high fidelity without temporal distractions. (ii) Motion Residuals: The Basis of the Identity LoRAs is frozen and the coefficient part is augmented with new coefficients trained on the temporal sequence of full video clip, allowing the model to capture the motion dynamics of the concept. This two-stage approach unlocks transformative capabilities in video generation. For the first time, we demonstrate seamless scene composition and adaptation with preserved motion and appearance. Tasks such as blending disparate video componentse.g., combining the fluid motion of ocean waves with the flickering dynamics of bonfireare achieved with unprecedented fidelity as shown in Fig. 1 and in the supplementary video. Moreover, our framework enables intuitive editing of camera motion, refining expressions, and introducing localized changes, all driven by text prompts. These advancements represent significant leap in compositionality, scalability, and adaptability, setting new benchmark for personalized generative video models."
        },
        {
            "title": "2 Related Work\n2.1 Foundational Video Models.",
            "content": "Foundational video models, such as Imagen Video [23], Sora [38], CogVideoX [60], Veo2 [12], MovieGen [40] and others have made significant strides in synthesizing visually stunning and semantically aligned videos from textual descriptions. They were originally based on U-net-like [50] architectures [6, 20, 24, 55] and were extending image generators to video synthesis by training additional temporal layers to model dynamics. However, in the pursuit of greater scalability, the community switched to transformer-based , Vol. 1, No. 1, Article . Publication date: February 2025. backbones with joint spatio-temporal modeling (e.g., [26, 36, 38]), which quickly became the dominant paradigm for large-scale video generation (e.g., [29, 38, 40, 60]). While these models excel at generating coherent content, they primarily rely on generic motion trajectories, limiting their ability to capture nuanced human expressions, individualized mannerisms, or complex dynamic interactions within shared scene [36, 38]. These limitations highlight the need for methods capable of personalization, dynamic scene composition, and precise editing in generative video models. To address these challenges, we build on the video DiT (DiT version of Snap Video [36]) architecture and extend its capabilities with our proposed Set-and-Sequence framework, enabling the representation and compositionality of dynamic concepts with unprecedented fidelity and adaptability."
        },
        {
            "title": "2.2 Video Personalization and Motion Representation.",
            "content": "While personalization in image generation has seen significant advancementsenabling identity preservation, stylization, and tailored manipulation [16, 27, 31, 51, 52]video personalization remains relatively underexplored. In the video domain, personalization methods predominantly build upon UNet-based architectures [3, 22, 34, 59, 62, 66], inheriting their shortcomings. Furthermore, approaches in this domain can be broadly categorized into three domains. First, works like Token Flow [18] focus on video stylization [7, 28, 30, 63]. Second, methods like DreamVideo [58] and others [5, 35, 64] emphasize extracting motion dynamics from several videos to perform motion transfer. Third, approaches like Customize-a-Video [47] , Fate/Zero [41], and DreamMix [37] perform local editing on single videos by optimizing specific parts. Although promising, these methods, such as Customize-a-Video [47] are architecture specific and operate on the assumption that motion Dynamic Concepts Personalization from Single Videos 3 Fig. 3. Local and Global Editing. Our Set-and-Sequence framework enables text-driven edits of dynamic concepts while preserving both their appearance and motion. Edits can be global (e.g., background and lighting) or local (e.g., clothing and object replacement), ensuring high fidelity to the original dynamic concepts. and appearance are disentangled, optimizing distinct LoRA [25] modules or layers for each. This rigid separation often leads to artifacts, losing fidelity and contextual realism. Moreover, they primarily target applications like motion transfer, diverting focus from video personalization that captures the inherent entanglement of appearance and motion in dynamic concepts. To solve this, we introduce shared spatio-temporal weight space that cohesively encodes dynamic concepts using two-stage LoRA [25] training."
        },
        {
            "title": "2.3 Scene Composition in Video Models.",
            "content": "Scene composition and dynamic editing remain significant challenges in video synthesis due to the complexities of maintaining temporal coherence and contextual fidelity. Approaches like BreakA-Scene [2] enable concept-level blending but are limited to static, image-like representations, relying heavily on predefined masks and cross-attention mechanisms. In video models, scene composition often involves generating composed images using personalized text-to-image methods [42, 57] and then applying image-to-video techniques to synthesize motion dynamics on top of the static image [6, 9, 10, 19, 21, 46]. However, these models face several inherent limitations. First, they depend on powerful image composition models capable of blending multiple objects into cohesive scene effectively ignoring the motion [6, 10]. Second, they lack awareness of object-specific attributes, such as viewpoints, dynamic evolution of motion, and spatial relationships [6, 10]. Third, they fail to account for nuanced expressions and intricate motion patterns that cannot be adequately captured through text descriptions alone [9, 10]. These shortcomings render such models incompatible with the goals of video personalization and advanced compositionality. For the first time, we demonstrate advanced compositionality by merging disparate dynamics, such as fire and water, while capturing both appearance as well as motion from single videos. Our approach overcomes the limitations of previous methods, offering unified framework that enables personalization of dynamic concepts. Fig. 4. Stylization. Top: Stylization of dynamic concepts achieved by reweighting the identity basis. Bottom: Stylization and motion editing performed using prompt derived from the video in the top row."
        },
        {
            "title": "3 Method",
            "content": "We propose Set-and-Sequence (See Fig. 2), novel framework for personalizing text-to-video models using dynamic concepts extracted from single-video examples. Our approach learns these dynamic , Vol. 1, No. 1, Article . Publication date: February 2025. 4 Abdal et al. Fig. 5. Dynamic Concepts Composition. Composition results achieved by our framework showcasing seamless integration of dynamic concepts. with each concept color-coded for clarity. For more comprehensive demonstration, refer to the supplementary videos. concepts as decomposition of appearance and motion into unified spatio-temporal weight space inspired by the state-of-the-art generators [36, 38]. We impose this weight space in DiT-based diffusion architecture [39], an architecture that does not explicitly separate spatial and temporal features unlike UNet-based architectures [47], resulting in seamless compositionality, editing, and adaptation. Central to our framework is two-stage learning technique. In the first stage, Identity Basis Learning, we train Low-Rank Adaptation (LoRA) layers on an unordered set of video frames, extracting static, motion-independent identity basis that captures the appearance of the concept. In the second stage, Motion Residual Encoding, the identity basis is augmented with motion dynamics by fine-tuning coefficients on the full video sequence. We employ additional regularizations and employ text conditioning at each stage, using static prompts for appearance learning and combination of static and dynamic prompts for encoding motion dynamics. At inference time, this enables intuitive reprompting, recomposing, and editing of content using only text descriptions, facilitating advanced personalization and dynamic scene composition."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Video Diffusion with Flow Matching Loss. Our framework builds on video diffusion model trained with flow matching loss [1, 32]. This objective aligns the predicted and true velocity fields and is defined as: Lflow = Ex,ùë° (cid:34)(cid:13) (cid:13) vùúÉ (xùë° , ùë°) (cid:13) (cid:13) (cid:35) xùë° ùë° (cid:13) 2 (cid:13) (cid:13) (cid:13) , (1) where xùë° represents the perturbed data at time ùë°, vùúÉ is the predicted velocity field, and is the true data flow. xùë° ùë° Low-Rank Adaptation (LoRA). LoRA fine-tunes pretrained model by introducing low-rank updates to its weight matrices: = + ŒîW, ŒîW = AB, , Vol. 1, No. 1, Article . Publication date: February 2025. where is the original weight matrix, Rùëöùëü and Rùëü ùëõ are low-rank matrices with rank ùëü ùëöùëñùëõ(ùëö, ùëõ). LoRAs parameter efficiency and adaptability make it an ideal choice for disentangling identity and motion in video data."
        },
        {
            "title": "3.2 Stage 1: Identity Basis Learning",
            "content": "In the first stage, we extract static identity features from an unordered set of frames as images in the input video. This stage creates time-independent identity representation, forming the foundation for subsequent motion encoding. By decomposing and separating identity from motion, it enables the independent editing of appearance and motion during inference. The LoRA weight modification for this stage is defined as: = + A1B1, (3) where A1 Rùëöùëü and B1 Rùëü ùëõ represent the low-rank parameters capturing the identity. Static text tokens Tstatic are used to describe the subjects appearance (e.g., as an illustration in Fig. 2, [v] person). In practice, for efficient editing and composition; appearance, background and expression information is also included in the static prompts to make it detailed (See supplementary materials). The [v] token is initialized with zeros. The resulting conditional velocity field is defined as: vùúÉ (xùë° , ùë°; Tstatic). (4) The identity-specific flow matching loss ensures accurate reconstruction of static features. The learned parameters A1 and B1 are obtained by solving the following optimization problem: (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (A1, B1) = arg min A1,B vùúÉ (xùë° , ùë°; A1, B1, Tstatic) xùë° ùë° Ex,ùë° (cid:13) (cid:13) (cid:13) (cid:13) (5) (cid:35) 2 . 2 This stage creates robust, low-dimensional basis for identity (2) representation. Dynamic Concepts Personalization from Single Videos 5 Fig. 6. Comparison with baselines. Comparison of our method with baseline approaches (NewMove [35], DreamVideo [58], DB-LoRA [51, 53], and DreamMix [37]) on two editing scenarios: changing the background and shirt, and adding glass. Our method demonstrates superior adherence to the prompt while preserving the subject identity, outperforming the baselines."
        },
        {
            "title": "3.3 Stage 2: Motion Residual Encoding",
            "content": "Building upon the static identity basis established in Stage 1, the second stage introduces an additional low-rank matrix B2, encoding motion as residual deformation on top of the identity. This stage captures the temporal evolution of motion dynamics, enabling independent manipulation and composition of motion during inference. The weight modification for this stage is defined as: = + A1B1 + A1B2, (6) where A1 and B1 remain fixed to preserve identity, and B2 Rùëü ùëõ encodes motion-specific deformations. Motion encoding uses union of static and motion-specific text tokens: Tmotion := Tstatic Tdynamic, where Tdynamic describes motion attributes (e.g., as illustrated in the Fig. 2, \"... in [u] motion\"). In practice, we also augment the dynamic part with the course action e.g., \"... dancing with legs up\" and camera motion e.g., \"... as the camera zooms in\" (See supplementary materials). The predicted velocity field is conditioned on both text components: (7) vùúÉ (xùë° , ùë°; Tmotion). (8) The flow matching loss for motion encoding ensures that the motion is reconstructed as deformation. The learned parameters B2 is obtained by solving the following optimization problem: B2 = arg min B2 Ex,ùë° (cid:34)(cid:13) (cid:13) vùúÉ (xùë° , ùë°; A1, B1, B2, Tmotion) (cid:13) (cid:13) xùë° ùë° (cid:35) . (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (9) This stage ensures that motion is complementary to identity, enabling robust, adaptable representations for dynamic content."
        },
        {
            "title": "3.4 Regularization",
            "content": "Regularization is integral to our framework, ensuring robust training, preventing overfitting, and maintaining efficient representations of identity and motion. We employ four distinct regularization strategies to achieve these goals: 3.4.1 Prior Preservation [48, 51]. To ensure fidelity to the pretrained base model, we regularize both training stages to reconstruct videos generated by the base model. We sample videos with text having two parts i.e. appearance and motion, for example, in case of humans, \"A man in blue t-shirt. He is walking in park\". These prompts generalize for non-human dynamic concepts as well. 3.4.2 High Dropout Regularization for High-Rank LoRA. Training on single video introduces challenges of underfitting with lowrank LoRA and overfitting with high-rank LoRA. To address these issues, we adopt high-rank LoRA configuration combined with selective dropout applied exclusively to the matrix in the LoRA update. Dropout regularization is applied to LoRAs in both stages as follows: = M, (10) where is binary mask with dropout probability ùëù (e.g., ùëù = 0.8). This selective dropout ensures that A‚Ñé remains stable, providing consistent basis for encoding appearance and motion. By introducing sparsity in the learned coefficients, this approach mitigates overfitting while encouraging exploration of diverse parameter combinations facilitating applications like dynamic concept composition (See Fig. 1). 3.4.3 Context-Aware Regularization. To enhance robustness and generalization, we incorporate text token masking and self-conditioning as complementary regularization techniques. Text Token Masking: Random tokens in the input text are masked during training, requiring the model to infer the missing information from the remaining context. This prevents overfitting to specific token patterns and improves adaptability to diverse or incomplete prompts which we leverage for editing and re-composition. Self-Conditioning: Inspired by [8], intermediate model outputs are reintroduced as inputs during subsequent steps, enabling iterative refinement. This feedback loop improves temporal consistency, ensuring stable identity and motion across frames."
        },
        {
            "title": "4 Experiment Settings\n4.1 Evaluation Dataset",
            "content": "We evaluate our framework on curated dataset of human-centric videos, as identity preservation poses significant challenge in editing and composition. Unlike general objects, humans are highly sensitive to subtle inconsistencies in appearance, motion, and expressions, making deviations immediately perceptible. The dataset includes five distinct identities performing actions such as dancing and walking, as well as scenario with two identities interacting in the same video. This setup tests our frameworks ability to preserve identity, maintain motion coherence, and achieve compositional , Vol. 1, No. 1, Article . Publication date: February 2025. 6 Abdal et al. Fig. 7. Ablation. Ablation of design choices on the editing task of adding different shirt and background. Low-rank LoRA (LoRA-1) results in underfitting, failing to capture sufficient detail, while high-rank LoRA (LoRA-8) overfits, compromising adaptability. Our two-stage approach with added regularization achieves balanced trade-off, preserving both fidelity and editability. fidelity compared to existing baselines. We evaluate our method using this dataset on the local and global editing tasks."
        },
        {
            "title": "4.2 Baselines.",
            "content": "We evaluate our method against several baselines, including stateof-the-art UNet-based approaches, architecture-agnostic methods, and LoRA-based variations as part of an ablation study. Among UNet-based models, we compare against DreamVideo [58] and NewMove [35] which are state-of-the-art frameworks for video personalization. Additionally, code for methods like Customize-a-Video [47] is unavailable, restricting direct comparison. We adapt UNet-based DreamMix [37] (Code not available) to the DiT architecture, enabling mixed image-video training to benchmark its performance. To ensure broader evaluation, we include architecture-agnostic methods like DreamBooth LoRA [51, 53] and Textual Inversion [17]. In our ablation study, we compare several LoRA setups to highlight the efficacy of our approach."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We employ the following metrics to quantitatively assess the quality of the generated videos: Semantic Alignment. We utilize CLIP-Text Similarity [43] (C-T) to measure the alignment between the generated video and the input text prompt. This metric computes the cosine similarity between the text embedding and the aggregated embeddings of all video frames, providing global assessment of semantic consistency. Identity Preservation. Maintaining identity consistency is crucial, especially in videos featuring human subjects. We utilize ArcFace Identity Similarity (ID) [13] to measure how well the identity of person is preserved between the generated and reference videos. Reconstruction Fidelity. To quantify pixel-level fidelity, we compute the Mean Squared Error (MSE) between corresponding frames in the generated and reference videos. Temporal Coherence. Ensuring smooth transitions and motion consistency across frames is critical. For Temporal Coherence (TC), we compute as CLIP image embeddings on all generated frames and , Vol. 1, No. 1, Article . Publication date: February 2025. report the average cosine similarity between all pairs of consecutive frames."
        },
        {
            "title": "5 Experiment Results\n5.1 Quantitative Evaluation",
            "content": "In order to show the effectiveness of our approach, we first ablate with the baselines and show that our two stage approach is essential for the model to integrate the dynamic concepts into the prior. Table 1 and Fig. 7 illustrates the results of our ablation study. They are evaluated on two editing tasks i.e. \"changing the shirt and background\" and \"holding glass\". When using low-rank LoRA, we observe significant loss in identity preservation due to underfitting, as the rank is insufficient to model the complexity of dynamic concepts. Conversely, high-rank LoRA overfits, resulting in diminished adaptability to new prompts. In contrast, our two-stage approach strikes balance by using the same rank to separately train an Identity Basis and Motion Residual. This enables better adaptation to novel prompts while preserving both appearance and motion. Finally, with the added dropout regularization discussed in Sec 3.4, our framework achieves seamless integration of local edits (e.g., clothing changes) and global edits (e.g., background replacement) while maintaining motion fidelity, as shown in the supplementary videos. To evaluate the effectiveness of our method with state-of-the-art approaches, we compare the results of various baseline methods in Fig. 6. Each method is provided with the same editing prompt, and we assess their ability to preserve both the identity and adherence to the textual description. As shown in the figure, our framework achieves high fidelity in identity preservation and text adherence, significantly outperforming other methods. For motion preservation and coherence, we provide additional results in the supplementary videos, highlighting the seamless integration of dynamic motion with edits. Table 2 shows the quantitative analysis of the results, where our two-stage set-and-sequence results in better trade-off compared to other methods."
        },
        {
            "title": "5.2 Qualitative Results",
            "content": "Our framework demonstrates significant advancements in both editing and composition of dynamic concepts, setting new benchmark in personalized video generation. In this section, we explore two primary applications: editing and composition. 5.2.1 Editing. Our framework excels in both local and global editing, as demonstrated in Fig. 3. The core objective is to capture intricate expressions and mannerisms while adapting dynamic concepts to new scenarios. For example, in Fig. 3, our method models complex athletic movements that current video generation models fail to replicate. These results are further demonstrated in the supplementary videos, where the coherency of motion and appearance across edits is observed. Moreover, our framework supports wide range of edits, such as changing expressions, age, and camera angles, or seamlessly integrating new objects and backgrounds into the scene. This level of control is achieved without compromising motion fidelity or identity preservation. For instance, by adjusting the weights of the LoRAs or modifying associated text prompts, our framework can produce creative outputs such as Pixar-style (See Fig. 4) characterizations or adapt the same dynamic concept to completely different context. The ability to selectively adapt parts of video, as shown in the supplementary videos, further emphasizes the flexibility of our approach. 5.2.2 Composition. One of the most significant contributions of our framework is the ability to compose dynamic concepts in novel and diverse settings. Leveraging the shared spatio-temporal weight space and our regularization techniques, we use single examples of multiple dynamic concepts and jointly train them using unified identity basis and associated motion residual module. Here, after the first stage, identity basis jointly represents multiple concepts and in the second stage, the motion deformations for each dynamic concept is learned jointly. For instance, we demonstrate the composition of multiple entities, such as intricate movements, ocean waves and bonfire, within the same scene. Fig. 5 illustrates how our method ensures coherence across these entities while maintaining their distinct characteristics. Additionally, we address challenges such as identity leakage (see Fig. 8), which arises when semantically similar concepts are combined (humans in our case). To mitigate this, we employ simple yet effective strategy of additionally training on stitched videos as regularization. These videos are trained less frequently and although not necessary, removing backgrounds in the such stitched videos helps further in composing results involving two or more humans. More examples are provided in the supplementary videos. Table 1. Ablation of Baselines. Table evaluating Mean Square Error (MSE), Identity Preservation (ID), CLIP-T (C-T), and Temporal Coherency (TC) on the editing task. Our method demonstrates better reconstruction-edibility trade-off. Method MSE ID C-T TC LoRA-1 LoRA-8 + Two-Stage + Reg 0.0432 0.0223 0.0461 0.0221 0.622 0.703 0.629 0. 0.226 0.224 0.250 0.239 0.9974 0.9969 0.9971 0.9972 Table 2. Editing Task Evaluation. Table evaluating Mean Square Error (MSE), Identity Preservation (ID), CLIP-T (C-T), and Temporal Coherency (TC) on the editing task. Our method achieves superior reconstructioneditability trade-off compared to the competing approaches. Method MSE ID C-T TC Tex-Inv DB-LoRA NewMove DreamVideo DreamMix Ours 0.0714 0.0223 0.2223 0.2021 0.0429 0.0221 0.145 0.703 0.270 0.118 0.579 0.680 0.201 0.224 0.204 0.218 0.226 0.239 0.9927 0.9969 0.9914 0.9657 0.9965 0. Dynamic Concepts Personalization from Single Videos 7 Fig. 8. Stitched Example. To address identity leakage when generating multiple identities and motions from multiple videos (Second Column), we augment training with stitched examples by combining videos side by side to generate new compositions with preserved identities (Third Column)."
        },
        {
            "title": "5.3 User Study",
            "content": "To evaluate the quality of identity preservation, motion fidelity, and adherence to prompts on the editing task, we conducted user study with 10 participants. We omit UNet based methods due to the overall lower quality (See Supplementary Videos). Participants were presented with pairs of videos generated by different methods and were asked to separately select the video that performed better in terms of identity preservation, motion fidelity, and adherence to the prompt. The results of the user study, summarized in Table 3, demonstrate that our method consistently outperforms competing approaches by achieving better tradeoff on the editing task. Table 3. User Study. User study results comparing methods on Identity Preservation (ID), Motion Preservation (MP), Adherence to Prompt (AP), and Overall Preference of the edits (OP). Preference is computed in percentages. Method IP MP AP OP Ours vs DreamMix Ours vs LoRA-1 Ours vs LoRA-8 (DB-LoRA) Ours vs Two-Stage 87% 88% 98% 100% 99% 95% 94% 100% 98% 78% 75% 98% 90% 86% 97% 76%"
        },
        {
            "title": "6 Limitations",
            "content": "While our framework achieves state-of-the-art performance in video personalization and dynamic concept modeling, it does have limitations. The training process, which involves LoRA optimization with additional regularization, can be computationally intensive. An encoder based approach would be an ideal solution for future work. Additionally, while the method captures most motions with high fidelity, it may struggle with high-frequency or highly complex motion patterns, such as erratic or rapid movements, where temporal consistency could be further improved. These challenges present opportunities for future work to enhance efficiency, speed, and robustness in handling more dynamic scenarios."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced novel framework for personalized video generation that captures dynamic concepts using two-stage Set-and-Sequence paradigm i.e. the first stage of identity encoding and then learning coupled motion residuals on the top. By embedding these concepts into this unified spatio-temporal weight space, our method achieves , Vol. 1, No. 1, Article . Publication date: February 2025. 8 Abdal et al. high fidelity in appearance preservation, motion coherence, and text adherence, surpassing state-of-the-art baselines. The evaluations demonstrated versatility of our framework in editing and composition, while maintaining identity and motion fidelity. The ability to compose and adapt dynamic concepts in novel ways highlights the transformative potential of our approach. This work addresses long-standing challenges in video personalization and sets new benchmark for personalized and compositional video generation."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Gordon Guocheng Qian and Kuan-Chieh (Jackson) Wang for their feedback and support."
        },
        {
            "title": "8 Architecture and Training Details",
            "content": "We build our framework as video diffusion model operating in the latent space of video autoencoder. The latent representation is based on causal video autoencoder following the architecture of MAGVITv2 [61]. Our autoencoder presents high compression ratio of 8 16 16 on the time and spatial dimensions, respectively, with bottleneck dimensionality of 32 channels. Full model details are given in Table 4. The video diffusion backbone consists in 11.5B parameters DiT [39] detailed in Table 5. The model is organized into 32 DiT Blocks with hidden dimensionality of 4096 channels, each of which consists of self-attention layer, followed by cross-attention layer to attend to text conditioning, and final MLP with an expansion factor of 4. To further reduce the input dimensionality, ViTlike [14] 122 input patchification operation is applied, increasing the effective video compression factor to 8 32 32. This allows modeling of 121 frames 1024 576px video using only 9216 tokens, and enables the use of full 3D self-attention for high-quality motion modeling [40] without incurring large computational penalty associated with its quadratic cost, which is further reduced by the use of 6144 tokens attention window. Each self-attention block consists of 32 attention heads with QK-Normalization [15] and is augmented with 3D-RoPE [56] embeddings, separately applied to the attention head channels in ratio of 2 : 1 : 1 for the temporal and spatial dimensions, respectively. Text prompts are encoded by the T5 [44] model and combined with video tokens through the crossattention layers. Following [39], diffusion timestep information is injected through modulation. We perform pretraining of the model by jointly training it on mixture of image and video data with resolution of 512 or 1024 px, aspect ratios of 16 : 9 and 9 : 16 for videos, and 16 : 9, 1 : 1, and 9 : 16 for image content. We adopt progressive training strategy on the video temporal dimension, progressively extending the number of frames from 17 to 121, corresponding to 5 seconds at our fixed framerate of 24 fps. During the pretraining stage, we use the AdamW [33] optimizer with fixed learning rate of 1ùëí 4, 10k steps warmup, weight decay of 0.01, ùõΩ = [0.9, 0.99], ùúñ = 1ùëí 8 and total of 822k training steps. Model training is accelerated through flash attention [11] with bf16 precision, and is distributed on 256 H100 GPUs using FSDP [65]. The training details of our Set-and-Sequence approach are summarized in Table 6. The model is trained in two stages. For single , Vol. 1, No. 1, Article . Publication date: February 2025. video, Stage of the two-stage approach without regularization is trained for 150 steps, while Stage II is trained for 400 steps, requiring total of approximately 90 minutes to converge. However, for our final method, which incorporates dropout regularization, convergence is slower. The number of training steps varies based on the complexity and number of videos. For single video, Stage is trained for 600 steps and Stage II for 900 steps. For multiple dynamic concepts and videos with complex motions, such as athletic dance sequences, Stage II requires extended training of 2k to 2.5k steps. To optimize text tokens effectively while avoiding overfitting, we use lower learning rate of 1ùëí 5. We observe that our method is able to generalize without optimizing for these special tokens as well. Our training uses the AdamW optimizer [33] with constant learning rate of 1ùëí 4. To ensure stable training, we set ùõΩ = [0.9, 0.99], apply weight decay of 0.01, and use gradient clipping with value of 0.05. Additionally, text prompt tokens are randomly dropped with probability of 0.1. All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory, using batch size of 8. We use cfg value of 8 to generate the results. Autoencoder MAGVIT Base channels Channel multiplier Encoder blocks count Decoder blocks count Stride of frame Stride of and Padding mode Compression rate Bottleneck channels Use KL divergence Use adaptive norm 16 [1, 4, 16, 32, 64] [1, 1, 2, 8, 8] [4, 4, 4, 4, 4] [1, 2, 2, 2, 1] [2, 2, 2, 2, 1] replicate 8 16 16 32 (decoder only) Table 4. Autoencoder and MAGVIT specifications. Backbone DiT 32 Input channels 1 2 2 Patch size 4096 Latent token channels 3D-RoPE Positional embeddings 32 DiT blocks count 32 Attention heads count 6144 (center) Window size Layer normalization Normalization Use flash attention Use QK-normalization Use self conditioning 0.9 Self conditioning prob. 1024 Context channels Table 5. Backbone and DiT specifications. Optimizer Learning rate LR scheduler Beta Weight decay Gradient clipping Dropout (Stage I) Dropout (Stage II) AdamW 1 104 constant [0.9, 0.99] 0.01 0.05 0.8 0. Table 6. Training stages and optimization settings."
        },
        {
            "title": "9 Prompts",
            "content": "Providing detailed prompts at the initialization stage is crucial for achieving high-quality editing and composition. Prompts describe not only the appearance but also their environment and dynamic behavior in detail. This allows the model to align the text description with the corresponding video frames effectively, leading to better identity and motion preservation during editing and composition. For example, in the dancer case shown in Figure 2 of the main paper, the prompt explicitly defines the appearance, surroundings, and action: \"A [v] man in black track pants, gray shirt, and cap near road on bridge with hands down and legs up. The man is performing [u] dancing motion with hands and legs.\" Here, the description of the attire (black track pants, gray shirt, and cap) ensures the model captures the subjects visual identity, while the mention of the environment (a road on bridge) provides spatial context. The prompt also includes details about the motion (\"dancing motion with hands and legs\"), guiding the model to encode temporal dynamics accurately."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. 2022. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571 (2022). [2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-A-Scene: Extracting Multiple Concepts from Single Image. arXiv preprint arXiv:2305.16311 (2023). [3] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. 2024. UniEdit: Unified Tuning-Free Framework for Video Motion and Appearance Editing. arXiv preprint arXiv:2402.13185 (2024). [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. 2024. Lumiere: spacetime diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers. 111. [5] Xiuli Bi, Jian Lu, Bo Liu, Xiaodong Cun, Yong Zhang, WeiSheng Li, and Bin Xiao. 2024. CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training. arXiv preprint arXiv:2412.15646 (2024). [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). [7] Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Huang, Tuanfeng Wang, and Gordon. Wetzstein. 2024. Generative Rendering: Controllable 4DGuided Video Generation with 2D Diffusion Models. In CVPR. [8] Ting Chen and Lala Li. 2023. FIT: Far-reaching Interleaved Transformers. arXiv preprint arXiv:2305.12689 (2023). [9] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Ivan Skorokhodov, Jun-Yan Zhu, Kfir Aberman, Ming-Hsuan Yang, and Sergey Tulyakov. 2024. VideoAlchemy: Open-set Personalization in Video Generation. https: //openreview.net/forum?id=popKM1zAYa [10] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. 2023. AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance. arXiv:2311.12886 [cs.CV] Dynamic Concepts Personalization from Single Videos 9 [11] Tri Dao. 2024. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In International Conference on Learning Representations (ICLR). [12] Google DeepMind. 2024. VEO2. https://deepmind.google/technologies/veo/veo-2/ (2024). [13] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou. 2022. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 10 (oct 2022), 59625979. https://doi.org/10.1109/tpami.2021. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. OpenReview.net. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. OpenReview.net. [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). [17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. 2023. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR. [18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2023. TokenFlow: Consistent Diffusion Features for Consistent Video Editing. arXiv preprint arxiv:2307.10373 (2023). [19] Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge, and Bo Zheng. 2024. AtomoVideo: High Fidelity Image-to-Video Generation. arXiv:arXiv:2403.01800 [cs.CV] [20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). [21] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. 2024. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103 (2024). [22] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. 2024. ID-Animator: Zero-Shot Identity-Preserving Human Video Generation. arXiv:2404.15275 [cs.CV] https://arxiv.org/abs/2404.15275 [23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022). [24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. 2022. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 (2022). [25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685 [26] Allan Jabri, David J. Fleet, and Ting Chen. 2023. Scalable adaptive computation for iterative generation. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML23). JMLR.org, Article 594, 21 pages. [27] Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, and Jun-Yan Zhu. 2024. Customizing Text-to-Image Models with Single Image Pair. arXiv:2405.01536 [cs.CV] https://arxiv.org/abs/2405.01536 [28] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. 2024. RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. [29] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. HunyuanVideo: Systematic Framework For Large Video Generative Models. arXiv preprint arXiv:2412.03603 (2024). [30] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. 2023. FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis. arXiv preprint arXiv:2312.17681 (2023). [31] Chang Liu, Viraj Shah, Aiyu Cui, and Svetlana Lazebnik. 2024. UnZipLoRA: Separating Content and Style from Single Image. arXiv:2412.04465 [cs.CV] https://arxiv.org/abs/2412.04465 [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. 2022. Flow straight and fast: arXiv preprint Learning to generate and transfer data with rectified flow. arXiv:2209.03003 (2022). , Vol. 1, No. 1, Article . Publication date: February 2025. [56] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing 568 (2024), 127063. https://doi.org/10.1016/j.neucom.2023.127063 [57] Kuan-Chieh Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, and Kfir Aberman. 2024. MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation. arXiv:2404.11565 [cs.CV] https: //arxiv.org/abs/2404.11565 [58] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. 2024. DreamVideo: Composing Your Dream Videos with Customized Subject and Motion. In CVPR. [59] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-AVideo: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. arXiv:2212.11565 [cs.CV] https://arxiv.org/abs/2212.11565 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv preprint arXiv:2408.06072 (2024). [61] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. 2024. Language Model Beats Diffusion - Tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations. https://openreview. net/forum?id=gzqrANCF4g [62] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. 2024. Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions. arXiv:2401.01827 [cs.CV] https://arxiv.org/ abs/2401.01827 [63] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. 2023. MotionCrafter: One-Shot Motion Customization of Diffusion Models. arXiv preprint arXiv:2312.05288 (2023). [64] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. 2023. MotionDirector: Motion Customization of Text-to-Video Diffusion Models. arXiv preprint arXiv:2310.08465 (2023). [65] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. 2023. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. arXiv:2304.11277 [cs.DC] https://arxiv.org/abs/2304.11277 [66] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. 2024. StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation. arXiv:2405.01434 [cs.CV] https://arxiv.org/abs/2405.01434 Abdal et al. [33] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. https://openreview.net/ forum?id=Bkg6RiCqY7 [34] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. 2024. Magic-Me: Identity-Specific Video Customized Diffusion. arXiv:2402.09368 [cs.CV] https://arxiv.org/abs/2402.09368 [35] Joanna Materzy≈Ñska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and Bryan Russell. 2024. NewMove: Customizing text-to-video models with novel motions. In Proceedings of the Asian Conference on Computer Vision. 16341651. [36] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, TsaiShien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. 2024. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 70387048. [37] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video Diffusion Models are General Video Editors. arXiv:2302.01329 [cs.CV] https://arxiv.org/ abs/2302.01329 [38] OPENAI. 2024. SORA. https://openai.com/sora/ (2024). [39] William Peebles and Saining Xie. 2023. Scalable Diffusion Models with Transformers. IEEE, 41724182. [40] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. 2024. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720 (2024). [41] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. 2023. FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. arXiv:2303.09535 (2023). [42] Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil Ostashev, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. 2024. Omni-ID: Holistic Identity Representation Designed for Generative Tasks. arXiv:2412.09694 [cs.CV] https://arxiv.org/abs/2412.09694 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res. 21, 1, Article 140 (Jan. 2020), 67 pages. [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022). [46] Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. 2024. ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation. arXiv preprint arXiv:2402.04324 (2024). [47] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. 2024. Customize-a-video: One-shot motion customization of text-to-video diffusion models. arXiv preprint arXiv:2402.14780 (2024). [48] Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. 2022. Pivotal tuning for latent-based editing of real images. ACM Transactions on Graphics (TOG) 42, 1 (2022), 113. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. 1068410695. [50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234241. [51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR. 2250022510. [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. 2023. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949 (2023). [53] Simo Ryu. 2023. DreamboothLoRA. https://github.com/cloneofsimo/lora [54] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. In NIPS. 3647936494. [55] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022). , Vol. 1, No. 1, Article . Publication date: February 2025."
        }
    ],
    "affiliations": [
        "SNAP RESEARCH"
    ]
}