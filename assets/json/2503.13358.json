{
    "paper_title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation",
    "authors": [
        "Daniil Selikhanovych",
        "David Li",
        "Aleksei Leonov",
        "Nikita Gushchin",
        "Sergei Kushneriuk",
        "Alexander Filippov",
        "Evgeny Burnaev",
        "Iaroslav Koshelev",
        "Alexander Korotin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 5 3 3 1 . 3 0 5 2 : r One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation Daniil Selikhanovych*1, 2, 4 David Li*1, 3, 5 Aleksei Leonov*3, 5 Nikita Gushchin*1, 6 Iaroslav Koshelev5 Sergei Kushneriuk1, 2, 5 Alexander Filippov5 Evgeny Burnaev1, 6 1 Skoltech Alexander Korotin1, 6 3 MIPT 2 HSE University 4 Yandex Research 5 AI Foundation and Algorithm Lab 6 AIRI"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models for super-resolution (SR) produce highquality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusionbased SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K. 1. Introduction Single image super-resolution (SR) [12, 16, 24] belongs to the category of inverse imaging problems aiming to reconstruct the high-resolution (HR) image given its lowresolution (LR) observation suffering from degradations. These degradations are usually complex and unknown for real-world scenarios when dealing with digital single-lens reflex cameras [3, 23, 61], referred to as the blind real-world SR problem. The SR problem is highly ill-posed, and many methods have been proposed in the literature to address it. *Equal contribution. Corresponding author: <selikhanovychdaniil@gmail.com> Skolkovo Institute of Science and Technology Moscow Institute of Physics and Technology Artificial Intelligence Research Institute Figure 1. comparison between the recent diffusion-based methods for SR - ResShift [72], SinSR [58], OSEDiff [62], SUPIR [70] - and the proposed RSD method. Our model has two advantages compared with other distillation-based models: (1) It achieves superior perceptual quality compared to SinSR; (2) It requires less computational resources compared to OSEDiff, see Tab. 4. (- behind the method name represents the number of inference steps, and the value in the bracket is MUSIQ [26] for full images). Please zoom in 5 times for better view. Recently, diffusion models have been developed for the blind SR problem [6, 7, 40, 42, 44, 55, 72] and became strong alternative for methods based on generative adversarial networks (GAN) [25, 57, 73] due to their good capabilities to learn complex data distributions [10]. The competitive perceptual quality of diffusion models for different real-world SR problems is also supported by bigger human evaluation preferences compared to GAN-based methods, as shown in [44, 55]. Early diffusion methods for SR constructed denoising process, which starts from Gaussian prior and ends in the HR image, while the LR image is used as condition for the input of the denoiser [40, 42, 44]. However, this strategy also leads to significant computational resources and slow inference time, requiring dozens or hundreds for the number of function evaluations (NFE) of the denoiser and limiting diffusion models based on these strategies from practically important real-time SR on consumer devices. Consequent research works for diffusionbased SR methods developed different approaches to accelerate those models while maintaining their high quality. Among them, ResShift [72] achieves perceptually high results in solving the real-world SR problem using only 15 NFE. This model surpasses or provides competitive performance when compared with state-of-the-art (SOTA) models from the other classes, including GANs [25, 33, 57, 73], transformers [31] and previous diffusion models [42]. Figure 2. Comparison among diffusion SR methods on RealSR. RSD (Ours) achieves top scores on most metrics while remaining computationally efficient compared to T2I methods such as OSEDiff[62] and SUPIR[70]. However, the inference time for ResShift still remains 10x times larger than that of GAN-based models, as shown in [72, Tab. 2]. The challenge arises when considering the problem of further acceleration of diffusion models while maintaining their perceptual quality at the same level. As shown in SinSR [58], ResShift exhibits degraded performance with artifacts if NFE is further reduced. To overcome this problem, SinSR [58] proposed knowledge distillation algorithm for ResShift in 1 NFE, which is based on the deterministic sampling formulation of the reverse process for ResShift inspired by DDIM sampling [48]. But SinSR tends to produce not satisfactory perceptual results with blurriness, as can be seen in the first row of Fig. 4 and was also pointed out in several recent works [14, 52, 62]. Another promising direction in acceleration of diffusion models for super-resolution is to add conditioning on the LR image to pre-trained text-to-image (T2I) models [4143] with LoRA [21] and distill them with variational score distillation [8, 60, 69] as proposed by OSEDiff [62]. While this approach greatly reduced NFE from tens or even hundreds to one across the class of T2I-based SR models [35, 55, 63, 67, 70] and achieved better perceptual results than ResShift and SinSR, we observe the following issues with T2I-based models for SR problem: (1) as we show Tab. 4, using computationally expensive T2I architectures like Stable Diffusion [41, 42] still leads to high 10 more parameters than computational cost and requires SinSR; (2) T2I-based models for SR also produce lower full-reference fidelity metrics such as PSNR and SSIM [59] when compared with ResShift and SinSR, as shown for various synthetic and real-world SR benchmarks in Tab. 2 and Tab. 3 , aligning with [52, 62]."
        },
        {
            "title": "Due to these issues of distillation methods for diffusion",
            "content": "SR, in our work we address the following 3 questions. 1. Are knowledge and variational score distillation the best candidates for achieving efficient 1-step diffusion SR? 2. Can we unite the best of two worlds for those distillation methods and achieve 1-step diffusion-based SR model that has good perceptual quality comparable to SOTA T2I-based diffusion SR models like OSEDiff and good fidelity preservation like SinSR at the same time? 3. Can we achieve this goal and avoid computationally demanding T2I models, bringing diffusion models closer to being deployed in practical SR scenarios with limited computational budget? Contributions. Our main contributions are as follows: (I) Theory. Inspired by the successful distillation of ResShift achieved by SinSR and recent progress in the distillation of image-to-image diffusion models [19], we propose novel objective for the 1-step distillation of the diffusion-based SR model and derive its tractable version. Motivated by ResShifts superior perceptiondistortion trade-off across SOTA diffusion-based models and its mathematically justified diffusion process, we build our method on top of it and name our method as RSD: Residual Shifting Distillation. (II) Practice. We show that our models trained with the proposed objective combined with additional supervised losses notably surpass the teachers results on the real world SR problem for various perceptual metrics, including LPIPS [76], CLIPIQA [53], and MUSIQ [26]. Our method aims to improve the compromise between fidelity, perceptual quality, and computational efficiency for diffusion realworld SR models in several aspects, see Fig. 2 and Tab. 4: 1. Perceptual quality. Compared to the other 1-step distillation-based method for ResShift - SinSR - our method achieves better perceptual quality on synthetic and real-world benchmarks for the blind SR problem. 2. Fidelity quality. Compared to the other 1-step diffusion SR model, which is based on pre-trained T2I models - OSEDiff - our method provides competitive perceptual results or even surpasses it while having better fidelity. 3. Computational efficiency. Similarly to SinSR, to bring diffusion models closer to real-time SR applications, our method suggests an alternative algorithm for the 1-step distillation of ResShift model, leading to lower computational budget compared to T2I-based SR models. 2. Related work GAN-based SR models. With the rise of GAN perspective [17], one line of research works adapted the GAN framework to the SR problem [28, 45, 57, 73] and achieved much better perceptual quality of the generated HR images than previously developed regression-based methods [12, 13, 27, 34, 77], which minimize the mean squared error (MSE) between the recovered HR image and the ground truth. Among those works, Real-ESRGAN [57] and BSRGAN [73] suggested effective degradation pipelines to synthesize explicitly LR-HR image pairs for modeling realworld data. Previous methods assumed pre-defined degradation process (e.g., bicubic), which leads to limited generalizability. The degradation pipelines of [57, 73] improved the results of GAN-based SR models for real-world images and have also been widely used by diffusion-based [55, 62, 72] and transformer-based SR models [31]. Diffusion-based SR Models. Existing methods, which adapt diffusion models [20, 47, 49, 50] for the blind SR problem, can be split into several categories depending on how they utilize the LR image. The first category of methods uses the LR image as an additional condition for the input of the denoiser and trains the denoiser from scratch [40, 42, 44]. The second category of methods utilizes unconditional to the LR image pre-trained diffusion priors and modifies the reverse process of diffusion models [6, 7, 55]. The third category of diffusion-based SR methods argues that big NFE is needed for those models due to the Gaussian prior, which is not optimal for an SR problem where the LR image already contains structural information about the HR image. Following this motivation, methods from the third category suggest starting the denoising process from the combination of the LR image and random noise while solving the blind SR problem [36, 71, 72]. As representative SR model from this class, ResShift [72] has several advantages: (1) it achieves SOTA results for blind real-world SR using only 15 NFE; at the same time methods [36, 71] considered only simple degradations and used hundreds of NFE; (2) similar to LDM [42], ResShift performs diffusion process in the latent space of an autoencoder [15], but also is 2-4 times faster and provides better perceptiondistortion trade-off than LDM. Acceleration of diffusion-based SR Models. While diffusion models surpass GANs in generative performance [10], their slow inference remains the key challenge. To mitigate this issue, various acceleration techniques have been proposed, with distillation emerging as one of the most effective approaches. These methods have also been extended to diffusion-based SR models. For instance, to further improve the efficiency of ResShift, SinSR [58] applied knowledge distillation [39] to its diffusion process. By introducing consistency-preserving loss that uses ground-truth data during training, SinSR achieved performance comparable to or better than the teacher ResShift model for blind real-world SR while requiring only 1 NFE. In our work, we draw inspiration from distillation techniques that involve training an auxiliary fake model [18, 22, 68, 79]. T2I-based SR models. However, as pointed out in several recent works [14, 62, 63], ResShift and SinSR show lower perceptual quality metrics and may fail to synthesize realistic structures when compared with other diffusion-based methods, which exploit pre-trained T2I diffusion models for blind real-world SR problem. The possible reason for this is the limited generalization of ResShift and SinSR, which is constrained due to the absence of large-scale data for the training. On the contrary, T2I models [4143] were trained on billions of image-text pairs and became the natural choice for applying to real-world SR. To adapt T2I models for SR problem, such methods usually have two (1) conditioning on the LR image is realcomponents: ized with T2I controllers such as LoRA layers [21] (OSEDiff [62]), ControlNet approach [75] (SeeSR [63], DiffBIR [35], SUPIR [70]) or other modules (StableSR [55], PASD [67]); (2) prompts for LR images is used as predefined (StableSR [55], DiffBIR [35]) or extracted with additional models such as DAPE [63] (SeeSR [63], OSEDiff [62]), LLaVA [37] (SUPIR [70]), or BLIP [29] (PASD [67]). However, such adaptations to image restoration also leads to different challenges. The first challenge is their computationally demanding requirements, as many methods utilizing pretrained T2I models for real-world SR require tens or even hundreds of NFE [35, 55, 63, 67, 70]. The recently developed one-step diffusion distillation methods utilize different ideas, including variational score distillation (VSD) [8, 60, 69] (OSEDiff [62]), adversarial diffusion distillation [46] (AddSR [64]), or target score distillation (TSDSR [14]). These methods significantly reduce the inference time of T2I-based SR models but do not solve the problem of inheritance of costly demanded T2I architectures with billion parameters. The second challenge of those models is their unstable predictions for the fixed input due to high dependence on noise initialization for the start of the denoising process, as pointed out in CCSR [52]. Such instability may lead to poor fidelity and random unfaithful details. 3. Method We start with recalling the ResShift formulation in 3.1. Then, we propose our method for distillation of the ResShift teacher model in one-step generator and derive its compu3.2. We expand the method tationally tractable form in for the multistep generator in 3.3 and add additional su- (cid:77) pervised losses in 3.4. We then combine everything and finalize the total objective for our RSD method in 3.5. (cid:77) (cid:77) (cid:77) (cid:77) Remark. While we derive our distillation method RSD for ResShift, we note that ResShift is essentially conditional DDPM [20] diffusion, where the forward process ends in Gaussian centered at the LR image. Therefore, our distillation method can be generalized, if needed, to any diffusion model built on the DDPM framework. 3.1. Background As part of the diffusion model class, ResShift can be described by specifying the forward (noising) process, the parametrization of the reverse (denoising) process, and the objective for training the reverse process. Forward process. ages (y0, x0) ResShift uses the forward process with Gaussian kernel: Consider pair of (LR, HR) imx0, pdata(y0, x0). For residual e0 = y0 q(xt xt1, y0) = (xt xt1 + αt e0, κ2αtI), (1) where αt = ηt t=1 is schedule, } while is hyper-parameter controlling the noise variance. The corresponding posterior distribution is given as: ηt1, α1 = η1 and { η q(xt xt,x0,y0) = xt1 (cid:18) (cid:12) (cid:12) (cid:12) ηt1 ηt xt + αt ηt x0, κ2ηt1 ηt (cid:19) αtI (2) Reverse process. ResShift suggests construction of the reverse process in the following parametrized form: pθ(x0 y0) = (cid:90) p(xT y0) (cid:89) t=1 pθ(xt1 xt, y0)dx1:T (3) (xT y0) = y0, κ2I) and pθ(xt Here p(xT xt, y0) is the reverse transition kernel from xt1 to xt approximated with Gaussian distribution with parameters µθ and Σθ. Objective. Σθ(xt, y0, t) reparametrizes the parameter µθ(xt, y0, t) as: the variance parameter to be independent of xt and y0 and"
        },
        {
            "title": "ResShift",
            "content": "sets (cid:124) µθ(xt, y0, t) = ηt1 ηt xt + αt ηt fθ(xt, y0, t), (4) where fθ is deep neural network with parameter θ, aiming to predict x0. The training objective then is: min θ Ep(x0,y0,xt) (cid:34) (cid:88) t=1 wt fθ(xt, y0, t) 2 (cid:35) , (5) where wt are some positive weights and p(x0, y0, xt) is provided by the forward process of ResShift. More detailed information on ResShift can be found in Appendix E. is to distill given ResShift 3.2. Residual Shifting Distillation (RSD) teacher model Our goal (xt, y0, t) into stochastic one-step student generator Gθ, which maps the LR image y0 to the HR image x0. To achieve this goal, we parametrize the generator (cid:98)x0 = Gθ(xT , y0, ϵ) to have three inputs: the LR image y0, its noisy version xT y0) and additional noise input xT , y0) the distribu- (ϵ ϵ tion of Gθ produced for given y0, xT and random ϵ. Then, y0), we force the generator to produce such data pθ((cid:98)x0 q(xT 0, I). We denote by pθ((cid:98)x0 that ResShift trained on it will coincide with the teacher model (xt, y0, t). We consider the following objective: (cid:88) t=1 (cid:124) wtEpθ((cid:98)x0,y0,xt) fGθ (xt, y0, t) (cid:123)(cid:122) def= Lθ 2 2 (xt, y0, t) (cid:125) , (6) where pθ((cid:98)x0, y0, xt) is provided by mapping LR image by generator (cid:98)x0 = Gθ(xT , y0, ϵ) and using posterior distirbution q(xt (cid:98)x0, y0) given in (2). In turn, fGθ (xt, y0, t) is the ResShift model trained on the generator data pθ((cid:98)x0 y0). θfGθ (xt, y0, t), The gradient θ includes the term which is not tractable since the backpropagation through the whole learning of the ResShift model fGθ (xt, y0, t) is computationally not feasible. To alleviate the problem, we propose another expression of θ θ: Proposition 3.1. For given teacher model (xt, y0, t), objective (6) can be expanded in tractable form as: θ = min ϕ (cid:110) (cid:88) t= wtEpθ((cid:98)x0,y0,xt) (cid:16) 2 fϕ(xt, y0, t) fϕ(xt, y0, t) (cid:123)(cid:122) This objective Lfake is equivalent to training fake model fϕ with objective (5). (xt, y0, t), (cid:98)x0 (cid:125) 2 2 2+ (xt, y0, t) (cid:17)(cid:111) . (7) Here, fϕ represents an additional ResShift model trained to optimize objective (7) in order to estimate θ. Furthermore, minimizing (7) over ϕ is equivalent to training fake ResShift model using data generated by Gθ. Thus, we alleviate the original problem of the intractable gradient of (6) by incorporating the loss of training of fake ResShift model into objective θ. We provide the proof of Proposition 3.1 in Appendix G. For completeness, in Appendix A, we also compare our method with another method involving fake models used in OSEDiff [62]. 3.3. Multistep RSD training To further improve the quality of images produced by our method, we consider the multistep training of the generator following previous diffusion distillation works [51, 68, 79]. < tN = We fix subset of timesteps 1 < t1 < and append additional time conditioning for the generator Gθ(xt, t, y0, ϵ). We denote by (cid:98)xtn 0 output of generator Gθ(xt, t, y0, ϵ) at timestep tn. In this setup the generator Gθ should approximate distributions pθ((cid:98)x0 xtn , y0) for all fixed timesteps tn instead of only apq(x0 proximation the distribution pθ((cid:98)x0 xT , y0) in one step training. For multistep training, we generate y0) using ground truth data distribution input data q(xtn y0) of LR and HR images and posterior distribution p(x0 (2). Then, we use the objective from Proposition 3.1 to train the generator for all tn simultaneously. At inference, we xtn , y0) xT , y0) q(x0 Figure 3. The training framework of RSD. We begin by encoding the (LR, HR) pair (y0, x0) into the latent space (zy, z0). First, to compute the LPIPS loss LLPIPS, we use zy to sample zT and generate the output (cid:98)z0 from timestep (following procedure of one-step inference), then decode it back to pixel space to obtain (cid:98)x0. Second, for the multistep distillation loss, we obtain input data ztn from the forward diffusion process in latent space (1) and generate (cid:98)ztn 0 . We then perform posterior sampling (2) to obtain zt, process it using both the fake and teacher ResShift models, and compute the distillation losses Lθ and Lfake from Proposition 3.1. Finally, to compute the GAN loss LGAN, we extract features from the encoder part of the fake U-Net model fϕ and use an additional discriminator head. Inspired by OSEDiff [62], we used LPIPS use single sampling step to maximize speed. This strategy shows better results than one-step training since training across multiple time steps appears to help the network learn more robust mappings (see Tab. 5). For consistency, we denote (cid:98)x0 single-step network output at the timestep . 3.4. Supervised losses In our distillation approach, we rely on the teachers prediction to guide the solution. However, this approach may yield suboptimal results due to inherent approximation errors in the estimation of x0. To mitigate this issue, we integrate additional losses into the distillation process. LPIPS Loss. loss in our approach. By employing LPIPS loss ( LPIPS [76]), we enable the student model to directly compare its output with the high-resolution ground truth in terms of perceptually features. This comparison helps the network to recover essential textures and structural details that might be missed when relying on the teachers guidance. Despite OSEDiff also used MSE loss for better fidelity alignment we found that it did not help in our setup. GAN Loss. In line with DMD2 [68], we integrate GAN loss into our framework. Incorporating the GAN loss enhances the student models capacity to align its predictions with the distribution of high-resolution images, thereby yielding overall superior image quality. Our minimalist design - adding classification branch to the bottleneck of the fake ResShift (see Fig. 3) - mirrors DMD2. While previous works [65, 68] implemented GAN loss for comparing marginal distributions of noised data and generator ouput, we notice, that using GAN loss to compare clean y0) with clean generator distridata distribution pdata(x0 bution pθ((cid:98)xtn 0 GAN = y0) at each timestep tn is more effective: (cid:2)log D(cid:0)x (cid:2)log D(cid:0) (cid:1)(cid:3). (8) y0 (cid:1)(cid:3) y0 pθ((cid:98)xtn 0 y0) (cid:98)xtn 0 pdata(x0y0) 3.5. Putting everything together Translation into Latent Space. Thus far, we assumed that loss functions operate in the image space (denoted as x), although the ResShift model was originally trained in the latent space (denoted as z). We also move our losses to the latent space, eliminating redundant latent encoding and decoding when computing losses. Specifically, we calculate GAN) in the latent the distillation loss ( space, while the LPIPS loss ( LPIPS) remains in the image space, as the LPIPS network was originally trained there. Final Algorithm. The final loss function for each tn is: θ) and GAN loss ( L θ + λ1 LPIPS + λ2 GAN (9) L detailed description of the complete algorithm is provided in Appendix B, with corresponding illustration in Fig. 3. (1) 4. Experiments In this section, we aim to achieve two main objecto demonstrate that our proposed distillatives: tion method outperforms existing distillation methods under the same experimental setup. We chose the setup of ResShift [72] due to its computational effectiveness. We show our enhancements compared to the current best SOTA Figure 4. Visual comparison on real-world samples from RealSet65 [72]. Please zoom in 5 times for better view. ResShift distillation method known as SinSR [58], and for comprehensive evaluation of our distillation method, we also implement OSEDiff VSD-based method applied to ResShift setup, called ResShift-VSD (see Appendix A); (2) to show that RSD achieves competitive perceptual performance to SOTA T2I-based SR methods such as OSEDiff [62] and SUPIR [70] while maintaining smaller architecture and better fidelity quality. These objectives are supported by evaluations using the experimental setups employed in SinSR and OSEDiff. We present two types of models: RSD (Ours, distill only), where we used only disθ loss during training, and RSD (Ours), where tillation we use distillation combined with additional losses ( 3.4). Appendix provides all relevant experiment details. (cid:77) 4.1. Experimental setup Training and Evaluation Details. For fair comparison, we follow the training setup of SinSR and ResShift, using 256 256 HR images randomly cropped from ImageNet [9] and generating LR images via the Real-ESRGAN [57] 4 SR factor. We also adopt degradation pipeline with the ResShift teacher model used in SinSR. For the evaluation, we follow two different protocols from SinSR and 4 SR factor). Following SinSR , we use the OSEDiff ( following datasets: (1) for real-world degradations, we use full-size images from RealSR [3] and RealSet65 [72]; (2) for synthetic degradations, we use ImageNet-Test [72]. Following OSEDiff , we use test sets of HR crops 512 512 from StableSR [55], including synthetic DIV2K-Val [1] and real-world pairs from RealSR and DRealSR [61]. Compared Methods. Our study follows two distinct experimental setups with different baseline comparisons. Following [58, Tab. 1 and Tab. 2], we incorporate several baselines from SinSR evaluation setups for real-world and synthetic datasets. In the main text, we compare our method against diffusion-based SR models: ResShift, SinSR, and additionally recent SOTA T2I-based SR models - 1-step OSEDiff [62] and multistep SUPIR [70]. In Appendix we provide quantitative results of other baselines, including GAN-based models [25, 56, 57, 73], SwinIR [31], LDM [42], and DASR [33]. For the OSEDiff evaluation setup, we compared our method in the main text against diffusionbased SR models, including ResShift, SinSR, OSEDiff, and SUPIR. We also compare RSD with other baselines in Appendix D, including multistep T2I-based [35, 55, 63, 67] and GAN-based SR methods [5, 32, 57, 73]. Metrics. Each setup employs different evaluation metrics, which we adopt from SinSR [58, Tab. 1 and Tab. 2] and"
        },
        {
            "title": "RealSR",
            "content": "RealSet65 SUPIR [70] OSEDiff [62] ResShift [72] SinSR (distill only) [58] SinSR [58] ResShift-VSD (Appendix A) RSD (Ours, distill only) RSD (Ours) 50 1 15 1 1 1 1 1 PSNR 24.38 25.25 26.49 26.14 25.83 23.96 24.92 25.91 SSIM 0.698 0.737 0.754 0.732 0.717 0.616 0.696 0.754 LPIPS 0.331 0.299 0.360 0.357 0.365 0.466 0.355 0. CLIPIQA 0.5449 0.6772 0.5958 0.6119 0.6887 0.7479 0.7518 0.7060 MUSIQ 63.676 67.602 59.873 57.118 61.582 63.298 66.430 65.860 CLIPIQA 0.6133 0.6836 0.6537 0.6822 0.7150 0.7606 0.7534 0.7267 MUSIQ 66.460 68.853 61.330 61.267 62.169 66.701 68.383 69.172 Table 1. Quantitative results of models on two real-world datasets. The best and second best results are highlighted in bold and underline. Methods SUPIR [70] OSEDiff [62] ResShift [72] SinSR (distill only) [58] SinSR [58] ResShift-VSD (Appendix A) RSD (Ours, distill only) RSD (Ours) NFE 50 1 15 1 1 1 1 1 PSNR 22.56 23.02 25.01 24.69 24.56 23.69 23.97 24.31 SSIM 0.574 0.619 0.677 0.664 0.657 0.624 0.643 0.657 LPIPS 0.302 0.253 0.231 0.222 0.221 0.230 0.217 0.193 CLIPIQA 0.786 0.677 0.592 0.607 0.611 0.665 0.660 0.681 MUSIQ 60.487 60.755 53.660 53.316 53.357 58.630 57.831 58. Table 2. Quantitative results of models on ImageNet-Test [72]. The best and second best results are highlighted in bold and underline. OSEDiff [62, Tab. 1]. For all evaluation setups from SinSR , we compute image-quality no-reference metrics CLIPIQA [54] and MUSIQ [26] following SinSR. For RealSR and ImageNet with available GT images, we additionally compute full-reference metrics, including fidelity metrics PSNR and SSIM [59], and perceptual metric LPIPS [76] following evaluation protocol on the ImageNet from In the OSEDiff configuration, evaluation [58, Tab. 2]. is conducted using fidelity metrics, including PSNR and SSIM, full-reference perceptual metrics, including LPIPS and DISTS [11], and no-reference image-quality metrics, including NIQE [74], MANIQA-PIPAL [66], MUSIQ, and CLIPIQA. We calculate PSNR and SSIM on the channel in the YCbCr space following SinSR and OSEDiff . 4.2. Experimental results Quantitative Comparisons. The key quantitative results are summarized in Tab. 1 , Tab. 2 , Tab. 3 and visualized in Fig. 2 . We make the following observations based on them. (1) Our model outperforms the teacher ResShift model and our closest competitor, SinSR, by large margin for all perceptual metrics (LPIPS, CLIPIQA, MUSIQ, DISTS, NIQE, MANIQA) and all test datasets while training on the same data. At the same time, it also has competitive fidelity metrics such as PSNR and SSIM. Furthermore, RSD demonstrates comparable or even better results than the implemented OSEDiff distillation method for the ResShift model, ResShift-VSD (Appendix A). (2) Compared to T2I-based OSEDiff and SUPIR models on realworld benchmarks, our model achieves the best value of the latest image-quality CLIPIQA and top-1 or top-2 results in terms of MUSIQ. Our model achieves worse CLIPIQA than SUPIR for synthetic datasets but better than OSEDiff. We hypothesize the gap with SUPIR is due to its multistep nature and rich SDXL prior [41], which leads to better details, poor consistency with the LR, and better preferences by no-reference metrics. We highlight that our model, even with slightly worse MUSIQ, achieves much better fidelity metrics than OSEDiff and SUPIR for most setups while utilizing much smaller number of parameters and (3) In Tab. 3 , we GPU memory, as shown in Tab. 4. show that our model achieves top-2 or top-1 perceptual reference (LPIPS, DISTS) and no-reference (NIQE, MUSIQ, MANIQA, CLIPIQA) metrics across all compared diffusion models. For some perceptual metrics, OSEDiff sometimes achieves better values. However, we highlight the dif-"
        },
        {
            "title": "Datasets",
            "content": "DIV2K-Val"
        },
        {
            "title": "RealSR",
            "content": "Methods SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) NFE PSNR 22.18 50 24.65 15 24.41 1 23.72 1 23.91 1 24.93 50 28.46 15 28.36 1 27.92 1 27.40 1 23.61 50 26.31 15 26.28 1 25.15 1 25.61 1 SSIM LPIPS 0.5303 0.3971 0.6181 0.3349 0.6018 0.3240 0.6108 0.2941 0.6042 0.2857 0.6360 0.4263 0.7673 0.4006 0.7515 0.3665 0.7835 0.2968 0.7559 0.3042 0.6606 0.3589 0.7421 0.3421 0.7347 0.3188 0.7341 0.2921 0.7420 0.2675 DISTS NIQE 0.2338 5.6806 0.2213 6.8212 0.2066 6.0159 0.1976 4.7097 0.1940 5.1987 0.2823 7.4336 0.2656 8.1249 0.2485 6.9907 0.2165 6.4902 0.2343 6.2577 0.2492 5.8877 0.2498 7.2365 0.2353 6.2872 0.2128 5.6476 0.2205 5.7500 MUSIQ 63.04 61.09 62.82 67.97 68.05 59.39 50.60 55.33 64.65 62.03 63.21 58.43 60.80 69.09 66.02 MANIQA 0.5861 0.5454 0.5386 0.6148 0.5937 0.5537 0.4586 0.4884 0.5899 0.5625 0.5895 0.5285 0.5385 0.6326 0. CLIPIQA 0.7085 0.6071 0.6471 0.6683 0.6967 0.6799 0.5342 0.6383 0.6963 0.7019 0.6709 0.5442 0.6122 0.6693 0.6793 Table 3. Quantitative results of models on crops 512 512 from [55]. The best and second best results are highlighted in bold and underline. Methods Inference Step Inference Time (s) # Total Param (M) Maximum GPU memory (MB) ResShift [72] 15 0.643 174 1167 SinSR [58] 1 0.060 174 SUPIR [70] OSEDiff [62] RSD (Ours) 1 0.075 1775 3651 50 17.704 4801 52535 1 0.059 174 539 Table 4. Complexity comparison among different methods. All methods are tested with an input LR image of size 64 64 for scale factor 4, and the inference time is measured on an NVIDIA A100 GPU. The best values are highlighted in bold. ferent training HR resolution of RSD with OSEDiff - we 256 as ResShift, while used HR crops of the size 256 OSEDiff used HR crops of the size 512 512 for training on LSDIR [30], which aligns with crop size in Tab. 3 . This discrepancy in training resolutions indicates that the evaluation setup is more naturally suited to OSEDiff, introducing potential bias in favor of OSEDiff that should be considered when interpreting the results. Additional quantitative results are presented in Appendix D. Qualitative Comparisons. We visually compare our method with SinSR, OSEDiff, ResShift, and SUPIR on several test images from RealSet65 in Fig. 4. As illustrated in the top two images - bicycle and vegetables - SUPIR tends to produce rich details that semantically dont correspond to the LR image. Please zoom in for excessive lines and broccolis, respectively. ResShift and SinSR produce more conservative images, which may struggle from severely blurred details like the houses roof on the bottom right image. OSEDiff also sometimes hallucinates excessive details, as can be seen for the bears nose in Fig. 4 and pandas nose in Fig. 1. Our method compromises between the good details of OSEDiff and SUPIR and the high fidelity of ResShift and SinSR. Additional visual results are presented in Appendix H. Complexity Comparisons. We compare the complexity of competing diffusion-based SR models in Tab. 4, including the number of inference steps, inference time, total number of parameters, and maximum required GPU memory during inference. All methods are tested on an NVIDIA A100 GPU with an HR image of size 256 256 following the training setup of ResShift, SinSR and RSD. We observe that 5 less GPU memory and RSD and SinSR require at least 10 less parameters than T2I-based models, which have highlights the efficiency of those models in terms of computational budget and can be useful for consumer devices. (cid:77) 4.3. Ablation study Multistep Training. We analyze the performance of our method under different timestep configurations in multistep training 3.3. As shown in Tab. 5, we compare various timestep ranging from 1 to 15 with the maximum number matching that of ResShift; timesteps are evenly placed. Selecting = 4 provides the optimal choice for the compromise between perceptual quality and distortion, which is known as perceptual-distortion trade-off [2]. Supervised Losses. Tab. 6 examines the impact of incorporating supervised losses, as discussed in 3.4. Our results show that adding these losses significantly enhances (cid:77) Timesteps 1 2 4 8 15 PSNR 24.82 24.77 24.92 25.63 25.91 SSIM 0.6730 0.6790 0.6956 0.7268 0.7435 LPIPS 0.4052 0.3772 0.3552 0.3199 0.2940 CLIPIQA 0.7444 0.7523 0.7518 0.7286 0. MUSIQ 64.290 65.760 66.430 66.445 65.689 Table 5. Impact of multistep training of our RSD on RealSR [3]. The best and second best results are highlighted in bold and underline quality in PSNR, SSIM and in LPIPS while introducing compromised yet acceptable changes in no-reference metrics (CLIPIQA, MUSIQ). In all evaluations, we use full-size images with real-world degradations from RealSR. Method RSD (distill only) w/ LPIPS w/ GAN RSD (Ours) PSNR 24.92 26.01 24.98 25.91 SSIM 0.6956 0.7531 0.7241 0. LPIPS 0.3552 0.2708 0.3064 0.2726 CLIPIQA 0.7518 0.7089 0.6970 0.7060 MUSIQ 66.430 65.178 67.615 65.860 Table 6. Effect of adding supervised losses on RealSR [3].The best and second best results are highlighted in bold and underline 5. Conclusion and future work In this work, we propose RSD, novel approach to distill the ResShift model into student network with single inference step. Our model is computationally efficient thanks to its ResShift framework but remains constrained by its teacher model. more advanced teacher, such as T2I-based model, could improve performance and enable the application of our method at higher resolutions."
        },
        {
            "title": "References",
            "content": "[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 126135, 2017. 6, 18, 19 [2] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62286237, 2018. 8 [3] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: In 2019 IEEE/CVF Innew benchmark and new model. ternational Conference on Computer Vision (ICCV), pages 30863095, 2019. 1, 6, 9, 18, 19, 26, 27 [4] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch [Online]. Availtoolbox for image quality assessment. able: https : / / github . com / chaofengc / IQA - PyTorch, 2022. 18 [5] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo. Real-world blind super-resolution via feature matching with implicit highresolution priors. 2022. 6, 19 [6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1434714356, 2021. 1, [7] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1240312412, 2022. 1, 3 [8] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In Computer Vision ECCV 2024, pages 176192, Cham, 2025. Springer Nature Switzerland. 2, 3 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 6, 18 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pages 87808794. Curran Associates, Inc., 2021. 1, 3 [11] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 7 [12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2):295307, 2016. 1, [13] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the super-resolution convolutional neural network. In Computer Vision ECCV 2016, pages 391407, Cham, 2016. Springer International Publishing. 3 [14] Linwei Dong, Qingnan Fan, Yihong Guo, Zhonghao Wang, Qi Zhang, Jinwei Chen, Yawei Luo, and Changqing Zou. Tsd-sr: One-step diffusion with target score distillation arXiv preprint for real-world image super-resolution. arXiv:2411.18263, 2024. 2, 3 [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1286812878, 2021. 3 [16] Daniel Glasner, Shai Bagon, and Michal Irani. SuperIn 2009 IEEE 12th Interresolution from single image. national Conference on Computer Vision, pages 349356, 2009. 1 [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. [18] Nikita Gushchin, David Li, Daniil Selikhanovych, Evgeny and Alexander Korotin. arXiv preprint Burnaev, Dmitry Baranchuk, Inverse bridge matching distillation. arXiv:2502.01362, 2025. 3 [19] Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, and Jun In Advances Zhu. Consistency diffusion bridge models. in Neural Information Processing Systems, pages 23516 23548. Curran Associates, Inc., 2024. 2 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising difIn Advances in Neural Inforfusion probabilistic models. mation Processing Systems, pages 68406851. Curran Associates, Inc., 2020. 3, 22 [21] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3 [22] Zemin Huang, Zhengyang Geng, Weijian Luo, and GuoarXiv preprint Flow generator matching. jun Qi. arXiv:2410.19310, 2024. 3 [23] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, and Kenneth Vanhoey. Dslr-quality photos on mobile devices with In 2017 IEEE International deep convolutional networks. Conference on Computer Vision (ICCV), pages 32973305, 2017. 1 [24] Michal Irani and Shmuel Peleg. Improving resolution by image registration. Graphical Models and Image Processing, 53:231239, 1991. 1 [25] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via kernel In 2020 IEEE/CVF Conestimation and noise injection. ference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 19141923, 2020. 1, 2, 6, 19, 20 [26] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 51285137, 2021. 1, 2, 7 [27] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 16461654, 2016. 3 [28] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using In 2017 IEEE Conference generative adversarial network. on Computer Vision and Pattern Recognition (CVPR), pages 105114, 2017. [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. 3 [30] Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Lsdir: large scale dataset for image restoraIn 2023 IEEE/CVF Conference on Computer Vision tion. and Pattern Recognition Workshops (CVPRW), pages 1775 1787, 2023. 8 [31] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration In 2021 IEEE/CVF International using swin transformer. Conference on Computer Vision Workshops (ICCVW), pages 18331844, 2021. 2, 3, 6, 18, 19, 20 [32] Jie Liang, Hui Zeng, and Lei Zhang. Details or artifacts: locally discriminative learning approach to realistic image In 2022 IEEE/CVF Conference on Comsuper-resolution. puter Vision and Pattern Recognition (CVPR), pages 5647 5656, 2022. 6, 19 [33] Jie Liang, Hui Zeng, Efficient and degradation-adaptive network for real-world image super-resolution. In Computer Vision ECCV 2022, pages 574591, Cham, 2022. Springer Nature Switzerland. 2, 6, 19, 20 and Lei Zhang. [34] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 11321140, 2017. 3 [35] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Yu Qiao, Wanli Ouyang, and Chao Dong. Diffbir: Toward blind image restoration with generative diffusion In Computer Vision ECCV 2024, pages 430448, prior. Cham, 2025. Springer Nature Switzerland. 2, 3, 6, 19 [36] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. I2SB: In Proceedings of the Image-to-image schrodinger bridge. 40th International Conference on Machine Learning, pages 2204222062. PMLR, 2023. 3 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, pages 3489234916. Curran Associates, Inc., 2023. 3 [38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 18 [39] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 3 [40] Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B. Schon. Image restoration with mean-reverting stochastic differential equations. In Proceedings of the 40th International Conference on Machine Learning, pages 2304523066. PMLR, 2023. 1, [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 2, 3, 7 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, 2022. 1, 2, 3, 6, 19, 20 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep lanIn Advances in Neural Information guage understanding. Processing Systems, pages 3647936494. Curran Associates, Inc., 2022. 2, 3 [44] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4713 4726, 2023. 1, 3 [45] Mehdi S. M. Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through In 2017 IEEE International automated texture synthesis. Conference on Computer Vision (ICCV), pages 45014510, 2017. 2 [46] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In Computer Rombach. Adversarial diffusion distillation. Vision ECCV 2024, pages 87103, Cham, 2025. Springer Nature Switzerland. [47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In Proceedings of the nonequilibrium thermodynamics. 32nd International Conference on Machine Learning, pages 22562265, Lille, France, 2015. PMLR. 3 [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 2 [49] Yang Song and Stefano Ermon. Generative modeling by esIn Advances in timating gradients of the data distribution. Neural Information Processing Systems. Curran Associates, Inc., 2019. 3 [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 3 [51] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya In Proceedings of the Sutskever. Consistency models. 40th International Conference on Machine Learning, pages 3221132252. PMLR, 2023. 4 [52] Lingchen Sun, Rongyuan Wu, Zhengqiang Zhang, Hongwei Yong, and Lei Zhang. Improving the stability of diffusion models for content consistent super-resolution. arXiv preprint arXiv:2401.00877, 2024. 2, [53] Jianyi Wang, Kelvin C.K. Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. Proceedings of the AAAI Conference on Artificial Intelligence, 37(2):25552563, 2023. 2 [54] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, pages 25552563, 2023. 7 [55] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 2024. 1, 2, 3, 6, 8, 18, 19, 21 [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Computer Vision ECCV 2018 Workshops, pages 6379, Cham, 2019. Springer International Publishing. 6, 19, 20 [57] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 19051914, 2021. 1, 2, 3, 6, 18, 19, 20, 26 [58] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex C. Kot, and Bihan Wen. Sinsr: Diffusion-based image superresolution in single step. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2579625805, 2024. 1, 2, 3, 6, 7, 8, 18, 19, 20, 21, 26 [59] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 2, [60] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan LI, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Advances in Neural Information Processing Systems, pages 84068441. Curran Associates, Inc., 2023. 2, 3, 14 [61] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divideand-conquer for real-world image super-resolution. In Computer Vision ECCV 2020, pages 101117, Cham, 2020. Springer International Publishing. 1, 6, 18, 19, 26, 28 [62] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image In Advances in Neural Information Prosuper-resolution. cessing Systems, pages 9252992553. Curran Associates, Inc., 2024. 1, 2, 3, 4, 5, 6, 7, 8, 14, 17, 18, 19, 20, 21, 26 [63] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semanticsaware real-world image super-resolution. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2545625467, 2024. 2, 3, 6, 19 [64] Rui Xie, Ying Tai, Kai Zhang, Zhenyu Zhang, Jun Zhou, and Jian Yang. Addsr: Accelerating diffusion-based blind superresolution with adversarial diffusion distillation, 2024. 3 [65] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generIn Proceedings of the IEEE/CVF ation via diffusion gans. Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. 5 [66] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference In 2022 IEEE/CVF Conference image quality assessment. on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 11901199, 2022. 7 [67] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. In Computer Vision ECCV 2024, pages 7491, Cham, 2025. Springer Nature Switzerland. 2, 3, 6, [68] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. In Advances in Neural Information Processing Systems, pages 4745547487. Curran Associates, Inc., 2024. 3, 4, 5, 18 [69] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66136623, 2024. 2, 3, 14 [70] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong. Scaling up to excellence: Practicing model scaling for photorealistic image restoration in the wild. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2566925680, 2024. 1, 2, 3, 6, 7, 8, 18, 19, 20, 21, 26 [71] Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du, Pengxu Wei, and Dongyu Zhang. Image restoration through generalized ornstein-uhlenbeck bridge. In Proceedings of the 41st International Conference on Machine Learning, pages 5806858089. PMLR, 2024. 3 Efficient diffusion model [72] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. image superResshift: In Advances in Neural Inresolution by residual shifting. formation Processing Systems, pages 1329413307. Curran Associates, Inc., 2023. 1, 2, 3, 5, 6, 7, 8, 18, 19, 20, 21, 23, 26 for [73] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind In 2021 IEEE/CVF International image super-resolution. Conference on Computer Vision (ICCV), pages 47714780, 2021. 1, 2, 3, 6, 18, 19, 20, 26 [74] Lin Zhang, Lei Zhang, and Alan Bovik. feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):25792591, 2015. 7 [75] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 38133824, 2023. 3 [76] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In 2018 IEEE/CVF deep features as perceptual metric. Conference on Computer Vision and Pattern Recognition, pages 586595, 2018. 2, 5, 7 [77] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Computer Vision ECCV 2018, pages 294310, Cham, 2018. Springer International Publishing. 3 [78] Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024. [79] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Proceedings of the 41st International Conference on Machine Learning, pages 62307 62331. PMLR, 2024. 3, 4,"
        },
        {
            "title": "Supplementary materials and its structure",
            "content": "We organize the structure of supplementary materials as follows: 1. Appendix includes the derivation of variational score distillation for ResShift and its comparison with Lθ. 2. Appendix details the correspondence between propositions and their implementation with the pseudocode of RSD. We also present the pseudocode for ResShift-VSD which introduced in Appendix 3. Appendix consists of experimental details for the implementation of RSD and baselines. 4. Appendix consists of full quantitative results including additional baselines and results on full-size DRealSR, which werent shown in the main text due to space limitations. 5. Appendix includes additional details of ResShift, which have not been shown in the main text due to space limitations. 6. Appendix presents proofs of the main propositions in the paper. 7. Appendix discusses the limitations of RSD and failure cases. 8. Appendix contains additional visual results for comparison between RSD and baselines. A. Derivation of VSD objective for ResShift (ResShift-VSD) and comparative analysis with our objective. θ L"
        },
        {
            "title": "VSD",
            "content": "Illustration of the distinct distribution alignment strategies employed by the Lθ and VSD loss functions. We denote by Figure 5. p(x0:T y0) reverse process of teacher ResShift model and by p(x0:T y0) reverse process of ResShift trained on generator Gθ data. The Lθ loss enforces alignment of the joint distributions p(x0:T y0) and p(x0:T y0) across all timesteps, whereas the VSD loss aligns the marginal distributions at each timesteps simultaneously between distributions of teacher ResShift and ResShift trained on generator Gθ data. For formal derivations, see Eqs. (18) and (12). In this section, we aim to: (1) derive the VSD loss in the ResShift framework to compare it with our distillation loss under the same experimental conditions (see Tab. 1 and Tab. 2); and (2) explain the main differences between our approach and the VSD loss. To achieve this, we consider generator Gθ with parameters θ and seek an update rule for them. We use fake ResShift model to solve the following problem: Since it is the optimization with MSE function the solution is given by the conditional expectation: arg min (cid:88) t= wtEpθ ((cid:98)x0,y0,xt) (cid:2)f (xt, y0, t) (cid:98)x02 (cid:3), fGθ (xt, y0, t) = Epθ ((cid:98)x0y0,xt)[(cid:98)x0]. Notation. Further we will use the following notation: teacher ResShift. xt1:t def= (xt1 , xt1+1, . . . , xt2 ) and dxt1:t2 def= t2(cid:81) i=t1 dxi for any integer t1 < t2. The joint distribution across all timesteps is defined as follows: p(x0:T y0) def= p(xT y0) ities are determined: p(xt1xt, y0) = (xt1 ηt1 ηt xt + αt ηt fGθ (xt, y0, t), κ2 ηt1 ηt p(x0:T y0) def= p(xT y0) (cid:81) t=1 p(xt1xt, y0), where the transition probabilities are determined using . (cid:81) t= p(xt1xt, y0). The transition probabilαtI) using Eq. (2). In the same way we define (10) (11) p(xty0) def= (cid:82) p(x0:T y0)dx0:t1dxt1:T and p(xty0) def= (cid:82) p(x0:T y0)dx0:t1dxt2:T are marginal distributions. Derivation of VSD Loss for ResShift (ResShift-VSD). Initially, main objective of VSD loss [60, 62, 69] is: LVSD = (cid:88) wtDKL (p(xty0)p(xty0)) . (12) We can get another expression for this loss using reparametrization based on the Eq. (20): LVSD = (cid:88) wtDKL (p(xty0)p(xty0))) = (cid:88) wtEp(xty0) log p(xty0) p(xty0) (cid:88) = wt xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) log p(xty0) p(xty0) (13) Initially, this loss is intractable because it requires computing probability densities. However, taking the gradient facilitates its computation: θLVSD = (cid:88) t=1 wt (cid:20) xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (xt log p(xty0) xt log p(xty0)) (cid:21) = dxt dθ (cid:88) t= wt (cid:20) xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (xt log p(xty0) xt log p(xty0)) (cid:21) = dxt d(cid:98)x0 d(cid:98)x0 dθ (cid:88) t=1 t (cid:20) xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (xt log p(xty0) xt log p(xty0)) (cid:21) , d(cid:98)x0 dθ where def= wt dxt d(cid:98)x0 = wt(1 ηt). By applying the reparametrization from [78, Proposition 3.2], the expression xt log p(xty0) can be utilized as follows: xt log p(xty0) = p(x0xt,t,y0) (cid:104) xt ηty0 (1 ηt)x0 κ2ηt (cid:105) which leads to: θLVSD = (cid:88) t=1 E xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (cid:20) (f (xt, y0, t) fGθ (xt, y0, t)) (cid:21) d(cid:98)x0 dθ (14) (15) (16) (17) def= where t ResShift-VSD. Reformulation of Lθ loss. We can express our loss function RSD as: 1ηt κ2ηt . As result this loss can be implemented to match the gradients with θLVSD (see Algorithm 2). We call this model Lθ = DKL (p(x0:T y0)p(x0:T y0)) (18) Since this loss can be decomposed as: Lθ = DKL (p(x0:T y0)p(x0:T y0)) = DKL (p(xT y0)p(xT y0)) (cid:125) (cid:123)(cid:122) (cid:124) =0 since p(xT y0) = p(xT y0) from Eq. (20) + 1 (cid:88) t=1 p(xty0) DKL (p(xt1xt, y0)p(xt1xt, y0)) (cid:125) (cid:123)(cid:122) (cid:124) (cid:13) 2 (cid:13) = 1 (xt,y0,t)f (xt,y0,t) (cid:13) 2κ (cid:13) (cid:13) (cid:13)fGθ αt ηtηt1 This derivation demonstrates that the loss function reconstructs the initial objective presented in Eq. (6). Conceptual comparison of VSD and Lθ losses. The key difference between the VSD and Lθ losses lies in how they match distributions. For more clear intuitive explanation one can see on formulations of losses with DKL for VSD (Eq. (12)) and Lθ (Eq. (18)). The VSD loss aligns the marginal distributions at each timestep between the teachers and fakes distributions. In contrast, the Lθ loss matches the joint distribution across all timesteps. This difference is illustrated in Fig. 5, where the Lθ loss enforces joint distribution alignment, while the VSD loss aligns marginal distributions separately and then sums them. Computational analysis of VSD and Lθ losses. As was shown in Proposition 3.1, our loss is equal to: Lθ = min ϕ (cid:110) (cid:88) wtEpθ ((cid:98)x0,xt,y0) (cid:16) fϕ(xt, y0, t)2 2 (xt, y0, t)2 2 + 2f (xt, y0, t)fϕ(xt, y0, t), (cid:98)x0 (cid:17)(cid:111) . Using Eq. (11) we can rewrite it and make reparametrization: Lθ = (cid:88) wtEpθ ((cid:98)x0,xt,y0) (cid:16) fGθ (xt, y0, t)2 2 (xt, y0, t)2 2 + 2f (xt, y0, t)fGθ (xt, y0, t), (cid:98)x0 (cid:17) = (cid:88) wt xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (cid:16) fGθ (xt, y0, t)2 2 (xt, y0, t) 2 + 2f (xt, y0, t)fGθ (xt, y0, t), (cid:98)x0 (cid:17) To compare it with VSD loss, we can take the gradient from Lθ loss and get: dLθ dθ = (cid:88) wt xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (cid:16) dfGθ (xt, y0, t)2 dθ 2 df (xt, y0, t)2 2 dθ + 2 df (xt, y0, t) dθ dfGθ (xt, y0, t) dθ , (cid:98)x0 + (cid:88) wt xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (cid:16) dfGθ (xt, y0, t)2 dθ 2 df (xt, y0, t)2 2 dθ + 2 df (xt, y0, t) dθ 2f (xt, y0, t)fGθ (xt, y0, t), d(cid:98)x0 dθ dfGθ (xt, y0, t) dθ (cid:17) = (cid:17) , (cid:98)x0 (cid:88) wt (cid:124) xt=(1ηt)(cid:98)x0+ηty0+κ2ηtϵ (cid:98)x0=Gθ (y0,ϵ) ϵ,ϵN (0;I) (cid:16) 2f (xt, y0, t)fGθ (xt, y0, t), (cid:123)(cid:122) =2αtθ LVSD (cid:17) d(cid:98)x0 dθ (cid:125) Consequently, gradients of Lθ loss contain gradients of VSD loss up to some constant 2αt but preserve some additional gradients from teacher and fake models. To convert Lθ loss to VSD, one can use the stopgradient operator to stop the calculation of additional gradients, see Algorithm 2 for details. B. Algorithm of RSD The pseudocode for our RSD training algorithm is presented in Algorithm 1. Algorithm 1: Residual Shifting Distillation (RSD). Input: Training dataset pdata(x0, y0); Pretrained ResShift Teacher model ; Frozen encoder and decoder of VAE: Enc, Dec; Fixed timesteps 1 < t1 < < tN = T.; Number of fake ResShift (fϕ) training iterations K; Output: trained generator Gθ; 1 func SampleEverything() 33 Sample (x0, y0) pdata(x0, y0) zy Enc(upsample(y0)) z0 Enc(x0) Sample tn {t1, . . . , tN }, ztn q(ztn z0, zy), ϵ (0, I) // Eq. (cid:98)ztn 0 Gθ(ztn , y0, tn, ϵ) Sample {0, . . . , }, zt q(zt(cid:98)ztn return (x0, y0, z0, zy, tn, ztn , (cid:98)ztn 0 , t, zt) 0 , zy) // Eq. (20) (20) 16 17 // Initialize generator from pretrained model 18 // Initalize fake ResShift from pretrained model 19 // and GAN discriminator head randomly 20 Gθ copyWeightsAndUnfreeze(f ); 21 fϕ copyWeightsAndUnfreezeAndAddNoiseChannels(f ) // See Appendix 22 Dψ randomInitOfDiscriminatorHead() 23 24 while train do 25 // Train fake ResShift model for 1 to do 0 , t, zt) SampleEverything() // Generate data for training 0 , y0, 0)), Dψ(f encoder ϕ (z0, y0, 0))) // Eq. (8) (x0, y0, z0, zy, tn, ztn , (cid:98)ztn Lfake wtfϕ(zt, y0, t) (cid:98)ztn LGAN calcGANLossD(Dψ(f encoder Ltotal ϕ Lfake + λ2LGAN // Eq. (9) Ltotal ϕ ϕ 0 2 ϕ Update ϕ by using Update ψ by using LGAN ψ 2 // Eq. (5) ((cid:98)ztn end for // Train Generator Model // Generate data for training (x0, y0, z0, zy, tn, ztn , (cid:98)ztn 0 , t, zt) SampleEverything() // Compute Lθ loss Lθ calcThetaLoss(f (zt, y0, t), fϕ(zt, y0, t), (cid:98)ztn 0 ) // Eq. (7) // Compute LLPIPS loss Sample zT (zT zy, κ2I) // Eq. (cid:98)z0 Gθ(zT , y0, T, ϵ) LLPIPS LPIPS(x0, Dec((cid:98)z0)) (20) // Compute generator LGAN loss ((cid:98)ztn LGAN calcGANLossG(Dψ(f encoder ϕ 0 , y0, 0))) // Eq. (8) Ltotal θ Lθ + λ1LLPIPS + λ2LGAN // Eq. (9) Update θ by using 51 52 end while Ltotal θ θ 55 77 99 1313 1515 26 27 28 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 The pseudocode for the proposed ResShift-VSD training algorithm is presented in Algorithm 2, while the foundational theoretical framework is detailed in Appendix A. To ensure fair comparison with the distillation loss in OSEDiff [62], specifically the VSD Loss, under an identical experimental setup (i.e., ResShift), we adapted it to the ResShift framework using the same implementation details. Algorithm 2: ResShift-VSD. Input: Training dataset pdata(x0, y0); Pretrained ResShift Teacher model ; Frozen encoder and decoder of VAE: Enc, Dec; Number of fake ResShift (fϕ) training iterations K; Output: trained generator Gθ; 1 func SampleEverything() Sample (x0, y0) pdata(x0, y0); zy Enc(upsample(y0)) Sample zT (zy, κ2ηT I) // Eq. (20) (cid:98)z0 Gθ(zT , y0, ) Sample {0, . . . , }, zt q(zt(cid:98)z0, zy) // Eq. return (y0, t, zt, (cid:98)z0) (20) 14 15 // Initialize generator from pretrained model 16 // Initialize fake ResShift from pretrained model 17 Gθ copyWeightsAndUnfreeze(f ); 18 fϕ copyWeightsAndUnfreeze(f ); 19 20 while train do 21 // Train fake ResShift model for 1 to do 55 99 1111 1313 22 23 25 26 27 28 29 (y0, t, zt, (cid:98)z0) SampleEverything() // Generate data for training Lfake wtfϕ(zt, y0, t) (cid:98)z02 Update ϕ by using Lfake ϕ 2 // Eq. (5) end for // Train Generator Model (y0, t, zt, (cid:98)z0) SampleEverything() // Generate data for training Lθ calcThetaLoss(stopgrad( (zt, y0, t) ), stopgrad( fϕ(zt, y0, t) ), (cid:98)z0) // Eq. Update θ by using Lθ θ (7) 31 32 end while C. Experiments details (cid:77) Noise Condition. By default, fake ResShift and generator models are initialized with teacher weights. Furthermore, for noise conditioning, as described in 3.2, we implement an extra convolutional channel to expand the generators first convolutional layer to accept noise as an additional input. The noise is concatenated with the encoded low-resolution image and is processed by separate zero-initialized convolutional layer. Training hyperparameters. We use the same hyperparameters as SinSR for training, including batch size, EMA rate, and optimizer type. To achieve smoother convergence, we replace the learning rate scheduler with constant learning rate of 5 105, matching the base learning rate of SinSR. Additionally, we adjust the AdamW [38] optimizers β parameters to [0.9, 0.95] to further stabilize training. To ensure controlled adaptation between the generator and the fake ResShift models, we update the generators weights once for every = 5 updates of the fake model, following the strategy in DMD2 [68]. Furthermore, we adopt the loss normalization technique proposed in [79] to improve training stability. In the final loss function (Eq. 9) we set λ1 = 2 and λ2 = 3 103 following OSEDiff [62] and DMD2, respectively. Training time. The complete training process, performed on 4 NVIDIA A100 GPUs, takes approximately 5 hours. During this time, the student model undergoes around 3000 gradient update iterations, while the fake model completes 15000 iterations. Codebase. Our method is implemented based on the original SinSR repository [58], which serves as the primary code source for our experiments. We build upon this framework to integrate our training algorithm, which is described in Appendix B. Datasets and baselines. Table 7 lists details on the datasets used for training and testing, including their sources, download links, and associated licenses. Table 8 lists the models used for training and quality comparison and links to access them. Metrics calculation of SR models. For calculating SR metrics, we use PyTorch Toolbox for Image Quality Assessment and pyiqa package [4]. We also used the image quality assessment script provided in the OSEDiff GitHub repository. URL Name GitHub Link RealSR-V3 GitHub Link RealSet65 GitHub Link DRealSR Website Link ImageNet Google Drive Link ImageNet-Test DIV2K-Val-512 Hugging Face Link Hugging Face Link DRealSR-512 Hugging Face Link RealSR-512 License Citation ODbL v1 [3] NTU S-Lab License 1.0 [72] - [61] Custom (research, non-commercial) [9] NTU S-Lab License [72] [1, 55] NTU S-Lab License [55, 61] NTU S-Lab License NTU S-Lab License [3, 55] Table 7. The used datasets and their licenses."
        },
        {
            "title": "URL",
            "content": "Name Real-ESRGAN GitHub Link GitHub Link BSRGAN GitHub Link SwinIR GitHub Link ResShift GitHub Link SinSR GitHub Link SUPIR GitHub Link OSEDiff Citation [57] [73] [31] [72] [58] [70] [62] License BSD 3-Clause License Apache-2.0 license Apache-2.0 license S-Lab License 1.0 CC BY-NC-SA 4.0 SUPIR Software License Apache License 2.0 Table 8. The used baselines we compare our method with. In each case, we used original code from Github repositories and model weights as it is. D. Additional quantitative results We present the additional set of quantitative results, including additional baselines and evaluations on full-size DRealSR images [61], which were not included in the main text due to space limitations: Tab. 9 provides results on full-size images from the DRealSR dataset [61]. Tab. 10 presents an extension version of Tab. 1 on RealSR [3] and RealSet65 [72] datasets with additional baselines. Tab. 11 presents an extension version of Tab. 2 on the ImageNet-Test dataset [72] with additional baselines. Tab. 12 presents an extension version of Tab. 3 on crops from DIV2K [1], RealSR, and DRealSR used in StableSR [55] with additional baselines. Tab. 9. We evaluated the following models for Tab. 9 and followed their official implementations listed in Tab. 8: 1. Diffusion-based SR models. We ran pre-trained models of ResShift [72], SinSR [58], OSEDiff [62], and SUPIR [70] as representative members of diffusion-based SR models. We used the following checkpoints from the respective official repositories listed in Tab. 8: resshift realsrx4 s15 v1.pth, SinSR v2.pth, osediff.pkl, and SUPIR-v0Q.ckpt. Due to the high resolution of DRealSR images and the high demand for GPU memory for the SUPIR model, we ran it with tiled VAE using the flag --use tile vae. 2. GAN-based SR models. We ran pre-trained Real-ESRGAN [57] and BSRGAN [73] GAN-based SR models with the checkpoint names RealESRGAN x4plus.pth and BSRGAN.pth, which were provided in the respective GitHub repositories listed in Tab. 8. 3. Transformer-based SR models. We ran pre-trained SwinIR model [31] with the checkpoint name 003 realSR BSRGAN DFOWMFC s64w8 SwinIR-L x4 GAN.pth as the representative model from transformer-based SR models using the respective GitHub repository listed in Tab. 8. We compute the same set of metrics as in Tab. 3 - PSNR, SSIM, LPIPS, CLIPIQA, MUSIQ, DISTS, NIQE, and MANIQA-PIPAL. Tab. 10. We report an extended version of Tab. 1 with additional baselines used in ResShift and SinSR papers: 1. GAN-based SR models. We evaluated Real-ESRGAN [57] and BSRGAN [73] on RealSR and RealSet65. 2. SwinIR. We also evaluated SwinIR on RealSR and RealSet65. Tab. 11. We report an extended version of Tab. 2 with additional baselines used in ResShift and SinSR papers: 1. Diffusion-based SR models. We borrow results of Tab. 2 from SinSR for LDM-15 and LDM-30 [42], and SinSR [58]. We borrow the results of Tab. 3 from [72] for ResShift. 2. GAN-based SR models. We borrow the results of Tab. 2 from SinSR for ESRGAN [56], RealSR-JPEG [25], Real-ESRGAN [57], BSRGAN [73]. 3. DASR and SwinIR. We also borrow results of Tab. 2 from SinSR for DASR [33] and SwinIR [31]. Tab. 12. We report an extended version of Tab. 3 with additional baselines used in the OSEDiff paper: 1. Diffusion-based SR models. We borrow the results of Tab. 1 from OSEDiff for StableSR [55], DiffBIR [35], SeeSR [63], PASD [67], ResShift [72], and SinSR [58]. 2. GAN-based SR models. We borrow the results of Tab. 1 from OSEDiff for Real-ESRGAN [57], BSRGAN [73], LDL [32], and FeMASR [5]. Methods ResShift [72] SinSR [58] OSEDiff [62] SUPIR [70] Real-ESRGAN [57] BSRGAN [73] SwinIR [31] RSD (Ours) NFE PSNR 28.76 15 27.32 1 26.67 1 25.73 50 27.91 1 28.34 1 28.31 1 27.66 1 SSIM LPIPS 0.7863 0.4310 0.7233 0.4452 0.7922 0.3123 0.7224 0.3906 0.8249 0.2818 0.8206 0.2929 0.8272 0.2741 0.7864 0. CLIPIQA 0.5838 0.7223 0.7264 0.5862 0.5180 0.5704 0.5072 0.7398 MUSIQ 32.042 32.800 37.761 36.089 35.255 35.500 35.826 38.340 DISTS NIQE 0.2314 6.6335 0.2368 5.5748 0.1617 4.1768 0.1944 4.4685 0.1464 4.7142 0.1636 4.6811 0.1387 4.6665 0.1868 4.6098 MANIQA 0.4297 0.4757 0.5883 0.5720 0.4756 0.4682 0.4617 0.5314 Table 9. Quantitative results of models on full size images from DRealSR [61]. The best and second best results are highlighted in bold and underline. The results in these tables demonstrate that our RSD model achieves performance comparable to state-of-the-art models across broad range of metrics and methods."
        },
        {
            "title": "RealSR",
            "content": "RealSet65 BSRGAN [73] Real-ESRGAN [57] SwinIR [31] SUPIR [70] OSEDiff [62] ResShift [72] SinSR (distill only) [58] SinSR [58] ResShift-VSD (Appendix A) RSD (Ours, distill only) RSD (Ours) 1 1 1 50 1 15 1 1 1 1 1 PSNR 26.51 25.85 26.43 24.38 25.25 26.49 26.14 25.83 23.96 24.92 25.91 SSIM 0.775 0.773 0.786 0.698 0.737 0.754 0.732 0.717 0.616 0.696 0.754 LPIPS 0.269 0.273 0.251 0.331 0.299 0.360 0.357 0.365 0.466 0.355 0. CLIPIQA 0.5439 0.4898 0.4654 0.5449 0.6772 0.5958 0.6119 0.6887 0.7479 0.7518 0.7060 MUSIQ 63.586 59.678 59.636 63.676 67.602 59.873 57.118 61.582 63.298 66.430 65.860 CLIPIQA 0.6163 0.5995 0.5782 0.6133 0.6836 0.6537 0.6822 0.7150 0.7606 0.7534 0.7267 MUSIQ 65.582 63.220 63.822 66.460 68.853 61.330 61.267 62.169 66.701 68.383 69.172 Table 10. Extended quantitative results of models on two real-world datasets. The best and second best results are highlighted in bold and underline. Methods ESRGAN [56] RealSR-JPEG [25] BSRGAN [73] SwinIR [31] Real-ESRGAN [57] DASR [33] LDM-30 [42] LDM-15 [42] SUPIR [70] OSEDiff [62] ResShift [72] SinSR (distill only) [58] SinSR [58] ResShift-VSD (Appendix A) RSD (Ours, distill only) RSD (Ours) NFE 1 1 1 1 1 1 30 15 50 1 15 1 1 1 1 1 PSNR 20.67 23.11 24.42 23.99 24.04 24.75 24.49 24.89 22.56 23.02 25.01 24.69 24.56 23.69 23.97 24.31 SSIM 0.448 0.591 0.659 0.667 0.665 0.675 0.651 0.670 0.574 0.619 0.677 0.664 0.657 0.624 0.643 0.657 LPIPS 0.485 0.326 0.259 0.238 0.254 0.250 0.248 0.269 0.302 0.253 0.231 0.222 0.221 0.230 0.217 0.193 CLIPIQA 0.451 0.537 0.581 0.564 0.523 0.536 0.572 0.512 0.786 0.677 0.592 0.607 0.611 0.665 0.660 0.681 MUSIQ 43.615 46.981 54.697 53.790 52.538 48.337 50.895 46.419 60.487 60.755 53.660 53.316 53.357 58.630 57.831 58. Table 11. Extended quantitative results of models on ImageNet-Test [72]. The best and second best results are highlighted in bold and underline."
        },
        {
            "title": "Datasets",
            "content": "DIV2K-Val"
        },
        {
            "title": "Methods\nBSRGAN",
            "content": "LDL FeMASR StableSR DiffBIR SeeSR PASD SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) BSRGAN"
        },
        {
            "title": "1\nReal-ESRGAN 1\n1\n1\n200\n50\n50\n20\n50\n15\n1\n1\n1\n1\nReal-ESRGAN 1\n1\n1\n200\n50\n50\n20\n50\n15\n1\n1\n1\n1\nReal-ESRGAN 1\n1\n1\n200\n50\n50\n20\n50\n15\n1\n1\n1",
            "content": "LDL FeMASR StableSR DiffBIR SeeSR PASD SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) BSRGAN NFE PSNR 24.58 24.29 23.83 23.06 23.26 23.64 23.68 23.14 22.18 24.65 24.41 23.72 23.91 28.75 28.64 28.21 26.90 28.03 26.71 28.17 27.36 24.93 28.46 28.36 27.92 27.40 26.39 25.69 25.28 25.07 24.70 24.75 25.18 25.21 23.61 26.31 26.28 25.15 25.61 LDL FeMASR StableSR DiffBIR SeeSR PASD SUPIR [70] ResShift [72] SinSR [58] OSEDiff [62] RSD (Ours) SSIM LPIPS 0.6269 0.3351 0.6371 0.3112 0.6344 0.3256 0.5887 0.3126 0.5726 0.3113 0.5647 0.3524 0.6043 0.3194 0.5505 0.3571 0.5303 0.3971 0.6181 0.3349 0.6018 0.3240 0.6108 0.2941 0.6042 0.2857 0.8031 0.2883 0.8053 0.2847 0.8126 0.2815 0.7572 0.3169 0.7536 0.3284 0.6571 0.4557 0.7691 0.3189 0.7073 0.3760 0.6360 0.4263 0.7673 0.4006 0.7515 0.3665 0.7835 0.2968 0.7559 0.3042 0.7654 0.2670 0.7616 0.2727 0.7567 0.2766 0.7358 0.2942 0.7085 0.3018 0.6567 0.3636 0.7216 0.3009 0.6798 0.3380 0.6606 0.3589 0.7421 0.3421 0.7347 0.3188 0.7341 0.2921 0.7420 0.2675 DISTS NIQE 0.2275 4.7518 0.2141 4.6786 0.2227 4.8554 0.2057 4.7410 0.2048 4.7581 0.2128 4.7042 0.1968 4.8102 0.2207 4.3617 0.2338 5.6806 0.2213 6.8212 0.2066 6.0159 0.1976 4.7097 0.1940 5.1987 0.2142 6.5192 0.2089 6.6928 0.2132 7.1298 0.2235 5.9073 0.2269 6.5239 0.2748 6.3124 0.2315 6.3967 0.2531 5.5474 0.2823 7.4336 0.2656 8.1249 0.2485 6.9907 0.2165 6.4902 0.2343 6.2577 0.2121 5.6567 0.2063 5.8295 0.2121 6.0024 0.2288 5.7885 0.2288 5.9122 0.231 5.5346 0.2223 5.4081 0.2260 5.4137 0.2492 5.8877 0.2498 7.2365 0.2353 6.2872 0.2128 5.6476 0.2205 5.7500 MUSIQ 61.20 61.06 60.04 60.83 65.92 65.81 68.67 68.95 63.04 61.09 62.82 67.97 68.05 57.14 54.18 53.85 53.74 58.51 61.07 64.93 64.87 59.39 50.60 55.33 64.65 62.03 63.21 60.18 60.82 58.95 65.78 64.98 69.77 68.75 63.21 58.43 60.80 69.09 66. MANIQA 0.5071 0.5501 0.5350 0.5074 0.6192 0.6210 0.6240 0.6483 0.5861 0.5454 0.5386 0.6148 0.5937 0.4878 0.4907 0.4914 0.4420 0.5601 0.5930 0.6042 0.6169 0.5537 0.4586 0.4884 0.5899 0.5625 0.5399 0.5487 0.5485 0.4865 0.6221 0.6246 0.6442 0.6487 0.5895 0.5285 0.5385 0.6326 0.5930 CLIPIQA 0.5247 0.5277 0.5180 0.5997 0.6771 0.6704 0.6936 0.6788 0.7085 0.6071 0.6471 0.6683 0.6967 0.4915 0.4422 0.4310 0.5464 0.6356 0.6395 0.6804 0.6808 0.6799 0.5342 0.6383 0.6963 0.7019 0.5001 0.4449 0.4477 0.5270 0.6178 0.6463 0.6612 0.6620 0.6709 0.5442 0.6122 0.6693 0.6793 Table 12. Extended quantitative results of models on crops from [55]. The best and second best results are highlighted in bold and underline. E. Details of ResShift As part of the diffusion model class, ResShift can be described by specifying the forward (degradation) process, the parametrization of the reverse (restoration) process, and the objective for training the reverse process. Forward process. Consider pair of (LR, HR) images (y0, x0) pdata(y0, x0). For residual e0 = y0 x0, ResShift proposes to transit from x0 to y0 with the Markov chain {xt}T t=1 of length through the following Gaussian transition distribution: q(xtxt1, y0) = (xtxt1 + αte0, κ2αtI), (19) where: αt = ηt ηt1 for > 1 and α1 = η1 are defined by the shifting sequence {ηt}T κ is hyper-parameter controlling the noise variance and the shifting sequence {ηt}T The transition distribution (1) leads to analytically tractable marginal distribution of q(xtx0, y0) at any timestep t: t=1, denotes the identity matrix. t=1 monotonically increases with the timestep t. q(xtx0, y0) = (xtx0 + ηte0, κ2ηtI), [1, ], (20) The shifting sequence satisfies η1 0 and ηT 1, which guarantee the convergence of marginal distributions of x1 and xT to approximate distributions of the HR image and the LR image respectively. Notably, the posterior distribution q(xt1xt, x0, y0) for the transition distribution (1) is tractable and can be derived using the Bayess rule: q(xt1xt, x0, y0) = xt1 (cid:18) (cid:12) (cid:12) (cid:12) ηt1 ηt xt + αt ηt x0, κ2 ηt1 ηt (cid:19) αtI . (21) Reverse process. ResShift suggests constuction of the reverse process to estimate the posterior distribution p(x0y0) in the following parametrized form: pθ(x0y0) = (cid:90) p(xT y0) (cid:89) t=1 pθ(xt1xt, y0)dx1:T (22) Here p(xT y0) (xT y0, κ2I) and pθ(xt1xt, y0) is the inverse transition kernel from xt1 to xt with learnable paremeters θ. Folllowing DDPM [20], ResShift parametrizes this transition kernel with the Gaussian: pθ(xt1xt, y0) = (xt1µθ(xt, y0, t), Σθ(xt, y0, t)) (23) Objective. To derive the minimization objective for parameters θ, ResShift applies the variational bound estimation on negative loglikelihood for the pθ(x0y0) as DDPM: min θ E(x0,y0) (cid:88) t=1 Extq(xtx0,y0) (cid:2)DKL(q(xt1xt, x0, y0)pθ(xt1xt, y0))(cid:3) (24) Inspired by the tractable formula for posterior q(xt1xt, x0, y0) in (21), ResShift put the variance parameter Σθ(xt, y0, t) to be independent of xt and y0 and reparametrized the parameter µθ(xt, y0, t) as follows: Σθ(xt, y0, t) = κ2 ηt1 ηt αtI µθ(xt, y0, t) = ηt1 ηt xt + αt ηt fθ(xt, y0, t), (25) (26) where fθ is deep neural network with parameter θ, aiming to predict x0. Based on the normality of distributions q(xt1xt, x0, y0) (21) and pθ(xt1xt, y0) (23), the objective (24) can be simplified as follows: min θ E(x0,y0,xt) (cid:34) (cid:88) t=1 wtfθ(xt, y0, t) x02 , (cid:35) (27) where wt = conclusion in DDPM. αt 2κ2ηtηt1 . Empirically, the omitting weight wt leads to the evident improvement in performance, which aligns with the F. Limitations and failure cases Below, we present failure case for image restoration. Our method may produce images with mistakes since the teacher model is not perfect. However, we stress that T2I-based SR models also have such problems. Specifically, in Fig. 6, we observe that the teacher model produces image an indistinguishable image from simple bicubic upsampling. Similar occurs with OSEDiff, while all other methods, including ours, SinSR, SUPIR and GAN-based models, produce images with visible artifacts. Figure 6. Failure case from RealSet65 [72]. Please zoom in for better view. G. Proofs Proof of Proposition 3.1. First stage. We first prove that using objective Lfake is equivalent to training fake model fϕ with objective (5). We remind the Lfake objective: Lfake = (cid:16) (cid:88) t=1 wtEpθ ((cid:98)x0,y0,xt) (cid:110) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t) + (xt, y0, t) (cid:125) (cid:123)(cid:122) (cid:124) Does not depend on ϕ (cid:111)(cid:17) , (cid:98)x0 (28) Then we prove: arg min ϕ (cid:16) (cid:88) t=1 (cid:124) wtEpθ ((cid:98)x0,y0,xt)fϕ(xt, y0, t) (cid:98)x0 2 (cid:123)(cid:122) Training fake model fϕ with objective (5) arg min ϕ (cid:16) (cid:88) t=1 wtEpθ ((cid:98)x0,y0,xt) (cid:110) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t), (cid:98)x0 (cid:111) + (cid:88) t=1 (cid:124) wtEpθ ((cid:98)x0,y0,xt)(cid:98)x02 (cid:125) (cid:123)(cid:122) Does not depend on ϕ 2 (cid:17) (cid:125) (cid:17) = = arg min ϕ (cid:16) (cid:88) t=1 wtEpθ ((cid:98)x0,y0,xt) (cid:110) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t), (cid:98)x0 (cid:111)(cid:17) = arg min ϕ (cid:16) (cid:88) t=1 wtEpθ ((cid:98)x0,y0,xt) (cid:110) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t) + (xt, y0, t) (cid:125) (cid:123)(cid:122) (cid:124) Does not depend on ϕ (cid:111)(cid:17) = , (cid:98)x0 arg min ϕ (cid:16) (cid:88) t= (cid:124) wtEpθ ((cid:98)x0,y0,xt) (cid:110) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t) + (xt, y0, t), (cid:98)x0 (cid:123)(cid:122) Lfake (cid:111)(cid:17) . (cid:125) Second stage. Now we prove that: min ϕ (cid:110) (cid:88) wtEpθ ((cid:98)x0,y0,xt) (cid:16) (cid:88) t=1 (cid:124) wtEpθ ((cid:98)x0,y0,xt)fGθ (xt, y0, t) (xt, y0, t)2 (cid:125) (cid:123)(cid:122) Lθ fϕ(xt, y0, t)2 2 (xt, y0, t)2 2 + 2f (xt, y0, t)fϕ(xt, y0, t), (cid:98)x0 = (cid:17)(cid:111) (29) (30) Note, that since ResShift objective (27) is an MSE, than the solution fGθ for the data produced by generator Gθ is given by the conditional expectation as: fGθ (xt, y0, t) = Epθ ((cid:98)x0y0,xt)[(cid:98)x0]. (31) We start from the right part of (30) and transform it back to the left part: (cid:16) (cid:110) (cid:88) (xt, y0, t)2 2 + fϕ(xt, y0, t)2 wtEpθ ((cid:98)x0,y0,xt) min ϕ (cid:88) wtEpθ ((cid:98)x0,y0,xt) 2 + 2f (xt, y0, t)fϕ(xt, y0, t), (cid:98)x0 (cid:110) (xt, y0, t)2 2 2f (xt, y0, t), (cid:98)x0 (cid:17)(cid:111) 2 2fϕ(xt, y0, t), (cid:98)x0 (cid:16) fϕ(xt, y0, t)2 (cid:17)(cid:111) (cid:111) = = (cid:110) (cid:88) min ϕ wtEpθ ((cid:98)x0,y0,xt) wtEpθ (y0,xt) (cid:16) (cid:88) (xt, y0, t)2 2 2f (xt, y0, t), Epθ ((cid:98)x0y0,xt)(cid:98)x0 (cid:125) (cid:124) (cid:123)(cid:122) fGθ (xt,y0,t) (cid:17) min ϕ (cid:110) (cid:88) wtEpθ ((cid:98)x0,y0,xt) (cid:16) fϕ(xt, y0, t)2 2 2fϕ(xt, y0, t), (cid:98)x0 (cid:17)(cid:111) = (cid:88) wtEpθ (y0,xt) (cid:16) (xt, y0, t)2 2 2f (xt, y0, t), fGθ (xt,y0,t) (cid:17) wtEpθ (y0,xt) (cid:16) (cid:88) fGθ (xt, y0, t)2 2 2 fGθ (xt, y0, t), fGθ (xt,y0,t) (cid:125) (cid:124) (cid:123)(cid:122) fGθ 2 2 (cid:17) = wtEpθ (y0,xt) (cid:16) (cid:88) f (xt, y0, t)2 2 2f (xt, y0, t), fGθ (xt,y0,t) + fGθ (xt, y0, t)2 2 (cid:17) (cid:88) wtEpθ (y0,xt) fGθ (xt, y0, t) (xt, y0, t)2 2 (cid:124) (cid:125) (cid:123)(cid:122) Does not depend on (cid:98)x0 so we can add (cid:98)x0 in expectation. wtEpθ ((cid:98)x0,y0,xt)fGθ (xt, y0, t) (xt, y0, t)2 2 (cid:88) = = H. More visual results This section provides an additional qualitative visual comparison between RSD and other baselines on two real-world full-size benchmarks with available ground truth data. 1. Full-size benchmark RealSR [3]. The results are shown in Fig 7. 2. Full-size benchmark DRealSR [61]. The results are shown in Fig. 8. Baseline methods include multistep diffusion-based SR methods (ResShift [72], SUPIR [70]), 1-step diffusion-based SR methods (SinSR [58], OSEDiff [62]), and GAN-based SR methods (Real-ESRGAN [57] and BSRGAN [73]). Figure 7. Visual comparison on real-world samples from RealSR [3]. Please zoom in for better view. Figure 8. Visual comparison on real-world samples from DRealSR [61]. Please zoom in for better view."
        }
    ],
    "affiliations": [
        "AI Foundation and Algorithm Lab",
        "AIRI",
        "HSE University",
        "MIPT",
        "Skoltech",
        "Yandex Research"
    ]
}