{
    "paper_title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "authors": [
        "Pierre Colombo",
        "Malik Boudiaf",
        "Allyn Sweet",
        "Michael Desa",
        "Hongxi Wang",
        "Kevin Candra",
        "Sym√©on del Marmol"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 8 5 6 8 1 . 2 1 5 2 : r EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Pierre Colombo, Malik Boudiaf, Allyn Sweet, Michael Desa, Hongxi Wang, Kevin Candra, Symeon del Marmol {firstname}@equall.com Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (e.g., shares, options, warrants) and issuance term (e.g., vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems: the task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose world model architecture towards tie-out automationand more broadly as foundation for applied legal intelligence. *Equal contributions"
        },
        {
            "title": "Introduction",
            "content": "Verifying legal ownership of company is bottleneck in private market transactions, from venture financings to M&A deals. Before deal closes, lawyers must manually reconcile thousands of pages against ownership records, process commonly referred to as cap table tie-out or capitalization due diligence. This work is critical to confirm financial allocations up to and arising from the transaction, while also tedious, error-prone, and typically performed under capped fees that compress margins. Large language models have shown impressive capabilities on legal reasoning benchmarks [7, 5, 6, 11, 10] and contract analysis tasks [14, 1], yet this recurring verification workflow remains categorically manual. This paper investigates why and what is required to bridge the gap. In any financing or acquisition the disbursing party must verify all ownership claims, contractual obligations, potential liabilities, and regulatory risks before funds change hands [2]. This legal risk assessment centers on the dataroom, repository containing the companys legal and financial history: incorporation filings, stock purchase agreements, option grants, convertible instruments, board consents, and their amendments. Market data shows over 25,000 venture capital financings per year, with circa 15,000 originating in the US alone. Routine Seed and Series rounds generate datarooms with thousands of pages, and Series and after often reach tens of thousands of pages. Tie-out is the core verification task to reconcile legal ownership and related positions. Each entry in the capitalization table, such as share issuances, option grants, and SAFE conversions must trace to signed agreements, board approvals, and payment records. As shown in Figure 1, tie-out transforms heterogeneous document collection into verified ownership records while surfacing discrepancies for legal review. The task is combinatorial: single stock grant may depend on the original equity plan, board consent, the signed option agreement, and subsequent amendments. Errors propagate, and even single missed cancelation or overlooked amendment can misstate ownership across dozens of stakeholders. Why LLMs alone are not enough. Tie-out exposes limitations that legal AI benchmarks do not test. Unlike single-document question answering [16, 4, 8] or clause extraction [9], tie-out requires multi-document reasoning combining information scattered across dozens of files. It demands strict traceability in that every claim must link to source evidence. And 1 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital it requires consistency: the same document processed twice must yield identical outputs. These requirements challenge the dominant agentic LLM paradigm, where performance excels at fluent extraction and summarization but struggles with combinatorial verification and reproducibility at scale [3]. This paper examines how LLMs can be effectively applied to capitalization tie-out, characterizing the complexity of the task and the architectural requirements for its reliable automation. Figure 1: The tie-out workflow. Lawyers review and cross-reference heterogeneous dataroom documentsone or more capitalization tables, and supporting legal documentation (e.g., SAFEs, option agreements, amendments, cancellations, etc.)to understand the legal reality of the company and ensure the capitalization table is accurate by comparing it against the non-cap table documents, which provide the ground truth. The output: verified legal positions with document traceability, and flags for discrepancies requiring further review and/or action."
        },
        {
            "title": "2 Background and Formulation",
            "content": "This section describes the capitalization due diligence context, introduces the dataroom and its core artifactthe cap tableand formally defines the tie-out problem. 2.1 The Dataroom In any financing or M&A transaction, the dataroom is repository containing the companys legal and financial history. We represent it as = {D1, . . . , DN}, where each document Di encodes partial, overlapping, or sometimes conflicting information about ownership, rights, and past transactions. Figure 2: Document category. Red dashed lines indicate Zipfs law fits (R2 1 is perfect fit). 2 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Table 1: Document categories for capitalization tie-out in venture financings Document Type Role in Tie-Out Convertible Securities Stock Purchase Agreements Capitalization Table (Cap Table) Records ownership of Company stock, convertible securities, and purchase rights; used as the purported correct representation of the Companys ownership against which non-cap table documents are compared. Document purchases of Company stock by stockholders; used to verify stock ownership and contractual rights. Document purchases of convertible securities (e.g., SAFEs, convertible notes) by investors; used to verify convertible securities ownership and conversion rights. Document equity ownership promised by the Company to employees, contractors, and advisors; used to verify that there are no promised equity unrepresented on the cap table. Modify prior agreement terms (e.g., vesting schedule, acceleration terms); used to reconcile historical changes in ownership and document terms. Establish the Companys legal existence, share class structure, and governance rules; used to verify issued securities comply with legal limit of authorized shares. Certificates of Incorporation / Bylaws Employment Contracts Amendments Term Sheets Board and Shareholder Consents Approve corporate actions (e.g., stock issuances, amendments, or corporate actions); used to validate formal board and/or stockholder authorization of transactions. Outline terms of proposed financing; used to cross-check final agreements and conversion terms. Document additional equity-related events; used to ensure timely and accurate reflection of outstanding ownership on the cap table. Ancillary Equity Agreements (e.g., Option Exercise, Repurchase, and Transfer) Documents vary widely in formats, from contracts, registers, notices, certificates to spreadsheets. But each belongs to functional category with distinct role in verification  (Table 1)  . As shown in Figure 5, the distribution follows Zipfs law: small number of categories (e.g. board consents, SPA) account for the majority of documents, while long tail of specialized instruments appears infrequently but remains critical when present [13, 12]. 2.2 The Capitalization Table The capitalization table (cap table), denoted C, plays central role in the tie-out. It provides consolidated snapshot of all outstanding securitiescommon stock, preferred shares, options, SAFEsalong with their holders and ownership percentages. It typically takes the form of an Excel spreadsheet, with each tab (or ledger) detailing the current ownership for one specific class of shares. An example of such ledger is provided in Appendix A. In the tie-out problem, the ledgers from the cap table functions as the master claims ledger: every entry must trace to authoritative source documents in the dataroom. The goal is not to reconstruct the companys full legal history but to confirm that each line is supported by controlling evidenceand to flag where it is not. 2.3 The Tie-Out Problem At high level, tie-out is reconciliation problem. The company presents reference (or nominal) cap table, while the dataroomcharters, board consents, SAFEs, warrants, option plans, and so onprovides the ground truth legal materials, implicitly defining second, virtual (or legal) cap table. Tie-out asks whether these two capitalization states coincide, and if not, where and why they diverge. Formally, let = {S1, . . . , SM} denote the set of securities (common stock, each preferred series, SAFEs, options, warrants, etc.), and let denote the dataroom. We write for the space of possible capitalization states over (authorized and issued amounts, liquidation preferences, conversion mechanics, option pools, and so on). The companys cap table is distinguished element Cref 3 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital (the reference cap table). Conceptually, the dataroom induces virtual cap table Cvirt = Tdoc(D) , where Tdoc : is the (typically implicit) extraction map that reads the legal documents and infers the capitalization they imply. Tie-out does not usually construct Cvirt explicitly. Instead, it operates through finite family of verification transforms = {T1, . . . , TK}, Tk : Yk. Each transform Tk, for each k, zooms in on specific aspect of the cap table, for example The date the board approved grant CS-001. Conceptually, the tie-out process checks whether the corresponding views of the virtual and reference cap tables coincide: The family of equalities (cid:0)Cvirt(cid:1) ?= Tk (cid:0)Cref(cid:1). Tk (cid:0)Cvirt(cid:1) = Tk (cid:0)Cref(cid:1) Tk for = 1, . . . , are the tie-out constraints. In practice, lawyer may never write Cvirt down; each verification task is just one instance of checking such an equality for particular Tk using the underlying documents. The outcome of tie-out is the set of anomalies detected, i.e., the constraints that fail or cannot be established from the dataroom. We write = (cid:8) (k, Tk(Cvirt), Tk(Cref), Ek) (cid:12) (cid:12) Tk(Cvirt) = Tk(Cref) Ek D(cid:9), where Ek is the evidentiary subset of the dataroom used to compute (or fail to compute) the relevant quantities. The lawyers task is to produce with full traceability: every recorded anomaly must be backed up by an explicit and minimal evidence set Ek. Coarse taxonomy of anomalies. At this level of abstraction, different types of flags correspond to different ways in which tie-out constraints fail or cannot be established. Coarsely, we can distinguish: Missing from cap table. position or security is present in the virtual cap table but absent from the reference cap table. In terms of the transforms, there exists such that Tk(Cvirt) encodes nonzero position (e.g., issued shares, an outstanding warrant) while Tk(Cref) does not reflect it at all. Typical examples include SAFE, warrant, or option grant that appears in the documents but is not carried onto the actual cap table. Missing documentation. position or change is reflected on the reference cap table, but the dataroom does not fully determine the corresponding virtual view. Formally, Tk(Cref) is well-defined, while Tk(Cvirt) is undefined or under-specified given (e.g., missing issuance, transfer, repurchase, or board consent). The anomaly is not numerical mismatch but the absence or incompleteness of the evidentiary path that would support Tk(Cref). Inconsistent terms. Both cap tables carry the same object (e.g., particular series, grant, or instrument), but the associated economic or administrative terms differ. In the transform view, Tk isolates those terms (vesting schedule, acceleration provisions, price per share, liquidation preference, etc.), and the anomaly is simply Tk(Cvirt) = Tk(Cref). Here the mismatch lives in the qualitative or parametric description of the security rather than its mere presence or absence. 4 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Figure 3: Real-world example of grant lifecycle. single stock option grant that is later repriced, then affected by 10:1 stock split, then partially exercised into common stock (with the remainder expiring), after which the resulting common shares are sold or transferred to three different holders. Challenges of Real-World Tie-Out at Scale The lifecyle of grants. The reference cap table provides only partial view of the world: compressed snapshot of nominal positions, stripped of the lifecycle that produced them. Each visible row aggregates potentially long lineage of corporate actionsgrants, repricings, stock splits, exercises, expirations, and transfers. As the company matures, those lineages branch and interact: verifying single cap table row may require cross-referencing dozens of agreements spanning years of history. Figure 3 illustrates this on concrete example. The combinatorics compound fast: 30 stakeholders, 5 security classes, and 200 documents already create thousands of distinct verification paths. Fractured data landscape. Documents are concatenated (one PDF containing an agreement, amendments, and exhibits), scanned with variable OCR quality, or missing pages. Near-duplicate versions proliferate: draft and executed copies, redlines, clean versions, and standalone signature packets, sometimes with only one of them actually operative. Filenames are misleading; metadata is absent. Temporal reasoning is difficult: amendments modify or supersede prior agreements, but the chain is rarely explicit. Documents come in separate batches and reference others not in the dataroom; parties appear under variant names or through affiliated entities."
        },
        {
            "title": "3 Empirical Complexity Analysis",
            "content": "The formal definition above frames tie-out as mapping dataroom to virtual cap table Cvirt via extraction (Tdoc) and verification transforms (Tk). In an idealized setting, is small, structured, and internally consistent. However, empirical data from real-world financing diligence reveals that the complexity of this process does not scale linearly. By analyzing dataroom statistics across four representative companies spanning Seed to Series financing stages (Fig. 4, Fig. 5, Fig. 7, and Fig. 6), we identify three primary drivers of complexity related to scaling evidence, shifting anomaly types, and verification workload. 5 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital The evidentiary burden scales super-linearly relative to document volume. Fig. 4 illustrates the velocity at which the diligence surface area expands. While the growth in raw volumepages quadrupling and total documents more than doubling between Seed (Comp. S.1) and late Series (Comp. B.2)is significant, the critical insight lies in the changing relationship between documents and the securities they govern. While the size of the dataroom roughly doubles, the number of individual securities trackedthe cardinality of the set Sincreases by factor of seven (from 184 to 1,292). This widening gap indicates fundamental shift in the required granularity of the extraction map Tdoc. At the Seed stage, the ratio of documents to securities is roughly 1:1. By Series B, single document in (such as major recapitalization agreement) may define hundreds of distinct security issuances. Consequently, the verification transforms Tk must become increasingly granular to isolate individual data points buried within dense legal text. Figure 4: Comparison of key dataroom statistics, including total pages, documents, securities, and shareholders, across companies in different financing stages. Figure 5: Distribution of categories across four companies at varying financing stages. As governance matures, anomalies shift from informal omissions to complex inconsistencies. As the volume expands, the qualitative nature of the source of truthand the errors it containsevolves. Fig. 5 shows that Seed stage datarooms are dominated by unstructured documents like employment agreements, reflecting early-stage informal governance. Here, lawyers must scan unstructured text to identify where business intent never translated into formal evidence, leading to rudimentary Missing Information anomalies. By Series B, governance is structured, dominated by dense financing records. The challenge shifts to catching up on years of intricate historical transactions to spot inconsistencies between interrelated documents. Fig. 6 confirms that total anomaly counts grow significantly with maturityComp. B.2 surfaces nearly 2.5 the issues of Comp. S.1. Notably, the persistence of Missing Information and Missing Consent/Approval as top categories in later stages indicates that the primary challenge in mature tie-outs becomes verifying the *completeness* of an exponentially growing historical recorda needle in haystack retrieval problem that human reviewers struggle to scale. 6 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Figure 6: Breakdown of issues detected in real-world datarooms, showing the total frequency of each issue type (left) and their distribution across four different companies (right). The verification workload explodes in both volume and cognitive difficulty. The combined effects of super-linear evidence growth and increasingly buried anomalies materialize as severe increase in the verification workload of the lawyer. Fig. 7 quantifies this burden by tracking the total number of atomic steps executed by the counsel to complete the tie-out, acting as practical proxy for K, the total family of required verification transforms. We observe near-tripling of workload, from approximately 2,700 steps at Seed to nearly Figure 7: Number of verification steps. 8,000 steps at Series B. Critically, this increase is not merely numerical; each step becomes intrinsically harder to perform. As the haystack (D) grows, the retrieval task required for every individual check becomes more cognitively demanding for human reviewer. In practice, the tie-out process often turns into grueling exercise over tens of hours of burdensome review."
        },
        {
            "title": "4 Towards Tie-Out Automation",
            "content": "The core technical challenge of tie-out is bridging the semantic gap between highly unstructured, heterogeneous legal documents (D) and the rigid, combinatorial constraints required for verification (Tk(Cvirt) ?= Tk(Cref)). We investigate two fundamentally different architectural paradigms for addressing this challenge: lazy approach that attempts extraction and verification simultaneously via agentic reasoning, and an eager approach (Equall) that decouples the process by explicitly constructing layered, structured world model. 4.1 The Agentic Paradigm: Lazy Construction via RAG The dominant paradigm for tackling complex, document-based tasks with LLMs is the agentic framework augmented by Retrieval-Augmented Generation (RAG). In this approach, construction of the virtual view Cvirt is lazyit occurs ad-hoc in response to specific verification queries. How it works: To verify specific transform Tk (e.g., Verify vesting start date for Option Grant CS-102), an LLM agent orchestrates multi-step process: 1. Query Generation: The agent formulates search queries using the target field and entities. 2. Retrieval: retrieval system fetches relevant chunks from the indexed dataroom D. 3. Reasoning & Extraction: The LLM synthesizes retrieved chunks to extract the requested value and identify supporting evidence Ek. 7 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital 4. Verification: The agent compares the extracted value against the reference cap table Cref. Limitations in the Due Diligence Context: While flexible, this paradigm faces severe challenges at the scale and complexity described in Section 3. First, standard RAG struggles with global reasoning constraints. Proving Missing Documentation anomaly requires establishing that evidence does not exist across thousands of pagesa task where retrieval failure is often indistinguishable from actual absence. Second, compounding errors in lineage tracking. Verifying single position may require tracing multi-year chain of dependent documents; the probability of an agent successfully retrieving and correctly reasoning over every link in that chain decreases exponentially with chain length. 4.2 The Equall Paradigm: Eager Construction of World Model To overcome the limitations of ad-hoc reasoning, we propose fundamentally different approach: decoupling extraction from verification through the eager construction of symbolic world model. This model is built in two stages, moving from raw text to atomic facts, and finally to strong inductive representation of the companys lifecycle. Stage 1: Foundational Extraction (Low-Level Nodes). The first stage processes the raw dataroom to build the foundation of knowledge graph. We employ specialized LLM-based parsers to first classify documents according to the taxonomy established in Fig. 2. Subsequently, extracting agents identify and instantiate low-level nodes such as Stakeholders (individuals, funds), Securities (specific stock classes, warrants), and structured atomic values (dates, share counts, prices, clauses), linking each explicitly to its source document span for provenance. Stage 2: Inductive Event Modeling (Conceptual Nodes). The critical innovation lies in the second stage: organizing these low-level facts into coherent, temporal representation of the companys legal history. We define high-level Conceptual Nodes representing business events: Issuance, Transfer, Amendment, Conversion, Exercise, and CorporateAction (e.g., stock splits). LLM reasoning is used to synthesize low-level nodes into these event nodes. For example, an Amendment event node is constructed by linking the amending document, the specific clauses changed, and crucially, relationship edge pointing to the prior Issuance or Agreement event it modifies. This stage results in rich Event Grapha strong inductive representation of the companys entire lifecycle. This structured history is reusable asset whose utility extends beyond tie-out to adjacent legal tasks flowing from the same legal reality. Stage 3: Targeted Neuro-Symbolic Verification. Finally, tie-out verification is executed via targeted queries over this Event Graph. We adopt neuro-symbolic approach: 1. The neuro component is the robust, LLM-driven extraction and event synthesis that built the graph (Stages 1 & 2), handling the ambiguity of legal text. 2. The symbolic component is the application of deterministic logic to aggregate these events into the final virtual cap table state Cvirt. For example, verifying stakeholders current share count is no longer fuzzy retrieval task. It is structured query: traverse the Event Graph for all Issuance events to that stakeholder, subtract subsequent Transfer-Out events, add Transfer-In events, and apply adjustments from linked CorporateAction events (like splits). The result of this targeted query is compared against Cref to identify anomalies."
        },
        {
            "title": "5 Experiments",
            "content": "Task Definition. We evaluate automated tie-out as the anomaly detection task defined in Section 2.3. Given dataroom and reference capitalization table Cref, the system must produce the set of anomalies A. For every verification transform Tk where the virtual view differs from the reference view (Tk(Cvirt) = Tk(Cref) or is undefined), the system must flag 8 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital the discrepancy, classify its type, identify the affected stakeholder/security, and provide the supporting evidentiary subset Ek D. Harmonized Flag Types. To connect empirical evaluation with our theoretical framework, we categorize anomalies based on the taxonomy established in Section 2.3: Terms Discrepancy (Tk(Cvirt) = Tk(Cref)): The entry exists in both views but differs. Practically, this includes numerical mismatches (share counts, prices, dates) often labeled as Data Discrepancy or conflicting terms across agreements (Issuance Discrepancy). Missing Documentation (Tk(Cvirt)): An item on the reference cap table lacks sufficient supporting evidence in D. Practical examples include Board Approval Missing for recorded issuances or broken chains of title. Missing from Cap Table (Tk(Cref)): Valid securities or stakeholders identified in are absent from the reference cap table. Dataset & Metrics. We evaluate on the four anonymized datarooms presented in Section 3, spanning Seed to Series B. Ground-truth flags were annotated by experienced legal professionals. We report precision, recall, and F1 per flag category; prediction is correct only if the type of anomaly and supporting evidence match the ground truth. Baselines. We evaluate the paradigms detailed in Section 4 by comparing three approaches to constructing and verifying the virtual view Cvirt: Agentic Baseline: Represents the lazy construction paradigm (Section 4.1). It uses GPT5.1 with iterative RAG and multi-step reasoning to perform extraction and verification ad-hoc for each query directly from raw documents. Agentic + Structured Repr.: An ablation operating over pre-extracted low-level nodes (Stage 2 of Equalls pipeline, see Section 4.2) rather than raw text. It relies on the ad-hoc, lazy agentic reasoning pipeline to dynamically connect evidence during verification. Equall (Ours): The full eager construction approach (Section 4.2). It first builds the complete, layered world modelprogressing from low-level extraction to the inductive Event Graph representing the companys lifecycle. Verification transforms Tk are then executed as deterministic neuro-symbolic queries over this structured graph. 5.1 Results Overall Performance. Figure 8 shows results across flag types. Equall achieves an average F1 of 85%, significantly outperforming agentic + structured representations (42%) and pure agentic (29%). The performance gap is interpreted through our theoretical lens. Agentic Figure 8: Precision, recall, and F1 across harmonized flag categories. baselines perform respectably on Inconsistent Terms (e.g., Data Discrepancy), which are often local comparison tasks requiring the retrieval of only one or two source documents. 9 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital However, agentic performance collapses on Missing Documentation and Missing from Cap Table. These anomalies require global reasoning over the dataroom (i.e., the legal source of truth) to prove negative or establish complete lineage. As analyzed in Section 3, the non-linear scaling of evidentiary volume makes constructing this global context via ad-hoc retrieval statistically precarious. Equalls pre-computed knowledge graph turns these complex reasoning chains into reliable graph queries, providing consistent gains across the board. The Speed Trade-off: Lazy vs. Eager Construction. This comparison highlights the fundamental architectural trade-off between the paradigms detailed in Section 4. The agentic approach relies on lazy, ad-hoc reasoning at query time. In contrast, Equall invests in the eager construction of the layered world modeltransforming raw text into the inductive Event Graph before the first check is run. This shifts the computational burden of reasoning out of the critical verification path. Agentic Equall Indexing (one-time) Time 2 mins 15 mins Inference (per check) Time 45 sec 2 sec 100 checks 500 checks 77 mins 377 mins 18 mins 32 mins embodies The agentic baseline the lazy paradigm: minimal setup cost, but high marginal cost per verification due to repeated, complex ad-hoc reasoning. Equalls eager approach requires upfront investment to construct the Event Graph. However, this transforms verification into inexpensive, deterministic graph traversals (the symbolic phase of our approach). Equalls approach offers critical 22 speed advantage per check, essential for responsive human-in-the-loop workflows. Figure 9: Speed comparison on 300-document dataroom. Eager world model construction amortizes reasoning costs over subsequent verification steps. Real-World Efficiency at Scale. Our experiments empirically validate the non-linear scaling of diligence complexity discussed in Section 3. As datarooms grow in volume and semantic density (from Seed to Series B), the verification burden explodes for both automated baselines and human professionals. Figure 10 shows the agentic baseline buckling under retrieval noise and compounding errors, with F1 scores dropping sharply from 55% to 28%. Conversely, Equalls structured, eager modeling remains robust, widening the performance gap significantly at later stages. This complexity crisis is mirrored in manual workflows: Figure 11 reveals that human effort scales super-linearly, growing more than fivefold from 5 hours at Seed to nearly 27 hours at Series B. Equall effectively breaks down this complexity through upfront world modeling, resulting in smooth scaling curve for assisted review (64m 300m). By combining high-recall automated verification with targeted, high-precision human review, the Equall-assisted workflow delivers massive efficiency gainsscaling from roughly 79% at Seed to 81.5% at Series Bprecisely where manual efforts become unsustainable. World Model as Multi-Purpose Utility. Equalls massive improvement over the Agentic + Structured Repr. baseline shown in Figure 8 highlights the architectural importance of the intermediate Event Graph (Stage 2 in Section 4) in driving performance. Notably, this inductive representation models generic corporate lifecycle eventsissuances, amendments, transfersrather than task-specific tie-out logic. The fact that strongly typed, yet taskagnostic, structure yields state-of-the-art results on highly specialized verification task suggests that the Event Graph successfully captures the fundamental legal reality of the company. This indicates that Equalls world model is not merely single-purpose utility, but robust foundational substrate suitable for wider array of downstream legal applications that rely on the same underlying historical ground truth. 10 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital Figure 10: Scaling behavior. F1 on flag detection vs. dataroom size. Agentic quality degrades rapidly with complexity, while Equall system remains robust even on large series-B datarooms. Figure 11: Tie-out time: manual vs. Equallassisted. Error bars represent 95% intervals obtained by comparing the time reported by customers/partners on real-world tieouts vs. end-to-end time obtained internally by using Equalls system."
        },
        {
            "title": "6 Conclusion: A Framework for Applied Legal Intelligence",
            "content": "Towards Autonomous Tie-Out Agents. The cap-table tie-out problem represents the critical frontier for deploying truly autonomous systems in high-stakes legal scenarios. Realizing this potential requires converging three essential ingredients, all present in this domain. First, dense, verifiable reward signal: unlike subjective legal tasks, tie-out provides objective ground truth for training robust policy networks [15]. Second, scalable training environments: our empirical anomaly taxonomy enables the algorithmic generation of vast synthetic curricula by injecting known error patterns into validated clean datarooms. Third, and most critically for handling massive unstructured state spaces, robust world model. Standard agents struggle with ad-hoc retrieval over raw text. The layered world model developed in Equalls approachspecifically the inductive Event Graph described in Section 4.2provides the necessary structured, temporal memory bank. Integrating this foundational world model with RL-driven training on synthetic data will enable the next generation of agents to navigate complex, multi-step lineage tasks with superhuman reliability, fundamentally transforming the practice of capitalization due diligence and comparable legal risk analysis work. Generalized Representation as Foundation for Legal Intelligence. Equalls superior performance owes substantially to its explicit, inductive world model rather than to taskspecific engineering. The Event Graph captures legally operative events as structured, temporally ordered state transitions grounded in primary evidence. Because these event primitives recur across legal domains, the architecture is inherently generalizable. Verification over this model reduces to deterministic queries on well-defined state space, yielding stable performance even under substantial combinatorial complexity. Together, these findings suggest that explicit world-model construction is promising architectural foundation for building reliable autonomous systems for applied legal reasoning. 11 EQUALL: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital"
        },
        {
            "title": "References",
            "content": "[1] Rohan Bhambhoria, Samuel Dahan, Jonathan Li, and Xiaodan Zhu. Evaluating ai for law: Bridging the gap with open-source solutions. arXiv preprint arXiv:2404.12349, 2024. [2] Audra Boone and Harold Mulherin. How are firms sold? The Journal of Finance, 62(2):847875, 2007. [3] Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. [4] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in english. arXiv preprint arXiv:1906.02059, 2019. [5] Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, and Michael Desa. Saullm-54b & saullm-141b: Scaling up domain adaptation for the legal domain. Advances in Neural Information Processing Systems, 37:129672129695, 2024. [6] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera ucia Raposo, Sofia Morgado, et al. Saullm-7b: pioneering large language model for law. arxiv. 2024. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [8] Neel Guha, Daniel Ho, Julian Nyarko, and Christopher Re. Legalbench: Prototyping collaborative benchmark for legal reasoning. arXiv preprint arXiv:2209.06120, 2022. [9] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. arXiv preprint arXiv:2308.11462, 2023. [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [11] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. Available at SSRN 4389233, 2023. [12] Mark EJ Newman. Power laws, pareto distributions and zipfs law. Contemporary physics, 46(5):323351, 2005. [13] Steven Piantadosi. Zipfs word frequency law in natural language: critical review and future directions. Psychonomic bulletin & review, 21(5):11121130, 2014. [14] Richard Re and Alicia Solow-Niederman. Developing artificially intelligent justice. Stan. Tech. L. Rev., 22:242, 2019. [15] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [16] Don Tuggener, Pius Von Daniken, Thomas Peetz, and Mark Cieliebak. Ledgar: large-scale multi-label corpus for text classification of legal provisions in contracts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 12351241, 2020."
        },
        {
            "title": "A Example of Common Stock Ledger",
            "content": "Table 2: trimmed example of typical Common Stock (the most common class of shares issued) ledger. Every Common Stock grant that was ever issued by the company should appear on this ledger in its most up-to-date state (i.e reflecting any subsequent amendments or corporate events that could affect it). We call the full capitalization table Cref the ensemble of all the ledgers submitted for the diligence process. Security ID Stakeholder Name Quantity Issued Share Class Price Per Share Vesting Schedule Acceleration 1 3 CS-01 CS-02 CS-03 CS-04 CS-05 CS-06 CS-07 CS-08 CS-09 CS-10 CS-11 CS-12 CS-13 CS-14 CS-15 CS-16 CS-17 CSGrand Total Paul Reynolds Sarah Lawson Thomas Alvarez Julien Moreau Zara Ryman Leigh Bartlett Tim Branson Lucas Costa David Velner John Jackson Nadia Mansouri Monica Phillips Christopher Knight James Smith Hassan Murphy Keisha Young Daniel Brown Michael Gray 3,162,500 Common (CS) 700,000 Common (CS) 262,500 Common (CS) 262,500 Common (CS) 200,000 Common (CS) 150,000 Common (CS) 150,000 Common (CS) 50,000 Common (CS) 37,500 Common (CS) 5,000 Common (CS) 5,000 Common (CS) 10,000 Common (CS) 15,000 Common (CS) 10,000 Common (CS) 125,000 Common (CS) 15,000 Common (CS) 20,000 Common (CS) 8,000 Common (CS) 8,355,000 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 $0.00001 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/30th monthly, no cliff Double Trigger 1/60th monthly, 1 year cliff Double Trigger 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes Yes 1/12 monthly, no cliff 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes 1/48th monthly, 1 year cliff Yes U : s i t ? a A n u g g s e r p l"
        }
    ],
    "affiliations": [
        "EQUALL"
    ]
}