{
    "paper_title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning",
    "authors": [
        "Alexander Golubev",
        "Maria Trofimova",
        "Sergei Polezhaev",
        "Ibragim Badertdinov",
        "Maksim Nekrashevich",
        "Anton Shevtsov",
        "Simon Karasik",
        "Sergey Abramov",
        "Andrei Andriushchenko",
        "Filipp Fisin",
        "Sergei Skvortsov",
        "Boris Yangel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models."
        },
        {
            "title": "Start",
            "content": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning Alexander Golubev1, Maria Trofimova1, Sergei Polezhaev1, Ibragim Badertdinov1, Maksim Nekrashevich1, Anton Shevtsov1, Simon Karasik1, Sergey Abramov1, Andrei Andriushchenko1, Filipp Fisin1, Sergei Skvortsov1, Boris Yangel2 1Nebius AI 2Humanoid Work done while at Nebius AI Correspondence to alex golubev@nebius.com 5 2 0 2 5 ] . [ 1 1 0 5 3 0 . 8 0 5 2 : r Abstract Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with stateful environment that responds to each action with nontrivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Using modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve realworld software engineering tasks. Our approach increases the agents success rate on the SWE-bench Verified benchmark from 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering viable path toward building more capable autonomous agents for complex real-world problems based on open models."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models are increasingly deployed within autonomous agents in complex, real-world domains. Software engineering is an especially compelling application area, promising substantial economic impact through automation of debugging, code generation, and software maintenance tasks. However, current approaches to developing effective SWE agents rely predominantly on one of three strategies: (i) combining sophisticated scaffolding with proprietary LLMs (AugmentCode 2025; Refact AI 2025; Trae 2025), (ii) leveraging extensive inference-time scaling techniques (Antoniades et al. 2025), or (iii) supervised finetuning (SFT) of open-weight models using demonstrations from stronger teacher models (Yang et al. 2025; Zeng et al. 2025; Pan et al. 2025). While these methods have yielded strong initial results, they are often resource-intensive and depend on powerful, proprietary models. This reality underscores the need for methods that can build comparably Figure 1: Performance of the RL-trained agent over training iterations. To accelerate evaluations, Pass@1/10 are computed on random subset of 50 tasks from SWE-BENCH VERIFIED and averaged over 10 independent runs to reduce variance. Pass@1 is also reported together with the standard error of the mean. effective systems from smaller, open-weight models. Reinforcement learning offers promising alternative by directly optimizing an agents policy through interaction with responsive environment, potentially achieving stronger performance without reliance on teacher models. The interactive, structured nature of SWE, where actions produce observable transitions and verifiable outcomes, makes it an ideal domain for RL. Yet, to date, most RL applications for LLMs have been limited to single-turn tasks, such as math reasoning or single-shot code generation, which can be trivially modeled as multi-armed bandits or degenerate MDPs with no intermediate environmental feedback (Figure 2). In contrast, SWE scenarios require agents to manage stateful, multi-turn interactions. Successfully applying RL in this context involves several key challenges: Long-horizon, multi-turn interaction: Agents must maintain coherence across dozens of steps with context windows spanning hundreds of thousands of tokens. Complex, informative feedback: Actions elicit rich outputs (e.g., compiler traces, test logs) that must be interpreted to guide subsequent decisions effectively. Figure 2: Illustration of task structure differences between bandit-style problems (top, e.g., math) and POMDPs (bottom, e.g., software engineering). In bandit settings, the agent takes single action to produce final solution based on an initial observation. In contrast, POMDPs require multi-step interaction loop where the agent repeatedly takes actions and interprets new environmental feedback to guide its subsequent decisions. Data scalability and fidelity: Generating high-quality trajectories requires the reproduction of specific repository states in controlled environments, which limits dataset scale. Large-scale datasets such as SWESMITH (Yang et al. 2025) and SWE-REBENCH (Badertdinov et al. 2025) begin to address this gap; we primarily build on the latter. Sparse, delayed rewards: Success signals typically emerge only at the end of long action sequences, complicating credit assignment. Expensive and noisy evaluation: Unrolling trajectories and subsequent evaluation are costly, and flakiness in tests introduces noise in the reward signal. In this paper, we address these challenges by developing complete RL pipeline tailored explicitly to interactive SWE tasks. Our core contributions include: scalable RL framework, based on modified DAPO algorithm (Yu et al. 2025), specifically adapted to handle the demands of long-horizon, multi-turn SWE scenarios. Empirical demonstration of RL effectiveness by trainthat achieves aping Qwen2.5-72B-Instruct agent proximately 39% success on the SWE-BENCH VERIFIED benchmark, doubling the baseline performance of rejection-fine-tuned agent. Moreover, our agent matches or surpasses top open-weight models, such as DeepSeek-V3-0324 and Qwen3-235B-A22B, on the SWE-REBENCH benchmark  (Table 1)  . Detailed analysis of our RL training methodology, including algorithmic modifications, hyperparameter settings, key findings and discussion of promising future directions for applying RL to LLM-based agents in interactive, stateful environments. Our results show that RL can be successfully applied to general, interactive environments, advancing the capabilities of open-weight LLM agents beyond single-turn benchmarks and toward real-world scenarios, where future autonomous agents are expected to operate."
        },
        {
            "title": "2.1 Task Formulation\nWe formalize the task of autonomous SWE agent\na Partially Observable Markov Decision Pro-\nas\ncess (POMDP)\n(Murphy 2025), defined by the tuple\n⟨Z, A, Ω, T, O, R, γ⟩:",
            "content": "Z: Set of true, latent environment states. A: Set of actions available to the agent. Ω: Set of observations the agent can receive. (zz, a): Transition probability to state from state after action a. O(oz): Probability of receiving observation from state z. R(z, a): Reward provided from taking action from state z. γ [0, 1]: Discount factor. At each step t, the environments latent state zt is unobservable. Instead, the agent maintains history ht of all previous actions and observations: ht = (o0, a0, o1, a1, . . . , at1, ot). complete history corresponding to finished episode is called trajectory τ . The agents policy, parameterized by θ, selects the next action based on this history: πθ(atht). Due to the LLMs autoregressive nature, the policy probability factorizes over tokens within an action: πθ(atht) = at (cid:89) k= πθ(at,kht, at,<k). (1) Here, at,k is the k-th token of the action at, and at is the length of the action in tokens. In our SWE setting, these components are instantiated as follows: Environment State (zt): Complete, hidden software environment state, including file system, source code, and running processes. Action (at): command string generated autoregressively by the LLM (potentially with reasoning and tool calls). Observation (ot): The execution output from command (typically stdout, stderr, and exit codes). The initial observation o0 contains the natural-language description of the task from the GitHub issue. History (ht): Complete observable history of past actions and observations, conditioning the policys next decision. Our goal is to find policy πθ that maximizes the expected cumulative reward. The cumulative reward for trajectory τ is the sum of rewards over all its steps, G(τ ) = (cid:80)τ 1 t=0 R(zt, at), where τ denotes the number of actions in the trajectory. Given our sparse reward structure where immediate rewards R(zt, at) are zero for all non-terminal steps, this simplifies to the terminal reward, which we denote as R(τ ). R(τ ) is 1 if the final proposed patch passes the validation test suite, and 0 otherwise. We use this terminal reward as the sole signal for policy optimization."
        },
        {
            "title": "2.2 From PPO to DAPO\nReinforcement learning has traditionally been dominated by\nProximal Policy Optimization (PPO) (Schulman et al. 2017),\nwhich employs a learned critic to estimate the advantage of\neach generated action. While powerful, PPO introduces ex-\ntra overhead and sensitivity to hyperparameter tuning. In our\nsetting, the PPO objective can be the following:",
            "content": "JPPO(θ) = o0D, τ πold(o0) (cid:34) 1 τ τ 1 (cid:88) t= 1 at at (cid:88) k=1 (cid:16) min ρt,k(θ)At,k, clip(ρt,k(θ), 1 ε, 1 + ε)At,k (cid:35) (cid:17) , (2) where o0 indicates that initial observations are sampled from the distribution of tasks in our dataset. ρt,k(θ) πθ(at,kht,at,<k) πθold (at,kht,at,<k) is the probability ratio for the k-th token in action at, and At,k is its corresponding advantage estimate. Group-Relative Policy Optimization (GRPO) (Shao et al. 2024) eliminates the need for learned advantage estimator. Instead, it computes Monte Carlo estimate of the advantage for each trajectory by normalizing its terminal reward against the mean and standard deviation of rewards from group of trajectories sampled from the same policy. For given initial observation, GRPO samples group of complete trajectories {τ (1), . . . , τ (G)} from the policy and collects their terminal rewards {R(τ (1)), . . . , R(τ (G))}. The advantage of trajectory τ (i) is calculated relative to the average reward of the group, = 1 i=1 R(τ (i)): (cid:80)G ˆA(i) = R(τ (i)) σR + δ , (3) where σR is the standard deviation of rewards in the group and δ is small constant for numerical stability. This single advantage value ˆA(i) is then applied to all tokens within the trajectory τ (i). The GRPO objective function uses the same clipped surrogate objective as PPO but substitutes the critic-based advantage with this group-wise advantage estimate: (cid:34) JGRPO(θ) = {τ (i)}G o0D, i=1πold(o0) 1 (cid:88) i=1 1 τ (i) τ (i)1 (cid:88) t= 1 a(i) a(i) (cid:88) k=1 (cid:16) min t,k(θ) ˆA(i), clip(ρ(i) ρ(i) t,k(θ), 1 ε, 1 + ε) ˆA(i)(cid:17) (cid:35) . (4) DAPO extends GRPO by introducing several practical improvements for enhanced stability and efficiency. Key modifications include: Clip higher: Instead of symmetric clip range (1ε, 1+ ε), DAPO uses asymmetric bounds (1 εlow, 1 + εhigh). Typically, εhigh is set higher than εlow to better prevent policys entropy collapse. Dynamic sampling: Sampled commands that carry no learning signal (i.e., ˆA(i) = 0) are filtered out, focusing computation on effective updates. Soft overlong punishment: Specifically, when the response exceeds predefined length threshold, gradual penalty within specified interval is applied. Within this interval, the longer the response, the greater the punishment it receives. This penalty is added to the original testbased reward, thereby signaling to the model to avoid excessively long responses. Token-level loss: The original GRPO algorithm uses sample-level averaging, where each trajectory has equal weight. DAPO suggests averaging the loss over all tokens in the batch, as shown in Equation (5). This method ensures every token across the batch contributes equally to the gradient, giving greater influence to longer trajectories. (cid:34) JDAPO(θ) = {τ (i)}G o0D, i=1πold(o0) 1 (cid:80)τ (i)1 t=0 (cid:80)G i= t,k(θ) ˆA(i), clip(ρ(i) ρ(i) (cid:88) τ (i)1 (cid:88) a(i) (cid:88) (cid:16) min i=1 a(i) (cid:35) t,k(θ), 1 εlow, 1 + εhigh) ˆA(i)(cid:17) k=1 t=0 . (5) Our implementation builds on DAPO, but introduces some modifications for the multi-turn SWE setting, further discussed in detail."
        },
        {
            "title": "2.3 Agent Scaffolding",
            "content": "in our work employs ReAct-style The SWE agent loop (Yao et al. 2023), interacting with the environment through predefined set of tools. The entire actionobservation history conditions every decision. We provide an evaluation infrastructure supporting over 7,000 curated SWE tasks, each with natural-language issue description and reproducible environment. Our SWE agent interacts with an environment using the following tools: Arbitrary shell commands (ls, cat, grep, etc.). An edit command that replaces specified range of lines in file with new text. The command requires the agent to provide the replacement text with precise indentation and can operate on either the currently open file or one specified by file path. Custom search and navigation utilities (e.g., search file, open, goto). submit command that takes no arguments, signals that the agent has finished its work. This action terminates an episode. Each SWE task includes GitHub-style issue with natural language description, failing test suite that evaluates final patch correctness, and sandboxed environment initialized from repository snapshot."
        },
        {
            "title": "3.1 Data\nWe start\nfrom the publicly available SWE-REBENCH\ndataset, containing 21,336 tasks sourced from approxi-\nmately 3,400 Python GitHub repositories. Each task in-\ncludes a GitHub issue, a ground-truth solution patch, a vali-\ndation test suite, and a reproducible environment.",
            "content": "To ensure high-quality and stable training, we apply rigorous filtering criteria, resulting in 7,249 tasks selected according to: Task correctness: Remove tasks causing test failures due to invalid references or imports (e.g., AttributeError, ImportError), as indicated in task metadata. These cases would require agent to guess particular identifier names. Controlled complexity: Include only tasks modifying up to seven files and fewer than 500 lines of code to maintain manageable complexity. LLM-assessed quality: Exclude tasks with unclear issue descriptions, overly complex tasks, or flawed test patches according to LLM-generated scores from the original dataset. This is done by removing all problems with the assigned LLM score of 3.0 in the metadata field. Deterministic tests: Remove tasks exhibiting flakiness upon repeated executions of tests (50 trials), ensuring stable training signals. Non-deterministic test behavior occurs mostly due to external service calls or floating-point inaccuracies. For evaluation, we use the standard SWE-BENCH VERIFIED benchmark, 50-problem random subset (referred to as VERIFIED-50) for faster intermediate evaluations, and the latest monthly splits of SWE-REBENCH (MayJune) which are not included in the training set, ensuring fair and decontamination-free comparison. The full list of VERIFIED-50 problems can be found in Appendix B."
        },
        {
            "title": "3.2 Phase 1: Rejection Fine-Tuning (RFT)\nWe start from the open-weight Qwen2.5-72B-Instruct\nmodel. Out of the box it reaches only ∼11% on SWE-\nBENCH VERIFIED with the dominant failure being incorrect\ninstruction following, resulting in commands that are not in\nthe appropriate format.",
            "content": "To address this problem and warm up the model for the RL stage, we perform rejection fine-tuning (RFT). First, we run the initial checkpoint 10 times on the 7,249 SWEREBENCH tasks and keep only those trajectories whose patches pass the test suite. This yields set of 6,548 successful trajectories, on which we run single epoch of supervised fine-tuning. During this epoch, we mask every agent turn that triggered an environment-formatting error, thereby focusing the loss on valid actions and improving adherence to the tool structure (see Figure 3). After RFT, the models accuracy rises to 20%  (Table 1)  . All hyperparameters for this and subsequent stages are listed in the training details section. This section outlines our training methodology, consisting of carefully curated data and two-phase training pipeline optimized explicitly for multi-turn SWE tasks."
        },
        {
            "title": "3.3 Phase 2: Multi-Turn RL\nThe core of our work involves applying RL over thousands\nof problems in an iterative loop. Each RL iteration includes:",
            "content": "This second phase boosts performance to 39.0% on SWE-BENCH VERIFIED. On the held-out SWE-REBENCH evaluation sets, it achieves 35.0% on the May split and 31.7% on the June split. The significant gap between our final Pass@1 score (39.0%) and Pass@10 score (58.4%) suggests that while the agents single best guess may be incorrect, valid solution frequently exists within its top proposals. This indicates strong potential for application of re-ranking or best-of-n selection mechanisms to further improve performance."
        },
        {
            "title": "4.1 Main Results",
            "content": "Our two-phase procedure yields substantial improvements. Rejection fine-tuning provides an initial performance boost by enhancing the models ability to interact correctly with the environment. This is followed by over 100 RL iterations that progressively refine the agents policy. The final model achieves 39.0% on the full SWE-BENCH VERIFIED. Performance trends are shown in Figure 4. For head-to-head comparisons, we evaluate DeepSeekV3-0324, Llama-4 Maverick, Qwen3-235B no-thinking, and Qwen3-32B no-thinking within the same environment and tool setup (see Table 1) on both SWE-BENCH VERIFIED and SWE-REBENCH. To benchmark our final model against specialized SWE agents, we summarize recent results in Table 2. 4."
        },
        {
            "title": "Infrastructure",
            "content": "The described process is based on fully synchronous RL training process, meaning that inference and training stages are interleaved: once rollout generation is completed, trajectories are used for training. This setup enables fully onpolicy training with no policy lag between sampling and updates. However, as described earlier, we sample 10 trajectories for each problem. This results in 2-8 optimization steps per iteration depending on batch size and the number of trajectories with ˆA(i) = 0. We believe that asynchronous frameworks can offer greater scalability, but to eliminate complexities like trajectory lag, synchronous framework is reasonable choice for these experiments. key drawback of the synchronous approach is the straggler problem: the time for each generation iteration is determined by the single slowest trajectory to complete, which can reduce overall throughput. To enable full-parameter training on sequences up to 131k tokens, we leverage context parallelism, which partitions long sequences across GPUs. All training and inference are conducted on 16 H200 nodes. We describe more details regarding infrastructure in Appendix C."
        },
        {
            "title": "4.3 Hyperparameters",
            "content": "Inference. For rollout generation, we run the model with temperature of 1.0, explicitly disabling all other decoding parameters such as top p, top k, min p, repetition penalty Figure 3: An example trajectory from the agents interaction used in RFT. Only green (successful) assistant turns contribute to training loss. Problem sampling: subset of problems is selected from the training pool. Rollouts generation: We sample = 10 complete trajectories per problem using the current policy. Reward computation: The final reward signal, Rfinal(τ ), combines the binary success reward from testing with penalty for trajectory length. We add linear penalty for exceeding certain number of steps. The reward is computed as follows: Rfinal(τ ) = R(τ ) + Rlength(τ ) (6) (cid:40) Rlength(τ ) = if τ < Lthr if τ Lthr 0, Lthrτ TmaxLthr Here, R(τ ) {0, 1} is the terminal reward defined in the previous section, Lthr and Tmax are hyperparameters representing the penalty threshold and maximum episode length, respectively. (7) , Advantage estimation: Rewards are averaged and normalized within each 10-sample group; samples with zero advantage are dropped. Optimization: We update models all parameters using DAPOs clipped token-level objective. We begin RL training at 65k context length, already beyond the default 32k of most open-weight LLMs, but still insufficient for repositories with long stack traces or diff histories. Once performance plateaus at approximately 32%, we switch to longer context setting where we increase the context window to 131k and double the maximum number of agent turns Tmax from 40 to 80. For stable updates in this second stage, we also decrease the high clip bound ϵhigh, increase the batch size, and lower the number of instances sampled on each iteration, i.e., shifting training closer to onpolicy mode (see Table 3). As part of the transition between stages, we perform one-time curriculum adjustment by refreshing the training pool. We define tasks cumulative resolution rate as its success rate over all prior training iterations. Tasks are removed from the pool if their cumulative resolution rate exceeds 2 3 (indicating the task can be solved reliably) or remains at zero (indicating it is likely unsolvable). This filtering reduces the training pool from 7,249 to 2,028 tasks. This strategy focuses computational resources in Stage 2 on problems that are most beneficial for learning. Model SWE-bench Verified SWE-rebench May SWE-rebench June Pass@ Pass@10 Pass@1 Pass@10 Pass@1 Pass@10 Qwen2.5-72B-Instruct + RFT @ 65k (ours) + Stage 1 RL @ 65k (ours) + Stage 2 RL @ 131k (ours) DeepSeek-V3-0324 Qwen3-235B no-thinking Qwen3-32B no-thinking Llama4 Maverick 11.42 0.24 20.46 0.42 35.74 0.28 39.04 0.50 39.56 0.47 25.84 0.37 20.40 0.34 15.84 0.54 31.0 43.0 54.6 58.4 62.2 54.4 44.0 47.2 14.50 1.33 22.50 1.18 36.50 1.59 35.00 1.54 36.75 0.92 27.25 1.15 21.75 1.54 19.00 1.72 40.0 45.0 55.0 52.5 60.0 57.5 50.0 50.0 14.63 1.03 20.98 1.51 31.22 0.80 31.71 1.31 31.46 1.38 22.93 2.16 17.56 1.30 13.66 1.79 36.6 43.9 53.7 53.7 58.5 48.8 36.6 39. Table 1: Comparison of our models against open-weight baselines on SWE-BENCH VERIFIED and SWE-REBENCH. Pass@1 metrics are averaged over 10 runs and reported with the standard error of the mean. Our final model and the baseline models are evaluated with 131k context length; our intermediate models are evaluated at the context length used during their respective training stages (65k). All models use their default decoding parameters as specified in their Hugging Face configuration. Agent Base Model SWE-bench Ver. before SWE-bench Ver. after Teacher Distillation Qwen2.5-72B-Instruct Qwen3-32B Qwen3-14B Qwen2.5-72B-Base Ours DeepSWE-32B, Luo et al. SkyRL-Agent-14B-v0, Cao et al. SWE-Fixer-72B, Xie et al. SWE-agent-LM-32B, Yang et al. Qwen2.5-Coder-32B-Instruct Qwen2.5-Coder-32B-Instruct Skywork-SWE-32B, Zeng et al. Qwen2.5-Coder-32B-Instruct SWE-Gym-32B, Pan et al. Qwen2.5-72B-Instruct SWESynInfer-72B, Ma et al. Qwen2.5-Coder R2EGym-Agent-32B, Jain et al. 11.4% 23% 18.0% 14.3% 6.4% 7.0% 25.4% 7.0% 39.0% 42.2% 21.6% 30.2% 40.2% 38.0% 20.6% 30.2% 34.4% No No No Yes Yes Yes Yes Yes Yes Table 2: Comparison of specialized SWE agents on SWE-BENCH VERIFIED. The before column shows the Pass@1 score of the base model prior to training, while the after column shows the final performance of the fully trained agent. and others. This ensures unbiased sampling, which is critical for the validity of importance sampling ratios used during training. We demonstrate the dangers of biased sampling procedures later in our findings. Training. For RFT, we perform one epoch of training at 65k context length, with learning rate of 5106, AdamW optimizer with weight decay of 0.1, 10 warmup steps, and cosine decay scheduler with end lr = 0.0. We use batch size of 64, resulting in 50 gradient update steps. For the RL pipeline, we list all hyperparameters changed across stages in Table 3. Both setups share common settings: gradient clipping = 1.0; AdamW with β1 = 0.9, β2 = 0.999, ε = 1 108, weight decay of 0.1; learning rate of 106 and num epochs = 1. For Qwen2.5-72B-instruct models we use YaRN positional encoding with factor = 4.0 to enable 131k context length training and inference."
        },
        {
            "title": "4.4 Findings\nA commonly adopted practice in dataset preparation, also\nmentioned in DeepSWE (Luo et al. 2025), is to filter or\nmask trajectories that exceed the model’s maximum context\nlength. This is often motivated by the desire to reduce reward\nnoise. However, we find this must be applied with caution.",
            "content": "Manually crafted heuristics can introduce biases, breaking the assumption that the training data is sampled from the same distribution as the policy being optimized. In our setting, these long trajectories often occur when the agent is stuck in repetitive loop. By discarding these trajectories, one also discards specific negative examples of this failure mode. As result, the agent is not penalized for this looping behavior and fails to learn how to break out of such cycles, which can lead to it occurring more frequently during training. We also observe more subtle instability related to discrepancies between sampling and training. Midway through training, we upgraded the vLLM (Kwon et al. 2023) runtime, which introduced internal changes to decoding parameters, enabling top and min by default. While this initially improved evaluation metrics, it caused performance to degrade after 510 training iterations. We attribute this to mismatch between the rollout and training distributions. The DAPO objective relies on the importance sampling ratio ρt,k(θ) to weight the advantage term. This is valid only if rollouts are sampled from πold. Using different decoding parameters amounts to sampling from modified distribution, = πold, which makes the ratio an incorrect estimaπrollout tor and leads to biased gradient updates. Once the sampling configuration was restored, stability recovered. Figure 4: detailed performance trend of RL-trained agent over all iterations. Statistics include Pass@1, Pass@10, the number of submit commands and the average number of steps per trajectory. All metrics are computed on VERIFIED-50. Training Stage Problems / Iteration Total Problems Batch Size Clip Range Stage 1 Stage 2 300 7249 2028 128 256 [0.2, 0.3] [0.2, 0.26] Table 3: Key hyperparameters across the two RL training stages."
        },
        {
            "title": "5 Discussion and Future Work\nOur work successfully demonstrates that modern reinforce-\nment learning algorithms, specifically a modified DAPO,\ncan train capable agents for complex, interactive software\nengineering tasks. This process, however, highlights funda-\nmental challenges in agent-based learning and reveals sev-\neral key directions for future research:",
            "content": "Sparse Rewards and Credit Assignment. fundamental challenge is that the agent receives only single binary success signal at the end of long trajectory. This sparsity makes it difficult to perform credit assignment, that is, to identify which specific actions in long sequence were crucial for the outcome. Broadcasting single advantage estimate across thousands of preceding tokens can result in noisy and inefficient policy updates. Several research directions could address this: (i) reward shaping, which involves designing intermediate rewards based on signals like passing subset of tests or reducing compiler errors; (ii) training an auxiliary critic or value head to provide step-level advantage estimates, enabling more granular updates; and (iii) prefix sampling, where rollouts are initiated from shared non-empty trajectory prefix to better isolate the impact of later decisions. Uncertainty and Risk-Awareness. The binary, successbased reward objective encourages the agent to submit patch at any cost, which leads it to act confidently even when solution is unlikely. For real-world deployment, agents must recognize when to abstain. This requires better uncertainty estimation; for instance, by training the model to explicitly output confidence score or by using the policys output entropy as proxy for uncertainty. Such estimates would enable precision-recall trade-off, allowing the agent to decide when to halt or to apply more compute for best-of-n selection without an external outcome-supervision model. References Antoniades, A.; Orwall, A.; Zhang, K.; Xie, Y.; Goyal, A.; and Wang, W. 2025. SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement. arXiv:2410.20285. AugmentCode. 2025. AugmentCode: AI Software Developer Platform. Badertdinov, I.; Golubev, A.; Nekrashevich, M.; Shevtsov, A.; Karasik, S.; Andriushchenko, A.; Trofimova, M.; Litvintseva, D.; and Yangel, B. 2025. SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents. arXiv:2505.20411. Cao, S.; Hegde, S.; Li, D.; Griggs, T.; Liu, S.; Tang, E.; Pan, J.; Wang, X.; Malik, A.; Neubig, G.; Hakhamaneshi, K.; Liaw, R.; Moritz, P.; Zaharia, M.; Gonzalez, J. E.; and Stoica, I. 2025. SkyRL-v0: Train Real-World Long-Horizon Agents via Reinforcement Learning. Dou, S.; Liu, Y.; Jia, H.; Xiong, L.; Zhou, E.; Shen, W.; Shan, J.; Huang, C.; Wang, X.; Fan, X.; Xi, Z.; Zhou, Y.; Ji, T.; Zheng, R.; Zhang, Q.; Huang, X.; and Gui, T. 2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback. arXiv:2402.01391. Jain, N.; Singh, J.; Shetty, M.; Zheng, L.; Sen, K.; and Stoica, I. 2025. R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents. arXiv:2504.07164. Jimenez, C. E.; Yang, J.; Wettig, A.; Yao, S.; Pei, K.; Press, O.; and Narasimhan, K. 2024. SWE-bench: Can Language Models Resolve Real-World GitHub Issues? arXiv:2310.06770. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Luo, M.; Jain, N.; Singh, J.; Tan, S.; Patel, A.; Wu, Q.; Ariyak, A.; Cai, C.; Venkat, T.; Zhu, S.; Athiwaratkun, B.; Roongta, M.; Zhang, C.; Li, L. E.; Popa, R. A.; Sen, K.; and Stoica, I. 2025. DeepSWE: Training Stateof-the-Art Coding Agent from Scratch by Scaling RL. https://pretty-radio-b75.notion.site/DeepSWE-Traininga-Fully-Open-sourced-State-of-the-Art-Coding-Agentby-Scaling-RL-22281902c1468193aabbe9a8c59bbe33. Notion Blog. Ma, Y.; Cao, R.; Cao, Y.; Zhang, Y.; Chen, J.; Liu, Y.; Liu, Y.; Li, B.; Huang, F.; and Li, Y. 2024. Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement. arXiv:2411.00622. Murphy, K. 2025. Reinforcement Learning: An Overview. arXiv:2412.05265. Pan, J.; Wang, X.; Neubig, G.; Jaitly, N.; Ji, H.; Suhr, A.; and Zhang, Y. 2025. Training Software Engineering Agents and Verifiers with SWE-Gym. arXiv:2412.21139. Refact AI. 2025. Refact.ai: AI Software Engineering Agent. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347. Seed, B.; :; Chen, J.; Fan, T.; Liu, X.; Liu, L.; Lin, Z.; Wang, M.; Wang, C.; Wei, X.; Xu, W.; Yuan, Y.; Yue, Y.; Yan, L.; Yu, Q.; Zuo, X.; Zhang, C.; Zhu, R.; An, Z.; Bai, Z.; Bao, Y.; Bin, X.; Chen, J.; Chen, F.; Chen, H.; Chen, R.; Chen, L.; Chen, Z.; Chen, J.; Chen, S.; Chen, K.; Chen, Z.; Chen, J.; Chen, J.; Chi, J.; Dai, W.; Dai, N.; Dai, J.; Dou, S.; Du, Y.; Du, Z.; Duan, J.; Dun, C.; Fan, T.-H.; Feng, J.; Feng, J.; Feng, Z.; Fu, Y.; Fu, W.; Fu, H.; Ge, H.; Guo, H.; Han, M.; Han, L.; Hao, W.; Hao, X.; He, Q.; He, J.; He, F.; Heng, W.; Hong, Z.; Hou, Q.; Hu, L.; Hu, S.; Hu, N.; Hua, K.; Huang, Q.; Huang, Z.; Huang, H.; Huang, Z.; Huang, T.; Huang, W.; Jia, W.; Jia, B.; Jia, X.; Jiang, Y.; Jiang, H.; Jiang, Z.; Jiang, K.; Jiang, C.; Jiao, J.; Jin, X.; Jin, X.; Lai, X.; Li, Z.; Li, X.; Li, L.; Li, H.; Li, Z.; Wan, S.; Wang, Y.; Li, Y.; Li, C.; Li, N.; Li, S.; Li, X.; Li, X.; Li, A.; Li, Y.; Liang, N.; Liang, X.; Lin, H.; Lin, W.; Lin, Y.; Liu, Z.; Liu, G.; Liu, G.; Liu, C.; Liu, Y.; Liu, G.; Liu, J.; Liu, C.; Liu, D.; Liu, K.; Liu, S.; Liu, Q.; Liu, Y.; Liu, K.; Liu, G.; Liu, B.; Long, R.; Lou, W.; Lou, C.; Luo, X.; Luo, Y.; Lv, C.; Lv, H.; Ma, B.; Ma, Q.; Ma, H.; Ma, Y.; Ma, J.; Ma, W.; Ma, T.; Mao, C.; Min, Q.; Nan, Z.; Ning, G.; Ou, J.; Pan, H.; Pang, R.; Peng, Y.; Peng, T.; Qian, L.; Qian, L.; Qiao, M.; Qu, M.; Ren, C.; Ren, H.; Shan, Y.; Shen, W.; Shen, K.; Shen, K.; Sheng, G.; Shi, J.; Shi, W.; Shi, G.; Cao, S. S.; Song, Y.; Song, Z.; Su, J.; Sun, Y.; Sun, T.; Sun, Z.; Wan, B.; Wang, Z.; Wang, X.; Wang, X.; Wang, S.; Wang, J.; Wang, Q.; Wang, C.; Wang, S.; Wang, Z.; Wang, C.; Wang, J.; Wang, S.; Wang, X.; Wang, Z.; Wang, Y.; Wang, W.; Wang, T.; Wei, C.; Wei, H.; Wei, Z.; Wei, S.; Wu, Z.; Wu, Y.; Wu, Y.; Wu, B.; Wu, S.; Wu, J.; Wu, N.; Wu, S.; Wu, J.; Xi, C.; Xia, F.; Xian, Y.; Xiang, L.; Xiang, B.; Xiao, B.; Xiao, Z.; Xiao, X.; Xiao, Y.; Xin, C.; Xin, S.; Xiong, Y.; Xu, J.; Xu, Z.; Xu, C.; Xu, J.; Xu, Y.; Xu, W.; Xu, Y.; Xu, S.; Yan, S.; Yan, S.; Yang, Q.; Yang, X.; Yang, T.; Yang, Y.; Yang, Y.; Yang, X.; Yang, Z.; Yang, G.; Yang, Y.; Yao, X.; Yi, B.; Yin, F.; Yin, J.; Ying, Z.; Yu, X.; Yu, H.; Yu, S.; Yu, M.; Yu, H.; Yuan, S.; Yuan, J.; Zeng, Y.; Zhan, T.; Zhang, Z.; Zhang, Y.; Zhang, M.; Zhang, W.; Zhang, R.; Zhang, Z.; Zhang, T.; Zhang, X.; Zhang, Z.; Zhang, S.; Zhang, W.; Zhang, X.; Zhang, Y.; Zhang, Y.; Zhang, G.; Zhang, H.; Zhang, Y.; Zheng, R.; Zheng, N.; Zheng, Z.; Zheng, Y.; Zheng, C.; Zhi, X.; Zhong, W.; Zhong, C.; Zhong, Z.; Zhong, B.; Zhou, X.; Zhou, N.; Zhou, H.; Zhu, H.; Zhu, D.; Zhu, W.; and Zuo, L. 2025. Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning. arXiv:2504.13914. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. TractoAI. 2025. TractoAI: Cloud platform for deploying, scaling, and monitoring AI and Big Data workloads. Trae. 2025. Trae: AI Software Developer Platform. Wang, X.; Li, B.; Song, Y.; Xu, F. F.; Tang, X.; Zhuge, M.; Pan, J.; Song, Y.; Li, B.; Singh, J.; Tran, H. H.; Li, F.; Ma, R.; Zheng, M.; Qian, B.; Shao, Y.; Muennighoff, N.; Zhang, Y.; Hui, B.; Lin, J.; Brennan, R.; Peng, H.; Ji, H.; and Neubig, G. 2025. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. arXiv:2407.16741. Xia, C. S.; Deng, Y.; Dunn, S.; and Zhang, L. 2024. Agentless: Demystifying LLM-based Software Engineering Agents. arXiv:2407.01489. Xie, C.; Li, B.; Gao, C.; Du, H.; Lam, W.; Zou, D.; and Chen, K. 2025. SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution. arXiv:2501.05040. Yang, J.; Jimenez, C. E.; Wettig, A.; Lieret, K.; Yao, S.; Narasimhan, K.; and Press, O. 2024. SWE-agent: AgentComputer Interfaces Enable Automated Software Engineering. arXiv:2405.15793. Yang, J.; Leret, K.; Jimenez, C. E.; Wettig, A.; Khandpur, K.; Zhang, Y.; Hui, B.; Press, O.; Schmidt, L.; and Yang, D. 2025. SWE-smith: Scaling Data for Software Engineering Agents. arXiv:2504.21798. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629. Yu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.; Dai, W.; Fan, T.; Liu, G.; Liu, L.; Liu, X.; Lin, H.; Lin, Z.; Ma, B.; Sheng, G.; Tong, Y.; Zhang, C.; Zhang, M.; Zhang, W.; Zhu, H.; Zhu, J.; Chen, J.; Chen, J.; Wang, C.; Yu, H.; Song, Y.; Wei, X.; Zhou, H.; Liu, J.; Ma, W.-Y.; Zhang, Y.-Q.; Yan, L.; Qiao, M.; Wu, Y.; and Wang, M. 2025. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. arXiv:2503.14476. Zainullina, K.; Golubev, A.; Trofimova, M.; Polezhaev, S.; Badertdinov, I.; Litvintseva, D.; Karasik, S.; Fisin, F.; Skvortsov, S.; Nekrashevich, M.; Shevtsov, A.; and Yangel, B. 2025. Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents. arXiv:2505.13652. Zeng, L.; Li, Y.; Xiao, Y.; Li, C.; Liu, C. Y.; Yan, R.; Wei, T.; He, J.; Song, X.; Liu, Y.; and Zhou, Y. 2025. Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs. arXiv:2506.19290. Related Work Software engineering agents. Early systems, notably SWE-agent (Yang et al. 2024), demonstrated that large language models (LLMs) could effectively operate within sandboxed software environments using predefined toolkits (e.g., shell commands, code editors) and be evaluated via automated unit tests (Jimenez et al. 2024). Subsequent frameworks introduced alternative scaffolding and prompting strategies to enhance model interactions, including Agentless (Xia et al. 2024), OpenHands (Wang et al. 2025), and Moatless (Antoniades et al. 2025). Our work closely aligns with the original SWE-agent design, leveraging similar prompting structures and tool configurations, enabling direct evaluation of reinforcement learning improvements within this established context. Strategies to improve SWE agents. Prior efforts to enhance SWE agent performance beyond improving scaffoldings broadly focus on either test-time exploration or modellevel improvements through supervised training. Test-time exploration strategies, such as Monte Carlo Tree Search (MCTS) (Antoniades et al. 2025) and guided 1-step lookahead (Zainullina et al. 2025), significantly boost task success by exploring multiple solution trajectories. However, these methods are computationally expensive and sometimes introduce infrastructure complexity due to operations such as environment rollbacks and parallel execution paths. Alternatively, many efforts have targeted model-level improvements via supervised fine-tuning (SFT) using expertcurated demonstrations. Prominent examples include SWESMITH (Yang et al. 2025), SWE-Fixer (Xie et al. 2025), and Skywork-SWE (Zeng et al. 2025), all of which achieved success by training open-weight models on extensive demonstration data. In contrast, our method relies exclusively on self-generated interaction data obtained through direct RL training. This approach simplifies data collection, eliminates need for strong teacher models and opens way for iterative self-improvement. learning for Reinforcement coding. Reinforcement learning (RL) has shown notable success in structured like mathematics, exemplified by reasoning domains DeepSeekMath (Shao et al. 2024) and Seed-Thinkingv1.5 (Seed et al. 2025). In the coding domain specifically, RL applications traditionally focused on single-turn code generation tasks (Dou et al. 2024), neglecting the complexities introduced by multi-turn interactions and extended contexts. Recent advancements, however, have started addressing these complexities. Concurrent to our work, DeepSWE (Luo et al. 2025) successfully scaled critic-free RL training to 32B-parameter model (Qwen3-32B), while Sky-RL (Cao et al. 2025) introduced an asynchronous RL pipeline for long-context tasks. Our work contributes to this emerging area by demonstrating complete, multi-stage methodology to successfully apply RL to larger 72B-parameter model with 131k token context. Verified-50 Problems The VERIFIED-50 dataset, which we curate from the SWEBENCH VERIFIED by randomly selecting problems, contains the following problem instances: 1 sympy__sympy-22080 2 django__django-15315 3 django__django-11333 4 matplotlib__matplotlib-20826 5 django__django-11532 6 django__django-16642 7 django__django-14855 8 sphinx-doc__sphinx-8721 9 pylint-dev__pylint-4604 10 sympy__sympy-13615 11 django__django-13089 12 django__django-15987 13 django__django-14725 14 sympy__sympy-14248 15 pytest-dev__pytest-7982 16 django__django-15280 17 scikit-learn__scikit-learn-13142 18 pytest-dev__pytest-5809 19 matplotlib__matplotlib-23299 20 django__django-16560 21 django__django-15103 22 sympy__sympy-16792 23 django__django-14007 24 psf__requests-2317 25 django__django-11880 26 django__django-16136 27 django__django-16661 28 sympy__sympy-17139 29 sphinx-doc__sphinx-8595 30 sympy__sympy-14531 31 django__django-10880 32 sympy__sympy-19346 33 sphinx-doc__sphinx-9229 34 django__django-11265 35 matplotlib__matplotlib-25332 36 scikit-learn__scikit-learn-13135 37 pydata__xarray-6744 38 pydata__xarray-6461 39 sympy__sympy-15017 40 django__django-13417 41 matplotlib__matplotlib-24870 42 django__django-15368 43 django__django-11095 44 django__django-15554 45 pydata__xarray-6992 46 django__django-15863 47 django__django-13363 48 sympy__sympy-13852 49 django__django-14017 50 pylint-dev__pylint-"
        },
        {
            "title": "C Infrastructure Details",
            "content": "Figure 5: One synchronous iteration of the RL pipeline (green: GPU heavy; yellow: CPU heavy). The entire pipeline of distributed agent execution and evaluation is orchestrated at scale using Kubernetes and Tracto AI (TractoAI 2025). The primary compute units are pods, each provisioned with eight H200 GPUs, 32 CPUs, and 960 GiB of CPU RAM. Within this environment, model training is conducted using an internal framework built on JAX, while inference is accelerated using the vLLM framework (Kwon et al. 2023), version 0.7.4. Figure 5 demonstrates the workflow of single synchronous iteration. As illustrated, the training process for new policy begins only after the verification for all trajectories in the batch is complete."
        }
    ],
    "affiliations": [
        "Humanoid",
        "Nebius AI"
    ]
}