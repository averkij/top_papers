{
    "paper_title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians",
    "authors": [
        "Cailin Zhuang",
        "Yaoqi Hu",
        "Xuanyang Zhang",
        "Wei Cheng",
        "Jiacheng Bao",
        "Shengqi Liu",
        "Yiying Yang",
        "Xianfang Zeng",
        "Gang Yu",
        "Ming Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art."
        },
        {
            "title": "Start",
            "content": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians Yaoqi Hu3 Xuanyang Zhang2 Wei Cheng2 Yiying Yang2 Xianfang Zeng2 Gang Yu2 Ming Li4 3 AIGC Research Cailin Zhuang1,2,3 Shengqi Liu2 1 ShanghaiTech University 4 Guangming Laboratory Jiacheng Bao1 2 StepFun https://styleme3d.github.io/ 5 2 0 2 1 2 ] . [ 1 1 8 2 5 1 . 4 0 5 2 : r Figure 1: Our StyleMe3D approach enables versatile, high-quality 3D stylization across diverse styles. Abstract 3D Gaussian Splatting (3D GS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusions latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art. CCS Concepts Computing methodologies Computer vision tasks. Keywords 3D gaussian splatting, style transfer, visual priors"
        },
        {
            "title": "1 Introduction\nThe advent of 3D Gaussian Splatting (3D GS) [29] has transformed\n3D scene representation, offering high reconstruction fidelity and\nreal-time rendering through explicit, anisotropic Gaussian model-\ning. However, its application remains largely confined to photore-\nalistic domains, as existing methods rely heavily on real-world 3D\ndata or multi-view 2D captures, leaving stylized scenariosâ€”such as\ncartoons, anime, games, and virtual worldsâ€”underserved. These",
            "content": "Xuanyang Zhang is the project lead; Corresponding Authors. ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. domains demand not only geometric precision but also artistic expressiveness, where naive extensions of 3D GS often fail due to inadequate style-texture alignment, semantic incoherence, and limited adaptability to abstract aesthetics. While recent works explore 3D GS stylization via 2D priors (e.g., VGG [60] for texture transfer [12, 40, 84], CLIP [54] for semantic guidance [34]), their reliance on simplistic feature extraction and trial-and-error optimization leads to fragmented stylization, over-smoothed details, and inconsistent results across objects and scenes. To address these challenges, we present StyleMe3D, holistic framework for 3D GS style transfer that systematically integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our work is grounded in three critical insights: Geometric preservation: optimizing only the RGB attributes of 3D Gaussians preserves structural integrity while enabling stylization, avoiding the instability of geometry-altering methods. Semantic-aware stylization: effective style transfer requires disentangling and aligning features at low-, medium-, and high-semantic levels, which existing single-prior approaches (e.g., VGG or CLIP alone) cannot achieve. Scalability: robust solution must generalize across isolated 3D objects (e.g., virtual assets) and complex scenes (e.g., openworld environments), capability absent in prior art. Furtehr, StyleMe3D introduces four key components to address these stylization challenges following the above insights, namely Dynamic Style Score Distillation (DSSD), Simultaneously Optimized Scale (SOS),Contrastive Style Descriptor (CSD) and 3D Gaussian Quality Assessment (3DG-QA). Leveraging Stable Diffusion (SD) [55] as semantic prior, DSSD dynamically aligns style patterns from text prompts or reference images with 3D content through gradientbased score matching. To our knowledge, this is the first work to exploit SDs latent space for 3D GS style transfer, overcoming the limitations of VGG/CLIP in capturing nuanced artistic semantics. Existing methods often homogenize style application due to overdependence on low-level features (VGG) or global semantics (CLIP). CSD introduces contrastively trained encoder that extracts medium-level style descriptors from curated style dataset, enabling localized, content-aware stylization (e.g., applying distinct textures to buildings vs. vegetation in scene). We propose multiscale optimization to decouple style-texture details (via VGGs shallow layers) and structural coherence (via deeper layers), preserving high-frequency artistic patterns without distorting geometry. Inspired by conventional image quality assessment (IQA) metrics (e.g., CLIP-IQA [69]), 3DG-QA serves as an aesthetic prior explicitly designed for 3D GS optimization. Trained on human-rated stylized 3D scenes, 3DG-QA encodes holistic aesthetic criteriacomposition harmony, texture sharpness, depth-aware color consistencyinto differentiable loss. During optimization, 3DG-QA guides the model to suppress artifacts (e.g., over-saturation in occluded regions) while enhancing artistic appeal, acting as \"virtual art director\" for 3D style transfer. We validate StyleMe3D on 3D object dataset NeRF synthetic dataset [47] and 3D scene dataset tandt db [29], demonstrating its universality across geometric complexities and artistic styles. StyleMe3D achieves superior stylization fieldlity compared with several state-of-the-art methods StyleGaussian [40], ARF [85] and SGSST [15]. For objects, our framework preserves fine details (e.g., intricate carvings on sculptures) while transferring styles with high fieldlity precision. For scenes, it ensures holistic stylistic consistencymaintaining coherent lighting and color palettes across various settingswithout sacrificing real-time rendering capabilities. We summary our contributions as follows: systematic framework for 3D GS style transfer, resolving geometric preservation, multi-modal style conditioning, and multi-level feature alignment. First integration of Stable Diffusion into 3D GS optimization, enabling semantically coherent stylization beyond VGG/CLIP priors. Novel technical components (DSSD, CSD, SOS and 3DG-QA) that collectively address style localization, detail preservation, and aesthetic quality. By bridging photorealistic 3D reconstruction and artistic stylization, StyleMe3D unlocks new possibilities for immersive, stylized environments while preserving the core advantages of 3D GS: precision, scalability, and real-time performance."
        },
        {
            "title": "2 Related Works\n2.1 2D Generation and Stylization",
            "content": "2D generation has rapidly advanced across generative modeling, customization, conditional control, editing, and stylization. Initial breakthroughs in 2D synthesis with VAEs and GANs [2, 20, 28] were furthered by diffusion models [35, 55, 78, 83], enhancing image quality and diversity for complex manipulation. For efficiency, frequency-based fine-tuning and wavelet VAEs have enabled lightweight models [18, 57]. Personalized generation has also progressed, focusing on customized images [27, 83], video [3, 26], and motion [33]. Text-driven editing now offers extensive control frameworks [1, 11, 23, 32, 45, 56, 62], with character consistency essential for coherent multi-image outputs [21, 38, 71, 82, 89]. Stylization advances emphasize style-content separation, with cross-attention-based transfer [10, 67, 82] and shared attention mechanisms for coherence [77]. Frequency-domain techniques aid diffusion control [17], while Aligning style with textual cues [37], cross-domain fusion [52] and FFT-based transfer [22] expand style applications. In this paper, we aim to style 3D GS and these 2D methods give us lot of insights and priors that can be reused in the 3D field. 2.2 3D Generation Native 3D generation has progressed significantly with core representations such as meshes [5, 75, 79] and point clouds [50, 59, 81]. Meshes enable continuous surface modeling, while point clouds allow flexible spatial detail. This field now includes singleview 3D generation [42, 43] for full reconstructions from minimal input and multi-view methods [4, 41, 58, 63, 70] that ensure crossview consistency. Texture synthesis, particularly with advanced UV mapping [7, 30, 44], enhances realism and surface detail in 3D models. StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Figure 2: Overview of our 3D stylization framework (StyleMe3D): (a) Style Purification: Extracts and refines style representations via Style Cleaning in CLIP space, removing content interference from reference images. (b) Multi-Expert Stylization: The Dynamic Style Score Distillation (DSSD) module employs dynamic noise scheduling and adaptive style guidance, integrating latent losses to achieve consistent stylization step by step. Integrates three specialized components within the Dynamic Style Score Distillation (DSSD) framework: Simultaneously Optimized Scale (SOS): Adaptive noise scheduling for texture preservation. Contrastive Style Descriptor (CSD): Separates style and content via contrastive learning for style similarity score. CLIP-IQA: Quality-guided refinement using antonymic semantic prompts. (c) Progressive Consistency Optimization (Style Outpainting): Progressive outpainting achieves multi-view style propagation. Ensures coherent through iterative latent alignment, eliminating multi-view dependencies. Text-guided 3D generation has also advanced with Score Distillation Sampling (SDS) [53] and its variants [66, 73], enabling controllable, diverse 3D synthesis. These techniques support artistic scene generation [36] and multimodal inputs (text and image) [63, 64, 74, 80]. Recent improvements in latent diffusion models further enhance the expressiveness and creative potential of text-to-3D generation [80, 88], and more and more multi-view [6, 8, 51] and 3D dataset [13, 76] still stimulate the development of this field. While 3D generation is not our core task in this work and we define the 3D GS stylization as post-training task which further boradcasts 3D GS to more various applications. 2.3 3D Style Transfer For 3D stylization, methods like [25, 48] embed styles directly into 3D structures, while radiance field-based methods [49, 65, 85] achieve style transfer through optimization for enhanced scene realism. Though HyperNet [9] enables arbitrary style embedding in MLPs, it suffers from slow rendering and detail loss, while StyleRF [39] offers zero-shot stylization by transforming radiance field features but lacks adaptability and control. Recent advances in 3D stylization have explored various techniques to embed artistic styles into 3D content, with reference-based methods like [46, 87] for controlled stylization and arbitrary reference techniques [40, 84] for flexible style transfer. Scalable 3D style transfer brings the 3d stylized resolution up to 4K by SOS Loss [15]. Stylized Score Distillation [31] and 3D-aware diffusion models [74] further expand these capabilities. Different from previous works, we systematically analyzed the 3D GS stylization task, proposed more comprehensive approach to allivate the core challenges within this task and achieved superior performance."
        },
        {
            "title": "3 Method",
            "content": "In this section, we elaborate on our comprehensive algorithmic framework for 3D style transfer using 2D priors. We first formally define our core task: performing style transfer on reconstructed 3D Gaussian Splatting (3D GS) representations while preserving structural fidelity in 3.1. To address the inherent challenges in crossdimensional style adaptation, we propose StyleMe3D - systematic framework comprising mixture of four encoders that collectively resolve critical challenges in 3D style migration from Sec.3.2 to Sec.3.5 and is unified in Sec.3.6."
        },
        {
            "title": "3.1 The definination of Stylizing 3D GS\nWe define initial 3D GS as a pre-trained task, while redefining 3D\ngaussian stylization as a post-training task. Unlike conventional\n3D generation tasks that begin from scratch, our approach applies\nstylization to pre-reconstructed 3D gausion for both 3D objects and\nscenes, allowing for enhanced control over style application while\npreserving the underlying geometry.\nFirstly, we define the 3D gaussian reconstruction process as:",
            "content": "1 ğ‘ min Î˜ ğ‘ ğ‘£=1 ğ‘€ğ‘†ğ¸ (R (Cğ‘£; Î˜), ğ¼ğ‘”ğ‘¡ ğ‘£ ) (1) where Î˜ = {(ğ‘¢ğ‘–, Î£ğ‘–, ğ›¼ğ‘–, ğ‘ğ‘–,0, (ğ‘ğ‘–,ğ‘—,ğ‘˜ ) ğ‘—,ğ‘˜ ))}ğ‘ ğºğ‘ğ‘¢ğ‘ ğ‘ ğ‘–ğ‘ğ‘›ğ‘  represents the 3D gaussian, ğ‘ğ‘–,0 is the main color and ğ‘ğ‘–,ğ‘—,ğ‘˜ is the coefficient. (Cğ‘£; Î˜) means render 3D gaussian and ğ¼ğ‘”ğ‘¡ ğ‘– means the ground truth image from the viewpoint Cğ‘£ respectively. ğ‘–=1 ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. After obtaining the optimized 3D gaussian, we further formulate the 3D gaussian style transfer process with 2D prior as follows: 1 ğ‘ min Î˜ ğ‘ ğ‘£=1 (R (Cğ‘£; Î˜); ğœ™, ğ‘…) (2) where ğœ™ means the 2D prior and ğ‘… means the reference prompt, like text prompts or image prompts. means the loss function to further optimize the 3D gaussian which is initialized with Î˜ from Eq.1. In the style transfer task, we aim to only change the 3D gaussian stylization rather than the geometry content. We achieve geometrystyle decoupling in 3D gaussian by leveraging the inherent separation of geometric and color parameters in its parametric representation. Specifically, our style transfer framework exclusively optimizes the color parameters Î˜ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ while maintaining frozen geometric attributes during the stylization process as: min Î˜ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ 1 ğ‘ ğ‘ ğ‘£=1 (R (Cğ‘£; Î˜); ğœ™, ğ‘…) (3) where ğ‘›view represents the total number of rendering views and ğ‘›opt denotes the required optimizations per view, managed iteratively by ğ‘–step. Local Guidance. Local guidance focuses on single-view optimization, maximizing stylization quality for individual views, albeit at the potential expense of global consistency. The local guidance schedule is defined as: ğ›¼step = ğ‘–step mod ğ‘›opt ğ‘›opt (5) The effectiveness of these modes in balancing stylization strength and consistency is discussed in Sec. 4.3. To maximize the stylization outcome, we combine both guidance modes for complementary strengths. Fine Timestep Sampling. Fine timestep sampling enhances temporal resolution by focusing on low-noise intervals for more granular optimization, with noise progressively decreasing from high to low levels. This sampling strategy is formulated as: ğ‘¡ = ğ‘…ğ‘œğ‘¢ğ‘›ğ‘‘ ((1 ğ›¼ 0.5 step) T).clip(ğ‘‡min,ğ‘‡max) (6) We further discuss how to instantiate the L, ğœ™ and ğ‘… with different formulations and jointly improve the stylization effectiveness in the following sections. where denotes the total timesteps, with ğ‘‡min and ğ‘‡max setting the bounds. Higher noise initialization effectively eliminates outlier Gaussian, refining the stylization outcome."
        },
        {
            "title": "3.2 Dynamic Style Score Distillation",
            "content": "In this section, we distill the prior from the 2D stable diffusion model [55] and use both text and image prompt for style transfer. Style Cleaning. Inspired by InstantStyle [67], we use pre-trained CLIP model for Style Cleaning to isolate pure style information. In CLIP space, we filter out style-irrelevant details by subtracting content descriptors from style embeddings. Specifically, descriptions of the style reference image are generated using captioning model (e.g., GPT-4V) to distinguish content-related descriptors. The CLIP Text Encoder extracts Content Text Embedding (or both content and style) from these descriptors, while the CLIP Image Encoder produces Style Image Embedding. Subtracting Content Text Embedding from Style Image Embedding (and adding Style Text Embedding) yields Final Style Embedding containing only style-related information. The style clean process is shown in Fig. 2. Progressive Style Outpainting (PSO). PSO is novel style guidance method for consistent and detailed style propagation in multiview 3D stylization (see Fig. 2). Using 2D style priors provided by an image stylization diffusion model [16], we redefine multi-view guidance as progressive outpainting task. By integrating sparseview RGB loss with dense-view SDS loss, PSO ensures consistent 3D stylization across views. Instead of random view selection, our method incrementally propagates style information to adjacent views, enhancing style coherence with each step. Specifically, PSO consists of two primary guidance modes, namely gobal guidance and local guidance. Global Guidance. In the global gudance stage, uniform noise level is applied to all views before stepwise reduction, defined as: Dynamic Style Score Distillation (DSSD). As shown in Fig. 2(b). DSSD further extends score distillation by applying dynamic CFG (Classifier-Free Guidance) [24] scale coefficient to optimize the intensity of style guidance. Fixed CFG values can lead to oversmoothing (low CFG) or oversaturation (high CFG). To counter this, we introduce dynamic guidance coefficient that adaptively balances fixed CFG values throughout optimization. The adaptive coefficient is defined as: Î”ğœ† = max (cid:16)7.5, ğœ†max (cid:16) ğ›¼ 2 step (cid:17)(cid:17) (7) With this method, we extend the SSD proposed by [31], and define the style loss in latent space as: DSSDğ‘§ 2D = (1 Î”ğœ†ğ‘  )ğœ–ğœ™2D (ğ‘§ğ‘¡ğ‘  ğ‘¦, ğ‘¡ğ‘  ) + Î”ğœ†ğ‘  Ë†ğœ–ğœ™2D (ğ‘§ğ‘¡ğ‘  ğ‘¦, ğ‘ , ğ‘¡ğ‘  ) ğœ–ğ‘ , (8) where ğœ–ğœ™2ğ· () is the predicted noise by the style-based 2D diffusion prior ğœ™. The latent space loss aligns abstract style features, whereas pixelspace loss emphasizes visible characteristics. For stylizing given 3D model, latent loss ensures style feature transfer, while pixel loss provides further reliability in visual output. Defining ğ‘¥ğ‘¡ğ‘  = Decoder(ğ‘§ğ‘¡ğ‘  ), the pixel-space loss is given by: DSSDğ‘¥ 2D = (1 Î”ğœ†ğ‘  )ğœ–ğœ™2D (ğ‘¥ğ‘¡ğ‘  ğ‘¦, ğ‘¡ğ‘  ) + Î”ğœ†ğ‘  Ë†ğœ–ğœ™2D (ğ‘¥ğ‘¡ğ‘  ğ‘¦, ğ‘ , ğ‘¡ğ‘  ) ğœ–ğ‘  (9) Our Dynamic Style Score Distillation (DSSD) objective function integrates latent DSSD and pixel DSSD: Î˜ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ LDSSD (ğ‘¥ = (Cğ‘£; Î˜ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ ); ğœ™, ğ‘…) = Eğ‘¡ğ‘§ ğ‘  ,ğ‘¡ ğ‘¥ ğ‘  ,ğœ–ğ‘§ ğ‘  ,ğœ–ğ‘¥ ğ‘  (cid:34) ğœ” (ğ‘¡) (cid:16) 2DDSSDğ‘§ ğœ†ğ‘§ 2D + ğœ†ğ‘¥ 2DDSSDğ‘¥ 2D (cid:35) (cid:17) ğ‘¥ ğœƒ (10) ğ›¼step = (cid:16) (cid:106) ğ‘–step ğ‘›view (cid:17) (cid:107) mod ğ‘›opt ğ‘›opt (4) where ğœ” (ğ‘¡) is weighting function regulating timestep contributions. StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Further, we optimize the stylized multi-view image ğ¼rgb and the associated mask ğ¼mask for alignment with the input data. If required, additional loss terms such as SSIM loss [72] or LPIPS loss [86] may be integrated to enhance alignment. Thus, our final objective function is: Lstyle = ğœ†DSSDLDSSD + ğœ†RGBLRGB + ğœ†maskLmask (11) This setup ensures multi-view consistency in 3D stylization, achieving refined style expression and geometric fidelity through the dynamic coefficient adjustment and adaptive optimization strategy."
        },
        {
            "title": "3.3 Simultaneously Optimized Scale (SOS)\nTo further enhance the texture details of 3D gaussian, multiscale\nstylization strategy is introduced into the optimization process.\nFollowing the silimar approach from [15, 19], we employ VGG-19\n[60] to extract high-resolution texture features through its shallow\nconvolutional layers. We use N rendered images (each image is\nrepresented as ğ¼v) from the source 3D gaussian and style reference\nimage ğ¼ref to compute multi-scale Gram matrix correlations and\nformulate the style objectiveness as follows:",
            "content": "LSOS = 1 ğ‘ ğ‘ ğ‘£=1 ğ‘™ ğ¿ğ‘  ğº (ğœ™ğ‘™ VGG (ğ¼ğ‘£)) ğº (ğœ™ğ‘™ VGG (ğ¼ref))2 2 (12) where ğº () denotes Gram matrix computation,ğœ™ğ‘™ VGG represents features from the ğ‘™-th VGG layer and ğ¿ğ‘  = {ğ‘…ğ‘’ğ¿ğ‘ˆ _ğ‘˜_1, ğ‘˜ 1, 3, 5}."
        },
        {
            "title": "3.4 Contrastive Style Descriptor (CSD)\nCSD[61] aims to build a high-performance model (variants of ViT [14],\nlike ViT-B and ViT-L) for the representation of the image style. The\nViT is trained with both self-supervised learning and supervised\nobjectives. As a result, the ViT can extract image descriptors with\nconcise and effective style information. To further align to the\nstyle between the 3D gaussian and the given reference image, we\nleverage the ViT to extract style feature from rendered images and\nreference image respectilvely and then calculate the pairwise cosine\nsilimarity score. Finally, the CSD loss term reduces to:",
            "content": "LCSD = 1 ğ‘ ğ‘ ğ‘£=1 (1 cos(ğœ™ViT (ğ¼ğ‘£), ğœ™ViT (ğ¼ref))) (13) 3.5 3D gaussian Quality Assessment (3DG-QA) In addition to preserving the original content and migration style of 3D gaussian, we also need to ensure the overall quality between the migrated style and content. CLIP-IQA [68] has been developed to evaluate the look or quality of an image. CLIP-IQA leverages CLIP for perception assessment and calculate the cosine similarity between the feature embeddings of the given text promt and image as follows: ğ‘¥ ğ‘¡ ğ‘¥ ğ‘¡ where ğ‘¥ Rğ¶ and ğ‘¡ Rğ¶ represents the image embedding and text embedding, is the embedding channel dimension. CLIP-IQA further introduces antonym prompts (e.g., Good photo. and Bad photo.) to address the linguistic ambiguity. ğ‘¡1 and ğ‘¡2 are obtained ğ‘  = (14) from text prompts with good quality and bad quality respectively and the ğ‘ ğ‘– can be obtained with the corresponding ğ‘¡ğ‘– , then the final CLIP-IQA score can be formulate as: ğ‘’ğ‘ 1 ğ‘’ğ‘ 1 + ğ‘’ğ‘ 2 We adopt the CLIP-IQA property and extend CLIP-IQA to the 3D style transfer field to ensure the perception quality of 3D gaussian. More specifically, we define the 3D gaussian Quality Assessment (3DG-QA) as objective term as: (15) ğ‘  = L3DG-QA = 1 ğ‘ ğ‘ ğ‘£=1 (1 ğ‘ ğ‘£) (16) where ğ‘£ means the viewpoint index rendered from the 3D gaussian representation."
        },
        {
            "title": "3.6 Stylizing 3D GS with mixture of encoders\nThe StyleMe3D approach systematically addresses five fundamen-\ntal aspects in 3D gaussian stylization: (1) Style-content decoupling,\n(2) Adaptive style conditioning, (3) Multi-scale feature alignment,\n(4) Texture detail enhancement, and (5) Global aesthetic optimiza-\ntion with four principal components. The DSSD stablishes effective\nstyle conditioning through high-level semantic alignment, lever-\naging score-based stable diffusion to extract and transfer domain-\ninvariant style features. SOS addresses low-level feature alignment\nvia multi-scale optimization, preserving stylistic textures through\nscale-aware importance sampling and geometric consistency con-\nstraints. CSD facilitates mid-level style-content harmonization us-\ning contrastive learning to disentangle and recompose style at-\ntributes while maintaining content integrity. At last, 3DG-QA en-\nhances global aesthetic quality through metric-guided refinement,\nemploying perceptual quality evaluation to optimize both local\ntextural coherence and global visual appeal.",
            "content": "We integrate the whole optimization goal as: Lğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ = ğœ†1Lstyle + ğœ†2LSOS + ğœ†3LCSD + ğœ†4L3DG-QA where ğœ†1, ğœ†2, ğœ†3 and ğœ†4 stand for the multi-task coefficients. For shortly, this multi-faceted approach ensures semantic-aware style, fine-grained style, style fidelity and global aesthetics quality. (17) As demonstrated in Sec. 4.3, the combined losses enable simultaneous preservation of geometric integrity and artistic expression while suppressing common artifacts like over-stylization and texture flickering."
        },
        {
            "title": "4 Experiment\n4.1 Visual Result",
            "content": "As shown in Fig. 3, we applied six styles to showcase our experimental results on both object and scene datasets (NeRF synthetic dataset [47] and tandt db [29]). The style references fall into two main categories: non-photorealistic art styles (e.g., vangogh, cartoon, sketch, hand-drawing, watercolor, painting) and state-based styles (e.g., fire, water, clouds, hair). These categories highlight our methods ability to handle traditional art styles and capture realistic physical characteristics in 3D. To highlight our methods advantage in preserving detail textures and shadows, we zoom in on details like the legs and detail texture of the chair, texture of the fire on the ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. Figure 3: Visual Result. Demonstration of our methods performance across five styles (vangogh wheat field, star night, fire nezha, colorful oil, and lighting tiger) applied to five objects(chair, ship, hotdog, lego and mic) and two scenes (man face and train). The results illustrate our models capability to handle two main categories of styles: (1) Non-photorealistic Art Styles (e.g., cartoon, drawing), showcasing traditional artistic expressions, and (2) State-based Styles (e.g., fire, oil), which capture physical properties. This figure demonstrates our methods versatility and semantic-aware ability in stylizing 3D models while preserving style fidelity and geometric consistency across diverse artistic and physical characteristics. For Example, semantic separation of the legs of the chair from the seat cushion, detail texture of chair, texture of the fire on the hot dog, and metallic sheen on the mic are all effectively preserved. hot dog, and metallic sheen on the mic. Experimental results indicate that Gaussian Splatting effectively enhances non-photorealistic and state-based style representations, showing strong adaptability in diverse stylized scenarios. Additional results are provided in the Supplementary."
        },
        {
            "title": "4.2 Comparison Studies\nQualitative Result. we show objects and scene stylization com-\nparisons in Fig.4 and Fig.5 respectively. For objects, we applied\nvangogh, fire nezha, and sketch styles to chair, hotdog and mic. For\nscene stylization, we select truck and train from tandt db dataset\nusing landscope and lighting tiger styles. We evaluate our method\nagainst others, including SGSST [15], StyleGaussian [40] and ARF\n[85]. The horizontal axis lists competing methods and the vertical\naxis denotes datasets.",
            "content": "Different from traditional methods based only on VGG networks like SGSST [15], StyleGaussian [40] and ARF [85], which focus on simple style transfer, our approach prioritizes vivid, expressive and semantic-aware stylization. They relies on VGG networks [60] with empirical-based style decoupling, which limiting style extraction with customized references, our diffusion-based and multi-Expert method, pre-trained on large-scale style image-text data, captures style features with greater fidelity. Moreover, training on image-text data enhances semantic understanding, allowing content filtering in CLIP space for precise style extraction. Unlike ARF [85], which depends on carefully pre-stylized views for effective color matching and risks texture drift if the initial view is misaligned, our method only requires single arbitrary style reference image. While we incorporate pre-stylized multi-views, they serve solely for pixel-level style guidance in our outpainting process rather than relying on single-view matching, establishing distinct way from that of ARF. Table 1: Quantitative comparison with competing methods Method ARF SGSST StyleGaussian Ours PSNR 17.537 11.963 7.279 18.015 SSIM LPIPS 0.802 0.678 0.129 0. 0.188 0.306 0.558 0.174 Quantitative Evaluation We evaluate our method with three standard image quality metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) [72], and Learned Perceptual Image Patch Similarity (LPIPS) [86]. PSNR quantifies pixellevel accuracy, indicating how closely the stylized image matches the original. SSIM measures structural similarity, capturing perceptual features like textures and edges. LPIPS assesses perceptual StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Figure 4: Qualitative Comparisons on Object Level Stylization. We compare our method against other SOTA (SGSST [15], StyleGaussian [40] and ARF [85]) on nerf synthetic dataset (selected chair, hotdog, and mic) using vangogh wheat field, fire nezha, and sketch styles. The horizontal axis represents the compared methods, and the vertical axis displays different data. Our method effectively retains semantic and details of original model and style feature of reference image, such as semantic separation of the legs of the chair from the seat cushion, texture of the fire on the hot dog, and metallic sheen on the mic. Compared to others, our method exhibits stronger semantic understanding, clearly distinguishing elements like the cushions, backrest and legs on the chair. Figure 5: Qualitative Comparisons on Scene Level Stylization. We compare our method against other SOTA (SGSST [15], StyleGaussian [40] and ARF [85]) on tandt db dataset (selected truck and train) using landscope and lighting tiger styles. The horizontal axis represents the compared methods, and the vertical axis displays different data. Our method effectively retains semantic and details of original model and style feature of reference image, such as the truck wheel and train fence (as shown in Zoom-in). Compared to others, our method exhibits stronger semantic understanding, clearly distinguishing elements like the fence, tire and rail. ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. Figure 6: Ablation study on style outpainting guidance mode. (a) Baseline without style outpainting exhibits limited stylization scope and view-dependent artifacts (red boxes). (b) Local Guidance enables single-view enhancement but causes multiview inconsistencies. Global-Local Fusion achieves crossview style propagation through adaptive attention weighting, improving style consistency while preserving view-specific details. Figure 8: Ablation study loss design. (a) DSSD-only initialization yields semantically coherent but texture-deficient results with color shifts (see missing curvilinear patterns in Van Gogh stylization). (b) DSSD+SOS achieves texturegeometry equilibrium through gradient mutual regularization, recovering fine details while suppressing oversmoothing. (c) Full Model (DSSD+SOS+CSD+CLIP-IQA) enhances perceptual quality via knowledge-driven style assessment, achieving remarkable improvement over baseline  (Table 2)  . Figure 7: Ablation study on dynamic noise scheduling. Low Scale (7.5) produces incomplete stylization with missing texture details. High Scale (50) introduces oversaturation artifacts and structural distortions. Dynamic Scale (7.5-30) adaptively balances detail preservation and style intensity. quality based on deep network features, emphasizing visual similarity as perceived by humans. As shown in Tab. 1, our method achieves significantly higher SSIM and PSNR scores, demonstrating enhanced structural and perceptual fidelity compared to SGSST, StyleGaussian and ARF. Our higher PSNR and SSIM score indicates better fidelity in color and texture reproduction while preserving structural details. Furthermore, the LPIPS score, measuring perceptual similarity, supports our methods superior style consistency and stylization quality across multiple viewpoints. Table 2: Quantitative comparison with DSSD version and Multi-Expert version Method Ours (DSSD) Ours (Multi-Expert) PSNR 17.270 18.015 SSIM LPIPS 0.776 0.830 0.181 0."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We conducted ablation studies to assess the impact of various components and parameters in our method, focusing on style outpainting mode, DDSD and multi-expert module. Ablation on Style Outpainting. As shown in Fig. 6. We present an ablation study on the impact of Style Outpainting. Without it, the degree of stylization is visibly limited, whereas applying Style Outpainting allows effective style propagation across views. We compares different guidance schemes: local mode & global-local mode. Local mode shows inconsistencies, resulting in artifacts and missing details in certain views. In contrast, global-local mode enhances stylization intensity and detail refinement, achieving more coherent stylization across views. Ablation on DSSD. As shown in Fig. 7. We conducted an ablation study on the effectiveness of dynamic guidance scale in DSSD. Comparing results at low scale of 7.5, high scale of 50, and dynamic scale ranging from 7.5 to 30, we observed that the dynamic scale approach consistently outperforms static setting. Ablation on Multi-Expert. As shown in Fig. 8, we analyze the impact of SOS, CSD and 3DG-QA on stylization quality. our analysis reveals that initial stylization using DSSD alone produces semantically coherent results but suffers from two critical limitations: 1) Insufficient low-level texture details (e.g., missing curvilinear patterns in Van Gogh-inspired wheat field renderings), and 2) Systematic color deviation artifacts. The introduction of SOS loss establishes dual-optimization framework where DSSD and SOS operate concurrently within single-view projections. This configuration enables mutual regularization of their gradient optimization directions - DSSDs tendency toward over-smoothing is counterbalanced by SOSs capacity for detail enhancement, while SOSs potential overemphasis on low-level features is constrained by DSSDs semantic guidance. Subsequent integration of CSD and 3DG-QA implements knowledgedriven perceptual assessment through CLIP-space cosine similarity StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, metrics. The CSD module specializes in style authenticity evaluation through learned artistic aesthetics criteria, while 3DG-QA provides quality-focused guidance via antonymic text prompts. Quantitative analysis shows this combined approach achieves remarkable improvement in human perceptual quality scores compared to baseline configurations (see Table 2)."
        },
        {
            "title": "5 Conclusion\nWe redefine the 3D Gaussian Splatting (3D GS) stylization task\nthrough comprehensive analysis and propose the StyleMe3D frame-\nwork, establishing a novel paradigm for artistic 3D scene stylization.\nStyleMe3D enables artistic 3D Gaussian Splatting stylization via\nStable Diffusion-guided score distillation (DSSD), contrastive style\ndescriptors (CSD), and multi-scale optimization (SOS). The 3DG-QA\nmodule ensures aesthetic coherence while preserving geometry. Ex-\nperiments show superior detail retention, style consistency across\nvarious objects and scenes.",
            "content": "References [1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In European conference on computer vision. Springer, 707723. [2] Andrew Brock. 2018. Large Scale GAN Training for High Fidelity Natural Image Synthesis. arXiv preprint arXiv:1809.11096 (2018). [3] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. 2024. Still-moving: Customized video generation without customized video data. arXiv preprint arXiv:2407.08674 (2024). [4] Hao Chen, Jiafu Wu, Ying Jin, Jinlong Peng, Xiaofeng Mao, Mingmin Chi, Mufeng Yao, Bo Peng, Jian Li, and Yun Cao. 2024. VI3DRM: Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis. arXiv preprint arXiv:2409.08207 (2024). [5] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. 2024. Meshxl: Neural coordinate field for generative 3d foundation models. Advances in Neural Information Processing Systems 37 (2024), 9714197166. [6] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. 2023. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 19982 19993. [7] Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, et al. 2024. MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D. arXiv preprint arXiv:2411.02336 (2024). [8] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li. 2022. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798 (2022). [9] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-Sheng Lai, and Wei-Chen Chiu. 2022. Stylizing 3d scene via implicit representation and hypernetwork. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 14751484. [10] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. 2024. Style injection in diffusion: training-free approach for adapting large-scale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 87958805. [11] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. Vqgan-clip: Open domain image generation and editing with natural language guidance. In European Conference on Computer Vision. Springer, 88105. [12] Valentin De Bortoli, AgnÃ¨s Desolneux, Alain Durmus, Bruno Galerne, and Arthur Leclaire. 2021. Maximum entropy methods for texture synthesis: theory and practice. SIAM Journal on Mathematics of Data Science 3, 1 (2021), 5282. [13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: Universe of 10M+ 3D Objects. arXiv preprint arXiv:2307.05663 (2023). [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). [15] Bruno Galerne et al. 2024. SGSST: Scaling Gaussian Splatting StyleTransfer. arXiv preprint arXiv:2412.03371 (2024). [16] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. 2024. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414 (2024). [17] Xiang Gao, Zhengbo Xu, Junhan Zhao, and Jiaying Liu. 2024. FrequencyControlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18241832. [18] Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. 2024. Parameter-Efficient Fine-Tuning with Discrete Fourier Transform. arXiv preprint arXiv:2405.03003 (2024). [19] Leon Gatys, Alexander Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. 2017. Controlling perceptual factors in neural style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition. 39853993. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). [21] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. 2024. PuLID: Pure and Lightning ID Customization via Contrastive Alignment. arXiv preprint arXiv:2404.16022 (2024). [22] Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li, and Li Shen. 2024. Freestyle: Free lunch for text-guided style transfer using diffusion models. arXiv preprint arXiv:2401.15636 (2024). [23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022). [24] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022). [25] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and MingHsuan Yang. 2021. Learning to stylize novel views. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1386913878. [26] Zhitong Huang, Mohan Zhang, and Jing Liao. 2024. LVCD: Reference-based Lineart Video Colorization with Diffusion Models. arXiv preprint arXiv:2409.12960 (2024). [27] Kyungmin Jo and Jaegul Choo. 2024. Skip-and-Play: Depth-Driven PosePreserved Image Generation for Any Objects. arXiv preprint arXiv:2409.02653 (2024). [28] Tero Karras, Samuli Laine, and Timo Aila. 2019. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 44014410. [29] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. 42, 4 (2023), 1391. [30] Jangyeong Kim, Donggoo Kang, Junyoung Choi, Jeonga Wi, Junho Gwon, Jiun Bae, Dumim Yoon, and Junghyun Han. 2024. RoCoTex: Robust Method for Consistent Texture Synthesis with Diffusion Models. arXiv preprint arXiv:2409.19989 (2024). [31] Hubert Kompanowski and Binh-Son Hua. 2024. Dream-in-Style: Text-to-3D Generation using Stylized Score Distillation. arXiv preprint arXiv:2406.18581 (2024). [32] Gwanhyeong Koo, Sunjae Yoon, Ji Woo Hong, and Chang Yoo. 2024. Flexiedit: Frequency-aware latent refinement for enhanced non-rigid editing. arXiv preprint arXiv:2407.17850 (2024). [33] Divya Kothandaraman, Kuldeep Kulkarni, Sumit Shekhar, Balaji Vasan Srinivasan, and Dinesh Manocha. 2024. ImPoster: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models. arXiv preprint arXiv:2409.15650 (2024). [34] Ãron Samuel KovÃ¡cs, Pedro Hermosilla, and Renata Raidou. 2024. Style: Stylized Gaussian Splatting. In Computer Graphics Forum, Vol. 43. Wiley Online Library, e15259. [35] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. 2024. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245 (2024). [36] Pengzhi Li, Chengshuai Tang, Qinxuan Huang, and Zhiheng Li. 2024. Art3d: 3d gaussian splatting for text-guided artistic scenes generation. arXiv preprint arXiv:2405.10508 (2024). [37] Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, and Ming Yang. 2024. StyleTokenizer: Defining Image Style by Single Instance for Controlling Diffusion Models. arXiv preprint arXiv:2409.02543 (2024). [38] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. 2024. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. Pattern Recognition. 86408650. [39] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Eric Xing. 2023. Stylerf: Zero-shot 3d style transfer of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 83388348. [40] Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, and Shijian Lu. 2024. StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting. arXiv preprint arXiv:2403.07807 (2024). [41] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. 2024. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1007210083. [42] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision. 9298 9309. [43] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. 2024. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 99709980. [44] Jiawei Lu, Yingpeng Zhang, Zengjun Zhao, He Wang, Kun Zhou, and Tianjia Shao. 2024. GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation. arXiv preprint arXiv:2409.18401 (2024). [45] Yang Luo, Yiheng Zhang, Zhaofan Qiu, Ting Yao, Zhineng Chen, Yu-Gang Jiang, and Tao Mei. 2024. Freeenhance: Tuning-free image enhancement via contentconsistent noising-and-denoising process. arXiv preprint arXiv:2409.07451 (2024). [46] Yiqun Mei, Jiacong Xu, and Vishal Patel. 2024. Reference-based Controllable Scene Stylization with Gaussian Splatting. arXiv preprint arXiv:2407.07220 (2024). [47] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99106. [48] Fangzhou Mu, Jian Wang, Yicheng Wu, and Yin Li. 2022. 3d photo stylization: Learning to generate stylized novel views from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16273 16282. [49] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. 2022. Snerf: stylized neural implicit representations for 3d scenes. arXiv preprint arXiv:2207.02363 (2022). [50] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022). [51] Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, et al. 2023. RenderMe-360: large digital asset library and benchmarks towards high-fidelity head avatars. Advances in Neural Information Processing Systems 36 (2023), 79938005. [52] Kien Pham, Jingye Chen, and Qifeng Chen. 2024. TALE: Training-free Crossdomain Image Composition via Adaptive Latent Manipulation and Energy-guided Optimization. In Proceedings of the 32nd ACM International Conference on Multimedia. 31603169. [53] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. 2022. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022). [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [56] Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David Jacobs, and Shlomi Fruchter. 2024. Magic Insert: Style-Aware Drag-andDrop. arXiv preprint arXiv:2407.02489 (2024). [57] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann Weber. 2024. LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models. arXiv preprint arXiv:2405.14477 (2024). [58] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. 2023. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023). [59] Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 2019. 3d point cloud generative adversarial network based on tree structured graph convolutions. In Proceedings of the IEEE/CVF international conference on computer vision. 3859 3868. [60] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). [61] Gowthami Somepalli, Ayan Bansal, Micah Goldblum, Jonas Geiping, Tom Goldstein, et al. 2024. Measuring Style Similarity in Diffusion Models. arXiv preprint arXiv:2404.01292 (2024). [62] Zihan Su, Junhao Zhuang, and Chun Yuan. 2024. TextureDiffusion: Target Prompt Disentangled Editing for Various Texture Transfer. arXiv preprint arXiv:2409.09610 (2024). [63] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. 2025. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision. Springer, 118. [64] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. 2023. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023). [65] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2023. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics (2023). [66] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. 2023. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1261912629. [67] Haofan Wang, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. 2024. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733 (2024). [68] Jianyi Wang, Kelvin C.K. Chan, and Chen Change Loy. 2022. Exploring CLIP for Assessing the Look and Feel of Images. arXiv preprint arXiv:2207.12396 (2022). [69] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2023. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 25552563. [70] Peng Wang and Yichun Shi. 2023. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023). [71] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. 2024. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024). [72] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600612. [73] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. 2024. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems 36 (2024). [74] Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. 2024. Phidias: Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion. arXiv preprint arXiv:2409.11406 (2024). [75] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. 2024. Unique3D: High-Quality and Efficient 3D Mesh Generation from Single Image. arXiv preprint arXiv:2405.20343 (2024). [76] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. 2023. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 803814. [77] Zongze Wu, Yotam Nitzan, Eli Shechtman, and Dani Lischinski. 2021. Stylealign: arXiv preprint Analysis and applications of aligned stylegan models. arXiv:2110.11323 (2021). [78] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. 2024. SVGDreamer: Text guided SVG generation with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 45464555. [79] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. 2024. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191 (2024). [80] Zizheng Yan, Jiapeng Zhou, Fanpeng Meng, Yushuang Wu, Lingteng Qiu, Zisheng Ye, Shuguang Cui, Guanying Chen, and Xiaoguang Han. 2024. DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors. arXiv preprint arXiv:2407.16260 (2024). [81] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. 2019. Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF international conference on computer vision. 45414550. [82] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023). [83] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, and Yogesh Balaji. 2024. JeDi: Joint-Image Diffusion Models for FinetuningFree Personalized Text-to-Image Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 67866795. [84] Dingxi Zhang, Yu-Jie Yuan, Zhuoxun Chen, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, and Lin Gao. 2024. Stylizedgs: Controllable stylization for 3d gaussian splatting. arXiv preprint arXiv:2404.05220 (2024). [85] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. 2022. Arf: Artistic radiance fields. In European Conference on Computer StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Vision. Springer, 717733. [86] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. [87] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Jiaya Jia. 2023. Refnpr: Reference-based non-photorealistic radiance fields for controllable scene stylization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 42424251. [88] Junsheng Zhou, Weiqi Zhang, and Yu-Shen Liu. 2024. DiffGS: Functional Gaussian Splatting Diffusion. arXiv preprint arXiv:2410.19657 (2024). [89] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. 2024. StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation. arXiv preprint arXiv:2405.01434 (2024). ArXiv Preprint, 2025,"
        },
        {
            "title": "Appendix",
            "content": "A Preliminary A.1 Style-aware Image Customization In recent advancements in style transfer, StyleShot [16] and IP-Adapter [82] represent two prominent techniques, each employing distinct methods to transfer style from reference image to target image. StyleShot emphasizes the extraction of detailed style features using style-aware encoder, which leverages multiscale patch partitioning to capture both low-level and high-level style cues. Specifically, StyleShot divides the reference image into non-overlapping patches of three sizes, corresponding to different scales. For each patch scale, there is dedicated ResBlock at different depths. The following are the key formulas for style injection in StyleShot: Attention(ğ‘„, ğ¾ğ‘ , ğ‘‰ğ‘  ) = softmax (cid:19) (cid:18) ğ‘„ğ¾ğ‘‡ ğ‘  ğ‘‘ ğ‘‰ğ‘  (S1) where ğ‘„ is the query projected from the latent embeddings ğ‘“ , and ğ¾ğ‘  and ğ‘‰ğ‘  are the keys and values, respectively, that the style embeddings ğ‘“ğ‘  are projected onto through independent mapping functions . The attention outputs of the text embeddings ğ‘“ğ‘¡ and ğ‘Šğ¾ğ‘  style embeddings ğ‘“ğ‘  are then combined into new latent embeddings ğ‘“ , which are fed into subsequent blocks of Stable Diffusion: and ğ‘Šğ‘‰ğ‘  ğ‘“ = Attention(ğ‘„, ğ¾ğ‘¡ , ğ‘‰ğ‘¡ ) + ğœ† Attention(ğ‘„, ğ¾ğ‘ , ğ‘‰ğ‘  ) (S2) where ğœ† represents the weight balancing the two components. A.2 Score Distillation Sampling for 3D"
        },
        {
            "title": "Generation",
            "content": "Text-guided 3D generation has gained significant attention due to advancements in methods such as Score Distillation Sampling (SDS) [53], which facilitates the optimization of 3D representations using pre-trained diffusion models. SDS optimizes the parameters ğœƒ of 3D model ğ‘”(ğœƒ ) by distilling gradients from diffusion model ğœ™, ensuring that 2D projections generated from ğ‘”(ğœƒ ) align with target text prompt. The gradient of the SDS loss is defined as: (cid:20) ğœ” (ğ‘¡) ğœƒ ğ¿SDS (ğœ™, ğ‘¥ = ğ‘”(ğœƒ )) = Eğ‘¡,ğœ– (cid:16) Ë†ğœ–ğœ™ (ğ‘§ğ‘¡ ; ğ‘¦, ğ‘¡) ğœ– (S3) (cid:21) , (cid:17) ğ‘¥ ğœƒ where Ë†ğœ–ğœ™ (ğ‘§ğ‘¡ ; ğ‘¦, ğ‘¡) represents the predicted noise residual from the pre-trained diffusion model, ğœ– is the actual noise used in the forward process, ğ‘§ğ‘¡ is the latent variable at timestep ğ‘¡, and ğœ” (ğ‘¡) is timestep weighting function. These have been extended to artistic scene generation [36] and combined input conditions, including text and images [74, 80]. Recent advances leveraging latent diffusion models have improved the scope and expressiveness of text-to-3D synthesis [80, 88], supporting more nuanced and creative 3D outputs. A.3 3D Gaussian Splatting 3D Gaussian Splatting (3D GS) [29] represents 3D scene using collection of spatial Gaussians. Each Gaussian ğ‘”ğ‘– is defined by mean position ğœ‡ğ‘– R3 and covariance matrix Î£ğ‘– R33, which determines its shape and orientation. The Gaussians influence on point is given by: Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. ğº (x) = ğ‘’ 1 2 (xğœ‡ğ‘– ) Î£1 ğ‘– (xğœ‡ğ‘– ) (S4) where Î£ğ‘– = RSSR is decomposed into rotation and scaling matrices. Each Gaussian has an opacity ğ›¼ğ‘– and view-dependent color ğ‘ğ‘– . During rendering, Gaussians are projected to 2D and blended using alpha compositing. The final pixel color ğ¶ is calculated as: ğ‘› ğ‘ğ‘–ğ›¼ğ‘– ğ‘– 1 (cid:214) ğ¶ = (1 ğ›¼ ğ‘— ) (S5) ğ‘–=1 ğ‘—=1 Here, ğ›¼ğ‘– is the effective opacity of the ğ‘–-th Gaussian in sorted depth order. Gaussian Splatting enables real-time, differentiable rendering and can reconstruct scenes with multi-view supervision. Compared to NeRF [47], 3D Gaussian Splatting is significantly more efficient in both time and memory usage. By representing scenes with Gaussian primitives rather than dense neural networks, it allows for faster rendering and lower computational costs, making it more suitable for real-time applications. Implementation Details Computational Environment: All experiments were conducted on single NVIDIA L40S GPU with 46GB of VRAM. Dataset: NeRF synthetic dataset [47] and tandt db [29], was used for all experiments. B.1 Details of Dynamic Style Score Distillation (DSSD) (1) Backbone Models: For the style-aware diffusion model, we adopt StyleShot, which builds on IP-Adapter and incorporates style-aware encoder to enhance style representation, enabling robust style transfer through score distillation guidance. (2) Fine Timestep Sampling: We employ fine-grained timestep sampling strategy with timestep constant = 1000. Minimum and maximum timesteps were set as ğ‘‡min = 0.02 and ğ‘‡max = 0.75 T, respectively. The noise intensity was dynamically reduced to high, medium, and low levels to stabilize the updates during training. (3) Dynamic Guidance Coefficients: The dynamic guidance coefficient Î”ğœ† was tuned to adapt to varying scales of the dataset and style variations. For the NeRF Synthetic dataset, we selected ğœ†max = 20 and confined Î”ğœ† within [7.5, 20]. (4) Guidance Modes and Outpainting Strategy: total of 2800 steps were employed, segmented into specific guidance modes: Main RGB Loss (Local Mode): Steps 100 to 600. Adaptive Iteration (Global Mode): Steps 1 to 1000, alternating between global RGB and global SDS losses. Fixed or Free Global Modes: Steps 1000 to 1900, alternating between global-fix and global-free modes. Local Mode: Steps 1900 to 2800. This hybrid strategy begins with global optimization before transitioning to local refinement, requiring 1800 iterations for SDS loss. (5) Iteration Time and Cost Analysis: StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Average Time Per Iteration: Single-view RGB loss averaged 0.1 seconds, while SDS loss averaged 2.5 seconds. Total Iteration Count and Convergence: Using RGB loss for the initial 1000 steps and SDS loss for the subsequent 2000 steps, convergence was achieved in approximately 2600 seconds. For enhanced local convergence, an additional 500 to 1000 SDS iterations were applied. B.2 Details of Simultaneously Optimized Scale (SOS) (1) VGG Feature Extraction Style layers: [r11,r21,r31,r41,r51] Content layer: [r42] Gram matrix weights: [1e3/64Â², 1e3/128Â², 1e3/256Â², 1e3/512Â², 1e3/512Â²] (2) Two-Phase Optimization Pretraining phase: Trigger: optimize_iteration=10000 and current_iter < 10000 Fixed scale: optimize_size=0.5 (uses minimum resize_images if unspecified) Downsampling: Bilinear interpolation mode=\"bilinear\" Full multi-scale phase: Activates all resize_images scales B.3 Details of Contrastive Style Descriptor (CSD) Deployed CSD ViT-L style encoder pretrained on LAIONStyles dataset. B.4 Details of 3D Gaussian Quality Assessment (3DG-QA) Integrated CLIP-ViT-B with antonymic prompts: \"Good, Sharp, Colorful\" vs \"Bad, Blurry, Dull\", prompts=(\"quality\", \"sharpness\", \"colorfullness\") Figure S1: Optimization Pathways for Pre-training vs. Posttraining. The plot illustrates the optimization pathways for pre-training (blue solid line) and post-training (orange dashed line), highlighting the optimization gap (gray shaded area) between 3D reconstruction and stylization. The pretraining pathway shows smooth, steady convergence, while the post-training pathway oscillates due to inherent uncertainty in stylization. The optimization gap represents misalignment between the stages, emphasizing the need for alignment techniques, such as style-aware priors and dynamic guidance, to achieve stable and consistent 3D stylization. Disjoint Loss Landscapes: The loss landscapes for pretraining and post-training differ significantly. Pre-training minimizes reconstruction errors, while stylization involves abstract priors from style information, leading to potential misalignment. The optimization pathways during pre-training and post-training can be represented as two distinct loss functions: Lpre = Lrecon (ğºpre (ğ‘¥), ğ‘¥gt), (S6) loss = 1 - (0.4*scores[quality] + 0.4*scores[sharpness] + 0.2*scores[colorfullness]).mean() , where ğ‘¤ğ‘ = 0.4, ğ‘¤ğ‘  = 0.4, ğ‘¤ğ‘ = 0.2 denote quality, sharpness, and colorfulness weights respectively."
        },
        {
            "title": "C Additional Method Analysis",
            "content": "The challenges of directly transferring 3D generation techniques to 3D stylization stem from the optimization gap between pretraining and post-training stages. This section provides theoretical and visual analysis of this gap. C.1 Misalignment in Optimization Pathways Pre-training Objective: The goal of 3D reconstruction during pre-training is to capture geometric and photometric properties accurately. This optimization process is typically smooth and guided by explicit ground truth data. Post-training Objective: In the post-training phase, the focus shifts to aesthetic alignment using style-aware guidance, which lacks explicit supervision and introduces higher uncertainty. where Lrecon minimizes geometric and photometric errors between the predicted ğºpre (ğ‘¥) and ground truth ğ‘¥gt, and: Lpost = Lstyle (ğºpost (ğ‘¥), ğ‘ ref), (S7) where Lstyle aligns the generated results ğºpost (ğ‘¥) with style reference ğ‘ ref using abstract priors. The optimization gap can then be formulated as: Î”L = (cid:12) (cid:12)Lpre Lpost(cid:12) (cid:12) , (S8) where Î”L quantifies the divergence between the loss landscapes, reflecting the mismatch in optimization objectives. C.2 High Uncertainty in Style Information Multi-modal Style Representations: Styles are inherently diverse and lack well-defined ground truth, making the optimization process less predictable. Temporal Instability: Stylization optimization pathways often exhibit oscillations due to conflicts between style priors and geometric constraints. ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. The uncertainty in style optimization can be modeled as the Introducing regularization terms: Lalign = ğœ†priorLstyle + ğœ†geoLrecon, (S14) where ğœ†prior and ğœ†geo balance style fidelity and geometric preservation, helps align the pathways. C.5 Conclusion This analysis highlights the inherent challenges in aligning pretraining and post-training optimization pathways. The visualization emphasizes the need for dedicated techniques to bridge the gap, ensuring high-fidelity and consistent stylization while maintaining geometric coherence."
        },
        {
            "title": "D More Visual Result",
            "content": "As shown in Figure S2, S3, and S4, we demonstrate our methods performance across nine distinct styles (sky painting, cartoon, watercolor, fire, cloud, Wukong, drawing, color oil, and sketch) on three datasets (chair, hotdog, and mic). variance in style priors: ğœ2 style = Var(ğ‘ ref), where ğ‘ ref represents multi-modal style representations. Temporal oscillations in optimization can be expressed as: (cid:12)Lpost,ğ‘¡ +1 Lpost,ğ‘¡ (cid:12) (cid:12) , ğ›¿ğ‘¡ = (cid:12) (S10) (S9) where ğ›¿ğ‘¡ measures the instability between consecutive timesteps ğ‘¡ and ğ‘¡ + 1. C.3 Visualization Analysis The graph (Figure S1) visualizes the optimization gap between pre-training and post-training: Pre-training pathway (blue solid line) shows smooth convergence, reflecting steady optimization for geometric fidelity. Post-training pathway (orange dashed line) exhibits oscillations, driven by the abstract and subjective nature of style priors. Optimization gap (gray shaded area) represents the divergence between the two pathways, indicating the challenges of transitioning between the stages. To bridge the optimization gap, alignment strategies must minimize: min ğº Î”L + ğœ†consLconsistency, (S11) where Lconsistency enforces multi-view consistency, and ğœ†cons is weighting factor to balance consistency with style fidelity. C.4 Key Observations and Insights (1) Mismatch in Optimization: The smooth convergence of pre-training contrasts with the oscillatory adjustments in post-training, reflecting the differences in objectivesgeometric accuracy vs. subjective style transfer. The loss landscapes Lpre and Lpost differ fundamentally in their curvature: ğœ…pre ğœ…post, (S12) where ğœ… represents the curvature, indicating smoother optimization for pre-training compared to post-training. (2) Impact of the Gap: The optimization gap introduces challenges such as: Optimization Instability: Misaligned pathways can lead to instability during post-training. Inconsistent Stylization: Divergent trajectories may result in geometric distortions or incomplete stylization. Misaligned pathways can exacerbate: Instability: Î”L leads to higher gradients: Lpost Lpre. Inconsistency: Variance in style priors ğœ2 inconsistencies in multi-view stylization. style (S13) introduces (3) Bridging the Gap: Effective strategies such as style-aware diffusion priors, dynamic style score distillation, and progressive style outpainting are critical to aligning pathways and ensuring robust stylization. StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians ArXiv Preprint, 2025, Figure S2: More visual results. Demonstration of our methods performance across nine distinct styles (sky painting, cartoon, watercolour, fire, cloud, Wukong, drawing, color oil, and sketch) applied to chair. Figure S3: More visual results. Demonstration of our methods performance across nine distinct styles (sky painting, cartoon, watercolour, fire, cloud, Wukong, drawing, color oil, and sketch) applied to hotdog. ArXiv Preprint, 2025, Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng et al. Figure S4: More visual results. Demonstration of our methods performance across nine distinct styles (sky painting, cartoon, watercolour, fire, cloud, Wukong, drawing, color oil, and sketch) applied to mic."
        }
    ],
    "affiliations": [
        "AIGC Research",
        "Guangming Laboratory",
        "ShanghaiTech University",
        "StepFun"
    ]
}