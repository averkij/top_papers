{
    "paper_title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "authors": [
        "Abdelrahman Shaker",
        "Ahmed Heakl",
        "Jaseel Muhammad",
        "Ritesh Thawkar",
        "Omkar Thawakar",
        "Senmao Li",
        "Hisham Cholakkal",
        "Ian Reid",
        "Eric P. Xing",
        "Salman Khan",
        "Fahad Shahbaz Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/"
        },
        {
            "title": "Start",
            "content": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device Abdelrahman Shaker1,, Ahmed Heakl1, Jaseel Muhammad1 Ritesh Thawkar1 Omkar Thawakar1 Senmao Li1 Hisham Cholakkal1 Ian Reid Eric P. Xing1,2 Salman Khan1 Fahad Shahbaz Khan1,3 1Mohamed bin Zayed University of Artificial Intelligence 3Linkoping University 2Carnegie Mellon University Codebase: github.com/Amshaker/Mobile-O Project Page: amshaker.github.io/Mobile-O Models: https://huggingface.co/collections/Amshaker/mobile-o-models Datasets: https://huggingface.co/collections/Amshaker/mobile-o-datasets 6 2 0 2 3 2 ] . [ 1 1 6 1 0 2 . 2 0 6 2 : r Figure 1. Comparison of our approach with existing unified models. Left: Qualitative comparison illustrating Mobile-Os capabilities in text-to-image generation, visual understanding, and visual prompt understanding. Right: Quantitative comparison with Show-O, Janus, and JanusFlow, demonstrating that Mobile-O achieves superior trade-off. Our Mobile-O outperforms Show-O by 5.0% on GenEval and runs significantly faster on iPhone. *Corresponding author: abdelrahman.youssief@mbzuai.ac.ae Equal contributions"
        },
        {
            "title": "Abstract",
            "content": "Unified multimodal models can both understand and generate visual content within single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, compact vision-language-diffusion model that brings unified multimodal intelligence to mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses visionlanguage features with diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only few million samples and post-trained in novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6 and 11 faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only 3s per 512512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available. 1. Introduction Unified multimodal models capable of both understanding and generating visual content have recently gained popularity in vision. Inspired by the success of large language models (LLMs), recent works extend their reasoning and generative capabilities to vision-language tasks, where the unified multimodal models can caption images, answer visual questions, and generate visuals within single framework [4, 10, 44, 45]. Earlier unified approaches [14, 33] explore single transformer design that can perform both multimodal understanding and generation, when trained jointly on text and image tokens. Subsequent works [52] incorporate diffusion-based generation directly into unified architectures. Recent methods [4, 10] further explore unified model training on large-scale interleaved multimodal data, achieving improved performance. Despite these advances, existing unified multimodal models face two critical challenges that limit their practical deployment on consumer devices. First, most existing unified models employ computational and memorydemanding visual encoders and denoising modules. For instance, BLIP-3o [4] requires 2.6B-parameter UNet for denoising and 3B vision-language model (VLM), in addition to 1.5B for diffusion transformer (DiT), resulting in 7.1B total parameters. While few recent works [20] explore computational efficiency in unified multimodal models, they still remain unsuitable for real-time deployment on edge devices (see Fig. 1). Second, effective cross-modal alignment within unified models often depends on massive pre-training datasets, typically 50M1B samples [4, 10], making pre-training expensive and time-consuming. These observations motivate us to explore key question: Can we build unified multimodal model that is effective for both tasks (understanding and generation), while being efficient for deployment on consumer devices like mobile phones? In this work, we present Mobile-O, compact, efficient unified multimodal model that can run directly on mobile device with low memory overhead and real-time latency, as shown in Fig. 1. Unlike prior approaches that require extensive pre-training, our Mobile-O achieves strong understanding and generation performance with only few million pre-training samples and carefully curated unified post-training data. At the core of our approach is the Mobile Conditioning Projector, mobile-optimized connector that fuses the final hidden states of the VLM with the conditioning space of the diffusion model. Furthermore, we address key limitation in existing training paradigms. Prior unified models either mix disjoint task-specific datasets [35, 45] or adopt sequential training that isolates understanding and generation tasks [4, 24]. In contrast, we propose unified multimodal post-training stage that leverages compact unified dataset where each sample simultaneously supports both tasks through quadruplet (generation prompt, image, question, answer) representation for improved cross-modal alignment. Finally, we demonstrate real-time deployment of our MobileO on edge devices, including iPhone, NVIDIA Jetson Nano, and MacBook. The model achieves 3 seconds per 512 512 image generation on an iPhone device, setting new benchmark for on-device unified multimodal generation. In summary, our key contributions are: We introduce Mobile-O, an efficient unified vithat achieves state-ofsionlanguagediffusion model the-art multimodal understanding and image generation performance, while enabling real-time inference on mobile device (see Fig. 1). To build Mobile-O, we first design solid baseline mobile unified architecture, which is further enhanced with two contributions. First, we introduce the Mobile Conditioning Projector (MCP), lightweight cross-modal fusion module that effectively bridges visual understanding and diffusion-based generation using depthwiseseparable convolutions and layerwise alignment. Second, we propose unified multimodal post-training scheme 2 that leverages quadruplet data representation (generation prompt, image, question, answer) with unified dataset of 105k samples, enabling joint optimization of multimodal understanding and generation tasks. Our Mobile-O, with only 1.6B total parameters, achieves 74% on GenEval, outperforming Show-O and JanusFlow by 5% and 11%, respectively, while being up to 11 faster. For multimodal image understanding, it surpasses them by 15.3% and 5.1%, respectively, on average across seven widely used benchmarks (see Fig. 1). 2. Related Work Multimodal Understanding & Generation: Earlier unified multimodal models [19, 31, 43] unify both understanding and generation tasks with single transformer. Hybrid designs, such as Janus [39], BLIP3-o [4], and JanusFlow [20] integrate diffusion decoders for better textto-image generation, while Emu3 [38] shows that autoregression can suffice for text-to-image generation. While achieving promising results, the aforementioned unified models either rely on heavy UNet-style [4] or computationally heavy architectures [45, 47] (e.g., CLIP-ViT image encoder). Moreover, most existing unified models depend on disjoint supervision across understanding and generation, thereby improving one task while freezing the In contrast, we present unified mobileother [4, 24]. optimized architecture that utilizes unified multimodal post-training stage where the performance of both tasks is simultaneously improved through multi-task objective. Efficient Multimodal Understanding Models: Recent advances in efficient vision-language modeling have focused primarily on optimizing visual encoding strategies [21, 27, 37]. FastVLM [37] addresses the computational bottleneck of processing high-resolution images by introducing FastViTHD, hybrid vision encoder with competitive visual understanding performance. Similarly, SmolVLM [21] shows that careful architectural optimizations and aggressive tokenization enable compact models to achieve competitive performance, while consuming less GPU memory. While these approaches focus at efficient multimodal understanding, our work advances this research line of efficient multimodal intelligence by introducing unified framework that couples compact vision-language understanding model with lightweight diffusion through novel conditioning projector to perform both multimodal understanding and image generation tasks in single architecture. Efficient Text-to-Image Generation Models: Recent works [7, 41] have explored efficient text-to-image (T2I) generation. SANA [41] introduces high-resolution image generation through deep compression autoencoders and linear attention mechanisms. However, they use heavy text encoders (i.e, Gemma-2B [25]). SnapGen [7] proposes systematic architecture optimization and cross-architecture distillation, generating images efficiently with multiple steps on resource-constrained devices. Both approaches are designed for T2I generation and lack multimodal understanding capabilities like FastVLM [37]. In contrast, our work strives to design unified mobile-optimized approach that can effectively perform both multimodal understanding and generation tasks within single framework. Data Efficiency and Training Stages in Unified Models: Training unified multimodal models typically requires extensive datasets. BAGEL [46] studies emerging properties in unified multimodal pre-training, revealing fundamental insights about data requirements. Existing unified approaches generally follow two training strategies: (i) Joint Training: Methods like Metamorph [35] and Show-o [45] perform multitask learning by mixing data for image understanding and image generation. While joint training allows the two tasks to potentially benefit from each other [32, 53], its effectiveness strongly depends on the total data size and the ratio between understanding and generation samples. Current unified training datasets often consist of disjoint subsets for each task [32], e.g., LLaVA-665K for understanding and BLIP3o-60K for generation, which limits the models ability to learn fully aligned cross-task understanding. (ii) Sequential Training: Other unified works [24, 47] adopt two-stage approach: first training the VLM, then freezing the backbone and training only the generation module. For instance, BLIP3-o [4] uses pre-trained VLM and freezes it in all stages. This strategy preserves understanding capability, while dedicating to enhance generation performance. However, it does not exploit potential crosstask interactions during training to improve both tasks. To address these limitations, we introduce unified post-training stage with 105k samples, where each sample simultaneously supports both understanding and formatted as Each training sample is generation. (generation prompt, image, question, answer), enabling the model to learn aligned understanding and generation capabilities during post-training. This unified format allows us to effectively leverage cross-modal transfer while avoiding the task imbalance and inter-task interference. 3. Method Motivation: To motivate our approach, we distinguish two desirable characteristics to be considered when designing an efficient unified multimodal model for edge deployment. Efficient Understanding and Generation Connection: Generally, standard unified models employ connection module that contains MLP layers to connect understanding and generation components. In addition, the connection module leverages set of learnable queries that act as bridge between multimodal LMM and diffusion, enabling improved generation performance. However, such connection design achieves sub-optimal performance 3 Figure 2. Overview of Mobile-O. Left: The proposed framework consists of an efficient image encoder with compact autoregressive language model for visual understanding. For image generation, lightweight linear diffusion transformer (DiT) is employed alongside simple yet effective VAE-based encoderdecoder. Right: Our novel Mobile Conditioning Projector (MCP) bridges the understanding and generation tasks by directly conditioning the diffusion model on weighted hidden states from the VLM without the need for intermediate query tokens. The projector leverages layer-wise feature fusion, depthwise separable convolutions, and efficient channel attention to produce high-fidelity conditioning signals with minimal cost, enabling seamless deployment on edge devices. when using substantially less pre-training data (around 5 less than BLIP3o [4]). Therefore, an efficient yet effective connection design is desired to achieve superior performance when constructing data-efficient mobile unified framework. Unified Post-training for Symbiotic Learning: As discussed earlier, most existing unified models either employ joint training [32, 45] or utilize sequential training [4, 47] for understanding and generation. However, joint training typically relies on careful balancing of disjoint understanding and generation data samples, whereas sequential training only aims to improve one task (e.g., generation) while freezing the other (e.g., understanding). To address this, unified post-training approach is desired based on multi-task objective using joint set of understanding and generation data samples to simultaneously improve both understanding and generation tasks. 3.1. Baseline Mobile Unified Framework Since existing mobile-optimized models are designed to either perform multimodal visual understanding or image generation, we first aim at building solid baseline mobile unified architecture capable of handling both tasks. Motivated by recent unified models such as BLIP-3o [4], which build generation capabilities directly on top of existing understanding models (e.g., Qwen2-VL), we adopt similar yet mobile-optimized design strategy. To establish strong mobile unified baseline, we consider efficient pretrained vision-language model (VLM) backbones and diffusion decoders in configurations reflecting prior unified models. Specifically, as our baseline, we employ FastVLM [37] for multimodal understanding and integrate it with DiTstyle diffusion decoder [41] for multimodal generation. Let fθ denote the vision-language encoder-decoder (FastVLM [37]) and gϕ the diffusion image decoder (SANA-0.6B [41]). Given text prompt and an optional image (for understanding), the VLM produces layerwise hidden states {H (1), . . . , (L)}, where (ℓ) RN dvlm for token length and hidden size dvlm. The diffusion model gϕ is DiT-style decoder with cross-attention blocks accepting encoder features of dimension dcond. Following recent unified models [4, 44, 45, 51], gϕ remains fully learnable, but we avoid introducing extra textual tokens beyond those produced by fθ. Unlike SANA-0.6B [41], which uses the Gemma-2B [25] model as text encoder to process generation prompts, we employ the same LLM used for the understanding model to handle the generation prompts, resulting in more parameter-efficient design. Our goal is to jointly learn θ and ϕ so the model can (i) perform visual understanding tasks (e.g., question answering) and (ii) generate images from prompts, all within mobile-optimized architecture. Next, we discuss how to further improve the performance of the baseline mobile unified framework through an efficient yet effective projector design and unified post-training approach with multitask objective to improve understanding and generation. 4 3.2. Mobile Conditioning Projector (MCP) Unified frameworks usually insert learnable query tokens between the VLM and the image decoder [4, 24, 51]. While this approach is effective for large models, it requires massive pre-training data for effective alignment. To this end, we design an efficient yet effective conditioning projection (MCP) layer that directly connects VLM hidden states to the diffusion decoder, as shown in Fig. 2. The MCP maps the VLMs final-layer features (or fusion of the last layers) to diffusion-compatible conditioning sequences with minimal parameters and FLOPs. Layerwise Fusion. Let = {LK+1, . . . , L} denote the last VLM layers. We compute temperature-scaled softexp(wℓ/τ ) max weighting αℓ = jS exp(wj /τ ) , and form fused representation, (cid:80) Hfuse = (cid:88) ℓS αℓ (ℓ) RN dvlm . (1) where the weights {wℓ} are learned; τ is cosine-annealed during the training. Compression and Refinement. We project Hfuse to compact space and refine it using depthwise-separable 1D convolutions and lightweight channel attention: = LN(cid:0)HfuseWc SeqRefine(cid:0) H(cid:1), (cid:1), Wc Rdvlmdh, (2) (3) where SeqRefine applies depthwise-separable Conv1D followed by pointwise mixing and tiny MLP-based channel attention. Operating along sequence length (not spatial grids) avoids expensive 2D convolutions and retains token-level alignment with language stream. Output Projection. The diffusion cross-attention expects dcond-dimensional keys and values. We compute = LN( HWo), Wo Rdhdcond , RN dcond . (4) All cross-attention layers in gϕ use the same sequence as encoder features, analogous to CLIP-conditioning in latent diffusion, but learned end-to-end with the VLM. Compared to query-token approaches [4, 24, 51], the proposed MCP introduces no extra token budget and reduces parameter count and requires less pre-training data. Complexity. For hidden size dh and kernel k, the refinement block costs O(k dh) (depthwise) + O(d2 h) (pointwise) per token, substantially cheaper than full 2D convolution or attention over new query tokens. 3.3. Training Scheme We propose three-stage training scheme for our Mobile-O that progressively enhances multimodal understanding and generation capabilities. The three stages are: cross-modal alignment, supervised fine-tuning and unified multimodal Figure 3. Overview of the proposed unified multimodal posttraining pipeline. We jointly optimize multimodal understanding and generation through multi-task objective using quadruplet format (generation prompt, image, question, answer). Both I2T and T2I losses are computed simultaneously, enabling aligned cross-modal learning where each training sample supports both multimodal understanding and generation. post-training. During the first two stages, the visual encoders and LLM backbone are frozen to learn better multimodal generation. The focus of our design is the introduction of novel unified multimodal post-training stage (stage 3), where both multimodal understanding and generation are improved using joint set of data samples via multi-task objective (see Fig. 3). Stage 1: Cross-Modal Alignment. Here, the primary objective is to establish robust connections between visual and linguistic representations within unified embedding space. We adopt parameter-efficient approach by freezing the visual encoders and LLM backbone, and update only the DiT and MCP. In this stage, we conduct pre-training on JourneyDB [29], which provides high-quality 4 million textimage pairs covering diverse visual concepts, and 5 million pairs from BLIP3o-Short-Caption [4], curated subset emphasizing compositional understanding. Stage 2: Supervised Fine-tuning. Following initial alignment, we perform targeted fine-tuning on 105K curated prompt-image pairs (60K from BLIP3o [4], 45K from ShareGPT-4o-Image [6]) to address specific weaknesses observed after pre-training [4]. Due to our compact pretraining corpus (only 20% of BLIP-3os data during stage 1), the model initially struggled with complex human gestures, common objects and landmarks. This stage specifically targets these underrepresented domains while maintaining the same frozen/trainable component configuration as in the previous stage. Stage 3: Unified Multimodal Post-Training. This stage aims to improve both multimodal understanding and generation. To this end, we construct training samples as quadruplets = {p, ximg, q, a}, where denotes the gen5 Table 1. Comparison with recent multimodal understanding models. Und. and Gen. denote understanding and generation, respectively. Total Params represent the sum of visual encoder, language model, and diffusion/unet components (when applicable). Compared to unified models with similar size ( 2B), our Mobile-O-0.5B achieves superior overall performance with score of 61.9 averaged over seven datasets. Further, Mobile-O-0.5B also outperforms its understanding-only counterpart (FastVLM) by 1.6% in average performance. Type Model # Total Params MMMU TextVQA MMVet SEED ChartQA POPE GQA Average Und. Only > 1B Und. Only 1B LLaVA-Phi [54] LLaVA-v1.5-Phi-1.5 [54] MobileVLM [8] MobileVLM-V2 [9] LLaVa-OV [16] Smol-VLM-0.5B [21] FastVLM-0.5B [37] Und. and Gen. > 2B EMU3-8B [38] BLIP3o-4B [4] Und. and Gen. 2B Janus [40] Show-o [45] Show-o-Clip-ViT [45] JanusFlow [20] Mobile-O-0.5B (Ours) 3.1B 1.6B 1.7B 1.7B 1.6B 0.6B 0.6B 9.0B 7.1B 2.1B 1.5B 1.6B 2.1B 1.6B - 30.7 - - 31.4 33.7 33.3 31.6 46.6 30.5 25.1 27.4 29.3 34. 48.6 - 41.5 52.1 - 60.2 68.0 64.7 78.0 50.2 - 41.2 55.5 67.8 28.9 - - - 29.1 - 37. 37.2 60.1 34.3 - 20.9 30.9 38.1 - - - - 65.5 - 69.3 68.2 73.8 63.7 - 51.6 70.5 69. - - - - 61.4 62.8 71.6 68.6 - 53.0 - 44.7 64.6 75.2 85.0 84.1 84.5 84.3 - - 81. 85.2 - 87.0 73.8 84.5 88.0 86.4 - 56.5 56.1 59.3 - - 62.7 60.3 - 59.1 48.7 57.5 60.3 62. - - - - - - 60.5 59.4 - 54.0 - 46.8 57.0 62.1 eration prompt, ximg represents the image, and (q, a) form question-answer pairs (see Fig. 3). Since no existing dataset supports such quadruplet format, we construct the data as follows: 1. Prompt GPT-4o [23] to generate highly detailed compositionally-aware caption for each image. 2. Synthesize diverse question-answer sets probing different aspects of visual understanding. This yields unified dataset with bi-directional multimodal learning within single framework, where both understanding with image-to-text (I2T) and generation with text-to-image (T2I) tasks share the same embedding layer and autoregressive language model, as shown in Fig. 3. 3.4. Training Objectives level σ [0, 1] and form: xσ = (1 σ)x + σϵ, v(xσ; σ) = ϵ (7) The DiT model predicts velocity field vϕ(xσ, σ, cp) conditioned on MCP features cp derived from the generation prompt (see Eq. 4). The loss minimizes the weighted mean-squared error: Ldiff = Ex,p,ϵ,σ (cid:104) w(σ) vϕ(xσ, σ, cp) (ϵ x)2 (cid:105) (8) where, w(σ) is scale-dependent weighting function. This formulation directly learns the probability flow ODE, yielding faster and more stable training compared to standard diffusion objectives. Our unified training optimizes weighted combination of multimodal understanding and generation objectives: 4. Experiments Lunified = λlangLlang + λdiffLdiff (5) Image-to-Text (I2T) Loss. For multimodal understanding, we employ standard cross-entropy loss on the autoregressive language models output tokens: Llang = (cid:88) t=1 log (atximg, q, a<t) (6) where, the model predicts answer tokens conditioned on the image encoding and question q. Text-to-Image (T2I) Loss. For multimodal image generation, we employ flow-matching objective [4, 44] instead of standard noise prediction. Given clean latent from the VAE encoder and noise ϵ (0, I), we sample noise 4.1. Implementation Details We use FastVLM-0.5B [37] as the image understanding model, which extends FastViT [36] as the vision encoder and Qwen2-0.5B [34] as the language backbone. For image generation, we adopt the SANA-600M-512 [41] diffusion model as the visual generator. Both understanding and generation branches are connected through the proposed Mobile Conditioning Projector, implemented as lightweight linear layers with depthwise-separable convolutions for efficient cross-modal alignment. All images used for understanding tasks are resized to 1024 1024 resolution using bicubic interpolation, while generation tasks operate at 512 512. All experiments are conducted on single node equipped with 8 NVIDIA A100 GPUs, requiring approximately 3 days for 50k pre-training steps (roughly 3 epochs). The subsequent SFT and unified multimodal post-training 6 Table 2. Evaluation of text-to-image generation performance on the GenEval benchmark. Und. and Gen. denote understanding and generation, respectively. Total Params represent the sum of the visual encoder, language model, and diffusion/unet components (when applicable). Compared to unified models with similar size ( 2B), our Mobile-O-0.5B achieves superior overall score of 0.74 and outperforms Show-o-Clip-ViT [45] by 5.0%. Type Method # Total Params Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Gen. Only > 1B Gen. Only 1B LlamaGen [30] LDM [26] PixArt-α [3] SDXL [26] SDv2.1 [26] SANA-0.6B [42] SDv1.5 [26] SnapGen [5] Und. and Gen. > 2B SEED-X [11] Chameleon [33] LWM [18] BLIP3o-4B [40] Und. and Gen. 2B Janus [40] Show-o [45] Show-o-Clip-ViT [45] JanusFlow [20] Mobile-O-0.5B (Ours) 3.8B 1.5B 4.9B 3.0B 1.9B 2.6B 1.0B 0.4B 16.0B 34.0B 7.2B 7.3B 2.1B 1.5B 1.6B 2.1B 1.6B 0.71 0.92 0.98 0.98 0.98 0.99 0.97 1. 0.97 - 0.93 - 0.97 0.98 0.98 0.97 0.98 0.34 0.29 0.50 0.74 0.51 0.77 0.38 0.84 0.58 - 0.41 - 0.68 0.80 0.85 0.59 0. 0.21 0.23 0.44 0.39 0.44 0.62 0.35 0.60 0.26 - 0.46 - 0.30 0.66 0.67 0.45 0.57 0.58 0.70 0.80 0.85 0.85 0.88 0.76 0. 0.80 - 0.79 - 0.84 0.84 0.81 0.83 0.86 0.07 0.02 0.08 0.15 0.07 0.21 0.04 0.18 0.19 - 0.09 - 0.46 0.31 0.28 0.53 0. 0.04 0.05 0.07 0.23 0.17 0.47 0.06 0.45 0.14 - 0.15 - 0.42 0.50 0.55 0.42 0.49 0.32 0.37 0.48 0.55 0.50 0.66 0.43 0. 0.49 0.39 0.47 0.81 0.61 0.68 0.69 0.63 0.74 Figure 4. Qualitative comparison of text-to-image generation (left) and visual understanding (right) across unified multimodal models. Each column shows Janus, JanusFlow, Show-O, and Mobile-O (ours) for the same prompts/questions. Mobile-O yields more consistent, detailed, and semantically faithful images with high fidelity and style diversity for image generation. For visual understanding, it delivers more accurate and contextually coherent responses. Additional results are presented in suppl. material. Best viewed zoomed in. stages run for 20 epochs and 7 epochs, taking 15 hours and 5 hours, respectively. Detailed hyperparameter configurations for each stage are provided in the suppl. material. 4.2. Quantitative Comparison Multimodal Visual Understanding: We evaluate MobileO-0.5B on diverse suite of understanding benchmarks. General multimodal understanding and reasoning are evaluated on MMMU [50], MM-Vet [49], and SEED [15]. For OCR and text-based VQA, we employ TextVQA [28] and ChartQA [22]. Text hallucination robustness is examined on POPE [17], while scene understanding is assessed on GQA [13]. Tab. 1 shows the comparison with understanding-only models having <1B and >1B and unified models with <2B and >2B, on seven benchmarks. Here, the total number of parameters reflects all components, and not only the LLM. Mobile-O-0.5B offers distinct merits over models in its scale range (2B), such as Janus [47], JanusFlow [20], and Show-O [45]. Compared to JanusFlow [20], our Mobile-O-0.5B obtains an absolute gain of 4.9% averaged over seven benchmarks with less total parameters (JanusFlow: 2.1B vs. Ours: 1.6B). It is worth mentioning that our Mobile-O-0.5B obtains an absolute gain of 1.6% over FastVLM [37], highlighting the effectiveness of our unified multimodal post-training, where both understanding and generation tasks are improved via multi-task 7 objective using joint training samples as quadruplets. Text-to-Image Generation: We evaluate our model on the widely-used GenEval [12] benchmark. We follow strictly to raw prompts for GenEval. As shown in Tab. 2, we evaluate Mobile-O-0.5B with generation-only models having different sizes (> 1B and 1B) and unified models ( > 2B and 2B). Here, total number of parameters reflects all components. Compared to unified models with similar size ( 2B), our Mobile-O-0.5B achieves best overall results with score of 0.74, outperforming Show-o [45] by 5.0%. Text-and-Image-to-Image Generation: Beyond text-toimage generation and visual understanding, the Mobile-O framework naturally supports image editing, taking both source image and textual instruction as input and producing an edited image as output. This capability emerges from the MCP design, which bridges the understanding and generation pathways through shared multimodal representation. Because MCP captures low-level visual details from the input image, it is well-suited for editing tasks that require preserving the global scene structure while applying localized modifications. To enable image editing, we fine-tune Mobile-O on small subset of 46k editing samples from ShareGPT4V [6]. During editing, the source image is encoded through the vision encoder and projected via MCP, while the textual editing instruction is processed by the language model. The generation backbone then produces the edited image conditioned on both the visual and textual representations. No architectural modifications are requiredthe same MCP, language model, and generation backbone used for text-toimage generation and visual understanding are reused for editing. We evaluate Mobile-O-0.5B on the ImageEdit [48] benchmark, which measures both edit fidelity and scene preservation. Mobile-O-0.5B achieves an overall score of 2.5 on ImageEdit, despite being fine-tuned on only 46k editing samples. We note that Mobile-O-0.5Bs editing capability is achieved with minimal dedicated training data compared to specialized editing models such as BLIP3o [4] and Emu-Edit [38], which are trained on significantly larger editing-specific datasets. With dedicated fine-tuning on larger-scale editing data, we expect both the edit fidelity and global scene preservation to further improve. 4.3. Qualitative Comparison Fig. 4 illustrates the generation and understanding capabilities of Mobile-O-0.5B with other unified models 2B parameters. Compared to Janus, JanusFlow, and Show-O, Mobile-O-0.5B produces images with sharper details, more coherent layouts, and more consistent illumination. It maintains higher visual fidelity in complex scenes, such as tree leaves or strands of monkeys hair. Janus and JanusFlow show counting errors in the second row of Fig. 4, consistent with their lower counting scores in Tab. 2. These Figure 5. Qualitative image editing results of Mobile-O-0.5B. Given source image and textual editing instruction, MobileO-0.5B produces the edited output. The model is fine-tuned on only 46k editing samples from ShareGPT4V [6] counting issues sometimes yield higher diversity but reduce textimage alignment. For understanding, MobileO-0.5B correctly answers samples from ChartQA [22] and TextVQA [28], and in the last row accurately summarizes book cover, mentioning both title and author. Complete output comparison is provided in suppl. material. In Fig. 5, Mobile-O-0.5B successfully performs range of basic editing operations, including adding an object, attribute modification, and style transfer. 4.4. Ablation Study Generality of Mobile-O: natural question is whether the Mobile-O framework, specifically the Multi-modal Connector Projector (MCP), unified post-training data format, and training recipe, generalizes beyond the specific backbone choices presented in the main paper. To address this, we construct Mobile-O-1.5B by replacing the original components with larger counterparts: FastVLM-1.5B [37] as the vision-language understanding backbone and SANA1.5B [41] as the image generation backbone, yielding unified model with approximately 3.5B parameters. The MCP dimensions are adjusted accordingly to match the hidden sizes of the larger backbones, while the overall architecture and training procedure remain unchanged. We evaluate understanding performance across seven established benchmarks: MMMU [50], TextVQA [28], SEED-Bench [15], 8 Table 3. Mobile-O-1.5B: Scaling to FastVLM-1.5B and SANA1.5B components. Understanding performance is averaged over seven benchmarks (MMMU, TextVQA, SEED-Bench, ChartQA, POPE, GQA, MM-Vet). Generation quality is measured by GenEval overall score. The proposed post-training stage consistently improves both capabilities. Model Und. Acc. (%) Gen. Acc. (%) FastVLM-1.5B [37] SANA-1.5B [41] Mobile-O-1.5B (SFT) Mobile-O-1.5B (Post-train) 64.8% 64.8% 66.2% 66% 75% 78% ChartQA [22], POPE [17], GQA [13], and MM-Vet [49]. For generation quality, we report the GenEval [12] overall score. Results are summarized in Table 3. Mobile-O-1.5B after supervised fine-tuning preserves the standalone the full understanding capability of FastVLM-1.5B (64.8% average across the seven benchmarks) while simultaneously gaining strong generation ability (75% GenEval), which the original FastVLM entirely lacks. After the post-training stage, both capabilities improve further: understanding increases to 66.2% (+1.4% absolute over SFT) and generation reaches 78% (+3% absolute over SFT). Notably, the post-trained Mobile-O-3B also surpasses the standalone SANA-1.5B generation backbone (78% vs. 66%), demonstrating that the unified training and post-training recipe not only preserves but enhances the individual component capabilities. These results confirm that the Mobile-O framework is architecture-agnostic: the MCP design, unified data format, and post-training recipe transfer effectively to larger backbones, consistently improving both understanding and generation. We analyze the contributions of the proposed MCP design and the effectiveness of our post-training data strategy. On the MCP Design. Tab. 4 shows how different MCP configurations influence cross-modal alignment and generation quality. Notably, all experiments in this table are conducted without pre-training. Using simple MLP connector between the VLM and diffusion decoder achieves 68.5% on GenEval but requires over 3.2M trainable parameters. Replacing it with our single-layer MCP with compression module reduces parameter count by nearly half, while maintaining comparable performance of 68.4%. Extending to the last four layers with uniform fusion further improves alignment to 69.6%. Introducing learnable weights across layers enables the model to dynamically attend to informative representations, boosting accuracy to 70.0%. Finally, adding the lightweight refinement block leads to best results of 70.4% with only 2.4M parameters. On the Effect of Unified Post-Training. Tab. 5 evaluates our efficient post-training phase designed to enhance both understanding and generation tasks. We compare stanTable 4. Ablation on the Mobile Conditioning Projector (MCP). We study the effect of layer fusion, learnable weighting, and the refinement block. Proj. # Layers Fusion Compress CA Acc. (%) Params (M) MLP MCP MCP MCP MCP 1 4 4 4 Uniform Uniform Learnable Learnable 68.5 68.4 69.6 70.0 70.4 3.3 1.7 1.7 1.7 2.4 Table 5. Effect of Unified Post-Training. Our post-training data improves both understanding and generation alignment when using joint quadruplets. Method Und. Acc. (%) Gen. Acc. (%) SFT SFT + Post-Train (image-text pairs) SFT + Post-Train (quadruplets) 60.5 60.6 62.1 73.3 73.4 74.2 dard SFT against two post-training variants. Adding posttraining with generation-only triplets slightly improves results across benchmarks, showing better consistency in generative alignment. When generation and understanding triplets are used jointly, we observe measurable improvements, increasing average accuracy on seven image understanding tasks from 60.5% to 62.1% and GenEval by 1%. These results demonstrate that multi-objective post-training is straightforward yet effective approach to enhance crossmodal coherence without need for large-scale pre-training. 4.5. Edge Deployment To assess the practicality on consumer devices, we evaluate recent unified methods below 2B parameters on three representative edge platforms: MacBook M2 Pro, NVIDIA Jetson Orin Nano, and iPhone 17 Pro. Tab. 6 reports inference times for visual understanding (vision encoder + text token forward time, TTFT) and total latency for image generation with 20 denoising steps. Mobile-O-0.5B demonstrates notable efficiency gains over prior unified models. On the MacBook M2 Pro, it is 28 faster than Janus and Show-O for understanding and 1146 faster for image generation. On Jetson Orin Nano, Mobile-O-0.5B generates images in only 4 s, vs. 2252 for other methods. On iPhone 17 Pro, Mobile-O-0.5B achieves vision encoder latency of 102 ms, TTFT of 248 ms, and image generation in 3.0 s, highlighting its suitability for real-world deployment. For mobile deployment, Mobile-O-0.5B components are converted using MLX [2] and CoreML [1]. The language model runs in MLX Swift with 8-bit weights on GPU for efficient token decoding, while the vision encoder, DiT backbone, VAE decoder, and MCP are exported to Core ML in float32, keeping the total memory footprint below 2GB. 9 Image understanding and generation performance Table 6. comparison on MacBook M2 Pro, Jetson Orin Nano, and iPhone for Mobile-O-0.5B. Vision Enc. and TTFT denote understanding latency, while Latency indicates image generation latency. Model Vision Enc. (ms) TTFT (ms) Latency (s) MacBook M2 Pro Janus JanusFlow Show-O Mobile-O-0.5B (Ours) 783 244 1909 466 699 107 56 7 289 19 935 152 797 5 187 31 201 15614 24 0.8 47 0.2 4 0.5 Jetson Orin Nano Janus JanusFlow Show-O Mobile-O-0.5B (Ours) 745 19 741 27 403 4 88 7 749 19 745 27 720 14 488 9 44 0.8 22 0.1 52 4 4 0.6 Mobile-O-0.5B (Ours) 102 4 248 3 0.5 iPhone 17 Pro 5. Conclusion We introduce unified visionlanguagediffusion model, Mobile-O, with new quadruplets format for unified posttraining and mobile conditioning projector to achieve highquality image understanding and text-to-image generation on edge devices. Experiments on MacBook M2 Pro, Jetson Orin Nano, and iPhone device show that Mobile-O outperforms recent unified models in both latency and memory efficiency, while preserving visual fidelity and semantic accuracy. Mobile-O-0.5B maintains memory footprint below 2GB on iPhone within 3 seconds, making it practical for real-time on-device deployment. 6. Acknowledgment The computations were enabled by resources provided by NAISS at Alvis partially funded by Swedish Research Council through grant agreement no. 2022-06725, LUMI hosted by CSC (Finland) and LUMI consortium, and by Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the NSC."
        },
        {
            "title": "References",
            "content": "learning models into Integrate machine https : / / developer . apple . com / [1] Core ml: your app. documentation/coreml, . Accessed: 2025-11-13. [2] Mlx: Machine learning for apple silicon. https:// . Acopensource.apple.com/projects/mlx, cessed: 2025-11-13. [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [4] Jiuhai Chen et al. Blip3-o: family of fully open unified 10 multimodal modelsarchitecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [5] Jierun Chen et al. Snapgen: Taming high-resolution text-toimage models for mobile devices with efficient architectures and training. In CVPR, 2025. [6] Junying Chen et al. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025. [7] Jierun Chen et al. Snapgen: Taming high-resolution text-toimage models for mobile devices with efficient architectures and training. In CVPR, 2025. [8] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. [9] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Faster and Lin, Bo Zhang, et al. Mobilevlm v2: stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [11] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NIPS, 2023. [13] Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. [14] Sungwoong Kim, Daejin Jo, Donghoon Lee, and Jongmin Kim. Magvlt: Masked generative vision-and-language transformer. In CVPR, 2023. [15] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal large language models with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [16] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. [17] Yixuan Li, Dongxu Li, Wenguan Li, and Yi Yang. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. [18] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. [19] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. In CVPR, 2024. [20] Yiyang Ma et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In CVPR, 2025. [21] Andres Marafioti et al. SmolVLM: Redefining small and efficient multimodal models. In Second Conference on Language Modeling, 2025. [22] Ahmed Masry, Xuan Long Do, Jianshuo Qi Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. [23] OpenAI et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Xinzhe Pan, Shijie Sun, Jianwei Yang, and Zicheng Liu. arXiv Transfer between modalities with metaqueries. preprint arXiv:2501.09234, 2025. [25] Morgane Riviere et al. Gemma 2: Improving open language models at practical size, 2024. [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [27] Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, and Fahad Shahbaz Khan. Mobile-videogpt: Fast and accurate video understanding language model. arxiv, 2025. [28] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. [29] Keqiang Sun et al. Journeydb: benchmark for generative image understanding. In NIPS, 2023. [30] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [31] Quan Sun, Yufeng Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. [32] Weijia Sun et al. Onecat: Decoder-only auto-regressive arXiv model for unified understanding and generation. preprint arXiv:2504.01240, 2025. [33] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. arXiv preprint [34] Qwen Team. Qwen2 technical report. arXiv:2407.10671, 2024. [35] Shengbang Tong et al. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [36] P.K. Anasosalu Vasu, J. Gabriel, J. Zhu, O. Tuzel, and A. Ranjan. Fastvit: fast hybrid vision transformer using structural reparameterization. In ICCV, 2023. [37] Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, and Hadi Pouransari. Fastvlm: Efficient vision encoding for vision language models. In CVPR, 2025. 11 [38] Xinlong Wang, Quan Sun, Yufeng Zhang, Yuming Cui, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [39] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Janus: Decoupling visual encodYu, Chong Ruan, et al. ing for unified multimodal understanding and generation. In CVPR, 2025. [40] Chengyue Wu et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In CVPR, 2025. [41] Enze Xie, Junsong Chen, Han Cai, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [42] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In ICLR, 2025. [43] Jinheng Xie, Weijia Yang, Zhenheng Yang, and Mike Zheng Show-o: One single transformer to unify mularXiv preprint Shou. timodal understanding and generation. arXiv:2408.12528, 2024. [44] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv Improved native unified multimodal models. o2: preprint arXiv:2506.15564, 2025. [45] Jinheng Xie et al. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025. [46] Haotian Xu, Yucheng Zhang, Zhiwei Wang, Yang Liu, and Jie Zhou. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.12345, 2025. [47] Yuchen Yao, Feng Li, Junnan Wu, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2407.12345, 2024. [48] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. In NeurIPS, 2025. [49] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023. [50] Xiang Yue et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [51] Xiang Zhang, Wei Liu, and Tianlong Wang. Tbac-uniimage: Unified understanding and generation by ladder-side diffusion tuning. In CVPR, 2025. [52] Chunting Zhou et al. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2411.05882, 2024. [53] Tianyi Zhou et al. Mm-r1: Unleashing the power of unified multimodal large language models for personalized image generation. arXiv preprint arXiv:2505.10073, 2025. [54] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. Llava-phi: Efficient multi-modal assistant with small In Proceedings of the 1st International language model. Workshop on Efficient Multimedia Computing under Limited, 2024. Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 7. Ablation on the number of layers for the Mobile Conditioning Projector (MCP). We systematically vary the number of VLM layers aggregated by MCP to condition the diffusion process. All configurations use the final design of MCP: learnable fusion, compression , and channel attention (CA). Proj. # Layers Fusion Compress CA Accuracy (%) MCP MCP MCP MCP 1 2 4 8 - Learnable Learnable Learnable 68.7 69.8 70.4 70.2 7. Mobile Conditioning Projector Depth The Mobile Conditioning Projector (MCP) aggregates features from multiple VLM layers to provide rich semantic conditioning for the diffusion model. Tab. 7 investigates how the number of aggregated layers affects text-to-image generation quality on GenEval. Using single layer yields 68.7% accuracy, suggesting that features from one depth level provide insufficient semantic diversity for accurately capturing complex compositional prompts. Aggregating 2 layers with learnable fusion improves performance to 69.8%, demonstrating the value of combining features from different network depths. The best performance (70.4%) is achieved with 4 layers, striking an optimal balance between Interestsemantic richness and computational efficiency. ingly, further increasing to 8 layers slightly degrades performance to 70.2%, indicating that excessive aggregation may introduce redundant or conflicting information that complicates the conditioning process. With four layers, it suggests that mid-depth VLM features capture the most relevant semantic abstractions for guiding compositional image generation, while avoiding the diminishing returns and increased computational cost associated with deeper aggregation. 8. On-Device Mobile Deployment Fig. 6 demonstrates Mobile-O running natively on an iPhone 17 Pro, validating the practical feasibility of deploying unified models on consumer mobile devices. The implementation showcases both core capabilities within chat-based interface: text-to-image generation produces detailed Bengal tiger image from complex compositional prompt in 3 seconds, while image-to-text generation provides rich visual descriptions analyzing scene composition, subject positioning, depth perception, and atmospheric qualities in 0.3 seconds for text token forward time. The chat-based interface enables seamless switching between understanding and generation tasks within single unified model, showcasing practical mobile AI applications withFigure 6. Mobile-O running natively on iPhone 17 Pro. We demonstrate real-world deployment of Mobile-Os unified capabilities on consumer hardware. (a) Text-to-image generation: Given detailed prompt describing Bengal tiger. (b) Image-to-text generation: Mobile-O provides detailed visual descriptions, analyzing composition and subject positioning out cloud dependency, ensuring user privacy and enabling offline functionalitycritical requirements for real-world mobile applications. This deployment validates our architectural optimizations for the design choices, including the Mobile Conditioning Projector, proving that an efficient yet effective unified model can maintain high-quality unified capabilities with less than 2GB of memory. 9. More Implementation Details All experiments are conducted on single node with 8 NVIDIA A100 GPUs (80GB VRAM). We employ DeepSpeed ZeRO-3 during Stage 1 to efficiently handle the 9M training samples and large model parameters, then switch to ZeRO-1 for the last two stages, where smaller dataset sizes allow for reduced communication overhead. Mixedprecision training with BF16 throughout due to better numerical stability with transformer architectures. TF32 is enabled for matrix multiplications to leverage Ampere architecture acceleration. Images for understanding tasks undergo bicubic interpolation to 10241024, while generation tasks use 512512. We use LoRa with reduced rank (r=16) and α=32 to prevent overfitting during unified training on the smaller 105K quadruplet dataset while still allowing fine-grained adaptation. All LoRA modules use dropout of 0.1 for regularization. All stages use cosine annealing with minimum learning rate thresholds: Stage 1: LR decays from 2e-4 to 12 Stage 1: Cross-Modal Alignment Stage 2: Supervised Fine-tuning Stage 3: Unified Post-Training Data Source Total Samples Format Learning Rate Batch Size LR Schedule Min LR LR Warmup Ratio Optimizer Weight Decay Epochs Training Time JourneyDB (4M) + BLIP3o-Short (5M) 9M Prompt-image pairs 2e-4 512 cosine w/ min LR 2e-6 0.02 AdamW (β2=0.95) 0. 5 3 days BLIP3o-60K + ShareGPT-4o (45K) 105K Prompt-image pairs 2e-4 384 cosine w/ min LR 1e-6 0.05 AdamW (β2=0.95) 0.01 20 15 hours BLIP3o-60K + ShareGPT-4o (45K) 105K Quadruplet {p, ximg, q, a} 1e-4 128 cosine w/ min LR 1e-6 0.05 AdamW (β2=0.95) 0.01 7 5 hours Trainable Modules Frozen Modules DiT + MCP VE + LLM + VAE DiT + MCP VE + LLM + VAE DiT + MCP + LLM + VE VAE Table 8. Three-stage training setup for Mobile-O. Stage 1 establishes cross-modal alignment using large-scale image-text pairs. Stage 2 performs targeted fine-tuning to address weaknesses in complex gestures, common objects, and landmarks. Stage 3 introduces unified multimodal post-training with quadruplet samples {p, ximg, q, a} for joint understanding and generation. All experiments were conducted on 8A100 GPUs. 2e-6 over 50K steps with 2% warmup (1,000 steps), allowing aggressive initial learning while maintaining stability in later training. Stage 2: LR decays from 2e-4 to 1e-6 with 5% warmup, providing more gradual adaptation for the targeted fine-tuning phase. Stage 3: Reduced initial LR of 1e-4 (min: 1e-6) with 5% warmup accommodates the unified training paradigms increased complexity. 10. More Image-to-Text Qualitative Results Fig. 7 evaluates the models ability to perform dense text understanding and information extraction from real-world imagery. The task requires reading small, low-contrast text from books back cover and summarizing its bibliographic informationa challenging scenario combining OCR, reading comprehension, and structured information extraction. Mobile-O accurately identifies the book as From the Pest Zone: The New York Stories authored by H.P. Lovecraft, correctly extracts the editors names (S.T. Joshi and David E. Schultz), identifies specific story titles mentioned in the synopsis, and even captures the price 15.00 USD. In contrast, competing models exhibit significant hallucinations and misidentify the book title, authors, and fail to display the price. These results validate MobileOs robust text understanding capabilities even in challenging real-world conditions with dense text, complex layouts, and varying contrast levels. Fig. 9 presents comprehensive qualitative evaluation of visual understanding capabilities across unified visionlanguage models on diverse question-answering tasks. The Figure 7. Qualitative comparison on dense text understanding and information extraction. We evaluate Mobile-O against other models on challenging OCR and comprehension task requiring the model to read, parse, and summarize the back cover text of book. Green text indicates correctly extracted information, while red indicates hallucinations or errors. Mobile-O demonstrates superior performance in accurately extracting key bibliographic details, including the correct title, author, editors, and price information from the densely-packed text on the book cover. comparison spans multiple cognitive domains including scientific reasoning (organic chemistry reaction analysis), optical character recognition with challenging perspectives and lighting conditions (theater signage reading), fine-grained 13 mountain landscape, Mobile-O captures more realistic geological textures and natural color grading, while SANA exhibits somewhat exaggerated saturation in the foreground flowers. The portrait comparison reveals Mobile-Os superior handling of skin tones and facial features with more natural lighting and realistic depth of field. Mobile-O achieves these results while simultaneously supporting visual understanding tasks within the model. 12. More Text-to-Image Qualitative Results Fig. 10 presents comprehensive qualitative comparison between Mobile-O and recent unified models across diverse and challenging prompts. The comparison includes Janus [47], JanusFlow [20], and Show-O [45], evaluating generation quality on prompts ranging from fantastical scenes (underwater cities, fire-breathing dragons) to photorealistic scenarios (bio-luminescent bays, space nebulae, portrait photography). Mobile-O demonstrates competitive visual quality while maintaining significantly lower computational requirements suitable for mobile deployment. Notably, Mobile-O excels at rendering fine details and maintaining prompt adherence across complex compositional scenarios, such as the intricate architectural details in the underwater city scene and the nuanced lighting in the portrait photography example. While competing models occasionally produce visually striking results, MobileO achieves favorable balance between generation quality, prompt fidelity, and computational efficiency. The nebula scene particularly highlights Mobile-Os ability to capture subtle color gradations and spatial depth, while the elderly woman portrait demonstrates proficient handling of photorealistic skin textures and natural lighting. Fig. 11 showcases Mobile-Os text-to-image generation capabilities across diverse categories, including photorealistic portraits, macro nature photography, food imagery, and creative scenes with complex lighting effects. The model demonstrates proficiency in rendering fine details (facial features, textures), managing challenging optical effects (bokeh, volumetric lighting, caustics), and maintaining color accuracy across varied subjects. These results validate Mobile-Os versatility in generating high-quality imagery across different styles and compositional complexities while operating within mobile computational constraints. The prompts used in Fig. 11 are provided in Tab. 9. 13. Limitations Mobile-O currently reuses the same lightweight LLM from the unified VLM as its text encoder, rather than employing dedicated standalone language model optimized solely for textual understanding. This design choice significantly reduces memory footprint and allows on-device deployment, but it may limit the expressiveness and depth of text repreFigure 8. Qualitative comparison with SANA-0.6B on text-toimage generation. We compare Mobile-O (1.6B total parameters) against SANA-0.6B (2.6B total parameters), our generation baseline, on challenging prompts requiring photorealistic rendering, complex lighting, and fine-grained details. Mobile-O demonstrates competitive or superior visual quality across diverse scenarios, including wildlife photography, landscape composition, and portrait rendering. Best viewed zoomed in. object recognition requiring specific domain knowledge (retro gaming console and software identification), text extraction from stylized fonts (comic book titles), and cultural artifact classification (ancient civilization identification) from MMMU [50], ChartQA [22], and TextVQA [28]. These results validate that Mobile-Os mobile-optimized architecture preserves robust visual understanding capabilities, demonstrating that aggressive model compression need not compromise the ability to accurately interpret and reason about diverse visual information. 11. Comparison with Generation-Only Baseline Fig. 8 compares Mobile-O against SANA-0.6B, the generation component that serves as our baseline architecture. Despite Mobile-O having 1.6B total parameters compared to SANA-0.6Bs 2.6B parameters (38% reduction), Mobile-O achieves competitive or superior generation quality across diverse prompts. In the rainforest scene, Mobile-O produces sharper feather details and more natural background compared to SANAs slightly oversaturated rendering. For the 14 Figure 9. Qualitative comparison of image-to-text across unified models below 2B. Mobile-O is compared against Janus [47], JanusFlow [20], and Show-O [45] on diverse visual question answering tasks, including scientific reasoning, OCR, object recognition, and cultural knowledge. Green indicates correct answers, red indicates errors. Mobile-O demonstrates competitive visual understanding, despite its mobile-optimized architecture, correctly answering complex questions that require fine-grained visual analysis and domain knowledge. sentations compared to approaches that use larger text-only models. For instance, SANA [41] adopts Gemma-2B-it [25] as dedicated text encoder, benefiting from more powerful linguistic backbone that can yield better alignment. ically increases total memory requirements by several additional GBs. This exceeds the memory constraints of most mobile and resource-limited edge devices, where efficiency and low latency are core deployment objectives. However, integrating such model into Mobile-O is currently impractical for on-device deployment. 2Bparameter model in FP16 requires approximately 4.0 GB just for the weights alone, excluding memory for activations, attention caches, and runtime overhead, which typ15 Figure 10. Qualitative comparison of text-to-image generation across unified models below 2B. Mobile-O is compared against Janus [47], JanusFlow [20], and Show-O [45] on challenging prompts spanning fantasy, photorealism, and scientific visualization. Despite its mobile-optimized architecture, Mobile-O maintains competitive visual quality and prompt adherence. Best viewed zoomed in. 16 Figure 11. Additional Text-to-Image generation examples of Mobile-O. Best viewed zoomed in. Table 9. Text-to-image generation prompts used for visualization. Row 1 (1,1) (1,2) (1,3) (1,4) Detailed Prompts for Image Generation in Fig. 11 (i, j) denotes the image at row i, column Portrait Photography Young woman with freckles, green eyes with detailed iris and catchlight, natural skin texture, flowing red hair catching golden hour sunlight, warm vibrant lighting, gentle expression, shallow depth of field Elderly Asian man with silver hair and peaceful expression, kind brown eyes, colorful traditional clothing, bright natural window lighting, detailed skin texture Young man with curly hair and beard, bright turquoise shirt, outdoor lighting Child with bright blue eyes and blonde hair, curious expression, rosy cheeks, individual eyelashes visible, natural freckles, bright daylight from side creating dimension, colorful playground in blurred background Row 2 (2,1) Flowers & Garden Scenes Sunflower field with detailed centers, yellow petals with texture, bees visiting flowers, blue sky with white clouds, warm summer sunlight, cheerful atmosphere, depth of field (2,2) (2,3) (2,4) Single red tulip with dewdrops on petals, water droplets reflecting light, delicate petal veins, green stem, bright spring morning light, shallow depth of field Cherry blossom branch in full bloom, pink petals with detailed stamens, some petals floating in air, bright blue sky background, spring sunshine creating glow, intricate branch structure Cluster of wildflowers, purple lupines and orange California poppies with detailed petal texture, yellow daisy centers, white Queen Annes lace intricate patterns, bright midday sun, morning dew Row 3 (3,1) Food & Culinary Compositions Rainbow cake slice showing colorful layers, white frosting, sprinkles, bright studio lighting (3,2) (3,3) (3,4) Basket with red apples on wooden kitchen table, bright natural window lighting Colorful layered smoothie in clear glass, vibrant pink strawberry, purple acai, yellow mango, green spinach layers, fresh fruit garnish, straw, bright natural lighting Ice cream sundae with rainbow sprinkles, individual sprinkle shapes and colors sharp, melting ice cream texture, whipped cream peaks, glossy cherry, colorful syrup drizzle, bright studio lighting Row 4 Colorful Objects & Items (4,1) Colorful hot air balloons in mid-air, red, yellow, and blue, wicker baskets, golden morning sunlight, blue sky, countryside below (4,2) (4,3) (4,4) Colorful kite with rainbow geometric patterns, ribbon tails, bright blue sky, sunlight Ornate stained glass window with intricate patterns in reds, blues, yellows, greens, bright sunlight streaming through, colorful light on floor Soap bubble bursting in mid-air, water droplets spraying outward, iridescent rainbow colors on fragmenting bubble surface, dynamic motion, dramatic lighting Row 5 Natural Landscapes & Outdoor Scenes (5,1) Golden retriever dog sitting on tropical beach, turquoise water, white sand, bright blue sky, sunny day (5,2) (5,3) (5,4) Autumn forest path with vibrant orange, red, and yellow fall foliage, sun rays piercing through misty air, leaves falling mid-flight, dramatic golden light beams, warm glowing atmosphere Orange and white clownfish among pink sea anemone tentacles with texture, vibrant purple and yellow corals with polyp detail, blue tang fish nearby, bright sunlight filtering through water creating god rays, bubbles rising Landscape after rain with vibrant double rainbow arching across sky, green rolling hills with visible grass texture, wildflowers, small farmhouse, sunlight breaking through dramatic clouds, puddle in foreground reflecting rainbow, wet grass sparkling"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Linkoping University",
        "Mohamed bin Zayed University of Artificial Intelligence"
    ]
}