{
    "paper_title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
    "authors": [
        "Yansong Qu",
        "Zihao Sheng",
        "Zilin Huang",
        "Jiancong Chen",
        "Yuhao Luo",
        "Tianyi Wang",
        "Yiheng Feng",
        "Samuel Labi",
        "Sikai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl."
        },
        {
            "title": "Start",
            "content": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving Yansong Qua, Zihao Shengb, Zilin Huangb, Jiancong Chena, Yuhao Luob, Tianyi Wangc, Yiheng Fenga, Samuel Labia, Sikai Chenb* Lyles School of Civil and Construction Engineering, Purdue University, West Lafayette, 47907, USA Department of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, 53706, USA Department of Civil, Architectural, and Environmental Engineering, University of Texas at Austin, Austin, 78712,"
        },
        {
            "title": "USA",
            "content": "* Corresponding author. E-mail address: sikai.chen@wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has emerged as dominant paradigm for end-to-end autonomous driving (AD) with real-time inference. However, RL typically suffers from sample inefficiency and lack of semantic interpretability in complex scenarios. To mitigate these limitations, Foundation Models (particularly, Vision-Language Models (VLMs)) can be integrated because they offer rich, context-aware knowledge. Yet still, deploying such computationally intensive models within high-frequency multi-environment RL training loops is severely hindered by prohibitive inference latency and the absence of unified integration platforms. To bridge this gap, we present Found-RL, specialized platform tailored to leverage foundation models to efficiently enhance RL for AD. core innovation of the proposed platform is its asynchronous batch inference framework, which decouples heavy VLM 1 reasoning from the simulation loop. This design effectively resolves latency bottlenecks, supporting real-time or near-real-time RL learning from VLM feedback. Using the proposed platform, we introduce diverse supervision mechanisms to address domain-specific challenges: we first implement Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Furthermore, for dense supervision, we adopt high-throughput CLIP for reward shaping. We mitigate CLIPs dynamic blindness and probability dilution via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields normalized, margin-based bonus from contextspecific action-anchor scoring. Found-RL delivers an end-to-end pipeline for fine-tuned VLM integration with modular support, and shows that lightweight RL model with millions of parameters can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl."
        },
        {
            "title": "Keywords",
            "content": "Autonomous driving Vision-language models Reinforcement learning"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of robust autonomous driving requires agents that can navigate complex, dynamic environments with human-like understanding (Qu et al., 2025b). Traditional rule-based approaches struggle with corner cases. On the other hand, data-driven approaches, particularly RL, have emerged as promising paradigm for learning end-to-end control policies through interaction with environments. However, training RL agents from the ground up is akin to student attempting \"self-study\" with textbook (Lake et al., 2017; Sutton and Barto, 1998); the process is often inefficient and \"scattergun\" (Fig. 1 (a)), relying on exhaustive trial-and-error. To mitigate this, Human-in-the-loop RL (Huang et al., 2024a, 2025a; Peng et al., 2024) introduces human experts to correct behaviors, acting as private tutor. While this \"Ask your teacher\" approach is accurate (Fig. 1 (b)), it is intrinsically demanding and unscalable due to the high cost of human labor and attention fatigue. 2 The recent surge in the use of Foundation Models, specifically VLMs, offers transformative third path: Foundation Model-enhanced RL. Foundation Models are large, pre-trained models trained on massive and diverse datasets and can be adapted to many downstream tasks. As illustrated in Fig. 1 (c), these models function as \"Tireless Mentor\" because they possess the semantic reasoning capabilities of human teacher while remaining \"Always Ready\" to provide scalable feedback. VLMs can interpret complex driving scenes and generate meaningful supervision signals, potentially combining the data efficiency of imitation learning with the selfexploration of RL. Despite this promise, integrating these computationally heavy models into multi-environment high-frequency RL pipelines remains significant engineering hurdle. The inference latency and the lack of unified frameworks have largely confined such research to theoretical exploration rather than efficient, closed-loop training. To bridge this gap, we propose Found-RL, platform tailored to foundation model-enhanced RL in autonomous driving. Found-RL provides streamlined pipeline spanning from CARLA (Dosovitskiy et al., 2017)- based simulation to specialized training modules. Unlike generic implementations, the proposed platform features novel asynchronous batch inference framework which decouples heavy VLM reasoning from the real-time simulation loop. This design effectively resolves the latency bottleneck, enabling real-time or near-real-time training with online supervisions from VLMs. Found-RL also serves as versatile testbed for diverse supervision mechanisms, we systematically explored multiple strategies to leverage VLM feedback, specifically implementing VLM-action guidance to provide online expert actions and VLM-based reward shaping to densify sparse environmental rewards with semantic understanding. By unifying efficient engineering with flexible algorithm support, Found-RL lowers the barrier for researchers to validate new architectures, ultimately advancing the development of the next generation of semantically aware autonomous driving agents. The main contributions of this work are summarized as follows: (1) Unified Platform for Foundation Model-Enhanced RL: We propose Found-RL, framework specifically tailored for autonomous driving. It features novel asynchronous batch inference architecture that 3 decouples computationally intensive VLM reasoning from the RL simulation loop, successfully resolving latency bottlenecks and enabling efficient, real-time or near-real-time closed-loop RL training with VLMs. (2) VLM Action Guidance Mechanisms: To effectively leverage expert-like action advice from VLMs, we introduce Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG). These mechanisms facilitate the distillation of VLMs action knowledge into the RL policy, significantly improving exploration efficiency and decision-making quality. (3) CLIP-based Reward Shaping: We introduce Conditional Contrastive Action Alignment, which conditions CLIP (Radford et al., 2021) text prompts on discretized ego speed and route command and scores small context-specific set of action anchors to derive normalized, margin-based reward bonus, mitigating CLIPs dynamic blindness and probability dilution while providing dense supervision. Fig. 1. Conceptual comparison of Reinforcement Learning, Human-in-the-loop RL, and Foundation Modelenhanced RL."
        },
        {
            "title": "Related work",
            "content": "In this section, we review three related threads: reinforcement learning-based autonomous driving, foundation models in autonomous driving, and simulation platforms for autonomous driving. We focus on these aspects 4 because our work lies at their intersection: RL provides the learning framework, foundation models offer semantic priors to guide training, and simulation platforms enable scalable and reproducible evaluation. 2.1. Reinforcement learning-based autonomous driving"
        },
        {
            "title": "RL  has  emerged  as  a  promising  paradigm  for  autonomous  driving  due  to  its  ability  to  learn  complex  sequential",
            "content": "decision-making policies through interaction with the environment. Its potential to optimize long-term objectives without requiring explicit supervision makes it well-suited for handling dynamic and uncertain traffic scenarios (Qu et al., 2025a). Numerous studies (Sheng et al., 2025b, 2025c) have applied RL to various aspects of autonomous driving, including decision-making, scenario generation, etc. However, most existing works (Sheng et al., 2024) are developed in isolated environments with customized pipelines, leading to limited reproducibility and comparability. Moreover, despite the recent success of foundation models such as VLMs and Large-Language Models (LLMs), mainstream RL approaches (Long et al., 2024) still heavily rely on self-exploration and sparse reward feedback. They often overlook the potential of expert-guided learning (Peng et al., 2022) or semantically-rich reward shaping mechanisms (Huang et al., 2024b) to accelerate and stabilize policy learning, especially in safety-critical domains. To address these gaps, we propose VLM-enhanced platform that supports training, evaluation, and benchmarking of RL-based autonomous driving agents. Our platform enables plug-and-play integration of VLMs to guide exploration and augment reward design, significantly improving the performance of traditional RL algorithms such as SAC (Haarnoja et al., 2018) and DrQv2 (Yarats et al., 2021). Extensive experiments across diverse benchmarks demonstrate the effectiveness and generalizability of our framework. 2.2. Foundation models in autonomous driving Foundation models are pre-trained on large-scale and diverse data, and have shown strong generalization ability across domains. When fine-tuned with domain-specific data, they can be adapted into specialists that perform well in targeted tasks. Recent efforts such as DeepSeek-R1 (Guo et al., 2025) have focused on using RL to enhance the 5 reasoning and generalization capabilities of LLMs. In contrast, our focus is on the reverse direction, leveraging foundation models to assist RL agents. In the context of autonomous driving, VLMs and LLMs have been applied to tasks such as semantic understanding (Sheng et al., 2025a), language-guided decision-making (Jiang et al., 2024), and natural language explanations of driving behavior (Xu et al., 2024). For example, VLMs can match visual inputs from the driving environment with natural language goals or instructions, helping the agent to navigate complex scenarios in more human-aligned manner. LLMs have been used to interpret and explain driving decisions, define high-level goals, and assess the plausibility of planned trajectories (Zhang et al., 2024). These applications highlight the potential of VLMs and LLMs to improve transparency, flexibility, and safety in autonomous driving systems. To bridge the gap between foundation models and RL, we present Found-RL, platform designed to integrate VLMs into RL pipelines for autonomous driving. Found-RL enables training, evaluation, and benchmarking with support for plug-and-play modules based on foundation models. These modules can be used to guide exploration, provide reward signals and so on. By enabling broad integration of foundation models, Found-RL serves as flexible and extensible platform that supports the autonomous driving community in advancing research at the intersection of RL and foundation model technologies. 2.3. Simulation platforms for autonomous driving Several frameworks have been developed to support RL research, such as Stable-Baselines3 (SB3) (Raffin et al., 2021), RLlib (Liang et al., 2018), and CleanRL (Huang et al., 2022). These libraries offer modular and well-tested implementations of popular RL algorithms, making them accessible for general-purpose experimentation. However, they are not specifically designed for autonomous driving scenarios and often lack support for the complex, highdimensional, and safety-critical environments that characterize this domain. To address this gap, platforms such as CARLA Leaderboard (Dosovitskiy et al., 2017) and Sky-Drive (Huang et al., 2025b) provide simulation-based environments and evaluation protocols tailored to autonomous driving. These platforms allow for policy training, testing, and benchmarking under realistic driving scenarios. Nevertheless, 6 they have yet to integrate emerging foundation model technologies, such as VLMs, which have shown strong potential in improving sample efficiency, semantic understanding, and planning capabilities. To enable deeper integration of foundation models into RL workflows for autonomous driving, we introduce new platform FoundRL. Our platform is designed to support foundation model-enhanced RL with features that address current bottlenecks in model-agent interaction. Specifically, we implement asynchronous batch inference, which allows VLMs to process observations in parallel with environment simulation, ensuring that the environment is not paused while waiting for model outputs. This design effectively mitigates the inference-induced delays that typically hinder real-time training. The platform supports CLIP (Radford et al., 2021)-based reward computation, expert action guidance from VLMs, and can be extended to support more applications. Together, these capabilities establish flexible and extensible foundation for advancing RL in autonomous driving with integrated foundation model support."
        },
        {
            "title": "Preliminaries",
            "content": "In this section, we first introduce the problem formulation of foundation model-enhanced RL, and we then illustrate the off-policy actor-critic learning. These serve as the base for the explanation of our proposed method. 3.1. Problem Formulation We formulate an autonomous driving task as discounted Markov decision process â„³ = (ğ’ª, ğ’œ, ğ‘ƒ, ğ‘Ÿ, ğ›¾), where ğ‘œğ‘¡ ğ’ª is the observation at time ğ‘¡, ğ‘ğ‘¡ ğ’œ is the action, ğ‘Ÿğ‘¡ = ğ‘Ÿ(ğ‘œğ‘¡, ğ‘ğ‘¡) is the environment reward, and ğ›¾ (0,1) is the discount factor. policy ğœ‹ğœ™(ğ‘ ğ‘œ) induces trajectory distribution through the transition dynamics ğ‘ƒ(ğ‘œğ‘¡+1 ğ‘œğ‘¡, ğ‘ğ‘¡). The goal is to learn policy that maximizes the expected discounted return ğ½(ğœ‹ğœ™) = ğ”¼ğœ‹ğœ™ [ ğ‘¡=0 ğ›¾ğ‘¡ğ‘Ÿğ‘¡]. (1) 7 In VLM-enhanced RL, VLM provides auxiliary feedback signals derived from the current observation and context. We denote the feedback at time ğ‘¡ as ğ‘”ğ‘¡ = â„±ğœ“(ğ‘œğ‘¡), which can represent an action suggestion or action prior, critique signal, or constraint-related indicator. This yields an augmented formulation where the RL agent conditions on ğ‘œğ‘¡ = (ğ‘œğ‘¡, ğ‘”ğ‘¡), and optimization can incorporate the VLM guidance either through reward shaping ğ‘Ÿğ‘¡ = ğ‘Ÿğ‘¡ + ğœ†ğ‘Ÿvlm(ğ‘”ğ‘¡) or through policy regularization toward VLM action prior ğœ‹vlm (ğ‘ ğ‘œğ‘¡, ğ‘”ğ‘¡) . generic objective that captures these uses is max ğœ™ ğ”¼ [ ğ‘¡=0 ğ›¾ğ‘¡ (ğ‘Ÿğ‘¡ + ğœ†ğ‘Ÿvlm(ğ‘”ğ‘¡))] ğ›½ğ”¼ [KL (ğœ‹ğœ™( ğ‘œğ‘¡)ğœ‹vlm( ğ‘œğ‘¡))], (2) where ğœ† weights the VLM-based reward shaping term and ğ›½ controls the KL regularization toward the VLM action prior. 3.2. Off-policy actor-critic learning We consider discounted Markov decision process â„³ = (ğ’ª, ğ’œ, ğ‘ƒ, ğ‘Ÿ, ğ›¾) and parameterized stochastic policy ğœ‹ğœ™(ğ‘ ğ‘œ) . Off-policy actor-critic methods learn an action-value function ğ‘„ğœƒ(ğ‘œ, ğ‘) and optimize ğœ‹ğœ™ using transitions stored in replay buffer ğ·, where ğ· contains tuples (ğ‘œğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, ğ‘œğ‘¡+1, ğ‘‘ğ‘¡) collected by behavior policy that can differ from ğœ‹ğœ™1 and ğ‘‘ğ‘¡ {0,1} indicates episode termination. The critic is learned via temporal-difference bootstrapping with slowly updated target network ğ‘„ğœƒ , using the one-step Bellman target: ğ‘¦ğ‘¡ = ğ‘Ÿğ‘¡ + (1 ğ‘‘ğ‘¡)ğ›¾ğ”¼ğ‘ğœ‹ğœ™(ğ‘œğ‘¡+1)[ğ‘„ğœƒ (ğ‘œğ‘¡+1, ğ‘)], and minimizing the squared Bellman error: ğ¿ğœƒ = ğ”¼(ğ‘œğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡,ğ‘œğ‘¡+1,ğ‘‘ğ‘¡)ğ·(ğ‘„ğœƒ(ğ‘œğ‘¡, ğ‘ğ‘¡) ğ‘¦ğ‘¡)2. (3) (4) The actor is updated to maximize the critic value of its actions for states sampled from the replay buffer, which can be written as minimizing ğ¿ğœ™ = ğ”¼ğ‘œğ‘¡ğ· [ğ”¼ğ‘ğœ‹ğœ™(ğ‘œğ‘¡)[ğ‘„ğœƒ(ğ‘œğ‘¡, ğ‘)]]. (5) 8 In practice, off-policy actor-critic algorithms commonly incorporate additional stabilizers such as clipped double-Q learning, target-policy smoothing, and entropy regularization, while retaining the same core structure of learning ğ‘„ğœƒ from replay and improving ğœ‹ğœ™ using the critic."
        },
        {
            "title": "Method",
            "content": "Fig. 2. Overall framework. 9 Fig. 2 summarizes the overall framework of Found-RL as unified platform with three coupled parts (simulation, algorithms, and applications), designed to make foundation model-enhanced RL practical for autonomous driving. In the simulation part, Found-RL builds on CARLA to provide standardized benchmarks and diverse traffic scenarios, producing multi-modal driving observations (e.g., bird's-eye view (BEV) or BEV masks) together with lightweight structured context from the simulator (such as speed and traffic-rule states) and task-defined rewards/termination signals. In the algorithms part, the platform integrates classical RL learners with foundation models via an asynchronous batch inference pipeline: rollout workers turn observations and context into prompts, send them to shared request queue, and an inference server micro-batches requests across parallel environments to run VLM/CLIP inference and return feedback without blocking the simulator loop; the resulting feedback (e.g., expert-like action suggestions or semantic scores) is stored in the replay buffer alongside standard transitions and consumed by modular learning objectives. In the applications part, Found-RL instantiates these modules into endto-end workflows for learning from foundation-model feedback, including VLM action guidance and CLIP-based reward shaping, and supports systematic benchmarking/ablation under consistent evaluation protocols with comprehensive route, safety, and efficiency metrics. 4.1. Asynchronous batch inference framework VLM inference is substantially more expensive than single simulator step or an actor-critic forward pass, so issuing separate VLM query for every environment step quickly becomes the dominant training bottleneck. To make VLM feedback practical at scale, we implement an asynchronous batch inference framework that decouples environment rollout from VLM inference through client-server design, and batches requests across parallel environments and across time, as illustrated in Fig. 2. At each step ğ‘¡ in environment ğ‘’, the rollout worker constructs query prompt ğ‘ğ‘¡ ğ‘’ from the current observation and lightweight environment metadata, and sends it to shared request queue ğ‘’ = ğ’«(ğ‘œğ‘¡ ğ‘ğ‘¡ ğ‘’, ğœ‚ğ‘¡ ğ‘’), ğ‘ğ‘¡ ğ‘’ = (ğ‘’, ğ‘¡, ğ‘ğ‘¡ ğ‘’), ğ‘ğ‘¡ ğ‘’ ğ’¬ğ‘Ÿğ‘’ğ‘, (6) 10 where ğœ‚ğ‘¡ ğ‘’ can include speed, route command, traffic-light state, and other structured signals already available from the simulator. An inference server continuously pops requests from ğ’¬req and forms micro-batches using size cap ğµmax and short timeout ğœ, which prevents stragglers from stalling the pipeline. â„¬ğ‘˜ can be defined as follows: â„¬ğ‘˜ = Batch(ğ’¬req ; ğµmax, ğœ) = {ğ‘ğ‘˜,1, , ğ‘ğ‘˜,ğµğ‘˜}. (7) Given batch â„¬ğ‘˜, the server runs the selected VLM â„±ğœ“ in parallel over the batch and returns the per-request outputs through shared output queue ğ’¬out . In our setting, the VLM output is represented as an VLM feedback vlm,ğ‘’ (e.g., VLM expert action ğ‘ğ‘¡ ğ‘“ğ‘¡ vlm,ğ‘’ ) with an availability indicator ğ‘šğ‘¡ ğ‘’ {0,1}, which supports missing or delayed responses without breaking training. (ğ‘“ğ‘¡ vlm,ğ‘’, ğ‘šğ‘¡ ğ‘’) can be defined as follows: (ğ‘“ğ‘¡ vlm,ğ‘’, ğ‘šğ‘¡ ğ‘’) = â„±ğœ“(ğ‘ğ‘¡ ğ‘’), (ğ‘’, ğ‘¡, ğ‘ğ‘¡ vlm,ğ‘’, ğ‘šğ‘¡ ğ‘’) ğ’¬out. (8) On the rollout side, outputs are asynchronously matched back to environments through the key (ğ‘’, ğ‘¡) . The resulting transition stored in the replay buffer is augmented with the VLM fields: (ğ‘œğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, ğ‘œğ‘¡+1, ğ‘‘ğ‘¡, ğ‘“ğ‘¡ vlm, ğ‘šğ‘¡) ğ·, (9) This way, the downstream learning components can consume VLM guidance through masked objectives whenever ğ‘šğ‘¡ = 1. This design makes VLM inference an overlapped service rather than blocking per-step call, and batching amortizes the fixed overhead of tokenization, vision encoding, and decoding across several requests. In practice, this framework allows us to scale VLM feedback to many parallel environments while keeping the actor-critic training loop fully utilized and robust to variable VLM latency. 4.2. VLM action guidance We introduce VLM action guidance as form of constrained exploration, where VLM provides step-wise action proposals to restrict the policys search space and improve early-stage learning efficiency. In our implementation, we use fine-tuned InternVL3-1B (Zhu et al., 2025) (see Section 5.2 Vision-Language Models for details) as the default guidance model for its quality-latency trade-off, while the guidance interface is model11 agnostic and can be replaced by other instruction-following VLMs with the same inputoutput format. Found-RL currently supports Value-Margin Regularization (VMR) and Advantage-weighting Action Guidance (AWAG). (1) Value-Margin Regularization We augment the standard off-policy actor-critic critic update with value-margin regularizer by sampling extended transitions (ğ‘œğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, ğ‘œğ‘¡+1, ğ‘‘ğ‘¡, ğ‘ğ‘¡ vlm, ğ‘šğ‘¡) ğ·, where ğ‘ğ‘¡ vlm is the VLM-proposed feedback ğ‘“ğ‘¡ vlm stored for the same state ğ‘œğ‘¡ and ğ‘šğ‘¡ {0,1} indicates whether this VLM action is available. For the Temporal Difference (TD) target, we sample the next action from the current actor with scheduled exploration noise, and compute the clipped double-Q bootstrap: ğ‘¦ğ‘¡ = ğ‘Ÿğ‘¡ + (1 ğ‘‘ğ‘¡)ğ›¾ min ğ‘–{1,2} ğ‘„ğœƒ ğ‘– (ğ‘œğ‘¡+1, ğ‘ ğ‘¡+1), ğ‘ ğ‘¡+1 ğœ‹ğœ™( ğ‘œğ‘¡+1). The critic is trained with the usual TD regression: ğ¿TD = ğ‘–{1,2} 1 2 ğ”¼ğ· [(ğ‘„ğœƒğ‘– (ğ‘œğ‘¡, ğ‘ğ‘¡) ğ‘¦ğ‘¡) 2 ], (10) (11) and when ğ‘šğ‘¡ = 1 we add VMR term that encourages the critic to assign higher value to the VLM action than to the replay action by fixed margin Î” > 0, ğ¿VMR = ğ‘–{1,2} ğ”¼ğ· [ğ‘šğ‘¡ (ğ‘„ğœƒğ‘–(ğ‘œğ‘¡, ğ‘ğ‘¡ vlm) (stopgrad (ğ‘„ğœƒğ‘– (ğ‘œğ‘¡, ğ‘ğ‘¡)) + Î”)) ] The final critic objective is: ğ¿critic = ğ¿TD + ğœ†(step)ğ¿VMR (12) (13) where stopgrad() detaches the replay-action value so the auxiliary term primarily pushes up ğ‘„(ğ‘œğ‘¡, ğ‘ğ‘¡ vlm). We apply cosine-decayed coefficient for ğœ†(step), with ğ‘(step) = min(step/ğ‘‡, 1), ğ‘(step) = ğ‘(step)ğœ…, and ğœ†(step) = ğœ†end + (ğœ†start ğœ†end) 1 + cos(ğœ‹ğ‘(step)) 2 (14) 12 The decay is necessary in practice because ğ¿VMR can continuously push Q-values upward and increasingly causes bias of the policy toward the VLM action. This reduces effective action exploration and could impair longhorizon learning if the regularizer remains strong throughout the training phase. (2) Advantage-weighting Action Guidance Similar to VMR, we sample extended transitions: (ğ‘œğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, ğ‘œğ‘¡+1, ğ‘‘ğ‘¡, ğ‘ğ‘¡ vlm, ğ‘šğ‘¡) ğ·, (15) where ğ‘ğ‘¡ vlm denotes the VLM-proposed expert action aligned with ğ‘œğ‘¡ , and ğ‘šğ‘¡ {0,1} indicates whether this transition contains valid VLM action. At update step, we sample ğ‘Ë† ğ‘¡ ğœ‹ğœ™( ğ‘œğ‘¡) and evaluate the clipped double critic: ğ‘„(ğ‘œğ‘¡, ğ‘Ë† ğ‘¡) = min ğ‘–{1,2} ğ‘„ğœƒğ‘– (ğ‘œğ‘¡, ğ‘Ë† ğ‘¡). (16) When AWAG is enabled, we use an adaptive scale factor, ğœ†, computed from the first critic head and detached from gradients, ğœ† = ğ›¼ stopgrad (ğ”¼(ğ‘œğ‘¡,ğ‘ğ‘¡)ğ’Ÿ [ğ‘„ğœƒ1 (ğ‘œğ‘¡, ğ‘ğ‘¡) 2 ]) + ğœ– , leading to the base actor loss expressed as follows: â„’base (ğœ™) = ğ”¼ğ·[ğœ† ğ‘„(ğ‘œğ‘¡, ğ‘Ë† ğ‘¡)]. (17) (18) To incorporate VLM action guidance, we only use samples with ğ‘šğ‘¡ = 1. Let ğ‘ğ‘¡ ğœ‹ be the actor forward output (deterministic action used for advantage estimation). We compute ğ‘ğœ‹ as: ğ‘ğœ‹ = min ğ‘–{1,2} ğ‘„ğœƒğ‘– (ğ‘œğ‘¡, ğ‘ğ‘¡ ğœ‹), ğ‘vlm = min ğ‘–{1,2} ğ‘„ğœƒğ‘–(ğ‘œğ‘¡, ğ‘ğ‘¡ vlm), and define the advantage, At, as follows: ğ´ğ‘¡ = ğ‘vlm ğ‘ğœ‹. 13 (19) (20) We apply Q-filter gate ğ‘”ğ‘¡ = ğŸ{ğ´ğ‘¡ > 0} so that the policy imitates VLM actions only when they are estimated to outperform the current policy action under the critic. The per-sample weight, ğ‘¤ğ‘¡, is computed as: ğ‘¤ğ‘¡ = clip (exp ( ğ´ğ‘¡ ğ›½ ) , ğ‘¤max), (21) with ğ›½ = 2 and ğ‘¤max = 20 . In accordance with the implementation, ğ´ğ‘¡ and ğ‘¤ğ‘¡ are treated as constants via stopgrad(). The guidance loss maximizes the likelihood of ğ‘ğ‘¡ vlm under the current stochastic policy: â„’AWAC(ğœ™) = ğ”¼ğ·[ğ‘šğ‘¡ ğ‘”ğ‘¡ stopgrad(ğ‘¤ğ‘¡) log ğœ‹ğœ™(ğ‘ğ‘¡ vlm ğ‘œğ‘¡)]. Finally, the actor is updated by combining the base RL term and the VLM guidance term, â„’actor (ğœ™) = â„’base (ğœ™) + ğœ„(step)â„’AWAC (ğœ™), where ğœ„(step) follows the cosine-decayed schedule used in our implementation: ğ‘(step) = min ( step ğ‘‡ , 1) , ğ‘( step ) = ğ‘( step )ğ›¾ ğœ„(step) = ğœ„end + (ğœ„start ğœ„end) 1 + cos(ğœ‹ğ‘( step )) (22) (23) (24) with faster decay in the early stage. 4.3. VLM-based reward shaping Leveraging VLM-based reward shaping has emerged as promising paradigm to provide dense, semantically meaningful supervision (Huang et al., 2024b). Within this domain, CLIP-based approaches are particularly favored for their high inference throughput. However, directly applying CLIP models to continuous control tasks remains challenging due to their inherent limitations in perceiving dynamic states in the form of continuous numerical values (e.g., distinguishing throttle of 0.99 from 1.0). To address these limitations of CLIPs (Radford et al., 2021), we introduce Conditional Contrastive Action-Alignment Reward. We discretize the vehicle's speed into 4 categories and navigation goals into 6 commands, defining the context at timestep ğ‘¡ as cğ‘¡ = (cmdğ‘¡, spdğ‘¡). To explicitly compensate for the vision encoder's inability to perceive ego-states, we combine every possible context 14 tuple with compact semantic action space ğ’œğ¶ğ¿ğ¼ğ‘ƒ of 6 longitudinal actions (e.g., \"idling\", \"braking hard\") and 5 lateral actions (e.g., \"turning left sharply\"), which result in comprehensive library of 6 4 6 5 = 720 context-aware prompts. At each timestep, rather than performing classification over the entire global space, which would lead to severe probability dilution, we dynamically retrieve only the specific subset of 6 5 = 30 action anchors aligned with the current cğ‘¡ . This conditional slicing mechanism ensures that the text embeddings effectively encode the necessary state information invisible to the image encoder, while simultaneously maintaining concentrated and discriminative probability distribution over the relevant action candidates. See Fig. 2 (CLIPbased reward shaping) for visual demonstration and more details can be found in Appendix A. The probability of specific action ğ‘ ğ’œğ¶ğ¿ğ¼ğ‘ƒ given the visual observation ğ¼ğ‘¡ is computed as: ğ‘ƒ(ğ‘ ğ¼ğ‘¡, ğ‘ğ‘¡) = exp (ğœ cos (ğ¸ğ¼(ğ¼ğ‘¡), ğ¸ğ‘‡(ğ‘‡cğ‘¡,ğ‘))) ğ‘ğ’œğ¶ğ¿ğ¼ğ‘ƒ exp (ğœ cos (ğ¸ğ¼(ğ¼ğ‘¡), ğ¸ğ‘‡(ğ‘‡cğ‘¡,ğ‘))) (25) where ğ‘‡cğ‘¡,ğ‘ is the context-conditioned prompt. However, absolute probability values from CLIP can be noisy and highly dependent on image complexity (Wang et al., 2024). Therefore, we focus on the relative confidence of the decision. We define semantic neighbor set ğ’©(ğ‘ğ‘¡) to exclude logically similar actions (e.g., \"braking\" vs. \"braking hard\") and calculate the reward margin. This margin quantifies the distinctiveness of the chosen action against the hardest valid negative specifically, the maximum probability among the remaining non-neighboring, competitive actions: ğ‘Ÿraw = max (0, ğ‘ƒ(ğ‘ğ‘¡ ) max ğ‘ƒ(ğ‘neg )) ğ‘negğ’©(ğ‘ğ‘¡) (26) Finally, we employ Running Mean-Std (RMS) filter to normalize this margin. Since the VLM is pre-trained on vast expert datasets, high alignment score serves as proxy for \"expertness\" or \"familiarity.\" By standardizing the raw margin against its running mean and standard deviation, the resulting signal acts as fuzzy reward bonus that measures how much better the current action aligns with current state compared to the agent's average 15 performance. The final shaped reward is computed by scaling this normalized bonus by ğœ† and adding it to the environmental reward ğ‘Ÿğ‘’ğ‘›ğ‘£: ğ‘Ÿvlm = clip ( ğ‘Ÿraw ğœ‡rms ğœrms , 1,1) ğ‘Ÿfinal = ğ‘Ÿğ‘’ğ‘›ğ‘£ + ğœ† ğ‘Ÿvlm (27)"
        },
        {
            "title": "Results",
            "content": "5.1. Experiment Setting"
        },
        {
            "title": "Observation space and action space",
            "content": "For the observation space, the VLM-based driving agents (Fig. 3 (a)) take BEV image (1921923) as input, along with text prompt. In contrast, the RL-based driving agents (Fig. 3 (b)) use BEV masks (969615) together with compact state vector (e.g., vehicle motion, last-step control signals, and traffic-rule context such as traffic lights and stop signs). The action space is continuous. The VLM outputs 3D action (throttle, steer, brake), whereas the RL-based agents output 2D action (throttle/brake, steer), where throttle and brake are combined into single signed longitudinal value (positive for throttle and negative for braking). In the VLM-enhanced RL method, the 3D VLM action is mapped into the 2D RL action accordingly. Fig. 3. Observation space."
        },
        {
            "title": "Reward functions and terminal criteria",
            "content": "In this work, we adopt the ROACH-style reward shaping scheme (Zhang et al., 2021), where the total reward is defined as the sum of speed-tracking term toward desired speed (bounded by fixed maximum speed), route16 keeping penalty based on lateral deviation from the reference centerline, heading-alignment penalty based on angular error relative to the route direction, smoothness penalty that discourages abrupt steering changes between consecutive steps, and terminal reward/penalty. Following the same obstacle-aware rule (Toromanoff et al., 2019), the desired speed equals the maximum speed when no obstacle is detected and is linearly reduced to zero as the distance to detected obstacle decreases. For terminal criteria, an episode is terminated if the ego vehicle becomes blocked, violates the route constraint by exceeding an adaptive lateral-deviation threshold from the reference path, runs red light, runs stop sign, or triggers collision event. In evaluation mode, termination further occurs when the route is completed. Compared to ROACHs evaluation setting, the protocol for evaluation applies stricter and more training-consistent termination logic. In ROACH, evaluation termination is triggered by small set of coarse events, which makes episodes less likely to end early. In contrast, our evaluation mode mirrors the training mode much more closely, including tighter route adherence checks and additional failure triggers, so episodes terminate more readily; consequently, while route completion can still be reasonable, the overall success rate tends to remain lower."
        },
        {
            "title": "Benchmarks",
            "content": "For the Leaderboard benchmark, training is conducted on Town01, Town03, Town04, and Town06, and evaluation is performed on Town01, Town02, Town03, Town04, Town05, and Town06 (CARLA Team, n.d.). For the NoCrash benchmark, training is conducted on Town01, and evaluation is performed on Town01 and Town02 (Codevilla et al., 2019a). The map layouts are shown in Fig. 4. Fig. 4. Map layouts. We additionally visualize representative and diverse scenarios from both benchmarks in Fig. 5, where white denotes the ego vehicle, dark blue indicates background vehicles, cyan indicates pedestrians, red/green denote traffic lights, dark gray denotes trajectories, light gray denotes the drivable area, and purple denotes road markings. Fig. 5. Driving scenarios."
        },
        {
            "title": "Simulation Optimization",
            "content": "To accommodate the partial observability of BEV inputs and enhance training stability, we implemented two key environmental adjustments. First, we observed recurring artifact in BEV/BEV-mask settings: many static objects (e.g., poles, fences, and vegetation) are not represented in the observation, yet minor side contacts with these objects frequently trigger collision terminations. From the agents perspective, such episodes often end with an end state that is nearly indistinguishable from the immediately preceding states, which introduces noisy credit assignment and substantially hampers early-stage learning. To remove this unobservable failure mode, we disable the physics of BEV-invisible static objects across all experiments. Importantly, this modification does not remove their functional roles when they are part of traffic logictraffic lights and stop signs remain fully operational (e.g., phase control and rule enforcement) so that dynamic realism and compliance constraints are preserved. In addition, our reward design encourages centered lane keeping, making agents prefer driving near the lane centerline and further reducing the likelihood of boundary scraping behaviors. Second, to mitigate simulator instability caused by frequent re-initialization, we adopted hybrid reset strategy. During early exploration (e.g., episode return < 100), we apply soft resets (teleporting the ego-vehicle) with certain probability (e.g.,10 % or 25%) to reduce resource overhead, transitioning to hard resets later to ensure background actor vitality."
        },
        {
            "title": "Data collection",
            "content": "18 We collected demonstration data on three benchmarks, including the CARLA Leaderboard benchmark (CARLA Team, n.d.), the NoCrash benchmark (Codevilla et al., 2019b), and the CARLA Challenge benchmark (CARLA Team, n.d.) to fine-tune VLMs. For the Leaderboard benchmark, we collected data from 160 episodes using the Roach PPO expert policy (Zhang et al., 2021), resulting in about 457k state-action transitions. For the NoCrash benchmark, we collected data from 80 episodes using the autopilot roaming expert policy, resulting in about 235k state-action transitions. For the CARLA Challenge benchmark, we collected data from 240 episodes using the autopilot roaming expert policy, resulting in about 682k state-action transitions. Across all benchmarks, this yields about 1.374 million transitions in total. For all benchmarks, the maximum duration of each episode was set to 300 seconds. The data logging further applies terminal step filtering rule, where short segment of steps immediately before collision is not saved, so the dataset contains only the valid portion of each episode prior to the collision event. We use these data and open-source frameworks (Ilharco et al., 2021; Zheng et al., 2024) to fine-tune our VLMs."
        },
        {
            "title": "Metrics",
            "content": "To comprehensively evaluate the performance of the driving agents, we use set of metrics organized into four primary categories: Comprehensive, Route, Energy, and Safety. The Comprehensive metrics include Return, which represents the accumulated rewards over the entire episode; Driving Score, defined as the product of route completion and the infraction penalty; and Infraction Penalty, discount factor aggregating all triggered infractions. Under Route performance, we report the Success Rate (the fraction of episodes successfully completed), Route Completion (the percentage of the route distance finished), and the average driving Speed. For Energy efficiency, we utilize Icell to estimate the battery cell current based on single-cell equivalent circuit model, and Fuel Rate to measure instantaneous fuel consumption derived from longitudinal road-load model. Finally, Safety is assessed by tracking the total number of Collisions of Pedestrian, Collisions of Vehicle, and Red Light violations over the completed route length. 5.2. Baselines Vision-Language Models. We fine-tune three state-of-the-art VLM architectures on the full dataset collected from the three benchmarks (totaling 1.374M state-action transitions) to serve as both expert-action guides and standalone baselines. InternVL3 (Zhu et al., 2025) employs native multimodal pre-training strategy with Variable Visual Position Encoding; we utilize the 1B and 2B variants. Qwen2.5-VL (Bai et al., 2025) supports dynamic resolution and absolute time encoding for precise temporal localization; we fine-tune the 3B and 7B models. Visual RWKV (Hou et al., 2025) is built on linear recurrent backbone (Peng et al., 2023) that enables constant memory usage and efficient token-wise inference; we use the v7 0.1B variant. Online RLs. We train standard online RL baselines using stable-baselines3 (Raffin et al., 2021). DrQ-v2 (Yarats et al., 2021) is model-free algorithm optimized for pixel-based control via data-regularized Q-learning and image augmentation. SAC (Haarnoja et al., 2018) maximizes an entropy-augmented objective to encourage robust exploration, though high initial entropy can hinder efficiency in safety-critical tasks. TD3 (Fujimoto et al., 2018) mitigates overestimation bias and instability through clipped double Q-learning and delayed policy updates. Other methods. We implement additional baselines including AD-domain methods, Imitation Learning (IL) and offline RLs. IL and offline RLs are trained using d3rlpy (Seno and Imai, 2022). 5.3. Compare with RLs & ablation analysis To evaluate the effectiveness of the proposed Found-RL platform and the accompanying VLM supervision mechanisms, we conducted comprehensive experiments using the CARLA simulator. We benchmarked our approach against standard RL algorithms, including TD3, SAC, and DrQv2, to establish performance baseline. All models were trained over three independent runs with different random seeds to ensure statistical reliability. The performance metrics are categorized into four distinct groups: Comprehensive, Route, Safety, and Energy, providing holistic assessment of the driving agents. Table 1 presents the comparative results on the Leaderboard benchmark, demonstrating that integrating VLM feedback significantly enhances performance across all metric categories. In terms of Comprehensive and Route capabilities, the proposed methods consistently outperform baselines; notably, DrQv2-CLIP achieves state-of-the20 art results with Driving Score of 0.77 and Success Rate of 57%, substantially surpassing the vanilla DrQv2 baseline. Crucially, this aggressive progress does not compromise Safety, as VLM-enhanced agents exhibit reduced collision ratesfor example, SAC-VMR lowers vehicle collisions from 0.50 to 0.20 per km compared to SAC. Furthermore, the methods maintain high Energy efficiency, with SAC-VMR achieving the lowest battery consumption (Icell =0.07), proving that VLM-derived supervision effectively guides agents toward policies that are not only robust and safe but also smooth and energy-efficient. Table 1. Compare with RLs on Leaderboard benchmark Comprehensive Route Energy Safety Return Driving Score Infra. Penalty Success Rate Route Compl. Speed Icell Fuel Rate Red Light Collisions Collisions Ped. Veh. 1703.83843.48 0.530. 0.890.03 0.370.30 0.580.26 3.480.17 0.180.01 0.0040.000 0.000.00 0.210.07 0.030.02 527.03102.66 0.210. 0.900.04 0.030.02 0.230.06 3.730.09 0.150.02 0.0030.000 0.010.02 0.500.35 0.050.03 Algorithm TD SAC SAC-AWAG 556.33152.82 0.280.05 0.820.01 0.080. 0.330.06 5.210.10 0.080.01 0.0020.000 0.010.01 0.630.09 0.150.09 SAC-VMR 1623.34265.96 0.530.10 0.890. 0.310.14 0.580.11 3.630.27 0.070.01 0.0020.000 0.000.00 0.200.05 0.020.02 DrQv2 1507.66517.97 0.560. 0.880.01 0.380.13 0.610.12 3.720.20 0.090.03 0.0020.001 0.010.01 0.190.01 0.020.03 DrQv2-AWAG 1626.35497.37 0.610. 0.900.02 0.460.15 0.660.10 3.680.27 0.060.01 0.0010.000 0.010.01 0.130.03 0.020.02 DrQv2-VMR 2237.31173. 0.720.04 0.890.03 0.600.04 0.780.05 3.560.09 0.090.05 0.0020.001 0.000.00 0.160.04 0.020.01 DrQv2-CLIP 2188.54204.03 0.770.05 0.900.02 0.570.06 0.770.05 3.600.10 0.130.03 0.0020.002 0.000.00 0.140.05 0.010. Table 2 and Table 3 detail performance on the NoCrash benchmark across Town01 (the training town of NoCrash benchmark) and Town02 (an unseen town of NoCrash benchmark), collectively demonstrating that VLMenhanced RL not only stabilizes weaker baselines in familiar environments but also fosters significant generalization capabilities in novel scenarios. In terms of comprehensive driving metrics, the impact is transformative for the SAC family; SAC-VMR elevates the Driving Score in Town01 from negligible 0.24 to competitive 0.71, and substantially recovers performance in the unseen Town02, boosting the Success Rate from failing 7% to 47%. Meanwhile, the strong DrQv2-CLIP baseline maintains peak performance in the training environment and achieves the highest overall Driving Score of 0.76 in Town02, confirming its resilience to increased traffic complexity. Crucially, VLM feedback proves vital for collision avoidance and efficiency across both domains. In Town01, SAC-VMR significantly reduces the high vehicle collision rate of the vanilla agent from 21 1.81 to 0.59 per km, while in the more challenging Town02, DrQv2-CLIP nearly halves the baselines rate (reducing it from 1.27 to 0.73 per km), confirming that the contrastive alignment reward successfully encodes transferable obstacle-awareness. Finally, the energy analysis highlights consistent efficiency gains from action guidance, with SAC-AWAG achieving remarkable energy conservation (ğ¼ğ‘ğ‘’ğ‘™ğ‘™ 0.04) compared to baselines (> 0.11), demonstrating that VLM-guided policies promote smoother, less aggressive actuation alongside their safety benefits. Table 2. Compare with RLs on NoCrash benchmark (Town01) Comprehensive Route Energy Safety Return Driving Score Infra. Penalty Success Rate Route Compl. Speed Icell Fuel Rate Red Light Collisions Collisions Ped. Veh. 817.82113.51 0.450.04 0.800.04 0.200. 0.520.08 2.780.22 0.130.02 0.0030.000 0.010.02 0.680.15 0.070.01 640.28312.40 0.240.11 0.870.01 0.040. 0.270.12 2.500.51 0.110.06 0.0030.001 0.000.00 1.810.44 0.070.06 Algorithm TD3 SAC SAC-AWAG 995.53186.42 0.500.04 0.800.04 0.300.07 0.600.04 3.180.14 0.040.00 0.0010.000 0.000.00 1.100.26 0.130. SAC-VMR 1639.0433.76 0.710.05 0.850.03 0.560.06 0.810. 2.700.08 0.050.01 0.0010.000 0.000.00 0.590.07 0.090.03 DrQv2 1658.44126.95 0.730.06 0.860.02 0.660. 0.840.08 2.750.14 0.100.06 0.0020.001 0.000.00 0.560.06 0.050.07 DrQv2-AWAG 1393.96345.09 0.740.10 0.910.02 0.540. 0.790.11 2.570.34 0.060.04 0.0010.001 0.010.01 0.340.10 0.040.02 DrQv2-VMR 1448.10144.54 0.700.03 0.880. 0.560.09 0.800.05 2.920.13 0.110.06 0.0020.001 0.000.00 0.480.14 0.040.01 DrQv2-CLIP 1535.69324.61 0.750. 0.900.03 0.650.05 0.810.04 2.510.09 0.090.04 0.0020.000 0.000.00 0.320.17 0.070.07 Table 3. Compare with RLs on NoCrash benchmark (Town02) Comprehensive Route Energy Safety Return Driving Score Infra. Penalty Success Rate Route Compl. Speed Icell Fuel Rate Red Light Collisions Collisions Ped. Veh. 463.76116.39 0.390.10 0.780.06 0.200.11 0.470. 2.080.27 0.110.01 0.0020.000 0.000.00 1.940.11 0.130.07 576.45381.10 0.240.16 0.910.02 0.070.12 0.250. 1.760.48 0.080.04 0.0020.001 0.000.00 2.440.98 0.440.32 Algorithm TD3 SAC SAC-AWAG 474.22178. 0.460.04 0.790.03 0.290.07 0.560.04 2.830.32 0.050.01 0.0010.000 0.000.00 2.380.72 0.630.35 SAC-VMR 819.41105.87 0.610.03 0.840.04 0.470.04 0.700.05 2.240.10 0.060.01 0.0010.000 0.000.00 1.320.21 0.270. DrQv2 1023.42121.77 0.730.03 0.830.03 0.770.06 0.860. 2.070.12 0.080.03 0.0020.001 0.010.02 1.270.33 0.040.04 DrQv2-AWAG 830.53256.79 0.690.11 0.870.07 0.630. 0.790.15 2.020.38 0.060.03 0.0010.001 0.000.00 0.900.64 0.100.13 DrQv2-VMR 1060.89115.57 0.640.02 0.790. 0.690.11 0.810.07 2.210.11 0.080.03 0.0020.001 0.030.03 1.510.37 0.070.03 DrQv2-CLIP 953.98116.80 0.760. 0.870.06 0.710.11 0.850.04 1.850.28 0.080.02 0.0020.000 0.000.00 0.730.51 0.070.04 We further analyze the training dynamics and sample efficiency in Fig. 6. As the agents are trained in an endless environment, we report the Average Reward and Route Completed on per-step basis rather than per episode. The 22 curves illustrate that incorporating VLM feedback significantly accelerates the learning process, particularly in the early training stages. For instance, in the SAC-based experiments (columns and d), the VLM-guided variants (AWAG and VMR) exhibit steep initial ascent in both reward acquisition and route progression, rapidly distancing themselves from the slower-learning baseline. While the strong DrQv2 baseline remains competitive, the VLM-enhanced methods, especially DrQv2-CLIP, demonstrate superior asymptotic performance and reduced variance. Ultimately, the plateauing trends across all subplots confirm that the proposed algorithms successfully converge to stable, high-performing policies, validating that the integration of semantic supervision expedites the discovery of optimal control strategies without destabilizing the learning process. Fig. 6. Comparison of average reward and route completion in meters per step between Found-RL and classical RLs on Leaderboard and NoCrash benchmarks. Fig. 7 illustrates the evolution of actor and critic losses throughout the training process, offering insight into the internal optimization dynamics of the proposed methods. distinct characteristic is observed in the AWAG variants (orange curves), where the Actor loss remains exceptionally flat and stable compared to the baselines; this stability is intrinsic to the AWAG formulation, where the VLM guidance term is adaptively scaled by the inverse of the Q-value magnitude ( 1 ğ‘„ ) to normalize gradient magnitudes and prevent the auxiliary supervision from overwhelming the primary RL objective. In contrast, the VMR method (blue curves) exhibits characteristic \"dipand-recover\" dynamic driven by its margin-based objective. Mathematically, the auxiliary VMR loss forces the 23 critic to assign higher value to VLM-proposed actions than to the policy's actions by fixed margin Î” , systematically inflating Qvalues in the early stages. Since the actor's objective is to maximize these values (minimizing ğ‘„), this inflation causes the actor loss to drop sharply, creating the deep valleys observed in the curves. As the regularization coefficient ğœ† undergoes cosine decay and the margin constraint relaxes, the Q-values gradually realign with true environmental returns, allowing the loss trajectory to recover and stabilize. Ultimately, both actor and critic losses for all VLM-enhanced algorithms converge to steady state, confirming that the asynchronous integration of semantic feedback provides robust learning signal without inducing divergence or long-term instability. Fig. 7. Comparison of actor and critic losses between Found-RL and classical RLs on Leaderboard and NoCrash benchmarks. Fig. 8 depicts the evolution of the auxiliary VMR and AWAG losses, effectively illustrating the dynamic trade-off between VLM-guided imitation and autonomous self-exploration. For most configurations, such as DrQv2-AWAG and DrQv2-VMR, we employ decay schedule for the guidance coefficient (marked by the vertical dashed line). Initially, the losses are low as the agent closely mimics the VLM's priors. However, as the coefficient decays and the \"Self-exploration\" phase begins, the auxiliary loss naturally rises; this trend is desirable, indicating that the agent is no longer strictly bound by the teacher's behavior and is successfully exploring the environment to maximize the primary RL objective. notable exception is observed in the SAC-AWAG variant (Fig. 8 (c)), where 24 the loss remains stable and near-zero throughout training. In this specific setting, we maintained constant guidance coefficient without decay, as empirical observations revealed that relaxing the VLM constraint in the early stages led to performance instability and policy collapse. This constant regularization ensures that the SAC agent remains firmly anchored to the semantic safety constraints provided by the VLM throughout the entire learning process. Fig. 8. Comparison of AWAG and VMR losses between Found-RL and classical RLs on Leaderboard and NoCrash benchmarks. 5.4. Compare with VLMs Table 4 benchmarks our foundation model-enhanced RL agents against VLMs on the Leaderboard benchmark, demonstrating that our agents achieve highly competitive performance despite vast differences in scale. In terms of comprehensive metrics, DrQv2-CLIP attains Driving Score of 0.77, which not only significantly outperforms the 1B-parameter InternVL3 (0.63) and RWKV (0.65) baselines but also edges out the 7B-parameter Qwen2.5-vl (0.76). Furthermore, the RL agents exhibit superior fine-grained control in safety metrics, maintaining exceptionally low red light violation rates (0.010.02 per km) compared to the VLM baselines (0.040.08). Notably, these results are achieved using lightweight model of only 3.82 parameters and reduced 9696 visual inputs (only 25% of the spatial size used by VLMs), effectively rivaling the performance of multi-billion parameter models that process significantly larger 192192 inputs. Table 4. Compare with VLMs on Leaderboard benchmark Algorithm Model Size Input Modality Driving Infra. Succ. Route Speed Fuel Coll. Coll. Red Return Icell Score Penalty Rate Compl. Rate Ped. Veh. Light Comprehensive Route Energy Safety Visual 0.1B BEV (1921923) 2044. 0.65 0.86 0.52 0.70 3.63 0. 0.001 0.01 0.15 0.08 25 RWKV 0B Qwen2.5-vl7b Qwen2.5-vl3b Internvl3-2b Internvl3-1b 7B BEV (1921923) 2702.49 0.76 0.91 0. 0.80 3.50 0.04 0.001 0.00 0. 0.05 3B 2B 1B BEV (1921923) 2763. 0.74 0.90 0.59 0.79 3.38 0. 0.001 0.00 0.13 0.05 BEV (1921923) 3185. 0.85 0.92 0.75 0.89 3.40 0. 0.001 0.00 0.08 0.04 BEV (1921923) 2157. 0.63 0.87 0.49 0.68 3.83 0. 0.001 0.00 0.19 0.07 DrQv2-VMR 3.82 2237.31 0.72 0.89 0.62 0.78 3. 0.09 0.002 0.00 0.16 0.02 BEV Masks (96 15) + State (10, ) BEV Masks (9696 DrQv2-CLIP 3.82 2188.54 0. 0.90 0.57 0.77 3.60 0.13 0. 0.00 0.14 0.01 15) + State (10, ) Table 5 and Table 6 extend the comparative analysis to the NoCrash benchmarks, revealing nuanced trade-off between model capacity and efficiency. While the large-scale VLMs (1B7B parameters) generally achieve higher upper-bound Driving Scores, peaking at 0.92 in Town01 and 0.94 in Town02, our lightweight RL agents remain highly competitive against comparable baselines. Specifically, DrQv2-CLIP achieves Success Rates of 0.65 (Town01) and 0.71 (Town02), consistently outperforming the Visual RWKV baseline (0.61 and 0.63, respectively). Objectively, the RL agents exhibit higher vehicle collision rates than the multi-billion parameter models, likely limitation of the reduced 9696 visual resolution in handling complex dynamic agents; however, they maintain superior adherence to static traffic rules, recording significantly lower red light violation rates (e.g., 0.07 per km in Town02) compared to even powerful VLMs like Qwen2.5-vl-7b (0.29). Table 5. Compare with VLMs on NoCrash benchmark (Town01) Algorithm Model Input Modality Comprehensive Route Energy Safety Size Driving Infra. Succ. Route Speed Fuel Coll. Coll. Red Return Icell Score Penalty Rate Compl. Rate Ped. Veh. Light Visual 0.1B BEV (1921923) RWKV 0B1 Qwen2.5-vl7B BEV (1921923) 7b Qwen2.5-vl3B BEV (1921923) 3b 1588.76 0.82 0.91 0.61 0.87 2. 0.03 0.001 0.00 0.32 0.11 1731. 0.92 0.97 0.86 0.94 2.73 0. 0.001 0.02 0.04 0.08 1709.61 0. 0.96 0.75 0.87 2.56 0.03 0. 0.00 0.08 0.13 Internvl3-2b 2B BEV (1921923) 1851.79 0.85 0.94 0.74 0.88 2. 0.03 0.001 0.00 0.15 0.15 Internvl3-1b 1B BEV (1921923) 1922.43 0.89 0.96 0. 0.91 2.58 0.03 0.001 0.00 0. 0.08 DrQv2-VMR 3.82 BEV Masks (96 96 1448.10 0. 0.88 0.56 0.80 2.92 0.11 0. 0.00 0.48 0.04 26 15) + State (10, ) DrQv2-CLIP 3.82 BEV Masks (96 96 15) + State (10, ) 1535.69 0.75 0. 0.65 0.81 2.51 0.09 0.002 0. 0.32 0.07 Table 6. Compare with VLMs on NoCrash benchmark (Town02) Algorithm Model Input Modality Comprehensive Route Energy Safety Size Driving Infra. Succ. Route Speed Fuel Coll. Coll. Red Return Icell Score Penalty Rate Compl. Rate Ped. Veh. Light Visual 0.1B BEV (1921923) RWKV 0B1 Qwen2.5-vl7B BEV (1921923) 7b Qwen2.5-vl3B BEV (1921923) 3b 768.25 0.86 0.94 0.63 0. 1.54 0.02 0.001 0.00 0.30 0. 1132.63 0.89 0.96 0.83 0.91 2. 0.03 0.001 0.00 0.07 0.29 947. 0.94 0.97 0.91 0.95 2.18 0. 0.001 0.08 0.12 0.00 Internvl3-2b 2B BEV (1921923) 1166.86 0.91 0.96 0.85 0. 2.02 0.03 0.001 0.00 0.17 0. Internvl3-1b 1B BEV (1921923) 1005.63 0.85 0. 0.60 0.89 1.97 0.03 0.001 0. 0.52 0.16 DrQv2-VMR 3.82 BEV Masks (96 96 15) + State (10, ) DrQv2-CLIP 3.82 BEV Masks (96 96 15) + State (10, ) 1060.89 0. 0.79 0.69 0.81 2.21 0.08 0. 0.03 1.51 0.07 953.98 0.76 0. 0.71 0.85 1.85 0.08 0.002 0. 0.73 0.07 In conclusion, the comparative evaluation across both Leaderboard and NoCrash benchmarks demonstrates that our foundation model-enhanced RL agents achieve driving proficiency comparable to large-scale VLMs while operating under drastically tighter resource constraints. Despite the massive disparity in scale, our DrQv2-CLIP agent delivers competitive Success Rates and superior rule compliance (e.g., minimal red light violations) using lightweight model of only 3.82 parameters. This stands in stark contrast to the 1B7B parameter VLM baselines, representing model size reduction of approximately 260x to 1800x. Furthermore, our approach achieves these results using compressed 9696 visual inputsa 75% reduction in spatial resolution compared to the 192192 inputs required by VLMs. These results validate that our framework effectively distills the semantic reasoning of foundation models into compact, efficient policies, offering viable solution for deploying high-performance autonomous driving agents on resource-constrained platforms. 27 5.4. Compare with other baselines To strictly evaluate performance  (Table 7)  , we compare Found-RL against two categories of baselines: AD-domain methods that utilize high-fidelity RGB and LiDAR inputs, and general-purpose Imitation Learning (IL)/offline RL algorithms operating on lightweight BEV maps. It is important to note that this comparison is conducted under asymmetric conditions; while AD-domain methods results are derived from official Leaderboard that allow continued driving after minor infractions, our evaluation enforces stricter safety-critical termination criteria to ensure trustworthy behavior. Although this rigorous protocol naturally constrains Route Completion by terminating episodes early upon unsafe deviations, Found-RL still achieves the highest Driving Score (0.77) with superior Infraction Penalty (0.90). This demonstrates that despite relying on significantly lower-dimensional BEV inputs, our RL paradigm of learning from VLM feedback fosters robust and compliant driving policies that prioritize safety over the aggressive trajectory tracking often observed in standard baselines. Table 7. Compare with other baselines on Leaderboard benchmark. denotes results cited from the official Leaderboard. Unlike these baselines, our method uses stricter termination criteria and compact inputs, achieving the highest Driving Score through superior rule compliance rather than aggressive driving. Catrgories Algorithms Input Modality Driving Score Route Compl. Infra. Penalty AD-domain methods NEAT (Chitta et al., 2021) IARL (Toromanoff et al., 2020) Rails (Chen et al., 2021) RGB RGB RGB Latent TransFuser (Chitta et al., 2022) RGB + LiDAR GRIAD (Chekroun et al., 2023) RGB + LiDAR TransFuser (Chitta et al., 2022) RGB + LiDAR LAV (Chen and KrÃ¤henbÃ¼hl, 2022) RGB + LiDAR TCP (Wu et al., 2022) RGB InterFuser (Shao et al., 2023) RGB + LiDAR IL and offline RLs BC AWAC (Nair et al., 2020) TD3-BC (Fujimoto and Gu, 2021) CQL (Kumar et al., 2020) IQL (Kostrikov et al., 2021) BEV BEV BEV BEV BEV 0.22 0.25 0.31 0. 0.37 0.61 0.62 0.75 0.76 0. 0.29 0.29 0.15 0.20 VLM-enhanced RL Found-RL (DrQv2-CLIP, ours) BEV Masks + State 0.77 5.5. Analysis on CLIP Scoring 28 0.42 0. 0.58 0.66 0.62 0.87 0.94 0. 0.88 0.40 0.37 0.37 0.18 0. 0.77 0.65 0.52 0.56 0.72 0. 0.71 0.64 0.87 0.84 0.89 0. 0.74 0.87 0.75 0.90 To quantitatively assess the discriminative capability of the fine-tuned CLIP model within the proposed semantic action space, we performed retrieval-based classification experiment on held-out evaluation subset comprising 20,000 samples. For each visual observation, we constructed 30 candidate text prompts representing the full spectrum of lateral and longitudinal behaviors, all conditioned on the corresponding ground-truth navigation command and speed state to isolate the action recognition performance. The predicted action was determined by identifying the specific text anchor among the candidates that maximized the cosine similarity to the image embedding, thereby testing the model's ability to disambiguate fine-grained driving maneuvers solely from visual cues. The resulting row-normalized confusion matrix as shown in Fig. 9 exhibits pronounced diagonal structure, confirming that the model effectively disentangles distinct driving maneuvers despite the high degree of visual similarity between adjacent frames in the bird's-eye view. Crucially, the off-diagonal predictions are not uniformly distributed but are heavily concentrated within local semantic clusters. Misclassifications predominantly occur between intensity variations of the same maneuver, such as identifying \"braking hard\" as \"braking\" or \"accelerating fast\" as \"accelerating\", reflecting the inherent ambiguity in discretizing continuous dynamics rather than fundamental perception failures. Conversely, the model exhibits clear separation between conflicting directional maneuvers, with negligible confusion observed between opposing lateral actions like turning left versus turning right. This pattern indicates that the learned latent space preserves the semantic topology of driving behaviors, where prediction deviations correspond to logical near-misses. This characteristic explicitly validates our reward formulation strategy, confirming that the semantic neighbor masking is necessary to accommodate these reasonable linguistic ambiguities while the underlying representation remains robust and directionally accurate. 29 Fig. 9. Action prediction confusion matrix. We analyze the system feasibility and training dynamics in Fig. 10. The left panel demonstrates that the reward availability and consistently remains near 100%, critical metric since dense reward shaping necessitates guiding signal for every transition; this high throughput explicitly justifies our choice of the efficient CLIP encoder over computationally heavier generative VLMs that would bottleneck the simulation. The right panel depicts the evolution of the average semantic alignment margin (Eq 26) across the entire replay buffer, where the initial upward trend indicates the agent effectively learning to align its visual behavior with safety prompts, while the plateau after approximately 300k steps suggests the agent has converged on coarse-grained safety behaviors by eliminating gross semantic errors, such as braking during acceleration commands or turning opposite to the navigation goal, thereby maintaining consistently high safety margin. Fig. 10. Reward percentage and average CLIP margin over replay buffer. 5.6. Efficiency analysis FPS is evaluated in an offline setting, calculated as the average rate over 1,000 consecutive inference runs. This metric highlights the substantial performance advantage shown in Table 8 where Found-RL, as lightweight reinforcement learning-based solution, demonstrates superior real-time capabilities compared to Vision-Language Models (VLMs). With compact size of only 3.82M parameters, Found-RL achieves remarkable 500 FPS, standing in stark contrast to the 1B to 7B parameter VLMs that operate at approximately 1 FPS. This speed difference of several orders of magnitude confirms that Found-RL's small-scale architecture is uniquely suited for high-frequency, real-time inference tasks that are computationally prohibitive for VLMs. Additionally, VLM inference speed does not strictly scale with model size. Qwen2.5-vl-3b is notably slower than its 7B counterpart, likely due to its specific layer number configuration. Furthermore, Visual RWKV 0.1B exhibits the lowest FPS despite being the smallest model, discrepancy attributed to its unique architecture or the high computational overhead of its visual encoder. Table 8. Comparison between Found-RL and VLMs on FPS and model size Found-RL (ours) Internvl3-1b Internvl3-2b Qwen2.5-vl-3b Qwen2.5-vl-7b Visual RWKV-0.1b"
        },
        {
            "title": "FPS",
            "content": ""
        },
        {
            "title": "Model Size",
            "content": "3.82M 1.26 1B 1.09 2B 0. 3B 0.89 7B 0.44 0.1B"
        },
        {
            "title": "Conclusions",
            "content": "In this paper, we proposed novel framework that effectively integrates Foundation Models/VLMs into the RL loop, providing both dense reward shaping and explicit action guidance to address sample inefficiency and safety challenges in autonomous driving. We demonstrated that our method not only accelerates learning compared to standard RL baselines but also achieves performance comparable to computationally intensive VLMs, despite utilizing significantly smaller spatial inputs, less training data, and compact model size (3.82M parameters) in contrast to massive 0.1B-7B parameter models. While our asynchronous batch inference framework enables efficient background reasoning, ensuring that VLM feedback remains synchronized with rapid RL training is crucial; thus, future work will explore inference acceleration and quantization to prevent stale feedback and ensure broad sample coverage when scaling to larger models. Additionally, we aim to validate our robustness on more challenging benchmarks like CARLA Leaderboard 2.0, and envision orchestrating multi-VLM ensemble where distinct models provide specialized feedback on different driving aspects to collaboratively enhance learning efficiency, or even integrating lightweight VLMs directly as policy backbones for end-to-end planning."
        },
        {
            "title": "Appendix",
            "content": "Appendix A. Fine-tuning CLIP and discretion scheme We discretize vehicle speed and control actions according to the semantic thresholds defined in Table 9, Table 10, and Table 11, while navigation goals are represented by six standard high-level commands (e.g., turn left at the intersection, turn right at the intersection, go straight at the intersection, follow the current lane, change to the left lane, change to the right lane). Using this discretized schema and the collected data, we fine-tune an OpenCLIP (Ilharco et al., 2021) ViT-B-16 model (initialized from laion2b_s34b_b88k) for 10 epochs to align these visual observations with the synthesized state-action captions. Table 9. Speed discretion Speed speed < 0.1 0.1 speed < 2.0 2.0 speed < 4.5 4.5 speed"
        },
        {
            "title": "Text Prompt \nThe car is currently stopped \nThe car is moving slowly \nThe car is driving at a moderate speed \nThe car is driving at a high speed",
            "content": "Table 10. Throttle/brake discretion 32 Throttle/brake 0.5 < ğ‘ğ‘Ÿğ‘ğ‘˜ğ‘’ 0.05 < ğ‘ğ‘Ÿğ‘ğ‘˜ğ‘’ 0.5 0.8 < ğ‘¡â„ğ‘Ÿğ‘œğ‘¡ğ‘¡ğ‘™ğ‘’ 0.3 < ğ‘¡â„ğ‘Ÿğ‘œğ‘¡ğ‘¡ğ‘™ğ‘’ 0.8 0.05 < ğ‘¡â„ğ‘Ÿğ‘œğ‘¡ğ‘¡ğ‘™ğ‘’ 0.3 else Table 11. Steering discretion Steering 0.05 < ğ‘ ğ‘¡ğ‘’ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” < 0.05 0.3 < ğ‘ ğ‘¡ğ‘’ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” 1 0.05 ğ‘ ğ‘¡ğ‘’ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” 0.3 ğ‘ ğ‘¡ğ‘’ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” < 0.3 1 ğ‘¡â„ğ‘Ÿğ‘œğ‘¡ğ‘¡ğ‘™ğ‘’ 0."
        },
        {
            "title": "The authors declare that they have no known competing financial interests or personal relationships that could have",
            "content": "appeared to influence the work reported in this paper."
        },
        {
            "title": "Acknowledgement",
            "content": "Funding for this research was provided by the Center for Connected and Automated Transportation under Grant No. 69A3552348305 of the U.S. Department of Transportation, Office of the Assistant Secretary for Research and Technology (OST-R), University Transportation Centers Program. General support was provided by the Center for Innovation in Control, Optimization, and Networks (ICON) and the Autonomous and Connected Systems (ACS) initiatives at Purdue Universitys College of Engineering. In addition, this research was supported by grants from NVIDIA and utilized NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition GPUs and A100 GPUHours on Saturn Cloud."
        },
        {
            "title": "Citations and References",
            "content": "Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. CARLA Team, n.d. CARLA Autonomous Driving Leaderboard [WWW Document]. URL https://leaderboard.carla.org/ (accessed 11.27.25a). 33 CARLA Team, n.d. CARLA Autonomous Driving Challenge [WWW Document]. URL https://leaderboard.carla.org/challenge/ (accessed 11.27.25b). Chekroun, R., Toromanoff, M., Hornauer, S., Moutarde, F., 2023. Gri: General reinforced imitation and its application to vision-based autonomous driving. Robotics 12, 127. Chen, D., Koltun, V., KrÃ¤henbÃ¼hl, P., 2021. Learning to drive from world on rails. Presented at the Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15590 15599. Chen, D., KrÃ¤henbÃ¼hl, P., 2022. Learning from all vehicles. Presented at the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1722217231. Chitta, K., Prakash, A., Geiger, A., 2021. Neat: Neural attention fields for end-to-end autonomous driving. Presented at the Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1579315803. Chitta, K., Prakash, A., Jaeger, B., Yu, Z., Renz, K., Geiger, A., 2022. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE transactions on pattern analysis and machine intelligence 45, 1287812895. Codevilla, F., Santana, E., LÃ³pez, A.M., Gaidon, A., 2019a. Exploring the limitations of behavior cloning for autonomous driving. Presented at the Proceedings of the IEEE/CVF international conference on computer vision, pp. 93299338. Codevilla, F., Santana, E., LÃ³pez, A.M., Gaidon, A., 2019b. Exploring the limitations of behavior cloning for autonomous driving. Presented at the Proceedings of the IEEE/CVF international conference on computer vision, pp. 93299338. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V., 2017. CARLA: An open urban driving simulator. Presented at the Conference on robot learning, PMLR, pp. 116. 34 Fujimoto, S., Gu, S.S., 2021. minimalist approach to offline reinforcement learning. Advances in neural information processing systems 34, 2013220145. Fujimoto, S., Hoof, H., Meger, D., 2018. Addressing function approximation error in actor-critic methods. Presented at the International conference on machine learning, PMLR, pp. 1587 1596. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Haarnoja, T., Zhou, A., Abbeel, P., Levine, S., 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. Presented at the International conference on machine learning, Pmlr, pp. 18611870. Hou, H., Zeng, P., Ma, F., Yu, F.R., 2025. Visualrwkv: Exploring recurrent neural networks for visual language models. Presented at the Proceedings of the 31st International Conference on Computational Linguistics, pp. 1042310434. Huang, S., Dossa, R.F.J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., AraÃƒÅ¡jo, J.G., 2022. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research 23, 118. Huang, Z., Sheng, Z., Chen, S., 2025a. PE-RLHF: Reinforcement Learning with Human Feedback and physics knowledge for safe and trustworthy autonomous driving. Transportation Research Part C: Emerging Technologies 179, 105262. Huang, Z., Sheng, Z., Ma, C., Chen, S., 2024a. Human as AI mentor: Enhanced human-in-the-loop reinforcement learning for safe and efficient autonomous driving. Communications in Transportation Research 4, 100127. 35 Huang, Z., Sheng, Z., Qu, Y., You, J., Chen, S., 2024b. VLM-RL: Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving. arXiv preprint arXiv:2412.15544. Huang, Z., Sheng, Z., Wan, Z., Qu, Y., Luo, Y., Wang, B., Li, P., Chen, Y.-J., Chen, J., Long, K., Meng, J., Leng, Y., Chen, S., 2025b. Sky-Drive: distributed multiagent simulation platform for humanAI collaborative and socially aware future transportation. Journal of Intelligent and Connected Vehicles 8, 9210070. https://doi.org/10.26599/JICV.2026. Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt, L., 2021. OpenCLIP. https://doi.org/10.5281/zenodo.5143773 Jiang, K., Cai, X., Cui, Z., Li, A., Ren, Y., Yu, H., Yang, H., Fu, D., Wen, L., Cai, P., 2024. Koma: Knowledge-driven multi-agent framework for autonomous driving with large language models. IEEE Transactions on Intelligent Vehicles. Kostrikov, I., Nair, A., Levine, S., 2021. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169. Kumar, A., Zhou, A., Tucker, G., Levine, S., 2020. Conservative q-learning for offline reinforcement learning. Advances in neural information processing systems 33, 11791191. Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J., 2017. Building machines that learn and think like people. Behavioral and brain sciences 40, e253. Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J., Jordan, M., Stoica, I., 2018. RLlib: Abstractions for distributed reinforcement learning. Presented at the International conference on machine learning, PMLR, pp. 30533062. Long, K., Shi, H., Zhou, Y., Li, X., 2024. Physics Enhanced Residual Policy Learning (PERPL) for safety cruising in mixed traffic platooning under actuator and communication delay. 36 Nair, A., Gupta, A., Dalal, M., Levine, S., 2020. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella, M., 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048. Peng, Z., Li, Q., Liu, C., Zhou, B., 2022. Safe driving via expert guided policy optimization. Presented at the Conference on Robot Learning, PMLR, pp. 15541563. Peng, Z.M., Mo, W., Duan, C., Li, Q., Zhou, B., 2024. Learning from active human involvement through proxy value propagation. Advances in neural information processing systems 36. Qu, Y., Huang, Z., Sheng, Z., Chen, J., Chen, S., Labi, S., 2025a. VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving. arXiv preprint arXiv:2505.16377. Qu, Y., Xu, Z., Huang, Z., Sheng, Z., Chen, S., Chen, T., 2025b. MetaSSC: Enhancing 3D semantic scene completion for autonomous driving through meta-learning and long-sequence modeling. Communications in Transportation Research 5, 100184. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., 2021. Learning transferable visual models from natural language supervision. Presented at the International conference on machine learning, PmLR, pp. 8748 8763. Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N., 2021. Stable-baselines3: Reliable reinforcement learning implementations. Journal of machine learning research 22, 18. Seno, T., Imai, M., 2022. d3rlpy: An offline deep reinforcement learning library. Journal of Machine Learning Research 23, 120. Shao, H., Wang, L., Chen, R., Li, H., Liu, Y., 2023. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. Presented at the Conference on Robot Learning, PMLR, pp. 726737. Sheng, Z., Huang, Z., Chen, S., 2024. Traffic expertise meets residual RL: Knowledge-informed model-based residual reinforcement learning for CAV trajectory control. Communications in Transportation Research 4, 100142. Sheng, Z., Huang, Z., Qu, Y., Chen, J., Luo, Y., Chen, Y.-J., Leng, Y., Chen, S., 2025a. Safeplug: Empowering multimodal llms with pixel-level insight and temporal grounding for traffic accident understanding. arXiv preprint arXiv:2508.06763. Sheng, Z., Huang, Z., Qu, Y., Leng, Y., Bhavanam, S., Chen, S., 2025b. CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with VisionLanguage Models. Transportation Research Part C: Emerging Technologies (accepted, inpress). Sheng, Z., Huang, Z., Qu, Y., Leng, Y., Chen, S., 2025c. Talk2traffic: Interactive and editable traffic scenario generation for autonomous driving with multimodal large language model. Presented at the Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 37883797. Sutton, R.S., Barto, A.G., 1998. Reinforcement learning: An introduction. MIT press Cambridge. Toromanoff, M., Wirbel, E., Moutarde, F., 2020. End-to-end model-free reinforcement learning for urban driving using implicit affordances. Presented at the Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 71537162. Toromanoff, M., Wirbel, E., Moutarde, F., 2019. Is deep reinforcement learning really superhuman on atari? leveling the playing field. arXiv preprint arXiv:1908.04683. Wang, S., Wang, J., Wang, G., Zhang, B., Zhou, K., Wei, H., 2024. Open-vocabulary calibration for fine-tuned CLIP. arXiv preprint arXiv:2402.04655. Wu, P., Jia, X., Chen, L., Yan, J., Li, H., Qiao, Y., 2022. Trajectory-guided control prediction for end-to-end autonomous driving: simple yet strong baseline. Advances in Neural Information Processing Systems 35, 61196132. Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.-Y.K., Li, Z., Zhao, H., 2024. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters. Yarats, D., Fergus, R., Lazaric, A., Pinto, L., 2021. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645. Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L., 2021. End-to-end urban driving by imitating reinforcement learning coach. Presented at the Proceedings of the IEEE/CVF international conference on computer vision, pp. 1522215232. Zhang, Z., Tang, S., Zhang, Y., Fu, T., Wang, Y., Liu, Y., Wang, D., Shao, J., Wang, L., Lu, H., 2024. AD-H: Autonomous Driving with Hierarchical Agents. arXiv preprint arXiv:2406.03474. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., Ma, Y., 2024. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models, in: Proceedings of the 62nd Annual"
        },
        {
            "title": "Meeting  of",
            "content": "the Association for Computational Linguistics (Volume 3: System Demonstrations). Association for Computational Linguistics, Bangkok, Thailand. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479."
        }
    ],
    "affiliations": [
        "Department of Civil and Environmental Engineering, University of Wisconsin-Madison",
        "Department of Civil, Architectural, and Environmental Engineering, University of Texas at Austin",
        "Lyles School of Civil and Construction Engineering, Purdue University"
    ]
}