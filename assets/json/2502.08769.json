{
    "paper_title": "Cluster and Predict Latents Patches for Improved Masked Image Modeling",
    "authors": [
        "Timothée Darcet",
        "Federico Baldassarre",
        "Maxime Oquab",
        "Julien Mairal",
        "Piotr Bojanowski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 9 6 7 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Cluster and Predict Latents Patches for\nImproved Masked Image Modeling",
            "content": "Timothée Darcet Meta FAIR Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK"
        },
        {
            "title": "Maxime Oquab\nMeta FAIR",
            "content": "Julien Mairal Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK"
        },
        {
            "title": "Piotr Bojanowski\nMeta FAIR",
            "content": "timdarcet@meta.com baldassarre@meta.com qas@meta.com julien.mairal@inria.fr bojanowski@meta.com"
        },
        {
            "title": "Abstract",
            "content": "Masked Image Modeling (MIM) offers promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in large-scale visual representation learning have established foundation models as cornerstone of modern computer vision. Self-supervised representations have proven particularly effective in domains with limited annotations, such as satellite imagery (Tolan et al., 2024) and medical imaging (Xu et al., 2024; Vorontsov et al., 2024; Chen et al., 2024; Moutakanni et al., 2024; Dermyer et al., 2025), while enabling breakthroughs in more fundamental vision tasks like monocular depth estimation (Yang et al., 2024a; Bochkovskii et al., 2024; Yang et al., 2024b), keypoint matching (Edstedt et al., 2024), and tracking (Tumanyan et al., 2025). This shift from small supervised models to large self-supervised generalist models mirrors the evolution in natural language processing since the publication of BERT (Devlin et al., 2018) and GPT (Radford et al., 2018), where large-scale models pretrained on web-scale unlabeled data have become ubiquitous foundation models. However, the impressive scalability of language models remains unmatched in vision: the best self-supervised visual encoders contain around one billion parameters (Oquab et al., 2024), hundreds of times smaller than 1 Figure 1: CAPI Method overview: image patches embedded by teacher are grouped into clusters. Their assignments are then used as the training signal for the student. The teacher and the student are jointly learned via self-distillation. The loss is purely about predicting the content of missing patches and does not rely on augmentations or contrastive loss. Evaluation scores: we evaluate frozen representations on ADE20K segmentation with k-nn and linear probe and on ImageNet-1k classification with an attentive probe. We compare to MAE, data2vec 2.0, I-JEPA, and AIM. Compared to other masked image models, CAPI achieves higher performance with fewer FLOP, scaling well with model size, and approaches the scores of DINOv2+reg. current language models (Liu et al., 2024). reason for this gap may be found in the discrepancy between the pretraining tasks used in vision and in language. The success of language models stems from the generality of the language modeling task: modeling the distribution of data, conditioned on context. Naturally, researchers have attempted to adapt this approach to computer vision, which resulted in masked image modeling (MIM): the prediction of missing image content given surrounding context. Yet, existing MIM approaches have not matched the representation quality of alternative self-supervised methods. Pixel-level reconstruction objectives, e.g. MAE (He et al., 2022), provide good initialization for fine-tuning, but yield poor frozen representations, possibly because the target is too low-level to capture the tasks inherent uncertainty (LeCun, 2022; Assran et al., 2023). Instead of reconstructing pixels, other works propose reconstructing in pretrained encoders latent space (Zhang et al., 2022; Fang et al., 2023; 2024). However, this approach requires an existing encoder and cannot learn representations from scratch. The most promising direction uses the latent representation of the online model or an exponential moving average (EMA) of it as learning target for the model, bootstrapping an informative latent space from scratch. This approach is used in the current state-of-the-art method for self-supervised learning of representations, DINOv2 (Oquab et al., 2024). However, these methods often suffer from poor stability (Zhou et al., 2022) and sensitivity to hyperparameters (Assran et al., 2023), requiring additional objectives like contrastive learning to produce competitive representations (Zhou et al., 2022; Oquab et al., 2024; Alkin et al., 2024). In this work, we focus on online latent masked image modeling. We isolate it from other stabilizing objectives, and systematically study the design choices it involves. We center the discussion around three aspects of the masked image modeling principle: the target representation, the formulation of the loss, and the architecture used to perform predictions. We show that with the right implementation, simple masked image modeling objective can lead to features competitive with the current state of the art in SSL. In brief, our method relies on pair of teacher-student vision transformers trained with self-distillation (fig. 1 left). The training signal for the student comes from the patch representations of the teacher, which is updated through an exponential moving average (EMA). The patch embeddings of the teacher are converted to soft assignments on pseudo-categories using learned online clustering, while the student receives partially masked image as input and is trained to predict the clustering assignments of the missing patches. Using this method, we train CAPI, 300-million parameter visual encoder whose representations approach DINOv2s performance while significantly outperforming previous masked image models (fig. 1 right)."
        },
        {
            "title": "2 Related Work",
            "content": "Self-supervised representation learning. Self-supervised learning (SSL) is pre-training paradigm in which model is optimized to solve pretext task on unlabeled data, often collected at scale. As the model is not trained to match specific human annotations, the benefit of this type of training is to produce generalist model, that can be adapted to solve many different downstream tasks. Depending on the application, these tasks can be solved either by fully fine-tuning the mode, or by using the representations extracted with the frozen model. In the SSL literature, some works have focused mainly on full fine-tuning (He et al., 2022; Huang et al., 2023), while others have shown that frozen representations can reach excellent performance on wide range of tasks, avoiding costly fine-tuning (Oquab et al., 2024) and generalizing to annotation-scarce domains where fine-tuning is not possible (Tolan et al., 2024; Xu et al., 2024). Historically, early work on self-supervised learning focused on hand-crafted pretext tasks such as predicting the rotation of an image (Gidaris et al., 2018), the relative position of patches (Doersch et al., 2015) or the color of grayscale image (Zhang et al., 2016). Subsequent works pushed the field forward with methods based on clustering (Caron et al., 2018; 2020; 2021) and contrastive learning (Chen et al., 2020b; He et al., 2020). Nowadays, the best self-supervised encoder inherits from these families (Oquab et al., 2024) and complements them with masked image modeling objective (Zhou et al., 2022). However, training with both global and MIM objectives can prove difficult, as multiple components can interact negatively1. In this work, we study the masked image modeling component in isolation, suggesting improvements to properly stabilize the optimization objective in the absence of global term. Pixel reconstruction. Learning by predicting missing part of an image was first proposed in Context encoders (Pathak et al., 2016). This was envisioned as conceptually similar to denoising autoencoders (Vincent et al., 2008), in the case where the noise is masking process. More recently, the success of masked language modeling (Devlin et al., 2018) and autoregressive pretraining in natural language processing (Radford et al., 2018) brought new wave of interest for transferring these ideas to vision. iGPT (Chen et al., 2020a) was the first effort to train transformer (Vaswani et al., 2017) by generating pixels. Chen et al. (2020a) proposed rasterizing images to very low resolution, then training for autoregressive next-token prediction. Then, the advent of the ViT (Dosovitskiy et al., 2021) architecture sparked further interest in the field. Following the initial exploration of Dosovitskiy et al. (2021), BeiT (Bao et al., 2021) tried using the quantized latents of dVAE as targets for masked image pretraining, using the tokenizer from DALLE (Ramesh et al., 2021). BeiT has proven useful as an initialization for further fine-tuning, but severely underperformed baselines in representation learning. To simplify BeiT, SimMIM, and MAE (Xie et al., 2021; He et al., 2022) concurrently proposed using raw pixels as targets. Thanks to clever encoder/decoder architecture, MAE proved very stable and reached interesting representations, despite its simplicity. However, it still fell short of previous SSL methods in terms of representation quality: MAE models need to be scaled to ViT-H size to match the linear probing performance of 25 smaller DINOv1 ViT-S/16(Caron et al., 2021). Latent reconstruction. Concurrently to MAE, Zhou et al. (2022) proposed iBOT. To obtain more semantic tokenization, iBOT used the online output of the model being trained as the reconstruction target for masked image modeling. This led to good representations, and the method was reused to obtain the current state-of-the-art (Oquab et al., 2024). However, the iBOT objective was very unstable and required an additional DINO (Caron et al., 2021) loss to stabilize the training. The idea of using online representations as targets was then reused in data2vec (Baevski et al., 2022) and I-JEPA (Assran et al., 2023; Bar et al., 2024). I-JEPA in particular proposed reusing the encoder/decoder architecture of MAE and removing the projection head of iBOT, to obtain more stable objective in the latent space. The improvements in I-JEPA established new tradeoff between stability and performance, but it was still both sensitive to hyperparameters (12 points on IN-1k when changing the target scale from [0.15, 0.2] to [0.125, 0.2] (Assran et al., 2023))) and weaker than DINOv2 (81.1 on ImageNet-1k, 5.4 below DINOv2 while using model twice bigger). 1Zhou et al. (2022) showed that using shared head for the DINO and iBOT losses gave better results at small scales. However, at large scale, Oquab et al. (2024) observed that this caused the iBOT loss to explode late into the training, significantly degrading performance. Untying the two heads prevented the two losses from competing directly, stabilizing the training. 3 Figure 2: Overview of the components of reconstruction-based model. We identify three main choices involved in designing masked image model: the choice of targets  (fig. 3)  , the loss function (Section 3.1, fig. 4) and the architecture of the predictor (Section 3.2, fig. 5). Clustering in self-supervised learning. Our approach is also related to methods that use clustering for self-supervised learning. First of this line of work, DeepCluster (Caron et al., 2018) proposed using simple k-means to obtain pseudo-labels. Subsequently, SwAV (Caron et al., 2020) introduced an online clustering to replace the offline k-means. DINO (Caron et al., 2021) built on SWaV, and made the clustering be implicitly learned by the MLP projection head of the student. Finally, iBOT (Zhou et al., 2022) proposed to reuse the DINO projection head for masked image modeling, implicitly using clustering as target. Our approach reuses this idea of using clustering to create the targets of masked image modeling objective, but backtracks to the origin of this line of work: instead of using an MLP head that performs an implicit clustering, we use an explicit clustering. By separating the clustering process from the rest of the training, we isolate the two components, making the training more stable and transparent."
        },
        {
            "title": "3 Approach",
            "content": "At high level, masked image modeling involves masking part of the input, feeding the visible region to prediction model, and optimizing it to predict the content of the missing parts. Despite the simple formulation, the effectiveness of reconstruction-based methods and the properties of the trained models are dramatically influenced by number of design choices. In this section, we discuss the three aspects of masked image modeling depicted in Figure 2: the type of patch representation used as target  (fig. 3)  , the loss formulation (Section 3.1, fig. 4), and the prediction architecture (Section 3.2, fig. 5). Based on our findings, we introduce CAPI, an SSL model that enjoys stable learning and strong representation capabilities. Overview of training. In short, our main design choices are: first, we reconstruct images in latent space with teacher-student framework, following iBOT. Then, we formulate our loss using clustering component, inspired by DeepCluster, SwAV, and DINO, and draw inspiration from the regularization methods they introduce, in particular the Sinkhorn-Knopp (Sinkhorn & Knopp, 1967). Finally, we employ cross-attention predictor model, separate from the encoder, to perform reconstructions, following crossMAE (Fu et al., 2024). Our encoder and the predictor are transformers (Dosovitskiy et al., 2021) and, during pre-training, they operate in tandem as the student (fig. 1 left). The teacher is an EMA of the encoder. The typical values for one training iteration using square images of side 224 pixels are: (a) Pixel targets (iGPT, MAE, AIM) (b) Frozen teacher (BeiT,PeCo,EVA) (c) EMA teacher (iBOT, Data2Vec, I-JEPA, Ours) Figure 3: The target representations commonly used in MIM. We focus on the EMA representations. 4 (a) Direct loss (MAE, I-JEPA) (b) DINO loss (iBOT, DINOv2) (c) Clustering (proposed) Figure 4: The different loss formulations considered here. We depict in red the flow of the gradient. We pass the full image to the teacher, collect = 14 14 = 196 patch tokens, and apply an online clustering to obtain soft assignments that will be used as learning targets. The encoder receives partial view of the input image: we apply patch embedding layer to obtain patch tokens, we drop pdropn of these patches and pass to the encoder the remaining nkeep = (1 pdrop)n = 69 patches (pdrop = 65%). The encoder takes these nkeep = 69 tokens along with nreg = 16 learnable register tokens (Darcet et al., 2024), and processes them to obtain nencoded = nkeep + nreg = 85 encoded tokens. We sample npred = 7 coordinates among the dropped set, and for each we forward [MSK] token through the predictor, which predicts their assignment by cross-attending to the encoded view. detailed visual diagram of the method can be found in fig. 8."
        },
        {
            "title": "3.1 Clustering-based loss formulation",
            "content": "Latent clustering methods such as SwAV and DINO employ cross-entropy loss between the student and teacher output distributions. These distributions, produced by linear or MLP head, can be seen as soft cluster memberships, where the centroids correspond to the prototypes. Replicating the DINO objective, iBOT proposes to pass the student predictions through an MLP head and to pass the teacher embeddings through the EMA of this head. But this ignores specificity of masked image modeling: while in DINO both targets and predictions are [CLS] tokens, in iBOT the targets are patch tokens while the predictions are special [MSK] tokens. This causes distribution mismatch: the MLP head of the student is trained with [MSK] inputs but is instead applied to regular patch tokens in the teacher. This mismatch would be even stronger in an asymmetric architecture as in MAE or I-JEPA, where the targets and predictions come from two different networks (see fig. 5). Without the stabilizing effect of the DINO loss, the iBOT formulation is unable to bootstrap itself and results in trivial representations (see table 1c). Our proposition is simple: we decouple the training of the teacher projection from the students head by directly learning an online clustering of the teacher patch tokens. This way, the training remains stable even in the absence of stabilizing loss. Online clustering. Inspired by the minibatch k-means algorithm and the SwaV loss, we define our online clustering process as follows. Let Rnd be the output of the teacher, with row vectors xi for {1, . . . , n}. (a) Fused predictor (BeiT, iBOT) (b) Self-att. predictor (MAE, I-JEPA) (c) Cross-att. predictor (CrossMAE, ours) Figure 5: The different predictor architectures discussed in the paper. Here, the boxes each represent transformer. The black lines represent the residual stream for token. 5 We apply an L2 normalization and linear projection to obtain the assignment logits li Rp: li = xi xi , (1) where is matrix in Rpd, whose rows are the centroids in dimension d. We then apply softmax with temperature τ to obtain soft assignments: ai = exp(li/τ ) Pp k=1 exp(lk/τ ) . We wish to estimate by solving the following problem: min X i=1 k=1 a(k) log a(k) , (2) (3) where a(k) is the k-th coordinate in ai. By minimizing the entropy, we force the assignments to be as close to one-hot as possible, which pushes the centroids towards their assigned samples. This can be seen as form of clustering with logistic loss. However, simply solving this problem can result in empty clusters. This is common problem in mini-batch k-means (Sculley, 2010), and is usually solved by adding reassignment phase: the centroids of the empty clusters are discarded and moved next to another non-empty centroid. In our case, we do not wish the centroids to move that abruptly, as it could disturb the training of the student. Instead, we propose to use the Sinkhorn-Knopp (SK) algorithm (Sinkhorn & Knopp, 1967), used in SwAV (Caron et al., 2020) and DINOv2 (Oquab et al., 2024). Using the SK algorithm, we obtain a, an assignment where the distribution of tokens over the clusters is near-uniform: {a 1, . . . , n} SK({l1, . . . , ln}, τ ), (4) with τ another temperature parameter. Note that we do not backpropagate through the SK algorithm. We adapt the loss in Problem (3), and learn the centroids by minimizing the cross-entropy between and a: X i= k=1 a(k) log a(k) . (5) We minimize this loss with AdamW (Loshchilov, 2017) alongside the training of the main model. Positional collapse. crucial point of self-supervised learning is avoiding trivial solutions, i.e. situations where the model minimizes the loss without learning useful features. These failure modes, also known as representation collapse, are usually addressed by adding regularization mechanisms. For example, color jittering (Chen et al., 2020b) helps prevent the model from focusing uniquely on color. While training CAPI, we observed specific type of representation collapse as the positional encoding started outweighing the content of the patch embeddings. In the extreme case, the model learns to predict the position of the masked tokens instead of their content, resulting in zero loss with trivial model. In most observed cases, both content and position information are entangled in the target representation, while in the ideal scenario, the target would consist purely of semantic features. We propose simple solution to alleviate the problem. By running the SK separately at each position in Eq. (4), we force the joint distribution of tokens over positions and clusters to be near-uniform. Uniformity of the joint distribution directly implies zero mutual information between the clustering and the targets. This way, the modified SK ensures that our targets contain no positional information."
        },
        {
            "title": "3.2 Predictor architecture",
            "content": "Model architecture is another important component in reconstruction-based SSL. Two broad categories are widely used in previous work. BeiT, iBOT, and SimMIM use fused architecture: single vision transformer that takes as inputs patches and mask tokens (Fig. 5a). Fused architectures are difficult to train and yield 6 poor results, as reported in previous work (Zhou et al., 2022) and confirmed by our experiments. MAE uses split architecture, with an encoder that only forwards the patches, saving memory and compute, and predictor that forwards both patches and mask tokens (Fig. 5b). In this work, we use an even lighter architecture, which was initially explored in crossMAE (Fu et al., 2024) for pixel-based reconstruction. In this case, the predictor forwards only the mask tokens (Fig. 5c), which can access the context of the encoded patches via cross attention. Using cross-attention predictor has two main advantages. First, it allows further efficiency gains, as we only forward reduced set of tokens in the predictor, and we can even subsample this set of tokens. Second, mask tokens do not interact with each other in the cross-attention mechanism, i.e. each prediction is independent of other positions. This alleviates the need for multiple predictor forward passes with different prediction sets, as used in I-JEPA."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we report empirical evaluations of our model. We describe experimental details and present some ablation studies. Then we discuss whole-image understanding results and dense prediction results."
        },
        {
            "title": "4.1 Experimental setup",
            "content": "Pretraining dataset. Most methods from the self-supervised learning literature choose to pretrain on ImageNet-1k. This dataset is usually chosen because of its relatively small size, allowing for easy experimentation, and the ability to compare to existing methods pretrained on it. However, this has led to an overspecialization of SSL methods to the type of object-centric images found in ImageNet-1k. Recent foundation models obtain state-of-the-art results by exploiting much larger datasets, such as ImageNet22k (Zhou et al., 2022) and LVD-142M (Oquab et al., 2024). If we are to design method that can produce new foundation models, we believe it is crucial to design it to be able to handle such large datasets. To this end, we carry out all ablation experiments on ImageNet-22k. It is composed of 14M images from 22k categories taken from the WordNet ontology. Although it is close to ImageNet-1k in nature, its much larger size and diversity make it suitable to train excellent foundation models, as reported by Oquab et al. (2024). For our longer experiments, we train on multiple datasets: ImageNet-1k, for comparability with previous works, ImageNet-22k, to test scaling, Places205, to test training on more diverse and less object-centric data, and finally LVD-142M, large-scale automatically curated dataset used in previous SSL foundation models. Model architecture. We do all our experiments with Vision Transformer (Dosovitskiy et al., 2021) of 300M parameters (ViT-L). This architecture is widely used in various computer vision tasks, and most baselines provide model of comparable size. We equip the vision transformer with registers (Darcet et al., 2024). These additional tokens were recently proposed as way to add an information buffer, which enabled the model to produce smoother feature maps. For the decoder, we use 12 transformer blocks that cross-attend to the output of the encoder. This is similar to standard transformer decoder (Vaswani et al., 2017), with the difference that we do not include self-attention layers. In this decoder, every token is forwarded independently and separately attends to the encoder output. When using different encoder size, we align the embedding dimension, MLP ratio, and number of attention heads of the decoder to those of the encoder, and use decoder depth equal to half that of the encoder. Implementation Details. The learning rate follows linear warmup followed by cosine annealing. We truncate out the last 20% of the cosine, as proposed in I-JEPA (Assran et al., 2023). To simplify the choices of parameters and schedules, we set the teacher EMA momentum to µ = 1 lr, and we set the learning rate for the clustering to half of the backbone learning rate. The impact of the most important hyperparameters will be discussed in section 4.2. All our pretraining hyperparameters are summarized in table 6. Evaluation protocol. All the evaluations reported in this paper fall into two categories: image classification and semantic segmentation. For all classification tasks, we use an attentive probe (Assran et al., 2023; 7 ADE IN1k Fused Split, self-attn Split, cross-attn 23.8 27.9 29.1 73.1 77.7 81.4 random block inv. block inv. block +roll 23.6 25.6 27.2 29.1 76.4 79.9 80.7 81.4 I-JEPA 23.7 1.7 iBOT MLP 26.4 MLP CAPI 29.1 Linear CAPI 79.3 11.1 80.8 81.4 ADE IN1k head loss ADE IN1k (a) Predictor architecture (b) Masking strategy (c) Loss formulation ADE IN1k ADE IN1k depth width ADE IN1k [0.2, 1.0] [0.6, 1.0] [1.0, 1.0] 27.9 29.1 28.9 81.4 81.4 80.9 (d) Crop range ADE IN1k 0 16 25.9 29.1 79.3 81.4 55% 28.0 65% 29.1 75% 28.1 81.1 81.4 81. (e) Masking ratio 5 12 21 1536 1024 768 30.9 29.1 28.3 81.5 81.4 81.3 (f) Predictor shape ADE IN1k ADE IN1k learnable RoPE 30.0 29.1 81.6 81.4 Standard Proposed 28.5 29.1 81.3 81.4 (g) Number of registers (h) Positional encoding (i) Sinkhorn-Knopp algorithm Table 1: Ablation study of the main parameters and design choices in our algorithm. We report both image segmentation and classification. We highlight the default setting in gray, and bold the best-performing solution. An in-depth analysis of these results is provided in Sec. 4.2. El-Nouby et al., 2024; Bardes et al., 2023). We use this evaluation protocol because our model does not learn single global image representation, preventing the use of linear probe. In this evaluation, we train an attentive pooling to extract global vector and use this vector as input to linear layer. The parameters of this probe are trained in supervised fashion, and we report accuracy on the validation set. For segmentation tasks, we use lightweight classifiers on top of frozen local features. Previous works used linear head trained with gradient descent on features of images augmented with various augmentations (Oquab et al., 2024). To obtain more lightweight evaluation, we simply extract the features from all images in the dataset without augmentations, then train linear logistic regression on these features with L-BFGS (Byrd et al., 1995) using an off-the-shelf library (Raschka et al., 2020). Although this results in lower mIoU numbers, the simplicity of the evaluation allows us to grid over different hyperparameters, producing very robust results. For an even more lightweight classifier, we also consider non-parametric k-NN segmentation evaluation. For each test patch, we retrieve most similar patches in the training data and pool the segmentation label for that patch. We chose the optimal regularization parameters by doing grid search using 10% of the training set. For all segmentation tasks, we measure performance using mIoU. Baselines. We compare to the performance of previous models trained using masked image modeling: BeiT (Bao et al., 2021), MAE (He et al., 2022), data2vec 2.0 (Baevski et al., 2023), I-JEPA (Assran et al., 2023), and AIM (El-Nouby et al., 2024). To provide additional points of comparison, we report in grey the performance of iBOT (Zhou et al., 2022) and DINOv2+reg (Oquab et al., 2024; Darcet et al., 2024), who use DINO loss to stabilize MIM objective."
        },
        {
            "title": "4.2 Ablation Studies",
            "content": "We conduct extensive ablation studies to study the effect of design choices on performance. To make the ablation study more tractable, we train on the ImageNet-22k dataset for 100k iterations with patch size of 16. To provide slightly more robust results, the default setting was run twice with different seeds, and the results of the two runs were averaged. All results are presented in Table 1. Predictor architecture. We evaluate the different predictor architectures. The split predictor produces better representations while training 32% faster than the fused predictor (table 1a). Using pure cross-attention 8 in the predictor obtains better representations and allows an additional 18% speedup by avoiding forward on patch tokens. It also removes the dependency between different predictions, alleviating the need for repeated forwards of the predictor as in I-JEPA. Masking strategy. The most common masking strategies in the literature are random masking, block masking (inpainting), or inverse block masking (outpainting). Inverse block masking induces bias on the position of the masked patches: most often, the model will see the center of the image, and predict the edges. To prevent this, we propose applying random circular shift to the mask before using it (+roll). This ensures that all positions in the image are equally likely to be masked. We ablate the type of masking in table 1b. The masking ratio is 65% for all strategies except random masking, where it is set to 90%, which increases performance. Random masking is much less effective than the other strategies, and inverse block masking works best, with clear improvement when using +roll. Loss formulation. We evaluate the different strategies for computing loss function discussed in Sect. 3.1. We compare the performance of direct Huber loss, an iBOT loss with an MLP head, as well as our clusteringbased loss with linear or MLP head. With no head and direct loss, the model starts well but quickly regresses to worse representations. The iBOT head does not work in our setup, probably because of the split predictor design. Finally, the proposed clustering head alleviates all these issues and allows for stable training and good representations. Crop range. To prevent overfitting, we use random cropping and flipping augmentations. We tweak the bounds of the cropping scale to [c, 1.0] and try various c. We observe that the method can train well without any augmentation, resizing the training images to fixed 224x224 resolution (80.9 on ImageNet with = 1.0). We get better scores with minor cropping augmentation with range of [0.6, 1.0]. Masking ratio. In our model, we need to set the ratio of image patches that are masked, and reconstructed. We train our model for various and check the final performance. The optimal masking ratio seems to be around 0.65, but the algorithm seems quite robust to the choice of this parameter. Predictor shape. We study the impact of the predictor depth on performance. We train the model with predictors of various depths and adapt the width to match the total number of parameters. Shallow networks will have larger width. We see that more shallow predictor leads to better performance, but in our informal experiments, we have observed that such architectures are less stable in long schedules. For this reason, we stick to the predictor with 12 layers and width of 1024. In our model, the local feature maps serve as supervisory signal to train the model, so Registers. high-quality feature maps are crucial. To this end, we use register tokens, which were proposed to improve the quality of feature maps. We see that using registers has large effect on performance, with an improvement of 3.2 points on ADE and 2.1 points on ImageNet when using 16 registers (table 1g). Positional encoding. In our experiments, we consider using different versions of positional encoding. We try the classic learnable position embeddings as well as relative ones such as RoPE (Su et al., 2024). Because of the ease of use and transferability to higher resolutions, we settle on using RoPE. Sinkhorn-Knopp algorithm. We describe the problem of positional collapse in section 3.1. Changing the set of points considered in the Sinkhorn solves it, improving stability and granting small performance increase (table 1i). We did not observe any positional collapse when using it. Number of prototypes. We evaluate the effect of varying the number of prototypes in our clustering-based loss. The performance generally increases with the number of prototypes, at the cost of higher memory footprint. We use = 16384, which strikes good balance between memory and performance. 9 #prototypes ADE IN1k 1024 2048 4096 8192 16384 32768 14.9 19.8 27.5 28.5 29.1 29.1 73.8 77.4 80.7 81.3 81.4 81. dataset ADE20K IN1k IN1k IN22k LVD-142M 28.0 29.1 30.6 81.7 81.4 81.2 Figure 6: Additional ablation experiments. (Left) Influence of the number of prototypes. (center) Influence of the training length. Each point here is an independent training. (right) Influence of the training dataset. ImageNet v2 ReaL ObjectNet iNat Places SUN397 Model Arch. Dataset ViT-L/16 iBOT ViT-H/14 I-JEPA MAE ViT-L/16 Data2Vec 2.0 ViT-L/16 IN1k IN1k IN1k IN1k IN1k CAPI DINOv2 BeiT I-JEPA AIM CAPI CAPI CAPI ViT-L/14 ViT-g/14 ViT-L/16 ViT-H/14 ViT-600M/14 DFN-2B+ LVD-142M IN22k IN22k ViT-L/14 ViT-L/14 ViT-L/14 IN22k Places205 LVD-142M val 80.9 79.5 79.4 80.1 86.5 85.3 85.3 85. 70.3 68.7 68.9 69.4 41.9 37.0 38.3 42.1 82.9 87.6 72.9 47. 87.4 40.8 78.1 79.0 83.6 79.2 83.8 90.3 46.1 84.3 84.8 88.1 84.7 88.2 80.1 30.7 67.4 67.9 74.3 68.4 74. 67.0 8.7 38.6 41.0 55.2 39.1 55.3 28.9 22.6 19.3 24.6 43.7 81.7 2.1 25.1 20.4 52.4 33.0 56. 70.5 64.2 69.3 65.4 76.8 88.3 26.5 67.7 73.5 82.0 73.4 81.2 62.0 59.9 60.6 61.9 65. 68.8 36.8 60.2 62.5 66.3 68.6 67.1 64.6 61.9 61.8 64.4 70.9 79.3 29.9 65.5 66.1 74.5 77.5 75. Table 2: Image classification results. For each baseline method, we report the model size closest to ViT-L/14. Scaling. We study the effect of three scaling axes on the performance of our model: number of parameters, training length, and dataset size. To study the scaling potential of our algorithm, we train additional ViT-S and ViT-B models. We report the performance of the family of models in Fig. 1. We see that performance consistently improves with model size, and for all models, our algorithm outperforms the state-of-the-art. In Fig. 6 (center), we report the effect of the number of training iterations and pretraining dataset on performance. We train models using the ablation configs, on ImageNet-22k and with patch size 16. In Fig. 6 (right), we show the influence of the training data on model performance. Using larger dataset has positive effect on segmentation, while only slightly deteriorating the ImageNet-1k accuracy."
        },
        {
            "title": "4.3 Results",
            "content": "Image classification. We evaluate our model and compare it to state-of-the-art reconstruction-based SSL models. We run the evaluation on four datasets including object recognition, fine-grained classification, and scene recognition. We use ImageNet-1k (Russakovsky et al., 2015), iNaturalist 2021 (Van Horn et al., 2021), Places205 (Zhou et al., 2017), and SUN397 (Xiao et al., 2010). For ImageNet, we also report OOD robustness by running inference on additional test sets: ImageNet-V2 (Recht et al., 2019), ImageNet-ReaL (Beyer et al., 2020), ImageNet-A (Hendrycks et al., 2021), and ObjectNet (Barbu et al., 2019). For each model, we resize the image to 224224 and collect the patch tokens output by the model. We feed those to an attentive probe implemented as single layer of multi-head cross-attention with single query (head size d//64, no residual). For classes and an embedding size d, the probe contains 2d2 + (3 + c)d parameters, which are trained with AdamW for 10 epochs, selecting the best learning rate for each model/task on held-out split of the training set. More details on the protocol are available in appendix F. All the results are summarized in Table 2. 10 Model Arch. Dataset k-NN linear k-NN linear k-NN linear ADE-20k Pascal-VOC Cityscapes ViT-L/16 iBOT ViT-H/14 I-JEPA MAE ViT-L/16 Data2Vec 2.0 ViT-L/ ViT-L/14 IN1k IN1k IN1k IN1k IN1k 26.0 20.8 21.5 24.2 30.7 25.7 27.4 27.6 60.2 56.7 53.7 57. 68.8 63.6 61.5 58.1 35.7 26.4 32.8 32.8 39.8 34.5 38.5 38.2 29.2 34.4 60. 69.7 35.6 41.7 ViT-g/14 ViT-L/16 ViT-H/14 ViT-600M/14 DFN-2B+ LVD-142M IN22k IN22k ViT-L/14 ViT-L/14 ViT-L/ IN22k Places205 LVD-142M 34.0 3.5 18.9 26.1 29.7 35.2 32.1 39.0 8.3 26.3 31.4 35.2 39.1 37.2 63.0 6.9 55.0 60. 61.1 61.7 63.8 72.8 19.1 64.2 67.0 70.4 69.4 72.7 42.0 15.6 23.2 32.1 35.2 39.5 38.9 46.8 24.0 34.2 38. 41.0 44.6 44.3 CAPI DINOv2 BeiT I-JEPA AIM CAPI CAPI CAPI Table 3: Comparison with the state of the art on image segmentation using frozen features. We report both k-NN and linear segmentation performance. For reference, we also report the performance of some other non-MIM SSL models. This shows that CAPI narrows the gap using only MIM approach. We see that our model outperforms all previous state-of-the-art models, by large margin. When training on ImageNet-1k, we observe very good results, outperforming all other reconstruction-based models of comparable size. CAPI excels particularly on out-of-distribution generalization, outperforming all baselines by more than 19 points on ObjectNet. Additionally, while the gap with other methods is somewhat limited on ImageNet, the difference in scene classification (SUN397) is much larger. Interestingly, we observe that CAPI works particularly well on larger and more diverse datasets. Our three CAPI models outperform all baselines on all datasets, except the CAPI-Places205 which is slightly below AIM on ImageNet-A. It should be noted, however, that AIM was trained on more than 2 billion images, with sampling distribution tailored towards ImageNet. CAPI significantly reduces the gap between reconstruction-based methods and our topline DINOv2+reg: the gap on ImageNet goes from 8.4 points to 3.6, and on SUN397 goes from 13.2 to 1.8. Dense image understanding. As we have seen above, our model allows training high-quality local features that can be successfully pooled to solve image-level tasks. We also want to evaluate our model on dense prediction problems such as image segmentation. To this end, we run k-NN and linear segmentation following the protocol described in the experimental details. We run this for all the baselines reported above on three datasets. We use ADE-20k (Zhou et al., 2017), Pascal VOC 2012 (Everingham et al., 2010), and Cityscapes (Cordts et al., 2016). We report mIoU for all configurations in Table 3. As in the classification evals, CAPI trained on ImageNet-1k outperforms all reconstruction-based baselines by quite wide margin on all evaluation setups. When training on larger datasets, the conclusion is similar: CAPI outperforms all baselines by wide margin and even beats DINOv2+reg in some setups. When trained on Places205, CAPI achieves mIoU 1.2 points higher than DINOv2+reg. To our knowledge, this is the first time DINOv2+reg is bested on segmentation with frozen features on ADE20K. DINOv2+reg, however, remains the most versatile model, with good results across the board and the best results on Cityscapes."
        },
        {
            "title": "4.4 Additional explorations",
            "content": "As final set of experiments, we investigate some additional properties of our model. We investigate its robustness to change of input resolution and try to obtain global representations using the predictor. High-resolution image understanding. Our model was trained on 224 224 images. To compare with the I-JEPA model trained natively at high resolution, we try to evaluate our model on 448 448 images. In table 4, we see that our model does not require high-resolution training or evaluation to achieve the best performance. Our model trained at 224 and evaluated at 224 outperforms the large I-JEPA model trained at 11 Figure 7: Visualization of the features of CAPI and baseline models. We apply PCA to the features and map the three first components to RGB. The features produced by CAPI are discriminative and smooth. 448 and evaluated at 448. Moreover, using RoPE embeddings, CAPI is more robust to resolution changes: it loses only 0.5% (versus 1.0% for I-JEPA) when increasing the resolution. Qualitative feature analysis. We propose qualitative assessment of the dense features in Fig. 7. The dense features computed with CAPI are amongst the most discriminative and smooth. We see the emergence of distinct objects, without much noise in uniform regions. We observe that the CAPI features are less noisy than the ones from DINOv2+reg, while being more focused on semantics and less on colors than the MAE features. For example, the shaded building in the first image has CAPI features similar to the other buildings, while in MAE the features are closer to other dark areas of the image. Obtaining global representations. In all the previous evaluations reported in the experimental section, we trained head on top of local features. Our model does not provide an aggregate representation like the [CLS] token in DINOv2. In this experiment, we try to exploit the predictor to obtain global image representations. We forward the whole image through the encoder, and then pass the same amount of mask tokens through the first attention layer of the predictor, cross-attending to the patch tokens. We obtain global representation by average-pooling the output of this predictor attention layer. We train linear model on this representation on several classification datasets and compare it with the average pooling of the patch embeddings in table 5. We see that using the attention pooling learned by the predictor provides better representations than averaging local features from the encoder."
        },
        {
            "title": "Method",
            "content": "train res. eval@224 eval@448 I-JEPA-H 224 I-JEPA-H 448 224 CAPI-L 79.4 79.6 83.8 78.4 82.5 83. Table 4: ImageNet-1k attentive probing accuracy of I-JEPA and CAPI at different input resolutions. 12 IN1k iNat21 SUN"
        },
        {
            "title": "Representation",
            "content": "avg. pooling predictor pooling k-NN Linear 77.1 57.1 81.1 73."
        },
        {
            "title": "Linear",
            "content": "49.1 69.6 73.3 77.4 Table 5: Classification using predictor representations, compared to the average pooling of the patch tokens."
        },
        {
            "title": "5 Discussion and Concluding Remarks",
            "content": "In this paper, we have proposed novel reconstruction-based self-supervised learning algorithm. Our algorithm is based on an online clustering of dense features computed with teacher network. The latent assignments are used as targets to train the student. We propose to implement the student as an encoder followed by predictor: cross-attention decoder. The teacher is updated as an EMA of the encoder. The proposed algorithm is simple and allows the training of state-of-the-art model. Our ViT-L outperforms all available reconstruction-based models, including much larger architectures. We have shown promising scaling trends until the 300M model sizes of the ViT family, opening up potential for further scaling in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Momo for fig. 7. Julien Mairal was supported by ERC grant number 101087696 (APHELEIA project)."
        },
        {
            "title": "References",
            "content": "Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, and Johannes Brandstetter. Mim-refiner: contrastive learning boost from intermediate pre-trained representations, 2024. 2 Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, 2023. 2, 3, 7, 8, 22 Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. data2vec: general framework for self-supervised learning in speech, vision and language. In ICML, 2022. 3, 22 Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. In ICML, 2023. 8 Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021. 3, 8, 17, 22 Amir Bar, Florian Bordes, Assaf Shocher, Mido Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, and Yann LeCun. Stochastic positional embeddings improve masked image modeling. In ICML, 2024. 3 Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019. 10 Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning, 2023. 8 Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? CoRR, abs/2006.07159, 2020. URL https://arxiv.org/abs/2006.07159. 10 Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second, 2024. 1 Richard Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. limited memory algorithm for bound constrained optimization. SIAM, 1995. 8, 20 13 Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. 3, 4 Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 3, 4, Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 3, 4, 17 Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020a. 3 Richard Chen, Tong Ding, Ming Lu, Drew FK Williamson, Guillaume Jaume, Andrew Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, et al. Towards general-purpose foundation model for computational pathology. Nature Medicine, 2024. 1 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020b. 3, 6 Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 11 Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 5, 7, 8, 22 Patrick Dermyer, Angad Kalra, and Matt Schwartz. Endodino: foundation model for gi endoscopy, 2025. 1 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, 2018. 1, 3 Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, 4, 7 Johan Edstedt, Qiyu Sun, Georg Bökman, Mårten Wadenbäck, and Michael Felsberg. Roma: Robust dense feature matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19790 19800, 2024. 1 Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. In ICML, 2024. 8, 21, Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 11 Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR, 2023. 2 Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. Image and Vision Computing, 2024. Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei Efros, and Ken Goldberg. Rethinking patch dependence for masked autoencoders, 2024. 4, 7 Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 3 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2, 3, 8, 17 14 Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021. 10 Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. TPAMI, 2023. 3 J. D. Hunter. Matplotlib: 2d graphics environment. Computing in Science & Engineering, 2007. 23 Diederik Kingma. Adam: method for stochastic optimization. In ICLR, 2014. 20 Yann LeCun. path towards autonomous machine intelligence, 2022. 2 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report, 2024. Loshchilov. Decoupled weight decay regularization, 2017. 6, 20 maintainers and TorchVision contributors. Torchvision: Pytorchs computer vision library. https://github.com/ pytorch/vision, 2016. 20 Théo Moutakanni, Piotr Bojanowski, Guillaume Chassagnon, Céline Hudelot, Armand Joulin, Yann LeCun, Matthew Muckley, Maxime Oquab, Marie-Pierre Revel, and Maria Vakalopoulou. Advancing human-centric ai for robust x-ray analysis through holistic self-supervised learning, 2024. 1 Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 1, 2, 3, 6, 7, 8 Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 3 Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. 1, 3 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. Sebastian Raschka, Joshua Patterson, and Corey Nolet. Machine learning in python: Main developments and technology trends in data science, machine learning, and artificial intelligence, 2020. 8, 20 Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 10 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015. 10 David Sculley. Web-scale k-means clustering. In WWW, 2010. Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 1967. 4, 6 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 9 Jamie Tolan, Hung-I Yang, Benjamin Nosarzewski, Guillaume Couairon, Huy Vo, John Brandt, Justine Spore, Sayantan Majumdar, Daniel Haziza, Janaki Vamaraju, et al. Very high resolution canopy height maps from rgb imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar. Remote Sensing of Environment, 2024. 1, 3 Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video. In ECCV, 2025. 1 Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Benchmarking representation learning for natural world image collections. In CVPR, 2021. 10 15 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3, P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. 3 Eugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Kristen Severson, Eric Zimmermann, James Hall, Neil Tenenholtz, Nicolo Fusi, et al. foundation model for clinical-grade computational pathology and rare cancers detection. Nature medicine, 2024. 1 Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 10 Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. arXiv, 2021. 3 Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier González, Yu Gu, et al. whole-slide foundation model for digital pathology from real-world data. Nature, 2024. 1, 3 Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024a. 1 Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2, 2024b. Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In ECCV, 2016. 3 Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, et al. Cae v2: Context autoencoder with clip target, 2022. 2 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 10, 11 Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. In ICLR, 2022. 2, 3, 4, 7, 8,"
        },
        {
            "title": "A Detailed overview",
            "content": "In Figure 8, we provide detailed overview of the complete method, with tensor sizes annotated for reference CAPI ViT-L/14 model."
        },
        {
            "title": "B Loss curve",
            "content": "We report in Figure 9 the loss curve of our CAPI ViT-L model. After an initial adjustment period, the loss trends smoothly downwards, with no sign of instability or plateauing. Compared to other latent masked image modeling methods such as I-JEPA or iBOT, this trend is reassuring, and might indicate good potential for further scaling."
        },
        {
            "title": "C Blockwise masking strategy",
            "content": "The so-called block masking strategy used in many masked image modeling methods is by no means standardizes and can actually refer to several different implementations. The most common block masking implementation was proposed in BeiT (Bao et al., 2021), then reused in iBOT (Zhou et al., 2022) and MAE (He et al., 2022). It involves sampling many rectangular regions and doing multiple attempts to mask out approximately the right number of patches. Another implementation was proposed in I-JEPA, adding multiple constraints on the masks, to obtain similar multi-block mask. Additionally, some methods postprocess the proposed mask to obtain constant number of masked patches, in order to keep the same sequence length in all batch elements. In CAPI, we propose simpler heuristic: we sample single rectangular mask, and truncate out the excess patches at the lower right end. Conversely, our implementation of inverse block masking is to sample block mask, then simply invert it. Self-distillation interpretation It was observed in DINO (Caron et al., 2021) that the downstream scores of the EMA model were consistently higher than the ones of the online model during training. This led to the interpretation of DINO as self-distillation method, where the EMA model, the \"teacher\" distilled its slightly better representations into the online model, the \"student\". We observe that this interpretation still seems to hold in CAPI, albeit to lesser extent, as evidenced by the comparison of teacher and student performance in Figure 10. Modified Sinkhorn-Knopp We provide the pseudo-code for the standard Sinkhorn-Knopp and for our modified version in Figure 11. Both the original code and the proposed change are very simple. The actual code additionally contains an initial additive shift to prevent numerical instabilities in the exponential, as well as collective all_reduce for distributed training."
        },
        {
            "title": "F Detailed evaluation protocol",
            "content": "In all cases, our evaluations are performed with frozen model, and use only the patch tokens outputted by the vision transformer. The input images are always at resolution 224224. F.1 Classification The backbone model is kept frozen, and we extract only the patch tokens from its output. On top of these features, we train an attentive pooling classifier, consisting of learned query, two learned anv projections, and final projection to the number of classes. The attention is multi-head, with the head dimension being . This head is optimized with cross-entropy loss and the fixed at 64 and the number of heads being dmodel 64 17 Figure 8: Detailed overview of our method with reference tensor sizes for CAPI ViT-L/14 model. We denote in red the parts that are trained by the main loss, in purple the parts that are trained with the clustering loss, and in blue the parts that are updated by the EMA. Figure 9: The loss curve of our CAPI ViT-L during training. Figure 10: Comparative downstream scores of the teacher model and the student model throughout training. (a) Standard SK (b) Modified algorithm Figure 11: PyTorch pseudo-code for the proposed modified Sinkhorn-Knopp algorithm. We normalize by the sum of the tokens for every given position, instead of normalizing across all positions."
        },
        {
            "title": "Value",
            "content": "Batch size Optimizer Learning rate Teacher momentum Clustering lr lr schedule Warmup length cosine truncation Weight decay AdamW β Number of prototypes Student temperature Teacher temperature Num SK iter Stochastic depth Weight init Norm layer Norm ε Patch embed lr Norm layer wd Image size Augmentations Training dtype Parallelism Pred. / im Layerscale Biases Rope frequencies Masking type Masking ratio"
        },
        {
            "title": "16384\nAdamW\n1e-3\n1 − lr\n1\n2 lr\nlinear warmup + trunc. cosine\n10%\n20%\n0.1\n(0.9, 0.95)\n16384\n0.12\n0.06\n3\n0.2\nxavier_uniform\nRMSnorm\n1e-5\n0.2 · lr\n0.1 · wd\n224\nRRCrop, HFlip\nbf16\nFSDP\n7\nNo\nNo\nlogspace(7e-4, 7), axial\ninverse block+roll\n65%",
            "content": "Table 6: CAPI pretraining recipe 19 model standardization ADE Cityscapes VOC2012 ADE Cityscapes VOC2012 knn logreg False CAPI True CAPI False aim 600M aim 600M True dinov2 vitg14+reg False dinov2 vitg14+reg True False ijepa vith14 in1k True ijepa vith14 in1k ijepa vith14 in22k False ijepa vith14 in22k True False mae vitl16 True mae vitl16 32.5 33.0 14.3 nan 33.8 34.0 20.1 20.8 17.9 18.9 7.8 21.5 39.2 39.2 27.8 32.1 42.0 42.0 25.9 26.4 22.9 23.2 25.3 32.8 64.9 65.2 38.5 60.2 63.1 63.0 57.4 56.7 56.2 55.0 17.3 53.7 37.9 37.7 7.1 31.5 38.9 39.0 25.2 25.7 24.1 26.4 27.4 27.4 44.7 44.3 28.3 38.2 46.8 46.8 33.5 34.5 32.8 34.2 38.4 38. 73.2 73.3 61.3 67.0 72.9 72.8 63.0 63.6 63.6 64.2 61.5 61.5 Table 7: Comparison of segmentation results with and without standardization AdamW optimizer (Kingma, 2014; Loshchilov, 2017) for 12500 iterations at batch size 1024 (10 ImageNet epochs). The learning rate is warmed up linearly for 1250 iterations then annealed with cosine schedule. We grid the weight decay over (5e-4, 1e-3, 5e-2) and the peak learning rate over (1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2), training one classifier head for each pair of hyperparameters (30 in total). We choose the optimal hyperparameters using the accuracy on 10% held-out part of the training set, then finally report the accuracy of this classifier on the test set. The training dataset is lightly augmented using torchvision RandomResizedCrop (maintainers & contributors, 2016) with default hyperparameters and random horizontal flip. During evaluation, the images are resized to 256 then center cropped to 224224 pixels. F.2 Segmentation We compute the features for the train and test set considered at resolution 224, and hold out 10% of the train set as validation set. Using these frozen features, the segmentation problem is reduced to simple classification problem, on which we can use simple k-NN and linear classifiers. The linear classifier is trained for logistic regression with L-BFGS (Byrd et al., 1995) regularized with L2 penalty, as implemented in the cuml library (Raschka et al., 2020). In both cases, grid of hyperparameters is tested, and the ones performing best on the validation set are retained. For the k-NN classifier, we grid the number of neighbors over (1, 3, 10, 30), and the distance used over (L2, cosine). For the linear classifier, we grid the regularisation parameter C, testing 8 values along log-space between 106 and 105. F.3 Feature standardization Some of the baselines suffer from poor conditioning of their features, which can cause very bad results when fitting logistic regression over these features. To reduce this issue, in the segmentation evaluation we standardize features by substracting their mean and dividing them by their standard deviation. These statistics are computed using the features from the training set only. This significantly improves the scores of pixel reconstruction-based methods, while the other methods are mostly unaffected. We report comparison of segmentation results with and without standardization in table 7. In the rest of the paper, all segmentation results are obtained with standardization. F.4 Baselines All baselines are vision transformers, allowing us to use the same evaluation protocol. We feed the 224224 image to the model after imagenet normalization of the pixel values, and extract the patch tokens after the 20 Table 8: Summary of all models mentioned in the paper. We associate to each unique uid, and detail the hyperparameters which are not constant across all runs. uid dataset #iter patch size enc depth pred depth Meb2b LVD-142M 500k Mc2dd LVD-142M 500k LVD-142M 500k M8c4d 100k IN22k Adcab 100k IN22k Ae3f9 100k IN22k A0dd4 100k IN22k A9b4a 100k IN22k A7cc0 100k IN22k A3bb3 100k IN22k A2fcb 100k IN22k Aeb48 100k IN22k A74f9 100k IN22k Ae7b3 100k IN22k A41b8 100k IN22k Ac8bc 100k IN22k Af989 100k IN22k A2da0 100k IN22k A9ce8 100k IN22k A1177 100k IN22k A72fb 500k IN22k M5e2e 500k M2d34 IN1k 500k M8319 P205 25k IN22k Abe05 50k IN22k Acd44 200k IN22k A2ca8 IN1k Aab94 100k LVD-142M 100k Aa 14 14 14 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 14 14 14 16 16 16 16 16 24 12 12 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 12 6 6 12 12 12 12 12 12 12 12 12 12 12 12 5 21 12 12 12 12 12 12 12 12 12 12 12 enc dim 1024 768 384 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 pred dim 1024 768 384 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1536 768 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 lr mom. 1e-03 1e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 2e-03 1e-03 1e-03 1e-03 2e-03 2e-03 2e-03 2e-03 2e-03 0.999 0.999 0.998 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.999 0.999 0.999 0.996 0.996 0.996 0.996 0.996 clust. lr 5e-04 5e-04 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 5e-04 5e-04 5e-04 1e-03 1e-03 1e-03 1e-03 1e-03 masking ratio roll teacher head student head loss pos. enc. SK #reg. inv. block inv. block inv. block inv. block inv. block random block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block inv. block 65% True 65% True 65% True 65% True 65% True 90% False 65% False 65% False 65% True 65% True EMA 65% True 65% True 65% True 55% True 75% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True 65% True clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear identity CE CE CE CE CE CE CE CE identity Huber MLP clustering MLP clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear clustering Linear CE CE CE CE CE CE CE CE CE CE CE CE CE CE CE CE CE CE CE modified rope modified rope modified rope modified rope modified rope modified rope modified rope modified rope standard rope modified rope modified rope modified rope modified rope modified rope modified rope modified rope modified rope rope modified learn. modified standard rope modified rope modified rope modified rope modified rope modified rope modified rope modified rope modified rope 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 0 16 16 16 16 16 16 16 16 16 crop scale [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [100%,100%] [20%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] [60%,100%] last transformer block. For the specific case of AIM (El-Nouby et al., 2024), we follow the advice from the original paper and extract the patch tokens after before the end of the ViT, specifically after layer 18."
        },
        {
            "title": "G Compute cost and environmental footprint",
            "content": "We measure the training of CAPI ViT-L model to take 180h on 32 A100 GPUs, amounting to 5763 A100 hours. This consumed around 2651 kWh of electricity, which we estimate to amount to approximately 928 kgCO2eq. The entire project used 3.75M A100 hours, which we similarly estimate to have emitted 604 tCO2eq for the electricity consumption. Note that the carbon footprint estimations here are purely scope 2 estimations, i.e. limited to electricity consumption, and are further limited to the electricity consumption of the GPUs. full carbon accounting should additionally include many other harder to estimate emissions, such as the electricity consumption of the other server components and the rest of the datacenter appliances, and scope 3 emissions from the component manufacturing, datacenter construction, and their respective end-of-life."
        },
        {
            "title": "H List of models used",
            "content": "We provide in Table 8 the list of all models presented in this paper, along with unique hash identifier and the relevant hyperparameters. Non-listed hyperparameters are detailed in Table 6. To disambiguate any possible unclarities in the presented results, Table 9 provides the mapping from tables and figures to model identifiers."
        },
        {
            "title": "I Visualisations",
            "content": "In Figures Figure 12 and 13, we provide visualisations of the feature maps of CAPI compared to other state-of-the-art self-supervised vision models. 21 Figure 12: Visualization of the features produced by CAPI and other vision models at various resolutions: CAPI ViT-L/14, DINOv2+reg ViT-g/14 (Darcet et al., 2024), BEiT ViT-L/16 (Bao et al., 2021), AIM ViT-3B/14 (El-Nouby et al., 2024), MAE ViT-H/14 (El-Nouby et al., 2024), I-JEPA ViT-H/14 (Assran et al., 2023), and data2vec2 ViT-L/16 (Baevski et al., 2022). We apply PCA decomposition to the dense outputs produced by each model for each image individually, and rescale the three first components to the RGB range for visualization. 22 Input PCA Channel 0 Channel 1 Channel 2 Channel 3 Channel 4 Channel 5 Channel 6 Channel 7 Figure 13: Visualization of the features produced by CAPI ViT-L/14 applied to images at 560 pixel resolution. We apply PCA decomposition to the dense outputs produced by the model across all images. The first column shows the first 3 components as RGB. The next eight columns show the first eight channels individually using coolwarm colormap from Matplotlib (Hunter, 2007). 23 Table 9: Reference of models used in different figures and tables."
        },
        {
            "title": "Models used",
            "content": "Meb2b, Mc2dd, M8c4d fig. 1 table 1a Adcab, Ae3f9, Aa5a3, A7d26 table 1b Adcab, Ae3f9, A0dd4, A9b4a, A7cc0 table 1c Adcab, Ae3f9, A3bb3, A2fcb, Aeb48 table 1d Adcab, Ae3f9, A74f9, Ae7b3 table 1e Adcab, Ae3f9, A41b8, Ac8bc table 1f Adcab, Ae3f9, Af989, A2da0 table 1g Adcab, Ae3f9, A9ce8 table 1h Adcab, Ae3f9, A1177 table 1i Adcab, Ae3f9, A72fb table 2 Meb2b, M5e2e, M2d34, M8319 table 3 Meb2b, M5e2e, M2d34, M8319 table 4 Meb2b fig. 7 Meb2b table 5 Meb2b fig. 6 fig. 10 fig. 13 fig. 12 Adcab, Ae3f9, Abe05, Acd44, A2ca8, Adcab, Ae3f9, Aab94, Aa428 Meb2b Meb2b Meb2b"
        }
    ],
    "affiliations": [
        "CNRS",
        "Grenoble INP",
        "Inria",
        "LJK",
        "Meta",
        "Univ. Grenoble Alpes"
    ]
}