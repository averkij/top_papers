{
    "paper_title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
    "authors": [
        "Zhuo Li",
        "Mingshuang Luo",
        "Ruibing Hou",
        "Xin Zhao",
        "Hao Liu",
        "Hong Chang",
        "Zimo Liu",
        "Chen Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion generation plays a vital role in applications such as digital humans and humanoid robot control. However, most existing approaches disregard physics constraints, leading to the frequent production of physically implausible motions with pronounced artifacts such as floating and foot sliding. In this paper, we propose \\textbf{Morph}, a \\textbf{Mo}tion-f\\textbf{r}ee \\textbf{ph}ysics optimization framework, comprising a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on costly real-world motion data. Specifically, the Motion Generator is responsible for providing large-scale synthetic motion data, while the Motion Physics Refinement Module utilizes these synthetic data to train a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. These physically refined motions, in turn, are used to fine-tune the Motion Generator, further enhancing its capability. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion generation quality while improving physical plausibility drastically."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 1 5 9 4 1 . 1 1 4 2 : r Morph: Motion-free Physics Optimization Framework for Human Motion Generation Zhuo Li* 1, Mingshuang Luo 2,3,4, Ruibing Hou2, Xin Zhao5, Hao Liu1, Hong Chang2,4, Zimo Liu3, Chen Li 1 1WeChat, Tencent Inc, 2Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China 3Peng Cheng Laboratory, China, 4University of Chinese Academy of Sciences, China 5MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Human motion generation plays vital role in applications such as digital humans and humanoid robot control. However, most existing approaches disregard physics constraints, leading to the frequent production of physically implausible motions with pronounced artifacts such as floating and foot sliding. In this paper, we propose Morph, Motion-free physics optimization framework, comprising Motion Generator and Motion Physics Refinement module, for enhancing physical plausibility without relying on costly real-world motion data. Specifically, the Motion Generator is responsible for providing large-scale synthetic motion data, while the Motion Physics Refinement Module utilizes these synthetic data to train motion imitator within physics simulator, enforcing physical constraints to project the noisy motions into physically-plausible space. These physically refined motions, in turn, are used to finetune the Motion Generator, further enhancing its capability. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion generation quality while improving physical plausibility drastically. 1. Introduction Accurate human motion generation is pivotal in various applications, including robotics, video gaming and virtual reality [1, 10, 36, 44, 45]. Recent advances in AI have paved the way for novel approaches to motion generation, enabling various control conditions including textual descriptions and music pieces. Mainstream approaches can be divided into three categories: conditional diffusion models [16, 32, 33, 39, 42], conditional autoregressive models [13, 18, 23, 46, 47] and generative masked model- *Equal contribution Corresponding author Figure 1. Examples of physical inconsistencies in generations. ing [7, 24]. These motion generation models have made significant progress in capturing the complex multimodal distribution of human motions, greatly improving motion generation quality. However, most existing motion generation approaches overlook fundamental aspect of human motion: the laws of physical. Although these generation models excel in capturing statistical distribution of human motions, they lack explicit mechanisms to enforce physical constrains. As result, the generated motions frequently exhibit pronounced artifacts such as ground penetration, leaning backward, interpenetration, foot sliding, floating and unnatural rotation, as shown in Fig. 1. Given human sensitivity to even slight physical inconsistencies, these physically implausible motions hinder many real-world applications such as animation and virtual reality [12, 27, 39]. Recently, few studies [11, 39] have attempted to enhance the physical plausibility of motion generation. For example, PhysDiff [39] and Reindiffuse [11] incorporate physical constraints into the denosing diffusion process. However, these approaches face several limitations. First, these works [11, 39] customize physics optimization operation for specific type of generation model, i.e., textconditional diffusion model. This customization restricts their transferability to broader range of generation models (such as autoregressive-based models) and motion generation tasks (such as music-to-dance). Second, these works apply physics and diffusion iteratively, embedding physics optimization into multiple steps of diffusion process. The frequent execution of physics optimization significantly increases computational costs during inference. Last but most importantly, these methods require large-scale, highquality realistic motion data to train the physics optimization mechanism. However, collecting such realistic motion data is challenging. natural question arises: Is it possible to learn an efficient, model-agnostic physical optimizer without relying on real motion data? To achieve this, we propose Motion-free physics optimization framework, namely Morph. As shown in Fig. 2, Morph consists of two main modules: Motion Generator (MG) that can be any existing motion generator, and Motion Physic Refinement (MPR) module for enhancing physical plausibility. Morph employs two-stage optimization process. In the first stage, using large-scale synthetic noisy motion data produced by the motion generator, MPR module is optimized to project input motions into physicallyplausible space. Specifically, the MPR module designs motion imitator that controls character agent to mimic the given noisy motions within physics simulator. The simulator enforces multiple physical constraints, effectively reducing artifacts such as floating and foot sliding. To further ensure the naturalness of simulated motions, MPR module introduces motion discriminator to align the distribution of physics-refined motions with that of input motions. Feedback signals from both physics simulator and motion discriminator guide the optimization of the motion imitator via reinforcement learning. In the second stage, we observe that due to the high cost of collecting 3D motion data, existing generators typically rely on limited motion data for training, which restricts their generative capabilities. In this stage, by leveraging the trained MPR module, large-scale set of physically plausible, high-quality motion data can be created to finetune the generator, further enhancing its ability to generate realistic motion. During inference, the finetuned motion generator and MPR module work in tandem to generate physics-plausible and high-quality motions. Through above process, Morph effectively decouples physic optimization from generation model, requiring only single step of physical refinement and without relying on real motion data. This results in an economical, efficient, and model-agnostic physical optimizer. We evaluate our Morph framework on two motion generation tasks: textto-motion and music-to-dance generation. Since Morph is agnostic to specific instantiation of generation models, we test it with three types of generation models, i.e., diffusionbased, autoregressive-based and generative mask modeling. Extensive experiments demonstrate that Morph achieves significant improvements in physical error metrics, while also achieving competitive generation metrics across different generators and tasks, despite not being trained on real motion data. 2. Related Work Human Motion Generation. Motion generation is long-history task that can be conditioned on various signals, such as text description, music and action [2, 4, 7, 8, 29, 30, 33, 37, 40, 43, 48]. Our work specifically focuses on text-to-motion [7, 8, 24, 32, 40, 41] and music-todance generation [16, 20, 29, 33]. Mainstream approaches can be roughly divided into three categories: diffusionbased methods [3, 16, 32, 33, 39, 42], autoregressive models [13, 17, 18, 29, 40, 46, 47] and generative masked modeling [7, 24]. For instance, MDM [32] uses transformerbased diffusion model with conditional text representations extracted from CLIP [26]. T2M-GPT [40] designs conditional autoregressive transformer model based on VQVAE [34] and GPT [25, 35]. MoMask [7] employs residual vector quantization and generative masked transformers to iteratively generate motions. For music-to-dance generation, Bailando [29] predicts discrete token sequences conditioned on music and uses an autoregressive transformer to regenerate the dance sequence. However, existing motion generation models often produce physically implausible motions, as they overlook physical laws during training. In contrast, our proposed MPR module effectively enforces physical constraints, significantly enhancing the physically plausibility of generated motions. Physically Plausible Motion Generation. Physical plausibility refers to the degree to which generated motions adhere to physical rules, such as foot sliding, foot-ground contact, and body leaning. Generating physically-plausible motions is crucial for many real-world applications, such as animation and virtual reality. Recently, few works have attempted to address this challenge [11, 33, 39]. For example, [11, 39] integrate physical constraints into the diffusion process, iteratively applying physics and diffusion to maintain alignment with motion data distribution while enhancing physical realism. EDGE [33] introduces auxiliary losses to align specific aspects of physical realism, specifically for generating more physically plausible dances. However, these methods tailor physic optimization to diffusion models and require real motion data, limiting their flexibility and scalability. In contrast, we propose model-agnostic, motion-free physics optimization framework that decouples physics optimization from the generation model and learns to enhance physical plausibility without real motion data, offering more economical and versatile solution. Figure 2. An overview of the Morph framework. Morph comprises Motion Generator and Motion Physics Refinement module. Morph employs two-stage training process: Motion Physics Refinement module training and Motion Generator fine-tuning. And Imitation Selection Operation is employed to ensure the motion quality after physics refinement. 3. Method generate motion sequence x1:L, as follows: To enhance physical plausibility of motion generation without relying on real motion data, we propose motionfree physics optimization framework, namely Morph. As illustrated in Fig. 2, Morph consists of two key modules: Motion Generator (MG) that could be any existing pretrained motion generation model, and Motion Physics Refinement module (MPR) independent of Motion Generator. Morph employs two-stage training process. In the first stage, the Motion Generator produces large-scale noisy motion data (Sec. 3.1), which is subsequently used to train the Physics Refinement module to map input motions into physically plausible space (Sec. 3.2). In the second stage, these physics-refined motions, in turn, are used to fine-tune Motion Generator, further enhancing its capabilities (Sec 3.3). Through this two-stage training process, Morph drastically reduces physical errors while improving the overall quality of generated motions. 3.1. Noisy Motion Data Generation Different from [11, 39] that rely on real motion data, our goal is to develop motion-free physical optimization framework. To achieve this, we utilize an existing pretrained motion generation model to produce large-scale, noisy motion data for training the physics refinement module. As shown in Fig. 2, given control signals (e.g., text or music), the pre-trained motion generator is employed to x1:L = fξ (c) , where, x1:L = (cid:8)xl = (cid:2)θl, pl(cid:3)(cid:9)L l=1 , (1) where fξ represents the motion generator with parameters ξ, xl denotes the lth pose of the synthetic motion sequence, represented by the joint rotations θ and positions p. Notably, our framework, Morph, is agnostic to the specific instantiation of fξ, allowing it to be generally compatible with various pretrained motion generators. 3.2. Physics-Based Motion Refinement Since most existing motion generation models lack explicit physical constraints, they often produce motions with noticeable artifacts, as shown in Fig. 1. To address this, the Motion Physics Refinement (MPR) module is tasked with projecting the generated motion x1:L, which disregards the laws of physics, into physically-plausible motion ˆx1:L. As shown in Fig. 2, the MPR module consists of three components: motion imitator, physics simulator and motion discriminator. Specifically, the motion imitator controls simulated character to mimic the input motion x1:L within the physics simulator. The resulting motion ˆx1:L from the physics simulator is considered physically plausible, as it adheres to the laws of physics. Additionally, the motion discriminator seeks to align the distribution of refined motions ˆx1:L with that of input motions x1:L, further ensuring the naturalness and realism of the physics-refined motions. Motion Imitator Learning. Motion Imitator Learning can be formulated as Markov decision process, represented by the tuple (S, A, , R) of states, actions, transition dynamics and reward function. Formally, the motion imitator is described by policy π (cid:0)alsl(cid:1), which specifies the probability distribution of selecting an action al given the current state sl S. The physic simulator, in turn, defines the transition dynamics (cid:0)sl+1sl, al(cid:1), which determines the next state sl+1 based on current state sl and action al. Specifically, starting from an initial state s1, character agent acts in the physic simulator according to policy π (cid:0)alsl(cid:1), iteratively sampling action al. Then, the physic simulator, governed by the transition dynamics (cid:0)sl+1sl, al(cid:1), generates the next state sl+1, from which the simulated pose ˆxl+1 is derived. By running the policy for steps, we can obtain the simulated motion sequence ˆx1:L. In implementation, we use reinforcement learning to train the motion imitator π, where reward is assigned based on how well the simulated motion ˆx1:L aligns with input motion x1:L. In the following, we elaborate on the design of states, rewards, policy and actions. States. The simulation state sl consists of the input motions next pose, along with the differences between the input next pose and the current simulated pose across multiple aspects, including joint rotation, position, velocity and angular velocity. This difference information informs the policy of the pose residuals that require compensation, enabling the system to better align the simulated motion with the input motion. Formally, the state is defined as sl = (cid:104) θl+1, pl+1, θl+1 ˆθl, pl+1 ˆpl, vl+1 ˆvl, ωl ˆωl(cid:105) Here, θ, p, and ω represent the joint rotation, position, joint velocity, and angular velocity, respectively, (cid:101)[]/ (cid:98)[] denote the quantities of the input/simulated motions, and []l denotes the quantities at the lth timestep. . Actions. We use the target joint angles of proportional derivative (PD) controllers as the action representation, where the action al specifies the PD target to enable robust motion imitation. Policy. Following [11, 19, 39], we employ parameter- (cid:0)sl(cid:1) , Σ(cid:1), where ized Gaussian policy π (cid:0)alsl(cid:1) = (cid:0)µϕ (cid:0)sl(cid:1) is the output by our motion imitathe mean action µϕ tor, simple multi-layer perceptron network with parameter ϕ, and Σ is fixed diagonal covariance matrix. Rewards. The reward function is designed to encourage the simulated motion to match the input motion. At each timestep l, the reward rl consists of mimic reward rl m, energy penalty rl a, formulating as rl = rl + rl measures the difference between the simulated pose and input pose across multiple aspects, including joint rotation θ, position p, joint velocity and angular velocity ω, as follows: and an adversarial reward rl a. The mimic reward rl + rl rl = wθ exp (cid:104) (cid:105) (cid:12) (cid:12) (cid:12) θl ˆθl(cid:12) (cid:12) (cid:12) (cid:105) + wp exp (cid:104) αθ (cid:12) (cid:12) (cid:104) αv (cid:12)vl ˆvl(cid:12) (cid:12) (cid:12) + wω exp αω (cid:104) (cid:12) (cid:12) αp (cid:12) (cid:12) (cid:12) pl ˆpl(cid:12) (cid:12)ωl ˆωl(cid:12) (cid:12) (cid:12) (cid:105) (cid:12) (cid:12) , (cid:105) (2) + wv exp where wθ, wp, wv, wω, are weighting factors, αθ, αp, αv, αω are scaling factors, (cid:101)[]/ (cid:98)[] denote the quantities of the input/simulated pose, and is the L1 norm."
        },
        {
            "title": "The energy penalty rl",
            "content": "e regulates the policy by discouraging high-frequency foot jitter, common issue in policy trained without external forces [5]. This energy penalty is computed as: = 0.0005 (cid:13) rl (cid:13)ˆνl ˆωl(cid:13) 2 2 , (cid:13) (3) where ˆνl and ˆωl denotes the joint torque and angular velocity of the simulated pose in the lth timestep, and 2 is the L2 norm."
        },
        {
            "title": "The adversarial reward rl",
            "content": "a encourages the distribution of simulated motions to align that of input motions through motion discriminator. Given the state of current simulated = [ ˆθl, ˆpl, ˆvl, ˆωl], the adversarial repose, denoted as ˆsl ward is calculated as: = log (cid:0)1 Dψ rl (cid:0)ˆsl (cid:1)(cid:1) . (4) where Dψ represents the motion discriminator with parameters ψ. Loss. Given the reward rl at each timestep l, we train the motion imitator using reinforcement learning (RL). The objective is to maximize the excepted reward, effectively enabling the imitator to closely mimic the input motions. To achieve this, we adopt standard RL algorithm Proximal Policy Optimization (PPO) [28] to solve for the optimal imitator policy. The objective function is defined as: (cid:104) min LPPO (ϕ) = η(ϕ)rl, clip (η(ϕ), 1 γ, 1 + γ) rl(cid:17)(cid:105) (cid:16) . (5) πϕ(alsl) πϕold (alsl) , where ϕ and Here, η(ϕ) is the probability ratio ϕold refer to the parameters of the motion imitator after and before the policy update, clip is an operation that restricts η(ϕ) within range [1 γ, 1 + γ] with hyperparameter γ. Motion Discriminator Learning. We observe that relying solely on physics simulator often produces motions that feel overly mechanical and unnatural. To improve the naturalness of simulated motions, following [22], we introduce motion discriminator to align the distribution of simulated motions with that of the input motions. Specifically, the motion discriminator Dψ learns to distinguish between the state sampled from input pose (sl a) and that of simulated pose (ˆsl a), with the objective function as: (cid:16) (cid:16) (cid:17)(cid:17)(cid:105) log LDis (ψ) = , (6) where ψ denotes the parameters of the motion distribution. 1 Dψ Dψ log (cid:17)(cid:17)(cid:105) sl ˆsl (cid:16) (cid:16) (cid:104) (cid:104) Initialization and Early Termination. During training, we employ reference state initialization [21], where starting point is randomly selected from motion clip for imitation. We observe that the character frequently falls in the early stages of training. To improve training efficiency and accelerate convergence, we adopt an early stopping strategy. Specifically, we terminate the episode when the MPJPE (Mean Per Joint Position Error) between the state of input pose and that of simulated pose exceeds 0.5 meter. Hard Negative Mining. As training progresses, the motion imitator gradually learns to imitate simple motion sequences. However, more challenging examples in the large-scale motion dataset may be overlooked, limiting the models ability to handle difficult samples. To address this, we implement Hard Negative Mining process that identifies motions where the physical simulator fails to imitate as hard samples. Specifically, dynamic weight is assigned to each motion sequence in the input data, doubling whenever imitation fails. This process progressively increases the focus on challenging samples, guiding the imitator to effectively learn from difficult examples. 3.3. Motion Generator Fine-tuning Existing motion generators are often trained with limited real motion data, which may limit their capabilities. However, collecting large-scale real 3D motion data is costly. To overcome this challenge, we leverage large volume of synthetic, physically-plausible motion data provided by MPR module, to further enhance the capabilities of the motion generator. As shown in Fig. 2, in the second stage, we further finetune the motion generator using the physics-refined motions produced by the MPR module. Notably, since the physical simulator cannot replicate non-grounded motions (e.g., sitting on chair or swimming), such simulated motions may deviate from the true data distribution. To this end, we apply an Imitation Selection Operation to filter out simulated data of non-grounded motions. Specifically, we calculate the average per-joint position error (MPJPE) between the samples before and after physical optimization. threshold τ is set to determine whether to accept the physically refined motion ˆx1:L (with MPJPE < τ ) or input motions x1:L (with MPJPE > τ ). The selected data is then paired with the original condition signals (e.g., text or music). Ultimately, large-scale, physically plausible motion data is constructed for fine-tuning the motion generator. Following [7,32], we use the mean squared error to optimize the motion generator. Denote the selected motion sequence paired with the condition as (cid:0)x1:L, c(cid:1), the objective function for motion generator fξ is defined as: LMG (ξ) = (cid:104)(cid:13) (cid:13)x1:L fξ (c)(cid:13) 2 (cid:13) 2 (cid:105) (7) where ξ is the parameters of the motion generator. Inference. As shown in Fig 2, after the two-stage optimization, the finetuned motion generator (Stage 2) and the trained MPR module (Stage 1) are combined to perform inference. To mitigate simulation error in non-grounded motions, the imitation selection operation described above is used as post-process step. 4. Experiment Extensive experiments evaluate the performance of our Morph across multiple motion generation tasks and datasets. Specifically, we assess Morph on two motion generation tasks, text-to-motion and music-to-dance. As Morph is agnostic to specific instantiation of generation models, we combine Morph with three types of generation models for text-to-motion, i.e., diffusion-based models, including MotionDiffuse [40] and MDM [31], autoregressivebased models, including T2M-GPT [40] and generative make modeling, including MoMask [7]. For music-todance, we combine Morph with the diffusion-based model EDGE [33] and autoregressive model Bailando [29]. The Appendix provides additional visualizations and experimental results showcasing Morphs performance on varying amounts of noisy motion data, different threshold τ in imitation selection opeartion, and multi-round optimization of the MPR module and motion generator. 4.1. Datasets and Evaluation Metrics HumanML3D. HumanML3D [9] is large-scale 3D human motion-language dataset, consisting of 14,616 motion clips and 44,970 motion descriptions annotations. The total motion duration is 28.59 hours, with each motion clip downsampled to 20 FPS and accompanied by 3-4 textual descriptions. The dataset is split into training, validation and test sets in an 80%, 5% and 15% ratios. AIST++. AIST++ [15] is human dance-music dataset, which includes 992 pieces of high-quality dance sequence across ten dance genres, each dance paired with corresponding music. The dance durations range from 7.4 to 48 seconds, with frame rate of 60 FPS. The dataset is split into training and evaluation sets, with allocations of 952 and 40, respectively. Evaluation Metrics. We evaluate motion generation from two perspectives: common generation metrics and physical plausibility metrics. We first present the common generation metrics. For text-to-motion generation, following [8, 41], we use three standard metrics: Frechet Inception Distance (FID) that measures the distance between the generated and ground-truth motion distributions; R-Precition (including RTOP-1, RTOP2, and RTOP-3) that computes the retrieval accuracy of the generated motions with respect Table 1. Ablation study on Morph-MoMask (combined with MoMask [7] generator) for text-to-motion task on HumanML3D dataset. IS: imitation selection operation; Adversarial: using adversarial reward training MPR module; Energy: using energy reward training MPR module; Real Data: using real motion data training MPR module; FT: fine-tuning Motion Generator with physics-refined motions. The arrows ( / ) indicate that higher/smaller values are better. Model Number Methods Common Generation Metrics Physical Plausibility Metrics IS Adversarial Energy Real Data FT RTOP-3 Baseline (only motion generator MoMask [7]) 0.807 0.792 0.782 0.790 0.801 0.785 0.816 D to the input text; Diversity (Div) that measures the diversity of generated motions. For music-to-dance generation, following [16, 29, 38], we use five standard metrics: FIDk and Divk that calculate Frechet Inception Distance and Diversity on kinetic features (denoted as k); FIDg and Divg that calculate Frechet Inception Distance and Diversity on geometric features (denoted as g); Beat Align Score (BAS) that measures the alignment between input music and generated dances. Next, we present the physical plausibility metrics. Following [33, 39], we use four physic-based metrics to assess the physical plausibility of generated motions: Penetrate that measures ground penetration; Float that measures floating; Skate that measures foot sliding; Physical Foot Contact score (PFC) that measures the realism of foot-ground contact. Additionally, we introduce an imitation selection metric, imitation failure rate (IFR), which calculates the failure rate of the physics refinement module in imitating motion. 4.2. Implementation Details Training Setup. We implement Morph based on PyTorch. The NVIDIAs Isaac Gym is used as the physics simulator. Rather than using real motion data, we train Morph on synthetic motion data produced by pre-trained model, increasing the motion data volume to three times the original. In the training phase of MPR module (Stage 1), we use the Adam optimizer with batch size of 64 and learning rate of 4 105. During the finetuning of motion generator (Stage 2), we follow the original training setup, only adjusting the learning rate to 1 105. For hyper-parameters setting, we set wθ, wq, wv and ww (Eq. 2) to 0.5, 0.3, 0.1 and 0.1, αθ, αq, αv and αw (Eq. 2) to 100, 10, 0.1 and 0.1, γ (Eq. 5) is set to 0.1 and the imitation selection threshold τ is set to 0.5. All our experiments are conducted on 8 Tesla V100 GPUs. Further details on implementation and hyperparameter analysis are provided in the Appendix. Data Preprocessing. In text-to-motion generation, some motion sequences involve environmental interactions, such as sitting down, climbing stairs, and swimming, which cannot be simulated by current physical simulator. ThereFID 0.045 0.194 0.276 0.165 0.077 0.183 0.043 PFC Penetrate Float Skate IFR 1.058 0.852 0.735 0.715 0.672 0.749 0.651 23.152 0.000 0.000 0.000 0.000 0.000 0.000 10.660 2.272 2.554 2.376 2.271 2.451 2.146 5.262 0.020 0.032 0.026 0.011 0.017 0.010 - 0.0155 0.0362 0.0272 0.0155 0.0338 0. fore, we annotate each text description in HumanML3D [9] based on its semantic content. Text Descriptions involving interaction-based motions are labeled as 0, while other descriptions are labeled as 1. Text labeled as 1 will be used to train the MPR module in Stage 1. We will release this annotation on HumanML3D in the final version. The generated motion sequences may exhibit issues such as body leaning, floating and ground penetration. When imported into the engine, these issues can cause instability in the robot, leading to falls, bouncing off the ground, or dropping from mid-air. To address this, we apply preprocessing step to the motion sequences. Specifically, we first compute the bodys tilt angle, defined as the angle between the projection of the center of mass onto the ground and the line connecting both feet, and apply this angle to the pelvis for the entire sequence. To correct for floating and penetration, we determine the lowest point height and adjust the entire sequence by this offset. The resulting sequence is then used for training and inference. More details can be found in the Appendix. 4.3. Ablation Studies In this section, we conduct ablation studies to validate the effectiveness of each part of our method. We use MoMask [7] as the motion generator in Morph, denoted as Morph-MoMask, throughout the ablation studies. The ablation results are shown in Tab. 1. Effectiveness of training MPR module using only generated data. We investigate the effectiveness of using only synthetic motion data from two perspectives: its overall effectiveness and its performance relative to training with real motion data. First, we compare the baseline model (only the MoMask generator) and model which combines MoMask with the trained MPR module. As shown in Tab. 1, model outperforms model on all physical metrics (0.651/1.058 on PFC, 0.0/23.152 on Penetrate, 2.146/10.66 on Float, 0.01/5.262 on Skate), while maintaining competitive performance on generation metrics. These results confirm the feasibility of training the Table 2. Comparison results for text-to-motion task on HumanML3D dataset. Morph is combined with different types of motion generators. MG: Motion Generator; MPR: Motion Physics Refinement module; FT: fine-tuning motion generator with the physics-refined motion data. denotes Morph without fine-tuning the motion generator (only Stage 1 training) MG MPR FT Common Generation Metrics Physical Plausibility Metrics RTOP-1 RTOP-3 PhysDiff w/ MD [39] PhysDiff w/ MDM [39] Reindiffuse [11] MDM [31] Morph-MDM Morph-MDM MotionDiffuse(MD) [40] Morph-MD Morph-MD T2M-GPT [40] Morph-T2M-GPT Morph-T2M-GPT MoMask [7] Morph-MoMask Morph-MoMask - - - - - - - - - - - - - - - - - 0.455 0.441 0.467 0.491 0.485 0.494 0.491 0.487 0.497 0.521 0.511 0.525 0.780 0.631 0.622 0.749 0.744 0.752 0.782 0.778 0.786 0.775 0.770 0.781 0.807 0.801 0.816 FID Diversity 0.551 0.433 0.385 0.489 0.510 0.485 0.630 0.661 0.571 0.116 0.136 0.109 0.045 0.077 0.043 - - - 9.920 9.735 9.863 9.410 9.508 9.492 9.761 9.646 9.768 9.641 9.579 9.687 PFC - - - 0.811 0.720 0.707 0.533 0.437 0.416 0.998 0.769 0.751 1.058 0.672 0.651 Penetrate 0.898 0.998 0.000 17.384 0.000 0.000 16.670 0.000 0.000 72.250 0.000 0.000 23.152 0.000 0. Float 1.368 2.601 0.711 17.502 2.278 2.261 5.698 1.168 0.856 8.918 2.706 2.703 10.660 2.271 2.146 Skate 0.423 0.512 0.058 3.540 0.019 0.018 3.419 0.032 0.031 7.801 0.042 0.042 5.262 0.011 0.010 IFR - - - - 0.0177 0.0154 - 0.0183 0.0166 - 0.0208 0.0197 - 0.0155 0.0151 Table 3. Comparison results on common generation metrics for text-to-motion on HumanML3D dataset. Methods RTOP-1 RTOP-2 RTOP-3 FID MM-Dist Diversity MModality MDM [31] MotionDiffuse (MD) [41] MLD [3] T2M-GPT [40] AttT2M [47] MMM [24] MoMask [7] BAMM [23] Morph-MDM Morph-MD Morph-T2M-GPT Morph-MoMask 0.455 0.491 0.481 0.491 0.499 0.515 0.521 0.525 0.467 0.494 0.497 0.525 0.645 0.681 0.673 0.680 0.690 0.708 0.713 0.720 0.658 0.687 0.684 0.722 0.749 0.782 0.772 0.775 0.786 0.804 0.807 0.814 0.752 0.786 0.781 0.816 0.489 0.630 0.473 0.116 0.112 0.089 0.045 0.055 0.485 0.571 0.109 0. 3.330 3.113 3.196 3.118 3.038 2.926 2.958 2.919 3.106 3.001 2.997 2.923 9.920 9.410 9.724 9.761 9.700 9.577 9.641 9.717 9.863 9.492 9.768 9.687 2.290 1.553 2.413 1.856 2.452 1.226 1.241 1.687 2.177 1.584 1.903 1.652 MPR module solely with synthetic data. Next, we compare model with model F, which uses real motion data to train the MPR module. As shown in Tab. 1, model achieves inferior performance compared to model E. This can be attributed to the significant domain gap between the training data (real motions) and test data (generated motions). This gap hinders the MPR module trained on real data from effectively adapting to the generated motion data, leading to performance decline. Effectiveness of adversarial reward and energy reward. When training the MPR module, we add an adversarial reward and energy reward in addition to the common imitation task reward. In Tab. 1, comparing model with model C, which omits the adversarial reward, we observe performance decline in both generation and physical metrics on model C. The significant drop in FID metric indicates that, without the adversarial reward, the simulated motions lack the distribution constraints of the input motions, leading to deviation from the intended motion distribution. Next, we compare model with D, which omits the energy reward. As shown in Tab. 1, model outperforms model in generation metrics. We argue that introducing energy reward helps suppress high-frequency jitter during the humanoid robot control process, resulting in more natural motions. Effectiveness of imitation selection strategy. Like [11, 39], the proposed Morph focuses on addressing physical inconsistencies in non-interactive and ground-contact human motions. In training and testing data, there are some interactive and non-grounded motions, such as sitting on chair, swimming, or climbing stairs. To ensure the quality of the motions after refining, we design an imitation selection operation to filter out the simulations of such motions. In Tab. 1, we compare model with B, which omits the imitation selection operation. As shown, model outperforms in both generation and physic metrics, validating the effectiveness of the imitation selection strategy. Table 4. Comparison results for music-to-dance on AIST++ dataset. denotes Morph without fine-tuning motion generator. MG MPR FT Common Generation Metrics Physical Plausibility Metrics FACT [14] TM2D [6] EDGE [33] Bailando [29] Morph-EDGE Morph-EDGE Morph-Bailando Morph-Bailando - - - - FIDk 35.35 19.01 42.16 28.16 43.05 39.29 35.57 26.32 FIDg Divk Divg 6.18 5.94 22.11 9.45 6.36 20.09 4.61 3.96 22.12 9.62 6.34 7.83 5.06 4.27 23.72 5.23 4.91 19.88 6.47 7.64 12.12 6.55 9.43 7.86 BAS 0.2209 0.2049 0.2334 0.2332 0.2228 0.2327 0.2406 0.2411 PFC - - 0.610 0.074 0.309 0.287 0.047 0. Penetrate - - 76.490 31.183 0.000 0.000 0.000 0.000 Float - - 60.982 12.650 3.722 3.524 2.082 2.061 Skate - - 7.906 4.466 0.011 0.010 0.025 0.023 IFR - - - - 0.0100 0.0085 0.0067 0.0059 - - - - Effectiveness of finetuning motion generator using simulated motions. Furthermore, we compare model and model G, which finetunes the motion generator using the physically refined motions produced by the MPR module. As shown in Tab. 1, model achieves further gains in both generation and physic metrics, with RTOP-3 increasing by 1.5% and FID improving by 0.034. These results suggest that the proposed MPR module can, in turn, enhance the performance of the motion generator. 4.4. Evaluation with Different Motion Generators We evaluate the adaptability of our Morph with different motion generators, including MDM [31], Motiondiffuse [41], T2M-GPT [40], and MoMask [7]. As shown in Tab. 2, Morph significantly enhances physical fidelity while maintaining competitive generation metrics across different motion generation models. For example, after integrating Morph with MDM (Morph-MDM), RTOP-1 metric improved by 0.012. In terms of physical plausibility metrics, penetration dropped to zero, float decreased from 17.502 to 2.261, and skate reduced from 3.540 to 0.018. Similar performance improvements are observed with other motion generators. The consistent gains across different generators highlight the versatility of our Morph framework. 4.5. Comparisons with State-of-the-arts Main results on Text-to-Motion Generation. On the text-to-motion dataset, HumanML3D, we compare Morph framework with other state-of-the-art methods. The comparison results are shown in Tab. 2 and Tab. 3. As shown, Morph achieves significant gains in physical metrics while maintaining competitive performance in generation metrics, demonstrating its capability to enhance physical plausibility. Fig. 3 illustrates the generated motions from both our Morph-MoMask and MoMask. As shown, motions generated by MoMask often exhibit physically unrealistic artifacts, such as penetration, floating, and leaning forward. In contrast, Morph-MoMask effectively reduces these artifacts, producing motions that are both physically plausible and realistic. Main results on Music-to-Dance Generation. On the Figure 3. Qualitative comparison between our Morph-MoMask and MoMask in text-to-motion task. Morph-MoMask significantly reduces physical artifacts such as leaning forward, floating and penetration. music-to-dance dataset, AIST++, we compare Morph combined with EDGE [33] and Bailando [29] generators against other state-of-the-art methods. As shown in Tab. 4, Morph significantly improves physical metrics compared to existing methods. In terms of generation metrics, our MorphEDGE model achieves the best results in multiple metrics, including FIDg, Divg, and BAS, while also demonstrating competitive performance in FIDk metric. These results validate the superiority of our framework in music-to-dance generation. 5. Conclusion In this paper, we present Morph, model-agnostic physical optimization framework, designed to enhance physical plausibility in motion generation without relying on costly real-world motion data. To accomplish this, we first leverage pretrained motion generator to synthesize large-scale noisy motion data. We then introduce Motion Physics Refinement module, which utilizes these synthesized data to train motion imitator that enforces physical constraints. The physically refined motions can, in turn, be used to finetune the motion generator, further enhancing its capabilities. Our framework is compatible with various motion generation models across both text-to-motion and music-to-dance generation tasks. Extensive experimental results demonstrate that Morph substantially improves physical plausibility while achieving competitive generation quality."
        },
        {
            "title": "References",
            "content": "[1] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. BEHAVE: Dataset and method for tracking human object In Proceedings of the IEEE/CVF Conference interactions. on Computer Vision and Pattern Recognition, pages 15935 15946, 2022. 1 [2] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18551866, 2024. 2 [3] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 2, 7 [4] Sisi Dai, Wenhao Li, Haowen Sun, Haibin Huang, Chongyang Ma, Hui Huang, Kai Xu, and Ruizhen Hu. Interfusion: Text-driven generation of 3d human-object interaction. arXiv preprint arXiv:2403.15612, 2024. 2 [5] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep wholebody control: learning unified policy for manipulation and locomotion. In Conference on Robot Learning, pages 138 149, 2023. 4 [6] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 99429952, 2023. 8 [7] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 1, 2, 5, 6, 7, 8, 11, 12 [8] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. 2, [9] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 5, 6 [10] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. Human POSEitioning system (HPS): 3D human pose estimation and self-localization in large scenes from In Proceedings of the IEEE/CVF body-mounted sensors. Conference on Computer Vision and Pattern Recognition, pages 43184329, 2021. 1 [11] Gaoge Han, Mingjiang Liang, Jinglei Tang, Yongkang Cheng, Wei Liu, and Shaoli Huang. Reindiffuse: Crafting physically plausible motions with reinforced diffusion model. arXiv preprint arXiv:2410.07296, 2024. 1, 2, 3, 4, 7 [12] Ludovic Hoyet, Rachel McDonnell, and Carol OSullivan. Push it real: Perceiving causality in virtual interactions. ACM Transactions on Graphics (TOG), 31(4):19, 2012. 1 [13] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [14] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401 13412, 2021. 8 [15] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Learn to dance with aist++: Music conditioned 3d dance generation, 2021. [16] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation In Proceedguided by the characteristic dance primitives. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15241534, 2024. 1, 2, 6 [17] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: In Forty-first Text-aligned whole-body motion generation. International Conference on Machine Learning, 2024. 2 [18] Mingshuang Luo, Ruibing Hou, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan. M3gpt: An advanced multimodal, multitask framework for moarXiv preprint tion comprehension and generation. arXiv:2405.16273, 2024. 1, 2 [19] Zhengyi Luo, Jinkun Cao, Alexander W. Winkler, Kris Kitani, and Weipeng Xu. Perpetual humanoid control for realtime simulated avatars. In International Conference on Computer Vision (ICCV), 2023. 4 [20] Matthew Marchellus and In Kyu Park. M2c: Concise music representation for 3d dance generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31263135, 2023. 2 [21] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic. ACM Transactions on Graphics, page 114, 2018. 5 [22] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics, page 120, 2021. 4 [23] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: Bidirectional autoregressive motion model. arXiv preprint arXiv:2403.19435, 2024. 1, [24] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen In ProChen. Mmm: Generative masked motion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 1, 2, 7 [25] Alec Radford. Improving language understanding by generative pre-training. 2018. 2 [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763, 2021. 2 [27] Paul SA Reitsma and Nancy Pollard. Perceptual metrics for character animation: sensitivity to errors in ballistic moIn ACM SIGGRAPH 2003 Papers, pages 537542. tion. 2003. 1 [28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 [29] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory. Computer Vision and Pattern Recognition, pages 11050 11059, 2022. 2, 5, 6, [30] Mikihiro Tanaka and Kent Fujiwara. Role-aware interaction generation from textual description. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1599916009, 2023. 2 [31] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano. Human motion diffusion model, 2022. 5, 7, 8 [32] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 1, 2, 5 [33] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: In Proceedings of Editable dance generation from music. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448458, 2023. 1, 2, 5, 6, 8 [34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [35] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2 [36] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3D human pose in the wild using imus and moving camera. In Proceedings of the European Conference on Computer vision, pages 601617, 2018. 1 [37] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 469480, 2023. 2 [38] Jiashuo Yu, Yaohui Wang, Xinyuan Chen, Xiao Sun, and In InYu Qiao. Long-term rhythmic video soundtracker. ternational Conference on Machine Learning, pages 40339 40353. PMLR, 2023. 6 [39] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1601016021, October 2023. 1, 2, 3, 4, 6, [40] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. arXiv preprint arXiv:2301.06052, 2023. 2, 5, 7, 8 [41] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2, 5, 7, 8 [42] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364373, 2023. 1, 2 [43] Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. arXiv preprint arXiv:2404.01284, 2024. 2 [44] Siwei Zhang, Yan Zhang, Qianli Ma, Michael Black, and Siyu Tang. PLACE: Proximity learning of articulation and contact in 3D environments. In International Conference on 3D Vision, pages 642651, 2020. 1 [45] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael Black, and Siyu Tang. Generating 3D people in scenes withIn Proceedings of the IEEE/CVF Conference out people. on Computer Vision and Pattern Recognition, pages 6194 6204, 2020. [46] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generaIn Proceedings of the AAAI Conference on Artificial tors. Intelligence, volume 38, pages 73687376, 2024. 1, 2 [47] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. 1, 2, 7 [48] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
        },
        {
            "title": "Appendix",
            "content": "In this Appendix, we present more details for Morph, including data preprocess, additional experimental results, qualitative comparisons. First, we describe the data preprocessing procedure used for training the Motion Physics Refinement (MPR) module with generated motion data (Sec. A). Then, we present experimental results analyzing the impact of τ in the imitation selection operation (Sec. B), the effect of varying the quantity of noisy motion data for MPR training (Sec. C), and effect of the number of training rounds for Morph (Sec. D). Finally, we provide additional qualitative comparisons for text-to-motion and music-to-dance tasks (Sec. E). A. Details for Data Preprocess As discussed in the main text, the generated motion sequences may exhibit issues such as body leaning, floating and ground penetration. When imported into the simulator, these issues can cause instability in the robot, potentially causing it to fall, bounce off the ground, or drop from mid-air. To address this issue, we apply preprocessing step to the motion sequences, detailed in Fig. 4 and Alg. 1. Specifically, we first compute the bodys tilt angle, defined as the angle between the projection of the center of mass onto the ground and the line connecting both feet. If this angle exceeds 10, we apply the necessary adjustment to the pelvis throughout the sequence. To correct floating and penetration, we determine the lowest mesh height and adjust the entire sequence by this offset. The preprocessed sequence is then used for training and inference. B. Effect of τ in Imitation Selection on Morph In Tab. 5, we analyze the effect of the threshold τ in the imitation selection operation on Morph. Different values of τ are tested to assess the performance of Morph-MoMask (combined with MoMask [7] motion generator, without fine-tuning motion generator). When τ is set to 0, the motion refined by the MPR module is not utilized, and Morph directly outputs the results from the motion generator. As τ increases, the physical plausibility metrics improve significantly. However, the generation metrics show slight decrease due to the inclusion of some incorrectly refined or non-grounded motions at higher thresholds. Larger values of τ incorporate more refined motions, improving the physical plausibility metrics. However, this also increases the acceptance of incorrectly refined motions, leading to shift in the motion distribution and corresponding decline in the generation metrics. According to Tab. 5, we observe that τ = 0.5 strikes balance between generation and physical plausiibility metrics. Therefore, we set τ to 0.5 in this paper. Figure 4. flowchart illustrating the data preprocessing process. The parameters are calculated from the first frame and then applied to all generated motion sequences before they are fed into the MPR module. Algorithm 1 Preprocessing Motion Sequences Require: Motion sequence with frames F1, F2, . . . , Fn Ensure: Preprocessed motion sequence Step 1: Calculate the angle θ (1) Compute the projection of the center of mass of F1 onto the ground. (2) Determine the line connecting the pelvis point and the center of both feet in F1. (3) Calculate the angle θ between the projection and the line. Step 2: Correct posture if θ > 10 (1) Apply an additional rotation to the pelvis for the entire sequence S. Step 3: Ensure F1 is on the ground (1) Infer the lowest point height of the mesh in F1. (2) Add uniform offset to the entire sequence S. Step 4: Output the preprocessed sequence S. generated motion data are used to train the MPR module in MorphMoMask. As shown in Tab. 6, increasing the amount of training data for the Motion Physics Refinement (MPR) module leads to improvements in both the generation and physical plausibility metrics on the test set. These results indicate that larger volume of generated motion data enhances the MPR modules ability to better mimic the input motion and produces higher-quality outputs. Conversely, when the MPR module is trained with smaller dataset, its motion imitation capability diminishes, leading to greater discrepancies between the generated and input motions. This results in decline in both the generation and physical plausibility metrics. These results further highlight the effective data augmentation capability of our proposed Morph. C. Effect of Varying Amounts of Noisy Motion D. Effect of Multi-Round Optimization of the"
        },
        {
            "title": "MPR module and MG on Morph",
            "content": "In Tab. 6, we investigate the impact of varying amounts of generated motion data on the training of Morph. Different numbers of In Tab. 7, we analyze the effect of multi-round optimization of the Physics Refinement (MPR) module and Motion Generator Table 5. Hyper-parameter analysis of τ in Imitation Selection operation. Comparison with different values of τ based on Morph-MoMask (combined with MoMask [7] motion generator, without fine-tuning motion generator) for text-to-motion task on HumanML3D dataset. The arrows ( / ) indicate that higher/smaller values are better."
        },
        {
            "title": "Methods",
            "content": "τ =0.0 τ =0.1 τ =0.2 τ =0.3 τ =0.4 τ =0.5 τ =0.6 τ =0.7 τ =0.8 τ =0.9 τ =1."
        },
        {
            "title": "Physical Plausibility Metrics",
            "content": "RTOP-1 RTOP-3 FID Diversity PFC Penetrate Float Skate IFR 0.521 0.520 0.518 0.515 0.514 0.511 0.508 0.506 0.500 0.496 0.488 0.807 0.806 0.806 0.804 0.803 0.801 0.799 0.799 0.795 0.793 0.790 0.045 0.051 0.058 0.071 0.073 0.077 0.080 0.081 0.081 0.087 0.088 9.641 9.633 9.629 9.578 9.582 9.579 9.575 9.543 9.522 9.412 9.260 1.058 0.879 0.774 0.760 0.728 0.672 0.686 0.663 0.647 0.638 0. 23.152 3.567 0.842 0.058 0.003 0.000 0.000 0.000 0.000 0.000 0.000 10.660 4.783 4.020 3.202 2.998 2.271 2.268 2.102 2.028 1.989 1.984 5.262 2.132 1.061 0.533 0.217 0.011 0.011 0.008 0.006 0.005 0.005 - 0.1287 0.0543 0.0262 0.0162 0.0155 0.0147 0.0131 0.0126 0.0120 0.0113 Table 6. Comparison of text-to-motion with different amounts of noisy motion data training for Morph-MoMask (combined with MoMask [7] motion generator, without fine-tuning motion generator). refers to the total number of generated noisy motion data samples, which is three times the amount of the original real training data. refers to the number of generated motion data used to train the MPR module. We set τ as 0.5 for testing."
        },
        {
            "title": "Methods",
            "content": "D=25%N D=50%N D=75%N D=100%N"
        },
        {
            "title": "Physical Plausibility Metrics",
            "content": "RTOP-1 RTOP-3 FID Diversity PFC Penetrate Float Skate IFR 0.492 0.495 0.500 0.511 0.788 0.793 0.797 0.801 0.089 0.082 0.078 0.077 9.475 9.529 9.566 9.579 0.869 0.822 0.765 0. 0.124 0.028 0.002 0.000 3.004 2.878 2.435 2.271 0.038 0.027 0.014 0.011 0.0266 0.0212 0.0181 0.0155 Table 7. Comparison of text-to-motion with multi-round optimization of the MPR module and motion generator based on Morph-MoMask. We set τ as 0.5 and use the total number of generated noisy motion data to train."
        },
        {
            "title": "Physical Plausibility Metrics",
            "content": "RTOP-1 RTOP-3 FID Diversity PFC Penetrate Float Skate IFR One-Round w/o FT One-Round Two-Round w/o FT Two-Round 0.511 0.525 0.525 0.527 0.801 0.816 0.818 0.821 0.077 0.043 0.043 0.044 9.579 9.687 9.689 9. 0.672 0.651 0.635 0.631 0.000 0.000 0.000 0.000 2.271 2.146 2.132 2.117 0.011 0.010 0.010 0.008 0.0155 0.0151 0.0137 0.0134 (MG) on Morph using Morph-MoMask. To further validate the effectiveness of this round-based training approach in enhancing both the MG and the MPR module, we conducted an additional round of training beyond this single-round training described in the main text. This extra round explores the potential for mutual enhancement between the two modules. In Tab. 7, the following terms are defined: One-Round w/o FT: The first round of training where only the MPR module is trained. One-Round: The first round of training that includes both training the MPR module and fine-tuning the MG. Two-Round w/o FT: Training the MPR module again using the motion data generated by the fine-tuned MG from the first round. Two-Round: Fine-tuning the Motion Generator using the results from Two-Round w/o FT. As shown in Tab. 7, in the first round of training, MG improves the performance of MPR module, enhancing the physical quality of its generated motion. The refined motion data from the trained MPR module is then used to fine-tune the MG, boosting its performance further. In the second round, the fine-tuned MG from the first round is used to generate training data for the MPR module (initialized with first-round weights). We observed improvements in Two-Round w/o FT compared to One-Round, with PFC increasing by 0.016, Float by 0.014, and IFR decreasing, indicating enhanced motion imitation by the MPR module. After finetuning the MG once again, Two-Round shows improvements in the RTOP-1 and RTOP-3 metrics. These results clearly demonstrate that the MG and MPR modules can mutually enhance each other. Moreover, alternating training between the MG and MPR modules across multiple rounds can further improve the performance of Morph. E. More Qualitative Results Fig. 5 and Fig. 6 provide the additional qualitative results for the text-to-motion and music-to-dance generation tasks using Morph. As shown in Fig. 5, in the text-to-motion generation task, floating and penetration are common artifacts in motion generation, often resulting from inaccuracies in the estimation of translation. However, Morph effectively addresses these issues, successfully mimicking the input motion and demonstrating significant improvement in mitigating these artifacts. The generated motions are both physically plausible and realistic, showcasing Morphs enhanced performance in this task. As shown in Fig. 6, in the music-to-dance generation task, floating and penetration are the most prominent issues. Due to the faster frequency of dance movements, these artifacts occur more frequently. Morph effectively mitigates these issues, generating motions that are not only physically plausible but also exhibit higher degree of realism. In summary, Morph demonstrates significant improvements in both the text-to-motion and music-to-dance tasks. By accurately estimating translational motion, Morph is able to generate motions that are not only physically feasible but also exhibit higher degree of realism. Figure 5. Qualitative comparisons for text-to-motion on HumanML3D test set between Morph-MoMask and MoMask. Figure 6. Qualitative comparisons for music-to-dance on AIST++ test set between Morph-Bailando and Bailando. For music-to-dance, the testing music samples will be used as inputs."
        }
    ],
    "affiliations": [
        "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
        "MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "Peng Cheng Laboratory, China",
        "University of Chinese Academy of Sciences, China",
        "WeChat, Tencent Inc"
    ]
}