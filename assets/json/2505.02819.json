{
    "paper_title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations",
    "authors": [
        "Dmitriy Shopkhoev",
        "Ammar Ali",
        "Magauiya Zhussip",
        "Valentin Malykh",
        "Stamatios Lefkimmiatis",
        "Nikos Komodakis",
        "Sergey Zagoruyko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 1 8 2 0 . 5 0 5 2 : r ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations Dmitriy Shopkhoev1,2, Ammar Ali1,2, Magauiya Zhussip1, Valentin Malykh1,2,3, Stamatios Lefkimmiatis1, Nikos Komodakis4,5,6, Sergey Zagoruyko 1MTS AI , 2ITMO University 3IITU, 4University of Crete 5IACM-Forth 6Archimedes Athena RC, 7Polynome d.shophoev@gmail.com Latency Comparison 105 1.17 105 1 0.5 T l H 4,522 689 0 s ( ) s e ) ( U Environmental Normalized Comparison Relative Accuracy ) ( u . 100 50 0 107 100 95 90 r A t R 92. 89.9 90.3 3.5 1 3.5 Emissions Energy 85 ( s i ) u ( ) L Duration (s) Ours (LS) Ours (Cosine) UIDL Figure 1: Comparison of the proposed LLM compression method with the state-of-the-art UIDL strategy. Subplots illustrate (a) compression time, (b) environmental impact (CO2 emissions and energy consumption), and (c) performance accuracy relative to the uncompressed baseline. Our approach attains the shortest compression time , lowest energy use , and reduced emissions , while achieving the highest accuracy, demonstrating superior efficiency, sustainability, and effectiveness over existing methods."
        },
        {
            "title": "Abstract",
            "content": "We introduce ReplaceMe, generalized training-free depth pruning method that effectively replaces transformer blocks with linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only small calibration dataset that is used to estimate linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original models performance on open benchmarkswithout any training or healing steps, resulting in minimal computational overhead (see Fig. 1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "In recent years, transformers have achieved unprecedented success across wide range of tasks on both computer vision and natural language processing. Modern LLMs typically scale up to billions or even hundreds of billions of parameters, significantly increasing the computational and memory requirements for both training and inference stages. This substantial resource demand poses critical challenge for their wider practical deployment and usability. Due to the excessive size of modern LLMs, there has been significant research effort on making such models accessible to users with limited hardware capabilities. These efforts primarily focus on three key strategies: quantization, distillation, and pruning. Pruning involves identifying and removing less important parameters or entire structural components to streamline the model, thereby reducing computational overhead without significantly compromising the performance. Structured pruning is distinct from unstructured pruning in that it focuses on entire groups of parameters or layers, allowing for their complete removal. This approach not only enhances hardware utilization efficiency but also potentially achieves greater reductions in resource consumption. Importantly, it operates independently of the hardware type used. In this work, we focus on structural depth pruning, operating under the hypothesis that contiguous set of transformer blocks can be effectively approximated by single linear transformation. To validate this idea, we propose ReplaceMe, novel training-free pruning method that replaces selected blocks with linear transformation estimated from small calibration dataset. It is worth noting that most existing pruning methods require post-pruning retraining phaseoften referred to as healing processto recover lost performance. This retraining stage can be timeconsuming and computationally expensive. In contrast, ReplaceMe preserves the majority of the models performance without any retraining for reasonable compression ratio scenarios. ReplaceMe generalizes depth pruning methods by compensating for the error introduced after block removal with simple, yet effective, linear transformation matrix. This transformation is subsequently fused with one of the remaining model weights, enabling seamless integration without adding parameters. The contributions of this work can be summarized as follows: 1. We propose ReplaceMe, generalized method for depth pruning that can maintain model performance without requiring any healing process, for reasonable compression ratios; 2. We conduct detailed study on estimating the linear transformation analytically and numerically with respect to different objectives; 3. We provide detailed ablation studies for different calibration data, solvers, and LLM architectures; 4. We validate the effectiveness and generality of ReplaceMe across diverse model families, including large language models and vision transformer architectures like ViT [6]. This paper is organized as follows: Section 2 presents the core methodology behind our trainingfree depth pruning approach. It introduces the framework for identifying prunable layers in large language models (LLMs) and estimating the corresponding linear transformations that compensate for the removed components. The section also discusses the selection of appropriate loss functions, regularization strategies to ensure generalizability, and the potential extension to multiple linear transformations for more flexible pruning. Section 3 then provides comprehensive experimental results and ablation studies, demonstrating the effectiveness and robustness of our method, and analyzing the key factors that influence its performance."
        },
        {
            "title": "2 Method",
            "content": "In this study, we introduce ReplaceMe, novel depth-wise neural network pruning method that balances simplicity and effectiveness to optimize model performance. Our approach is based on the idea of pruning multiple layers in transformer models and replacing them with single linear transformation. It consists of the following key steps: First, we identify layers suitable for pruning by targeting those with minimal impact on performance, in line with prior research (Section 2.1). Next, we compute an optimal linear transformation (LT) to compensate for the contributions of the pruned layers. Notably, this transformation is seamlessly integrated into the preceding layer, preserving model 2 performance without introducing additional parameters (Section 2.2). Furthermore, we study the effect of regularization strategies on the estimation of the transformation and show that this can be helpful strategy to maintain balance between model performance and perplexity (section 2.3). Finally, we outline how to extend our framework to support multiple linear transformations, enabling flexible and informed pruning decisions (section 2.4). Together, these components establish ReplaceMe as practical and robust advancement in training-free neural network pruning. Figure 2: ReplaceMe compresses and accelerates large language models (LLMs) by bypassing contiguous sequence of transformer blocksillustrated by the red linewhile preserving model performance. This is achieved by inserting an estimated linear transformation matrix that maps the MLP output of the i-th block directly to the input space expected by the (i + + 1)-th block, effectively pruning all blocks in between. 2.1 Layers selection Let Xi RN be the input to the i-th transformer block, where denotes the number of tokens and the hidden dimension of the transformer model. Then, typically, the conventional transformer block can be expressed in the following way: Yi = Xi + MHAi (cid:0)LN(1) (Xi)(cid:1) Mi = MLPi (cid:0)LN(2) (Yi)(cid:1) Li = Yi + Mi, (1) (2) (3) , LN(2) where MHAi and MLPi denote the multi-head attention (MHA) and MLP layers, respectively, while LN(1) correspond to the layer normalization operators before the MHA layer and after the MLP, respectively. The output of the attention sub-block is denoted as Yi, while Mi and Li represent the outputs of the MLP layer and the transformer block respectively. The layer selection strategy is based on the significance of each layer, which is determined by the distance between the activation outputs of different transformer blocks. Formally, for predefined number of layers to be pruned, denoted as n, the optimal cut index is determined by minimizing the distance between hidden states before and after the cut: = arg min (Li, Li+n) . (4) We evaluated various distance metrics D() and found that the cosine distance is particularly effective in identifying nearly optimal layers for pruning. This observation aligns with findings reported in recent studies [10]. In supplementary materials A.10, we present results from an exhaustive brute-force layer selection, which confirm that cosine distance consistently identifies optimal or near-optimal layers for removal. Additionally, we provide comparative analysis of L2 distance metric to further validate our choice. 3 2.2 Estimating the Linear Transformation To compensate for the pruned transformer blocks, we leverage small set of calibration data to compute activations directly before and after the removal point. Utilizing these activations, we estimate an appropriate linear transformation that accurately approximates the final output of the pruned blocks. Depending on the selected criterion, this estimation can be performed using either analytical or numerical approaches, enabling precise modeling of the omitted layers. As illustrated in Fig. 2, linear transformation is applied following the MLP and prior to the residual summation. The objective of the proposed method is to estimate an optimal linear transformation matrix so that: = arg min h(cid:0)Mi + Yi; Li+n (cid:1) (5) where () corresponds to distance function (e.g., L2 distance, cosine distance, etc.) between two input tensors and denotes the optical cut index estimated using equation (4). Once the transformation matrix is estimated, the transformer blocks from + 1 to + (inclusive) are removed. L2-Distance. classical solution for Eq. 5 is solving least-squares (LS) problem by setting the distance h() to be the L2-distance: = arg min (cid:13) (cid:0)Mi + Yi (cid:13) (cid:1) Li+n (cid:13) 2 2 = (MT (cid:13) Mi)1 MT (cid:0)Li+n Yi (cid:1). (6) This formulation allows to work with convex quadratic problem and admits closed-form analytical solution as shown above, for details refer to supplementary material A.2. Cosine Distance. As discussed in Section 2.1, our ablation study that compares various distance functions for assessing the importance of transformer blocks, revealed that the cosine distance is the most effective in identifying the least significant blocks. Motivated by these results, we further adopted the cosine distance as the objective function for estimating the optimal linear transformation. In this case, the optimization problem takes the form: = arg min cosine_distance (Mi + Yi, Li+n) = arg min (cid:88) (cid:18) 1 k=1 (Mi,k + Yi,k)T Li+n,k Mi,k + Yi,k2 Li+n,k2 (cid:19) , (7) where we use the notation Mi,k Rd to denote the k-th row of matrix Mi RN d, which we then represent as column vector. Here, the cosine distance is computed per token and aggregated over all tokens. Unlike the L2-distance formulation, this objective does not admit closed-form solution, thus requiring numerical optimization approach. In our experiments, we utilized the Adam [16] optimization algorithm. Furthermore, an ablation study involving various alternative numerical solvers can be found in the supplementary material A.6. To solve the optimization problem in Eq.(7), it would be necessary to store the hidden states Mi, Yi, and Li+n. To improve memory efficiency, we instead optimize the following simplified formulation: = arg min cosine_distance (Mi T, Li+n Yi) . (8) This alternative formulation requires us to store only Mi and the difference Li+n Yi, instead of keeping all three matrices. In supplementary material A.11, we empirically demonstrate that this simplification has negligible effect on performance while improving memory efficiency. Merging the Linear Transformation Once the optimal linear transformation has been estimated, our approach enables it to be incorporated into the MLP layer of the i-th transformer block, where it is merged with the weight matrix of the second FFN layer. Consequently, the overall architecture of the model remains unchanged, except for the removal of the non-effective transformer blocks. 4 2.3 Regularization We also investigate estimating the linear transformation matrix by imposing additional constraints on the matrix through regularization. Specifically, we reformulate the optimization problem as follows: = arg min Dist((cid:0)Mi + Yi (cid:1) Li+n) + α R(T), (9) where R() represents the regularization term, and α controls its strength. To promote sparsity in the transformation matrix and encourage more balanced distribution of feature importance, we use both L1 and L2 regularization terms for cosine distance and derived the analytical solution for l2 regularization with respect to LS. Empirical analysis shows that these regularizers improve the pruned models ability to generate accurate predictions, as reflected by better accuracy-based benchmarks (see Sec. 3), though this improvement comes at the cost of increased perplexity. 2.4 Multiple Linear Transforms The proposed ReplaceMe method can be applied to multiple non-overlapping blocks within the model, estimating separate linear transformation for each block (multi-LT). This approach provides flexibility in achieving the desired performance metrics, even under high compression ratios. Furthermore, if some of the selected blocks are consecutive, they can be merged into single block with one corresponding linear transformation."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we first describe our experimental setup. We then systematically compare our trainingfree pruning method against existing structured pruning approaches, including those that rely on healing mechanisms. Notably, we show that our method achieves competitive performance without requiring any additional training. To further analyze the factors influencing our approach, we conduct ablation studies on several key aspects: the calibration dataset (used for choosing the layers to be pruned and for estimating the linear transformation), the choice of distance function in Eq. 5, and the impact of incorporating regularizers. 3.1 Experimental setup In our experiments, we primarily focused on LLaMA-2-7B and LLaMA-3-8B-Instruct models, while also reporting results for Qwen2.5-7B and Falcon-11B for comparative analysis. Results for additional models are provided in Supplementary Material A.7. In the main results section, particularly in Table 1, we present outcomes based on different benchmarks to maintain consistency with prior research. We adhered to the benchmarks utilized in their respective papers: CMNLI [43], HellaSwag [44], PIQA [2], CHID [46], WSC [20], MMLU [14], CMMLU [21], Race-High/Middle [18], C3 [36]. Additionally, we benchmarked our proposed ReplaceMe using well-established public datasets, namely Winogrande [33], BoolQ [4], OpenBookQA [27], SciQ [42], and Lambada OpenAI [30]. For all benchmarks except Lambada OpenAI, we report accuracy as the evaluation metric, along with the average accuracy across all benchmarks. For Lambada OpenAI, we report perplexity. 3.2 Comparison with other structured-pruning methods In this section, we present the key findings from applying our approach across various model architectures and benchmarks. To ensure the statistical stability of our results, all experiments are reproduced multiple times. As demonstrated in Fig. 1, we conduct comparative analysis between ReplaceMe and UIDL, evaluating key metrics including time-to-get comparable accuracy, environmental impact, and final model performance. Notably, for UIDLs healing process, we restricted fine-tuning to Low-Rank Adaptation (LoRA) applied exclusively to the MLP layers, whereas alternative approaches incur significantly higher computational costs. Our proposed method exhibits 5 substantially reduced computational demands and achieves markedly faster recovery compared to other methods. In Table 1, we compare our method against other state-of-the-art structured depth pruning approaches. Notably, all competing methods rely on healing mechanisms and thus require extensive retraining, whereas our method remains entirely training-free (i.e., no healing process is applied). Despite this advantage, as shown in Table 1, our method consistently outperforms these baselines on average and achieves 92.5% of the performance of the uncompressed Llama 2 7B model at 25% compression ratio. Method Train-Free C3 CMNLI CHID (test) WSC Hella Swag PIQA Race-M Race-H MMLU CMMLU AVG RP Llama 2 7B (baseline) LLM-Streamline* LLMPruner* SliceGPT* LaCo* UIDL* Ours (Cosine) Ours (LS) 43.8 43.3 29.7 31.5 39.7 40.2 42.5 39.4 33.0 33.0 33.4 31.6 34.4 34.4 33.0 33.0 41.6 24.1 28.4 18.5 36.1 21.5 25.2 18.9 37.5 36.5 40.4 43.3 40.4 40.4 38.5 38.5 71.3 61.1 54.6 47.5 55.7 59.7 59.4 58. 78.1 71.5 72.0 68.3 69.8 69.0 71.1 70.5 33.1 34.8 22.9 27.0 23.6 35.2 35.4 37.1 35.5 37.0 22.0 29.4 22.6 34.7 36.7 36.5 46.8 45.5 25.3 28.8 26.5 44.6 46.4 45.2 31.8 29.4 25.0 24.8 25.2 28.9 30.4 29.2 45.3 41.6 35.4 35.1 37.4 40.9 41.9 40. 100.0% 92.0% 78.2% 77.5% 82.7% 90.3% 92.5% 89.9% Table 1: Comparing other pruning methods after healing and our training free approach ReplaceMe, * indicates that the numbers are taken from streamline paper [3]. After compressing Llama 2 7B with 25% compression ratio. indicates that the model is training-free. signifies that the model was trained following pruning, whereas In Table 2, we further compare our method against state-of-the-art structured pruning approaches on the more recent Llama 3 8B Instruct model, under the setting where no healing is applied. We note that SVD-LLM [41] employs low-rank approximation of the weights, while LLM-Pruner [25] combines both width and depth pruning. Despite these differences, as shown in Table 2, our method again outperforms these baselines. All results are reported at 25% compression ratio. The Multi_LT results correspond to the application of the method described in Section 2.4. While the identification of multi-sub-blocks typically yields consecutive blocks in most cases, we additionally evaluate the scenario where non-consecutive sub-blocks are enforced. This configuration demonstrates an improvement in perplexity but results in performance degradation across benchmark tasks. Method Linear transform Lambada-openai ppl Avg-acc RP Llama 3 8B Instruct [7] SVD-LLM [41] LLMPruner [25] UIDL [10] None None Identity ReplaceMe(ours) ReplaceMe(ours) ReplaceMe(ours) ReplaceMe(ours) ReplaceMe(ours) Linear (LS) Linear (Cosine) Multi_LT (LS) Multi_LT (Cosine) Multi_LT_nc (Cosine) 3.11 29.90 12.31 2216. 20.23 15.88 18.99 19.06 13.95 0.7 0.59 0.60 0.58 0.63 0.63 0.63 0.64 0.63 100 85.3 85.3 82.5 89.9 90.9 90.3 91.1 90.0 Table 2: Results of pruning Llama 3 8B instruct for 25% using different methods, without any healing or finetuning. avg-acc is the average performance of the model on race, winogrande, piqa, boolq, openbookqa, and sciq. and the perplexity on Lambada openai. We also present the relative performance (RP) of the original model performance on these benchmarks. Multi_LT_nc dentoes the non-consecutive blocks case In Figure 3, we also compare our training-free approach, ReplaceMe, against UIDL [10] across various models and different amounts of layer pruning. Our method consistently outperforms UIDL in both benchmark scores and perplexity evaluations, while also exhibiting greater stability. Finally, we note that at high compression ratios, applying healing process becomes necessary, as linear transformations alone are insufficient to fully recover performance. Although ReplaceMe continues to outperform UIDL under these conditions, healing phase is required at extreme compression levels to maintain model effectiveness."
        },
        {
            "title": "Comparison of Our Method vs UIDL Across Models",
            "content": "0.7 r A 0.65 0.6 0.55 2 3 4 5 6 8 Number of pruned layers 9 10 11 12 13 14 15 Llama 3 8B UIDL Falcon 2 11B ReplaceMe Adam Llama 3 8B ReplaceMe Adam Qwen 2.5 7B UIDL Falcon 2 11B UIDL Qwen 2.5 7B ReplaceMe Adam Figure 3: Comparison between our method and UIDL with different number of pruned layers and different models: Llama 3 8B instruct, Falcon 2 11B, and Qwen 2.5 7B instruct. Averaged accuracy on race, winogrande, piqa, boolq, openbookqa, and sciq and perplexity on wikitext. 3.3 Analysis Up to this point, we have outlined our primary goal: replacing series of transformer blocks with simpler, estimated linear transformation using calibration data. The nature of this calibration data is critical, as we demonstrate in Section 3.3.1, where the type of text (instructional vs. plain) and the amount of data significantly influence our results. Furthermore, in Section 3.3.2, we analyze the impact of regularization on results, revealing trade-off between performance metrics such as perplexity and accuracy. In the supplementary material we further explore structured linear transformations (e.g., diagonal or orthonormal matrices) A.3. 3.3.1 Calibration data ablation Our pruning method eliminates the need for additional training by leveraging small calibration datasets in place of conventional training data. These calibration datasets serve two core purposes: (1) assessing block importance to identify transformer blocks for removal (Section 2.1), and (2) capturing hidden states before and after the pruned blocks to solve the optimization problem defined in Equation 5, which estimates the corresponding linear transformation. The quality and characteristics of these calibration subsets are critical to the accuracy of the estimation. To understand the influence of calibration data, we conducted ablation studies exploring the impact of sample size and dataset typeincluding plain text, instruction-tuned data, and self-generated content. Our primary experiments utilized datasets such as Arcee [5], FineWeb [31], and SlimOrca [22], consistent with prior work like UIDL [10]. In particular, we investigated three key factors: (1) the effect of dataset type and its source, (2) the minimum amount of data required to produce stable and accurate estimates, and (3) the efficacy of masking as lightweight data augmentation strategy when working with limited calibration samples. As shown in Table 3, we apply our method to prune LLaMA 3 8B [7] by 25%, using ReplaceMe across all ablation settings. We evaluate three distinct calibration datasets: FineWeb, plain-text web corpus; SlimOrca, curated instruction dataset generated with ChatGPT; and orca_generated, 7 synthetic dataset where responses are generated by the baseline model (targeted for pruning) using prompts from SlimOrca. Method Objective Calibration Data Avg-acc Perplexity %"
        },
        {
            "title": "Baseline Model\nReplaceMe\nReplaceMe\nReplaceMe",
            "content": "- LS LS LS - fineweb 8k slim_orca 8k orca_generated 8k 0.70 0.56 0.62 0.61 3.11 26.74 21.21 13.58 100.00 80.47 89.59 87."
        },
        {
            "title": "ReplaceMe\nReplaceMe\nReplaceMe\nReplaceMe\nReplaceMe",
            "content": "83.16 90.67 90.51 90.64 87.33 Table 3: Pruning results for Llama 3 8B instruct using ReplaceMeby 25%. We estimate the linear transformation using different data in including plain text data such as Fineweb [31] and self generated data using Slim Orca instructions. fineweb 8k slim_orca 8k 4K SlimOrca + 4K Fineweb Mix of 66 languages orca_generated 8k 25.07 15.90 15.85 15.72 13.24 0.58 0.63 0.63 0.63 0.61 Impact of Calibration Dataset Type As shown in Table 3, calibration with instruction datasets leads to better performance on benchmark evaluations than using plain text, particularly for instructiontuned models. While self-generated data yields improved perplexity scores, it tends to underperform on downstream benchmarks. This trend holds consistently across both optimization objectives defined in Equation 5: the least-squares (LS) and cosine distance formulations. We also explored combining SlimOrca with other datasets, such as FineWeb and Aya [34]the latter being multilingual corpus covering 66 languages. Interestingly, these mixed datasets performed on par with SlimOrca alone, suggesting that high-quality instruction data dominates the calibration effect. LS Cosine Llama3 8B (Baseline) Average Accuracy vs. Calibration Samples Perplexity vs. Calibration Samples 0.8 0.6 0.4 0.2 r A r 10 x r 0 0.1 0.4 1.2 0.8 Calibration Samples 1. 104 0.1 0.4 1.2 0.8 Calibration Samples 1.6 104 Figure 4: Pruning Llama 3 8B with 25% using different number of samples to estimate the linear transform Impact of Calibration Dataset Size Figure 4 illustrates how the size of the calibration dataset affects the estimation of the linear transformation, using either an analytical L2 objective or numerical cosine distance objective. While increasing the number of calibration samples does not significantly improve benchmark accuracy, it does substantially reduce perplexityespecially when optimizing with the cosine distance objective. The linear transformation matrix has shape d, requiring approximately = d2 tokens for reliable estimation. With per-sample sequence length of S, this translates to at least d2/S samples. For 8 example, in LLaMA 3 8B, where = 4096 and = 1024, about 16,000 samples are theoretically needed. However, as seen in Figure 4, accuracy remains stable even with as few as 1,000 samples, suggesting robustness to sample size. That said, perplexity continues to improve with more data, indicating better model confidence and predictive quality. Impact of Masking for Data Augmentation We also investigated random token masking as lightweight data augmentation technique for scenarios with limited calibration data (e.g., 1,000 samples). As shown in Table 4, masking improves the stability of the numerical optimization and leads to better convergence when estimating the linear transformation. This masking strategy proves especially beneficial in low-data regimes, where it reduces overfitting and enhances generalization. However, when more data are available, the impact of masking becomes negligible. In summary, calibration dataset with approximately d2 tokens ensures stable and accurate estimation. When working with fewer tokens, random masking can mitigate overfitting and improve estimation quality. For instruction-tuned models, instruction-style calibration data consistently leads to better pruning outcomes. While self-generated data can reduce perplexity, it may degrade benchmark accuracy, highlighting trade-off between confidence and task-specific performance. Model Masking Calibration Data Avg-acc Perplexity % Llama3 8B ReplaceMe ReplaceMe ReplaceMe ReplaceMe - - 1k 1k 8k 8k 0.697 0.632 0.634 0.633 0.632 3.10 24.96 21.08 15.88 15.69 100.00 90.62 90.91 90.76 90. Table 4: Random token masking during the estimation of the linear transformation contributes to more stable solution, particularly when working with small datasets. 3.3.2 Regularization effect In this section, we aim to explore how regularization impacts the linear transform estimation. We apply Ridge regularization for LS and found that for 0 < α < 103 in Eq. 9, we notice slight improvement in perplexity, while the average accuracy on benchmarks remain the same. Conversely, increasing α further to 104 tends to enhance benchmark accuracy, though at the cost of higher perplexity. Thus, one can consider α as tradeoff parameter in between perplexity and accuracy of the pruned model. In contrast, L1 regularization with α = 104 gives higher boost to accuracy at the cost of perplexity performance for cosine method, as well as for L2 but the boost gain on benchmarks with L1 is much higher, So regularization is trade-off parameter in between accuracy performance and perplexity. Table 5: The affect of regularization on LS and cosine methods in terms of accuracy and perplexity Model Method α Avg-acc Perplexity RP Llama3 8B ReplaceMe LS ReplaceMe LS + L2 reg ReplaceMe LS + L2 reg ReplaceMe LS + L2 reg ReplaceMe LS + L2 reg ReplaceMe LS + L2 reg ReplaceMe LS + L2 reg 0 0.1 0.5 10 100 1000 10000 ReplaceMe Cosine ReplaceMe Cosine + L2 reg ReplaceMe Cosine + L1 reg 0 0.01 1104 0.697 0.624 0.625 0.625 0.624 0.624 0.624 0.626 0.634 0.635 0.638 3.1 21.2 21.2 21.2 21.2 21.1 20.8 22.9 15.9 20.7 22.1 100.0 89.6 89.6 89.6 89.5 89.6 89.5 89. 90.9 91.1 91.6 9 3.4 Vision Transformers pruning So far, weve focused on applying our pruning technique exclusively to the decoder transformer architecture, specifically within large language models (LLMs). This raises an important question: how well does this method generalize to other tasks beyond text generation, particularly when the transformer acts as an encoder. Thus, we directly applied ReplaceMe on the CLIP model, examining compression ratios of 13% and 25%. We utilized 8,000 samples from the MIMIC dataset and following the same evaluation procedure described in [32] we tested on well-known benchmarks, namely MS-COCO [23], Cifar-10 [17], EuroSAT [13], VTAB [45], and Pascal VOC-2007 [8]. Moreover, for the purpose of comparison with state-of-the-art method we also report results when the UIDL [10] method is applied on the same model. Model Compres. ratio MS-COCO Captions (retrieval) text recall@5 vision recall@5 Cifar10 (zero-shot) acc5 acc1 VOC2007 Multilabel (zero-shot) mean_avg_p CLIP-L/14 [32] UIDL ReplaceMe (LS) - 13% 13% 0.794 0.745 0.767 0. 0.609 0.620 0.956 0.996 0.927 0.939 0.996 0.996 0. 0.781 0.800 VTAB/EuroSAT acc1 0.625 0.490 0.552 acc 0.960 0.931 0.941 25% 25% UIDL ReplaceMe (LS) Table 6: Pruning CLIP vision encoder using ReplaceMe. The model performance after compressing by 13% is almost as good as the original model, while in both scenarios our method outperforms UIDL. 0.971 0.971 0.597 0. 0.381 0.395 0.814 0.823 0.693 0.780 0.418 0.471 0.515 0.556 As shown in Table 6, our proposed method preserves very strong performance on CLIP-ViT [32] even at 13% compression ratio, closely matching the original models accuracyall without requiring any additional training. While performance declines with higher compression ratios, this degradation is expected and consistent across benchmarks. Despite this, ReplaceMe consistently outperforms the training-free state-of-the-art method UIDL [10]. We also note that performance can be further improved by incorporating lightweight post-pruning healing procedure."
        },
        {
            "title": "4 Related Work",
            "content": "Model pruning [19, 12] has been at the frontier of deep-learning research since the early developments in this field. It has found practical applications not only in reducing the model size but also in enhancing the interpretability of the models under study. The same holds true for pruning large language models (LLMs). significant number of studies focuses on unstructured pruning, where individual weights within matrices throughout the model are zeroed out, resulting in sparse connections. SparseGPT [9] tackles the challenge of layer-wise reconstruction in pruning by leveraging approximations of the inverse Hessian matrix. Wanda [37] improves the SparseGPT idea of reducing computations via simplification of the Hessian approximation. The LLM Surgeon [40] uses Kronecker-factored curvature approximations to perform pruning of LLMs. Despite maintaining high model quality post-pruning, computational savings from unstructured pruning requires specialized hardware support for sparse computations, limiting its wide applicability. In contrast, structured pruning involves the complete elimination of certain structures inside the network. In this context, removing entire attention heads or MLP channels is referred to as width pruning. LLM-Pruner [25] suggested to calculate an importance metric based on the difference in the loss when this is computed with and without pruned group of weights, respectively. FLAP [1] proposed training free approach that is based on fluctuation pruning metric and an adaptive compression ratio. Another typical strategy within structured pruning, which is also the focus of this work, is depth pruning. Methods that fall in this category, aim to remove entire transformer layers of the network. In Shortened llama [15], the authors suggested to identify the significance of each decoder layer using perplexity analysis and Taylor metric. This Taylor metric is based on similar idea with the LLM-Pruner importance metirc, that is it measures the difference of the model loss when it is 10 computed with and without pruned layer. After pruning, the authors [15] further propose to do healing through LoRA fine-tuning, continual pre-training, or their combination. The authors of ShortGPT [26] introduced the Block Influence (BI) metric to quantify the contribution of each network layer. This metric corresponds to the cosine distance between the hidden states before and after the layer. After pruning, they optionally recommend retraining the model to recover the models performance. In contrast, UIDL [10] (PruneMe) suggested to compute the importance of fixed-length sequence of layers instead of computing this metric for each layer individually. They calculate the cosine distance between the input and the output of the sequence and then if the distance is below pre-defined threshold they remove the entire sequence of layers completely. Post-removal, healing with LoRA on the MLP is applied. In the recent LLM-Streamline paper [3], the authors propose to replace fixed-length sequence of layers with lightweight network, which can be either transformer layer or Feed-Forward Network (FFN). Then they train this lightweight network using the MSE loss and the LLM loss with LoRA. Recently, NVidia proposed Minitron LLM family [28] and its pruned versions that demonstrates an effective balance between depth and width pruning to mitigate performance degradation. This method calculates the importance of depth and width attributes, striving for an optimal equilibrium that minimizes performance loss. However, this method requires significant amount of data (around 100B tokens)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced novel method named ReplaceMe, which to the best of our knowledge is the first training-free depth pruning method. This technique involves substituting certain transformer blocks with linear transform, which we estimate using calibration data. ReplaceMe requires no retraining or fine-tuning, yet it consistently outperforms existing pruning techniques in training-free settings and remains competitive even when compared to approaches that rely on post-pruning healing stages. We conducted extensive experiments and outlined methodologies to accurately estimate these linear transformations using both analytical and numerical techniques. The proposed method was evaluated across range of transformer architectures including large language models and vision transformers demonstrating its robustness, adaptability, and effectiveness. These results represent significant step toward truly training-free pruning strategy."
        },
        {
            "title": "References",
            "content": "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based adaptive structured pruning for large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1086510873. AAAI Press, 2024. [2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press, 2020. [3] Xiaodong Chen, Yuxuan Hu, Jing Zhang, Yanling Wang, Cuiping Li, and Hong Chen. Streamlining redundant layers to compress large language models. In The Thirteenth International Conference on Learning Representations, 2025. [4] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 29242936. Association for Computational Linguistics, 2019. 11 [5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. [8] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88: 303338, 2010. [9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 1032310337. PMLR, 2023. [10] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers. CoRR, abs/2403.17887, 2024. [11] Shuxuan Guo, José M. Álvarez, and Mathieu Salzmann. Expandnets: Linear overparameterization to train compact convolutional networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [12] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2015. [13] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [15] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened LLaMA: simple depth pruning for large language models. 2024. [16] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 12 [17] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). [18] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785794. Association for Computational Linguistics, 2017. [19] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In Advances in Neural Information Processing Systems. Morgan-Kaufmann, 1989. [20] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. [21] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. [22] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [24] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(13):503528, 1989. [25] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [26] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv: 2403.03853, 2024. [27] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23812391. Association for Computational Linguistics, 2018. [28] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. CoRR, abs/2407.14679, 2024. [29] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, 2 edition, 2006. [30] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset, 2016. [31] Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv: 2406.17557, 2024. 13 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. [33] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, 2021. [34] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024. [35] Trond Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM Journal on Numerical Analysis, 20(3):626637, 1983. [36] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. [37] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [38] Qi Sun, Edoardo Cetin, and Yujin Tang. Transformer-squared: Self-adaptive llms, 2025. [39] S. Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(4):376380, 1991. [40] Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, and Tijmen Blankevoort. The LLM surgeon. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [41] Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. SVD-LLM: Truncation-aware singular value decomposition for large language model compression. In The Thirteenth International Conference on Learning Representations, 2025. [42] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pages 94106. Association for Computational Linguistics, 2017. [43] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: chinese language understanding evaluation benchmark. arXiv preprint arXiv:2004.05986, 2020. [44] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [45] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [46] Chujie Zheng, Minlie Huang, and Aixin Sun. Chid: large-scale chinese idiom dataset for cloze test. arXiv preprint arXiv:1906.01265, 2019. Appendix / supplemental material In this section, we revisit the central aspects of our research, beginning with closed-form solution presented in A.2. This provides foundation for examining structured linear transformations (LTs) as discussed in A.3, leading to key experimental findings. We evaluate multiple numerical solvers for optimizing the cosine objective (A.6) and investigate the behavior of structured LTs. We then assess the applicability of our method across diverse model architectures, demonstrating the effect of applied \"healing\" process to the proposed ReplaceMe. computational comparison between ReplaceMe and existing approaches, as introduced in our initial study, is also included. Further, we dissect the role of integrating linear layer as standalone block between full layer activations. This contrasts with our approach of merging LTs with MLP activation in the primary architecture, revealing that both methods yield remarkably similar results. We also analyze the impact of multi-LT transformations during training, showing how such overparameterization can enhance convergence, concept inspired by the ExpandNets [11] framework. Finally, to validate the robustness and generalizability of our method, we evaluate several models on wide range of benchmarks. These experiments demonstrate the stability and adaptability of our approach under varying conditions. A.1 Terminology and Definitions Relative Performance (RP): normalized metric quantifying performance relative to the baseline model, computed as the ratio of benchmark results between the evaluated model and the baseline. Compression Ratio: The proportional reduction in model parameters expressed as percentage, calculated as: (cid:18) (cid:19) Npruned Noriginal where Npruned and Noriginal represent the number of layers in the compressed and original models, respectively. 100 1 (10) ReplaceMe: Our proposed method for model compression, which replaces complete transformer blocks with optimized linear transformations. Average Accuracy (Avg-acc): The arithmetic mean of model accuracy scores across multiple benchmark datasets. Perplexity: metric for evaluating language model performance, defined as the exponential of the cross-entropy loss on the Lambada-openai benchmark. A.2 Closed-form solution for L2 Distance The optimization problem for the linear transform matrix estimation can be expressed as follows: = arg min (cid:0)Mi + Yi (cid:1) Li+n2 2. The objective is an Euclidean norm and thus we can expand it to: R2 2 = tr (cid:16)(cid:0)Mi + Yi Li+n (cid:1)(cid:0)Mi + Yi Li+n Mi)T + 2(Yi Li+n)Mi + = tr (cid:0)T(M (cid:1)(cid:17) i+nLi+n To minimize the objective, we take the gradient with respect to and set it to zero: 2M Mi + 2M Mi = (cid:1)1 = (cid:0)M Mi TR2 2 = 0 (Yi Li+n) = 0 (Li+n Yi) (Li+n Yi) 15 (cid:1) (11) (12) A.3 Structured LT Matrix To improve the interpretability of our approach, we further investigate additional constraints that can be imposed on the structure of the linear transformation T. The results of all the different constrained transformations are presented in Sec A.4. Motivated by the Transformers-squared work [38], we also consider the case where we condition to be diagonal matrix. This constraint is meaningful under the assumption that an adequate mapping of the activations is possible through only scaling of the hidden states. In this case, the optimization problem is of the form: = arg min TDdd (Mi + Yi)) Li+n2 2, where Dnn denotes the space of diagonal matrices of dimensions d. The corresponding closed-form solution is given by: = (cid:0)(M Mi) I(cid:1)1(cid:0)(M (Li+n Yi)) I(cid:1), (13) where denotes the Hadamard product. Another constraint that can be imposed on is the requirement for it to represent an orthonormal transformation [39]: = arg min (Mi + Yi) Li+n2 2 s.t. TT = I. This problem admits an analytical solution via singular value decomposition. Specifically, let (Li+n Yi)) = Σ V, then the optimal orthonormal matrix is given by: SVD(M = V. A.4 Results of Structured Linear Transformations Our empirical evaluation, summarized in Table 7, explores constrained forms of the linear transformation matrix Rdd. Inspired by the architectural design of Transformer-squared [38], we first examine the case where is restricted to diagonal matrix, i.e., = diag(t1, . . . , td). Although this parameterization successfully restored model functionality, the resulting perplexity remained suboptimal, with Pdiag > Pgeneric, where Pgeneric represents the baseline achieved using an unconstrained full matrix. Model LT Structure Multi-linear Transform Avg-acc Perplexity % Llama3 8B - - UIDL ReplaceMe Generic ReplaceMe Orthonormal ReplaceMe Diagonal -"
        },
        {
            "title": "ReplaceMe Generic\nReplaceMe Generic",
            "content": "non-consecutive consecutive 0.70 0.58 0.62 0.60 0.62 0.62 0.63 3.11 2216.96 21.21 700.57 89.09 16.07 18.99 100.00 82.5 89.59 85.67 88. 89.60 90.33 Table 7: Ablation study on (un)constrained LT matrix structure (Generic, Diagonal, Rotational) and multi-linear transforms. The base model is set to Llama 3 8B with 25% pruning ratio. Subsequent experiments with orthogonal transformations (where TT = I) demonstrated limited efficacy, yielding only marginal performance recovery. This indicates that actually scaling is much more important to recover model performance than rotations or reflections. More promising results emerged from employing multiple linear transformations {Ti}k examined two distinct approaches: i=1. We 1. Block decomposition: For consecutive parameter blocks, we partitioned the removed }k i=1, subsequently block into sub-blocks and estimated separate transformations {Tconsec merging the results. 2. Non-consecutive transformations: For disjoint parameter subsets, we learned independent transformations {Tdisjoint }k i=1. The experimental results indicate that applying multiple transformations yields consistent improvements in both perplexity (P) and accuracy (A) across both analytical and numerical solutions. For the analytical approach, these gains are achieved in single computation step , whereas the numerical solution requires iterative optimization, leading to significantly higher computational costs. A.5 Statistical Significance To ensure reproducibility and stability of our method we ran our method multiple times for Llama3-8B-Instruct with 8 pruned layers. While the analytical solution always leads to the same results, numerical methods might have different results. Howewer, we were able to get almost the same results: (1) the mean average accuracy over all benchmarks equals to 60.71 and the standartd deviation (std) is 0.08; (2) the mean lambada-openai perplexity equals to 15.90 and the standartd deviation (std) is 0.05. A.6 Comparative Analysis of Optimization Solvers This section investigates the effects of different optimization solvers when minimizing cosine distance objective function. Previous results relied on the Adam optimizer; here, we perform systematic comparison with alternative solvers, including: 1) Non-Linear Conjugate Gradient (NLCG), Newton-CG, and Limited-memory BFGS (L-BFGS). Additionally, we evaluate the performance of Mean Squared Error (MSE) with numerical optimization, and Least Squares (LS) with closed-form solution as baseline validation. This analysis provides insights into solver efficiency, convergence properties, and solution accuracy under different objective formulations. As demonstrated in Table 8, Solver Avg-acc RP Percentage Baseline[16] Adam[16] NLCG[29] L-BFGS[24] Trust-NCG[35] LS MSE + Adam 0.697 0.634 0.630 0.634 0. 0.624 0.624 3.106 15.875 15.960 16.200 47.000 21.206 20.633 100 90.940 90.412 90.921 88.945 89.588 89.536 Table 8: Comparison between different solvers to estimate the linear transform for Llama 3 8B after 25% compression with cosine distance objective, and sanity check between MSE with Adam solver as numerical solution and LS analytical solution nearly all solvers converge to similar point, with results on both benchmarks and perplexity metrics being very close. An exception is the Trust-NCG solver, which got trapped in local minimum. Furthermore, theres clear indication that solving for MSE, whether numerically or analytically, yields nearly identical outcomes but analytical solution is much faster and requires much less hardware requirements. A.7 Generalization Across Model Scales While the primary results presented in this work focus on subset of model architectures, these findings may not fully capture the scaling behavior across broader parameter spectrum, where model sizes can vary from (1) billion to (70) billion parameters or beyond. To rigorously investigate the scaling properties of our approach, we conduct extensive experiments across the following benchmark suite: (Winogrande [33], BoolQ [4], OpenBookQA [27], SciQ [42], Race [18] and PIQA [2]). Our scaling analysis spans Llama 3 architecture family, from the compact Llama 3.2 (1B parameters) to the largest available variant, Llama 3 (70B parameters). As evidenced in Table 9, we observe strong correlation between model size and achievable compression ratio η. Specifically, for large-scale models (70B parameters), we achieve optimal compression ratios of ηmax = 37.5% while maintaining performance retention RP 90% of the original models capability. 17 Model Avg-Acc RP Compression Ratio Llama-3.2-1B-Instruct Llama-3.1-8B-Instruct Baseline ReplaceMe Baseline ReplaceMe 0.7118 0.6098 0.5348 0.6537 Llama-3-70B-Instruct Baseline ReplaceMe ReplaceMe 0.728 0.7036 0. - 0.8771 25% - 0.9184 25% 0.9664 25% - 0.9060 37.5% Table 9: Results of utilizing ReplaceMe on various model sizes, characterized by the number of parameters, for the Llama 3 Families. A.8 Computational Efficiency Analysis This section presents quantitative comparison between our training-free method and the baseline UIDL framework, with focus on computational overhead, energy consumption, and associated CO2 emissions. As demonstrated in the primary results, our approach achieves competitive performance despite being training-free, whereas UIDL requires fine-tuning on limited dataset. To ensure an equitable comparison, we adopt this configuration to rigorously evaluate the computational advantages of our method. As demonstrated in Figure 5, comparative analysis of our two proposed Figure 5: Comparison between LS and Cosine in terms of computation and environmental impact. methodologies is presented. The LS approach offers significant advantages in terms of cost-efficiency, speed, and the reduced demand for computational resources and memory. However, this method incurs slight decline in performance metrics. Thus, the choice between these methods should be informed by the specific requirements and available hardware resources of the user. A.9 Mergable LT vs. LT as an Independent Block Objective Avg-accuracy LT Fusable into MLP Cosine Separate LT block Cosine 0.634 0.640 Perplexity RP 15.875 13. 0.905 0.913 Table 10: Compressing Llama 3 8B by 25% using cosine distance objective, without applying healing. This involves applying LT on the MLP output or on the full output activation of the transformer block. In our research, we propose integrating the linear transformation (LT) matrix such that it can be merged with the down-projection component of the MLP layer, preserving architectural compatibility with standard LLM designs. However, this approach raises critical question regarding the comparative efficacy of injecting the LT as mergable layer versus incorporating it as an independent, non-mergable block within the transformer architecture. In Table 10, we present the outcomes of integrating the linear transform (LT) directly into the Multi-Layer Perceptrons (MLP) down-projection through matrix fusion. This approach ensures parameter efficiency in contrast to employing standalone LT Block, where the LT functions as an independent layer between transformer blocks, thereby introducing additional computational overhead. The results demonstrate marginal improvement; however, the extent of this improvement is contingent upon user-specific requirements. 18 A.10 Block Selection Analysis In this section, we conduct systematic ablation study to investigate the relationship between inter-layer activation distance metrics and model performance under sequential layer removal. We employ sliding window approach, iteratively pruning contiguous blocks of eight layers starting from the initial layer index and incrementally shifting the window by single layer position. For each configuration, we quantify activation dissimilarity using both cosine distance and L2 norm , computed between intermediate activations of the original and pruned models. Subsequently, for our method we estimate the linear transformation (LT) and evaluate the pruned architectures on standardized subset of benchmark tasks. As demonstrated in Fig. 6, we observe strong inverse Figure 6: Comparative analysis of distance metrics and predictive accuracy across layer pruning configurations. Trendlines illustrate the inverse correlation between cosine distance and accuracy, contrasted with the positive correlation between L2 and accuracy degradation . Results are shown for ReplaceMe with LT estimation via Cosine/LS metrics and UIDL baselines. correlation between cosine distance reduction and accuracy improvement across all methodologies. Specifically, ReplaceMe with cosine-based LT estimation achieves peak performance at the lowest cosine distance values, Conversely, configurations exhibiting lower L2 correspond to significant accuracy degradation suggesting that L2 lacks power for optimal layer selection. A.11 Cosine Distance Approximation In this section, we will discuss further our proposed approximation of the loss, where we subtract the attention output from the activation after the cut, when using the cosine distance. First, we revisit the original Eq. (14) cosine_distance (Mi + Yi, Li+n) , (14) = arg min where we observe that in order to estimate numerically we need to store 3 activations (Mi, Yi, Li+n) for each token. This is not effective and requires considerable memory and time to compute. To overcome this issue, we apply the cosine distance after subtracting the attention output from the full activation at the end-block of the cut as formulated in Eq.(15): = arg min cosine_distance (Mi T, Li+n Yi) . (15) As evidenced by the experimental results presented in Table 11, the approximated cosine formulation achieves comparable performance to the exact loss computation, with marginal improvements 19 Table 11: Comparison between accurate cosine distance and approximated version. cosine loss (m, y) denotes that we subtracted the attention output of ith block from the activation of the(i + n)th block. Model Method Pruned Layers Calibration Data Training State Perplexity Avg-acc Llama 3 8B instruct Llama 3 8B instruct cosine loss (m, ly) cosine loss (m + y, l) Qwen 2.5 7B instruct Qwen 2.5 7B instruct cosine loss (m, ly) cosine loss (m + y, l) Llama 3 8B instruct Llama 3 8B instruct Multi_A cosine loss (m, ly) Multi_A cosine loss (m + y, l) 8 8 7 7 8 8 slim_orca slim_orca slim_orca slim_orca slim_orca slim_orca no training no training no training no training no training no training 15.88 16.63 7.92 10. 13.95 13.12 0.634 0.630 0.591 0.580 0.628 0.630 observed. This approximation yields memory efficiency gains, requiring only two stored activations per token (Mi, Li+n Yi) , thereby reducing memory usage by approximately 66% relative to the original implementation."
        }
    ],
    "affiliations": [
        "Archimedes Athena RC",
        "IACM-Forth",
        "IITU",
        "ITMO University",
        "MTS AI",
        "Polynome",
        "University of Crete"
    ]
}