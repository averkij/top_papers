{
    "paper_title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "authors": [
        "Mingzhe Du",
        "Anh Tuan Luu",
        "Bin Ji",
        "Xiaobao Wu",
        "Dong Huang",
        "Terry Yue Zhuo",
        "Qian Liu",
        "See-Kiong Ng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 9 2 1 0 . 3 0 5 2 : r CodeArena: Collective Evaluation Platform for LLM Code Generation Mingzhe Du1,2, Anh Tuan Luu1, Bin Ji2, Xiaobao Wu1, Dong Huang3, Terry Yue Zhuo4, Qian Liu5, See-Kiong Ng2 1Nanyang Technological University, 2National University of Singapore, 3The University of Hong Kong, 4Monash University, 5ByteDance. {mingzhe001,anhtuan.luu,xiaobao002}@ntu.edu.sg, {jibin,seekiong}@nus.edu.sg, dhuang@cs.hku.hk, terry.zhuo@monash.edu, liuqian@bytedance.com"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and thereby substantially programming syntax, boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede timely and accurate assessment. To address these limitations, we introduce CodeArena1,2, an online evaluation framework tailored for LLM code generation. The key innovation is collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) collective evaluation system for unbiased assessment, (2) public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration. Figure 1: The CodeArena framework allows users to interact with the system through APIs. The depicted workflow shows the code submission process."
        },
        {
            "title": "Introduction",
            "content": "Leveraging the exceptional language comprehension and generation capabilities of large language models (LLMs), automatic code generation has significantly transformed the landscape of software development (Lozhkov et al., 2024; Roziere et al., 2023; Zhu et al., 2024). By interpreting natural language instructions, LLMs can now directly generate codes, introducing new efficiencies and possibilities in the software development process. To evaluate the performance of LLMs in code generation, various benchmarks have emerged that assess 1Website: https://codearena.online 2Demo Video: https://youtu.be/yqF9Cdrh3ss the generated code from multiple perspectives. For instance, HumanEval (Chen et al., 2021) and its successors (Liu et al., 2023; Zhuo et al., 2024) are widely used to assess the functional correctness of LLM-generated codes. Beyond the functional correctness, Mercury (Du et al., 2024) and EffiBench (Huang et al., 2024) provide benchmarks to assess the efficiency of LLM-generated code, while CyberSecEval (Bhatt et al., 2023, 2024) quantifies LLM security risks. Furthermore, online judge (OJ) platforms, such as LeetCode (LeetCode, 2024) and CodeForces (Codeforces, 2024), offer online code assessment services, enabling code evaluation and profiling against predefined test cases across various programming languages. 1 Although existing evaluation approaches have achieved great success, they have three limitations: (1) Benchmark Contamination. Leakage of benchmark data into LLM training datasets can result in contamination, causing LLMs to perform abnormally on benchmarks (Jain et al., 2024). Regularly importing new problems into the evaluation can alleviate this issue. However, given the static and offline nature of most code evaluation benchmarks, it is hard to distribute the up-to-date benchmark to each LLM and dynamically get the realtime performance evaluation. Moreover, current benchmarks for LLM code generation predominantly evaluate individual models in isolation, neglecting holistic factors. For instance, the difficulty of problems is typically defined subjectively by human data curators, which may not accurately represent the true challenge posed to LLMs. (2) Data Dissipation. Most existing benchmarks merely record the final metrics, while discarding the generated code solutions. Similarly, many online platforms do not make user-submitted solutions publicly accessible (LeetCode, 2024; DMOJ, 2024). However, such solution data is crucial for advancing LLM code generation research. For example, to evaluate the execution efficiency of LLM-generated code, the Mercury benchmark requires sufficient amount of solutions to analyze the distribution of execution times (Du et al., 2024). Additionally, fine-tuning the code generation capabilities of LLMs necessitates substantial dataset of problem, solution pairs as well. (3) System Accessibility. Current code generation benchmarks employ disparate evaluation protocols, often necessitating local execution or manual submission to leaderboards (Zhuo et al., 2024; Chen et al., 2021; Liu et al., 2024). This complexity not only complicates model evaluation but also makes it unattainable to keep pace with the rapid LLM advancements. Although OJ platforms, such as Leetcode and DMOJ, offer unified online code evaluation services, they lack automation-friendly application programming interfaces (APIs) for submitting LLM-generated code. Consequently, researchers are compelled to use automation testing tools like Selenium to submit code to these platforms (Du et al., 2024; Huang et al., 2024), impeding rapid model evaluation. To address these challenges, this paper introduces CodeArena, an online evaluation framework tailored for LLM code generation. Regarding the data contamination issue, CodeArena proposes novel dynamic scoring mechanism instead of merely relying on the integration of new problems. The newly introduced metric, Dynamic Point, assigns rewards to each accepted solution in way that ensures even widespread leakage of an evaluation problem has minimal impact on the benchmark results. This approach effectively mitigates the influence of data contamination. In addition to serving as an assessment platform, CodeArena functions as solution repository. Rather than discarding submitted solutions after evaluation, CodeArena systematically records them and makes them publicly accessible. Moreover, to facilitate seamless user interaction, CodeArena offers suite of automation-friendly APIs. The main contributions are summarized as follows: 1) Dynamic Evaluation. We introduce CodeArena, an OJ framework that periodically integrates novel coding tasks to ensure they remain uncontaminated, and dynamically adjusts scoring metrics to effectively evaluate the code generation capabilities of LLMs. 2) Open Data Repository. All solutions and test cases are publicly accessible, prompting an open-source environment conducive to analyzing and improving LLM code generation. 3) Automation-friendly APIs. We provide APIs designed to streamline the automated code evaluation process, facilitating efficient user interaction."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Code Assessment Platforms LeetCode (LeetCode, 2024) is prominent online coding platform that offers an extensive array of problems across diverse domains such as algorithms, data structures, databases, and system design. The platform provides instant feedback and detailed analysis of code performance, enabling users to iteratively refine their solutions. Similarly, CodeForces (Codeforces, 2024) is another well-regarded competitive platform, renowned for its regular contests and vast, crowd-sourced collection of programming problems. Unlike these closed-sourced platforms, DMOJ (DMOJ, 2024) provides an open-source OJ framework, which includes the front-end user interface, runtime environments, and API endpoints. Despite offering plentiful coding evaluation resources, these platforms are not designed for automated LLM submissions. CodeArena bridges this gap by integrating these resources and providing automation-friendly APIs specifically for evaluating LLM-generated code. 2 2.2 Code Generation Benchmarks Most code generation benchmarks adopt fuzzing methodology (Zhu et al., 2022; Hendrycks et al., 2021), where predefined test cases are executed on the generated code, and the outputs are compared to expected results. For example, HumanEval (Chen et al., 2021) comprises 164 handcrafted programming problems and emphasizes the functional correctness of generated code. BigCodeBench (Zhuo et al., 2024) extends this evaluation framework by including more complex instructions and diverse function calls, thus testing the true programming capabilities of LLMs in realistic scenarios. LiveCodeBench (Jain et al., 2024) takes step further by continuously updating its problem set, ensuring contamination-free evaluations. Additionally, recognizing the gap in evaluating computational efficiency, Mercury (Du et al., 2024) introduces an efficiency-centric benchmark that considers the holistic runtime distribution, thereby assessing both the correctness and efficiency simultaneously."
        },
        {
            "title": "3 Code Arena",
            "content": "As depicted in Figure 1, CodeArena is an online code evaluation platform built upon an open-source OJ framework DMOJ (DMOJ, 2024). The platform is structured into four distinct layers: The API Layer provides set of APIs to facilitate user interactions. The Runtimes Layer offers standardized environment for code execution and evaluation. The Dynamic Evaluation Layer processes execution results from the Runtimes Layer and dynamically updates ranking scores after each submission. Finally, the Data Layer stores problems, test cases, and solutions. In this section, we will delve into the CodeArena framework breakdown (Section 3.1) and the detailed workflows (Section 3.2). 3.1 Framework Breakdown API Layer. While existing OJ platforms like LeetCode and DMOJ offer online code assessment services, significant limitation for LLM researchers is the lack of automation-friendly APIs. Researchers are compelled to harness automation testing tools to submit LLM-generated code, which can be cumbersome. To address this, CodeArena provides an automation-friendly interface via set of REST APIs (Rodríguez et al., 2016) and dedicated Python library, codearena3, enabling streamlined code submission to our platform. As 3https://pypi.org/project/codearena/ illustrated in Figure 2, CodeArena offers endpoints for Authentication, Problem, and Ranking utilizing standard RESTful API methods GET ((cid:1)) and POST ((cid:3)) (Richardson and Ruby, 2008): (cid:1) Authentication (/api/authentication/): To ensure secure submissions and data retrieval, we require all registered users to generate an API Token to access CodeArena. The API Token can be revoked and regenerated as necessary. (cid:1) Problem Creation (/api/problem/): We encourage the submission of new problems to diversify the problem set. Authorized benchmark curators can manage and distribute new problems via this API. For instance, LiveCodeBench (Jain et al., 2024) can regularly submit new problems to CodeArena, and the platform will automatically test and update the ranking of all code generator users with these new problems. (cid:1) Test Case Creation (/api/problem/<pid> /case): High-quality test case collection is challenging, as most OJ platforms do not release the test cases used for problem assessment. To solve this issue, Du et al. (2024) and Huang et al. (2024) utilize GPT-4 (Achiam et al., 2023) to write test case generators. In our work, we follow the same way to gather initial test cases for each problem and encourage users to upload their own test cases. Here, pid denotes the specific problem ID. (cid:1) Solution Submission (/api/submission): Code Generator users can submit their generated code for specific pid problem via this API. CodeArena executes the submitted code in sandbox environment and returns submission_id to the user immediately. Users can further check the detailed status and performance data through the Solution Retrieval interface. (cid:3) Problem Retrieval (/api/problem/): This API has two variants: /api/problem/ lists all problems with their corresponding IDs, whereas /api/problem/<pid>/ provides detailed information, such as problem descriptions and acceptance statistics, for specific problem pid. (cid:3) Submission Retrieval (/api/problem/<pid> /submission/): Similar to problem retrieval, submission retrieval has two variants: /api/ submission/ lists all submissions, and /api/ submission/<sid> provides detailed runtime information for specific submission sid. (cid:3) Ranking Retrieval (/api/ranking): This endpoint returns real-time ranking results in JSON format, identical to those shown on https:// codearena.online/users/. Figure 2: Overview of CodeArena. The Green component provides runtime environments for programming languages, capable of accepting either generated code or model prompt as the input, and outputting test results. The Yellow component is the dynamic evaluation unit, updating the LLM weighted ranking score based on each submission result. The Blue and Maroon components are RESTful API GET ((cid:1)) and POST ((cid:3)) calls, respectively. Runtimes Layer. To ensure the stable and secure execution of code submissions, CodeArena operates within an isolated sandbox runtime environment. This environment currently supports multiple programming languages, including Python 3, C, C++, Go, and Haskell, while holding the flexibility to integrate additional languages as needed. The runtime system reports both running time and memory overhead for each submission, and it raises exceptions and provides detailed error information if code submission fails to execute properly. The CodeArena runtime environment accommodates two types of inputs: Code runtime directly accepts and executes code submitted by code generator. Prompt runtime is designed for interactions with LLMs. Instead of submitting raw code, code generators provide model prompt. The runtime then uses this prompt to invoke the appropriate LLM locally, and the generated code is subsequently executed within the sandbox. Dynamic Evaluation Layer. Solving problems with varying difficulty levels should contribute accordingly to the ranking score. However, the difficulty of most benchmark problems is typically determined subjectively by data curators, which may not accurately represent the challenges posed to LLMs. As illustrated in Figure 5, the acceptance rate (AC) does not show significant variation across difficulty levels. To rectify this discrepancy, we propose the Challenge Score (CS): CS = BPS (1 ACi), (1) where BPS represents the basic problem score /Stotal of the i-th problem, and ACi = Ssolved denotes the proportion of solved problems. Essentially, all participating users share the BPS i. Resolving an easy problem that most users can solve yields minimal bonus, whereas solving challenging problem earns higher CS i. For instance, consider problem worth 5 points: if only one LLM successfully solves it, that model receives the full 5 points. However, if all LLMs solve the problem, indicating either widespread leakage or lack of discriminatory difficulty, the 5 points are distributed evenly among them. This ensures that leaked or overly simplistic problems have minimal influence on the overall leaderboard, effectively mitigating the risk of data contamination. Moreover, CodeArena also considers the Efficiency Score (ES i) of the generated code by calculating the runtime percentile of current solution (srt ) over to the runtime of other solutions (srt ): ES = sj srt srt , sj SSolved Ssolved . (2) Therefore, the final Dynamic Point (DP) for each user is given by: DP = (cid:88) (CS + ES i), i=0 (3) where is the problem number. We record the Dynamic Point ranking regularly to observe the performance trending of each user. Data Layer. In addition to evaluating code generation capabilities, CodeArena is envisioned as comprehensive open-source data platform. Its data layer is structured to store rich metadata for each problem, accompanied by diverse collection of solutions with detailed execution overhead metrics. This robust dataset serves as foundation for analyzing model performance trends and fostering advancements in code generation LLMs. 4 Figure 3: Example of Dynamic Point (DP) calculation. Each individual model score is influenced by the overall system performance. CS and ES are counted only when the model passes () all test cases. 3.2 Workflows In this section, we outline the workflow for each CodeArena user group: Benchmark Curators, Code Generators, and Data Readers. Each user group is assigned specific tasks and granted distinct system permissions. detailed definition of these user groups is provided in Appendix D. Problem Collection. To diversify our problem set and prevent benchmark leakage, we developed workflow for Benchmark Curators. This workflow integrates existing code evaluation datasets, such as Mercury (Du et al., 2024) and APPS (Hendrycks et al., 2021), through dedicated scripts and can easily incorporate other benchmarks with structured problem descriptions and test cases. For online coding platforms, we primarily collect source problems from weekly contests on CodeForces and LeetCode. To ensure practicality, we have implemented scheduled task that automatically collects problems from these platforms on monthly basis. Test Case Generation. Since most online coding platforms do not disclose test case data, We develop an automated test case generation workflow for Benchmark Curators to address this limitation. After gathering these problems regularly, we employ GPT-4o to generate corresponding test case generators for each problem. For instance, consider the example problem: Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. For this problem description, GPT-4o is able to return test case generator function similar to the example provided below. Building on the mechanism introduced in Mercury (Du et al., 2024), we generate diverse test cases by randomly invoking the test_case_generation function, as shown below. These test cases are subsequently fed into canonical code solutions to ensure they can process the inputs and produce consistent outputs. To prevent ambiguities where multiple outputs might be valid, we filter out questions that yield inconsistent outputs across different solutions for the same input. For instance, problems that allow answers in any order can complicate the evaluation process, making such cases unsuitable for inclusion. from random import randint , shuffle def test_case_generation () : = randint (2 , 10 ** 4) v1 = randint (- 10 ** 9 , 10 ** 9) v2 = randint (- 10 ** 9 , 10 ** 9) target = v1 + v2 nums = [v1 , v2 ] while len ( nums ) < : = randint (- 10 ** 9 , 10 ** 9) if ( target - not in nums ): nums . append ( new_val ) shuffle ( nums ) return nums Code Submission. As shown in Figure 1, the workflow for Code Generators comprises the 1) Problem Retrieval. following steps: Code Generator initiates the workflow by calling the /api/problem/ Get API, which retrieves the problem description. 2) Code Generation. Upon receiving the problem description, the Code Generator invokes the corresponding LLM and produces candidate solution for the given problem. 3) Solution Submission. The user submits the solution by calling the /api/submission Post method. Upon receiving the submission, Table 1: Leaderboard shows the code generation performance of leading open-source () and closedsource () LLMs as of July 30, 2024. DP stands for Dynamic Points, and the Pass score reports the percentage of solved problems out of total problems. Rank Model Name DP Pass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DeepSeek-Coder (Zhu et al., 2024) GPT-4o (Achiam et al., 2023) Claude-3-5-sonnet (Anthropic, 2024) Gemini-1.5-flash (Team et al., 2023) DeepSeek-Coder-V2-Lite (Zhu et al., 2024) Claude-3-Opus (Anthropic, 2024) Gemini-1.5-pro (Team et al., 2023) Llama-3.1-8B (Touvron et al., 2023) Llama-3-8B (Touvron et al., 2023) GPT-4-Turbo (Achiam et al., 2023) GPT-3.5-Turbo (Achiam et al., 2023) Mistral-Nemo (Jiang et al., 2023) CodeLlama-13b (Roziere et al., 2023) Claude-3-Haiku (Anthropic, 2024) Mistral-7B-v0.3 (Jiang et al., 2023) Codestral-22B-v0.1 (Jiang et al., 2023) Claude-3-sonnet (Anthropic, 2024) CodeLlama-34b (Roziere et al., 2023) CodeLlama-7b (Roziere et al., 2023) 249.28 247.32 227.87 225.67 223.67 221.93 209.16 177.34 164.51 160.55 157.70 141.78 123.15 100.37 77.43 77.43 56.17 53.83 50.38 90.63% 89.06% 74.22% 73.05% 71.24% 69.92% 61.72% 46.09% 40.63% 34.38% 33.98% 29.30% 25.39% 18.75% 14.84% 14.84% 8.98% 8.98% 6.25% CodeArena immediately returns submission_id to the user for tracking the submission status. 4) Isolated Execution. Subsequently, the submitted solution is executed against predefined test cases within an isolated sandbox. 5) Solution Persistence. The results of the solution execution, including whether it passed or failed each test case along with any associated performance metrics, are saved in the data layer. 6) Dynamic Evaluation. The dynamic evaluation layer processes the execution results and updates the dynamic points for the submission. 7) Submission Status. The user can query the status of the submission with submission_id. Detailed API usage instructions can be found in the documentation on our website."
        },
        {
            "title": "4 Results and Discussion\n4.1 Benchmarks",
            "content": "the initialize platform, we To imported APPS (Hendrycks et al., 2021) and Mercury (Du et al., 2024) benchmarks to evaluate each Code Generator (LLMs listed in Table 1). Notably, CodeArena has sufficient flexibility to accommodate arbitrary LLM code generation benchmarks (See Section 3.2) and offers online distribution and evaluation services. 4.2 Model Performance In the CodeArena formal leaderboard, each LLM Code Generator is allowed single attempt per problem, ensuring that dynamic point rankings are not skewed by excessive or irresponsible submissions. For demonstration purposes, we preDP 240 220 200 180 140 07/30 08/30 09/30 10/30 11/ CP DeepSeek-Code GPT-4o Claude-Sonnet Claude-Opus Gemini-Pro Gemini-Flash Llama-3.1-8B Llama-3-8B DeepSeek-V2-Lite Figure 4: We trace Dynamic Point (DP) changes of prominent open-source () and closed-source() LLMs over checkpoint (CP) from 30 July to 30 Nov, 2024. registered Code Generators for several prominent LLMs and submitted their generated solutions to CodeArena. Detailed model inference settings are provided in Appendix C. As shown in Table 1, most closed-source LLMs adhere to the scaling law, significantly outperforming their open-source counterparts. However, open-source LLMs do not consistently demonstrate improved performance with larger parameter scales. Notably, DeepSeekCoder-V2-Lite achieves the highest performance despite its relatively small parameter scale. 4.3 Dynamic Point Changes We analyze the changes in Dynamic Points (DP) of prominent open-source () and closed-source () LLMs across checkpoints (CP) from July 30 to November 30, 2024. Compared to closedsource LLMs, open-source LLMs exhibit clear downward trend in DP scores over time checkpoints, with \"DeepSeek-V2-Lite\" experiencing the most significant decline. In contrast, closed-source LLMs maintain stable DP scores throughout the evaluation period, even showing some improvement in the final checkpoint."
        },
        {
            "title": "Limitations",
            "content": "While CodeArena significantly advances the evaluation of LLM code generation, it has limitations. It relies on external data sources like LeetCode and CodeForces, leading to issues with availability and inconsistent problem quality. Additionally, the evaluation quality depends on test cases generated by automated tools like GPT-4 (Achiam et al., 2023), which may not always produce exhaustive test cases. In summary, CodeArena is major step forward, but it requires ongoing refinements to address these limitations."
        },
        {
            "title": "Ethics Statement",
            "content": "and Copyright The Data Management CodeArena platform upholds the highest standards in data management and copyright compliance. To ensure ethical fair use, we strictly adhere to copyright laws by using only original problems or those for which we have obtained the necessary permissions from their respective authors, ensuring they are not used for commercial purposes. We encourage researchers to utilize the platform and respect the intellectual property rights associated with all provided materials. Fairness Evaluation Ensuring fairness in the evaluation of LLM-generated code is core principle of CodeArena. We employ unified prompt to invoke both open-source and closed-source LLMs within standardized local environment to avoid inconsistencies in the evaluation process. Additionally, CodeArena maintains an open data policy where all solutions and test cases are publicly accessible. This transparency allows the research community to scrutinize and enhance evaluation methodologies, ensuring ongoing fairness and objectivity in the benchmarking process."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anthropic. 2024. Claude models. Manish Bhatt, Sahana Chennabasappa, Yue Li, Cyrus Nikolaidis, Daniel Song, Shengye Wan, Faizan Ahmad, Cornelius Aschermann, Yaohui Chen, Dhaval Kapil, et al. 2024. Cyberseceval 2: wide-ranging cybersecurity evaluation suite for large language models. arXiv preprint arXiv:2404.13161. Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, et al. 2023. Purple llama cyberseceval: secure coding benchmark for language models. arXiv preprint arXiv:2312.04724. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Codeforces. 2024. Codeforces. DMOJ. 2024. Github - dmoj/online-judge: modern open-source online judge and contest platform system. Mingzhe Du, Anh Tuan Luu, Bin Ji, and See-Kiong Ng. 2024. Mercury: An efficiency benchmark for llm code synthesis. arXiv preprint arXiv:2402.07844. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge competence with apps. NeurIPS. Dong Huang, Jie Zhang, Yuhao Qing, and Heming Cui. 2024. Effibench: Benchmarking the efficiency of automatically generated code. arXiv preprint arXiv:2402.02037. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. LeetCode. 2024. Leetcode - the worlds leading online programming learning platform. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36. 7 Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173. Leonard Richardson and Sam Ruby. 2008. RESTful web services. \" OReilly Media, Inc.\". Carlos Rodríguez, Marcos Baez, Florian Daniel, Fabio Casati, Juan Carlos Trabucco, Luigi Canali, and Gianraffaele Percannella. 2016. Rest apis: largescale analysis of compliance with principles and best practices. In Web Engineering: 16th International Conference, ICWE 2016, Lugano, Switzerland, June 6-9, 2016. Proceedings 16, pages 2139. Springer. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931. Xiaogang Zhu, Sheng Wen, Seyit Camtepe, and Yang Xiang. 2022. Fuzzing: survey for roadmap. ACM Computing Surveys (CSUR), 54(11s):136. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877."
        },
        {
            "title": "A Proprietary Model List",
            "content": "For closed-source LLMs, we utilize the respective provided APIs as shown in Table 2. Table 2: Closed-source models and their API links Model Name API Link DeepSeek-Coder GPT-4o Claude-3-5-sonnet Gemini-1.5-flash Claude-3-Opus Gemini-1.5-pro GPT-4-Turbo GPT-3.5-Turbo Claude-3-Haiku Claude-3-sonnet https://chat.deepseek.com https://chatgpt.com https://www.anthropic.com https://gemini.google.com https://www.anthropic.com https://gemini.google.com https://chatgpt.com https://chatgpt.com https://www.anthropic.com https://www.anthropic.com Problem placeholder represents the actual problem that needs to be solved by the LLM. By maintaining uniform prompt structure, we minimize the variability introduced by different interpretation styles of LLMs. This standardized approach ensures that each LLM is assessed on an equal footing, facilitating fair comparison of their coding capabilities."
        },
        {
            "title": "C Model Inference",
            "content": "For closed-source LLMs, we utilize the respective provided APIs (see Appendix A). For open-source LLMs available on HuggingFace 4, we employ the text-generation pipeline with temperature of 0.7. To achieve balance between inference efficiency and precision, we specifically use models formatted in bfloat16. All model inferences are conducted locally on 8 NVIDIA A100 GPUs."
        },
        {
            "title": "D User Groups",
            "content": "To ensure fair evaluation across all LLMs, we devise unified prompt for both open-source and closed-source LLMs. This consists of two components: system prompt and an inference prompt. System Prompt You are coding expert. You response in Pure Python code only (explicitly import all libraries). Consider each input is string, so use eval to parse these inputs, and use * to decouple arguments. Inference Prompt Example: {Example Problem Description} {Example Solution} Given the example coding style, write the solution for the following problem. Please ONLY generate the code solution (explicitly import all libraries). {Problem} The inference prompt The system prompt establishes the general instructions that guide LLMs to generate code solutions. is one-shot template with placeholders. Here, the Example Problem Description serves as placeholder for the example problem statement, while the Example Solution provides an example solution. The 9 In CodeArena, users are categorized into three distinct groups, each granted specific API permissions: Benchmark Curators, Code Generators, and Data Readers. Benchmark Curators are pivotal in maintaining the quality of the problem repository. They are tasked with creating, refining, and expanding the set of problems available on the platform. This role involves both developing new problems and curating comprehensive test cases to ensure the problems are sufficiently challenging and evaluative. In the current configuration, the administrator fulfills the role of benchmark curator. Code Generators can be either code-generation LLMs or human programmers. To maintain fairness and distinguish between these sub-groups, CodeArena registers dedicated account for selected code generation LLMs. Each LLM user is allowed single attempt to solve each problem. In contrast, human users are granted unlimited attempts to solve problems, and all their solutions are stored in the data repository. Data Readers encompass all users interested in accessing the solution repository on the platform. These users are granted to retrieve all solution data, which is invaluable for conducting model performance analysis. To facilitate exploration, we provide trial account (Account: Test / Password: Haveatry!) for anyone interested in browsing our data. 4https://huggingface.co/ Figure 5: Acceptance Rate (AC) distribution of problems clustered by the original difficulty levels inherited from Leetcode (LeetCode, 2024). The X-axis represents individual problems grouped by their difficulty levels, while the Y-axis indicates the AC of each problem. AC does not exhibit clear differentiation across difficulty levels."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Monash University",
        "Nanyang Technological University",
        "National University of Singapore",
        "The University of Hong Kong"
    ]
}