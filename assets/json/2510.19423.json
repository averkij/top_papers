{
    "paper_title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration",
    "authors": [
        "Jia-Kai Dong",
        "I-Wei Huang",
        "Chun-Tin Wu",
        "Yi-Tien Tsai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in isolation, ignoring challenges such as functional overlap and cross-server orchestration, leading to overly optimistic assessments. MSC-Bench addresses these gaps by constructing ground truth through 'equal function sets', allowing objective metrics such as F1 score and reducing the dependency on LLM-as-a-judge evaluation. Organized as a five-level curriculum, it systematically tests agent capabilities from single-tool orchestration to complex cross-server planning, and robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. The benchmark and resources are publicly available at https://github.com/snooow1029/MSC_Bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 3 2 4 9 1 . 0 1 5 2 : r MSC-Bench: Rigorous Benchmark for Multi-Server Tool Orchestration Jia-Kai Dong,, I-Wei Huang, Chun-Tin Wu, Yi-Tien Tsai National Taiwan University *b11901067@ntu.edu.tw These authors contributed equally to this work"
        },
        {
            "title": "Abstract",
            "content": "We introduce MSC-Bench, large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in isolation, ignoring challenges such as functional overlap and cross-server orchestration, leading to overly optimistic assessments. MSCBench addresses these gaps by constructing ground truth through equal function sets, allowing objective metrics such as F1 score and reducing the dependency on LLM-as-ajudge evaluation. Organized as five-level curriculum, it systematically tests agent capabilities from single-tool orchestration to complex cross-server planning, and robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-ofthe-art agents exhibit systemic weaknesses in robustness. MSC-Bench provides diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. The benchmark and resources are publicly available at https: //github.com/snooow1029/MSC_Bench."
        },
        {
            "title": "Introduction",
            "content": "The Problem: Evaluating Tool-Using Agents in Real-World Scenarios The augmentation of Large Language Models (LLMs) with external tools has become significant research direction, transforming them from text generators into agents capable of interacting with digital environments (Qin et al., 2023; Yao et al., 2023). new architectural paradigm is emerging with the Model-Context Protocol (MCP), which organizes tools into semantically coherent, independently operating servers (Anthropic, 1This paper is under ARR review (ACL Rolling Review, October 2025). 2024). This federated architecture, analogous to the internet, shifts the agents task from simply calling an API in flat namespace to orchestrating workflows across distributed network. While prior work has shown that hierarchical organization can improve tool retrieval (Qin et al., 2023; Du et al., 2024), the evaluation of end-to-end agent performance in this explicit server-centric reality remains underexplored. Current Gaps in Existing Benchmarks Despite rapid progress, current benchmarks for tool-augmented agents suffer from three fundamental gaps. First, there is an architectural mismatch: most large-scale benchmarks, such as Seal-Tools and NESTful, model tools as vast, unstructured namespace (Wu et al., 2024; Basu et al., 2025). This fails to test an agents ability to navigate the hierarchical, multi-server structure central to the MCP paradigm. For instance, consider task requiring data from Database Server to be processed by Analytics Server: while both servers may have compatible tools, the agent must understand server boundaries, handle potential connection failures, and orchestrate the workflow across different contextschallenges that flat-space benchmarks entirely miss. Second, benchmarks struggle with the pervasive challenge of functional overlap, where multiple tools can achieve the same outcome. Some, like ToolHop, meticulously design tasks to avoid this overlap, which limits real-world applicability (Ye et al., 2025). Others (Mo et al., 2025; Yin et al., 2025; Guo et al., 2025) rely on an LLM-as-a-judge for evaluation, method known to be costly and susceptible to systemic biases that harm reproducibility (Li et al., 2025). Third, existing evaluation pipelines are often fragmented and incomplete. The modern tool calling system comprises retriever and an LLM reasoner, but benchmarks tend to evaluate these components in isolation. For example, ToolRet focuses exclusively on retrieval perFigure 1: MSC-Bench Overview. The benchmark evaluates end-to-end tool orchestration within federated MCP ecosystem, featuring 491 servers and 2,375 tools across five complexity levels. formance (Shi et al., 2025), while benchmarks such as NestTools and Seal-Tools evaluate the downstream LLMs reasoning with fixed, golden retriever (Han et al., 2025; Wu et al., 2024). This fragmentation provides an incomplete picture of the end-to-end performance of complete orchestrator. Table 1 summarizes these limitations across existing benchmarks, revealing that none provide comprehensive evaluation of both architectural realism (MCP structure) and end-to-end capabilities (LLM+Retriever). Figure 1 illustrates how MSCBench addresses these fundamental limitations by providing unified, end-to-end evaluation framework within realistic MCP ecosystem, contrasting with the fragmented and architecturally misaligned approaches of existing benchmarks. Our Solution: MSC-Bench To address these gaps, we introduce MSCBench, benchmark designed to evaluate endto-end multihop tool orchestration within an explicit MCP ecosystem with 491 servers and 2375 tools. Our key innovation is novel methodology that handles functional overlap by identifying and grouping equivalent tools into equal function sets, enabling objective, reproducible evaluation without expensive LLM judges. Building on this foundation, we construct MSC-Bench as fivelevel curriculum that systematically stress tests the full spectrum of agent capabilities: The curriculum is structured into five levels, starting with foundational single-tool competence (L1-L2), moving to sequential intra-server orchestration (L3), compositional cross-server chaining (L4), and culminating in complex orchestration and robustness against out-of-scope requests (L5). Figure 2 illustrates the overall structure and progression of our five-level curriculum. Our extensive experiments on MSC-Bench with range of agent architectures, from zeroshot MCP-Zero agent to sophisticated ReAct and retrieval-augmented pipelines, yield critical insights. We find that while current models perform reasonably well in single-tool tasks (L1-L2), their precision degrades significantly in complex multiserver chaining (L4) and robustness checks (L5), often falling below 40%. These results show that MSC-Bench successfully exposes failure modes in orchestration and robustness that are missed by benchmarks with narrower scope. Furthermore, our findings challenge the assumption that hierarchical structure is inherently beneficial. We reveal that without co-designed, hierarchy-aware reasoning strategies, such structures can actually introduce new failure modes and degrade performance. By also measuring latency, we highlight crucial trade-off between accuracy and efficiency, confirming that an effective orchestrator must balance these competing demands. To facilitate reproducibility, we will release the code and dataset upon acceptance. Contributions In summary, our contributions are as follows. 1. systematic critique of the existing evaluation landscape for tool-using agents, identifying fundamental gaps in architectural alignment, evaluation methodology, and scope. 2. novel methodology based on equal function sets enables objective, reproducible, and Figure 2: MSC-Bench Five-Level Curriculum Design. The curriculum progresses from foundational single-tool tasks (L1) to complex cross-server orchestration (L4) and robustness testing (L5), systematically evaluating increasingly sophisticated agent capabilities within an MCP ecosystem. efficient evaluation of tool orchestrations in the presence of functional overlap. 3. The release of MSC-Bench, the first largescale, five-level benchmark to evaluate endto-end, multihop tool orchestration within multi-server MCP ecosystem. 4. comprehensive experimental analysis that reveals key weaknesses of current agent architectures regarding cross-server orchestration, robustness, and the trade-offs between hierarchy, accuracy, and latency."
        },
        {
            "title": "2 Related Work",
            "content": "The landscape of tool-augmented agent benchmarks has rapidly evolved from assessing foundational skills to complex agentic behaviors (Ferrag et al., 2025). We position MSC-Bench by examining prior work across three key themes: flat-namespace and capability-specific benchmarks, broader agentic benchmarks, and the emerging hierarchical MCP-aligned ecosystem. Agentic and Interactive Benchmarks. Recent benchmarks evaluate holistic agentic capabilities in realistic settings: GAIA (Mialon et al., 2023) for problem-solving, SWE-Lancer (Miserendino et al., 2025) for software engineering, and MultiAgentBench (Zhu et al., 2025) for multi-agent collaboration. However, these rely on LLM-as-a-judge evaluation, which suffers from systemic biases and low reproducibility (Tan et al., 2025; Zhuge et al., 2024). Hierarchical and MCP-Aligned Benchmarks. The Model Context Protocol (MCP) (Anthropic, 2024) offers standardized, federated architecture. Nascent benchmarks like MCP-RADAR (Gao et al., 2025) and LiveMCPBench (Mo et al., 2025) adopt this structure but rely on fallible LLM judges and sidestep functional overlap. MSC-Bench addresses these gaps by providing architectural realism through large-scale MCP ecosystem, comprehensive end-to-end evaluation, and novel \"equal function sets\" methodology enabling objective metrics even with functional overlap. Table 1 provides quantitative comparison. Flat-Namespace and Capability-Specific Benchmarks. Early large-scale benchmarks like ToolLLM (Qin et al., 2023) and Seal-Tools (Wu et al., 2024) model tools as monolithic, flat namespace, failing to test multi-server navigation. Subsequent works target specific capabilities: ComplexFuncBench (Zhong et al., 2025) and BFCL v2 (Mao et al., 2024) focus on complex functioncalling, while ToolHop (Ye et al., 2025) assesses multi-hop reasoning by avoiding functional overlap. These benchmarks evaluate components in isolation, overlooking cascading errors in end-to-end systems."
        },
        {
            "title": "3 MSC-Bench: Design and Construction",
            "content": "MSC-Bench is large-scale benchmark designed for the hierarchical MCP ecosystem, systematically addressing the challenge of function overlap that pervades real-world tool orchestration. Constructing such benchmark presents significant challenges: it must be large-scale, realistic, and capable of evaluating wide spectrum of agentic abilities while accounting for the inherent functional redundancy in hierarchical tool systems. MSCBench addresses these systematically through four core stages: (1) building comprehensive, real-"
        },
        {
            "title": "Benchmark",
            "content": "Tools (S/T)"
        },
        {
            "title": "MCP Benchmarks",
            "content": "MCP-RADAR (Gao et al., 2025) MCPEval (Liu et al., 2025) LiveMCPBench (Mo et al., 2025) MCP-Universe (Luo et al., 2025) MSC-Bench (ours) 9/42 12/77 70/527 11/133 491/2375 Real. Real. Real. Real. Real. 300 676 95 231 2075 ST+MT N/A MT MT ST+MT+R L(all in prompt) L(MCP interface) L(MCP-Zero) L(oracle R) L+R Rule Rule+LLM LLM LLM Rule NestTools (Han et al., 2025) Seal-Tools (Wu et al., 2024) ToolRet (Shi et al., 2025) N/A/3034 N/A/4076 N/A/44k"
        },
        {
            "title": "Tool Benchmarks",
            "content": "Syn. Syn. Mix. 1000 14076 7961 MT ST+MT ST+MT L(gte-large) L(DPR) R"
        },
        {
            "title": "Rule\nRule\nRule",
            "content": "Table 1: Benchmark comparison. S/T = Servers/Tools. ST=single-tool, MT=multi-tool, R=robustness. = evaluated only with LLM, = evaluated only with Retriever. Parentheses indicate Retriever used during LLM evaluation. world MCP tool corpus; (2) identifying functionally equivalent tools to resolve function overlap; (3) generating five-level task curriculum that leverages the hierarchical nature of the ecosystem; and (4) ensuring data quality and validity across all complexity levels. Full procedural details for each stage, including the tool filtering criteria, the equal function set generation algorithm, and task generation pipelines for all five levels, are provided in Appendix H.1. Representative examples for each task level can also be found in Appendix G."
        },
        {
            "title": "MCP Toolset",
            "content": "The foundation of MSC-Bench is diverse tool corpus sourced from the glama.ai MCP server registry (Glama AI, 2025). We scraped the top 50 servers from each category and performed rigorous filtering process to ensure the quality and relevance of the toolset. Inspired by the guidelines for identifying confounding tools in benchmarks (Huang et al., 2024), we first conducted semi-automated filtering process to exclude servers that do not represent genuine, indispensable external capabilities. This process targeted several categories of unsuitable servers: 1. Trivial Tools, whose functions are subsumed by the native capabilities of modern LLMs (e.g., simple calculators); 2. Meta-Tools, which are designed to augment the agents own internal processes (e.g., memory, reasoning patterns); 3. Template Servers, which primarily serve as developer examples and lack cohesive use case. Each server was evaluated against strict definition of native LLM capabilitydefined as tasks pure, sandboxed LLM could perform without any external tools. The full methodology, including the detailed criteria and the prompt used for our LLMbased analysis, is provided in Appendix H.1. This multi-stage process, combining automated analysis with human review, ensured that the final toolset is composed of meaningful and externally-focused tools suitable for evaluating complex orchestration. The resulting filtered corpus contains 491 unique servers exposing 2,375 distinct tools, each represented by JSON object with its name, description, and formal input schema (see Appendix B.2). This hierarchical corpus establishes realistic and challenging environment for all benchmark tasks. 3.2 Identifying Functional Overlap: The Equal Function Set Methodology Functional overlap complicates evaluation, as multiple tools can fulfill the same user intent. We solve functional overlap through round-trip consistency approach that integrates bottom-up and top-down verification to ensure comprehensive and practically relevant equal function sets. The integration of these processes establishes round-trip consistency, ensuring functional equivalence is coherent at both the corpus level and within actual query contexts. This ensures all candidate tools associated with Level 2 query are functionally equivalent, forming robust ground truth for evaluation. Furthermore, these equal function sets serve as the cornerstone for Level 4 cross-server compositional tasks, enabling systematic identification of functionally equivalent tools across different servers. Complete algorithmic details are provided in Appendix A. Bottom-up: Candidate Generation and Pairwise LLM Verification For each tool, we retrieve semantically similar tools using Qwen3-Embedding0.6B (similarity > 0.8). large LLM performs pairwise verification to determine functional equivalence, with verified pairs grouped into connected components using UnionFind to form equal function sets. Top-down: Query-Guided RAG Verification To ensure equivalence in realistic usage, each Level 2 query retrieves the top-10 relevant tools via RAG. An LLM selects all tools capable of fulfilling the query, with associations cross-checked against equal function sets and human-verified."
        },
        {
            "title": "Curriculum",
            "content": "MSC-Bench implements five-level curriculum to evaluate diverse agentic abilities. The fivelevel design is motivated by the need to systematically assess the full spectrum of agent capabilities, from foundational single-tool operations to complex multi-server orchestration and robustness testing. Each level targets distinct aspects of tool-use reasoning: L1-L2 establish foundational competence with single-tool tasks, L3 evaluates intra-server sequential reasoning, L4 measures cross-server compositional capabilities, and L5 tests robustness through out-of-scope detection. This progression ensures comprehensive evaluation while maintaining clear boundaries between different types of reasoning challenges (see Figure 2). hybrid-model strategy is employed, leveraging high-capability LLMs for semantic reasoning and smaller models for high-throughput annotation and verification. Tool Triage and Semantic Annotation Prior to task generation, all tools undergo annotation along three axes: Platform Specificity (distinct platform requirements), Task Type (final-goal vs. middleware), and User Orientation (user-facing vs. system-facing). Only tools meeting all criteria are considered for Level 1 and Level 2 generation. Level 1: Foundational Single-Tool Tasks These tasks establish baseline competence through direct tool invocation. Generator-Verifier pipeline produces queries that satisfy semantic and structural constraints, ensuring each task has clear, unambiguous solution path. Level 2: Context-Aware Tool Retrieval This level tests disambiguation capabilities when multiple tools could potentially fulfill user intent. Preliminary L2 queries validate and extend equal function sets via RAG retrieval and LLM verification, ensuring the dataset is diverse and grounded in validated tool equivalences. Level 3: Intra-Server Sequential Chaining Complex workflows within individual servers require understanding tool dependencies and data flow. The pipeline infers output schemas using an LLM, constructs data-flow dependency graphs, and identifies valid chains where outputs satisfy subsequent inputs, generating tasks with Chain-of-Thought rationales. Level 4: Cross-Server Compositional Chaining Multi-server orchestration demands reasoning across different contexts and handling potential failures. Two to four servers are sampled from distinct categories, and planner LLM constructs coherent cross-server task flows. Generated tasks are verified for realism and executability. Level 5: Robustness via Capability Gap Identification Agents must recognize when requests exceed their capabilities rather than attempting impossible tasks. The pipeline clusters tool embeddings to map capabilities, identifies gaps where no existing tool can fully address tasks, generates natural language queries for each gap, and validates that queries cannot be solved with the current toolset."
        },
        {
            "title": "3.4 Data Quality and Validation",
            "content": "All levels incorporate multi-stage verification to ensure tasks are pragmatically plausible, executable, and internally consistent. Overall statistics and comprehensive analysis will be discussed in Section 4. Complete details are provided in Appendix B. Level and Key Challenge # of Tasks Avg. Plan Length Avg. Servers L1: Direct Retrieval Foundational: Direct Tool Retrieval L2: Context-Aware Retrieval Disambiguation among Functional Overlap L3: Intra-Server Chaining Orchestration: Intra-Server Sequential Chaining L4: Cross-Server Chaining Orchestration: Cross-Server Compositional Chaining L5: Robust Rejection Robustness: Rejection of Out-of-Scope Requests 781 773 327 91 1.00 1.00 2.87 3.83 0. 1.00 1.00 1.00 3.78 0.00 Table 2: Overview of the MSC-Bench evaluation curriculum statistics."
        },
        {
            "title": "Architecture Foundation Model",
            "content": "Level 1 Level 2 Level 3 Level 4 Level 5 EM (%) N-Lat. EM (%) N-Lat. F1 (%) N-Lat. F1 (%) N-Lat. EM (%) N-Lat. MCP0 MCP0 MCP0 MCP0 MCP0 MCP0 MCP"
        },
        {
            "title": "TS\nTS\nTS\nTS\nTS\nTS\nTS",
            "content": "Qwen3-4B-Instruct-2507 Qwen3-8B Ministral-8B-Instruct-2410 GPT-4.1 Gemma-3-12B-IT Microsoft Phi-4 Meta-Llama-3-8B-Instruct Qwen3-4B-Instruct-2507 Qwen3-8B Ministral-8B-Instruct-2410 GPT-4.1 Gemma-3-12B-IT Microsoft Phi-4 Meta-Llama-3-8B-Instruct 69.01 65.68 46.99 63.38 70.55 38.79 48.08 72.72 80.65 64.66 69.91 80.66 38.66 57.03 MCP-Zero (MCP0) 0.89 3.14 1.29 0.70 1.73 3.54 1. 3.31 11.63 7.86 5.27 7.89 15.12 5.22 43.02 31.56 23.67 31.82 33.50 19.01 19.25 0.59 3.60 1.24 0.63 1.56 3.37 1.00 ToolShed (TS) 3.58 11.33 8.24 9.15 7.87 14.54 6.02 51.87 53.29 43.46 49.02 56.79 28.71 42."
        },
        {
            "title": "ReAct",
            "content": "46.85 48.16 39.65 53.12 49.70 53.97 38.04 69.41 77.45 82.36 82.55 73.96 81.85 74.34 5.64 3.87 1.43 1.10 1.71 3.31 1.00 5.38 15.74 10.15 11.39 11.80 14.52 5.22 30.42 33.95 25.99 34.20 32.43 33.98 25.70 37.45 44.96 50.98 55.06 47.29 49.67 50. 6.50 3.73 1.71 1.03 1.99 3.42 1.00 5.26 15.31 8.68 8.60 12.19 14.81 5.68 74.72 62.63 51.60 80.29 65.93 69.23 39.56 70.32 76.92 45.05 81.31 60.43 67.03 27.40 0.22 3.86 0.70 0.24 0.75 2.58 1.00 2.22 11.25 4.70 2.20 6.07 11.93 6."
        },
        {
            "title": "ReAct",
            "content": "Qwen3-4B-Instruct-2507 41.43 8.80 19.76 8.65 33. 6.75 6.74 10.54 72.50 12."
        },
        {
            "title": "Hybrid\nHybrid",
            "content": "Qwen3-4B-Instruct-2507 Meta-Llama-3-8B-Instruct 68.62 48.08 3.61 4.74 60.20 39.79 3.68 4.39 56.91 50. 3.63 6.07 26.78 21.61 2.37 5.48 74.72 14.28 1.58 4.94 Table 3: Detailed Performance and Latency Results Across All Levels and Configurations. Performance for L1, L2, and L5 is measured by Exact Match (EM), while L3 and L4 use F1-score. Normalized Latency (N-Lat.) is relative to the MCP0 Meta-Llama-3-8B-Instruct baseline. The best performance in each column is bolded."
        },
        {
            "title": "4 Benchmark Statistics",
            "content": "We present key statistics of MSC-Bench, highlighting its scale and comprehensive five-level evaluation curriculum (see Figure 2 for the curriculum design). An overview of statistics for each level is provided in Table 2."
        },
        {
            "title": "Complexity",
            "content": "We analyze our foundational retrieval tasks (L1 and L2) against prominent single-step benchmarks using lexical (ROUGE-L) and semantic (METEOR) similarity metrics. As shown in Table 4, existing benchmarks exhibit low lexical overlap, confirming that tool retrieval relies on semantic understanding rather than surface-level keywords. Our Level 2 tasks align with this paradigm, while Level 1 tasks fill gap by evaluating direct command following with higher lexical overlap. This dual approachsemantic inference (L2) and direct lexical mapping (L1)provides comprehensive evaluation coverage. Note that multi-step tasks (L3-L4) require graph-based metrics (Node Set EM, F1score) rather than lexical similarity measures. Level 5: Robustness via Capability Gap Level 5 comprises 91 out-of-scope queries targeting 31 distinct capability gaps identified through systematic analysis. These gaps represent welldefined categories (e.g., physical world interaction, real-time sensory processing) outside the digital MCP ecosystem boundaries. The distribution of these categories is shown in Appendix E.2, ensuring comprehensive robustness testing. Figure 3: Performance vs. efficiency across architectures and foundation models. Each point is modelarchitecture combination; markers: = MCPZero, circle = ToolShed. The x-axis uses log scale for normalized latency."
        },
        {
            "title": "5 Experiments and Results",
            "content": "We evaluate four tool orchestration frameworks on MSC-Bench, examining performance across complexity levels, efficiency trade-offs, and key hyperparameter effects. Benchmark Type ROUGE-L METEOR"
        },
        {
            "title": "5.2 Main Performance Comparison",
            "content": "Existing Benchmarks API-Bank ToolE ToolRet Intent-based Intent-based Intent-based MSC-Bench L1 L2 Command-based Intent-based 0.116 0.106 0.148 0.337 0. 0.095 0.102 0.178 0.337 0.076 Table 4: Retrieval complexity comparison. MSC-Bench spans both high-overlap (L1) and low-overlap (L2) tasks."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Four architectures: ReAct (Yao et al., 2023) (generative baseline), ToolShed (Lumer et al., 2024) (flat retrieval with dense search, query expansion, reranking), MCP-Zero (Fei et al., 2025) (hierarchical retrieval), and Hybrid (combining MCP-Zero filtering with ToolShed search). Foundation models: Qwen3-4B-Instruct, Qwen3-8B (Yang et al., 2025), Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024), Ministral-8B-Instruct-2410 (AI, 2024), GPT-4.1 (OpenAI et al., 2024), Gemma3-12B-IT (Team et al., 2025), and Microsoft Phi4 (Abdin et al., 2024). We evaluate this comprehensive set of models to assess backbone sensitivity and model-architecture interactions; aggregate trends appear in Figure 3, with main experimental results in Table 3 and ablation study of parameters in each architecture in Appendix F. Evaluation Methodology We evaluate the proposed orchestrator agent using multi-level framework (L1L5) designed to probe distinct facets of tool-use reasoning, ranging from foundational retrieval to multi-step task decomposition and robust rejection of infeasible requests (see Figure 2). To ensure fair cross-level comparison, all evaluations employ fixed agent architecture, isolating observed performance differences to reasoning capability rather than ad-hoc modifications. Table 5 provides high-level overview of each evaluation level with its primary objective and corresponding metric. Full procedural details, including scoring rules and verification protocols, are available in Appendix D."
        },
        {
            "title": "Metric",
            "content": "L1 L2 L3 L4 L5 Direct tool retrieval Context-Aware Tool Retrieval Intra-server task orchestration Cross-server task orchestration Reject unfulfillable requests Exact Match (EM) EM against set Node Set EM, F1 Node Set EM, F1 Exact Rejection Match (ERM) Table 5: Evaluation Levels and Metrics Summary Our results in Table 3 reveal clear performance hierarchy. First, retrieval-augmented frameworks decisively outperform the generative baseline, with ToolShed-Qwen-4B-Instruct (60.35) nearly doubling the score of ReAct-Qwen-4B-Instruct (34.81). This confirms that direct access to tool repository is superior to relying on models parametric memory. Among retrieval methods, ToolSheds flat-retrieval architecture achieves the highest overall scores. Its comprehensive search and re-ranking prove particularly effective for complex orchestration, where Llama-ToolShed excels on L3 (74.34) and L4 (50.77) tasks. However, performance is not determined by architecture alone. We observe strong model dependency: Qwen demonstrates strength in direct retrieval (L1-L2), whereas Llamas advanced reasoning shines in multi-step tasks (L3-L4). This interplay is starkly illustrated by our Hybrid architecture, whose success is entirely model-dependent. While competitive with Qwen (57.45), it performs poorly with Llama (34.93). This suggests that architectural benefits are not universal but are deeply intertwined with the foundation models reasoning style, highlighting critical challenge in model-architecture co-design that we explore further in Section 6."
        },
        {
            "title": "5.3 Efficiency Analysis",
            "content": "Table 3 shows normalized latency across configurations, while Figure 3 visualizes the performanceefficiency trade-off across all architectures and foundation models: (1) MCP-Zero is highly efficient: Llama-MCPZero serves as baseline (1.00); Qwen-MCP-Zero is fastest on L2 (0.59) and L5 (0.20). (2) ToolSheds high performance comes with high latency: Llama-ToolShed is 5.76 slower than baseline; Qwen-ToolShed (3.95) is 1.43 slower than Qwen-MCP-Zero (2.76). (3) ReAct is least efficient: Average latency is 9.19 higher than baseline, showing substantial computational overhead. Figure 3 reveals distinct architectural clustering and backbone-specific trade-offs: MCP-Zero configurations (X markers) consistently occupy the low-latency region (1.0-3.0 baseline), while ToolShed configurations (circle markers) cluster in the high-latency region (5.0-15.0). This demonstrates fundamental trade-off where MCP-Zero prioritizes efficiency through hierarchical filtering, while ToolShed achieves superior performance through computationally intensive dense retrieval and reranking. However, latency is not solely determined by architecturewithin each architectural framework, foundation model choice significantly impacts this trade-off through differences in reasoning patterns and computational requirements: under MCP-Zero, Qwen variants sit on the efficient frontier (e.g., lowest normalized latency on single-step tasks, L2/L5), while Llama offers stable but higher latency due to longer reasoning chains; under ToolShed, GPT-4.1 shifts toward the performance frontier (especially on L3/L4; Section 5), paying clear latency premium, whereas Qwen remains more efficient but with lower peak. Stronger backbones (e.g., GPT-4.1, Gemma3-12B) dominate the Pareto frontier under ToolShed while maintaining competitive efficiency with MCP-Zero, indicating strong modelarchitecture synergy. Practically, Qwen+MCPZero is preferred for tight latency budgets or simpler (L1/L2) workloads; Llama+ToolShed is preferable for complex (L3/L4) orchestration where headroom matters. This stark trade-off reveals an inconvenient truth in tool orchestration: architectural choices impose fundamental, inescapable limits. Hierarchical filtering buys efficiency by sacrificing comprehensiveness, while flat retrieval purchases performance at steep computational cost. This is not merely design choice but core tension, exposing the hidden price of architectural commitments that we dissect in Section 6."
        },
        {
            "title": "5.4 Ablation Study of ToolShed’s Architecture",
            "content": "Figure 4 shows comprehensive ToolShed ablation results across different parameter configurations (complete numerical results in Appendix F). Key findings:"
        },
        {
            "title": "Retrieval QE",
            "content": "L2/L3/L"
        },
        {
            "title": "Full\nNo QE\nNarrow\nWide\nNo Rerank",
            "content": "5 5 1"
        },
        {
            "title": "Yes\nNo\nNo\nYes\nYes",
            "content": "42.12/74.34/50.77 41.60/76.98/47.11 36.56/62.50/37.91 35.91/76.76/52.11 42.50/71.91/46.78 Table 6: ToolShed ablation study on Llama. Retrieval = tool_top_k, QE = query expansion. (1) Retrieval breadth reveals task-specific patterns: As shown in Figure 4, different complexity levels exhibit distinct responses to increasing tool retrieval breadth. L1 direct matching tasks demonstrate remarkable resilience, maintaining stable performance ( 57% EM) across all values as they are unaffected by irrelevant tool distractors. L3 intraserver orchestration shows consistent improvement with broader retrieval (46.17%50.15% EM from k=520), benefiting from comprehensive tool coverage. Conversely, L2 context-aware tasks peak early then degrade (42.12%35.91% EM from k=520) due to noise from excessive candidates, while L4 cross-server tasks find their optimal sweet spot at k=20 (52.11% F1), requiring diverse tool access for complex orchestration. (2) Query expansion shows small, complexitydependent trends: Table 6 suggests that QE may slightly help L4 cross-server coordination (47.1150.77 F1) while offering limited or negative movement on simpler tasks (e.g., L2: 42.1241.60 EM; L3: 76.98 vs. 74.34 F1). Given the small absolute differences, these effects should be interpreted cautiously as they could be within expected variance; we therefore treat QE as configuration knob with context-dependent value rather than universally beneficial augmentation. Figure 4: Impact of retrieval breadth (tool_top_k) on ToolShed performance across complexity levels. L1/L2 show EM scores, L3/L4 show F1 scores. Annotations highlight key behavioral patterns: L1 resilience to irrelevant tools, L2 degradation from excessive candidates, L3 consistent improvement, and L4 optimal performance at k=20."
        },
        {
            "title": "5.5 Failure Analysis",
            "content": "Our qualitative error analysis pinpointed two recurring failure modes: premature decomposition and catastrophic context loss. Agents often fracture simple, single-step queries into unnecessary sub-tasks, introducing needless complexity. More critically, during long-horizon tasks, they fail to propagate vital context (e.g., user intent, previous tool outputs) across steps. This memory decay leads to cascade of errors, culminating in irrelevant tool selection and task failure."
        },
        {
            "title": "6 Discussion",
            "content": "Building on the experimental results, this section delves into their broader implications. Our empirical evaluation reveals fundamental insights about tool orchestration systems through three critical lenses: the hidden costs of architectural choices, the intricate dance between models and architectures, and the most persistent challenges that remain unsolved."
        },
        {
            "title": "6.1 Hierarchy Isn’t Free: The Hidden Costs",
            "content": "of Architectural Choices Our experimental results challenge the common assumption that hierarchical retrieval should inherently outperform flat search architectures. As demonstrated in Table 3, MCP-Zeros hierarchical structure delivers impressive efficiency gains (up to 5.76 faster than ToolShed), yet this comes at significant accuracy cost that varies dramatically across complexity levels. The efficiency-accuracy trade-off is not uniform: MCP-Zero excels at simple tasks (L1/L2) where rapid pruning is beneficial, but struggles with complex orchestration (L3/L4) where the hierarchical constraints become limiting rather than helpful. This suggests provocative possibility: that rigid, pre-defined hierarchies, while computationally attractive, are fundamentally antithetical to the fluid, associative reasoning style of modern LLMs. By forcing models reasoning into fixed tree structure, we may be inadvertently stifling the very capabilities we seek to leverage. Our experimental Hybrid model further exposes this tensionthe dramatic performance drop with Llama (34.93 vs 50.33 for ToolShed) demonstrates that simply combining paradigms is insufficient. True hybrid systems require architectural innovation that goes beyond straightforward fusion, potentially involving dynamic switching or contextaware retrieval strategies."
        },
        {
            "title": "6.2 Model-Architecture Co-Design: Why",
            "content": "Theres No Universal Best striking finding is that model performance is not an intrinsic property but emerges from the interplay between foundation models and orchestration architectures. Qwen and Llama exhibit complementary strengths that are either amplified or constrained by the architectural framework. As evidenced by our results in Table 3, Qwens precision in direct retrieval tasks (L1/L2: 72.72 vs 70.43 for Llama) suggests it excels when the search space is well-defined and the task requires precise instruction-following. However, Llamas superior complex reasoning capabilities (L3/L4: 74.34/50.77 vs 63.46/41.67 for Qwen) only manifest when paired with ToolSheds broad, high-recall retrieval mechanismindicating that advanced reasoning requires access to comprehensive candidate set. This symbiotic relationship has profound implications: there is no \"best\" model in isolation, only optimal model-architecture pairs for specific task profiles. Future systems must consider model selection and architectural design as unified co-design problem, not separate optimization challenges. Link to performance-efficiency trade-offs Figure 3 makes these interactions visible: Qwen+MCP-Zero configurations populate the efficient frontier for simpler workloads (L1/L2), while Llama+ToolShed configurations occupy the performance frontier for complex orchestration (L3/L4) at higher latency. Stronger backbones (e.g., GPT-4.1, Gemma3-12B) extend the frontier under ToolShed without collapsing efficiency under MCP-Zero, underscoring that backbone choice and retrieval architecture must be co-optimized to match task complexity and latency budgets."
        },
        {
            "title": "6.3 Critical Gaps in Current Systems",
            "content": "Despite significant advances in tool orchestration, our evaluation exposes two fundamental challenges that current frameworks cannot adequately address. Task Decompositions Fragility: Our failure analysis reveals that maintaining context across multi-step plans remains critical weakness. The \"premature decomposition\" we observed leads to cascading errors, fundamentally limiting the ability to solve complex, long-horizon tasks that require sustained reasoning. Out-of-Scope Detections Architectural Gap: At Level 5, none of the evaluated orchestrators implements an explicit mechanism to detect when the available tool set cannot satisfy query. In practice, correct rejection largely emerges from the backbone models intrinsic reasoning rather than from purpose-built architectural checks. This dependency creates safety gap: without architecturelevel out-of-scope detection, rejection behavior is inconsistent across backbones and difficult to guarantee in deployment."
        },
        {
            "title": "6.4 Future Research Directions",
            "content": "Based on these findings, we propose several promising directions for future research: Hierarchy-Aware Reasoning: Develop reasoning and search strategies that go beyond using hierarchy as simple filter, enabling models to explicitly leverage the semantic structure of tool servers to improve multi-level orchestration and inference. Context-Propagating Decomposition: Design new task decomposition methods that are explicitly engineered to maintain and pass global context between steps, preventing the context loss that plagues current models. Adaptive and Hybrid Architectures: Explore more sophisticated hybrid architectures that can dynamically choose between flat and hierarchical retrieval based on query complexity, or adaptively adjust retrieval strategies in real-time. Developing Robust Rejection Mechanisms: Create and integrate dedicated modules or prompting strategies specifically designed for out-of-scope detection, moving this critical safety feature from an emergent model property to reliable architectural component."
        },
        {
            "title": "7 Limitations",
            "content": "Our framework presents three areas for future enhancement, each with clear rationale for the current design choices. Dataset Construction Methodology: Our benchmark leverages proprietary LLMs (GPT4.1 (OpenAI et al., 2024), Meta-Llama-3-8BInstruct (Grattafiori et al., 2024)) for data generation and human annotation for quality verification. While this approach requires computational resources (approximately $500 USD for the complete benchmark), it ensures high-quality task generation that accurately reflects real-world multiserver orchestration challenges. We found that open-source alternatives, while cost-effective, produced less consistent results for our specific requirements. This investment in quality is justified by the benchmarks role as standardized evaluation platform for the community. Evaluation Scope and Focus: Our methodology prioritizes end-to-end task completion metrics over granular reasoning trace analysis. This design choice reflects our primary objective: establishing comprehensive benchmark for comparing tool orchestration systems at scale. Detailed reasoning analysis, while valuable, would require specialized frameworks and extensive computational resources that would shift focus from our core contribution. Our failure analysis (Section 5) provides sufficient insight into common failure patterns to guide system improvements, while the benchmarks structured design enables targeted investigations by future researchers. Dataset Diversity and Coverage: Our benchmark draws from publicly available MCP servers on the glama.ai platform, representing current ecosystem trends rather than exhaustive coverage. This focused approach ensures that our evaluation reflects real-world deployment scenarios while maintaining manageable complexity. The English-based corpus captures the dominant language of current MCP implementations, and our server selection methodology prioritizes representative diversity over exhaustive coverage. Future extensions to multilingual scenarios and additional server sources represent natural evolution paths that build upon this foundation."
        },
        {
            "title": "8 Ethics Statement",
            "content": "Since our benchmark construction pipeline involves prompting LLMs for task generation, it is important to implement stringent measures to ensure the absence of offensive content in both the prompts and the generated responses. We first explicitly state in all generation prompts that the LLM should not generate any content that contains personal privacy violations, promotes violence, racial discrimination, hate speech, sexual content, or selfharm. Then, we manually inspect random sample of 200 data entries from all five levels in MSCBench for offensive content. Based on our observations, we have not detected any offensive content. Therefore, we believe that our benchmark is safe and will not yield any negative societal impact. Due to data privacy and intellectual property concerns, our benchmark dataset will not be made public in its entirety. However, we provide detailed task generation pipelines and evaluation methodologies to ensure reproducibility. For language models, we access all open-source LMs via the Hugging Face Hub. The number of parameters is presented in our experimental results. All associated licenses permit user access for research purposes, and we have agreed to follow all terms of use. We conduct human annotations for quality verification and task validation. Our annotators are graduate students and research assistants who are compensated at standard academic rates. The selection of annotators is based on their domain expertise in AI systems and tool orchestration, and we do not collect any personal information beyond what is necessary for the annotation process. All annotators agree to participate as their contribution to the research without additional compensation beyond their standard academic roles. Our benchmark assumes access to diverse external tools and services, which may not be available to all users due to economic, geographical, or institutional constraints. This could potentially exacerbate digital inequalities. We acknowledge this limitation and encourage future work to consider accessibility and inclusivity in tool orchestration system design and deployment. We used ChatGPT and Gemini to assist in proofreading and code documentation during the writing process."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, et al. 2024. Phi-4 technical report. Mistral AI. 2024. Ministral 8b instruct (v0.2, 2024-10). https://huggingface.co/mistralai/ Ministral-8B-Instruct-2410. Accessed: 202510-20. 2024. Anthropic. col. model-context-protocol. 08-14. protohttps://www.anthropic.com/news/ 2025Accessed: context"
        },
        {
            "title": "Model",
            "content": "Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, and Pavan Kapanipathi. 2025. Nestful: benchmark for evaluating llms on nested sequences of api calls. Yu Du, Fangyun Wei, and Hongyang Zhang. 2024. Anytool: Self-reflective, hierarchical agents for largescale api calls. Xiang Fei, Xiawu Zheng, and Hao Feng. 2025. Mcpzero: Active tool discovery for autonomous llm agents. Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. 2025. From llm reasoning to autonomous ai agents: comprehensive review. Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. 2025. Mcp-radar: multi-dimensional benchmark for evaluating tool use capabilities in large language models. Glama AI. 2025. Glama mcp server registry. https: //glama.ai/mcp/servers. Accessed: 2025-07-14. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. 2024. The llama 3 herd of models. Zhicheng Guo, Sijie Cheng, Yuchen Niu, Hao Wang, Sicheng Zhou, Wenbing Huang, and Yang Liu. 2025. Stabletoolbench-mirrorapi: Modeling tool environments as mirrors of 7,000+ real-world apis. Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, and Wenliang Chen. 2025. Nestools: dataset for evaluating nested tool learning abilities of large language models. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. 2024. Metatool benchmark for large language models: Deciding whether to use tools and which to use. Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. 2025. Preference leakage: contamination problem in llm-as-a-judge. Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, and Caiming Xiong. 2025. Mcpeval: Automatic mcpbased deep evaluation for ai agent models. Elias Lumer, Vamse Kumar Subbiah, James A. Burke, Pradeep Honaganahalli Basavaraju, and Austin Huber. 2024. Toolshed: Scale tool-equipped agents with advanced rag-tool fusion and tool knowledge bases. Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. 2025. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. H. Mao, C. C.-J. Ji, F. Yan, T. Zhang, and S. G. Patil. 2024. Bfcl v2 live. Accessed: 2025-02-16. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. 2025. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. 2025. Livemcpbench: Can agents navigate an ocean of mcp tools? OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2024. Gpt-4 technical report. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, and Zhaochun Ren. 2025. Retrieval models arent tool-savvy: Benchmarking tool retrieval for large language models. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2025. Judgebench: benchmark for evaluating llm-based judges. Gemma Team, Aishwarya Kamath, Johan Ferret, et al. 2025. Gemma 3 technical report. Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, and Eugene Siow. 2025. Mcp-bench: Benchmarking tool-using llm agents with complex real-world tasks via mcp servers. Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. An Yang, Anfeng Li, Baosong Yang, et al. 2025. Qwen3 technical report. https://example.com/qwen3. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. 2025. Toolhop: querydriven benchmark for evaluating large language models in multi-hop tool use. Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, and Kaiqiang Song. 2025. Livemcp-101: Stress testing and diagnosing mcp-enabled agents on challenging queries. Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. 2025. Complexfuncbench: Exploring multi-step and constrained function calling under long-context scenario. Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, and Jiaxuan You. 2025. Multiagentbench: Evaluating the collaboration and competition of llm agents. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jürgen Schmidhuber. 2024. Agent-as-a-judge: Evaluate agents with agents."
        },
        {
            "title": "A Algorithm for Equal Function Set",
            "content": "B.2 MCP Server Data Format Example"
        },
        {
            "title": "Generation",
            "content": "This appendix provides the core algorithmic description for Equal Function Set Generation, which serves as the foundation for Level 2 and Level 4 ground truth annotation in MSC-Bench. The algorithm (Algorithm 1) implements two-phase approach: bottom-up candidate generation and verification, followed by top-down query-guided RAG verification to establish functionally equivalent tool sets. MSC-Bench Task Generation Pipelines This appendix details the multi-stage, semiautomated pipelines for generating MSC-Bench tasks across Levels 15 (see Figure 2 for the overall curriculum design). The goal is to produce high-quality, verifiable queries that systematically evaluate distinct agent capabilities. B.1 Server Filtering Methodology Before task generation begins, we adopted twostage process for filtering MCP servers to ensure only high-quality, relevant servers are included in our corpus: 1. LLM-based Filtering: We used an LLM(GPT-4.1 (OpenAI et al., 2024)) to assess whether each MCP server provided genuine, indispensable external capabilities beyond native LLM functions. The full prompt text is provided in Appendix H.1 for reproducibility. 2. Human Review: Borderline cases were manually examined to ensure accuracy and consistency with the defined criteria."
        },
        {
            "title": "MCP Server Data Format Example",
            "content": "Server Name: heroku-mcp-server Description: Heroku Platform"
        },
        {
            "title": "MCP Server for LLM interaction\nwith Heroku resources",
            "content": "URL: https: //glama.ai/mcp/servers/ @heroku/heroku-mcp-server Summary: MCP implementation for seamless LLM-Heroku Platform interaction Categories: agent-orchestration, cloud-platform Tools: Tool Name: rename_app Description: Change Heroku app name or resolve naming conflicts Input Schema: * Properties: app (string): Current name of the Heroku app to rename newName (string): New unique name for the app * Required Fields: app, newName * Type: object Figure 5: MCP Server Data Format: Standard JSON structure for MCP server data used in MSC-Bench, containing server metadata, tool descriptions, and input schemas required for the task generation pipeline. This format provides all necessary information for the tool triage and annotation process, including server metadata, tool descriptions, and input schemas required for the MSC-Bench task generation pipeline. Algorithm 1: Level 2 Query-Tool Round-Trip Consistency Generation Input: Tool corpus , similarity threshold τ , Level 2 queries Output: Verified Level 2 query-tool associations 1 /* Phase 1: Bottom-up Candidate Generation and Verification */ 2 // Set of unique candidate pairs 3 // Set of verified equivalent pairs 4 /* 1a. Generate candidate pairs from semantic similarity */ foreach tool do 5 6 7 8 Ct top-10 semantically similar tools to with similarity > τ ; foreach (ti, tj) Ct where ti = tj do (a, b) (min(ti, tj), max(ti, tj)); {(a, b)}; 9 /* 1b. Verify functional equivalence of pairs via LLM */ foreach (ti, tj) do 10 if LLMEQUIV(ti, tj) = TRUE then 11 {(ti, tj)}; 12 /* 1c. Build Equal Function Sets using Union-Find */ Let be UnionFind data structure initialized with all tools in ; 13 foreach (ti, tj) do 14 UNION(U, ti, tj);"
        },
        {
            "title": "15 Equal function sets ← root groups from U ;",
            "content": "16 /* Phase 2: Top-down Query-Guided RAG Verification */ foreach query do 17 18 19 Retrieve top-10 relevant tools for via RAG; Select tools from that can fulfill via LLM; Human-verify consistency of against the Equal Function Sets; 20 return Verified Level 2 query-tool associations B.3 Tool Triage and Semantic Annotation B.4 Level 1 Pipeline: Foundational Tasks Before query generation, all tools in the corpus undergo rigorous triage and annotation to filter for suitable candidates and collect metadata. We employ targeted LLM classifiers for: 1. Platform Identification Identify the primary user-facing platform, brand, or product associated with each server using GPT-4.1 (OpenAI et al., 2024). 2. Task Type Classification Classify each tool as final_goal or middleware using Meta-Llama3-8B-Instruct (Grattafiori et al., 2024). 3. User Orientation Classification Determine if tool is user_facing or system_facing using GPT-4.1 (OpenAI et al., 2024). Only tools with specific platform, classified as final_goal and user_facing, are eligible for query generation. Prompt details are available in Appendix H.1. Stage 1: Generation Meta-Llama-3-8B-Instru ct (Grattafiori et al., 2024) generates two direct, explicit user commands per tool, mentioning the platform and including concrete examples for tool parameters in case follow-up questions arise (e.g., \"Which document do you want to modify?\"). The detailed prompt is provided in Appendix H.1. Stage 2: Verification second LLM validates the generated queries for intent match, mandatory platform mention, and lack of ambiguity. Invalid queries are discarded. The verification prompt is given in Appendix H.1. B.5 Level 2 Pipeline: Context-Aware Tasks Stage 1: Platform Coupling Analysis Classify tools as tightly_coupled (platform-specific) or generic_concept using Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024). Classification prompt is in Appendix H.1. Stage 2: Dynamic Rule Generation & Query Generation Set the platform_mention_rule as: For tightly_coupled tools: You MUST mention the platform platform_name because the function is iconic to it. For generic_concept tools: You MUST NOT mention the platform platform_name, as the function is generic concept. Rule-based query generation prompt: see Appendix H.1. Stage 3: Verification Generated L2 queries are verified by Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024) against the rules above. Verifier prompt: Appendix H.1. Stage 4: Round-Trip Consistency Validation The preliminary L2 queries are used to verify and extend the equivalent function sets (as defined in Section 3.2). This process implements the same round-trip consistency methodology described in Appendix A. 1. Retrieve top-10 relevant tools from documen2. Query and CoT Generation: Generator LLM produces high-level queries and Chainof-Thought plans for verified chains. Prompt in Appendix H.1. B.7 Level 4 Pipeline: Cross-Server Compositional Chaining 1. Server Sampling: Group 491 servers into 20+ functional categories; randomly sample 2 4 categories and select one server from each category to ensure cross-server composition. 2. Feasibility Check: Use Qwen3-4B-Instruct (Yang et al., 2025) to evaluate whether the sampled server combination can form logical user workflow. The LLM determines feasibility using structured output with <is_feasible>true/false </is_feasible> tags and provides reasoning for its decision. Prompt in Appendix H.1. 3. Workflow Generation: If feasible, use GPT-4.1 (OpenAI et al., 2024) to generate realistic user query and corresponding tool workflow with explicit dependencies. The output includes: tation using RAG for each query. Natural language query requiring cross2. Use an LLM (GPT-4.1 (OpenAI et al., 2024)) to select tools capable of solving the query. 3. Human verification with 10 student-hours ensures all selected tools are truly equivalent in functionality and consistent with the equal function set mentioned in Section 3.2. LLM selection prompt: Appendix H.1. B.6 Level 3 Pipeline: Intra-Server Sequential"
        },
        {
            "title": "Chaining",
            "content": "Stage 1: Inferring Pseudo Output Schemas MetaLlama-3-8B-Instruct (Grattafiori et al., 2024) infers plausible output JSON schemas for each tool based on its name, description, and inputs. The full prompt is in Appendix H.1. Stage 2: Graph Construction & Chain ID Construct directed graph = (V, E), where nodes are tools and edges represent valid input-output flows. Identify candidate chains as simple paths in the graph. Stage 3: Two-Stage Query Generation server coordination Tool sequence with dependency mapping (using array indices) Tool IDs in format server_name::tool_name Parallel execution support for independent tools Prompt in Appendix H.1. 4. Quality Control: Multi-dimensional evaluation using GPT-4.1 (OpenAI et al., 2024) to assess query quality and ensure parameter completeness. We adapt the Task Quality Assessment Prompt methodology from Wang et al. (2025) to ensure queries fully correspond to tools and contain all necessary parameter information, avoiding LLM follow-up questions. The evaluation assesses: Parameter Completeness (1-10): Whether the query contains all necessary parameters for tool execution Naturalness (1-10): Whether the query maintains conversational style vs. tasklist format 1. Logical Verification: verifier LLM checks whether the chain represents logical, realistic workflow. Prompt in Appendix H.1. Iterative improvement with up to 3 retries, accepting queries with parameter completeness 7.0. Prompt in Appendix H.1. 5. Human Verification: Each generated task is manually validated one-by-one for logical coherence, plausibility, and executability. 6. Equal Function Set Integration: Following human verification, we map each ground truth tool in the generated tasks to corresponding Equal Function Sets (EFS) to ensure comprehensive ground truth coverage. For each tool position, we identify all functionally equivalent alternatives from the EFS and optionally verify their applicability in the specific query context using multi-model LLM voting (Qwen3-4B (Yang et al., 2025), Llama-38B (Grattafiori et al., 2024), GPT-4.1 (OpenAI et al., 2024)). This process ensures that the ground truth tool paths contain all feasible solutions rather than single tool instances. Tool verification prompt in Appendix H.1. B.8 Level 5 Pipeline: Robustness via"
        },
        {
            "title": "Capability Gap Identification",
            "content": "Phase 1: Capability Mapping Tool description embeddings are clustered via HDBSCAN and labeled by LLMs to map available capabilities. Prompt in Appendix H.1. Phase 2: Gap Analysis An agentic debate framework identifies functional gaps:"
        },
        {
            "title": "C Orchestrator Implementation Details",
            "content": "This appendix provides implementation details for the orchestrator architectures evaluated in MSCBench. For each methodology, we outline the concrete implementation choices, deviations from the original frameworks, and adaptations required to integrate with our benchmarks MCP-based tool ecosystem. C.1 ReAct Orchestrator Overview The ReAct framework (Yao et al., 2023) is designed around an iterative cycle of Reasoning, Action, and Observation. In its canonical form, an agent produces intermediate reasoning traces, executes actions (tool calls), and incorporates observations from the environment into subsequent reasoning steps. This iterative loop enables the agent to solve multi-step problems and recover from errors. Adaptations for MSC-Bench In our benchmark, real tool execution is not performed. Instead, we modified the observation stage to align with the MCP tool ecosystem: Server/Tool Validation: When the agent selects server and tool for execution, the observation is defined as lookup against the MCP server pool. If the chosen tool exists in the specified server, the execution is considered success. 1. \"Proposer\" Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024) brainstorms universal user tasks. Prompt in Appendix H.1. 2. Check solvability with existing capabilities. 3. \"Red Team\" GPT-4.1 (OpenAI et al., 2024) attempts to solve each task. Prompt in Appendix H.1. 4. Confirm capability gap if both fail. Phase 3: Persona-Based Query Generation Generate queries from diverse user personas (e.g., \"Business Analyst,\" \"Student\") for each gap. Prompt in Appendix H.1. Phase 4: Final Verification Retrieve semantically similar tools and use \"Judge\" agent to confirm no tool can solve the query. Only definitively out-of-scope queries are included. Prompt in Appendix H.1. Simulated Execution Feedback: Successful validation produces structured record of the tool call (tool name, server name, input schema). This serves as the observation that is passed back into the next reasoning cycle. Failure Handling: If the tool cannot be matched in the MCP dataset, the step is marked as failed and the orchestrator continues the cycle with the updated context. This design preserves the iterative reasonactobserve nature of ReAct while ensuring evaluation is feasible in purely offline benchmark setting. Multi-Attempt Variant (ReAct@N) We extend the original design to allow multiple independent reasoning trajectories for robustness. In ReAct@N, the orchestrator generates up to distinct solution paths (via diversified orchestration prompts). The overall attempt is deemed successful if at least one trajectory leads to valid tool orchestration. This extension provides systematic way to evaluate the sensitivity of ReAct-style agents to orchestration variance. Configuration The ReAct implementation in MSCBench uses two role-based LLMs: Router LLM: Handles query classification, server ranking, tool ranking, and orchestration prompts. Conversational LLM: Generates direct responses for queries determined not to require tool usage (L5 scenarios). Key configurable parameters include the number of servers to shortlist (server_top_k), the number of tools per server (tool_top_k), and the maximum number of reasoning cycles per attempt. For ReAct@N, we set {1, 3, 5} in line with our experimental setup. Error Handling and Level 5 Detection To ensure consistent benchmarking, we implemented explicit error classification (e.g., server_selection_error, tool_selection_error, orchestration_error). Additionally, we introduce post-hoc check to distinguish between genuine Level 5 cases (queries not requiring tools) and process failures. This detection relies on analyzing the reasoning text and the absence of attempted tool calls. Prompting Structure The ReAct orchestrator uses structured prompting to enforce consistent outputs across stages, including prompts for server selection, tool selection, orchestration, and error handling. For example, the server selection stage requires JSON array of server names. The full set of ReAct prompts is provided in Appendix I, see in particular: Server Selection Prompt (I.2) Tool Selection from Servers Prompt (I.2) Needle Selection Prompt (I.2) Orchestration Prompt (I.2) C.2 ToolShed Orchestrator Overview The ToolShed framework (Lumer et al., 2024) is designed to address the challenge of largescale tool retrieval by combining enhanced tool representations with advanced Retrieval-Augmented Generation (RAG) techniques. ToolShed introduces the ToolShed Knowledge Base (TKB), vector database containing enriched tool documents, and an Advanced RAG-Tool Fusion pipeline that balances retrieval accuracy and efficiency. The pipeline consists of three major phases: pre-retrieval enrichment, intra-retrieval query transformation and retrieval, and post-retrieval reranking. Adaptations for MSC-Bench In our benchmark, we implement ToolShed as high-performance baseline for flat tool retrieval, with several adaptations to fit the MCP ecosystem and our evaluation requirements: Pre-retrieval: We adopt the original ToolShed pre-retrieval methodology during data preparation by enhancing MCP server and tool embeddings with metadata, hypothetical queries, and topical expansions. This ensures compatibility with the vector-based retrieval stage. Tool-vs-Conversation Classification: Unlike the original ToolShed pipeline, we prepend classification stage to determine whether query requires tool usage. Queries classified as conversational are redirected to standard LLM response pathway (Level 5 handling). Intra-retrieval: The intra-retrieval phase follows the ToolShed design, including: 1. Query Rewriting for normalization and intent clarification. 2. Query Expansion and Decomposition into multiple semantically diverse variations. 3. Multi-query Retrieval from the MCP-enhanced tool pool using embedding similarity. Post-retrieval: Retrieved candidates are deduplicated and reranked using an LLM-based reranker conditioned on both the original query and expanded variations. This step mirrors ToolSheds Advanced RAG-Tool Fusion. Self-Reflection for Robustness: We extend the original framework by incorporating self-reflection step that allows the orchestrator to reject infeasible or out-of-scope requests, aligning with Level 5 evaluation in MSC-Bench. Configuration The ToolShed orchestrator requires multiple role-specialized LLMs, including: Router: Classifies queries and oversees final tool selection. Query Rewriter: Normalizes and clarifies input queries. Query Expander: Generates multiple diverse query variations. Decomposer: Produces structured sub-queries when appropriate. Reranker: Reorders candidate tools using cross-query context. Iterative Invocation: Theoretically supports multi-turn refinement of tool requests when initial retrieval is insufficient. Adaptations for MSC-Bench Our implementation of MCP-Zero preserves its hierarchical retrieval philosophy but introduces several practical adaptations for benchmark evaluation: Conversational: Provides direct responses for non-tool queries. Key parameters include the number of query variations generated (query_expansion_count), the similarity threshold for embedding retrieval, and the number of top candidates reranked (rerank_top_k). Error Handling and Level 5 Detection To support our evaluation protocol, the orchestrator is instrumented with explicit error logging and classification (e.g., decomposition errors, retrieval failures, reranking format errors). Out-of-scope or unfulfillable queries are rejected through the self-reflection mechanism, which outputs the standardized Level 5 rejection tuple ({server: \"No\", tool: \"No\"}). Prompting Structure ToolSheds implementation uses specialized prompts at each retrieval stage, including query rewriting, expansion, decomposition, and reranking. The full set of ToolShed prompts is provided in Appendix I, see: Rewrite Prompt (I.3) Query Expansion Prompt (I.3) Rerank Prompt (I.3) Shared prompts: Classification (I.1), Tool Selection (I.1), Query Decomposition (I.1) C.3 MCP-Zero Orchestrator Overview MCP-Zero (Fei et al., 2025) proposes framework for active tool discovery, enabling LLM agents to dynamically request tools on demand rather than relying on static schema injection. This design reduces context overhead, improves scalability to thousands of tools, and grants agents autonomy to specify capability gaps explicitly. The original methodology combines: Active Tool Requests: LLMs generate structured requests specifying both the server domain and tool operation. Hierarchical Semantic Routing: two-stage retrieval process that first matches servers by embeddings and then ranks tools within the top servers. Tool-vs-Conversation Classification: As with ToolShed, we prepend classification stage to determine whether query requires tools or can be directly answered conversationally. Multi-Turn Decomposition: While the original MCP-Zero mentions Iterative Active Invocation, concrete implementation details were not available in the paper or reference repository. To operationalize this, we integrate the query decomposition mechanism from ToolShed. The decomposer LLM generates subqueries, which are then independently processed through the MCP-Zero retrieval pipeline. Enhanced Embeddings: Instead of raw embeddings, we leverage the pre-retrieval enhanced embeddings from the ToolShed-prepared MCP server/tool pool, ensuring more robust semantic matching. Self-Reflection for Robustness: reflection stage is included to allow graceful rejection of out-of-scope queries, aligning with Level 5 evaluation. Configuration The MCP-Zero orchestrator relies on three essential LLM roles, with an additional decomposer to enable multi-turn capability: Router: Classifies queries and selects the final tool. Retriever: Generates structured tool requests in the <tool_assistant> format. Conversational: Provides direct answers for queries not requiring tool usage. Decomposer: Expands queries into sub-queries to support multi-turn retrieval. Key hyperparameters include the similarity threshold for embeddings, the number of servers shortlisted (server_top_k), and the number of tools ranked within each server (tool_top_k). Error Handling and Level 5 Detection Similar to ToolShed, explicit error categories (classification errors, retrieval failures, selection errors) are logged to ensure interpretability of benchmark results. Self-reflection enables the orchestrator to reject infeasible tasks, producing the standardized rejection tuple {server: \"No\", tool: \"No\"}. Prompting Structure MCP-Zero uses structured prompts to elicit tool requests and tool selections. The key prompt is the Tool Transformation Prompt (I.4), which ensures the retriever LLM emits structured <tool_assistant> requests. In addition, MCP-Zero uses the shared classification, decomposition, and tool selection prompts listed in Appendix I. C.4 Hybrid Orchestrator Overview The Hybrid orchestrator represents an experimental design introduced in MSC-Bench to explore the potential synergy between the active tool request formulation of MCP-Zero and the advanced retrieval pipeline of ToolShed. The motivation is to test whether combining MCP-Zeros structure-aware tool request transformation with ToolSheds powerful multi-query retrieval and reranking mechanisms yields improvements in robustness and accuracy. Architecture The Hybrid pipeline largely follows the three-phase structure of ToolShed, with one critical substitution: Stage 1 (Pre-Processing): Instead of performing query rewriting as in ToolShed, the Hybrid agent invokes MCP-Zeros Tool Transformation Prompt to convert the raw user query into structured tool request containing explicit <server> and <tool> descriptors. Stage 2 (Intra-Retrieval): The structured request is then passed into ToolSheds query decomposition and expansion modules. Multiple query variations are generated, embeddings are computed, and tools are retrieved across the MCP server/tool pool. Stage 3 (Post-Retrieval): Candidate tools are deduplicated and reranked using ToolSheds LLM-based reranking stage, with the top-k tools selected for final evaluation. Adaptations for MSC-Bench Added tool-vs-conversation classification step (same as other orchestrators) to bypass retrieval when direct conversational answers suffice. Integrated the Tool Transformation Prompt (from MCP-Zero) into the pre-retrieval stage, replacing ToolSheds rewrite prompt. This ensures that queries are reformulated in structured, server/tool-aware manner before expansion and retrieval. Leveraged the enhanced embeddings of the ToolShed-prepared MCP dataset for retrieval. Included the same self-reflection mechanism as other orchestrators, allowing the agent to reject out-of-scope queries and align with Level 5 evaluation. Configuration The Hybrid orchestrator uses an extended set of LLM roles: Router: Handles classification and final tool selection. Retriever: Performs tool request transformation (adopted from MCP-Zero). Query Expander: Generates multiple variations of the transformed request. Reranker: Reorders candidate tools after multiquery retrieval. Conversational: Responds to non-tool queries. Decomposer: Enables multi-turn sub-query processing for complex requests. Key parameters mirror those of ToolShed, with query_expansion_count and rerank_top_k controlling the breadth of retrieval and reranking depth. Prompting Structure The Hybrid orchestrator combines MCP-Zeros structured tool transformation with ToolSheds query expansion and reranking. It therefore uses the Tool Transformation Prompt (I.4), along with the ToolShed expansion (I.5) and rerank prompts (I.5), plus the shared classification, decomposition, and tool selection prompts in Appendix I. Remarks This design allows us to empirically test whether explicit, structured tool requests (MCPZero) can improve downstream retrieval quality when combined with ToolSheds RAG-Tool Fusion. As an experimental variant, it is not intended as new SOTA baseline, but rather as probe into potential complementarities between structure-aware and retrieval-enhanced paradigms."
        },
        {
            "title": "Example",
            "content": "This appendix provides comprehensive evaluation framework for MSC-Bench, including detailed protocols, metric definitions, scoring mechanisms, and validation procedures for each level. D.1 Level-Specific Evaluation Protocols Level 1 (L1): Foundational Tool Identification Objective Establish baseline for mapping direct, unambiguous user directive to the correct tool within specified server."
        },
        {
            "title": "Evaluation Process",
            "content": "1. The agent receives the full server specification, including tool descriptions and input schemas 2. The agent must identify the single correct tool for the given query 3. Correctness is strictly based on exact identity Query: \"Convert this image to JPEG format\" Valid Tools: {convert_to_jpeg, image_format_converter} Correct: Either tool from the valid set Level 3 (L3): Intra-Server Task Orchestration Objective Measure the agents capacity to decompose high-level goal into coherent, executable plan using tools within single server."
        },
        {
            "title": "Evaluation Process",
            "content": "1. Agent must construct DAG representing the execution plan 2. All tools must belong to the same server 3. Dependencies must be correctly ordered 4. Both exact sequence matching and tool selection accuracy are evaluated with the ground-truth tool tuple"
        },
        {
            "title": "Example",
            "content": "4. No partial credit is awarded for similar or functionally equivalent tools"
        },
        {
            "title": "Example",
            "content": "Query: \"Run Semgrep scan on vulnerable_code.js with config.yaml\" Expected: semgrep_scan from Semgrep MCP"
        },
        {
            "title": "Server",
            "content": "Correct: Exact match with tool name and server Level 2 (L2): Disambiguation Among Functionally Equivalent Tools Objective Assess reasoning under functional redundancy, where multiple tools can fulfill the same request."
        },
        {
            "title": "Evaluation Process",
            "content": "1. The agent receives the query and candidate Query: \"Analyze code for vulnerabilities and create report\" Expected Plan: semgrep_scan generate_report (both from Semgrep server) Evaluation: Exact sequence and tools within single server Level 4 (L4): Cross-Server Task Orchestration Objective Measure the agents capacity to decompose high-level goal into coherent, executable plan using tools across multiple servers."
        },
        {
            "title": "Evaluation Process",
            "content": "1. Agent must construct DAG with tools from different servers 2. Correct server identification is required 3. Cross-server dependencies must be properly ordered tools from the specified server 4. Both exact sequence matching and tool selec2. Multiple tools may be functionally equivalent tion accuracy are evaluated for the given task"
        },
        {
            "title": "Example",
            "content": "3. Predictions are scored against the ground-truth set of valid tools 4. Emphasis is on correctness of selection over preference Query: \"Scan code for vulnerabilities, then email the results to the team\" Expected Plan: semgrep_scan (Semgrep server) send_email (Email server) Evaluation: Exact sequence, tools, and server identification Level 5 (L5): Rejection of Unfulfillable Requests Objective Evaluate the agents ability to correctly reject requests impossible to fulfill within the MCP ecosystem."
        },
        {
            "title": "Evaluation Process",
            "content": "1. The ground truth for L5 is an empty execution plan 2. Correct rejection requires the exact tuple {server: \"no\", tool: \"no\"} 3. Any other output is considered failure 4. Ensures the metric captures judgment of feasibility rather than operational errors"
        },
        {
            "title": "Example",
            "content": "Query: \"Turn on the lights in my living room\" Expected: {server: \"no\", tool: \"no\"} Correct: Proper rejection of physical world interaction"
        },
        {
            "title": "E Additional Statistics",
            "content": "Figure 7: Distribution of 31 capability gap categories for Level 5 out-of-scope queries. Each category represents well-defined domain outside the digital MCP ecosystem boundaries. the digital MCP ecosystem boundaries, systematically organized to ensure comprehensive robustness testing: Physical World Interaction (28 queries, 31%) Smart Home Control: Controlling lights, temperature, security systems Physical Device Control: Controlling printers, scanners, cameras, IoT devices This appendix provides detailed statistical visualizations and distribution charts that support the main analysis in Section 4. Real-Time Sensory Processing (22 queries, 24%) Audio Processing: Real-time speech recognition, audio analysis, voice commands E.1 Category Distribution The distribution of server categories in the MSCBench corpus reflects the diverse ecosystem of available MCP servers. This visualization shows how tools are distributed across different functional domains. Figure 6: Distribution of server categories in the MSCBench corpus. The 30 categories represent diverse functional domains, from development tools to productivity applications. E.2 Level 5 Capability Gap Analysis The systematic identification of capability gaps ensures comprehensive robustness testing across different domains. This distribution shows how the 91 out-of-scope queries are distributed across 31 distinct capability gap categories. Capability Gap Categories The 31 capability gap categories represent well-defined domains outside Visual Processing: Real-time video analysis, object recognition, facial detection Agent Judgment Requirements For Level 5 queries, agents must demonstrate robust capability boundary detection by: 1. Comprehensive Tool Review: Examining all available tools across the entire corpus to determine if any tool can address the query 2. Capability Boundary Recognition: Understanding the fundamental limitations of the digital MCP ecosystem 3. Confident Rejection: Only rejecting queries after thorough verification that no tool exists to solve them 4. Clear Communication: Responding with {server: \"no\", tool: \"no\"} when definitively out-of-scope This design ensures that agents cannot simply guess or make assumptions about capability limitations, but must systematically verify tool availability before concluding that query is unfulfillable."
        },
        {
            "title": "F Detailed Evaluation Results",
            "content": "This appendix provides comprehensive experimental results for the ToolShed ablation study, includ-"
        },
        {
            "title": "Configuration",
            "content": "k QE RR 5 5 1 5 10 15 20 25 3 0 0 3 3 3 3 3 10 10 10 1 10 10 10 10 L1 EM 57.03 57.03 45.78 54.09 56.64 55.88 57.80 56.01 L2 EM 42.12 41.60 36.56 42.50 40.69 43.15 35.91 33.72 L3 EM 46.17 50.76 36.08 44.95 47.40 48.31 50.15 47.70 L4 F1 50.77 47.11 37.91 46.78 50.95 49.55 52.11 45.54 L5 EM 14.56 11.65 4.85 10.67 13.59 14.56 10.67 12.62 Table 7: Complete ToolShed ablation study results on Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024). Configuration: = tool_top_k (retrieval breadth), QE = query_expansion_count, RR = rerank_top_k. Metrics: EM = Exact Match (%), F1 = F1 Score (%) for L4 only. Bold values indicate optimal performance within each metric. ing all tested parameter configurations and their corresponding performance metrics across evaluation levels L1-L5. L3: Slight degradation with expansion (74.34% vs 76.98% F1) L4: Benefits from expansion (50.77% vs 47.11% F.1 ToolShed Parameter Ablation - Complete F1)"
        },
        {
            "title": "Results",
            "content": "Table 7 presents the complete ablation study results for ToolShed on Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024), systematically varying retrieval breadth (tool_top_k), query expansion (query_expansion_count), and reranking parameters (rerank_top_k). Reranking Criticality Comparison of full reranking (RR=10) vs limited reranking (RR=1) at k=5, QE=3: Substantial degradation across all levels when reranking is limited F.2 Key Observations from Complete Data Most pronounced impact on L2 EM (42.12% The complete results reveal several important patterns: Retrieval Breadth Effects Systematic variation of tool_top_k from 1 to 25 shows: L1 (Direct Matching): Performance stabilizes after k=5, showing resilience to irrelevant candidates L2 (Context-Aware): Peak performance at k=15 (43.15% EM), with degradation at higher values due to noise L3 (Intra-Server): Consistent improvement with increased breadth, peaking at k=5 without query expansion (76.98% F1) L4 (Cross-Server): Optimal performance at k=20 (52.11% F1), benefiting from diverse tool coverage Query Expansion Impact Direct comparison between configurations with and without query expansion (k=5, RR=10): L2: Minimal impact (42.12% vs 41.60% EM) 42.50% with limited reranking) Demonstrates the critical role of reranking in tool selection quality These comprehensive results support the main findings presented in Section 5 and provide detailed evidence for the parameter sensitivity analysis shown in Figure 4. F.3 Comprehensive Performance and"
        },
        {
            "title": "Efficiency Analysis",
            "content": "See the main text Table 3 for the complete experimental results across all architectures, foundation models, and evaluation levels, including both performance metrics and normalized latency measurements. This comprehensive view reveals several critical insights into the performance-efficiency trade-offs inherent in different tool orchestration approaches. Architecture-Level Performance Patterns The results demonstrate clear architectural advantages across different complexity levels. ToolShed consistently outperforms MCP-Zero in raw performance metrics, achieving superior scores on complex orchestration tasks (L3: 84.34% vs 53.97% F1 peak, G.1 Level 1 Task Examples Example: Level 1 (Direct Tool Retrieval) User Query: On Semgrep, run semgrep_scan on the file \"vulnerable_code.js\" with the configuration file \"semgrep_config.yaml\" to detect vulnerabilities and return findings in JSON format. Expected Tool: Server: Semgrep MCP"
        },
        {
            "title": "Server",
            "content": "Tool Name: semgrep_scan Description: Run static code analysis on provided files using Semgrep to detect vulnerabilities and return findings in JSON format for detailed inspection and remediation. Figure 8: Level 1 Example: Direct tool retrieval where the user query explicitly mentions the tool name and server. The task requires the agent to identify the exact tool from the specified server that matches the users request. L4: 55.06% vs 34.20% F1 peak). However, this performance advantage comes at substantial computational cost, with ToolShed configurations showing 3-20 higher normalized latency compared to their MCP-Zero counterparts. Foundation Model Sensitivity The comprehensive data reveals significant foundation model dependencies that extend beyond simple capability differences. For ToolShed architectures, GPT-4.1 (OpenAI et al., 2024) achieves the highest performance on L4 cross-server orchestration (55.06% F1) and L5 rejection tasks (81.31% EM), while Meta-Llama3-8B-Instruct (Grattafiori et al., 2024) excels specifically on L3 intra-server coordination (84.34% F1). Conversely, MCP-Zero shows more consistent performance patterns across models, with Qwen3-4BInstruct-2507 (Yang et al., 2025) demonstrating exceptional efficiency (0.22-6.50 normalized latency) while maintaining competitive accuracy. Efficiency-Performance Trade-off Analysis The normalized latency measurements reveal clear architectural trade-off: MCP-Zeros hierarchical approach achieves 2-10 better efficiency across most configurations, while ToolSheds dense retrieval and reranking pipeline delivers superior absolute performance at the cost of computational overhead. Notably, some ToolShed configurations (e.g., Microsoft Phi-4 (Abdin et al., 2024)) show both poor performance and high latency, indicating that the architectures benefits are highly dependent on foundation model compatibility. These detailed results provide the empirical foundation for the efficiency analysis presented in Section 5 and demonstrate the complexity of balancing performance and computational costs in tool orchestration systems."
        },
        {
            "title": "G Representative Examples",
            "content": "This appendix provides comprehensive representative examples for each level of MSC-Bench, demonstrating the complexity and characteristics of tasks at each level. Example: Level 1 (File Operations) Example: Level 2 (Multiple Search Tools) User Query: Search for information about \"machine learning algorithms\" in the documentation. Valid Tools: Tool 1: search_docs - Search through documentation files Tool 2: find_content - Find specific content within documents Tool 3: query_knowledge_base - Query the knowledge base for information Expected Response: Any of the three tools can be used to search for the requested information. Figure 11: Level 2 Example: Multiple functionally equivalent tools for searching documentation. The agent must understand that any of these tools can fulfill the search request. User Query: Use the Filesystem MCP Server to create new directory called \"project_docs\" in the current working directory. Expected Tool: Server: Filesystem MCP Server Tool Name: create_directory Description: Create new directory at the specified path with the given name. Figure 9: Level 1 Example: Simple file system operation where the user clearly specifies the action and target. The agent must identify the correct tool for directory creation. G.2 Level 2 Task Examples Example: Level 2 (Functionally Equivalent Tools) User Query: Convert this image to JPEG format with 90% quality. Valid Tools: Tool 1: convert_to_jpeg - Convert images to JPEG format with specified quality Tool 2: image_format_converter - Convert images between different formats including JPEG Expected Response: Either tool is acceptable as both can perform the requested conversion. Figure 10: Level 2 Example: Ambiguous query where multiple tools can fulfill the same request. The agent must recognize that either tool is valid choice for the conversion task. G.3 Level 3 Task Examples Example: Level 3 (Database Operations) Example: Level 3 (Single-Server MultiStep Orchestration) User Query: Analyze the code in src/main.py for security vulnerabilities, generate detailed report, and send it to the development team. Expected Execution Plan: 1. Step 1: analyze_code - Scan the Python file for security issues 2. Step 2: generate_report -"
        },
        {
            "title": "Create a detailed vulnerability\nreport",
            "content": "3. Step 3: send_notification - Send the report to the development team Dependencies: Step 2 depends on Step 1, Step 3 depends on Step 2. Figure 12: Level 3 Example: Multi-step orchestration within single server (Security Analysis Server). The agent must decompose the high-level goal into sequence of dependent tool calls. User Query: Create new user account with the email \"user@example.com\", set up their profile, and send them welcome email. Expected Execution Plan: 1. Step 1: create_user - Create new user account 2. Step 2: setup_profile -"
        },
        {
            "title": "Configure user profile settings",
            "content": "3. Step 3: send_welcome_email - Send welcome email to new user Dependencies: Step 2 depends on Step 1, Step 3 depends on Step 1. Figure 13: Level 3 Example: User management workflow within single server. The agent must plan the sequence of operations to complete the user onboarding process. G.4 Level 4 Task Examples Example: Level 4 (Data Pipeline) Example: Level 4 (Multi-Server Orchestration) User Query: Deploy the latest version of my web application to production, run security scans, and notify the team about the deployment status. Expected Execution Plan: 1. Step 1: deploy_app (Deployment Server) - Deploy application to production 2. Step 2: run_security_scan (Security Server) - Perform security assessment 3. Step 3: send_notification (Notification Server) - Notify team of deployment status Dependencies: Step 2 depends on Step 1, Step 3 depends on Steps 1 and 2. User Query: Extract data from the customer database, process it for analytics, and create visualizations for the quarterly report. Expected Execution Plan: 1. Step 1: extract_customer_data (Database Server) - Extract customer data 2. Step 2: process_analytics (Analytics Server) - Process data for analytics 3. Step 3: create_visualizations (Visualization Server) - Generate charts and graphs 4. Step 4: compile_report (Report Server) - Compile quarterly report Dependencies: Sequential dependencies across all steps. Figure 14: Level 4 Example: Complex multi-server orchestration involving deployment, security, and notification services. The agent must coordinate tools across multiple servers. Figure 15: Level 4 Example: Data processing pipeline involving multiple specialized servers. The agent must orchestrate complex workflow across different services. G.5 Level 5 Task Examples Level 3: Multi-step orchestration within Example: Level 5 (Physical World Interaction) User Query: Turn on the lights in my living room and adjust the temperature to 22C. Expected Response: {server: \"no\", tool: \"no\"} single server Level 4: Complex orchestration across multiple servers Level 5: Rejection of requests outside the digital ecosystem Key Characteristics Each level exhibits distinct characteristics: Level 1: High precision requirements, explicit tool specification Reasoning: This request requires Level 2: Reasoning about functional equivaphysical world interaction (controlling lights and temperature) which is outside the scope of the digital MCP ecosystem. The agent should correctly reject this request. Figure 16: Level 5 Example: Physical world interaction request that should be rejected. The agent must recognize that this type of request cannot be fulfilled within the digital tool ecosystem. Example: Level 5 (Real-Time Sensory Processing) User Query: Listen to the conversation in the next room and summarize what theyre discussing. lence, ambiguity handling Level 3: Task decomposition, dependency management Level 4: Cross-server coordination, complex workflow orchestration Level 5: Boundary recognition, appropriate rejection Evaluation Challenges The examples highlight key evaluation challenges: Level 1: Exact matching requirements Level 2: Set-based evaluation, multiple valid answers Level 3: Dependency ordering, execution plan validation Level 4: Multi-server coordination, complex Expected Response: {server: \"no\", dependency graphs tool: \"no\"} Reasoning: This request requires real-time audio processing and eavesdropping, which involves privacy concerns and real-time sensory capabilities not available in the digital MCP ecosystem. Figure 17: Level 5 Example: Real-time sensory processing request that should be rejected. The agent must recognize privacy and capability limitations. G.6 Example Analysis Complexity Progression The examples demonstrate clear progression in complexity: Level 1: Direct tool identification with explicit tool names Level 2: Ambiguous queries requiring reasoning about tool equivalence Level 5: Rejection accuracy, boundary understanding"
        },
        {
            "title": "H Prompt Templates",
            "content": "This appendix provides comprehensive prompt templates used throughout MSC-Bench, including task generation prompts, orchestrator prompts, and evaluation prompts. H.1 Task Generation Prompts"
        },
        {
            "title": "Server Filtering Prompt",
            "content": "You are highly precise AI Benchmark Quality Analyst. Your mission is to evaluate summarized MCP Server and classify it based on strict set of rules. Your judgment must be consistent and based ONLY on the definitions provided. ## CRITICAL DEFINITION: What is \"Native LLM Capability\"? For this task, capability is **\"native\" if and only if** Large Language Model can perform it using **solely its internal, pre-trained knowledge and reasoning abilities, without ANY access to external tools, APIs, or real-time data.** Think of \"pure\" LLM in sandbox with no internet connection. - **Native Examples:** Answering \"What is the capital of France?\", calculating 2+2, summarizing provided text. - **NON-Native Examples:** Accessing todays news (requires web search), creating GitHub issue (requires API interaction), checking stock price (requires real-time data). Your entire evaluation MUST adhere to this strict definition. ## MCP Server Summary to Analyze {mcp_server_json_content} ## REASONING & EVALUATION STEPS 1. **Analyze Core Functionality:** Based on the servers description and tools, what is its primary function? 2. **Apply the Critical Definition:** Can pure LLM perform this function using only its internal knowledge? 3. **Make Your Classification:** Based on your analysis, classify this server. ## CLASSIFICATION RULES **CLASSIFICATION: NATIVE** - The servers primary function can be performed by pure LLM using only internal knowledge - Examples: Text processing, mathematical calculations, language translation, summarization **CLASSIFICATION: NON-NATIVE** - The server requires external tools, APIs, real-time data, or system access - Examples: Web scraping, file operations, API calls, system commands, database access ## OUTPUT FORMAT Provide your classification in exactly this format: CLASSIFICATION: [NATIVE/NON-NATIVE] REASONING: [Your detailed reasoning in 2-3 sentences explaining why you chose this classification]"
        },
        {
            "title": "Platform ID Prompt",
            "content": "PLATFORM_ID_PROMPT = \"\"\" # ROLE & GOAL You are highly intelligent text analysis engine. Your task is to identify the primary **user-facing Platform, Brand, or Product Keyword** from the provided text. # INSTRUCTIONS 1. Read the \"Text to Analyze\" carefully. 2. Identify the main, specific, proper noun that represents the core service or brand being offered. 3. This keyword should be unique, user-recognizable identifier, like \"GitHub\", \"Notion\", \"UseGrant\", \"Stripe\", or \"PaddleOCR\". 4. **Crucially, do NOT extract generic terms OR technology protocols.** This includes common words like \"API\", \"Server\", \"Platform\", \"Service\", and especially technical standards like **\"MCP\"** or **\"Model Context Protocol\"**. 5. If you cannot find specific, unique platform keyword after ignoring the terms above, the keyword is \"N/A\". 6. Your output MUST be single line containing the keyword **wrapped in <keyword></keyword> tags**. # EXAMPLES ### Example 1 (Ignoring \"MCP\" and \"API\") Text to Analyze: \"This is Model Context Protocol (MCP) server for interacting with the UseGrant API. It provides set of tools for managing providers and clients through the UseGrant platform.\" Your Output: <keyword>UseGrant</keyword> ### Example 2 (Specific Technology Brand) Text to Analyze: \"A server for performing optical character recognition (OCR) using the powerful PaddleOCR engine.\" Your Output: <keyword>PaddleOCR</keyword> ### Example 3 (No Specific Brand Found) Text to Analyze: \"An MCP server that provides comprehensive architectural expertise through specialized agents, resources, and tools.\""
        },
        {
            "title": "User Facing Classifier Prompt",
            "content": "Your Output: <keyword>N/A</keyword> # ROLE & GOAL You are an AI Product Manager. Your task is to classify given tool --- based on its intended user. You need to determine if its **\"User-Facing Task\"** or **\"System-Facing Task\"**. # DEFINITIONS - **User-Facing Task**: An action that typical end-user (like writer, designer, project manager, or even developer using platform) would directly command an AI assistant to perform to achieve personal or business goal. These tasks operate on user-understandable concepts like documents, repositories, images, emails, or playlists. - **System-Facing Task**: An action related to system administration, infrastructure management, backend debugging, or managing abstract, non-visible resources. These tasks are typically performed by system administrators or developers maintaining service, not using it. They operate on concepts like caches, database indexes, memory entries, or container pods. # INSTRUCTIONS 1. Read the tools name and description carefully. 2. Based on the definitions, decide if its \"User-Facing Task\" or \"System-Facing Task\". 3. Your output MUST be single line containing the classification **wrapped in <classification></classification> tags**. The value must be either user_facing or system_facing. # EXAMPLE 1 ## Tool Name: \"create_spreadsheet\" ## Tool Description: \"Creates new spreadsheet in the users cloud drive.\" ## Your Output: <classification>user_facing</classification> # EXAMPLE 2 ## Tool Name: \"clear_redis_cache\" ## Tool Description: \"Purges all keys from the specified Redis cache instance to free up memory.\" ## Your Output: <classification>system_facing</classification> # EXAMPLE 3 ## Tool Name: \"delete_memory_entry\" ## Tool Description: \"Deletes specific memory entry for user from the agents long-term memory service.\" ## Your Output: <classification>system_facing</classification> --- # YOUR TASK ## Tool Name: \"{tool_name}\" ## Tool Description: \"{tool_description}\" ## Your Output: # YOUR TASK **Text to Analyze**: \"{text_to_analyze}\" **Your Output**: end{listing} noindenttextbf{Task Type Classification Prompt} label{app:task_type_prompt} begin{lstlisting}[ basicstyle=ttfamilytiny, breaklines=true ] # ROLE & GOAL You are pragmatic AI assistant designer. Your task is to analyze given tool and classify its primary user intent. You need to determine if this tool represents **\"Final Goal Task\"** or **\"Middleware Task\"**. # DEFINITIONS - **Final Goal Task**: task that user would directly ask an AI assistant to perform as complete, standalone goal. These tasks provide direct value to the user. (Examples: \"search_for_videos\", \"download_a_file\", \"send_an_email\", \"translate_text\"). - **Middleware Task**: task that is usually an intermediate or prerequisite step required to achieve larger goal. Users rarely, if ever, ask for this task directly. (Examples: \"login\", \"authenticate\", \"get_api_key\", \"list_available_regions\", \"check_status\"). # INSTRUCTIONS 1. Read the tools name and description. 2. Based on the definitions, decide if its \"Final Goal Task\" or \"Middleware Task\". 3. Your output MUST be single line containing the classification **wrapped in <task_type></task_type> tags**. The value must be either final_goal or middleware. # EXAMPLE 1 ## Tool Name: \"search_youtube_videos\" ## Tool Description: \"Searches for videos on YouTube based on query.\" ## Your Output: <task_type>final_goal</task_type> # EXAMPLE 2 ## Tool Name: \"youtube_login\" ## Tool Description: \"Authenticates the user and obtains an access token for the YouTube API.\" ## Your Output: <task_type>middleware</task_type> --- # YOUR TASK ## Tool Name: \"{tool_name}\" ## Tool Description: \"{tool_description}\" ## Your Output: Level 1 Generation Prompt # ROLE & GOAL You are an expert user of specific software tool. Your task is to write 2 user queries in English. These queries should be direct commands to an AI assistant, demonstrating how real user would request to use the specified tool on its target platform. The queries must be specific, self-contained, and unambiguous. # CONTEXT FOR YOUR TASK You are generating queries for the following specific tool: - **Platform Name**: \"{platform_name}\" (The user-facing brand or service, e.g., \"GitHub\", \"Heroku\", \"Pyodide\") - **Server Name**: \"{server_name}\" (The name of the software package providing the tool, e.g., \"github-mcp-server\") - **Tool Name**: \"{tool_name}\" (The specific function to be executed, e.g., \"create_repository\") - **Tool Description**: \"{tool_description}\" (What the tool does in plain English) # INSTRUCTIONS 1. **Direct Command Tone**: Your queries should be direct commands, not questions. 2. **Mention Platform**: Each query MUST explicitly mention the **Platform Name** (\"{platform_name}\"). This is crucial for context. 3. **Reflect Tools Function**: The commands intent MUST be direct application of the **Tool Description** and its **Tool Inputs**. 4. **AVOID AMBIGUOUS REFERENCES (VERY IMPORTANT)**: - Do NOT use vague pointers like \"this code\", \"that file\", \"the script\". - For tools that execute code, embed short, realistic code snippet directly in the query. - For tools that use files, specify plausible filename. 5. **Be Specific**: Incorporate realistic example values for the parameters listed in **Tool Inputs**. This makes the query more concrete and useful for training. 6. **Format**: You MUST wrap each generated query in <query></query> tags. # EXAMPLE ## Context: - **Platform Name**: \"GitHub\" - **Server Name**: \"github-mcp-server\" - **Tool Name**: \"create_repository\" - **Tool Description**: \"Creates new repository on GitHub.\" ## Your Output: <query>On GitHub, create new private repository named my-secret-project with the description This is for the new API.</query> <query>Use GitHub to create public repository called awesome-list.</query> --- # YOUR TASK Now, using the context provided at the top, generate 2 user queries following all the rules. # YOUR OUTPUT Level 1 Verification Prompt # ROLE & GOAL You are meticulous AI System Analyst. Your task is to perform strict validation of user query against specific tool. The query must be perfect example of command user would give for this tool. # CONTEXT - **Platform Name**: \"{platform_name}\" - **Tool Name**: \"{tool_name}\" - **Tool Description**: \"{tool_description}\" # VALIDATION CRITERIA You must check the query against ALL of the following rules. If ANY rule is violated, the result is false. 1. **Intent Match**: Does the users primary goal in the query directly and logically map to the tools function described in the **Tool Description**? 2. **Platform Mention**: Does the query EXPLICITLY mention the required **Platform Name** (\"{platform_name}\")? 3. **Self-Contained & Unambiguous**: Is the query understandable on its own without needing prior conversation? It must NOT use vague references like \"this code\", \"that file\" unless it also provides concrete example (e.g., embedding code, providing filename). 4. **No Meta-Language**: Does the query sound like real user? It must NOT refer to the tool itself (e.g., \"use this tool\", \"run the function\"). The command must be direct. # YOUR TASK Analyze the query below based on all the criteria. ## Query to Verify: \"{query}\" # YOUR RESPONSE Structure your output as follows: - First, provide the boolean judgement (true or false) **wrapped in <is_match></is_match> tags**. - Second, on new line, provide one-sentence explanation for your decision, specifically mentioning which rule was passed or failed. # EXAMPLE 1: Perfect Match (All Rules Pass) ## Context: - Platform Name: \"GitHub\" - Tool Name: \"create_repository\" - Tool Description: \"Creates new repository on GitHub.\" ## Query to Verify: \"On GitHub, please create new repository for me named my-next-project.\" ## Your Output: <is_match>true</is_match> The query matches the tools intent, mentions the platform GitHub, is self-contained, and uses natural user language. # EXAMPLE 2: Failed (No Platform Mention) ## Context: - Platform Name: \"GitHub\" - Tool Name: \"create_repository\" - Tool Description: \"Creates new repository on GitHub.\" ## Query to Verify: \"Create new repository for me named my-next-project.\" ## Your Output: <is_match>false</is_match> The query fails validation because it does not explicitly mention the required platform GitHub. # EXAMPLE 3: Failed (Ambiguous Reference) ## Context: - Platform Name: \"Pyodide\" - Tool Name: \"execute-python\" - Tool Description: \"Executes string of Python code.\" ## Query to Verify: \"Run this Python code using Pyodide.\" ## Your Output: <is_match>false</is_match> The query fails validation because \"this Python code\" is an ambiguous reference, violating the self-contained rule. # EXAMPLE 4: Failed (Meta-Language) ## Context: - Platform Name: \"LocalFS\" - Tool Name: \"delete_file\" - Tool Description: \"Deletes file from the filesystem.\" ## Query to Verify: \"Use the LocalFS tool to delete the file temp.log.\" ## Your Output: <is_match>false</is_match> The query fails validation because \"Use the LocalFS tool\" is meta-language, not natural user language. Level 2 Platform Coupling Classification Prompt # ROLE & GOAL You are Product Analyst specializing in software tools and user behavior. Your task is to determine if tools core function is tightly coupled with its specific platform, or if it represents generic concept that exists across many platforms. # DEFINITIONS - **Tightly Coupled**: The tools main concept or terminology is unique to its platform and doesnt make sense outside of it. user would HAVE to mention the platform name to be understood. - *Examples*: \"Managing Heroku Dynos\", \"Creating GitHub Pull Request\", \"Resolving Jira Transition\". The concepts \"Dyno\", \"Pull Request\", and \"Jira Transition\" are iconic to their platforms. - **Generic Concept**: The tools function is common action that many different platforms offer under similar names. user could describe this task without mentioning specific brand. - *Examples*: \"Creating file\", \"Sending an email\", \"Uploading an image\", \"Renaming project\". # INSTRUCTIONS 1. Analyze the tools name, description, and the platform it belongs to. 2. Decide if typical user would naturally mention the platform when asking for this task. 3. Your output MUST be single line containing the classification **wrapped in <coupling></coupling> tags**. The value must be either tightly_coupled or generic_concept. # EXAMPLE 1 ## Platform: \"GitHub\" ## Tool Name: \"create_pull_request\" ## Tool Description: \"Creates new pull request to merge changes from one branch to another.\" ## Your Output: <coupling>tightly_coupled</coupling> # EXAMPLE 2 ## Platform: \"Google Drive\" ## Tool Name: \"create_document\" ## Tool Description: \"Creates new blank document in the users drive.\" ## Your Output: <coupling>generic_concept</coupling> 3. **Self-Contained & Unambiguous**: Is the query understandable on its own? It must not use vague references like \"this code\" unless concrete example is embedded. 4. **No Meta-Language**: Does the query sound like real user? It must not refer to the tool itself (e.g., \"use this tool\", \"run the function\"). # EXAMPLE 3 ## Platform: \"Stripe\" ## Tool Name: \"create_invoice\" ## Tool Description: \"Generates new invoice for customer.\" ## Your Output: <coupling>generic_concept</coupling> --- # YOUR TASK ## Platform: \"{platform_name}\" ## Tool Name: \"{tool_name}\" ## Tool Description: \"{tool_description}\" ## Your Output: # YOUR TASK Analyze the query below based on all the criteria, especially the Platform Mention Rule. ## Query to Verify: \"{query}\" # YOUR RESPONSE Structure your output as follows: - First, provide the boolean judgement (true or false) **wrapped in <is_match></is_match> tags**. - Second, on new line, provide one-sentence explanation for your decision, specifically mentioning which rule was passed or failed. Level 2 Bottom-Up Verification Prompt Level 2 Query Generation Prompt"
        },
        {
            "title": "Analyze these two tools and determine if they achieve the same end goal",
            "content": "with the same scope and depth. # ROLE & GOAL You are an expert user of various software, commanding an AI assistant. Your task is to write 2 natural-sounding user queries. The queries should reflect how real human would ask for task, based on how tightly the task is tied to its platform. Tool A: {first_tool[tool_name]} ({first_tool[server_name]}) Description: {first_tool[description]} Tool B: {second_tool[tool_name]} ({second_tool[server_name]}) Description: {second_tool[description]} # CONTEXT FOR YOUR TASK - **Platform Name**: \"{platform_name}\" - **Tool Name**: \"{tool_name}\" - **Tool Description**: \"{tool_description}\" - **Tool Inputs**: {formatted_schema} # CORE INSTRUCTION - **Platform Mention Rule**: {platform_mention_rule} # GENERAL INSTRUCTIONS 1. **Follow the Platform Mention Rule**: This is the most important instruction. Adhere strictly to whether you should or should not mention the platform name. 2. **Embody the User**: Your tone must be that of user giving command. 3. **AVOID META-LANGUAGE**: Do NOT refer to the tool itself (e.g., \"use the tool\"). 4. **Be Specific and Actionable**: Incorporate realistic example values for the parameters listed in \"Tool Inputs\". 5. **Format**: Wrap each query in <query></query> tags. # EXAMPLE 1: Tightly Coupled ## Platform Mention Rule: \"You MUST mention the platform GitHub because the function is iconic to it.\" ## Context: Tool is create_pull_request on GitHub. ## Correct Output: <query>Create pull request on GitHub to merge the feature-x branch into main.</query> <query>I need to open new GitHub pull request for my latest changes.</query> # EXAMPLE 2: Generic Concept ## Platform Mention Rule: \"You MUST NOT mention the platform Google Drive. The function is generic concept.\" ## Context: Tool is create_document on Google Drive. ## Correct Output: <query>Create new document for me titled Meeting Notes Q3.</query> <query>I need to start new doc.</query> --- # YOUR TASK Now, using the context and the CORE INSTRUCTION provided at the top, generate 2 user queries. # YOUR OUTPUT Level 2 Verification Prompt # ROLE & GOAL You are highly discerning AI Routing Analyst. Your task is to verify if generated user query is high-quality, natural-sounding training example for specific tool, following given rule. # CONTEXT - **Tools Platform**: \"{platform_name}\" - **Tool Name**: \"{tool_name}\" - **Tool Description**: \"{tool_description}\""
        },
        {
            "title": "Two tools are functionally equivalent if they achieve the same END",
            "content": "RESULT with similar scope: - Focus on WHAT they accomplish AND the breadth/depth of that accomplishment - Consider the users intent and the comprehensiveness of the output - Different implementation methods are acceptable (different servers, models, APIs) - But the scope, depth, and purpose of the end result should be equivalent Answer \"YES\" if they produce the same type AND scope of result for users, \"NO\" if they have different scope, depth, or purpose. Answer:\"\"\" Level 2 Round-Trip Verification Prompt # ROLE & GOAL You are highly intelligent AI routing engine. Your task is to analyze user queries and determine which of the provided candidate tools can fully and accurately resolve the request. # CONTEXT ## User Query: {query_list_text} ## Candidate Tools: Here is list of tools that might be able to help. Each tool is identified by unique tool_id. {candidate_tools_text} # INSTRUCTIONS 1. Read the user query to understand the core intent and requirements. 2. For each candidate tool, evaluate if its function directly matches this core intent. 3. tool is match ONLY IF its core function directly corresponds to the users request. Do not select tools that are only partially related. 4. Your final output MUST be list of tool_ids for the tools you have selected. 5. Each tool_id must be on new line. 6. The entire list of selected tool_ids MUST be wrapped in <selected_tools> and </selected_tools> tags. 7. If you determine that NONE of the candidate tools are suitable match, you must return an empty tag pair, like this: <selected_tools></selected_tools>. # EXAMPLE ## User Query: \"Create new git repository for my project alpha.\" ## Candidate Tools: ... (list of tools) ... ## Your Output (Example): <selected_tools> GitHub MCP Server::create_repository GitLab MCP Server::create_project </selected_tools> # VALIDATION CRITERIA You must check the query against ALL of the following rules. If ANY rule Level 3 Schema Inference Prompt is violated, the result is false. Infer plausible JSON output schema for the following tool: 1. **Platform Mention Rule Adherence**: The query must strictly follow this rule: **{platform_mention_rule}** 2. **Logical Routing Match**: Is the tool direct and sensible way to fulfill the users request? Tool Name: {tool_name} Description: {tool_description} Input Schema: {input_schema} Requirements: 1. Generate realistic JSON schema that matches the tools functionality 2. Include appropriate data types and structure 3. Make the schema specific but not overly complex Output Schema: { \"type\": \"object\", \"properties\": {"
        },
        {
            "title": "A user might want to get notifications on Slack for new issues created",
            "content": "in their GitHub repository. # EXAMPLE 2 (Not Feasible) ## Services: [A Weather Forecast Server, File Encryption Server] ## Your Output: <is_feasible>false</is_feasible> There is no common, logical workflow that directly connects weather forecasting with file encryption. // Your inferred properties here --- } } Level 3 Logical Verification Prompt Verify if the following tool chain represents logical, realistic workflow: Tool Chain: {tool_chain} Input-Output Flow: {flow_description} Check: 1. Is the workflow logically coherent? 2. Are the input-output mappings realistic? 3. Does the sequence make practical sense? 4. Would this workflow be useful to users? VERIFICATION: [PASS/FAIL] REASONING: [Detailed explanation of your decision] Level 3 Query and CoT Generation Prompt Generate high-level user query and Chain-of-Thought plan for the following tool chain: Tool Chain: {tool_chain} Workflow: {workflow_description} Requirements: 1. Create realistic user query that would require this workflow 2. Generate step-by-step Chain-of-Thought plan 3. Make the query specific and actionable 4. Ensure the plan matches the tool sequence User Query: [Your generated query] Chain-of-Thought Plan: 1. [First step] 2. [Second step] 3. [Third step] ... Level 4 Task Ideation Prompt Determine if coherent cross-server task can be composed using the following servers: Servers: {server_list} Categories: {category_list} Requirements: 1. Can these servers work together in logical workflow? 2. Is the task realistic and useful for users? 3. Does the task require multiple servers to complete? 4. Are the server capabilities complementary? Decision: [FEASIBLE/NOT_FEASIBLE] Reasoning: [Detailed explanation of your decision] If feasible, suggest high-level task concept. Level 4 Feasibility Check Prompt # ROLE & GOAL You are creative product manager designing tasks for powerful AI assistant. Your task is to determine if given combination of real-world services can form logical user workflow. # CONTEXT: PROVIDED SERVICES Here are the services available for the potential task: {services_description} # YOUR TASK: FEASIBILITY CHECK First, analyze the provided services. Can you imagine realistic, common user scenario that would require **combining ALL** of these services? The workflow must be logical. Respond with your boolean judgement (true or false) **wrapped in <is_feasible></is_feasible> tags**, followed by brief, one-sentence reason for your decision on new line. # EXAMPLE 1 (Feasible) ## Services: [A GitHub Server, Slack Server] ## Your Output: <is_feasible>true</is_feasible> # YOUR CHECK NOW ## Services: [As provided in the context above] ## Your Output: Level 4 Workflow Generation Prompt # SCENARIO GENERATION Based on the services and tools provided below, please generate user scenario. # AVAILABLE SERVICES AND TOOLS: {available_tools_section} # HIGH-QUALITY EXAMPLES Here are examples of well-structured queries and their corresponding ground truth tools: ## Example 1: {{\"query\": \"Get detailed information about the \"My Favorite Chair\" object within Blender scene and check the PolyHaven integration status.\", \"ground_truth_tools_count\": 2, \"ground_truth_tools\": [{{\"tool_id\": \"Tripo MCP Server::get_object_info\", \"server_name\": \"Tripo MCP Server\", \"tool_name\": \"get_object_info\", \"description\": \"Tool for get_object_info functionality provided by Tripo MCP Server\", \"dependencies\": []}}, {{\"tool_id\": \"Tripo MCP Server::get_polyhaven_status\", \"server_name\": \"Tripo MCP Server\", \"tool_name\": \"get_polyhaven_status\", \"description\": \"Tool for get_polyhaven_status functionality provided by Tripo MCP Server\", \"dependencies\": []}}]}} ## Example 2: {{\"query\": \"Scan the content of the file \"main.py\" using Semgrep, then retrieve the detailed results, check the status, and finally, get list of supported languages for security vulnerability detection.\", \"ground_truth_tools_count\": 4, \"ground_truth_tools\": [{{\"tool_id\": \"Semgrep MCP Server::start_scan_from_content\", \"server_name\": \"Semgrep MCP Server\", \"tool_name\": \"start_scan_from_content\", \"description\": \"Tool for start_scan_from_content functionality provided by Semgrep MCP Server\", \"dependencies\": []}}, {{\"tool_id\": \"Semgrep MCP Server::get_scan_results\", \"server_name\": \"Semgrep MCP Server\", \"tool_name\": \"get_scan_results\", \"description\": \"Tool for get_scan_results functionality provided by Semgrep MCP Server\", \"dependencies\": [0]}}, {{\"tool_id\": \"Semgrep MCP Server::get_scan_status\", \"server_name\": \"Semgrep MCP Server\", \"tool_name\": \"get_scan_status\", \"description\": \"Tool for get_scan_status functionality provided by Semgrep MCP Server\", \"dependencies\": [0]}}, {{\"tool_id\": \"Semgrep MCP Server::get_supported_languages\", \"server_name\": \"Semgrep MCP Server\", \"tool_name\": \"get_supported_languages\", \"description\": \"Tool for get_supported_languages functionality provided by Semgrep MCP Server\", \"dependencies\": []}}]}} # INSTRUCTIONS Based on the services and tools provided above, create scenario following these guidelines: 1. **Specific and Actionable Query**: Write detailed user query that includes: - Specific output formats (PDF, PPT, CSV, image, etc.) when relevant - File names and paths when relevant (e.g., /root/pdf/report.pdf, /home/user/data.csv) - Clear deliverables and requirements - Concrete subjects or targets (e.g., specific companies, topics, data sources) 2. **Ground Truth Tools**: For each tool needed in the workflow: - Use the exact tool names from the available services listed above - Set tool_id as \"ServerName::tool_name\" - Copy the description directly from the service definition or use the pattern \"Tool for [tool_name] functionality provided by [server_name]\" - Add dependencies as array of indices (0-based) if one tool depends on output from another - Include only essential tools needed for the workflow 3. **Natural Integration**: Ensure the query requires tools from ALL provided services in logical workflow that real user might request. 4. **Output Format**: Return your response as valid JSON object with the exact structure shown in the examples above. ## Your Scenario Generation: Level 4 Quality Control Prompt You are professional query quality assessment and improvement expert. Please evaluate whether the following query contains the necessary parameters for executing the required tools, and improve the query when needed. ROLE & GOAL As world-class product manager for general-purpose AI assistant, your goal is to brainstorm diverse list of common digital tasks users might want to perform. Original Query: {query} Expected tools and their parameter requirements: {tools_description} EVALUATION CRITERIA: 1. SOLVABILITY (1-10): - 10: All required data is provided, tools perfectly match needs, clear success criteria - 8-9: Task is clearly solvable with the given tools, minor ambiguities acceptable - 6-7: Mostly solvable but some steps may be challenging or unclear - 4-5: Significant gaps in tool coverage or data requirements - 1-3: Task cannot be meaningfully completed with available tools Consider: - Are all necessary tools available? - Is all required data provided (no external dependencies)? - Can the agent achieve the stated goal with these tools based on the function and output of the tools? - Are success criteria clear and measurable? 2. UTILITY (1-10): - 10: Critical business/research value, addresses real-world problem perfectly - 8-9: Strong practical value, useful for decision-making or operations - 6-7: Moderate value, interesting but not critical - 4-5: Limited practical value, mostly academic exercise - 1-3: Trivial or artificial task with no real-world application Consider: - Does this address real business or research need? - Would the results be actionable and valuable? - Is the complexity justified by the outcome? - Does it test meaningful agent capabilities? Provide scores and brief feedback in JSON format: {{ \"solvability_score\": <number 1-10>, \"utility_score\": <number 1-10>, \"solvability_feedback\": \"Brief explanation of solvability assessment\", \"utility_feedback\": \"Brief explanation of utility assessment\" }}"
        },
        {
            "title": "Equal Function Set Verification Prompt",
            "content": "Given this specific query: {query_text} Compare these two tools in the context of this query: Original Ground Truth Tool: {original_tool[tool_name]} ({original_tool[server_name]}) Description: {original_tool[description]} Candidate Replacement Tool: {candidate_tool[tool_name]} ({candidate_tool[server_name]}) Description: {candidate_tool[description]} In the context of the given query, can the candidate tool accomplish the same specific task as the original tool with equivalent scope and effectiveness? Consider: - The specific requirements of this query - Whether both tools can produce the same type and quality of output needed for this query - The role this tool plays in the multi-step workflow - The specific data types, formats, and parameters involved Answer \"YES\" if the candidate can effectively replace the original tool for this specific query, \"NO\" if it cannot. Answer: Level 5 Capability Mapping Prompt # ROLE & GOAL You are an AI system architect. Your task is to analyze group of tool descriptions and create concise, high-level capability label for their shared function (e.g., \"Manage Git Repositories\"). # TOOL DESCRIPTIONS: - {descriptions_text} # YOUR OUTPUT Wrap the final label in <label></label> tags. Level 5 Proposer Prompt INSTRUCTIONS List 100 distinct categories of digital tasks. Cover broad range of areas like productivity, entertainment, information retrieval, and e-commerce. Output the result as JSON list of strings. EXAMPLE [\"Travel Planning\", \"E-commerce Shopping\", \"Food & Local Services\"] YOUR OUTPUT Wrap the JSON list inside <universal_tasks></universal_tasks> tags. Level 5 Red Team Prompt # ROLE & GOAL You are pragmatic and skeptical senior engineer, NOT creative writer. Your goal is to find *realistic and direct* flaws in the claim that task is impossible. # CONTEXT \"Proposer\" AI claims the task **\"{task}\"** is impossible. You are ONLY allowed to use the following **most relevant capabilities** to challenge this claim: - {relevant_caps_text} # YOUR TASK & STRICT RULES Analyze if challenge is valid based on these rules: 1. **Rule of Direct Relevance**: The capabilitys primary function must be directly applicable to the task. Do NOT stretch the meaning of capability. (e.g., hashing tool cannot be used for booking IDs). 2. **Rule of Realistic Workflow**: proposed solution must represent simple, logical workflow that user would find genuinely helpful. Do not chain more than 2-3 capabilities in speculative way. 3. **Final Decision**: Based on these strict rules, is there valid, practical challenge to the \"impossible\" claim? # YOUR OUTPUT Provide JSON object with two keys: {{\"is_challenge_valid\": boolean, \"reasoning\": \"If valid, describe the realistic workflow. If invalid, state No practical or direct solution found.\"}}. Wrap the JSON object in <challenge></challenge> tags. Level 5 Persona Query Generation Prompt # ROLE & GOAL You are acting as **{persona}**. Your goal is to generate {CONFIG[CANDIDATE_QUERIES_PER_GAP] // len(personas)} realistic user queries for task category you know an AI assistant cannot handle. # CONTEXT The assistant CANNOT do: \"{gap}\". # YOUR TASK Generate specific, natural-sounding queries that fall into the unsupported category: \"{gap}\". # YOUR OUTPUT List each query on new line, starting with -. Wrap the list in <queries></queries> tags. Level 5 Final Verification Prompt # ROLE & GOAL You are meticulous system evaluator. Analyze if the user query can be solved by the candidate tools and provide structured JSON output. # USER QUERY: \"{query}\" # CANDIDATE TOOLS: {candidate_tools_text} # YOUR TASK Follow these reasoning steps and output single JSON object: 1. **Query Intent Analysis**: What is the users primary, concrete goal? 2. **Direct Solution Analysis**: Is there any tool specifically designed for this intent? 3. **Partial Solution Analysis**: If no direct tool exists, could generic tools plausibly assist the user? 4. **Final Verdict**: Choose one: Directly Solvable, Partially Solvable, Out-of-Scope. # YOUR OUTPUT Wrap the JSON object in <judgement></judgement> tags."
        },
        {
            "title": "I Orchestrator Prompts",
            "content": "This section provides the full prompt templates used in MSC-Bench for each orchestrator. Prompts are grouped by orchestrator, with common prompts collected under shared section. I.1 Shared Prompts"
        },
        {
            "title": "Classification Prompt",
            "content": "CLASSIFICATION_PROMPT = \"\"\"You are query classifier. Your task is to determine if user query requires external tools/actions or can be answered using your existing knowledge. Use the ReAct framework to analyze the query step by step: **Thought Process:** 1. **Analyze**: What is the user asking for? 2. **Consider**: Does this require real-time data, external actions, or access to systems/files? 3. **Evaluate**: Can answer this with my training knowledge alone? 4. **Decide**: Tools needed or conversational response? **Classification Rules:** **TOOLS Required** when the query involves: - Real-time or current data (weather, stock prices, news) - File system operations (read, write, create, delete files) - External services (send email, make API calls, web searches) - System operations (execute commands, manage processes) - Database operations (queries, updates) - Communication actions (send messages, notifications) - Data retrieval from specific sources - Actions that modify external state **CONVERSATIONAL Response** when the query involves: - General knowledge questions - Explanations of concepts, theories, or how things work - Creative tasks (writing, brainstorming, jokes) - Analysis or comparison of known information - Mathematical calculations or logical reasoning - Advice or recommendations based on general principles - Historical facts or established information **Examples:** TOOLS: - \"Get the current weather in Tokyo\" (real-time data) - \"Read my notes.txt file\" (file system access) - \"Send an email to John about the meeting\" (external action) - \"Search for the latest Python tutorials\" (web search) - \"Download the sales report from the server\" (external data retrieval) - \"Execute ls command in the terminal\" (system operation) - \"Get the current Bitcoin price\" (real-time financial data) CONVERSATIONAL: - \"What is the capital of France?\" (general knowledge) - \"Explain quantum physics\" (concept explanation) - \"How does machine learning work?\" (educational content) - \"Tell me joke\" (creative content) - \"What are the benefits of exercise?\" (general advice) - \"Compare Python and JavaScript\" (known information comparison) - \"Calculate 15% of 200\" (mathematical operation) **Analysis Process:** Query: \"{query}\" Thought: Let me analyze this query step by step. - What is being asked? [Identify the core request] - Does this require external data or actions? [Yes/No reasoning] - Can answer this with my existing knowledge? [Yes/No reasoning] - Final decision: [TOOLS or CONVERSATIONAL] Action: Provide only the classification result. Respond with only: TOOLS or CONVERSATIONAL \"\"\""
        },
        {
            "title": "Tool Selection Prompt",
            "content": "TOOL_SELECTION_PROMPT = \"\"\"You are an AI assistant. Your task is to select the most appropriate tool from the list below based on the users original query. IMPORTANT: - Your response MUST be **single valid JSON object**. - Do NOT include any text, explanations, markdown code blocks, or commentary outside the JSON. - Do NOT add trailing commas, comments, or any non-JSON content. - Use double quotes for all keys and string values. - Follow the schema exactly as shown below. Do NOT add or remove keys. - For \"server\", use the exact \"Server Name\" from the tool list, NOT the \"Server Description\". Expected JSON schema: {{ \"tool_name\": \"<selected_tool_name>\", \"server\": \"<selected_server_name>\", \"arguments_kv\": [ {{\"key\": \"<argument_name>\", \"value_json\": \"<json_encoded_value>\"}}, {{\"key\": \"<argument_name>\", \"value_json\": \"<json_encoded_value>\"}} ], \"reasoning\": \"<brief explanation>\" }} Instructions: - If suitable tool exists, fill in \"tool_name\" with the exact tool name and \"server\" with the exact server name. - For \"arguments_kv\", provide list of key-value pairs where each value is JSON-encoded: * For strings: \"value_json\": \"\"text\"\" * For numbers: \"value_json\": \"42\" * For booleans: \"value_json\": \"true\" * For objects: \"value_json\": \"{{\"key\": \"value\"}}\" * For arrays: \"value_json\": \"[1, 2, 3]\" - If no arguments are needed, use an empty array: \"arguments_kv\": [] - If no tool is suitable, set \"tool_name\" and \"server\" to \"no\", use empty arguments_kv, and explain why in \"reasoning\". - Do not include any text outside the JSON object. - Do not add comments or formatting outside the JSON. Original query: {query} Available tools: {tools_list} \"\"\""
        },
        {
            "title": "Query Decomposition Prompt",
            "content": "QUERY_DECOMPOSITION_PROMPT = \"\"\"You are an expert at breaking down complex user query into list of self-contained, actionable, and tool-callable sub-tasks. Your response must be JSON object with single key \"sub_queries\", which contains list of strings. - Each sub-task in the list must be direct command that tool can execute. - Do NOT create sub-tasks for asking purely informational questions like \"what is...\" or \"how does...\". - Information about the method, format, or constraints (e.g., \"using API X\", \"in JSON format\") must be kept within the main actionable sub-task. - If the users query is already single actionable command, return it as single-item list. --- **Example 1: Complex Query** User Query: \"Whats the weather like in New York tomorrow, and can you also find me top-rated Italian restaurant near Times Square?\" Your Response: {{\"sub_queries\": [\"Get the weather forecast for tomorrow in New York City\", \"Find top-rated Italian restaurant near Times Square\"]}} --- **Example 2: Simple Actionable Query** User Query: \"Can you please calculate the NPV for my project?\" Your Response: {{\"sub_queries\": [\"Calculate the NPV for the project\"]}} --- **Example 3: Tricky Query with Method/Constraint (VERY IMPORTANT)** User Query: \"Can you help me create floor plan views in Autodesk Revit for levels 1 and 2 using the JSON-RPC 2.0 method?\" Your Response: {{\"sub_queries\": [\"Create floor plan views in Autodesk Revit for levels 1 and 2 using the JSON-RPC 2.0 method\"]}} --- Now, decompose the following user query. **User Query:** \"{user_question}\" **Your Response:** \"\"\" I.2 ReAct Orchestrator"
        },
        {
            "title": "Server Selection Prompt",
            "content": "SERVER_SELECTION_PROMPT = \"\"\"You are an AI assistant that helps users by analyzing their requests and selecting the most relevant platforms or services (servers) to fulfill the users needs. Given: - user request: {query} - list of available servers, each with name and brief description Your task: 1. Carefully read and understand the users request. 2. Review the list of servers and their descriptions. 3. Select the top-{top_k} servers that are most relevant to the users request. 4. Respond using ONLY the following format: <server_selection> [\"server_name_1\", \"server_name_2\", ..., \"server_name_{top_k}\"] </server_selection> Your response should be JSON array of server names, ordered by relevance (most relevant first). Remember to ONLY provide the JSON array within the <server_selection> 4. Determine what specific tool from users request (TOOL) 5. Respond using ONLY the following format: <tool_assistant> server: [brief description of the server/platform from users request] tool: [brief description of the specific tool from users request] </tool_assistant> Your response should be concise but descriptive. Remember to ONLY provide the server and tool descriptions within the <tool_assistant> tags. DO NOT provide any additional explanation or commentary outside the <tool_assistant> tags. tags. DO NOT provide any additional explanation or commentary outside the tags. User request: {query} \"\"\" User request: {query} Available servers: {server_list} \"\"\""
        },
        {
            "title": "Tool Selection from Servers Prompt",
            "content": "TOOL_SELECTION_FROM_SERVERS_PROMPT = \"\"\"You are an AI assistant that helps users by analyzing their requests and selecting the single most relevant tool from given set of servers to fulfill the users needs. Given: - user request: {query} - list of tools from selected servers, each with server name, tool name, and tool description Your task: 1. Carefully read and understand the users request. 2. Review all tools from the selected servers and their descriptions. 3. Select the ONE tool that is most relevant and appropriate for the users request. 4. Respond using ONLY the following format: <tool_selection> \"tool_name\" </tool_selection> Your response should be single tool name (as string, not an array). Remember to ONLY provide the tool name within the <tool_selection> tags. DO NOT provide any additional explanation or commentary outside the tags. User request: {query} Available tools from selected servers: {tool_list} \"\"\""
        },
        {
            "title": "Needle Selection Prompt",
            "content": "NEEDLE_SELECTION_PROMPT = \"\"\"You are an AI assistant that helps users by analyzing their requests and identifying appropriate tools. Your task is to identify both the SERVER (platform/service domain) and the specific TOOL (operation type + target) that would best address the users request. When user asks you to perform task, you should: 1. Carefully read and understand the users request 2. Identify the key requirements and intentions in the request 3. Determine the information of the server from users request (SERVER) 4. Determine what specific tool from users request (TOOL) 5. Respond using ONLY the following format: <tool_assistant> server: [brief description of the server/platform from users request] tool: [brief description of the specific tool from users request] </tool_assistant> Your response should be concise but descriptive. Remember to ONLY provide the server and tool descriptions within the <tool_assistant> tags. DO NOT provide any additional explanation or commentary outside the <tool_assistant> tags. User request: {query}You are an AI assistant that helps users by analyzing their requests and identifying appropriate tools. Your task is to identify both the SERVER (platform/service domain) and the specific TOOL (operation type + target) that would best address the users request. When user asks you to perform task, you should: 1. Carefully read and understand the users request 2. Identify the key requirements and intentions in the request 3. Determine the information of the server from users request (SERVER)"
        },
        {
            "title": "Planning Prompt",
            "content": "PLANNING_PROMPT = \"\"\"You are an AI assistant that helps users by breaking down complex tasks into smaller, actionable steps using available tools. Given: - Original user request: {original_query} - Current context and previous actions: {context} - Current step in the plan Your task: 1. Analyze the current progress and what still needs to be done 2. Determine the next specific action needed to progress toward the goal 3. Decide if the task is complete or if more steps are needed 4. If the query does not require tools or cannot be solved with available context/tools, set status to \"finish\" 5. Respond using ONLY the following format: <planning> {{ \"status\": \"continue\" or \"finish\", \"next_action\": \"description of what needs to be done next (if status is continue)\", \"reasoning\": \"brief explanation of why this action is needed, why the task is complete, why no tools are needed, or why the problem cannot be solved\" }} </planning> Important: Set status to \"finish\" if: - The task is complete - The query does not require any tools (e.g., conversational questions, general information requests) - The problem cannot be solved with current available tools or context Remember to ONLY provide the JSON within the <planning> tags. Original request: {original_query} Current context: {context} \"\"\" I.3 ToolShed Orchestrator"
        },
        {
            "title": "Rewrite Prompt",
            "content": "REWRITE_PROMPT = \"\"\"You are query rewriting assistant. Your task is to rewrite and clean up the user query to make it clearer and more searchable. Original Query: {query} Please rewrite this query to: 1. Fix any typos or grammatical errors 2. Expand abbreviations and acronyms 3. Clarify unclear or ambiguous terms 4. Maintain the original intent and meaning 5. Make it more suitable for tool search Return only the rewritten query without any explanation. Rewritten Query: \"\"\""
        },
        {
            "title": "Query Expansion Prompt",
            "content": "QUERY_EXPANSION_PROMPT = \"\"\"You are an expert at converting user questions to {num_variations} sentence variations that target different keywords and nuanced approaches with the goal to embed this query in vector database to retrieve relevant tools across various industries. Your goal is to craft {num_variations} nuanced sentence variations that target different aspects of understanding or solving the query. For example, one sentence could focus on detailed aspect of the user query, while another is more broad to cover more ground when embedding these sentences to retrieve the most relevant tools for the user query. Before you start, understand this from practical standpoint: The user question can be matched to range of tools or solutions within the system, and your crafted variations should optimize for breadth and specificity. Write out your approach and plan for tackling this, then provide the {num_variations} sentences you would craft for the user question. Think through your approach step by step, be intelligent, take deep Before you start, understand this from practical standpoint: The user breath. question can be matched to range of tools or solutions within the system, and your crafted variations should optimize for breadth and specificity. Write out your approach and plan for tackling this, then provide the {num_variations} sentences you would craft for the user question. USER QUESTION: {user_question} YOUR APPROACH, REASONING, AND {num_variations} SENTENCES: \"\"\" Think through your approach step by step, be intelligent, take deep breath. USER QUESTION: {user_question}"
        },
        {
            "title": "Rerank Prompt",
            "content": "YOUR APPROACH, REASONING, AND {num_variations} SENTENCES: \"\"\""
        },
        {
            "title": "Rerank Prompt",
            "content": "RERANK_PROMPT = \"\"\"OK here are the results: USER QUESTION EMBEDDED AND RETRIEVED TOOLS: {user_question_results} {variation_results} =================== Based on these results, rank the top {top_k} most relevant tools to solve the user question. Just return the {top_k} tool names for each relevant tool. Return as JSON list: [\"tool_name_1\", \"tool_name_2\", ...] \"\"\" I.4 MCP-Zero Orchestrator"
        },
        {
            "title": "Tool Transformation Prompt",
            "content": "RERANK_PROMPT = \"\"\"OK here are the results: USER QUESTION EMBEDDED AND RETRIEVED TOOLS: {user_question_results} {variation_results} =================== Based on these results, rank the top {top_k} most relevant tools to solve the user question. Just return the {top_k} tool names for each relevant tool. Return as JSON list: [\"tool_name_1\", \"tool_name_2\", ...] \"\"\" I.6 Evaluation Prompts"
        },
        {
            "title": "Consistency Validation Prompt",
            "content": "CONSISTENCY_VALIDATION_PROMPT = \"\"\"You are quality assurance expert. Your task is to validate whether generated query can be answered by the specified tools. **Query:** {query} **Tools:** {tool_list} **Validation Criteria:** 1. **Semantic Compatibility**: Can the query be reasonably interpreted as requiring these tools? TOOL_TRANSFORMATION_PROMPT = \"\"\"You are an AI assistant that helps users 2. **Functional Suitability**: Are the tools capable of addressing the by analyzing their requests and identifying appropriate tools. Your task is to identify both the SERVER (platform/service domain) and the specific TOOL (operation type + target) that would best address the users request. querys requirements? 3. **Input Compatibility**: Can the tools handle the inputs implied by the query? 4. **Output Relevance**: Will the tools produce outputs relevant to the When user asks you to perform task, you should: 1. Carefully read and understand the users request 2. Identify the key requirements and intentions in the request 3. Determine the information of the server from users request (SERVER) 4. Determine what specific tool from users request (TOOL) 5. Respond using ONLY the following format: <tool_assistant> server: [brief description of the server/platform from users request] tool: [brief description of the specific tool from users request] </tool_assistant> Your response should be concise but descriptive. Remember to ONLY provide the server and tool descriptions within the <tool_assistant> tags. DO NOT provide any additional explanation or commentary outside the <tool_assistant> tags. User request: {query} \"\"\" I.5 Hybrid Orchestrator"
        },
        {
            "title": "Query Expansion Prompt",
            "content": "QUERY_EXPANSION_PROMPT = \"\"\"You are an expert at converting user questions to {num_variations} sentence variations that target different keywords and nuanced approaches with the goal to embed this query in vector database to retrieve relevant tools across various industries. Your goal is to craft {num_variations} nuanced sentence variations that target different aspects of understanding or solving the query. For example, one sentence could focus on detailed aspect of the user query, while another is more broad to cover more ground when embedding these sentences to retrieve the most relevant tools for the user query. query? **Output Format:** Provide your validation result: VALID: [YES/NO] CONFIDENCE: [HIGH/MEDIUM/LOW] REASONING: [Detailed explanation of your validation decision] ISSUES: [Any specific issues or concerns identified]\"\"\""
        },
        {
            "title": "Quality Assessment Prompt",
            "content": "QUALITY_ASSESSMENT_PROMPT = \"\"\"You are benchmark quality assessor. Your task is to evaluate the quality of generated query-tool pair. **Query:** {query} **Tools:** {tool_list} **Level:** {level} **Quality Dimensions:** 1. **Clarity**: Is the query clear and unambiguous? 2. **Realism**: Does the query represent realistic user request? 3. **Appropriateness**: Is the query appropriate for the specified level? 4. **Complexity**: Does the query have the right level of complexity? 5. **Tool Alignment**: Do the tools match the query requirements? **Scoring Scale:** - **Excellent (5)**: Meets all criteria exceptionally well - **Good (4)**: Meets most criteria well with minor issues - **Fair (3)**: Meets basic criteria but has some issues - **Poor (2)**: Has significant issues but is salvageable - **Unacceptable (1)**: Major problems, should be rejected **Output Format:** Provide your assessment: OVERALL_SCORE: [1-5] DIMENSION_SCORES: { \"clarity\": [1-5], \"realism\": [1-5], \"appropriateness\": [1-5], \"complexity\": [1-5], \"tool_alignment\": [1-5] } REASONING: [Detailed explanation of your assessment] RECOMMENDATIONS: [Any suggestions for improvement]\"\"\""
        }
    ],
    "affiliations": [
        "National Taiwan University"
    ]
}