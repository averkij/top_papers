{
    "paper_title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers",
    "authors": [
        "Zhaochen Su",
        "Peng Xia",
        "Hangyu Guo",
        "Zhenhua Liu",
        "Yan Ma",
        "Xiaoye Qu",
        "Jiaqi Liu",
        "Yanshu Li",
        "Kaide Zeng",
        "Zhengyuan Yang",
        "Linjie Li",
        "Yu Cheng",
        "Heng Ji",
        "Junxian He",
        "Yi R. Fung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental \"semantic gap\" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI."
        },
        {
            "title": "Start",
            "content": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers Zhaochen Su1, Peng Xia2, Hangyu Guo1, Zhenhua Liu1, Yan Ma, Xiaoye Qu, Jiaqi Liu2, Yanshu Li1, Kaide Zeng2, Zhengyuan Yang3, Linjie Li3, Yu Cheng4, Heng Ji5, Junxian He1, Yi R. (May) Fung1 1The Hong Kong University of Science and Technology 2UNC-Chapel Hill, 3Microsoft, 4The Chinese University of Hong Kong, 5UIUC"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as static, initial context, creating fundamental semantic gap between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as dynamic mental sketchpad. similar evolution is now unfolding in AI, marking fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from passive input into dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along spectrum of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the Thinking with Images paradigm and its three-stage framework. (2) We provide comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. Through this structured overview, we aim to offer clear roadmap for future research towards more powerful and human-aligned multimodal AI.1 5 2 0 2 3 ] . [ 3 8 1 9 3 2 . 6 0 5 2 : r 1We maintain real-time GitHub repository tracking progress at: https://github.com/zhaochen0110/ Awesome_Think_With_Images."
        },
        {
            "title": "Contents",
            "content": "1 Introduction"
        },
        {
            "title": "1.1 The Position of Our Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "2 Foundations of the Thinking with Images Paradigm"
        },
        {
            "title": "2.4 Unique Challenges",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Stage 1: Tool-Driven Visual Exploration"
        },
        {
            "title": "3.1 Formulation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 Categories of Visual Tools",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Implementation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Prompt-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 SFT-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 RL-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Conclusion and Future Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Stage 2: Programmatic Visual Manipulation 4.1 The Programmatic Leap: From Selection to Creation . . . . . . . . . . . . . . . . 4.2 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Categories of Composable Operations . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Implementation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Prompt-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 SFT-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.3 RL-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Conclusion and Future Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Stage 3: Intrinsic Visual Imagination 5.1 The Architectural Leap: From Execution to Imagination . . . . . . . . . . . . . . 5.2 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Categories of Generative Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . 5. Implementation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 SFT-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 RL-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Conclusion and Future Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Evaluations & Frameworks for Thinking with Images 6.1 Evaluations Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Implementation Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 5 6 6 7 8 9 9 10 11 11 14 14 15 16 16 17 17 18 19 20 21 21 21 22 22 23 24"
        },
        {
            "title": "6.3 Conclusion and Future Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "28 7 Applications 7."
        },
        {
            "title": "Interactive Systems and User Interfaces",
            "content": ". . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7.3 AI for Science .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7.5 Education and Training .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Future Directions"
        },
        {
            "title": "8.3 Novel Benchmarks and Evaluation Methodologies . . . . . . . . . . . . . . . . . .",
            "content": "8.4 Thinking with Audio, Video, and the World . . . . . . . . . . . . . . . . . . . . . 8.5 Open Questions and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.5.1 Thinking with Images v.s. Agentic Frameworks . . . . . . . . . . . . . . . 8.5.2 Blueprint for Unified Visual Thinker . . . . . . . . . . . . . . . . . . . . 9 Conclusion 28 29 29 30 31 32 32 32 33 33 34 35 36"
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMMs) have recently marked pivotal moment in artificial intelligence, demonstrating remarkable success in comprehending and generating multimodal content [Team et al., 2023, Liu et al., 2024a, Wang et al., 2024a, Chen et al., 2025a]. This progress has fundamentally reshaped AIs ability to bridge the cognitive gap between visual perception and linguistic abstraction. key catalyst for this first wave of advancement has been the adaptation of language-centric reasoning mechanisms, most notably the Chain-of-Thought (CoT) paradigm [Wei et al., 2022, Kojima et al., 2022]. By decomposing complex problems into sequence of textual reasoning steps, CoT has significantly elevated LMM performance on wide array of multimodal tasks, including visual question answering [Zhang et al., 2023a, He et al., 2025a, Shen et al., 2025], visually-grounded mathematical problem-solving [Lu et al., 2023], and intricate narrative generation [Wu et al., 2024a]. In this established paradigm, which we term Thinking about Images, the visual modality primarily serves as static, initial context. LMM first sees an image, encodes it into fixed set of features, and then operates exclusively within the textual domain to conduct its reasoning. The image is the premise, but language is the exclusive medium of thought. While powerful, this text-centric approach has exposed fundamental limitation: the semantic gap between the rich, continuous, and often ambiguous nature of the visual world and the discrete, symbolic structure of language [Li et al., 2022]. This initial, one-time encoding flattens the visual world into static representation, creating critical information bottleneck. Consequently, models often falter on tasks that demand deeper, iterative visual engagement, such as complex physical reasoning [Balazadeh et al., 2024], precise spatial manipulation [Gupta and Kembhavi, 2023], or long-horizon planning in interactive environments [Pahuja et al., 2025, Wang et al., 2025a]. Now, new revolution is quietly unfolding in multimodal reasoning. Models are evolving beyond merely thinking about images with text, towards new paradigm where they can truly think with images. This represents fundamental shift in the cognitive role of vision: from passive, fixed input to dynamic, manipulable cognitive workspace. Much like human using sketchpad, models are now being empowered to actively query, modify, and even generate new visual information as integral, intermediate steps within their reasoning process. This ability to form visual chain of thought is not mere extension of textual CoT, but revolutionary step towards more holistic and human-like form of cognition [Larkin and Simon, 1987]. We argue that this emerging paradigm, Thinking with Images, constitutes the next frontier for multimodal AI. It is defined by its core principle: to utilize visual representations as form of manipulable and verifiable thought, thereby empowering models to actively see, manipulate, and reason with visual information as intermediate steps in cognitive process. This survey provides the first comprehensive and systematic overview of this nascent and rapidly accelerating field. To structure this rapidly evolving landscape, we propose conceptual framework that charts the progression of this paradigm through three stages of increasing cognitive autonomy. These stages represent the different mechanisms (the How) through which model can achieve its visual reasoning goals (the Why): from acting as commander that orchestrates external visual tools, to evolving into visual programmer that generates code for tailored operations, and ultimately becoming visual thinker capable of intrinsic imagination and simulation. This three-stage evolution will be detailed in Section 2. The proliferation of methods across these stages naturally raises critical question that this survey seeks to answer: How are Large Multimodal Models evolving to Thinking with Images through these advancing stages of cognitive autonomy, and what are the foundational methods, evaluations, applications, and challenges defining this new paradigm? This survey is organized to systematically address this question, following the structure of our proposed taxonomy illustrated in Figure 1. We begin by establishing the foundations of this paradigm in 2. We then delve into the methodologies of each three stages in 3, 4, and 5, respectively. Following this, we review the critical landscape of evaluation benchmarks and implementation frameworks in 6. We then explore the transformative applications of this paradigm in 7, and discuss prominent challenges and future directions in 8. By providing clear taxonomy and forward-looking perspective, we aim to not only review existing knowledge but also to inspire future research towards building more powerful, intuitive, and truly multimodal AI. 4 Prompt-Based Apprs. (3.3.1) e.g., VoT [Wu et al., 2024b], ZoomEye [Shen et al., 2024], VisuoThink [Wang et al., 2025b], PromptCap [Hu et al., 2022], Shtedritski [Shtedritski et al., 2023], Socratic Models [Zeng et al., 2022], MM-REACT [Yang et al., 2023a], SoM [Yang et al., 2023b], DetToolChain [Wu et al., 2024c], VAT [Liu et al., 2025a], Visual Thoughts [Cheng et al., 2025], VisuoThink [Wang et al., 2025b], etc. Tool-Driven Visual Exploration (3) SFT-Based Apprs. (3.3.2) e.g., LLaVA-Plus [Liu et al., 2023], TACO [Ma et al., 2024], VTS-V [Bai et al., 2025a], CogCoM [Qi et al., 2024], v1 [Chung et al., 2025], Visual CoT [Shao et al., 2024], IVM [Zheng et al., 2024a], CMMCoT [Zhang et al., 2025a], V* [Wu et al., 2023a], etc. RL-Based Apprs. (3.3.3) e.g., Jigsaw-R1 [Wang et al., 2025c], V-Triune [Ma et al., 2025], VisionReasoner [Liu et al., 2025b], GRIT [Fan et al., 2025a], Point-RFT [Ni et al., 2025], Seg-Zero [Liu et al., 2025c], VisTA [Huang et al., 2025a], OpenThinkIMG [Su et al., 2025a], DeepEyes [Zheng et al., 2025], ACTIVE-o3 [Zhu et al., 2025a], Chain-of-Focus [Zhang et al., 2025b], Pixel-Reasoner [Su et al., 2025b], etc. Prompt-Based Apprs. (4.4.1) e.g., VisProg [Gupta and Kembhavi, 2023], ViperGPT [Surís et al., 2023], CAD-Assistant [Mallis et al., 2024], ReFocus [Fu et al., 2025a], SketchAgent [Vinker et al., 2024], VipAct [Zhang et al., 2024a], MMFactory [Fan et al., 2024], etc. Programmatic Visual Manipulation (4) SFT-Based Apprs. (4.4.2) e.g., VPD [Hu et al., 2024a], MathCoder-VL [Wang et al., 2025d], PROVISION [Zhang et al., 2024b], CoSyn [Yang et al., 2025], etc. RL-Based Apprs. (4.4.3) SFT-Based Apprs. (5.4.1) Intrinsic Visual Imagination (5) e.g., Visual-ARFT [Liu et al., 2025d], VRAG-RL [Wang et al., 2025e], etc. e.g., Chameleon [Team, 2024], Emu2 [Sun et al., 2024a], BLIP-3o [Chen et al., 2025b], GILL [Koh et al., 2023], GoT [Fang et al., 2025], Janus-Pro [Chen et al., 2025c], MetaMorph [Tong et al., 2024], MiniGPT-5 [Zheng et al., 2023], BAGEL [Deng et al., 2025a], MVoT [Li et al., 2025a], CoT-VLA [Zhao et al., 2025a], T2I-R1 [Jiang et al., 2025a], BLIP3-o [Chen et al., 2025b], etc. RL-Based Apprs. (5.4.2) e.g., Visual Planning [Xu et al., 2025], GoT-R1 [Duan et al., 2025], T2I-R1 [Jiang et al., 2025a], PARM PARM++ [Guo et al., 2025a], etc. Benchmark Evaluation (6.1) e.g., MMMU [Yue et al., 2024a], UniGeo [Chen et al., 2022a], PGPS9K [Zhang et al., 2023b], TrustGeoGen [Fu et al., 2025b], GeoMVerse [Kazemi et al., 2023], VisioMath [Li et al., 2025b], MV-MATH [Wang et al., 2025f], MathVista [Lu et al., 2024], ScienceQA [Lu et al., 2022], MMIE [Xia et al., 2024a], PuzzleBench [Zhang et al., 2025c], etc. Implementation Frameworks (6.2) e.g., LangChain [Chase, 2022], CrewAI [crewAIInc, 2023], ROLL [ROLL Team and Other ROLL Contributors, 2025], LLaMAFactory [Zheng et al., 2024b], Axolotl [Axolotl AI Cloud Contributors, 2023], Haystack [deepset, 2023], AutoGPT [Richards and Significant Gravitas, 2023], VeRL [ByteDance Seed team and verl community, 2024], Unsloth [UnslothAI, 2023], etc. Interactive Sys. and UI (7.1) e.g., CogAgent [Hong et al., 2024], MobileLMM [Liu et al., 2024b], ShowUI [Lin et al., 2024], UI-TARS [Qin et al., 2025], Aguvis [Xu et al., 2024], Explorer [Pahuja et al., 2025], TongUI [Zhang et al., 2025d], PC Agent-E [He et al., 2025b], etc. Embodied AI and Robotics (7.2) e.g., PaLM-E [Driess et al., 2023], Interleave-VLA [Fan et al., 2025b], VIMA [Jiang et al., 2023], CoT-VLA [Zhao et al., 2025a], Embodied ECoT [Zawalski et al., 2025], VPRL [Xu et al., 2025], Genie [Bruce et al., 2024], 3D-VLA [Zhen et al., 2024], etc. The Three-Stage Evolution a I w k T Benchmark & Frameworks (6) Applications (7) AI for Science (7.3) e.g., GeoCha [Kuckreja et al., 2023], PCBs [Balazadeh et al., 2024], MAPS [Zhu et al., 2025b], ChemMLLM [Tan et al., 2025], etc. Healthcare and Medicine (7.4) Education and Training (7.5) e.g., HealthGPT [Lin et al., 2025], AOR [Li et al., 2025c], QoQ-Med [Dai et al., 2025a], MMedAgent-RL [Xia et al., 2025a], etc. e.g., Interactive Sketchpad [Chen et al., 2025d], DiagramGPT [Zala et al., 2024], VidAAS [Lee et al., 2024], etc. Figure 1: The taxonomy of the Thinking with Images paradigm, organized into three main branches: (1) core methodologies, evolving through Tool-Driven Visual Exploration, Programmatic Visual Manipulation, and Intrinsic Visual Imagination; (2) supporting benchmarks and frameworks; and (3) key applications. Representative works are listed on the leaves. 1.1 The Position of Our Survey How our focus differs from prior surveys. Earlier reviews have laid essential groundwork for understanding LMMs, yet they share common perspective: vision is mainly treated as context while language remains the primary vehicle for reasoning. General-scope surveys such as Yin et al. [2024], Zhang et al. [2024c], and Wu et al. [2023a] catalogue architectures, pre-training corpora, and evaluation protocols, but only briefly touch on how model might use visual information once it has been encoded. Xie et al. [2024a] extends the discussion to agent settings, yet still emphasises tool calling rather than internal visual cognition. Domain-specific reviews on mathematical reasoning [Yan et al., 2024], hallucination [Liu et al., 2024c], and benchmark [Li et al., 2025d] delve deeper into task details but inherit the same text-centric framing. Very recent work has begun to consider explicit reasoning mechanisms. Wang et al. [2025g] surveys multimodal CoT prompting, and Li et al. [2025e] discusses perceptionreasonplan pipelines. Even here, the visual modality is largely passive: models describe or annotate an image once, then proceed with textual deliberation. Our position. This survey charts the paradigm shift from Thinking about Images to Thinking with Images. We are the first to systematically categorize the mechanisms that enable this evolution, where the image is transformed from static premise into dynamic, manipulable cognitive workspace. We structure our review along trajectory of increasing cognitive autonomy, detailing how models are evolving from (1) orchestrating tools for exploration, to (2) programmatically creating visual analyses, and ultimately to (3) intrinsically imagining visual information within closed cognitive loop. We believe this progression will fundamentally redefine what constitutes reasoning step, what counts as verifiable evidence, and how true visual intelligence should be measured. 5 Figure 2: Conceptual comparison of Thinking about Images versus Thinking with Images. Paradigm 1 shows failure case of static, error-prone reasoning. Paradigm 2 demonstrates dynamic cognition through three key capabilities: dynamic perceptual exploration, structured visual reasoning, and goal-oriented generative planning. The shift highlights the fundamental transition from static, perceptual input to dynamic, manipulable cognitive workspace."
        },
        {
            "title": "2 Foundations of the Thinking with Images Paradigm",
            "content": "This section establishes the conceptual foundations of the Thinking with Images paradigm. We first articulate the cognitive imperative for this shift away from purely text-centric reasoning. We then detail its implementation through three-stage evolutionary framework, provide formal definition of its mechanics, and conclude with balanced analysis of its core advantages and unique challenges. 2.1 The Cognitive Imperative: Why We Need Thinking with Images? The prevailing paradigm for multimodal reasoning, where model first perceives visual input and then performs subsequent reasoning exclusively through textual CoT [Wei et al., 2022, Zhang et al., 2023a, Wang et al., 2025g], has encountered fundamental ceiling. This approach is constrained by semantic gap between the high-dimensional, continuous nature of the visual world and the discrete, abstract structure of language. When an image is encoded into single, static feature vector, the rich, relational structure of the visual world is flattened, making the process of reasoning itself intractable. Consequently, even models with superhuman perception can appear cognitively brittle when facing problems that require more than static glance. We illustrate this fundamental contrast in Figure 2 (left), where the Thinking about Images approach, limited to single static perception, fails due to lossy, one-time encoding. The Thinking with Images paradigm, however, directly overcomes this limitation by transforming the image from static input into dynamic and manipulable cognitive workspace. This shift empowers models with more robust form of cognition, which, as illustrated in Figure 2 (right), manifests through three key capabilities: Dynamic Perceptual Exploration. The first capability is to transcend single, holistic interpretation of an image by engaging in active, iterative inquiry. This is crucial for challenges that require focusing on fine-grained details or understanding object interactions. The model can formalize this dynamic search by generating new visual reasoning step, for instance by producing cropped region to zoom in on an area of interest or by highlighting particular object. This newly acquired visual evidence enriches the models immediate understanding, building more granular and accurate representation of the scene than single glance could provide. This transforms perception from one-time encoding into robust, closed-loop process of observation and targeted examination, allowing the model to resolve visual ambiguities at the pixel level. 6 Structured Visual Reasoning. The second capability elevates the image from mere input into cognitive scratchpad, fundamentally altering the reasoning process. Consider complex geometry problem. purely textual CoT, confined to language, would struggle to solve it, perhaps resorting to lengthy and error-prone algebraic manipulations or failing entirely. In contrast, the Thinking with Images approach allows the model to generate new image where crucial auxiliary line is explicitly rendered. This is pivotal cognitive leap. When this new image is fed back into the model, previously abstract properties such as newly formed right angles or congruent triangles become perceptually evident features. The reasoning task is thus transformed from difficult symbolic deduction to more robust visual pattern recognition. This process of externalizing thought onto visual canvas grounds the entire reasoning chain, making it both more powerful and highly interpretable. Goal-Oriented Generative Planning. Finally, the third capability unlocks goal-oriented generative planning by transforming the models generative capacity into an engine for simulation. This is essential for tasks requiring physical intuition and forward planning. For example, to determine if robot arm can grasp an apple without knocking over nearby glass, textual CoT must rely on fallible symbolic logic about coordinates and trajectories. In contrast, model thinking with images can simulate the entire action by generating hypothetical future frame. In one such imagined future, the model might depict the robots gripper colliding with the glass. This generated image is not merely an illustration; it is form of perceptual self-critique. The visual inconsistency of the collision provides direct, non-linguistic error signal, forcing the model to re-evaluate its plan. This process of visual simulation is powerful because it outsources validation to the coherence of the physical world itself. physically implausible plan results in visually nonsensical image, grounding the models reasoning in way impossible by abstract text. In essence, the three capabilities of active exploration, structured reasoning, and generative planning are not isolated features but interconnected facets of single, more powerful cognitive process. They collectively transform the visual medium from static premise to be described into dynamic workspace for thought. This fundamental shift from passive analysis to active visual engagement enables models to overcome the brittleness of text-centric reasoning, paving the way for more robust, intuitive, and human-aligned multimodal AI. 2.2 How Does It Work? The Three-Stage Evolution The realization of Thinking with Images signifies an evolutionary journey. We categorize the diverse implementation pathways into three principal stages. These stages reflect general trend from relying on external systems to developing internalized cognitive functions. To illustrate this evolution, consider the complex task of planning how to rearrange cluttered room to accommodate new, larger sofa. Stage 1: Tool-Driven Visual Exploration. In this stage, the model orchestrates fixed inventory of external visual modules. Its primary role is to act as planner selecting an appropriate tool for the current sub-task. To address the furniture arrangement problem, the model might first invoke an object_detector to identify items and distance_estimator to measure the available space. As the models reasoning is grounded in the outputs of these tools, it might conclude that the current gap is 1.5 meters, while the new sofa requires 2.0 meters, so it will not fit. This approach is powerful for targeted data gathering but is limited by the static capabilities of its predefined toolset. Stage 2: Programmatic Visual Manipulation. This stage elevates the models autonomy by enabling it to function as visual programmer. Instead of selecting from finite menu, the model generates executable code to perform custom visual analysis. Faced with the furniture problem, the model could generate Python script using library like matplotlib. This script would create 2D top-down floor plan of the room, representing each piece of furniture as shape. The model could programmatically test various new arrangements in this abstract visual space. This method unlocks flexibility and its generated code serves as transparent, verifiable record of its thought process. The primary constraint remains its reliance on an external execution environment to run the code. Stage 3: Intrinsic Visual Imagination. At the most advanced stage, the model achieves full cognitive autonomy through intrinsic imagination. It transcends the dependency on external execution by leveraging its unified generative architecture to produce new image as an intermediate reasoning 7 step. To find the optimal room layout, the model could directly generate new, photorealistic image depicting the room with the furniture rearranged and the new sofa in place. This internally generated image functions as visual hypothesis or mental simulation. The model can then feed this image back into itself to critique the new layout, perhaps noticing that pathway is now blocked. This process enables seamless and truly integrated form of visual thought within closed cognitive loop, addressing the architectural bottlenecks of prior stages. Note on Non-Linearity. These three stages do not represent strictly linear progression. They are different implementation strategies (the How) that model can employ to achieve its cognitive goals (the Why). This distinction is critical within our running example of room arrangement. Before performing any complex rearrangement simulation, the model must first answer simple question: can the new sofa fit through the doorway? model capable of Stage 3 imagination could attempt to generate full physical simulation of maneuvering the sofa, but this is computationally expensive and potentially unreliable. far more efficient and robust strategy is to employ Stage 1 mechanism by invoking simple measure tool to get the exact width of the doorway. This illustrates that models intelligence lies not only in its peak capability but also in its ability to select the appropriate cognitive tool for the task. Our framework therefore provides structured way to understand an approachs primary mechanism while acknowledging the fields complex and interconnected nature. 2.3 What is Thinking with Images? Formal Definition While the three-stage evolution describes the various implementation pathways, precise, formal definition is necessary to delineate the paradigms fundamental mechanics. At its core, Thinking with Images is defined by its treatment of visual information as an operable, intermediate step within the reasoning process itself. This marks clear departure from prior methods where the image is fixed, initial condition. We can formalize this distinction by contrasting the two approaches. Paradigm I: Thinking about Images In this paradigm, the reasoning process is an autoregressive generation of textual thoughts conditioned on visual input. Given an LMM with parameters ΘLMM and an input query Q, the image is processed once by visual encoder ΦV ΘLMM to obtain set of features = ΦV (I). These features then serve as fixed context for language model. The model generates the reasoning chain by producing text tokens xt, where the context for each token is the sequence of previously generated text x<t and the visual features v. xt (x<t, v, Q; ΘLMM ) (1) Here, the visual information acts as an initial condition and is not modified during the process. Paradigm II: Thinking with Images This paradigm expands the models generative capabilities beyond the textual domain. The reasoning process is sequence of steps that can be either textual or visual. We define the reasoning history at step as state sequence St = (z1, . . . , zt1). The core action is to generate the next reasoning step zt based on this evolving multimodal history. Let Ttext denote the space of all possible textual outputs (e.g., the vocabulary of text tokens) and Ivis denote the space of all possible intermediate visual artifacts that can be introduced or modified during reasoning. The next step zt is sampled from the union of these two spaces. zt (St, I, Q; ΘLMM ) where zt Ttext Ivis (2) The critical distinction lies in the dynamic nature of state history St, which can contain both textual and visual reasoning steps. This formalization supports broad definition of the paradigm. An intermediate visual step zt Ivis can manifest in several forms: it can be the output from an external tool, such as the bounding box of detected object; it might be visualization generated by code, such as an auxiliary line drawn on diagram [Gupta and Kembhavi, 2023]; or it can be new image produced intrinsically by the model, such as predicted future frame [Xu et al., 2025]. 2.4 Unique Challenges While Thinking with Images unlocks profound capabilities, it also introduces distinct class of challenges that diverge significantly from those in textual CoT. The continuous and dense nature of visual information, which is the source of its power, also creates fundamental obstacles related to efficiency, robustness, and generalization. 8 Computational Cost: The Explosive Token Economy of Visual Thought. Thinking with Images is prohibitively expensive compared to its textual counterpart, limiting the length and complexity of reasoning. While textual reasoning has become more efficient [Sui et al., 2025, Qu et al., 2025], the cost of processing visual information remains orders of magnitude higher. single image is decomposed into dense grid of thousands of visual patches, each requiring its own intensive computation. Generating even one intermediate visual step is major computational event. multistep reasoning process that strings these events together results in compounding multiplicative, often prohibitive, burden. This token explosion makes exploring long visual reasoning paths practically infeasible with current architectures, creating hard ceiling on the depth of visual deliberation. Information Density: One Flawed Image versus Thousand Wrong Words. The dense nature of visual information causes errors to propagate in uniquely damaging ways. flawed assertion in textual chain can introduce logical contradiction [Su et al., 2024, Jia et al., 2025, Sun et al., 2024b]. However, visual error corrupts the underlying perception of reality [Dosovitskiy et al., 2020]. Consider model using zoom-in tool to identify products material. If it erroneously focuses on the wooden table underneath, its subsequent analysis, instead of simply failing, may begin new, internally coherent line of reasoning about completely irrelevant subject, generating plausible-sounding analysis of wood grain and varnish sheen. This initial mistake establishes false ground truth for all subsequent deliberation, creating foundational error in perception whose interconnected falsehoods poison the entire reasoning process. Architectural Divide: The Bottleneck Between Language and Pixels. Most current models for Thinking with Images use modular design separating vision and language systems [Liu et al., 2024a, Bai et al., 2025b]. While versatile, this separation creates challenges for iterative visual thinking. The architecture must translate rich spatial information into sequential format, process that risks losing fine-grained detail [Team, 2024]. True visual thinking ideally requires dynamic loop, allowing model to alternate between forming an intention and perceiving its immediate consequences. The current modular design can impede this loop by making the models perception of its actions indirect. Architectures enabling more direct engagement between the reasoning engine and the visual canvas are therefore crucial direction for the field. Cross-Task Generalization: Single Visual Strategy Does Not Fit All. The Thinking with Images paradigm introduces profound challenge: single mode of visual thinking is inadequate for the diverse nature of visual tasks. Different problems demand different modes of visual thinking. For example, solving geometry puzzle might require constructive strategy of generating new visual elements like lines and circles [Wang et al., 2025d]. Diagnosing detail-rich image may need an analytical strategy involving sequence of targeted zoom-ins and comparisons [Zheng et al., 2025]. Navigating maze could demand simulative strategy where the model imagines future visual states [Li et al., 2025a]. model trained with one strategy will fail at tasks requiring another. Developing agents that can maintain toolkit of diverse visual strategies and learn meta-policy to select the appropriate one for given task remains complex and unsolved challenge."
        },
        {
            "title": "3 Stage 1: Tool-Driven Visual Exploration",
            "content": "Having established the foundational principles of the Thinking with Images paradigm, we delve into the mechanics of its three evolutionary stages. This progression, visually charted in Figure 3, represents path of increasing cognitive autonomy, moving from the selection of external tools, to the creation of programmatic operations and to intrinsic imagination. Each stage builds upon the last, equipping models with more powerful and integrated forms of visual intelligence. We begin with the first pivotal step: Tool-Driven Visual Exploration. The models intelligence lies in acting as planner or orchestrator. It learns to leverage predefined suite of external visual tools to conduct its analysis, marking the crucial shift from passive perception to active, tool-driven inquiry. comprehensive overview of the methods in this stage, their underlying implementation approaches, and their primary application scenarios is provided in Table 1. 3.1 Formulation In this stage, the LMM acts as high-level planner that operates on predefined inventory of external visual tools, denoted as Ttool = {T1, . . . , Tm}. Each tool Ti performs specific, encapsulated 9 Figure 3: The specific operational categories within each of the three Thinking with Images paradigms, illustrating the evolution from selection to creation and imagination. Stage 1: The model selects from fixed toolkit, categorized into semantic grounding, perception exploration, and visual reasoning-aid tools. Stage 2: The model creates customized programs by composing visual, logical, and composite operations. Stage 3: The model generates internal thoughts, which can manifest as implicit latent reasoning, explicit visual reasoning, and interleaved multimodal reasoning. function. The models primary role is to determine which tool to use and how to apply it based on its evolving understanding of the problem. The core action at each reasoning step is for the LMMs policy to generate symbolic tool call, ct. This call specifies the chosen tool from the inventory Tsel and the necessary arguments atool. ct (St, I, Q; ΘLMM ) where ct = (Tsel, atool) (3) This generated command is then passed to an external tool execution engine Etool, which executes the specified operation on the image I. The engine returns an output ot: ot = Etool(ct, I) where ot Ttext Ivis (4) This output ot, which can be modified image, data layer like segmentation mask, or structured text, is then integrated back into the models state St+1 to inform the next reasoning step. The key characteristic of this stage is that visual operations are selected from fixed set. This marks significant departure from the Thinking about Images paradigm, where the visual input is static. Here, the model actively interrogates the visual world through controlled interface, but its operational vocabulary is finite and predefined. The models intelligence is demonstrated in its ability to orchestrate this fixed set of tools in logical sequence. 3.2 Categories of Visual Tools The effectiveness of Thinking with Images hinges on well-curated and diverse tools. In Figure 3, these can be grouped into three categories, each serving cognitive purpose in the reasoning chain. Semantic Grounding Tools. These tools bridge the gap between pixel space and symbolic language by transforming visual content into textual representations. This category includes tools for generating descriptive captions of scene [Hu et al., 2022] and employing Optical Character Recognition (OCR) to extract embedded text from images, function central to frameworks like TACO [Ma et al., 2024] and CogCoM [Qi et al., 2024]. Their primary role is to provide foundational semantic grounding, converting raw visual data into discrete, symbolic format that the LMM can directly process and reason about. Perception Exploration Tools. This category of tools serves to enhance perception, enabling the model to actively examine visual content for finer details that single, holistic view might miss. The operations occur within the visual domain itself to provide richer perceptual input. They facilitate this through mechanisms like programmatic cropping or zooming to inspect specific regions with high fidelity [Shen et al., 2024, Zheng et al., 2025]. This category also includes specialized models like object detectors and semantic segmenters, which identify and isolate key elements for more focused analysis [Wu et al., 2024c, Yang et al., 2023b] Visual Reasoning-Aid Tools. This category of tools directly supports the cognitive process itself, transforming the image into an active workspace for reasoning. The core function is to externalize thought, making abstract logical steps tangible and verifiable on the visual canvas. This is achieved by highlighting critical regions to direct attention, masking irrelevant information to reduce distraction, or drawing new elements like auxiliary lines in geometry problem or pointers for counting objects [Yang et al., 2023b, Fu et al., 2025a]. These actions modify the image not just for better perception, but to actively structure and guide the flow of thought. 3."
        },
        {
            "title": "Implementation Approaches",
            "content": "To equip models with tool orchestration, models must be enabled to act as effective coordinators. This section details the three main methodologies used to instill this capability: prompt-based methods that leverage in-context learning, supervised fine-tuning that teaches procedural competence through examples, and reinforcement learning that enables policy optimization for tool use. 3.3.1 Prompt-Based Approaches Prompt-based methods enable LMMs to coordinate predefined external visual tools without parameter updates. Carefully designed prompts transform static visual inputs into actively explorable workspaces, leveraging the models in-context learning to facilitate robust problem-solving. Mediating Collaboration through Language. This foundational approach uses language as universal interface to grant text-only Large Language Models (LLMs) visual reasoning capabilities. Socratic Models [Zeng et al., 2022] pioneered this concept by enabling specialized models to collaborate through structured dialogues, where vision-language model provides perceptual insights for an LLM to process. Building on this, PromptCap [Hu et al., 2022] introduced more targeted method by developing question-aware captioning models. These models generate task-specific visual descriptions to guide the LLM, rather than generic captions. Similarly, MM-REACT [Yang et al., 2023a] demonstrated that text-only model can act as central controller, orchestrating pool of vision experts through prompted instructions to decompose and solve complex visual tasks. These methods establish that multimodal reasoning can emerge from language-mediated coordination, allowing text-only models to function as visual reasoning systems without architectural changes. Manipulating Input for Perception. This category improves the perceptual capabilities of existing LMMs through simple input modifications and systematic exploration. Visual prompt engineering is technique where drawing red circle on an image directs models attention to specific regions [Shtedritski et al., 2023, Zhang et al., 2025f], with the goal of enhancing zero-shot performance without fine-tuning. Beyond spatial highlighting, Visualization-of-Thought (VoT) [Wu et al., 2024b] enhances spatial reasoning by prompting models to generate textual visualizations like ASCII grids. This process creates an explicit spatial workspace that enables text-only LLMs to solve spatial tasks. Another approach, Visual Abstract Thinking (VAT) [Liu et al., 2025a], simplifies visual inputs into abstractions like sketches or contours, filtering out distracting noise while preserving essential structural information. For systematic exploration, methods like ZoomEye [Shen et al., 2024] introduce training-free, tree-based search strategies that allow models to perform human-like zooming operations and explore image hierarchies. Similarly, ViCrop [Zhang et al., 2025e] and Chain-of-Spot (CoS) [Liu et al., 2024d] leverage the models internal attention to automatically crop and zoom into relevant regions. VisuoThink [Wang et al., 2025b] extends this by integrating multimodal tree search with interleaved reasoning to explore multiple solution paths. These approaches show that simple modifications and structured exploration can enhance model perception without retraining. Methods Base Model Data Tool Categories Scenario SG PE VR Prompt-Based Approaches Socratic M. [Zeng et al., 2022] PromptCap [Hu et al., 2022] MM-REACT [Yang et al., 2023a] Visual ChatGPT [Wu et al., 2023b] Shtedritski [Shtedritski et al., 2023] SoM [Yang et al., 2023b] VoT [Wu et al., 2024b] DetToolChain [Wu et al., 2024c] ZoomEye [Shen et al., 2024] Chain-of-Spot [Liu et al., 2024d] DyFo [Li et al., 2025f] ViCrop [Zhang et al., 2025e] Visual Thoughts [Cheng et al., 2025] VisuoThink [Wang et al., 2025b] VAT [Liu et al., 2025a] GPT-3 GPT-3 GPT-3.5 GPT-3.5 CLIP GPT-4V GPT-4 GPT-4V, etc. LLaVA-v1.5-7B, etc. LLaVA-v1.5-7B, etc. LLaVA-v1.5-7B, etc. LLaVA-v1.5-7B, etc. LLaVA-v1.5-7B, etc. GPT-4o, etc. GPT-4o, etc. SFT-Based Approaches LLaVA-7B CogVLM-17B Vicuna-7/13B, etc. LLaVA-7B LLaVA-Plus [Liu et al., 2023] CogCoM [Qi et al., 2024] Visual CoT [Shao et al., 2024] IVM [Zheng et al., 2024a] VisualReasoner [Cheng et al., 2024a] LLaVA-1.5-7B TACO [Ma et al., 2024] CMMCoT [Zhang et al., 2025a] v1 [Chung et al., 2025] V* (SEAL) [Wu and Xie, 2023] VTS-V [Bai et al., 2025a] VGR [Wang et al., 2025h] UniVG-R1 [Bai et al., 2025c] LLaMA3-8B, etc. Qwen2-VL-7B Qwen2.5-VL-7B LLaVA-7B Qwen2.5-VL-7B, etc LLaVA-NeXT Qwen2-VL-2B, etc 158K 91K 438K 334K 50K 293K 260K 300K 387K 301K 158K 94K RL-Based Approaches Chain-of-Focus [Zhang et al., 2025b] Qwen2.5-VL-7B Qwen2.5-VL-7B ACTIVE-o3 [Zhu et al., 2025a] Qwen2.5-VL-7B DeepEyes [Zheng et al., 2025] Qwen2.5-VL-3B, etc. GRIT [Fan et al., 2025a] Qwen2.5-VL-7B VisTA [Huang et al., 2025a] Qwen2.5-VL-3/7B, etc. Jigsaw-R1 [Wang et al., 2025c] Qwen2.5-VL-3B, etc. Seg-Zero [Liu et al., 2025c] Qwen2.5-VL-7B, etc. PixelThink [Wang et al., 2025i] Qwen2.5-VL-7B VisionReasoner [Liu et al., 2025b] Qwen2.5-VL-7/32B V-Triune [Ma et al., 2025] Qwen2.5-VL-7B Point-RFT [Ni et al., 2025] Qwen2-VL-2B OpenThinkIMG [Su et al., 2025a] Qwen2.5-VL-7B Pixel-Reasoner [Su et al., 2025b] VLM-R3 [Jiang et al., 2025b] Qwen2.5-VL-7B Qwen2.5-VL-7B VILASR [Wu et al., 2025a] 3.7K 1K* 47K 0.02K 0.8K* 180K 9K 9K 66K 47K 71K 14.5K 15K 12K 81K Table 1: Methods for Tool-Driven Visual Exploration. This table classifies approaches by their base model and data requirements. It details the categories of tools employed: Semantic Grouding (SG), Perception Exploration (PE), and Visual Reasoning-Aid (VR). It also specifies application scenarios: foundational Perception (P), complex multi-step Reasoning (R), and novel Generation (G). Data amounts marked with * denote per-task requirements. Leveraging Specialized Visual Experts. This approach integrates specialized visual expert models to achieve high-precision visual understanding. Set-of-Mark (SoM) prompting [Yang et al., 2023b] exemplifies this by using segmentation model to partition an image into regions, which are then overlaid with symbolic marks. This allows model like GPT-4V to achieve precise visual grounding by referencing these marks in its response. DetToolChain [Wu et al., 2024c] introduces prompting 12 paradigm that combines visual processing tools with structured detection reasoning chain to decompose complex detection tasks. Other work employs Monte Carlo Tree Search (MCTS) to enable bidirectional interaction between LMMs and visual experts, simulating human-like focus adjustments where expert feedback refines the models attention [Li et al., 2025f]. The Visual Thoughts [Cheng et al., 2025] framework provides theoretical basis for these methods, conceptualizing the outputs from expert models as intermediate visual thoughts that serve as cache for information transmission. These methods demonstrate that integrating expert systems provides the fine-grained visual information needed for precise spatial grounding and analysis. 3.3.2 SFT-Based Approaches SFT is primary method for teaching LMMs to use external tools or internal visual skills. The process involves fine-tuning model on datasets that demonstrate the procedural steps of tool use. This includes tool invocation and the integration of tool outputs into coherent reasoning chain. Learning to Orchestrate Tools primary application of SFT is teaching an LMM to compose and orchestrate set of external, predefined tools. This approach grounds the models reasoning in verified, external knowledge sources or specialized visual processors. For example, LLaVAPlus [Liu et al., 2023] is fine-tuned on multimodal instruction data, which enables it to manage skill repository and learn to activate the appropriate tool based on user intent. similar strategy is employed by TACO [Ma et al., 2024], which is trained on large, synthetic dataset of Chains-ofThought-and-Action (CoTA) traces. This data explicitly teaches the model to generate reasoning steps, call external tools such as OCR or calculators with the correct syntax, and integrate the returned information. In addition, VTS-V [Bai et al., 2025a] proposes novel framework where SFT is utilized to supervise the models generation of multi-step visual reasoning trajectories, enabling dynamic and verifier-guided use of visual information throughout the inference process. In these cases, SFT provides the necessary supervision to translate high-level plan into sequence of executable actions, giving the model true procedural competence. Developing Internal Visual Operations. Beyond orchestrating external APIs, SFT is crucial for developing models capacity for internal visual manipulation. This represents significant step towards cognitive autonomy, as the skills become part of the model itself. The CogCoM framework [Qi et al., 2024] exemplifies this by teaching model to perform intrinsic actions like object grounding, regional OCR, or image cropping without relying on external modules. The fine-tuning is performed on dataset of Chain-of-Manipulation (CoM) samples. These samples demonstrate not just the action, but also how to generate the necessary parameters for it, such as coordinates for crop. Similarly, VGR [Wang et al., 2025h] introduces novel multimodal large language model that enhances fine-grained visual perception by detecting relevant regions and providing precise answers based on replayed image regions, addressing limitations of relying solely on language space for multimodal chain-of-thought reasoning. Furthermore, UniVG-R1 [Bai et al., 2025c] tackles the challenge of universal visual grounding in multi-image scenarios with complex instructions by proposing reasoning-guided multimodal large language model that utilizes reinforcement learning and high-quality Chain-of-Thought grounding dataset to enhance its reasoning capabilities. Cultivating Dynamic Perception Exploration. particularly advanced application of SFT is the cultivation of dynamic visual attention as learned, internal tool. This approach directly addresses the limitations of static perception by enabling the model to actively inspect visual input. This is achieved by fine-tuning the model on datasets with specialized attentional annotations. For instance, Visual CoT [Shao et al., 2024] uses data with bounding box annotations to teach the model to highlight critical regions during reasoning. In contrast, Instruction-Guided Visual Masking (IVM) [Zheng et al., 2024a] trains the model to generate masks that hide instruction-irrelevant image areas, effectively sharpening its focus. SFT can also foster more complex cognitive operations, such as learning cross-image comparison from demonstration [Zhang et al., 2025a] or developing policy for selective visual revisitation [Chung et al., 2025]. Architectures like V* [Wu and Xie, 2023] also rely on this principle to train visual search component. The common thread is that SFT transforms attention from passive mechanism into an active, controllable skill, with the training data providing direct supervision for visual attention allocation. 13 3.3.3 RL-Based Approaches RL advances beyond supervised imitation by enabling models to discover and optimize policies for tool use through environmental interaction and reward feedback. This methodology allows models to learn effective strategies for visual reasoning rather than just replicating demonstrated paths. Establishing Foundational RL Principles. Before complex Thinking with Images could be realized, foundational work was necessary to establish the viability and core principles of applying RL to visual tasks. Foundational studies like Jigsaw-R1 [Wang et al., 2025c] used controlled environments to reveal key principles, such as that RL offers superior generalization over SFT and that complex reasoning patterns appear to be pre-existing in models rather than emergent from training. Concurrently, V-Triune [Ma et al., 2025] and VisionReasoner [Liu et al., 2025b] demonstrated the versatility of the RL approach by creating unified framework. VisionReasoner, for instance, employs novel multi-object cognitive strategies and tailored rewards to handle diverse perception tasks like detection, chart, and counting within single shared model. Together, this foundational work proved that LMMs could learn generalizable visual skills via reinforcement learning. Learning Direct Integration Policies. With the foundational viability of this learning paradigm established, the initial application focused on direct form of multimodal integration: optimizing policies that produce reasoning steps which are themselves multimodal. This represents crucial advance beyond purely textual chains of thought by embedding spatial or visual information directly into the models output stream. To date, this concept has been most prominently realized through policies that learn to associate reasoning with positional information. The GRIT framework [Fan et al., 2025a], for instance, learns to effectively interleave natural language with explicit bounding box coordinates. Following this, Point-RFT [Ni et al., 2025] demonstrates how to further optimize these grounded rationales for correctness. The Seg-Zero [Liu et al., 2025c] framework showcases decoupled architecture where model learns policy for generating positional prompts that guide separate segmentation model. Although represented as text tokens, these integrated outputs function as nascent form of visual thought, an intermediate representation that explicitly merges spatial awareness into the reasoning process itself. Exploring Active Tool Orchestration. Building upon this, the next logical evolution in Thinking with Images grants models the ability to actively manipulate the visual input through direct tool orchestration. Initial frameworks centered on the fundamental challenge of tool management. VisTA [Huang et al., 2025a], for example, employs an RL agent to learn tool selection strategies, decoupling the agent from the frozen reasoner for policy transfer. This work leads to active perception, where the model learns to dynamically control its visual focus, often through an adaptive zoom-in capability. Chain-of-Focus [Zhang et al., 2025b] creates chain of focus by progressively zooming based on visual cues, while ACTIVE-o3 [Zhu et al., 2025a] trains sensing policy to propose informative regions that guide task-execution component. Further advancing this, DeepEyes [Zheng et al., 2025] enables this capability to emerge natively from models inherent grounding ability, using an Interleaved Multimodal Chain-of-Thought (iMCoT) without requiring cold-start SFT. Recognizing that models may be reluctant to use these novel operations, Pixel-Reasoner [Su et al., 2025b] introduces curiosity-driven RL scheme to explicitly incentivize this exploration. The principle of active interaction, however, is not limited to single skill. VILASR [Wu et al., 2025a], for instance, proposes drawing to reason in space paradigm, enabling models to leverage basic drawing operations to interact visually. This progression from static image viewers to active participants results in the development of frameworks that empower agents to master diverse set of vision tools in human-like manner. Extending beyond single perceptual skill, OpenThinkIMG [Su et al., 2025a] introduced the first open-source end-to-end framework to train adaptive policies for invoking diverse set of external vision tools by directly optimizing for task success. 3.4 Conclusion and Future Frontiers Conclusion. The explicit visual tool exploration paradigm is powerful due to its directness and practicality. Its strength lies in allowing models to leverage existing, specialized visual tools to solve specific sub-tasks in transparent and debuggable manner [Yang et al., 2023a, Ma et al., 2024]. This capability is realized through methodological spectrum: in-context prompting offers zero-shot flexibility, Supervised Fine-Tuning instills reliable, procedural competence, and Reinforcement Learning enables the discovery of autonomous policies. Despite this versatility, the paradigm is 14 Methods Base Model Data Operation Categories Scenario VO LO CO G Prompt-Based Approaches GPT-3 VisProg [Gupta and Kembhavi, 2023] GPT-3 ViperGPT [Surís et al., 2023] GPT-4o CAD-Assistant [Mallis et al., 2024] LLaVA-7/13B, etc. MMFactory [Fan et al., 2024] GPT-4o Visual Sketchpad [Hu et al., 2024b] GPT-4o VipAct [Zhang et al., 2024a] Claude3.5-Sonnet SketchAgent [Vinker et al., 2024] ReFocus [Fu et al., 2025a] GPT-4o Interactive Sketchpad [Chen et al., 2025d] GPT-4o Training-Based Approaches VPD [Hu et al., 2024a] CoSyn [Yang et al., 2025] PROVISION [Zhang et al., 2024b] Flame VLM [Ge et al., 2025] MathCoder-VL [Wang et al., 2025d] PaLI-3 & PaLI-X CLIP & Mistral-7B xGen-MM4B SigLIP & DeepSeek-coder InternVL2-8B, etc. Visual-ARFT [Liu et al., 2025d] Qwen2.5-VL-3/7B 89.6K 400K 10M 3.3M+ 3M 1.2K Table 2: Methods for Programmatic Visual Manipulation. This table classifies approaches into promptbased and training-based (method marked with utilizes reinforcement learning, while the rest utilizes supervised fine-tuning). It details the categories of operations employed: Visual Operations (VO), Logical Operations (LO), and Composite Operations (CO); as well as their application scenarios, spanning Perception (P), Reasoning (R), and Generation (G), similar to Stage 1. fundamentally constrained by its reliance on fixed set of predefined tools [Liu et al., 2023]. model cannot invent novel visual operation beyond its given toolbox. This core limitation motivates the move towards the programmatic approaches discussed in the next section, which grants models the ability to construct their own unique visual operations. Future Frontiers. While the field is advancing, several frontiers remain for tool orchestration. Foundational work will continue to focus on the tools themselves: enhancing their quality to reduce error propagation, expanding their coverage to address long-tail scenarios, and developing standardized interfaces to ensure robust integration. Beyond these practical improvements, however, the most significant frontiers concern the agents cognitive capabilities. Autonomous Policy Learning. critical direction is moving beyond simple tool invocation toward autonomous policy learning. This involves training agents that can not only select the right tool but also compose them in novel sequences to solve complex, multi-step problems. The current shift from imitation learning (SFT) to goal-driven discovery (RL) is the first step. Future work will require developing novel methodologies that allow an agent to quickly adapt to new tools and learn generalizable strategies for visual problem-solving. Internalization of Skills. An even more profound frontier is the internalization of skills, which blurs the line between external tools and internal capabilities. Instead of always relying on an external tool, future model might develop an innate, fine-tunable skill for Thinking with Images. The current work on using RL to treat visual attention as controllable spotlight is an early example [Zhang et al., 2025b]. Pushing this frontier means designing architectures and training objectives that encourage models to absorb the functionality of common tools, transforming them from passive orchestrators into agents with rich set of intrinsic visual abilities."
        },
        {
            "title": "4 Stage 2: Programmatic Visual Manipulation",
            "content": "While Stage 1 teaches model to select from fixed toolkit, Stage 2 represents profound leap in autonomy by teaching it to create. Here, the model evolves from tool orchestrator into visual programmer. In particular, it learns to generate executable code that specifies tailored visual operations, granting it the flexibility to construct novel analysis pipelines tailored to unique demands. 15 Table 2 summarizes the main methods in this stage, categorizing them by their composable operations and primary application scenarios. 4.1 The Programmatic Leap: From Selection to Creation This programmatic leap from tool selection to code creation unlocks the fundamental advantages of this stage. By moving beyond finite toolbox to generative grammar of visual operations, the model gains compositional flexibility, enhanced control, and greater interpretability. Compositional Flexibility. By generating code, the model transcends the constraints of finite tool menu and gains access to potentially infinite space of visual operations. It can compose primitive functions to construct novel and highly specific analysis pipelines on the fly [Gupta and Kembhavi, 2023, Surís et al., 2023]. The core insight is the shift from finite toolbox to generative grammar of visual manipulation. This compositional power is critical for tasks with complex, multi-step visual criteria that no single pre-defined tool could address, such as workflow to find all objects whose color matches the average color of specified region. This empowers the model with robust form of generalization, enabling it to construct solutions for entirely new problems from first principles. Dynamic and Non-Linear Control Flow. Orchestrating fixed tools is often linear process (e.g., call Tool A, then Tool B), where the LMM handles all decision-making between steps. By generating code, the model can embed logical branches (if-else), loops (for), and stateful variables directly into its program [Chen et al., 2022b, Gao et al., 2023]. This means the model creates dynamic program that can adapt its execution path based on intermediate visual findings [Yao et al., 2023, Hu et al., 2024b]. For instance, it can first diagnose visual artifact like image blur and then generate code to apply deblurring filter before proceeding with its analysis, capability essential for robust performance in dynamic environments [Liu et al., 2025d]. Enhanced Interpretability and Control. The generated code serves as deterministic, and humanreadable reasoning trace. This provides transparent view into the models problem-solving process, in contrast to the opaque nature of models internal thought process or black-box tool call. Each line of code is an explicit, verifiable step in the models logic [Gupta and Kembhavi, 2023, Hu et al., 2024a]. This transparency is not merely for observation; it provides powerful mechanism for control and collaboration. The code acts as shared artifact that human user can inspect, debug, and even refine. This creates natural entry point for human-AI interaction, enabling an iterative workflow where human expertise can guide and correct the AIs visual analysis, crucial feature for high-stakes applications [Mallis et al., 2024, Chen et al., 2025d]. 4.2 Formulation In this paradigm, the LMMs primary action is to generate programmatic instructions C, typically as script in language like Python that utilizes computer vision libraries (e.g., OpenCV, Pillow) or domain-specific APIs. The state St includes the history of previously generated code snippets and outputs. The core action at each reasoning step is the generation of code segment Ct, conditioned on the reasoning history St, the initial image I, the original input query Q: Ct (St, I, Q; ΘLMM ) (5) These segments are composed into complete program = (C1, C2, . . . , CN ). This program is then passed to an external interpreter or execution engine Eexec, which applies the sequence of operations to an initial image I0 = I: (Ifinal, Oprog) = Eexec(C, I0) (6) The execution yields potentially modified image Ifinal and any textual or structured output Oprog generated by the program, such as calculated values or detected features. The LMM uses this information to continue reasoning or to produce the answer. The fundamental distinction from Stage 1 is that the visual operations are no longer selected, but are composed and defined by the LMM through code generation. This allows the model to design and execute tailored, complex algorithms. 16 4.3 Categories of Composable Operations In the programmatic paradigm, the models vocabulary consists of composable building blocks that it generates as code. As shown in Figure 3, these operations can be grouped into three main categories: Visual Operations. These are the fundamental functions that directly interact with image data, acting as the models hands and eyes on the visual canvas. This category includes perceptionoriented operations like region and object detection (e.g., find_objects, crop_region) and feature extraction (e.g., get_color, ocr). It also includes action-oriented operations such as geometric transformations (e.g., rotate, resize) and, critically, drawing functions that allow the model to create visual aids to guide its own reasoning, such as annotating an image with auxiliary lines or highlighting areas of interest [Bradski, 2000, Chen et al., 2025d]. These primitives form the essential vocabulary for any visual algorithm. Logical Operations. These operations leverage the power of the host programming language to provide structure and logic to the visual analysis. This includes state management via variable assignment, conditional execution through if/else blocks, and iterative processing over visual elements using for or while loops [Hu et al., 2024a, Ge et al., 2025]. This logical scaffolding is the grammar that arranges the visual operations into coherent and powerful algorithm. It enables the model to perform stateful, multi-step reasoning, transforming simple sequence of actions into true computational process. Composite Operations. This category represents the synergistic combination of visual and logical operations. By weaving visual operations into logical scaffolding, the model can create highly specific and complex algorithms that exhibit emergent behavior. task like find all red circles larger than the average area of all circles is perfect example. It requires composite algorithm that iterates through objects (loop), checks their properties (conditionals), performs calculations (stateful variables), and ultimately returns result. This ability to construct tailored algorithms on demand is what distinguishes this stage, moving beyond predefined capabilities to genuine, problem-driven visual computation [Fan et al., 2024, Mallis et al., 2024]. 4.4 Implementation Approaches Similar to the tool orchestration paradigm, programmatic manipulation is enabled by the same three core methodologies: training-free prompting, SFT, and RL methods. This section details how these approaches are specifically adapted to equip models with code-generation capabilities. 4.4.1 Prompt-Based Approaches In this stage, prompt-based approaches instruct an LMM to generate executable code, typically in Python, as transparent intermediate reasoning step. This method leverages in-context learning to create deterministic and verifiable visual analysis pipelines. Composing Programs from Instructions This line of work establishes the core paradigm of programmatic visual reasoning by leveraging LLMs to generate executable code that orchestrates visual computations. VisProg [Gupta and Kembhavi, 2023] pioneered this approach by utilizing the in-context learning capabilities of GPT-3 to generate Python-like programs from natural language instructions, decomposing complex visual tasks into sequences of modular operations that invoke off-the-shelf computer vision models, image processing functions, and logical operators without requiring task-specific training. Building upon this foundation, ViperGPT [Surís et al., 2023] refined the methodology by employing code-generation models like Codex to produce Python programs that explicitly define reasoning steps through predefined API, enabling the composition of various pre-trained vision modules as subroutines while leveraging Pythons native logic for operations such as counting, sorting, and conditional branching. Both approaches share the fundamental principle that code serves as an interpretable and verifiable intermediate representation for visual reasoning, where the generated program itself becomes the explanation of the reasoning process. This training-free paradigm demonstrates remarkable flexibility and strong zero-shot performance across diverse visual reasoning tasks, establishing code generation as powerful method for bridging natural language instructions with systematic visual computations through modular, composable operations. Sketching Thoughts onto Canvases. This advanced category extends beyond tool invocation to actively create and manipulate visual content as integral components of the reasoning process, embodying more refined form of visual thinking. Visual Sketchpad [Hu et al., 2024b] introduces the concept of visual CoT by enabling LMMs to synthesize Python code that generates intermediate visual aids (e.g., auxiliary lines for geometry problems). These visual aids then inform subsequent reasoning steps in an iterative loop of thought, action, and observation. Complementing this generative approach, ReFocus [Fu et al., 2025a] focuses on structured image understanding by empowering models to generate Python code for targeted visual editing operations, including drawing bounding boxes and masking irrelevant areas to implement selective attention for tables and charts. SketchAgent [Vinker et al., 2024] demonstrates language-driven sequential sketch generation through an intuitive sketching language that represents drawings as stroke-by-stroke actions, enabling diverse sketch creation without model training. Interactive Sketchpad [Chen et al., 2025d] showcases educational applications by generating accurate diagrams through Python code execution, facilitating collaborative problem-solving on shared interactive whiteboards. This paradigm establishes programmatic visual manipulation as powerful reasoning mechanism, enabling models to externalize and refine their visual understanding through iterative code-driven interactions with visual content. Weaving Expertise into Systems. This most advanced category represents the culmination of programmatic approaches through comprehensive system architectures and deep domain specialization that transcend simple code generation. VipAct [Zhang et al., 2024a] exemplifies advanced multi-agent integration by orchestrating collaboration framework with an orchestrator agent, specialized LMM agents, and vision expert models that systematically gather detailed visual evidence across multiple components. MMFactory [Fan et al., 2024] introduces universal solution search engine that employs committee-based multi-agent LLM conversations to generate diverse, executable programmatic solutions while benchmarking performance and computational costs for optimal selection. CAD-Assistant [Mallis et al., 2024] demonstrates deep domain specialization by creating comprehensive CAD agent fully integrated with FreeCAD software, enabling iterative interaction with CAD models through specialized tools for parameterization, rendering, and constraint analysis. This paradigm represents the maturation of programmatic visual reasoning into production-ready systems that combine complex architectural design, multi-component orchestration, and domain-specific expertise to tackle real-world applications with practical deployment requirements. 4.4.2 SFT-Based Approaches SFT is crucial method for teaching LMMs to reason programmatically. It fine-tunes model to either generate code as an intermediate reasoning step or to understand images rendered from code. This approach leverages the structured logic of programs to guide the learning process, pushing the model beyond simple pattern recognition toward more systematic and transparent problem-solving. Internalizing Logic from Programs. This strategy uses SFT to distill multi-step programmatic logic into LMMs. Visual Program Distillation (VPD) [Hu et al., 2024a], uses LLMs to generate executable programs that invoke specialized vision modules. The execution traces of verified programs are then converted into natural language rationales, which serve as the data for instruction-tuning the LMM. This distillation process enables the LMM to replicate the detailed programmatic workflow in an efficient, single forward pass. In similar effort, MathCoder-VL [Wang et al., 2025d] performs an initial image-to-code SFT stage to align the models vision encoder with the precise semantics of code (e.g., TikZ, Python) used to render mathematical figures, establishing crucial link between visual data and its symbolic code representation. PROVISION [Zhang et al., 2024b] presents programmatic approach employing scene graphs as symbolic representations of images and humanwritten programs to systematically synthesize vision-centric instruction data, enabling the scalable generation of interpretable, controllable, and accurate multimodal instruction data. Bootstrapping Datasets with Code. This approach leverages code generation to create perfectly aligned, high-quality multimodal data for instruction tuning. Frameworks like CoSyn [Yang et al., 2025] prompt text-only LLM to first generate code (e.g., Python, HTML) that renders synthetic, text-rich image and then use the same source code as ground truth to create corresponding questionanswer pairs and CoT rationales. SFT on such data has proven highly sample-efficient for teaching LMMs to reason over structured images like charts and tables. Likewise, the main training stage of MathCoder-VL [Wang et al., 2025d] fine-tunes the model on dataset of code-synthesized mathematical figures paired with LLM-generated problems and detailed solutions. The Flame 18 Methods Base Model Data Thoughts Categories Scenario LR VS MD G SFT-Based Approaches MiniGPT-4 & SD v2.1 OPT-6.7B, etc. Chameleon-7/34B Transformer-7B & U-Net LLaMA-33B, etc. LLaMA-2-13B, etc. Vicuna-7B, etc. Chameleon-7B LLaMA-2 & MoVQGAN Phi-1.5 & MAGVIT-v2 LLaMA-2-7B, etc. LLaMA-3.1-8B, etc. LLaMA-3-8B, etc. CLIP & SigLIP ViT SD v3-medium & DiT LLaVA-ov-0.5B, etc. Qwen2.5-0.5B, etc. Qwen2.5-7B, etc. Qwen2.5-3B, etc. Qwen2.5-VL-7B, etc. Minigpt-5 [Zheng et al., 2023] GILL [Koh et al., 2023] Chameleon [Team, 2024] Transfusion [Zhou et al., 2024a] Emu2 [Sun et al., 2024a] SEED-X [Ge et al., 2024] Next-GPT [Wu et al., 2024a] Anole [Chern et al., 2024] EMU-3 [Wang et al., 2024b] Show-o [Xie et al., 2024b] VILA-U [Wu et al., 2024d] Metamorph [Tong et al., 2024] LMFusion [Shi et al., 2024a] TokenFlow [Qu et al., 2024a] D-DiT [Li et al., 2024a] MetaQueries [Pan et al., 2025] UniFork [Li et al., 2025g] BAGEL [Deng et al., 2025b] Mogao [Liao et al., 2025] BLIP3-o [Chen et al., 2025b] Think while Gen. [Chern et al., 2025] Anole-7B CoT-VLA [Zhao et al., 2025a] GoT [Fang et al., 2025] Janus [Wu et al., 2024e] Janus-Pro [Chen et al., 2025c] Show-o2 [Xie et al., 2025] MVoT [Li et al., 2025a] VILA-U-7B Qwen2.5-VL-3B, etc. DeepSeek-LLM-1.3B, etc. DeepSeek-LLM-7B, etc. Qwen2.5-1.5/7B, etc. Anole-7B 3.62M 3.62M 4.4T+ 2T 160M+ - 7M 6K - 1.05B 15M 510M 380M+ 760M 40M 25M 130M+ 5.1T - 28M+ 4.5M - 9.37M 7M+ 162M+ 75M 17K RL-Based Approaches GoT-R1 [Duan et al., 2025] PARM [Guo et al., 2025a] T2I-R1 [Jiang et al., 2025a] Visual Planning [Xu et al., 2025] ControlThinker [Han et al., 2025] Janus-Pro-1B/7B LLaVA-OV-7B & Show-o Janus-Pro-7B LVM-3B Qwen2.5-VL-7B, etc. - 400K 6.8K 1K* 21.6M Table 3: Methods for Intrinsic Visual Imagination. This table classifies approaches by their base model and data requirements. It details the categories of generative thoughts employed: Implicit Reasoning via Latent Representations (LR), Explicit Reasoning via Visual Scratchpads (VS), and Interleaved Reasoning via Multimodal Dialogue (MD). It also specifies their primary application scenarios: Perception (P), Reasoning (R), and Generation (G). Data suffixes: * means for each task. VLM [Ge et al., 2025] exemplifies direct application of this, being SFT-trained on synthesized image-code pairs to translate visual UI designs into executable front-end code. These approaches collectively demonstrate that SFT is being used to bridge the gap between the symbolic, logical nature of code and programs, and the sub-symbolic, pattern-based nature of LMMs. By training on data derived from or representing code, LMMs are effectively taught to think like programmer when approaching visual tasks. The structured, compositional, and often hierarchical nature of code-based SFT data induces correspondingly more structured and compositional reasoning pathways within the LMM. This can be seen as form of implicit neuro-symbolic learning, where the symbolic component (the logic inherent in the code or program structure) provides strong inductive bias that shapes the learning of the neural component during SFT. 4.4.3 RL-Based Approaches Reinforcement Learning is key paradigm for advancing this programmatic approach, as it optimizes models ability to generate effective code-based operational sequences by learning directly from execution feedback and task-oriented rewards. This marks critical shift from imitating static 19 solutions to discovering novel strategies through trial and error, process that embodies more dynamic and powerful form of thinking with images. Visual Agency via Feedback RL training enables LMMs to act as autonomous agents, generating code to create tailored visual operations on the fly. primary challenge in this paradigm is the vast and sparse space of valid programs; single error can render generated script useless. To overcome this, RL-based methods learn from execution feedback. Visual-ARFT [Liu et al., 2025d] exemplifies this by employing RL with verifiable rewards. Instead of relying solely on the final task outcome, the model receives crucial intermediate rewards based on whether its generated code executes successfully. This dense reward signal provides more stable learning gradient, guiding the model to produce functional code and shifting its role from mere tool user to tool maker. Beyond local image manipulation, this programmatic autonomy extends to foraging for external evidence through agentic web search, more advanced capability also demonstrated by Liu et al. [2025d]. The model learns to seamlessly interleave these two powerful operations, creating refined, multi-step reasoning workflow. For instance, an agent might first formulate search query to gather real-time information or contextual knowledge, and then generate Python code to process, analyze, or compare the retrieved information with the original visual input. This interleaved search-then-code workflow represents higher level of agentic reasoning, enabling the model to tackle open-world problems that require dynamic information retrieval and precise visual analysis. 4.5 Conclusion and Future Frontiers Conclusion. The programmatic visual manipulation paradigm endows models with compositional flexibility and interpretability. By generating code, model can construct tailored algorithms from primitive operations, allowing it to tackle vast range of complex visual problems that are intractable for fixed toolsets [Surís et al., 2023]. The generated program also serves as transparent and verifiable reasoning trace, which is invaluable for debugging and human-AI collaboration [Wang et al., 2025d]. However, this paradigms primary limitation is its fundamental dependence on an external execution environment. The reliance on code interpreter or search API creates an efficiency bottleneck and fragile point of failure [Hu et al., 2024a]. More profoundly, semantic gap persists between the models internal, latent-space reasoning and the rigid syntax of code. This need to outsource visual actions motivates the next evolutionary stage: enabling the model to perform these operations intrinsically. This inquiry leads directly to the final stage of our survey, Intrinsic Visual Imagination, where the model generates not an instruction, but the new visual state itself. Future Frontiers. As critical stepping stone towards intrinsic imagination, advancing the programmatic stage remains vital research area. Several promising frontiers exist for creating more robust and capable visual programmers. Robustness and Self-Correction. key direction is developing models that can debug and correct their generated code. This involves training them to interpret error messages from the execution engine or, ambitiously, to visually inspect an incorrect output, diagnose the programmatic flaw, and autonomously rewrite the code. This would create resilient and reliable agents. Unifying Tool Use and Code Generation. Current models often specialize in either orchestrating tools (Stage 1) or generating code (Stage 2). significant frontier lies in developing unified agents that take the best of both worlds by combining the advantages of the two stages together. This addresses fundamental trade-off: the pragmatic efficiency of specialized tools versus the unbounded flexibility of programmatic creation. truly intelligent multimodal agent should not have to choose. Instead, it should be able to generate program (Stage 2) that strategically calls pre-existing, high-level tools (Stage 1) as subroutines."
        },
        {
            "title": "5 Stage 3: Intrinsic Visual Imagination",
            "content": "While Stage 2 teaches model to create programs for external execution, Stage 3 represents the ultimate leap in autonomy by teaching it to imagine solutions internally. Here, the model evolves from visual programmer into an intrinsic visual thinker. It learns to generate new visual states not as instructions to be interpreted, but as native part of its own thought process, enabling seamless and 20 closed cognitive loop where it reasons with its own mental imagery. In Table 3, we categorize the key approaches of this stage according to the operations they compose and the scenarios they address. 5.1 The Architectural Leap: From Execution to Imagination The transition from programmatic manipulation to intrinsic imagination represents an architectural shift. It closes the loop between reasoning and perception by internalizing the ability to create visual content, moving from model directing external actions to one that performs internal simulations. Unifying Generation and Reasoning. The key innovation of this Stage is the architectural integration of generative and reasoning capabilities within single, unified model. In prior stages, the models reasoning (language) and the visual manipulation (execution) are separate processes linked by an API or interpreter. This separation introduces latency and potential information loss. Intrinsic imagination, by contrast, enables model to generate new visual state directly from its internal representations, making the act of imagining native operation within its thought process [Team, 2024, Sun et al., 2024a]. This unified design removes the bottleneck of external calls and is the foundation for more fluid and efficient form of multimodal cognition. From Explicit Instruction to Implicit Simulation. Programmatic control requires the model to articulate an explicit, step-by-step procedure in code. Intrinsic imagination allows the model to perform more holistic and implicit form of simulation. For instance, to predict the outcome of pushing an object, programmatic model must calculate new coordinates. model with intrinsic imagination can instead generate an image that directly depicts the outcome, implicitly enforcing physical constraints like collision through the visual coherence of the generated image [Xu et al., 2025]. This moves the reasoning process from symbolic calculation to perceptual simulation, mode of thinking that is more robust for complex physical and spatial problems. 5.2 Formulation In this stage, the LMMs core capability is generative visual cognition. Rather than delegating visual tasks, the model directly produces new image states, t, as part of its reasoning sequence. The state St includes the current visual context (which could be the initial image or previously generated image t1), and the history of textual thoughts and generated images. At reasoning step t, the LMMs policy determines the modality and content of its next thought: zt (St, I, Q; ΘLMM ) where zt Ttext Ivis (7) If the model chooses to generate visual thought, zt = t, this new image is produced directly by the models internal generative mechanisms (e.g., its decoder in unified architecture). This generated image is then fed back as new perceptual input, becoming part of the state St+1 for subsequent reasoning. The critical distinction from prior stages is that the generation of the visual step is an intrinsic capability, not an outsourced function. This enables seamless loop between perception, reasoning, and imagination. 5.3 Categories of Generative Thoughts Intrinsic visual imagination can be realized through three primary paradigms, each defined by how the model integrates generative acts into its reasoning process. These approaches, also visually represented in Figure 3, represent different strategies for leveraging internal generative power. Implicit Latent Reasoning. This paradigm pursues architectural elegance and end-to-end integration. It operates in unified, efficient, and abstract manner, where visual thinking occurs within the models abstract feature space by manipulating latent representations or visual tokens that are not necessarily decoded into human-readable images at each step [Team, 2024, Sun et al., 2024a]. The reasoning is implicit in the transformation of these internal features. The core objective is to create unified autoregressive architecture that seamlessly predicts the next multimodal element, be it text or visual tokens. This approach prioritizes computational efficiency and deeply integrated, though less transparent, form of visual thought. Explicit Visual Reasoning. This paradigm focuses on making the models reasoning process transparent and interpretable. It achieves this through transparent, deliberate, and step-by-step process of generating explicit, human-readable images as intermediate steps. This approach extends the textual CoT into visual domain, creating visual scratchpad where the model can externalize its spatial thinking [Guo et al., 2025a, Li et al., 2025a]. For example, model might generate an image of geometry problem with auxiliary lines added, or visualize future state in planning task [Xu et al., 2025]. This approach makes the models visual thought process observable, analyzable, and more aligned with human problem-solving methods like sketching. Interleaved Multimodal Reasoning. This paradigm represents dynamic interplay between textual and visual thought, where reasoning is constructed through dynamic, synergistic, and conversational sequence of generated text and images [Duan et al., 2025]. The models thought process becomes multimodal dialogue with itself: it might generate text to form hypothesis, create an image to test it, and then generate more text to reflect on the visual evidence [Chern et al., 2025]. This synergistic approach combines the logical structure of language with the grounded intuition of vision, enabling powerful reasoning process for complex tasks like robotic planning and creative generation [Zhao et al., 2025a]. 5.4 Implementation Approaches Unlike prior stages centered on delegation, intrinsic visual imagination repurposes the models own generative architecture as reasoning tool. This is achieved primarily through SFT and RL, which shift the learning objective from directing external tasks to performing internal simulation. 5.4.1 SFT-Based Approaches SFT is the foundational technique for teaching model the grammar of multimodal thought. It works by showing the model explicit examples of how to generate interleaved text and images, create intermediate visual representations, or perform visual edits as part of reasoning chain. Unifying Modalities through Autoregression. significant push in this area is the development of unified models where SFT is crucial for balancing understanding and generation. Models like Chameleon [Team, 2024] and Emu2 [Sun et al., 2024a] are trained on vast quantities of interleaved mixed-modal data using unified autoregressive objective to predict the next multimodal element. SFT is then applied via specialized alignment recipes or instruction tuning to hone performance on specific tasks. diverse range of SFT strategies has emerged to manage the understanding-generation trade-off [Xie et al., 2024a, He et al., 2024a, Chen et al., 2024a]. Some models adopt sequential SFT approach, first training on understanding tasks and subsequently on generation, as seen with BLIP-3o [Chen et al., 2025b], which uses curated instruction-tuning dataset for the generation phase. Others pursue efficiency by fusing pre-trained frozen components; GILL [Koh et al., 2023], for example, applies SFT only to lightweight mapping layers that connect frozen LLM with image encoders/decoders. Further techniques include adjusting data ratios [Chen et al., 2025c], creating special visual tokens [Tong et al., 2024, Zheng et al., 2023], and freezing text-specific modules to preserve language capabilities while training new visual components [Shi et al., 2024a]. Finally, models like BAGEL [Deng et al., 2025b] demonstrate that SFT on trillions of tokens from highly diverse, interleaved web data can unlock emergent capabilities like free-form image manipulation. These varied strategies highlight core design challenge: carefully orchestrating SFT to create synergy, rather than destructive interference, between models dual capabilities. Externalizing Thoughts as Images. Another direction uses SFT to enable models to generate visual thoughts as part of transparent reasoning process. This approach moves beyond text-only chains of thought by externalizing the models visual and spatial reasoning. For instance, GoT [Fang et al., 2025] uses SFT to train model to first produce an explicit language plan before generating an image that respects prescribed relationships. MVoT [Li et al., 2025a] fine-tunes models to think visually by generating image visualizations of their reasoning traces. Similarly, CoT-VLA [Zhao et al., 2025a] prompts models to predict future image frames as visual subgoals before generating robotic actions. Others apply reasoning directly to the image creation pipeline. more advanced paradigm, seen in Thinking with Generated Images [Chern et al., 2025], involves SFT on curated datasets of reasoning chains to enable models to spontaneously generate visual thinking steps like 22 subgoals and self-critiques. This method of using SFT to create explicit multimodal reasoning chains that function as visual scratchpad marks shift towards more interpretable and robust AI. Fusing Reasoning and Generation. clear trend is the convergence of techniques for generation and reasoning, with SFT acting as the unifying mechanism. Reasoning frameworks are increasingly applied to refine generation. For example, T2I-R1 [Jiang et al., 2025a] integrates bi-level reasoning process into the text-to-image pipeline, using SFT and RL to improve output quality. Conversely, advanced generative mechanisms like diffusion models [Croitoru et al., 2023] are being integrated into reasoning frameworks. BLIP3-o [Chen et al., 2025b], for instance, uses SFT to train diffusion transformer that generates rich image features as basis for high-quality image synthesis. This interplay shows that robust generation demands advanced reasoning, while powerful reasoning is amplified by the ability to generate intermediate steps. SFT facilitates this powerful integration, blurring the lines between generative models and reasoning models and paving the way for more holistic AI systems. 5.4.2 RL-Based Approaches While SFT teaches model to imitate demonstrated reasoning patterns, RL empowers it to autonomously discover effective, multi-step generative strategies to achieve goal. Simulating Futures with Vision. One of the most advanced forms of thinking with images is imagination, in which model not only perceive or interact with existing visual information but also generates entirely new visual content as intrinsic steps within its reasoning process, representing move from visual analysis to visual simulation and planning. The clearest embodiment of this concept is found in Visual Planning [Xu et al., 2025], which proposes framework where models reason and plan purely through sequences of generated images, completely independent of text. This approach is trained with novel Visual Planning via Reinforcement Learning framework. It demonstrates that model can successfully navigate complex environments by imagining the sequence of future visual states, setting benchmark for true visual-native reasoning. Orchestrating Imagination with Text. While pure visual planning remains nascent field, more common approach bridges textual reasoning with visual imagination. These methods can be understood as form of planned imagination, where the model first creates high-level conceptual plan or script in text, and then executes this plan by generating the corresponding visual scene. This is often realized through bi-level CoT process. For instance, GoT-R1 [Duan et al., 2025] applies RL to the Generation CoT framework, enabling models to autonomously discover effective semantic-spatial reasoning plans before generating the image. Similarly, T2I-R1 [Jiang et al., 2025a] introduces framework that explicitly decouples this process into semantic-level CoT for high-level planning and token-level CoT for the patch-by-patch pixel generation, jointly optimizing both with RL. These works demonstrate how the reasoning capabilities of LMMs can be effectively transferred to the visual generation domain, transforming simple prompt into well-reasoned visual output. To bridge the semantic gap in controllable image generation, ControlThinker [Han et al., 2025] employs fine-tuned LMM as semantic interpreter. This MLLM reasons about the low-level control signal to generate an enriched, semantically dense text prompt, which then guides an unmodified generator to create images with superior semantic consistency and quality. Refining Thoughts via Reflection. Effectively training these complex, imaginative processes requires advanced RL methodologies that go beyond simple correctness rewards. key challenge is evaluating the quality of generated image, which serves as an imagined thought. To address the reward modeling problem, recent work [Guo et al., 2025a] introduces PARM and PARM++, novel reward models designed specifically for autoregressive generation. They adaptively assess the generation process step-by-step for clarity and potential, with PARM++ even incorporating reflection mechanism for self-correction, providing more in-depth learning signal. 5.5 Conclusion and Future Frontiers Conclusion. The emergence of intrinsic visual imagination represents the culmination of the Thinking with Images paradigm, marking transition towards true cognitive autonomy. Instead of merely processing visual data, models are learning to use vision as an internal medium for 23 thought. The three distinct paradigms discussed each advance this capability in complementary way. Unified models provide versatile and scalable foundation, efficiently balancing understanding and generation. Explicit reasoning via visual scratchpads introduces crucial transparency, making the models thought process interpretable and verifiable, which is invaluable for complex spatial problems. Finally, interleaved multimodal dialogue achieves powerful synergy, combining the logical structure of language with the grounded intuition of vision in dynamic reasoning loop. Collectively, these methods chart clear trajectory for AI, moving it beyond merely consuming visual content to genuinely thinking with it. Future Frontiers. While this progress is transformative, the journey into artificial imagination has only just begun, opening up several critical frontiers for future research. The path for intrinsic visual imagination involves not only refining existing methods but also exploring fundamental questions about the nature of visual thought. The most promising frontiers aim to build more capable, efficient, and collaborative imaginative agents. Beyond Photorealistic Imagination. crucial frontier involves questioning the necessity of generating photorealistic images for every thought. Human cognition often relies on abstractions and sketches [Weng et al., 2025a, Zhou et al., 2023, 2024b], not perfect mental photographs. Future research should explore spectrum of internal visual representations beyond raw pixels [Jiang et al., 2025b], such as sparse feature maps, semantic masks, or 3D scene graphs. Such abstract representations could offer significant advantages in computational efficiency and might lead to more generalizable and interpretable reasoning. Learning Dynamic World Models. For imagination to be useful, it must be grounded in coherent understanding of the world. major research direction is the development of robust internal world models. This requires training models to simulate not just static scenes but also the dynamics, physics, and causal relationships within an environment [Hao et al., 2023]. Learning these implicit rules will enable more consistent and physically plausible visual simulations, which are essential for robust long-horizon planning and problem-solving. Human-AI Collaborative Imagination. As AI develops more advanced imaginative capabilities, transformative frontier will be human-AI co-imagination. This involves designing interfaces and interaction protocols that allow humans to guide, refine, and co-create with an AIs visual thought process [Chen et al., 2025d]. Such collaboration could unlock unprecedented creativity in fields from scientific discovery to artistic expression. This also brings forth critical ethical considerations regarding the responsible deployment of AI that can fabricate realistic visual content, necessitating robust safeguards and transparency."
        },
        {
            "title": "6 Evaluations & Frameworks for Thinking with Images",
            "content": "The development and assessment of models capable of Thinking with Images rely on robust benchmarks and specialized implementation frameworks. This section provides comprehensive overview of this critical infrastructure. We first review the key benchmarks designed to assess these advanced visual reasoning capabilities. Following this, we describe the primary implementation frameworks that enable models to execute these complex reasoning processes. 6.1 Evaluations Benchmarks The benchmarks for Thinking with Images measure models ability to perform complex tasks that extend beyond static visual perception. As illustrated by the diverse examples in Figure 4, these tasks span multiple reasoning domains, from mathematical problem-solving to real-world applications. To provide structured overview of this critical landscape, we organize these benchmarks into seven primary categories based on their core objectives. Mathematical Reasoning. Multimodal mathematical problems are designed to evaluate the capabilities of LMMs in complex visual perception, multi-step reasoning, and the integration of mathematical knowledge. Within this domain, geometry problems represent key challenge in this domain [Cao and Xiao, 2022, Chen et al., 2022a, Fu et al., 2025b, Zhang et al., 2024d], as they require LMMs to perceive fundamental visual elements and perform multi-step logical reasoning, thereby serving as an effective benchmark for evaluating both the visual and linguistic reasoning capabilities of LMMs. To further assess the reasoning capabilities of LMMs across broader range of multimodal 24 Figure 4: Demonstration of the Thinking with Images paradigm across seven key evaluation domains. The figure showcases applications ranging from (a) Mathematical Reasoning and (b) STEM to (c) Perception Reasoning, (d) Code Generation, (e) Chart and Table Reasoning, (f) Real-world Applications, and (g) Puzzles and Games. mathematical scenarios beyond geometry, recent studies have expanded the evaluation scope [Lu et al., 2024, Sun et al., 2024c, Wang et al., 2025f, Sun et al., 2025] to encompass diverse mathematical tasks, including algebra, analytic geometry, and arithmetic. More recently, with the emergence of increasingly powerful multimodal reasoning models [Guo et al., 2025b, Gemini Team, Google, 2025], an increasing number of expertand competition-level benchmarks [Wang et al., 2024c, He et al., 2024b, Gupta et al., 2024] have been introduced to better guide model development and evaluate their reasoning capabilities. Building on these efforts, some studies have been proposed to evaluate multimodal mathematical reasoning from model robustness to diagrammatic variations [Zhang et al., 2024e, Zou et al., 2025], reasoning with minimal or no textual context [Kamoi et al., 2024, Xiao et al., 2024], and generating structured outputs such as visual diagrams or natural language explanations [Li et al., 2025b, Park et al., 2025]. The core challenge of multimodal mathematical reasoning lies in performing complex mathematical reasoning in the language space while simultaneously constructing intermediate visual content that is not explicitly provided in the input, such as drawing auxiliary lines or plotting function graphs. Therefore, success on multimodal mathematical benchmarks inherently requires dynamic reasoning on visual content, making them crucial testbed for evaluating the Thinking with Images capabilities of LMMs. STEM. STEM (Science, Technology, Engineering, and Mathematics) benchmarks are designed to evaluate the reasoning capabilities of LMMs across scientific and technical domains. In this domain, early efforts focus on broad science education benchmarks, aiming to assess reasoning and knowledge across various subjects at the K12 level [Lu et al., 2022, Hao et al., 2025, Zhou et al., 2025a, Ye et al., 2025]. As the reasoning abilities of LMMs have rapidly advanced, subsequent works extend evaluations to university-level tasks that require deeper subject knowledge and complex reasoning capabilities [Yue et al., 2024a, Xia et al., 2024a, Wang et al., 2025j], and more recently, to professionallevel benchmarks targeting expert reasoning in highly specialized domains [Yue et al., 2024b, Zhou et al., 2025b]. Unlike the constructive but often static reasoning observed in mathematical tasks, STEM problems demand dynamic reasoning over temporal and causal phenomena, including physical motion, chemical reactions, and mechanical operations. This shift enhances the Thinking with Images paradigm by redefining the image as dynamic representation of real-world processes. As result, these benchmarks provide testbed for evaluating models understanding of physical laws and, more broadly, the sophistication of its emerging internal world model. Puzzles and Games. Benchmarks for puzzles and games are designed to evaluate the capacity of LMMs for strategic planning and reasoning in dynamic, rule-governed environments. Early bench25 marks introduced range of tasks, from visual puzzles assessing logical and spatial reasoning [Zhang et al., 2024f, Ren et al., 2025, Zhang et al., 2025c, Wu et al., 2024f] to interactive games that require understanding of game mechanics and state transitions [Wang et al., 2025k, Tong et al., 2025a, Paglieri et al., 2024]. More recent benchmarks have increased in complexity, incorporating dynamic video-based contexts that test physical and commonsense reasoning [Zhang et al., 2025g, Cao et al., 2024, Li et al., 2024b], as well as unstructured, competition-style puzzles demanding creative and open-ended problem solving [Wang et al., 2025l, Li et al., 2025h]. Distinct from reasoning in STEM domains, the core challenge of games lies in long-horizon, prospective reasoning. This represents form of Thinking with Images in which model must visually simulate complex action sequences on an interactive game board in order to devise an optimal plan. The scalability and structured nature of these environments also makes them suitable for both training and evaluating the planning and reasoning capabilities of LMMs. Perception Reasoning. Perception and reasoning benchmarks evaluate the visual capabilities of LMMs, encompassing the accurate interpretation of fine-grained visual elements and multi-step visual reasoning. One line of these tasks focuses on perceptual acuity, using high-resolution and detailrich imagery to assess fine-grained visual recognition [Wu and Xie, 2023, Fu et al., 2024a, Wang et al., 2025m]. Another line targets complex cognitive functions, including multi-step reasoning [Bi et al., 2025], object hallucination detection [Li et al., 2023a], and cognitive evaluations [Song et al., 2024, Zhao et al., 2025b, Huang et al., 2023]. These benchmarks assess whether model can move beyond superficial pattern recognition to develop deeper, structured understanding of visual scenes that supports robust reasoning. This capability establishes the perceptual and cognitive foundation necessary for the Thinking with Images paradigm. Code Generation. Code generation from visual or textual inputs represents key application domain of the Thinking with Images paradigm, requiring LMMs to generate executable programming logic based on complex visual content and corresponding instructions. Benchmarks in this area evaluate this ability across diverse contexts, such as interpreting mathematical plots and general programming-related diagrams [Li et al., 2024c, Wu et al., 2025b]. One significant application is front-end development, where models are tasked with converting visual designs, ranging from hand-drawn sketches to polished UI screenshots, into functional webpage code [Si et al., 2025, Li et al., 2025i, Laurençon et al., 2024]. This visual-to-code process exemplifies distinctive form of Thinking with Images. It requires the model to act as visual decompiler, parsing spatial layouts and design hierarchies to reconstruct visual artifact into its underlying structural logic. The ability to convert visual intent into precise, executable code represents practically valuable application of advanced reasoning LMMs. Chart and Table Reasoning. Chart and table reasoning benchmarks evaluate the ability of LMMs to interpret and analyze structured visualizations. These benchmarks assess data literacy through tasks such as chart type classification, data extraction, and question answering on individual charts and tables [Xia et al., 2024b, Xu et al., 2023, Wang et al., 2024d, Wu et al., 2025c]. Tasks also require robust reasoning over OCR-derived text [Huang et al., 2025b]. More advanced evaluations test deeper comprehension through multi-table and multi-chart comparisons, data summarization, and code generation for chart reproduction [Zhu et al., 2025c, Tang et al., 2025, Li et al., 2024d, Shi et al., 2024b]. The core challenge in this setting is not limited to data parsing. It involves understanding the visual relationships embedded in the structure of chart. Meeting this challenge requires more advanced form of Thinking with Images in which model internally transforms the visualization. For example, it may highlight relevant bars or trace trend lines to clarify relationships before performing the logical or computational steps required for analysis. Real-world Applications. Real-world application benchmarks evaluate the ability of LMMs to handle diverse, authentic tasks that reflect practical scenarios from daily-life contexts. Work involves comprehensive, multi-task benchmarks that use authentic materials like examination questions to assess broad competency across wide range of subjects [Zhu et al., 2024, Zhang et al., 2023c, 2025h, Chen et al., 2024b]. In parallel, specialized benchmarks probe performance on specific daily-life skills, such as temporal reasoning with clocks and calendars, or visual numerical reasoning and counting [Weng et al., 2025b]. The core challenge of these benchmarks is to apply reasoning to noisy, unstructured visual data by leveraging broad commonsense and world knowledge. This serves as the ultimate test for the Thinking with Images paradigm, assessing whether the LMMs visual reasoning capabilities can generalize from controlled settings to practical, real-world utility. 26 6.2 Implementation Frameworks In this section, we introduce the frameworks for implementing Thinking with Images methods through prompt-based, SFT-based, and RL-based approaches mentioned above. Prompt-based Methods. Prompt-based methods provide an accessible and lightweight approach for enabling models to Thinking with Images. General frameworks such as LangChain [Chase, 2022] offer extensive tool integration and support for multimodal inputs through content blocks. CrewAI [crewAIInc, 2023] and AutoGPT [Richards and Significant Gravitas, 2023], by contrast, emphasize autonomous agents coordination to address complex reasoning tasks. In retrieval-augmented scenarios, LlamaIndex [Liu, 2022] excels in data indexing with metadata support for multimodal content, while Haystack [deepset, 2023] focuses on production-ready pipeline architectures tailored to multimodal systems. Unlike the previously discussed systems, FlowiseAIs [FlowiseAI, 2023] and AutoChains [Forethought Technologies, 2023] frameworks prioritize ease of use, offering userfriendly tools for quick prototyping and development. However, these frameworks still fall short in enabling LMMs to effectively utilize visual tools in their reasoning processes. To address this gap, recent work has adapted foundational infrastructures to support interactive visual thinking. For instance, MM-REACT integrates with systems like LangChain to orchestrate vision experts, whereas VisProg [Gupta and Kembhavi, 2023] and ViperGPT [Surís et al., 2023] introduce self-contained programmatic frameworks where the generated code itself serves as the reasoning plan. Building upon this code-generation paradigm, Visual Sketchpad [Hu et al., 2024b] and ReFocus [Fu et al., 2025a] utilize modern agentic frameworks such as AutoGen, allowing models to generate or modify images within an interactive reasoning manner. SFT-based Methods. Supervised fine-tuning provides an efficient way for enabling models to initialize and learn Thinking with Images patterns, and several frameworks have emerged to simplify and scale this process. Comprehensive platforms like LLaMAFactory [Zheng et al., 2024b] and Axolotl [Axolotl AI Cloud Contributors, 2023] emphasize flexible SFT workflows and provide unified SFT interfaces for more than 100 LLMs and LMMs. To alleviating computational effciency, Unsloth [UnslothAI, 2023] focuses on accelerating SFT training and reducing memory consumption through custom GPU kernels, achieving up to 60% memory reduction and 30x speedup improvements particularly beneficial for resource-constrained multimodal fine-tuning scenarios. For enhanced fine-grained control, Torchtune [PyTorch Team, 2024] offers precise SFT control through composable building blocks and native PyTorch implementations, enabling custom training loops and specialized data processing pipelines. For large-scale distributed training scenarios, Megatron-LM [Shoeybi et al., 2019] provides robust tensor and pipeline parallelism capabilities specifically designed for massive transformer models, enabling efficient scaling across hundreds of GPUs. Additionally, SWIFT [Zhao et al., 2024] offers scalable and lightweight infrastructure that streamlines the SFT process with minimal computational overhead while maintaining training effectiveness. In contrast to these general training frameworks, NVIDIAs NeMo AutoModel [Verma et al., 2025] provides enterprise-grade SFT capabilities with native optimization for NVIDIA hardware and comprehensive support for various parallelism strategies and precision configurations. Building upon these generalpurpose training frameworks, methods for Thinking with Images introduce specialized adaptations, primarily by curating highly structured SFT data. This data explicitly models multi-step reasoning through sequences of tool invocations, programmatic execution traces, or generative visual thought processes, thereby teaching the model complex cognitive skills beyond simple instruction following. RL-based Methods. Reinforcement learning offers powerful mechanism for enabling LMMs to acquire and internalize Thinking with Images behaviors through interaction with environments. To support this capability at scale, several frameworks have been developed to support scalable and customizable RL workflows on various LMMs. VeRL [ByteDance Seed team and verl community, 2024] achieves scalable RLHF solutions by integrating FSDP and Megatron-LM backends to optimize throughput, and has gained widespread adoption in academic and industry communities. To address high-performance distributed training requirements, OpenRLHF [Hu et al., 2024c] employs raybased architecture augmented with vLLM acceleration and advanced GPU-sharing mechanisms, significantly improving training speed over alternative methods. For efficient scaling across large GPU clusters, ROLL [ROLL Team and Other ROLL Contributors, 2025] utilizes multi-role distributed architectures with ray and integrates megatron-core and vLLM to acclerate model training and inference. In contrast to these training optimization frameworks, TRL [von Werra et al., 2020] 27 integrates seamlessly with the Hugging Face ecosystem to facilitate rapid development. To enhance the flexibility of frameworks, RL4LMs [Ramamurthy et al., 2022] and TRLX [Havrilla et al., 2023] provide modular, customizable components specifically designed for alignment research and novel algorithm development. Despite the flexibility and scalability of these frameworks, they remain limited in supporting LMMs to Thinking with Images through interactive learning. To address this limitation, recent approaches have extended general-purpose RL methods with specialized mechanisms tailored for visual reasoning. For example, OpenThinkIMG [Su et al., 2025a] introduces an end-to-end framework that learns tool orchestration policies, integrating disparate vision tools within unified optimization loop. In contrast, VILASR [Wu et al., 2025a] proposes novel drawing action space that enables direct image manipulation for spatial reasoning tasks. Building upon the VeRL, VisionReasoner [Liu et al., 2025b] implements unified multi-task framework that learns to route queries to specialized perception heads, jointly optimizing both routing and execution policies within single RL iteration. 6.3 Conclusion and Future Frontiers Conclusion. The development of evaluation and implementation frameworks for multimodal reasoning is progressively advancing the concept of Thinking with Images from theoretical paradigm to an empirical research domain. On the evaluation front, emerging benchmarks demonstrate the advantages of LMMs with Thinking with Images capabilities over those employing text-centric reasoning paragdigm. These benchmarks are also evolving from single-turn evaluations to multi-step, multi-task reasoning, mirroring the shift in academia and industry toward multi-round reinforcement learning, tool-augmented thinking, and multi-task reasoning. To enable thinking with images in LMMs, existing implementation frameworks based on prompting, SFT, and RL provide essential support with only minor adjustments. As LMMs with this reasoning paradigm demonstrate significant performance improvements, these general-purpose frameworks will offer more streamlined and customized support. Overall, these foundations of evaluation and implementation transform into assessable, reproducible, and scalable research practices, supporting the development of more robust and capable multimodal reasoning models. Future Frontiers. The established evaluation and implementation frameworks provide solid foundation for this paradigm. To achieve further advancement, several promising directions in evaluation and implementation frameworks exist for developing more powerful LMMs. Benchmarks for Visual Manipulation and Construction. While the number of multimodal reasoning benchmarks is growing, few require the multi-step reasoning and imagination necessary to arrive at final answer. This gap limits the ability to evaluate models truly capable of Thinking with Images. Therefore, future benchmarks could include tasks that require the manipulation and imagination of complex visual elements in the given images. For example, model might be required to draw an auxiliary line on geometry diagram or sketch navigational path through maze. Furthermore, evaluating models capacity for complex, real-world multimodal reasoning is of great significance. Such evaluations would prompt models to generate sequence of intermediate visual states, in guiding the development of world models. Enhancing Framework Efficiency and Scalability. Future work could focus on making implementation frameworks more efficient and scalable. Prompting frameworks will evolve to streamline the integration of visual tools, enabling lightweight commands for generation or editing to be embedded within the reasoning process. In parallel, SFT frameworks should become more efficient at training on diverse datasets, helping models internalize behaviors like tool use and programmatic logic without prohibitive costs. Finally, advancing RL frameworks requires dual focus on rollout and training efficiency: the policy rollout should be supported by robust, low-latency execution engines, while the policy update could leverage more data-efficient supervision signals from intermediate steps like code execution feedback, moving beyond sparse, outcome-based rewards."
        },
        {
            "title": "7 Applications",
            "content": "LMMs capable of Thinking with Images transform diverse range of applications, enabling more in-depth understanding of visual information and more intuitive human-machine interaction. This section explores key domains where these advanced capabilities show significant promise. 28 7.1 Interactive Systems and User Interfaces LMMs are increasingly applied to Graphical User Interfaces (GUIs), where agents must perceive on-screen content and plan actions in human-like manner [Cheng et al., 2024b, Zheng et al., 2024c, Wang et al., 2024e, Qin et al., 2025, Xie et al., 2024c, Wu et al., 2024g, Gou et al., 2024]. Recent work explores how models can Thinking with Images to understand interfaces and perform interaction tasks. Foundation models like CogAgent [Hong et al., 2024] utilize high-resolution vision encoders to recognize fine-grained UI elements directly from pixels. This approach achieves state-of-the-art results on GUI benchmarks and can outperform agents that rely on extracted HTML, demonstrating the power of direct visual perception. Similarly, MobileLMM [Liu et al., 2024b] employs multistage curriculum to master mobile app navigation, maintaining an internal visual state to reason about on-screen affordances and determine subsequent actions. significant trend is the development of unified vision-language-action (VLA) models that operate end-to-end. These agents use only visual observations and closed-loop reasoning to interact with GUIs. For example, ShowUI [Lin et al., 2024] introduces lightweight VLA model that processes screenshots and outputs UI actions through interleaved token modeling, achieving strong zero-shot accuracy. In similar effort, UI-TARS [Qin et al., 2025] functions as pure vision agent, executing actions based solely on screenshots, while its System-2 reasoning module handles task decomposition. Aguvis [Xu et al., 2024] further standardizes GUI interaction across platforms using only pixel inputs, training on multimodal trajectory dataset and using self-generated reasoning steps to guide its decisions. These models exemplify closed-loop visual reasoning by observing the screen, interpreting its structure, and generating executable actions without textual UI representations. primary challenge for GUI agents is obtaining sufficient training data to cover diverse applications and user goals. To address this, several data acquisition strategies have been explored. Explorer [Pahuja et al., 2025] uses automated web exploration to generate large dataset of task trajectories, significantly boosting web automation performance. TongUI [Zhang et al., 2025d] converts online GUI tutorials into multimodal training data via automatic extraction. PC Agent-E [He et al., 2025b] demonstrates that few-shot expert demonstrations, augmented with synthetic variations, can enable agents to achieve strong generalization and outperform large-scale baselines. Takeaway. The application of Thinking with Images to GUIs marks paradigm shift from reliance on structured data like HTML to direct, pixel-based reasoning. This enables agents to operate across diverse platforms such as web, desktop, and mobile environments where programmatic access is unavailable. The core advancement lies in creating closed-loop cognitive process. Models observe the visual state of an interface, interpret its affordances, and generate the next action, often using an internal visual chain-of-thought to plan multi-step tasks. This iterative cycle of perception and action allows for real-time, multi-turn control. While the diversity of interfaces poses significant data challenge, recent work demonstrates promising solutions through automated data generation from web exploration, conversion of online tutorials, and efficient learning from few expert demonstrations. These developments are paving the way for more general and capable UI agents that interact with digital environments as humans do. 7.2 Embodied AI and Robotics To operate effectively in unstructured physical environments, embodied agents such as robots must reason with visual inputs. Recent advances in LMMs are enabling visual thinking for action planning, from sensor-grounded language understanding to internal scene simulation [Driess et al., 2023, Fan et al., 2025b, Zhao et al., 2025a, Zawalski et al., 2025, Zhou et al., 2025c]. Pioneering work like PaLM-E [Driess et al., 2023] demonstrated that integrating language with visual and sensor inputs allows for positive knowledge transfer from web-scale pretraining to real-world robot manipulation. Subsequent models like Interleave-VLA [Fan et al., 2025b] empower robots to process interleaved visual-language prompts and generate executable action sequences in zero-shot settings, improving generalization to unseen objects. Similarly, VIMA [Jiang et al., 2023] advances general-purpose robot control by using multimodal prompts to support open-vocabulary manipulation, enabling robots to interact with novel objects through LMM-generated structured cues. Beyond direct perception-to-action policies, many approaches incorporate visual chain of thought to enhance planning. For example, CoT-VLA [Zhao et al., 2025a] improves temporal reasoning by prompting the model to predict intermediate visual subgoals before generating an action, which achieves substantial performance gains in manipulation tasks. Embodied ECoT [Zawalski et al., 2025] introduces multi-step reasoning framework that explicitly considers plans, object features, and robot state before action prediction, significantly improving policy generalization. Pushing this concept further, VPRL [Xu et al., 2025] conducts planning entirely through sequences of generated images, akin to mental sketches. In spatial navigation tasks, VPRL outperformed language-based methods, suggesting that image-only reasoning may be more effective for certain spatial problems. Another critical direction involves learning visual world models that simulate future states, allowing agents to reason through visual imagination. Genie [Bruce et al., 2024] introduces method to learn an interactive world model directly from internet videos, enabling frame-by-frame world generation in an unsupervised manner. For robotics, 3D-VLA [Zhen et al., 2024] builds 3D goal-conditioned world model using diffusion, generating future goal states to facilitate visualized intent planning. UniSim [Yang et al., 2024] learns from real-world interaction videos to simulate the visual outcomes of actions, enabling experience generation for policy learning. These works aim to develop Minds Eye for embodied agents, enabling them to imagine future outcomes and act on internal simulations. Takeaway. In embodied AI, the Thinking with Images paradigm provides crucial bridge between high-level reasoning and low-level physical action. Models are moving beyond simple perception-to-action mappings and adopting more deliberative processes. One key approach is the use of visual chain of thought, where agents generate intermediate visual subgoals or reasoning steps to guide their planning. This enhances performance and interpretability in complex manipulation tasks. more advanced frontier is the development of visual world models. These models empower agents with imagination, allowing them to simulate future visual states and predict the outcomes of their actions. This internal simulation capability is fundamental for long-horizon planning and robust decision-making in dynamic environments. Furthermore, some research indicates that for spatial tasks like navigation, reasoning with generated images may be more natural and efficient than using textual descriptions, pointing toward future of visual-native robotic intelligence. 7.3 AI for Science LMMs are beginning to support scientific inquiry by interpreting complex visual data from experiments, microscope slides, and satellite imagery [Yan et al., 2025, Zhu et al., 2025b, Tan et al., 2025, Burgess et al., 2025]. However, consistent challenge emerges across these domains. While general-purpose models can describe visual scenes, they often fail to grasp the underlying physical dynamics and domain-specific principles crucial for rigorous scientific reasoning [Schulze Buschoff et al., 2025]. The Thinking with Images paradigm directly addresses this gap by empowering models with deeper, more active form of visual cognition that is grounded in scientific principles. This advanced capability allows models to move beyond simple description to active analysis. For example, in remote sensing, model can think with images by invoking specialized tool to precisely quantify deforestation, functionality explored in systems like GeoChat which supports complex geospatial dialogues [Kuckreja et al., 2023]. In fields like microscopy where experts can easily interpret cluttered scenes that challenge LMMs [Verma et al., 2024], model thinking with images could generate code to segment and highlight specific cell types. This creates new, clarified visual representation that serves as verifiable intermediate step in its analytical process. The most profound application lies in using visual imagination for simulation. In physics, for instance, model can learn to simulate the outcomes of physical events, such as predicting the stability of block tower [Balazadeh et al., 2024]. This transforms the model from passive observer into an active simulator that can conduct virtual experiments. While current models are still developing this expertise, they are laying the foundation for future scientific agents that do not just parse data, but actively reason and experiment with it visually to uncover new insights [Bercovich et al., 2025]. Takeaway. The application of Thinking with Images in science is in its nascent stages, but it already holds significant promise for automating data analysis and hypothesis generation. Current LMMs show foundational capabilities in interpreting diverse scientific imagery, including physics simulations, microscopy slides, and remote sensing data. However, critical finding is that generalworld knowledge is insufficient for specialized scientific domains. The performance of these models improves dramatically with domain-specific adaptation. Training on curated datasets, such as physics 30 simulations or annotated satellite images, allows the models to learn the relevant scientific priors and visual patterns that general models miss. This points out key insight: for AI to become valuable partner in science, it must not only see but also understand the visual world through the lens of specific scientific disciplines. Future progress will likely depend on developing models that can integrate this domain knowledge more deeply, enabling them to perform more complex visual reasoning and even generate novel visual hypotheses. 7.4 Healthcare and Medicine In healthcare, LMMs are evolving from passive image interpreters to active partners in clinical decision-making [Li et al., 2023b, Moor et al., 2023, Chen et al., 2024c, Xia et al., 2024c,d, Seyfioglu et al., 2024, Xia et al., 2025b, Zhu et al., 2025d, Nath et al., 2025, Lai et al., 2025, Lin et al., 2025, Dai et al., 2025a]. The Thinking with Images paradigm is central to this evolution, enabling models to generate dynamic and explainable multimodal medical reasoning rather than static predictions. Early systems demonstrated this by combining vision models with LLMs to produce detailed diagnostic reports for chest X-rays, improving both accuracy and patient communication [Wang et al., 2024f]. key challenge in this high-stakes domain is ensuring reliability. Thinking with Images offers powerful new strategies to enhance trustworthiness. For instance, models can be guided by expert knowledge, such as using anatomical ontologies to direct multi-step visual search across medical scan [Li et al., 2025c, Guo and Terzopoulos, 2025]. This structured approach mitigates hallucination and mimics clinical workflows. For highly specialized fields like digital pathology, models are learning to programmatically construct visual hierarchies of cells and tissues, enabling precise analysis of gigapixel slides [Dai et al., 2025b]. These methods transform the model from black box into transparent reasoner. At the forefront of this domain, applications are beginning to model complex cognitive processes, either by simulating clinical consultations through multi-agent collaboration or by intrinsically generating visual hypotheses. Frameworks like MMedAgent-RL simulate consultation between generalist and specialist AI, using dialogue to refine diagnosis [Xia et al., 2025a]. This represents collaborative form of visual thought. Furthermore, generalist models like HealthGPT are beginning to natively synthesize new visual content, such as generating healthy organ scan for comparison, as part of unified reasoning process [Lin et al., 2025]. Efforts to evaluate these advanced capabilities are also emerging, with benchmarks like MedEBench assessing the fidelity of text-guided clinical image editing [Liu et al., 2025e]. Takeaway. In the medical domain, Thinking with Images is transforming diagnostic AI from predictive tool into collaborative reasoning partner. This paradigm grounds abstract medical analysis in concrete visual evidence, enabling models to generate interpretable reports, follow anatomical search paths, and even simulate clinical consultations. The critical insight from this high-stakes field is that the primary frontier is not just capability, but verifiability and safety. Therefore, the most promising advancements involve constraining and guiding the models visual thought process with expert knowledge and structured workflows. The ultimate objective is to develop AI systems that reason visually with clinical rigor, functioning as dependable assistants whose diagnostic conclusions are not only accurate but also transparent and auditable. 7.5 Education and Training LMMs are transforming education by enabling AI tutors that Thinking with Images, combining visual reasoning with natural language to create interactive learning experiences. representative system is Interactive Sketchpad [Chen et al., 2025d], which assists students in solving STEM problems by pairing step-by-step textual explanations with dynamic visual illustrations. For example, it can guide student through geometry proof by drawing auxiliary lines on diagram while narrating each logical step. This approach, which integrates code-based diagram generation to function as virtual whiteboard, improves student engagement and accuracy. The underlying capability for such systems is driven by benchmarks like MathVista [Lu et al., 2023], which push for advances in visual mathematical reasoning, and specialized models like DiagramGPT [Zala et al., 2024], which focuses on generating open-domain diagrams through two-stage planning and rendering process. Beyond K-12 education, LMMs are advancing professional training through visual feedback systems. VidAAS [Lee et al., 2024] uses GPT-4V to analyze classroom recordings and provide detailed assessments of teaching performance. It interprets non-verbal cues like body language and comments 31 on spatial arrangements to deliver context-aware feedback that helps educators improve. Similar applications are emerging in medical training, where virtual patients could be driven by LMMs that interpret trainees gestures or visual cues, simulating clinical scenarios with realism [Singhal et al., 2023]. These examples demonstrate shift from static instructional content to dynamic, visually-grounded educational agents delivering personalized, interactive learning experiences. Takeaway. In education, the Thinking with Images paradigm enables new AI tutors that do more than provide textual answers; they can visually demonstrate concepts. By generating diagrams, highlighting key features, or sketching solutions in real-time, these systems function as interactive whiteboards, making abstract subjects like mathematics more intuitive. This dynamic visual dialogue adapts to learners needs, offering more engaging and personalized educational experience. The application extends to professional training, where models provide detailed feedback by analyzing visual data from real-world scenarios like classroom videos. This capability for contextual visual analysis allows feedback previously only possible through human observation. These advances signal the emergence of visually-grounded educational agents blending image and language understanding to deliver more effective, interactive, and human-like learning support."
        },
        {
            "title": "8 Future Directions",
            "content": "Although Thinking with Images marks paradigm shift in artificial intelligence, we are still at the early stages of realizing its full potential. Looking forward, this section explores the pivotal research avenues that will propel the field into its next era, focusing on the innovations required to unlock more efficient, secure, and profoundly capable forms of visual cognition. 8.1 Computational and Cognitive Efficiency major challenge for Thinking with Images is its high computational cost [Ning et al., 2024, Zhou et al., 2024c]. Current methods often rely on sequence of visual steps, such as calling an external tool or generating an intermediate image. Each step adds significant delay and requires lot of computation, making these models too slow for real-time applications [Liu et al., 2024b]. Future research should find ways to achieve powerful visual reasoning without costly execution. This could involve teaching models to compress long reasoning process into fewer steps, or to predict the final outcome without needing to render every single intermediate image [Lee et al., 2025]. deeper challenge is improving cognitive efficiency, which is about using the right amount of effort for given task [Qu et al., 2025, Huang et al., 2025c]. truly smart system should not use its most powerful and expensive thinking abilities for every simple problem [Wang et al., 2025n]. The next generation of models must learn to decide when quick textual answer is enough, and when more careful visual analysis is actually needed. This is like human deciding whether to solve math problem in their head or to pull out piece of paper and pencil. Future work could train models with reward systems that penalize unnecessary visual steps, encouraging them to be more resourceful. The goal is to move from models that follow fixed procedure to models that can flexibly switch between fast, simple thinking and slow, deliberate visual reasoning. Achieving this dual efficiency will also require new model architectures [Deng et al., 2025a, Chen et al., 2025b]. Instead of calling slow external tools, future models could have small, built-in modules designed for common visual tasks, like searching for an object or comparing two elements. These specialized modules could work directly on the models internal data representations, or latent space, avoiding the slow process of creating and analyzing full pixels. The objective is to develop models that can think with images while also demonstrating sense of purpose and economy, making advanced visual reasoning scalable and accessible technology. 8.2 Safe and Trustworthy Visual Cognition The ability to Think with Images creates new safety and ethical challenges [Andriushchenko et al., 2024, Qu et al., 2024b]. The most direct danger is the potential for misuse. Because these models can generate series of images that seem to form logical argument, malicious user could guide model to create visual story that supports false claim [Nguyen et al., 2022]. This goes beyond single deepfake. It enables the automated creation of entire disinformation campaigns, where 32 fabricated visual evidence is presented step-by-step to make lie seem true. The challenge, therefore, is not just blocking harmful images, but ensuring the entire thinking process cannot be hijacked to build misleading narrative [Bai et al., 2022]. The security of these models is also major concern due to their complex structure. The points of attack are no longer just the initial inputs, but the entire reasoning loop [Liu et al., 2024c,e,f]. An attacker could poison the external tools model relies on, corrupting its analysis from the start. more subtle attack could involve designing an input that tricks the model into making bad choice, such as focusing on distracting detail or selecting the wrong tool for the job. In this way, the threat moves from simply causing wrong output to corrupting the very process of thinking itself. This requires new security measures to protect models decisions, tools, and internal imagination. Finally, the thinking process of these models is often \"black box\", which makes it hard to trust them. Hidden biases can affect not just the final output, but also the internal reasoning steps [Howard et al., 2024]. For example, model might learn to be more critical or generate stereotypical images only when reasoning about certain groups of people. This could create hidden, self-reinforcing cycle of prejudice that is very difficult to spot. Making these systems trustworthy requires more than just explaining result [Aflalo et al., 2022]. We need new methods to see and review the models internal conversation of tool use and image generation. Without this transparency into how model thinks with images, we cannot reliably find and fix internal biases, which prevents us from building models whose reasoning is not only powerful but also provably fair [Chang et al., 2024]. 8.3 Novel Benchmarks and Evaluation Methodologies key problem for the Thinking with Images paradigm is that current benchmarks are inadequate [Li et al., 2024e]. Most datasets only check the final answer and do not examine the reasoning process. This creates loophole: model can get the right answer for the wrong reasons, for example by using shortcuts instead of genuinely thinking with the image [Chen et al., 2024d]. To address this, future work should focus on two main areas: creating more complex scenarios that require thinking with images, and developing new ways to evaluate the thinking process itself. The first area of future work is to build new benchmarks with tasks that cannot be solved without actively thinking with images. One important direction is to design interactive and multi-step problems. For example, task could require model to solve virtual puzzle, where each action changes the visual scene and reveals the next clue. Another promising direction is to create greater variety of meaningful and challenging tasks that require visual construction. This means the model must generate new visual element to find the solution, such as drawing an auxiliary line on geometry diagram or sketching simple machine to solve physics problem [Wu et al., 2024h]. In these scenarios, simply looking at the image once is not enough; success depends on continuous process of visual interaction and creation. The second area of future work is to develop new methods for evaluating the thinking process, not just the final answer [Song et al., 2025]. Instead of only using accuracy, future metrics must assess the quality of the Thinking with Images chain. For example, we need evaluation methods that can check if each step of models thought is grounded in the visual evidence, penalizing any steps that are made up. We also need to measure the logical coherence and necessity of the process. practical way to test necessity is to remove an intermediate visual step generated by the model and see if it can still arrive at the correct solution. Establishing these process-oriented evaluation frameworks is foundational step toward building more transparent and trustworthy multimodal AI systems. 8.4 Thinking with Audio, Video, and the World While this survey has focused on static images, the paradigms critical extension is its evolution toward dynamic modalities like audio and video, which demand cognition that is not just spatial but temporally coherent. This gives rise to two parallel frontiers: Thinking with Audio and Thinking with Video. In Thinking with Audio, model might reason about the emotional prosody of speech or simulate soundscape to predict an event [Latif et al., 2023]. Likewise, Thinking with Video moves from static scenes to dynamic events, demanding profound shift in cognitive capabilities. It requires not only passive viewing but also the autonomy to actively navigate the flow of time by deciding when to replay, fast-forward, or focus on critical moments to build coherent understanding [Maaz et al., 2023]. This active navigation of the timeline extends beyond observed events, requiring the model to reason about the unseen through internal simulation. It might infer causality by constructing the unobserved events that connect disparate moments, or anticipate futures by generating plausible subsequent frames to predict the outcome of an action [Wu et al., 2024a, Chen et al., 2025e, Tong et al., 2025b, Hafner et al., 2023]. The chain of thought thus evolves from static script into dynamic mental model that unfolds with the narrative, transforming the model from passive processor of frames into an active cognitive observer of temporal world. The ultimate frontier of this research results in Thinking with the world, where the paradigm shifts from detached observation to embodied agency [Brohan et al., 2023]. Here, reasoning becomes an integral component of continuous perception-action loop. The models thoughts directly inform its physical actions, and the sensory consequences provide immediate feedback that grounds its understanding [Fu et al., 2024b]. Its generative power evolves into mechanism for mental rehearsal, allowing the agent to simulate action sequences before committing to physical choice. This process grounds abstract concepts in tangible reality, defining an object not by its appearance but by its physical affordances in planned task [Liu et al., 2024g]. The paradigms most profound shift is making the reasoning process itself manipulable object. Future progress depends on enriching this new language of thought. Achieving this synthesis promises truly autonomous agents that can compose their analytical steps into complex and reliable cognitive programs, allowing them to purposefully navigate and shape their environment. 8.5 Open Questions and Outlook The emergence of Thinking with Images opens profound questions about the nature of artificial cognition and its ultimate architectural form. To effectively guide future research, we first delineate the relationship between this internal cognitive paradigm and the external framework of generalist agents. We then propose blueprint for unified visual thinker, synthesizing the stages discussed in this survey into cohesive, forward-looking vision. 8.5.1 Thinking with Images v.s. Agentic Frameworks The Thinking with Images paradigm defines an internal cognitive process focused on how model reasons, whereas an agent framework provides an external execution cycle focused on how system acts. One is mechanism for thought; the other, an architecture for action. Deliberation v.s. Execution. The core distinction between the Thinking with Images paradigm and an agent framework is their fundamental objectives. The Thinking with Images paradigm is concerned with the fidelity and depth of the reasoning process itself. Its primary goal is to enhance understanding by generating and manipulating visual information to explore problem space, verify hypotheses, and uncover insights that are difficult to articulate through language alone. An agent framework, conversely, is oriented toward external task completion. Its objective is to successfully execute mission within an environment, where success is measured by the final outcome. The former prioritizes the quality of deliberation, while the latter prioritizes the efficacy of execution. Workspace v.s. Perception. The distinct objectives of the Thinking with Images paradigm and an agent framework also lead to fundamentally different role for visual information. For the Thinking with Images paradigm, vision serves as dynamic and manipulable cognitive workspace. It functions as mental sketchpad where ideas can be visually instantiated, modified, and examined as part of an unfolding reasoning sequence. For an agent, vision is primarily the medium of environmental perception. It provides snapshot of the external world state, which must be interpreted to inform the agents next decision. In one context, the image is an internal and dynamic tool for thought; in the other, it is an external stimulus for single action. Artifacts v.s. Instructions. The distinction between the Thinking with Images paradigm and an agent framework further manifests in the nature of their intermediate steps. The intermediate steps in Thinking with Images process are cognitive artifacts. They are internal mental constructs, such as an imagined future scene, an annotated diagram to clarify spatial logic, or digitally enhanced region of an image to reveal fine-grained details. These steps are not meant to act upon the world but to refine the models internal understanding. The intermediate steps for an agent, however, are typically executable instructions. They are commands intended to have direct effect, such as API calls, code execution, or signals sent to physical actuators. 34 Figure 5: conceptual blueprint for Unified Visual Thinker. The architecture features Metacognitive Controller that directs tasks to either an immediate inference pathway or multi-stage Visual Cognitive Workspace, enabling flexible and efficient multimodal reasoning. Engine v.s. Machine. Rather than being mutually exclusive, the Thinking with Images paradigm and an agent framework share interconnected and complementary relationship. The most effective way to conceptualize this is to view the Thinking with Images paradigm as the cognitive engine within the agents machine. An agent provides the essential architecture for worldly interaction, equipped with perception sensors, action effectors, and memory systems. The Thinking with Images paradigm provides the advanced reasoning power that makes the agents actions intelligent rather than merely reactive. This synergy becomes tangible when we consider how the stages outlined in this survey empower an agents capabilities. An agent leverages Stage 1 by learning to orchestrate external visual tools. It evolves by using Stage 2 to programmatically analyze visual data. Ultimately, truly advanced agent embodies Stage 3, using intrinsic imagination to simulate the consequences of potential actions. Therefore, the research presented in this survey is not an alternative to agent-based AI but is instead foundational step toward building autonomous systems that possess deeper, more human-like capacity for visual understanding and intelligent action. 8.5.2 Blueprint for Unified Visual Thinker In the final section, we propose conceptual blueprint for unified visual thinker, as illustrated in Figure 5. This architecture synthesizes the core ideas of our survey into single, cohesive framework, outlining path toward more capable and efficient form of multimodal AI. The central principle is not to create model that always relies on its most complex reasoning, but dynamic system that intelligently selects the right cognitive tool for the task. Metacognitive Controller. The Metacognitive Controller serves as the systems decision-making core. It receives inputs from the external world and its primary role is to assess the tasks complexity. For straightforward problems, it chooses simple, direct path: the model generates an answer based on quick, immediate interpretation of the visual data. This approach avoids unnecessary computational cost for tasks that do not require it. For more complex problems that demand deeper analysis, the controller activates the Visual Cognitive Workspace, signaling the need for more deliberate, visually-grounded thought process. Visual Cognitive Workspace. The Visual Cognitive Workspace is the active environment where this deeper reasoning happens. Guided by the controller, it can dynamically operate across the three stages of cognitive autonomy: leveraging Stage 1 to explore with tools, Stage 2 to create programmatic analyses, or Stage 3 to imagine and simulate outcomes. key feature of this workspace is the feedback loop, which allows the system to review and build upon its own generated thoughts. However, this 35 process is flexible; the system is not required to complete full cycle and can produce final output at any point if it determines satisfactory solution has been reached. Action & Output Interface. The Action & Output interface is the final component, responsible for translating the systems internal thoughts into concrete actions. This is where the models reasoning connects back to the external world. The outputs can take various forms depending on the task: textual answer for question, new visual output like an edited image or diagram, or sequence of executable commands that could guide robotic agent. Unified Vision. This integrated architecture provides the foundation for an agent that embodies the ultimate promise of this survey: an AI that does not merely think about images, but truly thinks with them. The synergy between the Metacognitive Controller and the multi-stage Visual Cognitive Workspace enables crucial cognitive flexibility, allowing the model to dynamically transition between tool-driven exploration, programmatic creation, and deep, internal simulation. This approach moves AI beyond text-centric reasoning, adding dynamic visual component to its language of thought. The key to true visual thinker, then, is not just the ability to imagine, but the metareasoning needed to choose the most effective and efficient path for given task. This blueprint provides clear direction for building more powerful and genuinely multimodal AI systems."
        },
        {
            "title": "9 Conclusion",
            "content": "This survey has charted the evolution of multimodal reasoning towards new paradigm: Thinking with Images. We have systematically organized this emerging field into three-stage framework, detailing the progression from models that leverage external tools, to those that perform programmatic manipulation, and finally to systems also capable of intrinsic imagination. Our analysis reveals fundamental shift in the role of vision from static input to dynamic cognitive workspace. This transformation enables deeper perceptual analysis and more robust physical simulation, laying the groundwork for more intuitive and powerful AI systems. The journey towards true visual cognition is still in its early stages. Significant challenges in efficiency, robustness, and generalization must still be overcome. The path forward requires developing more well-developed world models, exploring abstract visual representations beyond pixels, and building unified frameworks for human-AI collaboration. Ultimately, the research presented here marks foundational step toward creating AI agents that not only see the world, but reason within it as well, equipped with minds eye that is integral to their intelligence."
        },
        {
            "title": "Contributions",
            "content": "The contributions of all authors are listed as follows: Zhaochen Su drafted the abstract, introduction (1), foundations (2), and the introductory sections for each stage (3.1, 4.1-4.2, 5.1-5.2). For the core methods, authorship was divided by approach: Zhenhua Liu drafted the prompting sections (3.3.1, 4.4.1), Peng Xia the SFT sections (3.3.2, 4.4.2, 5.4.1), and Yan Ma the RL sections (3.3.3, 4.4.3, 5.4.2). Additionally, they authored key framing sections for their respective stages: Zhenhua for Stage 1 (3.2, 3.4), Peng for Stage 2 (4.3, 4.5), and Yan for Stage 3 (5.3, 5.5). Moreover, Hangyu Guo drafted 6, and Jiaqi Liu drafted 7. Zhaochen Su and Xiaoye Qu drafted 8. For visual elements, Zhaochen Su created the abstract figure and Figures 2, 3, and 5. Figure 1 was contributed by Peng Xia, Zhenhua Liu, and Kaide Zeng, while Figure 4 was contributed by Hangyu Guo. Yanshu Li provided visual assets and initial drafts. All tables (Tables 1, 2, 3) were collaborative effort by Zhaochen, Zhenhua, Peng, Yan, and Kaide. Junxian He and Yi R. (May) Fung supervised the project, providing invaluable guidance on its overall direction, structure, and refinement. Zhengyuan Yang, Linjie Li, Yu Cheng, and Heng Ji provide insightful feedback and critical suggestions on the manuscript."
        },
        {
            "title": "References",
            "content": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023a. Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, and Yi R. Fung. Mmboundary: Advancing mllm knowledge boundary awareness through reasoning step confidence calibration, 2025a. URL https://arxiv.org/abs/2505.23224. Chuming Shen, Wei Wei, Xiaoye Qu, and Yu Cheng. Satori-r1: Incentivizing multimodal reasoning with spatial grounding and verifiable rewards. arXiv preprint arXiv:2505.19094, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, and Rahul G. Krishnan. Synthetic vision: Training vision-language models to understand physics, December 2024. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, and Ahmed Awadallah. Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. arXiv preprint arXiv:2502.11357, 2025. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025a. URL https://arxiv.org/abs/2504. 20073. 37 Jill Larkin and Herbert Simon. Why diagram is (sometimes) worth ten thousand words. Cognitive Science, 11(1):65100, 1987. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, Dacheng Tao, and Xipeng Qiu. Visuothink: Empowering lvlm reasoning with multimodal tree search. arXiv preprint arXiv:2504.09130, 2025b. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022. Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1198711997, 2023. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023a. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023b. Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488, 2024c. Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual abstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025a. Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. Llava-plus: Learning to use tools for creating multimodal agents. arXiv:2311.05437, 2023. Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, Ranjay Krishna, and Silvio Savarese. Taco: Learning multi-modal action models with synthetic chains-of-thought-and-action, 2024. URL https://arxiv.org/abs/2412.05479. Tianyi Bai, Zengjie Hu, Fupeng Sun, Jiantao Qiu, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, and Wentao Zhang. Multi-step visual reasoning with visual tokens scaling and verification. arXiv preprint arXiv:2506.07235, 2025a. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and Jie Tang. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236, 2024. 38 Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, and Youngjae Yu. Dont look only once: Towards multimodal interactive reasoning with selective visual revisitation. arXiv preprint arXiv:2505.18842, 2025. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models, 2024. Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, and Xianyuan Zhan. Instruction-guided visual masking. arXiv preprint arXiv:2405.19783, 2024a. Guanghao Zhang, Tao Zhong, Yan Xia, Zhelun Yu, Haoyuan Li, Wanggui He, Fangxun Shu, Mushui Liu, Dong She, Yi Wang, et al. Cmmcot: Enhancing complex multi-image comprehension via multi-modal chain-of-thought and memory augmentation. arXiv preprint arXiv:2503.05255, 2025a. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip Yu. Multimodal large language models: survey. In 2023 IEEE International Conference on Big Data (BigData), pages 22472256. IEEE, 2023a. Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, and Matthew Blaschko. Jigsaw-r1: study of rule-based visual reinforcement learning with jigsaw puzzles. arXiv preprint arXiv:2505.23590, 2025c. Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, and Junjie Yan. One rl to see them all: Visual triple unified reinforcement learning. arXiv preprint arXiv:2505.18129, 2025. Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025b. Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images, 2025a. URL https://arxiv.org/abs/2505.15879. Minheng Ni, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Wangmeng Zuo, and Lijuan Wang. Point-rft: Improving multimodal reasoning with visually grounded reinforcement finetuning, 2025. URL https://arxiv.org/abs/2505.19702. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. SegarXiv preprint zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv:2503.06520, 2025c. Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, and Yong Jae Lee. Visualtoolagent (vista): reinforcement learning framework for visual tool selection. arXiv preprint arXiv:2505.20289, 2025a. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025a. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing \"thinking with images\" via reinforcement learning, 2025. URL https://arxiv.org/abs/2505.14362. Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large language models with active perception via grpo. arXiv preprint arXiv:2505.21457, 2025a. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing Li. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl, 2025b. URL https://arxiv.org/abs/2505.15436. 39 Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025b. Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023. Dimitrios Mallis, Ahmet Serdar Karadeniz, Sebastian Cavada, Danila Rukhovich, Niki Foteinopoulou, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada. Cad-assistant: Tool-augmented vllms as generic cad task solvers? arXiv preprint arXiv:2412.13810, 2024. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025a. Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. Sketchagent: Language-driven sequential sketch generation. arXiv preprint arXiv:2411.17673, 2024. Zhehao Zhang, Ryan Rossi, Tong Yu, Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang Chen, Zichao Wang, and Nedim Lipka. Vipact: Visual-perception enhancement via specialized vlm agent collaboration and tool-use. arXiv preprint arXiv:2410.16400, 2024a. Wan-Cyuan Fan, Tanzila Rahman, and Leonid Sigal. Mmfactory: universal solution search engine for vision-language tasks. arXiv preprint arXiv:2412.18072, 2024. Rex Hu et al. Visual program distillation: Distilling tools and programmatic reasoning into visionlanguage models. arXiv preprint arXiv:2403.01299, 2024a. Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, et al. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. arXiv preprint arXiv:2505.10557, 2025d. Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, et al. Provision: Programmatically scaling vision-centric instruction data for multimodal language models. arXiv preprint arXiv:2412.07012, 2024b. Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, et al. Scaling text-rich image understanding via code-guided synthetic multimodal data generation. arXiv preprint arXiv:2502.14846, 2025. Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246, 2025d. Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019, 2025e. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024a. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025b. Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:2148721506, 2023. 40 Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought, 2025a. URL https://arxiv.org/abs/2501.07542. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. arXiv preprint arXiv:2503.22020, 2025a. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025a. Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images, 2025. URL https://arxiv.org/abs/2505.11409. Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022, 2025. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and PhengAnn Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025a. Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 95569567. IEEE, 2024a. doi: 10.1109/CVPR52733. 2024.00913. URL https://doi.org/10.1109/CVPR52733.2024.00913. Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 33133323. Association for Computational Linguistics, 2022a. doi: 10.18653/V1/2022.EMNLP-MAIN.218. URL https://doi.org/10.18653/v1/2022. emnlp-main.218. Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. multi-modal neural geometric solver with textual clauses parsed from diagram. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 33743382. ijcai.org, 2023b. doi: 10.24963/IJCAI.2023/376. URL https://doi.org/10. 24963/ijcai.2023/376. Daocheng Fu, Zijun Chen, Renqiu Xia, Qi Liu, Yuan Feng, Hongbin Zhou, Renrui Zhang, Shiyang Feng, Peng Gao, Junchi Yan, et al. Trustgeogen: Scalable and formal-verified data engine for trustworthy multi-modal geometric problem solving. arXiv preprint arXiv:2504.15780, 2025b. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. CoRR, abs/2312.12241, 2023. doi: 10.48550/ARXIV.2312.12241. URL https://doi.org/10.48550/arXiv.2312.12241. Can Li, Ting Zhang, Mei Wang, and Hua Huang. Visiomath: Benchmarking figure-based mathematical reasoning in lmms. arXiv preprint arXiv:2506.06727, 2025b. Peijie Wang, Zhongzhi Li, Fei Yin, Dekang Ran, and Chenglin Liu. MV-MATH: evaluating multimodal math reasoning in multi-visual contexts. CoRR, abs/2502.20808, 2025f. doi: 10.48550/ARXIV.2502.20808. URL https://doi.org/10.48550/arXiv.2502.20808. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=KUNzEQMWU7. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/11332b6b6cf4485b84afadb1352d3a9a-Abstract-Conference.html. Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. arXiv preprint arXiv:2410.10139, 2024a. Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Puzzlebench: fully dynamic evaluation framework for large multimodal models on puzzle solving. CoRR, abs/2504.10885, 2025c. doi: 10.48550/ ARXIV.2504.10885. URL https://doi.org/10.48550/arXiv.2504.10885. Harrison Chase. LangChain, 10 2022. URL https://github.com/langchain-ai/langchain. An open-source framework for developing applications powered by language models. crewAIInc. CrewAI: Framework for orchestrating role-playing, autonomous AI agents, 2023. URL https://github.com/crewAIInc/crewAI. Python framework for orchestrating role-playing, autonomous AI agents, focusing on collaborative intelligence. Initial development by João Moura. ROLL Team and Other ROLL Contributors. Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library, 2025. URL https://arxiv.org/ abs/2506.06122. Citation derived from the ROLL GitHub repository. Anticipated publication. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Yixin Cao, Yang Feng, and Deyi Xiong, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.38. URL https://aclanthology. org/2024.acl-demos.38. Axolotl AI Cloud Contributors. Axolotl: Streamlined AI Model Post-Training, 2023. URL https: //github.com/axolotl-ai-cloud/axolotl. tool to streamline post-training for various AI models, emphasizing flexible Supervised Fine-Tuning (SFT) workflows and providing unified SFT interfaces. Documentation at https://axolotl-ai-cloud.github.io/axolotl/. deepset. Haystack: The Production-Ready Open Source AI Framework, 2023. URL https: //haystack.deepset.ai/. An open-source AI framework by deepset for building productionready applications with LLMs, including multimodal systems and RAG. 42 Toran Bruce Richards and Significant Gravitas. AutoGPT: An Autonomous GPT-4 Experiment, 3 2023. URL https://github.com/Significant-Gravitas/AutoGPT. An experimental open-source application designed to make GPT-4 fully autonomous. ByteDance Seed team and verl community. verl: Volcano Engine Reinforcement Learning for LLMs, 2024. URL https://github.com/Intelligent-Internet/ii_verl. Open-source version of HybridFlow. flexible, efficient and production-ready RL training library for large language models. UnslothAI. Unsloth: Finetune LLMs 2x faster with up to 70% less memory, 2023. URL https://github.com/unslothai/unsloth. library focusing on accelerating Supervised Fine-Tuning (SFT) training and reducing memory consumption through custom GPU kernels written in OpenAIs Triton and manual backpropagation engine. It aims for significant memory reduction (e.g., up to 60-70%) and speedup improvements (e.g., up to 30x with Pro versions), particularly beneficial for resource-constrained multimodal fine-tuning scenarios. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents, December 2024. Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing In Forty-first International sub-billion parameter language models for on-device use cases. Conference on Machine Learning, 2024b. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, SongChun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025d. Yanheng He, Jiahe Jin, and Pengfei Liu. Efficient agent training for computer use. arXiv preprint arXiv:2505.13909, 2025b. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, March 2023. Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, and Mingyu Ding. Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions, May 2025b. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts, May 2023. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning, March 2025. 43 Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, and Tim Rocktäschel. Genie: Generative interactive environments, February 2024. Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model, March 2024. Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing, November 2023. Erle Zhu, Yadi Liu, Zhe Zhang, Xujun Li, Jin Zhou, Xinjie Yu, Minlie Huang, and Hongning Wang. Maps: Advancing multi-modal reasoning in expert-level physical science. arXiv preprint arXiv:2501.10768, 2025b. Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, and Tianfan Fu. Chemmllm: Chemical multimodal large language model. arXiv preprint arXiv:2505.16326, 2025. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. Qingqiu Li, Zihang Cui, Seongsu Bae, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Junjun He, et al. Aor: Anatomical ontology-guided reasoning for medical large multimodal model in chest x-ray interpretation. arXiv preprint arXiv:2505.02830, 2025c. Wei Dai, Peilin Chen, Chanakya Ekbote, and Paul Pu Liang. Qoq-med: Building multimodal clinical foundation models with domain-aware grpo training. arXiv preprint arXiv:2506.00711, 2025a. Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, et al. Mmedagent-rl: Optimizing multi-agent collaboration for multimodal medical reasoning. arXiv preprint arXiv:2506.00555, 2025a. Steven-Shine Chen, Jimin Lee, and Paul Pu Liang. Interactive sketchpad: multimodal tutoring system for collaborative, visual problem-solving. arXiv preprint arXiv:2503.16434, 2025d. Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal. Diagrammergpt: Generating open-domain, open-platform diagrams via llm planning, July 2024. Unggi Lee, Yeil Jeong, Junbo Koh, Gyuri Byun, Yunseo Lee, Hyunwoong Lee, Seunmin Eun, Jewoong Moon, Cheolil Lim, and Hyeoncheol Kim. see you: Teacher analytics with gpt-4 vision-powered observational assessment. Smart Learning Environments, 11(1):48, October 2024. ISSN 2196-7091. doi: 10.1186/s40561-024-00335-4. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), November 2024. ISSN 2053-714X. doi: 10.1093/nsr/nwae403. URL http://dx.doi.org/10.1093/nsr/nwae403. Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024c. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116, 2024a. Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, and Xuming Hu. survey of mathematical reasoning in the era of multimodal large language model: Benchmark, method & challenges. arXiv preprint arXiv:2412.11936, 2024. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024c. 44 Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, and Guangyao Shi. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189, 1, 2025d. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025g. Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921, 2025e. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, 2025. URL https://arxiv.org/abs/2503.16419. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng. Conflictbank: benchmark for evaluating the influence of knowledge conflicts in llm. arXiv preprint arXiv:2408.12076, 2024. Yifan Jia, Kailin Jiang, Yuyang Liang, Qihan Ren, Yi Xin, Rui Yang, Fenze Feng, Mingcai Chen, Hengyang Lu, Haozhe Wang, et al. Benchmarking multimodal knowledge conflict for large multimodal models. arXiv preprint arXiv:2505.19509, 2025. Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. Surf: Teaching large vision-language models to selectively utilize retrieved information. arXiv preprint arXiv:2409.14083, 2024b. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023b. Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024d. Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo: training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90989108, 2025f. Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025e. Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. From the least to the most: Building plug-andplay visual reasoner via data synthesis. arXiv preprint arXiv:2406.19934, 2024a. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025h. 45 Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025c. Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, and Xinchao Wang. Pixelthink: Towards efficient chain-of-pixel reasoning. arXiv preprint arXiv:2505.23727, 2025i. Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang. Vlm-r3: Region recognition, reasoning, and refinement for enhanced multimodal chain-of-thought. arXiv preprint arXiv:2505.16192, 2025b. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025a. Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, and Yi R. Fung. Vlm2-bench: closer look at how well vlms implicitly link explicit matching visual cues, 2025f. URL https://arxiv.org/ abs/2502.12084. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024b. Tong Ge, Yashu Liu, Jieping Ye, Tianyi Li, and Chao Wang. Advancing vision-language models in front-end development via data synthesis. arXiv preprint arXiv:2503.01619, 2025. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 1076410799. PMLR, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Gary Bradski. The opencv library. Dr. Dobbs Journal: Software Tools for the Professional Programmer, 25(11):120123, 2000. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024a. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024b. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024d. 46 Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024a. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Guo, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024a. Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding, 2024a. URL https://arxiv. org/abs/2501.00289. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Teng Li, Quanfeng Lu, Lirui Zhao, Hao Li, Xizhou Zhu, Yu Qiao, Jun Zhang, and Wenqi Shao. Unifork: Exploring modality alignment for unified multimodal understanding and generation. arXiv preprint arXiv:2506.17202, 2025g. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025b. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024e. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Feng Han, Yang Jiao, Shaoxiang Chen, Junhao Xu, Jingjing Chen, and Yu-Gang Jiang. Controlthinker: Unveiling latent semantics for controllable image generation through visual reasoning. arXiv preprint arXiv:2506.03596, 2025. Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, et al. Llms meet multimodal generation and editing: survey. arXiv preprint arXiv:2405.19334, 2024a. Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, et al. Next token prediction towards multimodal intelligence: comprehensive survey. arXiv preprint arXiv:2412.18619, 2024a. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9): 1085010869, 2023. Zihan Weng, Lucas Gomez, Taylor Whittington Webb, and Pouya Bashivan. Caption this, reason that: Vlms caught in the middle. arXiv preprint arXiv:2505.21538, 2025a. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024b. 47 Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James dual parallel text encoding. Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 15111520. International Committee on Computational Linguistics, 2022. URL https://aclanthology.org/2022.coling-1.130. Jiaxin Zhang, Zhongzhi Li, Ming-Liang Zhang, Fei Yin, Cheng-Lin Liu, and Yashar Moshfeghi. Geoeval: Benchmark for evaluating llms and multi-modal models on geometry problem-solving. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 12581276. Association for Computational Linguistics, 2024d. doi: 10.18653/V1/ 2024.FINDINGS-ACL.73. URL https://doi.org/10.18653/v1/2024.findings-acl.73. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juan-Zi Li. MM-MATH: advancing multimodal math evaluation with process evaluation and fine-grained classification. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 13581375. Association for Computational Linguistics, 2024c. doi: 10.18653/V1/2024.FINDINGS-EMNLP.73. URL https://doi.org/10.18653/v1/2024.findings-emnlp.73. Yanpeng Sun, Shan Zhang, Wei Tang, Aotian Chen, Piotr Koniusz, Kai Zou, Yuan Xue, and Anton van den Hengel. MATHGLANCE: multimodal large language models do not know where to look in mathematical diagrams. CoRR, abs/2503.20745, 2025. doi: 10.48550/ARXIV.2503.20745. URL https://doi.org/10.48550/arXiv.2503.20745. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Gemini Team, Google. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. Google DeepMind Technical Report, jun 2025. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_ report.pdf. Technical Report. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024c. URL http://papers.nips.cc/paper_files/ paper/2024/hash/ad0edc7d5fa1a783f063646968b7315b-Abstract-Datasets_and_ Benchmarks_Track.html. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedscientific problems. ings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 38283850. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211. Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, and Chitta Baral. Polymath: challenging multi-modal mathematical reasoning benchmark. CoRR, abs/2410.14702, 2024. doi: 10.48550/ARXIV.2410.14702. URL https://doi.org/10.48550/arXiv.2410.14702. 48 Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, and Hongsheng Li. MATHVERSE: does your multi-modal LLM truly see the diagrams in visual math problems? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, volume 15066 of Lecture Notes in Computer Science, pages 169186. Springer, 2024e. doi: 10.1007/978-3-031-73242-3_10. URL https://doi.org/10.1007/978-3-031-73242-3_ 10. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=VOAMTA8jKu. Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, and Rui Zhang. Visonlyqa: Large vision language models still struggle with visual perception of geometric information. CoRR, abs/2412.00947, 2024. doi: 10.48550/ARXIV.2412.00947. URL https: //doi.org/10.48550/arXiv.2412.00947. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal LLM logical reasoning benchmark in visual contexts. CoRR, abs/2407.04973, 2024. doi: 10.48550/ARXIV.2407.04973. URL https://doi.org/10.48550/arXiv.2407.04973. Jaewoo Park, Jungyang Park, Dongju Jang, Jiwan Chung, Byungwoo Yoo, Jaewoo Shin, Seonjoon Park, Taehyeong Kim, and Youngjae Yu. Explain with visual keypoints like real mentor! benchmark for multimodal solution explanation. arXiv preprint arXiv:2504.03197, 2025. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? EMMA: an enhanced multimodal reasoning benchmark. CoRR, abs/2501.05444, 2025. doi: 10.48550/ARXIV.2501.05444. URL https: //doi.org/10.48550/arXiv.2501.05444. Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, et al. Mdk12-bench: multi-discipline benchmark for evaluating reasoning in multimodal large language models. arXiv preprint arXiv:2504.05782, 2025a. Xinwu Ye, Chengfan Li, Siming Chen, Wei Wei, and Xiangru Tang. Mmscibench: Benchmarking language models on chinese multimodal scientific problems. arXiv preprint arXiv:2503.01891, 2025. Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, and Xinzhu Ma. Physunibench: An undergraduate-level physics reasoning benchmark for multimodal models, 2025j. URL https://arxiv.org/abs/2506.17667. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. CoRR, abs/2409.02813, 2024b. doi: 10.48550/ARXIV.2409.02813. URL https://doi.org/10.48550/arXiv.2409.02813. Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, and Lei Bai. Scientists first exam: Probing cognitive abilities of mllm via perception, understanding, and reasoning, 2025b. URL https: //arxiv.org/abs/2506.10521. Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, and Ge Zhang. Ing-vp: Mllms cannot play easy vision-based games yet. arXiv preprint arXiv:2410.06555, 2024f. 49 Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Süsstrunk, and Filippos Kokkinos. Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models. CoRR, abs/2503.23064, 2025. doi: 10.48550/ARXIV.2503.23064. URL https://doi. org/10.48550/arXiv.2503.23064. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. VSP: assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. CoRR, abs/2407.01863, 2024f. doi: 10.48550/ARXIV.2407.01863. URL https://doi.org/10.48550/arXiv.2407.01863. Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025k. URL https://openreview.net/forum?id= c4OGMNyzPT. Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025a. Davide Paglieri, Bartlomiej Cupial, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Lukasz Kucinski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, and Tim Rocktäschel. BALROG: benchmarking agentic LLM and VLM reasoning on games. CoRR, abs/2411.13543, 2024. doi: 10.48550/ARXIV.2411.13543. URL https://doi.org/10.48550/arXiv.2411.13543. Alex Zhang, Thomas Griffiths, Karthik Narasimhan, and Ofir Press. Videogamebench: Can vision-language models complete popular video games? arXiv preprint arXiv:2505.18134, 2025g. Meng Cao, Haoran Tang, Haoze Zhao, Hangyu Guo, Jiaheng Liu, Ge Zhang, Ruyang Liu, Qiang Sun, Ian Reid, and Xiaodan Liang. Physgame: Uncovering physical commonsense violations in gameplay videos. CoRR, abs/2412.01800, 2024. doi: 10.48550/ARXIV.2412.01800. URL https://doi.org/10.48550/arXiv.2412.01800. Chenglin Li, Qianglong Chen, Zhi Li, Feng Tao, and Yin Zhang. Vcbench: controllable benchmark for symbolic and abstract challenges in video cognition. CoRR, abs/2411.09105, 2024b. doi: 10.48550/ARXIV.2411.09105. URL https://doi.org/10.48550/arXiv.2411.09105. Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, and Dan Hendrycks. Enigmaeval: benchmark of long multimodal reasoning challenges. CoRR, abs/2502.08859, 2025l. doi: 10.48550/ARXIV.2502. 08859. URL https://doi.org/10.48550/arXiv.2502.08859. Hengzhi Li, Brendon Jiang, Alexander Naehu, Regan Song, Justin Zhang, Megan Tjandrasuwita, Chanakya Ekbote, Steven-Shine Chen, Adithya Balachandran, Wei Dai, et al. Puzzleworld: benchmark for multimodal, open-ended reasoning in puzzlehunts. arXiv preprint arXiv:2506.06211, 2025h. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024a. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception In Proceedings of the AAAI Conference on Artificial in multimodal large language models. Intelligence, volume 39, pages 79077915, 2025m. Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, et al. Verify: benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity. arXiv preprint arXiv:2503.11557, 2025. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 50 292305, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.20. URL https://aclanthology.org/2023.emnlp-main.20/. Xiujie Song, Mengyue Wu, Kenny Zhu, Chunhao Zhang, and Yanyi Chen. cognitive evaluation benchmark of image reasoning and description for large vision-language models. arXiv preprint arXiv:2402.18409, 2024. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing, 2025b. URL https:// arxiv.org/abs/2504.02826. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models, 2023. URL https://arxiv.org/abs/2311.17982. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma. Mmcode: Evaluating multi-modal code large language models with visually rich programming problems. CoRR, abs/2404.09486, 2024c. doi: 10.48550/ARXIV.2404.09486. URL https://doi.org/10.48550/arXiv.2404. 09486. Chengyue Wu, Zhixuan Liang, Yixiao Ge, Qiushan Guo, Zeyu Lu, Jiahao Wang, Ying Shan, and Ping Luo. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 30063028. Association for Computational Linguistics, 2025b. URL https://aclanthology.org/2025.findings-naacl.164/. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 39563974. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.naacl-long.199/. Ryan Li, Yanzhe Zhang, and Diyi Yang. Sketch2code: Evaluating vision-language models for interactive web design prototyping. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 39213955. Association for Computational Linguistics, 2025i. URL https://aclanthology.org/2025.naacl-long.198/. Hugo Laurençon, Léo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into HTML code with the websight dataset. CoRR, abs/2403.09029, 2024. doi: 10.48550/ARXIV.2403. 09029. URL https://doi.org/10.48550/arXiv.2403.09029. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, and Yu Qiao. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. CoRR, abs/2402.12185, 2024b. doi: 10.48550/ARXIV. 2402.12185. URL https://doi.org/10.48550/arXiv.2402.12185. Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts. CoRR, abs/2312.15915, 2023. doi: 10.48550/ ARXIV.2312.15915. URL https://doi.org/10.48550/arXiv.2312.15915. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural 51 Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024d. URL http://papers.nips.cc/paper_files/paper/2024/hash/ cdf6f8e9fd9aeaf79b6024caec24f15b-Abstract-Datasets_and_Benchmarks_Track. html. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xeron Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Tongliang Li, Zhoujun Li, and Guanglin Niu. Tablebench: comprehensive and complex benchmark for table question answering. In Toby Walsh, Julie Shah, and Zico Kolter, editors, AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2549725506. AAAI Press, 2025c. doi: 10.1609/AAAI.V39I24.34739. URL https://doi.org/10.1609/aaai. v39i24.34739. Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, and Lianwen Jin. Ocrreasoning benchmark: Unveiling the true capabilities of mllms in complex text-rich image reasoning. arXiv preprint arXiv:2505.17163, 2025b. Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, and Meng Jiang. Multichartqa: Benchmarking vision-language models on multi-chart problems. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 1134111359. Association for Computational Linguistics, 2025c. URL https://aclanthology.org/2025. naacl-long.566/. Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, et al. Chartmuseum: Testing visual reasoning capabilities of large vision-language models. arXiv preprint arXiv:2505.13444, 2025. Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, and Arman Cohan. M3sciqa: multi-modal multi-document scientific QA benchmark for evaluating foundation models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1541915446. Association for Computational Linguistics, 2024d. URL https://aclanthology. org/2024.findings-emnlp.904. Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, and Yujiu Yang. Chartmimic: Evaluating lmms cross-modal reasoning capability via chart-to-code generation. CoRR, abs/2406.09961, 2024b. doi: 10.48550/ARXIV.2406.09961. URL https://doi.org/10.48550/arXiv.2406. 09961. Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao, Liangtai Sun, and Kai Yu. MULTI: multimodal understanding leaderboard with text and images. CoRR, abs/2402.03173, 2024. doi: 10.48550/ ARXIV.2402.03173. URL https://doi.org/10.48550/arXiv.2402.03173. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023c. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 117c5c8622b0d539f74f6d1fb082a2e9-Abstract-Datasets_and_Benchmarks.html. YiFan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, and Rong Jin. MME-realworld: Could your multimodal LLM challenge high-resolution real-world scenarios that are difficult for humans? In The Thirteenth International Conference on Learning Representations, 2025h. URL https://openreview.net/forum?id=k5VHHgsRbi. 52 Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, and Wenhu Chen. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. CoRR, abs/2410.10563, 2024b. doi: 10.48550/ARXIV.2410.10563. URL https://doi.org/10. 48550/arXiv.2410.10563. Tengjin Weng, Jingyi Wang, Wenhao Jiang, and Zhong Ming. Visnumbench: Evaluating number sense of multimodal large language models. CoRR, abs/2503.14939, 2025b. doi: 10.48550/ARXIV. 2503.14939. URL https://doi.org/10.48550/arXiv.2503.14939. Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/run-llama/llama_index. data framework for building LLM applications by connecting LLMs to external data sources. FlowiseAI. FlowiseAI: Build AI Agents Visually, 2023. URL https://flowiseai.com/. An open-source visual development platform for building LLM applications and agentic systems with no-code interface. Forethought Technologies. AutoChain: Build lightweight, extensible, and testable LLM Agents, 2023. URL https://github.com/Forethought-Technologies/AutoChain. lightweight and extensible Python framework for building LLM agents, inspired by LangChain and AutoGPT, with focus on OpenAI function calling and automated evaluation. PyTorch Team. Torchtune: PyTorch library for LLM post-training, 2024. URL https:// github.com/pytorch/torchtune. PyTorch native library offering precise Supervised FineTuning (SFT) control through composable and modular building blocks and native PyTorch implementations. It enables custom training loops, specialized data processing pipelines, and supports various fine-tuning techniques like LoRA and QLoRA. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. Shashank Verma, Alexandros Koumparoulis, Wenwen Gao, and Bernard Nguyen. Run Hugging Face models instantly with Day-0 support from NVIDIA NeMo Framework. NVIDIA Developer Blog, May 2025. Describes the AutoModel feature within the NVIDIA NeMo Framework, providing enterprise-grade Supervised Fine-Tuning (SFT) capabilities with native optimization for NVIDIA hardware and comprehensive support for various parallelism strategies (e.g., Tensor Parallelism, Pipeline Parallelism, Data Parallelism, Expert Parallelism) and precision configurations. Introduced with NeMo framework 25.02 release. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, Yu Cao, and OpenRLHF Team. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework. arXiv preprint arXiv:2405.11143, 2024c. URL https://arxiv.org/abs/2405.11143. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, Quentin Gallouédec, Thomas Wolf, Philipp Schmid, Louis Debut, and Victor Sanh. TRL: Transformer Reinforcement Learning. https://github.com/ huggingface/trl, 2020. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, and Louis Castricato. trlX: framework for large scale reinforcement learning from human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 85788595, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.530. URL https://aclanthology.org/2023.emnlp-main.530. 53 Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93139332, 2024b. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. In International Conference on Machine Learning, pages 6134961385. PMLR, 2024c. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024e. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024c. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024g. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, et al. Anyprefer: An agentic framework for preference data synthesis. arXiv preprint arXiv:2504.19276, 2025c. Sherry Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators, September 2024. Yibo Yan, Shen Wang, Jiahao Huo, Jingheng Ye, Zhendong Chu, Xuming Hu, Philip Yu, Carla Gomes, Bart Selman, and Qingsong Wen. Position: Multimodal large language models can significantly advance scientific reasoning. arXiv preprint arXiv:2502.02871, 2025. James Burgess, Jeffrey Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, et al. In Microvqa: multimodal reasoning benchmark for microscopy-based scientific research. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1955219564, 2025. Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, 7(1):96106, January 2025. ISSN 2522-5839. doi: 10.1038/s42256-024-00963-y. Prateek Verma, Minh-Hao Van, and Xintao Wu. Beyond human vision: The role of large vision language models in microscope image analysis, May 2024. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949, 2025. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. 54 Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Chen, Xidong Wang, Zhenyang Cai, Ke Ji, Xiang Wan, et al. Towards injecting medical visual knowledge into multimodal llms at scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 73467370, 2024c. Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. Advances in Neural Information Processing Systems, 37:140334140365, 2024c. Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10811093, 2024d. Mehmet Saygin Seyfioglu, Wisdom Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1318313192, 2024. Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. In The Thirteen International Conference on Learning Representations, 2025b. Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, and Huaxiu Yao. Mmedpo: Aligning medical vision-language models with clinical-aware multimodal preference optimization. FortySecond International Conference on Machine Learning, 2025d. Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, et al. Vila-m3: Enhancing vision-language models with medical expert knowledge. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1478814798, 2025. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. Sheng Wang, Zihao Zhao, Xi Ouyang, Tianming Liu, Qian Wang, and Dinggang Shen. Interactive computer-aided diagnosis on medical image using large language models. Communications Engineering, 3(1):19, September 2024f. ISSN 2731-3395. doi: 10.1038/s44172-024-00271-8. Danfeng Guo and Demetri Terzopoulos. Prompting medical large vision-language models to diagnose pathologies by visual question answering. Machine Learning for Biomedical Imaging, 3(March 2024):5971, March 2025. ISSN 2766-905X. doi: 10.59275/j.melba.2025-1a8b. Dawei Dai, Yuanhui Zhang, Qianlan Yang, Long Xu, Xiaojing Shen, Shuyin Xia, and Guoyin Wang. Pathologyvlm: large vision-language model for pathology image understanding. Artificial Intelligence Review, 58(6):186, March 2025b. ISSN 1573-7462. doi: 10.1007/s10462-025-11190-1. Minghao Liu, Zhitao He, Zhiyuan Fan, Qingyun Wang, and Yi R. Fung. Medebench: Revisiting text-instructed image editing on medical domain, 2025e. URL https://arxiv.org/abs/2506. 01921. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Agüera Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620(7972):172180, August 2023. ISSN 1476-4687. doi: 10.1038/s41586-023-06291-2. Zhenyu Ning, Jieru Zhao, Qihao Jin, Wenchao Ding, and Minyi Guo. Inf-mllm: Efficient streaming inference of multimodal large language models on single gpu. arXiv preprint arXiv:2409.09086, 2024. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024c. Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi R. Fung. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting, 2025c. URL https://arxiv.org/abs/2505.18822. Jiaqi Wang, Kevin Qinghong Lin, James Cheng, and Mike Zheng Shou. Think or not? selective reasoning via reinforcement learning for vision-language models. arXiv preprint arXiv:2505.16854, 2025n. Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safetyaligned llms with simple adaptive attacks. arXiv preprint arXiv:2404.02151, 2024. Xiaoye Qu, Qiyuan Chen, Wei Wei, Jiashuo Sun, Daizong Liu, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. ACM Transactions on Multimedia Computing, Communications and Applications, 2024b. Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh Nguyen, Thien HuynhThe, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet Pham, and Cuong Nguyen. Deep learning for deepfakes creation and detection: survey. Computer Vision and Image Understanding, 223: 103525, 2022. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Yu Cheng, and Wei Hu. survey of attacks on large vision-language models: Resources, advances, and future trends. arXiv preprint arXiv:2407.07403, 2024e. Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Xiang Fang, Keke Tang, Yao Wan, and Lichao Sun. Pandoras box: Towards building universal attackers against real-world large vision-language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024f. Phillip Howard, Kathleen Fraser, Anahita Bhiwandiwalla, and Svetlana Kiritchenko. Uncovering bias in large vision-language models at scale with counterfactuals. arXiv preprint arXiv:2405.20152, 2024. Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, Chenfei Wu, Nan Duan, and Vasudev Lal. Vl-interpret: An interactive visualization tool for interpreting vision-language transformers. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 2140621415, 2022. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, and Long Chen. survey on multimodal benchmarks: In the era of large ai models. arXiv preprint arXiv:2409.18142, 2024e. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024d. 56 Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024h. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayáhuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, et al. Sparks of large audio models: survey and outlook. arXiv preprint arXiv:2308.12792, 2023. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. Yuefei Chen, Vivek Singh, Jing Ma, and Ruxiang Tang. Counterbench: benchmark for counterfactuals reasoning in large language models. arXiv preprint arXiv:2502.11008, 2025e. Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, et al. Mj-video: Fine-grained benchmarking and rewarding video preferences in video generation. arXiv preprint arXiv:2502.01719, 2025b. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Zipeng Fu, Tony Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. arXiv preprint arXiv:2401.02117, 2024b. Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Ok-robot: What really matters in integrating open-knowledge models for robotics. arXiv preprint arXiv:2401.12202, 2024g."
        }
    ],
    "affiliations": [
        "Microsoft",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology",
        "UIUC",
        "UNC-Chapel Hill"
    ]
}