{
    "paper_title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment",
    "authors": [
        "Zhiting Gao",
        "Dan Song",
        "Diqiong Jiang",
        "Chao Xue",
        "An-An Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released."
        },
        {
            "title": "Start",
            "content": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment Zhiting Gao1 Dan Song1* Diqiong Jiang2 Chao Xue3 An-An Liu1* 1Tianjin University, China 2China University of Petroleum, China 3Tiandy Technologies, China dan.song@tju.edu.cn, anan0422@gmail.com (Corresponding Authors) 5 2 0 2 7 2 ] . [ 1 7 2 5 9 1 . 8 0 5 2 : r Figure 1: We propose MotionFlux, rectified flow matching-based motion generation framework that employs preference optimization for semantic alignment. In our visualization, darker colors denote later times, and red text highlights key events. Abstract Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released."
        },
        {
            "title": "Introduction",
            "content": "Natural language-driven motion generation presents significant challenges due to the inherent modality gap between linguistic descriptions and kinematic motions.To address these challenges, existing approaches typically learn precise mappings from language space to motion space through various latent representation learning frameworks. Early attempts employ Auto-Encoders (Ahuja and Morency 2019; Ghosh et al. 2021; Tevet et al. 2022a) and Variational Auto-Encoders (VAEs) (Petrovich, Black, and Varol 2021, 2022) to establish joint embeddings between text and motion. Building on these foundations, notable works include MotionClip (Tevet et al. 2022a) aligning motion space with CLIP (Radford et al. 2021) embeddings, while ACTOR (Petrovich, Black, and Varol 2021) and TEMOES (Plappert, Mandery, and Asfour 2016) propose transformerbased VAEs for action-to-motion and text-to-motion tasks respectively. While these methods demonstrate competence with simple descriptions, their performance degrades significantly when handling complex, lengthy textual prompts. Recent efforts by Guo et al. (Guo et al. 2022a) and TM2T (Guo et al. 2022b) attempt to address intricate text descriptions through multi-stage generation pipelines. However, these approaches suffer from non-intuitive architectures requiring three-phase processing pipelines, often failing to produce high-quality motions that strictly adhere to textual specifications. The emergence of diffusion models (Ho, Jain, and Abbeel 2020), particularly their successful adaptation to motion generation in MDM (Tevet et al. 2022b) and MotionDiffuse (Zhang et al. 2024), has established diffusion-based methods as state-of-the-art for text-to-motion tasks due to their superior distribution modeling capabilities. Nevertheless, these methods require extensive sampling steps (e.g., 24s for MDM (Tevet et al. 2022b) and 14s for MotionDiffuse (Zhang et al. 2024) per sequence) even with acceleration techniques (Song, Meng, and Ermon 2020), severely limiting their applicability in real-time scenarios. To overcome the slow inference inherent in diffusion-based models, we introduce MotionFLUX, novel high-speed text-tomotion generation framework based on rectified flow matching. Unlike conventional diffusion models that require hundreds of iterative denoising steps, MotionFLUX learns deterministic mapping of motion trajectories in latent space, enabling generation in single or just few steps. key advantage of rectified flow is its ability to learn global, timeconsistent velocity field, effectively avoiding the step-wise error accumulation common in diffusion processes. As result, MotionFLUX achieves substantial improvements in inference speedcrucial for real-time and interactive scenarioswhile maintaining or even surpassing the motion quality of state-of-the-art diffusion models. Moreover, current text-to-motion systems face critical alignment challenges. central challenge in achieving alignment for text-to-motion models is the creation of preference pairs. Unlike LLM alignment, where established reward models(Lambert et al. 2024) and human feedback data or verifiable gold-standard answers are available, alternatively, in the text-to-audio field(Hung et al. 2024), CLAP(Wu et al. 2023) can be introduced as reward model, the text-to-motion field currently lacks such resources. For instance, leading-edge LLMs, like GPT-4 (Achiam et al. 2023), are often directly used to evaluate candidate outputs (Zheng et al. 2023). Although video-language models(Zhang, Li, and Bing 2023) can process action video inputs and generate text outputs, they typically produce noisy feedback, which is not suitable for generating action preference pairs. If human annotators were to assign binary scores to each video sample based on its alignment with given prompt, this would quickly become economically unfeasible in large-scale applications. To overcome this, we introduce TAPO (TMR++ Aligned Preference Optimization), lightweight powerful alignment framework as depicted in Figure 2. TAPO leverages an internal scoring mechanism (TMR++) to automatically compare candidate motion-text pairs and construct preference dataidentifying which generated motion better matches the given prompt. This enables an online, self-supervised optimization loop that continuously refines the model without requiring manual annotation or external reward models. By integrating TAPO into our pipeline, we achieve much stronger alignment between generated motions and complex language instructions. Our main contributions are threefold: MotionFLUX - The first high-speed text-to-motion model leveraging rectified flow matching. It innovatively introduces rectified flow matching to text-driven motion generation, overcoming slow sampling limitations of diffusion models. It achieves high-quality generation with very few steps, greatly improving inference speed and enabling real-time applications. TAPO - An automatic online preference learning framework for motion-text alignment. We proposes novel self-supervised paradigm that uses TMR++ as surrogate reward model to automatically generate preference pairs. This addresses the challenge of costly human annotations and noisy feedback, enabling continuous self-improvement of alignment quality between complex text semantics and motion. Extensive experiments showing MotionFLUXs superior balance between motion quality, text alignment, and realtime efficiency across multiple evaluation protocols."
        },
        {
            "title": "2 Related Work\nHuman Motion Synthesis. Human motion synthesis has\nbeen widely explored in computer vision and graphics,\nwith early work focusing on deterministic motion predic-\ntion using RNNs (Butepage et al. 2017), GANs (Barsoum,\nKender, and Liu 2018), GCNs (Mao et al. 2019), and MLPs\n(Bouazizi et al. 2022). To enhance diversity, VAEs (Aliak-\nbarian et al. 2020) were introduced. Tasks like motion in-\nbetweening (Duan et al. 2021) and trajectory-based genera-\ntion (Pavllo, Grangier, and Auli 2018) expanded the scope\nof motion synthesis. Methods for animator control, includ-\ning convolutional autoencoders (Holden, Saito, and Komura\n2016) and phase-functioned networks (Holden, Komura, and\nSaito 2017), were also developed within the graphics com-\nmunity.",
            "content": "Recent advancements in large-scale datasets and multitask frameworks have driven further progress in textconditioned motion synthesis. The HumanML3D dataset (Guo et al. 2022a) and TM2T (Guo et al. 2022b) enabled more effective training and performance improvements. Diffusion models, such as MDM (Tevet et al. 2022b) and MotionDiffuse (Yan et al. 2018), have led to breakthroughs in both motion quality and diversity. Despite these advances, aligning generated motions with complex textual semantics, particularly for out-of-distribution descriptions, remains challenging. This work addresses this issue by using Rectified Flow Matching to achieve fine-grained semantic alignment, high-quality generation, and real-time synthesis for text-to-motion tasks. Preference Optimization. Preference optimization is widely recognized as key method for aligning large language models (LLMs), either by training dedicated reward model to encode human preferences (Ouyang et al. 2022) or Figure 2: Overview of MotionFlux. In the first stage, we begin by utilizing pre-trained VAE (with frozen parameters) to compress the raw motion sequence X1:Nframe into the latent space. The compressed representation, along with the text embedding and timestep, is then fed into the vector estimator to obtain the vector field prediction v. In the second stage, we freeze the model parameters trained in the first stage and use them as reference model. We then select random texts from the initial training set to generate an online dataset. The optimization objective in the second stage is to iteratively generate high-quality outputs that align with the expected targets by comparing the predictions of the main model and the reference model. by repurposing the LLM itself as the reward model (Rafailov et al. 2023). More recent work refines this process through iterative alignment, leveraging human annotators to form preference pairs or relying on pre-trained reward models (Kim et al. 2024a; Gulcehre et al. 2023). Incorporating verifiable answers can further facilitate the construction of preference pairs. Beyond LLMs, Diffusion-DPO(Wallace et al. 2024) demonstrates that diffusion and flow-based models can be aligned similarly. However, aligning Text-to-motion poses unique challenges, as there is no definitive gold motion for each text prompt and motion perception remains subjective.Therefore, preference optimization research for text-to-motion generation remains relatively limited."
        },
        {
            "title": "3 Method\nMotionFLUX is designed for motion synthesis by integrat-\ning FluxTransformer-based architectures with a two-stage\npreference-driven training pipeline. The model leverages la-\ntent motion embeddings extracted via a Variational Autoen-\ncoder (VAE) (Kingma, Welling et al. 2013) and combines\nthe Diffusion Transformer (DiT) framework (Peebles and\nXie 2023) with its multimodal extension MMDiT (Esser\net al. 2024). Figure2 provides an overview of the Motion-\nFLUX architecture and training workflow. Our approach fol-\nlows Two-Stage optimization pipeline:",
            "content": "Representation Learning Stage: Our approach concentrates on acquiring essential knowledge from large-scale motion datasets through variational autoencoder and transformer-based encoders. Preference Alignment Stage: We refine the model with the TMR++ Aligned Preference Optimization (TAPO) framework, which generates synthetic motion samples and applies comparative preference rankings to enhance semantic alignment and motion quality."
        },
        {
            "title": "3.1 MODEL CONDITIONING",
            "content": "Motion Encoder. In the MotionFLUX model, Motion Encoding captures temporal patterns in motion sequences using Variational Autoencoder (VAE) with Transformer-based architectures. Given motion sequence Rnj where denotes the number of frames for the action duration and represents the spatial positions of the human motion tree structure. The input motion are embedded into latent space through linear transformation and appends global motion token to represent overarching motion patterns across frames. Transformer encoder then produces latent distribution parameterized by mean µ and log variance σ, from which we sample latent motion representation RLC using the reparameterization trick. where and denote the latent sequence length and the number of channels, respectively. This encoding approach effectively models complex temporal and spatial motion dependencies, providing robust representations for tasks such as motion prediction and generation. Textual Conditioning. Given the textual description of motion, we obtain the text encoding ctext from pretrained text-encoder. Considering the strong performance of FLANT5 (Chung et al. 2024) as conditioning in text-to-motion generation (Dai et al. 2024), we select FLAN-T5(Chung et al. 2024) as our text encoder."
        },
        {
            "title": "3.2 MODEL ARCHITECTURE",
            "content": "MotionFLUX adopts hybrid Transformer backbone inspired by recent FLUX models for image generation 1. MotionFLUX integrates one MMDiT block for robust multimodal fusion and two DiT blocks for efficient temporal reasoning. Replacing part of MMDiT with DiT improves parameter efficiency while maintaining strong performance 2. Each block uses 6 attention heads (head dimension 128, hidden width 768), resulting in 43M-parameter model that balances scalability and expressiveness."
        },
        {
            "title": "3.3 Flow Matching",
            "content": "Flow Matching (FM) (Lipman et al. 2022; Albergo and Vanden-Eijnden 2022) provides robust alternative to diffusion-based generative models (Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2020), which are highly sensitive to noise scheduler. FM learns time-dependent vector field that transports samples from simple prior (e.g., Gaussian) to complex target distribution. Rectified Flows. In our framework, we employ rectified flows (Liu, Gong, and Liu 2022), which define the shortest linear transport path from noise to the target distribution. Let x1 denote the latent representation of motion sample, and let x0 (0, I) be noise sample. For given time step [0, 1], we construct training sample xt so that the model learns to predict velocity vt = dxt that drives dt xt towards x1. Although there are several strategies to construct the transport path xt, we adopt rectified flows(Liu, Gong, and Liu 2022), in which the forward process follows straight-line paths connecting the target and noise distributions, as defined in Eq.1. Empirical results have shown that rectified flows are sample-efficient and experience less degradation than alternative formulations when fewer sampling steps are used (Esser et al. 2024). We denote the parameters of the model by θ. The model is trained by directly regressing the predicted velocity v(x, c; θ) against the ground truth velocity vt using the loss function in Eq.2. xt = (1 t)x1 + tx0, vt = dxt dt = x0 x1, (1) LF = Ex1,x0,t v(x, c; θ) vt2 , Inference.During inference, noise sample x0 (0, I) is drawn from the prior, and an ordinary differential equation solver is used to compute x1 based on the predicted velocity of the model vt at each time step t. In our implementation, we utilize an Euler solver for this purpose: (2) xt+ϵ = + ϵ v(x, c; θ), (3) where ϵ is the step size, the latent representation obtained from sampling is then passed to the motion decoder to reconstruct the motion. Figure 3 illustrates the overall sampling process of our model. 1https://blackforestlabs.ai/ 2https://blog.fal.ai/auraflow/ Figure 3: Overview of the sampling pipeline employed in our rectified-flowbased text-to-motion framework"
        },
        {
            "title": "OPTIMIZATION",
            "content": "TMR++ Aligned Preference Optimization (TAPO) aligns the MotionFLUX model with textual intent through iterative, preference-based training. It leverages TMR++ (Textto-Motion Retrieval) (Bensabath, Petrovich, and Varol 2024) as proxy reward model to evaluate generated motions according to their semantic consistency with the input text and to construct preference pairs for direct optimization. TMR++ is contrastive learningbased cross-modal retrieval framework that jointly trains text-to-motion generation and retrieval tasks. It optimizes shared textmotion embedding space using the InfoNCE loss and applies textsimilaritydriven negative sampling strategy to enhance representation quality. In TAPO, the reward score is computed as the cosine similarity between the textual embedding and the generated motion embedding, providing quantitative measure of semantic alignment. As shown in Section 4.3, using TMR++ to perform best-of-N candidate selection improves both objective metrics and motion quality. The TAPO alignment process begins with pretrained MotionFLUX checkpoint π0 as the base policy. It then iteratively updates the policy πk = u(; θk) to πk+1 through three steps: (1) batched online data generation, (2) reward estimation and preference dataset creation, and (3) fine-tuning πk into πk+1 via direct preference optimization. Our approach is inspired by LLM alignment techniques (Zelikman et al. 2022; Kim et al. 2024a) but differs in two key aspects: 1. It focuses on rectified-flow motion generation rather than autoregressive language modeling. 2. The motion domain lacks off-the-shelf reward models that are commonly used in LLM alignment to produce reliable preference datasets. By leveraging TMR++ as proxy reward model and performing online preference data generation at each iteration, TAPO avoids preference saturation, maintains training stability, and progressively enhances semantic alignment and motion quality."
        },
        {
            "title": "GENERATION",
            "content": "During the construction of the kth iteration preference dataset, we first randomly sample subset Mk from the large-scale prompt pool to serve as the text input for the current batch. For each text prompt yi Mk, the current policy πk generates candidate motion samples, which are then ranked according to semantic similarity using the TMR++ (Bensabath, Petrovich, and Varol 2024) crossmodal evaluation framework. Specifically, by computing the semantic consistency score between the generated motion and the original text prompt yi, the sample with the highest score is designated as the superior sample xw and the one with the lowest score as the inferior sample xl i. The resulting , xl preference dataset Dk is composed of triples (xw i, yi), with each triple corresponding to specific text prompt in Mk.This process ensures data diversity through dynamic sampling mechanism and achieves objective selection with the aid of quantitative evaluation tools. It is important to emphasize that the cross-modal alignment capability of the TMR++ (Bensabath, Petrovich, and Varol 2024) framework plays crucial role in this process, as its fine-grained similarity measurement effectively captures the semantic correlation between text and motion. This contrastive selectionbased data construction strategy not only preserves the core idea of the original algorithm but also enhances the interpretability of the method through hierarchical sampling and quantitative evaluation."
        },
        {
            "title": "3.6 PREFERENCE OPTIMIZATION\nDirect preference optimization (DPO) (Rafailov et al. 2023)\nhas proven effective in encoding human preferences into\nlarge language models (LLMs) (Ouyang et al. 2022). Build-\ning on this success, DPO has been adapted into the DPO-\nDiffusion framework (Wallace et al. 2024) for aligning dif-\nfusion models. The DPO-Diffusion loss is defined as:",
            "content": "LDPODiff = En,ϵw,ϵl log σ (cid:0)β(ϵw 2 (ϵl 2))(cid:1) , ϵref (xw ϵref (xl ϵw ϵl )2 n)2 ϵθ(xw ϵθ(xl )2 2 n)2 (4) where the loss compares the noise-prediction errors of winning and losing samples during the diffusion process. By applying sigmoid function with scaling factor β, it reinforces the models capacity to capture human preferences, thereby ensuring effective alignment of the diffusion model. As demonstrated by (Wallace et al. 2024), through some algebraic manipulation the expression in Eq.4 can be simplified into the more tractable form presented in Eq.5. In this setting, represents the diffusion timestep with uniformly sampled from (0, ); xw and xl denote the winning and losing motions, respectively; and the noise term ϵ follows the distribution (0, I). Following (Esser et al. 2024), the DPO-Diffusion loss can be extended to rectified flows. By exploiting the equivalence (Lipman et al. 2022) between ϵθ and u(; θ), the noise matching terms can be replaced with flow matching terms, resulting in LDPOFM = tU (0,1),xw ,xl log σ{β(u(xw (cid:124) 2 2 (cid:125) , t; θ) vw (cid:123)(cid:122) Winning loss , t; θref ) vw (cid:123)(cid:122) Winning reference loss t2 2 (cid:125) u(xl (cid:124) ) (u(xw (cid:124) t, t; θ) vl (cid:123)(cid:122) Losing loss t, t; θref ) vl (cid:123)(cid:122) Losing reference loss where denotes the flow matching timestep, and xl with xw spectively. along correspond to the losing and winning motions, reu(xl (cid:124) 2 2 (cid:125) t2 2 (cid:125) )}, (5) In the context of LLMs, the DPO loss models the relative likelihood between winning and losing responses. Minimizing this loss effectively increases the margin between themeven if both log-likelihoods are reduced (Pal et al. 2024). Since DPO emphasizes the relative likelihood rather than the absolute values, convergence requires that both likelihoods decrease, which might seem counterintuitive (Rafailov et al. 2024b). Notably, this reduction in likelihood does not inherently harm performance; rather, it is necessary condition for improvement (Rafailov et al. 2024a). When applied to rectified flows, however, the situation becomes more complex due to the challenges in estimating the likelihood of samples generated under classifier-free guidance (CFG). closer inspection of LDPO-FM Eq. 5 reveals that minimizing it can be achieved by simply enlarging the gap between the winning and losing losses, even if both losses increase. To address this issue, we propose incorporating the winning loss directly into the optimization objective, thereby preventing its inadvertent increase. Our modified loss is defined as LTAPO = LDPO-FM + αLFM, (6) where LFM is the flow matching loss computed exclusively on the winning motion, as given in Eq. 6, and α is learnable parameter balancing its influence. Although the DPO loss effectively improves preference ranking between chosen and rejected motions, relying solely on it can cause overoptimization. This may compromise the semantic and structural fidelity of the winning motion, leading the models outputs to drift away from the intended distribution. By adding the LFM term, we anchor the model to the high-quality attributes of the winning examples, regularizing the training process. This additional loss stabilizes optimization and helps preserve the essential characteristics of the winning motions, resulting in more balanced and robust performance."
        },
        {
            "title": "4 Experiment\nIn this section, we provide a detailed description of the ex-\nperimental setup and evaluation process. Section 4.1 covers",
            "content": "Table 1: Comparison of text-conditional motion synthesis on the HumanML3D (Guo et al. 2022a) dataset. We compute the suggested metrics following (Guo et al. 2022a). The evaluation is repeated 20 times for each metric and the average is reported with 95% confidence interval. indicates that the closer to the real data, the better. Bold and underlined entries indicate the best and the second best results, respectively. The MotionFlux-ultra(5ms) surpasses all state-of-the-art models. Methods Real Seq2Seq (Lin et al. 2018) JL2P (Ahuja and Morency 2019) T2G (Bhattacharya et al. 2021) Hier (Ghosh et al. 2021) TEMOS (Petrovich, Black, and Varol 2022) T2M (Guo et al. 2022a) MDM (Tevet et al. 2022b) MotionDiffuse (Zhang et al. 2024) MLD (Chen et al. 2023) MotionLCM(Dai et al. 2024) MotionFlux-V1 MotionFlux-ultra AITS() R-Precision() FID () MM Dist () Diversity () MultiModality () Top 1 0.511.003 0.180.002 0.246.002 0.165.001 0.301.002 0.424.002 0.457.002 0.320.005 0.491.001 0.481.003 0.502.003 0.5300.002 0.5360.003 Top 2 0.703.003 0.300.002 0.387.002 0.267.002 0.425.002 0.612.002 0.639.003 0.498.004 0.681.001 0.673.003 0.701.002 0.7290.001 0.7320. Top 3 0.797.002 0.396.002 0.486.002 0.345.002 0.552.004 0.722.002 0.740.003 0.611.007 0.782.001 0.772.002 0.803.002 0.8250.001 0.8270.001 - - - - - 0.017 0.038 24.74 14.74 0.217 0.030 0.005 0.005 0.002.000 11.75.035 11.02.046 7.664.030 6.532.024 3.734.028 1.067.002 0.544.044 0.630.001 0.473.013 0.467.012 0.0860.003 0.0780. 2.794.008 5.529.007 5.296.008 6.030.008 5.012.018 3.703.008 3.340.008 5.566.027 3.113.001 3.196.010 3.022.009 2.870.006 2.840.005 9.503.065 6.223.061 7.676.058 6.409.071 8.332.042 8.973.071 9.188.002 9.559.086 9.410.049 9.724.082 9.631.066 9.5310.09 9.5310.09 - - - - - 0.368.018 2.090.083 2.799.072 1.553.042 2.413.079 2.172.082 1.8410.07 1.9980. the datasets, evaluation metrics, model configurations, training procedures, and computational resources used in our experiments. Section 4.2 presents comparison of our method with competitive approaches. Finally, in Section 4.3, we offer an analysis and discussion of the results."
        },
        {
            "title": "4.1 Experimental setup\nDatasets: We conduct experiments using the widely used\nHumanML3D dataset (Guo et al. 2022a), which contains\n14,616 unique human motion sequences and 44,970 asso-\nciated textual descriptions. To ensure a fair comparison with\nprior methods (Chen et al. 2023; Guo et al. 2022a; Petro-\nvich, Black, and Varol 2022; Tevet et al. 2022b; Zhang et al.\n2024), we utilize the redundant motion representation, in-\ncluding root velocity, root height, local joint positions, ve-\nlocities, rotations in root space, and foot contact binary la-\nbels.\nEvaluation Metrics: Building on the evaluation metrics es-\ntablished in previous works(Chen et al. 2023; Guo et al.\n2022a; Xie et al. 2023) , we extend these measures to com-\nprehensively assess our model. To quantify time efficiency,\nwe report the Average Inference Time per Sentence (AITS)\nas in (Chen et al. 2023). For motion quality, we adopt the\nFrechet Inception Distance (FID) as the primary metric,\nwhich evaluates the similarity between the feature distribu-\ntions of generated and real motions using the feature extrac-\ntor from (Guo et al. 2022a). Motion diversity is evaluated by\ntwo measures: MultiModality (MModality), which assesses\nthe diversity of generated outputs conditioned on identical\ntextual inputs, and Diversity, which computes the variance of\nthe extracted features (Guo et al. 2022a). Condition match-\ning is gauged by calculating the motion-retrieval precision\n(R-Precision) for text-motion Top-1/2/3 matching accuracy\nand the Multimodal Distance (MM Dist), which reflects the\nmean distance between motion and text embeddings.\nExperimental Settings: We pre-trained MotionFlux on the\nHumanML3D dataset (Guo et al. 2022a) for 500 epochs us-\ning the AdamW optimizer (Loshchilov and Hutter 2017)\nwith β1 = 0.9, β2 = 0.999, and an initial learning rate",
            "content": "of 1 104. linear learning-rate scheduler was applied throughout training, and the model was trained on single A100 GPU with batch size of 64. Following prior findings that sampling timesteps from the middle of [0, 1] improves generation quality (Hang et al. 2023; Kim et al. 2024b; Karras et al. 2022), we sampled from logitnormal distribution with mean 0 and variance 1, following (Esser et al. 2024). This pre-trained model is referred to as MotionFlux-V1. During the TAPO alignment phase, we used the same optimizer but with an effective batch size of 32, peak learning rate of 1 105, and linear warmup of 100 steps. Each TAPO iteration trained for 8 epochs, and the checkpoint from the last epoch was used for batched online data generation. We performed three TAPO iterations, as performance plateaued beyond that point. The final aligned model is referred to as MotionFlux-Ultra."
        },
        {
            "title": "4.2 Comparison with Other Methods\nComparison of Text-Conditional Motion Synthesis. As\nshown in Table 1, MotionFlux achieves the lowest AITS,\nconfirming its superior inference efficiency. It also obtains\nthe highest R-Precision and lowest MM Dist, demonstrating\nstrong semantic alignment with textual inputs. At the same\ntime, MotionFlux achieves the lowest FID, demonstrating\nthat its outputs remain close to real motion distributions, yet\nit maintains diversity at a level comparable to real data. This\nbalance shows that MotionFlux enhances alignment and re-\nalism without collapsing variability. Compared with prior\nmethods, MotionFlux achieves a better trade-off between se-\nmantic accuracy, motion fidelity, and variability.\nComparison of Fine-Grained Semantic Alignment. As il-\nlustrated in Figure 4, we compare MotionFlux with Mo-\ntionLCM(Dai et al. 2024), MLD(Chen et al. 2023), and\nMDM(Tevet et al. 2022b). MotionLCM frequently produces\nsemantic errors under fine-grained conditions, resulting in\nreduced motion quality and weaker control. In contrast, Mo-\ntionFlux consistently achieves higher fidelity and superior\nalignment. For example, in the second case, MotionLCM\nand MDM fail to capture the “glance” action, while in the",
            "content": "Figure 4: Qualitative comparison of the state-of-the-art methods in the text-to-motiontask,the darker the color, the later the time. We employed ChatGPT-o3 to randomly generate three promptsnone of which had appeared in the datasetfor inference. The visualization results show that MotionFlux exhibits strong semantic alignment on critical events (e.g., left and right, glance) and demonstrates robust generalization performance. third, both confuse left and right. MotionFlux, however, preserves pose constraints and faithfully follows detailed textual descriptions. In terms of inference speed, MotionFlux is 3 times faster than MotionLCM, 40 times faster than MLD, and 4800 times faster than MDM."
        },
        {
            "title": "4.3 Discussion",
            "content": "The Significance of Online Dataset Generation. Due to the lack of reliable offline data, we resorted to using the output generated by motion-v1 during its first iteration as our offline dataset. Figure 5 presents the results of four iterations comparing scenarios with and without new data generation. Our experiments indicate that repeatedly training on fixed offline dataset leads to rapid performance saturation and subsequent deterioration. Specifically, for the offline TAPO, the TMR++ score begins to decline by the second iteration while the fid metric rises sharply, and by the fourth iteration the models performance has substantially degradedhighlighting the drawbacks of relying on offline data. In contrast, the online TAPO, which generates new data at each iteration, consistently outperforms its offline counterpart on both the TMR++ score and fid metrics. plausible explanation for this phenomenon is reward over-optimization (Rafailov et al. 2024a). In the context of DPO training for large language models, (Kim et al. 2024a) demonstrated that the reference model functions as sampling lower bound; iteratively updating this reference with the same dataset can cause the current model to minimize the loss in unexpected ways. Furthermore, the true objective of our model is to train model that is jointly endorsed by both the reward proxy and the reference model (i.e., the model being optimized). The objective function is formuFigure 5: Trajectory of FID and TMR++ scores over training iterations. Offline training peaks by the second iteration with rising FID, while online training continues to improve, showing lower FID and higher TMR++ scores. FID Diversity TMR++score 1 5 10 15 0.102 0.093 0.086 0.087 9.635 9.576 9.531 9.542 0.803 0.812 0.819 0. Table 2: Comparison of Best-of-N policy. Metrics include Frechet Inception Distance (FID), Diversity and TMR++ score. lated as follows: π(y x) = 1 Z(x) πref(y x) exp (cid:18) 1 β (cid:19) r(x, y) , where z(x) denotes normalization factor, πref(y x) is the reference model, r(x, y) represents the reward function learned from TMR++ feedback, and β is hyperparameter that controls the temperature (or, equivalently, the inverse temperature). Best-of-N Inferred Policy. We conducted experiments to verify that the TMR++ model can serve as an effective proxy reward model for evaluating motion outputs. Specifically, we adopt Best-of-N strategy to assess MotionFlux, with {1, 5, 10, 15}. For each prompt, the generated motion sequences are ranked according to the TMR++ score. The results in Table 2 show that increasing in the Best-of-N strategy consistently improves both the TMR++ score and FID, while diversity remains stable. This indicates that TMR++ reliably ranks motion sequences aligned with textual descriptions, and that these quality gains are achieved without sacrificing motion variability, highlighting the methods ability to balance semantic alignment and motion quality."
        },
        {
            "title": "5 Conclusion\nIn summary, we have presented MotionFlux—a fast and ef-\nficient text-to-motion model that leverages synthetic pref-\nerence data generated online. By integrating advanced rec-\ntified flow matching with preference alignment techniques,\nMotionFlux not only produces animations that accurately re-\nflect user prompts but also outperforms existing diffusion-\nbased baselines in both quality and speed. Moreover, our",
            "content": "extensive experiments demonstrate state-of-the-art performance on benchmark datasets, thereby confirming the practical viability of our approach for real-time applications. Overall, this work lays solid foundation for future research on efficient and semantically robust motion generation. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ahuja, C.; and Morency, L.-P. 2019. Language2pose: Natural language grounded pose forecasting. In 2019 International conference on 3D vision (3DV), 719728. IEEE. Albergo, M. S.; and Vanden-Eijnden, E. 2022. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571. Aliakbarian, S.; Saleh, F. S.; Salzmann, M.; Petersson, L.; and Gould, S. 2020. stochastic conditioning scheme In Proceedings of for diverse human motion prediction. the IEEE/CVF conference on computer vision and pattern recognition, 52235232. Barsoum, E.; Kender, J.; and Liu, Z. 2018. Hp-gan: Probabilistic 3d human motion prediction via gan. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 14181427. Bensabath, L.; Petrovich, M.; and Varol, G. 2024. crossdataset study for text-based 3D human motion retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 19321940. Bhattacharya, U.; Rewkowski, N.; Banerjee, A.; Guhan, P.; Bera, A.; and Manocha, D. 2021. Text2gestures: transformer-based network for generating emotive body gestures for virtual agents. In 2021 IEEE virtual reality and 3D user interfaces (VR), 110. IEEE. Bouazizi, A.; Holzbock, A.; Kressel, U.; Dietmayer, K.; and Belagiannis, V. 2022. Motionmixer: Mlp-based 3d human body pose forecasting. arXiv preprint arXiv:2207.00499. Butepage, J.; Black, M. J.; Kragic, D.; and Kjellstrom, H. 2017. Deep representation learning for human motion prediction and classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, 6158 6166. Chen, X.; Jiang, B.; Liu, W.; Huang, Z.; Fu, B.; Chen, T.; and Yu, G. 2023. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 18000 18010. Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, Y.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70): 153. Dai, W.; Chen, L.-H.; Wang, J.; Liu, J.; Dai, B.; and Tang, Y. 2024. Motionlcm: Real-time controllable motion generation In European Conference on via latent consistency model. Computer Vision, 390408. Springer. Duan, Y.; Shi, T.; Zou, Z.; Lin, Y.; Qian, Z.; Zhang, B.; and Yuan, Y. 2021. Single-shot motion completion with transformer. arXiv preprint arXiv:2103.00776. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Ghosh, A.; Cheema, N.; Oguz, C.; Theobalt, C.; and Slusallek, P. 2021. Synthesis of compositional animations from textual descriptions. In Proceedings of the IEEE/CVF international conference on computer vision, 13961406. Gulcehre, C.; Paine, T. L.; Srinivasan, S.; Konyushkova, K.; Weerts, L.; Sharma, A.; Siddhant, A.; Ahern, A.; Wang, M.; Gu, C.; et al. 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998. Guo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and Cheng, L. 2022a. Generating diverse and natural 3d human In Proceedings of the IEEE/CVF conmotions from text. ference on computer vision and pattern recognition, 5152 5161. Guo, C.; Zuo, X.; Wang, S.; and Cheng, L. 2022b. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, 580597. Springer. Hang, T.; Gu, S.; Li, C.; Bao, J.; Chen, D.; Hu, H.; Geng, X.; and Guo, B. 2023. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF international conference on computer vision, 74417451. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Holden, D.; Komura, T.; and Saito, J. 2017. Phasefunctioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4): 113. Holden, D.; Saito, J.; and Komura, T. 2016. deep learning framework for character motion synthesis and editing. ACM Transactions on Graphics (ToG), 35(4): 111. Hung, C.-Y.; Majumder, N.; Kong, Z.; Mehrish, A.; Valle, R.; Catanzaro, B.; and Poria, S. 2024. TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization. arXiv preprint arXiv:2412.21037. Karras, T.; Aittala, M.; Aila, T.; and Laine, S. 2022. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577. Kim, D.; Kim, Y.; Song, W.; Kim, H.; Kim, Y.; Kim, S.; and Park, C. 2024a. sDPO: Dont Use Your Data All at Once. arXiv preprint arXiv:2403.19270. Kim, M.; Ki, D.; Shim, S.-W.; and Lee, B.-J. 2024b. Adaptive Non-Uniform Timestep Sampling for Diffusion Model Training. arXiv preprint arXiv:2411.09998. Kingma, D. P.; Welling, M.; et al. 2013. Auto-encoding variational bayes. Lambert, N.; Morrison, J.; Pyatkin, V.; Huang, S.; Ivison, H.; Brahman, F.; Miranda, L. J. V.; Liu, A.; Dziri, N.; Lyu, S.; et al. 2024. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Lin, A. S.; Wu, L.; Corona, R.; Tai, K.; Huang, Q.; and Mooney, R. J. 2018. Generating animated videos of human activities from natural language descriptions. Learning, 1(2018): 1. Lipman, Y.; Chen, R. T.; Ben-Hamu, H.; Nickel, M.; and Le, M. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747. Liu, X.; Gong, C.; and Liu, Q. 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003. Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Mao, W.; Liu, M.; Salzmann, M.; and Li, H. 2019. Learning trajectory dependencies for human motion prediction. In Proceedings of the IEEE/CVF international conference on computer vision, 94899497. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744. Pal, A.; Karkhanis, D.; Dooley, S.; Roberts, M.; Naidu, S.; and White, C. 2024. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228. Pavllo, D.; Grangier, D.; and Auli, M. 2018. Quaternet: quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485. Peebles, W.; and Xie, S. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 41954205. Petrovich, M.; Black, M. J.; and Varol, G. 2021. Actionconditioned 3D human motion synthesis with transformer VAE. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1098510995. Petrovich, M.; Black, M. J.; and Varol, G. 2022. Temos: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision, 480 497. Springer. Plappert, M.; Mandery, C.; and Asfour, T. 2016. The kit motion-language dataset. Big data, 4(4): 236252. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PmLR. Rafailov, R.; Chittepu, Y.; Park, R.; Sikchi, H.; Hejna, J.; Knox, B.; Finn, C.; and Niekum, S. 2024a. Scaling laws for reward model overoptimization in direct alignment algorithms. arXiv preprint arXiv:2406.02900. Rafailov, R.; Hejna, J.; Park, R.; and Finn, C. 2024b. From to q: Your language model is secretly q-function. arXiv preprint arXiv:2404.12358. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36: 5372853741. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Tevet, G.; Gordon, B.; Hertz, A.; Bermano, A. H.; and Cohen-Or, D. 2022a. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, 358374. Springer. Tevet, G.; Raab, S.; Gordon, B.; Shafir, Y.; Cohen-Or, D.; and Bermano, A. H. 2022b. Human motion diffusion model. arXiv preprint arXiv:2209.14916. Wallace, B.; Dang, M.; Rafailov, R.; Zhou, L.; Lou, A.; Purushwalkam, S.; Ermon, S.; Xiong, C.; Joty, S.; and Naik, N. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 82288238. Wu, Y.; Chen, K.; Zhang, T.; Hui, Y.; Berg-Kirkpatrick, T.; and Dubnov, S. 2023. Large-scale contrastive languageaudio pretraining with feature fusion and keyword-toIn ICASSP 2023-2023 IEEE Intercaption augmentation. national Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Xie, Y.; Jampani, V.; Zhong, L.; Sun, D.; and Jiang, H. 2023. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580. Yan, X.; Rastogi, A.; Villegas, R.; Sunkavalli, K.; Shechtman, E.; Hadap, S.; Yumer, E.; and Lee, H. 2018. Mt-vae: Learning motion transformations to generate multimodal human dynamics. In Proceedings of the European conference on computer vision (ECCV), 265281. Zelikman, E.; Wu, Y.; Mu, J.; and Goodman, N. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35: 1547615488. Zhang, H.; Li, X.; and Bing, L. 2023. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858. Zhang, M.; Cai, Z.; Pan, L.; Hong, F.; Guo, X.; Yang, L.; and Liu, Z. 2024. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence, 46(6): 41154128. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623."
        }
    ],
    "affiliations": [
        "China University of Petroleum, China",
        "Tiandy Technologies, China",
        "Tianjin University, China"
    ]
}