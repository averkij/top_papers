{
    "paper_title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models",
    "authors": [
        "Sathya Krishnan Suresh",
        "Tanmay Surana",
        "Lim Zhi Hao",
        "Eng Siong Chng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data."
        },
        {
            "title": "Start",
            "content": "CS-Sum: Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, Eng Siong Chng Nanyang Technological University Correspondence: sathyakr001@e.ntu.edu.sg 5 2 0 2 9 1 ] . [ 1 9 5 5 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Code-switching (CS) poses significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across MandarinEnglish (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. including open and Evaluating ten LLMs, closed-source models, we analyze performance across few-shot, translate-summarize, and finetuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data."
        },
        {
            "title": "Introduction",
            "content": "Code-switching (CS) is the practice of alternating between two or more languages within single conversation or utterance. Bilingual and multilingual speakers frequently engage in CS. This form of communication has also become prevalent in social media and is the norm in multilingual societies. Large Language Models (LLMs) (Hurst et al., 2024; Team et al., 2023; Dubey et al., 2024), are trained on huge volumes of data majority of which is written in English. Despite the prevalence of CS in real-world communication, existing benchmarks (Huzaifah et al., 2024; Zhang et al., 2023; Yong et al., 2023) fail to assess how well LLMs process CS dialogues. Improved CS comprehension would enable LLMs to interpret multilingual Figure 1: An instance from the benchmark prompts more effectively, enhancing accessibility for CS speakers (Bawa et al., 2020). Summarization is an ideal task to evaluate CS understanding, as it requires both grasping the dialogues main idea and generating concise, coherent summary across languages. The only existing CS dialogue summarization dataset, Gupshup (Mehnaz et al., 2021), covers only one language pair (English-Hindi). Other CS benchmarks like LinCE (Aguilar et al., 2020) and GLUECos (Khanuja et al., 2020) focus on tasks such as language identification and POS-tagging, which assess wordor sentence-level understanding, whereas summarization requires discourse-level comprehension, much harder test of CS ability. To truly evaluate the ability of LLMs on CS, we propose the CS-Sum benchmark, which contains 900-1300 CS dialogue-summary pairs for three language pairs: Mandarin-English (EN-ZH), Tamil-English (EN-TA) and Malay-English (ENMS). The CS dialogues were created by native speakers of the respective languages. Instead of generating dialogue from scratch, the speakers were asked to translate the English dialogues in test sets of DialogSum (Chen et al., 2021) and SAMSum (Gliwa et al., 2019) - two popular English dialogue summarization datasets - to CS dialogues in their respective languages. We evaluate 9 open source and 1 proprietary LLMs under few-shot, translate-summarize, LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023) on synthetic data and provide detailed analysis on the struggles of current SOTA LLMs(<=9B). For LoRA and QLoRA fine-tuning, the training data was generated using Gemini-2-flash (Team et al., 2023), as it is large-scale LLM with freeto-use API, making it practical choice for our experiments. Our qualitative analysis of the summaries generated by LLMs showed that tranditional summarization metrics like ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019) are not sufficient for measuring the quality of the summaries since the LLMs are capable of generating summaries that contain subtle errors that change the entire meaning of the CS dialogues. To this end, we identify the three most common errors (CSL, MST, SMA Section 5) that the LLMs make when summarizing CS dialogues and suggest an LLM-driven approach to analyse their summaries with respect to the errors. Our main contributions are: We introduce CS-Sum, the first CS dialogue summarization benchmark for EN-ZH, ENTA and EN-MS. We conduct an in-depth evaluation of stateof-the-art LLMs, identifying critical failure patterns in their ability to process and summarize CS dialogues. We release CS-Sum, along with the full synthetic training dataset."
        },
        {
            "title": "2 Related Work",
            "content": "While LLMs have demonstrated strong performance in many multilingual tasks, they still face significant challenges when dealing with CS. Studies (Zhang et al., 2023; Yong et al., 2023) have shown that LLMs, including GPT-4 and GPT-3.5, struggle with generating code-switched text, often resulting in language collapse, where the model fails to mix languages properly, and in task-specific failures such as poor performance in summarization and machine translation for CS data. These issues arise because LLMs are typically trained on monolingual data and lack the capacity to handle the intricacies of CS discourse. Additionally (Huzaifah et al., 2024), their performance is inconsistent, particularly when translating low-resource language pairs, with models showing better results for high-resource languages but underperforming when confronted with languages that are underrepresented in training datasets. These findings highlight big problem as people prefer to interact with LLMs in code-switch (Bawa et al., 2020). Recent benchmarks such as LinCE (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have been instrumental in advancing the understanding of CS phenomena by focusing on tasks like LID, NER, and POS tagging. While these datasets have provided valuable insights, its important to recognize that LLMs continue to encounter challenges when dealing with the complexities inherent in code-switched discourse. To truly evaluate the CS comprehension of LLMs, other benchmarks are necessaryones that go beyond wordlevel tasks and assess deeper linguistic understanding. The first step in this direction is CS-Sum, benchmark that evaluates LLMs on more complex task, such as summarization, which requires nuanced comprehension of code-switched text."
        },
        {
            "title": "3.1 Dataset Construction",
            "content": "The goal of building the CS-Sum benchmark, was to address the lack of benchmark that evaluates the CS comprehension of LLMs. We selected Mandarin (zh), Bahasa Melayu (ms), and Tamil (ta) because these languages have large bilingual speaker communities that frequently code-switch with English. To efficiently create diverse CS benchmark within resource constraints, we translated subset of the combined test sets from DialogSum and SAMSum. We recruited 7 native speakers for each language to translate the English dialogues to CS dialogues in their respective languages. The translators were native speakers, all university students (bachelors or masters level). The entire translation process lasted for about 5 months."
        },
        {
            "title": "3.2 Corpus Overview and Analysis",
            "content": "Table 2 presents key statistics of the CS-Sum benchmark across three language pairs. We observe that Language M-Index I-Index Burstiness Span Entropy Memory EN-ZH EN-MS EN-TA 0.40 0.41 0.42 0.36 0.38 0. -0.79 -0.65 -0.68 0.13 0.64 0.58 -0.08 -0.24 -0.22 Table 1: CS metrics measured on the CS-Sum dataset EN-ZH EN-TA EN-MS Number of instances EN utterances Lang utterances EN as matrix language Lang as matrix language Avg. monolingual utterance length Avg. CS utterance length 1320 1140 2079 4699 4779 2.344 5.816 1000 344 983 2340 6790 3.616 9.642 918 517 265 2984 1919 5.515 13.885 Table 2: CS statistics of CS-Sum. Lang refers to the language other than English non-English utterances outnumber English ones in EN-ZH and EN-TA, while the reverse is true for EN-MS, suggesting stronger local language dominance in the former two. The matrix language distribution further highlights this variation: Tamil dominates as the matrix language in EN-TA, whereas EN-ZH and EN-MS are more balanced. This indicates differing CS behaviorsembedding in EN-TA versus alternation in EN-ZH and ENMS. Utterance lengths also vary significantly. ENMS exhibits the longest average CS utterances (13.88 tokens), compared to 5.81 in EN-ZH, suggesting more complex span-level dependencies in Malay-English dialogues. The longer monolingual spans and varied matrix language roles imply that LLMs must manage different CS dynamics across language pairs."
        },
        {
            "title": "3.3 Quantifying Code-Switching",
            "content": "We quantify the structural properties of codeswitching in CS-Sum using five established metrics (Guzmán et al., 2017), reported in Table 1. The M-Index measures the distributional balance between languages, with values near 1 indicating equal usage. The I-Index captures the token-level likelihood of language alternation. Burstiness characterizes the temporal irregularity of switching, where negative values imply regular alternation and positive values indicate clustering. Span Entropy quantifies the unpredictability in monolingual segment lengths, while Memory reflects the autocorrelation of consecutive spanspositive values suggest persistence, negative values suggest CS pair KL Div. 2.4786 EN-ZH 0.5481 EN-MS 0.5089 EN-TA JS Div. 0.4918 0.1454 0.1017 Table 3: Divergence between CS-Sum and CS-Sum-Syn alternation. Across the three CS pairs, CS-Sum exhibits moderate switching behavior. The M-Index values (0.400.42) suggest relatively balanced bilingual distributions, and I-Index scores (0.360.40) indicate frequent intra-sentential switching, most prominently in EN-TA. Burstiness values are consistently negative, reflecting structured rather than chaotic switching. Span Entropy is lowest for ENZH (0.13), indicating predictable switch boundaries, while EN-TA (0.58) and EN-MS (0.64) exhibit higher variability. Memory values are near zero or negative across all pairs, implying weak temporal consistency in span lengths, and highlighting the need for LLMs to dynamically track and adapt to switching boundaries during summarization."
        },
        {
            "title": "3.4 Gemini-Generated Training Data",
            "content": "To support fine-tuning for evaluating LLMs on CSSum, we construct synthetic training set, CSSum-Syn, by converting 19,014 English dialoguesummary pairs from DialogSum and SAMSum into code-switched dialogues using Gemini-21 (Team et al., 2023). This augmentation is not contribution in itself, but means to enable parameterefficient adaptation (e.g., LoRA, QLoRA). To validate the distributional alignment of synthetic and human CS data, we compute CS metrics across both sets and visualize them for each language pair in Figures 2a, 2b, and 2c. We find that Gemini-generated data broadly approximates human CS patterns for EN-TA and ENMS. In EN-TA, distributions over I-Index, burstiness, and span entropy align closely, suggesting that Gemini captures intra-sentential switching 1https://ai.google.dev/, gemini-2-flash-exp (a) ZH Distribution (b) TA Distribution (c) MS Distribution well. EN-MS also shows good overlap, though with slightly more bursty and irregular switching in the synthetic set. In contrast, EN-ZH exhibits substantial divergence: synthetic dialogues show flatter memory and burstiness curves, as well as long-tailed span entropy distribution, indicating reduced switching regularity and predictability. This mismatch is quantified in Table 3, where the KL and JS divergence for EN-ZH are 45 higher than for the other pairs. These discrepancies may affect the quality of fine-tuning and are revisited in Section 5 during error diagnosis."
        },
        {
            "title": "4 LLM performance on CS-Sum",
            "content": "In this section, we present the performance of ten LLMsnine open-source2 and one proprietaryon CS-Sum across four settings: FewShot, Translate-Summarize, LoRA, and QLoRA. In LoRA and QLoRA, open-source models are finetuned on CS-Sum-Syn. The evaluated open-source LLMs include LLAMA-3-8B (Dubey et al., 2024), MISTRAL-7B (Jiang et al., 2023), MINISTRAL8B3, GEMMA-2-2B and GEMMA-2-9B (Team et al., 2024), QWEN2.5-2B and QWEN2.5-7B (Yang et al., 2024), SEA-LION-GEMMA-2-9B4, 2Instruction-tuned and latest versions 3https://huggingface.co/mistralai/ Ministral-8B-Instruct-2410 4https://sea-lion.ai/ and SEALLM-7B (Nguyen et al., 2024). The proprietary model used is GPT-4O (Hurst et al., 2024). These models were selected based on their multilingual pretraining exposure and computational feasibility, with sizes ranging from 2B to 9B parameters. Models like LLAMA-3-8B, MISTRAL-7B, and GEMMA-2-9B have shown strong generalization across NLP tasks, while QWEN2.5-7B, SEALION-GEMMA-2-9B, and SEALLM-7B were chosen for their targeted multilingual capabilities, especially in languages like Bahasa Melayu, Tamil, and Mandarin. GPT-4O serves as strong proprietary baseline due to its SOTA performance in cross-lingual comprehension. By evaluating these models across different parameter scales and varying degrees of multilingual exposure, we investigate how model size and pretraining diversity affect performance. LoRA and QLoRA fine-tuning on CS-Sum-Syn result in higher scores on automated metrics, suggesting improved alignment with reference summaries. However, it remains unclear whether these gains reflect actual comprehension of code-switched dialogue or simply the ability to replicate the distributional patterns of Gemini-generated synthetic data. To explore this, we conduct an in-depth error analysis in Section 5, which reveals that fine-tuning on synthetic data does not improveand often degradesthe models ability to accurately interpret Model Lang ROUGE-L BERTScore SBERT-COSINE JACCARD METEOR Gemma-2-2B Qwen2.5-3B Qwen2.5-7B SEALLM-7B Mistral-7B Ministral-8B LLaMA-3-8B Gemma-2-9B SEA-LION-Gemma-2-9B GPT-4o EN-ZH EN-TA EN-MS 0.2330 / -4 0.2202 / -7 0.2421 / -7. 0.8876 / -0.35 0.8812 / -0.38 0.8898 / -0.98 0.2393 / 9.88 EN-ZH EN-TA 0.2227 / -1.46 0.2408 / 6.21 EN-MS 0.8824 / 0.93 0.8805 / 0.44 0.8868 / 0.60 EN-ZH 0.2482 / 27.10 EN-TA 0.2800 / -5.11 0.3117 / -1.65 EN-MS 0.8106 / 11.20 0.8937 / -0.55 0.9041 / -0.46 EN-ZH 0.2115 / 19.25 0.2270 / 0.99 EN-TA 0.2575 / 8.41 EN-MS 0.8122 / 9.15 0.8686 / 1.21 0.8801 / 1.28 EN-ZH 0.2568 / -3.10 EN-TA 0.2198 / -2.15 0.2421 / 1.59 EN-MS EN-ZH 0.2560 / 4.23 EN-TA 0.2468 / -5.61 0.2700 / 6.97 EN-MS 0.8902 / -0.42 0.8792 / 0.04 0.8897 / -0.46 0.8875 / 0.52 0.8858 / -0.18 0.8917 / 0.37 EN-ZH 0.2868 / -2.29 0.2453 / 0.47 EN-TA 0.2599 / 8.06 EN-MS 0.8845 / 0.76 0.8769 / 0.99 0.8724 / 2.04 EN-ZH 0.2995 / -5.62 EN-TA 0.2761 / -3.85 0.3009 / -3.00 EN-MS EN-ZH 0.2799 / -3.96 0.2703 / 0.26 EN-TA 0.2860 / -1.56 EN-MS EN-ZH 0.2965 / -1.03 EN-TA 0.3157 / -4.92 0.3126 / -1.70 EN-MS 0.8987 / -0.27 0.8911 / 0.57 0.8975 / 0.01 0.8968 / -0.34 0.8898 / 0.48 0.8963 / -0. 0.8816 / 2.21 0.8981 / 0.39 0.9037 / -0.15 0.7268 / -1.40 0.6755 / -0.85 0.7352 / -4.35 0.7143 / 1.70 0.6880 / -2.35 0.7308 / -2.25 0.4927 / 54.12 0.7081 / -2.14 0.7520 / -0.67 0.4836 / 45.48 0.6470 / 2.53 0.6956 / 4.91 0.7208 / -0.34 0.6847 / -0.95 0.7293 / -2. 0.7037 / 1.06 0.6752 / -2.95 0.7262 / 0.03 0.7090 / 4.09 0.6707 / 3.28 0.6538 / 11.21 0.7603 / -0.91 0.7203 / 4.42 0.7573 / -0.94 0.7542 / -0.41 0.7138 / 3.79 0.7404 / 1.21 0.7082 / 9.63 0.7570 / 2.19 0.7750 / 0.12 0.1658 / -7.64 0.1523 / -9.19 0.1739 / -13. 0.2699 / -0.9 0.2453 / -5.23 0.2902 / -12.43 0.1643 / 5.82 0.1468 / -0.13 0.1694 / 1.14 0.2974 / 2.05 0.2591 / -4.82 0.3100 / -4.57 0.0783 / 162.62 0.1837 / -9.62 0.2122 / -8.37 0.1118 / 188.89 0.2723 / -9.20 0.3372 / -10.26 0.0834 / 89.27 0.1336 / 3.44 0.1633 / 9. 0.1297 / 104.29 0.2287 / 1.55 0.2800 / 5.95 0.1754 / -8.01 0.1470 / -6.91 0.1796 / -11.49 0.3010 / -7.82 0.2657 / -10.45 0.3084 / -8.40 0.1588 / 6.13 0.1561 / -3.66 0.1794 / 2.93 0.1918 / -6.99 0.1598 / 1.07 0.1804 / 0.02 0.2054 / -5.83 0.1832 / 4.63 0.2096 / -3. 0.1820 / -4.29 0.1727 / -0.12 0.1837 / -4.16 0.2517 / 1.15 0.2387 / -8.53 0.2843 / -0.09 0.2916 / 0.88 0.2570 / -2.13 0.2833 / 5.50 0.3246 / -3.06 0.3107 / 1.25 0.3279 / -0.07 0.2944 / -4.46 0.2884 / -2.38 0.2983 / -4.75 0.1792 / 11.66 0.2041 / -0.55 0.2045 / 0. 0.2853 / 16.52 0.3419 / 0.75 0.3365 / 1.09 Table 4: Few-Shot Performance/Translate-Summarize % improvement on CS-Sum and summarize code-switched content."
        },
        {
            "title": "4.1 Result Analysis",
            "content": "In this subsection, we present the results of the LLMs performance on the CS-Sum in Few-Shot, Translate-Summarize, LoRA and QLoRA settings. Appendix provides an explanation on the different settings. We use standard summarization metrics like ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), SBERT-Cosine (Reimers and Gurevych, 2019), Jaccard and METEOR (Banerjee and Lavie, 2005). This comprehensive set of metrics evaluate both word-level matching and semantic similarity. The few-shot performance of LLMs across the three CS language pairs is reported in Table 4. While semantic similarity metrics such as BERTScore and SBERT-Cosine report high values, these can be misleading. As illustrated in Figure 3, the summaries often fail to accurately capture the core content or intent of the original dialogue, despite appearing semantically plausible. This reflects common failure mode of LLMs in CS summarization: generating fluent, high-overlap summaries that omit or distort key factual elements. In this example, Gemma-2-9B misrepresents who confirms the location of the conference and fabricates speaker opinions, despite achieving high semantic similarity score. ROUGE-L scores further corroborate this gap, with LLMs scoring nearly 50% lower than the best-performing models on the monolingual DialogSum5 and SAMSum6 benchmarks. From Table 4, we observe that GPT-4o is the best-performing model overall. However, due to the lack of transparency in its training and architecture, we cannot provide deeper analysis on its performance characteristics. Among open-source 5https://paperswithcode.com/sota/ text-summarization-on-dialogsum 6https://paperswithcode.com/sota/ text-summarization-on-samsum-corpus Translate-Summarize % Improvement Table 4 reports percentage improvements under the Translate-Summarize setting. Contrary to expectations, most models show little to no improvement, and in many cases, performance declines across all metrics. This suggests that translation introduces additional challenges, likely due to LLMs inability to preserve the structural and discourse-level nuances of code-switched input. Notably, Qwen2.57B and SEALLM-7B improve significantly for ENZH (27.1% and 19.25% in ROUGE-L), reflecting benefits from Mandarin or SEA-specific pretraining. However, similar gains are absent for EN-TA and EN-MS, indicating that CS translation remains unreliable outside of high-resource language pairs. LoRA and QLoRA Results We fine-tune LLMs on the synthetic CS-Sum-Syn dataset using LoRA and QLoRA to evaluate their ability to handle CS summarization beyond just metric gains. As shown in Table 5 7, EN-ZH sees the largest improvements (e.g., SEALLM-7B and Qwen2.5-7B exceed 200% in METEOR), EN-TA shows consistent gains, while EN-MS exhibits mixed results across metrics. These improvements are surprising given the distributional mismatch between CS-Sum and CSSum-Syn (Figure 2a, Table 3). qualitative review reveals that models often produce summaries with subtle semantic errors, indicating they may have learned surface-level summary structure rather than true code-switching comprehension. This motivates the deeper analysis in Section 5."
        },
        {
            "title": "5 Error Analysis",
            "content": "In this section, we present an in-depth analysis of the common error types exhibited by LLMs when summarizing CS dialogues. Our qualitative analysis across the EN-ZH, EN-TA, and EN-MS language pairs revealed three major failure modes: Code-Switching Loss (CSL): The summary primarily utilizes the English parts of the dialogue, ignoring or missing critical information from non-English segments. Meaning Shift from Poor Translation (MST): The model misunderstands the codeswitched segments, resulting in summaries that deviate from the dialogues true meaning. 7Due to space constraints, rest of the scores are presented in Table Figure 3: Wrong summary with high BERTScore 0.903 models, Gemma-2-9B consistently outperforms others across most metrics and language pairs. This aligns with findings from the Gemma-2 paper (Team et al., 2024), which attributes its strong multilingual performance to extensive exposure to diverse languages and an optimized tokenizer. Interestingly, SEA-Lion-Gemma-2-9B lags slightly behind, despite its regional specialization in Southeast Asian languages, suggesting that broader multilingual exposure may be more beneficial than region-specific pretraining for CS comprehension. One notable observation is the underperformance of most models on the EN-ZH pair, which is surprising given that all models were exposed to Mandarin during pretraining. Manual inspection reveals that some LLMs generate summaries partially or entirely in Chinese, despite being explicitly prompted to produce English outputs. This mismatch leads to lower scores on automated metrics, particularly those that penalize non-English outputs. However, as discussed in our fine-grained evaluation (Section 5), these summaries often contain fewer semantic or structural errors than those in EN-TA or EN-MS, indicating that the low metric scores may not reflect actual comprehension quality. We also find that smaller models, such as Gemma-2-2B and Qwen2.5-3B, perform significantly worse than their larger counterparts, reinforcing the impact of model scale on CS summarization. Surprisingly, SEALLM-7B performs comparably to these smaller models, despite being explicitly trained on Southeast Asian languages. Qualitative analysis of its outputs reveals several failure modes in the few-shot setting: (a) copying utterances directly from the dialogue, (b) generating descriptive paraphrases rather than true summaries, and (c) producing outputs in the wrong language. These behaviors suggest that SEALLM struggles with incontext learning and generalization, even within its intended linguistic domain. Model Lang ROUGE-L BERTScore SBERT-COSINE JACCARD METEOR SEALLM-7B LLaMA-3-8B Gemma-2-9B SEA-LION-Gemma-2-9B EN-ZH 50.45 / 51.94 EN-TA 47.99 / 43.82 34.20 / 17.03 EN-MS 11.09 / 11.21 4.49 / 4.52 3.29 / 2.53 EN-ZH 23.51 / 25.21 EN-TA 51.45 / 47.07 46.81 / 21.73 EN-MS EN-ZH -1.45 / 10.66 6.54 / 19.37 EN-TA 7.81 / -6.27 EN-MS 3.14 / 3.12 4.32 / 4.00 4.85 / 3.27 0.12 / 0.82 0.63 / 1.56 0.71 / -0.61 EN-ZH -22.93 / -6.64 EN-TA -21.48 / 3.59 EN-MS -21.98 / -8.08 -3.10 / -0.71 -2.61 / 0.24 -2.95 / -0.99 60.22 / 60.69 19.84 / 19.86 12.01 / 27.78 12.88 / 12.79 18.91 / 18.28 22.19 / 16. 4.10 / 2.32 7.14 / 9.12 3.60 / -1.41 -3.27 / 1.73 1.74 / 7.08 -3.53 / -0.77 170.47 / 160.94 75.18 / 69.14 47.49 / 4.22 163.13 / 142.51 47.14 / 35.42 13.08 / 27.78 29.85 / 34.26 66.19 / 60.27 50.56 / 23.01 8.56 / 11.61 18.12 / 27.89 8.48 / -3. -4.13 / 8.67 -3.49 / 20.38 -8.16 / 3.18 19.70 / 29.68 39.72 / 42.35 39.34 / 17.83 1.68 / 7.95 7.28 / 13.01 5.31 / -7.06 6.29 / 14.39 7.38 / 17.21 9.09 / 3.03 Table 5: LoRA / QLoRA % Improvement over Few-Shot on CS-Sum Speaker Misattribution (SMA): Summaries incorrectly assign statements to speakers, thus distorting intended meanings or misrepresenting participants views. Figure 4 illustrates CSL, demonstrating how the summary incorrectly emphasizes English segments and misinterprets speaker intentions. Further examples across categories are provided in Tables 9, 10, and 11. Errors were identified automatically via GPT-4o, detailed further in Appendix C. To quantify these errors and compare model behaviors, we report the percentage of summaries exhibiting each error type across 9 LLMs under both Few-Shot and LoRA settings  (Table 6)  . The analysis reveals the following: CSL persists as the dominant failure mode in Few-Shot evaluation. Across all 9 models and the three language pairs, CSL exceeds 50% in the Few-Shot setting  (Table 6)  . Even the best performer, SEA-LION-GEMMA-2-9B, attains minimum of 53.73% CSL on EN-ZH, confirming that LLMs systematically ignore non-English spans when summarizing code-switched dialogues, regardless of architecture or pre-training recipe. Scaling within model family does not reliably reduce errors. Comparing parameter-matched variants shows no monotonic gains: GEMMA-29B lowers CSL on EN-ZH (77.6159.70) yet raises Speaker Misattribution (SMA) on EN-TA by 7 pp over its 2B sibling, while QWEN2.5-7B improves CSL on EN-MS but leaves MST virtually unchanged. These inconsistencies indicate that sheer scale is not substitute for task-specific multilingual evaluation. EN-TA exhibits consistently higher error rates across models. All evaluated models show their highest rates of CSL and SMA on EN-TA dialogues (e.g., QWEN2.5-7B: 91.81% CSL, 57.31% SMA). This suggests that certain language-specific properties, such as morphological complexity and syntactic divergence from English, may contribute to increased summarization difficulty in this setting. SEA-oriented models excel in error analysis despite mediocre automatic scores. Regional models such as SEALLM-7B and SEA-LIONGEMMA-2-9B achieve the lowest CSL and SMA within their parameter class (e.g., 53.73% CSL on EN-ZH), yet trail larger English-specific models models on ROUGE and BERTScore. This divergence reinforces that conventional metrics do not capture CS comprehension, underscoring the need for better metrics. Synthetic fine-tuning amplifies errors under distribution shift. Fine-tuning on the Geminigenerated CS-Sum-Syn corpus degrades performance when the training distribution diverges from CS-Sum, most notably for EN-ZH, whose KL divergence is 2.48 versus 0.55 for the other pairs  (Table 3)  . After adaptation, SEA-LION-GEMMA2-9Bs CSL jumps from 53.73% to 83.94% and MST from 11.19% to 76.64%  (Table 6)  . These results illustrate that even synthetic data generated using high-performing LLM does not improve the models ability to comprehend code-switched input, and the failure to produce data that enhances CS understanding also underscores the limitations of current LLMs comprehensibility of CS."
        },
        {
            "title": "SMA",
            "content": "Few-Shot LoRA Few-Shot LoRA Few-Shot LoRA Gemma-2-2B Qwen2.5-3B Qwen2.5-7B SEALLM-7B Mistral-7B Ministral-8B LLaMA-3-8B Gemma-2-9B SEA-LION-Gemma-2-9B EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS EN-ZH EN-TA EN-MS 77.61 94.15 77.19 55.22 91.81 60. 71.64 91.81 78.95 64.18 91.23 68.42 55.97 88.30 67.54 76.12 95.03 78.95 74.63 90.94 73.68 59.70 71.35 59. 53.73 60.53 48.25 88.32 92.69 94.87 83.94 88.01 84.62 81.75 79.23 82.05 84.67 93.57 85.47 83.21 89.47 77. 85.40 90.94 95.73 71.17 56.92 62.91 85.40 82.75 79.60 83.94 84.50 86.32 60.45 86.26 62.28 52.24 89.47 49. 32.84 76.32 35.96 41.79 81.87 45.61 37.31 81.58 44.74 52.24 88.80 49.12 35.82 80.70 40.35 18.66 35.09 18. 11.19 27.78 14.04 90.51 90.06 84.62 69.34 75.15 60.68 48.91 58.64 42.74 70.80 71.64 69.23 69.34 79.82 70. 74.45 79.82 82.91 45.07 37.89 33.38 57.66 67.84 52.53 76.64 76.61 64.96 40.30 69.88 35.09 36.57 76.02 31. 9.70 57.31 16.67 21.64 63.16 23.68 22.39 66.37 28.07 31.34 66.37 20.18 16.41 55.85 14.04 8.21 22.22 8. 3.73 17.54 7.02 77.37 74.85 67.52 39.42 50.58 40.17 26.28 32.58 24.79 33.58 47.95 40.17 40.88 53.22 44. 48.91 55.56 57.26 21.35 19.93 17.33 35.77 46.20 29.45 52.55 49.71 46.15 Table 6: Comparison of error rates (%) in Few-Shot vs. LoRA fine-tuned models across different CS language pairs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced CS-Sum, the first CS dialogue-to-English summarization benchmark across three language pairs: EN-TA, EN-ZH, and EN-MS. Our experiments revealed that LLMs perform inconsistently across CS pairs, with some open-source models outperforming SEA-specific models, highlighting that CS data must be present in pretraining, not just added through fine-tuning. Furthermore, our analysis identified three major error typesCSL, MST, and SMApersisting across all models, demonstrating that current LLMs lack intrinsic CS comprehension. The quantitative results reinforce the severity of these failures, showing that fine-tuning alone does not fully resolve CS-specific challenges. To drive further research in multilingual NLP, we will release CS-Sum along with our codebase and evaluation framework. Figure 4: CSL error example"
        },
        {
            "title": "7 Limitations",
            "content": "While this work provides comprehensive evaluation of CS dialogue summarization through the CS-Sum benchmark, there are some limitations that should be addressed in future work. Focus on Summarization: Our experiments primarily focus on the task of summarization. However, code-switching also poses significant challenges for other long-context NLP tasks, such as machine translation, dialogue generation, and question answering. Extending the CS-Sum benchmark to these tasks would provide broader understanding of LLMs capabilities in handling CS data across different domains. Finetuning with Synthetic Data: The finetuning experiments in this paper are limited to synthetic code-switched data generated using Gemini-2. While this approach allowed us to assess the potential of fine-tuning for CS comprehension, it does not capture the complexities of real-world CS data."
        },
        {
            "title": "References",
            "content": "Gustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020. LinCE: centralized benchmark for linguistic code-switching evaluation. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 18031813, Marseille, France. European Language Resources Association. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Anshul Bawa, Pranav Khadpe, Pratik Joshi, Kalika Bali, and Monojit Choudhury. 2020. Do multilingual users prefer chat-bots that code-mix? lets nudge and find out! Proc. ACM Hum.-Comput. Interact., 4(CSCW1). Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021. DialogSum: real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 50625074, Online. Association for Computational Linguistics. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: efficient finetuning of quantized llms. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: humanannotated dialogue dataset for abstractive summaIn Proceedings of the 2nd Workshop on rization. New Frontiers in Summarization, pages 7079, Hong Kong, China. Association for Computational Linguistics. Gualberto Guzmán, Joseph Ricard, Jacqueline Serigos, Barbara E. Bullock, and Almeida Jacqueline Toribio. 2017. Metrics for modeling code-switching across corpora. In Interspeech 2017, pages 6771. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Muhammad Huzaifah, Weihua Zheng, Nattapol Chanpaisit, and Kui Wu. 2024. Evaluating code-switching In Protranslation with large language models. ceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 6381 6394, Torino, Italia. ELRA and ICCL. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825. Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury. 2020. GLUECoS: An evaluation benchmark for code-switched NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 35753585, Online. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, and Rajiv Ratn Shah. 2021. GupShup: Summarizing Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. open-domain code-switched conversations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6177 6192, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. 2024. SeaLLMs - large language models for Southeast Asia. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 294304, Bangkok, Thailand. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Long Phan, Rowena Garcia, Thamar Solorio, and Alham Fikri Aji. 2023. Prompting multilingual large language models to generate code-mixed texts: The case of south East Asian languages. In Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching, pages 4363, Singapore. Association for Computational Linguistics. Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and Alham Fikri Aji. 2023. Multilingual large language models are not (yet) code-switchers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1256712582, Singapore. Association for Computational Linguistics."
        },
        {
            "title": "A Synthetic Data Generation Process",
            "content": "To create synthetic CS-dialogue summarization dataset, we combined the training sets of DialogSum and SAMSum, resulting in 19,014 English dialogue-summary pairs. We used Gemini-2-flash to translate the English dialogues to code-switched dialogues in three CS pairs, as it is large-scale SOTA LLM with free-to-use API, making it practical choice for our experiments. We used the following prompt to convert English dialogues into CS dialogues. Each prompt was augmented with few-shot examples that were human generated. Example Prompt Code-Switching for Malay-English You are {Malay/Tamil/Chinese} person in your 20s. You are recruited for translating English dialogues to English-{Malay/Tamil/Chinese} code-switched dialogues. The code-switched dialogues should follow the same structure as the English dialogue. This is for educational purposes, so DO NOT include swear words in your translation. Return just the code-switched dialogue."
        },
        {
            "title": "B Experiment settings",
            "content": "B.1 Few-shot and Translate-Summarize The Few-Shot and Translate-Summarize settings evaluate the inherent CS comprehension of LLMs. Evaluating the LLMs in this setting gives us an idea of their performance without any additional training. Few-shot In the Few-Shot setup, we provide each LLM with three human-crafted examples containing code-switched dialogues and their corresponding summaries. The results from this setup, will quantify the inherent CS understanding of the LLMs for the three language pairs. LLMs for CS dialogue summarization. Given that full fine-tuning is computationally expensive, we use parameter-efficient fine-tuning (PEFT) techniques to assess whether task-specific adaptation improves CS comprehension. The models are finetuned on CS-Sum-Syn. Model 32 Ministral-8B 16 LLaMA-3-8B 64 Qwen2.5-3B 32 Gemma-2-9B SEA-Lion-Gemma-9B 32 8 Gemma-2-2B 64 Qwen-2.5-7B 32 Mistral-7B 64 SEALLM-7B α 32 16 64 32 32 8 64 32 LR 5e-5 5e-5 3e-5 5e-5 5e-5 5e-5 3e-5 5e-5 3e-5 Table 7: Hyperparameters used for LoRA and QLoRA fine-tuning. The fine-tuning process follows model-specific hyperparameter configurations, as shown in Table 7. The main hyperparameters include the rank parameter (r), scaling factor (α), and learning rate (lr). These configurations were chosen based on empirical observations to balance training stability and performance. All the models were trained for 4 epochs with batch size of 8 and gradient accumulation for 8 steps. We followed the cosine learning rate scheduling after warming up for the first 3% of steps. We used bf16 and gradient checkpointing for memory efficient training. All the LLMs were trained on an H100 GPU and the training lasted between 1 to 4 hours depending on the architecture and size of the LLM. In the Translate-Summarize TranslateSummarize approach, each CS dialogue is first translated into English before being summarized. This serves as baseline to determine whether the presence of code-switching affects summary quality. The translation step is performed by the same LLM, ensuring that the summarization model receives only monolingual input. B.2 LoRA and QLoRA In this subsection, we describe the fine-tuning process using LoRA and QLoRA to adapt open-source"
        },
        {
            "title": "C Error Analysis Process",
            "content": "The summaries generated by the LLMs typically exhibit the three major errors listed in Section 5. In this section, we describe the process we followed to detect these errors in the generated summaries. We used GPT-4o to analyse the summary given the CS-dialogue for different errors. For each error type, we used separate prompt to guide the classification process. The LLM was asked to analyse the summary with respect to the particular error type before providing verdict on presence of the error. Below, we present the prompts used for each category."
        },
        {
            "title": "Prompt for CSL",
            "content": "You will be given code-switched (CS) dialogue between two speakers and its generated summary. Identify whether the summary has ignored or removed key information from the non-English parts of the dialogue. The summary has an error if it meets one or more of the following criteria: Does not include important content from the CS part of the dialogue. The summary only relies on the English part of the dialogue."
        },
        {
            "title": "Prompt for SMA",
            "content": "You will be given code-switched (CS) dialogue and its generated summary. Analyze whether the summary has mixed up who said what. The summary has an error if it meets one or more of the following criteria: Has swapped roles or perspectives between speakers. Statements are wrongly assigned."
        },
        {
            "title": "Prompt for MST",
            "content": "You will be given code-switched (CS) dialogue and its generated summary. Analyze whether the summary distorts the meaning of the original conversation due to incorrect understanding of the code-switching. The summary has an error if it meets one or more of the following criteria: The summary misinterprets the dialogue due to poor translation. Intent of the speakers has changed because of poor understanding of CS phrase. Key points of the dialogue are misrepresented. Figure 5: Distribution b/w filtered CS-Sum-Syn and CS-Sum for EN-ZH"
        },
        {
            "title": "D Training on filtered data",
            "content": "The difference in the distribution of CS-Sum and CS-Sum-Syn for EN-ZH (see Figure 2a) is much more than the differences in distribution for ENMS and EN-TA. We hypothesize that this might be due to the tokenization strategy of the LLMs which is more robust for Roman script compared to Chinese script. The % improvement gains for EN-ZH after LoRA and QLoRA finetuning on ENZH is also less compared to the other two CS pairs  (Table 5)  . To check if filtering CS-Sum-Syn to match CSSums distribution would improve the finetuning results, we treat the CS metrics shown in Table 1 as multivariate gaussian. We calculated the mean µ and the covariance matrix Σ of CS-Sum and used the Mahalanobis distance (eqn. 1) to calculate the distance between the instances in the training set and the test set. DM (x) = (cid:113) (x µ)T Σ1(x µ) (1) We filtered the training data aggressively by selecting only the top 20 percentile of the training instances with the shortest distances. The filtered dataset had 3801 instances. The resulting distribution is shown in Figure 5 which matches CS-Sums distribution better than the unfiltered one (see Figure 2a). We finetuned (QLoRA) all the 9 open-source LLMs on the filtered data for 3 epochs (4 for the unfiltered one) with all the other parameters staying the same. Figure 6 shows the % improvement of LLMs finetuned on filtered data over the LLMs finetuned on the unfiltered one. Figure 6: Filtered data % improvement over CS-Sum-Syn for EN-ZH It can be seen that only 2 LLMs - Mistral 7B and SEA-Lion-Gemma-9B show actual improvement while other LLMs either dont improve much or have worse performances. Qwen2.5-7B suffers the worst when finetuned on filtered data. This suggests that filtering using Mahalanobis distance may only be beneficial for certain architectures while being detrimental to others. Further investigation is required to determine if alternative filtering thresholds or strategies could yield better generalization across LLMs."
        },
        {
            "title": "Lang",
            "content": "ROUGE-L"
        },
        {
            "title": "BERTScore",
            "content": "SBERT-COSINE"
        },
        {
            "title": "METEOR",
            "content": "Gemma-2-2B Qwen2.5-3B Qwen2.5-7B Mistral-7B Ministral-8B EN-ZH EN-TA EN-MS 9.13 / 22.26 21.52 / 23.29 16.59 / 7.98 EN-ZH 38.32 / 36.44 39.08 / 35.50 EN-TA 34.00 / 14.60 EN-MS 0.15 / 1.21 1.62 / 1.79 0.76 / 0.34 2.72 / 2.80 2.73 / 2.55 2.11 / 1.03 EN-ZH 40.89 / 39.07 22.86 / 20.85 EN-TA 9.71 / 2.06 EN-MS 11.95 / 12.20 1.83 / 1.72 0.74 / -0. 17.91 / 2.18 EN-ZH EN-TA -1.79 / -3.62 EN-MS -11.88 / 15.12 0.73 / -0.80 -2.20 / -2.85 -2.93 / 0.56 EN-ZH EN-TA EN-MS 9.92 / 13.16 18.38 / 20.88 -24.12 / 7.51 1.15 / 0.69 1.41 / 1.41 -2.73 / 0.63 3.52 / 6.23 12.10 / 11.86 4.82 / 1. 10.99 / 11.20 12.30 / 11.96 5.97 / 3.56 59.88 / 60.92 11.50 / 11.10 6.27 / 3.64 7.78 / 2.43 3.90 / 1.90 -1.21 / 2.93 9.58 / 9.85 13.73 / 14.13 -1.64 / 4.51 19.28 / 21.69 23.88 / 29.04 14.51 / 2.31 44.42 / 41.06 47.58 / 45.49 33.91 / 17. 15.47 / 15.78 19.16 / 20.98 8.16 / -4.92 21.43 / 21.29 30.90 / 33.84 16.09 / 10.70 217.69 / 208.36 32.72 / 30.17 12.16 / 1.82 234.54 / 239.63 35.66 / 36.26 11.38 / 5.36 28.10 / 15.19 8.93 / 6.68 -8.56 / 8.96 30.37 / 34.98 34.19 / 36.83 -8.07 / 9. 20.10 / 9.41 13.76 / 11.95 3.83 / -2.09 29.41 / 33.61 35.43 / 39.83 7.89 / 2.76 Table 8: LoRA / QLoRA % Improvement over Few-Shot on CS-Sum"
        },
        {
            "title": "Matthew says that\nhe will try the pill\nbut\nthe summary\nmentions that he\nhad tried it and feels\nbetter",
            "content": "Anna was the one who suggest Marcus to go at 4 PM which is wrongly attributed to Leslie Matthew: have cold. Need yall support. Its painful Lisa: Fight it. Drink orange juice. Make some soup Matthew: Ill try Lisa: believe in ... Matthew: Enakku cold irukku. Unga elllaroda support um vennum. Athu romba painful Lisa: Atha fight pannu. Orange juice kudi. Soup pannu Matthew: Naa try panren Lisa: Naa unna believe panran ... could but when and Anna: is anyone going to pick Mark from the airport? Marcus: where from? Anna: Sydney, Thursday at 3 Marcus: am or pm? :D Leslie: haha fortunately pm:D Marcus: hmm have meeting at 1. dont think can make it Leslie: well guess it will take him some time after landing, reclaiming luggage etc Anna: yeah reckon its fine if youre there at 4 Marcus: oh well ok then ... Anna 有 人 去 机 场 接Mark 吗 Marcus 我 可 以 但 是 什 么 时候去哪里呢 AnnaSydney星期四3 点 Marcusam 还是pm:D Lesliehaha 幸 运 的 是 下 午:D Marcushmm 我1 点 有 个 会 议我想我去不了了 Lesliewell 我 想after his landing 他 需 要 一 些 时 间reclaiming luggage etc Anna是的我认为如果你4 点到那里没问题了 Marcusoh 那好吧 ... Matthew is feeling unwell due to cold and an allergy. Lisa suggests that he drink orange juice and soup to feel better. Matthew tries it and feels bit better. Marcus originally plans to pick up Mark at the airport but has meeting at 1 PM and cant make it. Leslie suggests Marcus arrive after 4 PM to give Mark enough time to reclaim luggage. Anna will inform Mark about Marcuss new plan. Ken: You are an idiot. Greg: What? Ken: Yeah you are, want my money back Greg: Was your account hacked by some prankster? Ken: No, Im the prankster, just having laugh at your expense Greg: Well, you are an idiot too then XD Ken操你你pimp Greg什么 Ken操你我要回我的钱 Greg 你 的 账 户 被 恶 作 剧 者hacked 了吗 Ken不我才是恶作剧者 只是开个玩笑以你为代价 GregWell那你也操XD Ken and Greg were joking with each other. Ken played prank on Greg but now he wants his money back. Greg was not happy about it The dialogue is just friendly banter and theres actually no transaction of money. Table 9: Error examples"
        },
        {
            "title": "Error Explanation",
            "content": "... Large deposits of Person2: coal were found nearly and so many industries located themselves here. The village quickly grew into key industrial centre. Person1: As the city grew, it must have absorbed many village nearby. Person2: Yes, it did. The names of those village survive as the names of parts of the city. Person1: see. Are there any building more than 200 years old in your city? Person2: Oh, yes. Several of the buildings from the villages still survive. Many of them were inns for travelers and today survive as pubs. There was castle near one village, so our city has castle too. Person1: Really? So your city does have some old history after all. Alan: <file_photo> Alan: look what just found Robert: dude, thats just nasty and you know it Robert: it has no sugar, no taste, and additional cinnamon flavoring Alan: yeah, know - thats awesome Robert: you sir have very strange tastes :P Alan: well, and found perfect company for it <file_photo> Robert: oh, thats more like it! Robert: but does the whiskey go well with the cinnamon? flavored whiskey is the worst... Alan: Actually it does taste surprisingly well. The cinnamon is not overpowering. If you put enough whiskey that is ... ... Person2: 附近发现了大量coal. 许多工业都设在这里这个 村庄迅速发展成为一个重要 的industrial centre Person1: 随 着 城 市 发 展 它 一定absorbed了附近的许多村 庄 Person2: 是的这些村庄的名 字作为城市部分地区的名字 而留存下来 see. 你 的 城 市 里 Person1: 有 超 过200 年 历 史 的building 吗 村 庄 里 的 一 Person2: 些buildings仍然保存完好其 中许多都是旅客的inns如今 作为pubs保留了下来一个村 庄附近有一座castle所以我 们的城市也有一座castle Person1: 真的吗原来你们城 市也有悠久的history 啊 Alan: <file_photo> Alan: tengok ape aku jumpe Robert: dude, thats just nasty and you know it Robert: it has no sugar, no taste, and additional cinnamon flavoring Alan: yeah, know - itulah yang awesome Robert: you sir ada tastes pelik :P Alan: well, and found perfect company for it <file_photo> Robert: oh, thats more like it! Robert: Tapi does the whiskey go well with the cinnamon? flavored whiskey rabak... Alan: Actually it does taste surprisingly well. The cinnamon tak terlalu kuat. If you put enough whiskey that is ... Table 10: Error examples Person2 tells Person1 how their city grew and has some buildings over 200 years old. Person2 also introduces castle nearby The summary is based on only the English words of the dialogue and also Person2 talks about castle, does not introduce it summary The wrongly states that Alan has companion when actually he was referring to eating cinnamon along with it Alan shares photo of cinnamonflavored whiskey with Robert, who it initially finds disgusting. Alan defends the unique taste and shares another photo of him enjoying it with companion. Robert jokingly points out Alans unusual taste preferences but admits the cinnamon taste isnt overpowering when paired with enough whiskey. They reminisce about previous cherry flavor and agree it was better than the current sugar-free version."
        },
        {
            "title": "Error Explanation",
            "content": "Callan: Somethings wrong with my Samsung S8. Wade: What, exactly? Callan: All get is black screen. Wade: Did you drop it or something? Callan: think know, this has happened before. Wade: It did? What happened before? Callan: It overheats and then it takes while for it to restart again.. Wade: That sucks.. Be carefull with the battery, it can be dangerous. ... Person1: What do you know about our company? Person2: Well, as know this company is one of the largest and best in this field of business. It mainly produces toys for children. It employs more than 10, 000 people throughout the world. The president now is Mr. Jackson. The Shanghai branch was founded five years ago with staff of more than 2, 000. Person1: Very good. If you enter this company, what department would you like to work in? Person2: May work in sales department? Person1: OK. Oh, how much do you make at your current job, if you dont mind my asking? Person2: Im paid 2, 000 yuan per month plus some allowances and bonus as that come to about 3, 000 yuan month. How much will you pay me? Person1: Well, the starting salary for clerk in our company is 2, 500 yuan monthly and after three months he would get raise if his work is satisfactory. Person2: Do you have any fringe benefits? ... Callan: Samsung S8 aku ada problem. Wade: What, exactly? Callan: Aku asyik dapat black screen. Wade: Kau terjatuhkan ke apa? Callan: Aku rasa aku tahu kot, ni pernah happened kat aku. Wade: It did? What happened before? Callan: Dia overheat pastu it takes while for it to restart again.. Wade: That sucks.. Be carefull with the battery, boleh jadi bahaya. ... Person1: Enga company pathi enna therium ungaluku? Person2: Indha field indha company dhaan largest and best nu therium. Main focus vandhu children toys produce panradhu. Current president Mr. Jackson. Shanghai la 5 year munnadi oru branch start panirkeenga. Anga 2000 employees irukanga. Total ah 10000 employees. Person1: Very good. Inga job kedaicha, endha department la work pananum ungaluku? Person2: Sales department la? Person1: OK. Current job la evlo If you dont earn panreenga? mind my asking? Person2: Current salary 2000 yuan plus konjam allowances and bonus ku apram 3000 yuan per month. Neenga evlo pay panuveenga? Person1: Clerks ku starting salary 2500 yuan. 3 months ku apram raise varum. Person2: Fringe benefits edhavadhu iruka? ... Table 11: Error examples"
        },
        {
            "title": "Callan was the one\nwho suggested the\nphone might be\noverheating",
            "content": "The speaker, Callan, is having issues with his Samsung S8 as it keeps getting black screens. Wade suggests it might be overheating. Callan intends to take it to store as its still under warranty comLLM has pletely switched Person1 and Person2 in the first half of the summary Person2 is describing their current company which is the largest and in the field, best focusing on childrens toys. They have branch in Shanghai with 2000 employees, totaling 10,000 employees worldwide. Person1 in interested is position sales and asks about the salary and benelearning that fits, Person2 currently earns 3000 yuan per month with allowances and bonuses. Person1 explains the starting salary for clerks is 2500 yuan, with raise after three months, plus benefits like semiannual bonuses, Spring Festival bonus, and 4 weeks of small that"
        }
    ],
    "affiliations": [
        "Nanyang Technological University"
    ]
}