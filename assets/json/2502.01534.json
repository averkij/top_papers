{
    "paper_title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
    "authors": [
        "Dawei Li",
        "Renliang Sun",
        "Yue Huang",
        "Ming Zhong",
        "Bohan Jiang",
        "Jiawei Han",
        "Xiangliang Zhang",
        "Wei Wang",
        "Huan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage."
        },
        {
            "title": "Start",
            "content": "Preference Leakage: Contamination Problem in LLM-as-a-judge Dawei Li * 1 Renliang Sun * 2 Yue Huang 3 Ming Zhong 4 Bohan Jiang 1 Jiawei Han 4 Xiangliang Zhang 3 Wei Wang 2 Huan Liu 1 Abstract Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is widespread and challenging problem in the area of LLMas-a-judge. We release all codes and data at: https://github.com/David-Li0406/ Preference-Leakage1. 5 2 0 2 3 ] . [ 1 4 3 5 1 0 . 2 0 5 2 : r 1. Introduction advancements Recent in Large Language Models (LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong et al., 2024; Zhang et al., 2024a) have empowered various *Equal contribution 1Arizona State University 2University of California, Los Angeles 3University of Notre Dame 4University of Illinois Urbana Champaign. Correspondence to: Dawei Li <daweili5@asu.edu>. 1More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io/ 1 downstream tasks and applications. However, this also poses substantial challenges to the automatic evaluation of these models. Representatively, LLM-based AI agents focus transfer from traditional natural language processing tasks (Yang et al., 2023; Zhang et al., 2023) to real-world (Liu et al., 2023b; Huang et al., 2023), open-ended response generation (Wu et al., 2024), which greatly limits the applicability of traditional n-gram matching methods (e.g., BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu et al., 2016; Reiter, 2018) or model-based evaluators (Zhang et al., 2020; Zhong et al., 2022) for evaluation. To address these challenges, the paradigm of LLM-as-ajudge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a; Zhong et al., 2024; Li et al., 2025) has been proposed, designed to leverage LLM as evaluators to assess response quality. By combining powerful LLMs with well-designed prompting strategies, LLM-as-a-judge enables human-like evaluation of long-form and open-ended generation in more cost-efficient and scalable manner. However, recent studies point out some weaknesses of such assessment. For instance, Ye et al. (2024) explores various biases and vulnerabilities of LLM-as-a-judge, highlighting the importance of developing reliable and fair LLM-based evaluation system. In this work, we aim to introduce another concern in LLMas-a-JudgePreference Leakage. This issue arises when the LLMs used for data generation and evaluation are closely related, as illustrated in Figure 1. Synthetic data generated by LLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c) has become cornerstone of model training (Lee et al., 2025). When combined with LLM-as-a-Judge, they offer significant efficiency gains in model development. However, limited attention has been given to the potential contamination that occurs when the generator and evaluator LLMs share close relationship. During our preliminary study, we find this issue is particularly pervasive in popular LLMas-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant studies (more details can be found in Appendix A), due to the common reliance on the most advanced LLMs, such as GPT-4 (Achiam et al., 2023), for both data synthesis and evaluation to ensure the highest quality outputs. In our work, we reveal this relatednessakin to the overlap between training data and evaluation sets in traditional data Preference Leakage: Contamination Problem in LLM-as-a-judge contaminationwould introduce systematic bias of judge LLMs towards their related student models (i.e., the model distilled by the data generator which is related to the judge). Compared to other biases in LLM-as-a-Judge, such as length bias or egocentric bias (Ye et al., 2024; Panickssery et al., 2024), preference leakage is subtler and more challenging to detect, especially given that most LLMs do not disclose their training data. To investigate and reveal the preference leakage problem, we first define three relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Each of these scenarios is commonly encountered in real-world applications. Then, we pose and answer three core research questions about preference leakage: RQ1: Does preference leakage introduce systematic biases in LLM-based evaluation? To answer it, we conduct experiments with various LLM baselines in two widely recognized LLM-as-a-judge benchmarks, also introduce the preference leakage score to quantify the bias caused by preference leakage. The analysis results suggest an obvious bias of judging LLMs toward their related student models. RQ2: What is the severity of preference leakage under various scenarios? We conduct experiments under various relatedness settings, tuning techniques, and data mixing strategies to address it, finding that preference leakage consistently affects judge LLMs. Moreover, the severity of preference leakage correlates with the degree of relatedness between the data generator and LLM judges, as well as the proportion of synthetic data. RQ3: What are the underlying mechanisms causing preference leakage? For this question, we analyze LLMs recognition capabilities on their related student models generation as well as the distribution of bias across different question types and judgment dimensions. The analysis reveals that preference leakage is subtle, hard-to-detect issue, particularly affecting subjective questions and judgment dimensions. To summarize, our contributions in this work are as follows: We introduce preference leakage, contamination issue arising from the relatedness between the data generator and judge LLMs. We conduct extensive experiments across various LLMs and benchmarks to study how and to what extent the potential bias brought by preference leakage influences judgment. Our further analysis reveals that preference leakage is prevalent in diverse scenarios and difficult for judge LLMs to detect, providing valuable insights for future research on this challenging issue. 2. Related Work 2.1. LLM-as-a-Judge LLM-as-a-Judge, introduced by Zheng et al. (2023), leverages LLMs to automatically evaluate responses and assign rewards. This approach has gained widespread adoption in areas such as model alignment (Zhang et al., 2024d) and benchmarking (Liu et al., 2023a; Zhang et al., 2024b; Gao et al., 2023; Zhong et al., 2024), driving significant progress in the field. Building on this concept, Zhuge et al. (2024) proposed Agent-as-a-Judge, where agentic systems are employed to evaluate other agentic systems. Additionally, Prometheus, series of open-source LLMs tailored for LLM-as-a-Judge (Kim et al., 2023; 2024), addresses the prohibitive costs associated with proprietary models, further democratizing the technology. Despite its promising potential, recent studies have highlighted the vulnerabilities and limitations of LLM-as-aJudge. Notable concerns include biases during evaluation. For example, Zheng et al. (2023) identify position bias, where LLMs may favor responses based on their order in the input, thereby compromising fairness. Other studies (Ye et al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al., 2023; Huang et al., 2024) further emphasize the risks of evaluation biases. Thakur et al. (2024) assessed the judgment capabilities of LLM judges, finding that only the most advanced models align reasonably well with human evaluators. Moreover, recent study (Shi et al., 2024) revealed the susceptibility of LLM-as-a-Judge to adversarial attacks, leading to incorrect judgments. In this paper, we explore another critical vulnerability of LLM-as-a-Judgepreference leakagewhich poses additional risks to the reliability of this evaluation paradigm. 2.2. Data Leakage The possible overlap between training data and evaluation benchmarks has become central issue, since LLMs are usually trained on extensive web corpora (Dodge et al., 2021). This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment (Deng et al., 2024a; Jiang et al., 2024b). Several researchers have proposed methods to detect and mitigate data contamination. Deng et al. (2024b) proposed retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. Golchin & Surdeanu (2023) have developed guided instruction to flag contaminated instances. Dong et al. (2024b) proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs (Balloccu et al., 2024) and report contamination such as cross-language contamination (Yao Preference Leakage: Contamination Problem in LLM-as-a-judge Figure 1. Overview of preference leakage. We make comparison between data leakage and preference leakage and present three types of relatedness: being the same model, having an inheritance relationship and belonging to the same model family. et al., 2024) and task contamination (Li & Flanigan, 2024) that can evade traditional detection methods. To address data contamination issues, Ni et al. (2024) have used web user query detection and benchmark mixture. White et al. (2024) use the most recent information to update the problem. 3. Preference Leakage In this section, we first provide the formal definition of data contamination as the preliminary (Section 3.1). Based on the concept, we demonstrate how LLM-based data synthesis and evaluation can lead to the evolving preference leakage problem (Section 3.2). 3.1. Preliminary: Data Leakage Data leakage, also known as data contamination, refers to the inadvertent inclusion of information from the evaluation benchmarks into the training corpus thus creating an overlap between training and testing sets (Kaufman et al., 2012). This overlap would significantly influence the evaluation fairness by inflating the models performance since the model has prior exposure to and memorized information that its expected to generalize during testing (Elangovan et al., 2021). Formally, let represent the training corpus and be the evaluation set during test time. Data contamination occurs if: 3.2. From Data Leakage to Preference Leakage With the advent of LLMs, synthetic data generated by these models (Tan et al., 2024) has been widely adopted in various stages of model training, including pre-training, reinforcement learning with AI feedback (RLAIF) and supervised fine-tuning. Concurrently, the concept of LLM-asa-judge has emerged, where LLMs are employed to automate the evaluation process. While these LLM-as-an-oracle approaches reduce human effort in data annotation, significantly enhancing the efficiency and scalability of model training and evaluation, they also blur the lines between models and data, introducing evolving challenges (Shumailov et al., 2024; Dai et al., 2024). In this work, we examine the evolving contamination problem brought by LLM-as-a-oracle and formally propose the concept of preference leakage. This refers to situation in which the LLMs used for synthetic data generation and evaluation are related. Formally, we define this as: LLMG LLMJ = , (2) where LLMG and LLMJ denote the LLMs used for training data generation and evaluation. represents the relatedness between the two (sets of) LLMs. This relatedness may involve: Being the same model: the data generator and evaluator are the same model: = , (1) LLMG = LLMJ . (3) where denotes the intersection between the two sets. Such overlap violates the fundamental assumption that training and testing datasets should be disjoint to ensure an unbiased assessment of the models generalization ability. Inheritance relationship: one model is trained on synthetic data generated by the other: LLMG = Inherit(LLMJ ), LLMJ = Inherit(LLMG). (4) (5) 3 Preference Leakage: Contamination Problem in LLM-as-a-judge Within the same model family: the data generator and evaluator belong to the same model family (e.g., GPT family (Achiam et al., 2023) and Gemini family (Team et al., 2024)): 1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each two student models and obtain three model pairs. For each model pair, we perform the pairwise comparison using the three judge models respectively. LLMG, LLMJ FX . (6) Due to this relatedness, the preference of the judge models (e.g., format, style and wording) can be leaked to the student models through the synthetic data, resulting in non-trivial bias from the judge LLMs during the test time. 4. Main Experiment 4.1. Experiment Setup Models. We choose three powerful LLMs as data generator/ judge models. They are GPT-4o-2024-11-20 (Achiam et al., 2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student model, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and Qwen-2.5-14B (Yang et al., 2024). To avoid potential preference leakage due to distilling data from other LLMs during the instruction-tuning process, we choose to use the -PRETRAINED version rather than the -INSTRUCT version of these student models. Evaluation Datasets. We choose two representative pairwise evaluation datasets, Arena-Hard (Li et al., 2024e) and AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the trained student models. Arena-Hard includes 500 challenging questions in English. Additionally, the evaluation agreement between Arena-Hard and Chatbot Arena (Zheng et al., 2023)s hard prompts achieved 96.7% Spearman correlation, demonstrating the consistency of Arena-Hard with human preferences (Li et al., 2024e). AlpacaEval 2.0 is an improved evaluation method based on AlpacaEval (Li et al., 2023) and contains 805 questions. Compared to version 1.0, AlpacaEval 2.0 significantly reduces the effect of text length on the evaluation results. Implementation Details. In our main experiment, we examine the preference leakage introduced by using the same data generator and evaluator in supervised fine-tuning (SFT). We will discuss other relatedness and learning methods in Section 5. To obtain synthetic datasets, We first randomly sample 30,000 prompts from the Ultrafeedback dataset (Cui et al., 2024). The Ultrafeedback dataset includes instructions from several publicly available high-quality datasets such as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al., 2023), and Evol-Instruct (Xu et al., 2023). For each data generator model, we provide these prompts for them to produce synthetic responses, resulting in three synthetic instruction datasets. We then use each dataset to supervised fine-tune the student model, obtaining three different versions for each baseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-GeminiMetrics & Annotation Based on our hypothesis, preference leakage would lead to bias of judge LLMs towards their related student models. Following this principle, we design the preference leakage score PLS(i, j) to measure the bias in model pair (i, j) caused by preference leakage: PLS(i, j) = (cid:16) WR(i,i)AVG(i,j) AVG(i,j) (cid:17) + 2 (cid:16) WR(j,j)AVG(j,i) AVG(j,i) (cid:17) AVG(i, j) = WR(i, i) + WR(i, j) 2 . , (7) (8) Here WR(i, j) represents the win-rate score from judge model to student model j. Intuitively, large preference leakage score indicates that the two judge models demonstrate strong bias toward their related student models, suggesting significant preference leakage phenomenon. While our proposed preference leakage score quantifies the degree of preference leakage in each model pair, we also perform manual annotation to assess the preference leakage in each individual model. We randomly select 100 questions from AlpacaEval 2.0 and have three well-trained annotators perform pairwise comparisons independently for each response pair. After the annotation, the majority voting is applied to each response pair to get the final annotation results. More details about model training, metric explanation, and annotation process can be found in Appendix B. Model Mistral-7B Qwen-2.5-14B Data Generator/ Judge Pair Arena-Hard AlpacaEval 2.0 GPT-4o & Gemini-1.5 GPT-4o & LLaMA-3.3 LLaMA-3.3 & Gemini-1.5 GPT-4o & Gemini-1.5 GPT-4o & LLaMA-3.3 LLaMA-3.3 & Gemini-1.5 18.4% 1.4% 19.8% 18.6% 2.3% 18.4% 28.7% -6.7% 13.1% 37.1% 1.0% 25.4% Avg. 23.6% -2.7% 16.4% 27.9% 1.7% 21.9% Table 1. Preference leakage score result on Arena-Hard and AlpacaEval 2.0. The blue background indicates negative preference leakage score value and the purple background indicates positive value. The deeper the color, the larger the absolute value. 4.2. Main Results In our main experiment, we aim to provide insights into RQ1. Preference leakage exists in most model pairs. The original judgment results from Arena-Hard and AlpacaEval 2.0, along with the calculated preference leakage scores, are shown in Figure 2, Figure 3, and Table 1. As the results demonstrate, in most model pairs (except Mistral-GPT-4o vs 4 Preference Leakage: Contamination Problem in LLM-as-a-judge Figure 2. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard. Figure 3. Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in AlpacaEval 2.0. Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA3.3), the judge LLMs exhibit strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same. Figure 4. Comparison between GPT-4 and humans judgment for LLaMA-2 from MTBench. Evaluators bias towards certain LLMs can be inherited 5 by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-4o towards Mistral/ QwenLLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT4 evaluator has bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT4, we compare GPT-4s and human evaluators judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators bias can be inherited. In this case, GPT-4s bias toward LLaMA has been passed on to LLaMAs student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage more complex and challenging problem."
        },
        {
            "title": "Model pairs with similar performance tend to have more",
            "content": "Preference Leakage: Contamination Problem in LLM-as-a-judge Figure 5. Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0. obvious preference leakage. As shown in Table 1, we observe that the preference leakage scores for Mistral-GPT-4o vs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini1.5 (23.6% and 27.9% respectively) are consistently higher than that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and Qwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9% respectively). We think that this is largely due to the more comparable performance between the student models of GPT-4o and Gemini-1.5. Intuitively, when the quality of the two responses is similar, the evaluator may rely more heavily on its inherent preferences to make judgment, thereby exacerbating the preference leakage issue. Larger student models cause more bias from judge LLMs. Another observation from Table 1 is that the overall preference leakage score for Qwen-2.5-14B is higher than that for Mistral-7B. Drawing on insights from previous studies on data leakage, which suggest that larger and more powerful LLMs are more capable of memorizing extensive information and are thus more susceptible to data contamination (Bordt et al., 2024; Duan et al., 2024), we attribute this difference in preference leakage to the size and capabilities of the student LLMs. We assume that larger student models, due to their better performance and generalization abilities, are more capable of learning and memorizing the hidden preference pattern from the synthetic data, thus leading to more serious preference leakage. Different data generator/ judge LLMs result in varying degrees of bias under preference leakage. While we have concluded that student model pairs with similar performance or more powerful student models tend to exhibit greater preference leakage, we also examine whether different data generator and judge LLMs contribute to varying degrees of preference leakage. Analyzing the manual annotation results presented in Table 5, we observe that Gemini-1.5 shows strong bias toward its students, followed by GPT-4o, with LLaMA-3.3 displaying the least bias. This variation in preference leakage may stem from differences in the level of leaked preference in the synthetic responses generated by the data generator LLMs. For instance, an LLM with distinctive style or format in its responses offers more opportunities for student models to learn these characteristics, potentially leading to more pronounced preference leakage during evaluation. Future work could further quantify the extent of leaked preference for each data generator model. 5. Further Analysis In this section, we conduct relatedness analysis, learning method analysis and data mixing analysis (Section 5.1 - 5.3) to answer RQ2. Due to the cost consideration, we conduct these analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5. Moreover, we perform recognition analysis and category analysis to answer RQ3. Same Model Inheritance w/ same ins. Inheritance w/ different ins. Same Family w/ same series Same Family w/ different series Arena-Hard AlpacaEval 2.0 Avg. 23.6% 18.4% 28.7% 17.8% 18.3% 10.1% 3.3% 20.7% 19.3% 26.3% 22.3% 7.6% 2.2% 8.9% 2.8% Table 2. Preference leakage score in different relatedness between the data generator and the judging LLM. 5.1. Relatedness Analysis We demonstrate the impact of different relatedness conditions between the data generator and the judge LLM on the preference leakage problem, as shown in Table 2. Preference leakage under inheritance settings causes obvious bias of judges towards their related students. For the inheritance relationship, we consider the situation where the data generator is inherited from the judge model. We conducted the following two experiments: (1). we give the same instructions again as in the SFT stage (Inheritance w/ same ins.), or (2). we sample the same number of different instructions from the Ultrafeedback (Inherence w/ different ins.). Then, we let the fine-tuned Mistral model generate the answers and use these generated data to fine-tune new Mistral student model. From the results, with the same instructions, the average preference leakage score is 19.3%. In comparison, the score with different instructions is 22.3%. Preference Leakage: Contamination Problem in LLM-as-a-judge Firstly, in an inheritance setting, data generators can inherit judges preferences, which are then passed on to new student models, thereby compromising the fairness of their evaluation. Second, even when different instructions are used, judges preferences leaked to data generators can still be transferred to the new student model through synthetic data, leading to high preference leakage score. Models within the same series tend to cause more significant bias. For two models within the same family, we consider two settings: (1) Same series, where training data is generated by GPT-4o and Gemini-1.5-flash, and judged by GPT-4-turbo and Gemini-1.5-pro; (2) Different series, where training data is still generated by GPT-4o and Gemini1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro. In the same series setting, the average preference leakage score is 8.9%, indicating that despite using different models for data generation and judgment, their relatedness in terms of model family leads to some preference leakage. In contrast, the different series setting yields significantly lower leakage score of 2.8%, likely due to differences in architecture, training data, and other factors, reducing the influence of model-related biases in evaluation. Arena-Hard AlpacaEval 2.0 SFT DPO ICL 28.7% 7.7% -4.2% 18.4% 2.7% -1.1% Avg. 23.6% 5.2% -2.7% Table 3. Preference leakage score in different learning methods. 5.2. Learning Method Analysis We also compare three learning methods, supervised fine-tuning (SFT), direct preference optimization (DPO) (Rafailov et al., 2024), and in-context learning (ICL) (Dong et al., 2024a), to explore the different influences to them under preference leakage. We first build data pool based on human-written instruction-tuning data from OASST (Kopf et al., 2024), LIMA (Zhou et al., 2024), and MOSS (Sun et al., 2024b) to supervised fine-tune the pre-trained model. For DPO, we sample 2 responses for each instruction from sampled UltraFeedback instruction and prompt each data generator to produce the pairwise feedback. Then we use the DPO loss to further train the fine-tuned policy on each synthetic pairwise dataset. Appendix shows the prompt we use to craft synthetic pairwise feedback. For ICL, we sample 4 instruction-response pairs from each LLMs synthetic dataset as the demonstration during inference. Tuning approaches would leak judges preference to the student models. Various learning methods show significant differences in preference leakage scores across learning methods. SFT exhibits the highest average leakage score at 23.6%. In contrast, DPO achieves much lower score of 5.2%, likely because its focus on preferences helps minimize the unintended transfer of judge model biases. Meanwhile, ICL, which relies on contextual examples without updating model parameters, is least affected by the data generators preferences, resulting in the lowest leakage scores. Figure 6. Experiment results on data mixing. Manual represents the original synthetic data mixed with manually-written data. Synthetic represents the original data mixed with other synthetic data. 5.3. Data Mixing Analysis In real-world applications, synthetic data from single LLM is often mixed with manually-written data or other multisource synthetic data to train student models. To mimic these scenarios and explore how much synthetic data could lead to preference leakage, we conduct data mixing analysis. Specifically, we randomly sample 10%, 30%, 50%, and 70% from the original synthetic dataset and mix it with manually-written data and multi-source synthetic data, respectively, in order to maintain consistent total volume of training data (30,000). For the manually-written data, we sample from the data pool collected in Section 5.2. For the multi-source synthetic data, we use the original synthetic data from Ultrafeedback, which includes responses generated by various LLMs (e.g., WizardLM, Flcon, etc.). After obtaining the mixing training data, we train the student models using SFT and calculate their preference leakage scores based on the judgment results. Figure 6 presents the results with two mixing strategies across two benchmarks. The degree of preference leakage is directly proportional to the amount of synthetic data. We observe strong correlation between the proportion of synthetic data in the mixture and the preference leakage score, with no clear threshold separating cases with preference leakage from those without. This suggests that preference leakage can occur even with small amount of leaked synthetic data, posing significant challenges for its detection. 5.4. Can Judges Recognize Student Models? Previous studies demonstrate the LLM judges can recognize and thus prefer their own generation (Panickssery et al., 2024). In this work, we pose similar question: Does preference leakage also source from the LLM judges recognition 7 Preference Leakage: Contamination Problem in LLM-as-a-judge (a) Question Type (b) Judgment dimension Figure 7. Category analysis results on question type and judgment dimension. Task Student Recognition Response Classification Model GPT-4o Gemini-1.5 LLaMA-3.3 BERT Accuracy 60.0% 25.4% 54.2% 82.4% Table 4. Student recognition (binary classification) and response classification results (three-class classification). of their related student models generation? To study this, we follow Panickssery et al. (2024) to prompt the three judge LLMs and test whether they could recognize their related student models generation. Additionally, we split three student models generation into training and testing sets, and train BERT classifier to perform three-class classification inspired by the previous study on detecting human-AI text (Zhang et al., 2024c). Detailed instruction and training settings can be found in Appendix D. Judge LLMs do not show good performance in recognizing the generation of their student models. As the result presented in Table 4, we find that the recognition performance of each judge LLM in the content of related students is poor, with accuracy around the performance of random guess. Moreover, we observe no correlation between recognition performance and the preference leakage degree for judge LLMs. For instance, while Gemini-1.5 leads to the most preference leakage (as shown in Section 4.2), it performs the worst in recognition tasks. These suggest that preference leakage is subtler and harder-to-detect for judge LLMs, in contrast to the more obvious egocentric bias. Certain features embedded in student models through synthetic data. Although judge LLMs do not perform well in related student recognition, we notice the fine-tuned BERT classification demonstrates high accuracy score in classifier response generated by each student model. This suggests that certain characteristicssuch as style and formatare embedded in the student models through the synthetic responses. This finding further supports the existence of preference leakage and lays the groundwork for future research aimed at detecting and preventing it. 5.5. Impact on Question Type & Judgment Dimension In this section, we explore the impact of preference leakage across various question types and judgment dimensions. For the question type analysis, we first propose several general question types based on the question clusters introduced by Arena-Hard. Then, we prompt GPT-4o to map each question in Arena-Hard and AlpacaEval to one of the question types and calculate the preference leakage score for each question category. For the judgment dimension analysis, we follow the judgment dimensions introduced by Liu et al. (2023a) and also utilize GPT-4o to map the rationale generated by judge LLMs to one or multiple judgment dimensions. More detailed prompt can be found in Appendix E. The analysis results are presented in Figure 7. Subjective question and judgment dimension tend to lead to more bias. For question type analysis, we find objective questions with definitive answer, like mathematical ones, demonstrate the least preference leakage. By contrast, subjective questions that have more than one standard answer, such as programming and writing, usually lead to more obvious preference leakage. This observation is also applied to judgment dimension analysis, as objective dimensions (like completeness) have an overall lower leakage degree compared with subjective ones (like fairness). This suggests that preference leakage tends to be more significant in objective questions and dimensions, where the contaminated model is more likely to receive biased preference. 6. Conclusion In this work, we formally highlight the preference leakage problem in LLM-as-a-judge systems. The results of our main experiment, measured using the proposed preference leakage score, reveal clear bias in each judge toward its respective student model. We also observe that this bias is more pronounced in comparable model pairs and larger student models. Furthermore, we conduct additional analysis on various factors, including the relationship between the data generator and judge LLMs, model tuning tech8 Preference Leakage: Contamination Problem in LLM-as-a-judge niques, and data mixing strategies. Our findings suggest that preference leakage can cause significant bias across diverse scenarios. Finally, through recognition and category analyses, we investigate the underlying mechanisms of preference leakage, demonstrating that it is challenging and hard-to-detect issue, especially in subjective questions and judgment dimensions. In the future, we aim to explore methods for detecting, preventing, and mitigating this evolving challenge in LLM-as-a-judge systems."
        },
        {
            "title": "Impact Statements",
            "content": "By revealing preference leakage, this work could help build more trustworthy and ethically grounded AI systems. The relatedness between data generators and evaluators can systematically bias evaluations, potentially compromising the fairness and reliability of the automatic evaluation paradigm. These biased evaluations may indirectly affect downstream tasks such as AI alignment and decision-making systems, leading to unintended ethical risks. To mitigate preference leakage, we hope that researchers will propose more reliable evaluation methods, diversify training data sources, and develop contamination-resistant benchmarks in the future."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774. Balloccu, S., Schmidtova, P., Lango, M., and DuË‡sek, O. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6793, 2024. Bordt, S., Nori, H., and Caruana, R. Elephants never forget: Testing language models for memorization of tabular data. In NeurIPS 2023 Second Table Representation Learning Workshop, 2024. Chen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669, 2024. Cui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y., Xie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting language models with scaled ai feedback. In Forty-first International Conference on Machine Learning, 2024. Dai, S., Xu, C., Xu, S., Pang, L., Dong, Z., and Xu, J. Bias and unfairness in information retrieval systems: New In Proceedings of the 30th challenges in the llm era. ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 64376447, 2024. Deng, C., Zhao, Y., Heng, Y., Li, Y., Cao, J., Tang, X., and Cohan, A. Unveiling the spectrum of data contamination in language models: survey from detection to remediation. arXiv preprint arXiv:2406.14644, 2024a. Deng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A. Investigating data contamination in modern benchmarks for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 86988711, 2024b. Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. Documenting large webtext corpora: case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 12861305, 2021. Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., et al. survey on incontext learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 11071128, 2024a. Dong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and Li, G. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938, 2024b. Duan, S., Khona, M., Iyer, A., Schaeffer, R., and Fiete, I. R. Uncovering latent memories: Assessing data leakage and memorization patterns in large language models. arXiv preprint arXiv:2406.14549, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Elangovan, A., He, J., and Verspoor, K. Memorization vs. generalization: Quantifying data leakage in nlp performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 13251335, 2021. Gan, R., Wu, Z., Sun, R., Lu, J., Wu, X., Zhang, D., Pan, K., Yang, P., Yang, Q., Zhang, J., et al. Ziya2: Data-centric learning is all llms need. arXiv preprint arXiv:2311.03301, 2023. Preference Leakage: Contamination Problem in LLM-as-a-judge Gao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan, X. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554, 2023. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. Golchin, S. and Surdeanu, M. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493, 2023. Hu, S., Luo, Y., Wang, H., Cheng, X., Liu, Z., and Sun, M. Wont get fooled again: Answering questions with false premises. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 56265643, 2023. Huang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y., Zhou, P., Wan, Y., Gong, N. Z., et al. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128, 2023. Huang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y., Gao, C., Huang, Y., Lyu, W., Zhang, Y., et al. Position: Trustllm: Trustworthiness in large language models. In International Conference on Machine Learning, pp. 2016620270. PMLR, 2024. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, B., Li, D., Tan, Z., Zhou, X., Rao, A., Lerman, K., Bernard, H. R., and Liu, H. Assessing the impact of conspiracy theories using large language models. arXiv preprint arXiv:2412.07019, 2024a. Jiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang, S., Han, J., and Koyejo, S. Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059, 2024b. Kaufman, S., Rosset, S., Perlich, C., and Stitelman, O. Leakage in data mining: Formulation, detection, and avoidance. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(4):121, 2012. Kim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun, S., Shin, S., Kim, S., Thorne, J., et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2023. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Koo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M., Benchmarking cognitive biases in and Kang, D. large language models as evaluators. arXiv preprint arXiv:2309.17012, 2023. Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi, A., et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In Forty-first International Conference on Machine Learning, 2024. Lee, S., Zhou, J., Ao, C., Li, K., Du, X., He, S., Liu, J., Yang, M., Wen, Z., and Ni, S. Distillation quantification for large language models. arXiv preprint arXiv:2501.12619, 2025. Li, C. and Flanigan, J. Task contamination: Language models may not be few-shot anymore. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1847118480, 2024. Li, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y., Chen, C., Wu, T., et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a. Li, D., Tan, Z., Chen, T., and Liu, H. Contextualization distillation from large language model for knowledge graph completion. arXiv preprint arXiv:2402.01729, 2024b. Li, D., Yang, S., Tan, Z., Baik, J. Y., Yun, S., Lee, J., Chacko, A., Hou, B., Duong-Tran, D., Ding, Y., et al. Dalk: Dynamic co-augmentation of llms and kg to answer alzheimers disease questions with scientific literature. arXiv preprint arXiv:2405.04819, 2024c. Li, D., Tan, Z., and Liu, H. Exploring large language models for feature selection: data-centric perspective. ACM SIGKDD Explorations Newsletter, 26(2):4453, 2025. Li, M., Chen, L., Chen, J., He, S., Gu, J., and Zhou, T. Selective reflection-tuning: Student-selected data recycling for llm instruction-tuning. arXiv preprint arXiv:2402.10110, 2024d. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024e. 10 Preference Leakage: Contamination Problem in LLM-as-a-judge Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models, 2023. Lin, C.-Y. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214 3252, 2022. Liu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin, L., and Pineau, J. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 21222132, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1230. URL https://aclanthology.org/D16-1230. Liu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. Liu, X., Lei, X., Wang, S., Huang, Y., Feng, Z., Wen, B., Cheng, J., Ke, P., Xu, Y., Tam, W. L., et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023a. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023b. Ni, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neubig, G., and You, Y. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. arXiv preprint arXiv:2406.06565, 2024. Panickssery, A., Bowman, S. R., and Feng, S. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076, 2024. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Reiter, E. structured review of the validity of BLEU. Computational Linguistics, 44(3):393401, 2018. doi: 10. 1162/coli 00322. URL https://aclanthology. org/J18-3002. Shi, J., Yuan, Z., Liu, Y., Huang, Y., Zhou, P., Sun, L., and Gong, N. Z. Optimization-based prompt inIn Proceedings of jection attack to llm-as-a-judge. the 2024 on ACM SIGSAC Conference on Computer and Communications Security, CCS 24, pp. 660674, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400706363. doi: 10.1145/ 3658644.3690291. URL https://doi.org/10. 1145/3658644.3690291. Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R., and Gal, Y. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Sun, R., Liu, M., Yang, S., Wang, R., He, J., and Zhang, J. Fostering natural conversation in large language models with nico: natural interactive conversation dataset. arXiv preprint arXiv:2408.09330, 2024a. Sun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Liu, X., Yan, H., Shao, Y., Tang, Q., Zhang, S., Zhao, X., Chen, K., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y., Li, L., Yang, X., Wu, L., Yin, Z., Huang, X., Jiang, Y.-G., and Qiu, X. Moss: An open conversational large language model. Machine Intelligence Research, ISSN 2731-5398. URL https://github. 2024b. com/OpenMOSS/MOSS. Tan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee, A., Karami, M., Li, J., Cheng, L., and Liu, H. Large language models for data annotation and synthesis: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 930957, 2024. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Thakur, A. S., Choudhary, K., Ramayapally, V. S., Vaidyanathan, S., and Hupkes, D. Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges. arXiv preprint arXiv:2406.12624, 2024. 11 Preference Leakage: Contamination Problem in LLM-as-a-judge Tong, Y., Li, D., Wang, S., Wang, Y., Teng, F., and Shang, J. Can llms learn from previous mistakes? investigating llms errors to boost for reasoning. arXiv preprint arXiv:2403.20046, 2024. Wang, S., Tong, Y., Zhang, H., Li, D., Zhang, X., and Chen, T. Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment. arXiv preprint arXiv:2411.10914, 2024. White, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain, S., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S., et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. Wu, S., Huang, Y., Gao, C., Chen, D., Zhang, Q., Wan, Y., Zhou, T., Zhang, X., Gao, J., Xiao, C., et al. Unigen: unified framework for textual dataset generation using large language models. arXiv preprint arXiv:2406.18966, 2024. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, S., Sun, R., and Wan, X. new dataset and empirical study for sentence simplification in chinese. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 83068321, 2023. Yao, F., Zhuang, Y., Sun, Z., Xu, S., Kumar, A., and Shang, J. Data contamination can cross language barriers. arXiv preprint arXiv:2406.13236, 2024. Ye, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N., Gao, T., Geyer, W., Huang, C., Chen, P.-Y., et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. Zhang, H., Li, D., Li, Y., Shang, C., Shi, C., and Jiang, Y. Assisting language learners: Automated trans-lingual definition generation via contrastive prompt learning. arXiv preprint arXiv:2306.06058, 2023. Zhang, H., Shang, C., Wang, S., Zhang, D., Yao, F., Sun, R., Yu, Y., Yang, Y., and Wei, F. Shifcon: Enhancing non-dominant language capabilities with shift-based contrastive framework. arXiv preprint arXiv:2410.19453, 2024a. Zhang, H., Wu, Y., Li, D., Yang, Z., Zhao, R., Jiang, Y., and Tan, F. Balancing speciality and versatility: coarse to fine framework for supervised fine-tuning large language model. arXiv preprint arXiv:2404.10306, 2024b. Zhang, Q., Gao, C., Chen, D., Huang, Y., Huang, Y., Sun, Z., Zhang, S., Li, W., Fu, Z., Wan, Y., and Sun, L. LLM-as-a-coauthor: Can mixed human-written and machine-generated text be detected? In Duh, K., Gomez, H., and Bethard, S. (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 409 436, Mexico City, Mexico, June 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-naacl.29. URL https://aclanthology. org/2024.findings-naacl.29/. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. Zhang, X., Peng, B., Tian, Y., Zhou, J., Jin, L., Song, L., Mi, H., and Meng, H. Self-alignment for factuality: Mitigating hallucinations in LLMs via selfevaluation. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19461965, Bangkok, Thailand, August 2024d. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.107. URL https: //aclanthology.org/2024.acl-long.107/. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Zhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P., Zhu, C., Ji, H., and Han, J. Towards unified multidimensional evaluator for text generation. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 20232038. Association for Computational Linguisdoi: 10.18653/V1/2022.EMNLP-MAIN. tics, 2022. 131. URL https://doi.org/10.18653/v1/ 2022.emnlp-main.131. Zhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu, C., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024. 12 Preference Leakage: Contamination Problem in LLM-as-a-judge Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Zhuge, M., Zhao, C., Ashley, D., Wang, W., Khizbullin, D., Xiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian, Y., et al. Agent-as-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024. 13 Preference Leakage: Contamination Problem in LLM-as-a-judge A. Preliminary Study of Preference Leakage in Real World In our preliminary study, we investigate whether preference leakage is real-world issue in mainstream leaderboards and benchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for various LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g., Vicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is pervasive issue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation (Yang et al., 2023; Liu et al., 2024; Lee et al., 2024; Li et al., 2024d; Wang et al., 2024; Sun et al., 2024a). All of these suggest preference leakage to be widespread problem in both LLM-as-a-judge datasets and LLM-relevant research. B. Experiment Details B.1. Training Details We use LLaMA-Factory (Zheng et al., 2024), an efficient LLM tuning library for our experiment. The maximum sequence length is set to 1024 tokens, and cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data preprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use per-device batch size of 2, with gradient accumulation over 2 steps to simulate larger batch size for SFT and per-device batch size of 1, with gradient accumulation over 4 steps to simulate larger batch size for DPO. The learning rate is set to 1.0e-5 and each model will be trained for 3 epochs. cosine learning rate scheduler is used with warmup ratio of 0.1 to gradually increase the learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining numerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8."
        },
        {
            "title": "Judge Model",
            "content": "Mistral-GPT-4o vs Mistral-Gemini-1.5 Mistral-GPT-4o Wins Mistral-Gemini-1.5 Wins GPT-4o Gemini-1.5 Preference Leakage Score 55.1% 36.8% 44.9% 63.2% 18.4% Table 5. case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage score is calculated. B.2. Detailed Explanation for Preference Leakage Score We present case in Table B.1 to show how we calculate the preference leakage score for the Mistral-GPT-4o vs MistralGemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate: AVG(Mistral-GPT-4o, Mistral-Gemini-1.5) = AVG(Mistral-Gemini-1.5, Mistral-GPT-4o) = 55.1 + 36.8 63.2 + 44.9 2 = 45.95% = 54.05% (9) (10) After that, we calculate the preference leakage score: PLS(Mistral-GPT-4o, Mistral-Gemini-1.5) = (cid:0) 55.145.95 45.95 (cid:1) + (cid:0) 63.254.05 54.05 (cid:1) = 18.4% (11) . B.3. Manual Annotation Details We randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise comparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an annotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and Preference Leakage: Contamination Problem in LLM-as-a-judge downloading annotation results (Figure 8). After annotation, we adopt the majority voting to get the final label for each response pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating relatively consistent annotation result. C. Learning Method Analysis Details The table below presents the prompt we use to generate synthetic pairwise feedback from each model."
        },
        {
            "title": "Pairwise Feedback Prompt",
            "content": "You will be given assistant As answer, Your job is to evaluate which assistants answer is You should independently solve the user question step-by-step first. Identify and correct Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. consider correctness and helpfulness. and assistant Bs answer. better. Then compare both assistants answers with your answer. any mistakes. responses were presented does not influence your decision. of the responses to influence your evaluation. assistants. Be as objective as possible. your final verdict by strictly following this format: better, \"[[B]]\" if assistant is better. Avoid any position biases and ensure that the order in which the After providing your explanation, output Do not favor certain names of the \"[[A]]\" if assistant is Do not allow the length Your evaluation should ## Instruction: [The Start of Assistant As Answer] [RESPONSE A] [The End of Assistant As Answer] [The Start of Assistant Bs Answer] [RESPONSE B] [The End of Assistant Bs Answer] Please output the generated content in json format, for example: { \"reason\": string, reasons behind the chosen preferred answer \"prefered answer\": the prefered answer you selected, [[A]] or [[B]] } // string, // Formatted the abovementioned schema and produce the reason and preferred answer: D. Recogniton Analysis Details The table below presents the prompt we use for the recognition analysis."
        },
        {
            "title": "Recognition Prompt",
            "content": "Given an instruction and response, your task is to judge whether this response is generated by model that is trained on synthetic dataset you produced (your student model). ## Instruction: [INSTRUCTION] ## Response: [Response] Please output the generated content in json format, for example: string, reasons behind the judgment \"judgment\": generated by your student model, choose from yes or no // string, whether the answer is \"reason\": // Formatted the abovementioned schema and produce the reason and judgment: For response classification, we split all the response from three student models into training (80%) and testing (20%) subsets. Preference Leakage: Contamination Problem in LLM-as-a-judge Figure 8. The annotation tool we develop for annotation efficiency. Then, we finetune BERT-base-uncased model in the training set. The model is trained for 3 epochs with learning rate of 2e-5, batch size of 16 for both training and evaluation, and weight decay of 0.01, with evaluations conducted at the end of each epoch. E. Category Analysis Details The tables below present the prompt we use for question type and judgment dimension cateogory analysis."
        },
        {
            "title": "Question Type Categorization Prompt",
            "content": "Given question, please categorize it to one of the following categories: 1. Computer Science & Programming 2. Mathematics & Statistics 3. Science & Engineering 4. Business & Finance 5. Writing & Communication 6. Social & Daily Life 7. Others ## Question: [QUESTION] Please output the generated content in json format, for example: { \"question category\": Programming\" } // string, specific category name, such as \"Computer Science & Formatted the abovementioned schema and categorize the given question: 16 Preference Leakage: Contamination Problem in LLM-as-a-judge"
        },
        {
            "title": "Judgment Dimension Categorization Prompt",
            "content": "Given pairwise comparison judgment made by an AI, please categorize each considered aspect in the rationale to one of the following categories: { \"Factuality\": on reliable facts and data.\", \"Whether the information provided in the response is accurate, based \"User Satisfaction\": provides comprehensive and appropriate answer to the question.\", \"Whether the response meets the users question and needs, and \"Logical Coherence\": \"Whether the response maintains overall consistency and logical coherence between different sections, avoiding self-contradiction.\", \"Richness\": \"Whether the response includes rich info, depth, context, diversity, detailed explanations and examples to meet user needs and provide comprehensive understanding.\", \"Creativity\": insights or solutions.\", \"Whether the response is innovative or unique, providing novel \"Fairness and Responsibility\": response is feasible, carries acertain degree of responsibility, and considers potential risks and consequences.\", \"Whether the advice or information provided in the \"Completeness\": to meet the users needs, and whether it avoids omitting important aspects.\", \"Whether the response provides sufficient information and details \"Clarity\": concise language and structure so that the user can easily understand it.\", \"Whether the response is clear and understandable, and whether it uses \"Others\": } \"Other aspects which is not listed above.\" ## Judgment: [JUDGMENT] Please output the generated content in json format, for example: { \"Factuality\": // list, all aspects that belong to this category, such as [\"correctness\", \"mistakes\"] ... } Formatted the abovementioned schema and categorize aspects in the judgment:"
        }
    ],
    "affiliations": [
        "Arizona State University",
        "University of California, Los Angeles",
        "University of Illinois Urbana Champaign",
        "University of Notre Dame"
    ]
}