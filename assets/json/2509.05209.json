{
    "paper_title": "Hunyuan-MT Technical Report",
    "authors": [
        "Mao Zheng",
        "Zheng Li",
        "Bingxin Qu",
        "Mingyang Song",
        "Yang Du",
        "Mingrui Sun",
        "Di Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic."
        },
        {
            "title": "Start",
            "content": "2025-09-10 Hunyuan-MT Technical Report"
        },
        {
            "title": "Abstract",
            "content": "In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MTChimera-7B, translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic. Hunyuan-MT-7B: https://huggingface.co/tencent/Hunyuan-MT-7B Hunyuan-MT-Chimera-7B: https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B Code Repository: https://github.com/Tencent-Hunyuan/Hunyuan-MT 5 2 0 2 9 ] . [ 2 9 0 2 5 0 . 9 0 5 2 : r Figure 1: Benchmark performance of Hunyuan-MT models and state-of-the-art baselines."
        },
        {
            "title": "Introduction",
            "content": "Machine Translation (MT) has emerged as both critically important practical application and one of the most formidable research challenges that the computational linguistics community has pursued over the past several decades (Brown et al., 1990; 1993; Papineni et al., 2002; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). The recent advent and rapid advancement of Large Language Models (LLMs) have revolutionized the learning paradigm underlying MT systems, catalyzing shift from traditional rule-based and statistical approaches toward sophisticated large-scale neural learning methodologies (Zhu et al., 2024; Kocmi et al., 2024; Pang et al., 2025). This continuous technological evolution of LLMs has dramatically pushed the boundaries of achievable translation quality to unprecedented levels, with state-of-the-art models such as GPT-4.1 (OpenAI, 2025), Gemini-2.5-Pro (DeepMind, 2025), and Claude-Sonnet-4 (Anthropic, 2025), demonstrating remarkable capabilities that exceed the performance of expert human translators across specific language pairs. Nevertheless, significant challenges persist (Pang et al., 2025), particularly in translating non-literal language, such as internet neologisms, slang, and specialized terminology, as well as place names. Furthermore, prevailing bias within MT research favors high-resource language pairs, leaving translation for low-resource and minority languages critically under-resourced. The translation between Chinas minority languages and Mandarin constitutes particularly acute manifestation of this neglect. Beyond its technical dimensions, facilitating high-quality translation in this context is pivotal for promoting social inclusion, preserving cultural heritage, and ensuring equitable access to essential services and information for minority communities (Hu et al., 2019; Lin & Jackson, 2021). Despite this pressing societal imperative, this specific domain represents significant lacuna within the MT field. Addressing these issues requires more than robust linguistic comprehension; it necessitates the ability to generate expressions that are both culturally resonant and idiomatically natural, thereby transcending literal word-for-word translation. Meanwhile, notable performance disparity remains between proprietary and open-source models, gap often attributed to the comparatively limited scale of open-source systems. This problem is compounded by scarcity of well-defined methodologies for developing advanced LLM-based MT systems, which impedes the broader communitys efforts to deploy and refine effective solutions (Jiao et al., 2023; Pang et al., 2025; Kocmi et al., 2024; Cheng et al., 2025). Through extensive evaluations on representative MT benchmarks, Hunyuan-MT demonstrates superior performance, outperforming not only translation-specialized models of comparable size and prominent closed-source systems, such as Google-Translator, but also range of larger LLMs, as detailed in Figure 1. Furthermore, it is noteworthy that our model demonstrates significant superiority over all state-of-the-art LLMs on the task of translation between Chinas ethnic minority languages and Mandarin Chinese (MinorityMandarin Translation). In this technical report, we introduce Hunyuan-MT, the culmination of our ongoing efforts to develop more effective LLM-based multilingual translation models. Below, we show the main contributions of this technical report: 1. Hunyuan-MT-7B. We present Hunyuan-MT-7B, novel, open-source multilingual translation model with 7B parameters, which facilitates bidirectional translation among diverse set of 33 languages. Through extensive benchmarking, our model demonstrates SOTA performance against existing models in the approximate parameter size. Furthermore, key feature of Hunyuan-MT-7B is its robust support for low-resource language pairs, specifically enabling translation between Mandarin and several ethnic minority languages as well as dialects. 2. Hunyuan-MT-Chimera-7B. We propose new paradigm for high-quality machine translation, embodied in our model, Hunyuan-MT-Chimera-7B. As the first open-source weak-to-strong fusion model of its kind, it is architected for slow thinking translation tasks where quality is paramount. Unlike traditional methods that rely on single decoding path or unstructured reasoning like CoT, our approach involves two-stage process. First, base model (Hunyuan-MT-7B) generates portfolio of diverse translation candidates. Second, Hunyuan-MT-Chimera-7B, an expert model trained specifically for this purpose, synthesizes these weaker candidates into single strong output. This learned synthesis mechanism yields translations of quality unattainable by any individual candidate and demonstrates superior performance over contemporary CoT-based models. 3. Training Recipe. We introduce simple yet effective training framework that progressively refines the model through sequence of several distinct stages. Our methodology begins with foundational knowledge acquisition via general and MT-oriented pre-training, followed by task-specific adaptation using Supervised Fine-Tuning (SFT). Crucially, we then employ two successive optimization phases: Reinforcement Learning (RL) stage and an advanced weak-to-strong RL stage. 4. MandarinMinority Translation. To the best of our knowledge, this work represents the first systematic effort to optimize bidirectional translation performance of Mandarin-Kazakh, MandarinUyghur, Mandarin-Mongolian, and Mandarin-Tibetan. Our empirical evaluation demonstrates that 2 the resulting models significantly outperform leading contemporary LLMs on these translation directions, achieving the state-of-the-art performance. The remainder of this paper is organized as follows: we first detail the training methodology for HunyuanMT series models, then present experimental results for their pre-trained and post-trained variants, and finally, conclude with discussion of key findings and future research directions."
        },
        {
            "title": "2 Pre-training Stage",
            "content": "In this section, we describe the details of our pre-training approach and present experimental results from evaluating the base models on standard benchmarks."
        },
        {
            "title": "2.1 General Pre-training",
            "content": "For the foundational pre-training stage, we employ multilingual corpus that jointly trains on data from Chinese and English, supplemented by substantial collection of low-resource languages. The multilingual data component comprises 1.3 trillion tokens, encompassing 112 languages and dialects (excluding Chinese and English) sourced from wide array of domains. To govern the quality of this diverse dataset, we developed and implemented proprietary quality assessment model. This model evaluates text along three key dimensions: Knowledge Value, which measures informational density and accuracy; Authenticity, which verifies the genuineness of the content; and Writing Style, which appraises linguistic quality and coherence. Each dimension is scored on three-point scale (0, 1, 2). weighted composite score is then calculated, where the weighting of these dimensions is strategically adjusted based on the provenance of the data. For instance, for data sourced from academic literature, books, and professional websites, we assign higher weight to the Knowledge Value dimension, prioritizing content that achieves the maximum score of 2. To maintain and control for content diversity within the training data, we established tripartite taxonomic framework for data categorization and balancing. This framework facilitates both data filtering and proportional adjustments within the training mixture: Disciplinary Tagging System This system categorizes data by academic discipline, enabling balanced distribution of subjects within the corpus. Industry Tagging System (24 categories) Comprising 24 distinct categories, this system ensures comprehensive coverage across various industrial sectors. Content Theme Tagging System (24 categories) This system serves dual purpose: it supports broad thematic diversity while also enabling the targeted exclusion of undesirable content, such as materials related to gambling or advertising. The integration of this quality assessment model with the tripartite taxonomic framework constitutes comprehensive data governance strategy. This strategy ensures the final training corpus is characterized by both high quality and broad diversity. The application of this curated data processing pipeline resulted in the pre-training of the Hunyuan-7b-Base1 model."
        },
        {
            "title": "2.2 MT-oriented Pre-training",
            "content": "During the MT-oriented pre-training stage, we incorporate curated mixture of monolingual and bilingual corpora. The monolingual data primarily comes from the mC4 (Raffel et al., 2019) and OSCAR (Ortiz Suarez et al., 2020; Ortiz Suarez et al., 2019) datasets. We subject this data to rigorous cleaning pipeline that includes language identification with fastText 2, document-level deduplication via minLSH, and quality filtering with KenLM-based model 3 to remove high-perplexity documents. For the bilingual data, we utilize publicly available parallel corpora, such as OPUS (Tiedemann, 2012) and ParaCrawl (Buck & Koehn, 2016), which we filter using reference-free quality estimation metrics, including CometKiwi (Rei et al., 2022), to ensure the selection of high-quality translation pairs. 1https://huggingface.co/tencent/Hunyuan-7B-Pretrain 2https://github.com/facebookresearch/fastText 3https://github.com/kpu/kenlm 3 Figure 2: Post-training pipeline of the Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B models. To determine the optimal data mixture ratio, we adopt strategy inspired by RegMix (Liu et al., 2025). We first conduct experiments on smaller-scale model to fit function that maps sampling ratios to training loss. By simulating this function, we identify the mixture that minimizes the predicted loss, and we then use this ratio for the MT-oriented pre-training stage of our final translation model. To mitigate catastrophic forgetting, we integrate 20% replay of the original pre-training corpus. We also design the learning rate schedule to warm up to the peak learning rate of the initial pre-training phase and then decay to its minimum value."
        },
        {
            "title": "3 Post-training Stage",
            "content": "Following pre-training, we aim to equip the base model with robust multilingual machine translation capabilities through Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), and Weak-to-Strong RL. The central challenge lies in optimizing performance for high-resource languages using model-generated and human-annotated data while ensuring effective generalization to low-resource languages. Prompt Template for ZHXX Translation. 把下面的文本翻译成<target language>不要额外解释 <source text> Prompt Template for XXXX Translation, excluding ZHXX. Translate the following segment into <target language>, without additional explanation. <source text> Table 1: Examples of prompt template. Table 2: Supported languages. Languages Abbr."
        },
        {
            "title": "Languages",
            "content": "Abbr. Languages Abbr."
        },
        {
            "title": "Chinese\nEnglish\nFrench\nPortuguese\nSpanish\nJapanese\nTurkish\nRussian\nArabic\nKorean\nThai\nItalian\nGerman\nVietnamese",
            "content": "zh en fr pt es ja tr ru ar ko th it de vi"
        },
        {
            "title": "Malay\nIndonesian\nFilipino\nHindi\nTraditional Chinese\nPolish\nCzech\nDutch\nKhmer\nBurmese\nPersian\nGujarati\nUrdu\nTelugu",
            "content": "ms id tl hi"
        },
        {
            "title": "Marathi\nHebrew\nBengali\nTamil",
            "content": "zh-Hant Ukrainian"
        },
        {
            "title": "Tibetan\nKazakh\nMongolian\nUyghur\nCantonese",
            "content": "pl cs nl km my fa gu ur te mr he bn ta uk bo kk mn ug yue"
        },
        {
            "title": "3.1 Supervised Fine-Tuning",
            "content": "We adopt two-stage SFT framework designed to progressively enhance the models translation capabilities and its ability to follow instructions. The initial stage is dedicated to establishing robust foundation by training the model on comprehensive parallel corpus of approximately 3 million pairs. This dataset is aggregated from diverse sources, including established benchmarks like the Flores-200 development set and past WMT test sets, human-annotated collection of Mandarin-centric minority language pairs, and synthetically generated corpus from DeepSeek-V3-0324 (DeepSeek-AI, 2024). To improve instruction-following, we also incorporate 20% component of general-purpose and MT-oriented instruction data (see an example in Table 1). To maintain high standard of data quality, we apply filtering process using the reference-free metrics CometKiwi and GEMBA (Kocmi & Federmann, 2023), discarding samples that fall below specified quality threshold. Notably, the GEMBA scoring leverages the DeepSeek-V3-0324 model as the evaluator. Building upon this foundation, the second stage aims to refine the models translation performance further using smaller, higher-fidelity dataset of approximately 268,000 pairs. The data for this stage undergoes more stringent selection process. Drawing inspiration from recent work (Agarwal et al., 2024; Song et al., 2025b), we employ many-shot in-context learning as mechanism to vet further and select high-quality training instances. To maximize data reliability, any samples that demonstrate significant score inconsistencies across multiple evaluation rounds are flagged for manual annotation and verification by human experts. This two-stage approach allows us first to build broad multilingual capabilities and then hone them with meticulously curated dataset."
        },
        {
            "title": "3.2 Reinforcement Learning",
            "content": "While large-scale RL has demonstrated significant effectiveness in enhancing reasoning capabilities for tasks with structured outputs, such as mathematical problem-solving and code generation (DeepSeek-AI, 2025; Luo et al., 2025; Song et al., 2025a; Team et al., 2025b), its application to MT presents unique challenge. Unlike in structured domains, MT outputs are characterized by considerable semantic diversity, which makes them resistant to evaluation through explicit, rule-based evaluation. To address this challenge, we adopt GRPO (Shao et al., 2024) as the RL algorithm and design comprehensive reward function comprising the following components: Quality-Aware Reward. To ensure translation quality during RL training, we employ two complementary reward signals. The first is XCOMET-XXL, widely adopted metric in translation evaluation scenarios that demonstrates high correlation with human assessments. The second reward utilizes DeepSeek-V3-0324 for scoring, with prompts adapted from the GEMBA framework. Terminology-Aware Reward. While XCOMET-based rewards primarily focus on overall semantic similarity between translated outputs and reference translations, they may inadequately capture critical information such as domain-specific terminology. To address this limitation, we incorporate the word alignment-based reward metric proposed in TAT-R1 (Li et al., 2025). This reward mechanism extracts key information, including terminology, through word alignment tools, then computes the overlap ratio of these critical elements between the translation output and reference. Higher overlap ratios yield greater rewards, thereby enhancing the models attention to terminology and other crucial information during training. Repetition Penalty. We observe that models tend to generate repetitive outputs in later stages of reinforcement training, potentially leading to training collapse. To mitigate this issue, we implement repetition detection mechanism that applies penalties when repetitive patterns are identified, thereby maintaining output diversity and training stability."
        },
        {
            "title": "3.3 Weak-to-Strong RL",
            "content": "Recent studies demonstrate that increasing inference time significantly enhances model performance on mathematical and coding tasks (Muennighoff et al., 2025; Zhang et al., 2025). However, initial experiments incorporating Chain-of-Thought (CoT) (Wei et al., 2023) reasoning into translation tasks yield limited improvements in translation quality, as discussed comprehensively in Section 6. Consequently, in this report, we explore new test-time scaling methodology from novel perspective to enhance test-time performance for MT. Specifically, to address the above issue, we propose weak-to-strong RL approach that generates multiple translation outputs and employs fusion model based on Hunyuan-MT-7B to aggregate these outputs through GRPO. The reward function comprises three primary elements: XCOMET-XXL scoring, 5 Prompt Template for Hunyuan-MT-Chimera-7B. Analyze the following multiple <target language> translations of the <source language> segment surrounded in triple backticks and generate single refined <target language> translation. Only output the refined translation, do not explain. The <source language> segment: ```<source text>``` The multiple <target language> translations: 1. ```<translated text1>``` 2. ```<translated text2>``` 3. ```<translated text3>``` 4. ```<translated text4>``` 5. ```<translated text5>``` 6. ```<translated text6>``` Table 3: Prompt template of Hunyuan-MT-Chimera-7B. DeepSeek-V3-0324 scoring, and repetition penalty term. This multi-faceted reward mechanism ensures comprehensive evaluation of translation quality while mitigating redundancy in generated outputs. This methodology culminates in the development of our Hunyuan-MT-7B-Chimera model. Specifically, the prompt template is shown in Table 3. During test-time inference, Hunyuan-MT-7B-Chimera accepts multiple translation candidates as input and synthesizes their respective strengths to produce superior unified translation output. This aggregation approach leverages the complementary advantages of diverse translation hypotheses to achieve enhanced translation quality."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "We conduct comprehensive evaluations of the base model of the Hunyuan-MT series, focusing primarily on its performance in general knowledge, reasoning, mathematics, scientific knowledge, coding, and multilingual capabilities. Specifically, the evaluation benchmarks for pre-trained models include nine widely used benchmarks: MMLU-Pro (Wang et al., 2024), SuperGPQA (Team et al., 2025c), BBH (Shi et al., 2023), GPQA (Rein et al., 2023), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MultiPL-E (Cassano et al., 2023), CRUX-O of CRUXEval (Gu et al., 2024), and INCLUDE (Romanou et al., 2025). To comprehensively evaluate the multilingual translation capabilities, we conducted extensive experiments using the following test sets: Flores-2004 (Team et al., 2022). We select 1,056 language pairs across 33 different languages (detailed in the Appendix) from the Flores-200 dataset. These pairs are systematically categorized into five groups: EnglishXX, XXEnglish, ChineseXX, XXChinese, and XXXX translations. WMT24pp5 (Deutsch et al., 2025). We incorporate development sets from WMT-25, encompassing English-to-XX translations across 25 target languages. WMT24pp serves as the official development set recommended by WMT25. We select 29 language pairs that overlap with the general translation track of the WMT25, with primary focus on English-to-XX directions. MandarinMinority Testset. This test set encompasses translations between Chinese and minority languages: Tibetan, Mongolian, Uyghur, and Kazakh."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "We evaluate translations using two complementary approaches: automatic metrics and human evaluation. For automatic evaluation, we use the neural metrics XCOMET-XXL (Guerreiro et al., 2023) and CometKiwi (Rei et al., 2022), which generally correlate with human judgments but can be unreliable for certain translation phenomena. To address these limitations, we conduct human evaluation in which multilingual 4https://huggingface.co/datasets/Muennighoff/flores200 5https://huggingface.co/datasets/google/wmt24pp 6 experts rate translations on 04 scale, focusing on pre-annotated error-prone points and considering accuracy, fluency, and idiomaticity. Table 4: Performances of state-of-the-art models on Flores-200, WMT-24pp, and MandarinMinority translation. Specifically, we report the Chinese-centric (ZHXX and XXZH), English-centric (ENXX and XXEN), XXXX, and Mand.Min. performances of Hunyuan-MT-7B, Hunyuan-MT-Chimera-7B, and prominent existing systems. Here, Mand.Min. denotes MandarinMinority translation. Models with open-source weights are marked with . Baselines are categorized into three groups: (1) ultra-large general models, (2) medium to small-sized general models, and (3) translation-specialized models. Models Metrics Flores-200 WMT24pp Mand.Min. ZH XX XX ZH EN XX XX EN XX XX XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi 0.8843 0.7859 0.9013 0.7883 0.9146 0.7859 0.8848 0.7722 0.7615 0. 0.7726 0.6633 0.7703 0.6795 0.8010 0.7089 0.8687 0.7604 0.8567 0.7603 0.8783 0. 0.7250 0.6605 0.7826 0.7116 0.7933 0.7139 0.8509 0.7551 0.6385 0.6581 0.8529 0. 0.8758 0.7963 0.8974 0.8066 GPT4.1 OpenAI (2025) Claude-Sonnet-4 Anthropic (2025) Gemini-2.5-Pro DeepMind (2025) DeepSeek-V3-0324 DeepSeek-AI (2024) Google-Translator Tower-Plus-9B Rei et al. (2025) Tower-Plus-72B Rei et al. (2025) Seed-X-PPO-7B Cheng et al. (2025) GemmaX2-28-9B-v0.1 Cui et al. (2025) Gemma-3-12B-IT Team et al. (2025a) Gemma-3-27B-IT Team et al. (2025a) Qwen3-8B Team (2025) Qwen3-14B Team (2025) Qwen3-32B (Team, 2025) Qwen3-235B-A22B Team (2025) Llama-3.1-8B-Instruct AI@Meta (2025a) Llama-4-Scout-17B-16EInstruct AI@Meta (2025b) Hunyuan-MT-7B Hunyuan-MT-Chimera-7B"
        },
        {
            "title": "4.3 Main Results",
            "content": "0.8593 0.7725 0.8590 0.7739 0.8748 0.7828 0.8542 0.7708 0.6243 0.5691 0.7912 0. 0.8235 0.7569 0.7702 0.7201 0.8280 0.7595 0.8249 0.7582 0.8441 0.7718 0.8056 0. 0.8318 0.7674 0.8436 0.7719 0.8569 0.7750 0.5148 0.6185 0.8266 0.7417 0.8528 0. 0.8719 0.7914 0.8996 0.8702 0.9114 0.8742 0.9295 0.8869 0.9010 0.8684 0.7638 0. 0.7884 0.7576 0.7829 0.7603 0.8181 0.8118 0.8806 0.8576 0.8781 0.8498 0.9036 0. 0.7468 0.7257 0.8027 0.7877 0.8186 0.8001 0.8765 0.8475 0.6848 0.7234 0.8673 0. 0.9112 0.8742 0.9306 0.8849 0.9405 0.8730 0.9390 0.8732 0.9432 0.8720 0.9319 0. 0.7761 0.7863 0.8704 0.8475 0.9002 0.8624 0.8442 0.8202 0.9108 0.8635 0.9189 0. 0.9331 0.8646 0.8825 0.8521 0.9049 0.8639 0.9154 0.8657 0.9292 0.8696 0.6412 0. 0.8929 0.8523 0.9018 0.8477 0.9132 0.8514 0.8258 0.7424 0.8548 0.7668 0.8773 0. 0.8082 0.7450 0.6225 0.5947 0.6608 0.6141 0.7002 0.6553 0.6896 0.6436 0.8119 0. 0.8020 0.7320 0.8381 0.7471 0.6544 0.6285 0.7228 0.6887 0.7433 0.6965 0.8018 0. 0.4408 0.5782 0.7788 0.7275 0.7829 0.7210 0.8268 0.7424 0.8032 0.7688 0.8120 0. 0.8250 0.7876 0.8109 0.7840 0.5796 0.5454 0.6977 0.6741 0.7276 0.7071 0.7388 0. 0.7173 0.7242 0.7527 0.7329 0.7742 0.7577 0.6532 0.6344 0.6983 0.6839 0.7099 0. 0.7665 0.7465 0.5130 0.5990 0.7550 0.7422 0.8585 0.8061 0.8787 0.8129 0.4904 0. 0.5111 0.5033 0.5811 0.5418 0.4865 0.4839 0.3692 0.2891 0.3912 0.3466 0.3855 0. 0.4206 0.4861 0.4269 0.5178 0.4280 0.4186 0.4558 0.4844 0.3737 0.3166 0.3944 0. 0.4110 0.3841 0.4493 0.4456 0.3016 0.2944 0.4472 0.4646 0.6082 0.4162 0.6089 0. As presented in Table 4, our experimental results indicate that the proposed Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B models achieve competitive performance across the XCOMET-XXL and CometKiwi evaluation metrics. On the WMT24pp benchmark, Hunyuan-MT-7B obtains an XCOMETXXL score of 0.8585. This result outperforms strong baselines, including large-scale models such as Gemini-2.5-Pro (0.8250) and Claude-Sonnet-4 (0.8120). noteworthy observation is the models performance on MandarinMinority language translation pairs. In this setting, both Hunyuan-MT-7B (0.6082) and Hunyuan-MT-Chimera-7B (0.6089) yield scores that are considerably higher than the evaluated baselines. For instance, the scores represent relative improvement of approximately 4.7% over the next-best performing system, Gemini-2.5-Pro (0.5811), and show substantial gains when compared to several models specialized for translation. 7 When compared with dedicated translation models, our Hunyuan-MT series demonstrates promising results. The models achieve higher scores than Google-Translator and the Tower-Plus series across multiple evaluation settings, while being considerably more parameter-efficient than the 72B variant of Tower-Plus. The performance also remains competitive against other recently developed translationoptimized models, such as the Gemma-3 series. Furthermore, the Hunyuan-MT-Chimera-7B variant exhibits significant improvements over its base model, achieving an average gain of 2.3% in XCOMET-XXL scores across all directions in the Flores-200 dataset. The most pronounced improvements are observed in the ZHXX (a 2.5% increase) and XXXX (a 5.6% increase) translation directions. These findings suggest that our proposed weak-to-strong method is effective in further refining translation quality, enhancing performance on high-resource languages while maintaining strong capabilities on diverse slow-thinking scenarios."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "To validate the efficacy of our proposed two-stage pre-training methodology, we conduct series of ablation studies. We first evaluate the performance of our base model, Hunyuan-7B-Base, resulting from the general pre-training phase. Subsequently, we assess the impact of the MT-oriented pre-training stage by comparing the specialized model Hunyuan-7B-Base against strong baseline. The results from our general pre-training stage, presented in Table 5, confirm the efficacy of our data curation strategy. Hunyuan-7B-Base establishes new state-of-the-art performance on knowledge-intensive benchmarks such as MMLU-Pro and BBH. This outcome highlights the success of our quality assessment model in creating corpus rich in factual and reasoning content. The models capabilities are particularly pronounced in mathematical reasoning, where it achieves score of 74.85 on the MATH benchmark, creating substantial 14-point lead over the next-best competitor. Table 5: Comparison among Hunyuan-7B-Base and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Llama-3-8B-Base Qwen2.5-7B-Base Qwen3-8B-Base Hunyuan-7B-Base MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH MultiPL-E CRUX-O"
        },
        {
            "title": "IINCLUDE",
            "content": "35.36 20.54 57.70 25.80 55.30 20.50 31.45 36.80 44.94 45.00 26.34 70.40 36.36 85.36 49. 50.73 48.50 53.98 56.73 31.64 78.40 44.44 89.84 60.80 58.75 62.00 59. 57.79 30.47 82.95 44.07 88.25 74.85 60.41 60.75 59.55 The subsequent MT-oriented pre-training phase yields dramatic improvements in translation capabilities, as shown in Table 6. Our specialized model, Hunyuan-7B-Base, establishes commanding lead over the strong Qwen3-8B-Base baseline across all machine translation benchmarks. This superiority is particularly evident on the challenging Flores-200 and MandarinMinority language benchmarks, where our models significant gains directly validate the impact of incorporating 1.3 trillion tokens of low-resource language data. The models advantage extends to high-resource scenarios as well, with substantial performance increase on the WMT24pp benchmark. Table 6: Comparison among Hunyuan-7B-Base and Qwen3-8B-Base. Here, Hunyuan-7B-Base denotes the Hunyuan-7B-Base model after MT-oriented Pre-training. The highest and second-best scores are shown in bold and underlined, respectively. WMT24pp XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi XCOMET-XXL CometKiwi MandarinMinority Flores-200 Qwen3-8B-Base Hunyuan-7B-Base 57.88 67.41 55.46 65. 35.89 48.34 36.69 46.29 32.02 39.95 23.98 28."
        },
        {
            "title": "5.1 Case Study",
            "content": "Table 7, Table 8, and Table 9 illustrate representative translation cases, comparing the outputs of HunyuanMT-7B and Hunyuan-MT-Chimera-7B on key testing points. These translation cases demonstrate the models sophisticated contextual understanding and proficiency in managing complex translations. Case analysis suggests that Hunyuan-MT-7B possesses notable capacity for handling culturally-specific and context-dependent phenomena that often challenge conventional translation systems. For instance, in the context of Chinese social media, the model correctly interprets 小红薯 as the platform REDnote and 砍一刀 as specific e-commerce promotional feature. In contrast, baseline system like Google Translate tends to produce literal but contextually incorrect renderings (sweet potatoes and cuts). This sensitivity to non-literal meaning appears to extend to English idiomatic expressions (e.g., You are killing me as amusement) and domain-specific terminology, including medical terms (blood disorders) and structured data like mailing addresses, which are often left untranslated by the baseline. These instances indicate an ability to leverage world knowledge and pragmatic context beyond direct lexical mapping. Table 7: Comparative case studies across different scenarios. Red text highlights translation errors in Google-Translator outputs. Example #1 Testing Points Huanyuan-MT-7B Google-Translator Example #2 Testing Points Huanyuan-MT-7B Scenario 1: Chinese Social 都知道小红薯在国外疯魔了数据来告诉你有多逆天 小红薯: REDnote. Everyone knows that REDnote has become incredibly popular abroad, and the data is proof of just how remarkable its success is! Everyone knows that sweet potatoes are popular abroad. Here are the statistics to tell you how popular they are! 没有一个外国人可以逃脱拼多多砍一刀 砍一刀: Its core mechanism revolves around social sharing to reduce product prices. No foreigner can escape the price cuts offered by Pinduoduo. Google-Translator No foreigner can escape Pinduoduos cuts Example #1 Testing Points Huanyuan-MT-7B Google-Translator Example #2 Testing Points Huanyuan-MT-7B Google-Translator Example # Testing Points Huanyuan-MT-7B Google-Translator Example #"
        },
        {
            "title": "Testing Points",
            "content": "Huanyuan-MT-7B Google-Translator Example #2 Huanyuan-MT-7B Google-Translator Oh!You are killing me! Scenario 2: English Slang You are killing me: it is an informal and exaggerated English expression used to emphasize intense feelings like extreme amusement, frustration, or being overwhelmed. 哦你真要把我笑死了 噢你要杀死我了 If you need advice on vintage vinyl, talk to Jim; hes the OG collector around here. OG: slang term originally short for Original Gangster, now mainly used to describe senior, authoritative, or pioneering person/thing in field (with no negative connotation, often term of recognition). 如果你需要关于复古黑胶唱片方面的建议可以找吉姆聊聊他是这里最资深的收藏家 如果您需要有关老式黑胶唱片的建议请与Jim 联系他是这里的 OG 收藏家 Scenario 3: Terminology 已知有血液疾病及尿酸性肾结石的患者不推荐使用本品二岁以下儿童不得服用 血液疾病: blood disorder, 尿酸性肾结石: uric acid kidney stone. This product is not recommended for patients with known blood disorders or uric acid kidney stones, and it should not be taken by children under the age of two. Patients with known blood diseases and uricidal kidney stones are not recommended for use, and children under two years of age are not allowed to take it. Scenario 4: Place Name The YMU community mourns the loss of Dr. Wang, beloved faculty member whose legacy lives on through his students. YMU: Yunnan University of Nationalities. 云南民族大学YMU的师生们为王博士的离世感到悲痛他是一位深受爱戴的教师他的精神将通过他的学生们 得以延续 杨百翰大学全体师生对王博士的去世深感悲痛王博士是一位受人爱戴的教师他的精神遗产通过他的学生得以传 承 Unit 602, HUANGPU Complex, 654 Huangpu Drive, Huangpu District, Guangzhou, Guangdong Province, 510000, China 中国广东省广州市黄埔区黄埔大道654号黄埔大厦602单元邮编510000 Unit 602, HUANGPU Complex, 654 Huangpu Drive, Huangpu District, Guangzhou, Guangdong Province, 510000, China Furthermore, Hunyuan-MT-7B demonstrates promising results across diverse set of language pairs, including those with fewer digital resources. For translations involving widely spoken European and Asian languages (e.g., Italian, German, Korean, Persian), the model tends to generate more idiomatic and contextually appropriate outputs. Its performance is particularly noteworthy in low-resource language translation. When translating from Mandarin to ethnic minority languages such as Kazakh and Tibetan, Hunyuan-MT-7B is often able to produce coherent sentences where the baseline system may yield nonsensical or failed outputs. This suggests our approach may be particularly beneficial for improving translation quality in low-resource scenarios, finding that aligns with the quantitative results on our MandarinMinority benchmark. Table 8: Comparative case studies across different scenarios. Red text highlights translation errors in Google-Translator outputs. Scenario 5: Less Commonly Used Languages Example #1 Questo avviene anche in Norvegia, Svezia Nuova Zelanda, ma in generale `e un fatto piuttosto singolare (ad es. nei Paesi Bassi il rapporto `e di uno quaranta). Testing Points Italian English Huanyuan-MT-7B Google-Translator Example #"
        },
        {
            "title": "Testing Points",
            "content": "This is also the case in Norway, Sweden, and New Zealand. However, overall its rather unusual phenomenon; for example, in the Netherlands the ratio is one to forty. This also occurs in Norway, Sweden and New Zealand, but in general it is rather singular fact (e.g. Netherlands the relationship is one by forty). in the Mehrere Geiseln wurden gerettet und es gibt bisher mindestens sechs bestatigte Tote. German English Huanyuan-MT-7B Several hostages have been rescued, and there are at least six confirmed deaths so far. Google-Translator Several hostages have been saved and there are at least six confirmed deaths so far. Example #3 Testing Points Huanyuan-MT-7B Google-Translator Example #4 Testing Points Huanyuan-MT-7B Google-Translator Example # Testing Points Huanyuan-MT-7B Google-Translator Example #2 Testing Points Huanyuan-MT-7B Google-Translator Example #"
        },
        {
            "title": "Testing Points",
            "content": "Huanyuan-MT-7B Google-Translator Example #"
        },
        {
            "title": "Testing Points",
            "content": "Huanyuan-MT-7B Google-Translator Example #"
        },
        {
            "title": "Testing Points",
            "content": "Huanyuan-MT-7B Google-Translator Korean Chinese 这样的群体通常由1到3只同类的成年雄性以及多达30只雌性和幼崽组成 一个狼群由一至三只同种成年雄性和最多30 只雌性及其幼崽组成 Persian Chinese 如果您在冬季前往北极或南极将会目睹极夜现象也就是说太阳始终无法升出地平线 如果您在冬季参观北极或南极您会看到一个极地夜晚这意味着太阳不会高于地平线 Scenario 6: Ethnic Minority Languages Kazakh Chinese 你担心你的丈夫不同意你这样做吗 不欢迎Kieuingulets吗 Tibetan Chinese 只有尊重他们才能真正改变他们 只有尊重它们才能进行改革 黄尸癫狗又出黎周围吠人了! Cantonese = Chinese 黄尸疯狗又出来到处吠人了 黄尸癫狗又出黎周围吠人了! Mongolian = Chinese 提升互联网+医疗水平内蒙古首家智慧医院上线 Uyghur = Chinese 创新将成为城市繁荣发展的基础 创新是城镇化的基础 10 Building upon this foundation, the Hunyuan-MT-Chimera-7B variant exhibits further refinements in nuanced interpretation. This is exemplified by its ability to resolve ambiguities in specialized domains, such as identifying d2 as the game Diablo II and interpreting make game as term for ingame trading. The model also shows sophisticated approach to informal language, pragmatically translating the intensifier fucking to convey emphasis rather than resorting to literal profanity. In other scenarios, it correctly maps 三分 to three points in sports context. It demonstrates greater situational awareness by rendering 穿过 as sped through crowd, avoiding the problematic implication of drove through. These cases suggest that the Chimera enhancement facilitates more fine-grained understanding of colloquialisms and domain-specific context, contributing to translations that are both more accurate and pragmatically sound. Table 9: Case studies of the Hunyuan-MT-Chimera-7B model. Scenario 7: Hunyuan-MT-Chimera-7B Case Study recently returned to d2 after several year, now Im windering: Where do you guys sell/buy your stuff? Do you just make game xxx yyy and hope for the best? Or are there website thats more efficient? d2: Diablo II, make game xxx yyy: it means creating game room where the room name uses (short for Offer, standing for the item xxx you want to sell) and (short for Need, standing for the item yyy you want in exchange) to inform other players of your trading needs. 我最近几年后又回到了d2现在我在想 你们一般在哪里买卖装备你们只是开个游戏房间O xxx yyy然后碰运气吗还是有更高效的网站 我最近在几年后重新回到了d2现在我想知道 你们都在哪里买卖东西你们是直接创建一个O xxx yyy的游戏然后碰运气吗还是有更高效的网站 我最近时隔几年又回来玩d2了想问一下 各位都是在哪里交易物品的是自己建个名叫出xxx换yyy的房间然后随缘等还是有更高效的网站 时隔多年重回暗黑2现在我想知道 你们都在哪买卖装备是直接建个O物品换N符文的房间碰运气吗还是有更高效的交易网站 我最近在离开几年后重新回到d2现在我很疑惑 你们都是在哪里买卖东西的你们只是做一款游戏叫O xxx yyy然后听天由命吗还是有更高效的网站 我最近在几年后回到了d2现在我在想 你们这些人在哪里卖/买你们的东西你们只是做一个游戏O xxx yyy然后希望是最好的吗还是有一个更有效率的网 站 时隔多年我又重新开始玩暗黑破坏神2Diablo II现在我想知道 大家都是在哪里进行物品交易的是通过自己创建名为O xxx yyy的游戏房间来交易然后听天由命吗还是有更高效的 交易网站 Now obviously cant get out and shut it on the main road, so pull down few side streets and ensure that no one fucking knows Ive quickly jumped out and shut it.I jump back in and drive home like nothings happened. no one fucking knows: it means that absolutely no one is aware of, with the word fucking used here as strong informal intensifier to emphasize the degree of secrecystressing that the action is completely unknown to anyone. 显然我不能在主路上下车去关门所以我驶进几条小路确保没人他妈的知道我迅速下车把门关上了我跳回车里像什么 都没发生一样开车回家 我显然不能在主路上下车去关门所以我拐进了几条小巷确保没有人知道我快速跳下车关了门我跳回车里像什么都没 发生一样开车回家 我显然不能在主路上停车关门于是我拐进几条小路确保他妈的没人知道我赶紧跳下车把门关上我再跳回车里若无其 事地开车回家 现在我显然没法在大马路上停车关门所以我拐进几条小巷确保没人他妈发现我迅速跳下车关好门我跳回车里若无其 事地开回家 现在我显然不能在主干道上停下来关门所以我拐进几条小街确保没人他妈的知道我迅速跳下车把门关上我跳回车上 像什么都没发生一样开车回家 显然我不能在主路上下车去关门所以我驶进几条小路确保没人他妈的知道我迅速下车把门关上了我跳回车里像什么 都没发生一样开车回家 显然我不能在主路上停车下车去关门于是我把车拐进几条小巷确保没有人看到我迅速下车把门关上之后我再跳回车 里若无其事地开车回家 希望我们能得到三分不过那将是一场棘手的比赛因为他们在上一场比赛中取得了不错的成绩但是有这么多人支持我 们我相信我们一定可以继续前进取得好成绩 三分: three points, not for winning three of the matches Hopefully, we can win all three of those matches. Its going to be tough competition, though, because they performed well in their last match. But with so many people supporting us, believe we can continue to do well and achieve good results. Hopefully, we can manage to win three of these matches. Its going to be tough competition, though, because they performed well in their last match. But with so many people supporting us, believe we can keep going and achieve good results. Hopefully, we can manage to win three of these matches. Its going to be tough challenge, though, because they performed well in their last match. But with so many people supporting us, Im confident that we can continue to move forward and do well. Hopefully, we can secure three points. Its going to be tough match, because they performed well in their last game. However, with so many people supporting us, believe we can continue to move forward and do well. Hopefully, we can secure three points. Its going to be tough match, though; they performed well in their last game. But with so many people supporting us, Im confident that we can continue to move forward and do well. Hopefully, we can secure three points. Its going to be tough match, though; they performed well in their last game. But with so many people supporting us, believe we can continue to move forward and do well. Example #1 Testing Points Model #1 Model #2 Model #3 Model # Model #5 Model #6 Hunyuan-MTChimera-7B Example #2 Testing Points Model # Model #2 Model #3 Model #4 Model #5 Model #6 Hunyuan-MTChimera-7B Example #3 Testing Points Model #1 Model #2 Model #3 Model #4 Model # Model #6 11 Hunyuan-MTChimera-7B Example #4 Model #1 Model #2 Model # Model #4 Model #5 Model #6 Hunyuan-MTChimera-7B hope we can secure those three points, but its going to be tough match, as they performed well in their last game. However, with so many people supporting us, Im confident that we can continue to move forward and achieve good results. 一辆1948 年的福特水星汽车穿过佐治亚州门罗小镇的一群围观者朝着小小的摩尔滩桥隆隆奔行 1948 Ford Mercury drove through crowd of onlookers in the town of Monroe, Georgia, and then made its way across the small Moorestown Bridge. 1948 Ford Mercury drove through crowd of onlookers in the town of Monroe, Georgia, and then made its way across the small Moores Bay Bridge. 1948 Ford Mercury convertible drove through crowd of onlookers in Monroe, Georgia, and then made its way across the small Moores Bend Bridge. 1948 Ford Mercury drove through crowd of onlookers in the town of Monroe, Georgia, and then made its way across the small Moorestown Bridge. 1948 Ford Mercury driven by man named George White sped through crowd in Monroe, Georgia, and then over the small Moores Bay Bridge. 1948 Ford Mercury driven by man named Steve Myers sped through crowd in Monroe, Georgia, and over the small bridge that crosses Moore Creek. 1948 Ford Mercury sped through crowd of onlookers in the small town of Monroe, Georgia, on its way towards the tiny Moorestown Bridge."
        },
        {
            "title": "5.2 The Role of CoT in MT",
            "content": "We also investigate the integration of Chain-of-Thought (CoT) methodologies within our training framework for translation. Our experiments indicate that when the reward signal is applied exclusively to the final translation output, it proves insufficient to elicit meaningful reasoning process. Under this condition, the model tends to generate generic, boilerplate statements for the CoT, such as, need to translate the English text into Chinese and ensure the translation accurately conveys the original meaning. Consequently, this approach yields no discernible improvement in translation performance when compared to baseline model trained without CoT. In contrast, we explore an alternative strategy that provides reward signals for both the reasoning process itself and the final translation, method consistent with our work on TAT-R1 (Li et al., 2025). We find that this dual-reward structure successfully incentivizes the model to produce more substantive and task-relevant CoT. This, in turn, correlates with measurable improvements in overall translation quality."
        },
        {
            "title": "5.3 Human Evaluation",
            "content": "Hunyuan-MT Challenge Testset, which encompasses diverse range of scenarios, including social interactions, emails, food ordering, shopping, and navigation inquiries. This test set involves bidirectional translation between Chinese and English. Existing open-source test sets predominantly focus on the news domain. To comprehensively evaluate the translation capabilities, we construct challenging test set that covers multiple domains, including news, medicine, government, literature, law, natural sciences, arts, computing, and the internet. As presented in Table 10, the experimental results reveal distinct performance tiers among the six evaluated translation models. The top-performing cluster, comprising Gemini-2.5-Pro (3.223), DeepSeekV3-0324 (3.219), and Huanyuan-MT-7B (3.189), demonstrates marginal performance differences of less than 0.034 points, suggesting convergence of state-of-the-art translation capabilities. Notably, Gemini2.5-Pro exhibits exceptional bidirectional balance with nearly identical scores for ZHEN (3.225) and ENZH (3.222) translations. Table 10: Human evaluation of translation quality for the Chinese-to-English (ZHEN) and English-toChinese (ENZH) directions."
        },
        {
            "title": "Models",
            "content": "ZHEN ENZH Gemini-2.5-Pro DeepSeek-V3-0324 Qwen3-32B Google-Translator Seed-X-PPO-7B Huanyuan-MT-7B 3.225 3.253 3.137 2.841 3.139 3.258 3.222 3.203 3.073 2.101 3.033 3.155 Avg. 3.223 3.219 3.094 2.344 3.068 3.189 At the same time, most other models display systematic bias favoring Chinese-to-English translation over the reverse direction, phenomenon potentially attributable to the greater complexity of generating grammatically correct Chinese text. The significant performance gap between these leading models and 12 Google-Translator (2.344), which underperforms by approximately 27%, underscores the superiority of modern transformer-based architectures over traditional translation systems. Particularly noteworthy is the competitive performance of Huanyuan-MT-7B, which, despite its relatively modest 7B parameters, achieves results comparable to larger models, suggesting that task-specific optimization can effectively compensate for model scale in specialized translation tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "In this report, we introduce Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B, family of open-source LLMs specifically engineered for machine translation, enabling bidirectional translation across 33 languages. We present comprehensive overview of our training methodology, which encompasses pre-training, supervised fine-tuning, and reinforcement learning phases. Throughout this exposition, we share critical insights and best practices gleaned from our iterative optimization process. Remarkably, with only 7B parameters, Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B achieve translation quality that rivalsand in certain cases surpassesthat of state-of-the-art LLMs and leading commercial translation systems, as validated through both automatic metrics and human evaluation. By making HunyuanMTs model weights publicly available, we aim to empower the research community with an accessible, high-performance foundation model that can accelerate innovation in MT research and applications."
        },
        {
            "title": "7.1 Core Contributors",
            "content": "Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, Di Wang"
        },
        {
            "title": "7.2 Contributors",
            "content": "Tao Chen, Jiaqi Zhu, Xingwu Sun, Yufei Wang, Can Xu, Chen Li, Kai Wang, Decheng Wu"
        },
        {
            "title": "7.3 Acknowledgments",
            "content": "We thank Shen Huang for providing the training data and sharing valuable insights on MandarinMinority translation."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie C. Y. Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal M. P. Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers .nips.cc/paper files/paper/2024/hash/8cb564df771e9eacbfe9d72bd46a24a9-Abstract-Conferenc e.html. AI@Meta. Llama-3.1-8b-instruct. https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instr uct, 2025a. Accessed: 2025-08-26. AI@Meta. Llama-4-scout-17b-16e-instruct. https://huggingface.co/meta-llama/Llama-4-Scout-17B-1 6E-Instruct, 2025b. Accessed: 2025-08-26. Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025. Accessed: 2025-08-26. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473. Peter F. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. statistical approach to machine translation. Comput. Linguistics, 16(2):7985, 1990. 13 Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguistics, 19(2):263311, 1993. Christian Buck and Philipp Koehn. Findings of the wmt 2016 bilingual document alignment shared task. In Proceedings of the First Conference on Machine Translation, pp. 554563, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W16/W16-2347. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):36753691, 2023. doi: 10.1109/TSE.2023.3267446. URL https://doi.org/10.1109/TSE.2023.3267446. Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, Tao Li, Yifu Li, Huiying Lin, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, and Yonghui Wu. Seed-x: Building strong multilingual translation llm with 7b parameters, 2025. URL https://arxiv.org/abs/2507.13618. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.or g/abs/2110.14168. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. Multilingual machine translation with open large language models at practical scale: An empirical study, 2025. URL https://arxiv.org/ab s/2502.02481. DeepMind. Were expanding our gemini 2.5 family of models. https://blog.google/products/gemini/ gemini-2-5-model-family-expands/, 2025. Accessed: 2025-08-26. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects, 2025. URL https://arxiv.org/abs/2502.12404. Alex Gu, Baptiste Rozi`ere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and Andre F. T. Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection, 2023. URL https://arxiv.org/abs/2310.10482. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Bojie Hu, Ambyer Han, Zheyang Zhang, Shen Huang, and Qi Ju. Tencent minority-mandarin translation system. In Shujian Huang and Kevin Knight (eds.), Machine Translation, pp. 93104, Singapore, 2019. Springer Singapore. ISBN 978-981-15-1721-1. Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt good translator? preliminary study. CoRR, abs/2301.08745, 2023. doi: 10.48550/ARXIV.2301.08745. URL https://doi.org/10.48550/arXiv.2301.08745. Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pp. 193203, Tampere, Finland, June 2023. European Association for Machine Translation. URL https://aclanthology.org/2023.eamt-1.19. 14 Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, Mariya Shmatova, Steinth or Steingrımsson, and Vilem Zouhar. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), Proceedings of the Ninth Conference on Machine Translation, WMT 2024, Miami, FL, USA, November 15-16, 2024, pp. 146. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.WMT-1.1. URL https://doi.org/10.18653/v1/2024.wmt-1.1. Zheng Li, Mao Zheng, Mingyang Song, and Wenjie Yang. Tat-r1: Terminology-aware translation with reinforcement learning and word alignment, 2025. URL https://arxiv.org/abs/2505.21172. Cong Lin and Liz Jackson. Assimilation over protection: rethinking mandarin language assimilation in china. Multicultural Education Review, 13(4):338361, 2021. doi: 10.1080/2005615X.2021.2006117. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training, 2025. URL https://arxiv.org/abs/2407.01492. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCod er-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. CoRR, abs/2501.19393, 2025. doi: 10.48550/ARXIV.2501.19393. URL https://doi.org/10.48550/arX iv.2501.19393. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. Accessed: 2025-08-26. Pedro Javier Ortiz Suarez, Benoit Sagot, and Laurent Romary. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp. 9 16, Mannheim, 2019. Leibniz-Institut fur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http://nbn-resolvi ng.de/urn:nbn:de:bsz:mh39-90215. Pedro Javier Ortiz Suarez, Laurent Romary, and Benoit Sagot. monolingual approach to contextualized word embeddings for mid-resource languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 17031714, Online, July 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.acl-main.156. Jianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu, Shuming Shi, Zhaopeng Tu, and Longyue Wang. Salute the classic: Revisiting challenges of machine translation in the age of large language models. Trans. Assoc. Comput. Linguistics, 13:7395, 2025. doi: 10.1162/TACL 00730. URL https: //doi.org/10.1162/tacl 00730. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pp. 311318. ACL, 2002. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040/. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, Jose G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, Alon Lavie, Luisa Coheur, and Andre F. T. Martins. Cometkiwi: Ist-unbabel 2022 submission for the quality estimation shared task, 2022. URL https://arxiv.org/abs/2209.06243. Ricardo Rei, Nuno M. Guerreiro, Jose Pombal, Joao Alves, Pedro Teixeirinha, Amin Farajian, and Andre F. T. Martins. Tower+: Bridging generality and translation specialization in multilingual llms, 2025. URL https://arxiv.org/abs/2506.17080. 15 David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. URL https://doi.org/10.48550/arXiv.2311. 12022. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, orje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando-Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, and Mike Zhang. INCLUDE: evaluating multilingual language understanding with regional knowledge. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=k3gCieTXeY. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arX iv.2402.03300. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language modIn The Eleventh International Conference on Learnels are multilingual chain-of-thought reasoners. ing Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/forum?id=fR3wGCk-IXp. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. Fastcurl: Curriculum reinforcement learning with stage-wise context scaling for efficient training r1-like reasoning models, 2025a. URL https://arxiv.org/abs/2503.17287. Mingyang Song, Mao Zheng, and Xuan Luo. Can many-shot in-context learning help llms as evaluators? preliminary empirical study. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend AlKhalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pp. 82328241. Association for Computational Linguistics, 2025b. URL https://aclanthology.org/2025.coling-mai n.548/. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 31043112, 2014. URL https: //proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gael Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andras Gy orgy, Andre Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Pluci nska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim oder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Leonard Hussenot. Gemma 3 technical report, 2025a. URL https://arxiv.org/abs/2503.19786. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025b. URL https://arxiv.org/abs/2507.20534. M.-A-P. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Jun Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling LLM evaluation across 285 graduate disciplines. CoRR, abs/2502.14739, 2025c. doi: 10.48550/ARXIV.2502.14739. URL https://doi.org/10.48550/arXiv.2502.14739. NLLB Team, Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah 17 Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022. URL https://arxiv.org/abs/2207.04672. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. org Tiedemann. Parallel data, tools and interfaces in opus. In Lrec, volume 2012, pp. 22142218, 2012. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper files/paper/2024/hash/ad236edc5 64f3e3156e1b2feafb99a24-Abstract-Datasets and Benchmarks Track.html. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma. survey on test-time scaling in large language models: What, how, where, and how well?, 2025. URL https://arxiv.org/abs/2503.2 4235. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. In Kevin Duh, Helena omez-Adorno, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 27652781. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGSNAACL.176. URL https://doi.org/10.18653/v1/2024.findings-naacl.176."
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}