{
    "paper_title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
    "authors": [
        "Daniil Plyusov",
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Viacheslav Sinii",
        "Alexey Malakhov",
        "Daniil Gavrilov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\\rightarrow$ 70.3 (GRPO), 69.3 $\\rightarrow$ 72.5 (DAPO), and 73.2 $\\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 7 1 7 6 0 . 2 0 6 2 : r F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Daniil Plyusov * 1 2 Alexey Gorbatovski * 1 Boris Shaposhnikov 1 Viacheslav Sinii 1 Alexey Malakhov 1 Daniil Gavrilov"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as function of group size, showing nonmonotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 70.3 (GRPO), 69.3 72.5 (DAPO), and 73.2 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost. 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) has become standard paradigm for post-training large language models (LLMs), enabling strong gains on reasoningintensive tasks without reliance on human preference data (Zhang et al., 2025). By leveraging automatically checkable reward signals, RLVR has driven state-of-the-art performance in mathematical reasoning (Li et al., 2024), code *Equal contribution 1T-Tech 2Saint Petersburg Electrotechnical University LETI. Correspondence to: Alexey Gorbatovski <a.gorbatovskiy@t-tech.dev>. Preprint. February 9, 2026. generation (Jimenez et al., 2023), and general problem solving (Chollet et al., 2025), and is now widely adopted in large-scale post-training (Guo et al., 2025; Yang et al., 2025; Team et al., 2025; Shao et al., 2024). Despite these successes, growing body of work suggests that RLVR does not primarily introduce new knowledge, but instead sharpens the output distribution toward solutions already accessible to the base model (Yue et al., 2025; Ni et al., 2025; Wu et al., 2025a; Dang et al., 2025). Empirical evidence based on pass@k (Chen et al., 2021) indicates that RLVR-trained models may underperform their base counterparts at sufficiently large sampling budgets, consistent with narrowing of solution diversity (Matsutani et al., 2025). At the same time, other studies argue that prolonged or carefully scaled RL can expand the effective reasoning boundary (Liu et al., 2025b; Yuan et al., 2025), leaving the role of RLVR an open question. Most modern RLVR systems rely on group-relative methods such as GRPO (Shao et al., 2024) and its variants (Yu et al., 2025; Chen et al., 2025a; Liu et al., 2025c), which compute advantages from multiple rollouts per prompt. The group size thus becomes critical design choice, yet existing work provides conflicting guidance: Wu et al. (2025b) show that two rollouts suffice and connect GRPO to DPO (Rafailov et al., 2023), while Hu et al. (2025) advocate scaling rollouts to broaden exploration. Since group size directly controls which trajectories receive learning signal, understanding its interaction with sharpening is essential. This raises fundamental question: how does group size affect the optimization dynamics of group-relative RLVR with binary rewards, and can we mitigate sharpening without scaling computational cost? In this paper, we analyze the sampling dynamics of grouprelative RLVR and propose F-GRPO, lightweight modification that addresses sharpening at practical group sizes. Our contributions are as follows: We derive closed-form tail-miss probability characterizing when active RLVR updates miss rare-correct modes, revealing non-monotonic dependence on group size that reconciles conflicting prior findings: small groups preserve diversity through inactivity, large groups through coverage, while intermediate groups, most common in F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Figure 1. (a) Probability that training update is active (mixed rewards in batch) yet misses rare-correct solutions, as function of group size . This probability peaks at intermediate : small groups rarely produce learning signal, large groups cover rare modes, but moderate groups combine active updates with poor coverage. (b,c) Empirical consequences on AIME 2025 (math) and IFEval (OOD): GRPO at =8 improves pass@1 over =2 but degrades pass@256, consistent with the sharpening regime. F-GRPO at =8 recovers pass@256 while maintaining pass@1, using 4 less compute than =32. practice, maximize sharpening risk. the success probability Building on the categorical framework of Hu et al. (2025), we analyze how probability mass redistributes within the correct set, showing that unsampled-correct mass can decrease even when total correct mass increases. We propose F-GRPO, difficulty-aware advantage scaling applicable to any group-relative objective including GRPO, DAPO, and CISPO, and demonstrate consistent pass@256 improvements on both reasoning math and OOD benchmarks while preserving or improving pass@1 across three model families, without additional computational cost. Figure 1 illustrates the core finding: tail-miss probability peaks at intermediate group sizes, and F-GRPO at =8 matches or exceeds GRPO at =32, achieving higher pass@256 (52.6 vs. 49.5 on AIME 2025; 75.7 vs. 71.4 on IFEval) and improved OOD pass@1 (34.0 vs. 31.0), while using 4 fewer rollouts. 2. Preliminaries 2.1. Reinforcement Learning with Verifiable Rewards We consider reinforcement learning with verifiable rewards (RLVR) for language model reasoning. Given prompt x, the policy πθ generates complete responses (trajectories). We sample group of i.i.d. rollouts {oi}N i=1 πθ( x) and assign binary outcome rewards Ri = Rw + (Rc Rw) I[oi is correct] (1) µpos(x) := Pr [o C(x)], oπθ(x) (2) For analysis, we consider designated subset Crare(x) C(x) of correct rollouts, with mass under the current policy τ (x) := Pr oπθ(x) [o Crare(x)]. (3) By construction 0 τ (x) µpos(x). We call Crare(x) rare-correct when ρ(x) := τ (x)/µpos(x) is small; this ratio can change as πθ evolves. 2.2. Group-Relative Policy Optimization Group Relative Policy Optimization (GRPO) (Shao et al., 2024) eliminates the learned value function by computing advantages relative to the sampled group. For prompt with rollouts {oi}N i=1, the grouprelative advantage is i=1 and rewards {Ri}N (cid:98)AGRPO = Ri σR + ϵ , (4) where = 1 (cid:80)N j=1 Rj and σR = std({Rj}N j=1). GRPO optimizes clipped surrogate objective. Let oi = (yi,1, . . . , yi,Ti) denote the token sequence for rollout i, with importance ratio ri,t(θ) = πθ(yi,tx,yi,<t) πθold (yi,tx,yi,<t) . The GRPO objective is LGRPO(θ) = Ex (cid:34) 1 (cid:88) i=1 1 Ti Ti(cid:88) t= (cid:35) Lclip i,t β DKL (πθ πref ) , where Rc > Rw (typically Rc = 1, Rw {0, 1}). We work with outcome-level rewards: the reward depends only on final correctness. For each prompt x, let Ωx denote the space of complete rollouts and C(x) Ωx the subset of correct rollouts. Define (cid:1). i,t = min (cid:0)ri,t (cid:98)Ai, clip(ri,t, 1ε, 1+ε) (cid:98)Ai where Lclip We set β = 0 following DAPO (Yu et al., 2025). DAPO modifies this with asymmetric clipping bounds clip(ri,t, 1εlow, 1+εhigh) where εhigh > εlow, relaxing the upper bound for low-probability actions. F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare CISPO (Chen et al., 2025a) clips the importance weights directly rather than the surrogate product. Define the clipped weight (cid:98)ri,t = clip(cid:0)ri,t, 1εIS and optimizes REINFORCE-style objective low, 1+εIS high (cid:1), (5) LCISPO(θ) = Ei,t (cid:104) sg((cid:98)ri,t) (cid:98)AGRPO where sg() denotes stop-gradient. log πθ(yi,t x, yi,<t) (cid:105) , key property of group-relative advantages is that when all sampled rewards are identical (σR = 0), we have (cid:98)AGRPO = 0 for all i, which yields zero learning signal. This occurs when all rollouts are correct or all are incorrect. 2.3. Categorical Policy Framework To analyze how RLVR updates redistribute probability mass, we adopt the categorical policy framework of (Hu et al., 2025). Consider = softmax(z) over finite action space A, partitioned into correct actions and incorrect = P. Define the total correct and incorrect masses Qpos := (cid:88) iP pi, Qneg := 1 Qpos. (6) iB pi, A2 := (cid:80) Draw i.i.d. samples from p. Let and denote sampled correct and incorrect actions, = (A B) the unsampled actions. Define the sampled masses and concentration measures Ppos := (cid:80) iA pi, Pneg := (cid:80) , and B2 := (cid:80) iB p2 . iU p iA p2 For the unsampled set, define Upos,2 := (cid:80) and Uneg,2 := (cid:80) . Assign rewards as in (1) for sampled actions, with Ri = 0 for unsampled. The batch baseline is SR := RcPpos + RwPneg. iU p2 We analyze TRPO-style linear surrogate updates and their unbiased Monte Carlo estimates. Under standard regularity conditions, expectation and differentiation may be interchanged (Asmussen & Glynn, 2007; Hu et al., 2025). Differentiating the sample surrogate with respect to the logits zj (using pi/zj = pi(δij pj)) yields the one-step logit update zi = η pi(Ri SR), (7) where η is the learning rate. For unsampled actions (i ), this reduces to zi = η SRpi. From this update rule, Hu et al. (2025) derive the one-step change in total correct mass: (cid:104) (Rc SR)QnegA2 + (SR Rw)QposB2 Qpos = η + SR(QposUneg,2 QnegUpos,2) (8) (cid:105) . The first two terms are always non-negative: promoting sampled correct actions and demoting sampled incorrect 3 actions both transfer mass to the correct pool. The third term, the unsampled coupling, can be positive or negative depending on SR and the relative concentration of unsampled masses. As the unsampled second moments decay with , increasing rollout size drives this coupling toward zero. This categorical framework directly models token-level update dynamics. For trajectory-level RLVR, we maintain separate notation to avoid conflation: µpos(x) denotes the per-prompt success probability (2), while Qpos refers to correct mass in the categorical setting (6). 3. Theoretical Analysis Recent work offers seemingly conflicting guidance on group size in RLVR: very small groups (N = 2) can match larger ones efficiently (Wu et al., 2025b), moderate sizes improve pass@1 while sharpening the distribution (He et al., 2025), and large groups stabilize learning (Hu et al., 2025). We develop theoretical framework that reconciles these findings. 3.1. Tail-miss probability and the group size trade-off We begin with sampling analysis at the trajectory level. Consider i.i.d. rollouts from πθ(x) with success probability µpos(x) and rare-correct mass τ (x) (Section 2.1), where 0 < τ (x) < µpos(x). Let denote the number of correct rollouts among the samples. For group-relative methods such as GRPO, the learning signal vanishes when all sampled rewards are identical, i.e., {0, }. Define the active event AN := {0 < < }, (9) with probability Pr(AN ) = 1µpos(x)N (1µpos(x))N . Let Yi = I[rollout Crare(x)], so Pr(Yi = 1) = τ (x). We are interested in the event that the update is active yet the rare-correct region receives no samples: Bτ := AN (cid:110) (cid:88) i=1 Yi = (cid:111) . (10) Lemma 3.1. For any 1, writing µpos = µpos(x) and τ = τ (x) for brevity, Pr(Bτ ) = (1 τ )N (µpos τ )N (1 µpos)N . (11) The proof partitions rollouts into three disjoint regions and applies inclusion-exclusion (Appendix A). Equation (11) reveals non-monotonic dependence on . Two competing effects determine Pr(Bτ ): the coverage factor (1τ )N decreases with , improving the chance of sampling rare-correct modes, while activity Pr(AN ) increases from near zero toward one. Their interaction produces three distinct regimes (Figures 1(a) and 2): F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Figure 2. Tail-miss probability Pr(Bτ ) from Lemma 3.1 versus group size . Each panel fixes µpos {0.8, 0.5, 0.2}; curves vary ρ = τ /µpos, the fraction of correct mass in the rare-correct region. Stars mark peaks. For all parameter combinations, Pr(Bτ ) peaks at intermediate : small yields low activity, large yields good coverage, but moderate combines active groups with poor coverage of rare modes. Smaller ρ shifts the peak rightward and upward. Small (e.g., = 2): Activity Pr(AN ) is low, most groups are homogeneous, yielding zero learning signal. The policy changes slowly from the base model, preserving output diversity. This regime favors pass@k for large but limits pass@1 improvement, consistent with the finding that minimal group sizes maintain diversity at the cost of sample efficiency (Wu et al., 2025b; Dang et al., 2025). Intermediate : Pr(Bτ ) peaks; updates are frequently active yet often miss rare-correct modes. He et al. (2025) observe this regime at = 32: pass@1 improves while pass@k for large degrades, indicating distribution sharpening. Large : Coverage improves as (1 τ )N 0 and unsampled mass diminishes. This is the regime analyzed by (Hu et al., 2025), where scaling stabilizes learning and can improve both metrics. This framework reconciles the seemingly contradictory recommendations: small preserves diversity through inactivity; large through coverage; intermediate , most common in practice due to computational constraints, is where sharpening is most likely. Figure 1(b,c) illustrates this empirically (in-domain: AIME 2025; OOD: IFEval). At = 8, pass@1 improves relative to = 2 but pass@256 degrades, reflecting the sharpening trade-off. Increasing to 32 improves both pass@1 and pass@256 compared to = 8, consistent with Hu et al. (2025); in our setup = 32 falls in the large-N regime, whereas for He et al. (2025) it was intermediate. This shift in regime boundaries, determined by µpos, τ , and their evolution during training, also explains the smaller degradation on OOD IFEval. 3.2. Unsampled-correct mass under finite sampling The tail-miss analysis identifies when rare-correct modes are vulnerable (intermediate where Pr(Bτ ) peaks). We now use the categorical framework (Section 2.3) to characterize the mechanism by which their mass decreases. While (8) shows that total correct mass Qpos tends to increase with , it does not reveal redistribution within the correct set. Define the unsampled-correct mass Qu,pos := (cid:88) iU pi = Qpos Ppos. (12) This quantity measures how much correct probability is left behind by sampling. Proposition 3.2. Under the one-step surrogate update (7), Qu,pos = (cid:104) η SR Upos,2 (cid:123)(cid:122) (cid:125) (cid:124) direct drift Qu,pos (cid:0)(RcSR)A2 + (RwSR)B2 SRU2 (cid:123)(cid:122) (cid:124) normalization coupling (cid:1) (cid:125) (13) (cid:105) . The proof applies the subset-mass identity from Appendix with = P; see Appendix for details. Equation (13) shows that Qu,pos can be negative even when Qpos > 0: RLVR can increase total correct mass while concentrating it onto sampled-correct actions at the expense of unsampled-correct ones. This complements Hu et al. (2025), who showed that reward-positive batches (SR > 0) push unsampled logits downward. Our formula makes explicit how this affects redistribution within the correct set. The mechanism operates through two terms. The direct drift SRUpos,2 pushes unsampled-correct mass downward when SR > 0, with magnitude scaling with the concentration Upos,2. The normalization coupling (analyzed in detail in Appendix E) captures how probability gains by sampledcorrect actions draw mass away from unsampled-correct ones through softmax normalization. In reward-positive batches, both terms contribute negatively. As Hu et al. (2025) observe, scaling suppresses Upos,2 and ensures Qpos 0 with the direct drift term tending to zero. However, practical constraints limit how far can be scaled: computational cost grows linearly with , and improving pass@1 requires active groups (ruling out very small where most groups are homogeneous). This 4 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare places typical RLVR training in the intermediate-N regime identified in Section 3.1, where Pr(Bτ ) peaks. 4. F-GRPO: Focal weighting for Group-Relative Policy Optimization The categorical analysis in Section 3.2 identifies SR > 0 as the condition driving concentration of correct mass. To operationalize this insight at the trajectory level, we need an observable per-prompt statistic that tracks the magnitude of the SR-driven drift. 4.1. Focal Weight"
        },
        {
            "title": "Define the empirical success rate for prompt x as",
            "content": "="
        },
        {
            "title": "X\nN",
            "content": "[0, 1], (cid:98)µpos(x) := R(x) Rw Rc Rw where is the number of correct rollouts and R(x) = 1 i=1 Ri is the group mean reward. This is an unbiased estimator of the true success probability: E[(cid:98)µpos(x)] = µpos(x). (cid:80)N (14) The token-level categorical analysis in Section 3.2 identifies SR > 0 as the regime driving concentration, but SR depends on the (unobserved) policy mass of distinct sampled rollouts. At the trajectory level, we therefore use (cid:98)µpos(x) = X/N as an observable proxy for this regime: under i.i.d. rollout sampling, conditioning on = implies the distinct sampled-correct mass E[Ppos = k] is non-decreasing in and E[Pneg = k] is non-increasing in k, so for standard RLVR rewards Rw 0, E[SR = k] is nondecreasing in k. See Appendix for formal proof of this sampling monotonicity. In the categorical update, unsampled actions receive zero reward but are still affected by the baseline subtraction, yielding zi SRpi for (Eq. (7)); as result, the downward drift on unsampledcorrect mass is strongest in reward-positive batches where SR > 0 (Section 3.2). We thus aim to reduce updates on prompts that are likely to fall into this regime, using (cid:98)µpos(x) as per-prompt proxy signal. Because E[SR = k] is non-decreasing in k, higher (cid:98)µpos(x) marks prompts where concentration pressure is strongest. Since high (cid:98)µpos(x) indicates easy/high-success prompts, we adopt functional form inspired by Focal loss (Lin et al., 2017) to down-weight their updates, where the drift mechanism is most pronounced. Define the difficulty weight g(x) := (cid:0)1 (cid:98)µpos(x)(cid:1)γ , γ 0. (15) When γ = 0, g(x) = 1 for all prompts, recovering standard GRPO. For γ > 0, prompts with high empirical success rate receive reduced weight: g(x) 0 as (cid:98)µpos(x) 1. Figure 3. Scaled advantage magnitude g(x) (cid:98)AGRPO versus success probability µpos(x) for binary rewards. Solid lines: correct rollouts; dashed lines: incorrect rollouts. Higher γ suppresses updates on high-success prompts, shifting gradient contribution toward prompts where the policy succeeds less frequently. 4.2. Integration with Group-Relative Methods We incorporate the difficulty weight by scaling the grouprelative advantage: (cid:98)AFGRPO := g(x) (cid:98)AGRPO . (16) This modification applies to any method using grouprelative advantages. While DAPO (Yu et al., 2025) and CISPO (Chen et al., 2025a) modify the clipping mechanism and importance weighting respectively, the concentration phenomenon we address arises from the sampling dynamics of group-relative advantage estimation, not from these algorithmic choices. The Focal weight g(x) is thus orthogonal and can be applied independently. We denote the Focalweighted variants as F-DAPO and F-CISPO. The modification is minimal: single scalar g(x) [0, 1] applied uniformly to all rollouts from the same prompt. No additional networks are required; γ is the only new hyperparameter. 4.3. Effect of Focal Weighting Figure 3 visualizes the effect of Focal weighting. With binary rewards, the GRPO advantage magnitudes vary with µpos(x). The Focal weight g(x) = (1 (cid:98)µpos(x))γ scales these magnitudes, suppressing updates on highsuccess prompts. Analogous to Focal loss suppressing wellclassified examples, this reduces gradient contribution from prompts where the concentration mechanism of Section 3.2 is most active. F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Figure 4. Categorical policy simulation following Hu et al. (2025) setup. (a) Total correct mass Qpos vs. training step. (b) Retained positive mass Mret vs. step. (c) Final metrics vs. group size , with three regimes: slow Qpos growth, diversity preserved; II concentration zone (shaded), Qpos grows but Mret collapses; III both metrics high. Solid: γ=0; dashed: γ=1. =131k maintains Mret1 throughout, consistent with Pr(Bτ ) < 103 (Appendix J). 5. Experiments & Results 5.1. Empirical Validation via Categorical Simulation To complement the simulation analysis of Hu et al. (2025), we conduct experiments under the same categorical policy framework (Section 2.3) with an additional focus on which correct actions retain probability mass. Following Hu et al. (2025), we simulate softmax policy over 128,000 actions (10,000 correct) trained with group-relative updates; see Appendix for details. Beyond tracking total correct mass Qpos, we track Mret(t), the retained positive mass, measuring the fraction of initial correct-action probability that remains at or above its starting value (Appendix Eq. 26). Values near 1 indicate diversity preservation; values near 0 indicate concentration onto subset of solutions. Figure 4 presents the results. Panel (a) confirms that Qpos increases for all group sizes, consistent with Hu et al. (2025). However, panel (b) support that Mret behaves nonmonotonically: both small and large preserve diversity, while intermediate values suffer severe concentration. This demonstrates that Qpos > 0 does not guarantee preservation of unsampled correct actions. Panel (c) summarizes the final state across all group sizes, with three regimes labeled: (I) small where Qpos grows slowly but diversity is preserved; (II) the concentration zone (shaded) where Qpos grows rapidly but Mret collapses; and (III) large where both metrics are high. Notably, =131,072 maintains Mret 1 throughout training, consistent with Lemma 3.1, which predicts Pr(Bτ ) < 103 at this group size (see Appendix J). Dashed lines (γ=1) show improved Mret retention, particularly in the concentration zone. In this single-tree setting, Focal weighting suppresses updates on high-success batches where concentration pressure peaks; with multiple prompts, it additionally reallocates gradient toward harder examples. The specific boundaries of the concentration zone depend on the initial distribution and should not be interpreted as quantitative predictions for LLM training. The key insight is the qualitative pattern: intermediate group sizes can exhibit worse diversity than either extreme. 5.2. LLM Experimental Setup Models & Datasets. We evaluate on Qwen2.5-7B (Yang et al., 2024a), Qwen2.5-1.5B-Math (Yang et al., 2024b), and Llama-3.2-3B-Instruct (Grattafiori et al., 2024), covering different model families and scales. All models are trained on DeepScaleR (Luo et al., 2025), challenging dataset of competition-level mathematics problems. Training. We implement our method using the verl framework (Sheng et al., 2024). Key hyperparameters: global batch size 256, mini-batch size 64, learning rate 1106, and 10 training epochs. The γ is selected from {0.5, 1.0} based on average math pass@1 on the best checkpoint. Full training details are in Appendix H. Evaluation. We report pass@1 and pass@256 to measure single-attempt accuracy and solution diversity. For in-domain evaluation, we use standard mathematical reasoning benchmarks: MATH500 (Hendrycks et al., 2021), AIME24/25 (Art of Problem Solving, 2024a), AMC23 (Art of Problem Solving, 2024b), Minerva Math (Lewkowycz et al., 2022), and Olympiad Bench (He et al., 2024). To assess whether diversity benefits transfer beyond the training distribution, we include out-of-domain (OOD) benchmarks spanning distinct reasoning types: GPQA Diamond (Rein et al., 2023) (graduate-level science QA), IFEval (Zhou et al., 2023) (instruction following), and SynLogic (Liu et al., 2025a) (synthetic logical reasoning). Evaluation details are in Appendix H. 5.3. Group Size Regimes and Focal Weighting Having observed the three-regime pattern in categorical simulation (Section 5.1), we examine whether analogous behavior arises in LLM training. Table 2 compares GRPO at {2, 8, 32} with F-GRPO at = 8 on Qwen2.56 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Method Avg. AIME24 AIME25 AMC MATH500 Minerva Olympiad Avg. OOD IFEval SynLogic GPQA In-domain Out-of-domain Qwen2.5-7B 37.3/64.1 15.0/37.7 6.7/40.8 52.9/87.3 75.8/92.8 36.0/60.2 37.8/65.8 GRPO F-GRPO 38.6/70.3 15.9/46.2 10.1/52.6 56.2/96.3 76.2/95.1 35.7/60.3 37.5/71.6 39.4/69.3 16.8/49.8 12.0/45.6 53.3/91.9 78.6/95.2 35.5/61.2 40.5/71.8 DAPO F-DAPO 40.5/72.5 20.9/53.4 11.5/52.9 55.9/93.7 79.1/96.6 35.0/62.9 40.9/75.6 39.5/73.2 14.6/45.9 9.7/59.8 57.8/96.1 78.7/97.0 34.7/63.3 41.5/76.9 CISPO F-CISPO 39.5/76.8 14.8/59.7 13.0/64.6 53.3/97.1 79.0/97.8 34.6/64.3 42.4/77.5 Qwen2.5-1.5B-Math 36.7/74.4 13.8/61.1 9.9/58.0 53.1/96.2 75.4/95.6 31.9/61.1 36.3/74.3 GRPO F-GRPO 36.3/74.5 13.0/60.7 10.5/57.9 51.6/95.9 74.7/96.1 31.0/61.0 37.0/75.5 37.7/74.3 16.5/58.4 9.8/59.2 54.5/95.2 76.5/96.2 32.6/63.5 36.4/73.2 DAPO F-DAPO 37.8/76.0 16.1/61.1 10.3/61.0 54.4/97.0 76.8/97.0 32.2/63.8 37.2/76.2 38.9/72.9 16.8/60.8 10.5/53.8 58.6/95.7 77.3/95.5 32.6/59.8 37.6/71.9 CISPO F-CISPO 37.4/76.1 14.5/64.2 11.2/59.7 53.8/99.1 76.8/96.5 31.7/63.2 36.4/74.0 Llama-3.2-3B-Instruct 23.0/59.9 10.7/40.7 0.7/21.5 30.5/88.2 55.0/90.6 21.8/59.0 19.4/59.3 GRPO F-GRPO 23.0/63.4 12.1/46.1 1.0/29.5 29.8/90.6 54.1/92.9 21.0/60.1 20.1/61.3 24.3/54.2 12.8/40.8 1.0/18.5 33.1/79.5 55.9/83.8 22.4/54.1 21.0/48.4 DAPO F-DAPO 24.8/62.3 11.1/44.4 1.7/28.7 31.9/88.3 58.6/92.0 22.3/59.3 23.2/61.3 1.0/25.4 32.9/79.1 56.9/89.1 21.8/59.5 22.5/55.4 CISPO F-CISPO 24.5/59.7 10.6/42.8 2.0/24.5 34.1/82.6 56.5/91.0 22.1/58.8 21.5/58.7 24.1/58.0 9.7/39. 17.1/55.9 19.2/63.3 15.7/58.4 17.9/63.6 14.9/59.0 18.1/65.9 7.9/43.1 8.3/46.5 8.7/45.4 9.1/46.3 8.6/41.0 10.1/47.7 25.5/56.5 25.4/57.6 23.9/51.3 24.8/55.4 25.7/52.5 25.0/53.0 32.1/70.3 7.9/51.3 11.3/46.2 34.0/75.7 8.7/57.0 15.0/57.3 24.1/67.1 7.5/53.3 15.4/54.9 30.8/71.1 7.9/62.4 15.0/57.4 24.2/67.9 8.0/53.6 12.6/55.5 30.7/70.6 8.2/60.0 15.4/67.1 12.2/52.0 4.9/27.4 6.6/50.1 11.4/55.4 4.8/27.5 8.8/56.5 12.7/50.0 5.0/26.9 8.6/59.4 13.2/51.8 4.9/26.4 9.2/60.7 13.2/48.2 5.2/26.3 7.4/48.4 13.4/52.9 4.9/26.1 12.0/64.2 54.1/78.0 4.7/36.4 17.5/55.1 56.4/79.6 4.6/35.5 15.2/57.6 51.2/77.8 4.8/28.9 15.7/47.0 53.0/79.5 4.3/33.0 17.0/53.7 54.6/78.4 4.3/29.4 18.2/49.7 52.6/77.3 5.4/33.9 17.0/47. Table 1. Pass@1 / pass@256 across three models and six methods at =8. Focal weighting (F-GRPO, F-DAPO, F-CISPO) consistently improves pass@256 with stable or improved pass@1. Bold: better within baseline/Focal pair; underline: statistically significant (p<0.05, see Appendix I). 7B. These values are chosen to span different operating regimes while keeping rollout cost tractable; we do not aim to exhaustively map performance as function of . GRPO exhibits non-monotonic behavior: =2 yields highest pass@256 but lowest pass@1, pattern consistent with diversity preservation through infrequent active updates. At =8, pass@1 improves but pass@256 drops to its lowest values across both in-domain and OOD benchmarks, suggesting distribution sharpening, consistent with prior observations (Yue et al., 2025; Dang et al., 2025). At =32, pass@256 partially recovers while pass@1 continues to improve. This pattern aligns qualitatively with the three-regime framework of Section 3.1. Method Avg. Math Avg. OOD NLLrare GRPO =2 GRPO =8 GRPO = 36.2 / 75.0 37.3 / 64.1 39.2 / 70.1 18.0/ 67.3 17.1 / 55.9 17.7 / 61.7 F-GRPO =8 38.6/ 70.3 19.2 / 63.3 0.19 0.68 0. 0.46 Table 2. Comparison of GRPO at varying group sizes versus FGRPO at fixed =8 on Qwen2.5-7B. Metrics: average pass@1 / pass@256 on in-domain math and OOD benchmarks. NLLrare: increase in negative log-likelihood on trajectories that were correct but low-probability under the base model (lower = less deviation from base distribution; see Appendix F.2). Bold: best; : second best. Full per-benchmark results in Appendix F. At =8, F-GRPO matches GRPO at =32 on pass@256 (70.3 vs. 70.1 on math; 63.3 vs. 61.7 on OOD) using 4 fewer rollouts. Pass@1 shows modest trade-off on indomain benchmarks but improves on OOD tasks, suggesting that Focal weighting can mitigate concentration relative to GRPO at higher rollout budgets (e.g., N=32) without increasing the rollout budget in this setting. Deviation from Base-Model Rare Solutions. We report NLLrare, an empirical proxy for redistribution of probability mass away from solutions that were correct but low-probability under the base model (details in Appendix F.2). Higher values indicate greater deviation from the base distribution on these trajectories. The ordering NLLrare(N =2) < NLLrare(N =32) < NLLrare(N =8) mirrors the pass@256 ordering, with FGRPO at =8 achieving an intermediate value (0.46) that reflects reduced concentration relative to its baseline. 5.4. Focal Weighting Across Methods We evaluate Focal weighting on GRPO, DAPO, and CISPO at =8, commonly used group size (Shao et al., 2024; Zeng et al., 2025; Liu et al., 2025d). Table 1 reports results across three model families and scales. On Qwen2.5-7B, Focal weighting improves average math pass@256 by +6.2 (GRPO), +3.2 (DAPO), and +3.6 (CISPO) points while pass@1 improves or remains sta7 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare ble; OOD metrics show consistent gains in both pass@1 (up to +3.2) and pass@256 (up to +7.4). On Llama3.2-3B-Instruct, all methods show pass@256 gains (+3.5, +8.1, +1.7) with stable pass@1. On Qwen2.5-1.5B-Math, pass@256 improves consistently though some configurations show minor pass@1 trade-offs. Across all nine method-model combinations, Focal weighting improves both math and OOD pass@256 (average +3.5 and +3.8). Notably, OOD pass@1 also improves in 7/9 cases (average +1.1), suggesting that preserving solution diversity benefits generalization without sacrificing singleattempt accuracy. 5.5. Comparison with Entropy and KL Regularization We compare F-GRPO against common diversity-preserving regularizers: GRPO with entropy bonus (GRPO-H) and GRPO with KL penalty (GRPO-KL) using Qwen2.5-7B setup. We tune coefficients following Appendix H; full results in Appendix G. F-GRPO achieves the highest math pass@1 (38.6 vs. 37.8/37.2) and OOD pass@256 (63.3 vs. 59.9/60.0). GRPO-KL obtains higher math pass@256 (72.0 vs. 70.3), but requires maintaining reference model in memory, increasing computational overhead. F-GRPO provides simpler alternative with stronger pass@1 and OOD transfer. 6. Related Work Distribution Sharpening in RLVR. growing body of work documents that RLVR improves pass@1 while degrading pass@k for large k, indicating concentration onto fewer solutions (Dang et al., 2025; Yue et al., 2025; Wu et al., 2025a). Chen et al. (2025b) attribute this to overconfidence induced by cross-entropy training and propose confidence limiting. We provide complementary perspective: finitesampling failure mode in group-relative methods where active updates systematically miss rare-correct modes. Group Size and Sampling Dynamics. The optimal rollout count remains debated. Wu et al. (2025b) show that =2 is theoretically justified and compute-efficient, while Hu et al. (2025) advocate large groups for coverage, showing that scaling ensures non-negative change in total correct mass. We derive closed-form tail-miss probability that reconciles these findings: both small and large preserve diversity (through inactivity and coverage respectively), while intermediate , most common in practice, maximizes the probability of active updates that miss rare-correct regions. Difficulty-Aware Training. Reweighting by difficulty has established roots in Focal loss (Lin et al., 2017) and curriculum learning (Bengio et al., 2009; Parashar et al., 2025). In RLVR, Zhou et al. (2025) dynamically rebalances loss contributions across difficulty groups to equalize loss scale. He et al. (2025) identify rank bias in GRPO and propose unlikeliness reward to up-weight rare correct trajectories. Concurrently, Gai et al. (2025) analyze selection and reinforcement bias, proposing differential smoothing that modifies rewards differently for correct versus incorrect trajectories. Our approach shares the motivation of addressing sharpening but differs in mechanism: rather than modifying trajectorylevel rewards, we scale the entire gradient contribution of high-success prompts, directly targeting the SR > 0 regime identified in our analysis. Entropy and Token-level Approaches. The role of entropy in RLVR remains debated, with some advocating maximization for exploration (Cui et al., 2025; Cheng et al., 2025) and others reporting benefits from minimization (Agarwal et al., 2025). Several methods address token-level concentration by reweighting tokens based on entropy dynamics or probability structure (Hao et al., 2026; Peng et al., 2025; Wang et al., 2025). These approaches regulate how probability mass is distributed within trajectories; our Focal weighting is orthogonal, regulating which prompts contribute to learning. 7. Conclusion This work identifies finite group size as critical factor driving distribution sharpening in group-relative RLVR with binary rewards, where intermediate rollout counts, most common in practice due to computational constraints, systematically suppress rare-correct trajectories while concentrating mass onto common solutions. Our theoretical analysis derives closed-form tail-miss probability exhibiting non-monotonic dependence on : small groups preserve diversity through inactivity, large groups through coverage, but intermediate maximizes active updates that miss rare-correct modes. We further characterize redistribution within the correct set, proving that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose Focal weighting, lightweight difficulty, aware advantage scaling applicable to any grouprelative objective including GRPO, DAPO, and CISPO. Empirically, we validate the three-regime behavior across different values and demonstrate consistent pass@256 improvements while preserving or improving pass@1 across three model families, at no extra computational cost. This work provides both theoretical lens for RLVR sampling dynamics and practical, drop-in solution for maintaining solution diversity in group-relative policy optimization."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Zhang, Z., Yuan, L., Han, J., and Peng, H. The unreasonable effectiveness of entropy minimization in 8 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Art of Problem Solving. Aime problems and solutions. https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions, 2024a. Accessed: 2025-04-20. Art of Problem Solving. Amc problems and solutions. https://artofproblemsolving.com/wiki/ index.php?title=AMC_Problems_and_ Solutions, 2024b. Accessed: 2025-04-20. Asmussen, S. and Glynn, P. W. Stochastic simulation: algorithms and analysis, volume 57. Springer, 2007. Curriculum learning. Bengio, Y., Louradour, J., Collobert, R., and Weston, the J. 26th International Conference on Machine Learning (ICML), pp. 4148. ACM, 2009. doi: 10.1145/1553374. URL https://ronan.collobert. 1553380. com/pub/2009_curriculum_icml.pdf."
        },
        {
            "title": "In Proceedings of",
            "content": "Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025a. Chen, F., Raventos, A., Cheng, N., Ganguli, S., and Druckmann, S. Rethinking fine-tuning when scaling test-time compute: Limiting confidence improves mathematical reasoning. arXiv preprint arXiv:2502.07154, 2025b. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. 2021. Cheng, D., Huang, S., Zhu, X., Dai, B., Zhao, W. X., Zhang, Z., and Wei, F. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Chollet, F., Knoop, M., Kamradt, G., Landers, B., and Pinkard, H. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., Liu, Z., Peng, H., Bai, L., Ouyang, W., Cheng, Y., Zhou, B., and Ding, N. The entropy mechanism of reinforcement learning for reasoning language models, 2025. URL https: //arxiv.org/abs/2505.22617. Dang, X., Baek, C., Wen, K., Kolter, Z., and Raghunathan, A. Weight ensembling improves reasoning in language models. arXiv preprint arXiv:2504.10478, 2025. Gai, J., Zeng, G., Zhang, H., and Raghunathan, A. Differential smoothing mitigates sharpening and improves llm reasoning. arXiv preprint arXiv:2511.19942, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hao, Z., Wang, H., Liu, H., Luo, J., Yu, J., Dong, H., Lin, Q., Wang, C., and Chen, J. Rethinking entropy interventions in rlvr: An entropy change perspective, 2026. URL https://arxiv.org/abs/2510.10150v2. He, A. W., Fried, D., and Welleck, S. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2555925571, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., and et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. URL https: //arxiv.org/abs/2103.03874. Hu, J., Liu, M., Lu, X., Wu, F., Harchaoui, Z., Diao, S., Choi, Y., Molchanov, P., Yang, J., Kautz, J., et al. Brorl: Scaling reinforcement learning via broadened exploration. arXiv preprint arXiv:2510.01180, 2025. Hugging Face. Math-verify: robust mathematical expression evaluation system. https://github.com/ huggingface/Math-Verify, 2026. GitHub repository, commit ba3d3aa (latest at time of access), accessed 2026-01-25. F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language modarXiv preprint els resolve real-world github issues? arXiv:2310.06770, 2023. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and Agarwal, R. The art of scaling reinforcement learning compute for llms. arXiv preprint arXiv:2510.13786, 2025. doi: 10.48550/arXiv.2510.13786. URL https: //arxiv.org/abs/2510.13786. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., and et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35: 38433857, 2022. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollar, P. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017. Liu, J., Fan, Y., Jiang, Z., Ding, H., Hu, Y., Zhang, C., Shi, Y., Weng, S., Chen, A., Chen, S., Huang, Y., Zhang, M., Zhao, P., Yan, J., and He, J. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025a. doi: 10.48550/arXiv.2505.19641. URL https: //arxiv.org/abs/2505.19641. Version v4, last revised 4 Jun 2025. Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025b. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025c. Liu, Z., Liu, J., He, Y., Wang, W., Liu, J., Pan, L., Hu, X., Xiong, S., Huang, J., Hu, J., et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025d. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on LearnURL https: ing Representations (ICLR), 2019. //openreview.net/forum?id=Bkg6RiCqY7. Poster. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W., Roongta, M., Cai, C., Luo, J., Zhang, T., Li, E., Popa, R. A., and Stoica, I. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Matsutani, K., Takashiro, S., Minegishi, G., Kojima, T., Iwasawa, Y., and Matsuo, Y. Rl squeezes, sft expands: comparative study of reasoning llms, 2025. URL https: //arxiv.org/abs/2509.21128. Ni, K., Tan, Z., Liu, Z., Li, P., and Chen, T. Can grpo help llms transcend their pretraining origin? arXiv preprint arXiv:2510.15990, 2025. Parashar, S., Gui, S., Li, X., Ling, H., Vemuri, S., Olson, B., Li, E., Zhang, Y., Caverlee, J., Kalathil, D., et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. Simko: Peng, R., Ren, Y., Yu, Z., Liu, W., and Wen, Simple pass@k policy optimization, URL https://arxiv.org/abs/2510. Y. 2025. 14807. arXiv:2510.14807v2, last revised 21 Oct 2025. Politis, D. N., Romano, J. P., and Wolf, M. subsampling. Springer Series in Statistics. Springer, New York, 1999. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. doi: 10.48550/ URL https://arxiv.org/ arXiv.2311.12022. abs/2311.12022. Submitted on 20 Nov 2023. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. URL https://arxiv. org/abs/2409.19256. Submitted: 28 Sep 2024; PDF available. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. 10 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12):38483860, 2023. doi: 10.14778/3611540.3611569. URL https: //doi.org/10.14778/3611540.3611569. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Sglang: Efficient execution of structured language model programs. arXiv preprint arXiv:2312.07104, 2023. doi: 10.48550/arXiv.2312. 07104. URL https://arxiv.org/abs/2312. 07104. Submitted 12 Dec 2023; revised (v2) 6 Jun 2024. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. doi: 10.48550/arXiv.2311. 07911. URL https://arxiv.org/abs/2311. 07911. Submitted on 14 Nov 2023. Zhou, J., Ma, L., Liang, H., Shen, C., Cui, B., and Zhang, W. Daro: Difficulty-aware reweighting policy optimization, 2025. URL https://arxiv.org/abs/2510. 09001. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Wu, F., Xuan, W., Lu, X., Harchaoui, Z., and Choi, Y. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843, 2025a. Wu, Y., Ma, L., Ding, L., Li, M., Wang, X., Chen, K., Su, Z., Zhang, Z., Huang, C., Zhang, Y., et al. It takes two: Your grpo is secretly dpo. arXiv preprint arXiv:2510.00977, 2025b. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024b. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuan, L., Chen, W., Zhang, Y., Cui, G., Wang, H., You, Z., Ding, N., Liu, Z., Sun, M., and Peng, H. From (x) and g(x) to (g(x)): Llms learn new skills in rl by composing old ones. arXiv preprint arXiv:2509.25123, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Zhang, K., Zuo, Y., He, B., Sun, Y., Liu, R., Jiang, C., Fan, Y., Tian, K., Jia, G., Li, P., et al. survey of 11 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare A. Proof of Lemma 3.1 Proof. Fix prompt and omit (x) for readability. Each rollout falls into one of three disjoint regions: the rare-correct region Crare with probability τ , the remaining correct region Crare with probability µpos τ , or the incorrect region Ω with probability 1 µpos. The probability that no rollout lies in the rare-correct region is (1 τ )N . Conditioned on this event, all rollouts lie in (C Crare) (Ω C). The group is inactive (hence Bτ does not occur) in two disjoint cases: all rollouts are correct but not rare-correct, with probability (µpos τ )N ; or all rollouts are incorrect, with probability (1 µpos)N . Thus Pr(Bτ ) = (1 τ )N (µpos τ )N (1 µpos)N . B. Monotonicity of Sampled Distinct Mass Conditioned on This appendix formalizes the monotonicity claim used in Section 4 (Focal Weight): although the categorical baseline SR depends on the probability mass of distinct sampled rollouts, its conditional expectation is monotone in the observed correct count X. Setup. Fix prompt and write π(o) := πθ(o x) for brevity. Let Ωx be the rollout space and := C(x) Ωx the set of correct rollouts (Section 2.1). Sample i.i.d. rollouts o1, . . . , oN π(), and let := (cid:80)N I[oi C] be the number of correct rollouts. i=1 Define the distinct sampled sets := { oi : oi }, := { oi : oi / }, where braces denote set (duplicates removed). Define the corresponding sampled masses Ppos := (cid:88) oA π(o), Pneg := π(o). (cid:88) oB These are the trajectory-level analogues of the categorical quantities in Section 2.3. As in that section, define SR := Rc Ppos + Rw Pneg. (17) Conditional Distributions. Let µpos := Proπ[o C]. For C, define the conditional (restricted) distribution Similarly, for / C, define qpos(o) := Pr[oi = oi C] = π(o) µpos . qneg(o) := Pr[oi = oi / C] = π(o) 1 µpos . By exchangeability of i.i.d. sampling, conditioning on = implies that the correct rollouts are i.i.d. from qpos over C, and the incorrect rollouts are i.i.d. from qneg over Ωx C. Lemma B.1. For all integers {0, 1, . . . , }, E[Ppos = k] = E[Pneg = k] = (cid:88) oC (cid:88) /C π(o) (cid:16) 1 (1 qpos(o))k(cid:17) , π(o) (cid:16) 1 (1 qneg(o))N k(cid:17) . (18) (19) Moreover, E[Ppos = k] is non-decreasing in k, and E[Pneg = k] is non-increasing in k. Proof. We prove the statement for Ppos; the argument for Pneg is identical with in place of k. F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Condition on = k. For any fixed C, the event {o A} is exactly the event that appears at least once among the correct i.i.d. draws from qpos. Thus Using linearity of expectation and the definition Ppos = (cid:80) oC π(o) I{o A}, Pr(o = k) = 1 (1 qpos(o))k. E[Ppos = k] = (cid:88) oC π(o) Pr(o = k) = π(o) (cid:16) 1 (1 qpos(o))k(cid:17) , (cid:88) oC which is (18). To show monotonicity, compute the discrete difference: E[Ppos = k+1] E[Ppos = k] = π(o) (1 qpos(o))k qpos(o) 0, (cid:88) oC so E[Ppos = k] is non-decreasing in k. For Pneg, conditioned on = k, each / is included in with probability 1 (1 qneg(o))N k. This yields (19). Since decreases as increases and (cid:55) 1 (1 q)m is non-decreasing in m, it follows that E[Pneg = k] is non-increasing in k. Corollary B.2. Assume standard RLVR rewards Rc > Rw and Rw 0 (Section 2.1). Then E[SR = k] is nondecreasing in k. Proof. By definition (17) and linearity of expectation, E[SR = k] = Rc E[Ppos = k] + Rw E[Pneg = k]. By Lemma B.1, the first term is non-decreasing in because Rc > 0, and the second term is also non-decreasing in because Rw 0 and E[Pneg = k] is non-increasing in k. Hence their sum is non-decreasing in k. C. First-order Softmax Expansion and Subset-mass Identity This appendix records standard first-order identities for the softmax map that underlie the analysis in Section 3. Let = softmax(z) over and consider small logit perturbation z. The softmax Jacobian pi zj implies the first-order probability change = pi(1{i = j} pj) pi = (cid:88) jA pi zj zj = pi (cid:16) zi pjzj (cid:17) . (cid:88) jA (20) For any subset A, define its probability mass QS := (cid:80) identity: iS pi. Summing (20) over yields the subset-mass QS := pi = pizi QS (cid:88) (cid:88) iS iS (cid:88) jA pjzj. (21) The first term captures the direct effect of logit changes on actions in S, while the second term captures the indirect effect through softmax normalization: when probability mass moves elsewhere, QS changes even if the logits of actions in are unchanged. Application to Qpos. Setting = and using the one-step update (7) recovers the mass balance equation (8) of (Hu et al., 2025). Application to Qu,pos. Setting = (unsampled correct actions) yields Proposition 3.2. The key observation is that for , we have Ri = 0, so zi = η SRpi from (7). F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare D. Proof of Proposition 3.2 Proof. Apply the subset-mass identity (Appendix C, Eq. (21)) with = P: Qu,pos = (cid:88) iU pizi Qu,pos (cid:88) jA pjzj. For P, we have Ri = 0, so by (7), zi = η (cid:88) iU pizi = SRpi. Thus the first sum becomes η SR Upos,2. p2 = η (cid:88) SR iU For the normalization term, partitioning by reward value: (cid:88) jA pjzj = = η η (cid:88) p2 (Rj SR) jA (cid:104) (Rc SR)A2 + (Rw SR)B2 SRU2 (cid:105) , (22) (23) (24) where we used Rj = Rc for A, Rj = Rw for B, and Rj = 0 for . Substituting both expressions yields (13). E. Detailed Term Analysis for Proposition 3.2 We analyze each term in (13) to understand when unsampled-correct mass decreases. Direct drift term. The term SR Upos,2 arises because unsampled actions receive zero reward but are still affected by the baseline subtraction. When SR > 0 (reward-positive batch), this term is negative and pushes unsampled-correct mass downward. The magnitude scales with Upos,2, the concentration of unsampled-correct probability. Normalization coupling. The second term couples Qu,pos to the mass changes elsewhere. The factor in parentheses has three components: (Rc SR)A2 0: sampled-correct actions gain probability, which through normalization draws mass away from unsampled-correct actions. (Rw SR)B2 0: sampled-incorrect actions lose probability, which through normalization donates mass to all other actions including unsampled-correct ones. SRU2: when SR > 0, unsampled actions (both correct and incorrect) lose probability through baseline subtraction. When does Qu,pos < 0 while Qpos > 0? Consider reward-positive batch (SR > 0) on prompt with high success probability. In this regime: The direct drift SRUpos,2 < 0 actively pushes unsampled-correct mass down. The normalization coupling is dominated by (Rc SR)A2 > 0 when sampled-correct mass is concentrated, further draining unsampled-correct mass. Meanwhile, Qpos from (8) remains positive because its first two terms (mass transfer from incorrect to correct pool) outweigh the unsampled coupling. Thus RLVR can increase total correct mass while concentrating it onto the sampled-correct subset, shrinking the probability of correct actions that happen not to be sampled. F. Group Size Comparison: Full Results F.1. Per-Benchmark Results Table 3 provide full per-benchmark results for the group size comparison discussed in Section 5.3. 14 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare In-domain Out-of-domain Method Avg. AIME24 AIME25 AMC MATH500 Minerva Olympiad Avg. OOD IFEval SynLogic GPQA 36.2 / 75.0 12.7 / 59.1 8.3 / 56.0 51.9 / 97.0 74.5 / 96.7 33.2 / 65.6 36.7 / 75.7 18.0/ 67.3 29.4 / 77.2 6.7 / 54.3 17.8 / 70.3 GRPO =2 37.3 / 64.1 15.0/ 37.7 6.7 / 40.8 52.9 / 87.3 75.8 / 92.8 36.0 / 60.2 37.8/ 65.8 17.1 / 55.9 32.1/ 70.3 7.9 / 51.3 11.3 / 46.2 GRPO =8 GRPO =32 39.2 / 70.1 13.0 / 50.2 10.4 / 49.5 60.9 / 95.5 77.3 / 94.3 34.9 / 59.9 38.9 / 71.3 17.7 / 61.7 31.0 / 71.4 8.9 / 61.6 13.4 / 51.9 F-GRPO =8 38.6/ 70.3 15.9 / 46.2 10.1/ 52.6 56.2/ 96.3 76.2/ 95.1 35.7/ 60.3 37.5 / 71.6 19.2 / 63.3 34.0 / 75.7 8.7/ 57.0 15.0/ 57.3 Table 3. GRPO with different and F-GRPO on both in-domain math and out-of-domain benchmarks (Qwen2.5-7B). Pass@1 / Pass@256. Bold: best; : second best. F.2. NLL on Rare-Correct Trajectories To construct proxy for rare-correct modes, we sample 256 prompts from the training set and generate 800 rollouts per prompt from the base model, retaining only correct trajectories. For each retained trajectory, we compute its lengthnormalized NLL under the base model. We define the rare-correct subset as the top 1% by base-model NLL among these correct trajectories, yielding 1,263 trajectories in total. We then compute the NLL of this fixed subset under each trained model; larger values indicate reduced probability assigned to these initially low-probability correct solutions. G. Entropy and KL Regularization: Full Results In-domain Out-of-domain Method Avg. AIME24 AIME25 AMC MATH500 Minerva Olympiad Avg. OOD IFEval SynLogic GPQA 38.6 / 70.3 15.9 / 46.2 10.1 / 52.6 56.2 / 96.3 76.2/ 95.1 35.7 / 60.3 37.5 / 71.6 19.2/ 63.3 34.0 / 75.7 8.7 / 57.0 15.0/ 57.3 F-GRPO 37.8/ 69.5 14.9/ 48.9 7.3 / 52.2 55.8/ 90.8 75.6 / 94.6 34.9/ 61.3 38.2 / 69.2 18.7 / 59.9 32.1 / 71.9 9.8 / 59.9 14.3 / 47.8 GRPO (H) GRPO (KL) 37.2 / 72.0 13.2 / 53.4 8.7/ 53.7 52.1 / 95.9 76.7 / 95.2 34.7 / 61.5 38.0/ 72.3 19.4 / 60.0 32.4/ 70.8 8.8/ 51.7 17.1 / 57.5 Table 4. F-GRPO vs. GRPO with entropy bonus (GRPO-H, coefficient 0.001) and KL penalty (GRPO-KL, coefficient 0.001) on Qwen2.5-7B at =8. Pass@1 / pass@256. Bold: best; : second best. H. Experimental Details H.1. Dataset Preprocessing All models are trained on the DeepScaleR math dataset (Luo et al., 2025). We filter samples longer than 1024 tokens and remove duplicates with conflicting answers, retaining 39,202 samples. The system prompt \"Please reason step by step, and put your final answer within boxed{}.\" is prepended to all training inputs. H.2. Training Configuration Training uses the verl pipeline (Sheng et al., 2024) with sglang (Zheng et al., 2023) for rollout generation, on 16 NVIDIA H100 GPUs with FSDP2 (Zhao et al., 2023). Maximum response lengths are 3072 tokens for Qwen2.5-1.5B-Math and 8192 tokens for other models. Following (Yu et al., 2025), we drop the KL-divergence regularization term and use token-mean loss aggregation. In all our experiments we use learning rate 1 106 according to (Shao et al., 2024; Yu et al., 2025). Clipping parameters: ϵlow=0.2, ϵhigh=0.2 for GRPO; ϵlow=0.2, ϵhigh=0.28 for DAPO; ϵlow=1.0, ϵhigh=5.0 for CISPO, following (Khatri et al., 2025). Rewards are assigned via math-verify (Hugging Face, 2026): 1.0 for correct, 0.0 for incorrect. Complete hyperparameters are in Table 5. Entropy and KL Regularization: for the comparison in Section 5.5, we tune the entropy bonus coefficient over {0.0001, 0.001} and the KL penalty coefficient over {0.001, 0.01}. We select the best checkpoint for each configuration based on average math pass@1. The best-performing coefficients are 0.001 for both entropy bonus and KL penalty. H.3. Focal Weight Hyperparameter γ We sweep the Focal exponent γ {0.5, 1.0, 2.0} for each Focal-weighted method (F-GRPO, F-DAPO, F-CISPO) and select the best value by average in-domain math pass@1 at the best checkpoint. For reproducibility, the selected γ values for the 15 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Parameter Optimizer (β1, β2) Weight decay Gradient norm clipping Learning rate LR scheduler Warmup steps Global batch size Mini-batch size Num training epochs PPO epochs Sampling temperature (top-p, top-k) Value AdamW (Loshchilov & Hutter, 2019) (0.9, 0.999) 0.01 1.0 1 106 Constant 15 256 64 10 1 1.0 (1.0, -1) Table 5. Training hyperparameters. Model F-GRPO γ F-DAPO γ F-CISPO γ Qwen2.5-7B Qwen2.5-1.5B-Math Llama-3.2-3B-Instruct 0.5 0.5 0.5 0.5 0.5 1.0 1.0 1.0 0. Table 6. Selected Focal weight γ for each method-model setup at =8  (Table 1)  . The sweep range is {0.5, 1.0, 2.0}. setups reported in Table 1 are summarized in Table 6. Overall, the method is robust to the choice of γ: across setups, the best results are attained at both γ = 0.5 and γ = 1.0. H.4. Evaluation Protocol We report unbiased pass@k estimator (Chen et al., 2021), the probability that at least one of samples is correct: pass@k := Problems 1 (cid:34) (cid:35) (cid:1) , (cid:0)nc (cid:1) (cid:0)n (25) where is the total number of samples and is the number of correct samples. We use = 256 samples per problem and report pass@1 and pass@256. For checkpoint selection, we save checkpoint at the end of each epoch. We choose the best baseline checkpoint by average math pass@1, then compare to the best F-GRPO checkpoint obtained with equal or less compute. Evaluation uses sglang (Zheng et al., 2023) and math-verify (Hugging Face, 2026). Configurations and system prompts are in Tables 7 and 8. I. Statistical Significance To assess the statistical significance of performance differences between the baseline and F-GRPO models, we employ paired m-out-of-n subsampling test following (Politis et al., 1999). For each benchmark, we generate = 1024 solutions per problem and use = 256 generations (i.e., subsample size m) to estimate pass@1 and pass@256 metrics. Specifically, for each subsampling iteration we randomly sample = 256 generations without replacement for each problem, compute (cid:1) where is the number of sampled generations and is (cid:1)/(cid:0)n the pass@k metric using the analytical formula 1 (cid:0)nc the number of correct solutions among them, and average across all problems to obtain single pass@k estimate for both baseline and F-GRPO models. We perform 50,000 subsampling iterations to obtain the distribution of paired differences in pass@k between the two models. k We conduct two-sided statistical test with significance level α = 0.05. difference is considered statistically significant if the two-sided p-value is less than 0.05, which is equivalent to the 95% confidence interval of the subsampling distribution not containing zero. 16 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Parameter Qwen2.5-7B Qwen2.5-1.5B-Math Llama3.2-3B Temperature top-p top-k Max length 1.0 1.0 -1 8192 1.0 1.0 -1 3072 1.0 1.0 -1 8192 Table 7. Evaluation configurations. Benchmark Qwen Llama Mathematical reasoning Please reason step by step, and put your final answer within boxed{}. GPQA Diamond Please reason step by step, and put your final answer within boxed{}. IFEval You are helpful assistant. SynLogic You are helpful assistant. Cutting Knowledge Date: December 2023nToday Date: [date]nPlease reason step by step, and put your final answer within boxed{}. Cutting Knowledge Date: December 2023nToday Date: [date]nPlease reason step by step, and put your final answer within boxed{}. Cutting Knowledge Date: December 2023nToday Date: [date] Cutting Knowledge Date: December 2023nToday Date: [date] Table 8. System prompts for evaluation. J. Categorical Simulation Details We validate the theoretical framework using categorical policy simulation. To enable direct comparison with prior work, we adopt the setup of Hu et al. (2025) with one modification to the learning rate, as described below. The policy is softmax distribution over = 128,000 actions. subset A+ of 10,000 actions is designated as correct with reward = +1; the remaining 118,000 actions receive = 1. Following Hu et al. (2025), logits are initialized as: one anchor correct action receives zanchor = 5.0; all other correct actions receive = 3.0; incorrect actions receive = 0.0. Under softmax with temperature τ = 1, this yields initial total correct mass Qpos 0.63, anchor probability panchor 4.7 104, and probability τleaf 6.3 105 for each non-anchor correct action. Given this initial distribution, we can compute the tailmiss probability Pr(Bτ ) from Lemma 3.1 for typical non-anchor correct action with τ = τleaf 6.3 105. Figure 5 shows Pr(Bτ ) as function of group size . The probability rises steeply for small , plateaus near 1 for intermediate values, and only declines toward zero for 215. At = 217 = 131,072, Pr(Bτ ) < 103, predicting that such group size should preserve probability mass on non-anchor correct actions. This prediction aligns with the simulation results in Figure 4: Figure 5. Tail-miss probability Pr(Bτ ) versus group size for µpos = 0.64 and τ = 6.3 105 (corresponding to non-anchor correct action in the simulation). The non-monotonic shape explains the concentration zone: intermediate maximizes the probability that correct action is unsampled while the batch contains mixed rewards. 17 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare =131,072 is the only configuration that maintains Mret 1 throughout training. (cid:80) Rk, and update logits via gradient ascent on = 1 At each training step, we sample actions i.i.d. from the current policy, compute group-relative advantages rj = Rj 1 rjpj. When Focal weighting is applied, objective is scaled by = (1 (cid:98)µpos)γ. We use learning rate η = 102, which differs from η = 103 in Hu et al. (2025). At the lower learning rate, policy entropy after 1,000 steps remained above 4 even for =65,536, whereas LLM generation entropy during RLVR training is typically below 1. The higher learning rate produces dynamics that better reflect the concentration regimes observed in practice. (cid:80) We sweep {2, 4, . . . , 131,072} and γ {0, 1}, running =1,000 steps per configuration. Results are averaged over 4 random seeds. Metrics. We track total correct mass Qpos(t) = (cid:80) aA+ πt(a) and retained positive mass: aA+ max(cid:0)0, π0(a) πt(a)(cid:1) aA+ π0(a) (cid:80) (cid:80) . Mret(t) = 1 (26) Mret=1 indicates no correct action has lost mass; Mret0 indicates concentration onto smaller subset. K. Notation Table 9 summarizes the main notation used throughout the paper. 18 F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Category Trajectory-level variables Symbol πθ o, yt Ri Rc, Rw µpos(x) τ (x) ρ(x) (cid:98)µpos(x) Meaning The policy parameterized by θ Given prompt complete response (trajectory) generated by πθ when given The t-th token of response Group size: number of rollouts sampled per prompt Binary reward for rollout (Rc if correct, Rw if incorrect) Reward values for correct and incorrect rollouts (Rc > Rw) Success probability: Proπθ (x)[o C(x)] Rare-correct mass: Proπθ (x)[o Crare(x)] Ratio of rare-correct to total correct mass: τ (x)/µpos(x) Empirical success rate: fraction of correct rollouts in the sampled group = softmax(z) Policy over finite action space Categorical framework variables Expressions and operators Events and probabilities Sets zi P, A, B, Qpos, Qneg Ppos, Pneg Qu,pos A2, B2 U2 Upos,2, Uneg,2 πθ( x, y<t) σR (cid:98)AGRPO (cid:98)AFGRPO ri,t(θ) SR zi Qpos Qu,pos g(x) γ η Mret(t) AN Bτ Pr(Bτ ) Ωx C(x) Crare(x) A+ Logit for action Sets of correct and incorrect actions Sampled correct actions, sampled incorrect actions, and unsampled actions Total correct and incorrect probability masses Sampled correct and incorrect probability masses Unsampled-correct probability mass , (cid:80) Second moments: (cid:80) Unsampled second moment: (cid:80) Unsampled second moments for correct and incorrect actions iB p2 iU p2 iA i (cid:80)N j=1 Rj Conditional probability of generating token given prompt and previous tokens y<t Group mean reward: 1 Standard deviation of rewards in the group Group-relative advantage: (Ri R)/(σR + ϵ) Focal-weighted advantage: g(x) (cid:98)AGRPO Importance ratio: πθ(yi,t x, yi,<t)/πθold (yi,t x, yi,<t) Batch baseline: RcPpos + RwPneg One-step logit update: η pi(Ri SR) One-step change in total correct mass One-step change in unsampled-correct mass Difficulty weight: (1 (cid:98)µpos(x))γ Focal loss parameter controlling difficulty weighting strength Learning rate Retained positive mass: fraction of initial correct probability that has not decreased at step Active event: {0 < < } where is the number of correct rollouts Tail-miss event: active update that misses rare-correct region Probability of tail-miss event Space of complete rollouts for prompt Subset of correct rollouts for prompt Subset of rare-correct rollouts for prompt Finite action space in the categorical framework Subset of correct actions in the categorical simulation Table 9. Notation used in the paper."
        }
    ],
    "affiliations": [
        "Saint Petersburg Electrotechnical University LETI",
        "T-Tech"
    ]
}