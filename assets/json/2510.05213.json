{
    "paper_title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing",
    "authors": [
        "Yixiao Wang",
        "Mingxiao Huo",
        "Zhixuan Liang",
        "Yushi Du",
        "Lingfeng Sun",
        "Haotian Lin",
        "Jinghuan Shang",
        "Chensheng Peng",
        "Mohit Bansal",
        "Mingyu Ding",
        "Masayoshi Tomizuka"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/."
        },
        {
            "title": "Start",
            "content": "Preprint. VER: VISION EXPERT TRANSFORMER FOR ROBOT LEARNING VIA FOUNDATION DISTILLATION AND DYNAMIC ROUTING Yixiao Wang1, Mingxiao Huo2, Zhixuan Liang3, Yushi Du4, Lingfeng Sun1, Haotian Lin2, Jinghuan Shang5, Chensheng Peng1, Mohit Bansal6, Mingyu Ding6, Masayoshi Tomizuka1 1UC Berkeley 4Peking University 2Carnegie Mellon University 5Stony Brook University 3University of Hong Kong 6UNC-Chapel Hill 5 2 0 O 6 ] . [ 1 3 1 2 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into vision expert library. It then fine-tunes only lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Developing robotic systems capable of perceiving and interacting with complex, unstructured environments remains fundamental challenge in embodied AI. Recently, visuomotor robot policy learning has emerged as promising approach, enabling robots to directly map visual observations to control actions. Pretrained vision foundation models (VFMs) such as DINOv2 (Oquab et al., 2024), CLIP (Radford et al., 2021), and ViT (Dosovitskiy et al., 2020), provide transferable visual representations that support robotic perception and control with certain generalizability, improving the scalability of robotic systems (Huang et al., 2024; Wan et al., 2024). However, executing even single robotic task, and especially diverse set of tasks, often requires multiple implicit visual competencies that single VFM cannot fully capture. Directly integrating multiple VFMs for robot tasks increases computational and operational complexity. Previous works (Ranzinger et al., 2024; Shang et al., 2024; Chen et al., 2025) distill diverse foundation models into unified representation, but three key challenges remain. First, heterogeneous VFM features are often misaligned, so unified representation tends to dilute or discard model-specific capabilities. Second, the policy head must extract task-relevant information from the fused representation, which limits flexibility to leverage the most relevant VFMs across tasks and leads to suboptimal results. Third, existing distilled models typically require full re-training to incorporate robot-domain knowledge and it is hard to scale computation (down for simple tasks and up for complex tasks). Corresponding Author. 1 Preprint. Figure 1: comparison between our VER and previous distillation framework. Our method not only enhances knowledge distillation from vision foundation models (VFMs) into vision experts but also offers two key advantages over previous works (Ranzinger et al., 2024; Shang et al., 2024). First, VER trains lightweight router that dynamically selects vision experts for downstream robot policies. Second, VER allows the integration of additional trainable experts, enabling the adaptation to robot-specific domain knowledge to further improve robotic performance. To address these limitations, we propose VER, Vision Expert transformer for Robot learning via foundation distillation and dynamic routing. VER distills knowledge from multiple vision foundation models into unified representation library and uses dynamic routing mechanism to selectively activate the most relevant experts for robot policy learning. Specifically, VER introduces Mixture-of-Experts (MoE)-based Vision Expert Library (VEL), replacing traditional static vision transformer backbones with collection of specialized experts, each capturing distinct aspects of visual understanding. This design enables robots to selectively leverage the specialized experts best suited for task-aware policy learning. Our method operates in three stages as shown in Fig. 1. First, during pretraining, we distill knowledge from multiple VFMs into vision expert library using Teacher-Specific Routers with mutualinformation regularization. This covers broad spectrum of visual knowledge while maintaining efficiency via sparse expert activation. Second, in the robotic policy learning phase, we freeze all pretrained vision experts and fine-tune only lightweight Robot Router that dynamically selects task-relevant experts, whose outputs are fed to policy head to generate actions. To expand selection capacity across patches and layers, enhance exploration, and prevent premature convergence to suboptimal expert combinations, we employ Patchwise Expert Routing with Curriculum Top-K Annealing, leading to more robust policy learning. Third, we offer parameter-efficient fine-tuning strategies that scale expert utilization and facilitate the integration of robot-domain knowledge. Across different types of policy heads, such as diffusion and flow matching policies (Chi et al., 2023b; Zhang & Gienger, 2024), extensive experiments on diverse robotic benchmarks show that VER achieves state-of-the-art performance. With Patchwise Expert Routing and Curriculum TopK Annealing, VER suppresses high-norm background outliers and reduces information in taskirrelevant patches while preserving details in task-critical regions, yielding more compact and discriminative visual features and robust policy learning."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 VISION FOUNDATION MODELS FOR REPRESENTATION Vision Foundation Models (VFMs) have revolutionized computer vision through self-supervised and weakly-supervised learning on large-scale datasets (Radford et al., 2021; Caron et al., 2021; Oquab et al., 2024). Notable examples include CLIP (Radford et al., 2021) which pioneered imagetext joint embeddings, DINOv2 (Oquab et al., 2024) which advanced self-supervised learning for representation, and SAM (Kirillov et al., 2023) specialized for segmentation tasks. Knowledge distillation has emerged as powerful paradigm for transferring learned representations from large teacher models to more compact student architectures (Hinton et al., 2015; Romero et al., 2014). While traditional distillation approaches focus on compressing single teacher into smaller student (Hinton et al., 2015), recent advances have explored multi-teacher distillation (You et al., 2017; Shang et al., 2024) where complementary knowledge from multiple source models is combined. Theia (Shang et al., 2024) demonstrated that careful fusion of representations from diverse vision foundation models can achieve superior performance for downstream robotic tasks. However, 2 Preprint. Figure 2: Overall structure of VER. VER comprises two key components: the Base Vision Transformer (BVT), which processes images into unified representations; the Vision Expert Library (VEL), which stores diverse set of specialized vision experts and selectively utilizes the experts to mimic teacher vision foundation models and enhance performance in downstream robotic tasks. Our framework consists of two phases: (1) Pretraining, where we distill multiple foundation models (DINOv2 (Oquab et al., 2024), ViT (Caron et al., 2021), CLIP (Radford et al., 2021)) into VER; (2) Downstream Robotic Tasks, where we freeze the experts and train lightweight Robot Router (< 0.4% parameters) that dynamically selects task-relevant visual features to guide the policy head in generating appropriate robotic actions. This two-stage approach enables efficient knowledge distillation from diverse vision foundation models and adaptive feature selection for robotic tasks. these methods typically produce static representations with fixed weights, limiting their adaptability to specific downstream tasks. Our work differs by distilling multiple VFMs into specialized expert library rather than single unified representation, preserving diverse knowledge from VFMs while enabling task-specific feature selection through learned routing mechanisms. 2.2 MIXTURE OF EXPERTS IN VISION AND POLICY Mixture of Experts (MoE) architectures (Shazeer et al., 2017a; Fedus et al., 2022; Riquelme et al., 2021; Wang et al., 2024) have gained popularity for their ability to scale model capacity without proportional increases in computational costs by activating only subset of expert networks for each input. Router design represents critical component in MoE systems, determining which experts process specific inputs. Top-k routing (Shazeer et al., 2017a) selects the highest-scoring experts for each token, while Switch Transformers (Fedus et al., 2022) employ simpler top-1 routing for efficiency. Recent works have explored learning routing mechanisms (Dai et al., 2022; Wang et al., 2024) to balance expert utilization while preserving specialization. Sparse MoE Router (Wang et al., 2024) introduced mutual information maximization between tasks and experts to encourage meaningful specialization while maintaining balanced utilization. While MoE has been widely applied in language processing and general computer vision (Riquelme et al., 2021; Fedus et al., 2022), its application to robotic learning remains relatively unexplored. Our work bridges this gap by adapting MoE principles for vision-based robotic policy learning, especially on embodied perception, introducing specialized routers that dynamically select visual representations most relevant to specific robotic tasks. 3 Preprint."
        },
        {
            "title": "2.3 VISUOMOTOR ROBOTIC POLICY LEARNING",
            "content": "Visuomotor robotic policy learning maps visual observations directly to robot actions (Levine et al., 2016; Brohan et al., 2023; Chi et al., 2023a; Liang et al., 2023; Mu et al., 2024), showing better generalization ability compared to state-based methods (Janner et al., 2022; Liang et al., 2023; Ajay et al.; Ni et al., 2023; Liang et al., 2025). Recent approaches have leveraged pre-trained vision models to improve sample efficiency and generalization (Nair et al., 2023; Xiao et al., 2022; Radosavovic et al., 2023; Chen et al., 2025; Kim et al.; Black et al., 2024), but typically use fixed visual encoders that may not capture optimal representations for specific tasks. persistent challenge is identifying which visual features are most relevant for different robotic tasks (Xiao et al., 2022; Luo et al., 2023). Current methods using attention mechanisms (Luo et al., 2023) or feature selection (Jiang et al., 2025) often lack the flexibility to incorporate diverse visual expertise from foundation models. Our work advances this field by introducing dynamic visual representation selection mechanism specifically for robotic tasks. Unlike fixed visual feature approaches, ours enables selective leveraging of different representations from diverse expert library based on task requirements, leading to more robust policies across varied robotic scenarios."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 OVERVIEW In this section, we present our VER framework for visuomotor robot policy learning, as illustrated in Fig. 2. Our approach begins with visual perception, where input images are processed by Base Vision Transformer (BVT) to extract foundational visual features, referred to as unified representations. These representations are then fed into Vision Expert Library (VEL), collection of specialized neural network experts designed to capture diverse aspects of visual understanding. dynamic routing mechanism determines which experts should be activated based on the specific task: during pretraining, to mimic the teacher vision foundation models (VFMs); and for downstream robot tasks, to select experts that enhance performance. This mechanism enables selective attention to the most relevant visual features. Finally, the outputs of the selected experts are integrated to generate more structured representation, which is either used to replicate the teacher VFMs or passed to policy head that translates these representations into robot actions. 3.2 MODEL ARCHITECTURE As illustrated in Fig. 2, our approach consists of two main components: Base Vision Transformer (BVT) that generates unified feature representations from input images, and Vision Expert Library (VEL) comprising specialized experts that capture diverse visual representations from various vision foundation models. We design VER based on modified vision transformer (Dosovitskiy et al., 2020) architecture, where the FeedForward Network in the last transformer layers is replaced with Mixture of Experts (MoE) (Shazeer et al., 2017b) modules. The initial unaltered ViT layers are referred to as BVT, while the MoE-enhanced later layers constitute the VEL. In the n-th MoE layer (Shazeer et al., 2017b) of VEL, where 1, 2, ..., , we incorporate experts {E }L l=1, each implemented as multilayer perceptron (MLP). We then introduce TeacherSpecific (TS) Routers Rn , where 1, 2, ..., corresponds to each teacher vision foundation model. takes in the input feature vector R1M and learn MLP to determine the score TS Router Rn for each expert, as well as score noise. During inference, only the top-K scoring expert networks are activated, while the remaining experts remain inactive, ensuring computational efficiency. The MoE output is computed as: (x, l) (x), l=1 Rn = (cid:80)L Rn (x, l) = Top-K(Softmax(s1 + ϵ), l), [s1; s2] = MLP(x), ϵ (0, SoftPlus(s2)) (1) where Top-K(v, l) returns the l-th element of vector if it is among the largest elements, and returns 0 otherwise. This sparse gating mechanism enables efficient computation while maintaining representation quality. 4 Preprint."
        },
        {
            "title": "3.3 DISTILLATION TRAINING",
            "content": "Building on our network architecture, we now describe the distillation process that enables our model to acquire diverse visual representations from multiple VFMs, forming comprehensive library of specialized vision experts for effective utilization in downstream robot tasks. Given an input image x, the BVT () first processes the image to produce unified rich representation = (x). Subsequently, the VEL g(, ) generates teacher-specific representations = g(z, i), where denotes the index of the specific teacher model being mimicked. Note that g(, i) utilizes the corresponding TS Router to select the most appropriate experts for emulating the i-th VFMs representational characteristics. Following (Shang et al., 2024), we formulate the distillation loss Ldistill as weighted combination of cosine and smooth L1 losses: (cid:88) Ldistill = αi[βLcos(hi(g(f (x), i)), ti(x)) i=1 + (1 β)LsL1(hi(g(f (x), i)), ti(x))], (2) where hi() represents projection head for the i-th teacher, ti(x) is the representation from the i-th teacher model, αi = 1/I and β = 0.9. To ensure balanced utilization of experts during pre-training and prevent expert collapse, we introduce teacher-level mutual information loss inspired by (Wang et al., 2024). This loss maximizes the mutual information between the VFMs Ii and the experts across all MoE layers: Lmi = (cid:88) (cid:88) n=1 i=1 p(Ii, ) log p(Ii, ) p(Ii)p(E ) , (3) where we assume each teacher model contributes equally to the overall knowledge, setting p(Ii) = 1 . Detailed implementation of Lmi is provided in Appendix C.2. Thus, the training objective is: Lpretrain = Ldistill + γLmi (4) where we empirically set γ = 0.0005. 3.4 ROBOT POLICY TRAINING After distilling diverse visual representations, we freeze Vision Expert Library (VEL) together with frozen base visual transformer (BVT) for downstream robot tasks. We introduce lightweight robot router Rn robot that selects task-relevant vision experts and feeds the resulting representations to newly trained policy head to produce actions. We consider two routing modes for robot tasks. Teacher Routing (TR) Because pretrained vision foundation models perform strongly on robot tasks, one option is to choose which VFM to use by selecting among the Teacher-Specific (TS) routers {Rn }M i=1 learned during distillation. Specifically, for each image/frame and layer n, the robot produces categorical distribution πt,n 1 over {Rn }M robot router Rn i=1. The selected TS router at layer is then used to select among the experts {E ℓ=1 in that layer. During training, we optimize the discrete teacher choice with the GumbelSoftmax estimator: ℓ }L zt,n = softmax (cid:18) log πt,n + τ (cid:19) , Gumbel(0, 1), (5) using straight-through estimator, while at inference we take arg max(πt,n). We can share teacher logits across layers within frame, i.e., πt,n πt, yielding single teacher choice per frame; or allow per-layer logits πt,n so that shallow and deep layers route to different teachers (e.g., DINOv2like early, CLIP-like late) for finer control. We refer to the former as Framewise Teacher Routing (FTR) and the latter as Layerwise Teacher Routing (LTR). 5 Preprint. Table 1: Per-task performance comparison (success rate in %) across various robotic benchmarks. The same policy head as (Shang et al., 2024) is used for fair comparison on vision encoder. The best result is in bold and the second best result is underlined. Our approach (VER) outperforms previous state-of-the-art methods across 11 diverse tasks from Franka Kitchen (Gupta et al., 2020), Meta-World (Yu et al., 2020), and Adroit (Rajeswaran et al., 2018) environments, achieving the highest average success rate (74.7%). Model LightOn DoorOpen DoorSlide KnobTurn Microwave BinPick ButtonPress DrawerOpen Hammer Pen Relocate Average VC-1 (Majumdar et al., 2023) MVP (Xiao et al., 2022) R3M (Nair et al., 2023) RADIO (Ranzinger et al., 2024) VIP (Ma et al.) Theia-B (Shang et al., 2024) VER-B (Ours) 1.6 13.6 67.3 35.2 61.3 58.8 67.2 0.2 5.3 31.2 19.7 25.2 34.1 38. 14.4 17.8 83.1 69.2 83.0 81.2 85.8 1.2 1.8 35.4 24.4 44.6 47.8 55.3 1.8 4.0 35.8 25.3 31.3 24.8 38. 66.7 73.3 92.0 82.7 70.7 76.0 93.3 56.0 82.7 68.0 80.0 76.0 82.7 94.7 100.0 100.0 100.0 100.0 98.7 100.0 100. 93.3 97.3 98.7 100.0 96.0 98.7 97.3 68.0 77.7 73.3 66.7 73.3 78.7 80.0 24.0 26.7 58.7 45.3 29.3 46.7 64. 42.6 48.7 67.6 61.3 62.8 67.1 74.7 Table 2: Per-task performance comparison (success rate in %) across different policy heads.VER consistently outperforms Theia across ViLT (Kim et al., 2021), Flow-Matching (Zhang & Gienger, 2024) and Diffusion heads (Chi et al., 2023b). Model ViLT head Flow-Matching head Diffusion head LIBERO LIBERO-OOD crossbin cubecup cylinderplate Real-world pour Theia-T VER-T 0.61 0. 0.58 0.71 0.65 0.95 0.50 0.75 0.70 0.85 0.45 0.90 Patchwise Expert Routing (PER) PER applies standard MoE routing per patch token (Eq. 1), offering maximal adaptivity to local content with small overhead (adds < 0.4% parameters). However, it can suffer from premature convergence (early collapse). Moreover, we do not apply the mutualusage regularizer Lmutual in Eq. 3, since the router serves as planning selector for task-relevant experts rather than enforcing balanced utilization. To mitigate early collapse and encourage exploration over expert combinations, we use Curriculum Top-K Annealing (CTA). We begin with all experts active (K0 = L) and gradually reduce to small target Kmin over training steps = 0 S: (cid:16) K(s) = max Kmin, (cid:106) (L Kmin) (cid:107)(cid:17) . (6) CTA is applied to PERs token-wise dispatch, promoting exploration early (large K) and training stability later (Kmin), while keeping inference efficiency at target Kmin."
        },
        {
            "title": "4.1 NETWORK STRUCTURE",
            "content": "To address the limited computational resources of robotic systems, we use DeiT-Tiny (Touvron et al., 2021) for VER-T, DeiT-Small for VER-S, and ViT-Base (Dosovitskiy et al., 2020) for VERB. Distillation is performed on ImageNet-1K (Deng et al., 2009) from three foundation modelsDINOv2 (Oquab et al., 2024), ViT (Caron et al., 2021), and CLIP (Radford et al., 2021). This configuration, aligned with Theia (Shang et al., 2024), controls for pretraining variations and enables fair comparison. We use total of = 6 experts and activate = 2 experts. To control complexity, VER replaces only the last three layers of 12-layer transformer with the Vision Expert Library, yielding 9 standard transformer layers for the Base Vision Transformer and = 3 MoE layers for the Vision Expert Library. Routing network is provided in Appendix B. Details of the pretraining procedure are provided in Appendix C. 4.2 PERFORMANCE ON ROBOT TASKS With different policy heads such as ViLT (Liu et al., 2023; Kim et al., 2021), flow-matching and diffusion policy, we evaluate VER against pretrained vision encoders, including VC-1 (Majumdar et al., 2023), R3M (Nair et al., 2023), MVP (Xiao et al., 2022), RADIO (Ranzinger et al., 2024), 6 Preprint. Figure 3: Cosine loss for DINOv2 distillation. Circle size indicates total parameters (TP). Figure 4: Expert utilization frequency across three MoE layers. Heatmap shows how each teacher model activates experts (16) during distillation on ImageNet-1K. Table 3: Ablation of robot routing strategies. Mean success rate standard deviation over 10 seeds. Best per task in bold; second best underlined. DINOv2, ViT, and CLIP denote the corresponding Teacher-Specific Routers. We can see different VFMs suit different tasks, and VER improves performance by dynamically routing to the appropriate experts distilled from these VFMs. Task DINOv2 ViT CLIP FTR LTR PER PER+CTA pen relocate 78.04.7 38.45. 72.89.4 41.66.6 80.04.6 41.23.8 81.23.8 41.26.0 79.26.2 36.45.8 78.06.3 47.65.1 80.85.3 56.46. VIP (Ma et al.), and Theia (Shang et al., 2024). Among these baselines, Theia is particularly strong as it distills multiple vision foundation models into unified representation, while VIP leverages large-scale human video datasets to learn transferable features for robotic control. We first follow Theia (Shang et al., 2024) and apply the same policy head across 11 diverse manipulation tasks spanning three benchmarks: 5 tasks from Franka Kitchen (Gupta et al., 2020) (LightOn, DoorOpen, DoorSlide, KnobTurn, Microwave), 4 from Meta-World (Yu et al., 2020) (Binpick, Buttonpress, DrawerOpen, Hammer), and 2 from Adroit (Rajeswaran et al., 2018) (Pen, Relocate). Second, we adopt the ViLT head (Liu et al., 2023; Kim et al., 2021) and evaluate VER on four LIBERO tasks (Liu et al., 2023), including LIBERO-OOD, where object colors are modified to test out-of-distribution generalization. Third, we apply flow-matching head on three Pick and Place task in the Robomimic(Mandlekar et al., 2021). Finally, we use diffusion policy head for the Pour task in real-world experiments. As shown in Table 1, VER consistently outperforms prior approaches, achieving the highest average success rate of 74.7%. Furthermore, Table 2 shows that VER surpasses Theia across all policy heads both in simulation and real world experiments. Figure 5 shows the performance of our VER. More details and results can be found in Appendix D. 4.3 DISTILLATION PERFORMANCE Figure 3 shows that our framework effectively distills more knowledge from diverse foundation models; additional results are provided in Table 7. Figure 4 further illustrates expert utilization on ImageNet-1K (Deng et al., 2009). Instead of pre-assigning experts to teacher models, our TeacherSpecific Routers dynamically allocate them, with mutual information regularization encouraging diverse expert usage. We observe that ViT activates fewer experts, whereas DINOv2 and CLIP engage more, suggesting that ViT is easier to mimic while DINOv2 and CLIP present greater challenges. This trend is confirmed in Table 7, where the cosine loss after pretraining is significantly lower for ViT than for DINOv2 and CLIP. Overall, these findings demonstrate that our method outperforms fixed expert assignments by adaptively allocating more experts to stronger foundation models and fewer to weaker ones, thereby improving both utilization and distillation performance. 4.4 ABLATION ON ROUTER DYNAMICS IN ROBOT TASKS In this subsection, we investigate the functional role of routers in robotic policy learning through three key experiments: (1) evaluating the impact of noisy gating on performance, (2) analyzing feature entropy evolution during training, and (3) examining the relationship between feature entropy and task performance. These experiments provide insights into how routers function as implicit planning modules for robotic tasks. 7 Preprint. Figure 5: Visualization of real world experiments. We find with human interference (not in the training dataset), our VER can successfully complete the task. Figure 6: Feature visualization of PER with and without CTA (seed = 0). Row 1: pen; Row 2: relocate. Without CTA, the Robot Router attends broadly to the dexterous hand, objects, and task signals (e.g., target pen pose, target ball region). With CTA, the Robot Router suppresses taskirrelevant patches and concentrates on task-related, object-centric regions throughout execution. Robot Routing Stratgies Performance. We compare: (1) select one frozen Teacher-Specific (TS) Routers (DINOv2, ViT, or CLIP); (2) Framewise Teacher Routing (FTR) and Layerwise Teacher Routing (LTR); and (3) Patchwise Expert Routing (PER), with and without Curriculum Top-K Annealing (CTA). Results in Table 3 show that relying on single VFM performs poorly across diverse tasks, whereas PER, when combined with CTA, adapts more effectively to local content across layers and achieves superior performance. Patch Feature Analysis. To investigate the mechanism of CTA, we compute the norm of the last-layer patch features in VER, comparing models trained with and without CTA. As shown in Figure 6, CTA reduces high-norm outliers and concentrates attention on task-critical patches, whereas models without CTA exhibit large outliers in background regions. We further analyze patch features on 30% of the robot dataset by measuring entropy and mutual information before and after the Vision Expert Library (VEL). Patch features are first reduced to five dimensions via Principal Component Analysis (PCA), and then NPEET (Steeg, 2022) is used to estimate entropy and mutual information. As shown in Figure 7, PER+CTA filters out task-irrelevant background patches (lower mutual information before vs. after expert selection) while preserving task-relevant information (e.g., the target pen pose in pen-v0, which consistently appears in the left region of the image). Finally, Figure 8 compares feature norms from Theia, from VER before VEL, and from VER after VEL in the Pick and Place task. This task consists of two stages: first pick up the cross, then place it into the bin. We supply patch features and robot proprioceptive state to the flow-matching policy head. We find that pretrained VFMs such as Theia, as well as VER before expert selection, broadly attend to all potentially important objects. After expert selection in VEL, however, the features focus exclusively on task-relevant objects (cross and bin) and suppress robot-related patchesconsistent with our design choice to provide robot proprioceptive state directly to the policy, eliminating the need for robot-related information in patch features. More analysis can be found in Appendix D.3. Figure 9: Trainable parameters vs average success rates. Performance is evaluated on pen and relocate tasks. Additional complexity and overhead. To control complexity, VER replaces only the last three layers of the 12-layer transformer with the Vision Expert Library, thereby incurring minimal overhead by design. In Table 2, the inference time with the diffusion policy is 0.105s on an RTX 4090 for both VER and Theia(Shang et al., 2024). In terms of trainable parameters, VER introduces only lightweight components for the Robot Router compared with Theia, yet achieves substantially bet8 Preprint. Figure 7: Mutual information between patch features before and after the Vision Expert Library. Row 1: pen; Row 2: relocate. PER+CTA suppresses information in background patches while preserving information in task-relevant regions, yielding more compact visual representation (lower average per-patch mutual information). For example, in pen, the left-middle region containing the target pen pose exhibits higher mutual information. Figure 8: Feature visualization compared with Theia (Shang et al., 2024) on place the cross into the bin. Both Theia and our features before the Robot Router tend to attend broadly to other objects, the robot itself, and background regions, resulting in noisy feature norm. After routing, VER concentrates on task-relevant objects and suppresses robot-related and background patches. Table 4: Ablation study on Top-K. More active experts lead to better performance. Table 5: Ablation on the mixture of Distilled-Foundation-Model (DFM) and Train-from-Scratch (TFS) experts. Model TopK AP(M) Relocate Pen Avg Theia-Tiny VER-Tiny - 1 2 3 5.3 4.8 5.2 5.7 74. 42.7 52.0 57.3 46.0 60.0 77.3 60.0 80.0 66.0 78.7 68.0 # DFM # TFS TopK Relocate Pen Avg 6 0 6 0 2 2 2 2 64.0 69.3 74.7 80.0 72.0 74.7 72.0 82.7 78.7 ter performance (Figure 9). For active/total parameters, VER perform better performance with less active and total performance as shown in Table 7. Overall, VER delivers significant performance gains over existing baselines with comparableor even lowercomputational complexity. Scalability and Extensibility. We examine the effect of the Top-K hyperparameter by finetuning only the lightweight Robot Router to adjust how many experts are selected per patch. As shown in Table 4, increasing the number of selected experts improves success rates but also increases computational cost. This demonstrates that VER enables controllable trade-off between accuracy and efficiency without retraining the backbone or the experts. Beyond scalability, VER also offers extensibility by adaptive robot-domain knowledge integration. While distilled experts from pretrained VFMs encode strong general visual knowledge, they may miss information critical for specific downstream tasks. Our framework allows seamless integration of trainable experts tailored to such tasks. As shown in Table 5, adaptively combining Distilled-Foundation-Model (DFM) experts with Train-from-Scratch (TFS) experts achieves the best performance, highlighting the complementarity between generalist and task-specialized experts in enhancing overall task success."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce VER, Vision Expert transformer for Robot learning. Our approach distills knowledge from diverse vision foundation models into vision expert library and employs task-adaptive Robot Router to select task-relevant features for downstream control. To maximize 9 Preprint. selection capacity and prevent early collapse during router learning, we further propose Patchwise Expert Routing with Curriculum Top-K Annealing. Across multiple policy heads and range of robotic benchmarks, VER achieves state-of-the-art performance. Patch-level analyses show that the Robot Router learns to selectively leverage pretrained experts, yielding increasingly structured representations that drive performance gains. In addition, VER is highly extensible: it seamlessly incorporates new robot-domain knowledge through expert addition, and scales the number of active experts to meet task complexity through lightweight router fineuning. These results highlight the value of expert-driven visual representation distillation and selection for robust, generalizable robot learning. 10 Preprint."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research adheres to the ICLR 2026 Code of Ethics and upholds the principles of responsible research. Our experiments were conducted using publicly available datasets and self-collected data. No human subjects or vulnerable groups were involved, and no personally identifiable, sensitive, or harmful data were used in any part of this work. We have carefully considered the potential societal impacts of our methods, including risks of misuse or unintended consequences. We believe that our contributions primarily advance scientific understanding and do not pose foreseeable harm."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We follow the reproducibility guidelines outlined in the ICLR 2026 Author Instructions. To support reproducibility, we include detailed descriptions of dataset construction, model training, and evaluation in the main text and appendix. The main code and checkpoints are provided in the supplementary materials. Furthermore, we will release the complete source code, configuration files, and scripts on public platforms (e.g., GitHub and Hugging Face) to enable others to fully reproduce our results."
        },
        {
            "title": "REFERENCES",
            "content": "Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Tianxing Chen, Yao Mu, Zhixuan Liang, Zanxin Chen, Shijia Peng, Qiangyu Chen, Mingkun Xu, Ruizhen Hu, Hongyuan Zhang, Xuelong Li, et al. G3flow: Generative 3d semantic flow for poseIn Proceedings of the IEEE/CVF international aware and generalizable object manipulation. conference on computer vision, 2025. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and Systems, 2023a. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pp. 02783649241273668, 2023b. Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. Stablemoe: Stable routing strategy for mixture of experts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 70857095, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Heigold, Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 11 Preprint. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning, pp. 10251037. PMLR, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. stat, 1050:9, 2015. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=9iG3SEbMnL. Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pp. 99029915. PMLR, 2022. Guangqi Jiang, Yifei Sun, Tao Huang, Huanyu Li, Yongyuan Liang, and Huazhe Xu. Robots pretrain robots: Manipulation-centric robotic representation from large-scale robot datasets. In International Conference on Learning Representations, 2025. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, et al. Openvla: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning. Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International conference on machine learning, pp. 55835594. PMLR, 2021. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):140, 2016. Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: In International Conference on Machine Diffusion models as adaptive self-evolving planners. Learning, pp. 2072520745. PMLR, 2023. Zhixuan Liang, Yao Mu, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, and Mingyu Ding. Dexhanddiff: Interaction-aware diffusion planning for adaptive dexterous manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17451755, 2025. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning visual affordance IEEE Transactions on Neural Networks and Learning grounding from demonstration videos. Systems, 2023. Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In The Eleventh International Conference on Learning Representations. Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? Advances in Neural Information Processing Systems, 36:655677, 2023. 12 Preprint. Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li FeiFei, Silvio Savarese, Yuke Zhu, and Roberto Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. In Conference on Robot Learning (CoRL), 2021. Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian GE, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et al. Robocodex: multimodal code generation for robotic behavior synthesis. In Proceedings of the 41st International Conference on Machine Learning, pp. 3643436454, 2024. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. In Conference on Robot Learning, pp. 892909. PMLR, 2023. Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang. Metadiffuser: Diffusion model as conditional planner for offline meta-rl. In International Conference on Machine Learning, pp. 2608726105. PMLR, 2023. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In Conference on Robot Learning, pp. 416426. PMLR, 2023. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. Robotics: Science and Systems XIV, 2018. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1249012500, 2024. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. Jinghuan Shang, Karl Schmeckpeper, Brandon May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. Theia: Distilling diverse vision foundation models for robot learning. In 8th Annual Conference on Robot Learning, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017a. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts In International Conference on Learning Representations, 2017b. URL https:// layer. openreview.net/forum?id=B1ckMDqlg. 13 Preprint. Greg Ver Steeg. Npeet: Non-parametric entropy estimation toolbox. https://github.com/ gregversteeg/NPEET, 2022. Commit 8b0d948, MIT License. Accessed: 2025-09-24. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 1034710357. PMLR, 2021. Weikang Wan, Yifeng Zhu, Rutav Shah, and Yuke Zhu. Lotus: Continual imitation learning for robot manipulation through unsupervised skill discovery. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 537544. IEEE, 2024. Yixiao Wang, Yifei Zhang, Mingxiao Huo, Thomas Tian, Xiang Zhang, Yichen Xie, Chenfeng Xu, Pengliang Ji, Wei Zhan, Mingyu Ding, and Masayoshi Tomizuka. Sparse diffusion policy: sparse, reusable, and flexible policy for robot learning. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=zeYaLS2tw5. Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 12851294, 2017. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 10941100. PMLR, 2020. Fan Zhang and Michael Gienger. Robot manipulation with flow matching. In CoRL 2024 Workshop on Mastering Robot Manipulation in World of Abundant Data, 2024. 14 Preprint."
        },
        {
            "title": "A LLM USAGE DISCLOSURE",
            "content": "We employed ChatGPT to assist with language refinement. All suggestions were reviewed and revised by the authors, who take full responsibility for the final manuscript."
        },
        {
            "title": "B ROUTING NETWORK",
            "content": "i , we use lightweight two-layer MLP (Linear GELU For the Teacher-Specific (TS) Router Rn Linear) that produces per-patch logits over experts. For the Robot Router Rn robot, we use different network structure for Teacher Routing and Patch Routing. For Patchwise Expert Routing, we adopt the same architecture as the TS Router. For Framewise Teacher Routing and Layerwise Teacher Routing, we first apply attention pooling over all patch features to obtain summary token, followed by three-layer MLP head (SiLU activations, dropout) that outputs logits over the Teacher-Specific Routers. Table 6 shows the details of router network. Table 6: Network structure for routers. Module Input Granularity Core Layers / Pooling Output TS Router PER FTR/LTR Single patch Single patch All patches Linear, GELU, Linear Linear, GELU, Linear Attn pooling; 3-layer MLP (SiLU, dropout) Expert logits (per patch) Expert logits (per patch) Teacher logits (per patch/layer)"
        },
        {
            "title": "C PRETRAIN EXPERIMENTS",
            "content": "C.1 PRETRAINING DETAILS We adopt DeiT-Tiny (Touvron et al., 2021) as the backbone for VER-T, DeiT-Small (Touvron et al., 2021) for VER-S, and ViT-Base (Dosovitskiy et al., 2020) for VER-B. We choose the last = 3 layers as Vision Expert Library (VEL). The projection head hi() consists of shallow CNNs, following the same design of Theia (Shang et al., 2024). The training dataset is ImageNet-1K (Deng et al., 2009). We initialize the model weights using Theia and train VER on four A6000 GPUs for 50 epochs. The learning schedule consists of linear warmup for the first 10% of training steps, followed by constant learning rate of 0.002 for the next 40%, and then Cosine Annealing LR for the remaining steps. C.2 MUTUAL INFORMATION LOSS In order to calculate Lmi, we need to get p(Ii), p(E ) can be calculated by counting the number of selection times of expert across all teacher vision foundation models. We assume each teacher model is equally important so p(Ii) = 1 where is the number of teacher models. For p(Ii, ) and p(Ii, ). p(E l ), we have p(Ii, ) = p(Ii)p(E Ii) = 1 p(E l Ii) Ii) can be calculated by counting the number of selection times of expert for ith teacher p(E model. C.3 PRETRAINING RESULTS Table 7 demonstrates the distillation performance compared with Theia. We can see that our VER can achieve better distillation performance with similar active parameters. And VER-S with less active and total parameters achieve comparable distillation performance compared to Theia-B. Preprint. Table 7: Distillation performance comparison between our VER and Theia across three foundation models (DINOv2, ViT, CLIP) using different loss metrics. TP(M) denotes total parameters (Million), and AP(M) denotes active parameters (Million). Model TP (M) AP (M) Theia-T VER-T (Ours) Theia-S VER-S (Ours) Theia-B VER-B (Ours) 5.3 7.0 20.7 27.7 81.8 110.1 5.3 5.3 20.7 20.8 81.8 82.2 Cosine Loss DINOv2 0.641 0.559 0.554 0.453 0.444 0.337 ViT 0.431 0.398 0.335 0.299 0.267 0.226 CLIP 0.651 0.592 0.587 0.517 0.521 0.455 DINOv2 L1 Loss ViT 0.377 0.351 0.351 0.311 0.308 0. 0.301 0.287 0.255 0.235 0.216 0.189 CLIP 0.373 0.357 0.356 0.332 0.334 0.307 MSE Loss DINOv2 0.873 0.800 0.800 0.695 0.688 0. ViT 0.673 0.636 0.556 0.507 0.462 0.400 CLIP 0.875 0.829 0.826 0.762 0.767 0."
        },
        {
            "title": "D ROBOT TASK EVALUATION",
            "content": "D.1 BENCHMARKS & EVALUATION SETTINGS Franka Kitchen We mainly follow R3M (Nair et al., 2023) evaluation protocol. Specifically, we train the policy for 20,000 steps and evaluate success results every 1,000 steps throughout training. The final reported performance is based on the best average of three success rates observed during evaluation. To ensure robustness, our results in each environment are averaged over different camera views (left and right) and different numbers of demonstrations (5, 10, and 25). We use the same policy network as Theia (Shang et al., 2024) for comparison. Specifically, we employ three-layer MLP for CNN-based models using vector-based representations. For transformer-based models, we introduce three-layer CNN before the MLP to process spatial inputs. Adroit & Meta-World We primarily follow the original evaluation setup of Cortex (Majumdar et al., 2023), with modifications to the training epochs for the Adroit environment. Since VER introduces additional training parameters for the router, which functions as planning module requiring extended training, we increase the training epochs for the pen task to 200 and for the relocate task to 400, ensuring full performance convergence. As shown in Tab. 8, training for 100 epochs is insufficient for policy convergence. For VER-T, 200 In this paper, our focus is epochs are enough for relocate. primarily on the functionality of router, while optimizing its training efficiency is left for future work. We use the same policy network as in Franka Kitchen. Table 8: Performance vs. epoch. We report average / highest success rates. 78.7/80.0 80.0/84.0 / / 48.0/52.0 50.7/52.0 56.0/60.0 64.0/68. 100 200 300 400 relocate Epoch pen LIBERO We select first four tasks from LIBERO OBJECT (Liu et al., 2023) and train ViLT (Kim et al., 2021) policy for 30 epochs, following the LIBERO evaluation protocol. In addition, we change colors for all the colors to evaluate performance in an out-of-distribution setting. Pick and Place We set up the Pick and Place task in robomimic (Mandlekar et al., 2021). The object is one of {cross, cube, cylinder}, and the container is one of {bin, cup, plate}. Objects are randomly positioned and oriented on the left side of the desk; containers are randomly positioned and oriented on the right side. We use SpaceMouse to teleoperate the robotic arm in robomimic, collecting 50 human demonstrations, and then use MimicGen (Mandlekar et al., 2023) to generate 450 additional demonstrations, yielding 500 demonstrations per task. We train the flow-matching policy for 16,000 steps and evaluate over 40 trials. The flow-matching policy network is U-Netbased. Real-World Experiment We conduct real-robot experiments on FANUC LR Mate 200iD/7L robotic arm equipped with an SMC gripper. The task is to pick up teapot and pour into cooking pot. Both the teapot and the cooking pot are randomly positioned and oriented. We collect 20 demonstrations, train diffusion policy for 120,000 steps, and evaluate over 20 trials. The diffusion policy network is U-Netbased. 16 Preprint. Figure 10: Performance on Pick and Place.. We find that when the first attempt fails, the VERequipped policy can retry and complete the task, as shown in the images with red boundaries. D.2 EXPERIMENT VISUALIZATIONS Figure 10 shows the performance of VER in Pick and Place tasks. We find that when the first attempt fails, the VER-equipped policy can retry and complete the task, as shown in the images with red boundaries. D.3 PATCH FEATURE ANALYSIS Figure 11: Mutual information vs. success rate. For each method (CLIP, DINOv2, ViT, FTR, LTR, PER, PER+CTA), we report the mean s.d. over 10 random seeds. The dashed line is an ordinary least-squares fit on 70 points (7 methods 10 seeds) summarizing the overall trend. Figure 11 shows that PER+CTA occupies the region with the lowest mutual information and the highest success rate. We fit linear model to all 70 training results (7 methods 10 random seeds) and observe negative association: lower mutual information correlates with higher success. For the effect of CTA, we find that adding CTA markedly reduces both mutual information and variance across seeds, leading to more stable and robust training. Additional per seed visualizations with and without CTA (Figure 12) further support this observation: without CTA, the mutual information distribution varies substantially across different random seeds; with CTA, the Robot Router always 17 Preprint. concentrates on the task relevant region (the left middle area with high mutual information corresponding to the target pen pose) and suppresses background regions with low mutual information. Figure 12: Mutual information between patch features before and after the Vision Expert Library on pen. We plot the image across 10 random seeds for training. Figure 13 shows that without CTA, the Robot Router produces noisy patch embeddings with extremely large feature norms in background regions. Although it can sometimes attend to the correct regions and ignore task-irrelevant patches, its behavior is highly sensitive to the training random seed. This indicates that training lightweight router is prone to early collapse and limited exploration. In contrast, with CTA, the Robot Router consistently focuses on task-relevant patches in more robust manner. To further analyze this phenomenon, we plot the expert utilization frequency over the entire robot dataset for 10 random seeds in Figures 14 and 15. With CTA, the utilization frequencies are noticeably more consistent across seeds, which indicates that CTA helps avoid early collapse and insufficient exploration, thereby leading to more robust Robot Router learning. 18 Preprint. Figure 13: Feature visualization of PER with and without CTA across 10 seeds. Without CTA, the Robot Router attends broadly to the background and generates extreme feature-norm outliers, and its behavior is strongly influenced by the training random seed. With CTA, the Robot Router robustly suppresses task-irrelevant patches and concentrates on task-related regions across all the seeds. Preprint. Figure 14: Expert utilization frequency with and without CTA on pen (10 seeds). Without CTA, expert utilization frequency varies substantially across random seeds. With CTA, it is more consistent across seeds, indicating improved training robustness and more stable Robot Router. 20 Preprint. Figure 15: Expert utilization frequency with and without CTA on relocate (10 seeds). Without CTA, expert utilization frequency varies substantially across random seeds. With CTA, it is more consistent across seeds, indicating improved training robustness and more stable Robot Router."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Peking University",
        "Stony Brook University",
        "UC Berkeley",
        "UNC-Chapel Hill",
        "University of Hong Kong"
    ]
}