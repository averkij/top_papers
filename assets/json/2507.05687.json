{
    "paper_title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs",
    "authors": [
        "Shangzhan Li",
        "Zefan Wang",
        "Ye He",
        "Yuxuan Li",
        "Qi Shi",
        "Jianling Li",
        "Yonggang Hu",
        "Wanxiang Che",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 7 8 6 5 0 . 7 0 5 2 : r AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs AUTOTRITON: AUTOMATIC TRITON PROGRAMMING WITH REINFORCEMENT LEARNING IN LLMS Shangzhan Li1,2, Zefan Wang1, Ye He1,2, Yuxuan Li1,, Qi Shi1,, Jianling Li3, Yonggang Hu4, Wanxiang Che2, Xu Han1, Zhiyuan Liu1, Maosong Sun1 1Tsinghua University 2Harbin Institute of Technology 3Tianjin University 4OpenBMB szli@ir.hit.edu.cn, yxuanl1995@gmail.com, qshi9510@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardwarespecific optimizations through extensive empirical tuning. Although domainspecific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AUTOTRITON, the first model dedicated to Triton programming powered by reinforcement learning (RL). AUTOTRITON performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TRITONBENCH and KERNELBENCH illustrate that our 8B model AUTOTRITON achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AUTOTRITON, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton."
        },
        {
            "title": "INTRODUCTION",
            "content": "Efficient kernel engineering serves as the bedrock of high-performance deep learning systems, enabling models to execute optimally across an increasingly heterogeneous hardware landscape (Abadi et al., 2016; Paszke et al., 2019). Historically, crafting such kernels in low-level languages like CUDA has been the exclusive domain of performance engineers, demanding intimate knowledge of hardware architecture and complex parallel programming patterns (Tillet et al., 2019). The advent of Pythonic GPU programming frameworks, most notably Triton (Tillet et al., 2019), has marked significant leap in programmability. Notwithstanding these advances, such high-level abstractions have not fully eliminated the complexities of performance tuning. Developers are still burdened with the manual configuration of crucial parameters like tiling configurations and data layouts, process of empirical trial-and-error that represents primary bottleneck to realizing performance portability and widespread adoption. Current research in AI-assisted kernel generation has attracted increasing attention. Several benchmarks, such as TRITONBENCH (Li et al., 2025) and KERNELBENCH (Ouyang et al., 2025), have been introduced to systematically evaluate the capabilities of LLMs in generating high-performance Corresponding authors. 1 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Figure 1: Overview of AUTOTRITON pipeline. The entire pipeline consists of three components: data collection, SFT stage, and RL stage. kernels. In addition to benchmarks, recent work such as AI CUDA Engineer (Lange et al., 2025) has gained widespread interest. This framework leverages general-purpose LLMs as foundation components to construct an automated workflow. However, its adaptability and flexibility remain limited due to the inherent capability boundaries of the underlying models. In this work, we introduce AUTOTRITON, the first model dedicated to Triton programming powered by reinforcement learning (RL). AUTOTRITON is built upon Seed-Coder-8B-Reasoning (Zhang et al., 2025), which is reasoning model dedicated to programming, further enhanced through synergistic combination of supervised fine-tuning (SFT) and RL, which is shown in Figure 1. In the SFT phase, we first design and implement dedicated data construction pipeline. This pipeline is instrumental in assembling high-quality Triton dataset that explicitly elucidates key programming concepts and reasoning steps inherent to Triton, thereby equipping AUTOTRITON with foundational programming capabilities. Subsequently, we leverage the data generated from the pipeline again and conduct RL with combined rule-based and execution-based reward. This phase encourages the model to explore and internalize effective Triton programming strategies, allowing it to capture practical nuances and efficiencies that are challenging to instill through supervised fine-tuning alone. Experimental results on two typical benchmarks TRITONBENCH and KERNELBENCH show that AUTOTRITON achieves performance comparable to mainstream large models, including GPT-4o, Claude-4-Sonnet, Qwen3-32B, DeepSeek-R1-0528, on all five benchmark channels with only 8B parameters, which indicates the effectiveness of AUTOTRITON on the Triton programming task and highlights the crucial impact of our proposed data gathering pipeline and RL training strategy. Further analysis underscores the pivotal roles of the SFT, RL, and reward design components in AUTOTRITON. These findings offer what we consider to be crucial guidance for future research in this direction."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 LLM FOR KERNEL GENERATION Computation kernel generation is crucial for optimizing AI workloads on diverse hardware. Typical approaches, including MLIR (LLVM Project, 2019), TVM (Apache Software Foundation, 2018), collectively enhance AI model performance and portability, addressing the complexities of modern heterogeneous computing environments (Al-Dujaili et al., 2024). Recently, the automation of GPU kernel generation, critical for optimizing machine learning performance, has attracted significant research attention. Systematic evaluation of LLMs in this domain is facilitated by benchmarks such as KERNELBENCH (Ouyang et al., 2025), which assesses the generation of fast and correct 2 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs kernels across diverse workloads using metrics like fast p. While frontier models excel at general programming tasks, they often fall short on kernel generation tasks, underscoring gap between general coding capabilities and specialized kernel optimization demands. Similarly, TRITONBENCH (Li et al., 2025) highlights the challenges LLMs face with domain-specific languages like Triton, revealing difficulties in generating efficient kernels due to unfamiliarity with Tritons specifications and GPU programming intricacies. Beyond benchmarks, frameworks like AI CUDA Engineer (Lange et al., 2025) utilize agentic approaches, leveraging LLMs for PyTorch-to-CUDA translation and iterative optimization. Despite achieving notable speedups, these training-free approaches are fundamentally constrained by the inherent limitations of the foundation LLMs. To directly enhance model capabilities, Kevin-32B (Baronio et al., 2025) employs multi-turn reinforcement learning, enabling the model to learn from environmental feedback and significantly improve kernel correctness and performance through self-refinement, particularly on complex tasks. Furthermore, the DeepSeek-R1 model, augmented with test-time scaling, demonstrates the efficacy of allocating increased inference compute for iterative refinement and verification, achieving high correctness on KERNELBENCH tasks (NVIDIA Developer Blog, 2025). These advancements collectively indicate trend towards iterative, feedbackdriven methodologies to enhance LLM proficiency in specialized high-performance code generation. Additionally, KERNELLLM (Fisches et al., 2025) generates Triton kernels via supervised fine-tuning. Despite achieving reasonable performance, it is fundamentally constrained by the ceiling of imitation learning. It fails to leverage exploration, thereby limiting its ability to produce higher-quality Triton kernels. Different from the above works, in this work, we propose AUTOTRITON, the first model specifically designed for Triton programming with reinforcement learning, achieving remarkable improvements across five typical benchmark channels. 2.2 RL FOR CODE RL provides powerful paradigm for agents to learn optimal policies through interaction with dynamic environments, maximizing cumulative rewards. Early applications in code generation formulate the problem within Markov Decision Process (MDP), where partial programs constitute states and grammar productions serve as actions (Chen et al., 2020). This formulation highlights RLs flexibility in adapting to different levels of abstraction. Modern advancements leverage LLMs, treating the code-generating model as an actor and code generation as actions, with functional correctness derived from unit test results providing the reward signal (Le et al., 2022). This approach has enabled systems like AlphaCode to achieve competitive performance in complex coding tasks (Li et al., 2022). Beyond generation, RL is extensively applied in code optimization, notably for learning optimal sequences of compiler passes (Bendib et al., 2024; Shahzad et al., 2022). Here, states are often represented by statistical analyses of Intermediate Representation (IR) or graph-based models, and rewards are tied to performance metrics such as cycle count, area, or resource utilization (Shahzad et al., 2022). The success of frameworks like CYCLE further demonstrates RLs capacity for iterative self-refinement of faulty code generations, learning from execution feedback, and significantly improving refinement capabilities (Ding et al., 2024). Despite these advancements, RL for code faces substantial challenges. Designing robust reward functions remains primary concern. Poorly engineered rewards can lead to unintended behaviors or reward hacking, where the agent exploits the reward structure rather than achieving the intended objective (Milvus, 2023). Training instability, especially when fine-tuning large language models, presents another hurdle, with algorithms like REINFORCE++ Hu (2025) often suffering from volatile policy updates. To address these instabilities and enhance training convergence, improved algorithms such as Group-Relative Policy Optimization (GRPO) have been developed, which also help in eliminating reward hacking within RL frameworks for LLMs (Shao et al., 2024). Our proposed AUTOTRITON aligns with this perspective. In the field of kernel generation, we employ the GRPO algorithm and design reasonable rewards to build the LLMs understanding of kernel queries and conduct RL-powered Triton programming accordingly."
        },
        {
            "title": "3 AUTOTRITON",
            "content": "In this section, we introduce AUTOTRITON, specialized model tailored for the Triton programming task. AUTOTRITON is characterized by sequential two-stage process. Initially, the model undergoes 3 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Figure 2: Data gathering pipeline of AUTOTRITON. Our pipeline begins with the systematic collection of PyTorch kernels, then generates corresponding Triton kernels by instruction-guided LLM distillation and compilation with LLM enhanced refinement simultaneously. SFT to establish strong foundation in Triton programming principles. Following this, an RL framework is applied, which allows execution-based feedback of GPU code, guiding the model to further optimize the generated kernels correctness. To elucidate AUTOTRITON, we will first formally perform the task formulation, then detail the supervised fine-tuning procedure, and subsequently present the design of the reinforcement learning framework. 3.1 PROBLEM FORMULATION Developing custom kernels traditionally demands substantial domain expertise and involves significant amount of empirical trial-and-error. To accelerate this development lifecycle, we define the task of Triton programming. This task aims to learn mapping from comprehensive kernel specification to its corresponding executable Triton implementation. kernel specification D, generally comprises two primary components: concrete PyTorch implementation or formal interface definition that details its functional description, the signatures of input and output parameters (including data types), and the dimensionality (shapes) of the tensors involved. The core challenge is to develop model that, given D, synthesizes Triton kernel that is not only syntactically correct and executable but also semantically faithful to all requirements outlined in D. 3.2 SUPERVISED FINE-TUNING Recent studies (Li et al., 2025) have highlighted that even models proficient in general-purpose programming exhibit limited capabilities in generating specialized Triton kernels. To bridge this capability gap and equip our model with essential Triton programming expertise, we develop meticulous data gathering pipeline to produce high-quality data for supervised fine-tuning. This pipeline automates the crucial steps of data collection, synthesis, and validation, with the explicit goal of retaining only high-fidelity, syntactically sound, and demonstrably correctly executable data for training. The architecture of the pipeline is illustrated in Figure 2. Our proposed pipeline for data acquisition begins with the systematic collection of PyTorch kernels. This entails harvesting kernels from established open-source platforms like GitHub and HuggingFace, supplemented by the algorithmic composition of basic kernels through the PyTorch interface. We then leverage an open-source LLM proficient in programming, such as Qwen 2.5 Coder (Hui et al., 2024), for the automated generation of test cases, which are subsequently used to validate and retain executable PyTorch kernels. 4 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Following the collection of PyTorch kernels, we employ two distinct strategies for generating corresponding Triton kernels: instruction-guided LLM distillation and compilation with LLMenhanced refinement. The distillation-based approach involves creating targeted instructions that encapsulate both the PyTorch kernels functionality and relevant Triton-specific knowledge. capable deep-reasoning LLM, such as DeepSeek R1 (Guo et al., 2025), is prompted with these instructions to produce Triton code, accompanied by step-by-step Chain-of-Thought (CoT) explanation. The generated Triton snippets are then cross-validated against the original PyTorch kernels using the previously generated test cases, and only functionally equivalent pairs are selected for supervised fine-tuning. Recognizing the inherent limitations of general-purpose LLMs in proficiently generating Triton code (Li et al., 2025), we also leverage compilation-based approach for enhanced data acquisition efficiency. Specifically, PyTorch code snippets are processed using torch.compile. The resultant compiled artifacts are then refined by an LLM to improve human readability; this involves tasks such as inserting explanatory comments, removing extraneous decorators, and renaming variables to be more semantically meaningful. After verifying functional equivalence with PyTorch using test cases, we leverage an LLM to craft instructions. These, along with the verified Triton code, are used to prompt an LLM to generate detailed CoT narratives, aiming to instill Triton programming paradigms during supervised fine-tuning. Finally, the culminating dataset, comprising <instruction, Triton code with CoT> pairs, is leveraged for supervised fine-tuning. The model learns to predict the Triton code and its CoT justification conditioned on the instruction prompt. thereby developing foundational capabilities in Triton programming. 3.3 REINFORCEMENT LEARNING To further push the border of the coding ability of AUTOTRITON, we adopt common Reinforcement Learning with Verifiable Reward (RLVR) pipeline. Our training process is based on the GRPO algorithm (Shao et al., 2024), updating the policy with group normalized objective: JGRP O(θ) = E[q (Q), {oi}G i=1 πθold(Oq)] 1 (cid:88) i=1 1 oi oi (cid:88) (cid:26) min t=1 (cid:20) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) ˆAi,t, clip (cid:18) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:21) ˆAi,t βDKL(πθπref ) (cid:27) (1) where πθ and πθold are the policy model and reference model, and ˆAi,t is the group-wise advantage: ri mean({rj}N std({ri}N i=1) ˆAi,t = j=1) (2) . The reward function is defined by the following equation, which combines an execution-based component with rule-based one: R(ˆa) = (cid:26)1, is Triton(ˆa) & test passed(ˆa) 0, otherwise (3) where the test passed component confirms functional correctness. It verifies that the generated code passes all test cases and is semantically equivalent to reference PyTorch implementation, within tolerance of ϵ. While the is Triton component ensures syntactic validity by using rule-based linter to check if the code conforms to the Triton language specification. This component regularizes the policy to discourage reward hacking, scenario where the model might generate non-Triton code (such as simpler PyTorch code) that would trivially pass the functional tests. The data for our RL stage is also generated using the pipeline from 3.2. In this stage, we only retain <instruction, PyTorch code> pairs, as the reference PyTorch code is sufficient for deriving reward signal through test-case execution, eliminating the need for labeled Triton code. This enables the inclusion of more difficult, out-of-distribution (OOD) data well-suited for RL exploration. The final training data is strategic mix of these novel instances and small portion of in-distribution data from the SFT phase to ensure smooth policy transition. 5 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate the performance of AUTOTRITON and conduct comprehensive analysis from multiple aspects."
        },
        {
            "title": "4.1 EVALUATION SETUP",
            "content": "Evaluation Benchmarks We evaluate AUTOTRITON using two established benchmarks: TRITONBENCH (Li et al., 2025) 1 and KERNELBENCH (Ouyang et al., 2025) 2. TRITONBENCH assesses LLM capabilities in generating Triton kernels, which is divided into two evaluation channels: TRITONBENCH-G consists of 184 real-world kernels from GitHub and TRITONBENCH-T consists of 166 kernels aligned with PyTorch interfaces. While KERNELBENCH evaluates LLM proficiency in generating efficient GPU kernels for neural network optimization across 250 tasks, categorized into Level 1 (100 single-kernel tasks, e.g., convolution, for CUDA replacement), Level 2 (100 simple fusion tasks, e.g., conv+bias+ReLU, for fused CUDA kernels), and Level 3 (50 full architecture tasks, e.g., MobileNet, for end-to-end CUDA optimization). The prompts used during inference are detailed in figure 5. Evaluation Metrics Regarding the evaluation metrics, we synthesize those from the above two benchmarks and categorize them into two aspects: (1) Compilation Accuracy (errorfree compilations); (2) Call Accuracy (error-free invocation); (3) Execution Accuracy (correct input-output behavior); (4) Speed Up (relative execution time improvement) on both benchmarks. Following the original settings of both benchmarks, we evaluate Compilation Accuracy, Execution Accuracy, Speed Up on KERNELBENCH, and evaluate Call Accuracy, Execution Accuracy, Speed Up on TRITONBENCH respectively. For evaluations on TRITONBENCH-G, the Speed Up value is derived by comparing against their supplied reference Triton code, while for all other evaluations, Speed Up is calculated against PyTorch implementations. Following KERNELBENCH, we report fastp to measure the absolute speedup of Triton codes across the entire benchmark, which is calculated as follows: fastp = 1 (cid:88) i= 1(correcti {SpeedUpi > p}), (4) Training Details In the SFT stage, we utilize the LLaMA-Factory framework (Zheng et al., 2024) with dataset of 14, 102 samples. We set the maximum sequence length to 16, 384 and use training batch size of 1 per device. The model is fine-tuned with learning rate of 1 105 for 3 epochs. This stage is completed in approximately 16 hours on single node with 8 A800 GPUs. For the subsequent RL stage, we adopt the VeRL framework (Sheng et al., 2025), using the dataset containing 6, 302 samples. In this phase, the training batch size is set to 64. The maximum prompt length is capped at 4, 096 tokens, while the maximum response length is set to 16, 384 tokens. The learning rate for the actors optimizer is configured to 1 106. The model is trained for 1 epoch. This phase requires approximately 32 hours of training time on two nodes, utilizing total of 16 A800 GPUs. 4.2 MAIN RESULTS Table 1 and Table 2 show the experimental results of AUTOTRITON on TRITONBENCH and KERNELBENCH, respectively. Experiments are systematically conducted across five evaluation channels of the TRITONBENCH and KERNELBENCH benchmarks. In terms of correctness (Exec in Table), AUTOTRITON decisively surpasses powerful models including DeepSeek-R1-0528, GPT-4o, Claude4-Sonnet, DeepSeek-R1-0120, and Qwen3-32B, which proves the effectiveness of AUTOTRITON across both its SFT and RL phases, underscoring the significant contributions of our proposed data gathering pipeline and training framework. The superiority of AUTOTRITON is also evident in the runtime performance evaluation. It achieves performance comparable to mainstream large models, including claude-4-sonnet and DeepSeek-R1-0528, on most evaluation channels, underscoring the effectiveness of our proposed data gathering pipeline in yielding high-fidelity training instances. 1We use TRITONBENCH-T version in https://github.com/thunlp/TritonBench/pull/6. 2We use the Triton backend version of KERNELBENCH in https://github.com/ScalingIntell igence/KernelBench/pull/35. 6 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs It is further observed that on the TRITONBENCH-G channel, all evaluated models struggle significantly on both correctness and performance metrics. The inherent difficulty of this channel, which evaluates models on real-world requirements from GitHub against reference Triton implementations, highlights the substantial challenges that persist in automated Triton programming. This suggests the task is far from solved and warrants deeper investigation. Model #Params Seed-Coder-Reasoning Qwen3 Qwen3 GPT-4o Claude-4-Sonnet DeepSeek-R1-0120 DeepSeek-R1-0528 AUTOTRITON w/o RL (SFT only) 8B 8B 32B - - 671B 685B 8B 8B TRITONBENCH-G TRITONBENCH-T Call / Exec fast1 / fast2 Call / Exec fast1 / fast2 2.72 / 2.72 2.17 / 1.63 10.33 / 9.24 10.87 / 10.33 9.24 / 9.24 13.59 / 13.05 16.30 / 15.22 15.76 / 15.76 14.67 / 14.13 0.54 / 0.00 0.54 / 0.00 2.17 / 1.63 4.89 / 1.63 1.64 / 1.09 4.89 / 0.54 3.80 / 0.54 7.61 / 2.17 4.89 / 1.09 3.61 / 3.61 6.02 / 5.42 21.96 / 21.96 18.67 / 15.06 10.84 / 10.84 28.92 / 28.37 30.72 / 30.12 40.36 / 39.16 34.94 / 34. 1.20 / 0.00 2.41 / 0.00 10.84 / 3.01 7.84 / 1.20 4.22 / 1.84 22.89 / 3.01 11.45 / 4.82 17.04 / 6.02 15.06 / 7.83 Table 1: Main results on TRITONBENCH. We present Call Accuracy (Call), Execution Accuracy (Exec), fast1 and fast2. The best-performing and second-best-performing methods are highlighted in Bold and Underline, respectively. Model #Params LEVEL1 LEVEL LEVEL3 Comp / Exec fast1 / fast2 Comp / Exec fast1 / fast2 Comp / Exec fast1 / fast2 Seed-Coder-Reasoning Qwen3 Qwen3 GPT-4o Claude-4-Sonnet KernelLLM DeepSeek-R1-0120 DeepSeek-R1-0528 AUTOTRITON w/o RL (SFT only) 8B 8B 32B - - 8B 671B 685B 8B 8B 48.0 / 10.0 52.0 / 16.0 84.0 / 23.0 91.0 / 15.0 87.0 / 33.0 72.0 / 20.2 95.0 / 30.0 90.0 / 35.0 83.0 / 36.0 65.0 / 29.0 4.0 / 2.0 9.0 / 9.0 5.0 / 4.0 3.0 / 1.0 11.0 / 7.0 / 5.0 / 1.0 7.0 / 1.0 10.0 / 6.0 10.0 / 4.0 44.0 / 11.0 73.0 / 16.0 98.0 / 25.0 83.0 / 5.0 92.0 / 26.0 76.0 / 16.0 91.0 / 26.0 90.0 / 42.0 97.0 / 45.0 85.0 / 27.0 5.0 / 4.0 8.0 / 1.0 15.0 / 2.0 3.0 / 0.0 10.0 / 1.0 / 21.0 / 2.0 28.0 / 2.0 17.0 / 0.0 8.0 / 3. 52.0 / 10.0 40.0 / 14.0 92.0 / 16.0 74.0 / 8.0 82.0 / 18.0 / 74.0 / 4.0 76.0 / 26.0 82.0 / 20.0 64.0 / 6.0 4.0 / 4.0 4.0 / 2.0 6.0 / 0.0 4.0 / 2.0 2.0 / 0.0 / 0.0 / 0.0 14.0 / 2.0 10.0 / 4.0 2.0 / 2.0 Table 2: Main results on KERNELBENCH. We present Compilation Accuracy (Comp), Execution Accuracy (Exec), fast1 and fast2. The best-performing and second-best-performing methods are highlighted in Bold and Underline, respectively. 4.3 ANALYSIS Cross Comparisons for Triton and CUDA Models To further assess the performance of AUTOTRITON, we conduct comparative analysis against prominent kernel generation models not specifically focused on Triton programming, namely AI CUDA Engineer Lange et al. (2025) and Kevin-32B Baronio et al. (2025). We select the KERNELBENCH benchmark for this evaluation due to its capability to assess both Triton and CUDA kernels, ensuring fair comparison. As illustrated in Table 3, we report the P75 and P50 speedups over the PyTorch baseline in the pass@10 setting, which represent the speedup ratios at the 75th and 50th percentiles of the kernel performance distribution. key finding from our analysis is the persistent, systematic gap in automated programming proficiency between Triton and CUDA, which highlights the formidable challenges associated with high-performance Triton code generation. Even against this backdrop, AUTOTRITON establishes its superiority over recent specialized framework AI Cuda Engineer (Lange et al., 2025), delivering quantitatively better results on metrics of both correctness and runtime efficiency. While these results validate the strength of AUTOTRITON, it still lags behind the Kevin-32B (Baronio et al., 2025) model, which we attribute to three potential causes: the intrinsic programming model differences between CUDA and Triton, parameter scale mismatch (8B vs. 32B), and the use of 90% of the evaluation data in Kevin-32Bs training, which likely results in higher evaluation scores. Effects of Reinforcement Learning The final rows of Table 1 and Table 2 present the performance of AUTOTRITON without the RL stage. clear performance uplift is observed when comparing AUTOTRITON to its SFT-only counterpart, demonstrating that RL is effective at raising the performance ceiling for the Triton programming task. This result suggests that RL enables the model to 7 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Model Lang. #Params LEVEL1 LEVEL2 LEVEL3 Comp / Exec P75 / P50 Comp / Exec P75 / P50 Comp / Exec P75 / P50 AI Cuda Engineer - o1-preview - o1-high CUDA CUDA CUDA Claude-4-Sonnet DeepSeek-R1-0528 CUDA CUDA Kevin* Triton KernelLLM Triton Claude-4-Sonnet Triton DeepSeek-R1-0528 Triton AUTOTRITON - - - 685B 32B 8B - 685B 8B / 63.0 / 50.0 99.0 / 64.0 99.0 / 97.0 100.0 / 88.0 99.0 / 52.0 99.0 / 57.0 100.0 / 74.0 100.0 / 68.0 0.96 / 0.45 0.97 / 0.37 1.26 / 0.97 1.23 / 0.85 1.14 / 0.78 / 1.01 / 0.76 1.04 / 0.62 1.01 / 0.69 / 95.0 / 81.0 100.0 / 92.0 100.0 / 100.0 98.0 / 86.0 97.0 / 34.0 100.0 / 68.0 100.0 / 74.0 100.0 / 88.0 1.01 / 1.00 1.00 / 0.87 1.42 / 1.19 1.74 / 1.33 1.64 / 1.24 / 1.41 / 1.12 1.56 / 1.28 1.17 / 1.01 / 19.0 / 12.0 100.0 / 66.0 100.0 / 70.0 100.0 / 70.0 / 99.0 / 60.0 100.0 / 72.0 88.0 / 52.0 1.00 / 0.99 1.00 / 0.93 1.22 / 1.00 1.17 / 1.00 1.10 / 0.92 / 1.12 / 1.00 1.03 / 0.92 1.03 / 1. Table 3: Cross comparison results for Triton and CUDA models on KERNELBENCH. We report pass@10 results for each model. We present Compilation Accuracy (Comp), Execution Accuracy (Exec), Torch P75 (P75) and Torch P50 (P50). The best-performing and secondbest-performing methods are highlighted in Bold and Underline, respectively. * denotes that they use 180 of the evaluation data for training purpose. Model AUTOTRITON w/o rule-based reward w/o RL (SFT only) w/o SFT&RL (Backbone model) TRITONBENCH-T KERNELBENCH-Level 1 5 18 4 6 25 4 10 Table 4: Numbers of generated Triton code that do not contains keyword @triton.jit. transcend the inherent limitations of imitation learning, which aligns with findings across numerous other domains. Furthermore, the performance gains achieved during the RL stage also validate the efficacy of our proposed data gathering pipeline. This pipeline effectively generates training dataset that is highly suitable for exploration during RL, which plays crucial role in pushing the boundaries of the Triton programming task. Effects of Reward Design As mentioned in 3.3, primary challenge in the Triton programming task is reward hacking, where models learn to satisfy test cases without generating correct Triton code. To address this, we introduce auxiliary rule-based rewards alongside the primary execution-based reward to explicitly incentivize adherence to the Triton language specification. The impact of this strategy is quantified in Table 4. By checking for the mandatory @triton.jit decorator, we find that rule-based rewards significantly decrease the count of invalid generations on TRITONBENCH-T (from 18 to 5) and KERNELBENCH-Level1 (from 25 to 6), confirming the importance of explicit syntactic guidance in the reward mechanism. Despite these improvements, rule-driven reward function can still be hacked. Models may learn to generate low-quality code that satisfies the explicit rules but fails to fulfill the complete semantic requirement of Triton. For instance, as illustrated in Figure 3, when tasked with implementing kernel composed of convolution and ReLU (Figure 3(a)), the model often generates valid Triton kernel for the simpler ReLU part while leaving the more complex convolution as fallback PyTorch implementation (Figure 3)(b). More critically, the model might circumvent the reward rules entirely by generating fake Triton kernel that it never calls (Figure 3)(c). This phenomenon of low-quality implementation is highly prevalent across all evaluation models. potential countermeasure involves incorporating runtime-based performance rewards, which we reserve for future work. Effects of Supervised Fine-tuning As shown in Table 1 and Table 2, after undergoing SFT, the model achieves superior performance compared to the original backbone model (Seed-Coder-8BReasoning). This initial result suggests that SFT is effective in familiarizing the model with the fundamental paradigms of Triton programming and further proves the effectiveness of our proposed data gathering pipeline in generating high-quality SFT data. This conclusion is further substantiated by the training dynamics in Figure 4. Although the model without SFT also exhibits notable upward trend in its reward curve, it suffers from severe reward hacking, with the majority of instances displaying the behavior illustrated in Figure 3(c). Specifically, while its generated code can pass the test cases, they often fail to adhere to Tritons basic syntax, defaulting instead to simpler Torch implementations, which is consistent with the phenomenon observed in Table 4. This behavior AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Figure 3: Example of the phenomenon of the low-quality implementation of Triton code. Figure 4: Reward scores of AUTOTRITON and AUTOTRITON w/o SFT stage. indicates that SFT is crucial not only for learning the correct syntax but also for preventing reward hacking, where the model learns to exploit test cases with trivial Torch code rather than mastering genuine Triton programming. Limitations Based on the multi-faceted evaluation of AUTOTRITON, the primary limitation is that our current training framework lacks performance-guided training. Since the compiled or distilled kernels lack efficient runtime feedback, AUTOTRITON does not contain performance-guided training in the current stage. Future work could focus on procuring or generating higher-quality data and integrating performance-aware training framework simultaneously, rewarding the model for generating kernels that are not only functionally correct but also achieve high performance on target hardware, thereby guiding it toward more efficient solutions."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we study the task of automated Triton programming and propose AUTOTRITON, the first model dedicated to Triton programming powered by RL. AUTOTRITON involves two-stage training process: an SFT stage where AUTOTRITON learns essential Triton programming expertise from high-quality data generated by our novel data curation pipeline, followed by an RL stage where it further improves by exploring more challenging problem instances. Evaluations across five channels of TRITONBENCH and KERNELBENCH show that AUTOTRITON achieves performance comparable to mainstream large models, including claude-4-sonnet and DeepSeek-R1-0528. Our in-depth analysis of each component validates the significant potential of RL-based methods for automatic Triton programming. Ultimately, AUTOTRITON demonstrates promising pathway toward 9 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs the automated generation of efficient kernels, offering new paradigm for building high-performance AI systems."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The work is initiated and supported by the AI9Stars Team. We are grateful for the support of the OpenBMB and InfiniteTensor teams."
        },
        {
            "title": "REFERENCES",
            "content": "Martın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow}: system for {Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pp. 265283, 2016. Ahmed Al-Dujaili, Ali Al-Dujaili, Husam Al-Dujaili, Mustafa Al-Dujaili, and Zaid Al-Dujaili. Looper: learned optimizer for polyhedral compilers. arXiv preprint arXiv:2403.11522, 2024. URL https://arxiv.org/pdf/2403.11522. Apache Software Foundation. Apache TVM Open Deep Learning Compiler Stack. https: //tvm.apache.org/, 2018. Accessed: June 18, 2025. Carlo Baronio, Pietro Marsella, Ben Pan, and Silas Alberti. Multi-turn training for cuda kernel generation. Cognition AI Blog. URL: https://cognition.ai/blog/kevin-32b, 2025. Accessed on May 06, 2025. Zakaria Bendib, Duc-Manh Le, Khanh-Duy Nguyen, Anh-Duy Le, Quang-Thuan Le, Duc-Trong Nguyen, and Duc-Anh Le. Enhancing code llms with reinforcement learning in code generation: survey. arXiv preprint arXiv:2412.20367, 2024. URL https://arxiv.org/html/2412. 20367v4. Yanju Chen, Xinyu Wang, and Isil Dillig. Program synthesis using deduction-guided reinforcement learning. Proceedings of the ACM on Programming Languages, 4(POPL):129, 2020. URL https://pmc.ncbi.nlm.nih.gov/articles/PMC7363208/. Yangruibo Ding, Marcus Min, Gail Kaiser, and Baishakhi Ray. Cycle: Learning to self-refine the code generation. arXiv preprint arXiv:2403.18746, 2024. URL https://arxiv.org/abs/ 2403.18746. Zacharias Fisches, Sahan Paliskara, Simon Guo, Alex Zhang, Joe Spisak, Chris Cummins, Hugh Leather, Joe Isaacson, Aram Markosyan, and Mark Saroufim. Kernelllm, 5 2025. URL https: //huggingface.co/facebook/KernelLLM. Corresponding authors: Aram Markosyan, Mark Saroufim. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948, 2025. URL https://arxiv.or g/abs/2501.12948. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186, 2024. URL https://arxiv.org/abs/2409.12186. Robert Tjarko Lange, Aaditya Prasad, Qi Sun, Maxence Faldor, Yujin Tang, and David Ha. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. 2025. Cong Duy Vu Le, Jinxin Chen, Zihan Li, Hongyu Sun, Yuan Liu, Ming Chen, Yicheng Zhang, Zhihong Zhang, Hong Wang, Sheng Yang, et al. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiv preprint arXiv:2207.01780, 2022. URL https://ar5iv.labs.arxiv.org/html/2207.01780. 10 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, et al. Tritonbench: Benchmarking large language model capabilities for generating triton operators. arXiv preprint arXiv:2502.14752, 2025. Yujia Li, David Choi, Junyoung Chung, Nate Glaese, Rew Beattie, Markus Pex, Huanling Wu, Edward Zielinski, Quandong Ma, Timo Wicke, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. URL https://www.researchgate.n et/publication/366137000_Competition-level_code_generation_with_ AlphaCode. LLVM Project. MLIR Multi-Level Intermediate Representation. https://mlir.llvm.org/, 2019. Accessed: June 18, 2025. Milvus. What are the limitations of reinforcement learning. https://milvus.io/ai-qui ck-reference/what-are-the-limitations-of-reinforcement-learning, 2023. NVIDIA Developer Blog. Automating gpu kernel generation with deepseek-r1 and inference time scaling. NVIDIA Developer Blog, February 2025. URL https://developer.nvidia.c om/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-i nference-time-scaling/. Accessed on May 20, 2025. Anne Ouyang, Simon Guo, Simran Arora, Alex Zhang, William Hu, Christopher Re, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlcheBuc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 80248035, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abs tract.html. Adeel Shahzad, Muhammad Shahzad, Muhammad Khan, Irfan Ullah, Abdulbasit Al-Sumaiti, Abdulrahman Al-Sumaiti, and Abdulrahman Al-Sumaiti. Reinforcement learning strategies for compiler optimization in high level synthesis. Boston University, 2022. URL https: //www.bu.edu/caadlab/Shahzad22.pdf. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 1019, 2019. Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, et al. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. 11 AUTOTRITON: Automatic Triton Programming with Reinforcement Learning in LLMs"
        },
        {
            "title": "A INFERENCE PROMPTS",
            "content": "Figure 5: AUTOTRITON prompts for experimental reasoning."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "OpenBMB",
        "Tianjin University",
        "Tsinghua University"
    ]
}