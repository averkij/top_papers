{
    "paper_title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
    "authors": [
        "Shijie Lian",
        "Changti Wu",
        "Laurence Tianruo Yang",
        "Hang Yuan",
        "Bin Yu",
        "Lei Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 7 4 4 2 . 9 0 5 2 : r Euclids Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks Shijie Lian1,2*, Changti Wu3,2*, Laurence Tianruo Yang4,1, Hang Yuan2, Bin Yu2, Lei Zhang3, Kai Chen5 1Huazhong University of Science and Technology 2Zhongguancun Academy 3East China Normal University 4Zhengzhou University 5Zhongguancun Institute of Artificial Intelligence"
        },
        {
            "title": "Abstract",
            "content": "Spatial intelligence spans rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as surrogate task. Specifically, we meticulously constructed curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSIBench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM. To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in this. 1. Introduction The whole is greater than the part. Euclid, Elements I, Common Notion 5 *These authors contributed equally Corresponding author In recent years, multimodal large language models (MLLMs) [2, 41, 46] have achieved remarkable success across broad range of vision-language tasks, from image captioning and visual question answering (VQA) to document understanding [4, 9, 28, 29, 37, 47]. State-of-the-art models like GPT-4o, Qwen2.5VL-series, and InternVL2.5series now rival or even surpass human performance on certain benchmarks, especially tasks requiring advanced language understanding or mathematical reasoning [9, 17, 31]. For instance, model like GPT-4o [31], Claude-3.5-Sonnet [3], InternVL2-40B [17], and Qwen2.5VL-32B [9], have exceeded the average human score on the MathVista multimodal math benchmark [49], reflecting the rapid progress in integrating visual perception with high-level reasoning. Despite recent progress, state-of-the-art MLLMs still fall short of genuine spatial intelligence [72, 78]. Spatial intelligence involves perceiving and mentally manipulating spatial relationships and spans several tasks, such as estimating quantity, interpreting spatial relations, and understanding geometric configurations [27, 72]. Nowadays, leading visionlanguage models (VLMs) still make occasional mistakes on tasks that young children solve with ease, such as determining object orientation or identifying which object is the nearest neighbor to given object on its left [42, 53]. recent evaluation on the Visual-Spatial Intelligence Benchmark (VSIBench) shows that more than 70% of the recorded errors arise from faulty spatial reasoning, not from deficiencies in visual recognition or language parsing [72]. This phenomenon is consistent with Moravecs paradox [55], which suggests that high-level reasoning tasks are computationally simpler for VLM than low-level perceptual and sensorimotor skills. Closing this gap is essential for the next generation of VLMs [22]. Recent work on spatially aware VLMs, including Spatial-MLLM [69], SpaceVLM [13], RoboBrain [34], and RoboBrain2.0 [8], attempts to provide specially constructed spatial datasets to improve model performance. However, tasks in these spatial datasets usually cover only subset of real-world spatial tasks and may not enhance the models overall spatial intelligence. For example, Spatial-MLLM 1 Figure 1. Performance gains on VSIBench after model training on Euclid30K, for more complete data please refer to Tab. 2. collects data from ScanQA [7], SQA3D [51], and selfcurated spatial QA data, and follows the eight tasks introduced in VSI-Bench [72] to build the Spatial-MLLM-120K dataset. The trained model therefore achieves state-of-theart results on VSI-Bench, ScanQA, and SQA3D. However, its accuracy drops on the out-of-domain MindCube benchmark [74]. This highlights critical challenge in the field: while fine-tuning on task-specific datasets can achieve high in-domain performance, it may lead to over-specialization and fail to cultivate more fundamental, generalizable spatial intelligence. To bridge this gap, VLMs must learn from broader and more foundational range of spatial phenomena, thus extending their capabilities beyond the limitations of any single dataset. In order to develop generalized spatial skills beyond any single benchmark, in this work, we attempt to explore novel training paradigm that incorporates solving geometric problems as surrogate task for enhancing spatial intelligence in the VLM. Geometry compresses centuries of mathematical study into formal descriptions of space. Therefore, learning to solve planar and solid geometry problems forces the model to internalize the axioms and constraints of Euclidean geometry, and provides the model with stronger out-of-domain (OOD) generalization capabilities, because these principles are universal and independent of any single task. As shown in Fig. 1, these low-level geometry priors, like Euclids Gift, provide principled foundation that supports zero-shot transfer to wide range of downstream spatial tasks. This suggests that the abilities required to solve geometric problems, including recognizing shapes and configurations, inferring spatial relationships such as parallel, angular, and relative positions, calculating or measuring geometric elements, and performing multistep logical reasoning, are also required for spatial perception tasks, like object counting, relational localization, and size estimation [19, 72]. Thus, by using geometry as surrogate task, we aim to instill foundational Euclidean priors that support significant and critical subset of spatial intelligence, i.e., spatial perception and reasoning. [20, 57]. and released Euclid30K, which contains 29,695 geometry VQAs collected from Geometry3K [48], MMK12 [54], and SolidGeo [67]. The answers to each question in Euclid30K were reformatted so that they could be directly recognized by the rule-based reward function in Group Relative Policy Optimization (GRPO) [62]. Euclid30K spans the full K12 and Olympiad geometry syllabus, covering planar and solid reasoning, metric relations, and classical theorems. Furthermore, in order to attribute the performance improvement strictly to the supervision of geometric knowledge rather than the interference of complex algorithms or data augmentation, we intentionally used the wellestablished GRPO framework and Euclid30K to train the models. Specifically, we used GRPO to fine-tune the Qwen2.5VL series (3B, 7B, 72B parameters) and the RoboBrain2.0 series (7B, 32B parameters). The resulting geometry-trained models deliver consistent gains on SuperCLEVR [42], Omni3D-Bench [53], VSI-Bench [72], and MindCube[74]. This suggests that the abstract geometric knowledge distilled from Euclid30K can be migrated to different spatial tasks and supports the models static spatial perception and reasoning capabilities, is an effective alternative for enhancing spatial intelligence in VLMs. To our knowledge, this is the first work to use geometry problems as surrogate tasks to cultivate spatial intelligence in general-purpose VLMs. Unlike prior approaches that fine-tuned models on single skill, our training paradigm uses geometric tasks to endow the model with deeper, principle-driven understanding of space. This allows the model to go beyond the baseline in multiple spatial reasoning tasks without additional task-specific training. In summary, our contributions are as follows: We demonstrate that tackling geometry tasks can serve as an effective surrogate task for spatial intelligence. Learning to solve geometry problems assists models in acquiring the most basic perceptions of space, such as the axioms of Euclidean geometry. These low-level priors provide principled foundation that supports the zero-shot transfer to wide range of downstream spatial tasks. To operationalize geometry-centric training, we curated We collected and constructed the Euclid30K, VQA 2 dataset of 29,695 geometry questions. Euclid30K provides comprehensive range of varied and challenging geometry problems that cover wide variety of geometric concepts to better help models learn from geometry problems formal descriptions of space. Extensive experiments on the Qwen2.5VL-series and RoboBrain2.0-series show significant performance gains on four spatial benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube), providing empirical evidence that the geometric curriculum reliably enhances spatial reasoning across diverse evaluation settings. 2. Related Work 2.1. Spatial Intelligence Spatial intelligence is the capacity to reason about spatial relations, including mental rotation, viewpoint switching, and object counting [27, 72]. Benchmarks such as CLEVR [35], Super-CLEVR [42], Omni3D-Bench [53], VSI-Bench [72], and MindCube [74] show that accurate visual recognition alone is not enough; models must also perform explicit spatial reasoning. Early VLMsfor example ViLT [36], METER [25], Flamingo [2], and PaLI [16]improved perception yet still struggled with counting and orientation. Recent spatial-aware systems such as SpaceVLM [13], Spatial-MLLM [69], SpatialLLM [50], RoboBrain [34], and RoboBrain 2.0 [8] fine-tune on curated spatial datasets, but these datasets cover only part of real-world scenarios and are costly to build. Our approach instead trains on readily available Euclidean geometry problems, treating them as surrogate tasks that transfer their principles to general visualspatial perception and reasoning without costly data collection or architectural changes. 2.2. Multimodal Math & Geometry Datasets Research on multimodal mathematical reasoning has produced range of datasets that progressively broaden both topic scope and problem format. GeoQA [14], Geometry3K [48], and UniGeo [15] pioneered the use of diagram-text pairs for plane-geometry question answering, laying the groundwork for visual-symbolic reasoning. Later collections such as MMMU [76], MathVista [49], We-Math [58], We-Math2[59], MMMU-Pro[77], and GeoSense [71], expanded to wider array of mathematical domains and introduced more challenging multimodal tasks. The recently released SolidGeo3K [67] benchmark focuses on three-dimensional geometry and offers fine-grained skill tags along with difficulty annotations, providing dedicated testbed for solid-geometry reasoning. The GeoReasoning10K [70] contains 10,000 pairs of carefully constructed geometric images and descriptions, ensuring perfect alignment between visual and textual information. 3. Method This section first explores the potential mechanisms by which solving geometric problems enhances spatial intelligence through perspective that integrates transfer learning with educational psychology. Then, we aim to enhance the geometric problem-solving capabilities of VLMs through reinforcement learning. Finally, we validate our hypothesis by verifying whether their spatial perception and reasoning skills improve through series of benchmark tests. 3.1. Rationale This subsection presents domain-adaptation view explaining why training on geometric problem solving serves as an effective surrogate for spatial intelligence. Let the source distribution DS denote geometry problem solving (e.g., Euclid30K) and the target distribution DT denote spatial intelligence. We train VLM policy πθ on the source. The policy induces hypothesis: h(x) = I(cid:2) Ans(x; πθ) is correct(cid:3) {0, 1}. (1) Let be the hypothesis class containing h. For h, H, define the error (disagreement probabilities): ϵS(h, h) := Pr xDS [h(x) = h(x)], Specifically, when is an absolutely correct hypothesis, we abbreviate ϵS(h, ) as ϵS(h). The definition of ϵT (h, h) and ϵT (h) follows similarly. Furthermore, the HH distance [11] between DS and DT is: dHH(DS, DT ) = 2 sup h,hH (cid:12) (cid:12)ϵS(h, h) ϵT (h, h) (cid:12) (cid:12) (cid:12) (cid:12). (2) Based on this definition, we can invoke the standard domain-adaptation bound [11], which connects the ϵT (h) to the ϵS(h) through the HH distance. Standard Domain-Adaptation Bound For any H, we have: ϵT (h) ϵS(h) + ϵideal + 1 2 dHH(DS, DT ), (3) where ϵideal denotes the error of the ideal model and can be considered as an extremely tiny constant. In light of the bound in Eq. (3), the target error is governed by the source error and the distribution discrepancy dHH(DS, DT ). In practice, ϵS(h) can typically be reduced through optimization and data curation, which makes the discrepancy term pivotal [11, 52]. Consequently, if dHH(DS, DT ) is small, we can regard the source distribution DS as surrogate for the target distribution DT . detailed proof of Eq. (3) can be found in Appendix A. 3 Statistic Total Number Mathematical Expression Numeric Multiple-Choice Type Number 29,695 16,804 6,321 2,618 Plane (2D) / Solid (3D) 18,577 / 11, Newly collected Newly collected Solid Questions Newly collected Images Length Max and Avg question length Max and Avg answer length Max and Avg image numbers 3,996 3,792 1,598 / 229.8 8.9 1. 501 / 8 / Table 1. Statistics of Euclid30K, Mathematical Expression, Numeric and Multiple-Choice are the three types of answers. We hypothesize that formal Euclidean geometry compresses broad set of spatial regularitiescongruence, similarity, perspective, parallelism, intersection, and positional reasoninginto theorem-like constraints reused across downstream spatial-intelligence tasks. Compared to training on narrow sub-skill (e.g., object counting, depth ordering, or size estimation), this breadth plausibly yields smaller dHH between geometry and target tasks, consistent with cognitive-science evidence on the generality of geometric knowledge in human perception and reasoning [26, 39]. We further elaborate on this connection from an educational-psychology perspective in Appendix B. Since the spatial-intelligence target-domain distribution is unknown and there is no canonical proxy dataset ˆDT that faithfully spans the full space of spatial-intelligence tasks, dHH is not directly observable in practice. Nevertheless, our subsequent empirical results align with this qualitative prediction: after Euclid30K fine-tuning, models exhibit consistent gains across Omni3D-Bench, VSI-Bench, Super-CLEVR, and MindCube (in Tables 2, 4, and 3), and qualitative cases in the Appendix (Figs. 712) further illustrate acquired, transferable skills such as perspective (near to far size), parallelism and similarity, and positional inference. These observations provide principled rationale for geometry as surrogate task for spatial intelligence. 3.2. Euclid30K Plane and solid geometry give axiomatized abstractions of spatial phenomena. Training on such problems compels models to internalize Euclidean constraints such as angle and ratio preservation, similarity, and congruence, thereby providing an effective surrogate curriculum for cultivating spatial perception and reasoning skills. Unfortunately, there are currently no large-scale, highquality training datasets tailored for diverse geometric probFigure 2. The examples of the newly collected questions in Euclid30K. More examples can be found in the appendix. lems. To address this, we filtered the required corpus from open-source resources such as Geometry3K [48], MMK12 [54], SolidGeo [67], and WeMath2 [59]. We employed Qwen2.5-VL-72B as an annotator to determine whether problem belongs to plane or solid geometry. After filtering the existing corpus, we identified significant imbalance: only about 7,000 solid geometry problems remained, while there were about 20,000 plane geometry problems. However, solid geometry encompasses more explicit three-dimensional spatial phenomena (e.g., perspective invariance, polyhedron truncation, volume-area relationships), which are equally crucial for VLMs in learning spatial knowledge. Furthermore, existing solid geometry problems predominantly focus on shape recognition, coordinate/angle/area calculations, and similar question types, with insufficient coverage of richer problem types. To address these gaps, we newly collected approximately 4,500 additional problems from commercially available K12 textbooks and standard/competition practice books on junior/senior high school mathematics. These cover categories such as positional relationship determination, dynamic or moving-point problems, folding/unfolding problems, and contextualized geometry word problems. As result, we compiled question pool containing approximately 32,500 candidate questions. To ensure the quality of the collected geometric data, we designed threestage filtering process: Duplicate Filtering: Since even similar texts can lead to vastly different meanings or solution processes when paired with different images, we uniquely identify each question through its image. Specifically, we use imagebased perceptual hashing to filter duplicate questions. Question Splitting: Many materials contain multiple sub-questions under single main question. We utilize the GPT-4o API [31] to detect and enumerate each subquestion, splitting them into independent instances. Formula Formatting: We standardize formulae in questions and answers to LaTeX format via the DeepSeekV3.1 API [24]. This ensures answers can be correctly parsed by MATHVERIFY [32] for subsequent verification. 4 Figure 3. Enhancing Spatial Perception and Reasoning Capabilities in Models Using the Geometric Problem-Solving Dataset (Euclid30K). For example, we replace π/2 with $frac{pi}{2}$. After the aforementioned filtering, splitting, and formatting, we ultimately obtained 29,695 geometry problems, which were compiled into the Euclid30K dataset. Summary statistics about Euclid30K are reported in Table 1. Additionally, we also list some examples of our newly collected data in Fig. 2. Notably, the final Euclid30K dataset contains 3,996 newly collected/generated solid geometry problems, exceeding SolidGeo [67], the largest prior solid geometry dataset, which contained 3,113 problems. Finally, each instance in Euclid30K is represented as text problem p, answer a>, where triple <image(s) i, may contain one or multiple figures, states the given conditions and questions, and is one of: LaTeX expression, numeric value, or the index of the correct option. 3.3. RL Training in Euclid30K We follow the standard GRPO training practice [62] to enhance the geometry-solving ability of the Qwen2.5-VL models (3B, 7B, 72B) and the RoboBrain2.0 models (7B, 32B), but introduce some modifications based on recent work [75] to accelerate training. Specifically, during training, we first sample set of output {oi}G i=1 for each questions = <image i, textual problem p> from the policy model πθold, and computing (θ): (θ) = 1 γ (cid:88) oi (cid:88) i=1 t= min(cid:0)ri,t(θ) ˆAi, clip(ri,t(θ), 1 ϵ) ˆAi (cid:1), (4) 5 std({Ri}G i=1 oi and ri,t(θ) = πθ(oi,tq,oi,<t) where γ = (cid:80)G πθold (oi,tq,oi,<t) . Advantage function ˆAi = Rimean({Ri}G i=1) computed using i=1) the group rewards. In Eq. 4, we follow DAPO [75] using the token-level policy gradient loss and ensure that the prompts in each batch have valid gradients by over-sampling and filtering out prompts with the accuracy either entirely correct or entirely wrong. And then, we optimize the policy model by maximizing the following objective: (5) LGRPO(θ) = Eq,oi [J (θ) β KL[πθπref]] . In reinforcement learning with verifiable rewards for LLMs, the design of the reward function is critical [62, 69]. In addition to formatting reward applied to all task types, we introduce task-dependent reward modelling to ensure that it accurately reflects the proximity between the predicted and ground-truth answers. Specifically, if the answer is mathematical expression containing variables in LaTeX format, we invoke MATHVERIFY [32] to perform exact symbolic equivalence checking, so algebraically identical forms (e.g., 2πr and (2r)π) receive the same reward. For purely numeric answers, to mitigate the risk of reward hacking [68], we forgo the conventional mean relative accuracy (MRA) metric [72] and instead grant the reward if the prediction lies within 1% band around the ground truth: (cid:12) (cid:12) (cid:12) (cid:12) otherwise, 0.01, rans = (6) (cid:12) (cid:12) (cid:12) (cid:12) 0, 1, where is the model prediction and is the ground truth value. This strategy filters out rough answers that have no Methods Proprietary Models GPT-4o [31] Gemini-1.5 Pro [29] Gemini-2.0 Flash [30] Gemini-2.5-Flash-preview-04-17 [21] Gemini-2.5-Pro-preview-05-06 [21] Open-source Models LongVA-7B [80] InternVL2-40B [18] VILA-1.5-40B [43] LLaVA-OneVision-72B [40] LLaVA-Video-72B [81] Spatial Models M2-Reasoning-7B [33] Spatial-MLLM-4B [69] Qwen2.5VL-series Qwen2.5VL-3B [9] Qwen2.5VL-Euclid-3B Qwen2.5VL-7B [9] Qwen2.5VL-Euclid-7B Qwen2.5VL-72B [9] Qwen2.5VL-Euclid-72B RoboBrain2.0-7B-series RoboBrain2.0-7B [8] RoboBrain2.0-Euclid-7B RoboBrain2.0-32B [8] RoboBrain2.0-Euclid-32B Numerical Question Multiple-Choice Question Overall Obj. Cnt. Abs. Dist. Obj. Size Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order 46.2 49.6 52.4 - - 38.0 34.9 22.4 43.5 48.9 41.0 65.3 35.6 38.3 39.5 38.8 13.6 22.5 46.0 66.4 50.5 59.2 5.3 28.8 30.6 - - 16.6 26.9 24.8 23.9 22. 34.0 34.8 23.4 26.8 17.8 22.8 19.6 27.2 32.7 36.9 37.0 39.4 43.8 58.6 66.7 - - 38.9 46.5 48.7 57.6 57.4 60.9 63. 34.9 35.4 16.9 37.3 40.9 55.7 58.9 66.3 59.2 63.4 38.2 49.4 31.8 - - 22.2 31.8 22.7 37.5 35.3 55.4 45.1 16.6 22.2 5.8 23.2 41.1 43.3 35.9 40.5 28.4 47.8 37.0 46.0 56.0 - - 33.1 42.1 40.5 42.5 42.4 40.7 41.3 34.4 37.0 33.8 38.3 37.7 44.9 45.9 48.3 43.2 48.7 41.3 48.1 46.3 - - 43.3 32.2 25.7 39.9 36.7 47.3 46.2 40.7 43.2 36.7 38.5 35.3 37.1 41.5 45.3 46.1 47.5 31.5 42.0 24.5 - - 25.4 34.0 31.5 32.5 35.0 29.9 33.5 26.3 36.6 24.7 25.8 34.0 32.5 30.9 35.6 34.5 33.5 28.5 68.0 55.1 - - 15.7 39.6 32.9 44.6 48. 28.8 46.3 21.8 16.3 22.8 26.5 36.2 36.6 55.2 57.8 39.5 57.0 34.0 48.8 45.4 48.8 47.8 29.2 36.0 31.2 40.2 40.9 42.3 48. 29.2 32.0 24.8 31.4 32.3 37.5 43.0 49.6 43.1 49.6 Table 2. Evaluation Results on VSI-Bench [72]. The performance of Gemini-2.5 is reported from RoboBrain2.0 [8], and the performance of Spatial-MLLM-4B [69] and M2-Reasoning-7B [33] is reported from its original paper, while the results for the other Baseline, Proprietary Models, and Open-source Models are taken from the VSI-Bench benchmark [72]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL [9] and RoboBrain2.0 [8] trained with GRPO [62] on the Euclid30K dataset. practical relevance to the geometric conclusions, while allowing for rounding or floating-point truncation errors that are common in the generation process. In addition, for multiple choice questions, we use the exact match reward. 4. Experiment In this section, we evaluate the zero-shot generalisation of models trained on geometry data to spatial intelligence tasks, using four benchmarks: Super CLEVR, Omni3D Bench, VSI Bench, and MindCube. We also conduct causal ablation study that contrasts models trained on equal amounts of geometry data and spatial data. Additional experiments, along with detailed settings, prompt templates, and dataset settings, are provided in the Appendix C. 4.1. Comparisons on VSI-Bench Tab. 2 shows uniform rise in overall accuracy after Euclid30K fine-tuning: every checkpoint in the Qwen2.5VL and RoboBrain2.0 families increases its VSI-Bench overThe effect all score, with gains of 27% points relative to the corresponding vanilla models. is most pronounced on RoboBrain2.0, where the Euclid-trained 7B and 32B versions reach 49.6% overall, outstripping the best open-source reference (Spatial-MLLM-4B at 48.4%) and also surpassing the strongest proprietary baselines reported by VSI-Bench (Gemini-1.5 Pro and Gemini-2.5-Flash-preview-04-17, both at 48.8%). We note occasional drops in performance on the subtasks like Appearance Order and Route Planning; these tasks emphasize temporal sequencing and planning rather than the static Euclidean priors that Euclid30K provides. Therefore, slight decrease in performance is acceptable and does not offset overall performance on other tasks. 4.2. Comparisons on SuperClevr & Omni3DBench Tab. 3 shows that Euclid30K fine-tuning consistently raises accuracy. Qwen2.5VL-3B climbs from 70.0% to 75.2% on Super-CLEVR (5.2% higher) and from 24.7% to 26.5% on Omni3D-Bench (1.8% higher). The 7B and 72B check6 Methods SuperClevr Omni3DBench Methods Rotation Among Around Overall Qwen2.5VL-series Qwen2.5VL-3B Qwen2.5VL-Euclid-3B Qwen2.5VL-7B Qwen2.5VL-Euclid-7B Qwen2.5VL-72B Qwen2.5VL-Euclid-72B RoboBrain2.0-series RoboBrain2.0-7B RoboBrain2.0-Euclid-7B RoboBrain2.0-32B RoboBrain2.0-Euclid-32B 70.0 75.2 76.1 86.2 72.6 83.1 47.4 85.2 59.5 71.0 24.7 26.5 28.3 31.1 30.4 32.9 14.2 21.2 34.8 36.2 Evaluation Results on SuperClevr [42] and Table 3. Omni3DBench [53]. Qwen2.5VL-Euclid and RoboBrain2.0Euclid indicate the Qwen2.5VL [9] and RoboBrain2.0 [8] trained with GRPO [62] on the Euclid30K dataset. points gain about 10% absolute accuracy on Super-CLEVR and roughly 23% on Omni3D-Bench. Improvements are even larger for RoboBrain2.0, the 7B variant leaps from 47.4% to 85.2% on Super-CLEVR (37.8% higher) and from 14.2% to 21.2% on Omni3D-Bench (7.0% higher), while the 32B version adds 11.5% and 1.4% on the two datasets. Overall, Euclid30K delivers substantial accuracy gains, demonstrating that the geometry-centric curriculum provides broadly transferable spatial priors rather than mere data volume benefits. The unusually large boost observed for RoboBrain2.0 after Euclid30K fine-tuning likely stems from over-specialisation in its original training data: the base model had memorised patterns tailored to its own spatial QA corpus, leaving limited capacity to generalise. Exposure to Euclid30K re-anchors the network in precise Euclidean principles, and forces it to reason deductively across much broader range of geometric configurations. In effect, the geometry curriculum supplies universal spatial priors that counteract earlier overfitting and restore the models ability to extrapolate to unseen spatial tasks. 4.3. Comparisons on MindCube As shown in Tab. 4, fine-tuning on Euclid30K improves the overall MindCube accuracy of every variant in both model families. It is also worth noting that models trained using Euclidean geometry datasets outperform existing spatial models (most of which are also based on Qwen backbones, but trained on larger spatial corpus) in terms of generalization ability. For example, Spatial-MLLM trained the Qwen 2.5-VL-3B backbone on 120K spatial dataset and scored an overall score of 32.1% on MindCube. In contrast, Qwen2.5VL-Euclid-3B, trained with only 30k Euclidean geometric features, scored 38.9% on MindCube, repreProprietary Models GPT-4o [31] Claude-4-Sonnet [4] Spatial Models RoboBrain1.0-7B [34] SpaceMantis [13] Space-Qwen [13] Spatial-MLLM [69] Qwen2.5VL-series Qwen2.5VL-3B [9] Qwen2.5VL-Euclid-3B Qwen2.5VL-7B [9] Qwen2.5VL-Euclid-7B Qwen2.5VL-72B [9] Qwen2.5VL-Euclid-72B RoboBrain2.0-series RoboBrain2.0-7B [8] RoboBrain2.0-Euclid-7B RoboBrain2.0-32B [8] RoboBrain2.0-Euclid-32B 32.7 48.4 35.8 37.7 38.0 38.4 14.3 33.5 35.6 34.2 31.5 43.0 39.4 36.0 21.2 39.0 40.2 44. 38.3 21.3 33.7 20.9 22.8 43.0 25.9 28.7 31.4 35.6 38.8 46.5 35.4 42.4 29.2 47.6 29.5 29.3 26.3 32.8 24.1 40.0 28.4 30.4 30.6 31.6 38.6 36.2 31.0 34.8 38.8 44.8 37.4 22.8 33.3 32.1 20.4 38.9 30.0 31.1 31.2 36.7 38.9 39.4 29.2 38.8 Table 4. Evaluation Results on MindCube [74]. The performance of Proprietary Models and Spatial Models are taken from the MindCube benchmark [74]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL [9] and RoboBrain2.0 [8] trained with GRPO [62] on the Euclid30K dataset. senting relative improvement of 6.8 percentage points. These results suggest that learning accurate Euclidean priors from compact geometry course provides more transferable spatial knowledge than extending generalized spatial data alone. 4.4. Ablation Study To isolate the contribution of our Euclid30K dataset from the potential reasoning enhancements provided by the GRPO algorithm, we conducted causal ablation study. Specifically, we randomly sampled subset equal in size to Euclid30K on the non-geometric spatial intelligence dataset Clevr-CoGenT [35] and used the exact identical GRPO setup to train Qwen2.5VL and RoboBrain2.0. This design ensures that performance gains after training on geometric data can be directly attributed to the fact that the geometric task as surrogate task for spatial intelligence, rather than due to the effects of GRPO or data incrementation. Tab. 5 shows that models trained on Euclid30K achieve markedly higher overall accuracy than those fine-tuned on the equal-sized Clevr-CoGenT split. Because Clevr-CoGenT mainly targets object counting and positional-relation queries, the Clevr-CoGenT variants re7 Methods Qwen2.5VL-series Qwen2.5VL-3B Qwen2.5VL-ClevrCoGenT-3B Qwen2.5VL-Euclid-3B Qwen2.5VL-7B Qwen2.5VL-ClevrCoGenT-7B Qwen2.5VL-Euclid-7B Qwen2.5VL-72B Qwen2.5VL-ClevrCoGenT-72B Qwen2.5VL-Euclid-72B RoboBrain2.0-7B-series RoboBrain2.0-7B RoboBrain2.0-ClevrCoGenT-7B RoboBrain2.0-Euclid-7B RoboBrain2.0-32B RoboBrain2.0-ClevrCoGenT-32B RoboBrain2.0-Euclid-32B Numerical Question Multiple-Choice Question Overall Obj. Cnt. Abs. Dist. Obj. Size Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order 35.6 40.5 38. 39.5 42.4 38.8 13.6 15.6 22.5 46.0 66.4 66.4 50.5 58.0 59.2 23.4 24.5 26.8 17.8 17.8 22. 19.6 24.8 27.2 32.7 34.4 36.9 37.0 36.9 39.4 34.9 30.1 35.4 16.9 24.5 37.3 40.9 40.7 55. 58.9 65.8 66.3 59.2 62.2 63.4 16.6 29.8 22.2 5.8 9.2 23.2 41.1 41.4 43.3 35.9 41.0 40. 28.4 47.8 47.8 34.4 33.9 37.0 33.8 36.7 38.3 37.7 43.4 44.9 45.9 46.6 48.3 43.2 46.9 48. 40.7 43.0 43.2 36.7 38.5 38.5 35.3 37.8 37.1 41.5 46.5 45.3 46.1 44.5 47.5 26.3 29.9 36. 24.7 29.4 25.8 34.0 29.4 32.5 30.9 36.6 35.6 34.5 34.0 33.5 21.8 18.8 16.3 22.8 23.8 26. 36.2 33.5 36.6 55.2 52.3 57.8 39.5 42.1 57.0 29.2 31.3 32.0 24.8 27.8 31.4 32.3 33.2 37. 43.0 48.7 49.6 43.1 46.7 49.6 Table 5. Ablation experiment on VSI-Bench [72]. We compare training model on 25K subset of the spatial intelligence dataset Clevr-CoGenT v.s. the geometric dataset Euclid30K to verify that the geometric dataset serves as surrogate task to improve the spatial intelligence capabilities of the model. Bolding indicates the best score within each model type. tain slight edge in object counting and relative-direction tasks. In contrast, the Euclid30K variant has better generalization performance, improving in almost all task categories, resulting in oneto two-percentage-point improvement in the overall metric. This result is strong evidence that the performance improvement stems primarily from the unique, principled knowledge embedded in Euclidean geometry problems, rather than just the generic reasoning gains from the RL training process. In addition, all Qwen2.5VL variants show slight decline on the Appearance Order task after training on either Clevr-CoGenT or Euclid30K, which we attribute to the fact that Appearance Order primarily probes temporal reasoning and memory skills, and not directly reinforced by either corpus. By contrast, the RoboBrain2.0 variants were exposed to similar temporal tasks during their original pre-training; once Euclid30K fine-tuning alleviates their overfitting to the original data distribution, their temporal reasoning rebounds, and RoboBrain2.0-Euclid-32B, in particular, registers marked improvement in Appearance Order. 5. Limitations Although Euclidean geometry tasks can serve as surrogate task for spatial intelligence and supply useful Euclidean priors that boost spatial reasoning in many situations, these priors do not always translate into higher scores on every task. For example, the Appearance Order task in VSI-Bench depends on temporal memory, whereas static geometry data does not encode temporal information, so accuracy on this task can remain flat or decline. Rotation tasks in MindCube may show limited benefit because plane-geometry problems rarely require reasoning about object rotation, which offsets the expected gains. More detailed analysis about the limitations can be found in Appendix E. 6. Conclusion This study shows that using Euclidean geometry as surrogate task provides an alternative way to achieve transferable Spatial Perception and Reasoning. MLLMs are trained on planar and solid geometry problems with basic Euclidean priors that can be transferred to various spatial benchmarks without additional fine-tuning. After training on our proposed Euclid30K alone, our model achieves consistent and significant performance gains on four different, unseen benchmarks. This increase in generalization validates our core hypothesis that learning basic Euclidean geometric principles is more effective strategy for developing transferable spatial skills. More importantly, by identifying the boundaries of this transferability, our work provides direction for future research: namely, to provide the ability to combine geometry lessons with data (e.g., video) containing rich temporal information to achieve more comprehensive and balanced spatial intelligence."
        },
        {
            "title": "Future Work",
            "content": "We are closely monitoring the updates of Qwen3VL [65]. After the release of Qwen3-VL-30B-A3B-Instruct and Qwen3-VL-4B-Instruct, we will further update our results."
        },
        {
            "title": "References",
            "content": "[1] David Acuna, Guojun Zhang, Marc T. Law, and Sanja Fidler. f-domain adversarial learning: Theory and algorithms. In Proceedings of the 38th International Conference on Machine Learning, pages 6675. PMLR, 2021. 13 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1, 3 [3] Anthropic. Claude 3.5 sonnet. Blog, 2024. Accessed: November 22, 2024. 1 [4] Anthropic. Claude 4 sonnet system card, 2025. Version 20250514, accessed 2025-06-23. 1, [5] Laszlo Aszalos and Maria Bako. How can we improve the spatial intelligence. In 6th International Conference on Applied Informatics, Eger, Hungary, 2004. 13 [6] Jafar Aziz, Dwi Juniati, and Pradnyo Wijayanti. Students reasoning with logical mathematical and visual spatial inIn International telligence in geometry problem solving. Joint Conference on Science and Engineering (IJCSE 2020), pages 203207. Atlantis Press, 2020. 13 [7] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2 [8] BAAI-RoboBrain-Team. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 1, 3, 6, 7, 16, 19, 20, 21, 22 [9] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. 1, 6, 7, 16, 17, 18 [10] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 14 [11] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. theory of learning from different domains. Machine Learning, 79(1-2):151175, 2010. 3, 13 [12] Jeffrey Buckley, Niall Seery, and Donal Canty. Investigating the use of spatial reasoning strategies in geometric problem solving. International Journal of Technology and Design Education, 29(2):341362, 2019. 13 [13] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Spatialvlm: Endowing vision-language modFei Xia. arXiv preprint els with spatial reasoning capabilities. arXiv:2401.12168, 2024. 1, 3, 7 [14] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. 3 [15] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3313 3323, 2022. [16] Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 3 [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1 [18] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2025. 6 [19] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Egothink: Evaluating first-person perspective thinking capability of visionIn Proceedings of the IEEE/CVF Conlanguage models. ference on Computer Vision and Pattern Recognition, pages 1429114302, 2024. 2 [20] Douglas Clements and Michael Battista. Geometry and spatial reasoning. Handbook of research on mathematics teaching and learning: project of the National Council of Teachers of Mathematics, pages 420464, 1992. 2 [21] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 15 [22] Yasser Dahou, Ngoc Dung Huynh, Phuc Le-Khac, Wamiq Reyaz Para, Ankit Singh, and Sanath Narayan. arXiv Vision-language models cant see the obvious. preprint arXiv:2507.04741, 2025. 1 [23] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas A. Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. 2017 IEEE Conference on Computer Vision and Pattern Recognition, pages 24322443, 2017. 14 [24] DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 4 [25] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training In Proceedend-to-end vision-and-language transformers. ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1816618176, 2022. 3 [26] Jacob Feldman. What is visual object? Trends in Cognitive Sciences, 7(6):252256, 2003. 4, 13 [27] Howard Gardner. Frames of mind: The theory of multiple intelligences. Basic books, 2011. 1, 3, 15 [28] GemmaTeam, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 1 Gemma 3 technical report. [29] Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 6 [30] Google. https : Gemini 2.0 flash: Model card. / / storage . googleapis . com / model - cards / documents/gemini2flash.pdf, 2025. Model card published April 15, 2025. 6 [31] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 4, 6, 7 [32] Kydlıˇcek Hynek and Gandenberger Greg. Math-verify. https : / / github . com / huggingface / Math - Verify, 2025. 4, 5 [33] Inclusion-AI. M2-reasoning: Empowering mllms with unified general and spatial reasoning, 2025. 6, 16 [34] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. 1, 3, 7 Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 29012910, 2017. 3, 7 [35] Justin Johnson, [36] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Visionand-language transformer without convolution or region su10 pervision. In International conference on machine learning, pages 55835594. PMLR, 2021. 3 [37] KimiTeam, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025. 1 [38] K. M. Kyaw and T. Vidakovich. The relationship between spatial reasoning and geometric reasoning in teachers. Eurasia Journal of Mathematics, Science and Technology Education, 21(8):em2684, 2025. 13 [39] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. 4, 13 [40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [42] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose doIn Proceedings of main robustness in visual reasoning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1496314973, 2023. 1, 2, 3, 7, 14, 15, 21, 22 [43] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2667926689, 2023. 6 [44] Dingkun Liu, Zhu Chen, Jingwei Luo, Shijie Lian, and Dongrui Wu. Mirepnet: pipeline and foundation model for eeg-based motor imagery classification. arXiv preprint arXiv:2507.20254, 2025. 13 [45] Dingkun Liu, Siyang Li, Ziwei Wang, Wei Li, and Dongrui Wu. Spatial distillation based distribution alignment (sdda) for cross-headset eeg classification. arXiv preprint arXiv:2503.05349, 2025. 13 [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1 [47] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding, 2024. [48] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symIn The Joint Conference of the 59th Anbolic reasoning. nual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), 2021. 2, 3, 4 [49] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations, 2024. 1, 3 [50] Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1724917260, 2025. 3 [51] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated In International Conferquestion answering in 3d scenes. ence on Learning Representations, 2023. 2 [52] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009. 3, 13 [53] Damiano Marsili, Rohun Agrawal, Yisong Yue, and Georgia Gkioxari. Visual agentic ai for spatial reasoning with dynamic api, 2025. 1, 2, 3, 7, 14, 15, 17, 18 [54] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 2, [55] Hans Moravec. Mind children: The future of robot and human intelligence. Harvard University Press, 1988. 1 [56] Nora Newcombe and Andrea Frick. Early education for spatial intelligence: Why, what, and how. Mind, Brain, and Education, 4(3):102111, 2010. 13 [57] Marios Pittalis and Constantinos Christou. Types of reasoning in 3d geometry thinking and their relation with spatial ability. Educational Studies in mathematics, 75(2):191212, 2010. 2 [58] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 3 [59] Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, Jie Wang, Chong Sun, Chen Li, and Honggang Zhang. We-math 2.0: versatile mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433, 2025. 3, 4 [60] Nova Riastuti, Mardiyana, and Ikrar Pramudya. Analysis of students geometry skills viewed from spatial intelligence. In AIP Conference Proceedings, page 020024. AIP Publishing LLC, 2017. [61] Riastuti, Mardiyana, and Pramudya. Students errors in geometry viewed from spatial intelligence. In Journal of Physics: Conference Series, page 012029. IOP Publishing, 2017. 13 [62] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 5, 6, 7, 16 [63] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 13 [64] Nese Dokumacı Sutc and Behcet Oral. The effects of geometrical-mechanical intelligence games on the spatial International Online Journal of Primary Educaabilities. tion, 9(2):171196, 2020. 13 [65] Qwen Team. Easyr1: An efficient, scalable, multi-modality https : / / github . com / training framework. rl QwenLM/Qwen3-VL, 2025. 9 [66] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 15 [67] Peijie Wang, Chao Yang, Zhong-Zhi Li, Fei Yin, Dekang Ran, Mi Tian, Zhilong Ji, Jinfeng Bai, and Cheng-Lin Liu. Solidgeo: Measuring multimodal spatial math reasoning in solid geometry, 2025. 2, 3, 4, 5 [68] Lilian Weng. Reward hacking in reinforcement learning. \"https://lilianweng.github.io/posts/ 2024-11-28-reward-hacking/\", 2024. [69] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. 1, 3, 5, 6, 7, 15, 16 [70] Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Shizhe Diao, Renjie Pi, and Tong Zhang. Generalizable geometric image caption synthesis. arXiv preprint arXiv:2509.15217, 2025. 3 [71] Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, et al. Geosense: Evaluating identification and application of geometric principles in multimodal reasoning. arXiv preprint arXiv:2504.12597, 2025. 3 11 [72] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 2, 3, 5, 6, 8, 14, 15, 16, 19, 20 [73] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1222, 2023. [74] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, and Li Fei-Fei. Spatial mental modeling from limited views, 2025. 2, 3, 7, 14, 15, 16 [75] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 5 [76] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [77] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of The Association for Computational Linguistics, 2025. 3 [78] Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Ivan Vulic, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei, et al. call for new recipes to enhance spatial reasoning in mllms. arXiv e-prints, pages arXiv2504, 2025. 1 [79] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. [80] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. ArXiv, abs/2406.16852, 2024. 6 [81] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. ArXiv, abs/2410.02713, 2024. 6 [82] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https:// github.com/hiyouga/EasyR1, 2025. 13 12 Euclids Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Proof of Standard Domain-Adaptation"
        },
        {
            "title": "Bound",
            "content": "This section provides the complete proof for the standard domain-adaptation bound introduced in Eq. (3). We follow the same notation as in the main text. Let be reference (ideal) hypothesis induced by near-optimal policy. There is clearly that: (cid:12)ϵS(h, h) ϵT (h, h)(cid:12) (cid:12) (cid:12) 1 2 dHH(DS, DT ). (7) By the triangle inequality and Eq. (7), we have: ϵT (h) ϵT (h) + ϵT (h, h) = ϵT (h) + ϵS(h, h) + (cid:0)ϵT (h, h) ϵS(h, h)(cid:1) (cid:12)ϵT (h, h) ϵS(h, h)(cid:12) ϵT (h) + ϵS(h, h) + (cid:12) (cid:12) ϵT (h) + [ϵS(h) + ϵS(h)] + 1 2 dHH(DS, DT ) = ϵS(h) + ϵideal + 1 2 dHH(DS, DT ), (8) where ϵideal = ϵS(h) + ϵT (h) can be considered as an extremely tiny constant (e.g., the error rate of an AGI model on these two datasets). The above proof refers to [11] and [52]. Therefore, if dHH(DS, DT ) is sufficiently small, the population gap ϵT (h) ˆϵS(h) also becomes small, which allows the source distribution to serve as reliable surrogate for the target distribution [1, 44, 45]. B. Evidence from Educational Psychology Complementing the domain-adaptation view in the previous subsection, we now present evidence from educational psychology that echoes the cognitive-science perspective on the generality of geometric knowledge in perception and reasoning [26, 39]. There is extensive evidence in educational psychology that geometry problem solving is closely related to spatial intelligence, can serve as an informative indicator of spatial ability, and can be used to improve it through targeted practice. First, numerous correlational studies document substantive link between geometric and spatial reasoning. Kyaw and Vidakovich report moderate positive correlation between teachers geometric and spatial reasoning (r = 0.47), with 3D matching and measurement tasks In STEM and graphical predicting spatial scores [38]. education, higher spatial ability is associated with better problem-solving performance and more effective strategies 13 [12]. Newcombe and Frick emphasize that spatial representations and transformations are central cognitive resources that support reasoning in domains that are not obviously spatialfor example, through the use of graphs and diagrams [56]. Second, several studies show that performance on geometry tasks is sensitive proxy for spatial ability. Analyses of middle-school students reveal that geometry skills and error patterns systematically vary with spatial-intelligence levels [60, 61]. Differences in dominance between logicalmathematical and visualspatial intelligence yield distinct pathways for geometric reasoning, further tying geometry problem solving to spatial constructs [6]. These results support the use of geometry assessments as indicators of students spatial proficiency. Third, intervention studies demonstrate that providing structured geometric activities can improve spatial intelligence. Programmatic practice with polyhedra and computer-generated spatial problems yields measurable gains [5]. Geometricalmechanical intelligence games, implemented in quasi-experiments with pre/post testing, significantly enhance spatial visualisation and spatial relations skills [64]. Overall, the balance of evidence indicates that well-designed geometric practice is an effective means to cultivate spatial abilities. that Taken together, these findings motivate our surrogatetask choice. Our results suggest the same relationship generalises beyond human learners to large multraining on formal geometry induces timodal models: domain-invariant structure that transfers to diverse spatialintelligence benchmarks. This observation is consistent with the domain-adaptation analysis in the previous subsection and provides an educational-psychology rationale for our geometry-first curriculum. C. Detailed Experimental Setup This section summarises the key hyperparameters, evaluation settings, prompt templates, and datasets settings used throughout the paper. C.1. Training setup In this paper, we follow the default settings of VeRL [63] and EasyR1 [82] to train the Qwen2.5-VL series and the RoboBrain2.0 series. Specifically, we train for 10 epochs in 64 NVIDIA H100 GPUs using Adam optimizer with learning rate of 1 106 and weight decay of 1 102. In GRPO, we perform 8 rollouts per question and set the default sampling temperature to 1. The KL divergence coefficient β in Eq. 5 is set to 1 102. Unless stated otherwise, we fix the random seed at 1 to guarantee determinism. We adopt context window of 2048 tokens for both the prompt and the response, and use rollout batch of 512 samples. The actor network updates with global batch size of 128 and maximum gradient norm of 1.0. Images are resized so that the total pixel count lies between 512 512 and 2048 2048 . All remaining hyper-parameters, including PPO clip ratio, learningrate schedule, and parallelism settings, follow the default EasyR1 recipe and can be found in the supplied supplementary material. C.2. Test setup Inference is conducted with the lmms-eval toolkit [79] to ensure consistent decoding across models. In the test, to ensure the reproducibility of the results, we follow VSIBench [72] and MindCube [74] to set the temperature to 0. Finally, to ensure that the model performs sufficient spatial inference, we set the maximum generation length of model responses at 1024 tokens. C.3. Prompt templates Euclid-tuned models During both training and evaluation, we use the following template: Euclid-tuned Models Prompt Template You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. Baseline variants RoboBrain2.0 expects the answer inside <answer> </answer> tags; we therefore replace the last line with, like: Vanilla RoboBrain2.0 Prompt Template You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in <answer> </answer>. Because Qwen2.5VL-Instruct was tuned with supervised instruction data that often begins with phrases like think step by step, keeping the same cue in your evaluation prompt aligns the test-time input with the style encountered during training. This consistency helps the model interpret the prompt as intended and reduces the risk of unexpected formatting effects. Vanilla Qwen2.5VL-Instruct Prompt Template You FIRST think step by step and then provide the final answer. The final answer MUST BE put in boxed{}. When processing VSI-Bench tasks, we make minor adjustments to the above templates to follow the benchmarks original settings as closely as possible and ensure consistent resultsfor example, we prepend These are frames of video. to every prompt. In addition, all exact prompt templates are provided in the lmms eval/task folder of the supplemental code. C.4. Dataset Setup in In this subsection, we provide an introduction and configuration details for the dataset used in the main page. Setup in VSI-Bench. VSI-Bench [72] contains more than 5,130 egocentric videos question-answer pairs sourced from ARKitScenes[10], ScanNet [23], and ScanNet++ [73]. The task types are divided into numerical question tasks (e.g. object counting, absolute distance estimation, object size estimation, and room size estimation) and multiple choice tasks (e.g. relative distance estimation, relative direction reasoning, route planning, and spatiotemporal appearance-order). For the evaluation metrics, we In addition, for the align with the VSIBench setting. Qwen2.5VL-series and RoboBrain2.0-series, we use 32 frames uniformly sampled from the scene video as input frames in the inference process. Super-CLEVR and Omni3D-Bench. Setup Super-CLEVR [42] contains 5,000-image test split that probes how well model handles changes in visual complexity, concept distribution, and composition, making it strong measure of two-dimensional spatial reasoning. Omni3D-Bench [53] adds 500 questions to the Omni3D dataset, each requiring model to locate objects in three-dimensional space and estimate their relative distances and sizes. Together, these benchmarks test both planar and volumetric aspects of spatial understanding, providing complementary evidence of models geometric competence. For the evaluation metrics, we follow the settings of VSIBench [72]. Specifically, we calculate mean relative accuracy (MRA) across confidence thresholds = {0.5, 0.55 . . . , 0.95} for the numerical question tasks and report exact-match accuracy for multiple-choice tasks. Setup in MindCube. MindCube [74] is recent benchmark crafted to scrutinize the spatial-reasoning capabilities of VLMs under partial observability and dynamic viewpoints, challenging the VLM to maintain object consistency 14 Figure 4. Performance improvement on SuperClevr [42], Omni3DBench [53], VSIBench [72], and MindCube [74] after the model has been trained on Eculid25K. across viewpoints and to reason about occluded or invisible elements. MindCube defines three canonical camera trajectories: Rotation (camera stays in place but rotates to look around; 1,081 samples), Around (camera moves around objects in circular path; 1,869 samples), and Among (camera moves among objects in circular path; 18,204 samples). Since all questions follow multiple-choice format, we evaluate models by exact-match accuracy between the predicted option and the ground-truth answer. D. More visualization and analysis of results To present the quantitative gains more intuitively, Fig. 4 plots the base models and their Euclid30K-tuned counterparts side by side. The light bars show consistent accuracy improvements on Super-CLEVR [42], Omni3D-Bench [53], VSI-Bench [72], and MindCube [74], confirming that compact geometry curriculum injects transferable spatial priors across both Qwen2.5VL and RoboBrain2.0 families. Fig. 5 and Fig. 6 compare our Euclid30K models with the strongest proprietary, open-source, and purpose-built spatial systems. On VSI-Bench, RoboBrain2.0-Euclid-7B and RoboBrain2.0-Euclid-32B achieve the highest overall scores, surpassing Gemini-2.5 [21] and the Spatial-MLLM [69]. We attribute this result to two factors: (1) the RoboBrain2.0 backbone has already learned rich spatial and temporal patterns from large-scale video and spatial datasets; (2) Euclid30K fine-tuning re-introduces general Euclidean constraints, which enhance the models ability to generalise without erasing its original strengths. similar trend appears on MindCube: Qwen2.5-VLEuclid-3B variant outperforms Spatial-MLLM, even though Spatial-MLLM augments Qwen 2.5-VL-3B with dedicated spatial encoder (VGGT) [66] and is trained on much larger 120 corpus. These observations highlight the efficiency and broad generalisation afforded by training on focused set of formal geometry problems. E. Further Analysis of Limitations E.1. Temporal skills beyond basic spatial abilities Educational psychology views spatial intelligence as one branch of hierarchical model of human abilities. According to Gardners theory of multiple intelligences [27] decomposes spatial intelligence into five Basic Spatial Abilities (BSAs)spatial perception, spatial relations, spatial orientation, mental rotation, and spatial visualisationthat serve as building-blocks for higher reasoning. The Euclid30K curriculum targets precisely these BSAs, which 15 Figure 5. Visualization Evaluation Results on VSI-Bench [72]. The performance of Gemini-2.5 is reported from RoboBrain2.0 [8], and the performance of Spatial-MLLM-4B [69] and M2-Reasoning-7B [33] is reported from its original paper, while the results for the other Baseline, Proprietary Models, and Open-source Models are taken from the VSI-Bench benchmark [72]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL [9] and RoboBrain2.0 [8] trained with GRPO [62] on the Euclid30K dataset. Figure 6. Visualization Evaluation Results on MindCube [74]. The performance of Proprietary Models and Spatial Models are taken from the MindCube benchmark [74]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL [9] and RoboBrain2.0 [8] trained with GRPO [62] on the Euclid30K dataset. explains the broad transfer we observe on Super-CLEVR, Omni3D-Bench, and VSI-Bench. However, tasks such as Appearance Order in VSI-Bench require form of spatio-temporal memory that goes beyond 16 Figure 7. The response and final answer for Qwen2.5VL-7B [9] and Qwen2,5VL-7B-Eculid in Omni3DBech [53]. the static BSAs. Euclid30K supplies no temporal cues, so models fine-tuned only on Euclidean geometry show limited or negative gains in this category. This boundary clarifies the scope of our surrogate task: mastering Euclidean principles is an effective route to transferable spatial skills, but additional temporal supervision is needed to reach full spatio-temporal intelligence. In addition, on the Rotation tasks in MindCube, the model sometimes shows comparable or slightly decreased performance. This may be because the mental rotation ability required by this task is primarily stimulated by solidgeometry problems, whereas Euclid30K is dominated by plane-geometry questions that rarely involve imagining object rotations. As result, transfer in this category is less effective. This observation suggests that future work should incorporate more solid-geometry data to achieve better balance across categories and further strengthen performance on rotation-related reasoning tasks. F. Discussion of Future Work Three avenues appear promising. Temporal augmentation. Combining Euclid30K with video-centered dataset may fill gap in the timing capabilities of models trained with Euclid30K. Higher-level spatial knowledge. Mining CAD blueprints, architectural plans, and even non-Euclidean geometries might expose the model to more abstract spatial rules. Layered RLVR schedules. The gains on RoboBrain 2.0 suggest that second RL-with-verifiable-reward (RLVR) stage on Euclid30K can refine model that was already trained on generic spatial data. Exploring multi-stage RLVR schedules across diverse spatial domains is fruitful next step. By expanding both the temporal and conceptual breadth of our surrogate tasks, future work can push multimodal models toward more comprehensive and balanced notion of spatial intelligence. Figure 8. The response and final answer for Qwen2.5VL-7B [9] and Qwen2,5VL-7B-Eculid in Omni3DBech [53]. 18 Figure 9. The response and final answer for RoboBrain2,0-7B [8] and RoboBrain2.0-7B-Eculid in VSIBench [72]. 19 Figure 10. The response and final answer for RoboBrain2,0-7B [8] and RoboBrain2.0-7B-Eculid in VSIBench [72]. Figure 11. The response and final answer for RoboBrain2,0-7B [8] and RoboBrain2.0-7B-Eculid in SuperClver [42]. 21 Figure 12. The response and final answer for RoboBrain2,0-7B [8] and RoboBrain2.0-7B-Eculid in SuperClver [42]."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Huazhong University of Science and Technology",
        "Zhengzhou University",
        "Zhongguancun Academy",
        "Zhongguancun Institute of Artificial Intelligence"
    ]
}