{
    "paper_title": "Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models",
    "authors": [
        "Yongding Tao",
        "Tian Wang",
        "Yihong Dong",
        "Huanyu Liu",
        "Kechi Zhang",
        "Xiaolong Hu",
        "Ge Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data contamination poses a significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, a critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL post-training becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents a critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by a key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, a benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that Self-Critique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to a random guess for RL-phase contamination, our method makes detection possible."
        },
        {
            "title": "Start",
            "content": "Preprint. DETECTING DATA CONTAMINATION FROM REINFORCEMENT LEARNING POST-TRAINING FOR LARGE LANGUAGE MODELS Yongding Tao1 Tian Wang1 Yihong Dong1 Huanyu Liu1 Kechi Zhang1 Xiaolong Hu2 Ge Li1 1 School of Computer Science, Peking University ydtao25@stu.pku.edu.cn lige@pku.edu.cn 2 New H3C Technologies Co., Ltd 5 2 0 O 0 1 ] . [ 1 9 5 2 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Data contamination poses significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL posttraining becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. SelfCritique probes for the underlying policy collapse, i.e., the models convergence to narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that SelfCritique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to random guess for RL-phase contamination, our method makes detection possible. Our benchmark and code are available at https://github.com/yongding-tao/RL-Data-Contamination."
        },
        {
            "title": "INTRODUCTION",
            "content": "The reliability of Large Language Model (LLM) evaluations is seriously threatened by data contamination. This happens when benchmark test samples accidentally get included in the training data, which can invalidate the models reported performance. To solve this problem, many researchers have developed detection methods, but they have almost exclusively focused on the pre-training and Supervised Fine-Tuning (SFT) (Dong et al., 2024; Fu et al., 2024; Shi et al., 2024; Zhang et al., 2024b; Xie et al., 2024b; Zhang et al., 2025a;b) stages. However, these efforts have left major gap: the increasingly important phase of Reinforcement Learning (RL) post-training. We believe this is critical oversight, because powerful techniques like Reinforcement Learning with Verifiable Rewards (RLVR) (Shao et al., 2024a; Guo et al., 2025; Yu et al., 2025) are now essential for improving LLM reasoning. This makes the RL stage major potential source of contamination that has been largely overlooked. The challenge of detecting RL-phase contamination stems from fundamental shift in the training objective, rendering existing methods ineffective. Both pre-training and SFT are likelihood-based paradigms; they train models to maximize the probability of observed data. This process naturally creates strong, likelihood-based signals, such as unusually low perplexity, which most current detectors are built to identify. By contrast, RL, especially RLVR, operates on reward-maximization principle. The policy is not trained to mimic ground-truth distribution but is instead guided by 1 Preprint. (a) Contaminated sample (b) Clean sample (c) AUC comparison Figure 1: Motivation behind Self-Critique. After RL post-training, entropy distributions become sparse. (a) For contaminated samples, the critique reasoning path remains highly similar to the original one, indicating policy collapse and memorization. (b) Clean samples exhibit greater divergence between the original and critique reasoning paths. (c) Our method achieves significantly higher AUC while existing baselines perform close to random guess. sparse reward signals to find successful reasoning path. This approach often enables stronger generalization than SFT (Kirk et al., 2024; Chu et al., 2025), but by decoupling from likelihood-based objectives, it also erases the very signals that traditional detectors rely on. Consequently, RL-phase contamination becomes uniquely challenging problem, creating an urgent need for new class of detection methods specifically designed for this reward-driven setting. Given that likelihood-based signals are ineffective, our search for new detection method begins with identifying signal inherent to the reward-driven training process. Recent studies on RLs training dynamics point to promising candidate: the phenomenon of policy collapse. Specifically, RL narrows the search space to improve pass@1 accuracy, often at the cost of lower pass@k performance (Havrilla et al., 2024; Shao et al., 2024b; Yue et al., 2025; Dong et al., 2025), and produces distinctive entropy patterns, such as high-entropy concentration on certain tokens (Wang et al., 2025a; Cheng et al., 2025; Song et al., 2025). These findings suggest that entropy could serve as powerful indicator of this collapse and its associated path dependency. However, our initial investigations revealed that using entropy directly as contamination signal is unreliable. The reason is that policy collapse is general behavior of RL and can occur even on clean samples not seen during training. As shown in Figures 1a and 1b, both contaminated and clean samples can exhibit sparse token-level entropy. This implies that simple passive check is insufficient. Therefore, we introduce an active probing mechanism to expose the underlying differences. We find that when the model is asked to generate an alternative reasoning path given its initial response (self-critique), contaminated samples struggle to deviate, resulting in highly similar entropy curves (Figure 1a). In contrast, the model shows greater flexibility on clean samples, leading to more distinct entropy patterns (Figure 1b).1 Building on these observations, we introduce Self-Critique, an entropy-based detection method that applies our self-critique probing strategy. The core idea is to instruct the model to generate two distinct responses for the same problem; samples where the two responses exhibit high similarity in their entropy space are flagged as contaminated. detailed workflow is shown in Figure 2. However, rigorously evaluating this method is challenging, as no existing benchmark can isolate and simulate contamination purely within the RL phase. To overcome this hurdle, we also developed RL-MIA (Reinforcement Learning Membership Inference Attack), new benchmark constructed for this specific purpose. Using RL-MIA across challenging math and logic datasets, we show that Self-Critique is highly effective. As previewed in Figure 1c, our method significantly outperforms existing detectors, which operate near the level of random guess. Our main contributions are summarized as follows: ❶ To the best of our knowledge, we present the first systematic study of data contamination detection in the RL post-training phase of LLMs, highlighting critical yet overlooked problem. ❷ We propose Self-Critique, an entropy-based detector that measures RL-induced policy collapse via self-critique probing. Across four tasks and multiple models, Self-Critique 1We also provide visualizations of the contamination score distribution in Appendix D. 2 Preprint. consistently outperforms baselines, which perform near random guess, achieving an AUC improvement of up to 30%. ❸ We introduce RL-MIA, new benchmark that simulates RL-specific contamination scenarios across math and logic tasks, enabling the rigorous evaluation of detection methods."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "In this section, we outline the two most relevant directions and associated papers of this work. Data Contamination Detection Data contamination detection can be regarded as specific instance of membership inference attacks (MIA), which were initially introduced to measure memorization and privacy risks (Shokri et al., 2017; Carlini et al., 2019; Mireshghallah et al., 2022a). Recently, the issue of data contamination in LLMs has drawn increasing attention, as it directly undermines the validity of benchmark evaluations (Sainz et al., 2023; Xu et al., 2024a;b; Wu et al., 2025). Prior work on data contamination detection in LLMs has mainly focused on the pre-training and Supervised Fine-Tuning stages (Mireshghallah et al., 2022b; Fu et al., 2023; Mattern et al., 2023; Shi et al., 2024; Xie et al., 2024b; Gonen et al., 2023; Dong et al., 2024; Zhang et al., 2025b). In these stages, models largely rely on memorizing training data for learning (Zeng et al., 2024; Chu et al., 2025; Wang et al., 2025b), process that naturally creates strong, likelihood-based signalssuch as unusually low perplexitythat most current detectors are built to identify. In contrast, during the RL post-training phase, LLMs are optimized to autonomously explore reasoning trajectories. This reward-driven objective decouples the models behavior from simple likelihood metrics, posing unique challenge for conventional detection methods. Entropy in Reinforcement Learning Post-training Reinforcement learning has become crucial paradigm for post-training large language models. Leveraging reinforcement learning with verifiable rewards substantially enhances LLMs reasoning capabilities (Jaech et al., 2024; Guo et al., 2025). key factor in RL post-training is entropy: high entropy promotes exploration via stochastic policies, while low entropy favors exploitation through deterministic behavior. common challenge in RL post-training is entropy collapse (Cui et al., 2025; Liang et al., 2025), where policy entropy decreases dramatically in the early stages of training, leading to premature convergence and restricted exploration. To address this, entropy management strategies regularize entropy to prevent rapid collapse (ODonoghue et al., 2016; He et al., 2025; Wang et al., 2025c) or use high-entropy signals to encourage inherently exploratory reasoning behaviors (Cheng et al., 2025; Vanlioglu, 2025; Tan & Pan, 2025), thus maintaining balance between exploration and exploitation. In reasoning tasks, high-entropy tokens indicate uncertain decision points and are assigned stronger RL updates, while low-entropy tokens, which correspond to more deterministic outputs, receive smaller updates (Li et al., 2025; Wang et al., 2025a; Tang et al., 2025). As result, the trained model develops distinct entropy patterns across tokens. In this work, we analyze these entropy patterns before and after self-critique to detect potential data contamination."
        },
        {
            "title": "3 THE CHALLENGE OF CONTAMINATION DETECTION IN RL",
            "content": "In this section, we formalize the problem of detecting data contamination in the RL post-training phase of LLMs, and then highlight why detection methods based on likelihood, which are effective in pre-training and SFT, become unreliable in RL. Finally, we introduce token-level entropy as lens to analyze RL-induced policy collapse, which lays the foundation for our proposed method. 3.1 PROBLEM DEFINITION We consider the task of detecting data contamination in the RL post-training phase of Large Language Models. Formally, this can be framed as Membership Inference Attack(MIA) problem: given model that has undergone RL post-training and sample x, the goal is to determine whether was included in the RL training dataset DRL. detector is function (M, x) {0, 1}, where 1 indicates membership (contamination) and 0 indicates non-membership. We focus on the black-box setting (Shi et al., 2024), where the detector can only query for outputs, without access to internal states, gradients, or training data. 3 Preprint. 3.2 WHY RL POST-TRAINING IS UNIQUE CASE Existing data detection methods are largely designed for training paradigms whose objectives are rooted in Maximum Likelihood Estimation (MLE). However, the objective of RL post-training is fundamentally different, which introduces unique and significant challenges for detection. Both pre-training and Supervised Fine-Tuning are governed by an MLE-based objective. Their goal is to train model with parameters θ to maximize the likelihood of the observed data by minimizing the negative log-likelihood loss. For pre-training, the model learns from vast corpus of unlabeled text Dpretrain. The objective is next-token prediction, aiming to learn general distribution of the language. For text sequence = (x1, x2, . . . , xT ), the loss is: LPretrain(θ) = (cid:88) (cid:88) xDpretrain t=1 log pθ(xtx<t) (1) For SFT, the model learns from dataset of prompt-response pairs DSFT = {(q, r)}. The objective is to learn to follow instructions and generate helpful responses. The model is trained to maximize the likelihood of the target response = (r1, . . . , rK) given the prompt q: LSFT(θ) = (cid:88) (cid:88) (q,r)DSFT t=1 log pθ(rtq, r<t) (2) Despite their different data sources, both paradigms (Eq. 1 and 2) share the same underlying principle: they directly train the model to assign high probabilities to sequences seen in the training data. This provides clear signal for detection methods like Perplexity (Gonen et al., 2023) and Min-K% Prob (Shi et al., 2024), which are built upon this likelihood principle. In stark contrast, RL post-training (and specifically RLVR) does not directly optimize for likelihood. Its objective is to update the policy πθ to maximize the expected reward from set of generated outputs {oi} given prompt q. The objective for method like GRPO (Shao et al., 2024a) can be abstracted as: JRL(θ) = EqDRL,{oi}πθold (3) where () is function of the reward, the current policy, and reference policy. The key distinction is that the optimization is driven by an external, often sparse, reward signal R(oi) (e.g., 1 for correct final answer, 0 otherwise), not by the token-level log-probabilities of the ground-truth response. This decouples the models final behavior from simple likelihood metrics, rendering many existing detection approaches that rely on this signal ineffective. [f (R(oi), πθ)] , 3.3 ENTROPY AS NEW SIGNAL FOR RL DETECTION Recent studies (Yue et al., 2025; Wang et al., 2025a; Cui et al., 2025) have shown that RL posttraining frequently leads to policy collapse: for samples that receive consistent reward, the model converges to narrow reasoning path, producing overly stable outputs. This phenomenon is reflected in the token-level entropy. For each decoding step t, the token-level entropy is (cid:88) Ht = pθ(v x<t) log pθ(v x<t), (4) and the entropy sequence = {Ht}T vV t=1 measures uncertainty along the generated trajectory. Empirical observations show that RL tends to push entropy sequences into sparse patterns, where many tokens are nearly deterministic. Crucially, this collapse is stronger for contaminated samples that were explicitly rewarded during RL training, whereas clean samples retain more variability when probed. These insights suggest that contamination detection in RL requires moving beyond likelihood and instead measuring the policys dependence on specific reasoning paths. Token-level entropy provides natural signal for this purpose, which directly motivates our Self-Critique method: by asking the model to regenerate an alternative reasoning path conditioned on its initial response and comparing the entropy sequences, we can reveal whether sample was memorized during RL training. 4 Preprint."
        },
        {
            "title": "4 DETECTION VIA SELF-CRITIQUE",
            "content": "Figure 2: Overview of the Self-Critique detection workflow. The method compares token-level entropy sequences between the initial response and the self-critique response. High similarity in entropy space indicates contamination (policy collapse), while low similarity indicates clean samples. Our method is motivated by the hypothesis that RL post-training induces high-reward path dependence for contaminated samples. Concretely, for problem seen during RL training, the policy πθ tends to converge to highly rewarded and thus similar response trajectory. In contrast, for problems not seen during RL training, the model is more likely to produce an alternative reasoning path when prompted. The Self-Critique method quantifies this dependency. We first elicit the models most confident (deterministic) response, and then ask the model to produce different solution conditioned on the initial response. We compare the token-level entropy sequences of the two generations to measure the degree of path dependence. An overview is shown in Figure 2. 4.1 THE SELF-CRITIQUE DETECTION PROCESS Let be large language model with parameters θ, and let be the problem under test. We use deterministic decoding strategy (e.g., greedy decoding) to obtain the models most confident response as the reference. Step 1: Initial response. We construct the initial prompt P1 by embedding into chat template , and obtain the models response: r1 = M(T (q)). (5) We then compute the token-level entropy sequence for this response, E1 = {Ht(r1)}r1 serves as the baseline reasoning trajectory. t=1, which Step 2: Self-critique response. We form self-critique prompt P2 by augmenting with an instructional meta-prompt Icritique 2 and the text of r1: = Icritique(r1), (6) where denotes appending to the user content within the prompt structure. We then generate the second response and its entropy sequence: r2 = M(T (q)), E2 = {Ht(r2)}r2 t=1. (7) Step 3: Similarity score. The contamination score is the similarity between the two entropy sequences. higher similarity indicates that the model remains on the same reasoning path despite being instructed to change it, suggesting memorization. We use length-aware (penalized) cosine similarity: where ScoreSelf-Critique(q) = cospenalized(E1, E2), cospenalized(A, B) = cos(cid:0)pad(A), pad(B)(cid:1) min(A, B) max(A, B) . (8) (9) 2The exact Icritique is shown in Appendix E. 5 Preprint. Here cos(, ) is the standard cosine similarity between vectors (dot product over the product of L2 norms), and pad() zero-pads the shorter sequence to the maximum length so that non-overlapping positions contribute zero. The multiplicative length ratio penalizes cases where one response is much shorter/longer than the other, since response length itself reflects facet of the reasoning mode. Overall, higher score indicates higher likelihood of contamination. formal description of the procedure is provided in Appendix A."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we present comprehensive empirical evaluation of our proposed Self-Critique. We first introduce the RL-MIA benchmark, which we constructed specifically for RL data contamination detection. We then describe the baseline methods we compare against and detail our experimental setup. Finally, we present the main results, followed by analysis and ablation studies. Additional experimental results are in Appendix and C. 5.1 RL-MIA: BENCHMARK FOR RL MEMBERSHIP INFERENCE ATTACK To the best of our knowledge, no benchmark currently exists for systematically detecting data contamination during the RL post-training stage. To address this gap, we introduce the RL-MIA (Reinforcement Learning Membership Inference Attack). The key idea behind RL-MIA is to simulate controllable data contamination by selectively including subset of data in the RL post-training process, while the objective of the contamination detection task is to identify which samples have been used. The problems used for this simulation are drawn from four benchmarks selected to cover diverse styles and potential pre-training exposure. We include the widely used mathematical reasoning benchmarks, i.e., AIME 2024 and AIME 2025. AIME 2024 may have appeared in some models pre-training corpora, allowing us to test robustness to prior exposure, whereas AIME 2025 is postcutoff and thus unlikely to be present in pre-training data. To obtain controlled setting free from prior exposure, we also include two synthetically generated logical reasoning datasets: Knights & Knaves (K&K) (Xie et al., 2024a) and SAT (Liu et al., 2025). The synthetic nature of these datasets ensures that any detected training signal can be attributed to the RL post-training phase. To approximate realistic setting, we embed the selected benchmarks into larger RL post-training corpus. For AIME24 and AIME25, we use the widely adopted OpenR1-Math-46K (Guha et al., 2025) corpus as the base and inject 50% of each benchmarks items into the RL training data. For K&K and SAT, following their original papers , we use the provided training portions to form the contaminated split and synthesize additional items as held-out clean samples. We primarily use Qwen2.5-7B-Instruct (Qwen et al., 2025) and DeepSeek-Math-7B-Instruct (Shao et al., 2024b) as the model for simulating RL-stage contamination. We also run experiments on Qwen2.5-0.5BInstruct, Qwen2.5-3B-Instruct (Qwen et al., 2025) and Qwen2.5-7B-Math (Yang et al., 2024). All RL runs are implemented with VeRL (Sheng et al., 2025) framework on 8 NVIDIA A100 (40 GB). Detailed dataset splits and training settings are provided in Appendix F. 5.2 EXPERIMENTAL SETUP Baseline Methods We compare Self-Critique against set of representative baselines. key consideration in the RL post-training setting is that training changes the response distribution rather than the likelihood of the prompt. Therefore, for baselines originally designed for pre-training data detection (which operate on input text), we adapt them to operate on the models responses to ensure fair comparison. The baselines include: ❶ Perplexity (PPL) (Gonen et al., 2023), which assumes memorized text has lower perplexity; ❷ Min-K% Prob (Shi et al., 2024), which posits that memorized text is less likely to contain low-probability outlier tokens; and ❸ Min-K%++ (Zhang et al., 2025b), which normalizes token probabilities for more robust score. We also include ❹ Recall (Xie et al., 2024b), which prefixes the text with non-member content and measures the relative change in log-likelihood, and ❺ CDD (Dong et al., 2024), which measures output consistency under stochastic sampling via the average token-level edit distance across multiple generations. 6 Preprint. We summarize these baselines in Table 1: PPL, Min-K%, and Min-K%++ directly use logprobability properties of the text; Recall and CDD expose differences by injecting non-member prefix or by randomly sampling multiple outputs, respectively. In contrast, we propose new probing mechanism, i.e., self-critique probing, and use entropy as the core signal for RL-stage contamination detection. Following the probing ideas in CDD and Recall, we also introduce two additional entropy-based baselines, ❻ Entropy-Temp and ❼ Entropy-Noise, which keep the probing mechanisms but replace the consistency/likelihood metric with entropy. Table 1: taxonomy of data contamination detection methods. Our work is the first to specifically address the challenges in the RL Post-training phase. Method Probing Mechanism Core Metric Designed for Existing Methods for Pre-training / SFT PPL (EMNLP23) Min-K% (ICLR24) Min-K%++ (ICLR25) Recall (EMNLP24) CDD (ACL24) Intrinsic Property Intrinsic Property Intrinsic Property Non-member prefix Stochastic Sampling Log Probability Log Probability Log Probability Log Probability Edit Distance Pre-training / SFT Pre-training / SFT Pre-training / SFT Pre-training / SFT Pre-training / SFT Our Proposed Methods for RL Post-training Entropy-Temp Entropy-Noise Self-Critique(Ours) Stochastic Sampling Non-member prefix Self-Critique Probing Entropy Entropy Entropy RL RL RL Evaluation Metrics We primarily report the Area Under the ROC Curve (AUC), standard metric for detection problems such as data contamination and membership inference (Shi et al., 2024; Duan et al., 2024; Zhang et al., 2025b). AUC is threshold-independent and reflects the probability that the detector ranks randomly chosen contaminated sample higher than randomly chosen clean one; higher AUC indicates stronger detection performance (50% corresponds to random guess). We also report the F1 score at the Youden threshold (Fluss et al., 2005) as threshold-specific reference. 5.3 MAIN RESULTS Table 2: Performance of different detection methods on the RL-MIA benchmark across two models. The AVG column is the average AUC across all benchmarks. Best AUC is in bold; the second best is underlined. Method AIME AIME25 K&K SAT F1 score AUC F1 score AUC F1 score AUC F1 score AUC Qwen2.5-7B-Instruct PPL (Gonen et al., 2023) Min-K% (Shi et al., 2024) Min-K%++ (Zhang et al., 2025b) Recall (Xie et al., 2024b) CDD (Dong et al., 2024) Entropy-Temp Entropy-Noise Self-Critique(Ours) DeepSeek-Math-7B-Instruct PPL (Gonen et al., 2023) Min-K% (Shi et al., 2024) Min-K%++ (Zhang et al., 2025b) Recall (Xie et al., 2024b) CDD (Dong et al., 2024) Entropy-Temp Entropy-Noise Self-Critique(Ours) 0.33 0.59 0.73 0.62 0.50 0.73 0.70 0. 0.42 0.55 0.67 0.54 0.30 0.60 0.70 0.76 0.51 0.49 0.58 0.61 0.57 0.64 0.57 0.72 0.53 0.47 0.53 0.46 0.49 0.48 0.56 0. 0.42 0.52 0.46 0.55 0.67 0.12 0.67 0.76 0.62 0.12 0.67 0.52 0.65 0.70 0.72 0.71 0.67 0.70 0.67 0.67 0.67 0.59 0.68 0. 0.34 0.67 0.15 0.39 0.08 0.23 0.48 0.60 0.47 0.54 0.40 0.47 0.47 0.49 0.52 0.66 0.54 0.46 0.47 0.54 0.48 0.43 0.52 0. 0.54 0.32 0.00 0.62 0.57 0.66 0.79 0.69 0.68 0.69 0.62 0.69 0.66 0.64 0.67 0.66 0.50 0.50 0.31 0.62 0.47 0.69 0.77 0. 0.64 0.35 0.49 0.62 0.50 0.61 0.45 0.67 0.56 0.44 0.45 0.65 0.52 0.42 0.63 0.72 0.41 0.40 0.56 0.56 0.51 0.54 0.69 0. 7 AVG 0.51 0.49 0.44 0.59 0.51 0.56 0.62 0.70 ( 19%) 0.53 0.42 0.51 0.54 0.50 0.52 0.55 0.64 ( 19%) Preprint. The main results of different data contamination methods based on Qwen2.5-7B-Instruct and DeepSeek-Math-7B-Instruct3 are shown in Table 2. Across both models, Self-Critique is the most reliable detector: it attains the best average AUC on Qwen2.5-7B-Instruct (0.70, +19% over the best non-ours baseline) and on DeepSeek-Math-7B-Instruct (0.64, also +19% improvement), and it leads on most per-dataset AUCs. In contrast, likelihood-based baselines (PPL, Min-K%, MinK%++) often perform near random guesses and can be unstable. Among existing methods, Recall achieves relatively strong performance (it is also state-of-the-art approach for pre-training contamination (Xie et al., 2024b)), suggesting that probing to expose membership signals is more effective than relying solely on intrinsic text properties. Moreover, when using the same probing mechanisms, the entropy-based variants perform better (Entropy-Temp vs. CDD; Entropy-Noise vs. Recall), indicating that entropy is sensitive indicator of RL-induced changes and thus better suited for RL-stage detection. Finally, compared to random sampling or prefix injection, our self-critique probing aligns more closely with the RL property of dependence on high-reward paths. Within our entropy-based methods, this leads to notable gains, i.e., +13% on Qwen2.5-7B-Instruct and +16% on DeepSeek-Math7B-Instruct. 5.4 DUAL-STAGE CONTAMINATION IN PRE-TRAINING & RL To isolate the effects of RL-phase contamination, our previous experiments were primarily conducted on synthetic data or datasets with low levels of contamination. However, for public benchmarks released before an LLMs training cutoff, contamination from both pretraining and the RL phase can co-occur. Therefore, we design study to distinguish between these two sources of contamination. Concretely, we choose GSM8K (widely acknowledged to suffer from substantial pretraining leakage (Zhang et al., 2024a; Dekoninck et al., 2024; Mirzadeh et al., 2025)) and train Qwen2.5-0.5B-Instruct with the PPO algorithm.4 We then simulate RL-phase contamination by injecting half of the test set into the RL training data. As discussed in Section 3.2, pretraining (which optimizes via Maximum Likelihood Estimation) and RL (which uses reward-driven policy optimization) pursue different objectives, so their respective forms of contamination are likely to produce distinct effects. score using proxy We first assign each test item pretrainingcontamination likelihood-based detector (e.g., PPL) to separate these effects. We then evaluate RL-stage contamination detection under two conditions: ❶ lower-pretrainingcontamination subset, created by selecting the bottom-q quantile of items by PPL score (e.g., the lowest 50%); and ❷ random-control subset. To control for any confounding effects from smaller sample size on AUC, the random-control subset is formed by uniformly sampling the same number of items, thereby matching the subset size while preserving the original data distribution. Our hypothesis predicts that Self-Critiques performance will significantly improve on the former subset, but not on the latter. Figure 3: Dual-stage contamination analysis. SelfCritique on the lower-pretraining-contamination subset (green) improves sharply as the rate decreases. The results are shown in Figure 3, and we also provide the numerical results in Table 6 of Appendix B. As the pretraining contamination level decreases, the performance of Self-Critique on the lowerpretraining-contamination subset improves significantly. In contrast, its performance on the randomcontrol subset shows slight decrease, while the PPL-based detectors performance approaches that of random guess. This outcome rules out pure sample-size effect (as performance on the random3We also provide additional results on Qwen2.5-7B-Math in Appendix B. 4Actually, the setting is the same as the quick start in verl, which makes it easy to reproduce the results. 8 Preprint. control subset does not improve) and supports our hypothesis: conditioning the RL detector on items with weaker pretraining signal allows it to identify RL-phase memorization or path dependence far more clearly. Meanwhile, the fact that likelihood-based cues (PPL) remain unstable and close to 0.5 further demonstrates the effectiveness of our method in specifically targeting RL-phase data contamination. 5.5 ABLATION STUDIES Table 3: Detection performance on K&K with Qwen2.5-3B-Instruct trained by different RL algorithms. AVG column reports the mean AUC across three RL algorithms. Method PPO GRPO DAPO F1 score AUC F1 score AUC F1 score AUC PPL (Gonen et al., 2023) Min-K% (Shi et al., 2024) Min-K%++ (Zhang et al., 2025b) Recall (Xie et al., 2024b) CDD (Dong et al., 2024) Entropy-Temp Entropy-Noise Self-Critique(Ours) 0.65 0.43 0.39 0.61 0.66 0.41 0.64 0.67 0.41 0.50 0.50 0.46 0.53 0.55 0.59 0.61 0.63 0.58 0.61 0.35 0.65 0.53 0.43 0. 0.46 0.54 0.52 0.49 0.51 0.60 0.52 0.61 0.61 0.64 0.26 0.55 0.60 0.57 0.60 0.64 0.51 0.46 0.49 0.50 0.49 0.59 0.49 0. AVG 0.46 0.50 0.51 0.48 0.51 0.58 0.53 0.60 ( 18%) Contaminate with Different RL Algorithms Table 3 reports results on the K&K task using Qwen2.5-3B-Instruct trained with three RL algorithms, including PPO (Havrilla et al., 2024), GRPO (Shao et al., 2024a), and DAPO (Yu et al., 2025). Across all algorithms, Self-Critique is the most reliable detector: it attains the best AUC for each algorithm and the highest average AUC (0.60). Entropy-based probes are consistently stronger than likelihood-based methods: EntropyTemp is the next best on average (0.58), while likelihood baselines (PPL, Min-K%, Min-K%++) are around 0.460.51. These trends suggest that our probe, measuring path dependence via entropy similarity, captures an RL-induced rigidity signal that is algorithm-agnostic. F1 scores follow the same pattern, further indicating that Self-Critique yields both better ranking (AUC) and better thresholded decisions. Ablation on Top-K Entropy Approximation As LLM vocabularies are large, it is often impractical to obtain the full next-token distribution at every decoding step to compute exact entropy. In many APIs, only Top-K token probabilities are available, so we approximate entropy using those Top-K masses. We conduct an ablation on the choice of  (Table 4)  . Reducing does not harm performance; even in the extreme case = 3, the AUC drops only slightly. We attribute this to the long-tailed nature of next-token distributions: most probability mass concentrates on small set of tokens, and the tail contributes little to entropy. Hence, Top-K entropy is both efficient and accurate enough for our detector. Table 4: Ablation on Top-K entropy approximation (Qwen2.5-7B-Instruct). We report AUC for different and the row-wise variance across {3, 5, 10, 20, 50}. Dataset = 3 = 5 = 10 = 20 = 50 AIME25 K&K 0.7022 0.6460 0.7111 0.6572 0.7111 0. 0.7156 0.6608 0.7156 0.6584 Variance 2.39 105 3.62 105 We also provide additional ablation studies about why self-critique probing is better and the sampling strategy in Appendix C."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we presented the first systematic study of data contamination in the overlooked RL post-training stage, demonstrating that existing data contamination detection methods are ill-suited 9 Preprint. for this reward-driven paradigm. To address this gap, we proposed Self-Critique, novel method that identifies RL-induced policy collapse by actively probing the models reasoning path dependencies using token-level entropy. To validate our method in controlled setting, we also developed RL-MIA, new benchmark for RL-phase contamination. Experiments show that Self-Critique consistently outperforms baselines, improving the average AUC by up to 30%, and its ability to isolate RL-specific signals is further highlighted in dual-contamination scenarios, where performance improves by up to 55%. As the communitys understanding of RL post-training grows, we expect that more detectors tailored to this unique setting will emerge."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "All authors have read and adhered to the ICLR Code of Ethics. The primary objective of our work is to enhance the integrity and reliability of Large Language Model evaluations. We address this by developing Self-Critique, method to detect data contamination in the Reinforcement Learning (RL) post-training phase, aiming to prevent misleading performance claims and promote transparent research practices. Our method is specific application of Membership Inference Attack (MIA) techniques, and we acknowledge the potential dual-use concerns regarding data privacy. However, our study is carefully scoped to mitigate these risks. The goal of our work is defensive, i.e., providing validation tool for researchers, and it is applied exclusively to public, non-sensitive benchmarks (AIME, K&K, SAT, GSM8K) that contain no personal data. We believe the societal benefit of ensuring robust and honest model evaluation significantly outweighs the minimal risk of misuse in this context."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the full reproducibility of our research. To facilitate this, we have made our code and the newly constructed RL-MIA benchmark available at https://github. com/yongding-tao/RL-Data-Contamination. This repository includes the implementation of our proposed Self-Critique method, baseline methods, and scripts to run the experiments. formal, step-by-step description of the Self-Critique algorithm is provided in Appendix (Algorithm 1). The specific prompts used for the self-critique probing mechanism are detailed in Appendix E. Details regarding the construction of the RL-MIA benchmark, including dataset sources, injection methods, and data splits, are described in Section 5.1 and further elaborated in Appendix  (Table 7)  . The key training hyperparameters for all RL models and algorithms are provided in Appendix  (Table 8)  , ensuring that our training process can be accurately replicated. Our evaluation metrics are standard in the field and are defined in Section 5.1. REFERENCES Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium, pp. 267284, 2019. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Jasper Dekoninck, Mark Muller, and Martin Vechev. Constat: Performance-based contamination detection in large language models. Advances in Neural Information Processing Systems, 37: 9242092464, 2024. 10 Preprint. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1203912050, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025. Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024. Ronen Fluss, David Faraggi, and Benjamin Reiser. Estimation of the youden index and its associated cutoff point. Biometrical Journal: Journal of Mathematical Methods in Biosciences, 47(4):458 472, 2005. Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. arXiv preprint arXiv:2311.06062, 2023. Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Membership inference attacks against fine-tuned large language models via self-prompt calibration. Advances in Neural Information Processing Systems, 37:134981135010, 2024. Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 10136 10148, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.679. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Alexander Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane DwivediYu, et al. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. In B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, M. Khan, and Y. Sun (eds.), International Conference on Representation Learning, volume 2024, pp. 2062020653, 2024. 11 Preprint. Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, et al. Cure: Critical-token-guided re-concatenation for entropycollapse prevention. arXiv preprint arXiv:2508.11016, 2025. Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, and Weizhu Chen. Beyond pass@ 1: Self-play with variational problem synthesis sustains rlvr. arXiv preprint arXiv:2508.14029, 2025. Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, and Ge Li. Saturn: Sat-based reinforcement learning to unleash language model reasoning. arXiv preprint arXiv:2505.16368, 2025. Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Scholkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462, 2023. Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference atIn Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. tacks. 83328347, 2022a. Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, and Taylor BergKirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 18161826, 2022b. Iman Mirzadeh, Keivan Alizadeh-Vahid, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. In Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu (eds.), International Conference on Representation Learning, volume 2025, pp. 9474394765, 2025. Brendan ODonoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Oscar Sainz, Jon Campos, Iker Garcıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1077610787, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024b. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, M. Khan, and Y. Sun (eds.), International Conference on Representation Learning, volume 2024, pp. 5182651843, 2024. 12 Preprint. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy (SP), pp. 318, 2017. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning. arXiv preprint arXiv:2509.06941, 2025. Hongze Tan and Jianfei Pan. Gtpo and grpo-s: Token and sequence-level reward shaping with policy entropy. arXiv preprint arXiv:2508.04349, 2025. Xinyu Tang, Zhenduo Zhang, Yurou Liu, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, and Jun Zhou. Towards high data efficiency in reinforcement learning with verifiable reward. arXiv preprint arXiv:2509.01321, 2025. Abdullah Vanlioglu. Entropy-guided sequence weighting for efficient exploration in rl-based llm fine-tuning. arXiv preprint arXiv:2503.22456, 2025. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, and William Yang Wang. Generalization vs memorization: Tracing language models capabilities back to pretraining data. In International Conference on Learning Representations (ICLR), 2025b. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025c. Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Huijie Lv, Ming Zhang, et al. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532, 2025. Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning. arXiv preprint arXiv:2410.23123, 2024a. Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, and Bhuwan Dhingra. Recall: Membership inference via relative conditional log-likelihoods. arXiv preprint arXiv:2406.15968, 2024b. Cheng Xu, Shuhao Guan, Derek Greene, Kechadi, et al. Benchmark data contamination of large language models: survey. arXiv preprint arXiv:2406.04244, 2024a. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2501.12948, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 13 Preprint. Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, and Dawei Yin. Exploring memorization in fine-tuned language models. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 39173948, 2024. Hengxiang Zhang, Songxin Zhang, Bingyi Jing, and Hongxin Wei. Fine-tuning can help detect pretraining data from large language models. In Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu (eds.), International Conference on Learning Representations, volume 2025, pp. 6090260921, 2025a. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, et al. careful examination of large language model performance on grade school arithmetic. Advances in Neural Information Processing Systems, 37: 4681946836, 2024a. Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, and Hai Li. Min-k%++: Improved baseline for pre-training data detection from large language In Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu (eds.), International Conference on models. Representation Learning, volume 2025, pp. 6484564862, 2025b. Weichao Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. Pretraining data detection for large language models: divergence-based calibration method. arXiv preprint arXiv:2409.14781, 2024b. 14 Preprint. ALGORITHM OF SELF-CRITIQUE Algorithm 1 Self-Critique Contamination Detection Require: Model (black-box with per-token log-prob access or top-k probs), meta-prompt Icritique, problem Ensure: Contamination score Score(q) Deterministic decoding. Use greedy decoding (temperature = 0) in all generations. 1: (Initial response) Construct P1 (q) and generate r1 M(P1). 2: Compute token-level entropy sequence E1 {Ht(r1)}r1 3: (Self-critique response) Form Icritique(r1); construct P2 (q) and generate r2 t=1 using per-token probabilities. M(P2). 4: Compute entropy sequence E2 {Ht(r2)}r2 t=1. 5: (Similarity) Pad the shorter sequence with zeros: E1 pad(E1), E2 pad(E2). 6: Compute cosine similarity cos( E1, E2). 7: Compute length penalty λ min(E1,E2) max(E1,E2) . 8: return Score(q) λ."
        },
        {
            "title": "B ADDITIONAL RESULTS",
            "content": "B.1 ADDITIONAL RESULTS ON QWEN2.5-7B-MATH As shown in Table 5, on Qwen2.5-7B-Math, Self-Critique achieves the best AUC on both AIME24 (0.76) and AIME25 (0.72), and the highest average AUC (0.74). The two entropy baselines are competitive, while likelihood-based methods (PPL, Min-K%, Min-K%++) and Recall/CDD are clearly weaker here. We do not report K&K and SAT for this model because we could not get stable RL training on these synthetic datasets with non-instruct base; such setups typically require more careful RL hyperparameters and data curation. This engineering detail is outside the scope of our study. Table 5: Detection results on Qwen2.5-7B-Math under RL-MIA (higher is better). AVG reports mean AUC across AIME24 and AIME25. Method AIME24 AIME F1 score AUC F1 score AUC PPL(Gonen et al., 2023) Min-K%(Shi et al., 2024) Min-K%++(Zhang et al., 2025b) Recall(Xie et al., 2024b) CDD(Dong et al., 2024) Entropy-Temp Entropy-Noise Self-Critique (Ours) 0.71 0.72 0.69 0.45 0.67 0.72 0.70 0.81 0.59 0.41 0.48 0.55 0. 0.50 0.64 0.76 0.60 0.63 0.58 0.32 0.67 0.70 0.67 0.71 0.55 0.45 0.46 0.49 0.43 0.63 0.51 0.72 AVG 0.57 0.43 0.47 0.52 0.47 0.56 0.57 0.74 ( 30%) B.2 NUMERICAL RESULTS FOR DUAL-STAGE CONTAMINATION For completeness, we provide the numerical data corresponding to the dual-stage contamination analysis presented in Figure 3 of the main paper. The results, detailed in Table 6, quantify the trend shown in the figure: as the pretraining contamination signal is reduced (i.e., when evaluating on subsets with lower PPL scores), the AUC of Self-Critique on these filtered subsets increases sharply from 0.59 to 0.88, confirming its effectiveness at isolating RL-specific signals. 15 Preprint. Table 6: Numerical AUC results for the dual-stage contamination analysis. The header row indicates the quantile of pretraining contamination retained. Method 1.0 0.9 0.8 0. 0.3 0.1 PPL Self-Critique (random subset) Self-Critique (lower pretraining contamination) 0.52 0.59 0.59 0.48 0.58 0.62 0.47 0.60 0. 0.56 0.57 0.65 0.54 0.49 0.74 0.56 0.54 0."
        },
        {
            "title": "C ADITIONAL ABLATIONS",
            "content": "C.1 WHY SELF-CRITIQUE PROBING IS BETTER DETECTOR (a) Qwen2.5-7B-Instruct (b) DeepSeek-Math-7B-Instruct Figure 4: Self-critique probing vs. no self-critique. As our self-critique idea asks the model to propose an alternative reasoning path given its initial response, naive variant is to skip the initial response and simply tell the model to answer using an unusual technique5. The results in Figure 4 show that without the initial response as an anchor, performance collapses to near random guess . This variant fails because it removes the anchor that makes the probe meaningful: without conditioning on the initial answer, the alternative path is unconstrained, instruction following is noisy, and both members and non-members produce heterogeneous trajectories whose entropy sequences are not comparable. Moreover, RL can induce mode-seeking even on unseen items, so we end up measuring two arbitrary paths rather than the deviation from memorized one. Self-critique fixes concrete baseline and probes deviation from ithence the large AUC gains over the no-response variant. C.2 ABLATION ON SAMPLING STRATEGY We perform an ablation study of the sampling strategies used to generate model responses. Specifically, we experiment with three settings: (1) using greedy sampling for both the initial response and the second self-critique response; (2) using greedy sampling for the initial response and temperature sampling for the self-critique response; and (3) using temperature sampling for both the initial response and the self-critique response. For temperature sampling, we test multiple temperatures. The ablation results, shown in Figure 5, indicate that the best performance is achieved when both the initial and self-critique responses are generated by greedy sampling. This is because greedy sampling eliminates the effect of randomness, thereby better revealing the sharp policy distributions caused by entropy collapse from RL post-training."
        },
        {
            "title": "D VISUALIZATION OF CONTAMINATION SCORE DISTRIBUTION",
            "content": "For the contaminated and uncontaminated samples in the AIME and AIME25 dataset, we computed their Self-Critique similarity scores, and present the histograms in Figure 6. Through Kernel Den5Detail prompt is shown in Appendix Prompt 1 16 Preprint. Figure 5: Ablation on Sampling Strategy sity Estimation (KDE), we observe clear difference in the distribution of Self-Critique similarity scores between contaminated and uncontaminated samples, demonstrating the effectiveness of our proposed Self-Critique method for data contamination detection. (a) AIME (b) AIME25 Figure 6: Histograms and Kernel Density Estimation (KDE) of Self-Critique similarity scores between contaminated and uncontaminated samples."
        },
        {
            "title": "E INSTRUCTIONS",
            "content": "We present here the prompts used in the data contamination detection process. We employ Prompt 1 to encourage the model to generate responses that deviate from conventional reasoning. In particular, for the Self-Critique process, we use Prompt 2 to guide the model to regenerate second response that differs as much as possible from the initial one. 17 Preprint. Prompt 1: Unconventional Reasoning Answer using technique youd typically avoid or deliberately unconventional line of reasoning. Prompt 2: Self-Critique Instruction possible answer is provided below (it may or may not be correct). Please provide response that follows different reasoning path or provides an alternative solution: {Initial Response} Please now provide your new, different response: BENCHMARK DETAIL & TRAINING SETTINGS This section provides detailed breakdown of the RL-MIA benchmark construction and the specific training configurations used in our experiments. To create controlled environment for evaluating data contamination in the RL phase, we constructed the RL-MIA benchmark. The methodology involves injecting known subset of test samples (contaminated items) into larger base corpus for RL post-training. The detection task is then to distinguish these injected samples from clean, unseen samples. Table 7 summarizes the data splits for each source dataset, detailing the size of the base RL corpus, the number of injected items, the total size of the final training set, the number of times each contaminated item appears (Occurrences), and the total number of items in the final detection task (Contaminated + Clean). For reproducibility, we also provide the key hyperparameters used during RL post-training. Table 8 lists the shared training parameters for our two primary experimental models: Qwen2.5-7B-Instruct and Deepseek-math-7b-Instruct. Table 7: RL-MIA data splits for training and evaluation. The last column reports the total number of evaluation items (Contam + Clean). Source Base RL Corpus (size) Injected items Train Size Occurrences Detection Tasks AIME24 AIME25 OpenR1-Math-46K 15 15 46K + 30 K&K SAT K&K train: 950 SAT train: 450 K&K test: 50 SAT test: 50 950 + 50 450 + GSM8K GSM8K train: 7473 GSM8K test: 659 7473 + 659 2 3 30 30 100 100 1319 Table 8: Shared training hyperparameters Parameter Qwen2.5-7B-Instruct Deepseek-math-7b-Instruct Actor learning rate Batch size (train / val) Max prompt length Max generation length Temperature (train / val) Samples per prompt (n) Tensor model parallel (TP) micro / mini-batch Max tokens per GPU Use KL loss Entropy coefficient 1.0 106 128 / 512 1024 4096 1.0 / 0.6 8 2 2 / 2 16384 No 0.001 1.0 106 128 / 512 1024 3072 1.0 / 0.6 8 2 2 / 2 16384 No 0.001 Due to the 4,096 context length of DeepSeek-Math-7B-Instruct, we set its maximum generation length to 3,072 (with 1,024-token prompt). For other models (Qwen2.5-7B-Math, Qwen2.518 Preprint. 3B-Instruct, and Qwen2.5-0.5B-Instruct), the settings are essentially the same as for Qwen2.5-7BInstruct. Full details are available in the training scripts included with our released code."
        },
        {
            "title": "G LIMITATIONS AND FUTURE WORK",
            "content": "While our work presents robust framework for detecting contamination in the RL post-training phase, we acknowledge several avenues for future research that build upon our findings. Generalization Across Diverse Domains. Our experiments primarily focused on mathematical and logical reasoning tasks, as these are prominent domains where RL has demonstrated significant benefits. However, the unique characteristics of RL-induced contamination may vary across different problem domains. For instance, in areas such as code generation, where wider diversity of valid solutions is common, the signature of policy collapse might manifest differently. Future work could extend the evaluation of Self-Critique and other reward-aware detection methods to these and other domains, thereby assessing the broader applicability and potential domain-specific adaptations of our method. Scalability to Larger Models. The models used in our study, ranging from 0.5B to 7B parameters, are representative of widely used class of open-source LLMs. However, the landscape of foundation models is rapidly evolving, with state-of-the-art proprietary and open-source models now exceeding hundreds of billions of parameters. While we have no reason to believe our methods core principles would not apply, the dynamics of policy collapse and memorization at such scales are not yet fully understood. Investigating the effectiveness of Self-Critique on these frontier models represents an important next step in ensuring the reliability of the entire LLM ecosystem."
        },
        {
            "title": "H LLM USAGE STATEMENT",
            "content": "In preparing this manuscript, we use LLMs to aid and polish the writing. Specifically, LLMs improve clarity, grammar, and phrasing, ensuring the text is concise and readable. The use of LLMs does not influence the technical contributions or the interpretation of experimental findings. All content polished by LLMs is carefully checked by the authors."
        }
    ],
    "affiliations": [
        "New H3C Technologies Co., Ltd",
        "School of Computer Science, Peking University"
    ]
}