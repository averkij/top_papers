{
    "paper_title": "RecoWorld: Building Simulated Environments for Agentic Recommender Systems",
    "authors": [
        "Fei Liu",
        "Xinyu Lin",
        "Hanchao Yu",
        "Mingyuan Wu",
        "Jianyu Wang",
        "Qiang Zhang",
        "Zhuokai Zhao",
        "Yinglong Xia",
        "Yao Zhang",
        "Weiwei Li",
        "Mingze Gao",
        "Qifan Wang",
        "Lizhu Zhang",
        "Benyu Zhang",
        "Xiangjun Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present RecoWorld, a blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents a proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with a dual-view architecture: a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating a dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where \"user instructs, recommender responds,\" jointly optimizing user retention and engagement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 7 9 3 0 1 . 9 0 5 2 : r RecoWorld: Building Simulated Environments for Agentic Recommender Systems Fei Liu, Xinyu Lin, Hanchao Yu, Mingyuan Wu, Jianyu Wang, Qiang Zhang, Zhuokai Zhao, Yinglong Xia, Yao Zhang, Weiwei Li, Mingze Gao, Qifan Wang, Lizhu Zhang, Benyu Zhang, Xiangjun Fan Meta Modern Recommendation System (MRS) We present RecoWorld, blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with dual-view architecture: simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where user instructs, recommender responds, jointly optimizing user retention and engagement. Date: September 15, 2025 Correspondence: Fei Liu at feiliu1@meta.com, Xiangjun Fan at maxfan@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Recommender systems have long relied on offline metrics such as Recall@N, NDCG, and counterfactual policy evaluation, paired with online A/B tests (Zhai et al., 2024; Deng et al., 2025b). Offline evaluation is based on historical user behavior, which can introduce exposure bias. This means the system often reinforces known patterns instead of discovering new user interests. In contrast, online A/B tests provide valuable insights, yet they involve slow feedback loop and must be handled carefully with real users. As agentic recommender systems emerge, there is an increasing demand for simulated online environments that use LLMs to replicate user feedback. These simulations allow recommender systems to aggressively test bold, radically different strategies without compromising user experience. Simulated environments have accelerated the progress of reinforcement learning (RL) research. For instance, OpenAI Gym (Brockman et al., 2016) provides collection of benchmarks for comparing the performance of RL algorithms. More recently, Park et al. (2023) pioneered the use of LLMs to simulate human behavior by developing small-town simulation of generative agents. AgentCompany (Xu et al., 2025a) investigated selfcontained environment that mimics software company. AgentSociety (Piao et al., 2025) is simulator with over 10k agents designed to study societal impacts, such as universal basic income. CRMArena-Pro (Huang et al., 2025b) utilizes simulated LLM agents to evaluate diverse business scenarios. These studies have shown promising initial results in using LLMs to simulate human behavior, paving the way for creating simulated environments to evaluate agentic recommender systems. An ideal simulator would replicate real-world user interactions in controlled environment (Deffayet et al., 2024; Corecco et al., 2024). It allows developers to refine recommendation algorithms before deploying them. The simulator may include variety of user profiles drawn from different demographics to reflect the diversity 1 Case 1 Case Case 3 Case"
        },
        {
            "title": "Example Use Cases of RecoWorld",
            "content": "Evaluating RecSys Instruction Following Ability. Instruction-following capabilities are essential for agentic RecSys, much like they are for LLMs. E.g., user might tell the recommender system their preferences through natural language or voice, e.g., There are too many recommendations about hairstyling; wanna see something different but related, or Id like to watch more content that people in the San Francisco Bay Area are watching right now. If the agentic RecSys adjusts its recommendations based on these instructions, we need simulated environment to evaluate how well it performed. One approach is to measure the systems success in instruction-following by using simulated user feedback as proxy. This approach is important because agentic recommenders will need to handle wide range of user instructions. Our RecoWorld provides promising way to estimate system performance without relying on ground-truth annotated data. Enabling Creators to Experiment with Publishing Strategies. simulated environment helps creators explore different content strategies and reach their goals, e.g., growing followers, boosting views, or getting more likes. It lets them try bold, different approaches without risking real user experience. For example, they can test new topics and styles to attract fresh audiences, find the right posting frequency to keep followers interested without annoying them, or see how controversial opinions might be received. By mimicking user feedback, this environment lets creators test the waters safely. Simulated user profiles can be based on their current audience, general population sample, or even specific target groups. Supporting New and Marginal Users in Exploring Interests. Algorithms such as the multi-armed bandit are frequently used in RecSys to find the right balance between exploring new user interests and exploiting known preferences. I.e., an agent chooses from fixed set of actions (interests) to maximize cumulative reward over time. challenge here is to explore without compromising user experience. RecoWorld addresses this by collecting simulated user engagement metrics, such as watch time and clicks, which serve as pseudo-rewards to evaluate the relevance of each interest. Our simulator can seamlessly integrate with contextual bandit by simulating groups of users with similar features, such as age, location, and past engagements, to provide collective exploration feedback. Building Community Leaderboard for Agentic RecSys. Many enterprise recommender systems are developed in-house, making it difficult to replicate their results. The challenge is even greater with Agentic RecSys, which can take on various forms. They share an agentic nature, e.g., an agent (or multiple agents) interacts with an environment; the agent takes actions and receives observations and rewards in return. However, each task may have different setup, e.g., variations in environmental states and reward signals. RecoWorld provides shared platform with community leaderboard, allowing different systems to be compared fairly. We invite practitioners to contribute their agentic tasks to RecoWorld using common API. They can receive feedback signals from our simulated users and make their tasks available for the community to participate in. RecoWorld thus accelerates RL development, organizes various recommendation tasks from simple to complex for curriculum learning, and even fosters an agent-agent collaborative ecosystem. Table 1 We provide four example use cases of RecoWorld, while recognizing that building simulated environments for agentic recommender systems can go well beyond these applications. of the actual user base. It would simulate realistic user behaviors, such as clicking, sharing, buying, and giving feedback, using historical data or even starting from scratch. It might also factor in contextual information such as time of day, location, and device type, which can influence user interactions. While feedback from simulated users may not perfectly replicate every interaction, it remains valuable by providing reward signals that reflect population-level preferences. We introduce RecoWorld, blueprint for building simulated environments that propel the development of agentic recommender systems (Huang et al., 2025a). As these systems gain capabilities such as perception, reasoning, planning, memory, tool use, and autonomy, having environments for rapid system testing and refinement becomes essential before deploying them widely. We do not propose new agentic recommenders here; instead, our focus is on creating simulated environments that replicate user feedback. RecoWorld harnesses LLMs strong reasoning, long-context understanding, and multimodal abilities to simulate user responses. It focuses on optimizing for long-term value through simulated interactions. The approach also emphasizes collective impacts, e.g., post is distributed to community of interconnected users rather than isolated individuals. The design of user simulation and reward signals in RecoWorld offers promising potential for enhancing existing recommender systems in modeling both shortand long-term user interests, as well as for generative recommenders. (Xia et al., 2025; Wang et al., 2025b). Our contributions in this paper are summarized as follows. We introduce RecoWorld, simulated environment where virtual users interact with agentic recommender systems to maximize engagement. RecoWorld generates multi-turn interaction trajectories, from which we extract engagement statistics that serve as reward signals for RL-based agent training. Our user simulator realistically models sequential user behavior. When user is about to disengage, they can issue instructions, such as show me more interesting content, instead of simply dropping out. The agentic RecSys responds to these instructions by creating new item lists, aiming to re-engage the user. This cycle continues until the user chooses to exit without further instructions. The simulators effectiveness can be conveniently evaluated using existing RecSys datasets and validated by human annotators. successful simulator triggers accurate instructions that guide the recommender, while an effective recommender responds with item lists that enhance user satisfaction. Together, this leads to improved recommendation quality. To our knowledge, RecoWorld is the first environment to enable instruction-following recommenders in Gym-like RL framework, marking significant step toward more interactive recommender systems."
        },
        {
            "title": "2 Agentic Environments for RecSys",
            "content": "Agentic is emerging as defining characteristic of next-generation foundation models. An agentic recommender system acts as an autonomous agent that actively learns from user interactions. It follows user instructions, acquires new skills to make proactive recommendations, and adapts its behavior based on these experiences (Silver and Sutton, 2025). This marks shift from traditional recommender systems that primarily make passive suggestions (Huang et al., 2025a; Shang et al., 2025; Maragheh and Deldjoo, 2025). With new capabilities to reason and interact with users, there comes the urgent need to develop new environments for evaluation. Simulated environments enable rapid system iteration, enhance data curation, and are increasingly leveraged for agent training. For example, Claude-4 uses simulated environment to check how the model responds to risky system prompts, such as being asked to act as dark web shopping assistant (Anthropic, 2025). Kimi K2 introduces an agentic data synthesis pipeline that creates tool-use demonstrations via simulated and real environments, generating agentic interactions at scale (Kimi Team et al., 2025). Fundamental Components. Our RecoWorld utilizes dual-view architecture. user simulator (a) generates simulated feedback by replicating the behavior of individual users or groups in response to recommendations, and (b) initiates requests to the recommender via explicit instructions or implicit signals. second component is the recommender system itself, which consists of modules for candidate retrieval, ranking, re-ranking, etc.; these modules work together to deliver personalized suggestions. An agentic recommender system extends this setup by involving one or multiple agents that interact with the simulated user. These agents may engage in 3 Figure 1 simulated user interacts with an agentic RecSys over multiple turns within session. conversation to clarify requests or better understand user preferences. When such system can respond to user instructions and dynamically update recommendation lists to keep the user immersed in the session, it is referred to as an instruction-following recommender. Our focus is on developing robust user simulators, and we provide concise overview of the Agentic RecSys architecture in 4. Multi-Turn Interactions Within Session. simulated user interacts with an agentic RecSys over multiple turns (see Figure 1). The user receives list of recommended items and can take actions such as clicking or skipping. In the first turn, the user clicks on two items (green), skips two items (yellow), and then chooses to leave the session after the next item (red). Before leaving, the user reflects on their experience, identifies reasons for dissatisfaction, and gives short instruction to the RecSys as feedback. If the RecSys cannot process user instructions, it simply updates recommendations based on recent interactions. If it can, it refreshes the recommendations to better address the users feedback. The user reviews the new list, continues interacting, and may repeat the process of providing feedback if considering leaving. This cycle continues until the user decides to end the session without further feedback. The sequence of recommended items and the users actions form the interaction trajectory for the session. Consider an example instruction: User: Show me more interesting content. System: Absolutely! Here are videos tailored to your interests and engagement history. Would you like to specify what you find interesting, such as trending topics, educational content, or entertainment? You can also tell me if you want more from specific creators or communities. The agent perceives the users explicit instruction and reasons over their engagement history, demographics, and behavior to create configuration for the recommender system to execute. The system returns an updated recommendation list along with strategies to guide the user. The user simulator then generates feedback signals indicating whether the Agentic RecSys met the users instructions and may provide the next instruction. successful simulator triggers accurate instructions that guide the recommender, while an effective recommender responds with item lists that enhance user satisfaction. The user simulator and agentic RecSys collaboratively enhance session retention. We use trajectory-level interaction metrics such as total time spent as reward signals to measure the effectiveness of agentic RecSys. Following Brown et al. (2024), we generate multiple interaction trajectories per user and initial recommendations. An LLM-based judge evaluates each trajectory against predefined task rubrics. Only trajectories that satisfy the success criteria are retained for training. Such creation of high-quality agentic trajectories, combined with general RL techniques that leverage reward signals and self-critique, is crucial for developing advanced agentic RecSys. These reward signals are orthogonal to offline metrics. Metrics such as NDCG favor exploitation by optimizing for immediate relevance, while our simulator encourages exploration by modeling session dropout risk and optimizing for long-term user retention. We hypothesize that (a) high NDCG combined with high retention signals strong exploitation, with recommendations that are highly relevant; (b) high NDCG with low user retention reveals suboptimal recommendations; the content may be repetitive or insufficiently diverse, leading to user disengagement; and (c) low NDCG with high user retention likely suggests effective exploration, where users are exposed to novel content, supporting long-term retention. 4 (a) When simulated users are replaced by human annotators, the RecoWorld is designed to be highly flexible. agentic RecSys interacts with actual users and receives authentic feedback signals, effectively functioning as an online experiment. (b) When Agentic RecSys is replaced with traditional RecSys, the system ignores user instructions and generates recommendations only from the users past engagement history, without adapting to additional feedback. (c) When instructions are disabled, the user simulator defaults to an evaluator that scores list of items, without giving instructions to improve. Since no improvement is requested, there will be no follow-up iterations; the session will end after single turn. Importantly, user preferences are not conveyed solely through explicit Explicit Versus Implicit Instructions instructions; they are also conveyed implicitly through behavior, whether users realize it or not. E.g., if user frequently finishes watching videos that are >5 minutes, it indicates receptiveness to long-form content, unlike those who prefer quick 10or 20-second clips. If user continues session without interruption, it likely means they are satisfied with the recommendations. Conversely, if they request something more interesting, or if user does not return for while, it likely indicates that the previous recommendations failed to engage them. Implicit instructions can be inferred by monitoring user state. Users with similar behavioral patterns often share underlying implicit signals. Collaborative filtering can exploit these shared patterns to identify implicit instructions that reflect the collective interests of these user groups. Both implicit and explicit signals are crucial for agentic recommenders. Evaluating User Simulators. Our paper does not present experimental results; instead, we outline evaluation designs to ensure user simulators are assessed in realistic settings. Our approach leverages existing RecSys datasets and validation by human annotators. When an annotator is available, they receive the same initial item list as the simulator and perform single action on each item. The annotator can instruct the RecSys to update recommendations or end the session at any point. The system continues to provide updated recommendations in multi-turn setting until the annotator requests break without further instructions. We compare session-level interaction statistics between the simulated user and the human annotator as sanity check for simulator effectiveness. Our user simulator can also be evaluated using existing recommendation datasets. The simulated user interacts with recommended list (e.g., {i1, i2, i3, i4, i5}), identifies break points (e.g., at i3), and issues instructions to reorder the remaining items ({i4, i5} {i5, i4}) to better match user instructions. This process continues until no further instructions are given, resulting in an updated list (e.g., {i1, i2, i3, i5, i4}). Essentially, the recommender periodically reorders the remaining items based on user feedback. The final recommendations can be directly compared to the initial list and ground-truth recommendations using metrics such as Recall@N and NDCG@N. Benefits of Simulated Environments. Simulated environments provide safe training space for RL-based recommendation agents (Putta et al., 2024), where they can learn from errors without inflicting them on real users. They enable modeling of long-term user feedback over session or across sessions (Zhao et al., 2023), unlike conventional offline evaluations that focus on immediate responses, e.g., clicks. Further, LLM-driven simulators add interpretability by allowing simulated users to articulate their reasoning with thought traces, and give rich, natural language feedback, e.g. dont like this recommendation because Ive seen something similar . Simulators also help alleviate data-sharing concerns. Some simulators combine real item data with synthetic user populations. So one can test algorithms on synthetic yet behaviorally realistic users without exposing any personal user data."
        },
        {
            "title": "3 User Simulation",
            "content": "LLMs strong reasoning and content understanding capabilities offer significant potential for simulating human behavior (Binz et al., 2025). We simulate real user behavior by predicting their next action when presented with list of recommended items. As illustrated in Figure 2, the simulated users action space for each item includes: (1) Click, (2) Comment, (3) Share, (4) Like, (5) Watch [specify duration in seconds], (6) Skip, and (7) Leave the session. If the user chooses to leave (action 7), they are prompted to reflect on their experience, specify reasons for dissatisfaction, and provide an instruction to the RecSys for improvement, or they may exit without further input. User decisions are influenced by both the current context and past interactions H. As shown in Figure 2, we consider contextual factors related to the user, such as temporal (time of day, seasonality), demographic (age, gender, location, etc.), behavioral (time spent, search queries, etc.), and social connections (group affiliations, etc.). 5 Figure 2 Three modeling alternatives for engagement history that leverage LLMs powerful reasoning capabilities (3). session begins when the user opens the app and ends when they exit. Each recommendation presents list of items, selected from candidate set and shown in order. The agent and simulated user may interact over multiple turns within session, e.g., the user requests more interesting content, and the agent updates the list. reward signal is generated after each list is shown. Crucially, we aim to optimize for long-term user retention as the reward signal, i.e., maximizing session duration and minimizing the interval between sessions, which relates to Daily Active Users (DAU). Specifically, the simulated user goes through each recommendation one by one. For each item, the user takes three steps: 1. Think it through: What is your reasoning behind how youll respond? 2. Take action: Write down what youll actually do. 3. Update your mindset: How does this affect your current thinking? Figure 3 shows summary table containing initial simulator results, including simulated user actions and reasoning for each item."
        },
        {
            "title": "3.1 Multimodal User Engagment Modeling",
            "content": "Effectively modeling user engagement history is essential for building simulators. user may engage with multimodal items spanning text, audio, images, and videos. Processing them within unified semantic space is nontrivial. Incorporating extended histories further requires the simulator to handle long contexts, with recent interactions likely carrying greater weight. For example, summarizing that in the past 7 days, the user watched 10 videos on deep-sea fishing, and commented on 5 friends posts can provide useful insights. Such compression can be achieved through statistical aggregation, semantic embeddings, or RAG techniques, all of which improve simulation efficiency. Below, we present three modeling alternatives for engagement history that leverage LLMs powerful reasoning capabilities (Figure 2). Text-based modeling. One approach is to represent all user and item data in textual form. User demographics can be specified in the system prompt, while each items metadata, such as publish time, genre, creator, content type, interest cluster, and brief summary, together with the users interaction history, are provided as textual context. This approach offers flexibility in determining which information to include in lifelong user behaviors, supports prompt compression for improved efficiency, and leverages the reasoning ability of LLMs to analyze user behavior and replicate user responses. Nonetheless, relying solely on textual descriptions may overlook certain nuances, including the mood of video, sarcasm in content, or musical 6 Figure 3 LLM uses their reasoning abilities to simulate user responses step-by-step. Using GPT-4.1, we set up user profile: 30-year-old male interested in deep sea fishing and has history of liking UFC events. The RecSys serves him five short videos (see 1st column). For each, the simulator performs three tasks: Reasoning (2nd column), predicts the Action Taken (3rd column), and updates the Mindset (4th column). The simulator successfully recognizes that Lobster Fishing matches his fishing interest, so he watches for 20 seconds. When UFC Fight Night appears, his past engagement suggests hell watch for 30 seconds. He isnt interested in hairstyling, and when second hairstyling video is shown (last row), the simulator notes, Not interested, expected fewer recommendations. This example shows that large reasoning models, such as GPT-4.1, hold significant potential for modeling user mindset. characteristics, which can limit the fidelity of preference modeling. Multimodal modeling. The second alternative is to use multimodal LLM (MLLM) as the backbone, such as Qwen2.5s Omni model (Xu et al., 2025b) or vision-language models (VLM), Gemini-2.5-Pro. In this setting, items are directly ingested by the model rather than represented through textual descriptions. Images are tokenized into patches, while short-form videos with their audio tracks can be jointly processed through frameworks that synchronize visual and acoustic signals. Demographic attributes and textual metadata can also be incorporated as auxiliary inputs. The model output may take multiple modalities, though we restrict it to text for predicting user behavior. Compared with text-only LLMs, multimodal models provide richer representational capacity for multimodal content, enabling more faithful input understanding. Nevertheless, their reasoning ability, typically weaker, and the substantially larger token space of audiovisual inputs imposes significant context and efficiency challenges, making prompt compression particularly difficult. Semantic ID modeling. The third approach is to represent items using semantic IDs, where each items content, including video and affiliated multimodal features such as audio and textual tags, is encoded Items with similar into semantic ID (Rajput et al., 2023; Deng et al., 2025a; Zheng et al., 2025). semantics share similar IDs, with the sequence from the first to the last character encoding progressively finer-grained semantic information. During continuous pre-training, these semantic IDs are integrated into text descriptions so that their vector representations are learned jointly with the LLMs. At inference, user demographic information is provided in the prompt, while historical item engagements are represented using semantic IDs rather than textual descriptions or raw multimedia inputs. This approach leverages the strong reasoning capabilities of language models while providing structured, compact representation of items. limitation is that continuous pre-training is required to obtain well-trained model, and periodic retraining is necessary to incorporate new content, such as newly released music albums. Unlike the first two approaches, no continual pre-training, supervised fine-tuning, or reinforcement learning is performed on the backbone LLMs or VLMs in this setting. In addition to effective multimodal user engagement modeling, it is essential to achieve robust lifelong user behavior modeling. Users continuously interact with diverse items in recommender systems, resulting in ever-growing interaction histories. Over time, these histories can become effectively unbounded, making it computationally infeasible and costly to store and process all past behaviors. Precisely, we highlight two crucial principles built upon multimodal modeling: 1. Dynamic memory modeling: Users may have infinite historical 7 interactions. However, not all behaviors contribute equally to reflecting the users current intention, and many may introduce noise. It is therefore crucial to design mechanisms that can dynamically and intelligently store, filter, and retrieve user historical behaviors. 2. Evolving preference modeling: User interests are not static but continuously evolve with changing contexts and the information they are exposed to through recommendations. Therefore, user simulator needs to effectively capture temporal dynamics, long-term dependencies, and potential shifts in user interests, ensuring that the generated behaviors and the resulting recommendations remain both personalized and timely throughout the interaction lifecycle. In our proposed method, this can be achieved via mindset updates (refer to step-by-step feedback example in Figure 3)."
        },
        {
            "title": "3.2 Candidate Items",
            "content": "We use real-world content whenever possible (e.g., engagement history and recommended items) to ensure realistic user behavior modeling. Using both simulated content and simulated users can be challenging, as combining these approximations may affect the reliability of behavioral insights. Specifically, each item is presented by its content (e.g., short-form video) along with its metadata and text description. The metadata includes details such as the timestamp, creator information, engagement metrics (e.g., likes, shares, comments), and content category. user may be recommended various types of content, such as: (a) short-form videos, (b) text posts, (c) mixed media (posts, videos, and images), and (d) friend suggestions. Multiple representations can be used for content understanding: text descriptions provide summary of the image or videos content; audio/visual encoders extract multimodal features from the media; and semantic IDs capture high-level semantic categories. The simulator may also leverage metadata tags, such as hashtags, captions, or timestamps, to better mimic realistic user behavior."
        },
        {
            "title": "4 Instruction-Following RecSys",
            "content": "Once users are accustomed to giving instructions to LLMs, they tend to do the same with every system they use (Zhou et al., 2023; Zhang et al., 2023; Xu et al., 2025c; Zhuang et al., 2025). For recommender systems, this means developing new agentic capabilities that allow them to respond to user commands and adjust their behavior accordingly. Users might ask to see more or less of the content that interests them, request content from specific sources such as particular friends or topics, or ask questions about the content they are seeing in their screen. This applies to various media forms, ranging from text, photos, videos, multimedia carousels, to products and even social connections. An instruction-following recommender could be built around an autonomous agent with four core capabilities (see Figure 4): (a) perception: detecting user state and context; (b) reasoning and planning: decomposing user instructions into subtasks and delegating them to retrieval and ranking modules; (c) action (tool use): delivering updated recommendations and responding to user information needs; and (d) memory: maintaining record of user behavior to inform future interactions. In the context of recommender systems, the interaction between user and the system can be modeled as Markov Decision Process. The task involves series of states and actions (s0, a0, s1, . . . , aT 1, sT ), beginning at an initial state s0 and finally reach the goal state sT . Here, the environment state st represents the users mindset, which evolves as the user interacts with recommended items. The state transition is modeled as pθ(st+1st, at). It captures how the users mindset changes after the recommender system presents new set of items, and this process can be simulated using LLM-based user simulator, with parameters θ. At each step, the agentic recommender system selects an action at pϕ(ast), yielding new list of recommendations by considering the users explicit or implicit instruction provided by st. The recommender models parameters ϕ could be related to its perception, memory, planning, tool use modules or the architecture itself in the case of generative recommenders. These parameters are updated iteratively to improve the policy. The ultimate goal is to develop policy, denoted as = pϕ(ast), which focuses on identifying the optimal recommendations given the current state st, that maximizes the total expected reward over time. reward function assigns scalar reward Rη(rtst, at) after an action is taken from given state. We can perform multiple rollouts to estimate cumulative rewards, using on-policy learning methods such as Proximal Policy Optimization (PPO) or off-policy learning techniques such as DPO. The reward function may incorporate metrics such as total session time, number of clicks, self-critique scores, or 8 Figure 4 An instruction-following recommender can be powered by an autonomous agent with four essential capabilities: perception, memory, reasoning and planning, and action (tool use). These agentic skills enable the recommender to interpret user commands, adapt its behavior, and deliver responses as per user instructions. combination of factors that measure the systems ability to follow user instructions and maximize engagement. This approach provides more robust feedback compared to action-level rewards. Voice-Driven Feedback. There is increasing interest in developing agentic recommender systems. Companies have introduced easy-to-access voice entry points that allow users to adjust the content they see in their feed, friend graph, and across various product surfaces. E.g., user might say want more content about F1, Show me more Reels like this, Show me whats new with her, Show me content from my high school classmates, Show me the latest posts from my family, Show me more interesting content and Show me less political content. Such user instructions provide valuable feedback signals that agentic recommender systems can learn from. Instruction-Following (InFoRec) vs. Conversational Recommender System (CRS) Both systems help users discover suitable items through multi-turn interactions. The key difference is that InFoRec respond directly to user commands, while CRS takes more proactive approach, suggesting items during the conversation. For example, in CRS chat: User: Hi, whats the date today? \" CRS: Today is September 16, 2025. By the way, have you heard of Brad Pitts new movie F1 ? Its super popular right now. (Zhao et al., 2025) Previous research has explored variety of persuasion strategies, such as authority, emotional appeal, logic, and social proof, to recommend items tailored to users with different personality traits. InFoRec is about following user instructions precisely. It allows user-facing LLM to manage the conversation, often through voice interactions. Any persuasion strategies are handled by the LLM itself, not by InFoRec. Instead, InFoRec focuses on fine-tuning the list of recommended items by adding, removing, or reordering items among the top results. Think of InFoRec as tool that is triggered during the conversation: by utilizing the entire conversation and engagement history, InFoRec dynamically adjusts the recommendations shown to the user. This context-aware, instruction-following capability is crucial step toward developing more agentic recommender systems."
        },
        {
            "title": "5 Multi-Agent Simulators",
            "content": "Recent advances in multi-agent simulations have substantially enhanced our ability to model the dynamics of individuals making decisions and interacting over time (Park et al., 2023, 2024; Dubois et al., 2024; Gao et al., 2024; Yang et al., 2025; Louie et al., 2025; Qian et al., 2025; Anthis et al., 2025). Particularly, Anthis et al. (2025) highlight the potential of LLM-based social simulations. They identify five key tractable challenges that pave the way for innovations in this space. AgentTorch (Chopra et al., 2024) uses the COVID-19 pandemic to test whether LLM agents can replicate population-level behavioral statistics. It further examines the trade-off between simulation scale and individual agency. OASIS (Yang et al., 2025) simulates social media platforms, e.g., and Reddit, to study information diffusion, group polarization, and herd behavior. AgentSociety (Piao et al., 2025) provides testbed for computational social experiments, examining polarization, the spread of inflammatory content, universal basic income, and the impact of shocks such as hurricanes. These simulators 9 vary in scale, from thousands of agents to millions (e.g., OASIS, AgentScope). Consider an environment consisting of simulated users. We denote by ui(t) the state of user at simulation time t, which includes set of user attributes such as age, location, interests, and the set of recommendations with which they have interacted, among others. As the simulation progresses, user updates their state ui(t) by interacting with their neighbors Ni(t) and the environment e(t), both of which may also evolve over time. The neighborhood of user can be defined by social network graph, similarity metrics, or other criteria. We denote by sij(t) the information that user receives from their interaction with user j. For example, sij(t) may represent shared post or message exchanged between users. Following Chopra et al. (2024), the users update rule is specified in Eq. (1), and the environment evolves in response to users actions and states, as described by Eq. (2). Both and are governed by the recommender system dynamics. The strength of user-to-user influence is specified by environment parameters. ui(t + 1) = (cid:0)ui(t), jN (i)sij(t), e(t)(cid:1) . e(t + 1) = (cid:0){ui(t + 1)}N i=1, e(t)(cid:1) . (1) (2) In recommender systems, these tools show great promise for creator support. They may provide projections to optimize content planning and distribution, which helps creators grow their audience. When creator compiles new video, they may want to test the waters by exposing this content to group of simulated users to gauge potential audience reactions. These simulated users can represent the creators existing followers, general population sample, or targeted demographic (e.g., teens in the US and Canada). Creators might use this simulation to experiment with different publishing strategies, such as adjusting frequency to avoid audience fatigue, diversifying content to attract new viewers, or even publishing controversial material to boost engagement. Without simulator, the outcomes of these strategies are often unpredictable and, if mishandled, could have negative consequences. By leveraging network of interconnected simulated users, we can aggregate collective feedback and track predicted user responses at multiple intervals: 6 hours, 12 hours, 24 hours, 2 days, and 1 week post-exposure. Additionally, the simulation can be reset and rerun to measure variability and improve confidence in the predictions. This approach provides an early estimate of how the content might perform in the real world."
        },
        {
            "title": "6 Related Work",
            "content": "Evaluation of Recommender Systems. Recommender systems have seen tremendous progress in recent years (Zhai et al., 2024; Wang et al., 2024; Hou et al., 2025; Zhu et al., 2025; eun Yoon et al., 2025; Zhang et al., 2025a; Xia et al., 2025; Deng et al., 2025b; Zhou et al., 2025), with generative recommenders, e.g. Metas HSTU and Kuaishous OneRec, and emerging agentic recommenders holding promise for delivering highly personalized content. E.g., HSTU achieves up to 65% better ranking metrics than baselines, with 1.5-trillion-parameter version boosting online A/B metrics by 12.4% (Zhai et al., 2024). Similarly, OneRec replaces the traditional retrieve-then-rank pipeline with an end-to-end generative model and was deployed to millions of users, yielding +1.6% increase in watch-time. critical challenge comes with these more powerful models is evaluation. Traditional offline metrics often prove insufficient: promising ideas are dropped due to unconvincing offline results, and improvements that look good offline do not always translate to real user gains. Yet running A/B tests is costly and time-consuming, and it is impractical (and risky) to expose users to every experiment. This has led to growing interest in large-scale simulated environments for recommender evaluation. Simulated Environments for Training Agentic Systems. Simulated environments have been driving force in training autonomous agents through RL, ranging from physics sandboxes such as MuJoCo (Zakka et al., 2025) to games like Go and the Arcade Learning Environment (Farebrother and Castro, 2024) to computer-use tasks such as WebArena (Zhou et al., 2024). In the recommender systems domain, one notable effort is KuaiSim, which provides rich user environment with multi-behavior feedback and cross-session interactions. This enables training RL-based recommenders on sessions and retention scenarios. Another example is AgentRecBench (Shang et al., 2025), which constructs simulation environment by processing multi-domain datasets (e.g. Yelp, Goodreads, Amazon) and exposing standardized world APIs for an agent to query and act. These environments provide sandbox for agents to plan, explore, and learn complex behaviors without real-world risks, and hold promise to propel the next generation of agentic recommenders. 10 User Simulation with LLMs. LLM-empowered user simulator has shown promising results in imitating real-world user behaviors across different domains, such as social simulation (Gürcan, 2023), legal judgement (Jiang and Yang, 2025), and recommender systems (Peng et al., 2025). In recommender systems, LLM agents are typically developed to generate simulated user feedback for recommender model evaluation (Yoon et al., 2024) and optimization (Cai et al., 2025). Existing works can be mainly categorized into two groups: 1) Non-conversational user simulator (Zhang et al., 2025b), where the LLM agent mainly give realistic feedback (e.g., like, dislike) on recommendations as exemplified by Agent4Rec (Zhang et al., 2024a) and AgentCF (Zhang et al., 2024b). Typically, non-conversational user simulator cannot proactively give instructions to recommender systems or proactively interact with environments (e.g., other LLM agent). Another line of work lies in 2) conversational user simulator (Guo et al., 2024; Yoon et al., 2024), which endows LLM agents with the flexibility to engage in dialogue with recommender models and other agents (e.g., RecAgent (Wang et al., 2025a)), often incorporating self-reflection mechanisms to model user intent more accurately. To support the development of agentic recommender systems, we borrow the idea from conversational user simulators, which offer critical advantage in evaluating agentic recommender systems: they can issue context-aware instructions at appropriate moments. For example, indicating desire to shift away from prior preferences due to fatigue or boredom with previous recommendations."
        },
        {
            "title": "7 Conclusion",
            "content": "Our RecoWorld offers novel simulated environment that enables instruction-driven interactions between users and agentic recommender systems. Distinguished from previous works, it features dual-view architecture, where simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. By modeling realistic user behavior and leveraging multi-turn feedback, RecoWorld provides meaningful reward signals for reinforcement learning agents. It facilitates evaluation using existing datasets and human validation, demonstrating its potential to improve recommendation quality. RecoWorld represents significant advancement toward more interactive and user-responsive recommender systems in RL settings."
        },
        {
            "title": "References",
            "content": "Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, and Michael Bernstein. Llm social simulations are promising research method, 2025. https://arxiv.org/abs/2504. 02234. Anthropic. System card: Claude opus 4 & claude sonnet 4. Technical report, Anthropic, May 2025. https: //www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. Accessed: 2025-08-09. Marcel Binz, Elif Akata, Matthias Bethge, Franziska Brändle, Fred Callaway, Julian Coda-Forno, Peter Dayan, Can Demircan, Maria K. Eckstein, Noémi Éltető, Thomas L. Griffiths, Susanne Haridi, Akshay K. Jagadish, Li Ji-An, Alexander Kipnis, Sreejan Kumar, Tobias Ludwig, Marvin Mathony, Marcelo Mattar, Alireza Modirshanechi, Surabhi S. Nath, Joshua C. Peterson, Milena Rmus, Evan M. Russek, Tankred Saanum, Johannes A. Schubert, Luca M. Schulze Buschoff, Nishad Singhi, Xin Sui, Mirko Thalmann, Fabian J. Theis, Vuong Truong, Vishaal Udandarao, Konstantinos Voudouris, Robert Wilson, Kristin Witte, Shuchen Wu, Dirk U. Wulff, Huadong Xiong, and Eric Schulz. foundation model to predict and capture human cognition. Nature, Jul 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09215-4. https://doi.org/10.1038/s41586-025-09215-4. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. https://arxiv.org/abs/1606.01540. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. https://arxiv.org/abs/2407.21787. Shihao Cai, Jizhi Zhang, Keqin Bao, Chongming Gao, Qifan Wang, Fuli Feng, and Xiangnan He. Agentic feedback loop modeling improves recommendation and user simulation. In Proceedings of the 48th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 22352244, 2025. Ayush Chopra, Shashank Kumar, Nurullah Giray-Kuru, Ramesh Raskar, and Arnau Quera-Bofarull. On the limits of agency in agent-based models, 2024. https://arxiv.org/abs/2409.10568. 11 Nathan Corecco, Giorgio Piatti, Luca A. Lanzendörfer, Flint Xiaofeng Fan, and Roger Wattenhofer. Suber: An rl environment with simulated human behavior for recommender systems, 2024. https://arxiv.org/abs/2406.01631. Romain Deffayet, Thibaut Thonet, Dongyoon Hwang, Vassilissa Lehoux, Jean-Michel Renders, and Maarten de Rijke. Sardine: simulator for automated recommendation in dynamic and interactive environments, 2024. https: //arxiv.org/abs/2311.16586. Jiaxin Deng, Shiyao Wang, Kuo Cai, Lejian Ren, Qigen Hu, Weifeng Ding, Qiang Luo, and Guorui Zhou. Onerec: Unifying retrieve and rank with generative recommender and iterative preference alignment. arXiv preprint arXiv:2502.18965, 2025a. Jiaxin Deng, Shiyao Wang, Kuo Cai, Lejian Ren, Qigen Hu, Weifeng Ding, Qiang Luo, and Guorui Zhou. Onerec: Unifying retrieve and rank with generative recommender and iterative preference alignment, 2025b. https://arxiv. org/abs/2502.18965. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback, 2024. https://arxiv.org/abs/2305.14387. Se eun Yoon, Xiaokai Wei, Yexi Jiang, Rachit Pareek, Frank Ong, Kevin Gao, Julian McAuley, and Michelle Gong. Omulet: Orchestrating multiple tools for practicable conversational recommendation, 2025. https://arxiv.org/abs/ 2411.19352. Jesse Farebrother and Pablo Samuel Castro. Cale: Continuous arcade learning environment, 2024. https://arxiv.org/ abs/2410.23810. Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. Agentscope: flexible yet robust multi-agent platform, 2024. https://arxiv.org/abs/2402.14034. Taicheng Guo, Chaochun Liu, Hai Wang, Varun Mannam, Fang Wang, Xin Chen, Xiangliang Zhang, and Chandan Reddy. Knowledge graph enhanced language agents for recommendation. arXiv preprint arXiv:2410.19627, 2024. Onder Gürcan. Llm-augmented agent-based modelling for social simulations: Challenges and opportunities. HHAI 2024: Hybrid Human AI Systems for the Social Good, page 134, 2023. Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, and Derek Zhiyuan Cheng. Actionpiece: Contextually tokenizing action sequences for generative recommendation, 2025. https://arxiv.org/abs/2502.13581. Chengkai Huang, Junda Wu, Yu Xia, Zixu Yu, Ruhan Wang, Tong Yu, Ruiyi Zhang, Ryan A. Rossi, Branislav Kveton, Dongruo Zhou, Julian McAuley, and Lina Yao. Towards agentic recommender systems in the era of multimodal large language models, 2025a. https://arxiv.org/abs/2503.16734. Kung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey, Yixin Mao, Silvio Savarese, Caiming Xiong, and Chien-Sheng Wu. Crmarena-pro: Holistic assessment of llm agents across diverse business scenarios and interactions, 2025b. https://arxiv.org/abs/2505.18878. Cong Jiang and Xiaolei Yang. Agentsbench: multi-agent llm simulation framework for legal judgment prediction. Systems, 13(8):641, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, and Yulun Du etc. Kimi k2: Open agentic intelligence, 2025. https://arxiv.org/abs/2507.20534. Ryan Louie, Ifdita Hasan Orney, Juan Pablo Pacheco, Raj Sanjay Shah, Emma Brunskill, and Diyi Yang. Can llm-simulated practice and feedback upskill human counselors? randomized study with 90+ novice counselors, 2025. https://arxiv.org/abs/2505.02428. Reza Yousefi Maragheh and Yashar Deldjoo. The future is agentic: Definitions, perspectives, and open challenges of multi-agent recommender systems, 2025. https://arxiv.org/abs/2507.02097. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. https://arxiv.org/abs/2304.03442. Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, and Michael S. Bernstein. Generative agent simulations of 1,000 people, 2024. https: //arxiv.org/abs/2411.10109. Qiyao Peng, Hongtao Liu, Hua Huang, Qing Yang, and Minglai Shao. survey on llm-powered agents for recommender systems. arXiv preprint arXiv:2502.10050, 2025. Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, and Yong Li. Agentsociety: Largescale simulation of llm-driven generative agents advances understanding of human behaviors and society, 2025. https://arxiv.org/abs/2502.08691. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. https://arxiv.org/abs/2408.07199. Cheng Qian, Zuxin Liu, Akshara Prabhakar, Zhiwei Liu, Jianguo Zhang, Haolin Chen, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, and Huan Wang. Userbench: An interactive gym environment for user-centric agents, 2025. https://arxiv.org/abs/2507.22034. Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems, 36:1029910315, 2023. Yu Shang, Peijie Liu, Yuwei Yan, Zijing Wu, Leheng Sheng, Yuanqing Yu, Chumeng Jiang, An Zhang, Fengli Xu, Yu Wang, Min Zhang, and Yong Li. Agentrecbench: Benchmarking llm agent-based personalized recommender systems, 2025. https://arxiv.org/abs/2505.19623. David Silver and Richard S. Sutton. report, DeepMind, 2025. https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of% 20Experience%20Paper.pdf. Preprint of chapter to appear in the book Designing an Intelligence, MIT Press. experience. Technical Welcome to the era of Jianling Wang, Haokai Lu, Yifan Liu, He Ma, Yueqi Wang, Yang Gu, Shuzhou Zhang, Ningren Han, Shuchao Bi, Lexi Baugher, Ed Chi, and Minmin Chen. Llms for user interest exploration in large-scale recommendation systems, 2024. https://arxiv.org/abs/2405.16363. Lei Wang, Jingsen Zhang, Hao Yang, Zhi-Yuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Hao Sun, Ruihua Song, et al. User behavior simulation with large language model-based agents. ACM Transactions on Information Systems, 43(2):137, 2025a. Yejing Wang, Shengyu Zhou, Jinyu Lu, Qidong Liu, Xinhang Li, Wenlin Zhang, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, and Xiangyu Zhao. Gflowgr: Fine-tuning generative recommendation frameworks with generative flow networks, 2025b. https://arxiv.org/abs/2506.16114. Yu Xia, Rui Zhong, Hao Gu, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai. Hierarchical tree search-based user lifelong behavior modeling on large language model, 2025. https://arxiv.org/abs/2505.19505. Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2025a. https://arxiv.org/abs/2412.14161. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report, 2025b. Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, and Yongfeng Zhang. iagent: Llm agent as shield between user and recommender systems, 2025c. https://arxiv.org/abs/2502.14662. Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Chaochao Lu, Wanli Ouyang, Yu Qiao, Philip Torr, and Jing Shao. Oasis: Open agent social interaction simulations with one million agents, 2025. https://arxiv.org/abs/2411.11581. Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, and Julian McAuley. Evaluating large language models as generative user simulators for conversational recommendation. arXiv preprint arXiv:2403.09738, 2024. Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa Haiderbhai, Samuel Holt, Jing Yuan Luo, Arthur Allshire, Erik Frey, Koushil Sreenath, Lueder A. Kahrs, Carmelo Sferrazza, Yuval Tassa, and Pieter Abbeel. Mujoco playground, 2025. https://arxiv.org/abs/2502.08844. 13 Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations, 2024. https://arxiv.org/abs/2402.17152. An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation. In Proceedings of the 47th international ACM SIGIR conference on research and development in Information Retrieval, pages 18071817, 2024a. Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and Enhong Chen. Notellm-2: Multimodal large representation models for recommendation, 2025a. https://arxiv.org/abs/2405.16789. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Recommendation as instruction following: large language model empowered recommendation approach, 2023. https://arxiv.org/abs/2305.07001. Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language agents for recommender systems. In Proceedings of the ACM Web Conference 2024, pages 36793689, 2024b. Zijian Zhang, Shuchang Liu, Ziru Liu, Rui Zhong, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Qidong Liu, and Peng Jiang. Llm-powered user simulator for recommender system. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1333913347, 2025b. Kesen Zhao, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng, Peng Jiang, and Kun Gai. Kuaisim: comprehensive simulator for recommender systems, 2023. https://arxiv.org/abs/2309.12645. Xiaoyan Zhao, Yang Deng, Wenjie Wang, Hongzhan lin, Hong Cheng, Rui Zhang, See-Kiong Ng, and Tat-Seng Chua. Exploring the impact of personality traits on conversational recommender systems: simulation with large language models, 2025. https://arxiv.org/abs/2504.12313. Carolina Zheng, Minhui Huang, Dmitrii Pedchenko, Kaushik Rangadurai, Siyu Wang, Gaby Nahum, Jie Lei, Yang Yang, Tao Liu, Zutian Luo, et al. Enhancing embedding representation stability in recommendation systems with semantic id. arXiv preprint arXiv:2504.02137, 2025. Guorui Zhou, Hengrui Hu, Hongtao Cheng, Huanjie Wang, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Lu Ren, Liao Yu, Pengfei Zheng, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Ruiming Tang, Shiyao Wang, and Shujie Yang et al. Onerec-v2 technical report, 2025. https://arxiv.org/abs/2508.20900. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. https://arxiv.org/abs/2311.07911. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. https://arxiv.org/abs/2307.13854. Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, and Zuotao Liu. Long-term interest clock: Fine-grained time perception in streaming recommendation system, 2025. https://arxiv.org/abs/2501.15817. Yuchen Zhuang, Aaron Trinh, Rushi Qiang, Haotian Sun, Chao Zhang, Hanjun Dai, and Bo Dai. Towards better instruction following retrieval models, 2025. https://arxiv.org/abs/2505.21439."
        }
    ],
    "affiliations": [
        "Meta"
    ]
}