{
    "paper_title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
    "authors": [
        "Hanshen Zhu",
        "Yuliang Liu",
        "Xuecheng Wu",
        "An-Lan Wang",
        "Hao Feng",
        "Dingkang Yang",
        "Chao Feng",
        "Can Huang",
        "Jingqun Tang",
        "Xiang Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 3 0 9 0 2 . 2 0 6 2 : r TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering Hanshen Zhu1,, Yuliang Liu1, Xuecheng Wu2, An-Lan Wang2, Hao Feng2, Dingkang Yang2, Chao Feng2, Can Huang2, Jingqun Tang2,, Xiang Bai1,(cid:66) 2ByteDance 1Huazhong University of Science and Technology {zhs, ylliu, xbai}@hust.edu.cn, jingquntang@bytedance.com https://github.com/CIawevy/TextPecker"
        },
        {
            "title": "Abstract",
            "content": "Visual Text Rendering (VTR) remains critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating critical bottleneck for both VTR evaluation and RL-based optimization. As result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct recognition dataset with character-level structural-anomaly annotations and develop stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing new state-of-the-art in high-fidelity VTR. Our work fills gap in VTR optimization, providing foundational step towards reliable and structural faithful visual text generation. 1. Introduction Text-to-image generation has witnessed remarkable progress in producing photorealistic and detail-rich results [2, 10, 41, 44]. With these advancements, Visual Text Rendering (VTR), the task of generating legible and semantically consistent text within images, has emerged as challenging and evolving frontier [3, 55, 65]. However, the recent surge Part of this work was done during Hanshen Zhus internship at ByteDance. Project Leader. (cid:66)corresponding authors. Figure 1. Existing OCR models and MLLMs struggle to perceive fine-grained structural anomalies in rendered text images, creating key bottleneck for both VTR evaluation and RL-based optimization. Misrecognized characters are highlighted in RED. of specialized image generators (e.g., Flux-series [28]) and unified generative models (e.g., GPT-4o [39], BAGEL [12]) still struggle with VTR tasks, producing visual text with distortion, blurriness, misalignment, or missing characters [65]. Prior works [9, 18, 20, 55] mitigate these issues with reinforcement learning (RL): the generated text is first recognized through OCR models [15, 53] or Multimodal Large Language Models (MLLMs) [1, 60, 69], then rule-based scores (e.g., edit distance to the prompt) are computed and used as rewards. To evaluate text rendering performance, existing metrics [6, 14, 20] follow an analogous paradigm. This prevailing paradigm, however, rests on flawed premise. We identify critical bottleneck shared by both VTR evaluation and reinforcement learning process: lack of fine-grained structural anomaly perception in rendered 1 text. As illustrated in Fig. 1, OCR models [15, 53] and MLLMs [1, 40] are inherently ill-suited for this task. Their failures manifest in two primary ways: (1) Misinterpretation: They over-rely on linguistic priors to correct or hallucinate semantic content from structurally flawed text, thereby ignoring subtle glyph-level defects (e.g., stroke deletions, misalignments, or spurious attachments). (2) Invisibility: They often fail to detect or simply dismiss low-confidence text regions, such as those with significant blurriness or distortion, treating them as non-existent. Therefore, evaluators yield unreliable text-accuracy estimates and fail to assess structural quality, and reward signals become misleading. As direct result, even state-of-the-art generators (e.g., Qwen-Image [55], SeedDream4.0 [3]) still struggle to render structurally faithful text. Our quantitative analysis in Tab. 2 further substantiates this issue. Building on these insights, we propose TextPecker, plug-and-play structural anomaly perceptive RL strategy for visual text rendering. At its core, TextPecker replaces noisy OCR-based rewards with perception-guided composite reward that jointly captures semantic alignment and structural fidelity. Its structural term is sensitive to fine-grained glyph deformation and distortion, assigning reliable penalties to subtle defects that deceive structure-blind OCR and destabilize policy learning. This yields stable credit assignment and seamlessly integrates into any text-to-image generator without architectural changes. To construct this reward, we address the scarcity of fine-grained structural annotations by building hybrid dataset that couples two complementary sources: (1) images with authentic generative artifacts from various text-to-image models, meticulously annotated at the character-level, and (2) synthetic data from our strokeediting engine, crafted to expand error diversity while including normal characters for robust recognition. Extensive experimental results demonstrate that our introduced TextPecker delivers consistent and significant improvements across diverse generators, including FLUX [28], SD3.5 [16], and Qwen-Image [55]. Remarkably, For FLUX, our method yields dramatic gains over its base version (e.g. +38.3% Sem. and +31.6% Qua.) while also substantially outperforming the OCR-reward baseline. This advancement becomes even more pronounced on the highly-optimized Qwen-Image. In the challenging domain of Chinese text rendering, our approach achieves gains of 8.7% in semantic alignment and 4% in structural fidelity, establishing new state-of-the-art in high-fidelity VTR. We summarize our contributions as follows: We identify critical bottleneck in VTR: the lack of finegrained structural perception in current OCR-based evaluators, which hinders effective VTR optimization. We propose TextPecker, plug-and-play structural anomaly perceptive RL strategy that seamlessly integrates into any text-to-image generators. We construct large-scale dataset with character-level structural anomaly annotations, addressing the data scarcity and enabling fine-grained structural perception for reward modeling. Our method consistently improves leading generators and sets new state-of-the-art in VTR, with notable gains even on the highly optimized Qwen-Image. 2. Related Work 2.1. Visual Text Rendering Text images are unique and crucial information media in modern digital society. Mainstream text rendering methods broadly fall into two categories. The first focuses on specialized modules to incorporate additional constraints, such as glyph information [38, 48, 61, 64] for text morphology control, or layout guidelines [7, 19, 52, 63] for precise text placement. The second focuses on improving text encoder designs. prevailing explanation for the text renderings difficulty is that the text information often degrades or is inadequately preserved during encoding. To mitigate this, subsequent works [8, 36, 67] introduce special tokens for the rendered text or adopt tokenizer-free encoders like ByT5 [59]. With recent advances in generative models, specialized generators [3, 16, 28, 55] and unified genertative models [12, 32, 56] already exhibit substantial text rendering capabilities without relying on ad-hoc designs such as glyph conditions. Despite these improvements, they still struggle with text rendering tasks, producing visual text with distortion, blurriness, misalignment, or missing characters [65]. 2.2. Evaluations for VTR The assessment of VTR has predominantly centered on textual accuracy [6, 14, 20, 49]. key challenge, particularly in non-glyph-conditioned generation, is the potential order mismatch between generated and target text. To address this, Lex-Art[66] proposed Pairwise Normalized Edit Distance (PNED), which combines Hungarian matching [26, 37] with penalty for unmatched words. TIIF-Bench added global normalization to yield GNED[54], reducing sensitivity to text-length imbalance. Fang et al. [17] directly use Qwen2.5VL [1] as end-to-end VTR evaluators, yet it suffers from hallucination and inaccuracies. Notably, recent works have also recognized the importance of structural quality of text. Font-Agent [29] presents stroke-aware font quality assessor, yet it is limited to single-character evaluations. He et al. [24] addresses OCR hallucinations in degraded documents, but is confined to document analysis domain. Reward models are crucial for aligning generative models with human preferences during post-training optimization. Numerous efforts have largely improved reward models for general visual quality assessment [51, 57, 58]. Unlike subjective protocols such as aesthetic or quality, existing VTR 2 Figure 2. Schematic illustration of the TextPecker framework. Given generative prompt, we first sample candidate outputs {oi}G i=1 from the reference policy model πθref . Each oi is sent to structure-aware recognizer to extract fine-grained generated text, with markers indicating structurally anomalous text. We then compute the joint reward Ri, comprising weighted sum of semantic alignment and structural quality scores (Sec. 3.2.1). Each Ri is normalized to group relative advantage Ai. Finally, we optimize the current policy model πθ by maximizing Ai while enforcing proximity to πref via KL divergence. reward modeling [9, 18, 20, 21] has primarily focused on textual accuracy, which is typically assessed by de facto evaluators such as standard OCR models [15, 53] or MLLMs like Qwen2.5-VL [1]. However, these methods for both evaluation and reward modeling are fundamentally constrained by their inability to perceive fine-grained structural anomalies, leading to unreliable accuracy estimates and misleading rewards. In response, TextPeckers fine-grained, perception-guided composite reward moves beyond the noisy signals of OCR-based methods, enabling the joint quantification of semantic accuracy and structural fidelity. 3. Methodology 3.1. Preliminaries Reinforcement learning (RL) is common and effective paradigm for improving text rendering in text-to-image models [3, 28, 55], with widely adopted variants such as DPO [42] and GRPO [22]. We focus on GRPO, critic-free on-policy method that stabilizes policy optimization via group-wise relative advantages and intra-group reward normalization. However, due to the deterministic nature of flowmatching [33, 35] models, they are not intrinsically designed for reinforcement learning. Flow-GRPO [34] extends GRPO to the rectified-flow setting by injecting stochasticity into the integration process. Specifically, it converts the deterministic dynamics into stochastic differential equation: (cid:18) dxt = vt + σ2 2t (xt + (1 t)vt) dt + σt dwt, (1) (cid:19) where vt = vθ(xt, t, c) is the network-predicted velocity, dwt denotes Brownian motion, and σt = 1t controls the magnitude of stochasticity. (cid:113) For VTR Optimization, prior works [9, 18, 20, 34] predominantly leverage string-level accuracy reward: = 1 Ne/Nt, where Nt denotes the length of target string and Ne is the edit distance between the target string and the OCRextracted content. As discussed earlier (cf. Fig. 1), existing OCR Models [15, 53] and MLLMs [1] prioritize semantic recovery over glyph integrity, hallucinating corrections for structurally flawed text and omitting low-confidence distorted regions. Such behaviors depress the edit distance Ne and inflate the reward score S, resulting in biased rewards that hinder effective optimization. 3.2. TextPecker As shown in Fig. 2, we introduce TextPecker, plug-andplay RL strategy for enhancing VTR with fine-grained structural perception. Unlike conventional methods that rely on noisy OCR signals and overlook structural flaws, TextPecker redefines reward modeling by jointly optimizing semantic alignment and structural fidelity. Integrating this perceptionguided composite reward into the RL loop yields consistent gains across diverse generators. 3.2.1. Structure-aware Reward Functions To address the structural blindness and overconfident scoring of prior OCR-based methods, our reward formulation is built upon structure-aware assessment module. As illustrated in Fig. 2, this module identifies fine-grained structural anomalies in the generated text (e.g., missing or spurious strokes) and flags them with special markers. The detailed construction of this module is presented in Sec. 3.2.2. Assuming we have such module, we formulate our composite reward as follows. Structural Quality Score (SQ). An intuitive way to quantify structural quality is to measure the proportion of bad characters. We define the structural quality score, SQ, based on the ratio of structurally anomalous characters to the total number of characters. However, for powerful generators, structural errors are often rare but visually jarring when they do occur. To amplify the penalty for such infrequent yet critical failures, we introduce scaling factor ω > 1. The 3 final score is thus formulated as: SQ = clip(cid:0)1 ω Na NP , 0, 1(cid:1), (2) where NP is the total number of characters in the generated text P, and Na is the number of characters flagged as anomalous by our assessor. Here, clip constrains the value to the range [0, 1]. Semantic Alignment Score (SE). Unlike prior OCRbased rewards that treat text as simple long string, we argue that word-level matching is crucial for accurately assessing text that may not be rendered in the same order as the prompt. Inspired by [54, 66], we also find it necessary to penalize any unmatched words, which typically include extraneous or repeated texts in the generated output, as well as missing textual content from the target prompt. Therefore, we formulate our semantic alignment score as: SE = 1 (cid:80) (ti,pj )M NED(ti, pj) + Penalty(T , P, M) max(T , P) . (3) Here, and are the sets of words in the target and generated text, respectively. represents the optimal word pairing between and P, found via the Hungarian algorithm to achieve the minimum alignment cost based on Normalized Edit Distance (NED). The Penalty() term is the count of any unmatched words. This ensures that both superfluous generated words and missing target words contribute to the overall error. The final score SE is also clipped to the range of [0, 1], where higher value indicates better semantic alignment. Composite Reward (R). Finally, we formulate the overall TextPecker reward as weighted sum of the two scores, allowing for joint optimization of both aspects: = wE SE + wQ SQ, wE + wQ = 1. (4) 3.2.2. Structural Perceptive Data Construction Our structure-aware reward relies on robust assessor for fine-grained structural anomalies, but the requisite labeled data is critically scarce. To construct large-scale, highquality dataset, we proceed in three steps  (Fig. 3)  : Step 1: Text-rich Image Generation. The initial step involves constructing large-scale dataset of text-rich visual images, covering diverse structural error types. Specifically, for English text generation, we draw prompts from TextAtlas5M [49] and Lex-10k [66], and leverage multiple Englishcapable generative models (Anytext, Stable Diffusion v1-5 [43], Stable Diffusion 3.5 [16], Flux [28], SeedDream3.0 [18], Qwen-Image [55]) to generate such images. For Chinese text generation, we first sample comprehensive text Figure 3. The illustration of proposed data construction pipeline. corpus from WanJuan1.0 [23], ensuring coverage of modern Chinese common characters We then use Qwen3-235BA22B [60] to generate descriptions of various font styles, which are integrated with the corpus to form final prompts for models including Cogview4 [13], Kolors [27], SeedDream3.0 [18], and Qwen-Image [55]. Step 2: Structural Anomaly Annotation. Generated textrich images exhibit diverse structural anomalies. We define such anomalies as any structural distortion impairing semantic recognition, caused by blurring, warping, missing strokes, or redundant artifacts. To streamline annotation, we first leverage OCR models [15] to obtain preliminary recognition results. Annotators then identify and rectify fine-grained character-level structural flaws with special marker (as illustrated in Fig. 2). For words with severe structural adhesion that prevents accurate character counting, we use distinct placeholder, yielding dataset with refined fine-grained labels for structural anomalies. Step 3: Synthetic Data Augmentation. While Step 2s annotations capture common structural anomalies, models trained solely on them exhibit two key limitations: poor generalization to unseen anomalies and degraded recognition of Chinese characters (Tab. 4). This stems from the intrinsic complexity of Chinese: unlike the linear morphology of English, Chinese characters have 2D spatial composition and vast inventory of over 8,000, causing combinatorial explosion of structural anomalies beyond what exhaustive annotation can cover. To overcome this, we introduce synthesis-based augmentation that programmatically generates diverse erroneous and canonical Chinese characters. We start by representing Chinese characters as compositions of fundamental strokes, modeled as ordered sequences using stroke order data from public resources. To streamline manipulation, we uniformly sample points along each stroke 4 Table 1. Statistics of our constructed text-rich image recognition dataset with structural-anomaly labels at box and image levels. Proportions are computed over all instances. Data Type Level Samples Proportion Manual Annotations Synthetic Anomaly Text Synthetic Normal Text Box Image Box Image Box Image 559.6K 131.1K 452.5K 100.0K 150.0K 30.0K 39.32% 9.21% 31.80% 7.03% 10.54% 2.10% Total 1.4M 100% to enable manipulation. With stroke sequences and their point representations, we define three stroke-level structural edit operators. We apply them sequentially and compose them to produce diverse structural anomalies. (1) Stroke Deletion: removes controlled subset of strokes. (2) Stroke Swapping: exchanges the locations of disjoint stroke pairs by aligning centroids. (3) Stroke Insertion: adds strokes sampled from other characters. These operators generate structurally anomalous characters, we then build rendering engine on top of SynthTIGER [62] and use it to place structurally anomalous and canonical text onto diverse backgrounds and layouts, producing text-rich images. We merge the annotated and synthetic data to form the final training and test splits, with dataset statistics and visualized distributions shown in Tab. 1. 4. Experiments 4.1. Experimental settings Additional implementation details of our method, Baselines, dataset, and metrics are provided in the Appendix. Implementation Details. We adopt Qwen3-VL-8B [60] and InternVL3-8B [69] as the base architecture for TextPecker, leveraging their strong general recognition performance, robust cross-modal alignment, and native support for boundary box input. Fully supervised fine-tuning uses batch size of 2, gradient accumulation steps of 32, learning rate of 5e-6, warm-up ratio of 0.05, and runs for 2 epochs. For VTR optimization, we employ Flow-GRPO [34] and validate on three popular models: SD3.5-M [16], Flux.1[dev] [28], and QwenImage [55]. We set ω = 5 and wE = wQ = 0.5 and adopt the Qwen3-VL-based variant for reward function. Following Flow-GRPO [34], other hyperparameters vary across models but are consistent within each; full details are provided in the Appendix. Both the recognizer training and VTR optimization experiments are conducted on 32 NVIDIA H20 GPUs. Metrics and Datasets. We design two tasks, Text Structural Anomaly Perception (TSAP) and Canonical Text Recognition (CTR), to evaluate models fine-grained recognition abila, ity under generated text images. TSAP measures whether the predicted anomalous character count Na lies within the interval [δ a/δ], where δ is tolerance hyperparameter set to 0.7, with such interval-matched cases used to compute Precision, Recall, and F1-scores. For CTR, we report two metrics: Recall and NED for fine-grained assesment. Evaluations are conducted on the test set proposed in Sec. 3.2.2. For Text Rendering tasks, we first evaluate models on three established benchmarks: OneIG-Bench [6], CVTG2K [14], and LongText-Bench [20]. We identified that their reliance on structurally-unaware OCR Models [1, 15] can yield unreliable metrics, critical limitation shared by broader range of prominent benchmarks [49, 54, 66]. To address this, we first re-evaluate these benchmarks using TextPecker, reporting both semantic alignment scores and structural quality scores (defined in Sec. 3.2.1 with ω = 1 for evaluation). To streamline this re-evaluation with TextPecker and leverage the strengths of existing benchmarks, we integrate English and Chinese prompts curated from these datasets [6, 14, 20, 49, 55, 66], referred to as GenTextEval for brevity. Given the scarcity of Chinese-rendering benchmarks [6, 20, 55], we supplement this integrated set with Chinese prompts constructed in Sec. 3.2.2, resulting in total of 314 English prompts and 417 Chinese prompts. More details are provided in the Appendix. Baselines. We compare TextPecker against both specialist OCR models and general MLLMs. The former includes PPOCRv5 [15], GOT-OCR-2.0 [53], MonkeyOCR [31]. For the latter, we benchmark against leading proprietary models such as GPT-5 [40], Gemini-2.5-Pro [11], and Doubao-Seed1.6 [4, 5], as well as strong open-source alternatives like Qwen3-VL [60] and InternVL3[69]. 4.2. Main Results Additional results on evaluator generalization, RL baselines, multi-reward and ablations are presented in the appendix. 4.2.1. Quantitative Results TSAP and CTR. As illustrated in Tab. 2, our quantitative analysis reveals significant limitations in existing models and highlights the superiority of TextPecker. First, we observe near-total failure of both leading MLLMs and specialist OCR models on the Text Structural Anomaly Perception (TSAP) task. While few MLLMs like Doubao-Seed-1.6 [4], GPT-5 [40], and InternVL3 [69] show nascent ability for structural anomaly perception, their performance remains rudimentary. This failure stems from core mismatch in task nature: (1) these models are trained for generalized text recognition, which prioritizes robust semantic extraction over structural authenticity; for instance, when facing text partially visible due to occlusion, they are encouraged to predict the complete semantics corresponding Table 2. Results of Text Structural Anomaly Perception (TSAP) and Canonical Text Recognition (CTR): measuring models recognition ability under generated text images and TSAP-demanding prompts. Box-level results of non-supporting models are marked as -. Methods English recognition PP-OCRv5 [15] GOT-OCR-2.0 [53] MonkeyOCR [31] Gemini-2.5-pro [11] Doubao-Seed-1.6 [4] Doubao-Seed-1.6-think [5] GPT-5 [40] Qwen3-VL-8B [60] InternVL3-8B [69] TextPecker (InternVL3-8B) [69] TextPecker (Qwen3-VL-8B) [60] Chinese recognition PP-OCRv5 [15] GOT-OCR-2.0 [53] MonkeyOCR [31] Gemini-2.5-pro [11] Doubao-Seed-1.6 [4] Doubao-Seed-1.6-think [5] GPT-5 [40] Qwen3-VL-8B[60] InternVL3-8B [69] TextPecker (InternVL3-8B) [69] TextPecker (Qwen3-VL-8B) [60] TSAP CTR Image-level Box-level Image-level Box-level F1 R F1 NED NED 0.000 0.000 0.000 0.179 0.157 0.259 0.196 0.286 0.206 0.795 0.777 0.300 0.500 1.000 0.079 0.306 0.216 0.233 0.400 0.190 0.889 0.874 0.000 0.000 0.000 0.076 0.167 0.183 0.150 0.017 0.165 0.960 0. 0.013 0.004 0.013 0.048 0.182 0.084 0.220 0.008 0.128 0.968 0.981 0.000 0.000 0.000 0.107 0.162 0.214 0.170 0.032 0.183 0.870 0.862 0.024 0.008 0.025 0.059 0.228 0.121 0.226 0.017 0.153 0.927 0. - - - 0.342 0.333 0.280 0.419 0.500 0.218 0.784 0.714 - - - 0.333 0.310 0.333 0.282 0.000 0.129 0.912 0.900 - - - 0.179 0.180 0.095 0.193 0.018 0.443 0.964 0. - - - 0.099 0.115 0.086 0.165 0.000 0.370 0.988 0.964 - - - 0.235 0.234 0.141 0.265 0.034 0.293 0.865 0.811 - - - 0.152 0.168 0.137 0.209 0.000 0.191 0.949 0. 0.720 0.610 0.578 0.415 0.714 0.736 0.556 0.807 0.759 0.944 0.918 0.921 0.853 0.917 0.574 0.918 0.915 0.758 0.943 0.927 0.962 0.972 0.137 0.186 0.209 0.557 0.169 0.119 0.359 0.078 0.102 0.035 0. 0.067 0.136 0.076 0.422 0.079 0.080 0.239 0.054 0.069 0.037 0.027 - - - 0.300 0.376 0.418 0.398 0.761 0.551 0.953 0.941 - - - 0.526 0.677 0.758 0.729 0.931 0.729 0.991 0. - - - 0.571 0.473 0.414 0.450 0.112 0.302 0.030 0.038 - - - 0.473 0.322 0.242 0.270 0.068 0.270 0.009 0.010 Table 3. Quantitative comparisons of different RL-optimized generative models on OneIG-Bench [6], LongText-Bench [20], CVTG-2k [14], and GenTextEval-Bench. Avg.: Average text score from original benchmarks; Qua.: structural Quality score. Sem.: Semantic alignment. Score measurement and reward computation are both conducted by TextPecker (Qwen3-VL). Models Rewards OneIG LongText CVTG-2K GenTextEval Avg. Qua. Sem. Avg. Qua. Sem. Avg. Qua. Sem. Qua. Sem."
        },
        {
            "title": "English Rendering",
            "content": "SD3.5-M [16] Flux.1[dev] [28] Qwen-Image [55] Chinese Rendering Qwen-Image [55] - OCR TextPecker - OCR TextPecker - OCR TextPecker - OCR TextPecker 0.441 0.572 0.581 0.567 0.754 0.845 0.871 0.986 0. 0.954 0.984 0.988 0.848 0.941 0.957 0.875 0.969 0.979 0.955 0.983 0.988 0.894 0.945 0.956 0.513 0.627 0. 0.585 0.708 0.734 0.814 0.894 0.910 0.732 0.856 0.875 0.296 0.295 0.344 0.613 0.736 0.811 0.935 0.949 0. 0.920 0.967 0.974 0.860 0.944 0.957 0.929 0.972 0.986 0.970 0.986 0.990 0.924 0.956 0.969 0.416 0.498 0. 0.591 0.707 0.672 0.844 0.912 0.918 0.834 0.886 0.908 0.368 0.513 0.596 0.491 0.778 0.777 0.834 0.893 0. - - - 0.869 0.943 0.944 0.908 0.951 0.961 0.964 0.978 0.987 - - - 0.491 0.671 0. 0.523 0.632 0.704 0.817 0.908 0.932 - - - 0.671 0.940 0.959 0.672 0.976 0.988 0.964 0.989 0. 0.933 0.953 0.973 0.265 0.462 0.506 0.336 0.602 0.719 0.729 0.827 0.837 0.810 0.874 0.897 to their expected full structure rather than verify structural fidelity  (Fig. 1)  . Moreover, generated text frequently contains diverse structural anomalies that are rarely encountered in standard text recognition scenarios. Besides, we identify widespread deficiency in box-level text recognition. While traditional OCR models fundamentally lack the ability to process region-specific inputs, modern MLLMs also struggle with structural-anomaly detection. As shown in Tab. 2, their box-level recognition recall is substantially lower than their image-level counterparts. This limitation severely hinders more fine-grained assessments, which are crucial for evaluating demanding tasks such as controllable text editing and translation of text in local areas. In stark contrast, TextPeckers models excel across all dimensions. They not only achieve high F1 and recall on the TSAP task but also improve recognition of generated text (CTR) compared to their respective baselines, underscoring the value of our box-level structure-aware dataset. Specifically, the Qwen3-VL-based variant attains state-of-the-art Chinese recognition performance, while the InternVL3-based variant 6 Figure 4. Qualitative Comparisons of Text Rendering for Qwen-Image and RL-Optimized Variants. Readers are highly recommended to refer to the appendix for extensive comparisons across generative models and RL baselines. exhibits the strongest overall capabilities at the box level. RL for VTR. Tab. 3 quantifies the effectiveness of our RLbased optimization across diverse scenarios. Overall, integrating TextPeckers structure-aware reward yields consistent improvements across four benchmarks, three base models, and both English and Chinese rendering tasks. For Flux.1[dev] [28] on English, gains are pronounced: +38.3% Sem. and +31.6% Qua. over the base model, and +11.7% Sem. over the OCR-reward baseline on GenTextEval. While gains are consistent overall, we also observe few seemingly contradictory results when evaluated by different metrics. For example, SD3.5-M [16] on CVTG-2K reports +8.3% average word accuracy across five regions, yet -7.8% Sem. compared to the OCR-reward baseline. This divergence indicates that structure-unaware, OCR-based metrics can overestimate performance on structurally flawed text, underscoring the necessity of structure-aware assessor for faithful evaluationdespite these localized divergences, the overall benchmark averages still show marked improvement. Notably, even for the well-optimized Qwen-Image [55], TextPeckerbased RL delivers significant improvements, especially on Chinese rendering: +14.3% on OneIG, +7.4% on LongText, and +8.7% on GenTextEval over prior SOTA; compared to the OCR-reward baseline, Qua. increases by +2.0% and Sem. by +2.3%. Taken together, these results establish 7 Table 4. Ablation study on effectiveness of the constructed data: Image-level recognition results across different baseline models. Table 5. Ablation study on the effectiveness of reward design: PM for Pair-wise Matching, and SQ for Structural Quality reward. Settings English Chinese Models Anno. Syn. TSAP CTR TSAP CTR F1 NED F1 NED InternVL3 [69] Qwen3-VL [60] 0.206 0.795 0.795 0.182 0.706 0.777 0.165 0.970 0.960 0.018 0.944 0.969 0.183 0.874 0. 0.032 0.808 0.862 0.759 0.938 0.944 0.810 0.899 0.918 0.102 0.042 0.035 0.076 0.068 0.046 0.190 0.583 0. 0.333 0.643 0.874 0.128 0.966 0.968 0.008 0.942 0.981 0.153 0.727 0.927 0.017 0.764 0.925 0.927 0.849 0. 0.947 0.839 0.972 0.069 0.148 0.037 0.049 0.117 0.027 TextPecker as generalizable, plug-and-play reward that advances structurally faithful and accurate text rendering. 4.2.2. Qualitative Results We present qualitative comparisons for Qwen-Image and its RL-optimized variants in Fig. 4. The vanilla Qwen-Image, despite achieving prior SOTA on several text rendering benchmarks, often produces off-target strings and exhibits blurred, distorted, or misaligned text, particularly in small and dense text regions. Optimizing with an OCR-based reward helps reducing off-target content and semantic alginment is enhanced, yet structural defects persist. In contrast, TextPecker-based RL achieves superior structural fidelity and semantic consistency (e.g. the English menu and Chinese paper cases). These observations align with the quantitative gains in Tab. 3 and the component analysis in Tab. 5, and we observe similar trends across other backbones (see Appendix), underscoring TextPeckers effectiveness as reward for structurally faithful and accurate text rendering. In stark contrast, further optimization with TextPecker achieves superior level of both structural fidelity and semantic consistency. This is particularly evident in challenging cases where OCR-based rewards falter. For instance, in the paper rendering case, our method successfully renders clean, aligned paragraphs where the OCR-rewarded model still produces distorted and wavy text lines. Similarly, in the English menu example, TextPecker accurately generates crisp, legible items that the baseline struggles to form correctly. 4.3. Ablation Studies We conduct ablation studies to validate the effectiveness of our proposed dataset and to dissect the contribution of each component within the TextPecker reward mechanism. Effectiveness of Data Composition. As shown in Tab. 4, training on our annotated data alone yields substantial improvements on the TSAP task, with F1 scores for both models significantly outperforming the baseline. This data also enhances English text recognition. However, we observe noticeable degradation in Chinese recognition performance, which we attribute to the increased complexity of structural anomalies in Chinese characters. The addition of our synthesized data effectively resolves this issue. By training on Generative Model OCR Model Settings GenTextEval-EN NED PM SQ Qua. Sem. Flux.1[dev] [28] - 0.672 PP-OCRv5 [15] 0.976 PP-OCRv5 [15] 0.976 0.984 0. TextPecker TextPecker 0.336 0.602 0.644 0.702 0.719 combined dataset, both models demonstrate dramatic boost in Chinese performance for both TSAP and CTR, achieving precise recognition and high accuracy in detecting structural anomalies. For English, the impact is model-dependent: while Qwen3-VL shows consistent enhancement, InternVL3 exhibits slight decline in TSAP performance, yet gains notable improvements in CTR. Analysis of Reward Components. We deconstruct the reward function step-by-step to isolate the impact of each component. As shown in Tab. 5, combining conventional, structure-unaware OCR model with Pairwise Matching (PM) yields 4.2% gain in the semantic alignment score, yet the structural quality score remains stagnantindicating that GNED sharpens semantic feedback but fails to improve structural fidelity when the recognizer lacks structural perception. Replacing the OCR model with TextPecker delivers gains across both dimensions (Sem. +5.8%, Qua. +0.8%), as TextPeckers reward is inherently structure-aware: characters identified with special markers directly modulate the semantic score. Finally, incorporating the structural quality term as an auxiliary reward brings further improvements and achieves the best overall performance, confirming the synergy of the full TextPecker reward design. 5. Conclusion We pinpoint and address the core bottleneck in VTR evaluation and RL-based optimization: leading MLLMs and specialist OCR models largely fail to perceive fine-grained structural anomalies. We present TextPecker, plug-andplay, structural-anomalyaware framework that couples standardized evaluator with an RL reward, providing complementary reward signals for semantic alignment and structural quality. Empirically, TextPecker delivers consistent gains across leading text-to-image generators, including notable improvements on the well-optimized Qwen-Image. Under structure-aware rewards, generation behavior shifts toward fewer off-target strings, reduced blur and distortion, and improved alignment. This work provides foundational step towards structurally faithful visual text rendering and supplies the community with essential tools for rigorous evaluation and post-training enhancement."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 3, 5 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1 [3] ByteDance. Seedream4.0. https://seed.bytedance. com/en/seedream4_0, 2025. Accessed: 2025-09-22. 1, 2, 3 [4] ByteDance. Doubao-seed-1.6. https : / / seed . bytedance.com/en/seed1_6, 2025. Accessed: 202509-22. 5, [5] ByteDance. Doubao-seed-1.6-thinking. https://seed. bytedance.com/en/seed1_6, 2025. Accessed: 202509-22. 5, 6 [6] Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arxiv:2506.07977, 2025. 1, 2, 5, 6, 12, 14, 16, 17 [7] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. NIPS, 36:93539387, 2023. 2 [8] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In ECCV, pages 386402. Springer, 2024. 2 [9] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025. 1, 3 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 18 [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 5, 6, 12, [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 2 [13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia 9 Yang, et al. Cogview: Mastering text-to-image generation via transformers. NIPS, 34:1982219835, 2021. 4, 15 [14] Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. 1, 2, 5, 6, 14, 16, 17 [15] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020. 1, 2, 3, 4, 5, 6, 8, 12, 17 [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 4, 5, 6, 7, 12, 14, 15, 16, 17 [17] Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m & prism-bench: millionscale text-to-image reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680, 2025. 2 [18] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. 1, 3, 4, [19] Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, and Hongtao Xie. Postermaker: Towards highquality product poster generation with accurate text rendering. In CVPR, pages 80838093, 2025. 2 [20] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. 1, 2, 3, 5, 6, 12, 14, 16, 17 [21] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. 3 [22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. 3 [23] Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. 4, 16 [24] Zhentao He, Can Zhang, Ziheng Wu, Zhenghao Chen, Yufei Zhan, Yifan Li, Zhao Zhang, Xian Wang, and Minghui Qiu. Seeing is believing? mitigating ocr hallucinations arXiv preprint in multimodal large language models. arXiv:2506.20168, 2025. [25] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. 12, 13, 14 [26] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. 2 [27] Kwai-Kolors Kolors. Kolors. https://huggingface. co/Kwai-Kolors/Kolors, 2025. Accessed: 2025-0922. 4, 15 [28] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 17, 18 [29] Yingxin Lai, Cuijie Xu, Haitian Shi, Guoqing Yang, Xiaoning Li, Zhiming Luo, and Shaozi Li. Font-agent: Enhancing font understanding with large language models. In CVPR, pages 1967019680, 2025. [30] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flowbased grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. 12, 17 [31] Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. 5, 6 [32] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 2 [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [34] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 3, 5, 12, 13, 14, 16 [35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [36] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Lin Liang, Lijuan Wang, Ji Li, and Yuhui Yuan. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208, 2024. 2 [37] Dongliang Luo, Hanshen Zhu, Ziyang Zhang, Dingkang Liang, Xudong Xie, Yuliang Liu, and Xiang Bai. Semiets: Integrating spatial and content consistencies for semisupervised end-to-end text spotting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 93299338, 2025. 2 [38] Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, and Zhenyu Yang. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models. In AAAI, pages 59555963, 2025. 2 [39] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o, 2024. Accessed: 2025-09-22. 1 [40] OpenAI. Gpt-5. https://openai.com/gpt-5, 2025. Accessed: 2025-09-22. 2, 5, [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 1 [42] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 3 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10674 10685, 2022. 4, 15 [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 1 [45] Christoph Schuhmann. Laion-aesthetics. LAION Blog, 2022. 12, 13, [46] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 18 [47] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. 15, 18 [48] Yuxiang Tuo, Yifeng Geng, and Liefeng Bo. Anytext2: Visual text generation and editing with customizable attributes. arXiv preprint arXiv:2411.15245, 2024. 2 [49] Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, Lijuan Wang, and Min Li. Textatlas5m: large-scale dataset for dense text image generation. arXiv preprint arXiv:2502.07870, 2025. 2, 4, 5, 14, 16 [50] Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, et al. Grpo-guard: Mitigating implicit overoptimization in flow matching via regulated clipping. arXiv preprint arXiv:2510.22319, 2025. 12, 17 [51] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. 2 [52] Yibin Wang, Weizhong Zhang, Honghui Xu, and Cheng Jin. Dreamtext: High fidelity scene text synthesis. In CVPR, pages 2855528563, 2025. [53] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. 1, 2, 3, 5, 6 [54] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. 2, 4, 5, 14 10 [68] Hanshen Zhu, Zhen Zhu, Kaile Zhang, Yiming Gong, Yuliang Liu, and Xiang Bai. Training-free geometric image editing on diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1913019140, 2025. 18 [69] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 1, 5, 6, 8, 12 [55] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1, 2, 3, 4, 5, 6, 7, 12, 14, 15, 16, 17, [56] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2 [57] Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, and Kede Ma. Visualquality-r1: Reasoning-induced image quality assessment via reinforcement learning to rank. arXiv preprint arXiv:2505.14460, 2025. 2 [58] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2 [59] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards token-free future with pre-trained byte-tobyte models. Transactions of the Association for Computational Linguistics, 10:291306, 2022. 2 [60] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1, 4, 5, 6, 8, 16 [61] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36:4405044066, 2023. 2 [62] Moonbin Yim, Yoonsik Kim, Han-Cheol Cho, and Sungrae Park. Synthtiger: Synthetic text image generator towards better text recognition models. In International conference on document analysis and recognition, pages 109124. Springer, 2021. 5, [63] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, and Yu Zhou. Textctrl: Diffusion-based scene text editing with prior guidance control. NIPS, 37:138569138594, 2024. 2 [64] Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. Brush your text: Synthesize any scene text on images via diffusion model. In AAAI, pages 72157223, 2024. 2 [65] Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi Zhang, and Lianwen Jin. Aesthetics is cheap, show me the text: An empirical evaluation of state-of-the-art generative models for ocr. arXiv preprint arXiv:2507.15085, 2025. 1, 2 [66] Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, Dongyang Liu, Kai Zhang, Hongsheng Li, Yu Qiao, Peng Gao, Bin Fu, and Zhen Li. Lex-art: Rethinking text generation via scalable high-quality data synthesis. arXiv preprint arXiv:2503.21749, 2025. 2, 4, 5, 14 [67] Yiming Zhao and Zhouhui Lian. Udifftext: unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. In ECCV, pages 217233. Springer, 2024. 2 11 TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering (Supplementary Materials) A. Additional Results We provide additional visualizations of manually annotated structural anomalies from diverse generative models and synthetic structural anomalies generated by our rendering engine in Fig. 12, evaluation samples of TextPecker in Fig. 13, and qualitative comparisons between Flux.1[dev] [28] and its RL-optimized variants in Fig. 5. B. Additional Ablation Studies Table 6. Ablation study on the effectiveness of reward design: PM for Pair-wise Matching, and SQ for Structural Quality reward, measured by TextPecker (InternVL3). Generative Model OCR Model Settings GenTextEval-EN NED PM SQ Qua. Sem. for font variability, our dataset spans large and diverse font pool across training and evaluation (Tab. 14) Table 7. Robustness evaluation of TextPecker on Gemini-2.5-flashimage [11]: Performance under normal condition, extreme stylization, and low-contrast layouts. Tab. & Methods Normal Extreme Stylization Low Contrast TSAP-F1 CTR-R TSAP-F CTR-R TSAP-F1 CTR-R InternVL-3-8B [69] TextPecker-8B 0.000 0.752 0.666 0. 0.087 0.571 0.588 0.577 0.364 0.800 0.742 0.839 Table 8. Additional Quantitative Comparisons of RL-Optimized Generative Models on Chinese Visual Text Benchxmarks (OneIG [6], LongText [20], GenTextEval) with multi reward setting. O: OCR Reward [34], S: TextPecker Semantic Reward, Q: TextPecker Structural Quality reward, P: Pickscore Reward [25], A: Aethetic Reward [45]. Results measurement and reward computation are both conducted by TextPecker (InternVL-3). SD3.5-M [16] - 0.671 PP-OCRv5 [15] 0.907 PP-OCRv5 [15] 0.910 0.956 0.959 TextPecker TextPecker 0.265 0.470 0.482 0.498 0.506 method rewards weights OneIG LongText GenTextEval Qua. Sem. Qua. Sem. Qua. Sem. Qwen-Image [55] 0.888 0.747 0.900 0.815 0.922 0.805 OPA 0.898 0.788 0.912 0.845 0.944 0.859 SQPA 5:2:1:2 0.943 0.828 0.941 0.889 0.970 0. 7:1:2 We also provide additional ablation studies to deconstruct the reward function step-by-step and isolate the impact of each component, using StableDiffusion3.5-Medium [16] as the baseline, as shown in Tab. 6. Combining conventional, structure-unaware OCR model with Pairwise Matching (PM) yields 1.2% gain in semantic alignment, while structural quality sees marginal 0.3% improvementindicating PM enhances semantic feedback but minimally boosts structural fidelity without structural perception. Replacing the OCR model with TextPecker delivers gains across both dimensions (Sem. +1.6%, Qua. +4.6%), demonstrating the value of our structure-aware assessor. Finally, incorporating the structural quality term as an auxiliary reward brings further improvements (Sem. +0.8%, Qua. +0.3%) and achieves the best overall performance, confirming the synergy of the full TextPecker reward design. C. Additional Generalization Results We conduct cross-model validation on Gemini-2.5-flashimage [11] renderings to assess robustness under normal and extreme conditions (Tab. 7), and the results are consistent across these settings, with failures mainly on extremely stylized fonts where artistic deformations distort canonical glyph structure and blur the boundary between style and true structural errors (see Fig. 6, cases are all from Gemini). As D. Additional Results on RL for VTR To further validate the efficacy of TextPecker and attain more robust performance in Visual Text Rendering, we conduct additional experiments under strengthened RL baseline setting, with key design choices elaborated as follows: Backbone enhancement. We adopt recent GRPO-related techniques[30, 50] to substantially enhance the efficiency and stability of the VTR optimization process, with implementation details supplemented in Sec. G: (i) Flow-GRPO-Fast [30] is employed to accelerate training convergence by injecting stochasticity only on partial optimization steps instead of all steps; (ii) GRPO-Guard [50] is employed to stabilize the training dynamics and mitigate implicit over-optimization issues in flow matching; (iii) KL regularization enhancement (discussed in FlowGRPOs [34] GitHub issues) is introduced to further alleviate over-optimization and reward hacking problems. The original formulation is: DKL(πθ πref) = xt+t,θ xt+t,ref2 2σ2 To stabilize training dynamics and mitigate overoptimization more effectively, we redefine the KL divergence 12 Figure 5. Qualitative Comparisons of Text Rendering for Flux.1[dev] [28] and RL-Optimized Variants. Figure 6. Hard Cases of TextPecker on Gemini-Rendered [11] Visual Text with Extreme Stylization and Low Contrast Layout. propose multi-reward regularization strategy: we augment the original TextPecker reward with PickScore [25] and Aesthetic Score [45], implicitly regularizing the VTR model to yield more robust RL optimization results. We present quantitative results of TextPecker under this enhanced RL baseline in Tab. 8 and Tab. 9. Please note that all figures in the main paper and appendix are based on our original RL baseline, except for the additional visual comparisons between the two TextPecker variants provided in Fig. 7 and Fig. 8. to operate over velocity-based policy distributions: DKL(πθ πref) = vθ(xt, t) vref(xt, t)2 E. Details of Dataset where all symbols follow the definitions in the FlowGRPO [34] paper, with the core adjustment being the switch from state (x) to velocity (v) as the regularization target. Multi-Reward Regularization. In the main paper, we validated TextPeckers efficacy via experiments exclusive to text-rendering rewards. However, this single-reward setup inevitably degrades the models aesthetic and image quality performance. To yield more robust VTR optimization, we E.1. Details on Text-rich Image Generation This section supplements the text-rich image generation dataset construction details in the main text. We present detailed statistics on the number of generated images categorized by language and various generative models employed. English dataset statistics in Tab. 10 and Chinese dataset statistics shown in Tab. 11. 13 Table 9. Additional Quantitative Comparisons of RL-Optimized Generative Models on English VTR Benchmarks (OneIG [6], LongText [20], CVTG [14], GenTextEval, TIIF [54], TextAtlas [49], LeX [66]) with multi reward setting. O: OCR Reward [34], S: TextPecker Semantic Reward, Q: TextPecker Structural Quality reward, P: Pickscore Reward [25], A: Aethetic Reward [45]. Results measurement and reward computation are both conducted by TextPecker (InternVL-3). method rewards weights"
        },
        {
            "title": "LeX",
            "content": "Qua. Sem. Qua. Sem. Qua. Sem. Qua. Sem. Qua. Sem. Qua. Sem. Qua. Sem. SD3.5-M [16] 0.840 0.507 0.836 0.407 0.843 0.466 0.666 0.262 0.758 0.347 0.646 0.269 0.810 0.454 0.908 0.588 0.913 0.508 0.895 0.621 0.896 0.461 0.886 0.483 0.916 0.436 0.894 0.563 OPA SQPA 5:2:1:2 0.940 0.607 0.959 0.534 0.926 0.587 0.954 0.519 0.941 0.506 0.954 0.462 0.940 0.591 7:1:2 Flux.1[dev] [28] 0.870 0.578 0.925 0.584 0.889 0.510 0.664 0.332 0.933 0.540 0.683 0.307 0.946 0.667 OPA 0.977 0.739 0.977 0.763 0.974 0.780 0.982 0.739 0.986 0.719 0.983 0.640 0.988 0.741 SQPA 5:2:1:2 0.990 0.775 0.992 0.780 0.993 0.824 0.991 0.762 0.991 0.735 0.993 0.649 0.991 0. 7:1:2 Qwen-Image [55] 0.954 0.812 0.961 0.831 0.960 0.817 0.958 0.723 0.933 0.682 0.953 0.665 0.927 0.760 OPA 0.963 0.840 0.967 0.858 0.962 0.848 0.974 0.808 0.964 0.764 0.970 0.728 0.958 0.850 SQPA 5:2:1:2 0.983 0.888 0.982 0.891 0.976 0.889 0.990 0.876 0.975 0.800 0.982 0.746 0.968 0.883 7:1:2 Figure 7. Qualitative comparisons of text rendering results (English) among different RL baseline settings. RL-TextPecker denotes the RL setting in the main paper, and RL-SQPA refers to our enhanced RL setting as described in Sec. D. E.2. Details on Synthetic Data Augmentation As demonstrated in the main paper, models trained solely on manually annotated data generalize poorly to unseen structural anomalies. This limitation is particularly acute for Chinese characters, whose 2D structure and vast inventory ( 8,000 common characters) create combinatorial explosion of anomalies impossible to annotate exhaustively. To over14 Figure 8. Qualitative comparisons of text rendering results (Chinese) among different RL baseline settings. RL-TextPecker denotes the RL setting in the main paper, and RL-SQPA refers to our enhanced RL setting as described in Sec. D. Table 10. Statistics of English text-rich images generated by different models, labeled at box and image levels. Proportions are computed over all instances. Table 11. Statistics of Chinese text-rich images generated by different models, labeled at box and image levels. Proportions are computed over all instances."
        },
        {
            "title": "Proportion",
            "content": "AnyText [47] Flux.1[dev] [28] Qwen-Image [55] SD3 [16] SDv15 [43] SeedDream3.0 [18] Box-level Image-level Box-level Image-level Box-level Image-level Box-level Image-level Box-level Image-level Box-level Image-level 30647 7405 91705 19181 20308 2647 105725 17766 61961 6033 47921 7.38% 1.78% 22.07% 4.62% 4.89% 0.64% 25.45% 4.28% 14.91% 1.45% 11.53% 1.00% come this, we extend the SynthTIGER [62] renderer with two key enhancements: (1) image-level layout arrangements to simulate complex scenes, (2) Structural Anomaly Construction engine tailored to systematically generate diverse 15 CogView4 [13] Kolors [27] Qwen-Image [55] SeedDream3.0 [18] Box-level Image-level Box-level Image-level Box-level Image-level Box-level Image-level 36000 17005 35549 26597 6225 146395 36032 11.14% 5.26% 10.99% 5.98% 8.23% 1.93% 45.31% 11.15% structural errors in Chinese. Key parameters for our rendering engine, covering both canonical text generation and structural anomaly construction, are detailed in Table 14. Our parameter choices are guided by the goal of training precise structural perception, not robust text extraction. Consequently, to preserve clear structural features, we intentionally disabled heavy postprocessing (e.g., noise, blur) and certain style effects (e.g., extrusion), while limiting geometric transformations (e.g., skew, rotation) to moderate ranges. Notably, we have font pool of 976 types to enhance font diversity. This ensures the rendered text maintains high structural clarity amidst realistic diversity, which is critical for our training objective. E.3. Structural Anomaly Perception Test Set Table 12. Statistics of our constructed text-rich image structural perception test dataset with structural-anomaly labels at box and image levels. Proportions are computed over all instances. Data Type Level Samples Proportion Manual Annotations Synthetic Anomaly Text Synthetic Normal Text Box Image Box Image Box Image 444 417 50 50"
        },
        {
            "title": "Total",
            "content": "1061 41.85% 39.29% 4.71% 4.71% 4.71% 4.71% 100% We provide detailed statistics of the structural perception test set in Tab. 12. To further validate the fairness and effectiveness of our results, we additionally conduct evaluations on real-only test split (all synthetic samples excluded), with results presented in Tab. 13. Table 13. Performance of TextPecker on Real-only Test Splits Methods Chinese English Image Box Image Box TSAP-F1 CTR-R TSAP-F1 CTR-R TSAP-F1 CTR-R TSAP-F1 CTR-R InternVL3-8B (Baseline) TextPecker-8B (Anno) TextPecker-8B (Anno + Syn) 0.106 0.866 0. 0.955 0.849 0.917 0.244 0.906 0.955 0.791 0.815 0.995 0.183 0.874 0.850 0.759 0.938 0.931 0.304 0.809 0. 0.570 0.918 0.944 E.4. Statistics of the RL Prompt Set for VTR Figure 9. Statistics of the RL prompt set for RL-based VTR optimization (English: word-level; Chinese: character-level). 16 We present the statistics of the curated prompt set used for RL-based VTR optimization in Fig. 9. The prompt set is designed to encompass diverse text lengths and content for effective reinforcement learning. For English text rendering, we curate prompts from TextAtlas5M [49], ensuring rich and varied dataset. For Chinese text rendering, we adopt similar paradigm as described in the main paper, starting with comprehensive text corpus sampled from WanJuan1.0 [23], which covers wide range of modern Chinese common characters. Additionally, we use Qwen3-235B-A22B [60] to generate diverse style descriptions of fonts. These style descriptions are integrated with the corpus to create the final prompt set. The statistics are visualized in Fig. 9. E.5. Statistics of the GenTextEval Dataset To facilitate re-evaluation with TextPecker and build upon the strengths of existing benchmarks, we construct dataset named GenTextEval, which integrates English and Chinese prompts from multiple sources [6, 14, 20, 49]. In light of the limited availability of Chinese-rendering benchmarks [6, 20, 55] (with ChineseWord remaining unavailable for open-source at the time of our experiments), this dataset is further enriched with Chinese prompts curated as described in Sec. E.4. The final GenTextEval dataset comprises 314 prompts for English rendering and 417 prompts for Chinese rendering. Following the traditional paradigm, each prompt generates four distinct image outputs to ensure fairer assessment. We offer statistical overview of the GenTextEval dataset in Fig. 10. F. Prompt Template for TextPecker We present the prompting templates used for TextPeckers training and testing in Fig. 11. To ensure consistency and comparability across evaluations, we adopt the identical template for all other MLLM baselines. G. Additional Implementation Details This section provides further implementation details, supplementing the overview in the main paper. We employ the Flow-GRPO [34] framework for all Reinforcement Learning (RL) based VTR optimization experiments. Notably, FlowGRPO is an actively evolving repository, and the implementations reported here reflect the stable version available at the time of our experiments. As noted in the main paper, the overall methodology adheres to Flow-GRPOs core design, while the specific hyperparameters are carefully tuned for each base model (building upon the frameworks default configurations) to ensure stable and effective training. The resolution for all generated images is set to 512 512 pixels. The model-specific configurations are detailed below. SD3.5-M [16]: We use 30 sampling steps for training and Figure 10. Comparison among CVTG-2K[14], OneIG-Bench[6], LongText-Bench[20], and Our Proposed GenTextEval-Bench with Respect to the Length of Rendered Texts in English (Left) and Chinese (Right). Figure 11. The prompting template used for our TextPecker. 40 for evaluation. The noise level is set to 0.8, and the guidance scale is 1.0 (following the Flow-GRPO-Fast framework, CPS sampling and No-CFG are adopted to improve training efficiency). The KL ratio β is 0.04. For LoRA, we adopt rank of 32 and an alpha α of 64. Training is conducted using the Flow-GRPO-Fast framework. G.1. Computational cost and latency. The evaluator is used only during RL training and run as separate asynchronous service, hence it adds negligible overhead and does not affect inference latency; on SD3.5M[16], 100 RL steps take 5.52 (TextPecker) vs. 5.40 (PPOCRv5[15]). Flux.1[dev] [28]: We set 14 sampling steps for training and 28 for evaluation. The noise level is 0.9, the guidance scale is 3.5, and the KL ratio β remains 0.04. For LoRA, we use rank of 64 and an alpha α of 128. No Flow-GRPO variant is employed for this model. Qwen-Image [55]: We employ 10 sampling steps for training and 50 for evaluation. The noise level is set to 1.2, with guidance scale of 4.0 and KL ratio β of 0.004. For LoRA, we adopt rank of 64 and an alpha α of 128. Training is conducted using the Flow-GRPO-Fast framework. H. Additional Implementation Details on Sec. As mentioned in Sec. D, we conducted additional experiments utilizing an enhanced RL baseline. This baseline incorporates several advanced techniques including FlowGRPO-Fast [30], GRPO-Guard [50], Velocity KL loss, and multi-reward regularization. This section provides the specific hyperparameter details for experiments in Tab. 8 and Tab. 9. LoRA configurations remain identical to those described in Sec. D. SD3.5-M [16]: We use 40 sampling steps for both training yet impactful research direction, necessitating the integration of creative expression with principles of structure-aware textual modeling. and evaluation. An SDE window of size 12 is applied during the first half of the sampling process. The key hyperparameters are set as follows: noise level of 0.9, guidance scale of 4.5, and learning rate of 104. The ratio β for the Velocity KL loss is set to 104, and the clipping range is set to 2 106. Flux.1[dev] [28]: We set the number of sampling steps to 28 for both training and evaluation. An SDE window of size 9 is used in the first half of the sampling steps. The noise level is 0.9, the guidance scale is 3.5, and the learning rate is 104. The Velocity KL loss ratio β is configured to 104, and the clipping range is set to 2 106. Qwen-Image [55]: We employ 20 sampling steps for training and 50 for evaluation. During training, an SDE window of size 5 is applied to the initial half of the sampling steps. The noise level is set to 1.2, the guidance scale is 4, and the learning rate is 104. The Velocity KL ratio β is 103, and the clipping range is set to 2 105. I. Limitations TextPecker paves novel path for addressing the core bottleneck in VTR evaluation and RL-based optimization, leveraging structural-anomaly-aware RL reward that delivers complementary signals for semantic alignment and structural quality. While providing foundational step towards structurally faithful VTR, our work still has several limitations that point to meaningful directions to be explored. First, our structural anomaly synthesis is contingent upon the availability of stroke-level font data. This dependency currently restricts its application to standard fonts, precluding the generation of anomalies in artistic or proprietary typefaces lacking such data. Second, our work is currently confined to Chinese and English text rendering, with efficient multilingual extension as key area for future exploration. Third, our TextPecker evaluator is equipped with boxlevel perception ability, which theoretically enables it to support downstream VTR-related tasks such as text translation and local text editing, which are often challenging for general editing methods [10, 28, 46, 47, 68]. Validating the effectiveness of evaluation and RL optimization on these downstream tasks is left for future work. Fourth, challenges arise in handling artistic text generation (see Fig. 6 above), which is an increasingly demanded scenario. Artistic text often involves deliberate modifications to standard structures, such as connected strokes, added symbols, or pictorial variations, making it inherently difficult to define single standard or ground truth. Furthermore, artistic designs are continuously evolving, presenting moving target that conflicts with the structural consistency objectives of our current framework. Addressing the evaluation and optimization of artistic text generation remains challenging 18 Table 14. Key parameters for canonical text rendering and structural anomaly construction. Parameter Category Canonical Chinese Text Canonical English Text Structural Anomaly Construction Basic Text Configuration Vertical text probability Number of elements per sample Text length range Font Settings Number of font types Font size range Layout Parameters Horizontal spacing between elements Vertical line spacing Length ratio range Random offset probability Random offset range Image margin Flow layout probability Curve layout probability Style Effects Style application probability - Text border (probability) Size ratio Alpha - Text shadow (probability) - Text extrusion (probability) Geometric Transformation Transformation application probability - Perspective (weight) Percents - Perspective (weight) Percents - Trapezoidate (weight) Percent - Trapezoidate (weight) Percent - Skew (weight)"
        },
        {
            "title": "Angle",
            "content": "- Skew (weight)"
        },
        {
            "title": "Angle",
            "content": "- Rotate (weight)"
        },
        {
            "title": "Angle",
            "content": "Structural Anomaly Generation Anomaly generation probability - Deletion (probability) - Insertion (probability) - Swapping (probability) 10% 310 125 characters 10% 310 325 characters 10% 310 125 characters 976 50100 pt 50200 px 1020 px 0.81.0 20% 1030 px 15 px 80% 20% 25% 100% 515% 1.0 0% 0% 50% 1 0.8 1 0.81 1 0.81 1 0.81 2 030 2 010 3 010 50% 40% 40% 40% 976 50100 pt 50200 px 1020 px 0.81.0 20% 1030 px 15 px 80% 20% 25% 100% 515% 1.0 0% 0% 50% 1 0.8 1 0.81 1 0.81 1 0.81 2 030 2 010 3 010 0% 976 50100 pt 50200 px 1020 px 0.81.0 20% 1030 px 15 px 80% 20% 25% 100% 515% 1.0 0% 0% 50% 1 0.8 1 0.81 1 0.81 1 0.81 2 030 2 010 3 010 0% 19 Figure 12. This figure shows manually annotated structural anomalies (box-level and image-level) from various generative models, alongside synthetic structural anomalies generated by our rendering engine for data augmentation. 20 Figure 13. This figure presents evaluation samples of TextPecker, showcasing its performance in detecting structural anomalies across diverse text rendering scenarios. ω = 1 for structural quality score visualization."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Huazhong University of Science and Technology"
    ]
}