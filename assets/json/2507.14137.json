{
    "paper_title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
    "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 7 3 1 4 1 . 7 0 5 2 : r Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning Shashanka Venkataramanan1 Valentinos Pariza2 Mohammadreza Salehi2,3 Lukas Knobel2 Spyros Gidaris Elias Ramzi1 Andrei Bursuc1 Yuki M. Asano2 1valeo.ai, Paris. 2 Fundamental AI Lab, UTN. 3 VIS Lab, UvA. Figure 1: Overview of Franca. Top-left: We learn efficient Matryoshka-style [Kusupati et al., 2022] visual representations using multi-head clustering projection head. The encoder produces features Rd, which is sliced into progressively smaller subsets of dimensions d, . . . d/8, d/16. Each slice passes through projection head and corresponding clustering head with cluster counts c, . . . , c/8, c/16, inducing coarse-to-fine hierarchy of semantic abstraction. Top-right: Unlike prior approaches trained on curated academic datasets, e.g., LVD-142M in DINOv2 or proprietary data like WebLI in SigLIPv2, Franca is trained on open-source internet-scale uncurated data. Bottom: Despite this, it generalizes well across model scales and achieves strong performance on diverse downstream tasks, including in-context learning [Balazevic et al., 2023], out-of-distribution detection [Yang et al., 2022], and 3D understanding [Chen et al., 2025]. equal contribution. Correspondence: shashanka.venkataramanan@valeo.com equal last authors."
        },
        {
            "title": "Abstract",
            "content": "We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matchesand in many cases surpassesthe performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in transparent training pipeline inspired from Web-SSL and uses publicly available data: Imagenet-21K and subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish new standard for transparent, high-performance vision models and open path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca"
        },
        {
            "title": "Introduction",
            "content": "The development of strong and robust large vision models has emerged as critical axis in artificial intelligence research, with implications extending far beyond traditional vision tasks. These foundation models serve as essential building blocks for complex 3D systems and multimodal large language models (LLMs). Self-supervised learning (SSL) approaches have gained prominence due to their scalability advantage; the volume of available image-only data vastly exceeds that of paired image-caption data, enabling more comprehensive training regimes and more generalizable representations. Despite the demonstrated importance of these vision foundation models, there exists striking scarcity of publicly available high-performance options. The current state-of-the-art models, including DINOv2 [Oquab et al., 2024], SEER [Goyal et al., 2021], billion-scale MAE [Singh et al., 2023], SigLIPv2 [Tschannen et al., 2025], rely exclusively on proprietary datasets, creating significant barrier to reproducibility, accessibility, and scientific advancement. To address this gap, our primary contribution is an open-data, open-weight and open-code vision foundation model inspired from Web-SSL [Fan et al., 2025] that not only matches but surpasses the performance of these proprietary counterparts while maintaining complete transparency in implementation and training methodology. Moreover, we release intermediate checkpoints alongside our final model to provide insight into the full training trajectory. This enables the community to analyze convergence behavior, conduct representation analysis, and study emergent properties across time. From Table 1, Franca stands out as the first model to make all aspectstraining code, data, model weights, and intermediate checkpointspublicly available, setting new benchmark for openness in the field. Furthermore, we identify fundamental limitation in current SSL vision models that utilize clustering approaches, such as DINO and DINOv2. These methods assign images or patches to meaningful pseudo-labels according to clustering metrics (typically Sinkhorn-Knopp optimization), but they overlook critical issue: clustering is inherently ambiguous. For example, vehicles can be meaningfully organized by manufacturer, color, model year, or numerous other attributes. Rather than addressing this ambiguity, existing approaches simply employ extremely large fine-grained cluster sets (e.g., DINOv2s 131K codebook [Oquab et al., 2024]), which may be suitable for certain domains but inappropriate for others. To overcome this limitation, we draw inspiration from early deep clustering works which demonstrated that employing multiple clustering heads can enhance performance ([Ji et al., 2019, Asano et al., 2020]). However, naively extending this approach to modern architectures with MLP projection heads (rather than simple linear heads) would result in an explosion of parameters. Our solution 2 ATTRIBUTE / MODEL METACLIP WEB-SSL SIGLIPV2 AIMV2 CLIP DINOV2 SAM OPENCLIP Franca Model Publicly Available? Training Code Public? Intermediate Weights Public? Training Data Public? Data Deduplication Code Public? Data NSFW & CSAM Filtered? Training Code/ Checkpoints Training Data ? ? ? ? Table 1: Openness of Visual Foundation Models. We analyze various models based on the public availability of their components. Notably, Franca exemplifies fully open-source approach, providing complete transparency from model weights to underlying data and processing methods. : partially; ?: not specified. is novel multi-head clustering projector that implements nested Matryoshka [Kusupati et al., 2022] representations, where progressive subsets of neurons cluster the dataset into increasingly finer-grained groupings. This approach not only reduces parameters compared to conventional approaches but also improves performance and decreases memory requirements for downstream tasks such as k-nearest neighbors classification, leading to higher performances at equal memory. Finally, we address subtle but significant issue with dense clustering methods: they often produce clusters that reflect image position rather than semantic content. To mitigate this problem, we introduce lightweight post-pretraining technique that first learns linear projections to predict patch positions, then projects the latent space to an orthogonal, of the positional information. The result is dense representation space that emphasizes semantic content over spatial positioning, leading to substantial improvements on challenging benchmarks such as In-context learning with HummingBird benchmark [Balazevic et al., 2023] and OverClustering [Pariza et al., 2025] on Pascal-VOC [Everingham et al.] and COCO-Things [Caesar et al., 2018]. Overall, our key contributions are: We present Franca: the first open-source open-data vision foundation model trained on publicly available data that matches and even outperforms DINOv2 on broad range of downstream tasks. We introduce parameter-efficient nested multi-head clustering approach that efficiently improves representation quality. We develop spatial-semantic disentanglement post-pretraining method that refines representations to yield stronger backbones. We achieve strong results on dense prediction tasksincluding in-context learning, linear segmentation, and overclusteringoutperforming DINOv2-G by up to 3%. We also surpass DINOv2-G by 4% on average across five OOD detection benchmarks, improve 3D understanding, and match its performance on classification, all without relying on proprietary data."
        },
        {
            "title": "2 Related Work",
            "content": "Our work builds on and contributes to four major areas of prior research: self-supervised learning for visual representation learning, scaling strategies for data and model capacity in vision models, open-source foundation models, and techniques for disentangling semantic content from positional or representational biases. Self-Supervised Learning (SSL) for Vision. Self-supervised learning has emerged as powerful paradigm for visual representation learning without any manual annotations. By designing pretext tasks that utilize image structure as supervision signals, SSL methods enable models to learn transferable features. Early approaches used handcrafted objectives such as context prediction [Doersch et al., 2015], patch reordering [Noroozi and Favaro, 2016], colorization [Zhang et al., 2016, 2017], inpainting [Pathak et al., 2016], geometric transformation prediction [Gidaris and Ko3 modakis, 2018], and instance discrimination [Dosovitskiy et al., 2014, Wu et al., 2018]. Modern SSL methods primarily focus on learning invariances across augmented data views. While early approaches leverage contrastive learning [Oord et al., 2018, Misra and Maaten, 2020, Chen et al., 2020a, He et al., 2020, Chen et al., 2020b, 2021] by aligning positive pairs and separating negatives, bootstrap-based [Grill et al., 2020, Chen and He, 2021, Gidaris et al., 2021] and distillation-based methods [Caron et al., 2021, Oquab et al., 2024] refine targets through teacher-student networks, often removing the need for negative pairs. More recently, Masked Image Modeling (MIM) has emerged as dominant SSL strategy, where models learn to reconstruct masked patches [He et al., 2022, Zhou et al., 2022a, Bao et al., 2022, Wei et al., 2022]. Beyond these, clustering-based methods [Caron et al., 2018, 2020, Ji et al., 2019] have gained prominence, assigning pseudo-labels through algorithms like K-means or Sinkhorn-Knopp. The combination of MIM with clustering has shown particular promise, as exemplified by recent works such as MOCA [Gidaris et al., 2024] and CAPI [Darcet et al., 2025]. This hybrid approach leverages the strengths of both paradigms. While current vision foundation models often fall into categories like vision-language or MAE-like architectures, which have their own strengths and limitations (e.g., reliance on text supervision or need for task-specific adaptations), DINOv2 [Oquab et al., 2024] stands out as powerful pretrained model employing clustering-based approach. However, DINOv2 has two key limitations: as clustering method, it doesnt inherently capture the ambiguity often present in assignments at fixed granularity, nor does it explicitly incorporate the benefits of modern hierarchical masking strategies. Our work addresses these concerns by integrating nested Matryoshka projections [Kusupati et al., 2022] directly into its objective. This allows each subspace to perform clustering at different granularity, yielding diverse pseudo-labels efficiently (see Figure 1). Combined with an improved input masking strategy, our approach enables the joint learning of coarse-to-fine semantics without increasing model size, leading to strongly improved performances and reduction in memory. Open Foundation Vision Models. The reliance on proprietary datasets in the training of current vision foundation models raises critical concerns regarding transparency, reproducibility, and the disentanglement of contributions. Models such as SEER [Goyal et al., 2019], DINOv2 [Oquab et al., 2024], CLIP [Radford et al., 2021], and billion-scale MAE [Singh et al., 2023] are all trained on proprietary data. This practice makes it challenging for the research community to isolate the true impact of models novelty and training strategies from the unique characteristics and biases of the datasets themselves. The lack of access to these datasets hinders independent verification, fair comparison, and comprehensive understanding of what truly drives model performance. WebSSL [Fan et al., 2025] extends the study of large-scale self-supervised pretraining by training models on publicly available MetaCLIP-2B [Xu et al., 2024] dataset, showing that models trained on open data can approach the performance of those trained on proprietary data on VLM tasks. Building on this, we present fully open-source vision foundation model using publicly available datasets, ReLAION [LAION, 2024], as it represents the most popular and safe public dataset for large-scale vision model training. Spatial correlations in learned representations. common issue in dense self-supervised learning is the entanglement of semantic content with positional cues, causing models to rely on location rather than object identity. For instance, model trained on cows in grassy fields and camels in deserts may misclassify cow on beach as camel, due to learned associations with background context [Arjovsky et al., 2019]. Such spatial biases reduce generalization and can hinder performance when objects appear in atypical locations (e.g., cow in the sky) [Singh et al., 2020]. Several works have addressed this by proposing methods invariant to positional information. Lenc and Vedaldi [2015] enforce equivariance to geometric transformations; Wang et al. [2023] disentangle representations into orthogonal subspaces for content and style. Invariant Risk Minimization [Arjovsky et al., 2019] seeks features stable across environments, minimizing reliance on spurious cues. We propose simple post-training strategy that learns linear projection to identify and remove spatial information from features. Since, we use it as post-training strategy, it requires no architectural changes and can be easily adapted to any pretrained model to reduce spatial bias."
        },
        {
            "title": "3 Method",
            "content": "We propose Franca, scalable open-source self-supervised learning framework built on iBoT [Zhou et al., 2022b] and pretrained on large public image datasets. Franca tackles key limitations in existing vision SSL models through three main components. First, we use CyclicMask inspired from [Darcet et al., 2024], masking strategy that circularly shifts masked patches to break simple spatial continuity and promote the learning of semantic features. Second, we introduce Matryoshka embeddings [Kusupati et al., 2022], nested multi-head clustering approach that shares projection layers to generate compressed multi-resolution representations; and finally, lightweight post-pretraining step that identifies and removes feature components correlated with absolute patch positions resulting in spatially invariant representations. Figure 2 shows that each of these components provides consistent gains in both in-context segmentation and linear classification performance on standard benchmarks."
        },
        {
            "title": "An input",
            "content": "We follow the multi-crop training strategy from DINO [Caron et al., image 2021]. Rhwc is transformed into multiple augmented views (global and local crops). Each view is split into = hw non-overlapping patches p2 of size p. Each patch is embedded into Rd, and classification token ([CLS]) Rd is prepended to form the input sequence. Vision Transformer (ViT) backbone [Dosovitskiy et al., 2021] processes this sequence, producing + 1 embeddings (n patch embeddings and one [CLS] embedding). The same ViT is shared between the student fθ and teacher fθ, producing Zs = fθ(x) R(n+1)d, Zt = fθ(x) R(n+1)d, where Zs represents the students output embeddings and Zt represents the teachers output embeddings. The teachers parameters θ are updated via exponential moving average (EMA) of the students parameters. For supervision, we apply projection heads to the student embeddings Zs. The [CLS] embedding is passed through DINO-style head (a 3-layer MLP with softmax over prototypes) that produces image-level prototype scores, while the patch embeddings are processed by an iBOT-style head that produces patch-level prototype scores. For brevity, we denote both heads as hθ for the student and hθ for the teacher (same architecture, EMA-updated). The teachers projected outputs are clustered using Sinkhorn-Knopp [Cuturi, 2013] to produce balanced target distributions. The student is trained to match these targets via cross-entropy loss, denoted as L. Figure 2: Pretraining ablation of Franca. Starting from ViT-B/14 pretrained on ImageNet-21K, we show the impact of each proposed components. The inner bar represents in-context segmentation performance on the Hummingbird benchmark [Balazevic et al., 2023], while the outer bar shows linear probing accuracy on the ImageNet-1K [Russakovsky et al., 2015]. Each addition, i.e., CyclicMask, Matryoshka representations, RASA, and High resolution finetuning, results in consistent improvements."
        },
        {
            "title": "3.1 Matryoshka Representations for Efficient Multi-Granular Learning",
            "content": "Standard self-supervised models learn to produce fixed-size embeddings, limiting their versatility with respect to varying computational budgets or downstream constraints. To enable flexible and semantically meaningful representations across multiple feature granularities, we use Matryoshka representations [Kusupati et al., 2022], which nest progressively truncated subspaces of highdimensional embedding. The standard Matryoshka approach slices the encoders output along the feature dimension and applies the same projection head to each sub-embedding. In contrast, we extend this setup by attaching dedicated projection head and clustering objective to each subspace. This allows each slice to produce distinct prototypes and prototype assignments, encouraging specialization across representational granularities. Additionally, we reduce the number of prototypes per head proportionally to the subspace sizeyielding form of hierarchical clustering that aligns naturally with the granularity of the features across training steps. 5 Formally, let Zs = fθ(x) R(n+1)d be the ViTs output (patch + [CLS] embeddings). We define nested dimensions = {m1, . . . , mk}, where m1 < < mk = d, and extract sub-embeddings (i) = Zs[:, 1 : mi] mi is processed by an independent projection head h(i) Each (i) θ , with proportionally fewer prototypes as mi decreases. cross-entropy loss L(i) is applied to each heads output. The total loss is the sum across all levels with equal weights: Ltotal = (cid:88) i=1 L(i). The ViT encoder is shared across all levels, enabling simultaneous training of multi-granular representations that vary in capacity but remain semantically aligned. Our framework supports hierarchical learning: coarse heads capture global semantics, while fine heads focus on local structure akin to early clustering works [Ji et al., 2019, Asano et al., 2020, Van Gansbeke et al., 2020] and unlike most recent representation learning works that optimize only single feature space [Oquab et al., 2024, Tschannen et al., 2025, Radford et al., 2021]. As shown in Figure 3, Franca significantly outperforms DINOv2 [Oquab et al., 2024] across all embedding sizes, especially under heavy compression. For fairness, we note that DINOv2 was not trained for dimensional truncation; its information is spread uniformly across the feature space. Even more notably, Figure 2 shows that the largest gains (4%) from our Matryoshka framework occur in dense prediction tasks. Figure 3: k-NN classification accuracy on ImageNet-v2 at varying embedding slice levels using ViT-L backbone. Franca consistently outperforms DINOv2 across all subspace dimensions, maintaining high performance even under strong compression (dim/64). Note that DINOv2 was not trained with sliced dimensions and its features are uniformly distributed across the full embedding space. Figure 4: Masking strategies used in masked image modeling. Compared to Random (a), Block (b), and Inverse (c) masking, our CyclicMask (d) circularly shifts the visible region across spatial axes, preventing the model from being biased toward specific spatial locations."
        },
        {
            "title": "3.2 Balancing Spatial Distribution of Visible Patches with CyclicMask",
            "content": "Masked image modeling (MIM) is core component in many self-supervised vision frameworks [Oquab et al., 2024, Zhou et al., 2022b,c], where portion of input patches are masked and the model learns to predict the unmasked regions. Commonly adapted strategies include random masking and block masking, where patches are masked randomly or as block across the image. While simple and stochastic, this approach lacks spatial structure, often leading to fragmented visible regions that provide limited contextual coherence as shown in Figure 4 (a) and (b). 6 To improve context continuity, inverse block masking [Baevski et al., 2023] retains contiguous visible region, typically fixed central block, while masking the periphery. This design offers structured context and improves prediction stability. From Figure 4 (c), we observe that the visible region is always at the center, which introduces strong imbalance in the spatial distribution of the processed patches: certain patches are consistently unmasked (e.g., center), while others are rarely observed, leading to uneven training exposure across spatial locations. Inspired from [Darcet et al., 2025], we introduce CyclicMask, simple modification that mitigates this imbalance by circularly shifting the mask across both spatial axes during training. Specifically, the mask from inverse block masking is shifted randomly along the vertical and horizontal axes using wraparound (cyclic) shifts. This retains the benefit of contiguous visible region while ensuring uniform exposure across all patch positions over time. As shown in Figure 4 (d), CyclicMask preserves structured context and, unlike fixed block masking, it avoids deterministic exposure patterns. While conceptually simple, this adjustment leads to more spatially balanced learning signals and eliminates positional bias. From Figure 2, CyclicMask improves performance of the baseline for linear probing on Imagenet-1K (IN-1K) and for in-context learning using open-hummingbird evaluation [Pariza et al., 2024] by 0.2%."
        },
        {
            "title": "3.3 RASA: Removal of Absolute Spatial Attributes",
            "content": "While ViTs have achieved strong performance across variety of visual tasks, they often retain unintended spatial biases, particularly in self-supervised settings where no explicit location labels are used. These biases can emerge due to fixed patch layouts and learned positional embeddings, which may entangle spatial cues into representations that ideally should be semantic and positioninvariant. Figure 5: Entropy of patch locations for each cluster. For each visual cluster predicted from the projection head on the patch embeddings, we compute the entropy of the 2D spatial coordinates of the patches assigned to it. low entropy value indicates that the cluster consistently activates mostly at specific spatial positions (e.g., always top left patch), revealing positional bias in the representation. Left: We find several position-agnostic low-entropy clusters in DINOv2, that fire at specific image locations. Right: Compared to DINOv2, Franca produces clusters with significantly higher spatial entropy, demonstrating more uniform and position-invariant distribution of patch activations across the image. This supports the effectiveness of RASA in removing linear positional components from the representation space. Preliminary study We first study the presence of spatial biases in the projection head that clusters the patches. To this end, we run the frozen model across images from Pascal VOC and COCO and keep track of the argmax cluster predictions for each patch location. Next, we compute the entropy across patch locations for each cluster, shown in Fig. 5. Notably, we find several semantically agnostic clusters, that instead of activating for certain content, only fire at specific image locations (see Fig. 5, left). More generally, we find that clusters tend to mix semantics and spatial position (e.g., cluster for car tire on the right vs another cluster for the same on the left of the image), indicating that even DINOv2 contains undesirable mixing of content and geometry. In contrast, Franca and RASA significantly increase the entropy of patch locations for almost all clusters, indicating better separation of content and geometry. 7 To address this, we propose Removal of Absolute Spatial Attributes (RASA), post-training strategy to disentangle position information from patch embeddings. Once our model is pretrained, we use patch features Zs to suppress the dimensions of the embedding space that are predictive of patch location. Our approach follows an alternating optimization procedure: at each iteration t, we learn simple 2D regression head (t) pos to predict the normalized 2D grid coordinates of each patch using sigmoid activation. This head is trained on the patch embeddings (t1) to minimize mean squared error between the predicted and actual normalized 2D positions. Once trained, the direction vectors in feature space used to predict the positional components are extracted and orthogonalized via GramSchmidt [Golub and Van Loan, 2013] to form 2D subspace that encodes spatial layout. Finally, we obtain new representation for each patch that is less aligned with positional information by simply removing the components parallel to this plane and keeping only the orthogonal ones. Formally, given Zs = {Zh,w RD}n s=1, where is the number of patches in an image, we optimize the position prediction head parametrized by on small set of images: (cid:1), (cid:98)ys = σ(cid:0)W Zs 1 (cid:88) (cid:13) (cid:13)(cid:98)ys ys Lpos = s= (cid:13) 2 2, (cid:13) where σ is sigmoid function, R2D and ys denotes the ground-truth normalized 2D grid coordinate for patch n. (0) (1) uc ur ps Positional Plane After training, the two row-vectors wr, wc of are orthonormalized to produce ur, uc, spanning the positional plane (in gray in Figure 6. To obtain the component of the feature vectors that are orthogonal to this positional component, we project Zs onto this subspace and subtract the projection: Each iteration of RASA projects Figure 6: patch embedding Zs onto learned positional plane span{ur, uc} and subtracts its projection ps. ps = Zs, urur + Zs, ucuc, p(t) = (t1) . (t) (1) (2) This is an iterative refinement where at each iteration t, the remaining positional plane ps is used to generate the next subspace Zs. This iterative process is visually summarized in Figure 6. We run this process multiple times until the positional loss Lpos stops decreasing, indicating positional bias is abundant in the feature space. Empirically, we find that small number of iterations (e.g., = 9) suffices to remove the linear positional bias while preserving the core semantics of the learned representation. Integrating RASA into the last linear layer As each iteration of RASA in Eq. 2 can be parameterised as linear matrix-multiply with L(t) as follows, (t) = (t1) (3) the final transformation remains again linear matrix multiplication = (cid:81) L(t), which can be multiplied into the weights of the final ViT layer, leading to no architectures changes. ), ) = (t1) L(t) = (t1) + ucu (I uru (I p(t) s"
        },
        {
            "title": "Implementation details",
            "content": "Pretraining Datasets We pretrain Franca on large-scale, publicly available image-only datasets to ensure full reproducibility. We use the ImageNet-21K [Ridnik et al., 2021], which contains approximately 13.1M high-quality, hierarchically labeled images across 21,841 classes. This dataset 8 offers broad visual coverage and is widely used in foundation model pretraining. To further scale up training and improve generalization, we also leverage LAION-600M3, subset of ReLAION-2B, which is research-safe version of the LAION-5B dataset [Schuhmann et al., 2022, LAION, 2024]. While LAION-5B is originally paired image-text data, we discard the text and use only the image modality. Training Francas architecture follows DINOv2 [Oquab et al., 2024] without registers, using Vision Transformers [Dosovitskiy et al., 2021] of varying model capacities: ViT-B with 86M parameters, ViT-L with 300M, and ViT-G with 1.1B. All models are trained from scratch for 500 epochs (625K iterations) without distillation from larger models, unlike DINOv2, which distills from ViT-G into smaller variants. We employ Matryoshka [Kusupati et al., 2022] with five nested heads on the ViT backbone with feature dimension [d, 2 , . . . , 16 ]. For LAION-600M, we use global crops scale of [0.48, 1.0], following DINOv2-style augmentations. Stochastic depth regularization is set to 0.1 for ViT-B and 0.4 for ViT-L and ViT-G. We use total batch size of 2048 for ViT-B and 3072 for both ViT-L and ViT-G, distributed across 32, 64 and 128 H100 GPUs for ViT-B, ViT-L and ViT-G respectively. The learning rate is set to 1 103 for the Base model and 3.5 104 for the Large and Giant variants, using cosine schedule with warmup of 80 epochs. We train RASA on top of our frozen backbone on Pascal VOC using crops of resolution 518 518, batch size of 128, with AdamW [Loshchilov and Hutter, 2019] optimizer. For each of the nine incremental head training iterations, we used two-layer positional projection head with sigmoid activation. In every iteration, only the top head was trained for 5 epochs with no weight decay and an initial learning rate of 2 104, while all heads from previous iterations were kept frozen. High-resolution adaptation We initialize the model with pretrained weights and finetune it at input resolution 518 518 for 20K iterations using the same training procedure as in the initial pretraining phase. All schedules are preserved but temporally compressed to fit within the shorter training horizon. We retain the same hyperparameters as the original run, except for the base learning rate, which is reduced to 1.51 105, following [Fan et al., 2025]. The teacher network undergoes warmup phase during the first 10K iterations to stabilize early training dynamics. Due to computational constraints, high-resolution finetuning is performed only for the Base model."
        },
        {
            "title": "5 Experimental Results",
            "content": "5."
        },
        {
            "title": "Image Classification",
            "content": "Probing self-attention We compare self-attention maps from the final layers [CLS] token of DINOv2, DINOv2R (with registers) [Darcet et al., 2024] and Franca in Figure 7. DINOv2 often fails to localize objectsespecially under clutter or occlusionwhile DINOv2R offers only minor improvements. In contrast, Franca yields sharply focused attention maps aligned with object boundaries, even for small or partially occluded instances. This suggests that our Matryoshka-style multi-head clustering promotes semantically rich features and finer-grained representations. Linear probing We begin by evaluating the global image representations learned by Franca via linear probing on ImageNet-1K [Russakovsky et al., 2015]. linear classifier is trained on top of frozen backbone and evaluated on the standard validation set, as well as on ImageNet-ReaL [Beyer et al., 2020] and ImageNet-v2 [Recht et al., 2019] to assess generalization (see Table 2). Despite being trained on only 13M images (IN-21K) and without distillation, Franca achieves performance comparable to DINOv2 and outperforms Web-SSL [Fan et al., 2025], which is trained on significantly larger dataset. Notably, our Franca-G model matches the linear probing accuracy of Web-SSL-7B (85.9% vs. 86.0%) while using 150 less data and nearly 7 fewer parameters. To ensure fair comparison, in Table 2 we report results for DINOv2 models (ViT-B/14 and ViTL/14) re-trained from scratch on IN-21K using the same training setup as Franca, but without distillation. These models perform significantly worse than the original DINOv2 (with distillation), 3https://huggingface.co/datasets/laion/laion-coco 9 Figure 7: Self-attention maps utilizing 14 14 patches. These maps are visualized using the [CLS] token on the last layers heads on the validation set of ImageNet-1K [Russakovsky et al., 2015]. Franca has better localization than DINOv2 with registers [Darcet et al., 2024] without requiring the use of registers, where the nested Matryoshka clustering captures fine-grained details e.g. feathers, beaks of bird. Figure 8: Out-of-Distribution Detection across five robustness-benchmarks: SSB-Hard [Vaze et al., 2022], NINCO [Bitterwolf et al., 2023], iNaturalist [Huang and Li, 2021], OpenImage-O [Wang et al., 2022a], and Texture [Kylberg, 2011]. Franca consistently outperforms DINOv2, at larger scales, demonstrating its robustness across distribution shifts. DINOv2-B and DINOv2-L are distilled from DINOv2-G and trained on LVD 142M, while neither variants of Franca are distilled. We report AuROC; higher is better. showing that much of their strength comes from the use of distillation. In contrast, Franca achieves strong results without relying on any distillation or extra supervision. Among text-supervised models, SigLIPv2 achieves state-of-the-art results in both classification and robustness metrics, aided by large-scale web supervision and strong pretraining data alignment. In comparison, our LAION-600M variant of Franca performs slightly worse, which we attribute to the domain gap between the pretraining distribution and the ImageNet evaluation benchmarks. Nonetheless, Franca remains competitive across ViT-B, ViT-L, and ViT-G backbones, providing robust alternative to distillation-heavy or multimodal approaches with strong data efficiency. Robustness We evaluate the generalization ability of Franca under natural distribution shifts using linear probing on ImageNet-A [Hendrycks et al., 2021a], ImageNet-R [Hendrycks et al., 2021b], and Sketch [Wang et al., 2019], which retain the original label space but introduce semantic or stylistic variations. As shown in Table 2, Franca demonstrates strong robustness. For instance, FrancaG matches the performance of DINOv2-G across all three datasets and outperforms OpenCLIP-G by 11.5% on ImageNet-A, despite OpenCLIP being trained on over 30 more data. 10 CLASSIFICATION ROBUSTNESS ARCH. DATA TEXT KNN IN-VAL REAL V2 IN-A IN-R Sketch METHOD IBoT DINOv2 DINOv2 ViT-B/16 ViT-B/16 ViT-B/16 IN-21K IN-21K LVD-142M Franca (ours) ViT-B/14 IN-21K SigLIP SigLIP2 PEcore Web-SSL DINOv2 DINOv2 ViT-L/16 WebLI ViT-L/16 WebLI MC-2B ViT-L/16 MC-2B ViT-L/16 IN-21K ViT-L/14 LVD-142M ViT-L/14 Franca (ours) Franca (ours) ViT-L/14 ViT-L/14 IN-21K LAION-600M OpenCLIP DINOv2 Web-SSL ViT-G/14 ViT-G/14 ViT-G/14 MC-2B LAION-2B LVD-142M Franca (ours) Franca (ours) ViT-G/14 ViT-G/14 IN-21K LAION-600M 77.1 77.6 82.1 80.9 83.5 76.8 82.1 83. 82.6 82.0 83.2 83.1 79.2 83.1 82.8 79.5 81.2 84.5 82.0 80.5 82.5 83.9 82.4 84.0 86. 84.2 83.8 86.2 86.0 84.7 85.9 85.0 84.5 88.3 85.6 85.9 87.3 84.5 86.8 89. 88.3 87.0 89.4 89.0 86.8 88.7 87.0 70.3 75.1 72.8 74.2 76.8 77.9 71.0 73.9 78. 77.4 76.5 77.2 77.9 74.3 77.8 77.2 52.2 55.1 54.8 76.5 84.3 89.0 67.3 70.5 71. 71.0 71.3 63.8 75.9 73.3 75.3 74.8 59.9 63.3 63.2 95.0 95.7 95.2 68.9 70.9 74. 72.8 72.0 87.8 78.8 75.9 78.6 78.1 48.0 50.6 48.7 73.6 75.5 73.4 54.8 55.2 59. 58.4 56.9 66.4 62.5 60.9 62.2 62.2 Table 2: Classification and Robustness. performance across vision-language and vision-only models. We report top-1 linear probing accuracy on IN-1K (val, ReaL, v2) and robustness benchmarks (IN-A, IN-R, Sketch). Franca, trained without text supervision, matches or exceeds the performance of larger text-supervised models and outperforms DINOv2 when reproduced on the same data and training strategy. : reproduced on IN-21K without distillation; : distilled from DINOv2G on LVD-142M. Bold: Best; Underline: second best. We further assess Franca on out-of-distribution (OOD) detection using the OpenOOD benchmark [Zhang et al., 2024], across five datasets: SSB-Hard [Vaze et al., 2022], NINCO [Bitterwolf et al., 2023], iNaturalist [Huang and Li, 2021], OpenImage-O [Wang et al., 2022a], and Texture [Kylberg, 2011]. We report Area under the ROC curve (AuROC) to measure the ability to distinguish in-distribution from OOD inputs. As shown in Figure 8, Franca consistently outperforms DINOv2 across large and giant model variants, showing both strong robustness to distribution shifts and effective scaling for OOD detection. Method MAE DINOv2 DINOv2 Web-SSL DINOv2 Web-SSL OpenCLIP Franca (ours) Franca (ours) Franca (ours) Arch Food ViT-H/14 ViT-L/14 ViT-L/14 ViT-L/ ViT-G/14 ViT-G/14 ViT-G/14 ViT-B/14 ViT-L/14 ViT-G/14 78.4 93.4 94.3 91.0 94.7 94.1 94.5 91.6 94.3 95.0 96.1 99.2 99.3 98.9 99.5 99.4 98.7 99.0 99.0 99.5 C100 SUN Cars Aircr DTD 83.9 93.9 93.4 90.7 94.4 93.1 91.0 92.0 93.7 95.1 63.9 78.1 78.7 77. 78.7 78.0 84.0 77.0 78.9 78.9 56.1 89.9 89.9 88.9 91.4 90.3 96.1 88.7 89.5 91.3 63.4 81.7 81.5 80. 87.2 83.7 80.2 79.3 81.3 85.5 75.4 82.9 84.0 83.6 84.5 84.7 86.0 82.2 83.8 85.0 Pets 89.4 95.2 96.5 93.1 96.7 92.4 95.7 93.8 96.8 97.2 Cal Flwrs CUB 95.9 87.2 97.5 95.1 97.6 96.8 98.1 96.2 97.4 97.5 92.3 99.6 99.7 98.8 99.7 99.4 99.5 99.7 99.5 99. 57.2 90.3 90.5 90.9 91.6 91.2 89.9 88.3 90.9 91.3 Avg 77.5 90.1 91.4 89.9 92.3 91.2 92. 89.8 91.4 92.3 Table 3: Linear evaluation of frozen features on fine-grained datasets. top-1 accuracy measured across 11 benchmarks across objects, scenes, and textures, following [Chen et al., 2020a]; : reproduced on IN-21K without distillation; : distilled from DINOv2-G on LVD-142M. Fine-grained classification We evaluate the transferability of the learned representations on 11 classification benchmarks introduced in SimCLR [Chen et al., 2020a]. These benchmarks cover variety of tasks, including scene recognition, fine-grained object classification (such as food, cars, and aircraft), and texture recognition. Following [Oquab et al., 2024], we train logistic regression classifier on features extracted from frozen backbone. This approach focuses solely on the quality of the visual features and provides fair way to compare performance across different tasks. Although some of these benchmarks tend to favor models trained with text supervision, our features perform strongly and competitively across many categories. 11 Figure 9: Visualization of the first PCA components. We compute PCA across patches on DAVIS [Pont-Tuset et al., 2017] and illustrate the first three components using RGB color channels. Despite variations in pose, style, or even object identity, corresponding parts are consistently matched. Background regions are removed by thresholding the first PCA component. Images were selected randomly with np.random.randint(seed=42). As shown in Table 3, our method, Franca, transfers well across wide range of downstream tasks. Our ViT-G/14 model (Franca-G) achieves the same performance as DINOv2-G and outperforms Web-SSL-G by 1.1% despite being trained on much less data. It also matches the performance of larger models like OpenCLIP-G. On datasets such as CIFAR-100 and Oxford Pets, Franca-G achieves 0.7% and 0.5% gains over DINOv2G respectively, demonstrating Francas strong generalization ability across both natural and fine-grained classification tasks. PCA of patch features We apply PCA to the patch-level feature embeddings of each image and retained only those patches whose projection onto the first principal component was positive (a simple thresholding that effectively isolates the foreground). We then compute second PCA on the remaining patches, visualizing the top three principal components as RGB channels. The resulting color maps reveal stark contrast between models as shown in Figure 9 with DINOv2, the PCA highlights only few scattered high-variance patches (outliers) and fails to cover the full object, yielding fragmented, noisy segmentation, whereas Franca produces dense, coherent color segments aligned with the actual object. In Franca, we observe that object contours are sharply delineated and semantically similar parts (e.g., limbs or object components) receive consistent colors across instances, whereas these structures do not emerge in the DINOv2 maps. Importantly, neither model was trained on DAVIS [Pont-Tuset et al., 2017], so these segmentation patterns are entirely emergent from the self-supervised features."
        },
        {
            "title": "5.2 Dense tasks",
            "content": "In-Context Learning We evaluate Franca on the Hummingbird Benchmark [Balazevic et al., 2023, Pariza et al., 2024], which measures visual in-context learning by retrieving dense segmentation masks from patch-level memory bank without fine-tuning. We report mean Intersection-overUnion (mIoU) on Pascal VOC [Everingham et al.] and ADE20K [Zhou et al., 2017] in Table 4. Franca consistently achieves strong segmentation results, with performance improving systematically with model scale. On ViT-B/14, Franca outperforms SigLIP and WebSSL by up to +5% mIoU on both datasets and performs within 23% of DINOv2-B (distilled from DINOv2G and trained on LVD-142M dataset). On ViT-G/14 scale, Franca surpasses DINOv2-G and WebSSL by up to +2% mIoU on Pascal VOC, while maintaining competitive performance on ADE20K. We hightlight that DINOv2 is pretrained on both VOC and ADE20K images while Franca is not. The gains become 12 more pronounced as model capacity increases, highlighting the ability of Franca to learn spatially precise and semantically meaningful features that transfer well without fine-tuning. METHOD BACKBONE LIN. SEG. IN-CONTEXT OVERCLUSTERING COCO-STUFF VOC ADE20K VOC ADE20K VOC COCO-THINGS = 100 = 300 = 100 = 300 SigLIP iBOT EVA-CLIP DINOv2 DINOv2 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/14 ViT-B/14 Franca (ours) ViT-B/14 SigLIPv2 Web-SSL DINOv2 DINOv2 ViT-L/16 ViT-L/14 ViT-L/14 ViT-L/ Franca (ours) ViT-L/14 Web-SSL DINOv2 ViT-G/14 ViT-G/14 Franca (ours) ViT-G/14 36.2 55.9 48.0 59.6 58.9 60. 60.3 60.8 60.3 58.0 60.5 57.1 58.1 60.4 44.6 73.1 70.4 77.2 80.3 80. 51.5 79.0 79.3 80.3 81.3 80.2 81.3 81.3 15.9 30.1 34.6 33.8 42.6 39. 45.4 40.3 37.8 41.8 41.4 39.4 42.4 33.9 66.6 34.8 69.6 77.1 75.7 46.4 71.3 72.0 74. 73.5 73.3 73.7 42.4 76.7 10.6 26.9 11.3 30.0 37.7 34. 21.3 35.3 33.5 38.6 37.6 36.7 37.7 36.5 29.5 21.2 43.3 25.9 39.2 30. 24.3 28.2 25.9 26.5 30.3 26.0 19.5 39.4 36.9 29.1 49.1 34.7 52.5 46. 40.4 37.7 34.7 43.0 44.4 33.4 27.7 49.2 41.5 18.3 41.3 20.5 46.5 37. 43.5 26.3 24.1 34.8 33.8 15.5 20.7 25.8 53.4 26.3 52.0 28.7 54.0 43. 50.7 33.1 35.1 45.7 40.7 21.5 29.2 31.9 Table 4: Linear and In-context Segmentation Performance. Comparison of linear segmentation and in-context segmentation (1/1 split). Linear segmentation uses linear head on frozen spatial features, evaluated via mIoU on COCO-Stuff, Pascal VOC, and ADE20K. In-context segmentation reports mIoU using cluster-based retrieval matched to ground truth via Hungarian matching. : reproduced on IN-21K, without distillation; : distilled from DINOv2-G on LVD-142M. Linear segmentation We further evaluate Franca in supervised semantic segmentation setting by training linear classifier on top of frozen pretrained backbone in Table 4. During training, the output features are upsampled to the input resolution using bilinear interpolation, and pixelwise cross-entropy loss is applied. This setup avoids fine-tuning and instead assesses the linear separability of the learned representations. We conduct experiments on COCO-Stuff [Caesar et al., 2018], Pascal VOC [Everingham et al.], and ADE20K [Zhou et al., 2017], reporting segmentation performance in terms of mean Intersection-over-Union (mIoU). To ensure robustness, we report the best performance across five learning rates (0.005, 0.01, 0.02, 0.05, 0.1). Across all three backbone scales, Franca consistently matches or outperforms prior methods. On the ViT-B/14 backbone, it achieves the highest mIoU on COCO-Stuff (60.5), while performing competitively on Pascal VOC (80.0) and ADE20K (39.1), on par with DINOv2-B which is distilled from DINOv2-G and pretrained on LVD142M. Notably, non-distilled reproduction of DINOv2-B results substantially lower performance (e.g., 77.2 vs. 80.0 on VOC; 33.8 vs. 39.1 on ADE20K), indicating that its competitive results rely heavily on distillation from larger model and quality of its pretraining data (LVD142M). With ViT-L/14 backbone, Franca maintains strong performance, outperforming DINOv2-L (pretrained on LVD142M) on COCO-Stuff and Pascal VOC, and remaining competitive on ADE20K. In this setting, Web-SSL [Fan et al., 2025] surpasses both Franca and DINOv2-L only once. Interestingly, despite its extended training and strong image-text alignment, SigLIPv2 [Tschannen et al., 2025] performs poorly on segmentation, particularly on VOC (51.5 mIoU), suggesting limited spatial localization capabilities. Finally, on the ViT-G/14 backbone, Franca achieves the highest performance across all benchmarks, outperforming all other models. These results suggest that the strong performance of DINOv2-L on smaller backbones is largely due to distillation from the larger ViT-G model. Moreover, while WebSSL benefits from pretraining on substantially larger dataset (2B images), Franca still outperforms it on several benchmarks, demonstrating the effectiveness of our approach. Overclustering We evaluate Franca in an unsupervised setting using the overclustering protocol from [Ziegler and Asano, 2022], which assesses semantic alignment of spatial features by clustering patch embeddings with K-Means and matching them to ground-truth masks via Hungarian match-"
        },
        {
            "title": "METHOD BACKBONE",
            "content": "VOC-07 VOC-12 SigLIPv2 DINOv2 OpenCLIP Franca (ours) O DINOv2 DINOv2 + regTr DINOv2 + regTe Franca (ours) 7.8 12.2 30.8 36.8 30.8 55.0 53.8 56. 9.7 14.1 35.9 42.1 35.9 59.3 57.9 60.9 Table 5: Unsupervised object discovery us- [Wang et al., 2022b] and ing TokenCut LOST [Siméoni et al., 2021], across different visual backbones. Franca consistently outperforms baselines, achieving the highest localizatrained registers [Darcet tion accuracy. regTr: test-time registers [Jiang et al., 2024]; regTe: et al., 2025]. Figure 10: Unsupervised clustering. We compare self-supervised clustering results of Franca with DINOv2 and DINOv2-R. Each method generates pseudo-segmentations from self-attention maps without labels or fine-tuning. Franca yields sharper boundaries and more semantically coherent regions, especially on fine-grained objects such as birds and bicycles. ing [Kuhn, 1955]. Performance is measured by mean Intersection-over-Union (mIoU), averaged over five seeds. This task emphasizes fine-grained structure in label-free setting. Overclustering is particularly relevant in dense self-supervised learning, where the learned features often serve as foundation for downstream tasks such as semantic segmentation and object detection. Increasing the number of clusters can promote more fine-grained and localized representations, allowing the model to capture detailed object parts and subtle region boundariescritical for dense prediction tasks. As shown in Table 4 and Figure 10, Franca shows strong and consistent performance across backbones and cluster sizes, with the largest gains at ViT-G/14 scale. On ViT-B/14, it achieves top performance on VOC with = 300 (46.9 mIoU) and competitive results on COCO-Things (37.9 / 43.1 with = 100 / = 300), outperforming WebSSL and EVA-CLIP, but slightly trailing DINOv2-B (trained on LVD-142M). With ViT-L/14, Franca surpasses DINOv2-L (trained on LVD-142M) on VOC (30.3 / 44.4 vs. 26.5 / 43.0), and beats WebSSL on COCO-Things, though SigLIPv2 remains the strongest on that dataset. At ViT-G/14 scale, Franca achieves the best results: 49.2 mIoU on VOC (K = 300) and 31.9 on COCO-Things (K = 100), outperforming all unimodal and several multimodal baselines, including DINOv2-G (trained on LVD-142M). These results underscore Franca ability to scale spatial representations effectively for unsupervised dense prediction."
        },
        {
            "title": "5.3 Probing 3D awareness",
            "content": "Probing 3D understanding We evaluate the geometric understanding of Franca on two tasks: keypoint correspondence and monocular depth estimation. For the former we use SPair-71k [Min et al., 2019], where the goal is to predict dense keypoint correspondences under varying viewpoint changes. For depth estimation, we follow the AdaBins protocol [Bhat et al., 2021] and evaluate single-image depth predictions on NYUv2 [Silberman et al., 2012] using the scale-invariant rootmean-square error (SI-RMSE). Unlike keypoint recall, which can reflect dataset bias toward semantically distinctive or consistently placed features (e.g., eyes, beaks), we evaluate correspondence accuracy across all keypoints and report performance under increasing viewpoint disparity. For fair comparison, we consider two evaluation settings: (i) low-resolution: using image size of 224 224 for SPair-71k and NYUv2, to mimic the resolution used during pretraining by most methods; (ii) 14 MODEL ARCH. DATA d= d=1 d=2 All SI-RMSE IMAGE RESOLUTION 224 224 (SPAIR-71K) AND 224 224 (NYUV2) ViT-B/16 WebLI SigLIPv2 ViT-B/14 Open-CLIP DINOv2 ViT-B/14 DINOv2 ViT-B/14 Franca (ours) ViT-B/ LAION-2B IN-21K LVD-142M IN-21K 8.00 17.54 32.96 37.97 35.41 ViT-L/16 WebLI SigLIPv2 ViT-L/14 Open-CLIP DINOv2 ViT-L/14 DINOv2 ViT-L/14 Franca (ours) ViT-L/14 Franca (ours) ViT-L/14 8.58 19.10 LAION-2B 38.98 IN-21K 36.86 LVD-142M IN-21K 39.10 LAION-600M 39.45 ViT-G/16 WebLI SigLIPv2 ViT-L/14 Open-CLIP DINOv2 ViT-G/14 Franca (ours) ViT-G/14 Franca (ours) ViT-G/14 13.53 LAION-2B 27.20 LVD-142M 38.09 36.41 IN-21K LAION-600M 39. 5.87 16.07 25.37 30.27 26.48 5.97 16.75 30.31 30.33 30.64 30.65 10.25 22.97 31.09 28.99 31.55 6.14 16.89 25.81 28.98 26.59 6.14 17.72 31.11 30.35 31.09 31.89 10.79 25.21 31.97 28.51 32. 7.22 16.76 29.32 33.59 31.31 7.22 17.60 34.61 32.81 35.13 35.45 11.55 25.01 33.99 33.17 36.04 IMAGE RESOLUTION 800 800 (SPAIR-71K) AND 480 480 (NYUV2) DINOv2 ViT-B/14 DINOv2 ViT-B/14 Franca (ours) ViT-B/14 IN-21K LVD-142M IN-21K 42.82 63.04 53.46 DINOv2 ViT-L/14 DINOv2 ViT-L/14 Franca (ours) ViT-L/14 Franca (ours) ViT-L/14 57.60 IN-21K 63.91 LVD-142M IN-21K 58.13 LAION-600M 55.38 DINOv2 ViT-G/14 Franca (ours) ViT-G/14 Franca (ours) ViT-G/"
        },
        {
            "title": "63.06\nLVD-142M\nIN-21K\n51.84\nLAION-600M 56.66",
            "content": "33.38 51.09 40.12 42.91 53.11 43.29 42.14 52.01 38.08 45.87 34.82 48.76 38.92 44.17 51.91 43.29 43.40 52.72 36.36 46. 38.71 55.98 46.67 50.68 56.92 50.61 48.87 56.37 44.08 51.23 0.52 0.36 0.34 0.28 0.33 0.52 0.35 0.29 0.25 0.28 0.28 0.43 0.31 0.24 0.26 0. 0.33 0.25 0.31 0.31 0.23 0.27 0.28 0.22 0.25 0.28 Table 6: Probing 3D understanding via keypoint matching on SPair-71K and monocular depth estimation on NYUv2. We report correspondence accuracy (higher is better) under increasing viewpoint changes (d=0, 1, 2, and overall) on SPair-71K, and scale-invariant RMSE (lower is better) for depth estimation on NYUv2. Results are shown for both low (2242) and high (8002) resolution SPair-71K input. : reproduced on IN-21K without distillation; : distilled from DINOv2-G. high-resolution: using image size of 800 800 for SPair-71k and 480 480 for NYUv2, that suits better methods with high-resolution fine-tuning, e.g., DINOv2. As shown in Table 6, Franca achieves strong performance across both tasks, outperforming DINOv2 (reproduced on IN-21K) and exceeding Open-CLIP and SigLIPv2 baselines of similar scale. On SPair-71k, using an input resolution of 224 224, Franca outperforms DINOv2 distilled from DINOv2-G and pretrained on LVD142M by 3% on ViT-L and 1.5% on ViT-G. Notably, Franca maintains high correspondence fidelity even for large viewpoint changes (d=2), suggesting robustness to geometric deformations. On NYUv2, Franca achieves comparable performance as DINOv2 distilled from larger DINOv2 model. This is also due to the fact that DINOv2 uses NYU Depth v2 during pretraining (as part of LVD 142M). These results indicate that Franca learns spatial representation that captures both fine-grained 2D alignment and coarse 3D structure, generalizing well from object-centric pretraining to downstream geometric tasks. Probing with Gaussian Splatting We probe the texture and geometry awareness of our models on the Feat2GS [Chen et al., 2025] framework. This framework uses novel view synthesis, which serves as an effective proxy for 3D evaluation of visual foundation models (VFM). It uses VFMs to extract dense features that are then transformed into 3D Gaussians using lightweight readout layer trained with photometric loss. For fair comparison, the input images are resized to 512, the output feature maps are upsampled to 512, and PCA is used to unify the feature dimensionality to 256 channels. We use three metrics for evaluation: PSNR, SSIM, and LPIPS. The three evaluation setups (\"Geometry\", \"Texture\", and \"All\") refer to different ways of using the VFM features to predict the parameters (a) Geometry: VFM features predict the geometric parameters of the 3D of the 3D Gaussians. 15 Figure 11: Probing with Gaussian Splatting, Normalized average metrics using Feat2GS [Chen et al., 2025] across six datasets for two probing schemes: geometry (G), and all (A), i.e., Geometry + Texture with ViT-L backbone. We measure PSNR, SSIM (higher is better) and LPIPS (lower is better) showing that Franca achieves significantly better performance than state-of-the-art vision encoders suggesting strong geometrical awareness. Gaussians (position, opacity, and covariance), while the textural parameters (spherical harmonic coefficients) are freely optimized. (b) Texture: VFM features predict the textural parameters of the 3D Gaussians, and the geometric parameters are directly optimized. (c) All: here, the VFM features are used to predict all parameters of the 3D Gaussians, including both geometry and texture. We use the Large model variants and compare our Franca DINOv2 [Oquab et al., 2024], WebSSL [Fan et al., 2025], AIMv2 [Fini et al., 2025], EVA-CLIP [Sun et al., 2023], SigLIPv2 [Tschannen et al., 2025], and CLIP [Radford et al., 2021]. The results reported in Figure 11 are averaged over 5 runs using six datasets. They show that Franca performs the best regarding the Geometrical awareness and also when evaluated in the complete setup (All), suggesting strong geometrical awareness. On Texture, all methods have similar performance with SigLIPv2 having an edge over them."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Franca (the free one), new Vision Foundation Model that is openweight, open-data and open-code. We build this model using novel Matryoshka-nested clustering self-supervised loss that allows for learning hierarchical representations and have introduced RASA, simple post-pretraining method to remove overtly spatial biases in the final representations. Across evaluations in image recognition, segmentation, robustness, OOD detection and 3D understanding, we find that it matches and frequently outperforms DINOv2 and other state-of-the-art models such as SigLIPv2. Acknowledgments. This work was performed using HPC resources from GENCI-IDRIS (Grants AD011015626, AD011016023, AD011016410, AD011016429, AD011016514, AD011016523, and AD011016537). The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-AlexanderUniversität Erlangen-Nürnberg (FAU) under the BayernKI project v115be. BayernKI funding is provided by Bavarian state authorities. Andrei Bursuc is partially funded by the Horizon Europe project ELLIOT (GA No. 101214398). We also thank Antonin Vobecky for his valuable contribution to certain experiments and Rémi Lacroix for setting up the LAION-600M dataset."
        },
        {
            "title": "Detailed Contributions",
            "content": "PROJECT LEAD Research direction, technical roadmap, project coordination, pretraining, evaluation on classic vision tasks classification, OOD, fine-grained evaluations. Shashanka Venkataramanan CORE CONTRIBUTOR Development and integration of RASA, evaluation on dense tasks in-context learning, linear segmentation, and overclustering. Valentinos Pariza CONTRIBUTORS Mohammadreza Salehi TokenCut and LOST evaluation Lukas Knobel Probe3D evaluations on SPair-71K and NYUv2 Elias Ramzi Dataset preparation and all aspects of the codebase Rémi Lacroix LAION-600M dataset setup TECHNICAL REPORT Manuscript preparation, design, visualizations, and figures Shashanka Venkataramanan, Valentinos Pariza, Spyros Gidaris, Andrei Bursuc, and Yuki Asano"
        },
        {
            "title": "References",
            "content": "Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. In NeurIPS, 2022. 1, 3, 4, 5, 9 Ivana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, and Olivier Henaff. Towards incontext scene understanding. NeurIPS, 2023. 1, 3, 5, 12 Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. OpenOOD: Benchmarking generalized out-of-distribution detection. In NeurIPS Datasets and Benchmarks, 2022. 1 Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, and Yuliang Xiu. Feat2gs: Probing visual foundation models with gaussian splatting. CVPR, 2025. 1, 15, Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 2, 4, 6, 9, 11, 16 Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in the wild. arXiv preprint arXiv:2103.01988, 2021. 2 Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae prepretraining for billion-scale pretraining. In ICCV, 2023. 2, 4 Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 2, 6, 13, 16 David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, and Saining Xie. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. 2, 4, 9, 13, 16 Xu Ji, Joao Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, 2019. 2, 4, 6 Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 2, 6 Valentinos Pariza, Mohammadreza Salehi, Gertjan J. Burghouts, Francesco Locatello, and Yuki Asano. Near, far: Patch-ordering enhances vision foundation models scene understanding. In ICLR, 2025. 3 M. Everingham, L. Van Gool, C. K. I. Williams, CAL Visual Object Classes Challenge network.org/challenges/VOC/voc2012/workshop/index.html. 3, 12, 13 2012 J. Winn, (VOC2012) Results. and A. Zisserman. The PAShttp://www.pascalHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context, 2018. 3, 13 Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 3 Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 3 Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In ECCV, 2016. 3 Richard Zhang, Phillip Isola, and Alexei Efros. Split-brain autoencoders: Unsupervised learning by crosschannel prediction. In CVPR, 2017. 3 Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 3 Spyros Gidaris and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 3 Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. NeurIPS, 2014. 4 Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018. 4 Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv, 2018. 4 Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020. 4 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020a. 4, 11 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. learning. arXiv preprint arXiv:2003.04297, 2020b."
        },
        {
            "title": "Improved baselines with momentum contrastive",
            "content": "Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021. 4 Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020. 4 Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. 4 Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick Pérez. Obow: Online bag-of-visual-words generation for self-supervised learning. In CVPR, 2021. 4 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 4, 5 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 4 Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pretraining with online tokenizer. In ICLR, 2022a. 4 Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: Bert pre-training of image transformers. In ICLR, 2022. 4 Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, 2022. 4 Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. 4 Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020. 4 Spyros Gidaris, Andrei Bursuc, Oriane Siméoni, Antonín Vobeck`y, Nikos Komodakis, Matthieu Cord, and Patrick Perez. Moca: Self-supervised representation learning by predicting masked online codebook assignments. TMLR, 2024. 4 Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Cluster and predict latents patches for improved masked image modeling. TMLR, 2025. 4, 7 Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In ICCV, 2019. 18 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4, 6, 16 Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. 4 LAION. Releasing re-laion 5b: Transparent iteration on laion-5b with additional safety fixes. 2024. URL https://laion.ai/blog/relaion-5b/. 4, Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 4 Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Dont judge an object by its context: learning to overcome contextual bias. In CVPR, 2020. 4 Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In CVPR, 2015. Chao Wang, Yujun Liu, Yang Zou, and Philipp Krähenbühl. Projective manifold disentanglement for selfsupervised learning. In CVPR, 2023. 4 Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. iBOT: Image BERT pre-training with online tokenizer. In ICLR, 2022b. 5, 6 Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 5, 9, 10, Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 5, 9, 10 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 5, 9 Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS, 2013. 5 Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels. In ECCV, 2020. 6 Pan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: multi-granular self-supervised learning framework. arXiv preprint arXiv:2203.14415, 2022c. Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. In ICML, 2023. 7 Valentinos Pariza, Mohammadreza Salehi, and Yuki Asano. Hummingbird evaluation for vision encoders, 2024. URL https://github.com/vpariza/open-hummingbird-eval. 7, 12 Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University Press, 4th edition, 2013. Tal Ridnik, Elad Ben-Baruch, Amir Zamir, and Ido Friedman. Imagenet-21k pretraining for the masses. In NeurIPS, 2021. 8 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Aleksei Drozd, Marius Cuadros, Dmitry Gritsenko, Sebastian Kintscher, Maxim Botros, Christoph Müller, Patrick Ludwig, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks, 2022. 9 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 9 Lucas Beyer, Olivier Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 9 Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 9 Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: good closed-set classifier is all you need. In ICLR, 2022. 10, 11 Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In ICML, 2023. 10, Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In CVPR, 2021. 10, 11 Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In CVPR, 2022a. 10, 11 Gustaf Kylberg. Kylberg texture dataset v. 1.0. Centre for Image Analysis, Swedish University of Agricultural Sciences and Uppsala University, 2011. 10, Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021a. 10 19 Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-ofdistribution generalization. In ICCV, 2021b. 10 Xiaolong Wang, Allan Jabri, and Alexei Efros. Learning correspondence from the cycle-consistency of time. In CVPR, 2019. Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Yixuan Li, Ziwei Liu, Yiran Chen, and Hai Li. OpenOOD v1.5: Enhanced benchmark for out-ofdistribution detection. DMLR, 2024. 11 Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 12 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 12, 13 Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L. Crowley, and Dominique Vaufreydaz. Selfsupervised transformers for unsupervised object discovery using normalized cut. In CVPR, 2022b. 14 Oriane Siméoni, Gilles Puy, Huy Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. arXiv preprint arXiv:2109.14279, 2021. 14 Nick Jiang, Amil Dravid, Alexei Efros, and Yossi Gandelsman. Vision transformers dont need trained registers. In arXiv preprint arXiv:2506.08010, 2025. 14 Adrian Ziegler and Yuki Asano. Self-supervised learning of object parts for semantic segmentation. In CVPR, 2022. 13 Harold Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 1955. 14 Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Spair-71k: large-scale benchmark for semantic correspondence. arXiv preprint arXiv:1908.10543, 2019. 14 Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In CVPR, 2021. 14 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 14 Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Béthune, Zhe Gan, Alexander Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multimodal autoregressive pre-training of large vision encoders. In CVPR, 2025. 16 Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023."
        }
    ],
    "affiliations": [
        "Fundamental AI Lab, UTN",
        "VIS Lab, UvA",
        "valeo.ai, Paris"
    ]
}