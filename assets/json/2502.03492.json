{
    "paper_title": "Teaching Language Models to Critique via Reinforcement Learning",
    "authors": [
        "Zhihui Xie",
        "Jie chen",
        "Liyu Chen",
        "Weichao Mao",
        "Jingjing Xu",
        "Lingpeng Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose $\\texttt{CTRL}$, a framework for $\\texttt{C}$ritic $\\texttt{T}$raining via $\\texttt{R}$einforcement $\\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks."
        },
        {
            "title": "Start",
            "content": "Zhihui Xie * 1 Jie Chen * 2 Liyu Chen 2 Weichao Mao 2 Jingjing Xu 2 Lingpeng Kong 1 https://critic-rl.github.io 5 2 0 2 5 ] . [ 1 2 9 4 3 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, framework for Critic Training via Reinforcement Learning, which trains critic model to generate feedback that maximizes correction performance for fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks. 1. Introduction Recent advances in Large Language Models (LLMs) have sparked interest in their potential for self-improvement through iterative feedback mechanisms (Pan et al., 2023). Methods like Reflexion (Shinn et al., 2024) and Self-Refine (Madaan et al., 2024) demonstrate that LLMs can, in principle, critique their own outputs and generate refined responses. This self-improvement paradigm offers promising direction toward more autonomous AI systems that can learn from their mistakes. However, the effectiveness of such self-improvement mechanisms remains challenging in practice. Huang et al. (2023) demonstrate that without appropriate external feedback, such self-improvement loops may lead to performance degradation. To address this, existing approaches primarily rely on reward models (Sun et al., 2023; Yuan et al., 2024) *Equal contribution 1The University of Hong Kong 2Bytedance, Seed. Correspondence to: Zhihui Xie <zhxieml@gmail.com>. Figure 1. Performance scaling of our CTRL critic (finetuned on Qwen2.5-Coder-32B-Ins, henceforth Qwen2.5-Coder) compared to other critics across different generators on CodeContests. CTRL demonstrates strong critiquing capabilities not only when paired with its base model but also with stronger generator (GPT-4o, right). Shaded regions indicate standard error across 5 seeds. or automated verification tools (Gou et al., 2023; Chen et al., 2023). However, these mechanisms often fail to provide actionable guidance reward models compress complex evaluation criteria into simplified numerical signals (Gao et al., 2023; Pan et al., 2024), while verification tools generate low-level execution traces that do not directly translate to high-level fixes (Zhong et al., 2024). Even in domains like code generation (Li et al., 2022; Sun et al., 2024) where such feedback mechanisms are readily available, previous work (Zheng et al., 2024) as well as our experiment  (Table 1)  reveal that such feedback alone struggles to drive meaningful improvements. At the heart of this issue lies the feedback bottleneck: feedback needs to both accurately discriminate the correctness of solutions and provide informative yet actionable suggestions for improvement. To address these challenges, we propose CTRL (Critic Training via Reinforcement Learning), framework that decouples the critic model from the task-performing model (e.g., GPT-4o) and focus on developing specialized critic that can effectively drive the task-performing model toward optimal solution generation through iterative critiquerevisions (Figure 2). This decomposition naturally introduces well-defined proxy task for training the critic model: while directly evaluating the quality of generated critiques remains challenging, the effectiveness of critic can be 1 Teaching Language Models to Critique via Reinforcement Learning Figure 2. Illustration of the critique-correction process for coding problem. Top: An initial solution is proposed by the task-performing using min-heap approach. Bottom: The critic identifies flaws in the implementation (incorrect heap access and inefficient query handling) and suggests specific improvements, leading to corrected max-heap solution. This example is taken from critiques of CTRL on LiveCodeBench, which demonstrates how structured feedback from the critic can guide meaningful improvements in code generation. measured by its ability to drive the task-performing model toward correct outputs. Though such indirect optimization signals lead to large space of possible critiques and therefore high variance during training, we address this through two-stage pipeline: first synthesizing high-quality critiques using execution feedback for supervised finetuning, then optimizing the critic through Group Relative Policy Optimization (GRPO; Shao et al. 2024). et Through extensive evaluations on diverse benchmarks including CodeContests al., 2022), Live- (Li CodeBench (Jain et al., 2024), MBPP+ (Liu et al., 2024a), and JudgeBench (Tan et al., 2024), we demonstrate that training with CTRL significantly outperforms both self-critique approaches and methods using stronger critic models. Notably, we observe remarkable generalization capabilities of the decoupled critic LLM across different problem domains and model scales. Our experiments demonstrate that relatively weaker critic models can effectively guide stronger task-performing models such as GPT-4o  (Table 3)  , exhibiting similar phenomenon to weak-to-strong generalization (Christiano et al., 2018; Burns et al., 2023), where weaker models can be trained to effectively supervise more capable ones. Furthermore, CTRL enables efficient test-time scaling (Figure 1). By providing targeted and actionable feedback, our critic significantly reduces the number of revision iterations needed, leading to both lower token consumption and higher success rates. Our empirical analysis (Figure 4) demonstrates that this efficiency stems from reduced error compoundingthe critic effectively identifies and corrects mistakes early, guiding the model toward more direct solution paths without compromising solution quality. Our work makes four key contributions: (1) We propose CTRL, novel framework that decouples critic LLMs from task-performing models and trains them through two-stage GRPO to guide code improvement. (2) Through extensive evaluation on programming benchmarks, we demonstrate that CTRL significantly outperforms both self-critique methods and approaches using stronger critic models. (3) We establish that relatively weaker critic models can effectively guide stronger task-performing models, demonstrating promising weak-to-strong generalization phenomenon in LLM guidance. (4) We show that trained critic enables test-time scaling through iterative critique-revisions, achieving up to 106.1% and 23.5% relative Pass@1 improvements on the challenging CodeContests benchmark when paired with its base model and stronger model, respectively. 2. Preliminaries and Motivation The success of iterative improvement methods critically depends on their ability to leverage feedback to improve solutions. Formally, let be an input problem and be candidate solution, with R(y) being the evaluation function 2 Teaching Language Models to Critique via Reinforcement Learning that returns 1 if is correct and 0 otherwise. Starting with an initial proposal distribution y0 π( x), the iterative process generates subsequent solutions by incorporating feedback ( x, yi) and produce the next solution yi+1. In this context, the effectiveness of such feedback mechanisms relies on two key capabilities: (1) discrimination - the ability to evaluate and rank solutions, and (2) critiquing - the ability to provide actionable feedback for improvement. While discrimination has been extensively studied (Gao et al., 2023), we focus on the critiquing ability and propose to characterize it through the transition dynamics of Markov chain (Meyn & Tweedie, 2012) governing the correctness of the iteratively refined solutions {R(yi)}i: (R(y0) = 1) = pinit, (R(yi+1) = 1 R(yi) = 1) = pcc, (R(yi+1) = 1 R(yi) = 0) = pcw, where pcc represents the critiquing ability to avoid turning correct solutions into wrong ones, and pcw captures the helpfulness of the feedback in improving the solution. Varying Critiquing Ability. To understand the importance of the critiquing ability, we conduct simulations across different levels of critiquing strength while leveraging discrimination to aggregate the final solutions. We consider pinit = 0.1 and three scenarios: (1) No critiquing (pcw = pcc), special case representing methods that independently sample from the base distribution, or equivalently best-of-n sampling (Sessa et al., 2024); (2) Weak critiquing (pcc = 0.7, pcw = 0.15); and (3) Strong critiquing (pcc = 0.9, pcw = 0.3). For each scenario, we first generate solutions based on the specified transition dynamics, then apply the discrimination ability to select the best promising solution, and plot the final correctness probability against the number of attempts n. We present more details in Appendix C.1. Observations & Takeaways. As shown in Figure 3, our analysis reveals several key findings: (1) Strong critiquing abilities significantly improve success rates compared to no critiquing, with performance gains visible even with weak critiquing, aligning with recent empirical findings (Huang et al., 2023). (2) Strong critiquing ability can compensate for weaker discrimination system with weak discrimination but strong critiquing feedback can outperform one with stronger discrimination but no critiquing ability. (3) The benefits of critiquing compound with more iterations, while approaches with no critiquing plateau quickly. These findings highlight that effective iterative improvement requires careful attention to both discrimination and critiquing abilities. While perfect abilities are not necessary, systematically improving these capabilities particularly the ability Figure 3. Simulation results showing success probability (pcorrect) as function of the number of attempts, comparing different levels of critiquing and discrimination ability. to generate actionable critiques is crucial for realizing the full potential of iterative refinement approaches. 3. Method With analysis presented in Section 2, our goal is to teach LLMs the ability of critiquing without human supervision. We propose CTRL, two-stage training approach: (1) synthesizing high-quality critiques by reasoning about execution feedback, then (2) refining the critic through reinforcement learning. Once trained, the critic model can be used at test time, paired with any generator models, to iteratively refine solutions. complete overview of the pipeline is provided in Appendix A, with critique samples in Appendix E. 3.1. Problem Statement We focus on code generation as our primary domain as it provides clear objective metrics through test cases, following previous work (McAleese et al., 2024). Given programming problem (specified in natural language) and solution (code implementation), our goal is to enable iterative refinement of solutions, which centers on two key components: (1) generator model π(y x) that proposes solutions, and (2) critic model Qθ(cx, y) that provides textural feedback for improvement. Assumptions. Let = {(xi, Ti)}N i=1 be our training dataset, where each problem xi is paired with unit tests Ti. We have access to sandbox environment that executes code against test cases, which serves as the evaluation function R(y) that returns 1 if passes all tests, 0 otherwise. Notably, the sandbox does not assist critique generation at test time. While not required, we treat the generator model as black-box, allowing our approach to build upon existing strong generators without access to their parameters. Teaching Language Models to Critique via Reinforcement Learning Objective. While directly measuring the helpfulness of critiques remain challenging, we can define proxy task that evaluates whether the critique leads to improved solutions. Given an initial solution π( x), the critic analyzes it and produces textual feedback c. The generator then uses this feedback to revise the solution, producing an improved output y. Let = (x, y) represent the problemsolution pair. Our objective is to train the critic model Qθ to maximize the expected solution quality: (θ) = EzDπ,yπθ(z)[R(y)], (1) where πθ(y z) = (cid:80) Qθ(c z)π(y z, c) denotes the improved solution distribution through marginalization over possible critiques. Notably, although Equation (1) defines single-turn critique-revision task, we observe that the trained model generalizes to multi-turn revisions (Section 4.2). Defining the Critique Space. We structure the critique space into three components (Figure 2): (1) an analysis of the solutions strengths and weaknesses, (2) actionable improvement suggestions, and (3) final judgment of correctness (correct/incorrect). During inference, these components enable iterative critique-revision, where the process stops once the judgment indicates the solution is correct. This design balances discrimination and critiquing, both essential for iterative refinement, as discussed in Section 2. 3.2. Stage I: Execution-guided Critique Synthesis Although conceptually straightforward, learning effective critiques is challenging due to the large critique space, where only small fraction leads to successful revisions. Our experiments with Qwen2.5-Coder (Hui et al., 2024) (Table 1 show that models struggle to generate informative critiques for self-improvement, aligning with previous findings (Huang et al., 2023). Self-critique without additional feedback yields minimal gains (7.88% 8.36%) and rarely converts incorrect solutions to correct ones, highlighting the limited ability of models to correct their own mistakes. Reasoning over Execution. While the initial critiquing ability is limited, previous work (Ni et al., 2024) has shown that LLMs can effectively reason over execution feedback. Table 1 demonstrates that when LLMs reason over execution feedback to generate critiques (Self-critique w/ Execution Feedback), they achieve substantial improvements, as compared to directly using raw execution feedback for revisions (11.76% vs. 8.97%). This suggests that while directly using raw execution feedback is inefficient, we can leverage the models reasoning ability over execution feedback to help generate more accurate and informative critiques. Critique Synthesis. Building on the above insight, we develop critique synthesis approach that leverages execution Table 1. Critique-revision performance (Pass@1, %) on CodeContests. We fix the generator model to be Qwen2.5-Coder, and compare zer-shot performance with critique-revision performance using different feedback mechanisms. represents conducting iterative critique-revision times. using unit tests for generation. Zero-shot Execution Feedback (EF) Self-critique w/ EF Self-critique Critique w/ CTRLSFT Critique w/ CTRL Critique2 w/ CTRL Critique3 w/ CTRL Pass@1 7.88 8.97 11.76 8.36 8.36 11.76 14.18 15.15 0.00 2.42 3.88 2.30 3.52 4.73 7.27 8. 0.00 1.33 0.00 1.82 3.03 0.85 0.97 0.85 Table 2. Discrimination performance (F1 score, %) on CodeContests. Passed Failed Macro Qwen2.5-Coder CTRLSFT CTRL 88.21 95.54 93.19 34.16 41.26 45.02 61.19 68.55 69.10 feedback to train models in generating effective critiques. Our approach samples high-quality synthesized critiques from hinted distribution Qθ(c z, h), where hints are constructed by analyzing initial solutions through sandbox execution. We map different execution outcomes to specific hint templates as shown in Table 7: (1) for passing solutions, we encourage concise positive feedback; (2) for completely failing solutions, we suggest restarting from scratch; and (3) for partially failing solutions, we provide the exact error message and test case details to help pinpoint the issue. Supervised Finetuning. Similar to context distillation (Snell et al., 2022; Guan et al., 2024), we exclude these hints and conduct supervised finetuning to encourage the model to internalize the critiquing strategies. We observe leveraging execution feedback for supervised finetuning is beneficial mainly in two aspects: (1) it helps learn the format; (2) while it marginally improves the critique-revision performance due to the high frequency of instructing correct solutions to wrong  (Table 1)  , it substantially boosts discrimination by providing ground-truth correctness  (Table 2)  . 3.3. Stage II: Reinforced Critique Generation While our critique synthesis approach with predefined templates provides strong foundation, it may not capture all nuanced feedback scenarios required for complex programming tasks. To overcome this limitation, we formulate critique generation as reinforcement learning problem, allowing the critic to adaptively learn feedback strategies 4 Teaching Language Models to Critique via Reinforcement Learning through direct optimization of solution improvement. 4. Experiments Our goal is to maximize the performance in Equation (1). To optimize Qθ, one natural approach is using policy gradient methods (Sutton et al., 1999): θEyπθ [R(y)] We conduct extensive experiments to evaluate our methods effectiveness across multiple benchmarks. Our evaluation focuses on two key aspects: (1) the accuracy of the critic in identifying solution correctness, and (2) the quality improvement achieved through critique-guided revisions. Qθ(cx,y)π(yx,c)[R(y)] =θEy(cid:80) (cid:88) =θ R(y) (cid:88) Qθ(cx, y)π(yx, c) 4.1. Setup (cid:88) = R(y) (cid:88) θQθ(cx, y)π(yx, c) The double summation over both solution space and feedback space introduces high variance in gradient estimates: Var(θ) = E[(θ E[θ])2] C. where and are the sizes of solution and critique spaces respectively. In this scenario, using value networks to predict credit assignment remains challenging, as we observe significant instability when using Proximal Policy Optimization (PPO; Schulman et al. 2017) the learned networks produce noisy estimates of critique quality. We present detailed experimental observations inAppendix D. Variance Reduction. To combat these variance issues, we adopt Group Relative Policy Optimization (GRPO; Shao et al. 2024) that avoids using value networks for learning credit assignment and reduces variance through groupbased relative advantages. Specifically, for each problemsolution pair = (x, y), we samples group of critiques {c1, c2, ..., cG} from Qθ(z) and computes advantages: Ai = R(yi) µG σG , where yi π(z, ci) is the improved solution generated using critique ci, and µG and σG are the mean and standard deviation of rewards within the group. This approach normalizes rewards across different problem types and naturally focuses training on problems where critique quality can make meaningful difference, as problems that are too easy or too hard produce zero relative advantages. The final training objective is: (θ) = zD,{ci}G i=1Qθold (yx) (cid:104) (cid:88) (cid:16)"
        },
        {
            "title": "1\nG",
            "content": "min (cid:0) Qθ(yix) Qθold(yix) (cid:105) βDKL(QθQref) i=1 , Ai, clipε (cid:0) Qθ(yix) Qθold (yix) (cid:1)(cid:17) (cid:1)Ai where clipε represents clipping the value to [1 ε, 1 + ε] and DKL(QθQref) = Qref(yix) Qθ(yix) 1 denotes the KL regularization term that alleviates over-optimization. Qθ(yix) log Qref(yix) 5 Training Data. We use TACO (Li et al., 2023), dataset containing 26,443 programming problems collected from competitive programming platforms like CodeForces and LeetCode. Each problem includes natural language description and multiple test cases. Due to noise in the original dataset (malformed test cases and contaminated problems), we filter the dataset to 18,820 problems for training, with details presented in Appendix C.3. Models. We base our critic model on the open-source Qwen2.5-Coder-Ins (Hui et al., 2024) model. During training, we fix the generator model to be Qwen2.5-Coder-Ins itself. For evaluation, we assess the trained critics performance by pairing it with various generator models for initial solution generation and subsequent revision, comparing against other LLM critics such as GPT-4o. Benchmarks. We evaluate our approach on three programming benchmarks and one general-domain benchmark: (1) CodeContests (Li et al., 2022), collection of challenging competitive programming problems; (2) LiveCodeBench (24.08-24.11) (Jain et al., 2024), curated set of recent programming challenges designed to minimize data contamination; (3) MBPP+ (Liu et al., 2024a), an extension of the MBPP benchmark (Austin et al., 2021) focused on fundamental programming tasks; and (4) JudgeBench (Tan et al., 2024), where we evaluate the models effectiveness as generative reward model for comparing solution pairs. Metrics. To evaluate critiquing ability, we use three metrics: Pass@1 measures the success rate of the final solutions, represents the fraction of initially incorrect solutions that become correct after revision, and represents the fraction of initially correct solutions that become incorrect after revision. For discrimination ability, we employ F1 score when evaluating single solutions, and accuracy when comparing paired solutions in Judgebench, as the latter involves binary decisions between two alternatives. Execution Sandbox. We employ SandboxFusion (Liu et al., 2024b) as our execution environment, which provides unified interface for evaluating solutions across training data and benchmarks through both function-based and standard input-output formats. Teaching Language Models to Critique via Reinforcement Learning Table 3. Performance comparison across different generators and benchmarks. We evaluate different configurations, with critique-revision representing an iterative process where critic model provides feedback to guide solution improvement. Pass@1 shows the success rate, while and indicate the percentage of wrong solutions being correctly revised and correct solutions being revised to wrong solutions, respectively. Results are averaged over 5 random seeds. CodeContests LiveCodeBench Pass@1 Pass@1 MBPP+ Pass@1 Average Pass@1 Zero-shot Single-turn Critique-revision Critique w/ Qwen2.5-Coder Critique w/ GPT-4o Critique w/ CTRL Multi-turn Critique-revision Critique5 w/ Qwen2.5-Coder Critique5 w/ GPT-4o Critique5 w/ CTRL 7.88 8.36 10.67 11.76 9.21 12.48 16. Qwen2.5-Coder as Generator 0.00 0.00 30.54 0.00 0.00 77.83 0. 0.00 38.75 2.30 4.85 4.73 3.76 7.03 9.21 1.82 2.06 0.85 2.42 2.42 0. 32.14 32.32 33.21 29.64 32.86 33.39 2.50 2.32 3.39 2.14 4.82 3.75 0.89 0.54 0.71 3.04 2.50 0. 77.83 77.46 78.84 76.03 74.60 78.68 3.49 3.81 2.43 3.81 4.34 3.23 3.49 4.18 1.43 5.61 7.57 2. 39.45 40.15 41.27 38.30 39.98 42.77 Zero-shot Single-turn Critique-revision Critique w/ Qwen2.5-Coder Critique w/ GPT-4o Critique w/ CTRL Multi-turn Critique-revision Critique5 w/ Qwen2.5-Coder Critique5 w/ GPT-4o Critique5 w/ CTRL GPT-4o as Generator 20.61 0. 0.00 32.32 0.00 0.00 77.67 0. 0.00 43.53 20.24 20.97 23.03 19.52 20.61 25.45 3.52 2.30 4.97 5.21 3.39 7. 3.88 1.94 2.55 6.30 3.39 3.03 35.36 34.82 33.39 35.54 35.18 34.11 3.93 2.68 2.14 5.36 3.21 3. 0.89 0.18 1.07 2.14 0.36 1.43 76.67 77.41 77.83 76.08 76.61 77.94 0.85 1.01 0.53 1.53 2.06 0. 1.85 1.27 0.37 3.12 3.12 0.53 44.09 44.40 44.75 43.71 44.13 45.83 4.2. Evaluating Critics for Iterative Critique-revisions To evaluate the effectiveness of CTRL, we present comprehensive analysis of critique-revision strategies with different feedback mechanisms on CodeContests in Table 1. The discrimination performance of critics is shown in Table 2, while results across different benchmarks and generators are presented in Table 3. RL Significantly Boosts Critiquing Ability. Table 1 shows that our RL-trained critic significantly outperforms baseline approaches, achieving 11.76% pass@1 rate compared to 7.88% with zero-shot generation. This substantial improvement builds upon much reduced regression rate than its SFT counterpart (0.85% vs. 3.03%). CTRL Enables Test-time Scaling. As shown in Table 1, our approach enables test-time scaling through iterative critique-revisions. Notably, despite training exclusively on single-turn critiquing tasks, CTRL generalizes to multi-turn settings. By increasing the number of iterations from one to three (Critique3 w/ CTRL), we further improve the Pass@1 rate from 11.76% to 15.15% while maintaining low regression rate of 0.85%. This demonstrates that our critic provides consistently reliable feedback across multiple revision iterations, unlike baseline approaches that accumulate errors, as discussed below. CTRL Mitigates Compounding Errors. Figure 4 further illustrates this stability advantage - while both Qwen2.5Coder and GPT-4o show increasing error compounding rates over iterations, CTRL maintains significantly lower rate, enabling reliable multi-round improvements. CTRL Generalizes to Different Generators and Tasks. While we train the critic model with Qwen2.5-Coder as the generator, as shown in Table 3, our approach generalizes well across different programming tasks. Notably, weak critic model trained against itself can assist stronger model (GPT-4o), providing evidence for scalable oversight (Christiano et al., 2018; Kenton et al., 2024). Performance Scaling with Problem Difficulty. As shown in Figure 5, our critique-revision approach demonstrates increasingly substantial relative gains as both iteration and problem difficulty increases, revealing that CTRL is particularly effective for complex tasks, where iterative refinement through targeted critique and revision yields the most significant benefits compared to zero-shot generation. 4.3. Evaluating Critics as Generative Reward Models One advantage of unifying textural feedback is to balance discrimination and critiquing abilities. To assess our critics discrimination capabilities, we evaluate them on JudgeBench (Tan et al., 2024), comprehensive benchmark 6 Teaching Language Models to Critique via Reinforcement Learning Figure 4. Compounding error analysis. Regression rate measures the frequency of correct initial solutions being revised into incorrect ones. Shaded regions indicate standard error over 5 seeds. containing 350 GPT-4o completions across categories spanning general knowledge, reasoning, mathematics, and coding. This setup presents challenging out-of-distribution test in two aspects: (1) our critics must evaluate outputs from more capable model than their training distribution, and (2) they need to generalize to broader domains beyond coding tasks. This evaluation scenario is particularly interesting as it examines whether relatively weaker models can be effectively trained to judge outputs from more powerful models. As shown in Figure 6, CTRL critic achieves competitive performance compared to stronger models such as Claude3.5-Sonnet. Notably, while our critic is specifically trained on programming tasks, it maintains comparable overall accuracy (64.3%) while demonstrating superior performance on coding-specific evaluations. This suggests that our CTRL enables effective discrimination capabilities that generalize beyond the training domain. Figure 5. Comparison of pass@1 rates by problem difficulty with CTRL critics on CodeContests. Results are averaged over 5 seeds. 7 Figure 6. Model performance comparison on JudgeBench. 4.4. Analysis To better understand how CTRL boosts iterative refinement, we further conduct analyses on the similarity between original and revised solutions, execution time changes, and critique characteristics. Our findings reveal several key patterns in how different critique methods influence the process of critique-revision. The Effect of Generator Ability. As preliminary analysis before finetuning experiments, we examine how model sizes affect critique-revision performance using Qwen2.5Coder-Ins models (7B, 14B, and 32B) in an inference-only setting, comparing zero-shot generation against critiquerevision with critiques generated by another critic model conditioned on execution feedback. Table 4 reveals that critic capability significantly influences improvement potentialwhile smaller critics (7B) often lead to performance degradation, larger critics (32B) consistently yield better outcomes, achieving up to 50% improvement when paired with similarly-sized generators. The results also highlight the importance of critic-generator size relationships, as critics less capable than their generators typically degrade performance. These findings motivate us to focus our subsequent finetunTable 4. Relative improvement (%) on CodeContests when comparing critique-revision (using critics conditioned on execution feedback) against zero-shot generation, across different generatorcritic size combinations. Results are from inference-only experiments before any finetuning. Generator 7B 14B 32B Avg. 7B -33.33 -9.09 0. Critic 14B 22.22 -9.09 30.00 32B -11.11 9.09 50.00 Avg. -7.41 -3.03 26. -14.14 14.38 15.99 Teaching Language Models to Critique via Reinforcement Learning Table 5. Timeout rate and Pass@1 (%) on LiveCodeBench. While CTRL approach achieves higher pass rates, it tends to generate more comprehensive solutions that take longer to execute. Timeout Rate () Pass@1 () Zero-shot Critique w/ GPT-4o Critique w/ CTRL 10.54 8.93 16.61 30.54 32.32 33.21 LLM Critics. Several approaches have been proposed to train LLMs as critics for various purposes, including generative reward models (Ankner et al., 2024; Xiong et al., 2024) and scalable oversight (Saunders et al., 2022; Kenton et al., 2024). These approaches either learn from human feedback (Wang et al., 2023; McAleese et al., 2024) or much more capable models outputs (Xi et al., 2024), with recent work exploring reinforcement learning to improve feedback generation (Akyurek et al., 2023; Yao et al., 2023). Our approach differs in three key aspects: (1) leveraging execution feedback and model reasoning to synthesize high-quality critiques, (2) introducing variance reduction techniques to stabilize training, and (3) requiring only singleround critique-revision interactions. Additional discussion on related work is provided in Appendix B. Scaling Test-Time Compute. Recent work has explored various approaches to improve model performance at test time without fine-tuning (Snell et al., 2024). While existing approaches focus on techniques like repeated sampling with proper selection mechanisms (Brown et al., 2024) and more sophisticated modular frameworks with existing models (Saad-Falcon et al., 2024), we instead investigate testtime scaling through decoupled critic model trained to provides targeted feedback to guide solution improvements. Notably, while Saad-Falcon et al. (2024) demonstrates that strong models can serve as effective critics, their approach struggles with code generation tasks. 6. Conclusion We present CTRL, reinforcement learning framework for training critic LLMs to provide effective feedback for iterative refinement. Our trained critic demonstrates significant improvements over baselines across multiple benchmarks and enables efficient test-time scaling through iterative critique-revisions notably, even when guiding stronger generators. While this work focuses on improving pass rates, future directions include optimizing for efficiency and safety, and extending our training pipeline towards multiturn critique revision. We hope this work inspires further research into scalable LLM self-improvement through reinforcement learning. Figure 7. Comparison of solution similarities between original and revised code guided by CTRL on CodeContests. Left: Distribution of similarity scores for self-critique and our CTRL method. Right: Box plot showing the statistical distribution of similarity scores. Lower scores indicate more substantial revisions. ing experiments with CTRL on 32B models to maximize the benefits of critique-revision. CTRL Prevents Similar Revisions. We analyze how different critique methods influence solution revisions by measuring code similarity scores between original and revised solutions, as described in Appendix C.4. As shown in Figure 7, self-critique tends to make conservative modifications with higher similarity scores (mean 0.482), while our CTRL method proposes more substantial changes (mean 0.313). This suggests CTRL is more willing to recommend major structural revisions when needed, rather than just local optimizations, which may explain its superior performance in improving solution quality. CTRL Trade-offs between Accuracy and Efficiency. While our critique-revision approach improves solution accuracy on LiveCodeBench, we observe notable increase in timeout rates. Solutions guided by CTRL exhibit timeout rate of 16.61%, higher than both zero-shot (10.54%) and GPT-4o critic (8.93%). However, even with more timeouts, CTRL still achieves better overall Pass@1 accuracy. This suggests that our approach tends to generate more comprehensive solutions while these may take longer to execute, the solution quality is guaranteed. 5. Related Work Self-Improvement of LLMs. Recent work has explored various approaches for LLMs to improve their outputs autonomously, including self-critique (Madaan et al., 2024; Shinn et al., 2024), debates (Irving et al., 2018; Michael et al., 2023; Khan et al., 2024), and training models to selfcorrect (Welleck et al., 2022; Kumar et al., 2024). However, Huang et al. (2023) demonstrates that without appropriate external feedback, such self-improvement loops may lead to performance degradation. Our work addresses these challenges by learning specialized models that can provide effective feedback for improvement. 8 Teaching Language Models to Critique via Reinforcement Learning"
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance the field of Machine Learning by introducing framework for training LLM critics. While this research has the potential to improve the reliability and robustness of AI systems, we have not identified any immediate societal concerns requiring specific attention. However, as with any AI technology, careful consideration should be given to its broader deployment and potential misuse."
        },
        {
            "title": "References",
            "content": "Akyurek, A. F., Akyurek, E., Madaan, A., Kalyan, A., Clark, P., Wijaya, D., and Tandon, N. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844, 2023. Ankner, Z., Paul, M., Cui, B., Chang, J. D., and Ammanabrolu, P. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. Chen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. Christiano, P., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Gou, Z., Shao, Z., Gong, Y., Shen, Y., Yang, Y., Duan, N., and Chen, W. Critic: Large language models can selfcorrect with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Heylar, A., Dias, R., Vallone, A., Ren, H., Wei, J., et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Irving, G., Christiano, P., and Amodei, D. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Kenton, Z., Siegel, N. Y., Kramar, J., Brown-Cohen, J., Albanie, S., Bulian, J., Agarwal, R., Lindner, D., Tang, Y., Goodman, N. D., et al. On scalable oversight with weak llms judging strong llms. arXiv preprint arXiv:2407.04622, 2024. Khan, A., Hughes, J., Valentine, D., Ruis, L., Sachan, K., Radhakrishnan, A., Grefenstette, E., Bowman, S. R., Rocktaschel, T., and Perez, E. Debating with more persuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782, 2024. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Li, R., Fu, J., Zhang, B.-W., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation Teaching Language Models to Critique via Reinforcement Learning of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024a. Liu, S., Zhu, H., Liu, J., Xin, S., Li, A., Long, R., Chen, L., Yang, J., Xia, J., Peng, Z., et al. Fullstack bench: arXiv preprint Evaluating llms as full stack coder. arXiv:2412.00535, 2024b. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36, 2024. McAleese, N., Pokorny, R. M., Uribe, J. F. C., Nitishinskaya, E., Trebacz, M., and Leike, J. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. Meyn, S. P. and Tweedie, R. L. Markov chains and stochastic stability. Springer Science & Business Media, 2012. Michael, J., Mahdi, S., Rein, D., Petty, J., Dirani, J., Padmakumar, V., and Bowman, S. R. Debate helps supervise unreliable experts. arXiv preprint arXiv:2311.08702, 2023. Ni, A., Allamanis, M., Cohan, A., Deng, Y., Shi, K., Sutton, C., and Yin, P. Next: Teaching large language models to reason about code execution. arXiv preprint arXiv:2404.14662, 2024. Pan, J., He, H., Bowman, S. R., and Feng, S. Spontaneous reward hacking in iterative self-refinement. arXiv preprint arXiv:2407.04549, 2024. Pan, L., Saxon, M., Xu, W., Nathani, D., Wang, X., and Wang, W. Y. Automatically correcting large language models: Surveying the landscape of diverse selfcorrection strategies. arXiv preprint arXiv:2308.03188, 2023. Saad-Falcon, J., Lafuente, A. G., Natarajan, S., Maru, N., Todorov, H., Guha, E., Buchanan, E. K., Chen, M., Guha, N., Re, C., et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Snell, C., Klein, D., and Zhong, R. Learning by distilling context. arXiv preprint arXiv:2209.15189, 2022. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Sun, Q., Chen, Z., Xu, F., Cheng, K., Ma, C., Yin, Z., Wang, J., Han, C., Zhu, R., Yuan, S., et al. survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734, 2024. Sun, Z., Shen, Y., Zhang, H., Zhou, Q., Chen, Z., Cox, D. D., Yang, Y., and Gan, C. Salmon: Self-alignment with principle-following reward models. CoRR, 2023. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Tan, S., Zhuang, S., Montgomery, K., Tang, W. Y., Cuadron, A., Wang, C., Popa, R. A., and Stoica, I. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. Wang, T., Yu, P., Tan, X. E., OBrien, S., Pasunuru, R., Dwivedi-Yu, J., Golovneva, O., Zettlemoyer, L., Fazel-Zarandi, M., and Celikyilmaz, A. Shepherd: critic for language model generation. arXiv preprint arXiv:2308.04592, 2023. Welleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053, 2022. Sessa, P. G., Dadashi, R., Hussenot, L., Ferret, J., Vieillard, N., Rame, A., Shariari, B., Perrin, S., Friesen, A., Cideron, G., et al. Bond: Aligning llms with best-of-n distillation. arXiv preprint arXiv:2407.14622, 2024. Xi, Z., Yang, D., Huang, J., Tang, J., Li, G., Ding, Y., He, W., Hong, B., Do, S., Zhan, W., et al. Enhancing llm reasoning via critique models with test-time and trainingtime supervision. arXiv preprint arXiv:2411.16579, 2024. 10 Teaching Language Models to Critique via Reinforcement Learning Xiong, T., Wang, X., Guo, D., Ye, Q., Fan, H., Gu, Q., Huang, H., and Li, C. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. Yao, W., Heinecke, S., Niebles, J. C., Liu, Z., Feng, Y., Xue, L., Murthy, R., Chen, Z., Zhang, J., Arpit, D., et al. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023. Ye, Z., Greenlee-Scott, F., Bartolo, M., Blunsom, P., Campos, J. A., and Galle, M. Improving reward models with synthetic critiques. arXiv preprint arXiv:2405.20850, 2024. Yu, Y., Chen, Z., Zhang, A., Tan, L., Zhu, C., Pang, R. Y., Qian, Y., Wang, X., Gururangan, S., Zhang, C., et al. Selfgenerated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024. Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Zheng, K., Decugis, J., Gehring, J., Cohen, T., Negrevergne, B., and Synnaeve, G. What makes large language models reason in (multi-turn) code generation? arXiv preprint arXiv:2410.08105, 2024. Zhong, L., Wang, Z., and Shang, J. Ldb: large language model debugger via verifying runtime execution step-bystep. arXiv preprint arXiv:2402.16906, 2024. 11 Teaching Language Models to Critique via Reinforcement Learning A. Pipeline As shown in Figure 8, our pipeline consists of two main training stages. (1) The SFT training stage first generates initial solutions that are validated through execution feedback, followed by critique generation where the generator learns to provide critiques based on execution feedback. These components are then used to train the final critic model through supervised finetuning. (2) The RL training stage leverages the critics feedback to guide the generator in producing improved solutions, which are validated in sandbox environment. Figure 8. Overview of our two-stage training pipeline CTRL. B. Supplementary Discussion of Related Work Table 6 categorizes prior methods into reward models, generative reward models, and critic models. Reward models like Standard RM (Bradley & Terry, 1952) and SynRM (Ye et al., 2024) focus on discrimination by outputting scalar rewards but lack refinement or critique supervision. Generative reward models, such as CLoud (Ankner et al., 2024) and Critic-RM (Yu et al., 2024), enhance discrimination by producing both rewards and critiques c, but their critiques primarily serve as by-product for rewards rather than actionable refinement suggestions. Critic models, including UltraCM (Cui et al., 2023), Shepherd (Wang et al., 2023), and CriticGPT (McAleese et al., 2024), focus on generating critiques but rely heavily on human-annotated critique data, which limits scalability. In contrast, CTRL unifies discrimination and refinement by generating actionable critiques without direct supervision, leveraging execution feedback and reinforcement learning to enable scalable, iterative improvement. Table 6. Comparison of reward models, generative reward models, and critic models. Methods Input Output Discrimination Refinement Critique Supervision Standard RM (Bradley & Terry, 1952) SynRM (Ye et al., 2024) UltraCM (Cui et al., 2023) Shepherd (Wang et al., 2023) CriticGPT (McAleese et al., 2024) CLoud (Ankner et al., 2024) Critic-RM (Yu et al., 2024) CTRL (Ours) x, x, y, x, x, x, x, x, x, r c c, c, C. Implementation Details C.1. Simulation In our simulation (Section 2), we model the iterative refinement process using Markov chain with parameters pinit, pcc, and pcw to represent the initial correctness, the probability of maintaining correctness, and the probability of turning incorrect 12 Teaching Language Models to Critique via Reinforcement Learning solutions correct, respectively. Critiquing ability is controlled by varying pcc and pcw (e.g., strong critiquing: pcc = 0.9, pcw = 0.3; weak critiquing: pcc = 0.7, pcw = 0.15), while discrimination ability is adjusted via true positive rate (TPR) and false positive rate (FPR) (e.g., strong discrimination: TPR = 0.7, FPR = 0.2; weak discrimination: TPR = 0.6, FPR = 0.3). For each setting, we simulate refinement steps using Python, generating solutions based on refinement probabilities, applying classifier to predict correctness, and selecting the best solution from predicted correct ones. The results are computed over 50,000 iterations and plotted to analyze the impact of critiquing and discrimination on final success rates. Specifically, the two processes only using discrimination and using both discrimination and critiquing are illustrated in Figure 9 to provide clearer understanding of our simulation setup. y2 y3 ... yn y0 y1 yn w/o Critiquing w/ Critiquing Figure 9. Graphical models for refinement processes: (left) only using discrimination (best-of-n sampling) and (right) using both discrimination and critiquing (sequential critique-revision). C.2. Prompt Templates Critique-revision. The generator model π(y x) is implemented as simple zero-shot generation process, where the model generates solution directly from the problem statement without additional context or feedback. The critic model Qθ(c x, y), as described in the main paper, generates textual feedback using structured prompt that incorporates the problem x, the solution y, and explicit instructions to provide actionable and formatted suggestions. The improved solution distribution π(y x, y, c) is implemented as two-turn process: in the first turn, the generator model drafts the initial solution conditioned on the problem as the user message; in the second turn, the critique is presented as the user message, and the model revises the solution, conditioned on x, y, and c. Execution-guided Critique Generation. To generate high-quality critiques (Section 3.2), we leverage execution feedback from sandbox environment that evaluates the initial solution against the test cases for the problem x. The execution results are mapped to predefined hint templates, which guide the critique generation process. The critic model is prompted with structured template incorporating the problem x, the solution y, and the corresponding hint h, enabling it to produce actionable and context-aware feedback. To prevent hallucination, critiques that explicitly reference the hints are filtered out. This ensures that the generated critiques are grounded in observable failures while effectively supporting solution refinement. C.3. Training Data Curation. Our data curation process starts with the TACO dataset (Li et al., 2023) and handles both function-based and input-output-based programming problems. We filter out malformed problems by removing those containing image tags and unusual HTML spans. For unit tests, we process them differently based on their type: function-based tests are converted to assertion statements, while input-output tests are standardized into sandbox format with stdin-stdout pairs. We exclude problematic unit tests such as those with malformed string inputs (containing assignments or unexpected list operations) or invalid function calls. To avoid contamination, we further exclude 47 problems that overlap with our evaluation benchmarks. The final dataset is deduplicated based on problem descriptions, resulting in 18,820 problems. Supervised Finetuning. We leverage the synthesized critiques to perform supervised finetuning (SFT) on the model, enabling it to generate improved solutions. For each problem, we sample one initial solution and one corresponding synthesized critique, and train the model on these problem-solution-critique pairs. The training process follows the 13 Teaching Language Models to Critique via Reinforcement Learning"
        },
        {
            "title": "Prompt Template for Critique Generation",
            "content": "You are tasked with analyzing an answer to problem and providing constructive feedback. Do NOT provide (cid:44) direct solutions. Problem description: <problem> {problem} </problem> Answer: <answer> {answer} </answer> Structure your response using the following format (without <format> tags): <format> Analysis: {{Analysis}} Improvement suggestions: {{Suggestions}} Overall judgment: {{Correct/Incorrect}} </format> Prompt Template for Execution-guided Critique Generation You are tasked with analyzing an answer to problem and providing constructive feedback. Do NOT provide (cid:44) direct solutions. Please carefully reason about the hint to guide the user. **Important: Do NOT mention the hint in your feedback.** Problem description: <problem> {problem} </problem> Answer: <answer> {solution} </answer> Hint: <hint> {hint} </hint> Structure your response using the following format (without <format> tags): <format> Analysis: {{Analysis}} Improvement suggestions: {{Suggestions}} Overall judgment: {{Correct/Incorrect}} </format> hyperparameters outlined in Table 8. RL Training. We use VeRL (Sheng et al., 2024) as the codebase to optimize the models generation quality. During RL training, we sample 4 initial solutions for each problem and train the critic model on all corresponding problem-solution pairs. This approach helps mitigate overfitting by exposing the critic to diverse set of solutions for each problem. The RL training process follows the hyperparameters outlined in Table 9. 14 Teaching Language Models to Critique via Reinforcement Learning Table 7. Mapping between execution results and hint templates used for critique synthesis. Execution Result Hint Success (100%) Failure (0%) The draft solution is correct. concise and positive feedback is recommended. The draft solution is entirely wrong. concise feedback requesting fresh restart is recommended. Input: {input} Partial Success Expected Output: {expected output} Runtime Error Actual Output: {actual output} The code block: python {code block} raised {error}. Table 8. SFT Hyperparameters. Parameter Learning Rate Learning Rate Schedule Training Batch Size Maximum Token Length Training Epochs Mixed Precision Format Value 2 10-5 Cosine 256 2,048 1 bfloat Table 9. RL Hyperparameters. Parameter Training Batch Size Mini-Batch Size Group Size Learning Rate KL Coefficient Maximum Prompt Length Maximum Response Length Temperature Training Epochs Value 1,024 256 8 1 10-5 0.001 1,536 768 1.0 2 C.4. Evaluation. Inference. During inference, we use temperature of 0.7 for generating both initial solutions and critiques, while revised solutions are generated using greedy decoding. The maximum number of tokens generated is set to 1,024 for all stages. Reward Calculation. To calculate rewards for our JudgeBench evaluation (Section 4.3), we use critic model to assess the quality of solutions. Specifically, we generate multiple critiques for each solution and aggregate the results through majority voting. For each solution pair, the critic model compares the frequency of being labeled as Correct to determine which solution is better. As shown in Figure 10(a), we find that the accuracy of this majority voting strategy improves as the number of votes increases. Code Similarity Calculation. To measure code similarity while accounting for semantically equivalent code with different variable names, we follow (Zheng et al., 2024) and implement two-step comparison approach. We first normalize the code by parsing it into an Abstract Syntax Tree (AST), systematically renaming variables to canonical forms, and converting back to consistently formatted text. We then compute similarity ratio using Pythons difflib.SequenceMatcher, which represents the proportion of matching characters in the optimal alignment of the two normalized code sequences. This approach yields score between 0 and 1, allowing us to identify structurally similar solutions regardless of variable naming choices. 15 Teaching Language Models to Critique via Reinforcement Learning (a) The effect of the number of votes on the accuracy of majority voting in reward calculation. As the number of votes increases, the accuracy improves significantly, demonstrating the scalability and robustness of the majority voting approach. (b) Training curve of the value network in PPO, showing the mean predicted value over training steps. Figure 10. (a) Majority voting accuracy improvement with increasing votes. (b) Training curve of the value network. D. The Credit Assignment Issue for Critic Training Our initial attempts to train critics using Proximal Policy Optimization (Schulman et al., 2017) for RL training revealed challenges in credit assignment for critique generation, as evidenced by the unstable value predictions shown in Figure 10(b). This difficulty arises because the quality of critique is inherently holistic it depends on multiple interdependent aspects like accuracy, completeness, and constructiveness making it hard for the value network to learn which parts of the generated critique contributed to the final reward. These observations motivated our switch to GRPO, which circumvents the value prediction problem by using Monte Carlo sampling to directly estimate advantages, leading to more reliable credit assignment without the need for potentially unstable value network. E. CTRL Samples In Tables 10, 12 and 14, we present sample critiques generated by CTRL for solutions provided by Qwen2.5-Coder. 16 Teaching Language Models to Critique via Reinforcement Learning Table 10. sample problem from CodeContests (Li et al., 2022), with by the solution generated by Qwen2.5-Coder and the critique provided by the CTRL critic. Problem & Solution Critique You had positive integers a_1, a_2, ..., a_n arranged in circle. For each pair of neighboring numbers (a_1 and a_2, a_2 and a_3, ..., a_{n - 1} and a_n, and a_n and a_1), you wrote down: are the numbers in the pair equal or not. Unfortunately, youve lost piece of paper with the array a. Moreover, you are afraid that even information about equality of neighboring elements may be inconsistent. So, you are wondering: is there any array which is consistent with information you have about equality or non-equality of corresponding pairs ? def can_form_array(s): = len(s) # We will try to assign values to the array # Start with assigning 1 to the first element = [1] * # Try to assign values based on the given conditions for in range(n): if s[i] == E: a[(i + 1) % else: a[(i + 1) % # Check if the last and first elements are consistent with s[n-1] if s[-1] == and a[0] != a[-1]: return \"NO\" if s[-1] == and a[0] == a[-1]: return \"NO\" return \"YES\" def main(): import sys input = sys.stdin.read().splitlines() = int(input[0]) results = [] for in range(1, + 1): = input[i] results.append(can_form_array(s)) for result in results: print(result) if __name__ == \"__main__\": main() Analysis: The provided solution attempts to construct an array based on the sequence of and but does not correctly handle the circular nature of the array and the constraints imposed by these characters. Specifically, the approach does not ensure consistency across the entire circular arrangement, particularly at the boundary where the last element connects back to the first. The code assigns values to the array elements based on whether the current character is or N, but it does not account for potential conflicts that arise when the array wraps around. This means that the final check between the first and last elements might fail due to inconsistencies introduced earlier in the array construction process. Improvement suggestions: 1. Consider using more systematic approach to handle the circular nature of the array. One possible method is to use graph traversal algorithm, treating each position in the array as node and each or as an edge with label indicating equality or inequality. 2. Use union-find data structure to manage equivalence classes of indices that must hold the same value due to connections. 3. Iterate through the string and apply the rules: if E, ensure the indices are in the same set; if N, ensure they are in different sets. 4. After processing the string, verify that no conflicts exist in the union-find structure, particularly focusing on the circular connection between the first and last elements. 5. If no conflicts are found, it is possible to construct an array that satisfies the conditions; otherwise, it is not. Overall judgment: Incorrect 17 Teaching Language Models to Critique via Reinforcement Learning Table 12. sample problem from LiveCodeBench (Jain et al., 2024), with by the solution generated by Qwen2.5-Coder and the critique provided by the CTRL critic. Problem & Solution Critique You are given binary string and an integer k. binary string satisfies the k-constraint if either of the following conditions holds: The number of 0s in the string is at most k. The number of 1s in the string is at most k. Return an integer denoting the number of substrings of that satisfy the k-constraint. class Solution: def countKConstraintSubstrings(self, s: str, k: int) -> int: def count_substrings_with_max_char(s, max_char, k): left = 0 count = 0 current_count = 0 result = 0 for right in range(len(s)): if s[right] == max_char: current_count += while current_count > k: if s[left] == max_char: current_count -= 1 left += 1 result += right - left + 1 return result count_0s = count_substrings_with_max_char(s, 0, k) count_1s = count_substrings_with_max_char(s, 1, k) return count_0s + count_1s Analysis: The provided solution attempts to solve the problem by counting substrings based on the maximum occurrence of 0 and 1 up to k, but it does not correctly account for all possible substrings that satisfy the k-constraint. The approach incorrectly calculates the intersection of substrings that satisfy both conditions without properly considering the overlap between these substrings. Additionally, the logic for counting substrings is flawed as it does not consider every possible substring of s; instead, it focuses on contiguous sequences of 0s and 1s up to k. Improvement suggestions: 1. **Re-evaluate the Approach**: Instead of focusing on contiguous sequences of 0s and 1s, the solution should consider every possible substring of and check if it satisfies the k-constraint. 2. **Iterate Over All Substrings**: Use nested loops to iterate over all possible substrings of . For each substring, count the occurrences of 0 and 1 and check if either count is less than or equal to k. 3. **Count Valid Substrings**: Maintain counter to keep track of how many substrings meet the kconstraint criteria. 4. **Simplify the Logic**: Consider using Pythons slicing capabilities to easily extract and evaluate each substring without unnecessary checks for contiguous characters. Overall judgment: Incorrect 18 Teaching Language Models to Critique via Reinforcement Learning Table 14. sample problem from MBPP+ (Liu et al., 2024a), with by the solution generated by Qwen2.5-Coder and the critique provided by the CTRL critic. Problem & Solution Critique Write python function to check whether every odd index contains odd numbers of given list. def odd_length_sum(arr): = len(arr) res = 0 # Iterate through each element in the array for in range(n): # Calculate the number of subarrays in which arr[i] is present count = ((i + 1) * (n - i) + 1) // 2 # If the count is odd, add the element to the result if count % res += arr[i] return res Analysis: The provided solution checks if the parity (odd/even nature) of the numbers at odd indices matches the index itself. This means it verifies that number at an odd index is odd, and number at an even index is even. However, this does not correctly address the problem at hand, which requires checking whether every odd index contains an odd number, not just matching the parity of the index. Improvement suggestions: - Review the logic used in the function. Instead of checking if the parity of the number matches the parity of the index, the function should directly check if the number at each odd index is odd. - Consider using more straightforward loop or list comprehension to achieve the desired result. Overall judgment: Incorrect"
        }
    ],
    "affiliations": [
        "Bytedance, Seed",
        "The University of Hong Kong"
    ]
}