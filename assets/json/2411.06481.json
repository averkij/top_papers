{
    "paper_title": "KMM: Key Frame Mask Mamba for Extended Motion Generation",
    "authors": [
        "Zeyu Zhang",
        "Hang Gao",
        "Akide Liu",
        "Qi Chen",
        "Feng Chen",
        "Yiran Wang",
        "Danning Li",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM"
        },
        {
            "title": "Start",
            "content": "KMM: Key Frame Mask Mamba for Extended Motion Generation Zeyu Zhang12, Hang Gao3, Akide Liu3, Qi Chen4, Feng Chen4, Yiran Wang5, Danning Li6, Hao Tang1(cid:0) 1Peking University 4The University of Adelaide 2The Australian National University 5The University of Sydney 3Monash University 6McGill University https://steve-zeyu-zhang.github.io/KMM 4 2 0 2 0 1 ] . [ 1 1 8 4 6 0 . 1 1 4 2 : r Figure 1: The figure on the left illustrates the exceptional capability of the proposed KMM in generating continuous and diverse human motions based on extended text prompts across various durations. The figure on the right highlights that our method significantly outperforms the previous state-of-the-art in quantitative evaluations while utilizing substantially fewer parameters. Abstract Human motion generation is cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, novel architecture featuring Key frame Masking Modeling, designed to enhance Mambas focus on key actions in moEqual contribution. Work done while being visiting student researcher at Peking University. (cid:0)Corresponding author: bjdxtanghao@gmail.com tion segments. This approach addresses the memory decay problem and represents pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-ofthe-art performance with reduction of more than 57% in FID and 70% parameters compared to previous state-of-theart methods."
        },
        {
            "title": "Introduction",
            "content": "Text-to-motion (T2M) generation (Guo et al. 2022) involves creating realistic 3D human movements from text descriptions, with promising applications in game development, video creation, and digital humans. Previous generation methods that leverage VAE (Guo et al. 2022; Petrovich, Black, and Varol 2022; Zhong et al. 2023; Tevet et al. Figure 2: The figure illustrates that previous extended motion generation methods often struggle with directional instructions, leading to incorrect motions. In contrast, our proposed KMM, with enhanced text-motion alignment, effectively improves the models understanding of text queries, resulting in more accurate motion generation. 2022a), GAN (Lin and Amer 2018; Harvey et al. 2020; Barsoum, Kender, and Liu 2018), autoregressive (Jiang et al. 2024; Pinyoanuntapong et al. 2024b; Guo et al. 2024), and diffusion-based (Zhang et al. 2024a; Shafir et al. 2023; Chen et al. 2023; Zhang et al. 2023a) approaches using motiontext pair data have achieved overall success in downstream tasks (Raab et al. 2023; Zhang et al. 2024d; Xiao et al. 2023). The recent Mamba (Gu and Dao 2023) architecture shows promising potential for long-context modeling and has already made encouraging attempts in human motion grounding (Wang, Kang, and Mu 2024) and generation (Zhang et al. 2024c,b). Although there are many different methods for text-to-motion generation, they all share common approach: learning common latent space for both motion and text. However, these methods pose two significant challenges. First, previous works are based on transformers (Athanasiou et al. 2022; Lee et al. 2024) and diffusion (Yang, Su, and Wen 2023; Shafir et al. 2023; Zhang et al. 2023b; Petrovich et al. 2024) techniques, which are not solving the extended motion generation problem from an architecture perspective. In contrast, Mamba (Gu and Dao 2023) is naturally suitable architecture for long-sequence motion generation due to its recurrent modelling and linear scaling with sequence length (Dao and Gu 2024). Although Mamba supports long-sequence generation, adapting it for extended motion generation remains challenging due to implicit memory decay. This memory decay is the core limitation impacting Mambas performance in generating extended motion sequences. Additionally, despite some efforts to adapt Mamba to multimodal tasks (Wang, Kang, and Mu 2024; Zhang et al. 2024c), the results remain unsatisfactory. Mamba still has obvious shortcomings in multimodal fusion compared to attention mechanisms (Dong et al. 2024; Xie et al. 2024), leading to weak text-motion alignment and poor generation results. Specifically, previous works underperform in two key scenarios, including misunderstanding directional instructions and struggling to handle extended text queries. For instance, when testing on queries containing directions such as left and right, models often generate incorrect or opposite directional motions, as illustrated in Figure 2. Additionally, when testing on complex and lengthy text prompts, models often fail to generate sufficient motions corresponding to the instructions or omit latter part of the text. This phenomenon suggests the limitation of the text encoder. More specifically, most methods employ frozen CLIP (Radford et al. 2021) encoders, which fail to adequately tackle the difficulty of aligning with complex text prompts. Consequently, the models struggle to fully comprehend the text descriptions and generate the corresponding motion sequences. To overcome these challenges, our paper presents three key contributions: Firstly, to tackle the memory decay problem in Mamba, we introduce Key frame Masking Modeling (KMM), novel approach that selects key frames based on local density and pairwise distance. This method allows the model to focus on learning from masked key frames, which is more effective for the implicit memory architecture of Mamba than random masking. This advancement represents pioneering method that customizes framelevel masking in the Mamba model within the latent space. Additionally, to address the issue of poor text-motion alignment in the Mamba architecture caused by ineffective multimodal fusion, we proposed novel method that leverages contrastive learning. Instead of relying on fixed CLIP text encoder, our approach dynamically learns text encodings, enabling the generation of more accurate motions by encoding text queries with better alignment. Lastly, we conducted extensive experiments on the BABEL dataset (Punnakkal et al. 2021), the go-to benchmark for extended motion generation. Our method achieved more than 57% improvement in FID and reduced the number of parameters by 70% compared to previous state-of-the-art methods. Related Works Text-to-Motion Generation Autoencoders have become important to human motion generation, widely adopted and adapted across various models. JL2P (Ahuja and Morency 2019) utilizes RNN-based autoencoders (Hopfield 1982) to learn unified representation of language and pose, though it limits the mapping of text to motion to one-to-one relationship. MotionCLIP (Tevet et al. 2022a) builds on this by employing Transformer-based autoencoders (Vaswani 2017) for motion reconstruction, ensuring alignment with corresponding text labels within the CLIP (Radford et al. 2021) space, thus integrating semantic knowledge into the motion manifold. Further innovations include TEMOS (Petrovich, Black, and Varol 2022) and T2M (Guo et al. 2022), which leverage Transformer-based VAEs (Kingma and Welling 2014) with text encoders to generate distribution parameters within the VAE latent space, enhancing motion generation capabilities. Extending these advancements, AttT2M (Zhong et al. 2023) and TM2D (Gong et al. 2023) incorporate body-part spatio-temporal encoder into VQ-VAE (Van Den Oord, Vinyals et al. 2017), enriching the learning of discrete latent space with heightened expressiveness. These varied applications of autoencoders demonstrate their flexibility and efficacy in capturing the complexities of motion dynamics and their relationships with textual descriptions, reflecting the ongoing evolution and refinement of this approach. Diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020; Dhariwal and Nichol 2021; Rombach et al. 2022), having achieved notable success in 2D image generation, have naturally been extended to motion generation, emerging as promising research direction. MotionDiffuse (Zhang et al. 2024a) pioneered this area by introducing the first diffusion-based framework for text-driven motion generation, offering benefits like probabilistic mapping, realistic synthesis, and multi-level manipulation. Building on this foundation, MDM (Tevet et al. 2022b) developed classifier-free Transformer-based diffusion model specifically for human motion, innovating by predicting samples rather than noise at each diffusion step, leading to more refined motion outputs. MLD (Chen et al. 2023) further advanced this approach by applying the diffusion process in latent motion space, decoupling raw motion sequences from direct conditional inputs. Recently, Zhang et al. (Zhang et al. 2024c) introduced Motion Mamba, new architecture using hierarchical and bidirectional SSMs for efficient longsequence motion generation. These advances highlight the rapid growth and diverse applications of diffusion and state space models in motion generation, showcasing their transformative potential. Transformer-based architectures have recently gained significant traction in motion generation due to their powerful sequence modeling capabilities. MotionGPT (Jiang et al. 2024) spearheaded the concept of treating human motion as foreign language, leveraging large language models to generate and comprehend complex motion sequences. This approach allows for more nuanced interpretations of motion, facilitating tasks such as motion generation, text-to-motion translation, and motion captioning. Concurrently, masked modeling strategies have been adapted for motion generation, with MMM (Pinyoanuntapong et al. 2024b) introducing generative masked motion model that predicts missing motion tokens, demonstrating notable success in motion completion and generation tasks. Similarly, MoMask (Guo et al. 2024) applied masked modeling to 3D human motions, effectively learning representations that capture both local and global temporal dependencies. Building upon these concepts, BAMM (Pinyoanuntapong et al. 2024a) introduced bidirectional autoregressive model combining masked modeling and autoregressive techniques, enhancing motion generation coherence and diversity. These advancements demonstrate the versatility of Transformers in capturing human motion dynamics, leading to more advanced generation systems. Extended Motion Generation Recent advancements in motion generation have focused on creating longer, more complex sequences. MultiAct (Lee, Moon, and Lee 2023) pioneered the generation of long-term 3D human motions from multiple action labels, addressing the challenge of coherent transitions between diverse actions. TEACH (Athanasiou et al. 2022) further developed this concept by introducing temporal action composition framework for 3D humans, enabling fine-grained control over action sequencing. Diffusion models have shown promising results in this domain. PriorMDM (Shafir et al. 2023) demonstrated the effectiveness of using diffusion models as generative prior for extended motion synthesis. Building on this, DiffCollage (Zhang et al. 2023b) introduced parallel generation technique for large content, significantly improving the efficiency of long motion sequence creation. Transformer-based architectures have also made significant strides. T2LM (Lee et al. 2024) expanded the scope by generating long-term 3D human motions from multiple sentences, enhancing the models ability to interpret complex narratives. InfiniMotion (Zhang et al. 2024b) integrated Mamba architecture with Transformers to boost memory capacity, enabling the generation of arbitrarily long motion sequences. Recent works have also focused on improving the coherence and seamlessness of extended motions. FlowMDM (Barquero, Escalera, and Palmero 2024) proposed method using blended positional encodings for seamless human motion composition. PCMDM (Yang, Su, and Wen 2023) introduced coherent sampling technique for synthesizing long-term human motions with diffusion models, addressing the challenge of maintaining consistency over extended periods. Multitrack approaches have emerged as promising direction for finer control over extended motions. STMC (Petrovich et al. 2024) presented multi-track timeline control method for text-driven 3D human motion generation, allowing for more nuanced and varied long-term motion sequences. These advancements collectively represent significant progress in extended motion generation, addressing key challenges such as coherence, control, and computational efficiency in creating long and complex human motion sequences. Figure 3: The figure demonstrates our novel method from three different perspectives: (a) illustrates the key frame masking strategy based on local density and minimum distance to higher density calculation. (b) showcases the overall architecture of the masked bidirectional Mamba. (c) demonstrates the text-to-motion alignment, highlighting the process before and after alignment."
        },
        {
            "title": "Motivation",
            "content": "Extended motion generation faces two significant challenges: First, generating considerably long motion sequence without compromising the quality of the latter part is difficult. Second, long-sequence models must balance the trade-off between performance and efficiency, as longer sequences tend to reduce efficiency. From this perspective, Mamba, as an implicit memory recurrent architecture, is more effective and efficient in modeling long sequences than Transformers (Vaswani 2017), making it promising approach for long motion generation. However, directly applying Mamba to extended motion generation presents two major challenges. First, the memory matrix in Mamba has limited capacity for holding implicit memory, leading to memory decay when generating an entire long motion sequence. Second, Mamba intrinsically struggles with multimodal fusion due to its sequential architecture, which is less effective than Transformers. This leads to poor alignment between text and motion, ultimately decreasing generation performance. To tackle the first challenge, we conducted numerous experimental attempts. These intuitions suggested that focusing the model on learning the key actions within long motion sequence, while using limited implicit memory, would effectively address Mambas memory decay problem. Mask Motion Modeling has been explored in text-to-motion generation with methods like MMM (Pinyoanuntapong et al. 2024b), MoMask (Guo et al. 2024), and BAMM (Pinyoanuntapong et al. 2024a). However, the random masking used in these methods is less effective for extended motion generation. Instead of traditional random masking, we developed novel key frame masking strategy that calculates local density and pairwise distance to mask high-density motion embeddings in the latent space. This approach is more effective than random masking because it helps the model focus on learning key frames. Although key frame learning in motion generation has been explored by works like Diverse Dance (Pan et al. 2021) and KeyMotion (Geng et al. 2024), our method fundamentally differs from these previous approaches in selection and learning of key frames. Diverse Dance uses key frames as conditions to generate motion sequences around them. Similarly, KeyMotion treats key frames as anchors, generating key frames first and then performing motion infilling to complete the sequence. In contrast, our method introduces novel key frame selection technique based on local density, selecting high-density motion tokens as key frames. Instead of treating these key frames as conditions or anchors, we mask them out to enhance learning of motion representation. To address the second challenge, we revisit the text-tomotion task and the Mamba architecture. Although there have been attempts to address the multimodal fusion problem in Mamba for human motion modeling, such as using transformer mixer (Zhang et al. 2024c) or modifying selective scan (Wang, Kang, and Mu 2024), the results remain unsatisfactory. There are still misalignments between text descriptions and motion, especially when dealing with directions such as left and right or when the text queries are complex. Moreover, the misalignment between text and motion is not unique to the Mamba architecture, it is common issue that also affects other Transformer-based diffusion and autoregressive methods, as illustrated in Figure 2. Despite variety on architecture, existing methods share common approach, they use frozen CLIP text encoder to learn shared latent space for text and motion. This inspired us to improve text-motion alignment by designing robust contrastive learning paradigm that consistently learns the correspondence between motion and text, rather than relying on frozen CLIP encoder."
        },
        {
            "title": "Methodology",
            "content": "Key Frame Mask Modeling Our proposed key frame masking model introduces novel density-based key frame selection and masking strategy. First, we calculate the local density of each temporal token, then consecutively find the minimum distance to higher density. This process allows us to identify the tokens with the highest density as the key frame and mask them out. Local Density Calculation Let Rnl denotes the motion embedding in the latent space, where refers to the number of token in temporal dimension, and refers to the spatial dimension. = (x1, x2, ..., xn), xi Rl (1) We first compute the pairwise Euclidean distance matrix Rnn Di,j = xi xj2 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) k=1 (xi,k xj,k)2 (2) where xi and xj are the i-th and j-th rows of X, xi,k and xj,k are the k-th element of xi and xj. Then the local density Rn could be calculated as di = (cid:88) exp (D2 i,j) (3) which represents the sum of Gaussian kernel values centered as each latent vector xi, where the kernel bandwidth is determined by the squared distance Hence, the local density for the i-th token can be summai,j. rized by (cid:88) di = exp (xi xj2 2) (4) where xi is the latent vector for the i-th token. Minimum Distance to Higher Density We expand the local density into two intermediate matrices dcol R1n and drow Rn1 for broadcasting, ensuring that each column and row is duplicate of the local density d. We then create boolean mask matrix {0, 1}nn. Please note that this masking is intended to find the minimum distance to higher density, which is different concept from masking frames. (cid:26)1, if dcol,i < drow,j Mi,j = (5) 0, otherwise This means that Mi,j is 1 (True) only if the local density of the i-th token is less than the local density of the j-th token. We then apply the mask to distance matrix in-place Di,j = (cid:26)Di,j, if Mi,j = 1 , if Mi,j = 0 (6) This effectively sets all distances to infinity where the mask is 0 (False), meaning we discard distances from tokens to other tokens with lower or equal density. The masking operation ensures that for each token i, we only consider distances to other tokens that have strictly higher local density Dmasked = + (1 M) (7) where is the element-wise (Hadamard) product and 1 is matrix of all ones. This prepares the distance matrix for the subsequent step of finding the minimum distance to higher-density token. This can give us the masked distance matrix Dmasked Rnn, where distances to lower or equal density tokens have been set to infinity. For each row (corresponding to each token), we find the minimum distance along the columns of Dmasked: Si = min Dmasked,i,j (8) Due to the masking, this minimum value will be either: The actual minimum Euclidean distance to token with strictly higher density, if such token exists. Infinity, if no token with higher density exists. The resulting minimum distances are collected in Rn, which represents the distance to higher density for all frames. Hence, the minimum distance to higher density, denoted as Si for the i-th token, is calculated as Si = min j:dj >di xi xj2 (9) Key Frame Masking After calculating the local density and the minimum distance to higher density, we can determine the density parameter for all temporal tokens, denoted as Γ Rn. Γ = S, Γi = di Si (10) where Γi is the density parameter for the i-th token, di is the local density for the i-th token, and si is the distance to higher density for the i-th token. Hence, based on the density parameter Γ, we can select the temporal tokens with the highest density as the key frames in the motion latent space. = argmax : Γi (11) where is the index of the selected key frames, and Γi represents the index corresponding to the maxiargmax mum value in the Γ matrix. After obtaining the key frame index K, we can perform unidirectional mask along with the padding mask on Mambas sequential architecture. Text-Motion Alignment Text-to-motion alignment remains significant challenge in human motion generation tasks. This challenge arises because generation models, whether based on transformers or diffusion approaches, struggle to effectively understand the text features embedded by the CLIP encoder. This results in misalignment between the text and motion modalities. From latent space perspective, motion generation models operate within two distinct latent spaces: the text features encoded by CLIP and the motion features generated by the motion model. The substantial gap between these two modalities represents core challenge. Most previous works leverage CLIP as semantically rich text encoder, keeping it frozen while injecting text embeddings extracted from it into the generation model. In the context of multi-modal fusion, two latent spaces, z1 and z2, are typically aligned using an alignment mechanism falign. In our case, z1 and z2 correspond to the text latent space ztext and the motion latent space zmotion, respectively. In the common practice of motion generation tasks, the CLIP text encoder is frozen, and no explicit alignment mechanism is employed. Consequently, the generation model is implicitly required to learn the alignment between these modalities. However, since the generation model is not specifically designed to address the significant gap between the text and motion modalities, this often leads to misalignment. To address this issue, we propose leveraging contrastive learning objective to reduce the distance between these two latent spaces. This approach aims to decrease the learning difficulty and enhance the models overall multi-modal capabilities and performance. To be more specific, our text-motion alignment can be described as follows: Let Ti be the text latents for the i-th sample, and Mj be the motion latents for the j-th sample. The similarity between text latents Ti and motion latents Mj is calculated as: (12) Then, the similarity is scaled by the temperature paramei Mj simij = ter τ : simij = Mj τ (13) Furthermore, we define the contrastive labels as = [0, 1, 2, . . . , 1]. The contrastive loss for text and motion embedding can be represented as: Lcontrast = 1 2 (cid:16) CrossEntropy(sim, y) + CrossEntropy(sim, y) (cid:17) sequences, consisting of 65,926 segments, each with its corresponding textual label. BABEL-D Dataset To evaluate the performance of textmotion alignment in extended motion generation methods, we introduce new benchmark, BABEL-D. This benchmark is subset of the BABEL test set and includes directional conditions with keywords such as left and right. This also represents the more challenging subset of BABEL. The BABEL-D dataset contains total of 560 motion segments, enabling us to demonstrate improved alignment between generated motion and given text queries. We then evaluate our methods performance on BABEL-D and compare it with other state-of-the-art extended motion generation approaches. Why not HumanML3D? Despite HumanML3D (Guo et al. 2022) is an important dataset for text-to-motion generation, however its not suitable for extended motion generation since it only contains motion segments which is less than 196 frames. This limitation restrict the potential for evaluating long motion generation and thereby cannot reflect the real long sequence generation capability of these methods. However, the quality of text annotations in HumanML3D still benefits text-to-motion generation in general, including long motion generation. To showcase our methods generalizability, our qualitative visualization combines different text queries from HumanML3D for long motion generation. Evaluation Matrices For our experiments, we adopted the quantitative evaluation matrices for text-to-motion generation originally introduced by T2M (Guo et al. 2022) and later used in long motion generation studies (Shafir et al. 2023; Barquero, Escalera, and Palmero 2024; Zhang et al. 2024b). These include: (1) Frechet Inception Distance (FID), which measures overall motion quality by assessing the distributional difference between the high-level features of generated and real motions; (2) R-precision; (3) MultiModal Distance, both of which evaluate the semantic alignment between the input text and generated motions; and (4) Diversity, which calculates the variance in features extracted from the motions. The main difference in our evaluation compared to previous long motion generation methods is that we treat complex text queries as whole, rather than generating separate motion segments then interpolating transitions between them. This eliminates the need to evaluate transition interpolation and represents significant advancement in long motion generation. (14)"
        },
        {
            "title": "Comparative Studies",
            "content": "Experiments Datasets and Evaluation Matrices BABEL Dataset BABEL dataset (Punnakkal et al. 2021) is the go-to benchmark for long motion generation and has been widely adopted in previous extended motion generation work. Derived from AMASS (Mahmood et al. 2019), BABEL provides detailed annotations for extended motion sequences, with each segment linked to specific textual annotation. The dataset includes total of 10,881 motion Evaluation on BABEL To evaluate the performance of our KMM on extended motion generation, we trained and evaluated it on the BABEL dataset. The results, as shown in Tables 1, indicate that our method significantly outperforms previous text-to-motion generation approaches specifically designed for long-sequence motion generation. All experiments were conducted with batch size of 256 for RVQVAE, which utilized 6 quantization layers, and batch size of 64 for mask bidirectional Mamba. These experiments were carried out on single Intel Xeon Platinum 8360Y Models Ground Truth TEACH TEACH w/o Spherical Linear Interpolation TEACH PriorMDM PriorMDM w/ Trans. Emb PriorMDM w/ Trans. Emb & geo losses PriorMDM PriorMDM w/ PCCAT and APE MultiDiffusion DiffCollage T2LM FlowMDM Motion Mamba InfiniMotion KMM (Ours) R-precision 0.7150.003 0.4600.000 0.7030.002 0.6550.002 0.4300.000 0.4800.000 0.4500.000 0.5960.005 0.6680.005 0.7020.005 0.6710.003 0.5890.000 0.7020.004 0.4900.000 0.5100.000 0.6660.001 FID 0.000.00 1.120.00 1.710.03 1.820.02 1.040.00 0.790.00 0.910.00 3.160.06 1.330.04 1.740.04 1.450.05 0.660.00 0.990.04 0.760.00 0.580.00 0.340.01 Diversity MM-Dist 3.360.00 8.420.15 7.140.00 8.280.00 3.430.01 8.180.14 3.720.01 7.960.11 7.390.00 8.140.00 6.970.00 8.160.00 7.090.00 8.160.00 4.170.02 7.530.11 3.670.03 7.980.12 3.430.02 8.370.13 3.710.01 7.930.09 3.810.00 8.990.00 3.450.02 8.360.13 8.390.00 4.970.00 4.890.00 8.670.00 3.110.01 8.670.14 Table 1: This table presents comparison between our method and previous long motion generation techniques on the BABEL dataset (Punnakkal et al. 2021). The results show that our method outperforms the others, demonstrating superior performance. The right arrow indicates that closer values to real motion are better. Bold and underline highlight the best and secondbest results, respectively. Additionally, denotes results reproduced by FlowMDM. For results with 0.000 or 0.00, the corresponding paper does not provide error bars. Models Ground Truth PriorMDM KMM w/o Alignment KMM (Ours) R-precision 0.4380.000 0.3340.015 0.4840.007 0.5380.009 FID 0.020.00 6.820.76 5.500.15 3.860. Diversity MM-Dist 3.710.00 8.460.00 7.440.12 7.270.33 8.440.15 3.480.03 2.720.03 8.040.14 Models Ground Truth KMM w/ random masking KMM w/o Alignment KMM (Ours) R-precision 0.7150.003 0.6490.001 0.6710.001 0.6660. FID 0.000.00 0.480.01 0.400.01 0.340.01 Diversity MM-Dist 3.360.00 8.420.15 3.300.01 8.800.06 8.570.05 3.210.01 3.110.01 8.670.14 Table 2: This table compares our method with previous long motion generation techniques on the BABEL-D benchmark. The results demonstrate that our method excels in handling directional instructions, highlighting the advantages of our proposed text-motion alignment approach. The right arrow indicates that closer values to real motion are better. Bold and underline highlight the best and second-best results, respectively. CPU at 2.40GHz, paired with single NVIDIA A100 40G GPU and 32GB of RAM. Evaluation on BABEL-D To quantitatively demonstrate the advantages of our proposed text-motion alignment method in addressing directional instructions, we conducted comprehensive experiments on the newly introduced BABEL-D benchmark. The results have shown in Table 2. Compared to previous state-of-the-art methods, our approach significantly outperforms other extended motion generation techniques, indicating stronger alignment between text and motion. Ablation Study To further evaluate different aspects of our methods impact on overall performance, we conducted various ablation studies on the BABEL dataset, as shown in Table 3. The results show that our approach substantially outperforms other masking strategies, including random masking, KMeans (Lloyd 1982), and GMM (Reynolds et al. 2009) key frame selection. Additionally, our proposed text-motion alignment framework greatly improves the models ability Table 3: This table illustrates the ablation results from different aspects of the proposed method. The results show that both the key frame masking strategy and text-motion alignment contribute to the overall performance. The right arrow indicates that closer values to real motion are better. Bold and underline highlight the best and second-best results, respectively. to understand complex text queries, leading to better-aligned motion sequences. Conclusion In conclusion, our study addresses two significant challenges in extended motion generation: memory decay in long sequence generation and weak text-motion alignment. Our proposed method, KMM, presents innovative solutions that significantly advance the field. Our density-based key frame selection and masking strategy enhances Mambas ability to focus on critical actions within long motion sequences, effectively mitigating the memory decay problem. Additionally, our robust contrastive learning paradigm improves text-motion alignment, enabling more accurate motion generation for complex and directional text queries. Furthermore, the development of the BABEL-D benchmark provides valuable resource for evaluating text-motion alignment in extended motion generation, specifically focused on directional instructions. This new dataset, alongside our comprehensive experiments on the BABEL dataset, underscores our commitment to advancing the field of motion generation across various domains."
        },
        {
            "title": "KMM Supplementary Materials",
            "content": "(Punnakkal et al. 2021) test sets. The results, shown in figure 6, highlight superior performance in generating robust and diverse motions that closely align with lengthy and complex text queries. User Study In this work, we conduct comprehensive evaluation of KMMs performance through both qualitative analyses across various datasets and user study to assess its realworld applicability. We generated diverse set of 15 motion sequences, randomly extracted and combined from the HumanML3D (Guo et al. 2022) and BABEL (Punnakkal et al. 2021) test set, using three different methods: TEACH (Athanasiou et al. 2022), PriorMDM (Shafir et al. 2023), and FlowMDM (Barquero, Escalera, and Palmero 2024), alongside the generative results of KMM. Fifty participants were randomly selected to evaluate the motion sequences generated by these methods. The user study was conducted via Google Forms interface, as shown in figure 4, ensuring that the sequences were presented anonymously without revealing their generative model origins. Our analysis centered on four key dimensions: The fidelity of text-motion alignment for directional instructions. The robustness of the generated motion. The diversity of the generated sequences. The overall performance and real-world usability. The results shows that: There is 92% of users who believe that KMM offers better motion alignment in directional instructions than other methods. There is 78% of users who believe our method produces more robust and realistic motion with significantly fewer unrealistic rotation angles. There is 84% of users who believe that KMM generates more diverse and dynamic motion compared to the other three methods. For overall performance, there is 64% of users who believe that our generation results are satisfactory and have strong potential for real-world applications. Qualitative Comparison To further evaluate our method qualitatively, we compared KMM with TEACH (Athanasiou et al. 2022), PriorMDM (Shafir et al. 2023), and FlowMDM (Barquero, Escalera, and Palmero 2024) by generating diverse set of prompts, randomly extracted and combined from the HumanML3D (Guo et al. 2022) and BABEL (Punnakkal et al. 2021) test sets. Figure 5 shows three of these comparisons, demonstrating that our method significantly outperforms others in handling complex text queries and generating more accurate corresponding motions. Generative Results Visualization To further demonstrate the robustness and diversity of motions generated by our KMM, we produced 15 additional sequences using text prompts randomly extracted and combined from the HumanML3D (Guo et al. 2022) and BABEL Figure 4: The figure shows the user study interface where 50 participants evaluated motion sequences generated by TEACH, PriorMDM, FlowMDM, and KMM, focusing on text-motion alignment, robustness, diversity, and usability. The text prompt are randomly extracted and combined from the HumanML3D (Guo et al. 2022) and BABEL (Punnakkal et al. 2021) test set. Figure 5: The figure demonstrates qualitative comparison between the previous state-of-the-art method in extended motion generation and our KMM. The qualitative results show that our method significantly outperforms others in handling complex text queries and generating more accurate corresponding motions. Figure 6: The figure presents some qualitative visualization results of our proposed KMM model. The text prompts are sourced and combined from HumanML3D (Guo et al. 2022) and BABEL (Punnakkal et al. 2021). The number within the brackets indicates our ability to condition the generated motion on specific length, dynamically producing motion of the desired duration. The visualizations showcase KMMs superior performance in generating robust and diverse motions that align closely with lengthy and complex text queries. References Ahuja, C.; and Morency, L.-P. 2019. Language2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision (3DV), 719728. IEEE. Athanasiou, N.; Petrovich, M.; Black, M. J.; and Varol, G. 2022. Teach: Temporal action composition for 3d humans. In 2022 International Conference on 3D Vision (3DV), 414 423. IEEE. Barquero, G.; Escalera, S.; and Palmero, C. 2024. Seamless human motion composition with blended positional enIn Proceedings of the IEEE/CVF Conference on codings. Computer Vision and Pattern Recognition, 457469. Barsoum, E.; Kender, J.; and Liu, Z. 2018. Hp-gan: Probabilistic 3d human motion prediction via gan. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 14181427. Chen, X.; Jiang, B.; Liu, W.; Huang, Z.; Fu, B.; Chen, T.; and Yu, G. 2023. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1800018010. Dao, T.; and Gu, A. 2024. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060. Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34: 87808794. Dong, W.; Zhu, H.; Lin, S.; Luo, X.; Shen, Y.; Liu, X.; Zhang, J.; Guo, G.; and Zhang, B. 2024. Fusionmamba for cross-modality object detection. arXiv preprint arXiv:2404.09146. Geng, Z.; Han, C.; Hayder, Z.; Liu, J.; Shah, M.; and Mian, A. 2024. Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer. arXiv preprint arXiv:2405.15439. Gong, K.; Lian, D.; Chang, H.; Guo, C.; Jiang, Z.; Zuo, X.; Mi, M. B.; and Wang, X. 2023. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 99429952. Gu, A.; and Dao, T. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Guo, C.; Mu, Y.; Javed, M. G.; Wang, S.; and Cheng, L. 2024. Momask: Generative masked modeling of 3d human In Proceedings of the IEEE/CVF Conference on motions. Computer Vision and Pattern Recognition, 19001910. Guo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and Cheng, L. 2022. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5152 5161. Harvey, F. G.; Yurick, M.; Nowrouzezahrai, D.; and Pal, C. 2020. Robust motion in-betweening. ACM Transactions on Graphics (TOG), 39(4): 601. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33: 68406851. Hopfield, J. J. 1982. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8): 25542558. Jiang, B.; Chen, X.; Liu, W.; Yu, J.; Yu, G.; and Chen, T. 2024. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36. Kingma, D. P.; and Welling, M. 2014. Auto-Encoding Variational Bayes. stat, 1050: 1. Lee, T.; Baradel, F.; Lucas, T.; Lee, K. M.; and Rogez, G. 2024. T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18671876. Lee, T.; Moon, G.; and Lee, K. M. 2023. Multiact: Longterm 3d human motion generation from multiple action labels. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 12311239. Lin, X.; and Amer, M. R. 2018. Human motion modeling using dvgans. arXiv preprint arXiv:1804.10652. Lloyd, S. 1982. Least squares quantization in PCM. IEEE transactions on information theory, 28(2): 129137. Mahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.; and Black, M. J. 2019. AMASS: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, 54425451. Pan, J.; Wang, S.; Bai, J.; and Dai, J. 2021. Diverse dance synthesis via keyframes with transformer controllers. In Computer Graphics Forum, volume 40, 7183. Wiley Online Library. Petrovich, M.; Black, M. J.; and Varol, G. 2022. TEMOS: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision, 480 497. Springer. Petrovich, M.; Litany, O.; Iqbal, U.; Black, M. J.; Varol, G.; Bin Peng, X.; and Rempe, D. 2024. Multi-track timeline control for text-driven 3d human motion generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 19111921. Pinyoanuntapong, E.; Saleem, M. U.; Wang, P.; Lee, M.; Das, S.; and Chen, C. 2024a. BAMM: Bidirectional Autoregressive Motion Model. arXiv preprint arXiv:2403.19435. Pinyoanuntapong, E.; Wang, P.; Lee, M.; and Chen, C. 2024b. Mmm: Generative masked motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15461555. Punnakkal, A. R.; Chandrasekaran, A.; Athanasiou, N.; Quiros-Ramirez, A.; and Black, M. J. 2021. BABEL: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 722731. Raab, S.; Leibovitch, I.; Tevet, G.; Arar, M.; Bermano, A. H.; and Cohen-Or, D. 2023. Single motion diffusion. arXiv preprint arXiv:2302.05905. Zhang, Q.; Song, J.; Huang, X.; Chen, Y.; and Liu, M.- Y. 2023b. Diffcollage: Parallel generation of large content with diffusion models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10188 10198. IEEE. Zhang, Z.; Liu, A.; Chen, Q.; Chen, F.; Reid, I.; Hartley, R.; Zhuang, B.; and Tang, H. 2024b. InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation. arXiv preprint arXiv:2407.10061. Zhang, Z.; Liu, A.; Reid, I.; Hartley, R.; Zhuang, B.; and Tang, H. 2024c. Motion mamba: Efficient and long sequence motion generation with hierarchical and bidirectional selective ssm. arXiv preprint arXiv:2403.07487. Zhang, Z.; Wang, Y.; Wu, B.; Chen, S.; Zhang, Z.; Huang, S.; Zhang, W.; Fang, M.; Chen, L.; and Zhao, Y. 2024d. Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion. arXiv preprint arXiv:2405.11286. Zhong, C.; Hu, L.; Zhang, Z.; and Xia, S. 2023. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision, 509519. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PMLR. Reynolds, D. A.; et al. 2009. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663). Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Shafir, Y.; Tevet, G.; Kapon, R.; and Bermano, A. H. 2023. Human motion diffusion as generative prior. arXiv preprint arXiv:2303.01418. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Deep unsupervised learning using Ganguli, S. 2015. In International confernonequilibrium thermodynamics. ence on machine learning, 22562265. PMLR. Tevet, G.; Gordon, B.; Hertz, A.; Bermano, A. H.; and Cohen-Or, D. 2022a. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, 358374. Springer. Tevet, G.; Raab, S.; Gordon, B.; Shafir, Y.; Cohen-or, D.; and Bermano, A. H. 2022b. Human Motion Diffusion Model. In The Eleventh International Conference on Learning Representations. Van Den Oord, A.; Vinyals, O.; et al. 2017. Neural discrete representation learning. Advances in neural information processing systems, 30. Vaswani, A. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762. Wang, X.; Kang, Z.; and Mu, Y. 2024. Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion. arXiv preprint arXiv:2404.11375. Xiao, Z.; Wang, T.; Wang, J.; Cao, J.; Zhang, W.; Dai, B.; Lin, D.; and Pang, J. 2023. Unified human-scene interaction via prompted chain-of-contacts. arXiv preprint arXiv:2309.07918. Xie, X.; Cui, Y.; Ieong, C.-I.; Tan, T.; Zhang, X.; Zheng, X.; and Yu, Z. 2024. Fusionmamba: Dynamic feature enhancement for multimodal image fusion with mamba. arXiv preprint arXiv:2404.09498. Yang, Z.; Su, B.; and Wen, J.-R. 2023. Synthesizing longterm human motions with diffusion models via coherent In Proceedings of the 31st ACM International sampling. Conference on Multimedia, 39543964. Zhang, M.; Cai, Z.; Pan, L.; Hong, F.; Guo, X.; Yang, L.; and Liu, Z. 2024a. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence. Zhang, M.; Guo, X.; Pan, L.; Cai, Z.; Hong, F.; Li, H.; Yang, L.; and Liu, Z. 2023a. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 364373."
        }
    ],
    "affiliations": [
        "Peking University",
        "The University of Adelaide",
        "The Australian National University",
        "The University of Sydney",
        "Monash University",
        "McGill University"
    ]
}