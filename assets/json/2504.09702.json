{
    "paper_title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?",
    "authors": [
        "Yunxiang Zhang",
        "Muhammad Khalifa",
        "Shitanshu Bhushan",
        "Grant D Murphy",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Moontae Lee",
        "Honglak Lee",
        "Lu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AI's research capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 0 7 9 0 . 4 0 5 2 : r Leaderboard: https://huggingface.co/spaces/launch/MLRC Bench MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Yunxiang Zhang 1 Muhammad Khalifa 1 Shitanshu Bhushan 1 Grant Murphy 1 Lajanugen Logeswaran 2 Jaekyeom Kim 2 Moontae Lee 2 3 Honglak Lee 1 2 Lu Wang"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-BENCH, benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAIs MLE-Bench (Chan et al., 2024) and METRs RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the endto-end agentic pipeline by using LLM-as-a-judge, MLRC-BENCH measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-BENCH is dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AIs research capabilities. 1University of Michigan, Ann Arbor 2LG AI Research 3University of Illinois, Chicago. Correspondence to: Yunxiang Zhang <yunxiang@umich.edu>. 1 Evaluating large language model (LLM) research agents (Baek et al., 2024; Li et al., 2024b; Lu et al., 2024b) has so far been restricted to one of two directions. The first direction involves tasking the agent with end-to-end scientific discoveryproposing research idea, writing implementation code, running experiments, and eventually producing full paper as done by AI Scientist (Lu et al., 2024b). One issue with such evaluation is the lack of reliable baseline method that enables objective evaluation of the proposed approach. The second direction, on the other hand, evaluates the agents ability to produce code that solves Kaggle-style machine learning (ML) engineering competition, skipping idea proposal and paper writing altogether (Huang et al., 2024a; Chan et al., 2024). While evaluation in this case is straightforward, this setup rarely demands genuine research novelty beyond existing methods. Consequently, neither of these setups paints full picture of whether LLM research agents are able to come up with research ideas that are both novel and effective, which we aim to address here. Competitions at ML conferences and workshops provide valuable testbed for evaluating research agents by assessing both novelty and effectiveness against established baselines. Unlike Kaggle-style contests, these challenges address unresolved and important problems recognized by the ML community. In addition, public leaderboards facilitate objective comparisons to human experts. If an algorithm truly outperforms known baselines, improvements in benchmark scores provide reliable signal as to the effectiveness of the proposed method. Therefore, this paper introduces MLRC-BENCH as benchmark to evaluate the ability of LLM-based research agents to propose and implement novel methods. Drawing on tasks from recent ML conference competitions, MLRCBENCH enables the evaluation of both novelty and effectiveness of research agents ideas compared to reliable baseline method and the top human solution. In particular, it emphasizes objective metrics on tasks such as LLM merging (Tam et al., 2024) and machine unlearning (Triantafillou et al., 2024), closely mirroring ongoing research challenges. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Table 1. Comparison between MLRC-BENCH and existing work on automated scientific discovery in machine learning with LLM agents. means that some but not all of the tasks in that benchmark require the indicated capability. Compute Constraints indicates whether the solution code must adhere to specified runtime and GPU memory limitations, an important aspect ignored by most prior work. Problem Identification Method Proposal Experiment Design Code Implementation Evaluation Method Evaluation Object Compute Constraints (cid:33) (cid:33) (cid:33) (cid:33) AI Scientist (Lu et al., 2024b) Can LLMs Generate Novel Research Ideas? (Si et al., 2024) DiscoPOP (Lu et al., 2024a) MLE-Bench (Chan et al., 2024) MLAgentBench (Huang et al., 2024a) MLRC-BENCH (Ours) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) LLM & Human Judge Paper Human Judge Idea Proposal Performance-Based Function-Level Code Performance-Based File-Level Code Performance-Based File-Level Code Performance-Based Repository-Level Code (cid:33) Moreover, the challenges in MLRC-BENCH can dynamically grow by incorporating new competitions emerging from future ML conferences and workshops. We curate MLRC-BENCH starting with 7 competition tasks. We pick tasks that involve novel and high-impact problems, spanning areas including LLM safety, multimodal perception, and few-shot learning. Our experimental findings reveal that even the best-performing tested LLM agents, such as gemini-exp-1206 (Pichai, 2024) under the MLAB (Huang et al., 2024a) scaffolding, closes only 9.3% of the gap between baseline and top human participant score. Additionally, our analysis highlights poor correlation between the novelty judged by LLM and practical effectiveness of agents solutions, questioning the reliability of LLM-as-a-judge for research idea evaluation. These results underscore the limitations of current AI research agents in generating and implementing innovative ML solutions, providing crucial benchmark for future advancements. Our contributions can be summarized as below: We introduce MLRC-BENCH, dynamic benchmark suite curated from ML conference competitions, featuring open research problems that are both impactful and objectively measurable, and that demand the development of novel methodologies. We conduct large-scale, objective evaluations for wide array of prominent LLMs with representative agent scaffoldings, highlighting their inability to propose and implement innovative solutions with notable performance gains. We quantify the gap in subjective evaluations of LLMbased research agents, by showing that the perceived idea novelty is misaligned with practical effectiveness. 2. Related Work Table 1 presents summary of the differences between our benchmark and existing work on automating ML research workflow with LLM agents. Scientific discovery in machine learning typically includes four main stages: Problem Identification, where gaps in existing methods are recognized; Method Proposal, which introduces new approach to address the issue; Experiment Design, involving the selection of datasets, baselines, and metrics for evaluation; and Code Implementation, where the method is realized through executable code. While prior work (Lu et al., 2024b; Si et al., 2024) covers Problem Identification and Experiment Design, evaluation could be subjective based on idea proposal or final paper. Instead, MLRC-BENCH focuses on the critical stages of proposing and implementing novel methods, enabling objective performance assessment. While there are recent benchmarks that focus on code generation in machine learning domain, they do not always require methodological innovation. Works like MLAgentBench (Huang et al., 2024a) and MLE-Bench (Chan et al., 2024) evaluate agents on Kaggle-style ML tasks but prioritize code implementation over novel research contributions. Broader benchmarks such as ScienceAgentBench (Chen et al., 2024b) and DiscoveryBench (Majumder et al., 2024) span multiple scientific domains but lack granularity for ML-specific challenges, while CHIME (Gao et al., 2024) and OpenD5 (Du et al., 2024) target auxiliary tasks like literature review or hypothesis generation. DSBench (Jing et al., 2024) and AAAR-1.0 (Lou et al., 2024) extend evaluations to data science and general R&D workflows but still fall short of addressing cutting-edge ML research innovation. Unlike DiscoPOP (Lu et al., 2024a) and DACode (Huang et al., 2024b), which focus on function-level 2 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Table 2. 7 MLRC-BENCH tasks representing cutting-edge machine learning research. For each competition, we show the venue where the competition is held, research area, data modality, performance metric, along with the constraints presented to the agents, including maximum allowed runtime and GPU memory based on our hardware configurations. Detailed task descriptions are given in Appendix A. Competition Venue Research Area Modality Metric Test Runtime GPU Memory LLM Merging (Tam et al., 2024) Backdoor Trigger Recovery (Xiang et al., 2024) NeurIPS 2024 NeurIPS 2024 Efficient LLM LLM Safety Temporal Action Localisation (Heyward et al., 2024) ECCV 2024 Workshop Multimodal Perception Text Text Video, Audio Accuracy, ROUGE REASR, Recall 1 hour 48 GB 0.5 hour 48 GB mAP 0.5 hour 16 GB Rainfall Prediction (Gruca et al., 2022) Machine Unlearning (Triantafillou et al., 2024) NeurIPS NeurIPS 2023 AI for Science Satellite Data Critical Success Index 0.5 hour 48 GB Data Privacy Image Forgetting Quality, Accuracy 0.5 hour 16 GB Next Product Recommendation (Jin et al., 2023) KDD Cup Recommendation System Cross-Domain Meta Learning (Carrion-Ojeda et al., 2022) NeurIPS 2022 Few-Shot Learning Text Mean Reciprocal Rank 0.5 hour 16 GB Image Accuracy 3.5 hours 16 GB coding or data science, MLRC-BENCH requires repositorylevel code comprehension and generation. RE-Bench (Wijk et al., 2024) and MLGym (Nathani et al., 2025) provide collections of ML research task environments, but their problems either fail to represent most recent research directions (e.g., image classification with CIFAR-10 (Krizhevsky et al., 2009)), or only cover narrow range of research domains. Besides, existing benchmarks often fail to specify computation constraints (e.g. runtime and GPU memory limit), which are important to encourage efficient yet effective solutions. Because many of these benchmarks are static, they risk becoming contaminated soon after the release of next-generation LLMs. In contrast, our proposed benchmark emphasizes verifiable performance gains on impactful, unsolved research problems, and it can be updated by replacing solved challenges with new competitions to track progress against human experts. Frameworks like The AI Scientist (Lu et al., 2024b) and MLR-Copilot (Li et al., 2024b) automate end-to-end research workflows but rely largely on subjective reviews of papers or research proposals for evaluating success. In parallel, ResearchAgent (Baek et al., 2024) iteratively refines ideas through multi-agent feedback, and Chain-of-IdeaAgent (Li et al., 2024a) organizes literature into progressive chains to stimulate ideation. However, it remains unclear how subjectively evaluated novel ideas translate into actual performance gains. In contrast, we explicitly investigate how such subjective assessments of novelty or idea quality alignor fail to alignwith measurable performance improvements. By anchoring evaluations in real-world ML conference competitions, MLRC-BENCH emphasizes the importance of balancing subjective judgments with concrete, performance-driven benchmarksthus closing critical gap in the evaluation landscape for AI research agents. 3. MLRC-BENCH 3.1. Task Selection MLRC-BENCH prizes high-quality competitions that are both non-trivial and reproducible. To form our dataset, we screen competitions held at recent machine learning, natural language processing, data mining, and computer vision conferences or workshops using the following criteria: Novel Research-Focused: The tasks should require genuine methodological innovation, rather than being solvable through purely brute-force or superficial engineering approaches, such as exhaustive search for hyperparameters or features without any theoretical motivation or problem understanding. Non-Trivial: The problem must involve complexity so that it will not be solved by simply applying standard ML algorithms, e.g., calling the XGBoost classifier (Chen & Guestrin, 2016) on new dataset or prompt engineering with LLMs. Feasible: Starter code, data splits, and evaluation procedures must be publicly available so that researchers, either human or agentic AI, can reproduce the experiments while keeping computational costs manageable. The current version of MLRC-BENCH comprises 7 tasks adapted from competitions. These tasks represent diverse landscape of applied ML research. As shown in Table 2, topics range from LLM safety to multimodal perception, ensuring that benchmarks stress multiple facets of algorith3 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 1. Overview of our MLRC-BENCH and the evaluation pipeline. mic creativity. Besides, these tasks are cutting-edge. We primarily select competitions released within the past one year, whose problems remain actively researched, preventing trivial or purely off-the-shelf solutions. In addition, we have the following two considerations for MLRC-BENCH design. Continual Updates. MLR-Bench is designed to be updatable. Future releases will incorporate new competitions from the latest conferences and retire old ones if model performance saturates. This mechanism helps track agent progress on fresh, unsolved research questions, mirroring the continually advancing state of ML research. Data Contamination Mitigation. Many of these competitions occurred after the pre-training cut-off dates for major LLMs. As aforementioned, we will also regularly update the benchmark with the latest suitable competitions. As result, there is low chance that the top-performing solutions have already been memorized. This helps ensure that the benchmark measures the agents genuine research and development skills rather than simple data retrieval and reproduction. Computational Constraints. MLR-Bench specifies explicit computation constraintssuch as runtime and GPU memory limitsfor each task, mirroring real-world competition scenarios. These standardized constraints promote fair comparisons across different agents, preventing any from gaining unfair advantages through excessive computational resources. By enforcing these limitations, we encourage agents to propose and implement efficient yet effective methods, thereby discouraging trivial or brute-force solutions and incentivizing genuine innovation. 3.2. Task Environment MLRC-BENCH offers modular, agent-agnostic environment for specifying and automatically evaluating agents on research tasks. As shown in Figure 1, for each task, we provide: Task Description. detailed account of the research problem, including essential terminology, data format, desired model outputs, and constraints (e.g., limitations of model size or training time). Starter Code. Refactored from official competition repositories, it contains: simple, baseline model for comparison. python environment with the necessary ML frameworks/packages. Scripts for training, inference, and offline or online evaluation. Train, development and test data splits. Training data may not be available for some competitions. Human Idea. Insights from state-of-the-art papers or top-participant solution reports are included. Agents can optionally utilize these ideas to refine or inspire their own solutions. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Task-Agonistic Starter Code Structure. Because our primary goal is to focus on method development, we simplify ML experimentation by unifying the code execution pipeline. Some competition starter kits originally feature complex or inconsistent file structures, so we refactor each one into standardized, well-organized format, comparable to common ML research project layouts (Figure 1).1 The resulting codebase allows users to launch experiments with single command: python main.py --method my awesome method --phase dev/test, which applies the specified method to the task and evaluates the result in both development and test phases. To ensure fair comparison and preserve the integrity of evaluations, the repository enforces file permission management: agents may only modify the methods/ directory (where the algorithmic logic resides in MyMethod.py), while evaluation scripts remain read-only. This setup both promotes methodological innovation and prevents tampering. Additionally, files containing held-out test data are invisible to agents during development phase. Development and Test Splits. We prioritize preventing overfitting by providing explicit development and test splits for each competition. Agents can choose to refine their implementations based on the development set and then submit their best-performing solution to hidden test set. Wherever possible, we use the original competition test set (via local evaluation or online leaderboard API). Otherwise, we partition the existing development data into custom dev and test sets, reproduce the top human solution if available, and evaluate it on our new test split for valid comparison. 3.3. Objective Evaluation Metrics MLRC-BENCH supports objective evaluation based on model performance, which provides clear and measurable way to assess agents capabilities of approaching ML research challenges. Moreover, we assess the complexity of an agents ideas and the efficiency of their implementations. Importantly, we believe our evaluation supports an easy yet effective and reproducible evaluation on agents research novelty. For instance, more innovative approaches may involve rich reasoning and sophisticated mechanisms to push the research frontier, thus more complex. On the other hand, efficiency of implementation highlights whether the agents consider the importance of optimizing these ideas for real-world application. By incorporating performance, complexity, and efficiency evaluations, we ensure that the novelty of an idea is not just theoretical but also practical, offering more objective measure of its value. Effectiveness. Effectiveness is paramount because an ineffective method offers little practical value, regardless 1See concrete example here. of its efficiency or simplicity. Therefore, the core metric is single performance metric (e.g., accuracy) defined by the competition organizer. Efficiency. Competitions typically set baseline compute limits, but faster methods are preferred. We measure each solutions runtime during training (if applicable) and inference. Simplicity. Inspired by standard practice in software estimation (Nguyen et al., 2007), we measure simplicity of agents solutions in terms of logical lines of code (LLoC). LLoC excludes comments and blank lines, focusing on executable statements. This metric, while imperfect, offers rough gauge of code complexity and maintainability (Bhatt et al., 2012). For better readability, we refer to LLoC as lines of code throughout this paper. Metric: Relative Improvement to Human. Quantitative performance comparisons across competitions can be tricky, as each task may differ significantly in its intrinsic difficulty, and the official baseline may be weaker or stronger. To address this, we use the Relative Improvement to Human as our main leaderboard metric that convert each raw performance score sagent into normalized score agent, using linear transformation (Burns et al., 2024; Wijk et al., 2024). Therefore, the score of the baseline solution will be 0, and the top human solution in competition is set to 100. Formally, the normalization is computed as: agent = sagent sbaseline stop human sbaseline 100(%) We repeat each agent for 8 trials, and report the best normalized performance. Normalized scores can exceed 100 if an agents solution surpasses the top human solution, although we have not observed normalized scores higher than 100. Solutions performing worse than the baseline will be assigned negative normalized score. 3.4. Evaluation Protocol Our evaluation protocol is meticulously designed to prevent AI agents from test set overfitting. Agents will submit their implementation in the form of an edited codebase, particularly within their proposed method in the methods/ directory. Specifically, in single trial, an agent can iteratively modify the codebase multiple times. We store snapshots of the codebase immediately after each change. Whenever an execution occurs on the development set, we record the resulting metrics and the name of evaluated method for that snapshot. At the end of this iterative development phase, we pick the snapshot with the highest development performance (effectiveness). We then evaluate the method 5 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Table 3. For each research competition and agent, we report the test-phase relative improvement to human, defined as the agents solution margin over the baseline normalized by the top human solutions margin over the baseline, taking the best of 8 trials. Top human participants in each competition will score 100.0 due to the normalization. Additionally, we evaluate two other gpt-4o-based pipelines: MLAB augmented with ideas from either CoI-Agent (Li et al., 2024a) or humans. Best performing agent in each task is highlighted in bold. Our results indicate that providing additional ideas, whether sourced from AI or humans, does not consistently yield performance improvements. The best-performing configurationgemini-exp-1206 under MLABachieves only 9.3% of the human-level improvement over baseline on average, underscoring the inherent difficulty of these research tasks. See Table 4 in Appendix for absolute improvements to baseline. Agent temporal -action-loc llm -merging meta -learning product -rec rainfall -pred machine -unlearning backdoor -trigger MLAB (gemini-exp-1206) MLAB (llama3-1-405b-instruct) MLAB (o3-mini) MLAB (claude-3-5-sonnet-v2) MLAB (gpt-4o) Human Idea + MLAB (gpt-4o) CoI-Agent Idea (o1) + MLAB (gpt-4o) -0.5 0.5 0.3 0.8 0.3 0.5 0.4 5.0 -1.0 -1.0 5.0 2.0 -1.0 -1.0 -1.1 -4.9 -4.9 -4.9 -4.9 -4.9 -4.9 0.1 0.0 0.1 3.0 0.6 2.2 0.1 43.1 31.5 25.1 14.6 47.5 12.3 39.4 5.6 6.2 3.6 -94.7 -18.0 6.8 11. 12.9 11.5 6.2 39.9 10.4 8.8 4.0 Avg 9.3 6.3 4.2 -5.2 5.4 3.5 7.1 contained in that snapshot on the test set2 for our final result. This approach strictly follows standard ML practice and ensures reproducible experimentation. Future work may explore more sophisticated multi-objective selection criteria that additionally weigh runtime (efficiency) or lines of code (simplicity) of implementations. 4. Experiments and Results To evaluate the capability of LLM agents in solving ML research tasks, we conduct comprehensive experiments across different agent scaffoldings and language models. Each agent trial is conducted either on single NVIDIA Quadro RTX 8000 GPU with 48GB of memory (for llm-merging, backdoor-trigger and rainfall-pred tasks) or Tesla V100 GPU with 16GB memory (for all other tasks), determined by the size of the base model used in each task. Unless otherwise specified, we perform 8 trials per configuration and report the best attempt. 4.1. Agent Scaffolding Comparison In addition to allow agents to directly propose and implement ideas, we investigate whether providing AI-generated or human-sourced ideas can enhance agent performance. Due to computational cost limits, we first evaluate GPT4o (Hurst et al., 2024) under three scaffolding configurations: MLAB: We apply the MLAB agent developed by Huang et al. (2024a). It is ReAct-style (Yao et al., 2023) agent that alternates between thinking (such as reflection, research planning, fact-checking) and taking actions (file system operations or Python script 2Concretely, we execute the command python main.py --method best dev method --phase test. execution) to implement methods. CoI-Agent Idea + MLAB: We augment MLAB with ideas generated by Chain-of-Ideas (CoI) (Li et al., 2024a), an LLM-based agent that structures relevant literature into progressive chains to enhance ideation. We choose strong model, OpenAIs o1 (Jaech et al., 2024), as the backbone for the ideation agent to generate more creative ideas. Human Idea + MLAB: To investigate whether agents can achieve notable performance gain given the right direction to work on, we provide MLAB with reliable human ideas extracted from state-of-the-art papers and top-performing competition participants reports. For all tasks except Rainfall Prediction, we allow MLAB agents maximum of 50 steps and 5 hours per trial, following the protocol established by Huang et al. (2024a). However, for Rainfall Prediction, we extend these limits to 100 steps and 10 hours, accounting for the increased baseline training time. The results in Table 3 show that incorporating additional ideaswhether generated by AI or proposed by humansdoes not consistently lead to performance gains. This highlights the limited effectiveness and generalizability of AI-generated ideas, and emphasize the challenges agents face in implementing even human-proposed solutions effectively. 4.2. Model Comparison Taking MLAB as our major scaffold, we evaluate five prominent LLMs: Claude 3.5 Sonnet v2 (Anthropic, 2024), gemini-exp-1206 (Pichai, 2024), Llama 3.1 405B Instruct (Dubey et al., 2024), o3-mini-high (OpenAI, 2025) and GPT-4o (2024-11-20) (Hurst et al., 2024). The results in Table 3 demonstrate varying success rates across models and tasks. Gemini-exp-1206 performs best among all MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? models, reaching an average 9.3% relative improvement to human. Claude 3.5 Sonnet V2 performs the best on most tasks except failing drastically on machine unlearning. The agents proposed algorithm likely failed on machine unlearning because it treated the removal of unwanted data and the preservation of useful knowledge as separate steps, rather than jointly optimizing both goals to maintain model performance. We provide more detailed case study of its failure in Section 4.6. Table 3 also reveals that agent solutions performance gains remain modest compared to human solutions in many cases, if not degrading baseline performance. There are, however, few notable exceptions. For instance, MLAB (gpt-4o) achieves score of 47.5 on the rainfall prediction task, likely because similar solutions (e.g., variants of U-Net (Ronneberger et al., 2015)) are readily available online. In the backdoor-trigger task, the baseline GCG method (Zou et al., 2023) performs poorlyessentially making random predictionsthereby lowering the bar for agents to surpass it with more meaningful solutions. This substantial gap highlights the current limitations of AI agents in generating novel, effective methods, underscoring the need for further advances to match or surpass human-led research efforts. 4.3. Inference-Time Scaling on ML Research Tasks Increasing inference-time compute via repeated sampling (Chan et al., 2024; Chen et al., 2024a; Brown et al., 2024) has been shown to boost LLM performance on reasoning and coding tasks. Here we explore how LLM research agents scale with more inference-time compute on both the idea and solution spaces. We sample 4 ideas for each task from CoI-Agent (Li et al., 2024a) and repeat MLAB agent for 8 trials to implement each idea into code. Figure 2 plots pass@k (Chen et al., 2021), i.e., the probability that at least one of trials converges to successful implementation, defined as the agent closes at least 5% of the gap between baseline and top human participant scores (Relative Improvement to Human, Section 3.3). Our results show that providing high-quality ideas enhances an agents ability to generate meaningful solutions when given multiple attempts, and human ideas appear to be more effective than those produced by AI. Furthermore, under fixed inference budget, we did not observe significant difference between allocating resources to idea exploration versus exploitation. For example, there is no significant pass@k difference between using 4 ideas with 2 trials per idea, 2 ideas with 4 trials per idea, and 1 idea with 8 trials per idea. This phenomenon likely occurs because once high-quality idea is identified, the performance gains from additional trials tend to plateau, resulting in diminishing returns despite further exploitation. We hypothesize that performance-informed tree-search that Figure 2. We measure Pass@k as we scale the number of trials and ideas, running MLAB for eight trials per idea. The total inferencetime computes are equivalent among these points: = 4 for one-idea line, = 2 for two-idea line, = 1 for four-idea line, and = 4 for the remaining lines. For results breakdown on each task, please refer to Figure 9 in Appendix B. Our results indicate that 1) providing high-quality ideasespecially human-generated onessignificantly boosts an agents success rate across multiple attempts, 2) while varying the balance between idea exploration and exploitation under fixed budget yields similar outcomes due to diminishing returns from repeated trials. navigates the vast space of possible solutions (Jiang et al., 2025; Koh et al., 2024) or allocating more computational resources (Chan et al., 2024) could offer more promising scaling properties, and we leave detailed exploration of these exciting directions for future work. 4.4. Subjective Evaluation with LLM-as-a-Judge Our benchmark also facilitates the investigation on whether LLM-as-a-judge style evaluations can reliably assess the quality of research ideas by comparing subjective (LLMjudged) and objective metrics. As shown in Figure 1, we first prompt an LLM to explain each implementations underlying idea.3 We choose OpenAIs o1 (Jaech et al., 2024) model here because of its superior reasoning and coding capabilities. We then apply the rubric from prior work (Baek et al., 2024) to have the o1 model assign 15 Likert score on five dimensions: validity, clarity, rigorousness, generalizability, and innovativeness. These scores are all the higher the better. The prompts used for this evaluation are shown in Appendix C. Furthermore, we examine how the presence of code influences the assessments through two settings. (1) Without Code, in which the LLM judges only access the task descrip3We instruct the MLAB agent to include detailed comments in the code to enable faithful, post-hoc explanations. 7 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 3. Radar plots of objective and subjective evaluations for agent-generated solutions across seven research tasks. Each dimension is normalized on 15 scale, where higher values indicate better performance. Objective metrics include effectiveness, efficiency, and simplicity, which are highlighted in bold. The rest are subjective metrics, assessed by prompting an LLM as judge. Notably, more effective solutions identified by agents tend to be more complex and time-consuming (e.g., in backdoor trigger recovery task). Additionally, overlapping scores in subjective dimensions suggest that LLM-based evaluation struggles to distinguish the research capabilities of different models. tion and the proposed idea; and (2) With Code, in which the judges also see the code implementation. We then compute Spearmans correlation (Spearman, 1904) for each pair of objective and subjective metrics, using data from all valid implementations that include test-phase scores. Figure 3s radar plots provide holistic view of agent performance across both subjective and objective dimensions. The plots show that while agents occasionally produce effective solutions, they often struggle to balance other criteria such as efficiency and simplicity. For instance, on the backdoor-trigger task, Claude 3.5 Sonnet V2 scores well on effectiveness but poorly on efficiency and simplicity, suggesting that agent-generated solutions tend to be more complex and time-consuming. Notably, agents generally underperform compared to the baseline when evaluated using objective metrics. However, when subjective metrics are usedwhere LLMs serve as judgesthey often receive more favorable ratings. This discrepancy highlights risk of overly optimistic conclusions when relying solely on subjective evaluations. Figures 4 and 8 (in Appendix B) illustrate the correlation heatmaps for both settings.4 The overall correlations remain 4We find that removing the code leads to similar correlation results and does not significantly affect the conclusion we make. 8 Figure 4. Correlation heatmap between objective (x-axis) and subjective (y-axis) metrics for agent-generated solutions across all tasks. In this setting, code is included when prompting the LLM to evaluate subjective dimensions. No strong correlation is observed, suggesting that LLM-judged subjective metrics may not reliably indicate real-world impact. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 5. We track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within trial of LLM-based MLAB agent on the development set. Performance improvement is the higher the better, while increased runtime and lines of code are the lower the better. For results breakdown on each task, please refer to Figure 10 and 11 in Appendix B. (1) GPT-4o and Claude gradually improve their performance through refinement, while other models plateau after few iterations; (2) runtime consistently increases, probably because models are exploring more complicated solutions over time, which may naturally conflate with better solutions; and (3) code size expands over time, reflecting increasingly complex solutions that do not yield proportional performance gains. Together, these trends suggest that agents tend to over-refine their solutions, resulting in more complex and time-consuming implementations without further performance improvements. In Figure 6, we looked at how MLAB (gemini-exp-1206) agent progress through different stages when tackling tasks, categorizing each step into stages like exploring the problem, evaluating initial performance, generating new ideas, coding solutions, debugging errors, refining solutions, and final evaluations. We perform stage classification using gpt4o with the prompt in Appendix F. We found that agents typically begin by spending lot of time understanding the problem environment, then quickly move into writing and editing code. Interestingly, they dont spend many steps brainstorming new ideas before jumping into implementation. This leads to frequent debugging, as rapid coding often introduces mistakes. Towards the end, instead of repeatedly refining their solutions, agents often rush to final testing and submission, suggesting theres room to improve by encouraging more thoughtful planning and iterative improvement. We additionally perform analysis on logs of agent traces and show the types, frequencies, and resolution patterns of coding errors, as well as agent behavior trends and capability levels, in Appendix D. We highlight two takeaways here. First, large proportion of environmental errors were attributable to incorrect tool argument issues, which can be conceptualized as instances where the model hallucinates or misidentifies expected argument names. Notably, these errors accounted for approximately 11.5% of all environFigure 6. Stage distribution across each step, annotated using GPT4o and grouped into seven distinct stages to illustrate shifts in task focus and activity over the course of all tasks. weak. For example, there is near-zero correlation (-0.06) between innovativeness and effectiveness, implying that an agents ability to generate novel ideas, as judged by an LLM, does not necessarily equate to success in practical tasks. Consequently, our finding indicates that LLM-based evaluations alone are not reliable proxy for real-world research impact. While LLM agents can certainly assist in generating creative ideas, relying solely on LLM-based evaluations to gauge agents progress in improving machine learning research may lead to misinterpretations. This again highlights the importance of employing objective metrics to ensure that proposed solutions are not only novel but also effective. 4.5. Implementation Process Analysis Figure 5 illustrates how performance, runtime, and code complexity evolve as agents iteratively refine their implementations within single trial. Three key trends emerge: 9 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? ment steps. Second, further examination of the overall error correction performance revealed that, across all evaluated tasks, the MLAB (gemini-exp-1206) agent resolved only 17.2% of the total errors encountered. This modest correction rate underscores the persistent difficulty of achieving robust error handling. 4.6. Case Study We present two case studies below to illustrate failed solutions implemented by LLM agents. Please see Appendix for the concrete code implemented by AI agents. LLM Merging Challenge: The objective is to develop novel and effective algorithm to merge several (in our case, two) expert models into single model that demonstrates improved performance on held-out test set within the given time constraints. The MLAB agent (o3-mini) proposed median aggregation of parameters, which slightly underperforms the baseline of mean aggregation. We hypothesize that the median, while robust to extreme outliers, typically exhibits higher statistical variability when merging multiple parameter sets, especially with fewer models. Machine Unlearning Challenge: The goal is to develop efficient algorithms that enable model to forget specific training data, such that the resulting model closely resembles one that was never trained on that data in the first place. The MLAB agent (claude-3-5-sonnet-v2) proposed Gradient Ascent Unlearning Algorithm, two-phase approach combining gradient ascent for forgetting and fine-tuning for retaining knowledge. Specifically, the algorithm first performs gradient ascent on the forget set to maximize loss (achieving unlearning) and then fine-tunes the model on the retain set to restore the desired knowledge. While this approach sounds promising in theory, it scored significantly lower than the baseline. We hypothesize that by separating the gradient ascent on the forget set and the fine-tuning on the retain set into two distinct phases, the model may not effectively balance these two conflicting objectives. In contrast, joint optimization approachwhere both objectives are optimized at each gradient updatemight better balance the processes of forgetting and retaining knowledge. 4.7. Cost-Effectiveness Analysis In Figure 7, we analyze the agents success rates in costcontrolled setting, motivated by recent work (Kapoor et al., 2024) emphasizing the importance of jointly optimizing both performance and cost in agent design.5 Llama 3.1 5We exclude the gemini-exp-1206 model from this figure because it was experimental and its pricing was unavailable at the time of writing. Figure 7. We perform cost-effectiveness analysis of various setups. On the x-axis, we plot API cost, where lower is better, and on the y-axis, we show reletive improvement to human (Section 3.3), where higher is better. Among the settings evaluated, Llama 3.1 405B with the MLAB scaffolding emerges as Pareto-optimal setting that balances cost and performance improvement. 405b Instruct6 offers the most favorable trade-off, achieving higher success rates than GPT-4o and Claude 3.5 Sonnet at significantly lower cost. Although incorporating an ideation phase before implementation improves overall performance compared to the implementation-only MLAB setting, it incurs additional costs due to the generation of research ideas. Nevertheless, we believe the performance gain will increasingly justify the added cost as base models continue to grow stronger, particularly for complex research problems where strategic high-level planning leads to substantial gains in final outcomes. 5. Conclusion MLRC-BENCH draws upon the rigor of conference competitions to provide scalable, objective, and realistic benchmark for evaluating LLM agents in scientific discovery tasks. By isolating the core workflow of method proposal and implementation, MLRC-BENCH highlights whether an agent can innovate, code, and refine solutions to meet objective performance criteria. Grounding its tasks in well-designed ML conference competitions ensures that each challenge addresses significant research topics, is carefully scoped with robust evaluation protocols, and includes code and data for objective, scalable, and reproducible assessment. Our benchmark results show that MLRC-BENCH presents significant challenge for state-of-the-art LLMs and agent scaffoldings. By featuring modular tasks, well-defined evaluation 6We estimate the API cost for Llama models based on Amazon Bedrock service pricing. 10 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? metrics, tamper-proof processes, and ongoing updates when new suitable competitions are available, MLRC-BENCH can evolve alongside the rapid pace of ML research and continuous support the pursuit of AI-assisted or automated scientific discovery."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is supported by LG AI Research."
        },
        {
            "title": "Impact Statement",
            "content": "The ability of AI agents to perform high-quality ML research could accelerate breakthroughs in critical domains such as healthcare, climate modeling, and AI safety. However, agents capable of autonomously generating novel methods at scale may also amplify risks if their outputs outpace human understanding or oversight. While current agents underperform human researchers, MLRC-BENCH highlights the need for ongoing monitoring of their capabilities to ensure alignment with ethical standards and societal goals. To mitigate risks, MLRC-BENCH emphasizes tamper-proof evaluation protocols and objective performance metrics, reducing reliance on subjective judgments that may overestimate innovation. By releasing this benchmark, we aim to enhance transparency and encourage the development of safer, more reliable AI research agents. We caution against deploying such systems without robust safeguards and urge the community to prioritize evaluations that balance innovation with accountability."
        },
        {
            "title": "References",
            "content": "Anthropic, A. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3:6, 2024. Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. Researchagent: Iterative research idea generation over scientific literature with large language models. CoRR, abs/2404.07738, 2024. doi: 10.48550/ARXIV. 2404.07738. URL https://doi.org/10.48550/ arXiv.2404.07738. Bhatt, K., Tarey, V., Patel, P., Mits, K. B., and Ujjain, D. Analysis of source lines of code (sloc) metric. International Journal of Emerging Technology and Advanced Engineering, 2(5):150154, 2012. Brown, B. C. A., Juravsky, J., Ehrlich, R. S., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. CoRR, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https://doi. org/10.48550/arXiv.2407.21787. Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=ghNRg2mEgN. Carrion-Ojeda, D., Chen, H., Baz, A. E., Escalera, S., Guan, C., Guyon, I., Ullah, I., Wang, X., and Zhu, W. Neurips22 cross-domain metadl competition: Design and baseline results. In Brazdil, P., van Rijn, J. N., Gouk, H., and Mohr, F. (eds.), ECML/PKDD Workshop on Meta-Knowledge Transfer, 23 September 2022, Grenoble, France, volume 191 of Proceedings of Machine Learning Research, pp. 2437. PMLR, 2022. URL https://proceedings.mlr.press/ v191/carrion-ojeda22a.html. Chan, J. S., Chowdhury, N., Jaffe, O., Aung, J., Sherburn, D., Mays, E., Starace, G., Liu, K., Maksin, L., Patwardhan, T., Weng, L., and Madry, A. Mle-bench: Evaluating machine learning agents on machine learning engineering. CoRR, abs/2410.07095, 2024. doi: 10.48550/ARXIV. 2410.07095. URL https://doi.org/10.48550/ arXiv.2410.07095. Chen, L., Davis, J. Q., Hanin, B., Bailis, P., Stoica, I., Zaharia, M., and Zou, J. Are more llm calls all you towards scaling laws of compound inference need? systems, 2024a. URL https://arxiv.org/abs/ 2403.02419. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. Chen, T. and Guestrin, C. Xgboost: scalable tree boosting system. In Krishnapuram, B., Shah, M., Smola, A. J., Aggarwal, C. C., Shen, D., and Rastogi, R. (eds.), Proceedings of the 22nd ACM SIGKDD International Conference MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 785794. ACM, 2016. doi: 10.1145/2939672.2939785. URL https: //doi.org/10.1145/2939672.2939785. Chen, Z., Chen, S., Ning, Y., Zhang, Q., Wang, B., Yu, B., Li, Y., Liao, Z., Wei, C., Lu, Z., Dey, V., Xue, M., Baker, F. N., Burns, B., Adu-Ampratwum, D., Huang, X., Ning, X., Gao, S., Su, Y., and Sun, H. Scienceagentbench: Toward rigorous assessment of language agents for datadriven scientific discovery. CoRR, abs/2410.05080, 2024b. doi: 10.48550/ARXIV.2410.05080. URL https: //doi.org/10.48550/arXiv.2410.05080. Du, J., Wang, Y., Zhao, W., Deng, Z., Liu, S., Lou, R., Zou, H. P., Venkit, P. N., Zhang, N., Srinath, M., Zhang, H., Gupta, V., Li, Y., Li, T., Wang, F., Liu, Q., Liu, T., Gao, P., Xia, C., Xing, C., Jiayang, C., Wang, Z., Su, Y., Shah, R. S., Guo, R., Gu, J., Li, H., Wei, K., Wang, Z., Cheng, L., Ranathunga, S., Fang, M., Fu, J., Liu, F., Huang, R., Blanco, E., Cao, Y., Zhang, R., Yu, P. S., and Yin, W. Llms assist NLP researchers: Critique paIn Al-Onaizan, Y., Bansal, M., per (meta-)reviewing. and Chen, Y. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 50815099. Association for Computational Linguistics, 2024. URL https://aclanthology.org/ 2024.emnlp-main.292. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https: //doi.org/10.48550/arXiv.2407.21783. Gao, S., Fang, A., Huang, Y., Giunchiglia, V., Noori, A., Schwarz, J. R., Ektefaie, Y., Kondic, J., and Zitnik, M. Empowering biomedical discovery with AI agents. CoRR, abs/2404.02831, 2024. doi: 10.48550/ARXIV. 2404.02831. URL https://doi.org/10.48550/ arXiv.2404.02831. Gruca, A., Serva, F., Lliso, L., Rıpodas, P., Calbet, X., Herruzo, P., Pihrt, J., Raevskyi, R., ˇSimanek, P., Choma, M., Li, Y., Dong, H., Belousov, Y., Polezhaev, S., Pulfer, B., Seo, M., Kim, D., Shin, S., Kim, E., Ahn, S., Choi, Y., Park, J., Son, M., Cho, S., Lee, I., Kim, C., Kim, T., Kang, S., Shin, H., Yoon, D., Eom, S., Shin, K., Yun, S.-Y., Le Saux, B., Kopp, M. K., Hochreiter, S., and Kreil, D. P. Weather4cast at neurips 2022: Superresolution rain movie prediction under spatio-temporal shifts. In Ciccone, M., Stolovitzky, G., and Albrecht, J. (eds.), Proceedings of the NeurIPS 2022 Competitions Track, volume 220 of Proceedings of Machine Learning Research, pp. 292313. PMLR, 28 Nov09 Dec 2022. URL https://proceedings.mlr.press/ v220/gruca23a.html. Heyward, J., Carreira, J., Damen, D., Zisserman, A., and Patraucean, V. Perception test 2024: Challenge summary and novel hour-long videoqa benchmark. CoRR, abs/2411.19941, 2024. doi: 10.48550/ARXIV. 2411.19941. URL https://doi.org/10.48550/ arXiv.2411.19941. Huang, Q., Vora, J., Liang, P., and Leskovec, J. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https: //openreview.net/forum?id=1Fs1LvjYQW. Huang, Y., Luo, J., Yu, Y., Zhang, Y., Lei, F., Wei, Y., He, S., Huang, L., Liu, X., Zhao, J., and Liu, K. Dacode: Agent data science code generation benchmark for large language models. In Al-Onaizan, Y., Bansal, M., and Chen, Y. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 1348713521. Association for Computational Linguistics, 2024b. URL https://aclanthology. org/2024.emnlp-main.748. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., Madry, A., Baker-Whitcomb, A., Beutel, A., Borzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A., Paino, A., Renzin, A., Passos, A. T., Kirillov, A., Christakis, A., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A., Crookes, A., Tootoonchian, A., Kumar, A., Vallone, A., Karpathy, A., Braunstein, A., 12 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Cann, A., Codispoti, A., Galu, A., Kondrich, A., Tulloch, A., Mishchenko, A., Baek, A., Jiang, A., Pelisse, A., Woodford, A., Gosalia, A., Dhar, A., Pantuliano, A., Nayak, A., Oliver, A., Zoph, B., Ghorbani, B., Leimberger, B., Rossen, B., Sokolowsky, B., Wang, B., Zweig, B., Hoover, B., Samic, B., McGrew, B., Spero, B., Giertler, B., Cheng, B., Lightcap, B., Walkin, B., Quinn, B., Guarraci, B., Hsu, B., Kellogg, B., Eastman, B., Lugaresi, C., Wainwright, C. L., Bassin, C., Hudson, C., Chu, C., Nelson, C., Li, C., Shern, C. J., Conger, C., Barette, C., Voss, C., Ding, C., Lu, C., Zhang, C., Beaumont, C., Hallacy, C., Koch, C., Gibson, C., Kim, C., Choi, C., McLeavey, C., Hesse, C., Fischer, C., Winter, C., Czarnecki, C., Jarvis, C., Wei, C., Koumouzelis, C., and Sherburn, D. Gpt-4o system card. CoRR, abs/2410.21276, 2024. doi: 10.48550/ARXIV.2410.21276. URL https: //doi.org/10.48550/arXiv.2410.21276. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., Iftimie, A., Karpenko, A., Passos, A. T., Neitz, A., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar, A., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A., Mishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph, B., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B., McGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B., McKinzie, B., Eastman, B., Lugaresi, C., Bassin, C., Hudson, C., Li, C. M., de Bourcy, C., Voss, C., Shen, C., Zhang, C., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C., Roberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D., Farhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica, D., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung, E., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F., Such, F. P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F., von Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G., Chabot, G., Zhao, G., Brockman, G., Leclerc, G., Salman, H., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren, H., Lightman, H., Chung, H. W., Kivlichan, I., OConnell, I., Osband, I., Gilaberte, I. C., and Akkaya, I. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. URL https: //doi.org/10.48550/arXiv.2412.16720. Jiang, Z., Schmidt, D., Srikanth, D., Xu, D., Kaplan, I., Jacenko, D., and Wu, Y. AIDE: ai-driven exploration in the space of code. CoRR, abs/2502.13138, 2025. doi: 10.48550/ARXIV.2502.13138. URL https://doi. org/10.48550/arXiv.2502.13138. Jin, W., Mao, H., Li, Z., Jiang, H., Luo, C., Wen, H., Han, H., Lu, H., Wang, Z., Li, R., Li, Z., Cheng, M. X., Goutam, R., Zhang, H., Subbian, K., Wang, S., Sun, Y., Tang, J., Yin, B., and Tang, X. Amazon-m2: multilingual multilocale shopping session dataset for recommendation and text generation. arXiv preprint arXiv:2307.09688, 2023. Jing, L., Huang, Z., Wang, X., Yao, W., Yu, W., Ma, K., Zhang, H., Du, X., and Yu, D. Dsbench: How far are data science agents to becoming data science experts? CoRR, abs/2409.07703, 2024. doi: 10.48550/ARXIV. 2409.07703. URL https://doi.org/10.48550/ arXiv.2409.07703. Kapoor, S., Stroebl, B., Siegel, Z. S., Nadgir, N., and Narayanan, A. AI agents that matter. CoRR, abs/2407.01502, 2024. doi: 10.48550/ARXIV.2407. URL https://doi.org/10.48550/ 01502. arXiv.2407.01502. Koh, J. Y., McAleer, S., Fried, D., and Salakhutdinov, R. Tree search for language model agents. CoRR, abs/2407.01476, 2024. doi: 10.48550/ARXIV. 2407.01476. URL https://doi.org/10.48550/ arXiv.2407.01476. Krizhevsky, A. et al. Learning multiple layers of features from tiny images. 2009. Li, L., Xu, W., Guo, J., Zhao, R., Li, X., Yuan, Y., Zhang, B., Jiang, Y., Xin, Y., Dang, R., Zhao, D., Rong, Y., Feng, T., and Bing, L. Chain of ideas: Revolutionizing research via novel idea development with LLM agents. CoRR, abs/2410.13185, 2024a. doi: 10.48550/ARXIV. 2410.13185. URL https://doi.org/10.48550/ arXiv.2410.13185. Li, R., Patel, T., Wang, Q., and Du, X. Mlr-copilot: Autonomous machine learning research based on large language models agents. CoRR, abs/2408.14033, 2024b. doi: 10.48550/ARXIV.2408.14033. URL https:// doi.org/10.48550/arXiv.2408.14033. Lou, R., Xu, H., Wang, S., Du, J., Kamoi, R., Lu, X., Xie, J., Sun, Y., Zhang, Y., Ahn, J. J., Fang, H., Zou, Z., Ma, W., Li, X., Zhang, K., Xia, C., Huang, L., and Yin, W. AAAR-1.0: assessing ais potential to assist research. CoRR, abs/2410.22394, 2024. doi: 10.48550/ARXIV. 2410.22394. URL https://doi.org/10.48550/ arXiv.2410.22394. Lu, C., Holt, S., Fanconi, C., Chan, A. J., Foerster, J. N., van der Schaar, M., and Lange, R. T. Discovering preference optimization algorithms with and for large language models. CoRR, abs/2406.08414, 2024a. doi: 10.48550/ARXIV.2406.08414. URL https://doi. org/10.48550/arXiv.2406.08414. Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The AI scientist: Towards fully automated open-ended scientific discovery. CoRR, abs/2408.06292, 2024b. doi: MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Tam, D., Li, M., Yadav, P., Gabrielsson, R. B., Zhu, J., Greenewald, K., Yurochkin, M., Bansal, M., Raffel, C., and Choshen, L. Llm merging: Building llms efficiently through merging. In NeurIPS 2024 Competition Track, 2024. Triantafillou, E., Kairouz, P., Pedregosa, F., Hayes, J., Kurmanji, M., Zhao, K., Dumoulin, V., Junior, J. C. S. J., Mitliagkas, I., Wan, J., Sun-Hosoya, L., Escalera, S., Dziugaite, G. K., Triantafillou, P., and Guyon, I. Are we making progress in unlearning? findings from the first neurips unlearning competition. CoRR, abs/2406.09073, 2024. doi: 10.48550/ARXIV.2406.09073. URL https: //doi.org/10.48550/arXiv.2406.09073. Wijk, H., Lin, T., Becker, J., Jawhar, S., Parikh, N., Broadley, T., Chan, L., Chen, M., Clymer, J., Dhyani, J., Ericheva, E., Garcia, K., Goodrich, B., Jurkovic, N., Kinniment, M., Lajko, A., Nix, S., Sato, L., Saunders, W., Taran, M., West, B., and Barnes, E. Re-bench: Evaluating frontier AI r&d capabilities of language model agents against human experts. CoRR, abs/2411.15114, 2024. doi: 10.48550/ARXIV.2411.15114. URL https:// doi.org/10.48550/arXiv.2411.15114. Xiang, Z., Zeng, Y., Kang, M., Xu, C., Zhang, J., Yuan, Z., Chen, Z., Xie, C., Jiang, F., Pan, M., et al. Clas 2024: The competition for llm and agent safety. In NeurIPS 2024 Competition Track, 2024. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and In The Eleventh Internaacting in language models. tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043, 2023. doi: 10.48550/ARXIV.2307.15043. URL https://doi. org/10.48550/arXiv.2307.15043. 10.48550/ARXIV.2408.06292. URL https://doi. org/10.48550/arXiv.2408.06292. Majumder, B. P., Surana, H., Agarwal, D., Mishra, B. D., Meena, A., Prakhar, A., Vora, T., Khot, T., Sabharwal, A., and Clark, P. Discoverybench: Towards data-driven discovery with large language models. CoRR, abs/2407.01725, 2024. doi: 10.48550/ARXIV. 2407.01725. URL https://doi.org/10.48550/ arXiv.2407.01725. Nathani, D., Madaan, L., Roberts, N., Bashlykov, N., Menon, A., Moens, V., Budhiraja, A., Magka, D., Vorotilov, V., Chaurasia, G., Hupkes, D., Cabral, R. S., Shavrina, T., Foerster, J. N., Bachrach, Y., Wang, W. Y., and Raileanu, R. Mlgym: new framework and benchmark for advancing AI research agents. CoRR, abs/2502.14499, 2025. doi: 10.48550/ARXIV. 2502.14499. URL https://doi.org/10.48550/ arXiv.2502.14499. Nguyen, V., Deeds-Rubin, S., Tan, T., and Boehm, B. sloc counting standard. In Cocomo ii forum, volume 2007, pp. 116. Citeseer, 2007. OpenAI. 2025. o3-mini-system-card/. Openai o3-mini January URL https://openai.com/index/ system card, Pichai, S. Introducing gemini 2.0: our new ai model for the agentic era, 2024. URL https://blog. google/technology/google-deepmind/ google-gemini-ai-update-december-2024/ #ceo-message. Accessed: 2025-01-29. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Navab, N., Hornegger, J., III, W. M. W., and Frangi, A. F. (eds.), Medical Image Computing and ComputerAssisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, pp. 234241. Springer, 2015. doi: 10.1007/978-3-319-24574-4 28. URL https:// doi.org/10.1007/978-3-319-24574-4_28. Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? large-scale human study with 100+ NLP researchers. CoRR, abs/2409.04109, 2024. doi: 10.48550/ARXIV.2409.04109. URL https:// doi.org/10.48550/arXiv.2409.04109. Spearman, C. The proof and measurement of association between two things. American Journal of Psychology, 15: 72101, 1904. 14 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? A. Detailed Description of Research Competitions A.1. LLM Merging (Tam et al., 2024) [Link] Develop novel and effective LLM merging method to improve performance on held out test set within the time constraints. ## Description Training high-performing large language models (LLMs) from scratch is notoriously expensive and difficult task, costing hundreds of millions of dollars in compute alone. These pretrained LLMs, however, can cheaply and easily be adapted to new tasks via fine-tuning, leading to proliferation of models that suit specific use cases. Recent work has shown that specialized fine-tuned models can be rapidly merged to combine capabilities and generalize to new skills. The competition will provide the participants with list of expert models that have already been trained on task-specific dataset. The goal of this competition is to re-use the provided models to create generalist model that can perform well on wide variety of skills like reasoning, coding, maths, chat, and tool use. Along with these expert models, we have set of hidden tasks that will be used to evaluate the submissions from participants. A.2. Backdoor Trigger Recovery (Xiang et al., 2024) [Link] **Backdoor Trigger Recovery for Code Generation Models** ## Description Participants in this competition are tasked with developing algorithms to recover backdoor triggers embedded within large language models (LLMs) used for code generation. Each provided backdoored LLM contains multiple (trigger, target) pairs, where triggers are universal prompt injections designed to induce the generation of malicious code specified by the targets. In the development phase, participants receive model finetuned with five known (trigger, target) pairs, while in the testing phase, the models include tens of secret (trigger, target) pairs related to various categories of harmful code generation. The objective is to predict the triggers corresponding to each provided target, adhering to maximum token constraint of 10 tokens per trigger. Submissions will be evaluated using two metrics: recall, which measures the similarity between predicted and ground truth triggers, and the Reverse-Engineering Attack Success Rate (REASR), which assesses the effectiveness of the recovered triggers in eliciting the malicious code. Participants are provided with starter dataset of 100 code generation queries and their correct outputs for method development and local evaluation, with additional data encouraged for enhancing method robustness. However, any attempts to access or guess the secret online evaluation dataset will be considered rule violation. A.3. Temporal Action Localisation (Heyward et al., 2024) [Link] # Second Perception Test Challenge (ECCV 2024 Workshop) Temporal Action Localisation Track ## Description The goal of this challenge is to develop methods that accurately **localize and classify actions** in untrimmed videos (up to 35 seconds long, 30 fps, max resolution 1080p) from predefined set of classes. ## Data - **Training Data: Multimodal List** - 1608 videos - Includes both **action** and **sound** annotations - Contains **video and audio features** - **Validation Set** - 401 videos, used to tune hyperparameters. - **Test Set** - Held-out set for final evaluation of your methods performance containing 5359 videos. 15 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? ## Output Format For each video in test (or val), your model should output **all action segments**, with: 1. **Start timestamp** 2. **End timestamp** 3. **Predicted action class label** 4. **Confidence score** ## Evaluation - The main metric is Mean Average Precision (mAP), computed over your detected segments and averaged across: - Different action classes - IoU thresholds from 0.1 to 0.5 in increments of 0.1 (i.e., [0.1, 0.2, 0.3, 0.4, 0.5]) - You have separate splits for train, val, and test: - Train on the training set. - Use the validation set to tune, select models, etc. - Evaluate final performance on the **test set**. A.4. Rainfall Prediction (Gruca et al., 2022) [Link] Super-Resolution Rain Movie Prediction under Temporal Shifts ## Description The aim of the Weather4cast competition is to predict quantitatively future high resolution rainfall events from lower resolution satellite radiances. Ground-radar reflectivity measurements are used to calculate pan-European composite rainfall rates by the Operational Program for Exchange of Weather Radar Information (OPERA) radar network. While these are more precise, accurate, and of higher resolution than satellite data, they are expensive to obtain and not available in many parts of the world. We thus want to learn how to predict this high value rain rates from radiation measured by geostationary satellites operated by the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT). Competition participants should predict the exact amount of rainfall for the next 8 hours in 32 time slots from an input sequence of 4 time slots of the preceeding hour. The input sequence consists of four 11-band spectral satellite images. These 11 channels show slightly noisy satellite radiances covering so-called visible (VIS), water vapor (WV), and infrared (IR) bands. Each satellite image covers 15 minute period and its pixels correspond to spatial area of about 12km 12km. The prediction output is sequence of 32 images representing rain rates from ground-radar reflectivities. Output images also have temporal resolution of 15 minutes but have higher spatial resolution, with each pixel corresponding to spatial area of about 2km 2km. So in addition to predicting the weather in the future, converting satellite inputs to ground-radar outputs, this adds super-resolution task due to the coarser spatial resolution of the satellite data. We provide training and validation data from one Eureopean region in 2019, and testing data from the same region in 2020, measuring transfer learning performance under temporal shift. The task is to predict exact amount of rain events 4 hours into the future from 1 hour sequence of satellite images. Rain rates computed from OPERA ground-radar reflectivities provide ground truth. A.5. Machine Unlearning (Triantafillou et al., 2024) [Link] # Machine Unlearning Challenge **One-sentence summary** Develop efficient algorithms for machine unlearning such that, after forgetting certain training data, the resulting model closely matches one that was never trained on that data in the first place. ## Description MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? We focus on **machine unlearning**, i.e., removing the influence of subset of the training data (the *forget set*) from trained model, so that the resulting model behaves similarly to one trained *without* that subset. This is especially relevant for privacy regulations (e.g., right to be forgotten), where individuals can request removal of their data from model. ### Goal Our goal is to compare the strengths and weaknesses of different unlearning methods under *shared* and *standardized* evaluation. Participants receive: 1. **pre-trained** model (trained on facial images, CASIA-SURF, to predict age group in test phase, CIFAR-10 in dev phase). 2. **forget set** (data samples to remove) and **retain set** (the rest of training data). 3. hidden **test set** for final scoring. **Output**: An unlearned model that should: - **Erase** the forget sets influence to match the behavior of retrained model that never saw those forget samples. - **Retain** good accuracy on the remaining data and on the test set. - **Finish** within provided compute/runtime constraints. ### Data & Evaluation - **Dataset**: CASIA-SURF, containing facial images labeled by age group (10 classes) in test phase, CIFAR-10 in dev phase. - **Pretrained model**: classifier trained for 30 epochs on the entire dataset. - **Forgetting**: Must remove any trace of the forget set. - **Utility**: Must stay accurate on the retain data and hidden test set. - **Metrics**: 1. **Forgetting quality** compares unlearned model θu to model retrained from scratch θr without the forget set. 2. **Utility** checks retain/test accuracy relative to θr. 3. **Efficiency** run under time constraints (< 8h on provided compute). The challenge uses an *online* evaluation on Kaggle. Each submitted unlearning method will be run multiple times against multiple original and retrained-from-scratch checkpoints, producing final score that balances forgetting quality and model utility. A.6. Next Product Recommendation (Jin et al., 2023) [Link] This task focuses on next product recommendation by predicting the most likely product customer will engage with based on session data and product attributes, using test data from English, German, and Japanese locales. ## Description For each session, the participant should predict 100 product IDs (ASINs) that are most likely to be engaged with. The product IDs should be stored in list and are listed in decreasing order of confidence, with the most confident prediction at index 0 and least confident prediction at index 99. Evaluation is performed using mean reciprocal rank where the rank in your list of the ground truth next item is being assessed. For each session, you will be provided with the locale of the user and list of products already viewed in that session. separate file has metadata about each product. A.7. Cross-Domain Meta Learning (Carrion-Ojeda et al., 2022) [Link] The competition focuses on cross-domain meta-learning for few-shot image classification, challenging participants to develop scalable and robust models that can quickly adapt to diverse tasks with varying numbers of classes (ways) and training examples per class (shots) across domains like healthcare, ecology, and manufacturing. ## Description Goal and Data This competition challenges participants to develop meta-learning models that adapt quickly to few-shot classification tasks across ten diverse domains (e.g., healthcare, ecology, manufacturing). Drawing on the newly expanded Meta Album 17 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Table 4. For each research competition and agent, we report the test-phase best percentage improvement in the performance metric over the baseline among 8 trails provided in the starter code. Additionally, we present the improvements achieved by the top human participants at the time of competition under the same setup. Best performing agent in each task is highlighted in bold. Agents can only achieve marginal performance gains compared to human experts, and in many cases, the agents solutions even degrade baseline performance. Agent temporal -action-loc llm -merging meta -learning product -rec rainfall -pred machine -unlearning backdoor -trigger MLAB (gemini-exp-1206) MLAB (llama3-1-405b-instruct) MLAB (o3-mini) MLAB (claude-3-5-sonnet-v2) MLAB (gpt-4o) Human Idea + MLAB (gpt-4o) CoI-Agent Idea (o1) + MLAB (gpt-4o) -1.3 1.5 0.9 2.2 0.9 1.5 1.0 Top Human in Competition 284. 3.4 -0.7 -0.7 3.4 1.4 -0.7 -0.7 68.2 -3.2 -14.9 -14.9 -14.9 -14.9 -14.9 -14.9 0.6 0.0 0.6 12.3 2.6 8.9 0.6 304.5 412. 91.4 66.7 53.3 31.0 100.8 26.1 83.6 212.0 3.5 3.8 2.2 -58.6 -11.1 4.2 7.3 61.9 80.4 71.7 38.8 247.9 64.5 54.5 24.9 621. Avg 25.0 18.3 11.5 31.9 20.6 11.4 14.5 280.7 meta-dataset (10 image datasets unified at 128128 resolution), the final evaluation tasks vary in ways (220 classes) and shots (120 training examples per class). By combining such heterogeneous tasks, the challenge highlights the importance of scalability, robustness to domain shifts, and flexible generalization in the any-way any-shot meta-learning setting. 5 datasets will be used for training and 5 will be used for testing. Participants develop MetaLearner whose meta fit function returns Learner whose fit function returns Predictor with predict function. Evaluation and Metric Submissions are evaluated with blind testing on ten representative datasets. Each task includes support set (training) and query set (testing), and the competitions primary metric is random-guess normalized balanced accuracy. First, balanced classification accuracy (bac) is computed by averaging per-class accuracies (i.e., macro-average recall). Then, to account for varying numbers of classes (ways), the bac is normalized by the expected performance of random guessing. This ensures fair comparison across tasks with different ways/shots configurations and highlights each models true ability to learn effectively from limited examples in multiple domains. B. Additional Results This section presents additional results that complement the findings reported in the main paper. Table 4 reports the absolute improvement over the baseline, supplementing the success rate results shown in Table 3 (Section 4.2). Figure 8 displays the correlation heatmap between objective and subjective metrics when LLM-as-aJudge is applied without code as input, complementing Figure 4 (Section 4.4). Figure 9 shows inference-time scaling results broken down by task, complementing the aggregate results in Figure 2 (Section 4.3). Figures 10 and 11 provide task-level analysis of the implementation process, extending the results in Figure 5 (Section 4.5). Figure 8. Correlation heatmap between objective and subjective metrics when LLM-as-a-Judge is done without code as input. The with code version is shown in Figure 4. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 9. For each task, we measure Pass@k as we scale the number of trials and ideas, running MLAB for eight trials per idea. Pass@k is the probability that at least one of trials converges to successful implementation, defined as the agent closes at least 5% of the gap between baseline and top human participant scores. 19 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 10. For each task, we track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within trial of LLM-based MLAB agent. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 11. (Contd) For each task, we track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within trial of LLM-based MLAB agent. 21 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? C. Prompts for LLM-as-a-Judge Prompt for LLM Explainer in Figure Analyze the following Python code with comments and generate high-level idea proposal summarizing: 1. The main goal or purpose of the method or algorithm implemented. 2. The general approach or methodology used to achieve the goal. 3. Any core assumptions or requirements underlying the implementation. Focus on providing conceptual overview rather than implementation details. Code: {code} Provide the summary as an idea proposal, avoiding references to the code itself. Focus on describing the approach and methodology as standalone concept. Prompt for LLM Judge in Figure 1 You are an AI assistant whose primary goal is to assess the quality and soundness of scientific methods across diverse dimensions, in order to aid researchers in refining their methods based on your evaluations and feedback, thereby enhancing the impact and reach of their work. You are going to evaluate scientific method for its {metric} in addressing research problem, focusing on how well it is described in clear, precise, and understandable manner that allows for replication and comprehension of the approach. As part of your evaluation, you can refer to the research problem, which will help in understanding the context of the proposed method for more comprehensive assessment. Research problem: {researchProblem} Now, proceed with your {metric} evaluation approach that should be systematic: - Start by thoroughly reading the proposed method and its rationale, keeping in mind the context provided by the research problem, and existing studies mentioned above. - Next, generate review and feedback that should be constructive, helpful, and concise, focusing on the {metric} of the method. - Finally, provide score on 5-point Likert scale, with 1 being the lowest, please ensuring discerning and critical evaluation to avoid tendency towards uniformly high ratings (4-5) unless fully justified: The criteria for {metric} evaluation: {criteria} am going to provide the proposed method with its code implementation, as follows: Proposed method: {Method} Code implementation: {code} After your evaluation of the above content, please respond **only** with valid JSON object in the following format: { Review: Your review here, Feedback: Your feedback here, Rating: Your rating here } 22 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 12. Distribution of environment and non-environment errors across different MLRC-Bench tasks. Each task is represented with two pie charts: one for errors related to the environment (e.g., submission issues, argument mismatches) and another for non-environment errors (e.g., runtime failures, memory issues) . 23 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? D. Agent Trace Analysis In this section, we analyze the agent traces for Gemini-exp-1206 across different tasks. We collect trajectories across 7 tasks with 8 runs for each task, resulting in total of 56 trajectories. D.1. Error Type Categorization We categorize Gemini-exp-1206s actions on MLRC-Bench tasks into two main types: Non-execute Steps: Steps where the action Execute Script was not invoked. Execute Steps: Steps where the action Execute Script was invoked. Non-execute errors are further classified into incorrect argument and incorrect submissions. Execute errors include key errors, value errors, type errors, assertion errors, runtime errors, attribute errors, out-of-memory errors, import errors, and syntax errors. We briefly explain the errors encountered: EnvError: Occurs when submissions do not match the leaderboard records, files are missing, or arguments are passed incorrectly. KeyError: Results from passing incorrect argument names or not registering methods. ValueError: Triggered by invalid parameters, such as an improper learning rate or an empty parameter list. TypeError: Occurs from unexpected keyword arguments. AssertionError: Occurs when conditions such as shape compatibility or divisibility are not met. RuntimeError: Typically related to tensor shape issues. AttributeError: Happens when required attribute is missing. OutofmemoryError: Indicates CUDA out-of-memory condition. ImportError: Occurs when module cannot be imported. SyntaxError: Triggered by syntax issues, such as missing comma. Figure 12 shows that Gemini-exp-1206 successfully completes considerable number of steps without errors, yet its performance varies noticeably across tasks. In particular, while Meta Learning displays relatively few issues, Rainfall Prediction exhibits higher frequency of hallucination based errors such as incorrect argument handling, non-existent file references, and invalid parameter choices. This discrepancy indicates that certain tasks present greater challenges for the model, likely due to more complex or less familiar contexts. Within the Execute Steps, the most frequent error types are import, value, and type errors, reflecting tendency to reference nonexistent modules, pass invalid parameters, or supply arguments of the wrong data type. On the Non-execute Steps side, incorrect arguments remain recurring challenge, showing another case where the agent seems to be hallucinating the argument names. Taken together, these findings highlight the generally robust completion of tasks, but also highlight the need to refine the agents internal checks to reduce parameter mismatches and submission errors. Strengthening agent self-verification strategies could help mitigate hallucinations and further align its outputs with the intended specifications of each task. 24 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 13. Error response distribution across tasks. For each task, errors are classified as Fixed (fully resolved), Attempted (partially addressed), or Didnt attempt (unresolved). These labels were assigned by GPT-4o-mini after evaluating each error along with all its subsequent steps (action, reflection, thought, and observation). D.2. Error Response Distribution Figure 13 presents an overview of how errors are handled across the seven tasks, highlighting the proportion of errors that were fully resolved (Fixed), partially addressed (Attempted), or left unaddressed (Did not attempt). These groupings were derived by passing each error along with all its next steps containing its action, reflection, thought and observation to GPT-4o-mini, and the error was then labeled based on whether it was successfully resolved, partially addressed, or not addressed at all. From Figure 13, we observe notable variations in error-handling effectiveness across tasks. Specifically, LLM Merging demonstrates the highest proportion of fully resolved (Fixed) errors, indicating more effective resolution strategies, whereas Rainfall Prediction, Backdoor Trigger Recovery, Machine Unlearning, and Perception Temporal Action Localization predominantly exhibit errors that are only partially addressed (Attempted). Meanwhile, Meta Learning has the largest share of errors categorized as Did not attempt. These distinctions highlight task-specific differences in error management We also observe consistently high percentage of errors categorized as Attempted across nearly all tasks, indicating that the agent often struggles to fully resolve errors. This broadly suggests challenges in the agents comprehension or planning capabilities when addressing complex errors, potentially pointing to difficulties in fully interpreting the underlying problem or effectively formulating corrective actions. Additionally, the notable variability in fully resolved (Fixed) and unaddressed (Did not attempt) errors across tasks implies that certain tasks inherently pose greater cognitive complexity or ambiguity, further exacerbating the agents difficulty in error resolution. The prompts used for this annotation are shown in Appendix G. D.3. Error Solve Rate To further expand on this analysis of errors, we also show which error types are more effectively resolved and highlight their associated complexity during task execution. Errors which were categorised as Fixed are treated as solved while errors belonging to the other two categories are treated as unsolved. Using the prompt in Appendix G, we also had GPT-4o-mini return the step at which the error was fixed for those that were categorised as fixed. Figure 14 provides insights into the solve rates across different error types, revealing variability in the agents efficiency in resolving specific errors. Among the error types, Out of Memory Error achieved the highest solve rate, suggesting that these errors are relatively straightforward for the agent to diagnose and address. In contrast, Syntax Errors and Environment Errors demonstrated lower resolution rates, while Value Error, Runtime Error, and Assertion Error were never fixed, highlighting their inherent complexity or ambiguity. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 14. Solve rate and average steps taken for resolving various error types. The top chart shows the proportion of errors successfully resolved (Solve Rate), annotated with the total number of instances per error type. The bottom chart illustrates the average number of steps required to achieve resolution, only errors which were fixed were used to calculate average steps. Additionally, the average number of steps taken to resolve errors further underscores these differences. Notably, Out of Memory Errors required the highest average number of steps, indicating that, although these errors are ultimately resolved at high rate, their resolution involves complex, multi-step process. Conversely, when Syntax Errors and Environment Errors are fixed, they tend to be resolved more quickly, suggesting that these issues, while more challenging to fix overall, can be diagnosed and corrected with fewer steps when addressed successfully. D.4. Per-Step Action Distribution Figure 15 presents how frequently each action (List Files, Understand File, Edit Script (AI), Execute Script, Copy File, Undo Edit Script, Inspect Script Lines, and Final Answer) is used over the maximum allowed 50 steps. This breakdown helps us observe when the agent transitions from environment exploration to iterative code refinement and debugging. In particular, Rainfall Prediction was not used for this analysis, as it was run for 100 steps. Early steps are dominated by environment-inspection actions, particularly List Files and Understand File, which give the agent context about available files and their contents. As the trajectory progresses, the agent increasingly relies on Edit Script (AI) and Execute Script for iterative code modifications and testing, while Inspect Script Lines helps to target debugging. Undo Edit Script is used far less frequently, suggesting that the agent rarely reverts to previous state. This pattern highlights 26 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 15. Distribution of Actions Taken Across Steps. This visualization depicts how frequently different types of actions were taken at each step by the agent. an iterative development approach, but also indicates that the agent may underutilize rollback strategies when encountering errors. Although Final Answer typically signals the end, some runs exhibit early submission, indicating missed opportunities for further refinements. D.5. Per-Step Stage Distribution In this section, we analyze the per-step stage distribution, categorizing the steps into seven stages based on GPT4o annotations: Understanding & Exploration, Baseline Assessment, Problem Analysis & Idea Generation, Implementation, Debugging & Error Handling, Experimental Refinement, and Final Evaluation & Submission. Each step in the agents trajectorycomprising its Reflection, Thought, Action Input, and Actionwas labeled by GPT-4o, which matched the step content to the most relevant stage criteria. Figure 6 visualizes the distribution of these seven stages over the course of the maximum allowed 50 steps. In particular, Rainfall Prediction was not used for this analysis, as it was ran for 100 steps. The early steps are predominantly labeled Understanding & Exploration, reflecting the initial focus of the agent on examining files, reviewing the environment, and clarifying task requirements. smaller portion of these early steps is allocated to Baseline Assessment, where the agent measures the performance of the unmodified solution to establish reference point. As the agent progresses, the distribution shifts noticeably toward Implementation, reflecting transition from initial passive information gathering to active code modifications. Notably, the agent dedicates very few steps to Problem Analysis & Idea Generation, suggesting rapid move from conceptual planning to execution. This change is often accompanied by surge in Debugging & Error Handling steps, as newly introduced modifications lead to runtime or logical errors that must be diagnosed and fixed. The close interplay between Implementation and Debugging & Error Handling underscores the iterative nature of the agents development process. Interestingly, it should be noted that the agent continues to spend substantial number of steps in the Understanding & Exploration stage. This ongoing emphasis highlights the inherent complexity and cognitive demands of repository-level tasks, which often require extensive file navigation and conceptual understanding. Toward the latter steps, subset of runs proceeds to Experimental Refinement, engaging in repeated re-runs, parameter tuning, and exploring alternative strategies to optimize performance. However, in many cases, the agent transitions relatively quickly to Final Evaluation & Submission. This early move towards final submission implies potential underuse of iterative enhancement cycles, indicating an area for improvement in the agents approach. The prompts used for this annotation are shown in Appendix F. MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Figure 16. Combined stage timelines across multiple tasks. Each timeline represents an individual run, with block widths proportional to the time spent in each stage. The total duration of each run is shown on the right. 28 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Table 5. Highest Capability Levels Across Experimental Runs for Evaluated Agents. This table reports the highest capability level, as defined by L1L8 metric, achieved by each agent over eight runs across seven distinct tasks. Each run is assigned numeric score corresponding to its level (e.g., L6 = 6, L5 = 5, and so on). Agent temporal -action-loc llm -merging product -rec rainfall -pred meta -learning machine -unlearning backdoor -trigger MLAB (gemini-exp-1206) MLAB (llama3-1-405b-instruct) MLAB (o3-mini) MLAB (claude-3-5-sonnet-v2) MLAB (gpt-4o) Human Idea + MLAB (gpt-4o) CoI-Agent (o1) Idea + MLAB (gpt-4o) 4 5 5 5 5 5 5 5 3 3 5 5 3 3 5 5 5 5 5 5 5 6 6 6 6 6 6 4 3 3 3 3 3 3 6 6 5 4 4 6 6 6 6 6 6 6 6 5 D.6. Stage Timelines Using the stage annotation from the previous section, we now extend our analysis by visualizing stage timelines for each task and run. Figure 16 depict the duration the model spends in each stage, ranging from Understanding & Exploration to Final Evaluation & Submission, with block widths proportional to the time allocated. The overall run durations are also displayed, providing context for the stage-wise time distribution. Notably, Rainfall Prediction was not used for this analysis, as it was ran for 10 hours. D.7. Capability Level We categorize each experimental run into one of eight capability levels (L1 to L8) based on its performance relative to both baseline and the top human solution. Definitions of each level are described below: L1: No Valid Output. The agent fails to generate any valid evaluation outputs on either the development or test set, indicating complete inability to produce usable predictions. L2: Test Submission Failure. The agent processes the development set but fails to produce valid submission on the test set, meaning that while some processing occurs, the pipeline does not yield final result. L3: Unimproved but Valid. The agent produces valid predictions for both the development and test sets yet remains below the baseline performance throughout the run. L4: Overfitting. The agent outperforms the baseline on the development set but falls short on the test set, suggesting that the model may have overfitted to the development data. L5: Baseline-Comparable. The agents test performance surpasses the baseline but remains under 5% of the margin by which the top human solution exceeds the baseline. In this range, performance is very close to the baseline level. L6: Notable Gains. The agents test performance exceeds the baseline by an improvement margin between 5% and 50% of the gap between the baseline and the top human solution. In practical terms, this level is our success scenario because it indicates the agent has closed meaningful portion of the gap above the baseline. L7: Near Human-Level. The agent captures between 50% and 100% of the improvement margin from the baseline to the top human solution, demonstrating that the performance is approaching that of the best human score. L8: Superhuman. The agent exceeds top human performance not only by delivering superior quantitative results, but also by demonstrating the creative ability to generate novel ideas and implement them effectively. This level signifies that the agent can innovate beyond established human benchmarks. This metric places an agents performance on tiered scale, relative to both the baseline and the top human solution, ensuring that any level of improvement (or lack thereof) is still meaningfully captured, even when the agent falls short of surpassing the baseline. Table 5 shows that rainfall-pred and backdoor-trigger are relatively easier tasks in our benchmark as the agent can achieve meaningful improvement over the baseline (L6), though still far behind the human. The other tasks appear to be very difficult for all agents, as they cannot achieve capability levels greater than L5. 29 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? E. Agent Code Samples Here we show the two examples of solutions generated by LLM agents mentioned in Section 4.6. Please see all the agent code in our benchmark codebase7. 1 # High-Level Overview: 2 # Purpose: This script merges multiple HuggingFace model checkpoints into single base model by computing the median of each corresponding parameter across different checkpoints. 1. Load multiple HuggingFace model checkpoints along with their configurations. 2. For every parameter in the models, stack the corresponding tensors along new dimension and compute the median value. 7 # 8 # 9 # The median is computed in float precision and then cast back to the original data type. 3. Load the base model and its tokenizer. 4. Update the base models parameters with the aggregated median values and set the model to evaluation mode. 3 # 4 # Methodology: 5 # 6 # 10 # 11 # Key Steps: 12 # 13 # - Load checkpoints and configurations. - Iterate over each parameter, compute the median across checkpoints with careful type conversion. - Load the base model and tokenizer. - Update the model state and prepare it for inference. 14 # 15 # 16 17 import torch 18 from methods.BaseMethod import BaseMethod 19 from peft import get_peft_model, set_peft_model_state_dict 20 21 class MedianMethod(BaseMethod): 22 \"\"\" MedianMethod performs the merging of multiple checkpoint models by computing the median of each parameter. This class extends the BaseMethod to load HuggingFace checkpoints, compute robust median-aggregated state, and update the base model accordingly. \"\"\" def __init__(self, name): \"\"\" Initialize the MedianMethod instance. Parameters: name (str): The identifier for this method instance. Returns: None \"\"\" # Call the parent BaseMethods initialization method. super().__init__(name) def run(self): \"\"\" Execute the merging pipeline and load the updated base model. Detailed Steps: 1. Load HuggingFace model checkpoints and configurations. - Uses helper function to populate self.loaded_models with state dictionaries from different checkpoints. 2. Merge the checkpoints by iterating over each parameter key: - For each parameter, retrieve the corresponding tensor from every loaded 7https://github.com/yunx-z/MLRC-Bench 30 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 76 77 79 80 81 82 83 85 86 87 88 89 91 92 93 94 95 97 98 99 100 101 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? model. - Detach the tensor from the computation graph and move it to CPU. - Stack these tensors along new dimension (dim=0) to form single tensor. - Convert the stacked tensor to float for precise median computation, compute the median along the new dimension, and cast the result back to the original data type. 3. Load the base models architecture and its tokenizer via helper functions. 4. Update the base models parameters with the merged state dictionary and set it to evaluation mode. Returns: torch.nn.Module: The updated base model, now containing the median-aggregated parameters. \"\"\" # Step 1: Load HuggingFace model checkpoints and configurations. # This helper function populates self.loaded_models with state dictionaries from different checkpoints. super()._load_huggingface_models_and_configs() # Step 2: Merge checkpoints by computing the median of each parameter across all loaded models. # Retrieve all model state dictionaries as list. all_models = list(self.loaded_models.values()) # Assume all models share the same architecture; extract parameter names from the first model. all_parameter_names = all_models[0].keys() # Iterate over each parameter name. for parameter_name in all_parameter_names: # Retrieve the parameter tensor for the current parameter from each model, # detaching from its computation graph and moving it to CPU to ensure consistency in merging. param_list = [model[parameter_name].detach().cpu() for model in all_models] # Stack the tensors along new dimension (dim=0) to create single tensor. stacked_params = torch.stack(param_list, dim=0) # Compute the median across the new dimension. # The tensor is first cast to float for precision during the median computation, # then the median result is cast back to the original data type. median_value = torch.median(stacked_params.float(), dim=0)[0].to(stacked_params.dtype) # Save the computed median tensor in the merged_model dictionary for later use. self.merged_model[parameter_name] = median_value # Step 3: Load the base models architecture and its tokenizer. # These helper functions initialize the base model and configure its tokenizer. self._load_base_model() self._load_tokenizer() # Step 4: Update the base model with the merged parameters. # Load the merged state dictionary into the base model. self.base_model.load_state_dict(self.merged_model) # Set the base model to evaluation mode to disable training-specific layers like dropout. self.base_model.eval() # Return the updated base model ready for inference. 31 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? 103 return self.base_model Listing 1. Median Merging Solution by MLAB (o3-mini) for the LLM Merging Challenge 1 \"\"\" 2 Gradient Ascent Unlearning Algorithm 3 ----------------------------------- 4 Purpose: Selectively unlearn specific training samples while retaining knowledge of others 5 Methodology: Two-phase approach combining gradient ascent and fine-tuning 6 Key Steps: 7 1. Phase 1: Gradient ascent on forget set to maximize loss (unlearning) 8 2. Phase 2: Fine-tuning on retain set to restore desired knowledge 9 \"\"\" 10 11 from copy import deepcopy 12 import torch 13 from torch import nn, optim 14 from methods.BaseMethod import BaseMethod 15 16 DEVICE = cuda if torch.cuda.is_available() else cpu 17 18 class GradientAscentUnlearning(BaseMethod): 19 def __init__(self, name): 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 \"\"\"Initialize the unlearning method Args: name: Name identifier for the method \"\"\" super().__init__(name) def get_name(self): \"\"\"Return the name of the unlearning method Returns: String identifier for the method \"\"\" return \"gradient_ascent_unlearning\" def run(self, net, retain_loader, forget_loader, val_loader): \"\"\"Implement two-phase unlearning using gradient ascent and fine-tuning Args: net: The model to be unlearned retain_loader: DataLoader for retained training data forget_loader: DataLoader for data to be forgotten val_loader: DataLoader for validation data Returns: The unlearned model \"\"\" criterion = nn.CrossEntropyLoss() # Phase 1: Gradient Ascent on forget set optimizer_forget = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4) for epoch in range(2): # 2 epochs for forgetting net.train() for batch_idx, sample in enumerate(forget_loader): # Handle different data formats (dict vs tuple) if isinstance(sample, dict): inputs = sample[\"image\"] targets = sample[\"age_group\"] else: 32 61 63 64 65 66 67 69 70 71 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? inputs, targets = sample inputs, targets = inputs.to(DEVICE), targets.to(DEVICE) optimizer_forget.zero_grad() outputs = net(inputs) loss = criterion(outputs, targets) # Multiply gradients by -1 for gradient ascent loss.backward() for param in net.parameters(): param.grad = -param.grad optimizer_forget.step() # Phase 2: Fine-tune on retain set optimizer_retain = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_retain, T_max=1) # 5 epochs for fine-tuning for epoch in range(5): net.train() for batch_idx, sample in enumerate(retain_loader): # Handle different data formats (dict vs tuple) if isinstance(sample, dict): inputs = sample[\"image\"] targets = sample[\"age_group\"] else: inputs, targets = sample inputs, targets = inputs.to(DEVICE), targets.to(DEVICE) optimizer_retain.zero_grad() outputs = net(inputs) loss = criterion(outputs, targets) loss.backward() optimizer_retain.step() scheduler.step() net.eval() return net Listing 2. Gradient Ascent Unlearning Solution by MLAB (claude-3-5-sonnet-v2) for the Machine Unlearning Challenge 33 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? F. Prompt for Stage Annotation"
        },
        {
            "title": "Prompt for LLM Stage Annotator",
            "content": "You are researcher. You are given the following trace of an AI agent working on ML research challenges: {output json str} Your task is to analyze every step in the trace and assign stage to each step. Use the following 7 stages. For each stage, use the reasoning guidelines provided to decide if step belongs to that stage. 1. Understanding & Exploration: - Description: Investigate the problem statement, explore the codebase, review data files, and understand evaluation metrics. This stage is about gathering context and building solid grasp of the task and environment. - Reasoning Guideline: Assign step to this stage if it focuses on examining available resources, reading documentation or files, exploring the code structure, or otherwise building an initial understanding of the project. 2. Baseline Assessment: - Description: Evaluate the unmodified baseline solutions performance to collect performance metrics and establish reference benchmark. - Reasoning Guideline: Assign step to this stage if it focuses on measuring the performance of the original, unaltered solution, collecting data for baseline comparison, and ensuring the initial performance level is documented. Do not assign step to this stage if it executes the solution after changes have been made. 3. Problem Analysis & Idea Generation: - Description: Analyze the baseline results to identify shortcomings and brainstorm potential improvements or alternative strategies. - Reasoning Guideline: Assign step to this stage if it is centered on evaluating baseline outcomes, identifying issues, or generating ideas and strategies for potential improvements. 4. Implementation: - Description: Develop and integrate the proposed modifications into the codebase by editing, extending, or refactoring the existing solution. - Reasoning Guideline: Assign step to this stage if it involves writing new code, modifying existing code, or integrating changes aimed at improving the solution. 5. Debugging & Error Handling: - Description: Identify, isolate, and fix any errors or unexpected behaviors introduced during implementation to ensure the solution runs reliably. - Reasoning Guideline: Assign step to this stage if it is focused on diagnosing problems, investigating error messages, or making corrections to ensure proper functionality. 6. Experimental Refinement: - Description: Re-run experiments on an already implemented solution and iteratively test various configurations, tune parameters, and compare alternative approaches to upgrade performance. - Reasoning Guideline: Assign step to this stage if it involves re-executing or adjusting an implemented solution, making upgrades and modifications to improve performance after the initial implementation has been established. 7. Final Evaluation & Submission: - Description: Conduct comprehensive evaluation of the refined solution against benchmarks and prepare the solution for final submission. - Reasoning Guideline: Assign step to this stage if it involves performing final, thorough evaluation of the solutions performance, verifying that all improvements meet the required criteria, and preparing for submission. Your response must be JSON object where the keys are the step numbers (as strings) and the values are the corresponding stage numbers (from 1 to 7) that best describe the agents activity at that step. IMPORTANT: When assigning stage, review the steps before and after each step to understand the broader context. IMPORTANT: The original {original step count} keys, numbered from 1 to {original step count}. trace has {original step count} steps. Your response MUST contain exactly 34 MLRC-BENCH: Can Language Agents Solve Machine Learning Research Challenges? Example output format: { \"1\": 1, \"2\": 1, \"3\": 4, \"4\": 6, \"5\": 7, \"6\": 7, ... } G. Prompt for Error Response Annotation"
        },
        {
            "title": "Prompt for LLM Error Response Annotator",
            "content": "Below is detailed chain-of-thought from an agent after encountering an error message: {error step} Based on the provided debugging steps, classify the agent response regarding the error as follows: 1 -> Fixed the error: The agent identified the issue and implemented solution that resolved the error. 2 -> Tried to fix the error but didnt: The agent attempted to address the error but the fix was not successful. 3 -> Didnt even try to fix the error and just went off doing something else: The agent did not directly attempt to resolve the error but instead focused on other tasks unrelated to fixing it. If the error was fixed (status -> 1), also identify which step number was the error fixed at. Return JSON with two fields: - Status: The number (1, 2, or 3) corresponding to the classification - FixedAtStep: The step number where the error was fixed (only if Status is 1, otherwise null)"
        }
    ],
    "affiliations": [
        "LG AI Research",
        "University of Illinois, Chicago",
        "University of Michigan, Ann Arbor"
    ]
}