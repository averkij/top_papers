{
    "paper_title": "Could Thinking Multilingually Empower LLM Reasoning?",
    "authors": [
        "Changjiang Gao",
        "Xu Huang",
        "Wenhao Zhu",
        "Shujian Huang",
        "Lei Li",
        "Fei Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous work indicates that large language models exhibit a significant \"English bias\", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs."
        },
        {
            "title": "Start",
            "content": "Could Thinking Multilingually Empower LLM Reasoning? Changjiang Gao1, Xu Huang1, Wenhao Zhu1, Shujian Huang1*, Lei Li3, Fei Yuan2* 1National Key Laboratory for Novel Software Technology, Nanjing University 2Shanghai Artificial Intelligence Laboratory, 3Carnegie Mellon University {gaocj,xuhuang,zhuwh}@smail.nju.edu.cn, huangsj@nju.edu.cn leili@cs.cmu.edu, yuanfei@pjlab.org.cn 5 2 0 2 6 1 ] . [ 1 3 3 8 1 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Previous work indicates that large language models exhibit significant English bias, i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs 1."
        },
        {
            "title": "Introduction",
            "content": "Leading Large Language Models (LLMs; OpenAI, 2023; Gemini Team, 2024; DeepSeek-AI Team, 2025; Wei et al., 2022; Yao et al., 2023; Li et al., 2025) are built on robust multilingual foundation, developed through extensive exposure to diverse multilingual data (OpenAI, 2023; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a,b) and effective vocabulary sharing across various languages (Yuan et al., 2024). However, due to the dominance of English in the training resources, the models exhibit notable bias toward English; specifically, these models tend to achieve higher performance when tasks are presented in English (Huang et al., 2022; Fu et al., 2022), especially in reasoning-demanding tasks that are of *Corresponding authors. 1Our is CONE-MT/multilingual_reasoning available code at https://github.com/ Figure 1: English is not always better than other languages. Evaluation results on the human-translated GPQA (Rein et al., 2023) and MGSM (Shi et al., 2023) datasets (obtained from Huang et al. (2025)). The red cells indicate greater-thanEnglish scores. great research interest (Shi et al., 2023; She et al., 2024; Huang et al., 2024; Etxaniz et al., 2024). As result, multiple previous studies (Lai and Nissim, 2024; Zhu et al., 2024; She et al., 2024) aimed to improve non-English reasoning performance by aligning to English behaviors. However, as illustrated in Figure 1, recent LLMs sometimes show stronger reasoning ability in nonEnglish (not limited to high-resource ones) than in English (Huang et al., 2025), underlining the potential of thinking beyond English. Inspired by this, we want to quantify the potential of multilingual thinking, compare it to English thinking, and explore the factors affecting its potential. So, how can we quantify the potential gain from multilingual thinking? In this paper, we aggregate model responses to translated parallel inputs on two typical reasoning-specific tasks, namely GPQA (Rein et al., 2023) and MGSM (Shi et al., 2023), and evaluate the performance upperbound of multilingual reasoning, calculated by Acc@k (existence probability of at least one correct out of answers) on LLaMA3.1-70B, Qwen2.5-72B and R1-distill-LLaMA3.1-70B. The results show multilingual thinking can ideally boost GPQA accuracy from 45 to 90, and MGSM from 90 to 100."
        },
        {
            "title": "Is the gain genuinely a result of multilingual",
            "content": "1 thinking, rather than from variations in input or randomness in decoding? Compared to two English baselines: multiple random runs and paraphrased inputs, multilingual thinking starts to show advantages when using only 4 candidates, and the advantages continue to increase as more candidates are included. When using 17 languages (the total number of languages in Huang et al.), the gain from multilingual thinking surpasses the baselines by nearly 10 Acc@k points. What factors affects the multilingual thinking potential? We conduct experiments regarding language selection, multilingual text quality and answer selection. Previous experiments have shown that reasoning tasks can yield significant gains by aggregating results from 3-4 languages. In our comprehensive tests, we randomly selected results from 4 out of 17 languages, and interestingly, the average performance was similar to that of the optimal combinations, demonstrating robustness in language selection. Furthermore, switching from machine translation to high-quality human translations does not lead to any substantial change in performance. However, when we shift Acc@k to majority voting, the advantages of multilingual thinking diminish. Therefore, for multilingual thinking, answer selection is crucial to realize its benefits. In addition to majority voting, other common answer selection strategies, prompt-based selection and LLM-as-a-judge selection (Zheng et al., 2023) still fail to elicit the potential of multilingual reasoning. Prompt-based selection tries to inject language-related guidance into instruction, such as predefined allowable languages or requirements of question translation, and LLM-as-a-judge attempts to generate multilingual response independently, but the final answer requires additional judgment from the LLM. Unfortunately, experimental results reveal that performance gains occur inconsistently across different settings, suggesting that stable selection method for leveraging multilingualism for enhanced reasoning remains elusive. Finally, we analyze the possible reasons and challenges associated with multilingual thinking. Our experimental results suggest that questions of varying difficulty have different language requirements, and some languages can compensate for errors made in others, performing correctly even when other languages do not. Meanwhile, the challenges of different answer selection methods are different. Majority voting performance does not necessarily improve with the number of languages used and it depends on the optimal combination of languages, which contrast with the trend observed in Acc@k. Additionally, prompt-based methods and LLM-as-a-judge selection can introduce language bias. The main contribution can be summarized as: We comprehensively analyze how multilingualism can enhance reasoning performance, laying the groundwork for understanding its huge potential. We evaluate common answer selection methods and find it is challenge to tap into the advantage of multilingualism, highlighting the difficulties. We analyze the potential reasons for multilingual thinking gain, point out the limitations of existing methods, and share interesting findings for future research."
        },
        {
            "title": "2 Related Work",
            "content": "Enhancing LLMs Reasoning Performance Enhancing reasoning capabilities has emerged as central challenge in LLM research. Prior work has approached this challenge from both training and inference perspectives. The training perspective includes pretraining and post-training. For pretraining, Taylor et al. (2022) used specially designed training processes to optimize the models STEM-related performance, and Aryabumi et al. (2024) explored the relationship between code pretraining and reasoning capabilities. Meanwhile, post-training approaches have also shown promising results in enhancing reasoning performance. Hao et al. (2023) proposed using Markov Decision Processes to model reasoning steps and using Monte-Carlo Tree Search to optimize the output; DeepSeek-AI Team (2025) proposed novel training pipeline to elicit spontaneous long-chain reasoning in LLMs; And Muennighoff et al. (2025) adopted small-scale instruction-tuning to boost the reasoning performance on competition math problems. The inference perspective includes various prompting and sampling techniques. In terms of prompting, chain-of-thought (CoT) (Wei et al., 2022) and its variants (Yao et al., 2023; Besta et al., 2024) let models to break down complex problems into intermediate steps and achieve higher reasoning accuracy. Based on them, specialized CoT structures have been proposed for different tasks, including planning-oriented (Yasunaga et al., 2023; Jiang et al., 2024; Wang et al., 2023b,c) and 2 Figure 2: An introduction to input samples across various comparison methods, including Multilingual, Repeat, Paraphrase, Repeat-Mix, and Paraphrase-Mix. symbolization-oriented (Gaur and Saunshi, 2023; Wang et al., 2023a; Chen et al., 2023; Li et al., 2023a; Xu et al., 2024; Imani et al., 2023). In terms of sampling, the self-consistency methods Wang et al. (2022, 2024); Li et al. (2023b) have been widely acknowledged. Our work pushes these studies further by focusing on the impact of multilingualism on LLMs reasoning behavior. Multilingual Reasoning of LLMs Multilingual capability is crucial in LLM development. Earlier LLMs exhibited unbalanced performance across languages, with non-English CoT reasoning typically underperforming English CoT (Shi et al., 2023; Qin et al., 2023; She et al., 2024; Huang et al., 2024; Etxaniz et al., 2024). To mitigate this gap, previous studies have mainly proposed multilingual CoT (Lai and Nissim, 2024; Chai et al., 2024; Huang et al., 2023), finetuning with translation data (Chen et al., 2024; Zhu et al., 2024), and multilingual preference training (She et al., 2024; Yang et al., 2024). However, recent advances in pretrained language models have significantly transformed this landscape. Notably, Huang et al. (2025) demonstrated that state-of-the-art LLMs such as Qwen (Qwen Team, 2025) and LLaMA (Grattafiori et al., 2024) achieve superior reasoning accuracy with non-English CoT compared to their English counterparts. In this paper, we systematically investigate this phenomenon and explore how to leverage multilingual reasoning to probe LLMs performance ceiling."
        },
        {
            "title": "3.1 Study Setup",
            "content": "To examine multilingual reasoning benefits, we analyze LLM responses to questions translated into multiple languages, and ablate the gains of increasing multilingualism versus increasing sampled response numbers (Figure 2). Specifically, we compare following approaches to transform question and collect LLM responses: Multilingual: The English samples are translated into various languages and then all fed into the model using fixed random seed. Repeat: The English samples are repeatedly fed into the model with different random seeds. Paraphrase: The English samples are paraphrased by LLM and then all fed into the model using fixed random seed. Repeat-Mix: We combine Repeat and Multilingual samples in 50/50 proportion. 2 out of 4 random seeds are used by Repeat and 2 out of 17 languages are used by Multilingual. Paraphrase-Mix: We mixed Paraphrase and Multilingual samples also in 50/50 proportion. Models We use Qwen2.5-72B (Qwen Team, 2025) 2, LLaMA3.1-70B (Grattafiori et al., 2024)3 and R1-Distill-LLaMA-70B (DeepSeek-AI Team, 2Qwen/Qwen2.5-72B-Instruct 3https://huggingface.co/meta-llama/Llama-3. 1-70B-Instruct 3 Figure 3: Compared to Repeat and Paraphrase, Multilingual demonstrates higher performance upper bound. Acc@17 scores of Multilingual, Paraphrase and Repeat settings of the three models on the human-translated GPQA dataset. 2025)4 in our experiments. All results are based on their post-trained / instruction-tuned versions. We prompt these models to employ Chain-ofThought (CoT) reasoning for all questions during inference. The prompt template is reported in Appendix A. Testing Scenario We analyze the GPQA(Rein et al., 2023) and MGSM(Shi et al., 2023) datasets, which are supported in 17 languages5 by BenchMAX (Huang et al., 2025) with human translation. GPQA evaluates LLMs reasoning from scientific perspective, while MGSM assesses their mathematical reasoning capabilities. Additionally, we utilize Google Translate to provide machine translation in these languages and compare machine translation with human translation. Metric The default metric that we used is Accuracy (Acc), which measures the agreement of the prediction generated by the model with the ground truth. Acc represents the average accuracy across answer candidates. We use Acc@k metric to test the probability that at least one generated answer out of for problem is the ground truth. Vote@k is utilized to assess models accuracy after selecting answers from candidates using majority voting strategy. Judge@k is used in the LLM-as-a-judge experiments to denote the accuracy of the judged winners. 3."
        },
        {
            "title": "Intriguing Phenomena",
            "content": "We notice that using multiple languages in reasoning is beneficial and results in significant perfor4https://huggingface.co/deepseek-ai/ DeepSeek-R1-Distill-Llama-70B 5English, Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese. Figure 4: Multilingual surpasses Paraphrase and Repeat in Acc@k after = 3 in growing margin. Best Acc@k (out of 17) of Multilingual, Paraphrase and Repeat settings for Qwen2.5-72B with increasing numbers of languages or candidates on the human-translated GPQA dataset. mance improvements. Phenomenon 1: Mixing languages boosts performance, setting higher upper bound. Enhancing language diversity during model generation results in remarkable performance improvements, with the ceiling of these improvements notably high. Compared to Repeat and Paraphrase, as depicted in Figure 3, Multilingual can yield gains of around 8 accuracy points when using 17 candidates each. Phenomenon 2: few languages offer substantial performance boosts. In GPQA task, we rank 17 languages based on their performance in each model from high to low and combined the topperforming languages with varying numbers each time. As shown in Figure 4, as the number of candidate languages increases, the Acc@k performance consistently improves. Notably, just few ( 4) languages can significantly enhance performance, quickly surpassing that of Repeat / Paraphrase. Notably, even if none of the non-English language in the combination outperforms English, their combination can still achieve significant improvements. Phenomenon 3: Multilingual gain - Going beyond existing English benefits As illustrated in Figure 5, Multilingual significantly improves reasoning performance, surpassing the limits achieved by Repeat or Paraphrase. Notably, the improvements of Multilingual do not overlap with those derived from Repeat or Paraphrase. The experiments involving Repeat-Mix and Paraphrase-Mix indicate that replacing portion of the input with multilingual data results in additional benefits to the performance upper bound. This suggests that Multilingual unique advantage in reasoning tasks, 4 Figure 5: Fully utilizing non-English languages can improve the upper bound. Distribution of Acc@4 scores of all possible 4-candidate-combinations with Qwen2.5-72B on the humantranslated GPQA dataset, under different settings. Figure 6: The Multilingual upper bound is stable regardless of the question translation quality. Comparison of Acc@4 on humanand machine-translated GPQA dataset among all possible 4-language combinations in Multilingual setting. The values and error bars denote mean, max and min scores. enabling models to leverage diverse linguistic structures and contexts. Phenomenon 4: The upper bound is tolerant of sub-optimal language choice. Table 1 shows the evaluation results of all 4-combinations out of the 17 testing languages, where \"Best\", \"Worst\" and \"Random\" indicate the highest, lowest and average scores among all the language combinations. Interestingly, while the mean single-language performance varies across combinations (indicated by Acc), the \"Random\" Acc@k scores are close to \"Best\", meaning randomly selected language combination is expected to have similar upper bound to that of the best-performing combination. As depicted in Figure 6, the models exhibit slight differences in performances of random and best language combinations, when tested on humantranslated and machine-translated datasets. This experiment highlights that we can elicit multilingual reasoning with machine-translated data to upper bounds as high as with human-translated data."
        },
        {
            "title": "Selection Strategies",
            "content": "In this section, we explore commonly methods to reach the upper bound, including Majority voting (4.1), Prompt-based language selection (4.2), and LLM-as-a-judge selection (4.3). Model Combo Acc Acc@"
        },
        {
            "title": "4.1 Majority voting",
            "content": "Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best Worst 43.7 37.8 Random 41.5 Best Worst 38.0 32.6 Random 36.9 Best Worst 51.6 34.0 Random 49.0 74.3 65.6 70.0 73.9 65.2 70.2 80.1 64.7 75. Table 1: Multilingual upper bound is robust to language combination choices. Mean Acc (Acc) and Acc@4 of the best, worst and random language combinations (Combo) with the Multilingual setting on the human-translated GPQA dataset. While Acc varies, the gain in Acc@k remains high. Phenomenon 5: The upper bound is robust to translation quality. Obtaining humantranslated multilingual dataset of high quality is challenging and is not scalable for many tasks. Therefore, we investigate whether human translation quality is essential for model performance. We conduct series of experiments across different models to evaluate the effectiveness of various answer selection strategies, specifically Repeat, Paraphrase, and Multilingual settings, as shown in Table 2. In the Acc@4 metric, the Multilingual approach outperforms the other strategies. However, when analyzing the results with Vote@4, Multilingual does not achieve similarly favorable outcomes. This inconsistency suggests that while Multilingual strategies can be advantageous in certain contexts, their overall effectiveness may be limited by the answer selection criteria employed. These findings underscore the necessity of different answer selection strategies across various models."
        },
        {
            "title": "4.2 Prompt-based language selection",
            "content": "One straightforward method for answer selection is the prompt-based approach, which entails furnishing precise input instructions to direct the model in producing the desired outputs. To steer the model 5 Model Setting Best Combo Random Combo Acc@4 Vote@4 Acc@4 Vote@ Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B Repeat Paraphrase Multilingual-h Multilingual-m Repeat Paraphrase Multilingual-h Multilingual-m Repeat Paraphrase Multilingual-h Multilingual-m 71.2 71.0 74.3 75.0 71.0 73.0 73.9 74.8 77.9 74.8 80.1 80.4 53.7 54.4 54.2 53.0 50.4 51.3 49.8 54.4 63.2 60.8 61.2 64. 65.9 66.7 70.0 70.5 66.4 68.7 70.2 68.9 74.3 71.1 75.5 74.7 53.6 52.8 51.7 52.2 49.8 51.6 48.8 49.0 62.9 59.9 60.0 60. reasoning. Table 2: Answer selection is challenging and critical Comparison of for effective Multilingual Acc@4 and Vote@4 with the best (Best Combo) and random 4-combinations (Random Combo) out of 17 languages/candidates with respect to Acc@4, for each model under different settings on the human-translated (Multilingualh) and machine-translated (Multilingual-m) GPQA datasets. towards maximizing its multilingual capabilities, we have customized prompts for the LLM from three crucial viewpoints: language constraint, English allowance, and question translation. Language Constraint (LC) means whether to provide predefined set of languages that the model can utilize. English Allowance (EA) means to whether to incorporate English as one of the languages that can be used.6 Question Translation (QT) means whether to explicitly prompt the model to translate the questions to the target languages thus encouraging multilingual responses. Prompt-based methods cannot unlock models multilingual capabilities during reasoning. As shown in Table 3, no prompt-based approach stands out as superior, with minimal differences between methods. However, there are still some intriguing discoveries:1) Translating the original question from English to non-English before responding does not affect the models final performance. 2) Interestingly, when comparing LLaMA3.1-70B with R1-Distill-Llama-70B, prompt-based methods achieve better results than Repeat and Paraphrase."
        },
        {
            "title": "4.3 LLM-as-a-judge selection",
            "content": "Another commonly used answer selection method is LLM-as-a-judge (Li et al., 2024), where judge model evaluates two answers to given question, Model LC EA QT Setting Acc@4 Vote@4 Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B - - (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) - - (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) - - (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) - - (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) - - (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) - - (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) - - (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) - - (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) - - (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) Repeat Paraphrase - - - - - - Repeat Paraphrase - - - - - - Repeat Paraphrase - - - - - - 65.9 66.7 59.2 63.8 61.2 62.7 61.2 62. 66.4 68.7 58.9 61.8 65.6 62.5 63.2 65.0 74.3 71.1 75.9 73.2 72.8 76.8 72.8 72.8 53.6 52.8 48.2 51.8 53.2 50.6 52.5 52.0 49.8 51.6 46.6 47.5 50.1 46.6 50.6 49.6 62.9 59.9 64.9 59.2 58.7 66.3 56.8 57.7 Table 3: Different prompt-based settings show little performance difference, and self-translation is not the key setting. Acc@4 and Vote@4 of prompt-based selection methods, compared with the random-4 performances of Repeat and Paraphrase on the English GPQA dataset. LC, EA and QT stand for Language Constraint, English Allowance and Question Translation. and selects the best of them as the winner. Here, we use the tested models to judge their own outputs, and conduct pairwise judgments for each two of the candidates with position swapping, and take the one winning the most battles. To test the effectiveness of this method, we run judges on the machine-translated multilingual questions, using the best language combination found on them for each model and collect the accuracies of the judged outputs (Judge@k). Then, we compare them with English Repeat and Paraphrase with the same judging process. The results are shown in Table 4. Still, while Multilingual leads the Acc@k scores, its Judge@k scores are lower than the English baselines except for the R1-Distill-LLaMA model. Also, the Judge@k scores in most of the tested settings are lower than Vote@k scores, meaning LLM-as-ajudge is even less effective than simple majority voting in answer selection. This suggests LLM-asa-judge answer selection is not satisfactory."
        },
        {
            "title": "5 Analysis",
            "content": "6For LC and EA, we use the best language combinations for each model observed on the human-translated GPQA dataset, and substitute between English and non-English candidates while keeping the upper-bound performances as close as possible. While 3 shows the high upper bound gain of multilingualism, 4 shows that common selection approaches have difficulty realizing this gain. Here, we discuss the reasons behind this gap. 6 Model Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B Setting Repeat Paraphrase Multilingual-h Multilingual-m Repeat Paraphrase Multilingual-h Multilingual-m Repeat Paraphrase Multilingual-h Multilingual-m Acc@4* 61.4 63.0 67.0 66.7 Vote@4* 53.4 54.2 53.0 51. Judge@4* 48.9 50.4 48.0 46.4 62.1 65.8 67.2 67.6 71.2 71.9 76.6 76.1 50.6 49.2 50.2 50.9 57.2 59.2 62.3 62.6 47.1 46.2 39.3 41. 57.1 58.9 60.7 62.3 satisfactory. human-translated Table 4: LLM-as-a-judge exhibits Multilingual advantage only with R1-Distill-LLaMA-70B, which is LLM-as-a-judge performance on not the and machinetranslated (Multilingual-m) GPQA datasets. The asterisks(*) indicate that we only include 4 runs in each setting, using the best language combination for the dataset due to the cost of LLM judging, so the results are different from those in the previous tables. (Multilingual-h)"
        },
        {
            "title": "5.1 Possible reasons for the upper bound gain",
            "content": "We propose several possible reasons for the upper bound gain of multilingualism. Language correctness correlates with question difficulty. The first hypothesis is that different languages match questions of different levels of difficulty. For questions in each level, there can be certain suitable languages for the models to use to achieve higher accuracies. To verify this hypothesis, on the GPQA tasks where question difficulties are labeled, we calculate the per-language accuracy on questions with different difficulty levels. The results  (Table 5)  show that the accuracy per language varies, and for each model, there are at least two different languages leading the accuracies on different difficulty levels. This indicates different languages indeed match different difficulty levels. Model Combo Lang Easy Undergrad Hard Undergrad Hard Grad PostGrad Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B en es ja th fr ko sw vi ar es ko sr 47.6 57.1 47.6 47.6 47.6 23.8 47.6 47.6 61.9 66.7 76.2 57. 50.6 44.6 46.8 41.6 42.5 39.9 31.8 38.6 49.8 58.4 54.9 48.9 41.2 43.1 41.8 39.2 41.8 37.9 30.1 39.9 48.4 54.2 47.7 45. 44.1 38.2 35.3 32.4 35.3 38.2 35.3 38.2 41.2 58.8 50.0 35.3 Table 5: Languages to some extent match difficulty levels. Per-language accuracies across difficulty levels on the humantranslated GPQA dataset, where the languages are from the best-performing combinations (Combo Lang). Each difficulty has one or more advantage languages. 7 Task Model Advantage Langs GPQA MGSM Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B ja,en,fr,hu hu,en,fr,ru,de es,vi,cs,fr Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B ko,ar,es,en,sr,vi,hu ru,ko,en,es,vi,de sr,ar,ko,en,cs,hu Table 6: Each model has key advantage languages that often compensate for errors in other languages in the two tasks, and there is cross-model overlap. Key advantage languages (Advantage Langs) found by minority-majority overlap that filters language leading accuracy on questions correctly answered by few or many tested languages. Existence of key advantage languages Another hypothesis is that, for model on specific task with multilingual reasoning, there will be some key advantage languages that often compensate for errors in other languages, which contributes to the high Acc@k. Furthermore, if the key advantage languages overlap in different models, it will be likely that these languages are more suitable than others on the specific task. We set standard called minority-majority overlap to identify such language advantage. First, we collect the languages with high accuracies, both on questions correctly answered only in few languages and by vast majority of the languages. Then, we report the overlap of the leading languages in the both situations. Finally, we report the cross-model overlap of these languages. As shown in Table 6, each model has some key advantage languages in the two tasks, respectively, and there are also cross-model key advantage languages, namely French for GPQA and Korean and English for MGSM."
        },
        {
            "title": "5.2 Challenges to achieve the upper bound",
            "content": "We will discuss some challenges in meeting the multilingual reasoning upper bound with common approaches. Voting performance does not grow with language numbers. As shown in Figure 7, as the size of the language combination increases, the Vote@k score does not increase but instead decreases, which is the opposite to the Acc@k curve in Figure 4. This is mainly because the gain and advantage of multilingualism in Acc@k is often brought by only few languages, especially when the majority is wrong. Thus, larger number of languages can bring more noise, making it harder for the correct answer to win majority. Figure 7: Voting performance does not increase with candidate number. Best Vote@k (out of 17) of Paraphrase, Repeat and Multilingual with human (Multilingual-h) and machine translation (Multilingual-m) on the GPQA dataset for Qwen2.5-72B with increasing numbers of languages or candidates. Figure 8: Multilingual performance is sensitive to the optimality of the language combination. Comparison of Vote@4 of Repeat, Paraphrase and Multilingual with human (Multilingual-h) and machine translation (Multilingual-m) on the GPQA dataset. The values and error bars denote mean, max and min scores. Voting performance relies on optimal language combination. While we show the multilingual reasoning upper bound is tolerant to sub-optimal language combinations in 3.2, the multilingual majority voting performance relies on optimal language combinations to surpass English voting. As shown in Figure 8, the voting accuracy of Multilingual is higher than or quite close to the those of Paraphrase and Repeat if all of them use their best language combinations. However, when the language combination is random or the worst, the Multilingual voting accuracy will be lower than the other two, indicating that majority voting on the Multilingual setting is sensitive to the optimality of the language combination. Prompt-based and LLM-as-a-judge selection have language bias. For prompt-based selection, the models tend to choose high-resource languages for all the questions, thus decreasing the diversity of the Multilingual candidates. Table 7 shows the chosen rates of English and the most frequently chosen non-English language in different settings. The results show that, when English is allowed, the models will choose English in most cases; and when it is not allowed, the models tend to choose certain language (such as Spanish or Vietnamese) than other languages in most cases. Similarly, for LLM-as-a-judge selection, the judge models tend to prefer answers in highresource languages, even if the answer of that language is incorrect. Table 8 shows chosen rate of languages when the answer in that language is correct or incorrect. The results show that, only R1Distill-LLaMA-70B showed stable preference of the correct answers among all the tested languages. Model LC EA QT En Max Non-En Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) 4.4 99.9 99.7 62.1 99.8 99.8 1.1 46.5 99.7 25.6 85.6 99.9 (cid:33) 100.0 (cid:33) 99.9 (cid:33) 99.9 (cid:37) 99.9 (cid:37) 99.9 (cid:37) 99.8 45.5 0.1 0.3 17.2 0.2 0.2 83.4 53.3 0.2 52.5 14.1 0.1 0.0 0.1 0.1 0.1 0.1 0. Table 7: With prompt-based answer selection, models have strong tendency to choose 1-2 certain languages instead of all others. Chosen rates of English and the highest non-English language, with the prompt-based answer selection methods. Instead, all these models tend to roughly maintain the ratios (high for high-resource and low for low-resource) between languages regardless of the answer correctness. These results suggest that the models care more for the language instead of the correctness in judging, explaining why the Multilingual settings only beat Repeat and Paraphrase for R1-Distill-LLaMA-70B with LLM-as-a-judge selection."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we comprehensively explore the benefits of multilingualism in reasoning and highlight several intriguing phenomena. Our findings suggest that utilizing multiple languages can signifi8 Model Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B Human-Translated Dataset Machine-Translated Dataset Lang P(ChosenCorrect) P(ChosenIncorrect) Lang P(ChosenCorrect) P(ChosenIncorrect) en es ja th fr ko sw vi ar es ko sr 42.3 37.8 21.6 7. 41.9 27.6 10.8 19.6 17.2 50.2 28.7 16.4 39.3 32.4 15.1 6.2 38.2 26.6 15.0 19.7 14.2 40.0 23.2 8.5 ar de ja zh de en hu ru ar es ru vi 6.1 36.2 22.0 42.9 20.1 54.3 10.8 17.4 13.5 37.7 26.5 40.1 8.1 36.5 17.0 32. 21.5 45.4 13.6 17.4 8.2 32.6 13.7 26.5 Table 8: With LLM-as-a-judge answer selection, the models care more for the language instead of the correctness of the answer, and only R1-Distill-LLaMA revealed steady preference of the correct answers in all the judged languages. Language chosen rates in Multilingual LLM-as-a-judge answer selection, where (ChosenCorrect) refers to the chosen rate of this language when its answer is correct, and (ChosenIncorrect) when the answer is incorrect. The tested languages are from the best-performing combinations in the experiments in 4.3. cantly enhance reasoning capabilities, with high upper bound for this benefit. Notably, this advantage is resilient to variations in translation quality and language choice, yet it remains sensitive to the methods used for answer selection. We examine various commonly used answer selection techniques but find that they often fall short of fully harnessing the potential of multilingualism in reasoning tasks. This disparity between the theoretical upper bound and practical experimental outcomes presents both challenge and promising avenue for future research."
        },
        {
            "title": "Limitations",
            "content": "In this study, while providing valuable insights into the potential of multilingualism in reasoning, has several notable limitations. However, our focus is primarily on large models with over 70 billion parameters, which may not fully represent the capabilities or challenges faced by smaller models. This narrow scope could lead to an incomplete understanding of how multilingualism affects reasoning across various architectures and sizes. Additionally, although we observe several interesting phenomena, the absence of universal and stable method for leveraging multilingualism in reasoning."
        },
        {
            "title": "References",
            "content": "Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. To code, or not to code? exploring impact of code in pre-training. Preprint, arXiv:2408.10914. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, and Zhoujun Li. 2024. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. Preprint, arXiv:2401.07037. Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. 2024. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 70017016, Miami, Florida, USA. Association for Computational Linguistics. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Preprint, arXiv:2204.02311. DeepSeek-AI Team. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lacalle, and Mikel Artetxe. 2024. Do multilingual language models think better in English? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 550564, Mexico City, Mexico. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, and Pengfei Liu. 2022. Polyglot prompt: Multilingual multitask prompt training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 99199935, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Vedant Gaur and Nikunj Saunshi. 2023. Reasoning in large language models through symbolic math word problems. In Findings of the Association for Computational Linguistics: ACL 2023, pages 58895903, Toronto, Canada. Association for Computational Linguistics. Gemini Team. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, 10 Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, 11 Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12365 12394, Singapore. Association for Computational Linguistics. Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang. 2022. Zero-shot crosslingual transfer of prompt-based tuning with unified multilingual prompt. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1148811497, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, and Fei Yuan. 2025. Benchmax: comprehensive multilingual evaluation suite for large language models. Preprint, arXiv:2502.07346. Zixian Huang, Wenhao Zhu, Gong Cheng, Lei Li, and Fei Yuan. 2024. Mindmerger: Efficiently boosting In AdLLM reasoning in non-english languages. vances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Shima Imani, Liang Du, and Harsh Shrivastava. 2023. MathPrompter: Mathematical reasoning using large In Proceedings of the 61st Anlanguage models. nual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 37 42, Toronto, Canada. Association for Computational Linguistics. Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2024. Self-planning code generation with large language models. Preprint, arXiv:2303.06689. Huiyuan Lai and Malvina Nissim. 2024. mCoT: Multilingual instruction tuning for reasoning consistency in language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12012 12026, Bangkok, Thailand. Association for Computational Linguistics. Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023a. Structured chain-of-thought prompting for code generation. Preprint, arXiv:2305.06599. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. 2025. Codei/o: Condensing reasoning patterns via code input-output prediction. Preprint, arXiv:2502.07316. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. Preprint, arXiv:2406.11939. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware In Proceedings of the 61st Annual Meetverifier. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada. Association for Computational Linguistics. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774. Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 26952709, Singapore. Association for Computational Linguistics. Qwen Team. 2025. Qwen2.5 technical report. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Preprint, arXiv:2311.12022. Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. 2024. MAPO: Advancing multilingual reasoning through multilingual-alignment-as-preference optimization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001510027, Bangkok, Thailand. Association for Computational Linguistics. 12 Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. Preprint, arXiv:2211.09085. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, and Dong Yu. 2024. Selfconsistency boosts calibration for math reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 60236029, Miami, Florida, USA. Association for Computational Linguistics. Dingzirui Wang, Longxu Dou, Wenbin Zhang, Junyu Zeng, and Wanxiang Che. 2023a. Exploring equation as better intermediate meaning representation for numerical reasoning. Preprint, arXiv:2308.10585. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023b. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language In Proceedings of the 61st Annual Meetmodels. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26092634, Toronto, Canada. Association for Computational Linguistics. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. 2023c. Hypothesis Search: Inductive Reasoning with Language Models. In The Twelfth International Conference on Learning Representations. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, MongLi Lee, and Wynne Hsu. 2024. Faithful logical reasoning via symbolic chain-of-thought. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1332613365, Bangkok, Thailand. Association for Computational Linguistics. Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, and Jiajun Zhang. 2024. Language Imbalance Driven Rewarding for Multilingual Self-improving. In The Thirteenth International Conference on Learning Representations. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. 2023. Large Language Models as Analogical Reasoners. In The Twelfth International Conference on Learning Representations. Fei Yuan, Shuai Yuan, Zhiyong Wu, and Lei Li. 2024. How vocabulary sharing facilitates multilingualism in LLaMA? In Findings of the Association for Computational Linguistics: ACL 2024, pages 1211112130, Bangkok, Thailand. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, pages 4659546623, Red Hook, NY, USA. Curran Associates Inc. 13 Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. 2024. Question translation training for better multilingual reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 84118423, Bangkok, Thailand. Association for Computational Linguistics. R1-Distill-LLaMA-70B (MIT license), large language model developed by Deepseek. Qwen-2.5-72B (Qwen license), large language model developed by Qwen."
        },
        {
            "title": "A Models",
            "content": "A.1 Model Description Qwen2.5-72B is cutting-edge language model designed to enhance natural language processing tasks with its impressive 72 billion parameters. This model excels in generating coherent and contextually relevant text, making it particularly valuable for applications in content creation, conversational agents, and automated summarization. LLaMA3.1-70B represents the latest iteration in the LLaMA series, boasting 70 billion parameters that empower it to tackle complex reasoning tasks and generate high-quality text. This model is particularly noted for its ability to engage in multi-turn conversations, maintaining context and coherence over extended interactions. R1-Distill-LLaMA-70B is distilled version of the original LLaMA model, optimized for efficiency without compromising performance. With 70 billion parameters, this model is designed to deliver faster response times and reduced computational requirements, making it ideal for deployment in resource-constrained environments. A.2 Languages-Related Prompt We present the prompt templates utilized in our experiments, including the Default and Prompt-based selection, as shown in Table 9. In the prompt-based selection experiments, we incorporated languagerelated constraints regarding whether to translate the question. Consequently, there are two variations of prompt-based selection: Translation=True and Translation=False, as indicated in the table."
        },
        {
            "title": "B Results on MGSM",
            "content": "We demonstrate the results of the three models on the MGSM task. The results of the Repeat, Paraphrase, Multilingual, Repeat-Mix, and ParaphraseMix methods are presented in Table 10, Table 11, Table 12, Table 13, and Table 14, respectively. Table 15 shows the results on the Google translated MGSM task."
        },
        {
            "title": "C Used Scientific Artifacts",
            "content": "Below lists scientific artifacts that are used in our work. For the sake of ethic, our use of these artifacts is consistent with their intended use. LLaMA-3.1 (LLaMA3.1 license), large language model developed by Meta. 15 Setting Default Prompt-Based Selection Translation = True Prompt-Based Selection Translation = False Prompt GPQA System prompt: Always think step by step and give your final choice among (A), (B), (C) and (D) by Änswer: {Your Choice}ïn single last line. User prompt: What is the correct answer to this question:{Question} Choices: (A) choice1 (B) choice2 (C) choice3 (D) choice4 Lets think step by step: System prompt: Always choose the most suitable language, translate the question into that language, and think step by step in that language. Give your final choice among (A), (B), (C) and (D) by Änswer: {Your Choice}ïn single last line. User prompt: What is the correct answer to this question:{Question} Choices: (A) choice1 (B) choice2 (C) choice3 (D) choice4 Lets think step by step: System prompt: Always choose the most suitable language, and think step by step in that language. Give your final choice among (A), (B), (C) and (D) by Änswer: {Your Choice}ïn single last line. User prompt: What is the correct answer to this question:{Question} Choices: (A) choice1 (B) choice2 (C) choice3 (D) choice4 Lets think step by step: MGSM Default System prompt: Always think step by step and give your final answer by Änswer: Your Answerïn single last line. User prompt: Question: {Question} Step-by-Step Answer: Prompt-Based Selection Translation = True System prompt: Always choose the most suitable language, translate the question into that language, and think step by step in that language. Give your final answer by Änswer: Your Answerïn single last line. User prompt: Question: {Question} Step-by-Step Answer: Prompt-Based Selection Translation = False System prompt: Always choose the most suitable language, and think step by step in that language. Give your final answer by Änswer: Your Answerïn single last line. User prompt: Question: {Question} Step-by-Step Answer: Table 9: The prompt template we used in experiments for each task. Model Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Setting Best Worst Random Best Worst Random Best Worst Random English Seed English Seed2 English Seed3 English Seed4 Acc Acc@4 Major@4 93.6 91.6 - 91.6 90.0 - 93.6 93.6 - 91.6 92.0 - 92.0 90.8 - 92.8 94.0 - 92.0 91.2 - 92.4 91.6 - 94.0 91.6 - 92.8 92.0 - 93.6 91.6 - 94.4 92.0 - 92.5 91.7 92.4 92.4 91.0 91. 93.7 92.8 93.6 94.0 92.4 93.7 96.0 92.4 94.8 97.2 94.0 96.0 93.6 92.0 92.5 93.2 91.2 92. 94.4 94.0 94.0 Table 10: The results of the Repeat method on the MGSM task. 16 Model Setting English English English English Paraphrase1 Paraphrase2 Paraphrase3 Paraphrase4 Acc Acc@4 Major@4 Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best Worst Random Best Worst Random Best Worst Random 92.0 89.6 - 90.0 86.0 - 89.6 89.6 - 88.8 88.4 - 90.0 88.4 - 91.2 89.2 - 90.8 88.0 - 91.2 87.2 - 91.2 89.2 - 91.6 88.4 - 88.8 88.8 - 90.4 89.2 - 90.8 88.6 89. 90.0 87.6 88.9 90.6 89.3 90.0 96.0 92.4 94.6 96.4 92.4 94.8 96.8 92.8 94.9 93.2 90.0 91. 91.6 89.2 90.8 91.6 92.0 91.4 Table 11: The results of the Paraphrase method on the MGSM task. Model Setting LangLang-2 Lang-3 Lang-4 Acc Acc@4 Major@4 Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best (ar,cs,en,ko) Worst (bn,sw,te,zh) Random Best (ar,ru,sr,vi) Worst (bn,cs,sw,zh) Random Best (ar,bn,de,sr) Worst (bn,de,te,vi) Random 91.2 83.2 - 87.2 81.2 - 90.8 75.2 - 84.8 63.2 - 90.0 85.2 - 75.2 86.4 - 92.4 62.8 - 87.2 84.4 - 86.4 78.0 - 91.2 86.4 - 90.0 84.4 - 92.4 87.6 - 89.9 73.9 84.7 88.6 83.8 86. 86.2 81.8 86.4 98.0 92.8 95.8 99.6 93.6 96.9 99.6 95.6 97.8 94.0 86.8 91.7 92.8 90. 92. 92.8 90.4 92.6 Table 12: The results of the Multilingual method on the MGSM task. Model Setting Lang-1 LangLang-3 Lang-4 Acc Acc@4 Major@4 Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best (en,en,es,te) Worst (en,en,ar,bn) Random Best (en,en,es,th) Worst (en,en,cs,de) Random Best (en,en,es,vi) Worst (en,en,ar,bn) Random 92.4 92.4 - 91.6 91.6 - 92.8 92.8 - 92.8 92.8 - 91.6 91.6 - 93.2 93.2 - 90.8 91.2 - 92.0 85.2 - 87.2 90.8 - 62.8 83.2 - 87.2 88.8 - 87.6 75.2 - 84.7 89.9 85.8 90.3 89.3 87.2 90.2 88.0 87. 97.6 93.2 95.4 99.2 92.8 96.9 99.6 94.8 97.6 93.2 92.8 92.8 92.0 91.2 92.5 94.0 93.2 94. Table 13: The results of the Repeat-Mix method on the MGSM task. Model Setting Lang-1 Lang-2 LangLang-4 Acc Acc@4 Major@4 Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best (en,en,es,te) Worst (en,en,bn,cs) Random Best (en,en,es,te) Worst (en,en,ar,cs) Random Best (en,en,es,vi) Worst (en,en,cs,de) Random 92.0 88.8 - 90.0 90.0 - 89.6 89.6 - 88.8 89.6 - 90.0 90.0 - 91.6 89.2 - 90.8 83.2 - 92.0 87.2 - 87.2 87.2 - 62.8 84.8 - 82.8 85.2 - 87.6 86.4 - 83.6 86.6 85.4 88.7 88.1 86.9 89.0 88.1 86.8 98.4 92.0 95. 99.2 92.8 96.8 99.6 92.0 96.7 91.6 91.2 91.8 93.6 91.6 92.7 93.2 92.4 92.7 Table 14: The results of the Paraphrase-Mix method on the MGSM task. 17 Model Setting Lang-1 Lang-2 LangLang-4 Acc Acc@4 Major@4 Qwen2.5-72B LLaMA3.1-70B R1-Distill-LLaMA-70B Best (ar,en,es,hu) Worst (bn,cs,fr,sw) Human (ar,cs,en,ko) Random Best (de,fr,ja,vi) Worst (bn,cs,sw,zh) Human (ar,ru,sr,vi) Random Best (ar,hu,sr,zh) Worst (de,sw,te,vi) Human (ar,bn,de,sr) Random 88.8 29.2 88.8 - 88.0 81.6 86.4 - 89.6 86.8 89.6 - 92.4 82.4 82.4 - 82.8 84.0 89.2 - 82.0 80.0 52.8 - 89.2 82.4 92.4 - 83.6 79.6 85.6 - 87.2 77.2 86.8 - 78.8 62.8 88.0 - 89.2 84.8 89.2 - 88.4 84.8 87.2 - 87.3 64.2 87.9 80.5 85.9 82.5 87.6 85.0 86.8 82.2 79.1 83.6 98.0 92.4 96.4 96. 99.2 94.0 97.2 96.8 98.8 93.6 98.0 97.2 92.4 84.0 92.4 90.7 90.4 90.0 93.6 91.4 94.0 91.2 92.8 92.1 Table 15: The results of the Multilingual method on the Google translated MGSM task. Model Qwen2.572B LLaMA3.170B R1-DistillLLaMA-70B Langs repeat paraphrase ar,en,es,hu repeat paraphrase de,fr,ja,vi repeat paraphrase ar,hu,sr,zh Acc@k Major@k 93.6 94.0 98.0 94.8 95.2 99.2 96.4 93.6 98.8 93.2 91.6 92.4 92.0 91.6 90.4 93.6 91.6 94.0 Judge@k 91.2 90.0 89.6 92.0 91.6 88.0 92.8 88.8 91.6 Table 16: LLM-as-a-judge performance on MGSM dataset"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}