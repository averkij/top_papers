{
    "paper_title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
    "authors": [
        "Vladislav Pedashenko",
        "Laida Kushnareva",
        "Yana Khassan Nibal",
        "Eduard Tulchinskii",
        "Kristian Kuznetsov",
        "Vladislav Zharchinskii",
        "Yury Maximov",
        "Irina Piontkovskaya"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 1 2 5 1 . 1 1 5 2 : r Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story Vladislav Pedashenko1*, Laida Kushnareva2*, Yana Khassan Nibal2, Eduard Tulchinskii2, Kristian Kuznetsov2, Vladislav Zharchinskii1, Yury Maximov3, Irina Piontkovskaya *Equal Contribution, 1 Moscow State University, 2 Lomonosov Research Institute 3 Interdata Astana"
        },
        {
            "title": "Abstract",
            "content": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropybased metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (8), encyclopedic content medium ID (9), and creative/opinion writing high ID (10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively easy, whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results."
        },
        {
            "title": "Introduction",
            "content": "Quantifying data complexity is one of the key pillars supporting modern ML and LLM theory. Early informationtheoretic views (e.g., the Information Bottleneck) tie learning efficiency to compressibility (Tishby and Zaslavsky, 2015), while the manifold perspective motivates geometric view of representations. In NLP, complexity assessments have evolved from dataintrinsic measures (e.g., gzip ratio, lexical or syntactic indicators) to modelbased 1 ones that depend on how model organizes and predicts text. The latter divide into predictive metrics (entropy, cross-entropy, perplexity) and geometric metrics (anisotropy, intrinsic dimension). While entropy-like measures are ubiquitous in training and evaluation, geometric complexity remains comparatively underexplored. We focus on intrinsic dimension (ID) as geometric measure of degrees of freedom in embedding space. We establish conceptual separation between ID and prediction-based entropy: ID depends solely on the geometry of hidden representations, whereas entropy arises only after unembedding and reflects alignment with the vocabulary. Hence, the two quantities are not substitutes. We begin by showing empirically that, after controlling for text length, ID and (cross-)entropy are essentially uncorrelated; we then substantiate this separation with formal argument. In this paper, we provide comprehensive, semantics-grounded account of ID for text. First, we characterize how ID varies across domains and styles, showing that scientific/informational prose is consistently low-ID, while opinionated and fictional writing is high-ID, indicating increased representational degrees of freedom from the models perspective. Second, we link ID to observable linguistic regularities: higher lexical diversity and lower cross-sentence repetition associate with higher ID, whereas syntactic indicators are comparatively uninformative for well-formed text. Third, using sparse autoencoders and feature steering, we connect ID to interpretable semantic axes (e.g., genre, narrative, personalization, emotional tone) and probe their causal effects on generation. Together, these results position ID as complementary, geometry-based lens on textual complexity with practical implications for evaluation and training: specifically, the need to balance low-ID (encyclopedic/scientific) and high-ID (personal/narrative) data to faithfully assess model capabilities. The paper is organized as follows: Section 2 formalizes ID and its relation to predictive metrics; Section 3 details our SAE-based analysis methodology; Section 5 presents experimental observations. Finally, Section 6 summarizes our findings and discusses implications. 1.1 Related Work Intrinsic Dimension in Neural Networks. Intrinsic dimension (ID) analysis provides geometric framework for understanding and improving large language models (LLMs). Early work by Li et al. (2018) showed that good solutions lie in lowdimensional subspaces of the parameter space. Extending this idea to NLP, Aghajanyan et al. (2021) demonstrated that task-specific variation in pretrained models is confined to low-dimensional subspace, inspiring low-rank adaptation methods such as LoRA (Hu et al., 2022), which assume that effective fine-tuning occurs within compact intrinsic subspaces. Recent studies have shifted from parameter to representation space, using nonlinear estimators to measure ID in embeddings. While Havrilla and Liao (2024) link ID to model scaling, Razzhigaev et al. (2024) demonstrate that it may reflect training saturation and dynamics. Viswanathan et al. (2025) confirm previous observations and show that higher ID correlates with higher entropy and loss. ID has also been used for architecture search (He et al., 2023), providing an efficient geometric signal for model evaluation. Further, Arnold (2025) relate ID to memorization capacity, while Lee et al. (2025) show that nonlinear ID captures semantic compositionality, unlike linear measures such as PCA. Persistent Homology Dimension (PHD) in Neural Networks. Intrinsic dimension can be estimated via Persistent Homology Dimension (PHD), which combines local and global properties of point cloud (Schweinhart, 2021). Birdal et al. (2021) link it to generalization property of neural networks, while Tulchinskii et al. (2023a) show that texts generated by early GPT models exhibit lower PHD than human-written texts, enabling simple AI text detection. However, this distinction diminishes for newer models (Kuznetsov et al., 2024). Building on this, Kushnareva et al. (2024) demonstrate that local changes in intrinsic dimensionality can detect boundaries between human and machine-generated text segments. Other Intrinsic Dimension Approaches. Several studies explore ID-based analysis for downstream tasks. Tsukagoshi and Sasano (2025) show that substantial dimensionality reduction of taskspecific embeddings is possible with minimal performance loss across classification, clustering, and retrieval tasks, using estimators such as TwoNN and metrics like Isoscore. Yin et al. (2024) apply Local Intrinsic Dimension (LID) to hallucination detection, showing that high and highly variable LID signals fabricated or unsupported content, whereas truthful generations exhibit smoother, low-variance LID profiles. Ruppik et al. (2025) further argue that intrinsic dimensionality varies locally across embedding space: common tokens occupy simple, low-LID neighborhoods, while rare or domain-specific terms require higher-dimensional representations. Layer-wise analysis reveals that lower layers exhibit uniformly low LID, whereas higher layers show increased LID and variance, reflecting growing specialization and semantic disambiguation. This rapidly expanding body of research demonstrates that intrinsic dimension in neural networks is not merely mathematical curiosity, but rather practical tool that offers pathways toward more interpretable and capable models and opens new applications in downstream tasks."
        },
        {
            "title": "Intrinsic Dimension",
            "content": "Intrinsic dimension (ID) measures datasets degrees of freedom: for linear spaces and smooth manifolds it matches the usual dimension, and extensions to metric spaces aim to remain robust to noise and recoverable from finite samples. rich line of work has produced estimators of intrinsic dimension with complementary strengths. TwoNN estimates ID from ratios of first/second neighbor distances via simple loglog fit (Facco et al., 2017); MLE fits local Poisson model to kNN distances; TLE leverages extreme-value theory in tight neighborhoods to reduce variance (Amsaleg et al., 2022). Beyond distance methods, PHDIM uses persistent homology to capture multi-scale connectivity/holes and is comparatively stable under text domain shift and addition of noise to embeddings (Tulchinskii et al., 2023b). The formal definitions are given in App. A.1. 2 2.1 Relation between ID and model-prediction entropy Intrinsic dimension (ID) is computed from the geometry of hidden representations and thus captures text complexity from the models embedding space, complementing prediction-based metrics such as (cross-)entropy (Viswanathan et al., 2025). We show that ID and entropy are complementary: ID encodes structural information not reflected in entropy. Formally, let h1:T be last-layer embeddings (the point cloud used for ID). Entropy arises only after unembedding: logits zt = ht + and softmax. Hence, entropy depends on the alignment of ht with the vocabulary matrix , whereas ID depends solely on the geometry of {ht}. Figure 1 (left) illustrates this consideration: identical geometric structure (same ID) can yield different entropies if the local density of unembedding vectors differs. Consequently, ID cannot be replaced by entropy: synonym-rich neighborhoods raise entropy without changing geometry; conversely, structurally complex but predictable text can have low entropy yet high ID (see App. A.2). Empirically, after controlling for length, ID and entropy are essentially uncorrelated, indicating that they capture distinct facets of complexity (Fig. 1, right); residual dependence remains only in the very low-ID regime."
        },
        {
            "title": "Intrinsic Dimension through the Lens of\nText Semantics",
            "content": "We aim to characterize which semantic and stylistic properties of text are associated with higher or lower intrinsic dimension (ID). Our methodology combines sparse, interpretable representations learned from model activations with causal interventions and independent linguistic diagnostics. 3.1 Sparse Autoencoders (SAEs) and feature steering SAEs learn sparse, approximately monosemantic features from LLM activations, addressing polysemantic neurons (Olah et al., 2020) under the superposition view (Elhage et al., 2022). Given layer activation Rd, the encoderdecoder (Sharkey et al., 2023) = σ(Wencx + benc), ˆx = Wdecf + bdec (1) produces sparse, nonnegative code over decoder columns dj (feature directions), with bdec + (cid:80) fjdj. For text, we aggregate features tokenwise activations (e.g., sum) to obtain its sequence-level value. To probe causality, we modify the models hidden state during generation using chosen SAE feature (Kuznetsov et al., 2025): = + λ Ai di, (2) where Ai is the features reference-scale (max activation on held-out set) and λ controls intervention strength. SAE features give interpretable axes of variation; steering tests their causal effects on text style and content. We use this to examine features correlated with intrinsic dimension (ID) and to validate whether they systematically increase or decrease representational complexity in generated text. SAE features and selection. Building on evidence that sparse autoencoders (SAEs) separate semantic differences between synthetic and natural texts (Kuznetsov et al., 2025), we utilize SAEs trained on LLM activations to obtain sparse latent features. For each feature, we aggregate tokenlevel activations to the sequence level and measure its correlation with ID. Features with the strongest positive and negative correlations are retained for analysis. Interpreting and probing features. To ascribe semantic meaning to the selected features, we: (i) perform extremal-activation analysis by contrasting texts across activation quantiles; (ii) assess domain specialization via average activations over diverse human-written corpora; and (iii) conduct steering experiments, applying small interventions along feature direction during generation to observe controlled changes while preserving fluency. The three perspectives jointly provide convergent evidence about how each feature relates to ID. External linguistic validation. We relate ID to independent text properties using TAACO (Crossley et al., 2019) the tool for analysing lexical diversity and cohesion, e.g., moving-window typetoken ratios and sentence-to-sentence overlap (see Appendix C.3 for details) and syntactic metrics: syntactic diversity measured as average pairwise distance between dependency-graph representations via WeisfeilerLehman kernel (Guo et al., 2025), and the POS compression ratio based on gzip over POS-tag sequences (Shaib et al., 2024) see Appendix C.2. These diagnostics corroborate 3 Figure 1: Intrinsic dimension characterizes the geometry of hidden representations (blue points on the leftmost frame), while prediction-based metrics such as entropy and cross-entropy depend on the unembedding dictionary (red points on the leftmost frame). Sequences of embeddings with the same intrinsic dimension may yield very different prediction entropies, depending on how densely the unembedding vectors populate the surrounding space (i.e., on the number of close neighbors, shown by grey connections). Note significant correlation between PHD and Cross-Entropy loss (center frame) and weak correlation between PHD and Cross-Entropy loss, normalized by text length in gemma tokens (rightmost frame). SAE-based findings and ground ID variation in observable lexical and syntactic structure."
        },
        {
            "title": "4 Experimental details",
            "content": "Data. We use the GenAI Content Detection Task 1 dataset of Wang et al. (2025) (henceforth, COLING). Unless stated otherwise, experiments run on cleaned split of the development set: from 261,758 texts (human and multiple LLM families), we retain 172,750 after filtering out (i) samples with < 150 GEMMA tokens and (ii) cases where any PHDIM estimator on GEMMA-2-2B, QWEN2.5-1.5B, or ROBERTA-BASE was unstable or outside [2, 18]. COLING spans diverse domains (news, Wikipedia articles, essays, QA, summaries, reviews, scientific abstracts, forums, technical documentation) and generations from T5/T0, GPT, LLaMA, OPT, Mixtral, and others. Most semantic interpretation experiments are performed on the subset of Human-written texts. proficiency use ONESTOPENGLISH (Vajjala and Luˇcic, 2018): 189 human-authored texts, each in Elementary/Intermediate/Advanced versions (567 total). For controlled generation we use RAID (Dugan et al., 2024): (i) 1,000 prompts with temperatures 0.22.0 for (ii) 20 prompts from each of five domains (abstracts, books, news, Reddit, Wikipedia) for fine-grained steering. temperatureID studies; analysis we For Figure 2: Correlations among various ID estimators. (G) denotes ID estimators upon Gemma, (R) - RoBERTa, (Q) - Qwen. Note that PHD estimators upon all three models have correlation more than 0.5 with all other estimators, making it solid compromise. See Appendix for scatterplots and further discussion. 16k) on GEMMA-2-2B (base and instruct). For steering, we apply λ {0.25, 0.5, 1, 5, 10, 15} and report the largest λ per feature that preserves coherence (very large λ can induce repetitive or broken outputs)."
        },
        {
            "title": "5 Results",
            "content": "5.1 Comparison of ID estimators SAE setup. We employ GEMMA-SCOPE (Lieberum et al., 2024) SAEs for GEMMA-2 (Team, 2024), using the canonical residual SAE (width We compare PHDIM, MLE, TLE, and TWONN on GEMMA, QWEN, and ROBERTA embeddings  (Fig. 2)  . All estimators are substantially corre4 lated (pairwise > 0.45). TWONN deviates most, whereas PHDIM tracks the others best (r > 0.67 with each on the same embeddings), so we use PHDIM as the primary estimator. Across embeddings, GEMMA and QWEN agree closely for fixed estimator, while ROBERTA diverges more; we revisit this in Appendix. Within model family, ID grows slightly with model size  (Fig. 11)  . Nevertheless, across methods, sizes, and families, well-formed texts concentrate in narrow band (ID 515), with outliers due to very short texts  (Fig. 14)  or broken syntax. These IDs remain far below ambient embedding dimensions (hundreds to thousands). 5.2 PHDim and data properties Text length. We find PHDIM to be stable for sufficiently long samples, with high variance only for short texts (App. C.1). Hence, we exclude texts shorter than 150 tokens. Intrinsic data complexity. PHDIM is positively correlated with gzip compression ratio (CR); for texts longer than 150 tokens the correlation exceeds 0.3  (Fig. 3)  . The scatter exhibits wedge-shaped support: 10 CR ID 25 CR. Thus, CR bounds but does not pointwise predict ID. Embedding geometry. Appendix B.3 shows only weak correlations between ID and isotropy measures, indicating that radial/directional uniformity is not driver of PHDIM. By contrast, the correlation between PHDIM and explained variance at rank (EV-k) is sharply peaked at 60  (Fig. 12)  . This suggests global linear embedding dimension of 60 despite local ID of 10, highlighting strong manifold nonlinearity and cautioning against purely linear characterizations. Lexical properties. PHDIM correlates positively with lexical diversity (typetoken ratio and n-gram diversity) and negatively with sentencelevel overlap/repetition; syntax/discourse metrics show weak associations  (Fig. 4)  . Appendix experiments further show: (i) robustness to perturbations that preserve repetition patterns while destroying semantics; (ii) extreme out-of-range values on broken generations from weaker LLMs (very low for repetitive, very high for disconnected text); and (iii) limited informativeness of syntactic metrics for well-formed texts. 5.3 ID divergence by text source and domain Contrary to Tulchinskii et al. (2023b), we observe clear shift of ID values between domains  (Fig. 5)  . Figure 3: PHD and gzip Figure 4: Top-10 features from TAACO with the strongest correlation with PHD(Gemma). See Appendix C.3 for similar barplots with MLE, TLE, TwoNN. The domains can be divided into three groups with distinct semantic and stylistic characteristics: 1. Low-dimensional group (mean PHDIM 7.58): includes scientific and technical sources such as arxiv, medicine, pubmed, wiki_csai, and sci_gen. 2. Medium-dimensional group (mean PHDIM 89): consists of factual and reporting texts with encyclopedic or formal news style, such as cnn, xsum, and wikipedia. 3. High-dimensional group (mean PHDIM 9 10.5): includes essays, opinionated writing, and user-generated content with more informal or creative style, such as cmv, eli5, wp. 5 Figure 6: PHD by groups with different embedders models. correlated cases. First, we computed mean activations across domains. Figure 7 shows clear bifurcation: scientific cluster (typified by arXiv) and humanized cluster with strongest activations in wp (writing prompts/short fiction). The former correlates negatively with ID, the latter positively, confirming close link between ID and content  (Table 1)  . We then interpreted features via (i) extremalactivation examples and (ii) targeted steering. The evidence is consistent: features positively correlated with ID yield more emotional, personalized, rhetorically complex text, whereas negative features push generations toward academic/reporting styles (formal tone, rigid structure). Layer-wise, late layers (2425) set global discourse templates, e.g., movie-plot, social-media article, BBC-style report, analytical template (cf. 14085, 4610, 15879, 2409), whereas layer 16 modulates local stylistic details (e.g., scientific framing 5159, population statistics 8104, claim foregrounding 2433). Among informal signals, 5228 induces an uncertain or subdued tone, 9868 injects engaging elements into abstracts, and 6978 steers toward writing-assistant voice. Table 2 shows concise steering transformations; additional examples appear in App. D.2."
        },
        {
            "title": "6 Discussion",
            "content": "Intrinsic dimension (ID) remains an underexplored measure of textual complexity. Our extensive experiments (with details in the Appendix) elucidate its semantic and structural correlates and support the following conclusions. Complementarity to prediction-based metrics. ID is complementary to (cross-)entropy and related Figure 5: PHD(Gemma) by source with group differentiation. The lowest ID values appear in pubmed papers, while the highest (among public-domain corpora) occur in fictional stories from wp. Notably, peerread (OpenReview discussions), which blends scientific and opinionated styles, lies in the mid-PHDIM range. The ordering of domains by PHDIM is nearly identical across encoders  (Fig. 6)  . We next estimate PHDIM for student-written stories at three proficiency levelselementary, intermediate, and advanced  (Fig. 25)  . PHDIM rises with linguistic complexity: elementary texts have mean PHDIM 9.5 (near the lower edge of the fictional group), whereas advanced texts approach or slightly exceed wp fiction (mean 12.5). Although ID correlates with proficiency, genre is the stronger factor: even elementary fiction surpasses scientific and informational genres. These results imply that, for modern LLMs, scientific texts are the simplest, while informal, opinionated, and creative writing drives higher intrinsic dimensionality, peaking in fiction. This pattern aligns with our linguistic analysis: scientific prose shows strong topical/terminological coherence (high overlap/repetition, negatively correlated with PHDIM), whereas fiction demands greater lexical diversity. Simplified syntax at the elementary level has little effect on embedding dimensionality. 5.4 SAE-based interpretation We selected set of middle-layer features with the strongest absolute correlation with intrinsic dimension (ID), including both positively and negatively 6 Figure 7: Colormap of SAE feature activations table by source of the text (domain) Feature Domain Extreme values Steering Typical features with negative correlation with ID: 16-5984 arxiv, sci-gen 0.3 Polished, formal, logically structured documents (e.g., academic, reports) Academic reframing; nominalizations; enumerations 16-14485 arxiv, sci-gen 0.36 Complex arguments, structured aca16-2433 16arxiv, pubmed, sci-gen, peerread arxiv, pubmed, sci-gen 16-5159 arxiv, sci-gen 24-15879 arxiv demic or expository texts 0.35 Unified compositions with clear rhetorical purpose 0.33 Continuous formal prose (abstracts, summaries) 0.34 Academic/scientific style, technical vocabulary, objective tone 0.31 Highly formal, technical, objective academic prose 25-2409 arxiv, sci-gen, peerread 0.43 Formal, self-contained, expository prose Formal academic tone; expository structure; sectioning Outline/placeholder scaffold; bulletization; meta-headers Social-science framing; stats; institutional voice Analytical reportese; reference; repetitive passive BBC/news article template; anecdotal lede; call-to-action Stronger macro-structure; cohesive sections; finally cadence survey selfTypical features with positive correlation with ID: wp, cmv, wikihow, eli5, 16-15275 yelp, cnn wp, imdb 16-5228 0.36 Coherent, human-readable, publication-quality texts 0.33 Conversational, emphatic, length16-6978 16-9868 wp, wikihow, outfox, cnn wp 16-1693 wp, cnn, wikihow ened texts with repetition 0.36 Multi-source, composite, sometimes 0.37 incoherent synthesis Informal, noisy, user-generated style; robust to corrupted input 0.37 Long, elaborated, multi-threaded ar25-14085 wp, cnn, imdb 0. guments/narratives Personal voice; 1st/2nd person; informal, conversational, opinionated; narratives and reviews. introspecPersonalization; concrete characters; richer plot detail Uncertainty/hedging; tion; open-endedness Informal/meta voice; bracketed notes; assistant aside Media/teaser tone; punchy fragments; promo cadence Forum/discussion vibe; colloquial; community framing Movie/outline format; headings; character bios 24-4610 wp, tldr 0.36 Paragraphs or list items start with ,, , sometimes exaggerated Social-media article; bold subheads; quotes + how-to steps Table 1: Short interpretations of selected features and their effect on document style depending on activation. 7 Sign Shift Feature Default generation (excerpt): The classic Ford Falcon, symbol of American muscle in the 1960s, is popular choice for car enthusiasts. . . . The problem can stem from number of factors, including clogged fuel line, faulty fuel pump, or blocked fuel filter. 16-14485 Example change (steered) Research-report structure 16-5159 16-15275 16-1693 + + Analytical, self-referential Storytelling, humanized Forum-like conversational The article identifies three primary causes for the fuel supply deficiency; firstly..., secondly..., thirdly.... This paper examines potential causes and provides diagnostic framework; this paper is intended to be guide. John, 30-year restoration veteran, replaced the pump, lines, and filter yet the Falcon still refuses to start. This can be real headache were gonna get that carb working again! Table 2: Steering effect of several representative features. Sign reflects if the feature positively or negatively correlated with intrinsic dimension prediction-based measures: after controlling for length, the two are largely uncorrelated, indicating that ID captures geometric/structural information not reflected in likelihood. Narrow operating band, far below ambient dimension. For well-formed texts, ID concentrates in narrow range (515) and is remarkably stable across domains, encoder families/sizes, and ID estimators. Yet ID cannot be substituted by linear dimensionality reduction: the manifold is highly curved, with an effective global linear embedding dimension of 60100 (e.g., peak PHDimEVk correlation near 60). Failure detection. ID reliably flags broken generations: extremely low values for repetitive/looped text and extremely high values for disconnected fragments or incoherent mixtures. Compression is bounding, not predictive. ID correlates moderately with gzip compression ratio, but the relationship is not pointwise predictive; compression provides bounds on plausible ID rather than accurate estimates. Linguistic correlates. ID increases with lexical diversity and decreases with cross-sentence repetition/overlap; it is comparatively insensitive to syntactic indicators for well-formed text. Style and genre effects. ID is strongly conditioned by style/genre. Scientific and purely informational prose exhibits lower ID, whereas personalized, opinionated, and fictional writing attains the highest ID. Adding semantic dimensions, such as personality, emotion, narrative/plot, stance, informality, raises ID. Implications and cautions. Prior work has used ID for training dynamics evaluation, synthetic text detection, scaling-law validation, architecture search, and memorization studies. However, treating ID as monolithic proxy for difficulty risks misinterpretation: domain shifts, spurious correlations, and evaluations confined to low-ID corpora (e.g., Wikipedia-like text) can yield misleading conclusions. Our findings indicate that complexitybased LLM analyses should explicitly target higherID domains (forums, fiction, opinionated writing) and report local (ID) linear structure, rather than relying solely on linear projections or predictionbased metrics."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we have analyzed intrinsic dimensionality as lens through which to interrogate the geometric structure of text representations in large language models. Our findings reveal that ID is not monolithic indicator of complexity, but rather domainand style-sensitive property: scientific and informational texts occupy low-ID regions, while opinionated, narrative, and personalized writing exhibit substantially higher ID, reflecting greater semantic degrees of freedom. Critically, ID is largely orthogonal to prediction-based measures like crossentropy, underscoring its value in exposing structural regularities that likelihood alone cannot capture."
        },
        {
            "title": "8 Limitations",
            "content": "Our PHD analysis uses embeddings from only three models (Gemma, RoBERTa, Qwen); results may vary with other encoders, as intrinsic dimension estimates are sensitive to embedding choice. Moreover, PHD is computed on random subsamples of the data, and due to this stochastic sampling procedure, PHD estimates may exhibit some variability across runs. Finally, common estimatorsTLE, MLE, TwoNN, and PHDrest on different assumptions about the geometry and distribution of token embeddings. Consequently, these estimators capture complementary but non-equivalent aspects of intrinsic dimensionality and are not directly com8 parable."
        },
        {
            "title": "References",
            "content": "Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 73197328, Online. Association for Computational Linguistics. Laurent Amsaleg, Oussama Chelly, Michael E. Houle, Ken ichi Kawarabayashi, Miloš Radovanovic, and Weeris Treeratanajaru. 2022. Intrinsic dimensionality estimation within tight localities: theoretarXiv preprint ical and experimental analysis. arXiv:2209.14475. Stefan Arnold. 2025. Memorization in language models through the lens of intrinsic dimension. In Proceedings of the First Workshop on Large Language Model Memorization (L2M2), pages 2328, Vienna, Austria. Association for Computational Linguistics. Rajendra Bhatia. 1997. Matrix Analysis, volume 1. Springer. Tolga Birdal, Aaron Lou, Leonidas Guibas, and Umut Simsekli. 2021. Intrinsic dimension, persistent homology and generalization in neural networks. Advances in neural information processing systems, 34:67766789. S. A. Crossley, K. Kyle, and M. Dascalu. 2019. The tool for the automatic analysis of cohesion 2.0: Integrating semantic similarity and text overlap. Behavioral Research Methods 51(1), pp. 14-27. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. 2024. RAID: shared benchmark for robust evaluation of machinegenerated text detectors. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12463 12492, Bangkok, Thailand. Association for Computational Linguistics. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of superposition. Preprint, arXiv:2209.10652. Kawin Ethayarajh. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5565, Hong Kong, China. Association for Computational Linguistics. Elena Facco, Maria dErrico, Alex Rodriguez, and Alessandro Laio. 2017. Estimating the intrinsic dimension of datasets by minimal neighborhood information. Scientific Reports, 7. Yanzhu Guo, Guokan Shang, and Chloé Clavel. 2025. Benchmarking linguistic diversity of large language models. Preprint, arXiv:2412.10271. Alex Havrilla and Wenjing Liao. 2024. Understanding scaling laws with statistical and approximation theory for transformer neural networks on intrinsically lowdimensional data. In Advances in Neural Information Processing Systems, volume 37, pages 4216242210. Xin He, Jiangchao Yao, Yuxin Wang, Zhenheng Tang, Ka Chun Cheung, Simon See, Bo Han, and Xiaowen Chu. 2023. Nas-lid: Efficient neural architecture search with local intrinsic dimension. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):78397847. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In ICLR. OpenReview.net. Laida Kushnareva, Tatiana Gaintseva, German Magai, Serguei Barannikov, Dmitry Abulkhanov, Kristian Kuznetsov, Eduard Tulchinskii, Irina Piontkovskaya, and Sergey Nikolenko. 2024. Aigenerated text boundary detection with roft. Preprint, arXiv:2311.08349. Kristian Kuznetsov, Laida Kushnareva, Anton Razzhigaev, Polina Druzhinina, Anastasia Voznyuk, Irina Piontkovskaya, Evgeny Burnaev, and Serguei Barannikov. 2025. Feature-level insights into artificial text detection with sparse autoencoders. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2572725748, Vienna, Austria. Association for Computational Linguistics. Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, Serguei Barannikov, Sergey Nikolenko, and Irina Piontkovskaya. 2024. Robust AI-generated text detection by restricted In Findings of the Association for embeddings. Computational Linguistics: EMNLP 2024, pages 1703617055, Miami, Florida, USA. Association for Computational Linguistics. Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, and Emily Cheng. 2025. Geometric signatures of compositionality across language models lifetime. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 52925320, Vienna, Austria. Association for Computational Linguistics. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018. Measuring the intrinsic dimension of objective landscapes. Preprint, arXiv:1804.08838. 9 Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. 2024. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 278300, Miami, Florida, US. Association for Computational Linguistics. Philip M. McCarthy. 2005. An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity (MTLD). Ph.D. thesis, The University of Memphis. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction to circuits. Distill, 5(3):e00024.001. Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. 2024. The shape of learning: Anisotropy and intrinsic dimensions in transformer-based models. In Findings of the Association for Computational Linguistics: EACL 2024, pages 868874, St. Julians, Malta. Association for Computational Linguistics. Olivier Roy and Martin Vetterli. 2007. The effective rank: measure of effective dimensionality. Benjamin Matthias Ruppik, Julius von Rohrscheidt, Carel van Niekerk, Michael Heck, Renato Vukovic, Shutong Feng, Hsien chin Lin, Nurul Lubis, Bastian Rieck, Marcus Zibrowius, and Milica Gašic. 2025. Less is more: Local intrinsic dimensions of contextual language models. Preprint, arXiv:2506.01034. Benjamin Schweinhart. 2021. Persistent homology and the upper box dimension. Discrete & Computational Geometry, 65(2):331364. Chantal Shaib, Yanai Elazar, Junyi Jessy Li, and Byron Wallace. 2024. Detection and measurement of syntactic templates in generated text. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 64166431, Miami, Florida, USA. Association for Computational Linguistics. Lee Sharkey, Hoagy Cunningham, Aidan Ewart, Logan Riggs, and Robert Huben. 2023. Sparse autoencoders find highly interpretable features in language models. Preprint, arXiv:2309.08600. Lucas Shen. 2022. Lexicalrichness: small module to compute textual lexical richness. Hermann Somers. 1966. Statistical methods in literary analysis. The computer and literary style, 128:140. Gemma Team. 2024. Gemma 2: language models at practical size. arXiv:2408.00118. Improving open Preprint, Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pages 15. Ieee. Hayato Tsukagoshi and Ryohei Sasano. 2025. Redundancy, isotropy, and intrinsic dimensionality of prompt-based text embeddings. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2591525930, Vienna, Austria. Association for Computational Linguistics. Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Sergey Nikolenko, Evgeny Burnaev, Serguei Barannikov, and Irina Piontkovskaya. 2023a. Intrinsic dimension estimation for robust detection of ai-generated texts. In Advances in Neural Information Processing Systems, volume 36, pages 3925739276. Curran Associates, Inc. Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Sergey Nikolenko, Evgeny Burnaev, Serguei Barannikov, and Irina Piontkovskaya. 2023b. Intrinsic dimension estimation for robust detection of AI-generated texts. In Advances in Neural Information Processing Systems. 37th NeurIPS (poster). Sowmya Vajjala and Ivana Luˇcic. 2018. OneStopEnglish corpus: new corpus for automatic readability assessment and text simplification. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 297304, New Orleans, Louisiana. Association for Computational Linguistics. Karthik Viswanathan, Yuri Gardinazzi, Giada Panerai, Alberto Cazzaniga, and Matteo Biagetti. 2025. The geometry of tokens in internal representations of large language models. Preprint, arXiv:2501.10573. Yuxia Wang, Artem Shelmanov, Jonibek Mansurov, Akim Tsvigun, Vladislav Mikhailov, Rui Xing, Zhuohan Xie, Jiahui Geng, Giovanni Puccetti, Ekaterina Artemova, Jinyan Su, Minh Ngoc Ta, Mervat Abassy, Kareem Ashraf Elozeiri, Saad El Dine Ahmed El Etter, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Nurkhan Laiyk, and 7 others. 2025. GenAI content detection task 1: English and multilingual machine-generated text detection: AI vs. human. In Proceedings of the 1stWorkshop on GenAI Content Detection (GenAIDetect), pages 244261, Abu Dhabi, UAE. International Conference on Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. 2024. Characterizing truthfulness in large language model generations with local intrinsic dimension. In ICML. Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Anna Vasileva, Anna Antipina, Tatyana Zaitseva, Alina Ermilova, Evgeny Burnaev, and Egor Shvetsov. 2025. From internal representations to text quality: geometric approach to llm evaluation. Preprint, arXiv:2509.25359."
        },
        {
            "title": "Dimension",
            "content": "A.1 Intrinsic dimension definitions We outline several common notions of intrinsic dimension used in data analysis, and illustrate them with uniformly distributed points in ddimensional Euclidean space. Throughout, denotes asymptotic proportionality. MLE (Maximum Likelihood Estimation dimension). For point consider the probability that random point falls in its ε-neighborhood. For uniform distribution, this probability scales as: Pr(x x2 < ε) C(x) εd. (1) Hence, the exponent in Eq. (1) defines the MLE dimension. Formally, we compute dimMLE = Ex (cid:20) lim ε0 log Nε(x) log 1/ε (cid:21) , (1a) where Nε(x) is the number of points in the εneighborhood of x. TwoNN (Two Nearest Neighbors dimension). For each point x, let r1 and r2 be the distances to its nearest and second-nearest neighbors, and define the ratio µ = r2/r1. For uniform distribution in Rd, the following holds: Pr(µ t) = 1 td, 1. (2) Thus, the distribution of µ depends only on d. We can estimate by fitting this model CDF to the empirical CDF ˆF (µ), for example by minimizing divergence: dimTwoNN = argmin DKL (cid:16) ˆF (µ) (1 µd) (cid:17) , (2a) where ˆF (µ) is the empirical cumulative distribution function of the ratios. PHDim (Persistent Homology Dimension). Let Nε be the number of nontrivial homological features (e.g., connected components or cycles) that appear in simplicial complex built from the data at scale ε. For set in Rd we get Nε ε dimPH. Thus, the PH dimension is defined as dimPH = lim sup ε log Nε log 1/ε . (3) (3a) An equivalent characterization uses the total length of the minimum spanning tree (MST). Let Ln be the MST length on points sampled uniformly from the set. Asymptotically, one has Ln d1 , which yields dimPH = lim 1 1 log Ln log . (3b) (3c) A.2 Three Arguments Against Functional Dependence Between Entropy and Intrinsic Dimension In prior work (Viswanathan et al., 2025), the authors present several mathematical arguments suggesting possible dependency between intrinsic dimension and entropy, although no formal proof is provided. In contrast, we offer three complementary arguments in the opposite direction. Each of them serves as constructive counterexample, demonstrating that no functional dependency between entropy and intrinsic dimensionality can exist in the general case. Of course, neural networks do not represent the fully general case: their embeddings and parameter spaces are restricted by training dynamics and architectural constraints. However, our empirical results support these theoretical considerations, indicating that even in practical settings the observed correlations arise empirically rather than by necessity. (1) Dependence Argument. Let = {ui}V i=1 Rm be the unembedding matrix and p(h) = softmax(U h) the models prediction for hidden state h. The prediction entropy is H(h) = (cid:88) pi(h) log pi(h), 11 which depends explicitly on . In contrast, the intrinsic dimension (ID) of set Rm depends only on the geometry of embeddingsit is property of their relative arrangement in Rm and does not depend on the unembedding matrix . Even within single model, one can select disjoint subsets of tokens whose projections under differ strongly, leading to distinct entropy distributions while the geometry of their embeddings (and thus ID) remains unchanged. Example. Assume that consists of two parts: dense cluster of positive tokens ui with nonnegative components, and single negative token u0 = (1, 1, . . . , 1). Consider set H+ of hidden vectors with positive coordinates. For such h, all scalar products h are positive, so softmax(U h) is approximately uniform, and the average entropy H(h) is close to log . If we flip the signs of all these vectors, obtaining = {h : H+}, the geometric structure and intrinsic dimension are identical, but now all scalar products (h) are negative, so softmax(U h) becomes nearly one-hot and H(h) 0. Thus, entropy can vary drastically while intrinsic dimension remains the same. (2) Scale Invariance Argument. Consider scaling map Tα(h) = αh. Since affine transformations preserve local dimensionality, (3) Continuity Argument. With fixed unembedding matrix, (token-level) entropy induces continuous function : Rm on the embedding space: it depends only on points location in the ambient space (hence on distances to the unembedding vectors), not on its neighbors along the data manifold. For measurable set Rm, write H(A) := A1(cid:82) H(h) dh. Let Md be compact ddimensional submanifold and Nε(Md) = {x : dist(x, Md) ε} its tubular ε-neighborhood, then we have: H(cid:0)Nε(Md)(cid:1) ε0 H(Md). Assume further that for any compact d-manifold Md the mean H(Md) depends only on (denote this value by µ(d)). compact D-manifold MD admit finite cover by such tubular sets. Then H(MD) = (cid:88) i=1 αi(ε) H(cid:0)Nεi(M(i) )(cid:1) = µ(d), where αi(ε) = Nεi(M(i) convex weights. )/(cid:12) (cid:12) (cid:83) )(cid:12) Nεj (M(j) (cid:12) are Conclusion. The mean entropy is continuous under infinitesimal thickenings of dimension: it cannot exhibit jumps across intrinsic dimensions and therefore cannot be functional of intrinsic dimension alone."
        },
        {
            "title": "B The connection between various ID",
            "content": "ID(Tα(H)) = ID(H)."
        },
        {
            "title": "Estimators",
            "content": "However, the corresponding prediction distribution transforms as p(Tα(h)) = softmax(U (αh)) = softmax(αU h), which is equivalent to inverse-temperature rescaling: α 0 1 1, α ek, and therefore H(Tα(h)) [0, log ]. Thus, entropy can be continuously adjusted from its maximum to minimum while intrinsic dimension remains invariant. Entropy and ID are therefore affinely independent quantities. B.1 MLE, TLE and TwoNN upon embeddings from main models Figs. 8, 9, and 10 illustrate pairwise relationships among intrinsic-dimension estimators computed on embeddings produced by the same model. MLE and TLE exibit very similar tracks but differ substantially from TwoNN. Notable, that PHD remains strongly correlated with all other estimators. Most ID estimators are monotonically related; for Qwen and Gemma embeddings, the relationships are nearly linear. RoBERTa stands out as an exception having substantially non-linear relationships. The TwoNN estimator on RoBERTa is non-monotonic with respect to the others. See Fig. 10 for details. B.2 PHD dependence on model size We wondered how the number of parameters in the embedder model affects the internal dimension of 12 Figure 8: ID Estimators Gemma the text. To ensure consistency, we used only Qwen 3 base models with sizes 0.6B, 1.7B, 4B, 8B, 14B, 32B. We calculated PHDim on human texts with length of more than 150 tokens and obtained the following results, see Tab. 3. We see general trend towards an increase in PHDim as the model size increases. Qwen3-4B stands out from this picture. This may be due to the fact that only this model has 128k context window and uses the Embeddings Tie. For smaller models, the context window is 32k, and Embedding Tie is used. The Embedding Tie is no longer used for models, but the context window is 128k.(Yang et al., 2025). Figure 11: Boxplots of PHD across Qwen3 model sizes 13 Figure 9: ID Estimators Qwen B.3 Intrinsic Dimensionality and data geometry In this section, we study the connection between ID and other geometric properties of data in the embedding space, related to dataset complexity. There are several methods to measure the diversity (anisotropy) of given dataset and here we will study their correlation with the intrinsic dimensionality. Suppose we have text and the set of its tokens embeddings = (x1, . . . , xN ), xi Rd. As σ1, σ2, . . . , σr (r = min(N, d)) we denote the singular values of the matrix of stacked embeddings X. Following (Yusupov et al., 2025), we explore the following metrics: Maximal Explained Variance (MEV) (Razzhigaev et al., 2024) the proportion of variance explained by the first principal component of the data (i.e., . Additionally, we calculate the proportion of variance, explored by the first 20 components ( σ2 1+...σ2 , 20 (cid:80)r i=1 σ2 we denote it as 20-EV); we properly explore the effect of the number of components further down in this section. σ2 1 i=1 σ2 (cid:80)r Resultant Length (Ethayarajh, 2019). Let be the unit-normalized token embeddings (i.e., = xi , = 1..n). Then, the Resultant xi2 14 Figure 10: ID Estimators RoBERTa (cid:80)N Length of is the length of the mean directional vector R(X) = 1 i2. For any text, 0 R(X) 1, where = 0 implies perfect isotropy and = 1 implies that all tokens are perfectly aligned and lay on the same line. i=1 Schatten-p Norm (Bhatia, 1997) quantifies the global spectral energy in the matrix X. It is defined as XSp = ((cid:80)r )1/p . In this section we study only the case of = 2. i=1 σp Effective Rank (Roy and Vetterli, 2007) estimates the effective dimensionality of the embedding space. It is proposed as an entropy15 based continuous approximation of matrix rank that is robust to minor perturbations in the data. Let pk = then Effective (cid:80)r Rank of is calculated as ERank(X) = exp ( (cid:80)r σk i=1 σi i=1 pk log pk) We calculate the Pearson correlation coefficient between these metrics and 4 intrinsic dimensionality estimators on human-written text data from COLING dataset. Figure 12 presents the results for embeddings obtained from RoBERTa-base, Gemma2-2B and Qwen2.5-1.5B models. We can see that different estimators of intrinsic dimensionality (PHD, TwoNN, MLE, and TLE) are highly correlated with each other; for Gemma2 and Model PHD Emb. Num. layers size (median) Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B 9.429 9.534 10.063 9.601 10.046 10.449 1024 2048 2560 4096 5120 5120 28 28 36 36 40 64 Table 3: Median PHD dependence on Qwen3 size Qwen2.5 models, there is also strong negative correlation between them and maximal explained variance. The overall picture for RoBERTa is slightly different from other two models, likely because it is an encoder model while Gemma and Qwen are decoders. From plots on the right half of the Figure 12 we can see that the negative correlation between intrinsic dimensionalities and cumulative explained variance grows with the number of components up to certain point and then start to decrease. In all cases, the weakest (i.e., closest to zero) correlation is for the TwoNN estimator, which is to be expected because it relies almost entirely on the local structure of the data while principal components are computed for the whole dataset. Linguistic, syntax and other text properties C.1 PHD dependence on text length (tokens) Fig. 13 shows the standard deviation of PHD as function of human text length. The variance is high for short texts and stabilizes beyond about 150 tokens. In addition, Fig. 14 shows large spread of PHD(Gemma) values for short texts. Figure 13: Standard deviation of PHD (Gemma) in 20token bins. Figure 14: Scatter plot of Text length in tokens and PHD(Gemma) C.2 Syntatic diversity We observe that the PHD metric captures little of the syntactic diversity present in texts as in Fig. 15. Correlation between PHD and syntax diversity and correlation between PHD and POS compression ratio is small and differs among generation LLM models as depicted by Table 6. In addition we see that correlation for different sub sources of text: such as arxiv, imbd is diffenrent in sign, but small as it is below forty percent as shown by Table 4. This is because PHD is largely insensitive to syntax in high-quality texts. Any apparent correlation with syntactic features is likely driven by noisy or low-quality outputs rather than genuine structural variation. Moreover, domain differences do not significantly affect the relationship between PHD and syntactic diversity or POS compression ratio for good texts. The primary drivers of PHD are, in fact, lexical and thematic diversity, not syntax. Figure 12: Correlation between Intrinsic Dimensionality (ID) and distribution metrics for human text embeddings (left), correlation between ID and cumulative explained variance (right). 17 Table 5: Model Group Performance Metrics Model Group CR_pos Syntax_Diversity GPT-4(o) LLAMA 1/3 Cohere GPT-3/3.5 Mixtral Human Bloom/BloomZ Gemma OPT GPT-J/NeoX Dolly GLM Flan-T5/T -0.206 0.201 -0.042 -0.044 -0.015 0.030 0.074 0.016 -0.135 -0.320 0.195 0.603 0.744 0.382 0.028 0.190 0.185 0.039 -0.101 0.061 0.124 0.170 0.062 -0.030 0.414 0.297 Table 6: Pearson correlation of OS compression ratio (CR_pos) and Syntax Diversity with phd dimension Model peerread cmv outfox eli5 wikipedia wp reddit hswag reddit_eli5 finance xsum sci_gen wikihow arxiv medicine tldr yelp roct squad cnn open_qa imdb wiki_csai dialogsum pubmed CR_pos 0.235 0.196 -0.055 0.187 -0.086 0.167 0.063 0.317 0.044 0.155 0.050 0.002 -0.060 0.036 -0.037 0.099 -0.004 0.223 0.139 0.239 0.162 -0.200 0.004 -0.223 -0.016 Syntax Diversity 0.247 0.185 -0.016 0.146 0.122 0.240 0.129 0.267 0.147 0.197 0.011 -0.054 0.124 0.089 -0.034 0.043 -0.074 0.038 0.091 0.355 0.025 -0.164 -0.012 0.291 -0. Table 4: Pearson correlation of CR_pos and Syntax Diversity with phd dimension by source of text Figure 16: Syntax diversity across text models (humanwritten and generated by different models) Figure 15: Correlation between syntatic diversity and PHD metric In terms of syntactic diversity, human texts are, on average, the most diverse among current families of LLM-generated texts  (Fig. 16)  , making syntax another factor that should be considered in detecting AI-generated content. Part-of-speech (POS) tagging compression ratios are relatively high for human-written texts  (Fig. 17)  , though not the highest among all model groups. However, human texts exhibit notably low variance in this metric, indicating consistent syntactic structure across samples. Figure 17: POS compression ratio across text models (human-written and generated by different models) Figure 18: POS Compression ration across text domains Moreover,we categorized the texts by source into three distinct groups based on their PHD values: Low-dimensional, Medium-dimensional, and Highdimensional groups. The Low-dimensional group, which we designate as \"Science & Tech\", comprises sources such as PubMed, arXiv, Medicine, SciGen, and Wiki -CSAI. This category represents technical and scientific literature from various specialized fields, including medical research and scientific publications. The Medium-dimensional group, labeled \"News & Info\", encompasses news and informational resources including SQuAD, DialogSum, Wikipedia, TLDR, CNN, PeerRead, and XSum. These sources consist primarily of factual reporting, encyclopedia entries, and summarized information. The High-dimensional group, categorized as \"Opinion & Forum\", contains sources such as Outfox, Reddit-ELI5, Reddit, Finance, WikiHow, Yelp, ELI5, IMDb, CMV, and WP. This group represents opinion-based content, including forum discussions, user reviews, crowd-sourced advice, and personal perspectives across various platforms. Across these three groups the syntatic diversity metric do not differ lot, as well as POS Compression ratio Fig. 18. This can be certainly attributed to generation model itself as distribution of syntax diversity across different domains are similar with insignificantly smaller values for \"Science&Tech\" domain as shown by Fig. 19. Figure 19: Syntax diversity across text domains C.3 More details on TAACO features and their relations with ID These are descriptions of the text metrics from TAACO, used on Figures 4 and 20: function_mattr (Moving Average TypeToken Ratio): metric calculates the lexical diversity of function words (articles, prepositions, conjunctions, auxiliary verbs, pronouns) using sliding window approach to reduce text-length sensitivity. Higher values may indicate varied syntactic structures or register shifts, while lower values suggest more formulaic or consistent grammatical patterns. lemma_ttr, bigram_lemma_ttr and trigram_lemma_ttr: calculate the typetoken ratio of word unigrams, bigrams or trigrams, using their lemmatized forms. Lower values may indicate repetitive words or phrase patterns while higher values suggest more varied lexical combinations. adjacent_overlap_2_cw_sent: measures the overlap of content words (nouns, verbs, adjectives, adverbs) between adjacent sentences, counting multiple occurrences of the same word. Higher values indicate stronger thematic continuity through repeated emphasis on the same content words, while lower values suggest more varied vocabulary between sentences, potentially indicating topic shifts, diverse expression, or weaker local coadjacent_overlap_2_noun_sent hesion. is very similar, except that only nouns are counted. adjacent_overlap_2_all_sent and adjacent_overlap_all_sent: measure overlap of all words between adjacent The first metric is counting sentences. multiple occurrences of repeated words, while the second one is using binary counting (word presence or absence). High values indicate strong repetition and tight cohesion between consecutive sentences, while low values suggest more varied vocabulary and potentially weaker local connections or topic transitions. repeated_content_lemmas: measures the proportion of content word lemmas (nouns, verbs, adjectives, and adverbs in their base forms) that appear more than once throughout the text. Higher values indicate greater lexical cohesion through repetition of meaningful words, while lower values indicate greater lexical diversity but potentially weaker thematic unity or cohesion across the text. all_logical: counts the frequency of logical connectives and discourse markers that express logical relationships in text, including words and phrases like therefore, thus, hence, consequently, if-then, because, and since. Higher values indicate more explicit logical structuring and argumentative discourse, while lower values may characterize more narrative or descriptive texts with implicit rather than explicit logical connections. sentence_linking: counts explicit sentencelinking connectives (e.g., moreover, furthermore, however, in addition). High values indicate explicit discourse structuring with clear transitions between ideas, typical of formal or academic writing, while low values suggest more implicit connections or informal, narrative styles. lexical_subordinators: counts subordinating conjunctions that introduce dependent clauses (e.g., although, because, while, unless). High values indicate complex syntactic structures with embedded clauses and sophisticated reasoning, while low values suggest simpler, more direct sentence constructions with coordinate rather than subordinate relationships. pronoun_density: measures the proportion of pronouns relative to total words in the text. High values indicate referential cohesion and informal, interactive discourse or narrative style, while low values suggest more explicit naming, formal register, or introductory text where entities are being established rather than referenced. Figure 20 and Table 7 show additional correlations between TAACO features and ID estimators upon various models. C.4 Text transformations and their effect on PHD We describe three text transformations and examine their effect on the persistent homology dimension across several embedding models. We consider the following text transformations: Type 1: each letter is independently replaced by homoglyph with probability 0.2; Type 2: each letter is replaced by homoglyph with probability 0.2, but replacements are applied per word type: if two words are identical before the transformation, they remain identical after; Type 3: the internal letters of each word are shuffled while keeping the first and last letters fixed; identical words remain identical after the transformation. Results for the PHD across transformation types and embedding models are given in Tab. 8. We observe two notable differences between RoBERTa and Gemma/Qwen. First, applying Transformation 3 has little to no effect on the median PHD for RoBERTa, but increases it for Gemma/Qwen. Second, applying Transformations 1 or 2 decreases PHD for RoBERTa, while 20 Figure 20: Top-10 features from TAACO with the strongest correlation with MLE, TLE, TwoNN, calculated upon Gemma embeddings, correspondingly. Correlations with ID, calculated upon other embeddings are given as well TAACO Feature PHD (Gemma) PHD (Qwen) PHD (RoBERTa) TAACO Feature PHD (Gemma) PHD (Qwen) PHD (RoBERTa) bigram_lemma_ttr function_mattr lemma_mattr trigram_lemma_ttr adjacent_overlap_2_cw_sent adjacent_overlap_2_all_sent noun_ttr repeated_content_lemmas content_ttr adjacent_overlap_cw_sent negative_logical adjacent_overlap_all_sent adjacent_overlap_2_noun_sent repeated_content_and_pronoun_lemmas adjacent_overlap_noun_sent adjacent_overlap_2_fw_sent adj_ttr adjacent_overlap_fw_sent lemma_ttr adjacent_overlap_2_argument_sent adjacent_overlap_2_noun_sent_div_seg adjacent_overlap_2_cw_sent_div_seg argument_ttr adjacent_overlap_noun_sent_div_seg adjacent_overlap_2_adj_sent 0.446 0.493 0.469 0.452 -0.404 -0.368 0.339 -0.357 0.330 -0.370 0.279 -0.358 -0.344 -0.274 -0.309 -0.310 0.284 -0.316 0.226 -0.257 -0.298 -0.280 0.182 -0.282 -0.276 0.530 0.503 0.479 0.534 -0.486 -0.466 0.433 -0.426 0.416 -0.434 0.235 -0.438 -0.436 -0.365 -0.387 -0.386 0.360 -0.379 0.314 -0.354 -0.351 -0.352 0.284 -0.328 -0.324 0.684 0.659 0.706 0.609 -0.600 -0.576 0.636 -0.613 0.629 -0.554 0.295 -0.545 -0.552 -0.565 -0.508 -0.437 0.481 -0.425 0.565 -0.493 -0.382 -0.376 0.522 -0.375 -0.382 adjacent_overlap_argument_sent adjacent_overlap_cw_sent_div_seg adjacent_overlap_2_verb_sent adjacent_overlap_binary_noun_sent adjacent_overlap_binary_2_noun_sent adjacent_overlap_adj_sent sentence_linking adjacent_overlap_2_argument_sent_div_seg adjacent_overlap_argument_sent_div_seg function_ttr adjacent_overlap_verb_sent all_temporal opposition determiners adjacent_overlap_binary_cw_sent adjacent_overlap_all_sent_div_seg adjacent_overlap_2_adj_sent_div_seg adjacent_overlap_binary_2_adj_sent verb_ttr adjacent_overlap_2_all_sent_div_seg adjacent_overlap_adj_sent_div_seg adjacent_overlap_binary_adj_sent adjacent_overlap_binary_2_cw_sent all_negative pronoun_density -0.227 -0.270 -0.233 -0.241 -0.239 -0.247 0.320 -0.227 -0.213 0.153 -0.219 0.274 0.279 -0.216 -0.199 -0.195 -0.223 -0.214 0.127 -0.173 -0.208 -0.202 -0.171 0.259 0.276 -0.306 -0.328 -0.297 -0.306 -0.316 -0.279 0.254 -0.295 -0.272 0.217 -0.262 0.253 0.232 -0.238 -0.277 -0.274 -0.263 -0.263 0.216 -0.264 -0.238 -0.239 -0.267 0.191 0. -0.447 -0.370 -0.363 -0.337 -0.327 -0.341 0.285 -0.336 -0.331 0.446 -0.333 0.272 0.287 -0.342 -0.284 -0.283 -0.264 -0.258 0.375 -0.259 -0.249 -0.247 -0.249 0.217 0.183 Table 7: Top-50 TAACO features: correlations with PHD across embedding models Gemma Qwen Roberta PHD PHD Type 1 PHD Type 2 PHD Type 3 8.69 9.47 8.90 11.17 8.11 9.83 8.87 10. 9.11 7.91 6.54 9.14 Table 8: Impact of text transformations on PHD for Gemma/Qwen these transformations have no effect or slightly increase it. This experiment shows that there are differences between RoBERTa and Gemma/Qwen, which are confirmed by Fig 2. C.5 Distribution of distances to the nearest neighbor for tokens To understand why the intrinsic dimensionality of text positively correlates with its intrinsic dimensionality, we decided to examine the distance between nearest neighbors. To do this, we divided tokens into two types: those with the same nearest neighbor as the token itself (Type I) and those without (Type II). It turned out that the nearest neighbor distances for the Type tokens are, on average, smaller than those for the Type II, see Fig. 22. This indirectly confirms the positive correlation above. decrease in the average nearest neighbor distance leads to decrease in the MST, which in turn may affect its asymptotic behavior. C.6 PHD and proficiency level of text Figure 25 shows the distributions of PHD, lemma_mattr, and repeated_content_lemmas for texts written for readers of different proficiency levels. We observe that the PHD of texts written for beginners is lower than that of texts written for intermediate and advanced readers. This can be explained by the lower typetoken ratio and higher lemma repetition across consecutive sentences in 21 beginner-level texts. C.7 PHD and generation temperature We generated 10,000 texts with Qwen-3-8B-base and another 10,000 with Qwen-3-8B-instruct, using 1,000 prompts randomly sampled from the RAID dataset. For each prompt, we generated texts at 10 different temperatures (ranging from 0.2 to 2) and measured their PHD using embeddings from Gemma. The results are shown in the figure. We observe that the PHD of texts generated by Qwen-3-8B-base increases rapidly as the temperature rises from 0.2 to 0.8, then gradually levels off. In contrast, for Qwen-3-8B-instruct, PHD grows more slowly and almost linearly with temperature. We hypothesize that this difference arises because Qwen-3-8B-base tends to produce many repetitions at low temperatures, while Qwen-3-8Binstruct is less prone to such artifacts and maintains consistent generation quality across all temperatures. The lower panels of the figure support this: low-temperature generations from the base model exhibit very low lexical diversity (measured via lemma_mattr) and high repetition rates (measured via repetitive_lemmas), whereas the instruct model does not show this issue. Figure 23 shows an example of an anomalous text produced by Qwen-3-8B-base and wellformed text produced by Qwen-3-8B-instruct using the same temperature and prompt. We observe that the base version of Qwen failed to follow the prompt properly and generated many repetitions. These observations show that anomalies in PHD distribution may serve as indicators of low-quality text generation. SAE-based experiments D.1 Linear approximation for PHD We built simple interpretable model using SAE features, see Tab. 1 for details, and lexical diversity metrics. We trained the model on human texts longer than 150 tokens to predict the PHD(Gemma). We calculated the Pearson and Spearman correlation coefficients for the models test predictions for various internal dimensionality estimates; see Tab. 9 for details. Also, to obtain information about the importance of features, we built linear regressions on the same features, each of which was trained to predict its own estimate of the internal dimension, calculated its importance for each feature, and calculated the rank in such way that the lower the rank, the more important the feature. Then, for each feature, we took the median of its ranks and got the following Tab. 9. Here MTLD (McCarthy, 2005) and Summer (Somers, 1966) are metrics of lexical diversity which are calculated using the Python library LexicalRichness (Shen, 2022). D.2 Steering examples Table 11 demonstrates the steering effect of the selected features by illustrative examples."
        },
        {
            "title": "E Speeding up the running time of the",
            "content": "PHDim calculation. The most expensive part of the calculation PHDim is calculation of pairwise distances. For long texts with length of more than 1024 tokens, this may take nearly week on Intel(R) Core(TM) i7pearson spearman PHD(RoBERTa) MLE(RoBERTa) TwoNN(RoBERTa) TLE(RoBERTa) PHD(Gemma) MLE(Gemma) TwoNN(Gemma) TLE(Gemma) PHD(Qwen) MLE(Qwen) TwoNN(Qwen) TLE(Qwen) 0.597 0.548 0.512 0.416 0.684 0.744 0.449 0.709 0.593 0.680 0.430 0. 0.643 0.587 0.556 0.440 0.702 0.771 0.450 0.749 0.589 0.682 0.402 0.710 Table 9: Pearson and Spearman correlations for linear model predictions on test for various estimates of the internal dimension. Layer Feature - - 16 16 25 16 16 16 16 Summer MTLD 1693 9868 14085 5228 14885 8104 2409 Median rank 1 2 3 3.5 6 7 8 9 Table 10: Median rank of importance of each feature for the linear approximation model PHD 22 Sign Shift Feature Default generation (excerpt): The internet is awash with advice for anyone struggling to return to education after long break. . . . The first step is to acknowledge the gap in your education and accept that its okay to feel nervous. Local colleges often offer adult courses to help you catch up. 24-15879 Example change (steered) BBC-style templated explainer Im not sure what Im doing with my life, she said the anecdotal lede introducing structured explainer: paragraph 1 (anecdote), paragraph 2 (context and stats), paragraph 3 (options for learners), paragraph 4 (call to action). UK survey found that 12% of adults had never attended formal school or college. Feeling lost? Youre not alone. 1. Assess your goals 2. Research options 3. Talk to someone. Its okay lets take it step by step and outline plan that fits your life. 16-8104 24-4610 + Social-science report Social-media explainer + Informal helper tone 16-6978 Default generation (excerpt): The awkward silence hung heavy in the air as Sarah and sat across from each other at the cafe. We were supposed to be discussing her art, but all could think about was the way her eyes lingered on me. 16-2433 topic-sentence Outline / scaffolding Analytical / detached commentary Emotional uncertainty; introspective tone Character-focused ness; narrative expansion vividThe story focuses on: context of the situation comparison with other cases. The conversation is analyzed through framework of social and cultural factors influencing interaction. Im trying to move on, but its hard still dont know if she likes me as friend or more. Emilys fingers trembled as she met Sarahs gaze, unsure if the warmth in her smile meant love or fear. 16-5159 1615275 + + Default generation (excerpt): Elias Thorne, brilliant but troubled young programmer, is caught in the middle of this clash of worlds. Hes man of contradictions, haunted by the ghosts of his own making. 16-5984 Academic analytical tone 16-14485 15275 16-5228 + + Expository / research-like narrative Richer plot and personal emotion Psychological depth / uncertainty Informal helper tone The novels narrative structure is based on competing architectural firms design histories within the 1980s urban landscape. The narrative emphasizes the citys socio-economic contrasts and their impact on collective memory. Our protagonist, Alex, struggling journalist, fights to reclaim his place in city that forgot his name. Anya must race against time, yet her own doubts and past failures threaten to destroy her resolve. Its okay lets take it step by step and outline plan that fits your life. + 16-6978 Default generation (excerpt): This paper explores the theoretical framework for unconventional superconductivity in strongly correlated systems, focusing on the interplay between real-space pairing and mean-field theory. 16-5984 16-5159 16-6978 + Analytical / formalized research tone Repetitive academic phrasing; self-referential Informal rewrite / conversational 16-9868 + Media / popular-science tone The study reveals the critical role of the fuel supply system, including the pump, lines, and pressure regulator, in performance metrics. This paper examines the causes of this problem and provides diagnostic framework; this paper is intended to be guide. Heres the abstract Ive kept it short and jargon-free so its easier to follow. The emergence of superconductivity in correlated systems has fascinated scientists for decades and the story is still unfolding. Table 11: Steering effect of selected features for four different prompts. Sign denotes positive or negative correlation of the feature with ID. 14700K processor for full COLING-dev dataset. Therefore, we rewrote the code for calculating the matrix of pairwise distances with CPU to GPU, which significantly accelerated the work with long texts. For shorter texts, this gives slight speedup, which is associated with long transfer of data between the CPU and GPU. The code is located here 0.f"
        },
        {
            "title": "F Computational resources",
            "content": "For our experiments we used servers with the following computational resources: 0https://anonymous.4open.science/status/TopAnDatFE46 2 NVIDIA GeForce RTX 4090 GPU + Intel(R) Core(TM) i7-14700K CPU 4 NVIDIA A100 80 Gb + Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz"
        },
        {
            "title": "G Artifact licenses",
            "content": "COLING dataset - The MIT License, RAID dataset - The MIT license, TAACO - Creative Commons AttributionNonCommercial-Sharealike license (4.0), OneStopEnglish - CC by-SA 4.0 23 Figure 22: Boxplots of nearest neighbor distances for different types of token Prompt: Write the body of plot summary for novel titled \"The Drawing of the Dark\". Do not give it title. Do not use tables. Do not use prologues. Do not use forewords. Qwen-3-8B-base Do not write in the first person. Do not use contractions. Do not use dialogue. Do not use italics. Do not use bold. Do not use hyperlinks. Do not use lists. Do not use footnotes. Do not use endnotes. Do not use references. Do not use citations. Do not use bibliographies. Do not use indexes. Do not use glossaries. Do not use appendices. Do not use prefaces. Do not use introductions. Do not use conclusions. Do not use summaries. Do not use abstracts. Do not use acknowledgments. Do not use dedications. Do not use epigraphs. Do not use epilogues. Do not use interludes. Do not use intermissions. Do not use transitions. Do not use flashbacks. Do not use flashforwards. Do not use dream sequences. Do not use fantasy Do not sequences. use illusions. Do not use prophecies. Do not use omens. Do not use Do not use portents. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use auguries. Do not use hallucinations. Do not use visions. Elara named discovers Qwen-3-8B-instruct In world where the line between reality and illusion is increasingly blurred, young art student mysterious sketchbook hidden in the attic of her late grandmothers house. The pages are filled with intricate drawings that seem to shift and change when viewed from different angles, revealing hidden symbols and cryptic messages. As Elara begins to study the drawings, she notices that they begin to manifest in the real worldobjects appearing in her room, shadows moving on their own, and whispers echoing through the halls of her home. Drawn deeper into the mystery, Elara learns that the sketchbook is relic from an ancient order of artists known as the Keepers of the Veil, who once protected the world from force known as the Dark. This force, sentient void that feeds on fear and uncertainty, was sealed away centuries ago by the Keepers, who used their art to bind it in place called the Hollow. But the seal is weakening, and the drawings are not just record of the pastthey are warning. As Elara delves further into the history of the Keepers, she uncovers prophecy that speaks of chosen artist who will either restore the balance or unleash the Dark upon the world. With the help of reclusive scholar named Kael, who has spent his life studying the Keepers lost works, Elara begins to understand the true nature of the drawings and the power they hold. She learns that each drawing is fragment of larger, unfinished masterpiece that can either reinforce the seal or break it entirely. ... ... Figure 23: Comparison of texts generated by Qwen-3-8B-base and Qwen-3-8B-instruct with the same prompt and temperature (0.2). 24 Figure 24: PHD, lemma_mattr and repeated_content_lemmas as functions of generation temperature 25: Figure repeated_content_lemmas for proficiency levels PHD, lemma_mattr and texts of various"
        }
    ],
    "affiliations": [
        "Interdata Astana",
        "Lomonosov Research Institute",
        "Moscow State University"
    ]
}