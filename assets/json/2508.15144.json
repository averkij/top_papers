{
    "paper_title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
    "authors": [
        "Jiabo Ye",
        "Xi Zhang",
        "Haiyang Xu",
        "Haowei Liu",
        "Junyang Wang",
        "Zhaoqing Zhu",
        "Ziwei Zheng",
        "Feiyu Gao",
        "Junjie Cao",
        "Zhengxi Lu",
        "Jitong Liao",
        "Qi Zheng",
        "Fei Huang",
        "Jingren Zhou",
        "Ming Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent."
        },
        {
            "title": "Start",
            "content": "Mobile-Agent-v3: Foundamental Agents for GUI Automation August 22, 2025 Jiabo Ye Xi Zhang Haiyang Xu Haowei Liu Ziwei Zheng Feiyu Gao Junjie Cao Junyang Wang Zhengxi Lu Zhaoqing Zhu Ming Yan Jitong Liao Qi Zheng Fei Huang Jingren Zhou Tongyi Lab , Alibaba Group {shuofeng.xhy, ym119608}@alibaba-inc.com https://github.com/X-PLUG/MobileAgent"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces GUI-Owl, foundational GUI agent model that achieves new state-of-theart performance among open-source end-to-end models across ten GUI benchmarks spanning both desktop and mobile environments, covering grounding, question answering, planning, decision-making, and general procedural knowledge in GUI automation scenarios. Notably, GUI-Owl-7B achieves score of 66.4 on the AndroidWorld benchmark and 29.4 on the OSWorld benchmark. Building on this model, we propose general-purpose GUI agent framework, Mobile-Agent-v3, which further enhances GUI-Owls performance (73.3 on AndroidWorld and 37.7 on OSWorld), achieving new state-of-the-art among GUI agent frameworks based on opensource models. GUI-Owl incorporates several key innovations: 1) Large-scale Environment Infrastructure: We introduce cloud-based virtual environment infrastructure spanning different operating systems (including Android, Ubuntu, macOS, and Windows). This underpins our Self-Evolving GUI Trajectory Production framework, which generates high-quality interaction data through sophisticated query generation and correctness judgment. The framework leverages GUI-Owls capabilities to continuously refine trajectories, creating self-reinforcing improvement cycle. It supports multiple downstream data pipelines, enabling robust data collection while reducing manual annotation needs. 2) Diverse Foundational Agents Capability Construction: by incorporating foundational UI datasuch as grounding, planning, and action semantic recognitionalongside diverse reasoning and reflecting patterns, GUI-Owl not only supports endto-end decision making but can also serve as specialized module integrated into multi-agent frameworks; 3) Scalable Environment RL: we also develop scalable reinforcement learning framework that enables fully asynchronous training and better aligns the models decision with real-world usage. In addition, we introduce Trajectory-aware Relative Policy Optimization (TRPO) for online environment RL, which achieves 34.9 on the OSWorld benchmark. GUI-Owl and Mobile-Agent-v3 are open-sourced at: https://github.com/X-PLUG/MobileAgent. 5 2 0 2 1 2 ] . [ 1 4 4 1 5 1 . 8 0 5 2 : r Figure 1: Performance overview on mainstream GUI-automation benchmarks. Equal contribution Corresponding author and project leader 1 Figure 2: Overview of our Mobile-Agent-v3. We illustrate our multi-platform environment supporting, our core capability, and some GUI automation examples generated by Mobile-Agent-v3."
        },
        {
            "title": "Introduction",
            "content": "Graphical User Interfaces (GUIs) Agents (Hu et al., 2024; Zhang et al., 2024a; Nguyen et al., 2024; Wang et al., 2024d; Gao et al., 2024; Wang et al., 2024a) is designed to automate daily and professional tasks based on human instructions across various device environments, thereby enhancing production efficiency. With the rapid advancement of multimodal large models and reasoning technologies, vision-based GUI agents have demonstrated strong task execution capabilities across various device environments, including PCs, mobile devices, and web platforms. The existing methods can be broadly divided into two categories. The first category builds agent frameworks based on closed-source models (Yang et al., 2025; Xie et al., 2025; Agashe et al., 2025; Song et al., 2025; Wang et al., 2024b), however, these approaches struggle to handle unfamiliar tasks and adapt to dynamic environments. The second category focuses mainly on end-to-end model performance (Qin et al., 2025; Wang et al., 2025a), but such methods often fail to follow instructions faithfully and lack compatibility with diverse agent frameworks, significantly limiting their practical utility. The GUI agents require this foundational model to have the following capabilities: 1) The strong UI perception capabilities (such as for Mobile, PC, and Web); 2) The planning, reflection, and reasoning in various dynamic environments; 3) The flexibility to integrate with various multi-agent frameworks. In this paper, we propose GUI-Owl, native end-to-end multimodal agent designed as foundational model for GUI automation. Built upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl unifies perception, grounding, reasoning, planning, and action execution within single policy network. The model achieves robust cross-platform interaction, handling multi-turn decision making with explicit intermediate reasoning, and supports both autonomous operation and role-specific deployment in multi-agent systems. To further enhance its adaptability, we develop specialized datasets for core foundational tasksincluding UI grounding, task planning, and action semanticsand employ scalable reinforcement learning framework to align GUI-Owls decision policy with real-world task success. Beyond single-agent deployment, GUI-Owl can be instantiated as different specialized agents within multi-agent framework Mobile-Agent-v3 where multiple role 2 agents coordinate and share partial observations and reasoning traces to tackle complex, long-horizon automation workflows. Large-scale Environment Infrastructure. To train our GUI agent, we developed comprehensive large-scale environment infrastructure for GUI interaction data collection. This infrastructure leverages cloud-based technologies, including cloud phones and cloud computers on Alibaba Cloud (Cloud, 2018), spanning mobile, PC, and web platforms. It creates dynamic virtual environments that enable diverse and realistic interaction scenarios across various operating systems and devices. Central to this infrastructure is our Self-Evolving GUI Trajectory Production pipeline. This innovative system collects high-quality trajectory data through sophisticated process involving: high-quality query generation that mimics real-world user interactions, model roll-outs where GUI-Owl and Mobile-Agent-v3 interacts with virtual environments, rigorous correctness judgment to ensure data quality, and query-specific guidance generation for challenging scenarios. This self-evolving pipeline creates continuous improvement cycle, enhancing both our dataset quality and the GUI-Owl models capabilities over time. By combining cloud technology with multi-platform environments, our infrastructure enables efficient, scalable model development while reducing manual annotation needs. Diverse Foundational Agents Capability Construction. Based on the generated trajectories, we introduce multiple downstream data construction pipelines to enhance the agents fundamental UI capabilities, including: (i) grounding pipeline that covers both UI element localizationbased on functional, appearance, and layout instructionsand fine-grained word/character grounding; (ii) task planning pipeline that distills procedural knowledge from historical successful trajectories and large-scale pretrained LLMs to handle long-horizon, multiapplication tasks; and (iii) an action semantics pipeline that captures the relationship between actions and resulting state transitions through before/after UI observations. Furthermore, we synthesize reasoning and reflecting data using offline hint-guided rejection sampling, distillation from multi-agent framework, and iterative online rejection sampling. This supervision enables the agent to perform independent reasoning and to engage in complex, longhorizon collaborative reasoning within the Mobile-Agent-v3 framework, adapting its reasoning style to the specific role it assumes. Scalable Environment RL. We also develop scalable training framework grounded in unified interface for multi-task training that standardizes interactions for both single-turn reasoning and multi-turn agentic tasks, and we decouple experience generation from policy updates to provide fine-grained control over policy adherence. This design supports fully asynchronous training and better aligns the models decision-making with real-world usage. We further introduce Trajectory-aware Relative Policy Optimization (TRPO) to address training with long, variable-length action sequences in online-environment RL. TRPO uses trajectory-level rewards to compute step-level advantages and employs replay buffer to improve the stability of reinforcement learning. We evaluate GUI-Owl across wide range of benchmarks that comprehensively measure native agent capability on GUI automation including grounding, single-step decision, general question answering and evaluate on online environment. GUI-Owl-7B outperforms all state-of-the-art models of comparable size. In particular, GUI-Owl7B achieves scores of 34.9 on OSWorld and 66.4 on AndroidWorld. Moreover, GUI-Owl-32B demonstrates outstanding performance, surpassing even proprietary models. On MMBench-GUI and AndroidControl, GUIOwl-32B outperforms all models, including GPT-4o and Claude 3.7. In grounding capability evaluations, GUIOwl-32B surpasses all models of the same size and achieves competitive performance compared with proprietary models. When combined with Mobile-Agent-v3, it achieves scores of 37.7 on OSWorld and 73.3 on AndroidWorld, which clearly demonstrates its capability as fundamental agent for GUI automation."
        },
        {
            "title": "2 GUI-Owl",
            "content": "GUI-Owl is an end-to-end multimodal model that unifies capabilities such as perception, planning, decisionmaking, and grounding within GUI scenarios. By leveraging extensive and diverse datasets for post-training based on Qwen2.5-VL, GUI-Owl is able to interact with graphical user interfaces on Mobile, PC, and Web platforms. We further apply reinforcement learning to GUI-Owl to align its capabilities with diverse downstream requirements. This alignment enables the model not only to autonomously perform multi-turn GUI interaction tasks, but also to generalize to specific applications such as question answering, captioning, planning, and grounding. Moreover, GUI-Owl can assume various roles within multi-agent framework, in which individual agents fulfill their respective responsibilities, coordinate their actions, and collaboratively accomplish more complex tasks. 2.1 End-to-end GUI interactions We model the interaction process between GUI-Owl and the device, as well as the completion of the specified task, as multi-turn decision-making process. Given the available action space = {a1, a2, . . . , aA} of the environment, the current environment observation St S, which can be screenshot in common, and the history 3 Figure 3: Illustration of the interaction flow of GUI-Owl. The system message defines the available action space, the user message contains the task instruction, compressed histories, and current observation, while the response message includes the agents reasoning, action summaries, and the final action output. of past operations Ht = {(S1, a1), (S2, a2), . . . , (St1, at1)}, the model selects an action from the action space and executes it in the environment to obtain the next time-steps observation St+1. Formally, at each time step t, at π( St, Ht), here, π denotes policy model (GUI-Owl), which maps the current observation and historical operations to probability distribution over the action space A. We present the interaction flow of GUI-Owl in Figure 3. In practice, we support flexible prompts to organize the action space into system messages. By default, we adopt the Qwen function calling format. Detail action space definition is presented in Table 9 and Table 10. For user messages, we sequentially provide the original task, historical information, and observations. To save GPU memory and improve inference speed, we typically retain only the most recent 1 to 3 images. Notably, requiring robust reasoning process before the actual output of an action decision can enhance the models ability to adapt to complex tasks and situations. Therefore, we require the model to first \"reasoning\" before making decision, and then execute the action based on this reasoning content. However, since lengthy thoughts over multiple turn interactions may cause the conversation history to become excessively long, we additionally require the model to output \"conclusion\" summarizing the key information of the current step. Finally, only the conclusion is stored in the historical context. The actions output by the model are translated into actual device operation commands (for example, we use ADB commands for Android devices, and pyautogui code for desktop operations). Meanwhile, the latest screenshot of the devices display is further captured and used as the observation of the environment. 2.2 Foundational Agents Capability GUI-Owl can function not only as native agent capable of independently interacting with GUIs, but also provides variety of foundational capabilities to support downstream standalone calls or integration into multi-agent framework. To this end, we collect and construct datasets for various capabilities such as grounding, caption and planning. These datasets are mixed with general instruction data during training, and we found that the model also possesses zero-shot GUI question-answering capability as well as general instruction-following abilities for unseen tasks. 2.2.1 Self-Evolving Trajectory Production Framework To scale up the trajectory data, we propose Self-Evolving GUI Trajectory Production pipeline, which contrasts with traditional methods that strongly rely on manual annotation (Wang et al., 2025a; Qin et al., 2025). This framework leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through roll-out and assessing their correctness to obtain large-scale high-quality interaction data. Subsequently, these data are Figure 4: Illustration of the our self-evolving trajectory data production pipeline. utilized to enhance the models capabilities, creating reinforcing cycle of improvement. As shown in Figure 4, the process begins with constructing dynamic virtual environments across mobile, PC, and web platforms, paired with high-quality query generation. Given these queries, the GUI-Owl model and Mobile-Agent-v3 framework performs step-by-step actions in the environments to produce roll-out trajectories. Trajectory Correctness Judgment Module then evaluates these trajectories at both the step and trajectory levels to identify and filter out errors. For difficult queries, Query-specific Guidance Generation module provides humanor model-generated ground-truth trajectories to guide the agent. The cleaned and enriched data is then used for reinforcement finetuning, enabling the model to iteratively improve its ability to generate successful GUI trajectories, thereby reducing human annotation needs and achieving continuous self-improvement. More details can be found in Section 5.4. High-Quality Query Generation. For mobile apps, we developed screenshot-action framework utilizing human-annotated Directed Acyclic Graph (DAG) (Patil et al., 2025) that models realistic navigation flows and captures multi-constraint user queries. The process involves path sampling, metadata extraction, instruction synthesis using LLMs, refinement through few-shot LLM prompting, and interface validation via web crawlers. This framework minimizes LLM hallucinations while ensuring realistic and controllable query generation. For computer applications, we addressed the challenges of atomic operational skills and software operational pathways. We combined manual annotation and LLM-assisted generation to create queries for atomic operations (e.g., clicking, scrolling, dragging) and complex software interactions. We utilized accessibility trees and deep-search chains to acquire operational pathways, and employed MLLMs to generate executable commands based on screenshots and exemplar inputs. This comprehensive approach ensures diverse, realistic, and accurate query generation across different GUI environments. Trajectory Correctness Judgment Module. Our Trajectory Correctness Judgment Module employs two-tiered system to evaluate the quality of generated GUI trajectories. It consists of Step-Level Critic and Trajectory-Level Critic. The Step-Level Critic analyzes individual actions within trajectory by examining pre-action and post-action states, producing an analysis, summary, and categorical annotation (GOOD, NEUTRAL, HARMFUL) for each step. The Trajectory-Level Critic assesses the overall trajectory using dual-channel approach: Textual Reasoning Channel leveraging large language models, and Multi-Modal Reasoning Channel incorporating both visual and textual data. The final correctness judgment is determined through consensus mechanism. This comprehensive approach ensures robust evaluation of GUI trajectories, combining granular step-level insights with holistic trajectory-level assessment to maintain high-quality training data for our GUI-Owl model. Query-specific Guidance Generation. This module utilizes successful trajectories to create guidance for improved model performance. The process involves: (1) Action Description: VLM generates descriptions for each actions outcome in reference trajectories. Inputs include preand post-action screenshots and action decisions. For coordinate-based actions, we highlight interaction points to aid (2) Quality Control: For model-generated trajectories, the VLM cross-references the VLM analysis. models decision rationale to validate step effectiveness, filtering out suboptimal actions. (3) Guidance Synthesis: Action descriptions are concatenated and fed into Large Language Model (LLM), which summarizes the essential steps required to complete the query, producing query-specific guidance. This approach enables the generation of targeted guidance, potentially improving the models ability to handle complex queries and reducing the need for extensive rollouts or manual annotations. 2.2.2 Diverse GUI Data Synthesis Figure 5: Overview of our grounding data construction pipeline. Grounding. Accurate localization and semantic understanding of graphical user interface elements are essential for the development of robust and reliable visual interface agents, with these capabilities primarily embodied in grounding. As illustrated in Figure 5, to improve grounding capabilities, we construct two types of grounding task datasets from multiple data sources. For UI element grounding (with function-based or appearance- & layout-based instruction), we collect data from three sources: 1) Open-source datasets: Publicly released GUI datasets are utilized from UI-Vision (Nayak et al., 2025), and GUI-R1 (Luo et al., 2025). 2) Grounding data synthesis using A11y tree: Extracting bounding boxes and functional descriptive information of UI elements through the accessibility (a11y) tree in both mobile and computer environments. And the appearance and layout descriptions are additionally generated using MLLMs such as Qwen2.5VL (Bai et al., 2025). 3) Dense grounding generation on crawled PC images: To tackle the scarcity of PC grounding data, diverse screenshots are crawled from Google Images using popular app names as keywords. Given the high density and visual complexity of UI components in PC screenshots, we employ SAM (Kirillov et al., 2023) to segment images into subregions, enabling more precise grounding. MLLMs then perform dense grounding within each segmented region. To reduce noise and further improve the quality, we clean the collected grounding annotations by comparing them with UI detection results from Omniparser V2 (Yu et al., 2025) (bounding boxes with IoU < τg are removed, where τg = 0.5). Additionally, we rephrase the original instructions into more natural, task-oriented descriptions using LLMs (e.g., Qwen2.5-Max (Team, 2024)). For fine-grained words and characters grounding, we collect set of document images and employ OCR tools to extract textual content and their corresponding spatial positions. Based on the annotated data, we build fine-grained text localization data to enable precise grounding of specific words and characters. Task Planning. As foundational agents are often used to accomplish long-horizon, complex tasks, the model needs to possess background knowledge of complex task planning. We construct such data from two perspectives: 1) Distilling from Historical Trajectories. Given historical successful trajectory data, we first construct fine-grained descriptions of each page transition. This information is then combined with the models 6 historical actions and organized into task execution manual through an LLM. By feeding this manual into GUI-Owl, we evaluate its quality based on changes in the task completion rate. 2) Distilling from Large-scale Pretrained LLM. To further enhance the models generalization on various tasks, we further distill knowledge from large-scale pretrained LLMs. First, we collect list which covering mainstream apps, and then use either human annotators or models to synthesize plausible tasks. These tasks are designed to be as complex as possible and to span multiple features and even multiple applications; task specifications with obvious errors are filtered out. We then feed these tasks to language model (e.g., Qwen3-235B) and further consolidate and clean the resulting plans, yielding task-specific planning data. Action Semantics. We notice that models ability to perceive how actions affect page-state changes is crucial. Based on collected trajectories, we extensively collect large corpus of preand post-action screenshots and construct two-tier dataset. At the first tier, we require the model to directly predict the intervening actionincluding its type and parametersbased on the before-and-after images; such data can be obtained directly from offline-collected trajectories. Subsequently, we ask the model to produce natural-language description that covers both the executed action and its effects. To construct annotations for this data, we design workflow that first generates an action description from the pre-action screenshot and the given action parameters (for coordinate-awared actions, the target location is drawn on the image to cue the model), using multimodal model (e.g., Qwen-VL-Max). We then use the same multimodal model with the before-and-after images to analyze page changes and assess whether the changes are semantically consistent with the action. Through multiple rounds of voting, we retain the higher-scoring action descriptions. 2.2.3 Enhanced Robust Reasoning Reasoning ability is essential for fundamental agent, as it enables the model to move beyond merely imitating action sequences and instead capture the underlying logic that governs them. To this end, we first propose set of diverse data synthesis strategies to enrich the variety of reasoning patterns. We then integrate reinforcement learning to further align these reasoning patterns with the dynamics of real-world environments. Offline Hint-Guided Rejection Sampling. We synthesize reasoning data via rejection sampling. Specifically, given collected trajectory = {(a0, S0), (a1, S1), . . . , (at, St)}, we prompt VLMs to generate reasoning content for each step based on its preceding history. The generated reasoning is then separated from the original context and used independently for action prediction. We evaluate the validity of each reasoning sequence by checking whether the predicted action matches the ground-truth action. To encourage diversity in reasoning patterns, we adopt hints of different styles, for instance, requiring the model to follow fixed chain-of-thought template or encouraging it to produce the simplest possible reasoning process. During this procedure, we observe that, for certain steps, the VLMs struggle to obtain actions consistent with the ground truth. For such cases, we first manually verify the correctness of the ground-truth action. If the action is deemed reasonable, we then feed it back to the VLMs to guide the generation of reasoning conclusions consistent with that action type. Distillation from Multi-Agent Framework. We note that even when style prompts are provided to encourage end-to-end reasoning generation, the model can still be influenced by certain inherent biases, which in turn limit reasoning diversity. In contrast, multi-agent framework decomposes single-step decision into the collaboration of multiple specialized roles, each approaching the current step from different perspective. Since each agent focuses exclusively on its own subtask, it can more effectively avoid such biases. Motivated by this observation, we collect the outputs of individual agents from the Mobile-Agent-v3, and employ large language model to integrate their diverse reasoning contents into unified end-to-end reasoning output. The resulting reasoning content is paired with the action sequences obtained from Mobile-Agent-v3, forming the training dataset for reasoning. Iterative Online Rejection Sampling. We observe that improving the base models reasoning capability also enhances its ability to accomplish wider range of tasks. Moreover, newly generated trajectory data can be further exploited for model training. Therefore, we adopt an iterative online rejection sampling framework, in which our model rolls out trajectories on query data under two modes: 1. End-to-end generation: the model directly generates reasoning and action predictions in an end-to-end fashion, which is used to improve its holistic reasoning capability. 2. Integration with Mobile-Agent-v3: GUI-Owl is incorporated into the Mobile-Agent-v3 framework. The inputs and outputs are collected to train the corresponding agent role models. 7 Formally, given query data and model (k) at iteration k, trajectories are generated as: (k) = Rollout(cid:0)M (k), Q(cid:1), (1) where (k) contains both end-to-end outputs (k) are then used to update the model: E2E and role-specific outputs (k) Role. The newly collected trajectories (k+1) = Train(cid:0)M (k), (k) filtered (cid:1). (2) Directly training on all collected steps often yields suboptimal model. To address this, we apply the following filtering and balancing strategies: 1. Critic-based filtering: Critic pipeline scores each step st (k), and those with scores below threshold τc are removed: Tfiltered = {st CriticScore(st) τc}. 2. Thoughtaction consistency check: We verify the logical consistency between the reasoning content (thought) and the executed action. Steps that fail this check are discarded. 3. Task re-weighting: Let psucc(task) denote the success rate of given task. For tasks with high psucc, we downsample their training occurrence, while for tasks with low psucc, we upsample their instances to ensure balanced learning. 4. Reflector balancing: We observe that the Reflector Agent predominantly produces positive outputs, leading to class imbalance. We recalibrate its data as follows: If the Reflector marks step as negative and this feedback causes step i+1 to be judged positive, the feedback for step is retained. Otherwise, we retain Reflector inputs and responses only from trajectories where all steps are judged positive by the reflactor. Finally, we re-balance the dataset so that positive and negative samples have equal size."
        },
        {
            "title": "3 Training Paradigm",
            "content": "GUI-Owl is initialized from Qwen2.5-VL and trained through three-stage process designed to progressively enhance its capabilities in GUI understanding, reasoning, and robust execution. Pre-training Phase: We collect large-scale pre-training corpus covering fundamental UI understanding, interaction trajectory data, and general reasoning data. This data is used to continually pre-train Qwen2.5-VL, strengthening its grounding in basic GUI element recognition, action prediction, and general reasoning, thereby establishing strong foundation for subsequent interaction-oriented training. Iterative Tuning Phase: We deploy the model in real-world environments such as desktops and mobile devices to perform large-scale task execution. The resulting trajectories are cleaned, scored, and further transformed into diverse reasoning datasets, which are then used for offline training. This iterative Tuning process enables GUI-Owl to accumulate effective reasoning patterns applicable across varied scenarios, improving its adaptability and decision-making in complex UI tasks. Reinforcement Learning Phase: We develop an asynchronous RL framework that allows the model to efficiently learn from direct interaction with real environments. This phase focuses on reinforcing successful behaviors and increasing execution consistency, thereby improving both the success rate and stability of GUI-Owl in practical deployments. 3.1 Scalable Reinforcement Learning 3.1.1 Infrastructure Where enriched trajectory and reasoning data expand the models knowledge base and reasoning capabilities, the model is expected to exhibit lower uncertainty and higher stability in real-world usage. Therefore, we further introduce reinforcement learning to better align GUI-Owl with practical application. To facilitate an efficient and flexible training framework for training with environment multi-turn interactions, we develop general infrastructure with the following key features: Unified Interface for multi-task training: Our framework is built on unified task plug-in interface that standardizes interactions for both single-turn reasoning and complex, multi-turn agentic tasks. This modular design allows diverse new environments and tasks to be seamlessly integrated without altering the core infrastructure. 8 Figure 6: Overview of our scalable RL infrastructure, which unifies single-turn reasoning and multi-turn agentic training in fully decoupled rolloutupdate framework. All components can run in parallel for high throughput, with diverse task-specific interactions plugged into the scalable experience maker with unified interface. rollout manager assigns task IDs, collects trajectories and rewards, and coordinates data flow via shared data center. Decoupled, Controllable Rollout: We decouple the experience generation (rollout) phase from policy updates, giving operators precise control over the entire data supply chain. This control is multi-faceted: the manager can dictate the degree of policy-adherence, from strictly synchronous on-policy mode to an asynchronous, slightly It also has full control over resource allocation, deploying rollouts on hardware off-policy mode for speed. optimized for inference to maximize throughput. This granular control enables us to fine-tune the data generation process to achieve an optimal balance among optimization guarantees, speed, and cost. 3.1.2 Task Mixture We apply GRPO (Guo et al., 2025) to train GUI-Owl on static tasks, and apply the trajectory-aware Relative Policy Optimization (TRPO) strategy for training in online environments. In this section, we present the data preparation methods for different downstream reinforcement learning tasks and introduce trajectory-aware relative policy optimization. For grounding task, subset of data from GUI-R1 (Luo et al., 2025), UI-Vision (Nayak et al., 2025) serves as the foundational dataset. To further enhance RL performance in challenging fine-grained grounding, curated collection of high-difficulty fine-grained grounding samples is incorporated (i.e., where the target UI regions occupy less than 0.1% of the entire screenshot area). Subsequently, the all datasets are performed ng sampling iterations (where ng = 8 in our implementation) utilizing the policy model GUI-Owl before RL, and sample instances that exhibit partial failure cases as training corpus for RL optimization. To enhance the capabilities of low-level (i.e., step-level) actions, we introduce single-turn reinforcement learning. The training data is derived directly from individual steps within high-quality offline interaction trajectories. While the preceding RL phases build foundational skills, applying them to complex, multi-step tasks in an online environment introduces significant challenges. Therefore we also conduct online reinforcement learning for GUI-Owl on virtual environments. These tasks are selected from the training task pool and use either rule-based or critic-based rewards as feedback signals for determining task completion. Trajectory-aware Relative Policy Optimization for Online Environment RL. Real-world user tasks are often In such scenarios, rewards are typically sparse characterized by long and variable-length action sequences. and only available as delayed success signal upon task completion. To overcome these obstacles, we employ trajectory-aware relative policy optimization strategy (TRPO) extended to GRPO (Guo et al., 2025). This approach circumvents the challenge of assigning per-step rewards, task that is nearly impossible to perform accurately in complex GUI interactions. Instead, we evaluate the entire trajectory τ after its completion. Our experiments are conducted on the OSWorld-Verified benchmark (Xie et al., 2024), where the outcome of each task is programmatically verifiable, allowing us to obtain reliable, single, holistic reward scalar R(τ ). Specifically, this reward is the sum of an accuracy reward (1 for successful trajectory, 0 otherwise) and format reward, which penalizes malformed actions with value of -0.5. This trajectory-level reward is then used to compute normalized advantage estimate, which provides stable learning signal across all steps of the trajectory. The advantage for given trajectory τ is calculated as: ˆAτ = R(τ ) σR+ϵ , where and σR are the running mean and standard deviation of rewards observed across multiple trajectories. This normalized advantage ˆAτ is then uniformly distributed to every action taken within that trajectory, ensuring that all steps contributing to successful outcome receive consistent positive signal, thereby mitigating the credit assignment problem. Given the inherent sparsity of successful trajectories in GUI tasks, we incorporate replay buffer to stabilize training. This buffer stores historically successful trajectories, indexed by their task_id. During the sampling process, if generated group of trajectories for task results entirely in failures, one failing trajectory is randomly replaced with successful one from the buffer corresponding to the same task. This injection of positive examples ensures the effectiveness of the training signal in every batch. Our final policy optimization objective for batch of trajectories is defined by the following loss function: LTRPO ="
        },
        {
            "title": "1\nN",
            "content": "G (cid:88) Si(cid:88) oi,s (cid:88) i=1 s=1 t= (cid:110) min (cid:104) rt(θ) ˆAτi, clip (rt(θ), 1 ϵ, 1 + ϵ) ˆAτi (cid:105)(cid:111) (3) where is the total number of tokens in the batch, ˆAτi is the trajectory-level advantage for trajectory i, and rt(θ) = πθ(os,t... ) is the probability ratio of token under the current and old policies. This clipped objective πθold (os,t... ) function stabilizes training while effectively leveraging the holistic trajectory-level reward signal for long-horizon GUI automation tasks. In practice, the high resolution of GUI screenshots means that complete interaction trajectory can quickly exceed the models context length (e.g., 32k for Qwen2.5-VL). To manage this, we segment each full multi-turn trajectory into several single-step data instances for the policy update. The loss computed for each step-wise instance is then scaled by the total number of steps in its original, complete trajectory. This approach addresses the issue of unbalanced optimization for trajectories of different lengths."
        },
        {
            "title": "4 Mobile-Agent-v3",
            "content": "The agent-based framework method modularizes complex GUI tasks into multiple relatively simple tasks, and can achieve higher performance with the cooperation of agents with different roles. (Zhang et al., 2025a; Li et al., 2024; Wang et al., 2024b; 2025b; Agashe et al., 2025; 2024; Zhang et al., 2025b; Li et al., 2025; Liu et al., 2024; Nong et al., 2024; Wu et al., 2024a;b; Sun et al., 2024; Zhang et al., 2024b; Zheng et al., 2024; Patel et al., 2024; Niu et al., 2024; Tan et al., 2024). As noted earlier, GUI-Owl possesses multi-agent collaboration capabilities. Building upon this foundation, we further propose Mobile-Agent-v3, multi-agent framework endowed with capabilities for knowledge evolution, task planning, sub-task execution, and reflective reasoning. In this section, we discuss the architecture of Mobile-Agent-v3. As presented in Figure 7, the Mobile-Agent-v3 framework coordinates four specialized agents to achieve robust, adaptive, and long-horizon GUI task automation: Manager Agent (M): Serves as the strategic planner. At initialization, it decomposes high-level instruction into an ordered subgoal list SS0 using external knowledge KRAG. During execution, it updates the plan based on results and feedback, re-prioritizing, modifying, or inserting corrective subgoals. Worker Agent (W): Acts as the tactical executor. It selects and performs the most relevant actionable subgoal from SSt given the current GUI state St, prior feedback Ft1, and accumulated notes Nt, producing an action tuple At that records reasoning, action, and intent. Reflector Agent (R): Functions as the self-correction mechanism. It compares the intended outcome from the Worker with the actual state transition (St St+1), classifying the result as SUCCESS or FAILURE and generating detailed causal feedback ϕt for the Manager. Notetaker Agent (C): Maintains persistent contextual memory. Triggered only on SUCCESS, it extracts and stores critical screen elements (e.g., codes, credentials) as notes Nt. The cumulative memory Nt+1 supports both planning and execution in future steps. The Manager decomposes and dynamically updates the plan, the Worker executes selected subgoals, the Reflector evaluates outcomes and provides diagnostic feedback, and the Notetaker preserves valuable transient information. This loop continues until all subgoals are completed or the instruction is fulfilled. More details can be found in Section 7. 10 Figure 7: Mobile-Agent-v3 architecture. The system consists of six modules: (1) RAG module for retrieving external world knowledge, (2) Manager Agent for subgoal planning and guidance, (3) Worker Agent for GUI operation, (4) Reflector Agent for evaluation and feedback (5) Notetaker Agent for recording important note, and (6) GUI device interface supporting phone and PC environments."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Model Evaluation In this section, we evaluate GUI-Owl on wide range of benchmarks to thoroughly assess its performance as fundamental agent in GUI-based scenarios. We train GUI-Owl-7B and GUI-Owl-32B, which are initialized from the Qwen2.5-VL models of the corresponding sizes. We conduct extensive experiments to evaluate GUI-Owl in four key dimensions: grounding capability, comprehensive GUI understanding, end-to-end agent capability, and multi-agent capability. 5.1.1 grounding capability The grounding capability evaluates models ability to locate the corresponding UI element given naturallanguage query. We use ScreenSpot V2, ScreenSpot Pro, OSWorld-G, and MMBench-GUI L2 as benchmarks. ScreenSpot v2 covers mobile, desktop, and web scenarios, while ScreenSpot-Pro primarily evaluates models localization ability at ultra-high resolutions. OSWorld-G contains finely annotated queries. MMBench-GUI L2 has the broadest coverage and more faithfully reflects models grounding performance in real-world settings. The performance comparisons are shown in Table 1, Table 2, Table 3 and Table 4. GUI-Owl-7B achieves state-of-the-art performance among all 7B models. On screenspot-pro, which focuses on high-resolution images, we achieve score of 54.9, significantly exceeding the performance of UI-TARS-72B and Qwen2.5-VL 72B. GUI-Owl-7B also achieves competitive performance on OSWorld-G compared to UITARS-72B. GUI-Owl-32B surpasses all models of the same size. MMBench-GUI-L2 evaluates very broad and challenging set of queries, where our model scores 80.49, substantially outperforming all existing models. GUIOwl-32B further achieves performance level of 82.97 and demonstrates leading grounding capabilities across various domains. 5.1.2 Comprehensive GUI Understanding Comprehensive GUI Understanding examines whether GUI model can accurately interpret interface states and produce appropriate responses. We adopt two benchmarks for this evaluation. MMBench-GUI-L1 assesses the models UI understanding and single-step decision-making capability through question-answering format. Android Control evaluates the models ability to perform single-step decisions within pre-annotated trajectory contexts. 11 Agent Model Text Icon Text Icon Text Icon Overall Mobile Desktop Web Proprietary Models Operator (OpenAI, 2025a) Claude 3.7 Sonnet (Anthropic, 2025a) UI-TARS-1.5 (Qin et al., 2025) Seed-1.5-VL (Team, 2025) Open-Source Models SeeClick (Cheng et al., 2024) OmniParser-v2 (Yu et al., 2025) Qwen2.5-VL-3B (Bai et al., 2025) UI-TARS-2B (Qin et al., 2025) OS-Atlas-Base-4B (Wu et al., 2024b) OS-Atlas-Base-7B (Wu et al., 2024b) JEDI-3B (Xie et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) UI-TARS-72B (Qin et al., 2025) UI-TARS-7B (Qin et al., 2025) JEDI-7B (Xie et al., 2025) GUI-Owl-7B GUI-Owl-32B 47.3 - - - 78.4 95.5 93.4 95.2 95.2 96.2 96.6 97.6 94.8 96.9 96. 99.0 98.6 41.5 - - - 50.7 74.6 73.5 79.1 75.8 83.4 81.5 87.2 86.3 89.1 87.2 92.4 90.0 90.2 - - - 70.1 92.3 88.1 90.7 90.7 89.7 96.9 90.2 91.2 95.4 95. 96.9 97.9 80.3 - - - 29.3 60.9 58.6 68.6 63.6 69.3 78.6 74.2 87.9 85.0 87.9 85.0 87.8 92.8 - - - 55.2 88.0 88.0 87.2 90.6 94.0 88.5 93.2 91.5 93.6 94. 93.6 94.4 84.3 - - - 32.5 59.6 71.4 78.3 77.3 79.8 83.7 81.3 87.7 85.2 84.2 85.2 86.7 70.5 87.6 94.2 95.2 55.1 80.7 80.9 84.7 85.1 87.1 88.6 88.8 90.3 91.6 91. 92.8 93.2 Table 1: Comparison with state-of-the-art methods on the ScreenSpot-V2 dataset. Underlined denotes the secondbest open-source performance. Agent Model Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Development Creative CAD Scientific Office OS Avg Proprietary Models GPT-4o (Hurst et al., 2024) Claude 3.7 Sonnet (Anthropic, 2025a) Operator (OpenAI, 2025a) Seed-1.5-VL (Team, 2025) UI-TARS-1.5 (Qin et al., 2025) Open-Source Models UI-TARS-2B (Qin et al., 2025) Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) UI-R1-E-3B (Lu et al., 2025) UI-TARS-7B (Qin et al., 2025) InfiGUI-R1-3B (Liu et al., 2025) JEDI-3B (Xie et al., 2025) GUI-G1-3B (Zhou et al., 2025) UI-TARS-72B (Qin et al., 2025) JEDI-7B (Xie et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) SE-GUI-7B (Yuan et al., 2025) GUI-G2-7B (Tang et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) GUI-Owl-7B GUI-Owl-32B 1.3 - 50.0 - - 47.4 38.3 51.9 46.1 58.4 51.3 61.0 50.7 63.0 42.9 74.0 68.2 68.8 - 76.6 84.4 0.0 - 19.3 - - 4.1 3.4 4.8 6.9 12.4 12.4 13.8 10.3 17.3 11.0 21.4 19.3 17.2 - 31.0 39.3 1.0 - 51.5 - - 42.9 40.9 36.9 41.9 50.0 44.9 53.5 36.6 57.1 50.0 61.1 57.6 57.1 - 59.6 65.2 0.0 - 23.1 - - 6.3 4.9 8.4 4.2 9.1 7.0 8.4 11.9 15.4 11.9 13.3 9.1 15.4 - 27.3 18.2 2.0 - 16.8 - - 17.8 22.3 17.8 37.1 20.8 33.0 27.4 39.6 18.8 38.0 38.1 51.3 55.8 - 64.5 62.4 0.0 - 14.1 - - 4.7 6.3 1.6 12.5 9.4 14.1 9.4 9.4 12.5 14.1 15.6 42.2 12.5 - 21.9 28.1 2.1 - 58.3 - - 56.9 44.4 48.6 56.9 63.9 58.3 54.2 61.8 64.6 72.9 78.5 75.0 77.1 - 79.1 82.6 0.0 - 24.5 - - 17.3 10.0 8.2 21.8 31.8 20.0 18.2 30.0 20.9 25.5 29.1 28.2 24.5 - 37.3 39.1 1.1 - 60.5 - - 50.3 48.0 53.7 65.0 63.3 65.5 64.4 67.2 63.3 75.1 76.3 78.5 74.0 - 77.4 81.4 0.0 - 28.3 - - 17.0 17.0 18.9 26.4 20.8 28.3 32.1 32.1 26.4 47.2 37.7 43.4 32.7 - 39.6 39.6 0.0 - 34.6 - - 21.5 33.6 34.6 32.7 30.8 43.9 38.3 23.5 42.1 33.6 55.1 49.5 57.9 - 59.8 70.1 0.0 - 30.3 - - 5.6 4.5 7.9 10.1 16.9 12.4 9.0 10.6 15.7 16.9 27.0 25.8 21.3 - 33.7 36.0 0.8 27.7 36.6 60.9 61.6 27.7 25.9 27.6 33.5 35.7 35.7 36.1 37.1 38.1 39.5 47.6 47.3 47.5 53.3 54.9 58.0 Table 2: Comparison with state-of-the-art methods on the ScreenSpot-Pro dataset. Underlined denotes the secondbest open-source performance. On the MMBench-GUI-L1 benchmark, GUI-Owl scores 84.5, 86.9, and 90.9 on the easy, medium, and hard levels, respectively, substantially outperforming all existing models. On Android Control, it achieves score of 72.8, establishing the highest performance among all 7B models. We find that GUI-Owl-32B achieves score of 76.6, surpassing the current state-of-the-art UI-TARS-72B. GUI-Owl-32B significantly outperforms GUI-Owl-7B across different difficulty levels and domains, reflecting its more comprehensive and sufficient reserve of GUI knowledge. 12 Agent Model Proprietary Models Gemini-2.5-Pro (Deepmind, 2025b) Operator (OpenAI, 2025a) Seed1.5-VL (Team, 2025) Open-Source Models OS-Atlas-7B (Wu et al., 2024b) UGround-V1-7B (Gou et al., 2024) Aguvis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) JEDI-3B (Xie et al., 2025) JEDI-7B (Xie et al., 2025) GUI-Owl-7B GUI-Owl-32B Text Matching Element Recog. Layout Underst. Fine-grained Manip. Overall 59.8 51.3 73.9 44.1 51.3 55.9 60.2 69.4 41.4 45.6 63.2 67.4 65.9 64.8 67. 45.5 42.4 66.7 29.4 40.3 41.2 51.8 60.6 28.8 32.7 47.3 53.0 55.5 63.6 64.5 49.0 46.6 69.6 35.2 43.5 43.9 54.9 62.9 34.8 41.9 49.0 53.8 57.7 61.3 67. 33.6 31.5 47.0 16.8 24.8 28.2 35.6 45.6 13.4 18.1 36.9 44.3 46.9 41.0 45.6 45.2 40.6 62.9 27.7 36.4 38.7 47.5 57.1 27.3 31.4 46.5 50.9 54.1 55.9 58. Table 3: Comparison with state-of-the-art methods on the OSWorld-G dataset. Underlined denotes the second-best open-source performance. Model GPT-4o (Hurst et al., 2024) Claude-3.7 (Anthropic, 2025a) Qwen-Max-VL (Bai et al., 2025) Aguvis-7B-720P (Xu et al., 2024) ShowUI-2B (Lin et al., 2025) OS-Atlas-Base-7B (Wu et al., 2024b) UGround-V1-7B (Gou et al., 2024) InternVL3-72B (Zhu et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) UI-TARS-72B-DPO (Qin et al., 2025) GUI-Owl-7B GUI-Owl-32B Windows MacOS Linux iOS Android Web Basic 1.48 1.48 43.91 37.27 9.23 36.90 66.79 70.11 55.72 31.37 68.27 78. 86.35 85.61 Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. 1.10 0.74 36.76 21.69 4.41 18.75 38.97 42.64 33.82 16.54 38.97 51.84 61.76 65.07 8.69 12.46 58.84 48.12 24.06 44.35 71.30 75.65 49.86 31.30 68.99 80.29 81.74 84.93 4.34 7.51 56.07 33.27 10.40 21.68 48.55 52.31 30.06 21.97 44.51 62. 64.45 67.05 1.05 1.05 53.93 33.51 25.13 31.41 56.54 59.16 40.31 21.47 64.40 68.59 74.35 76.96 1.02 0.00 30.10 25.00 11.73 13.27 31.12 41.33 20.92 12.24 37.76 51.53 61.73 63.27 5.10 13.69 77.39 67.52 28.98 74.84 92.68 93.63 56.05 66.56 88.54 90. 94.90 95.22 3.33 10.61 59.09 65.15 19.70 48.79 70.91 80.61 28.18 55.15 69.39 81.21 83.03 85.45 2.53 1.40 79.49 60.96 17.42 69.60 93.54 92.70 55.62 35.11 90.45 92.98 95.78 96.07 1.41 1.40 70.14 50.99 8.73 46.76 70.99 78.59 25.35 35.21 69.29 80. 83.66 87.04 3.23 3.23 74.84 61.61 22.90 61.29 88.71 90.65 68.39 40.32 80.97 88.06 93.22 95.48 2.92 2.27 58.77 45.45 12.66 35.39 64.61 65.91 45.78 32.47 56.49 68.51 72.72 80.84 Overall 2.87 4.66 58.03 45.66 15.96 41.42 65.68 72.20 41.83 33.85 64.32 74.25 80.49 82.97 Table 4: Comparison with state-of-the-art methods on the MMBench-GUI-L2 dataset. Underlined denotes the second-best open-source performance. 5.1.3 End2end and Multi-Agent capability on Online environment While the aforementioned evaluations measure models performance in single-step decision-making, they suffer from two main limitations: (1) Errors in individual steps do not accumulate, making it impossible to assess the ability to accomplish complete tasks; (2) Although there may be multiple valid ways to complete task, the ground-truth step sequences may reflect specific preferences, which can result in an unfair evaluation across models. To more comprehensively evaluate both the end-to-end agent capability and the multi-agent capability, we adopt realistic interactive environments AndroidWorld and OSWorld. GUI-Owl-7B outperforms UITARS 1.5 on AndroidWorld, and Mobile-Agent-v3 achieves an even greater lead, significantly surpassing all existing models. On OSWorld, GUI-Owl also outperforms the open-source OpenCUA7B. We further adopt GUI-Owl-32B into Mobile-Agent-v3, it achieves 37.7 on OSWorld-Verified and 73.3 on AndroidWorld. This suggests that GUI-Owl is not only capable of independently solving tasks, but is also well-suited for integration into multi-agent framework. 5.2 Trajectory-level Online Reinforcement Learning To validate the efficacy of our trajectory-level online reinforcement learning strategy, we conducted series of experiments on the OSWorld-Verified (Xie et al., 2024) benchmark, with all tasks limited to maximum of 15 steps. The results, illustrated in Figure 8, demonstrate the clear advantages of our proposed approach. Starting from an initial checkpoint with 27.1 success rate, our method shows consistent, stable improvement throughout training, ultimately achieving peak success rate of over 34.9. This steady learning curve underscores the effectiveness of our trajectory-aware relative policy optimization. By calculating single, normalized advantage estimate ˆAτ for an entire trajectory, our method successfully mitigates the severe credit assignment problem inherent in long-horizon GUI tasks and provides coherent learning signal. 13 Model Windows MacOS Linux iOS Android Web Overall GPT-4o (Hurst et al., 2024) Claude-3.5 (Anthropic, 2024) Qwen2.5-VL-72B (Bai et al., 2025) UI-TARS-72B-DPO (Qin et al., 2025) InternVL3-72B (Zhu et al., 2025) GUI-Owl-7B GUI-Owl-32B GPT-4o (Hurst et al., 2024) Claude-3.5 (Anthropic, 2025a) Qwen2.5-VL-72B (Bai et al., 2025) UI-TARS-72B-DPO (Qin et al., 2025) InternVL3-72B (Zhu et al., 2025) GUI-Owl-7B GUI-Owl-32B GPT-4o (Hurst et al., 2024) Claude-3.5 (Anthropic, 2025a) Qwen2.5-VL-72B (Bai et al., 2025) UI-TARS-72B-DPO (Qin et al., 2025) InternVL3-72B (Zhu et al., 2025) GUI-Owl-7B GUI-Owl-32B 62.47 41.34 65.86 41.59 74.67 82.96 93.70 Easy Level 67.89 50.04 75.23 28.52 78.72 84.52 89.29 Medium Level 56.33 39.28 66.29 38.83 71.46 63.13 47.63 72.73 41.60 78.58 88.89 94.07 60.69 37.40 70.68 31.48 75.08 87.78 93.33 88.10 84. Hard Level 60.38 42.70 68.91 35.87 77.44 96.43 95.24 62.38 41.61 73.02 35.16 79.16 85.57 93.30 59.70 45.97 72.63 37.14 79.88 91.24 95. 52.42 34.07 70.98 24.19 76.19 94.33 95.88 58.52 42.03 67.24 31.08 83.57 82.61 95.65 54.06 44.57 59.27 41.72 78.43 84.35 87. 45.27 40.86 57.59 36.33 70.37 87.83 92.17 56.41 38.96 58.09 52.25 80.10 83.28 90.49 57.69 42.03 66.24 54.74 81.36 85.25 92. 50.93 36.96 53.94 58.13 75.73 88.85 95.41 58.51 41.79 72.08 35.33 81.18 88.13 94.06 54.98 34.33 68.24 31.55 78.67 83.56 88. 50.83 38.11 68.10 19.94 78.11 94.06 92.69 60.16 41.54 66.98 40.18 79.15 84.50 92.75 57.24 41.26 67.45 41.77 77.89 86.86 91. 53.49 37.55 64.56 35.78 75.70 90.90 94.19 Table 5: Comparison with state-of-the-art methods on the MMBench-GUI-L1 dataset. Underlined denotes the second-best open-source performance. Model Claude-3.5 (Anthropic, 2024) GPT-4o (Hurst et al., 2024) Gemini 2.0 (Deepmind, 2025a) Qwen2-VL-72B (Wang et al., 2024c) Aguvis-72B (Xu et al., 2024) Qwen2.5-VL-72B (Bai et al., 2025) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) GUI-Owl-7B GUI-Owl-32B Score 12.5 20.8 28.5 59.1 66.4 67.4 72.5 74. 72.8 76.6 Table 6: Model performance on the AnExtract match droid Control benchmark. scores with high-level instruction are reported. Underlined denotes the second-best open-source performance. Agent Model OSWorld-Verified AndroidWorld Online Proprietary Models SeedVL-1.5 (Team, 2025) Claude-4-sonnet (Anthropic, 2025b) OpenAI CUA o3 (OpenAI, 2025b) UI-TARS-1.5 (Qin et al., 2025) Open-Source Models UI-TARS-72B-DPO (Qin et al., 2025) OpenCUA-7B (Wang et al., 2025a) OpenCUA-32B (Wang et al., 2025a) UI-TARS1.5-7B (Qin et al., 2025) GUI-Owl-7B Mobile-Agent-v3 34.1 43.9 23.0 - 24.0 28.2 34.8 27.4 34.9* 37.7 62.1 - - 64. 46.6 - - - 66.4 73.3 Table 7: Online evaluation results on OSWorld-Verified and AndroidWorld benchmarks. Underlined denotes the secondbest open-source performance. *A variant of GUI-Owl specifically RL-tuned for desktop environment (Section 5.2). The general version of GUI-Owl achieves score of 29.4. The most critical insight comes from the ablation study, Online Filtering (DAPO). This variant, which disables our successful-trajectory replay buffer and the mechanism for carrying over unused rollouts, confirms the value of our specific design choices. While this model still shows positive learning trend, its performance is notably more volatile and ultimately inferior, peaking at around 31.5% before declining. This instability highlights the challenge of sparse positive feedback; without the replay buffer injecting successful examples, the agent struggles to learn from the vast space of failing trajectories. The final performance gap between our full model and this ablation underscores the importance of data efficiency. By retaining and reusing all generated rollouts, our full method maximizes the utility of costly interactions, providing richer training signal that leads to more stable and superior final performance. Our comparison with the Offline Filtering (GRPO) baseline further justifies our online data selection methodology. Offline filtering is very common technique for preparing RL data by removing tasks that are statically identified as all-successful or all-failing across multiple inference runs. However, the results show this approach is not suitable for GUI automation tasks that require long-range, multi-step planning. After an initial small gain, its performance stagnates around 29.1 success rate before degrading significantly. The failure arises because the 14 Figure 8: Training dynamics of GUI-Owl-7B on OSWorld-Verified. We limit the maximum interaction steps to 15 by default. Offline Filtering removes tasks with all-success or all-failure outcomes before applying vanilla GRPO, serving as common preprocessing. Online Filtering moves all tasks to online training and applies DAPO for selective filtering. Experience Managing activates both the replay buffer and the use of leftover rollouts after batch filling, as described in Section 3.1.2. final reward depends on long sequence of actions, making outcomes highly sensitive to minor policy changes during training. Such sensitivity causes abrupt, non-linear shifts in success rates, in contrast to the smoother improvement observed in single-step reasoning tasks. Relying solely on offline filtering further aggravates this issue, leading to severe overfitting. As our results confirm, more effective solution is dynamic online filtering strategy, which continuously adapts the training distribution to the agents evolving policy. In summary, the results validate that while trajectory-level optimization provides solid foundation, it is our novel experience management, which combines success-replay mechanism with maximum data utilization, that is crucial for achieving stable and efficient performance. This methodology allows GUI-Owl-7B to achieve state-ofthe-art results among open-source models of the same model size. Notably, under identical experimental settings, our model also surpasses the performance of powerful proprietary models like Claude-4-Sonnet. This demonstrates that our specialized online RL fine-tuning strategy can effectively elevate strong base models, enabling them to excel in complex, long-horizon interactive tasks and rival the capabilities of significantly larger systems. 5.2.1 Scaling of interaction steps and historical images Figure 9: Performance of GUI-Owl-7B on OSWorld-Verified with varying numbers of historical images and interaction-step budgets. We further analyze, on OSWorld, how GUI-Owls performance varies with the number of historical screenshots and the interaction-step budget. As shown in Figure 9, performance increases steadily as more historical images 15 are provided. This is because the models understanding of UI changes relies on contrasts between consecutive frames, and additional images also help the model promptly reflect on and correct persistent erroneous behaviors. We also observe that increasing the interaction-step budget improves performance, indicating that our model has significant advantage on long-horizon tasks. 5.3 Effect of Reasoning Data Synthesis Figure 10: Effect of reasoning data synthesis on Android World. Our offline reasoning data synthesis primarily comes from two methods: Offline Hint-Guided Rejection Sampling and Distillation from Multi-agent Framework. We also mix in general-purpose reasoning SFT data to maintain the models generalization. Beyond the offline data, we use online iterative sampling, continually leveraging updated models to synthesize trajectories with reasoning. We analyze these components separately in Figure 10. We begin validation from an early checkpoint and use performance on AndroidWorld to assess the impact of reasoning synthesis. First, we observe that as we incrementally add data from Offline Hint-Guided Rejection Sampling, distillation from multi-agent framework, and general-purpose reasoning SFT data, the models performance steadily improves. Moreover, adding general reasoning data yields modest performance gain, indicating that maintaining general reasoning capability is also important for GUI interaction reasoning. We also examine the gains from iterative training. Starting from the same checkpoint and iteratively training with newly updated trajectory data, we observe sustained performance improvements. It is because, as the models reasoning ability improves, an increasing share of tasks in the training query set can be completed, thereby enriching the diversity of the training data and enabling the model to learn more robust reasoning capability. 5.4 Evaluation on Agentic Frameworks To evaluate the adaptability of GUI-Owl in real-world scenarios, we benchmarked its performance as the core vision model within established agentic frameworks. We integrated various VLMs into two distinct setups: the Mobile-Agent-E (Wang et al., 2025b) framework on the dynamic AndroidWorld environment, and the AgentS2 (Agashe et al., 2025) framework on the OS World desktop environment. This tests the models ability to generalize across both mobile and PC platforms. The models evaluated include UI-TARS-1.5, UI-TARS-72B, Qwen2.5-VL, Seed-1.5-VL, alongside our GUI-Owl-7B and GUI-Owl-32B. The experimental results, presented in Table Section 5.4, show that GUI-Owl models achieve substantially higher success rates than all baselines on both mobile and desktop platforms. GUI-Owl-32B, in particular, sets the highest result with score of 62.1 on AndroidWorld and 48.4 on OSWorld. We attribute this superior agentic adaptability primarily to GUI-Owls enhanced instruction-following capability. Unlike baseline models that may struggle to interpret the specific directives from an agents planner, GUI-Owl excels at grounding these commands to the correct visual elements on the screen. This leads to more precise action generation (e.g., clicks and text inputs) and critically reduces the accumulation of errors in multi-step tasks. By more reliably executing each step in sequence, GUI-Owl ensures higher overall task success, making it more robust and effective \"brain\" for GUI agents. Model Baseline Models UI-TARS-1.5 (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) Seed-1.5-VL (Team, 2025) Our Models GUI-Owl-7B GUI-Owl-32B Success Rate (%) Mobile-Agent-E (Wang et al., 2025b) on AndroidWorld Agent-S2 (Agashe et al., 2025) on subset of OSWorld-Verified 14.1 14.8 52.6 56.0 59.5 62.1 14.7 19.0 38.6 39.7 40.8 48. Table 8: Performance comparison on agentic frameworks. We report the Success Rate (%) on both mobile (AndroidWorld) and desktop (OSWorld-Verified) environments. representative subset of OSWorld-Verified are selected to capture core challenges while reducing computational costs. Underlined denotes the second-best performance. Action key click long_press swipe type answer Definition Perform key event on the mobile device using adbs keyevent syntax. Click the point on the screen with specified (x, y) coordinates. Press the point on the screen with specified (x, y) coordinates for specified number of seconds. Swipe from starting point with specified (x, y) coordinates to endpoint with specified (x2, y2) coordinates. Input the specified text into the activated input box. Output the specified answer. system_button Press the specified system button: Back, Home, Menu, or Enter. open wait Open an application on the device specified by text. Wait for specified number of seconds for changes to occur. terminate Terminate the current task and report its completion status: success or failure. Table 9: Action Space of GUI-Owl on Mobile."
        },
        {
            "title": "6 Details of Self-Evolving Trajectory Data Production",
            "content": "In this section, we present the details of our self-evolving trajectory data production pipeline. 6.1 Overview GUI automation tasks operate in online interactive environments, which renders manual annotation of trajectory data exceedingly tedious and costly, posing significant challenges for GUI trajectory data collection. To address these challenges, we develop self-evolving GUI trajectory data production pipeline. This approach leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through rollout and assessing their correctness to obtain high-quality training data. Subsequently, these data are utilized to enhance the models capabilities, creating reinforcing cycle of improvement. Our pipeline, illustrated in Figure 4, operates through the following stages: (1) The process initiates with the construction of online virtual environments encompassing mobile, PC, and web platforms, alongside the generation of diverse queries covering wide range of potential GUI scenarios; (2) Given these queries, the GUI-Owl model predicts actions step-by-step, which are then executed within the online virtual environments, yielding rollout trajectories; (3) Trajectory Correctness Judgement module, incorporating multimodal critic framework, evaluates the correctness of all roll-out trajectories. Successful trajectories are collected to create rich dataset of interaction sequences that capture temporal dependencies and diverse GUI states; (4) For challenging queries where the GUI-Owl model struggles to produce successful trajectories despite numerous attempts, we introduce Query-specific Guidance Generation module. This module synthesizes step-level guidance based on ground-truth trajectories produced by human annotation or other models, facilitating GUI-Owls handling of difficult tasks and enhancing the efficiency of the entire data generation pipeline; (5) Finally, all processed data is compiled for 17 Action key type Definition Performs key down presses on the arguments passed in order, then performs key releases in reverse order. Input string of text. Use the clear parameter to decide whether to overwrite the existing text, and use the enter parameter to decide whether the enter key should be pressed after typing the text. mouse_move Move the cursor to specified (x, y) pixel coordinate on the screen. click drag Click the left mouse button at specified (x, y) pixel coordinate on the screen. Click at specified (x, y) pixel coordinate on the screen, and drag the cursor to another specified (x2, y2) pixel coordinate on the screen. right_click Click the right mouse button at specified (x, y) pixel coordinate on the screen. middle_click Click the middle mouse button at specified (x, y) pixel coordinate on the screen. double_click Double-click the left mouse button at specified (x, y) pixel coordinate on the screen. scroll wait Performs scroll of the mouse scroll wheel. Wait for specified number of seconds for changes to occur. terminate Terminate the current task and report its completion status: success or failure. Table 10: Action Space of GUI-Owl on Desktop. reinforcement fine-tuning of GUI-Owl. The model undergoes continuous updates, creating feedback loop where its ability to generate effective roll-out trajectories improves over time, progressively reducing reliance on manual data collection and achieving self-evolution. This self-evolving data production pipeline effectively addresses the unique challenges of GUI automation tasks, enabling the creation of robust and versatile GUI intelligent agents capable of handling the complexities of modern graphical user interfaces while continuously improving the efficiency and quality of the data production process itself. 6.2 High-quality Query Generation As highlighted in our overview, generating high-quality queries is critical component of our self-evolving GUI trajectory data production pipeline. These queries need to cover wide range of possible user intentions and tasks, reflecting the multifaceted nature of GUI interactions. In this section, we present our innovative approach to query generation for mobile and computer applications, which ensures diversity, realism, and accuracy in the produced queries. Mobile. For mobile applications, we develop screenshot-action framework that captures the essence of user interactions while maintaining controllability and extensibility. At the core of our query generation process is human-annotated directed acyclic graph (DAG) = P, for each task. Here, = {p1, . . . , pn} represents screenshots (e.g., home, ordering, payment), and defines valid transitions between them. Each screenshot pi includes description di of the screenshots content and purpose and set of available slot-value pairs that represent possible user choices or inputs on that screenshot. This structure allows us to model realistic navigation flows within apps and capture the multi-constraint nature of user queries. Specifically, our query generation process involves the following steps: (1) Path Sampling: We sample path = {pσ1, . . . , pσk } on the DAG G. This path represents realistic sequence of screenshot transitions within the app. (2) Metadata Extraction: From the sampled path, we obtain screenshot descriptions = {dσ1 , . . . , dσk } and the corresponding slot-value pairs , . (3) Instruction Synthesis: The extracted metadata is fed to Large Language Model (LLM) to synthesize constrained instructions. This approach ensures that the generated queries are both realistic and aligned with the apps structure. (4) Refinement: To enhance naturalness, we refine the raw DAG paths using few-shot LLM prompting. This step transforms explicit navigation instructions into more natural user queries. For example, \"Open the takeout app, click on the food entry\" becomes \"Order me takeout\". (5) Interface Validation: To maintain accuracy, we employ web crawlers to collect real-time interface data from target applications. This ensures that aligned with current app functionality. In conclusion, in our screenshot-action framework, the use of manually defined slot-value pairs minimizes LLM hallucinations, while the DAG structure ensures realistic and controllable navigation flows. 18 Computer. To acquire operational trajectories for the training of intelligent agents, the initial and most crucial step is the batch acquisition of command data. Unlike mobile phones, the computer usage domain typically involves productivity applications, such as web browsers, document editors, file explorers, and email clients. When it comes to intelligent agents, the utilization of these software tools via keyboard and mouse manipulations presents two primary challenges. Firstly, there is the fundamental issue of atomic operational skills. Humans, after learning, can proficiently use mouse for clicking and scrolling and keyboard for input and shortcut execution. However, intelligent agents driven by vision-language models often lack basic knowledge of atomic operations, such as scrolling through content on web pages or selecting editing targets via dragging in office software. Secondly, software operational pathways must be navigated, such as accessing privacy settings in Chrome or adjusting page margins in Microsoft Word. Accomplishing these objectives necessitates series of actions, including clicks, scrolls, and inputs, to reach the requisite configuration options. Therefore, to bestow intelligent agents with computer usage capabilities, we have synthesized user instructions, targeting both atomic operational skills and software operational pathways, through combination of manual annotation and automated generation facilitated by Large Language Models (LLMs). 1) Atomic Operations: For common atomic operations with the mouse and keyboard, we initially acquired operational objects within PC environment via manual annotation. Examples include: a) Double-clicking: This involves creating software icons, folders, etc., to train the model in double-click operations. b) Input: This involves creating files in formats such as Word, Excel, and PowerPoint to train the models capability to accurately input text at specified locations. c) Dragging: Similarly, files in Word, Excel, and PowerPoint formats are created to train the model to select specific text or move objects through dragging. Once operational objects are obtained, we input screenshots of these objects and exemplar commands into the Vision-Language Model (VLM), leveraging its in-context learning capabilities to generate additional executable commands within the current page. 2) Software Operational Pathways: For common software operational pathways, we devised set of automated deep-search chains. Utilizing an accessibility (a11y) tree, we acquire positional and functional information of actionable elements within software interfaces, and by integrating operational pathway memory and replay, we achieve tree-structured search of actionable elements (e.g., multi-level menus) to garner corresponding operational pathways. The endpoint settings of each operational pathway pertain to disparate objects. For example, some configurations alter global file attributes (such as image scaling), whereas others necessitate pre-selecting operational objects (such as altering the font size of text segment). Therefore, to derive legally executable commands based on operational paths, we employ an LLM to ascertain whether an operational pathway requires pre-selection of an operational object. For pathways necessitating selected objects, we input manually annotated file screenshots and operational pathways into the VLM, thereby generating commands executable within the current page. 6.3 Trajectory Correctness Judgment Module The Trajectory Correctness Judgment Module plays crucial role in our self-evolving GUI trajectory data production pipeline. Its primary purposes are twofold: to assess the correctness of roll-out trajectories generated by the GUI-Owl model, and to cleanse erroneous steps within otherwise correct trajectories, thereby enhancing the overall quality of our training data. This module is essential for maintaining high standards in our data collection process, ensuring that only accurate and complete trajectories are used for model training. Our approach to trajectory correctness judgment is comprehensive, operating at both the step level and the trajectory level. This two-tiered system allows for nuanced evaluation of each action within trajectory, as well as an overall assessment of the entire interaction sequence. Problem Definition of Trajectory Correctness Judgment. GUI automation tasks can be formalized as Markov Decision Process: = (E, A, P), where represents the environment state (including user instructions, interaction history, and screenshots), is the action space, and is the transition probability. The Trajectory Correctness Judgement Module consists of two interconnected components: (1) Step-Level Critic: it evaluates individual actions within trajectory. It analyzes the pre-action state, the executed action, and the post-action state to determine the appropriateness of each step. (2) Trajectory-Level Critic: This component assesses the overall correctness of the entire trajectory. It utilizes the outputs from the Step-Level Critic along with the original user instruction to make final judgment on the trajectorys success in accomplishing the users goal. 19 The relationship between these two levels is hierarchical and complementary. The Step-Level Critic provides granular insights into each action, which are then synthesized by the Trajectory-Level Reflection to form holistic evaluation of the entire interaction sequence. Step-Level Critic. Achieving reliable Step-Level Critic presents sophisticated challenge that demands nuanced environmental perception and comprehensive understanding. The methodology necessitates meticulous analysis of preand post-action screenshots, coupled with detailed examination of the executed operation, to accurately assess its contribution towards fulfilling the users designated objective. Initially, we annotate the critical interaction regions on the pre-action screenshot, enabling the model to focus on pivotal areas of intervention, including precise operational detailssuch as clicking, long-pressing, or scrollingto facilitate comprehensive evaluation of the actions alignment with the users goal. Formally, Step-Level Critic can be conceptualized as function πstep critic(ϵ, a, ϵ), where ϵ represents the pre-action environmental state (encompassing user instructions, interaction history, and the initial screenshot), denotes the executed operation, and ϵ encapsulates the post-action environmental state. The function generates three critical outputs: An analysis that provides detailed interpretation of the actions context and consequences summary that concisely captures the key insights of the action (typically within 30 words) An annotation {GOOD, EU RAL, HARM L} that categorizes the actions effectiveness towards the users objective This detailed evaluation at the step level is crucial for identifying and potentially correcting erroneous actions within trajectories, thus improving the overall quality of our training data. Trajectory-Level Critic. The Trajectory-Level Critic, πtraj critic), where represents the user instruction and represents the action trajectory, provides comprehensive evaluation of the entire trajectory. It employs two-channel approach: (1) Textual Reasoning Channel (πtext): Utilizes large language models to assess trajectory correctness based on screenshot caption, textual summaries of each step. (2) Multi-Modal Reasoning Channel (πmultimodal): Incorporates both visual screenshots and textual summaries for more comprehensive evaluation. The textual channel provides concise semantic reasoning, while the multi-modal channel enriches the analysis with visual context, the combination of them helps to mitigate potential biases and limitations inherent in single-modal evaluation. critic(I, T, πstep The final GUI trajectory correctness is determined by consensus mechanism: Trajectory Correctness = (cid:26)Correct, if πtext(T, I) = Correct πmultimodal(T, I) = Correct Incorrect, otherwise (4) In conclusion, this multi-channel approach enhances robustness, processes complementary information, and ensures rigorous validation of trajectories. 6.4 Query-specific Guidance Generation In our constructed query set, some queries pose significant challenges for the model, potentially requiring numerous rollouts to obtain successful trajectory. Some other queries are even more insurmountable and necessitate manual annotation for reference operational trajectories. To acquire more diverse training data, we devise Query-specific Guidance Generation module, which leverages existing successful trajectories to generate guidance that assists the model in producing more successful trajectories. Initially, for the obtained reference trajectories, we employ VLM to generate descriptions of the outcomes of each action. Specifically, the input consists of screenshots of the screen before and after the action execution, coupled with the models or humans action decisions. The VLM is prompted to observe and describe the result of the current action execution, such as \"clicked and activated the search box\" or \"entered the number 100\". When actions involve coordinates, we annotate the interaction locations with circles on the pre-action screenshots to help the VLM focus on detailed screen changes. Regarding the reference trajectories obtained from model rollouts, given the considerable difficulty of the queries, errors or ineffective operations are inevitable. Thus, the VLM also refers to the models decision rationale, determining whether the outcomes of each step align with the models expectations. Operations that do not meet expectations or fail to elicit effective responses are subsequently filtered out during the guidance synthesis process. 20 After acquiring descriptions for each step of the action execution results, we concatenate the descriptions for all steps within the trajectory. Utilizing LLM, we summarize the essential steps required to complete the query, thereby yielding query-specific guidance. 6.5 Examples of Training Data We show the format of end-to-end training data on desktop platform in Figure 11."
        },
        {
            "title": "7 Details of Mobile-Agent-v3",
            "content": "7.1 Core Components and Formalism The operational dynamics of the Mobile-Agent-v3 framework are defined by set of state variables and the specialized functions of its constituent agents. We formalize these components as follows. 7.1.1 State Variables and Definitions Let the entire process be sequence of operations indexed by timestep {0, 1, . . . , }. Device State (St): The state of the GUI device at timestep t, represented as high-dimensional tensor St RHW C, where H, W, are the height, width, and channel dimensions of the screen capture, respectively. S0 denotes the initial state. Subsequent Subgoals (SSt): An ordered list of pending subgoals formulated by the Manager Agent. It is defined as CSt = (g1, g2, . . . , gk), where each gi is natural language string describing discrete step towards the main goal. Compltetd Subgoals (CSt): set containing subgoals that have been successfully executed and verified. It is defined as SSt = {g1, g2, . . . , gm}. This prevents redundant operations and tracks progress. Action (At): The operation executed by the Worker Agent at timestep t. An action is structured tuple At = (τt, αt, σt) A, where: τt: The thought process, textual rationale for selecting the action. αt: The concrete, low-level action command (e.g., click(x, y), type(\"text\")). σt: concise summary of the actions intended effect. Reflection Feedback (Ft): The output generated by the Reflector Agent after observing the consequences of action At. It is tuple Ft = (jt, ϕt) {SUCCESS, FAILURE} Φ, where: jt: binary judgment on the outcome of At. ϕt: detailed textual feedback, particularly diagnostic analysis in case of \"FAILURE\". Φ represents the space of all possible feedback texts. Notes (Nt): collection of critical, potentially transient information captured by the Notetaker Agent. The cumulative knowledge base at step is Nt = (cid:83)t1 i=0 Ni. 7.2 Agent Architecture in Detail 7.2.1 External Knowledge Retrieval with RAG To enable the agent to complete tasks requiring real-time information or domain-specific knowledge (e.g., checking todays weather, finding recent sports scores, or looking up app-specific tutorials), we incorporate RAG module. This module is invoked at the beginning of task to retrieve relevant information from external sources, such as the internet, and provide it as context to the agent system. The process can be formalized as follows. Given an initial user instruction I, the RAG module first processes it into one or more search engine-friendly queries Q. = GenerateQueries(I) Subsequently, the system uses these queries to retrieve set of relevant documents or text snippets = {d1, d2, . . . , dn} from an external knowledge source (e.g., web search engine). = SearchEngine(Q) Finally, the retrieved content is processed and summarized to form concise, information-rich body of knowledge, KRAG. KRAG = Process(D) 21 This retrieved knowledge KRAG is passed to the Manager agent during its initialization phase (as shown in Algorithm Algorithm 1, lines 3-4). This allows the Manager to generate its initial plan (SS0, CS0) based on more comprehensive and accurate information, thereby significantly improving the quality of the plan and the likelihood of task success. For example, for an instruction like \"Should take an umbrella to the park today?\", KRAG would contain the weather forecast, enabling the Manager to create plan that includes steps like \"open weather app\" and \"check for rain probability\". 7.2.2 The Manager Agent: Dynamic Task Planning and Coordination The Manager Agent serves as the strategic core of the framework. Its function is responsible for decomposing the high-level user instruction into coherent sequence of subgoals and dynamically adapting this plan throughout the execution process. Initially, at = 0, the Manager performs decomposition: (SS0, CS0) = Minit(I, S0, KRAG) where KRAG is external knowledge retrieved by the RAG module to inform the decomposition of potentially domain-specific or complex instructions. CS0 is initialized as an empty set . In subsequent steps > 0, the Manager updates the plan based on the latest execution results: (5) (SSt, CSt) = Mupdate(I, St1, SSt1, CSt1, At1, Ft1, Nt) If the previous action was successful (jt1 = SUCCESS), the Manager identifies the completed subgoal in SSt1, moves it to CSt, and re-prioritizes the remaining tasks in SSt. If the action failed (jt1 = FAILURE), the Manager leverages the diagnostic feedback ϕt1 to revise the plan. This may involve re-ordering subgoals, modifying an existing subgoal, inserting new corrective subgoal, or even reverting to previous strategy. (6) 7.2.3 The Worker Agent: Grounded Action Execution The Worker Agent is the tactical executor, translating the strategic plan from the Manager into concrete interactions with the GUI. Its function aims to execute the highest-priority, currently feasible subgoal from the guidance list CSt. (7) At = W(I, St, SSt, Ft1, Nt) Upon receiving the subgoal list SSt, the Worker inspects small subset from the top of the list (e.g., the top subgoals). It analyzes the current screen St to determine which of these subgoals is most relevant and actionable. The decision-making process is informed by feedback from the previous step, Ft1, to avoid repeating errors, and the accumulated notes, Nt, to utilize previously stored information (e.g., using password saved in notes). The output, At = (τt, αt, σt), provides transparent record of its reasoning, action, and intent, which is crucial for reflection. 7.2.4 The Reflector Agent: Self-Correction through Reflection The Reflector Agent is critical component for ensuring robustness and learning from mistakes. It embodies the frameworks capacity for self-assessment. Its function evaluates the efficacy of an action by comparing the state transition with the Workers intent. (8) Ft = R(I, St, St+1, At) The Reflector analyzes the pre-action state St, the post-action state St+1, and the action tuple At. judgment jt = SUCCESS is rendered if the state change St St+1 aligns with the progress articulated in the Workers thought τt and summary σt. Conversely, jt = FAILURE is returned if the GUI presents an error, remains unchanged unexpectedly, or transitions to an irrelevant state. In case of failure, the feedback ϕt provides causal analysis, such as action click(123, 456) on button \"Submit\" did not proceed to the next page; an error message \"Invalid credentials\" is now visible. This detailed feedback is vital for the Managers replanning phase. 7.2.5 The Notetaker Agent: Persistent Contextual Memory The Notetaker Agent addresses the challenge of state volatility in GUI interactions, where crucial information may appear on one screen and be required on subsequent, different screen. The Notetakers function is to identify and persist such information. (9) Nt = C(St) This agent is triggered only upon successful action (jt = SUCCESS). It scans the state transition for pieces of information designated as vital for the ongoing task (e.g., reservation codes, order numbers, user-generated content, entered credentials). This information is structured into the note set Nt. The cumulative notes Nt+1 = Nt Nt are then made available to both the Manager and the Worker in future steps, creating persistent memory that informs long-horizon planning and execution. 22 Algorithm 1 Mobile-Agent-v3 Execution Loop 1: Input: User instruction I, initial device state S0, max timesteps Tmax 2: Initialize: Manager M, Worker W, Reflector R, Notetaker Init Manager Phase: Initialize Plan 3: Retrieve external knowledge KRAG RAG(I) 4: (SS0, CS0) Minit(I, S0, KRAG) 5: N0 , F1 null, 0 6: while < Tmax and SSt = do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Worker Phase: Execute Action At W(I, St, SSt, Ft1, Nt) if At = TERMINATE then Break end if St+1 ExecuteOnDevice(At) Reflector Phase: Evaluate Outcome Ft R(I, St, St+1, At) Notetaker Phase: Persist Information if Ft.status = SUCCESS then Nt C(St) Nt+1 Nt {Nt} else Nt+1 Nt end if Manager Phase: Update Plan (SSt+1, CSt+1) Mupdate(I, St, SSt, CSt, At, Ft, Nt+1) + 1 19: 20: 21: end while return Task Succeeded 22: if SSt = then 23: 24: else 25: 26: end if return Task Failed (Timeout or Stalemate) 7. Integrated Workflow and Algorithm The Mobile-Agent-v3 framework operates as cyclical, state-driven process. The workflow begins with user instruction and terminates when the task is complete or deemed unachievable. The entire process is formalized in Algorithm 1. The process is initialized with high-level user instruction I. The Manager Agent, aided by the RAG module, creates an initial subgoal plan SS0. The system then enters an iterative loop. In each iteration t, the Worker Agent selects and executes subgoal, resulting in action At. The environment transitions to new state St+1. The Reflector Agent evaluates this transition, producing feedback Ft. If the action was successful, the Notetaker Agent may record pertinent information as Nt. Finally, the Manager Agent updates the task plan to (SSt+1, CSt+1) based on the feedback. Termination occurs under two conditions: 1. Task Completion: The Manager determines the task is complete, resulting in an empty pending subgoal list (SSt = ). 2. Execution Stalemate: The Worker Agent determines that no pending subgoals in SSt can be executed on the current state St, even after several retries or plan revisions. This structured, reflective, and adaptive loop enables the framework to navigate complex sequences of interactions, handle unexpected events, and robustly pursue the completion of the users goal. 23 7.4 Case Study Figure 12 shows complete Mobile-Agent-v3 operation flow, including the outputs of the manager, worker, and reflector. The managers output shows that subgoals are constantly updated as the task progresses. The worker consistently outputs actions guided by the subgoals output by the manager. Notably, the red text in Figure 12 illustrates successful reflection. After the workers click operation in the previous step failed, the reflector successfully discovered the problem and provided feedback to the manager and worker in the next step. Finally, the worker repeated the click operation to correct the problem."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we present GUI-Owl, native end-to-end multimodal agent model that unifies perception, grounding, reasoning, planning, and action execution within single scalable framework for GUI automation. Building upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl achieves state-ofthe-art performance across broad range of challenging benchmarks, surpassing both open-source and proprietary systems, including GPT-4o and Claude 3.7. Through synthesized reasoning data and scalable reinforcement learning framework, GUI-Owl is capable of versatile decision-making from autonomous single-agent execution to collaborative multi-agent role coordination within our Mobile-Agent-v3 framework. 24 Figure 11: Format of end-to-end training data. 25 Figure 12: case of complete Mobile-Agent-3 operation process on desktop platform. The red text represents successful reflection content."
        },
        {
            "title": "References",
            "content": "Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. 10 Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents, 2025. URL https://arxiv.org/abs/2504.00906. 2, 10, 16, 17 Anthropic. Claude-3-5-sonnet. Technical report, Anthropic, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. 14 Anthropic. Claude 3.7 sonnet and claude code. Technical report, Anthropic, 2025a. URL https://www. anthropic.com/news/claude-3-7-sonnet. System Card. 12, 13, 14 Anthropic. Claude-4-sonnet. Technical report, Anthropic, 2025b. URL https://www.anthropic.com/news/ claude-4. 14 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shĳie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 12, 13, 14, 17 Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 93139332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505. 12 Alibaba Cloud. Introducing alibaba cloud, 2018. Deepmind. nical google-gemini-ai-update-december-2024/#project-astra. 14 Introducing report, Deepmind, Techfor URL https://blog.google/technology/google-deepmind/ new ai model gemini 2025a. agentic 2.0: era. our the Deepmind. Gemini 2.5: Our most intelligent ai model. Technical report, Deepmind, 2025b. URL https:// blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/. 13 Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, and Meng Wang. Generalist virtual agents: survey on autonomous agents across digital platforms. ArXiv preprint, abs/2411.10943, 2024. URL https://arxiv.org/abs/2411.10943. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 13 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9 Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use. 2024. 2 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 12, 13, 14 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, and Weinan Zhang. Mobileuse: gui agent with hierarchical reflection for autonomous mobile operation. arXiv preprint arXiv:2507.16853, 2025. 10 Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824, 2024. 10 27 Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lĳuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1949819508, 2025. 13 Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 12 Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. 12 Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. 6, 9 Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, Tamer Özsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. 6, 9 Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. ArXiv preprint, abs/2412.13501, 2024. URL https://arxiv. org/abs/2412.13501. Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024. 10 Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. Mobileflow: multimodal llm for mobile gui agent. arXiv preprint arXiv:2407.04346, 2024. 10 OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent. 12, 13 OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025b. URL https://cdn.openai. System com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Card. 14 Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. 10 Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. 5 Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shĳue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 2, 4, 12, 13, 14, 17 Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. 2 Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. 10 Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. 12 28 ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 12, 13, 14, 17 Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6 Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024b. 2, 10 Peng Wang, Shuai Bai, Sinan Tan, Shĳie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024c. 14 Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. Gui agents with foundation models: comprehensive survey. ArXiv preprint, abs/2411.04890, 2024d. URL https://arxiv.org/abs/2411.04890. 2 Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. URL https://arxiv.org/abs/2508.09123. 2, 4, 14 Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobileagent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b. 10, 16, 17 Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024a. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. 10, 12, 13 Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. 9, 13 Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computeruse grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505. 13227. 2, 12, 13 Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. 13, 14 Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. 6, 12 Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. 12 Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. Large language model-brained gui agents: survey. ArXiv preprint, abs/2411.18279, 2024a. URL https://arxiv.org/abs/2411.18279. 2 29 Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024b. 10 Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025a. Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025b. 10 Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 10 Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, et al. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. 12 Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weĳie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 13,"
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}