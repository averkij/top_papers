{
    "paper_title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
    "authors": [
        "Anurag Das",
        "Adrian Bulat",
        "Alberto Baldrati",
        "Ioannis Maniadis Metaxas",
        "Bernt Schiele",
        "Georgios Tzimiropoulos",
        "Brais Martinez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 2 1 8 7 0 . 1 0 6 2 : r More Images, More Problems? Controlled Analysis of VLM Failure Modes. Anurag Das1*, Adrian Bulat2,3, Alberto Baldrati2, Ioannis Maniadis Metaxas2 Bernt Schiele1, Georgios Tzimiropoulos2,4, Brais Martinez2 1MPI for Informatics, Saarland Informatics Campus 2Samsung AI, Cambridge 3Technical University of Ias, i, Romania 4Queen Mary University of London, UK andas@mpi-inf.mpg.de"
        },
        {
            "title": "Abstract",
            "content": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, comprehensive analysis of their core weakIn nesses and their causes is still lacking. this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), new benchmark designed to rigorously evaluate the multiimage capabilities of LVLMs. Using MIMIC, we conduct series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https: //github.com/anurag-198/MIMIC."
        },
        {
            "title": "Introduction",
            "content": "Current Large Vision Language Models (Li et al., 2024a; Liu et al., 2024a; Wang et al., 2024c,b; Yao et al., 2024) showcase impressive vision-language understanding capabilities (Goyal et al., 2017; Mathew et al., 2021; Kembhavi et al., 2016). Most of these models are built upon pre-trained vision encoders (Radford et al., 2021; Zhai et al., 2023) and large language models (LLMs) (Touvron et al., *Work done during internship at Samsung AI, Cambridge. 1 2023; Abdin et al., 2024; Jiang et al., 2024a). While early efforts primarily focused on single images (Liu et al., 2024a), recent works have extended them to support multiple images (Li et al., 2024a; Wang et al., 2024b; Chen et al., 2023) and videos (Zhang et al., 2023) by incorporating temporal modeling and adjusting the positional embeddings (Zhang et al., 2023; Li et al., 2024a). Despite their success, LVLMs continue to face significant challenges(Stogiannidis et al., 2025; Liu et al., 2023; Ouali et al., 2024; Guan et al., 2024; Kaul et al., 2024; Qian et al., 2024). Progress towards identifying and addressing these challenges can follow two primary avenues: the development of comprehensive evaluation benchmarks (Goyal et al., 2017; Mathew et al., 2021; Kembhavi et al., 2016; Masry et al., 2022; Li et al., 2024b; Fu et al., 2024a) and the study of the models inner workings (Deng et al., 2025; Qian et al., 2024; Kaul et al., 2024). To date, research in both areas has predominantly focused on the single-image setting. While early efforts have introduced benchmarks for multi-image scenarios (Wang et al., 2024a; Jiang et al., 2024b; Fu et al., 2024b), comprehensive, in-depth analysis to ascertain the true efficacy of these models and identify the root causes of their limitations is notably absent. In this work, we address this gap by conducting systematic study of LVLMs in multi-image contexts. We first analyze and characterize common failure modes using newly proposed benchmark, and then seek to mitigate these limitations using two novel complementary fine-tuning strategies. Our in-depth analysis is performed on the newly introduced MIMIC (Multi-Image Model Insights and Challenges) benchmark. Built from MSCOCO (Lin et al., 2014), using its bounding boxes and class labels, MIMIC procedurally generates multi-image sequences by leveraging per-image annotations that give fine-grained control over information spread, distractor presence, object-instance distributions, sequence length, and query complexity, while providing unambiguous ground-truth answers for robust, decorrelated analysis of the models strengths and weaknesses. Using both quantitative and qualitative assessments, our study reveals that current state-of-the-art LVLMs struggle to effectively aggregate information across multiple images, are unable to track/attend to multiple concepts simultaneously, while being susceptible to distractors. We attribute these shortcomings to combination of factors, including limitations in multi-image sequence modeling, training data biases, poor inter-image communication induced by the causal attention and the inherent complexity of multi-image reasoning tasks. Finally, to address the identified problems, we propose two new finetuning strategies: (1) data-centric approach that generates targeted multi-image training examples to provide rich, multi-image supervision derived from OpenImages (Kuznetsova et al., 2020); and (2) an optimization-centric approach that leverages layerwise attention analysis to derive an attentionmasking scheme tailored for multi-image inputs. Our proposed finetuning strategies lead to substantial performance gains across all scenarios. In summary, our main contributions are: We introduce MIMIC, comprehensive evaluation framework for multi-image LVLMs that probes various aspects of model performance through controlled and diverse set of tasks. We conduct an extensive evaluation of several state-of-the-art LVLMs using MIMIC, uncovering critical insights into their capabilities and limitations in multi-image settings. We propose novel data-centric finetuning approach using synthetically generated multiimage data, alongside an optimization-centric attention-masking strategy, both of which significantly enhance model performance in multi-image contexts. We set new state-of-the-art results on existing multi-image benchmarks, demonstrating the effectiveness of our proposed methods."
        },
        {
            "title": "2 Related work",
            "content": "Multi-Image Large Vision Language Models: Early LVLMs such as Flamingo (Alayrac et al., 2022) and PaLM-E (Driess et al., 2023) pioneered the integration of pre-trained vision encoders with powerful LLMs for VQA and captioning. Subsequent models (Dai et al., 2023; Li et al., 2024a) introduced expanded instruction tuning and multi-modal pre-training techniques. More recent advancements include MiniGPT-5 (Zheng et al., 2023), Qwen2-VL (Wang et al., 2024b), CogVLM2 (Hong et al., 2024) and InternVL3 (Zhu et al., 2025) further advanced the field by scaling training data and model capacity and adopting more sophisticated architectural designs. While early LVLMs primarily operated on low-resolution, single-image inputs (Liu et al., 2024a; Lin et al., 2024), later research significantly expanded their scope. High-resolution images (Li et al., 2024a; Wang et al., 2024b; Zheng et al., 2023) are commonly processed by splitting them into fixedresolution patches and treating them as image sequences, while videos are represented by extracting frames to form multi-image inputs. In addition, models have begun to explicitly support multiimage context (Wang et al., 2024b; Li et al., 2024a; Zhu et al., 2025), enabling reasoning across multiple visual inputs. Multi-image capability is introduced by fine-tuning single-image LVLMs on multi-image instruction-tuning data, while largely preserving the original model architecture and attention mechanisms. Evaluation of LVLMs: Early evaluation efforts focused on narrower domains with benchmarks such as MS-COCO (Lin et al., 2014), VQA (Antol et al., 2015), DocVQA (Mathew et al., 2021), GQA (Hudson and Manning, 2019) and AI2D (Kembhavi et al., 2016), primarily assessing single-image understanding and using templetized questions with limited diversity. Later work introduced more comprehensive benchmarks to evaluate wider range of skills, e.g. SEED-Bench (Li et al., 2024b), MMBench (Liu et al., 2024c) and MME (Fu et al., 2024a), which feature diverse question types and require complex reasoning abilities. Similarly, video benchmarks such as MMVU (Zhao et al., 2025) and VideoMME (Fu et al., 2025) require models to understand temporal dynamics and to reason across multiple frames. Closer to our work, several benchmarks have been proposed specifically for multi-image LVLMs. MuirBench (Wang et al., 2024a) introduced 12 tasks evaluating multi-image understanding, including image comparison and multi-image reasoning. Blink (Fu et al., 2024b) included 14 tasks deemed easy for humans, highlighting LVLMs limitations in truly understanding multi-image visual Figure 1: Counting performance under different settings. Left (Unbalanced): We compare different LVLMs by analyzing the trade-off between the number of query images and the total number of images without controlling for number of instances. Mid (Balanced): We fix the total number of images to 7 and of object instances distributed across query images to 4,3 and 2. In both settings, performance consistently drops when instances are spread over multiple images. Right (Multi Concept): We increase the complexity by adding more classes (concepts) to the counting task, and observe steep performance drop, indicating limited capacity for multi-concept tracking. content. Visual Haystack (Wu et al., 2025) focuses on retrieval-based tasks, assessing how well models can find certain concepts within long sequence of images. Instead, we provide more granular analysis of model performance across various controlled dimensions, such as information distribution, query complexity and distractor presence. Moreover, while prior works often repurposed existing datasets, we design our task from scratch to allow for selective performance exploration. This allows us to pinpoint specific strengths and weaknesses in current models that prior benchmarks may have overlooked and, importantly, provide deeper actionable insights on their underlying root causes. Analysis of LVLMs: Parallel to the development of benchmarks, there is growing interest in analyzing the internal mechanisms of LVLMs to better root-cause their limitations at data and architecture level. Current studies have investigated issues such as hallucination (Liu et al., 2023; Ouali et al., 2024; Guan et al., 2024), modality bias (Deng et al., 2025; Wang et al., 2025), and sensitivity to input phrasing (Qian et al., 2024). These works often involve probing the models with carefullydesigned inputs to reveal their decision-making process. Only recently have such analyses expanded to multi-image LVLMs (Wang et al., 2024d; Wu et al., 2025; Sharma et al., 2024). The closest to our work is the study by Wu et al. (Wu et al., 2025), which examines the retrieval capabilities of multi-image LVLMs as the sequence length increases, showing limitations when operating over long sequences. However, their focus is primarily on the models ability to locate specific items within an image set and does not control conflating factors, nor seek to identify the root causes beyond data scarcity. Instead, we systematically probe additional dimensions of multi-image understanding, such as information aggregation and multi-concept tracking. To control for confounding factors, our evaluation is designed to isolate specific unitary aspects of multi-image understanding, leading to precise conclusions and to the identification of areas for improvement. Moreover, we analyze the internal models behavior, and complement our analysis with proposed solutions to address the identified challenges at both data and optimization levels."
        },
        {
            "title": "LVLMs",
            "content": "Herein, we systematically investigate the current LVLMs limitations in multi-image scenarios across six complementary dimensions: information distribution, query complexity, reasoning patterns, robustness to visual distractors, scalability with the number of images, and multi-concept tracking. For this purpose, we introduce MIMIC, controlled testbed synthesized from curated subset of MSCOCO (Lin et al., 2014). Using the manually annotated bounding boxes and labels, MIMIC generates multi-image sequences that allow precise control over information spread, distractor presence, object-instance distributions, and sequence length. This design enables decorrelated, fine-grained analyses of the models behavior. Beyond these dimensions, our framework scrutinizes the models mechanisms for aggregating and reasoning over distributed visual information. Through this controlled analysis, we aim to isolate the specific limitations and offer actionable insights for the next generation of visual understanding models. Section 3.1 describes the tasks design and dataset construction of our probing benchmark. Sec3 the need to calibrate distractor choices. To further reduce prompt bias, we employ multiple templatized prompts per task (see appendix for template list). Below, we describe each task in detail. Figure 3: Effect of vision token sequence length on performance. Left: Sequence length reduction via 1D pooling. The square denotes the original sequence length. Right: Control experiment reducing the information via pixel space pooling while keeping the sequence length fixed. Results are reported for counting task with 3 query images and total 10 images. Counting: Given set of input images, and query containing object classes, the model is asked to count the total number of instances of each class. With increasing difficulty, we vary the distribution of object instances across images. For example, in the easiest setting, all may be concentrated in single image, while in more challenging cases, instances are spread across multiple images. We refer to this as the information spread. Additionally, we introduce distractors - images that do not contain any instances of the target objects - to assess the models ability to focus on relevant information. In summary, this task offers the following controllable dimensions: (1) number of object classes to count (k); (2) information spread across images (s); (3) number of distractor images and (4) total number of images. Each case probes different aspects and potential biases. For instance, increasing the number of object classes tests the models multi-concept tracking ability, while varying the information spread evaluates its capacity to aggregate information across images. To account for potential biases caused by the long-tail distribution of object instance counts in natural images, which may lead to models favoring smaller counts, we design two distinct settings: (1) Balanced, where the total number of object instances is fixed, but distributed across varying number of images; (2) Unbalanced, where the total number of object instances varies arbitrarily with the number of images. The metric of choice is binary accuracy, i.e. the answer is correct if it matches the ground truth count exactly. Listing: The model is presented with set of imFigure 2: MIMIC Bench: examples of each task. tion 3.2 describes the systematic probing of several state-of-the-art LVLMs to uncover their strengths and limitations in multi-image understanding. Counting Bal. Unbal. Common Odd One Listing Overall 5000 5800 5370 3761 44.3 132.1 Queries Images Obj. inst. per query 2 Min img per query 35 Max img per query Median img per query 15.0 Avg words per question 15.1 15.2 7 7 7.0 1000 3842 26.1 3 8 5.0 14.7 1000 3726 20.7 4 6 5.0 17.3 1000 4137 27.9 2 8 5.0 13. 13800 13145 77.0 2 35 7 15.2 Table 1: MIMIC Benchmark statistics per task. Counting settings: Balanced (Bal) and Unbalanced (Unbal)."
        },
        {
            "title": "3.1 Testbed benchmark construction",
            "content": "We build the probing dataset by procedurally generating multi-image, open-ended question-answering tasks that target distinct aspects of cross-image reasoning. To this end, we sample curated subset from MS-COCO (Lin et al., 2014) by filtering images with object bounding boxes less than 5% of the image in order to ensure visual recognizability at common LVLM input resolutions (e.g. LLaVAOVs 384 384px). To minimize the impact of potential class imbalance, we first select pool of classes and then sample from this pool, ensuring that each class is chosen with an equal probability. This ensures that the distributions of classes and instances remain consistent across settings. MIMIC defines four core tasks: Counting, Listing, Common, and Odd-One, each targeting distinct facet of multi-image reasoning. Fig. 2 provides some qualitative examples, and Table 1 reports dataset statistics. All tasks are formulated as open-ended question answering rather than multiple-choice to increase challenge, avoid shortcuts introduced by fixed option sets, and remove 4 ages and asked to list all object classes belonging to given category (e.g.: animals, vehicles, etc.) that it can identify. This task evaluates the models ability to exhaustively extract information in dense manner from multiple images. As byproduct, it also measures the models visual perception ability to recognize and categorize multiple objects, as well as its capacity to aggregate this information into coherent list. Similar to the Counting task, we vary the number of images and the distribution of object instances to assess the models robustness in multi-image understanding. The models response is evaluated on the completeness and accuracy of the list, using F1-score as metric. See the appendix for complete hierarchy of object categories and subcategories. Common and Odd-One: The two tasks are designed to assess the models ability to identify shared or unique elements across multiple images. Importantly, while previous tasks focus on aggregating information, these tasks require comparative reasoning across images, hence the model must first implicitly identify all objects before performing cross-image analysis. In the Common task, the model has to determine which object class is present in all provided images, while in the OddOne case, it must identify the object class that is present in minority of images. For simplicity, we ensure by design that the answers are unique. The models answers are evaluated based on their correctness, with binary accuracy as the metric."
        },
        {
            "title": "3.2 Empirical analysis",
            "content": "Setup: We evaluate several state-of-the-art LVLMs: LLaVA-OV (Li et al., 2024a), Qwen2VL (Wang et al., 2024b) and InternVL2 (Chen et al., 2024b). We use publicly available checkpoints and follow the official data processing pipeline. For test data, we use the MIMIC benchmark described in Section 3.1, selecting tasks and configurations that best isolate the dimensions we aim to probe. Performance in Relation to Sequence Length and Number of Images: LLMs are known to manifest position and sequence length biases (Ravaut et al., 2024), with tokens appearing earlier and late in the sequence receiving more attention. Unlike for LLMs, we distinguish two axes of sequence length growth: (1) increasing the number of images, and (2) increasing the input image(s) resolution. We seek to understand if the performance degradation stems from the models inability to handle long sequences, or from the inability to process many images. We disentangle these two factors with the following experiments: (a) directly increasing the number of images without explicitly controlling for sequence length. In this setting, we simply vary the number of images provided to the model, measuring performance on the counting task. As the results from Fig. 1 (left) show, performance degrades in all setting consistently for all models as the total number of images increases from 2 to 35s. (b) reducing the vision token sequence length through 1-D average pooling applied to the original multi-image vision tokens. To ensure that the observed behavior is not an artifact of reduced information, we also perform control experiment where we similarly reduce the amount of information by downsampling and then upsampling back the images in pixel space, prior to being passed to the vision encoder. This preserves the initial sequence length but reduces the amount of visual information available to the model. This allows us to assess if the performance degradation observed in (a) is primarily due to the increased sequence length or number of images. The results are summarized in Fig. 3. On the left, we plot performance changes for different models as we decrease the number of vision tokens via 1-D pooling. Due to different processing, each model allocates different number of tokens per image, hence we mark two points - extreme right (no downsampling) and central point that maximizes performance. On the right, we show the control experiment, that decreases the information in the pixel space artificially without reducing sequence length. Surprisingly, we find that reducing the sequence length in zero-shot manner via 1-D pooling up to 4 8 leads to significant performance improvements across all models. The control experiment confirms that gains are due to sequence length reduction rather than information reduction. This suggests that the models primarily struggle with long sequence understanding rather than with processing multiple distinct images. Finding 1: The performance degradation in multi-image scenarios stems primarily from increased sequence length rather than the increased number of images. Moreover, we observe that for LLaVA-OV performance peaks when the vision sequence length is approximately that of one or two images (i.e., roughly the number of vision tokens for 384384 Figure 4: Inter-image and intra-image token attention across layers. The attention patterns transitions from cross-image to intra-image interactions as we advance in depth. image/patch). This suggests the model effectively relies on single-image context and has limited practical multi-image integration; we later evaluate how targeted finetuning can mitigate this limitation. Finding 2: Current LVLMs primarily behave as single-image models: performance peaks when the vision-token sequence length matches that produced by one or two images. Information aggregation across images: Prior benchmarks rarely control for how information is distributed across images, making it difficult to isolate whether models can effectively aggregate information across images. To this end, we vary the information spread in the counting task, which defines how object instances are distributed across images. In Fig. 1 (left and middle) we show the results of increasing the number of images containing the object instance from 1 to 7. We observe sharp accuracy drop that approaches 0 even when very few distractors are present. This trend is consistent across all models tested and manifests both in balanced and unbalanced counting settings. This indicates that the models may rely on shortcuts, such as focusing on single or very small subset of images, rather than effectively integrating information from all provided images. Finding 3: Current LVLMs struggle to aggregate information across multiple images. Robustness to visual distractors. In real-world scenarios, models often encounter irrelevant or distracting information. To evaluate the robustness of LVLMs, we introduce varying numbers of irrelevant images into the input sequence. As shown in Fig. 1 (left), the accuracy decreases as the number of distractors increases (e.g: from 79.0% to 66.5% (1 vs 34 distractors) for 1 query image, from 75.0% to 12.5% for two query images, etc., for LLaVAOV). particularly pronounced drop occurs as the number of images containing the object of interest increases, suggesting that distractors exacerbate the models existing difficulties in aggregating information across multiple images. Finding 4: Models are sensitive to visual distractors, especially if information is spread out. Multi-concept tracking. The ability to track and attend to multiple concepts simultaneously is critical for multi-image understanding. To probe this capability, we vary the number of object classes that the model is required to count. As shown in Fig. 1 (right), the model performance degrades sharply as increases, indicating limited capacity to handle multiple concepts at once. Finding 5: LVLMs demonstrate limited capacity for multi-concept tracking, reducing their reliability on complex multi-object queries. Multi-image interaction. To probe how visual information is propagated and integrated across images at the token level, we analyze attention patterns among vision tokens in multi-image inputs. Concretely, we compute the normalized attention scores from each vision token to all other vision tokens in the input sequence subject to an autoregressive attention masking on subset of 50 samples. Fig. 4 summarizes the results across few layers of interest for LLaVA-OV model for multi-image inputs with 4 images. We find that Model Random Choice Human GPT-4o (OpenAI, 2023) Gemini Pro (Team et al., 2023) Mantis-8b-Idefics2 (Jiang et al., 2024b) Idefics-9B-Instruct (Laurençon et al., 2023) Emu2-Chat(37B) (Sun et al., 2024) VILA1.5-13B (Lin et al., 2024) LLaVA-NeXT-34B (Liu et al., 2024b) LLaVA-v1.5-7B (Liu et al., 2024a) LLaVA-v1.5-13B (Liu et al., 2024a) CogVLM (Wang et al., 2024c) MiniGPT-4-v2 (Chen et al., 2023) Qwen2-VL-7B (Wang et al., 2024b) Qwen2-VL-2B (Wang et al., 2024b) InternVL2-8B (Chen et al., 2024a) InternVL2-2B (Chen et al., 2024a) LLaVA-OV-0.5B (Li et al., 2024a) Ours Ours (Masked) LLaVA-OV-7B (Li et al., 2024a) Ours (Masked) Geographic. Counting Action. Grounding Matching. Ordering Scene. Difference. Cartoon. Diagram Attribute. Retrieval Overall 25.0 98.0 56.0 48.0 26.0 35.0 34.0 31.0 12.0 20.0 20.0 13.0 13.0 12.0 14.0 17.0 17.0 22.0 43.0 31.0 43.3 44.0 21.0 94.9 49.2 28.6 38.5 21.8 31.2 19.7 36.3 23.1 25.2 14.1 12.0 38.9 27.8 30.3 21.8 20.9 20.5 17.5 24.8 35.9 23.4 97.6 44.5 36.0 33.5 26.2 27.4 28.7 26.2 27.4 29.3 26.2 14.0 42.7 35.4 34.7 26.8 31.1 41.4 40.8 36.6 51.2 25.0 85.7 36.9 28.6 26.2 26.2 26.2 25.0 33.3 14.3 14.3 16.7 25.0 28.5 26.2 28.5 26.2 25.0 22.6 33.3 29.7 42. 24.1 94.8 86.9 66.6 53.9 24.8 37.3 40.9 37.9 23.5 20.3 21.3 17.0 57.5 34.3 43.5 31.7 30.2 38.4 37.3 45.3 59.9 22.8 87.5 23.44 12.5 18.8 15.6 15.6 10.9 21.9 23.4 20.3 12.5 18.8 10.9 10.9 17.2 10.9 7.8 9.4 14.1 17.2 12.5 25.0 94.6 71.5 59.1 57.0 56.5 48.4 56.5 54.3 35.0 36.6 41.4 14.5 75.3 51.1 60.2 52.2 46.7 52.7 53.8 71.5 71.0 23.2 92.9 60.3 45.3 28.8 27.6 32.6 24.7 22.1 20.0 20.0 19.7 20.0 32.9 19.4 26.2 17.4 24.1 22.1 21.5 30.0 43.5 25.0 82.1 51.3 47.4 38.5 39.7 43.6 30.8 41.0 24.4 25.6 41.0 21.8 38.5 39.7 46.2 35.9 42.3 37.2 42.3 35.9 38.5 29.6 98.99 88.7 64.8 67.6 25.4 37.7 42.7 38.2 25.1 31.7 19.6 21.6 49.2 21.4 46.5 21.6 25.1 39.4 34.9 54.2 62. 20.0 87.6 56.1 41.3 48.5 17.9 31.6 24.5 38.3 23.0 23.0 16.3 17.4 46.4 31.1 42.8 16.8 23.9 29.6 28.1 32.7 52.0 21.3 86.3 80.1 43.8 35.6 17.1 24.0 30.1 25.0 19.9 20.9 15.8 14.7 26.7 15.4 33.6 13.7 20.5 32.5 32.9 46.2 48.6 24.0 93.2 68.0 49.4 44.5 35.4 33.6 33.1 33.3 23.5 24.4 20.9 17.4 43.0 27.2 37.9 24.3 26.8 33.6 32.5 41.7 51.3 Table 2: Performance comparison across different MuirBench (Wang et al., 2024a) subtasks. Ours (Masked): Our efficient model trained with LoRA and masked attention. Ours: Fully fine-tuned model. Due to computational constraints, we do not fully finetune LLaVA-7B model. in earlier layers, there is significant amount of inter-image attention, indicating that the model is attempting to integrate information across images. However, as we progress to deeper layers, the attention becomes predominantly intra-image. This inflection point occurs somewhere around the middle of the network. This shift may contribute to the observed difficulty in aggregating information across multiple images. Conceptually, the build-up of representations appears to proceed from broad semantic correlations across images to finer-grained, instance-level integration. This has series of consequences: (1) the early inter-image attention may introduce noise or distractions that hinder the models ability to focus on relevant information in later layers; hence, early mistakes in cross-image interaction are harder to correct; (2) the cross-image interaction under causal attention mechanism may lead to error propagation, where tokens belonging to later images cumulate higher amount of noise with incorrect information from earlier images; this may reduce the vision perception capability of the model for later images and explain some of the performance degradation as the number of images increases; (3) the architecture and training objectives may not sufficiently encourage cross-image integration, leading to default behavior of treating images independently; (4) the observed attention patterns may reflect inherent biases in the training data, where the multi-image tasks dont require deep cross-image reasoning, leading the model to learn shortcuts that prioritize single-image understanding. Finding 6: Inter-image attention diminishes in deeper layers of LVLMs, indicating shift from cross-image integration to intra-image focus."
        },
        {
            "title": "4 Method",
            "content": "In the previous section, we identified key limitations of LVLMs on multi-image tasks via zero-shot evaluation using the MIMIC benchmark. Here, we investigate targeted fine-tuning strategies derived from our findings and aimed at improving In particumulti-image reasoning capabilities. lar, we explore two complementary approaches: data-centric fine-tuning strategy using synthetically generated multi-image data, and an optimizationcentric attention-masking strategy. Multi-Image Finetuning: We fine-tune LLaVAOV models on unified training dataset composed of samples procedurally generated using the MIMIC pipeline (see section 3.1) together with the original LLaVA-OV multi-image instructiontuning data (approximately 580K samples). Unlike the MIMIC benchmark used for evaluation, our fine-tuning data is built from OpenImages and provides explicit supervision for cross-image reasoning. It contains approximately 198K samples, with sequence lengths of up to 10 images (see appendix), deliberately exposing models to substantially longer vision-token sequences. All four MIMIC tasks are included to encourage diverse multi-image reasoning behaviors. Attention Masking: Our analysis shows that inter-image attention diminishes in deeper layers 7 Model GPT-4V InternVL2-Llama3-76B LLaVA-v1.5-7B InternVL2-2B InternVL2-8B Qwen2VL-2B Qwen2VL-7B LLaVA-OV-0.5B Ours Ours (masked) LLaVA-OV-7B Ours (masked) MuirBench Blink MMIU MIRB MMT (val) NLVR2 Avg. 53.1 58.2 28.5 25.0 48.6 45.9 60.8 31.8 32.8 28.5 47.2 51.0 54.6 56.8 37.1 16.3 23.4 12.7 17.7 40.4 38.9 39.1 50.4 51.9 - - - 18.9 8.7 41.6 41.5 61.2 68.0 65.1 84.2 87. - - - 24.1 35.6 36.3 46.2 39.3 42.7 41.2 54.2 57.1 62.3 51.2 20.0 24.3 37.9 27.2 43.0 26.8 33.6 32.5 41.7 51.3 64.3 67.4 - 46.7 57.9 51.9 61.7 41.1 45.6 45.9 56.6 55.3 - 44.2 19.2 13.6 36.8 38.7 52.6 34.2 37.2 36.3 45.0 45.5 Table 3: Comparisons on multi-image benchmarks: MuirBench, Blink, MMIU, MIRB, MMT, and NLVR2. Model Mantis-8B-llama3 (Jiang et al., 2024b) InternVL2-2B (Chen et al., 2024a) InternVL2-8B (Chen et al., 2024a) Qwen2VL-2B (Wang et al., 2024b) Qwen2VL-7B (Wang et al., 2024b) LLaVA-OV-0.5B (Li et al., 2024a) Ours Ours (masked) LLaVA-OV-7B (Li et al., 2024a) Ours (masked) Common Counting Odd-one Listing Avg. 15.2 16.6 31.0 29.4 38.4 26.4 45.5 49.4 54.0 63.8 13.0 25.6 45.2 41.9 58.6 44.7 68.5 68.9 71.5 75.5 19.9 11.7 18.9 21.7 35.7 29.7 37.8 35.8 29.7 51.2 10.9 9.6 30.2 30.2 35.9 8.3 41.0 50.9 58.1 72.1 17.0 19.6 29.8 23.8 23.4 22.8 34.5 42.0 56.6 55.0 Table 4: Comparisons on our benchmark. We report models accuracy for Odd-one, Common and Counting whereas f1 score for listing benchmark. (see fig. 4). Motivated by this, we apply layer-wise attention masking during fine-tuning, restricting vision tokens to attend only to tokens from the same image in selected layers, while leaving text-token attention unchanged. This design offers two key benefits. First, it reduces unnecessary cross-image interactions, leading to more efficient model with lower computational cost (see table 7 and fig. 7 of appendix). Second, it encourages cleaner imagelocal representations in deeper layers, which empirically improves performance across several benchmarks. For this setting, we employ LoRA-based fine-tuning to further improve parameter efficiency. See appendix for implementation details."
        },
        {
            "title": "5.1 Comparison with state-of-the-art",
            "content": "Existing multi-image benchmarks. We first report results on MuirBench (Wang et al., 2024a) and its subtasks in table 2. Across all model sizes, our approach consistently outperforms the corresponding LLaVA-OV baseline. Notably, for the 7B model, our masked-attention variant improves the overall score from 41.7 to 51.3%. We observe similar trend for the smaller 0.5B variant, indicating that the improvements are robust across model sizes. Interestingly, our method generalizes well to out-of-domain subtasks, including geographic, 8 action and diagram understanding, suggesting that our data construction strategy teaches the model multi-image processing concepts rather than object perception, which we argue develops already in the single-image training phase. Next, we extend the evaluation to additional multi-image benchmarks, including Blink (Fu et al., 2024b), MMIU (Meng et al., 2024), MIRB (Zhao et al., 2024), MMT (Ying et al., 2024), and NLVR2 (Suhr et al., 2019). Our approach achieves consistent improvements across all variants. As shown in table 5, our masked-attention fine-tuning strategy yields significant gains over the baseline even with very few trainable parameters, and in some cases outperforms full fine-tuning (e.g., LLaVA-OV 0.5B). MIMIC benchmark. We report results in table 4. Unless otherwise specified, all results for Counting subtask correspond to the balanced split. Our method significantly outperforms LLaVA-OV across all four tasks. For the 0.5B model, the average score improves from 26.4 to 49.4, while for the 7B model, masked fine-tuning increases performance from 54.0 to 63.8. Gains are most pronounced on the Common and Odd-One tasks, highlighting improved information aggregation and multi-concept reasoning across images."
        },
        {
            "title": "5.2 Ablation studies and analysis",
            "content": "Cross-task generalization. In this experiment, we train models on individual subtasks (e.g., Counting, Common, Odd-One, and Listing) to analyze their complementary roles and assess cross-task generalization. Table 5 (left) shows the results. We observe that training on the Common task generalizes well to Counting and Listing, but not to Odd-One; similar trend is observed when training on Odd-One. This behavior is expected, as the two tasks are complementary in nature: Common requires aggregating information across multiple images, whereas Odd-One emphasizes localizing distinctive evidence within single image. Training on Listing consistently improves performance across all other tasks, while training on Counting primarily benefits Odd-One. Efficiency analysis. Table 5 (mid) demonstrates that our masked attention variant achieves superior performance with substantially lower computational cost compared with vanilla attention. On the 0.5B backbone, masked finetuning reduces the FLOPs by 81%, while outperforming full fineOurs (all) LLaVA-OV-0.5B only Common only Counting only Odd One only Listing Common Count Odd-one List 34.5 22.8 30.7 20.7 31.1 28.3 68.5 44.7 73.7 35.8 34.4 46.0 41.0 8.3 3.7 12.2 53.6 11.1 37.8 29.7 32.0 39.4 31.8 29. FLOPs (Gain) Avg. Perf. LLaVA-OV-0.5B Ours Ours (masked) 58B (0%) 58B (0%) 11.2B (81%) 26.4 45.5 49.4 Layers masked Comm. Count. Odd. List. Avg. 37.9 44.5 46.1 No mask. 20.9 29.2 37.7 0-23 28.8 33.6 38.1 0-11 50.9 42.0 49.4 12-23 32.0 36.1 27.3 35. 70.0 64.5 62.5 68.9 Table 5: Left: Cross-task generalisation. We train LLaVA-OV model individually with each task and compare cross task generalisation. Ours (with all task): upperbound trained with all 4 tasks. Middle: Efficiency Analysis of Masked Attention. Right: Ablation wrt different layers for attention masking. Experiments on LLaVA-OV 0.5B. tuning. This confirms that selectively constraining inter-image attention is both effective and efficient. See appendix for details on FLOPs estimations. Attention masking strategy. Table 5 (right) ablates the layers at which attention masking is applied. Masking only deeper layers (layers 1223) yields the best performance, whereas masking early layers significantly degrades accuracy. These results suggest that early layers are important for effective cross-image information aggregation. Qualitative analysis. Figure 5 visualizes answerto-image attention for Counting example. The baseline fails to attend to relevant objects in the third image, resulting in an incorrect count. In contrast, our model exhibits balanced and semantically grounded attention across all images, leading to the correct prediction. This qualitative evidence corroborates our quantitative improvements. Figure 5: Answer-to-Image Attention: The baseline LLaVA OV (top row) fails to attend to the potted plant in the third image, whereas our method (bottom row) correctly focuses on the relevant object. Visualization is shown at the 15th layer of the LLM."
        },
        {
            "title": "6 Conclusions",
            "content": "We systematically investigated the capabilities of LVLMs in multi-image contexts through MIMIC, novel benchmark designed to isolate specific unitary behaviors. Our analysis reveals that current SOTA models fundamentally exhibit single9 image behavior, struggling to aggregate information across inputs or track multiple concepts in the presence of visual distractors. To address this, we introduced data-centric synthetic fine-tuning strategy and an optimization-centric attention-masking mechanism. These contributions not only resolve key failure modes but also establish new state-ofthe-art results, offering robust foundation for future research in multi-image understanding."
        },
        {
            "title": "7 Limitations",
            "content": "While our work offers rigorous analysis and effective solutions for multi-image LVLMs, we note the following boundaries of our study: Benchmark Domain: We constructed MIMIC using MS-COCO to maintain precise control over confounding variables (e.g., object counts, occlusion levels). While this design enables exact unit testing of model reasoning, extending this controlled methodology to specialized domains, such as dense documents or medical imaging, remains an exciting avenue for future research. Resolution Trade-offs: Our analysis demonstrates that reducing sequence length improves multi-image reasoning by mitigating context overload. While highly effective for semantic understanding and counting, tasks requiring pixel-perfect perception of extremely small details might benefit from adaptive resolution strategies, which were outside the scope of this study. Architectural Scope: Our proposed analysis focuses on models with open weights. While we expect conclusions to hold for closed models, additional validations (which induce budget constraints) may be useful for reinforcing our findings."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, and 1 others. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: visual language model for few-shot learning. Neural Information Processing Systems. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In IEEE International Conference on Computer Vision. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. MiniGPT-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024a. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, and 1 others. 2024b. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. Neural Information Processing Systems, 36:4925049267. Ailin Deng, Tri Cao, Zhirui Chen, and Bryan Hooi. 2025. Words or vision: Do vision-language models In IEEE Conference on have blind faith in text? Computer Vision and Pattern Recognition. evaluation benchmark of multi-modal LLMs in video analysis. In IEEE Conference on Computer Vision and Pattern Recognition. Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, and 1 others. 2024a. MME-survey: comprehensive survey on evaluation of multimodal LLMs. arXiv preprint arXiv:2411.15296. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, WeiChiu Ma, and Ranjay Krishna. 2024b. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, and 1 others. 2024. HallusionBench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In IEEE Conference on Computer Vision and Pattern Recognition. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, and 1 others. 2024. CogVLM2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500. Drew Hudson and Christopher Manning. 2019. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024a. Mixtral of experts. arXiv:2401.04088. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. 2024b. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, and 1 others. 2023. PaLM-E: An embodied multimodal language model. International Conference on Machine Learning. Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, CJ Taylor, and Stefano Soatto. 2024. Throne: An object-based hallucination benchmark for the free-form generations of large In IEEE Conference on vision-language models. Computer Vision and Pattern Recognition. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, and 1 others. 2025. Video-MME: The first-ever comprehensive Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. diagram is worth dozen images. In European Conference on Computer Vision. 10 Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, and 1 others. 2020. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal on Computer Vision, 128(7):19561981. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, and 1 others. 2023. Obelics: An open web-scale filtered dataset of interleaved imagetext documents. Neural Information Processing Systems. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and 1 others. 2024a. LLaVAOneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024b. Seedbench: Benchmarking multimodal large language models. In IEEE Conference on Computer Vision and Pattern Recognition. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On In IEEE pre-training for visual language models. Conference on Computer Vision and Pattern Recognition. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European Conference on Computer Vision. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In IEEE Conference on Computer Vision and Pattern Recognition. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, and 1 others. 2024c. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision. Springer. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 2263 2279. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. DocVQA: dataset for vqa on document In Winter Conference on Applications of images. Computer Vision. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, and 1 others. 2024. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718. OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5):1. Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. 2024. CLIP-DPO: Visionlanguage models as source of preference for fixing hallucinations in LVLMs. In European Conference on Computer Vision. Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan. 2024. How easy is it to fool your multimodal LLMs? an empirical analysis on deceptive prompts. arXiv preprint arXiv:2402.13220. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. Mathieu Ravaut, Aixin Sun, Nancy Chen, and Shafiq Joty. 2024. On context utilization in summarization with large language models. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27642781. Aditya Sharma, Michael Saxon, and William Yang Losing visual needles in image Wang. 2024. haystacks: Vision language models are easily distracted in short and long contexts. arXiv preprint arXiv:2406.16851. Ilias Stogiannidis, Steven McDonagh, and Sotirios Tsaftaris. 2025. Mind the gap: Benchmarking spatial reasoning in vision-language models. arXiv preprint arXiv:2503.19707. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 64186428. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. In IEEE Conference on Computer Vision and Pattern Recognition. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In IEEE International Conference on Computer Vision. Hang Zhang, Xin Li, and Lidong Bing. 2023. VideoLLaMA: An instruction-tuned audio-visual language arXiv preprint model for video understanding. arXiv:2306.02858. Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. 2024. Benchmarking multiimage understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742. Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, and 1 others. 2025. Mmvu: Measuring expert-level multi-discipline video understanding. In IEEE Conference on Computer Vision and Pattern Recognition. Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. MiniGPT-5: Interleaved vision-and-language genarXiv preprint eration via generative vokens. arXiv:2310.02239. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, and 1 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, and 1 others. 2024a. MuirBench: comprehensive benchmark for roarXiv preprint bust multi-image understanding. arXiv:2406.09411. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024b. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, and 1 others. 2024c. CogVLM: Visual expert for pretrained language models. Neural Information Processing Systems. Xidong Wang, Dingjie Song, Shunian Chen, Junyin Chen, Zhenyang Cai, Chen Zhang, Lichao Sun, and Benyou Wang. 2024d. Longllava: Scaling multimodal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889. Zhaochen Wang, Bryan Hooi, Yiwei Wang, MingHsuan Yang, Zi Huang, and Yujun Cai. 2025. Text speaks louder than vision: Ascii art reveals textual biases in vision-language models. arXiv preprint arXiv:2504.01589. Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph Gonzalez, Trevor Darrell, and David Chan. 2025. Visual haystacks: vision-centric needle-in-a-haystack benchmark. International Conference on Learning Representations. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, and 1 others. 2024. MiniCPMV: GPT-4V level MLLM on your phone. arXiv preprint arXiv:2408.01800. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, and 1 others. 2024. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional analysis Multi-image vs single composite image (stitching). To further understand whether the observed failure modes are due to the presence of multiple images or simply due to the increased number of vision tokens, we conduct an additional experiment involving multi-image stitching: Here, we create single composite image by stitching multiple images together in grid format, ensuring that the total number of vision tokens remains similar to that of the multi-image input. The conceptual changes that occur here are twofold: (1) the chat template is devoid of any image separators, and (2) the vision encoder may jointly process parts of different images together. The results across all tasks are summarized in Table 6. As the results show, general performance remains similar or increases slightly in some cases. Efficiency and FLOPs Analysis. Table 7 shows that our masked attention variant achieves superior performance with substantially lower computational cost compared to vanilla attention. On the 0.5B backbone, masked finetuning reduces FLOPs by approximately 81%, while also outperforming full finetuning. This demonstrates that selectively constraining inter-image attention is both effective and computationally efficient. Formally, given Nt text tokens and Nv visual tokens distributed across independent images, the FLOPs of standard transformer layer with full self-attention scale as O((Nt + Nv)2d + (Nt + Nv)d2), where denotes the hidden dimension. The first term corresponds to self-attention computation, while the second accounts for the MLP. In contrast, our masked attention restricts visual tokens to attend only within their respective image blocks, each of size = Nv , while preserving global visibility for text tokens. This modifies the complexity to (cid:32) FLOPs Nt(Nt + Nv) + (cid:33) (cid:88) n2 (cid:124) i= (cid:123)(cid:122) Masked Attention (cid:125) (1) Assuming uniform image sizes (ni = Nv ), this enables scaling to larger number of high-resolution images under fixed memory and compute budgets. In our MIMIC benchmark, we observe = 10.4 images per sample with an average of Nt = 17.4 Model LLaVA-OV-0.5B LLaVA-OV-0.5B (stitched) LLaVA-OV-7B LLaVA-OV-7B (stitched) Qwen2-VL-2B Qwen2-VL-2B (stitched) Qwen2-VL-7B Qwen2-VL-7B (stitched) InternVL2-2B InternVL2-2B (stitched) InternVL2-8B InternVL2-8B (stitched) Common 33.6 41.0 72.4 68.2 40.4 36.2 61.3 51.7 25.6 29.5 42.8 42. Counting 28.9 30.8 29.1 35.9 21.8 38.8 36.6 35.8 28.8 29.3 30.9 27.3 Odd-one 8.8 22.7 58.0 67.1 26.3 26.8 35.3 51.1 7.1 6.2 24.4 37.3 Listing 25.0 19.5 49.4 51.5 50.8 51.1 50.3 58.5 35.3 39.8 52.9 54.8 Table 6: Stitching Experiment. To ensure similar number of tokens for both stitched and multi-image, we resize images to 384384 for LLaVA-OV and 484484 for Qwen2-VL and InternVL2. LLaVA-OV-0.5B Ours Ours (masked) LLaVA-OV-1.5B Ours Ours (masked) FLOPs (Gain) 58B (0%) 58B (0%) 11.2B (81%) 107B (0%) 107B (0%) 26.7B (75%) Avg. Perf. 26.4 45.5 49.4 29.8 54.7 46.4 Table 7: Efficiency analysis. Our masked-attention variant is substantially more computationally efficient than the vanilla-attention version used in our method. text tokens. Since LLaVA-OV uses 730 visual tokens per image, this yields Nv = 7592. We use hidden dimensions = 896 for the 0.5B model and = 1536 for the 1.5B variant, consistent with the FLOPs reductions reported in Table 7. Performance vs. number of images. In Section 3.2 and Fig. 1, we analyze how the performance of the counting task varies with the total number of images and the number of query images. Here, we extend this analysis by also considering the Listing, Odd-One, and Common tasks. Fig. 6 reports the performance on these three tasks as the number of input images increases. We observe that for the Listing and Odd-One tasks, performance decreases as the number of total images grows, while performance on the Common task remains largely stable and is not affected by the number of images. This behavior is expected, since the Common task contains no distractors and therefore does not require exhaustive reasoning over all images to + (Nt + Nv)d2 predict the correct class. (cid:125) (cid:123)(cid:122) MLP Extended multi-image interaction. Fig. 4 in Section 3.2 reports the normalized attention scores from each vision token to all other vision tokens in an input sequence of 4 images for LLaVA-OV model. The scores are computed on subset of 50 samples and then averaged. Fig. 9 extends this analysis to an input sequence of 6 images. From (cid:124) . Figure 6: Performance vs. Number of Images. We report performance as function of the total number of input images for the Listing, Odd-One tasks, and common tasks. the figures, we observe that the observations made for 4 images also hold for 6 images. In particular, in the early layers, there is large amount of inter-image attention, while in deeper layers the attention becomes mostly intra-image, indicating that the model focuses more on individual images. Therefore, we can infer that the observed behavior is intrinsic to the model and does not depend on the number of input images. Counting (balanced) Performance Comparison. We extend the results from table 4 with finegrained analysis of performance improvements in fig. 8. We observe that fine-tuning substantially improves performance when object instances are distributed across multiple query images. For example, when four instances are spread across four images, accuracy increases from 9% to 45.8%. Similar gains are observed across different instance distributions, indicating improved cross-image information aggregation. Extended Performance Comparison with LLaVA-OV 1.5B. We extend the comparison to multi-image benchmarks from table 3 by including the LLaVA-OV 1.5B model. We observe that both of our fine-tuned models outperform the baseline. In particular, our fine-tuned model achieves 3.4% improvement over the baseline, demonstrating enhanced multi-image reasoning capabilities. Bigger and latest models. Fig. 10 reports results on the Counting task by increasing the number of images that contain the object instance from 1 to 7. We consider larger (LLaVA-OV 72B) or more recent (Qwen2.5-7B, Qwen3-VL-8B) models compared to those analyzed in Fig. 1 (left). Consistent with previous findings outlined in Section 3.2, even more powerful models achieve strong performance when the object of interest appears in single query image, but performance decreases as the same object is spread across multiple images. Model GPT-4V InternVL2-Llama3-76B (Chen et al., 2024a) LLaVA-v1.5-7B InternVL2-2B (Chen et al., 2024a) InternVL2-8B (Chen et al., 2024a) Qwen2VL-2B (Wang et al., 2024b) Qwen2VL-7B (Wang et al., 2024b) LLaVA-OV-0.5B (Li et al., 2024a) Ours Ours (masked) LLaVA-OV-1.5B (Li et al., 2024a) Ours Ours (masked) LLaVA-OV-7B (Li et al., 2024a) Ours (masked) MuirBench Blink MMIU MIRB MMT (val) NLVR2 Avg. 53.1 58.2 28.5 25.0 48.6 45.9 60.8 31.8 32.8 28.5 37.7 36.0 30.6 47.2 51.0 - - - 24.1 35.6 36.3 46.2 39.3 42.7 41.2 42.8 46.2 43.0 54.2 57.1 - 44.2 19.2 13.6 36.8 38.7 52.6 34.2 37.2 36.3 33.4 38.9 35.5 45.0 45. 64.3 67.4 - 46.7 57.9 51.9 61.7 41.1 45.6 45.9 47.5 48.8 48.1 56.6 55.3 62.3 51.2 20.0 24.3 37.9 27.2 43.0 26.8 33.6 32.5 31.1 39.7 32.3 41.7 51.3 54.6 56.8 37.1 16.3 23.4 12.7 17.7 40.4 38.9 39.1 36.4 40.0 42.2 50.4 51.9 - - - 18.9 8.7 41.6 41.5 61.2 68.0 65.1 70.9 73.7 69.0 84.2 87.3 Table 8: Extended Comparisons including LLaVA-OV 1.5B with the state-of-the-art on multi-image benchmarks: MuirBench, Blink, MMIU, MIRB, MMT and NLVR2. Figure 7: Our Masked Attention. Vision tokens are restricted to attend only to tokens from the same image, following block-diagonal attention pattern, while text tokens in both the prefix and suffix follow the standard autoregressive attention. A.2 Implementation Details. Training Data. Our method is trained on unified training set composed of synthetic samples generated using the MIMIC pipeline (see Section 3.1) together with the original LLaVA-OV multi-image instruction-tuning data (approximately 580K samples). Unlike the MIMIC benchmark 14 masking strategy, we increase the learning rate to 2.5 1e5. Training is performed on 8 NVIDIA H100 GPUs with approximately 80GB memory each. For the attention-masking strategy, we set the LoRA rank to 128. A.3 Additional MIMIC details Counting Common Odd One Listing Overall Queries Images Obj. inst. per query Min img per query Max img per query Median img per query Avg words per question 50000 157995 27.3 2 10 5 48.9 50000 80438 16.3 2 8 4 44.6 47561 81642 17.8 3 8 4 47.7 49267 196828 108300 18.6 2 8 4 52.1 20.0 2 10 4 48.3 Table 9: MIMIC synthetic training dataset statistics based on OpenImagesv7 (Kuznetsova et al., 2020). Prompt templates Fig. 12 shows the different prompt templates used for the four tasks. For each task, prompt is constructed by randomly sampling one template from the task-specific template set (Ptask) and one from the connector template set (Pconnector). The two templates are then combined to form the final prompt, i.e., = PtaskPconnector. Additional examples We report additional sample instances from the Common, Odd-One, Listing, and Counting categories, in Figs. 13, 14, 15 and 16, respectively. Figure 8: Comparison on Counting (balanced). Our fine-tuned model performs substantially better when object instances are distributed across multiple images. For example, when four instances are spread across four images, performance improves from 9% to 45.8%, indicating enhanced cross-image information aggregation. used for evaluation, the synthetic MIMIC training dataset is built from OpenImages (Kuznetsova et al., 2020) annotations and provides explicit supervision for cross-image reasoning. Additionally, it supports multi-turn conversations and option-based responses (see fig. 11). Dataset statistics are reported in table 9. In particular, the dataset consists of approximately 50K samples from each MIMIC subtask, with sequences containing up to 10 images. This design encourages learning effective cross-image information aggregation while retaining general vision-language capabilities through joint training with LLaVA-OV data. Training Procedure. For both training strategiesthe data-centric fine-tuning approach and the optimization-centric attention-masking approachwe start from the LLaVA-OV SingleImage variant (Stage 2.1) (Li et al., 2024a), which has been pre-trained on high-quality single-image instruction-tuning data. We freeze the vision encoder and train only the language model layers and the vision-to-language projector. For the optimization-centric attention-masking strategy, we do not fully fine-tune the language model layers; instead, we apply LoRA adapters to these layers. In addition, we introduce attention masking in the self-attention blocks, restricting vision tokens to attend only to tokens from the same image. For the data-centric fine-tuning strategy, we fully fine-tune the language model layers without attention masking. All models are trained with an effective batch size of 128, zero weight decay, and cosine learning rate schedule with warmup ratio of 0.03. For the data-centric approach, we use learning rate of 2.5 1e6, while for the LoRA-based attention15 Figure 9: Inter-image and intra-image token attention across layers for 6 images. Figure 10: Counting (unbalanced) performance with bigger and latest models. We analyze the trade-off between the number of query images and the total number of images for bigger (LLaVA-OV 72B) and latest models (Qwen2.5-7B and Qwen3VL-8B). Figure 11: Samples from the MIMIC training dataset.Unlike the evaluation benchmark, the training data follows the LLaVA data format and includes multi-turn conversations with option-based answers. 16 Figure 12: Prompt Templates for Different Tasks. For each task, prompt is constructed by randomly sampling one template from the task-specific template set (Ptask) and one from the connector template set (Pconnector). The two templates are then combined to form the final prompt, i.e., = PtaskPconnector. Figure 13: Our MIMIC Evaluation Benchmark. We show some samples from the category common from our benchmark. 17 Figure 14: Our MIMIC Evaluation Benchmark. We show some samples from the category Odd One from our benchmark. Figure 15: Our MIMIC Evaluation Benchmark. We show some samples from the category Listing from our benchmark. 18 Figure 16: Our MIMIC Evaluation Benchmark. We show some samples from the category Counting from our benchmark."
        }
    ],
    "affiliations": [
        "MPI for Informatics, Saarland Informatics Campus",
        "Queen Mary University of London, UK",
        "Samsung AI, Cambridge",
        "Technical University of Iasi, Romania"
    ]
}