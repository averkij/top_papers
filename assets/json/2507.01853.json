{
    "paper_title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages",
    "authors": [
        "Samridhi Raj Sinha",
        "Rajvee Sheth",
        "Abhishek Upperwal",
        "Mayank Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 5 8 1 0 . 7 0 5 2 : r EKA-EVAL : Comprehensive Evaluation Framework for Large Language Models in Indian Languages Samridhi Raj Sinha(cid:51), Rajvee Sheth(cid:51), Abhishek Upperwal, Mayank Singh(cid:51) NMIMS, Soket AI, Indian Institute of Technology Gandhinagar, (cid:51)LINGO Research Group Correspondence: singh.mayank@iitgn.ac.in"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond Englishcentric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, longcontext understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multiGPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish robust, multilingual evaluation ecosystem for LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have rapidly transformed natural language processing (NLP), enabling impressive generalization across diverse tasks including instruction following, reasoning, summarization, translation, and tool use. With the advent of general-purpose foundation models such as GPT-4 (Achiam et al., 2023), Claude (Anthropic, 2024), Gemini (Anil et al., 2023) and Llama-3 (Touvron et al., 2023), the focus of research has increasingly shifted from building task-specific models to systematically evaluating these powerful systems. Evaluation plays critical role not only in measuring progress but in identifying capabilities, 1 exposing limitations, and informing deployment strategies. In response, several evaluation frameworks have emerged, including HELM (Liang et al., 2022), EleutherAI Harness (Gao et al., 2021), lm-evalharness (Gao et al., 2021), and OpenCompass (OpenCompass Contributors, 2023). However, these frameworks are primarily centered on English or other high-resource languages and offer limited support for low-resource or multilingual settings. This lack of inclusivity significantly limits the effectiveness of such tools in linguistically diverse regions like India, which has 22 constitutionally recognized languages and more than billion native speakers. While notable benchmarks like IndicGLUE (Kakwani et al., 2020), IndicGenBench (Singh et al., 2024), and MILU (Verma et al., 2024) have addressed some gaps in Indic-language evaluation, they remain siloed efforts, lacking integration into unified evaluation workflows. Furthermore, popular frameworks like FreeEval (Yu et al., 2024) and lm-eval-harness (Gao et al., 2021) require extensive configuration and engineering expertise, limiting their adoption among developers and researchers operating in low-resource environments. These challenges create need for an accessible, multilingual, and task-diverse evaluation suite. To address these limitations, we introduce EKAEVAL, unified, extensible, and ready-to-use evaluation framework for LLMs that integrates more than 35 benchmarks spanning both global (English) and Indic-language tasks. EKA-EVAL is designed to be modular, easy to configure, and compatible with HuggingFace and proprietary models, offering plug-and-play usability with minimal overhead. To contextualize the capabilities of EKA-EVAL, Table 1 presents feature-level comparison of leading LLM evaluation frameworks. The comparison spans 13 dimensions, including support for longcontext and tool-use benchmarks, multilingual (InTable 1: Comparison of LLM Evaluation Frameworks Framework Custom Datasets Custom Models Custom Prompting Long Context Tool Use Distributed Inference Visual Analysis Multilingual Indic Production Optimization Interactive CLI MultiGPU Quantization LM-Harness OpenCompass HELM OpenAI Evals DeepEval FreeEval indic-eval EKA-EVAL Ready to Use Medium Medium Low Low Medium Low High High dic) tasks, production readiness, and ease of use. EKA-EVAL emerges as unique blend of extensibility, multilingual support, and practical usability, combining strengths found only partially across other toolkits. EKA-EVAL covers eight major evaluation categories: (i) General question answering, (ii) Mathematics and logical reasoning, (iii) Commonsense inference, (iv) World knowledge, (v) Reading comprehension, (vi) Long-context understanding, (vii) tool use and API reasoning, and (viii) Indic-specific NLP benchmarks. It includes popular benchmarks such as ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), SQuAD (Rajpurkar et al., 2018), APIBench (Patil et al., 2024), IndicGenBench (Singh et al., 2024). Our key contributions are: We propose EKA-EVAL, unified and modular evaluation framework that supports both global and Indic-language benchmarks, covering diverse LLM capabilities including longcontext reasoning and tool use. We release the framework as an open-source toolkit, designed for ease of use in real-world, low-resource, and multilingual settings to promote broad adoption and community-driven development. Our key contributions are: We propose EKA-EVAL, unified and modular evaluation framework that supports both global and Indic-language benchmarks, covering diverse LLM capabilities including longcontext reasoning and tool use. We release the framework as an open-source toolkit, designed for ease of use in real-world, low-resource, and multilingual settings to promote broad adoption and community-driven development."
        },
        {
            "title": "2 Related Work",
            "content": "The evaluation of large language models (LLMs) has evolved from task-specific benchmarks to comprehensive, modular frameworks that support diverse capabilities and deployment settings. This section categorizes prior work into three major strands: general-purpose evaluation frameworks, specialized capability benchmarks, and multilingual/Indic evaluations. We conclude by highlighting how EKA-EVAL integrates and advances these directions. General-Purpose LLM Evaluation Frameworks Early benchmark suites such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) laid the groundwork for multi-task evaluation, but were primarily English-centric and biased toward high-resource languages. Frameworks like HELM (Liang et al., 2022) expanded evaluation to multidimensional axesaccuracy, calibration, robustness, fairness, and efficiencyapplied uniformly across 30+ models. Similarly, BIGBench (Srivastava et al., 2022) curated over 200 tasks via crowd-sourced contributions, highlighting model generality but with limited support for multilingualism or real-world deployment constraints. Tooling infrastructure also emerged in parallel. lm-eval-harness (Gao et al., 2021) provided reproducible few-shot evaluation pipelines, while OpenAIs Evals (OpenAI Contributors, 2023) offered YAML-based structured evaluation for internal alignment research. Recent frameworks emphasize modularity, extensibility, and accessibility. For instance, OpenCompass OpenCompass Contributors (2023) integrates distributed inference and real-time leaderboards, lacks Indic language support. FreeEval Yu et al. (2024) introduces metaevaluation and contamination detection. DeepEval DeepEval Contributors (2024) supports longcontext and tool-use tasks but has limited multilingual and low-resource customization. though it 2 Specialized Capability Benchmarks To evaluate emerging LLM capabilities, task-specific frameworks and datasets have been proposed. Tooluse and agentic behavior are assessed in ToolBench (Qin et al., 2023) and API-Bank (Li et al., 2023), which evaluate model interactions with real-world APIs. Long-context reasoning is explored in InfiniteBench (Zhang et al., 2024) and RULER (Hsieh et al., 2024), which evaluate memory and coherence over 100K+ token sequences. Mathematical reasoning and code generation are evaluated using datasets like GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021), respectively. While powerful, these benchmarks have limited capabilities and lack integration into unified evaluation pipelines. Multilingual and Indic Language Evaluation Multilingual evaluation has gained prominence through benchmark suites such as XTREME (Hu et al., 2020), xP3 (Muennighoff et al., 2022), and MEGA (Ahuja et al., 2023), covering 4070+ languages. However, these benchmarks often rely on automatic translation, which may not reflect natural language usage or cultural nuances. FLORES (Goyal et al., 2022) improved translation evaluation with high-quality parallel corpora for lowresource languages. For Indian languages, IndicGLUE (Kakwani et al., 2020) and IndicXTREME (Kakwani et al., 2020) pioneered NLU benchmarking across major Indic languages. IndicGenBench (Singh et al., 2024) broadened the scope to generation tasks over 29 Indic languages, but remains dataset collection rather than complete evaluation framework, lacking system-level extensibility or plug-and-play usability. Frameworks such as indic-eval (IndicEval Contributors, 2024) wrap existing tools like LightEval to support select Indic tasks. However, they offer limited extensibility and lack support for long-context tasks, tool-use evaluation, and custom promptingfeatures increasingly essential in real-world use cases. EKA-EVAL is unified, extensible evaluation framework that integrates the breadth of multilingual and capability-centric benchmarks within single, production-ready system. It supports wide spectrum of evaluation settings, including both global and Indic benchmarks spanning 29+ Indian languages, while accommodating advanced model behaviors such as long-context reasoning and agentic tool use. Designed for practical deployment and large-scale experimentation, Eva-Eval includes built-in support for multi-GPU inference, quantized model evaluation, distributed execution, and customizable prompting strategies. In contrast to existing frameworks that are either limited to specific capabilities or lack multilingual extensibility, Eva-Eval offers modular, plug-and-play architecture that enables end-to-end evaluation of LLMs across diverse languages, domains, and task configurationsbridging critical gaps in current LLM assessment infrastructures."
        },
        {
            "title": "Frameworks",
            "content": "We identify thirteen foundational capabilities that are critical for the design and continuous advancement of modern LLM evaluation frameworks. Some of these capabilities are described in earlier evaluation frameworks such as ....: Custom Datasets: Support for loading and evaluating user-defined datasets beyond standard benchmark corpora. Custom Models: Compatibility with wide range of models, including local checkpoints and API-hosted endpoints. Custom Prompting: Provision for flexible, template-based prompting paradigms encompassing zero-shot, few-shot, and chain-ofthought configurations. Long Context: Ability to process and evaluate tasks involving extended input contexts (e.g., exceeding 4,000 tokens). Tool Use: Evaluation of LLMs exhibiting agent-like behavior, including tool use, external API calls, and autonomous multi-step decision-making. Distributed Inference: Support for parallelized evaluation across multiple processes or compute nodes. Visual Analysis: Generation of interpretable visualizations, including bar charts, radar plots, and heatmaps, to facilitate comparative analysis. Multilingual Indic: Native support for evaluation on benchmarks in Indic languages. Production Optimization: Implementation of runtime optimizations such as batching, caching, and prompt reuse to enhance evaluation efficiency. Interactive CLI: Availability of commandline interface for interactive configuration of datasets, models, prompting strategies, and 3 Figure 1: End-to-end architecture of the EKA-EVAL framework. visualizations. 4.1 System Architecture Multi-GPU: Built-in capability to leverage multiple GPUs for scalable and efficient inference. Quantization: Compatibility with quantized model formats (e.g., 8-bit, 4-bit) to minimize memory requirements. Ready to Use: Emphasis on ease of use with quick setup and configured tasks and models. Table 1 presents comparative analysis of EKAEVAL and seven widely used LLM evaluation frameworks across the thirteen capabilities outlined in this work. EKA-EVAL demonstrates robust support across all key capabilities."
        },
        {
            "title": "4 Design and Implementation",
            "content": "EKA-EVAL is architected as modular, extensible evaluation framework that balances comprehensive benchmark coverage with practical usability. The system is designed around three core principles: modularity for easy extension and customization, accessibility for low-barrier adoption across diverse research environments, and comprehensiveness to address the full spectrum of LLM capabilities, including underserved areas like long-context reasoning and tool use. The framework follows layered architecture consisting of four primary components: the Evaluation Engine, Benchmark Registry, Model Interface Layer, and Results Processing System, each containing specialized secondary components that handle specific functionality. This hierarchical design enables seamless integration of new benchmarks, model backends, and evaluation metrics while maintaining backwards compatibility and reproducibility. 4.1.1 Evaluation Engine The Evaluation Engine serves as the orchestration layer, comprising three key secondary components: Task Scheduler: Manages task scheduling, prompt formatting, and result aggregation across distributed inference setups. The scheduler implements intelligent work distribution as demonstrated in main_orchestrator(), which dynamically assigns evaluation tasks to available workers based on resource constraints and model requirements. Batch Optimizer: Implements intelligent batching strategies and supports various quantization schemes to optimize memory usage and inference speed. The optimizer automatically adjusts generation_batch_size parameters to maximize 4 throughput while preventing out-of-memory errors, as seen in the PIQA evaluation implementation. Distributed Coordinator: Coordinates evaluation across multiple GPUs and workers using Pythons multiprocessing library. The coordinator launches multiple worker_process instances that handle independent evaluation tasks, enabling parallel execution across different benchmarks and model configurations. 4.1.2 Benchmark Registry The Benchmark Registry provides unified interface for benchmark management through two main secondary components: Dataset Manager: Handles diverse dataset formats and sources through the BenchmarkRegistry class, which abstracts complexities of different The manager supports evaluation protocols. datasets from HuggingFace Hub, local files, and custom data formats through standardized interfaces. 4.1.3 Model Interface Layer The Model Interface Layer provides unified abstraction across local and API-based models through four secondary components: Local Model Loader: Initializes models using HuggingFace transformers with automatic device allocation, quantization support, and memory optimization. The loader handles diverse model formats while maintaining consistent interfaces. endpoints API Client Manager: Manages proprietary clients dedicated through (OpenAIClient, GeminiClient, ClaudeClient) that extend BaseAPIClient, providing unified request/response handling with rate limiting and authentication. Interactive Selection Interface: Implements get_model_selection_interface() dyfor namic model discovery and selection, supporting both local model paths and API configurations with environment variable management. Resource Manager: Ensures efficient memory management through explicit cleanup functions, preventing resource leaks during repeated evaluations and supporting smooth transitions between different model configurations. 4.1.4 Results Processing System The Results Processing System handles comprehensive output management through three secondary components: Metrics Calculator: Computes evaluation metrics and statistical significance using HuggingFaces evaluate library (e.g., accuracy, BLEU, F1, exact match, Pass@1). It also implements robust error handling for edge cases and missing data, for example, when model returns The answer is probably B, regex-based extraction retrieves the label; if that fails, default score is assigned. Code completions are sandboxed with timeout control to ensure safe execution and error logging. Visualisations analytics: Provides rich analytics including confidence intervals, comparative analysis across multiple models and benchmark configurations. Multiple visualisations like bar charts, heatmaps and radar plots can be generated including support for cross-model performance comparisons. Export Manager: Handles result export in multiple formats including JSON, CSV reports to support analysis and visualization workflows. The manager maintains evaluation metadata including model parameters, benchmark versions, execution timestamps, and system configurations to ensure reproducibility across different experimental setups. 4.2 Comprehensive Benchmark Coverage EKA-EVAL covers eight major evaluation categories with comprehensive benchmark support across both English and Indic languages: foundational capabilities, we General Reasoning and Knowledge: For include MMLU (Hendrycks et al., 2020) and MMLUPro (Wang et al., 2024) for multitask language understanding, IFEval (Zhou et al., 2023) for instruction following, BBH (3-shot) (Suzgun et al., 2022) for challenging reasoning tasks, and AGI-Eval (3-5 shot) (Zhong et al., 2023) for general intelligence assessment. Mathematics and Logical Reasoning: Mathematical capabilities are evaluated through GSM8K (Cobbe et al., 2021) for grade school math, MATH (Hendrycks et al., 2021) for competitionlevel problems, and ARC-Challenge (Clark et al., 2018) for scientific reasoning. Code Generation and Programming: Programming abilities are assessed using Hu5 manEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEval+ (Liu et al., 2023), MBPP EvalPlus (Liu et al., 2023) with Pass@1 accuracy metrics. Commonsense Reasoning: We incorporate PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), ARC Easy and Challenge (Clark et al., 2018), WinoGrande (Sakaguchi et al., 2021), CommonSenseQA (Talmor et al., 2018), and OpenBookQA (Mihaylov et al., 2018) for comprehensive commonsense evaluation. World Knowledge: Factual knowledge is tested through TriviaQA (5-shot) (Joshi et al., 2017) and NaturalQuestions (5-shot) (Kwiatkowski et al., 2019) with accuracy metrics. Reading Comprehension: Text understanding capabilities are evaluated using SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019) with F1 scores and exact match metrics. Long-Context Understanding: For extended context reasoning, we include ZeroSCROLLS (Shaham et al., 2023) with ROUGE and F1 metrics, Needle-in-a-Haystack for retrieval accuracy, and InfiniteBench (Zhang et al., 2024) for task-specific long-context evaluation. Tool Use and API Reasoning: Practical capabilities are assessed through API-Bank (Li et al., 2023) for API call accuracy and ROUGE-L and API-Bench (Patil et al., 2024) for API recommendation accuracy. 4.3 Multilingual and Indic Language Support distinguishing feature of EKA-EVAL is its comprehensive support for Indic languages, addressing critical gap in existing evaluation frameworks. The system includes dedicated benchmarks for major Indian languages including Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, and Odia. For multilingual evaluation, we support popular Indic language benchmarks including MMLU-IN, TriviaQA-IN, MILU (Verma et al., 2024), GSM8K-IN, BoolQIN, ARC-C-IN, Flores-IN with BLEU (Papineni et al., 2002) and ChrF metrics, XQuAD-IN and XorQA-IN with F1 and exact match scores. The multilingual architecture includes languagespecific prompt templates, culturally-appropriate evaluation protocols, and specialized tokenization handling for Indic scripts. This ensures fair and accurate evaluation across linguistic boundaries while maintaining consistency with global benchmarks. 4.4 Extensibility and Customization The framework is designed with extensibility as first-class concern. New benchmarks can be integrated through simple plugin architecture that requires minimal boilerplate code. The system supports custom evaluation metrics, prompt templates, and post-processing pipelines, enabling researchers to adapt the framework to their specific requirements. Configuration management is handled through hierarchical system supporting JSON formats, allowing users to define evaluation suites ranging from quick smoke tests to comprehensive benchmark runs. The configuration system supports parameter sweeping, enabling systematic exploration of prompt variations, few-shot examples, and model hyperparameters."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we introduced EKA-EVAL, unified and extensible framework for the evaluation of large language models across both global and Indic tasks. By supporting diverse set of benchmarks and accommodating multiple model backendsincluding local checkpoints and APIbased deploymentsEKA-EVAL enables reproducible, scalable, and backend-agnostic evaluation pipelines. Looking ahead, we plan to expand the framework to support over 100 tasks, with particular emphasis on underrepresented Indic languages and culturally grounded evaluations. Future extensions will include categorization of benchmarks by language diversity, task difficulty, and professional domains such as law, healthcare, and governance. We aim to introduce dynamic task calibration mechanisms that vary context length, ambiguity, and reasoning complexity, as well as India-specific knowledge grounding through tasks on government schemes and public health challenges. Additional priorities include designing benchmarks for bias detection, hallucination analysis, and privacy risk assessment; incorporating human-centered meta-evaluation; supporting domain-specific evaluation; and enabling adversarial human review. Finally, we plan to support phase-wise evaluationspanning pretraining, fine-tuning, and deployment stagesto offer comprehensive view of model behavior across the entire lifecycle. These enhancements will be included in EKAEVAL v2.0, strengthening its role as the evaluation 6 standard for Indic and other LLMs."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, and 1 others. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528. Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Anthropic. 2024. Claude 3.5 sonnet model card https://www-cdn.anthropic.com/ addendum. fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf. Addendum to the Claude 3 Model Card. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, and 1 others. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. arXiv preprint arXiv:1808.07036. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. DeepEval Contributors. 2024. DeepEval. https:// github.com/confident-ai/deepeval. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, and 1 others. 2021. framework for few-shot language model evaluation. Version v0. 0.1. Sept, 10:8 9. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International conference on machine learning, pages 44114421. PMLR. IndicEval Contributors. 2024. IndicEval. https:// github.com/adithya-s-k/indic_eval. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul NC, Avik Bhattacharyya, Mitesh Khapra, and Pratyush Kumar. 2020. Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages. In Findings of the association for computational linguistics: EMNLP 2020, pages 49484961. 7 Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, and 1 others. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, and 1 others. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786. OpenAI Contributors. 2023. OpenAI Evals. https: //github.com/openai/evals. Accessed: 2025-0701. OpenCompass Contributors. 2023. OpenCompass: Universal Evaluation Platform for Foundation https://github.com/open-compass/ Models. OpenCompass. Accessed: 2025-07-01. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, and 1 others. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: zeroshot benchmark for long text understanding. arXiv preprint arXiv:2305.14196. Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, and Partha Talukdar. 2024. Indicgenbench: multilingual benchmark to evaluate generation capabilities of llms on indic languages. arXiv preprint arXiv:2404.16816. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, and 1 others. 2022. Beyond the imitation game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and 1 others. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, and Jaydeep Sen. 2024. Milu: multi-task indic language understanding benchmark. arXiv preprint arXiv:2411.02538. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. 8 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, and Shikun Zhang. 2024. Freeeval: modular framework for trustworthy and efficient evaluation of large language models. arXiv preprint arXiv:2404.06003. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and 1 others. 2024. ınftybench: Extending long context evaluation beyond 100k tokens. In ACL (1). Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Instruction-following evaluand Le Hou. 2023. ation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Indic benchmark configuration This figure showcases sample configuration used to evaluate the ARC-Challenge-Indic benchmark across 11 Indic languages. It illustrates how task parameters, templates, and dataset references are modularly specified in EKA-EVAL. \"ARC-Challenge-Indic\": { \"description\": \"Zero-shot evaluation across 11 Indic languages\", \"evaluation_function\": \"indic.arc_c_in.evaluate_arc_c_in\", \"task_args\": { \"dataset_name\": \"sarvamai/arc-challenge-indic\", \"target_languages\": [\"bn\", \"en\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"], \"dataset_split\": \"validation\", \"num_few_shot\": 0, \"max_new_tokens\": 10, \"generation_batch_size\": 8, \"prompt_template_name_zeroshot\": \"arc_c_in_0shot\", \"prompt_template_name_fewshot\": \"arc_c_in_5shot\", \"prompt_file_benchmark_key\": \"arc_c_in\", \"prompt_file_category\": \"indic\" } } Figure 2: ARC-Challenge-Indic benchmark configuration example A.2 Demonstration The interactive CLI of the EKA-EVAL framework is shown below, which guides users through model selection and evaluation setup. It simplifies benchmarking workflows, making it accessible to both researchers and developers. Figure 3: Available benchmarks groups of EKA-EVAL framework. Users are prompted to select high-level task groups (e.g., Reading Comprehension) during CLI 9 setup. This enables fine-grained benchmarking organization and streamlined selection. vides intuitive insight into strengths and weaknesses of the model. Figure 4: Model selection in the EKA-EVAL framework. EKA-EVAL supports local HuggingFace models and API-based models like OpenAI, Claude, and Gemini. Users interactively select model source and configuration through CLI. Figure 5: Subtask selection within task group. After selecting task group, users choose specific benchmarks such as SQuAD, BoolQ, or QuAC for focused evaluation within that domain. A.2.1 Prompt Template System critical component of EKA-EVAL is its sophisticated prompt management system that handles diverse evaluation paradigms and languages. The framework implements flexible template system demonstrated through PIQA benchmark prompt 9: { \"piqa_generation\": { \"template\": \"Choose the most appropriate solution (0 or 1) to achieve the goal: nnQuestion: {goal}n0) {sol1} n1) {sol2}nAnswer:\", \"description\": \"Generation-based PIQA prompt\" }, \"piqa_5shot_generation\": { \"template_prefix\": \"Choose the most appropriate solution (0 or 1)...\", \"few_shot_example_template\": \"Question: {goal}n0) {sol1} n1) {sol2}nAnswer: {answer_label}\", \"few_shot_separator\": \"nn\", \"template_suffix\": \"Question: {goal} n0) {sol1}n1) {sol2}nAnswer:\", \"description\": \"Few-shot generation prompt\" }, \"default_few_shot_examples_piqa\": [ Figure 6: Consolidated evaluation results table. The CLI displays final benchmark scores for each model in tabular format, including per-task and average scores. Results are also exported as CSV. { } ] } \"goal\": \"To remove stain from clothing\", \"sol1\": \"Apply cold water immediately...\", \"sol2\": \"Set the clothing on fire...\", \"answer_label\": \"0\" Figure 9: PIQA prompt templates supporting multiple evaluation paradigms The template system supports zero-shot, fewshot, and chain-of-thought prompting strategies, ensuring consistency across evaluation modes and languages. Users can customize prompt strategies and easily configure them in the benchmark configuration file, as shown in Figure 2."
        },
        {
            "title": "B Limitations",
            "content": "While EKA-EVAL supports wide range of benchmarks and model backends, it currently lacks graphical user interface, relying instead on CLIbased workflows. The framework does not yet support vLLMs and provides limited support for detailed error analysis or explainability. Additionally, reproducibility may be affected by changes in external datasets or model versions unless explicitly versioned or cached. Figure 7: Interactive visualisation setup in EKA-EVAL. The framework allows users to generate multiple types of visualizations : bar charts, heatmaps, radar plots that are based on completed evaluations. Figure 8: Bar chart visualisation This chart shows performance breakdown across sub-tasks like BoolQ, SQuAD, and QuAC. It pro-"
        }
    ],
    "affiliations": [
        "Indian Institute of Technology Gandhinagar",
        "LINGO Research Group",
        "NMIMS",
        "Soket AI"
    ]
}