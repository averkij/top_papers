{
    "paper_title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs",
    "authors": [
        "Xinzhe Zheng",
        "Hao Du",
        "Fanding Xu",
        "Jinzhe Li",
        "Zhiyuan Liu",
        "Wenkang Wang",
        "Tao Chen",
        "Wanli Ouyang",
        "Stan Z. Li",
        "Yan Lu",
        "Nanqing Dong",
        "Yang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 1 0 1 5 0 . 7 0 5 2 : r PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs Xinzhe Zheng1 Hao Du2 Fanding Xu1,7 Jinzhe Li2,5 Zhiyuan Liu1 Wenkang Wang1 Tao Chen5 Wanli Ouyang2,6 Stan Z. Li3 Yan Lu2,6 Nanqing Dong2,4 Yang Zhang1 1National University of Singapore 2Shanghai Artificial Intelligence Laboratory 3Westlake University 4Shanghai Innovation Institute 5Fudan University 6The Chinese University of Hong Kong 7Xian Jiaotong University"
        },
        {
            "title": "Abstract",
            "content": "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking models capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates PRotein-protein INteraction prediction from Graph-level perspective. PRING curates high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topologyoriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the models capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING."
        },
        {
            "title": "Introduction",
            "content": "Protein-protein interactions (PPIs) refer to the physical or functional association between proteins, which is central to most biological processes, such as signal transduction [1], gene regulation [2], and immune response [3]. They also play key role in the study of disease mechanisms [4]. For instance, the interactions between certain cellular proteins can trigger abnormal signaling pathways that promote tumor growth and metastasis in cancer [5]. By revealing those types of PPI, small molecules are developed to disrupt the interactions, thereby inducing cancer cell death [6]. Therefore, characterizing the organization of PPIs is fundamental for understanding the molecular basis of health and advancing drug discovery in precision medicine [7, 8]. Equal contribution. Corresponding authors. Preprint. Figure 1: Overview of PRING benchmark. (a) Diverse databases are used to construct the PRING. (b) PRING includes two topology-oriented tasks and three function-oriented tasks for extensive evaluation. (c) List of baseline models, consisting of sequence similarity-based, naive sequence-based, structurebased, and PLM-based methods. (d) Evaluation metrics used for each task in the PRING. Experimental techniques are adopted to identify PPIs in traditional biological research, including instrument-based methods such as X-ray crystallography [9, 10] and cryo-electron microscopy [11, 12], as well as high-throughput approaches like yeast two-hybrid [13, 14] and cross-linking mass spectrometry [15, 16]. Nevertheless, these methods are limited by low scalability and incomplete coverage of the interactome. The rise of deep learning provides powerful computational alternatives for PPI identification [17, 18, 19, 20], which designs neural networks to capture structural and sequence patterns of proteins to predict PPIs and demonstrates promising performance. More recently, the development of AlphaFold [21, 22, 23] and protein language models (PLMs) [24, 25, 26, 27] has led to remarkable breakthroughs in PPI prediction, enabling more accurate and large-scale identification of PPI directly from protein sequences. Although current deep learning-based PPI prediction models achieve high pairwise accuracy on standard benchmarks [28, 29, 30, 31, 32, 17], these evaluations treat each interaction in isolation and overlook how individual predictions contribute to the cohesive PPI networks. While some studies [33, 17] have applied these models to reconstruct PPI networks in specific contexts (e.g., bovine rumen), systematic and fair evaluation of network-level performance remains absent. Since PPI networks support biological insights from both topological and functional perspectives [34, 35], it is important to evaluate how well models recover both aspects. Topologically, PPI networks exhibit properties such as sparsity and local communities, which reflect the modular organization of cellular processes. Functionally, they offer foundation for annotating uncharacterized proteins and discovering coherent biological modules. Hence, we pose the following research question: How well do current PPI models demonstrate their pure capabilities in recapitulating the structural and functional features of PPI networks? To fill this blank, we propose PRING, multi-organisms holistic benchmark that evaluates the PRotein-protein INteraction prediction from Graph perspective. As illustrated in Fig. 1, PRING compiles high-confidence physical interactions across four organisms, Human, Arath, Ecoli, and Yeast, sourced from STRING [36], UniProt [37], Reactome [38], and IntAct [39], yielding 21,484 proteins and 186,818 interactions, with efforts made to minimize both data redundancy and leakage. Through this golden dataset, we benchmark representative PPI prediction methods, including sequence similarity-based, naive sequence-based, PLM-based models, and structure-based, across five tasks: two topology-oriented, namely intra-species and cross-species PPI network construction, to assess whether models can recover the topology properties of PPI networks, such as network density and local community structures, and facilitating cross-species biological knowledge transfer study, and three function-oriented, including protein complex pathway prediction, GO functional module analysis, and essential protein justification, which can support disease mechanism analysis, protein function annotation, and therapeutic target identification. Collectively, these evaluations reveal 2 whether model is capable of capturing both the topological and the functional semantics of real interactomes, moving beyond pairwise classification to network-level understanding. Extensive experiments yield the following insights: (1) current models tend to generate overly dense graphs, diverging from the sparsity nature of real PPI networks; (2) predicted PPI modules exhibit limited functional alignment with the ground-truth, restricting their utility in downstream tasks such as pathway reconstruction and function annotation; and (3) reconstructed graphs struggle to separate essential from non-essential proteins, indicating that critical topological signals remain uncaptured; (4) classification metrics cannot completely reflect models ability to recover network structure, highlighting the need for graph-level evaluations. While these findings highlight challenges, we hope they can serve as positive step forward for the community. PRING complements existing benchmarks and provides reliable platform for more holistic research in modeling PPI networks. To sum up, our key contributions include: We propose PRING, to the best of our knowledge, the first comprehensive benchmark that evaluates PPI prediction models from topological and functional perspectives on PPI networks. Through extensive experiments, we demonstrate that existing models exhibit limited functional awareness and poor topological fidelity, revealing potential gap between computational approaches and their applicability in biological research. We release fully reproducible pipeline, including dataset construction and model evaluation tools, to facilitate future research and benchmarking efforts within the community."
        },
        {
            "title": "2.1 Protein-Protein Interaction Prediction Benchmark",
            "content": "Numerous PPI benchmarks have been developed to assess the effectiveness of prediction models. Most existing benchmarks [40, 41, 31, 30, 32] focus on physical interaction evaluations, which curate Human and Yeast PPI datasets from resources such as DIP [42], UniProt [37], and HPRD [43], applying filtering criteria to ensure data quality and evaluating models using binary classification metrics. To address the limitations of small-scale and single-species benchmarks, D-SCRIPT [17] introduced cross-species evaluation by sampling 65,138 interactions across multi-species from STRING [44, 45, 36]. Meanwhile, Bernett et al. [46] further raised concerns about data leakage caused by naive splitting strategies (e.g., random splits), which can inflate model performance by enabling shortcut learning [47]. To mitigate this, they proposed more rigorous splitting protocols that revealed performance drops across benchmarks. Other studies [17, 33] built PPI networks to perform GO enrichment analysis or functional module detection in specific biological scenarios, such as bovine rumen in cows, to broaden the scope of model applications, but lack the comparison with ground truth. Additionally, several works [48, 49] constructed PPI benchmarks annotated with functional interaction types or binding sites, aiming to train models to predict these properties via multi-class classification. However, the aforementioned benchmarks primarily focus on pairwise interaction accuracy and lack fair and holistic evaluation of models capability in PPI network construction. To address these gaps, we introduce PRING, graph-centric benchmark that evaluates PPI models from both topological and functional perspectives."
        },
        {
            "title": "2.2 Protein-Protein Interaction Prediction Model",
            "content": "Computational methods for PPI prediction are gaining popularity due to their efficiency and reliability in understanding complex biological systems [50, 51]. Early studies [52] often inferred unknown interactions by leveraging sequence similarity with known homologous interaction pairs. Although these methods are efficient, their accuracy is limited, especially for novel interactions. The advent of deep learning [53, 54, 55] brought powerful computational tool for PPI prediction. Sequencebased models [32, 48, 17, 56] utilized convolutional neural networks (CNNs) [57, 58], recurrent neural networks (RNNs) [59, 60], to learn interaction patterns from raw protein sequences. More recently, protein language models (PLMs) [61, 62], pre-trained on large-scale protein datasets, have shown strong performance across biological tasks such as contact prediction and function annotation. Motivated by this, PLM-based methods [19, 63, 64] leverage rich and context-aware representations to predict PPIs, achieving state-of-the-art results on some benchmarks. Since proteins can be naturally represented as graphs, some methods [65, 66] utilized protein structure as input and employed Graph Neural Networks (GNNs) [67, 68] to predict PPIs. Meanwhile, with the success of structure 3 Figure 2: Data collection pipeline for the PRING. PPIs are first curated from comprehensive databases. Proteins are then filtered and mapped using SwissProt and NCBI Taxonomy to target species. Redundant interactions are removed through sequence and functional similarity checks to ensure data quality. The resulting PPI networks include four species: Human, Arath, Yeast, and Ecoli. prediction tools like AlphaFold [21] and RoseTTAFold [69], structure-based methods [70, 71] have also emerged, offering improved biological explainability. Additionally, some studies [72, 73, 49] applied GNNs to infer functional interaction types based on the known PPI network as input. In this work, we evaluate PPI prediction models designed to identify physical interactions from graph-level perspective, providing insights into their ability to reconstruct biologically meaningful interactomes."
        },
        {
            "title": "Year",
            "content": "#Pairs Seq. Sim. #Proteins"
        },
        {
            "title": "Multi\nSpecies",
            "content": "Table 1: Comparison of PRING with existing PPI benchmarks. (Seq.Sim.: sequence similarity, N/A: not available.) In this section, we present PRING, comprehensive and fair benchmark designed to evaluate PPI prediction models from graph perspective. To construct such benchmark, we first curate PPIs from diverse resources and filter redundant proteins based on sequence similarity and functional overlap to ensure data quality, and then we partition data into training and test sets with non-overlapping proteins to prevent data leakage, yielding PPI network dataset comprising 21,484 proteins, 186,818 PPIs (Sec. 3.1). detailed comparison between PRING and existing PPI benchmarks is shown in Tab. 1. Based on this golden dataset, we develop set of tasks to assess model performance in capturing both the topology and biological functionality of PPI networks (Sec. 3.2). GUO [40] PAN [31] HUANG [41] DU [30] RICHOUX [32] SHS27K [48] SHS148K [48] D-SCRIPT [17] HIPPIE [46] 5,594 36,630 3,899 17,257 45,765 7,401 43,397 65,138 137,250 2,497 9,476 3,163 4,424 8,333 1,663 5,082 19,086 10,819 2008 2010 2016 2017 2019 2019 2019 2021 N/A N/A 25% 40% N/A 40% 40% 40% 40% (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37)"
        },
        {
            "title": "Leakage\nFree",
            "content": "PRING (Ours) 186,818 21,484 2025 40% (cid:33) (cid:33) (cid:33)"
        },
        {
            "title": "3.1 Data Collection",
            "content": "Diverse, reliable, and non-redundant interaction data is essential for effective and comprehensive evaluation of PPI prediction models [46]. To achieve this goal, PRING is mainly assembled in two stages: (1) integrating high-confidence multi-species PPIs; and (2) applying homologyand functionbased filters to eliminate redundancy and utilizing leakage-free splitting protocol to prevent potential leakage. pipeline overview is shown in Fig. 2, and detailed data documentation is given in Apx. C. Resource Integration. We begin by aggregating experimentally validated PPIs from UniProt [37], Reactome [38], and IntAct [39], and complement these sources with high-confidence interactions (combined score > 0.7 [74]) from STRING [36] to ensure both both data quality and comprehensive coverage, while mitigating the noise inherent in some STRING interactions. To further enhance data reliability and prepare for downstream function-oriented analyses, we retain only annotated proteins with known functions in SwissProt [75]. Additionally, four phylogenetically distinct organisms, including Human, Arabidopsis thaliana (Arath), Escherichia coli (Ecoli), and Yeast, are selected using NCBI Taxonomy identifiers [76] to match with the target species and increase the species 4 diversity. This step not only considers data quality but also emphasizes data diversity and coverage, providing critical foundation for constructing our gold-standard dataset. Minimal Redundancy and Leakage. Our primary objective is to build gold-standard dataset with minimal data redundancy and leakage for fair evaluations of PPI models. To this end, we first perform sequence clustering using MMSeqs2 [77, 78] and retain only protein pairs with sequence identity 40%, following prior studies [17, 30]. Meanwhile, we remove proteins that share identical function IDs across species as an additional safeguard to reduce data redundancy further. Additionally, leakage-free protocol [46] is used to split the dataset with non-overlapping proteins, further preventing data leakage in model learning. The above filtering steps result in high-quality PPI networks for each organism in the PRING dataset. Specifically, the Human network comprises 10,090 proteins and 129,861 PPIs; Arath includes 5,025 proteins and 23,584 PPIs; Ecoli contains 3,196 proteins and 17,452 PPIs; and Yeast consists of 3,173 proteins and 15,921 PPIs. We believe this dataset provides high-confidence resource for PPI benchmarking and can facilitate fair and rigorous comparisons among different PPI models."
        },
        {
            "title": "3.2 Task Suite",
            "content": "Through the PRING dataset, we develop five tasks to systematically evaluate how well PPI prediction models recapitulate both the structural topology and functional properties of PPI networks."
        },
        {
            "title": "3.2.1 Topology-Oriented Task",
            "content": "To evaluate whether PPI prediction model can faithfully recover the topology of interaction networks, we first formulate the PPI network reconstruction task as follows. Given PPI graph with proteins, each protein is represented by its features xi = {si, ci}, = 1, . . . , , where si is the amino-acid sequence and ci is any auxiliary context. The model predicts an interaction label for each protein pair (i, j) with 1 < , according to: pred i,j = 1 indicates predicted interaction between protein and protein j, while pred i,j = 0 denotes no predicted interaction. We then aggregate all positive predictions to form the reconstructed network: i,j = (xi, xj) {0, 1}. Here, pred Gpred = Aggregate(cid:8)(i, j) pred i,j = 1, 1 < (cid:9). (1) Each reconstructed graph Gpred is then compared against its corresponding ground-truth graph Gtrue. To evaluate the models ability of identifying topology properties like network density and crossspecies knowledge transfer, we introduce two tasks: (1) intra-species network construction, where both training and test are performed on the same species; and (2) cross-species network construction, where the model is trained on one species and evaluated on another. Intra-Species PPI Network Construction. In this task, we focus on the Human species, following the same intra-species subject used in previous studies [17, 19]. The full Human PPI graph is split into training and test sets with an 8:2 ratio, using an established protocol [46] to prevent potential data leakage. This obtains training subgraph of 8,072 proteins and test subgraph of 2,018 proteins. To investigate the influence of subgraph topology and size on model performance, we sample 500 subgraphs from the test set, ranging in size from 20 to 200 nodes, using each of three traversal algorithms, breadth-first search (BFS), depth-first search (DFS), and random walk (RW). Meanwhile, since existing PPI prediction models require PPI pairs for training, we sample positive and negative protein pairs from the training subgraph in 1:1 ratio and further partition them into 80% for training and 20% for validation, following the previous works [46]. In total, this yields 85,824 training pairs, 21,456 validation pairs, and 500 test graphs per traversal strategy for topology recovery evaluation. Cross-Species PPI Network Construction. For cross-species evaluations, we use models trained on the Human PPIs to reconstruct PPI networks in the other three species: Arath, Ecoli, and Yeast. As in the intra-species setting, we sample 500 subgraphs ranging from 20 to 200 nodes from each species using BFS, DFS, and RW, resulting in 500 test networks per traversal method per species. Subsequently, the above two tasks are evaluated using five topology-aware metrics, which are widely used in graph construction tasks (Detailed mathematical definitions are provided in Apx. D.1.4.): Graph Similarity (GS) [79] primarily quantifies the edge differences between the predicted and ground-truth graphs, as the node set in the PPI graph remains unchanged. Relative Density (RD) [80] evaluates the extent of overor under-prediction by comparing the edge density of the predicted network with that of the ground-truth network. 5 Degree Distribution (Deg.) [81] computes the discrepancy between node degree distributions of the predicted and ground-truth networks using maximum mean discrepancy (MMD) [82], providing quantitative assessment of global structural differences in terms of connectivity patterns. Clustering Coefficient (Clus.) [81] uses MMD to measure the discrepancy between the distributions of local clustering coefficients in the predicted and ground-truth networks, thereby assessing the preservation of community structure. Spectral [83] calculates the discrepancy between eigenvalue spectra of normalized Laplacian matrices of predicted and true networks using MMD, reflecting global structural alignment."
        },
        {
            "title": "3.2.2 Function-Oriented Task",
            "content": "Besides topology-based evaluation, we introduce three function-oriented tasks that are closely aligned with real-world biological applications. These tasks assess how well the reconstructed PPI networks preserve biologically meaningful properties and evaluate the practical applicability of existing models. Protein Complex Pathway Prediction. Complex pathways refer to biological processes involving multiple proteins that interact with each other to perform coordinated cellular functions [84, 85], which typically form densely connected subgraphs within the larger PPI network. Accurately reconstructing these pathways can enhance our understanding of disease mechanisms and support the development of targeted therapies [86, 87]. In this task, the model first predicts pairwise protein interactions based on the input complex, then aggregates these predictions to construct predicted subgraph, which are subsequently evaluated against the ground truth. To achieve fair evaluation, we collect 235 complex pathways from Reactome [38] that share no protein overlap with the Human training graph, with pathway sizes ranging from 4 to 20 proteins. We evaluate model performance using the following metrics (Detailed descriptions are provided in Apx. D.2.1.): Pathway Precision (PP) is the proportion of predicted interactions within the complex pathway that are present in the ground truth. Pathway Recall (PR) computes the proportion of ground-truth interactions within the complex pathway that are successfully predicted. Pathway Connectivity (PC) calculates the fraction of predicted protein complex pathway subgraphs that form connected component, reflecting biological consistency. GO Enrichment Analysis. GO enrichment connects network topology with biological function, supporting gene annotation and pathway discovery [88]. This task evaluates whether reconstructed PPI networks preserve functional coherence by comparing enriched GO terms of predicted communities to those of the ground-truth network. Specifically, we reconstruct the Human test subgraph (2,018 proteins) using trained PPI prediction model, detect communities via the Louvain algorithm [89], and perform GO enrichment using g: Profiler [90] tool across three ontologies: Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). Evaluations are conducted under two metrics (Thorough definitions are included in Apx. D.2.2.): Functional Alignment (FA) depicts the average Jaccard similarity of enriched GO terms between each predicted cluster and its best-matching ground-truth partner, measuring functional alignment. Consistency Ratio (CR) is the ratio of within-cluster GO term Jaccard similarity in the predicted network to that in the ground-truth network, reflecting how well functional coherence is preserved. Essential Protein Justification. Essential proteins are critical for an organisms survival and often occupy central positions in PPI networks [91, 92]. reliable model should differentiate essential from non-essential proteins based on node degree or centrality in the reconstructed network [93]. In this task, we use the Human test subgraph (2,018 proteins) as the evaluation set and annotate essential and non-essential proteins using CCDS [94] and UniProt [37]. In detail, we select 100 essential and 100 non-essential proteins whose network centrality scores [91] differ significantly (p < e4). Trained models are used to reconstruct the Human test graph, and performance is assessed using the following metrics (Further details are presented in Apx. D.2.3): Precision@K (P@K) calculates the proportion of essential proteins among the top nodes ranked by network centrality scores. is set to 100 in this task. Distribution Overlap (DO) evaluates the distribution overlap between the centrality scores of essential and non-essential proteins in the reconstructed network. 6 Table 2: Overall results of intra-species (Human) PPI network construction task. We use three color scales of blue to denote the first, second, and third best performance. RD closer to 1 is better. Sampling Method BFS DFS RW Category Model Seq. Sim. SPRINT Naive Sequence PIPR D-SCRIPT Topsy-Turvy PLM PPITrans TUnA PLM-interact (35M) PLM-interact (650M) Structure Struct2Graph TAGPPI RF2-Lite GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral 0.227 0.209 0.215 0.183 0.362 0.342 0.383 0.396 0.27 0.283 0.227 1.61 4.39 1.13 1. 3.39 2.99 2.29 1.64 5.58 4.56 0.524 10.9 81.7 15.4 17.3 52.2 47.8 24.4 30.9 95.1 70.8 43. 12.5 40.5 18.8 11.4 29.1 30.2 16.0 14.8 50.5 36.2 31.7 11.2 36.2 29.7 14. 26.3 22.9 12.8 16.5 39.2 29.5 34.9 0.145 0.0749 0.123 0.130 0.314 0.289 0.322 0.350 0.132 0.143 0. 5.91 12.3 6.40 3.22 4.54 4.44 2.85 2.03 10.8 9.94 1.10 316 518 302 418 416 209 236 508 491 226 207 335 237 201 270 272 78.2 63.6 321 309 39. 53.8 77.0 67.0 65.9 46.6 46.0 36.3 42.2 67.6 65.3 61.8 0.178 0.196 0.209 0. 0.449 0.450 0.429 0.491 0.237 0.256 0.171 1.32 5.75 5.82 4.88 2.57 2.23 2.47 1.76 5.83 4.87 0. 17.9 165 134 104 71.9 55.4 52.6 50.3 155 127 81.7 26 107 64.8 64. 46.1 42.5 22.6 21.2 103 74.2 73.9 15.1 41.1 52 36.7 19.6 13.7 13.6 15.4 38.3 28.6 60. Avg. Rank 3 11 8 5 5 4 2 1 10"
        },
        {
            "title": "4 Experiment",
            "content": "We conduct extensive experiments on five network-level tasks outlined in Sec. 3.2. Our experiments aim to answer the following research questions: Q1: How effectively do current models reconstruct PPI networks? (Sec. 4.3.1) Q2: How well do the PPI prediction models generalize? (Sec. 4.3.2) Q3: How do these models perform in biology practice? (Sec. 4.3.3) Q4: How well do standard classification metrics reflect network-level performance? (Sec. 4.3.4)"
        },
        {
            "title": "4.1 Evaluation Baseline",
            "content": "We consider four categories of PPI prediction models based on their design principles in our benchmark: sequence similarity-based models, naive sequence-based models, PLM-based models, and structure-based models. More details are provided in Apx. E. Sequence Similarity-based Model. These models assume that protein pairs resembling known interacting pairs are more likely to interact [46]. We adopt SPRINT [52] as representative non-deep learning method and investigate how effectively functional PPI network can be predicted using only sequence similarity information. Naive Sequence-based Model. These models implement conventional deep learning architectures [95, 96], using protein sequences as input and extracting features based on physicochemical properties or structure embeddings [46]. We evaluate three models: PIPR [48], which utilizes residual Siamese RCNN to capture hierarchical sequence patterns; D-SCRIPT [17], which augments sequence features with structure-aware embeddings to infer inter-protein contact maps; and TopsyTurvy [56], which integrates multi-scale sequence representations to refine interaction prediction. PLM-based Model. Protein language models (PLMs) are typically trained on large protein datasets (e.g., UniRef90 [97]) using self-supervised learning to produce context-aware embeddings. We consider three PLM-based models: PPITrans [19], which leverages ProtT5 [62] with multi-layer transformer blocks; TUnA [64], which applies ESM-2 [25] with an uncertainty-aware module; and PLM-interact (35M & 650M) [63], which fine-tunes ESM-2 on the STRING Human database. Structure-based Model. These models either take known protein structures as input or use end-to-end structure prediction for PPI modeling. We include three structure-based approaches: Struct2Graph [65] and TAGPPI [66], which use GNNs to model protein structure input, and RF2Lite [70], lightweight variant of RoseTTAFold2 [98] designed specifically for PPI prediction. In addition, case study of Chia-1 [99], the newest structure prediction model, is provided in Apx. G.3."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We adopt the model hyperparameters as recommended in the original papers, which have shown strong performance on their respective benchmarks. We assume these configurations to be robust and effective across diverse scenarios. Due to computational limitations, PLM-interact directly uses the released pre-trained weights for inference, and RF2-Lite is evaluated on only 10% of the test graphs, while all other models are trained from scratch. The model implementation details refer to Apx. F."
        },
        {
            "title": "4.3.1 From Pairs to Networks: Evaluating Structural Reconstruction",
            "content": "The intra-species PPI reconstruction results are summarized in Tab. 2. We can observe that: (1) PLM-based models consistently outperform other types of models; among the top five models, four 7 Figure 3: Cross-species generalization performance evaluated via graph similarity score. are PLM-based, with PLM-interact 35M and 650M ranking first and second, respectively, highlighting that PLM can provide rich representations while capturing biologically accurate interactions; (2) most naive sequence-based and structure-based models perform poorly; for example, PIPR ranks last among the 11 baselines, while Struct2Graph ranks second to last, indicating their limited capacity to capture interaction patterns with simple architectures and features; (3) SPRINT, non-deep learning method, ranks third, yielding more realistic topological structure than some deep learning models, suggesting that the basic sequence similarity can still effectively preserve basic network structure; and (4) all baseline models exhibit higher RD values under DFS sampling than under BFS or RW; this is likely due to their tendency to overpredict interactions inconsistent with the underlying network topology (RD > 1), which aligns with the distribution of DFS-sampled graphs that are sparse and pathway-like, in contrast to the dense local connectivity preserved by BFS and RW. Additionally, we find that the performance degrades as the PPI network size increases, demonstrating that existing computational methods still face challenges in reconstructing large-scale PPI networks (see Apx. G.1.4 for more details). Overall, the relative performance ranking of different model types remains consistent across traversal strategies. Findings. Current PPI models tend to over-predict interactions, resulting in potential false positives. Moreover, the reconstructed PPI networks preserve low topology consistency with the ground truth: the highest graph similarity score remains below 0.5, while the other four structural metrics even deviate from their ideal minimum value by an order of magnitude. These findings highlight the need for more capable models that preserve the global structure of PPI networks beyond pairwise accuracy."
        },
        {
            "title": "4.3.2 Beyond Boundaries: Generalization Across Species",
            "content": "The second topology-oriented task evaluates the cross-species generalization ability of PPI prediction models. As shown in Fig. 3, the overall performance consistently decreases as the evolutionary distance from Human increases, following the order of Arath, Yeast, and Ecoli, with the average performance drops by 15.2%, 25.3%, and 35.2%, respectively. In terms of model categories, PLM-based models continue to achieve the best performance among all baselines, with an average graph similarity score of 0.24 across the three species. This result indicates the robustness and generalizability of PLM-derived representations across diverse biological scenarios. In contrast, naive sequence-based models perform the worst, with an average score of only 0.17, demonstrating that simple deep learning architectures such as CNNs and RNNs are insufficient for capturing the complex evolutionary and structural signals required for cross-species. Findings. The cross-species results highlight the challenge of transferring knowledge to evolutionarily distant species on the genetic tree [100]. This underscores the need for developing more advanced models that can capture conserved features to enable reliable transfer across species."
        },
        {
            "title": "4.3.3 From Prediction to Practice: Biological Utility of PPI Models",
            "content": "The results of the function-oriented tasks are summarized in Tab. 3. Specifically, in the protein complex pathway reconstruction task, models tend to achieve high precision but low recall (e.g., PPITrans gets precision of 0.863 and recall of 0.583), suggesting that predictions are accurate yet incomplete, which in turn limits the comprehensive recovery of protein complex pathways. For the GO enrichment analysis task, nearly all models perform poorly in the function alignment metric FA, with scores below 0.4, demonstrating weak ability to preserve functional modularity in the reconstructed networks. Additionally, while most models are capable of identifying true positive hub proteins in the essential protein justification task (e.g., the PLM-interact (35M) achieves P@100 of 0.80), they still struggle to effectively distinguish essential from non-essential proteins based on the reconstructed networks (e.g., even the best-performing model, PLM-interact (650M), only attains DO score of 0.440). As illustrated in Fig. 4, the network centrality score distributions of essential and non-essential proteins in the ground-truth graphs are well-separated. In contrast, the distributions generated by the PLM-interact (650M) exhibit substantial overlap, indicating loss of discriminative 8 Table 3: Overall results of function-oriented tasks. We use three color scales of blue to denote the first, second, and third best performance. Function-oriented Tasks"
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequence",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI RF2-Lite PR PP 0.526 0.856 0.160 0.225 0. 0.583 0.526 0.550 0.549 0.413 0.487 0.554 0.588 0.466 0.613 0.863 0.864 0.847 0.862 0.742 0.781 0.874 PC 0.752 0.323 0.316 0.338 0.818 0.794 0.880 0.887 0.608 0.713 0.135 FA 0. 0.182 0.175 0.232 0.250 0.259 0.335 0.368 0.154 0.168 / CR 0.655 0.654 0.678 0. 0.756 0.622 0.785 0.844 0.649 0.649 / P@100 DO 0.54 0.52 0.47 0. 0.48 0.57 0.80 0.77 0.45 0.43 / 0.727 0.834 0.629 0.755 0.886 0.849 0.423 0.440 0.841 0.828 / Avg. Rank 5 8 6 8 3 4 2 1 10 6 / (a) The Figure 4: Essential protein analysis. network centrality score of essential and nonessential proteins in the ground-truth PPI network. (b) Network centrality score distribution of PLM-interact (650M). structural features. These observations suggest that current models fail to preserve the distinguishing topological characteristics between essential and non-essential proteins in the reconstructed networks. Figure 5: Relationship between classification performance and graph-level metrics. The recall rate is positively correlated with the degree distribution (MMD), while the precision is negative. Findings. Current methods still fall short in capturing the underlying functional organization of PPI networks, leading to low functional alignment, fragmented complex pathways, and disturbed essential and non-essential proteins. These shortcomings hinder key downstream applications such as function annotation, biological module detection, and disease mechanism discovery."
        },
        {
            "title": "4.3.4 Metric Alignment: Do Classification Scores Tell the Whole Story?",
            "content": "While standard classification metrics are commonly used to evaluate PPI models, their effectiveness in reflecting graph-level structural properties remains unclear. We investigate the correlation between traditional binary classification metrics and graph-level structural metrics, as shown in Fig. 5. Our analysis reveals an inverse relationship between recall and precision in terms of their impact on the degree distribution: higher recall increases distribution mismatch, whereas higher precision reduces it. This indicates that recall-optimized models recover more true positive interactions at the expense of false positives that distort the topology, whereas precision-focused models better preserve network structure. Additional correlation analyses are provided in Apx. G.1.3. Findings. Standard classification metrics cannot completely reflect network-level structure, revealing the necessity for graph-aware evaluations in robust PPI modeling."
        },
        {
            "title": "5 Conclusion\nIn this paper, we present PRING, the first comprehensive benchmark designed to evaluate PPI\nprediction models beyond pairwise classification, focusing on their ability to reconstruct biology-\naware PPI networks. By introducing topology and function-oriented tasks across multiple species\nwith well-designed strategies on both data redundancy and leakage, PRING enables a more rigorous\nand application-driven assessment of model performance. Extensive experiments reveal that current\nmodels typically fail to preserve the global structure and biological coherence of interaction networks.\nThese findings emphasize the limitations of traditional evaluation protocols and highlight the need\nfor more holistic and biology-informed approaches. We believe PRING is a valuable resource for\ndeveloping more reliable PPI prediction models that support real-world biological discovery.",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Arunachalam Vinayagam, Ulrich Stelzl, Raphaele Foulle, Stephanie Plassmann, Martina Zenkner, Jan Timm, Heike Assmus, Miguel Andrade-Navarro, and Erich Wanker. directed protein interaction network for investigating intracellular signal transduction. Science signaling, 4(189):rs8rs8, 2011. [2] Daogang Guan, Jiaofang Shao, Zhongying Zhao, Panwen Wang, Jing Qin, Youping Deng, Kenneth Boheler, Junwen Wang, and Bin Yan. Pthgrn: unraveling post-translational hierarchical gene regulatory networks using ppi, chip-seq and gene expression data. Nucleic acids research, 42(W1):W130W136, 2014. [3] Sheng Li, Xuefeng Zhang, Yiming Wang, and et al. Innate immunity interactome dynamics. Journal of Proteome Research, 13(12):56305641, 2014. [4] Nahid Safari-Alighiarloo, Mohammad Taghizadeh, Mostafa Rezaei-Tavirani, Bahram Goliaei, and Ali Asghar Peyvandi. Protein-protein interaction networks (ppi) and complex diseases. Gastroenterology and Hepatology from bed to bench, 7(1):17, 2014. [5] G. Steven Martin. Cell signaling and cancer. Cancer Cell, 4(3):167174, 2003. [6] X. Zhou, Y. Zhang, Y. Wang, and J. Wang. Small molecules targeting proteinprotein interactions for cancer therapy. Signal Transduction and Targeted Therapy, 8(1):124, 2023. [7] Yuan Zhou, Yuhong Liu, Yuxuan Zhang, Yuting Luo, Yifan Zhu, Qian Wang, Yufeng Zhang, Yitao Wang, Zhen Zhang, and Yixue Li. New insights into proteinprotein interaction modulators in drug discovery and therapeutic advance. Signal Transduction and Targeted Therapy, 9(1):13, 2024. [8] Duan Ni, Shaoyong Lu, and Jian Zhang. Emerging roles of allosteric modulators in the regulation of protein-protein interactions (ppis): new paradigm for ppi drug discovery. Medicinal research reviews, 39(6):23142342, 2019. [9] Pavol Skubák and Navraj Pannu. Automatic protein structure solution from weak x-ray data. Nature communications, 4(1):2777, 2013. [10] Richard Engh and Robert Huber. Accurate bond and angle parameters for x-ray protein structure refinement. Foundations of Crystallography, 47(4):392400, 1991. [11] Xiechao Zhan, Chuangye Yan, Xiaofeng Zhang, Jianlin Lei, and Yigong Shi. Structure of human catalytic step spliceosome. Science, 359(6375):537545, 2018. [12] Qiang Su, Mengying Chen, Yan Shi, Xiaofeng Zhang, Gaoxingyu Huang, Bangdong Huang, Dongwei Liu, Zhangsuo Liu, and Yigong Shi. Cryo-em structure of the human igm cell receptor. Science, 377(6608):875880, 2022. [13] Barry Causier and Brendan Davies. Analysing protein-protein interactions with the yeast two-hybrid system. Plant Molecular Biology, 50:855870, 2002. [14] Anna Brückner, Cécile Polge, Nicolas Lentze, Daniel Auerbach, and Uwe Schlattner. Yeast two-hybrid, powerful tool for systems biology. International journal of molecular sciences, 10(6):27632788, 2009. [15] Francis OReilly and Juri Rappsilber. Cross-linking mass spectrometry: methods and applications in structural, molecular and systems biology. Nature structural & molecular biology, 25(11):10001008, 2018. [16] Swantje Lenz, Ludwig Sinn, Francis OReilly, Lutz Fischer, Fritz Wegner, and Juri Rappsilber. Reliable identification of protein-protein interactions by crosslinking mass spectrometry. Nature communications, 12(1):3564, 2021. [17] Samuel Sledzieski, Rohit Singh, Lenore Cowen, and Bonnie Berger. D-script translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions. Cell Systems, 12(10):969982, 2021. 10 [18] Aerin Yang, Kevin Jude, Ben Lai, Mason Minot, Anna Kocyla, Caleb Glassman, Daisuke Nishimiya, Yoon Seok Kim, Sai Reddy, Aly Khan, et al. Deploying synthetic coevolution and machine learning to engineer protein-protein interactions. Science, 381(6656):eadh1720, 2023. [19] Sen Yang, Peng Cheng, Yang Liu, Dawei Feng, and Shengqi Wang. Exploring the knowledge of an outstanding protein to protein interaction transformer. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 21(5):12871298, 2024. [20] Varun Ullanat, Bowen Jing, Samuel Sledzieski, and Bonnie Berger. Learning the language of protein-protein interactions. bioRxiv, 2025. [21] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021. [22] Richard Evans, Michael ONeill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with alphafold-multimer. biorxiv, 2021. [23] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630(8016):493500, 2024. [24] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021. [25] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. [26] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with language model. Science, page eads0018, 2025. [27] Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. Prott3: Protein-to-text generation for text-based protein understanding. arXiv preprint arXiv:2405.12564, 2024. [28] Lun Hu, Xiaojuan Wang, Yu-An Huang, Pengwei Hu, and Zhu-Hong You. survey on computational models for predicting proteinprotein interactions. Briefings in bioinformatics, 22(5):bbab036, 2021. [29] Tao Tang, Xiaocai Zhang, Yuansheng Liu, Hui Peng, Binshuang Zheng, Yanlin Yin, and Xiangxiang Zeng. Machine learning on proteinprotein interaction prediction: models, challenges and trends. Briefings in Bioinformatics, 24(2):bbad076, 2023. [30] Xiuquan Du, Shiwei Sun, Changlin Hu, Yu Yao, Yuanting Yan, and Yanping Zhang. Deepppi: boosting prediction of proteinprotein interactions with deep neural networks. Journal of Chemical Information and Modeling, 57(6):14991510, 2017. [31] Xiao-Yong Pan, Ya-Nan Zhang, and Hong-Bin Shen. Large-scale prediction of human proteinprotein interactions from amino acid sequence based on latent topic features. Journal of Proteome Research, 9(10):49925001, 2010. [32] Florian Richoux, Charlène Servantie, Cynthia Borès, and Stéphane Téletchéa. Comparing two deep learning sequence-based models for protein-protein interaction prediction. arXiv preprint arXiv:1901.06268, 2019. [33] Jiahui Wu, Bo Liu, Jidong Zhang, Zhihan Wang, and Jianqiang Li. Dl-ppi: method on prediction of sequenced proteinprotein interaction based on deep learning. BMC bioinformatics, 24(1):473, 2023. 11 [34] Jean-François Rual, Kavitha Venkatesan, Tong Hao, Tomoko Hirozane-Kishikawa, Amélie Dricot, Ning Li, Gabriel Berriz, Francis Gibbons, Matija Dreze, Nono Ayivi-Guedehoussou, et al. Towards proteome-scale map of the human proteinprotein interaction network. Nature, 437(7062):11731178, 2005. [35] Anne-Claude Gavin, Patrick Aloy, Paola Grandi, Roland Krause, Markus Boesche, Martina Marzioch, Christina Rau, Lars Juhl Jensen, Sonja Bastuck, Birgit Dümpelfeld, et al. Proteome survey reveals modularity of the yeast cell machinery. Nature, 440(7084):631636, 2006. [36] Damian Szklarczyk, Rebecca Kirsch, Mikaela Koutrouli, Katerina Nastou, Farrokh Mehryary, Radja Hachilif, Annika Gable, Tao Fang, Nadezhda Doncheva, Sampo Pyysalo, et al. The string database in 2023: proteinprotein association networks and functional enrichment analyses for any sequenced genome of interest. Nucleic Acids Research, 51(D1):D638D646, 2023. [37] Uniprot: the universal protein knowledgebase in 2023. Nucleic acids research, 51(D1):D523 D531, 2023. [38] Marija Milacic, Deidre Beavers, Patrick Conley, Chuqiao Gong, Marc Gillespie, Johannes Griss, Robin Haw, Bijay Jassal, Lisa Matthews, Bruce May, et al. The reactome pathway knowledgebase 2024. Nucleic acids research, 52(D1):D672D678, 2024. [39] Noemi Del Toro, Anjali Shrivastava, Eliot Ragueneau, Birgit Meldal, Colin Combe, Elisabet Barrera, Livia Perfetto, Karyn How, Prashansa Ratan, Gautam Shirodkar, et al. The intact database: efficient access to fine-grained molecular interaction data. Nucleic acids research, 50(D1):D648D653, 2022. [40] Yanzhi Guo, Lezheng Yu, Zhining Wen, and Menglong Li. Using support vector machine combined with auto covariance to predict proteinprotein interactions from protein sequences. Nucleic Acids Research, 36(9):30253030, 2008. [41] Yu-An Huang, Zhu-Hong You, Xin Gao, Leon Wong, and Lirong Wang. Using weighted sparse representation model combined with discrete cosine transformation to predict protein-protein interactions from protein sequence. BioMed Research International, 2015(1):902198, 2015. [42] Ioannis Xenarios, Danny Rice, Lukasz Salwinski, Marisa Baron, Edward Marcotte, and David Eisenberg. Dip: the database of interacting proteins. Nucleic acids research, 28(1):289291, 2000. [43] TS Keshava Prasad, Renu Goel, Kumaran Kandasamy, Shivakumar Keerthikumar, Sameer Kumar, Suresh Mathivanan, Deepthi Telikicherla, Rajesh Raju, Beema Shafreen, Abhilash Venugopal, et al. Human protein reference database2009 update. Nucleic acids research, 37(suppl_1):D767D772, 2009. [44] Damian Szklarczyk, Annika Gable, David Lyon, Alexander Junge, Stefan Wyder, Jaime Huerta-Cepas, Milan Simonovic, Nadezhda Doncheva, John Morris, Peer Bork, et al. String v11: proteinprotein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Research, 47(D1):D607 D613, 2019. [45] Damian Szklarczyk, Annika Gable, Katerina Nastou, David Lyon, Rebecca Kirsch, Sampo Pyysalo, Nadezhda Doncheva, Marc Legeay, Tao Fang, Peer Bork, et al. The string database in 2021: customizable proteinprotein networks, and functional characterization of user-uploaded gene/measurement sets. Nucleic Acids Research, 49(D1):D605D612, 2021. [46] Judith Bernett, David Blumenthal, and Markus List. Cracking the black box of deep sequencebased proteinprotein interaction prediction. Briefings in Bioinformatics, 25(2):bbae076, 2024. [47] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. [48] Muhao Chen, Chelsea J-T Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted proteinprotein interaction prediction based on siamese residual rcnn. Bioinformatics, 35(14):i305i314, 2019. 12 [49] Guofeng Lv, Zhiqiang Hu, Yanguang Bi, and Shaoting Zhang. Learning unknown from correlations: Graph neural network for inter-novel-protein interaction prediction. arXiv preprint arXiv:2105.06709, 2021. [50] Jack Greenblatt, Bruce Alberts, and Nevan Krogan. Discovery and significance of protein-protein interactions in health and disease. Cell, 187(23):65016517, 2024. [51] Greta Grassmann, Mattia Miotto, Fausta Desantis, Lorenzo Di Rienzo, Gian Gaetano Tartaglia, Annalisa Pastore, Giancarlo Ruocco, Michele Monti, and Edoardo Milanetti. Computational approaches to predict proteinprotein interactions in crowded cellular environments. Chemical Reviews, 124(7):39323977, 2024. [52] Yiwei Li and Lucian Ilie. Sprint: ultrafast protein-protein interaction prediction of the entire human interactome. BMC bioinformatics, 18:111, 2017. [53] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436 444, 2015. [54] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016. [55] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and Wanli Ouyang. Layerwise optimization by gradient decomposition for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96349643, 2021. [56] Rohit Singh, Kapil Devkota, Samuel Sledzieski, Bonnie Berger, and Lenore Cowen. Topsyintegrating global view into sequence-based ppi prediction. Bioinformatics, turvy: 38(Supplement_1):i264i272, 2022. [57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [58] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [59] Mike Schuster and Kuldip Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):26732681, 1997. [60] Felix Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural Computation, 12(10):24512471, 2000. [61] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022:500902, 2022. [62] Suresh Pokharel, Pawel Pratyush, Michael Heinzinger, Robert Newman, and Dukka Kc. Improving protein succinylation sites prediction using embeddings from protein language model. Scientific reports, 12(1):16933, 2022. [63] Dan Liu, Francesca Young, Kieran Lamb, Adalberto Claudio Quiros, Alexandrina Pancheva, Crispin Miller, Craig Macdonald, David Robertson, and Ke Yuan. Plm-interact: extending protein language models to predict protein-protein interactions. bioRxiv, pages 202411, 2024. [64] Young Su Ko, Jonathan Parkinson, Cong Liu, and Wei Wang. Tuna: an uncertainty-aware transformer model for sequence-based proteinprotein interaction prediction. Briefings in Bioinformatics, 25(5):bbae359, 2024. [65] Mayank Baranwal, Abram Magner, Jacob Saldinger, Emine Turali-Emre, Paolo Elvati, Shivani Kozarekar, Scott VanEpps, Nicholas Kotov, Angela Violi, and Alfred Hero. Struct2graph: graph attention network for structure based predictions of proteinprotein interactions. BMC bioinformatics, 23(1):370, 2022. 13 [66] Bosheng Song, Xiaoyan Luo, Xiaoli Luo, Yuansheng Liu, Zhangming Niu, and Xiangxiang Zeng. Learning spatial structures of proteins improves proteinprotein interaction prediction. Briefings in bioinformatics, 23(2):bbab558, 2022. [67] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip Yu. comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):424, 2020. [68] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):6180, 2008. [69] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa Kinch, Dustin Schaeffer, et al. Accurate prediction of protein structures and interactions using three-track neural network. Science, 373(6557):871876, 2021. [70] Ian Humphreys, Jing Zhang, Minkyung Baek, Yaxi Wang, Aditya Krishnakumar, Jimin Pei, Ivan Anishchenko, Catherine Tower, Blake Jackson, Thulasi Warrier, et al. Protein interactions in human pathogens revealed through deep learning. Nature Microbiology, 9(10):26422652, 2024. [71] Patrick Bryant and Frank Noé. Rapid protein-protein interaction network creation from multiple sequence alignments with deep learning. bioRxiv, pages 202304, 2023. [72] Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for proteinprotein interaction. Nature Communications, 14(1):1093, 2023. [73] Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh Chawla, and Stan Z. Li. MAPE-PPI: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. In The Twelfth International Conference on Learning Representations, 2024. [74] Md Kamrul Islam, Diego Amaya-Ramirez, Bernard Maigret, Marie-Dominique Devignes, Sabeur Aridhi, and Malika Smaïl-Tabbone. Molecular-evaluated and explainable drug repurposing for covid-19 using ensemble knowledge graph embedding. Scientific Reports, 13(1):3643, 2023. [75] Amos Bairoch and Rolf Apweiler. The swiss-prot protein sequence database and its supplement trembl in 2000. Nucleic acids research, 28(1):4548, 2000. [76] Conrad Schoch, Stacy Ciufo, Mikhail Domrachev, Carol Hotton, Sivakumar Kannan, Rogneda Khovanskaya, Detlef Leipe, Richard Mcveigh, Kathleen ONeill, Barbara Robbertse, et al. Ncbi taxonomy: comprehensive update on curation, resources and tools. Database, 2020:baaa062, 2020. [77] Martin Steinegger and Johannes Söding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):10261028, 2017. [78] Martin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature communications, 9(1):2542, 2018. [79] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric Xing. Dags with no tears: Continuous optimization for structure learning. Advances in neural information processing systems, 31, 2018. [80] Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham Taylor. On evaluation metrics for graph generative models. arXiv preprint arXiv:2201.09871, 2022. [81] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022. 14 [82] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. Advances in neural information processing systems, 32, 2019. [83] Karolis Martinkus, Andreas Loukas, Nathanaël Perraudin, and Roger Wattenhofer. Spectre: Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators. In International Conference on Machine Learning, pages 1515915179. PMLR, 2022. [84] Qian Cong, Ivan Anishchenko, Sergey Ovchinnikov, and David Baker. Protein interaction networks revealed by proteome coevolution. Science, 365(6449):185189, 2019. [85] Michael Costanzo, Benjamin VanderSluis, Elizabeth Koch, Anastasia Baryshnikova, Carles Pons, Guihong Tan, Wen Wang, Matej Usaj, Julia Hanchard, Susan Lee, et al. global genetic interaction network maps wiring diagram of cellular function. Science, 353(6306):aaf1420, 2016. [86] Therese Bergendahl, Lukas Gerasimavicius, Jamilla Miles, Lewis Macdonald, Jonathan Wells, Julie PI Welburn, and Joseph Marsh. The role of protein complexes in human genetic disease. Protein Science, 28(8):14001411, 2019. [87] Mileidy Gonzalez and Maricel Kann. Chapter 4: Protein interactions and disease. PLoS computational biology, 8(12):e1002819, 2012. [88] Yingyao Zhou, Bin Zhou, Lars Pache, Max Chang, Alireza Hadj Khodabakhshi, Olga Tanaseichuk, Christopher Benner, and Sumit Chanda. Metascape provides biologist-oriented resource for the analysis of systems-level datasets. Nature communications, 10(1):1523, 2019. [89] Xinyu Que, Fabio Checconi, Fabrizio Petrini, and John Gunnels. Scalable community detection with the louvain algorithm. In 2015 IEEE international parallel and distributed processing symposium, pages 2837. IEEE, 2015. [90] Uku Raudvere, Liis Kolberg, Ivan Kuzmin, Tambet Arak, Priit Adler, Hedi Peterson, and Jaak Vilo. g: Profiler: web server for functional enrichment analysis and conversions of gene lists (2019 update). Nucleic acids research, 47(W1):W191W198, 2019. [91] Jianxin Wang, Min Li, Huan Wang, and Yi Pan. Identification of essential proteins based IEEE/ACM Transactions on Computational Biology and on edge clustering coefficient. Bioinformatics, 9(4):10701080, 2011. [92] Hawoong Jeong, Sean Mason, A-L Barabási, and Zoltan Oltvai. Lethality and centrality in protein networks. Nature, 411(6833):4142, 2001. [93] Jiashuai Zhang, Wenkai Li, Min Zeng, Xiangmao Meng, Lukasz Kurgan, Fang-Xiang Wu, and Min Li. Netepd: network-based essential protein discovery platform. Tsinghua Science and Technology, 25(4):542552, 2020. [94] Kim Pruitt, Jennifer Harrow, Rachel Harte, Craig Wallin, Mark Diekhans, Donna Maglott, Steve Searle, Catherine Farrell, Jane Loveland, Barbara Ruef, et al. The consensus coding sequence (ccds) project: Identifying common protein-coding gene set for the human and mouse genomes. Genome research, 19(7):13161323, 2009. [95] Keiron Oshea and Ryan Nash. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458, 2015. [96] Larry Medsker, Lakhmi Jain, et al. Recurrent neural networks. Design and Applications, 5(64-67):2, 2001. [97] Baris Suzek, Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy Wu. Uniref: comprehensive and non-redundant uniprot reference clusters. Bioinformatics, 23(10):1282 1288, 2007. [98] Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, and Frank DiMaio. Efficient and accurate prediction of protein structure using rosettafold2. BioRxiv, pages 202305, 2023. 15 [99] Chai Discovery team, Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhonikov, and Kevin Wu. Chai-1: Decoding the molecular interactions of life. BioRxiv, pages 202410, 2024. [100] Wayne Maddison. Gene trees in species trees. Systematic biology, 46(3):523536, 1997. [101] Mengxin Wu, Guantao Chen, Xiangtao Fan, and et al. Clusterbfs: method of identifying protein complexes by local searching in proteinprotein interaction networks. BMC Bioinformatics, 15:120, 2014. [102] Hongwu Ma and An-Ping Zeng. Constructing signaling pathway structures by depth-first search algorithm. Bioinformatics, 24(2):174181, 2008. [103] Wei Li, Hui Wang, Yong Zhang, and et al. random walk-based method for detecting essential proteins by integrating the topological and biological features of ppi network. BMC Bioinformatics, 2022. [104] Xiaoyang Zhang, Tianyu Chen, and Xiang Zhang. Pwn: Prior-guided random walk on warped networks for disease target prioritization. BMC Bioinformatics, 2023. [105] Felix Kallenborn, Alejandro Chacon, Christian Hundt, Hassan Sirelkhatim, Kieran Didi, Sooyoung Cha, Christian Dallago, Milot Mirdita, Bertil Schmidt, and Martin Steinegger. Gpu-accelerated homology search with mmseqs2. bioRxiv, pages 202411, 2024. [106] Charlotte Nicod, Amir Banaei-Esfahani, and Ben Collins. Elucidation of hostpathogen proteinprotein interactions to uncover mechanisms of host cell rewiring. Current opinion in microbiology, 39:715, 2017. [107] Zachary Davis, Stefan Huber, Rohan Suryawanshi, et al. Large-scale discovery of coronavirus-host factor protein interaction motifs reveals slim-mediated host-virus interactions. Nature Communications, 12(1):113, 2021."
        },
        {
            "title": "A Data and Code Availability",
            "content": "A.1 PRING Dataset . . . A.2 Evaluation Code . . . . . . . . Limitations & Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Data Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 ProteinProtein Interaction Scope . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Dataset Documentation",
            "content": "C.1 Data Sources . . . C.2 Collection Process . . C.3 Statistical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Task Documentation",
            "content": "D.1 Topology-oriented Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Function-oriented Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Baseline Models",
            "content": "E.1 Sequence Similarity-based Method . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Naive Sequence-based Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Protein Language Model-based Method . . . . . . . . . . . . . . . . . . . . . . . E.4 Structure-based Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "G Experimental Results",
            "content": "G.1 Topology-oriented Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Function-oriented Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Case Study . . . G.4 Scaling Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "H Broader Societary Impacts",
            "content": "18 18 18 18 18 18 18 19 20 20 23 25 25 25 25 26 26 27 31 33"
        },
        {
            "title": "A Data and Code Availability",
            "content": "A.1 PRING Dataset The PRING dataset is available at https://doi.org/10.7910/DVN/22AUPR. A.2 Evaluation Code The code for benchmarking PPI prediction models on PRING is available at https://github.com/ SophieSarceau/PRING. Limitations & Future Work B.1 Data Scope Though PRING currently benchmarks PPI prediction on four model organisms, including Human, Ecoli, Yeast, and Arath, which are widely studied, it does not cover the full phylogenetic diversity of life. As result, the generalizability of PPI prediction models to non-model or underrepresented taxa remains unexplored. To enable more comprehensive evaluation, future extensions of PRING will focus on expanding its taxonomic coverage by curating high-quality PPIs data from additional clades (e.g., archaea, non-model metazoans), enabling more comprehensive evaluation of evolutionary transfer and model robustness across the tree of life. Moreover, certain biological applications, such as antiviral drug development, require the modeling of PPIs between Human and pathogen proteins. To support such use cases, future versions of PRING will incorporate Humanvirus PPI networks, advancing the benchmarks utility in biomedical research. B.2 ProteinProtein Interaction Scope While PRING develops holistic downstream tasks to evaluate the models capability, all tasks currently treat PPIs as binary edges, ignoring other biological details such as interaction types (e.g., activation, inhibition). This would limit the ability to capture regulatory and signaling mechanisms. Moreover, PPIs are often dynamic and context-specific, varying across tissues, cell types, and conditions. Modeling them as static interactions overlooks this critical aspect of biological systems. For future work, we plan to extend PRING to support multi-type interactions and incorporate conditional PPIs under different biological contexts. This will enable more accurate and versatile benchmarking for real-world applications."
        },
        {
            "title": "C Dataset Documentation",
            "content": "C.1 Data Sources In this work, we construct the PPI benchmark dataset using data integrated from multiple public data sources, including UniProt, IntAct, Reactome, STRING, Gene Ontology (GO), and the Consensus CDS (CCDS) project. License information for each source is provided in Tab. 4. UniProt. UniProt is well-known comprehensive resource for protein sequence and functional information. In our work, we fetch PPIs and species information through manually reviewed entries from Swiss-Prot section of UniProt to ensure high-quality protein annotations. The raw data are available at: https://ftp.uniprot.org/pub/databases/uniprot/current_release/ knowledgebase/complete/uniprot_sprot.dat.gz https://ftp.uniprot.org/pub/databases/uniprot/current_release/ knowledgebase/complete/uniprot_sprot.fasta.gz Reactome. Reactome is curated knowledgebase of biological pathways and reactions. We take their high-quality PPIs to construct our dataset, and additionally collect protein - complex and pathway information to support the protein complex pathway prediction task (Sec. D.2.1). The raw data are available at: 18 https://reactome.org/download/current/interactors/reactome.all_ species.interactions.tab-delimited.txt https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_ human.txt https://reactome.org/download/current/Complex_2_Pathway_human.txt https://reactome.org/download/current/UniProt2Reactome.txt IntAct. IntAct provides free, open-source database system and analysis tools for molecular interaction data. We collect PPIs from IntAct to construct our dataset. The raw data of IntAct is available at: https://ftp.ebi.ac.uk/pub/databases/intact/current/psimitab/intact. zip STRING. STRING is database of known and predicted PPIs, integrating evidence from multiple sources. Following common practice, we include only high-confidence interactions with their Combine Score > 0.7 in our dataset. The raw data is available at: https://stringdb-downloads.org/download Gene Ontology. The Gene Ontology (GO) knowledgebase is the worlds largest source of information on the functions of genes, which provides structured vocabulary for annotating gene and protein functions across species. We extract GO terms related to our PPI networks to support the GO enrichment analysis task (Sec. D.2.2). The raw data is available at: http://current.geneontology.org/annotations/filtered_goa_uniprot_ all_noiea.gpad.gz CCDS. The Consensus CDS (CCDS) project provides curated set of protein-coding regions that are consistently annotated across major genome databases. We fetch reliably annotated proteins from CCDS for our essential protein justification task (Sec. G.2.3). The raw data is available at: https://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human/ Table 4: Licenses of datasets used in this work."
        },
        {
            "title": "Usage",
            "content": "PPI dataset construction (curated data) PPI dataset construction (curated data) and relation information for Protein Complex Pathway Prediction task PPI dataset construction (curated data) PPI dataset (high confidence data) Functional annotation for GO Enrichment Analysis task Protein information for Essential Protein Justification task construction C.2 Collection Process"
        },
        {
            "title": "Licence",
            "content": "CC BY 4."
        },
        {
            "title": "URL",
            "content": "https://www.uniprot.org/help/license CC0 1.0 Universal https://reactome.org/about/news CC BY 4.0 CC BY 4.0 CC BY 4. License not specified (redistributed via Bioregistry under CC BY 4.0) https://www.ebi.ac.uk/intact/download https://string-db.org/cgi/access? footer_active_subpage=licensing https://geneontology.org/docs/ go-citation-policy http://www.ncbi.nlm.nih.gov/CCDS Our data collection process starts from UniProt, we first download the complete Swiss-Prot entries dat file and parse each protein record to identify the \"CC -!- INTERACTION\" section, e.g:"
        },
        {
            "title": "CC\nCC\nCC",
            "content": "-!- INTERACTION: P12345; Q8N1H7:XYZ_HUMAN (xeno); Xeno interaction [EMBL:ABC123]; P12345; Q9Y6K1:DEF_HUMAN; NbExp=3; IntAct=EBI-12345, EBI-67890; 19 If the interacting partners is Swiss-Prot protein, we extract the interaction as PPI entry. Additionally, we download the full Swiss-Prot FASTA file containing 573,230 protein sequences. Using the NCBI taxonomy ID specified in the FASTA headers (e.g., OX=9606), we extract protein sets for four representative species: Human (Homo sapiens, 9606), Arath (Arabidopsis thaliana, 3702), Yeast (Saccharomyces cerevisiae, 559292), and Ecoli (Escherichia coli, 83333). We then download the complete contents file from IntAct and the ProteinProtein Interaction file from Reactome, extract all PPI entries, and filter them based on our constructed protein sets to retain only Swiss-Prot proteins from the four selected species. PPIs derived from UniProt, IntAct, and Reactome are considered curated, high-quality data. To further expand our dataset, we also download species-specific subsets from STRING for the four selected species, and retain only interactions with combined score greater than 0.7 as high-confidence PPIs. These are merged with the curated PPIs to form our complete raw dataset. To ensure fair evaluation of PPI models and minimize data redundancy, we first filter proteins to retain only those with sequence lengths between 50 and 1000. We then apply MMSeqs2 to cluster protein sequences and keep only those pairs with sequence identity 40%. Additionally, we remove proteins that share the same entry name across different species to eliminate functionally redundant cross-species homologs. C.3 Statistical Analysis We conduct statistical analysis of the fully processed PPI datasets obtained through the above pipeline, with results summarized as follows: Fig. 6 illustrates the distributions of protein sequence lengths and node degrees across the four species in our processed PPI dataset. Protein lengths are constrained between 50 and 1000 amino acids, with most sequences falling in the 250500 aa range, resembling roughly normal distribution. The Human PPI network, benefiting from richer data coverage, exhibits smoother and broader degree distribution with generally higher values. In contrast, the other three species show lower node degrees overall, indicating sparser interaction networks. Tab. 5 summarizes key statistics of the processed PPI datasets across the four selected species. The number of proteins and interactions varies notably after the filter process, with Human (contributing the training set) having the largest network (129,861 interactions among 10,090 proteins), while Yeast and Ecoli have more compact networks. In terms of the topology, Human exhibits the smallest diameter and average path length, suggesting denser and more interconnected PPI network. In contrast, Arath shows larger diameter and average shortest path length, indicating more fragmented or modular interaction structure. Figure 6: Distribution of protein sequence lengths (top row) and node degrees in the PPI network (bottom row) across four species in our dataset. (The y-axis indicates the density for both distributions)"
        },
        {
            "title": "D Task Documentation",
            "content": "This section outlines the formal definitions of each task included in PRING and details the corresponding evaluation metrics used to assess model performance. 20 (2) (3) Table 5: Summary statistics of processed PPI dataset across species."
        },
        {
            "title": "Species",
            "content": "PPI Count Protein Count Avg. Degree Avg. Seq. Len Diameter Avg. Path Len"
        },
        {
            "title": "Human\nArath\nYeast\nEcoli",
            "content": "129,861 23,584 15,921 17,452 10,090 5,025 3,173 3,196 25.74 9.39 10.04 10.92 429.55 417.10 413.52 310.67 11 15 19 14 3.64 5.62 5.23 4. D.1 Topology-oriented Task This task assesses whether PPI prediction models can learn and preserve the underlying topological structure of PPI networks. To this end, PRING requires each model to reconstruct the PPI network from pairwise predictions and evaluates the result using graph-level metrics. Formally, given PPI graph with proteins, each protein is represented by features xi = {si, ci} for = 1, . . . , , where si denotes the amino acid sequence and ci denotes any auxiliary context information. model predicts an interaction label for each protein pair (i, j), where 1 < , as follows: pred i,j = (xi, xj) {0, 1}. Here, pred i,j = 1 indicates predicted interaction, and 0 indicates no interaction. The reconstructed PPI network is then formed by aggregating all predicted positive interactions: Gpred = Aggregate(cid:8)(i, j) pred i,j = 1, 1 < (cid:9). Finally, the reconstructed graph Gpred is evaluated against the ground-truth graph Gtrue using suite of graph-level metrics detailed below. We consider two subtasks to evaluate the intra-species and cross-species capabilities of PPI prediction models. Below, we detail the task definitions and corresponding evaluation protocols. D.1.1 Comparison of Different Traversal Strategies Since our primary focus is on proteinprotein interaction (PPI) networks, we consider three widely used graph traversal strategies for sampling: breadth-first search (BFS), depth-first search (DFS), and random walk (RW). Each of these strategies captures different topological and biological characteristics of PPI networks. The distinct features they emphasize are summarized below, as shown in Tab. 6. Table 6: PPI subgraph structural characteristics and biological network types corresponding to different sampling strategies."
        },
        {
            "title": "DFS",
            "content": "RW Locally dense structures, such as protein complexes, modules Chain/tree structures, such as signaling or metabolic pathways Hub-dominant such as regulatory networks and panfunctional networks structures, Ribosomal protein complex, intracellular signaling module MAPK signaling pathway, glycolysis metabolic pathway TP53 regulatory network, transcription factor network [101] [102] [103] D.1.2 Intra-species PPI network Construction This task focuses on the Human species, following the same intra-species setting used in prior studies [17, 19]. The full Human PPI graph is partitioned into training and test subgraphs with an 8:2 split, using leakage-free protocol to avoid data contamination. This results in training subgraph of 8,072 proteins and test subgraph of 2,018 proteins with no protein overlap. To examine how subgraph topology and size affect model performance, we sample 500 subgraphs from the test sets, each ranging from 20 to 200 nodes, using three traversal algorithms: breadth-first search (BFS), depth-first search (DFS), and random walk (RW). These traversal strategies can be associated with distinct biological network types. BFS captures densely connected local neighborhoods, making it suitable for modeling protein complexes and functional modules characterized by high intra-cluster 21 interaction density [101]. DFS, in contrast, tends to generate chain-like or tree-structured subgraphs, resembling the topology of signaling or metabolic pathways, where interactions follow directional flow [102]. Random walk explores the network probabilistically and is more likely to visit hub proteins, aligning with regulatory or hub-based networks that reflect transcriptional or cellular control systems [103, 104]. By leveraging these biologically motivated sampling strategies, we systematically evaluate how PPI models generalize across different local topologies. Since existing PPI models are trained on protein pairs, we generate training data by sampling positive and negative protein pairs from the training subgraph in 1:1 ratio, and further divide them into 80% for training and 20% for validation, following protocols in [46]. In total, this yields 85,824 training pairs, 21,456 validation pairs, and 500 test subgraphs per traversal strategy for topological evaluation. D.1.3 Cross-species PPI network Construction To assess cross-species generalization, models trained on Human PPIs are used to reconstruct networks in three additional species: Arath, Ecoli, and Yeast. For each species, we sample 500 subgraphs ranging from 20 to 200 nodes using BFS, DFS, and RW, yielding 500 test graphs per traversal method per species for evaluation. D.1.4 Evaluation Metrics Both intra-species and cross-species PPI network construction tasks are evaluated under five graphlevel metrics, which are widely used in graph-generation tasks. Graph Similarity (GS) [79]. Graph similarity primarily quantifies the edge differences between the predicted and ground-truth graphs, as the node set in the PPI graph remains unchanged. Mathematically, it is defined as: Graph Similarity = 1 , (4) ˆA A1 + ˆE where {0, 1}N is the adjacency matrix of the ground-truth graph Gtrue, ˆA {0, 1}N is the adjacency matrix of the predicted graph Gpred. and ˆE are the sets of edges in the ground-truth and predicted graphs, respectively. This metric ranges between 0 and 1, and higher value indicates better alignment between the prediction and the ground truth. Relative Density (RD) [80]. Relative density evaluates the extent of overor under-prediction by comparing the edge density of the predicted network with that of the ground-truth network. Relative Density = ˆE/(cid:0)N E/(cid:0)N 2 2 (cid:1) (cid:1) = ˆE (5) where is the number of proteins in the PPI network, and and ˆE denote the number of edges in the ground-truth and predicted graphs, respectively. value of RD > 1 indicates over-prediction, while RD < 1 suggests under-prediction. An RD of 1 implies that the predicted graph has the same edge density as the ground truth. Degree Distribution (Deg.) [81]. Degree distribution computes the discrepancy between node degree distributions of the predicted and ground-truth networks using maximum mean discrepancy (MMD) [82], providing quantitative assessment of global structural differences in terms of connectivity patterns. However, the direct output of MMD lacks reference scale and does not reflect the relative extent of discrepancy. To address this limitation, we follow prior work [81] and report normalized ratio: Degree Distribution (MMD) = , (6) MMD2(pred, test) MMD2(test, test) where pred and test denote sets of predicted and ground-truth PPI networks, respectively. Each network is first transformed into degree histogram (i.e., vector summarizing its node degree distribution), and the MMD2 is then computed over these histogram sets. Ideally, the range of Deg. is equal to or larger than 1. lower value of Deg. indicates better alignment of degree distributions, with value close to 1 suggesting that the predicted networks are as similar to the test set as the test networks are to themselves. Clustering Coefficient (Clus.) [81]. Clustering coefficient uses MMD to measure the discrepancy between the distributions of local clustering coefficients in the predicted and ground-truth networks, thereby assessing the preservation of community structure. As with degree distribution, we compute the discrepancy in relative form by normalizing the MMD2 between the predicted and test sets. Each network is transformed into histogram of local clustering coefficients, and the MMD is applied over these aggregated distributions. Spectral [83]. Spectral calculates the discrepancy between eigenvalue spectra of normalized Laplacian matrices of predicted and true networks using MMD, reflecting global structural alignment. Again, we report the relative discrepancy by normalizing the MMD2 between predicted and test sets. Each network is represented by vector of eigenvalues of its normalized Laplacian matrix, and MMD is applied over the resulting spectral distributions. D.2 Function-oriented Task Besides topology-based evaluation, we introduce three function-oriented tasks closely aligned with real-world biological applications. These tasks assess how well the reconstructed PPI networks preserve biologically meaningful properties and evaluate the practical applicability of existing models. D.2.1 Protein Complex Pathway Prediction Complex pathways refer to biological processes involving multiple proteins that interact with each other to perform coordinated cellular functions, which typically form densely connected subgraphs within the larger PPI network. Accurately reconstructing these pathways can enhance our understanding of disease mechanisms and support the development of targeted therapies [86, 87]. In this task, the model first predicts pairwise protein interactions based on the input protein complex pathways, then aggregates these predictions to construct predicted subgraphs Gpred, which are subsequently evaluated against the ground truth Gtrue. For fair evaluation, we curate 235 human protein complex pathways from Reactome [38], ensuring no protein overlap with the Human training graph. Pathway sizes range from 4 to 20 proteins. The models performance is evaluated using the following metrics: Pathway Precision (PP). The proportion of predicted interactions within the complex pathway that are also present in the ground-truth subgraph: Pathway Precision = Epred Etrue Epred , (7) where Epred and Etrue represent the sets of predicted and ground-truth interactions, respectively. Pathway Recall (PR). The proportion of ground-truth interactions within the complex pathway that are successfully recovered by the prediction: Pathway Recall = Epred Etrue Etrue , (8) where Epred and Etrue represent the sets of predicted and ground-truth interactions, respectively. Pathway Connectivity (PC). The fraction of predicted pathway subgraphs that form single connected component, reflecting biological plausibility: Pathway Connectivity ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:104) Gpred (cid:105) is connected , (9) where is the total number of pathways. D.2.2 GO Enrichment Analysis GO enrichment connects network topology with biological function, supporting gene annotation and pathway discovery [88]. This task evaluates whether reconstructed PPI networks preserve functional coherence by comparing enriched GO terms of predicted communities to those of the ground-truth 23 network. Specifically, we reconstruct the Human test subgraph (2,018 proteins) using trained PPI prediction model, detect communities via the Louvain algorithm [89], and perform GO enrichment using g: Profiler [90] tool across three ontologies: Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). Evaluations are conducted under two metrics: Functional Alignment (FA). Measures the average Jaccard similarity between the enriched GO terms of each predicted cluster and its best-matching ground-truth cluster: Functional Alignment ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 max pred pred true true , (10) where is the number of predicted clusters, and pred ith predicted cluster and jth ground-truth cluster, respectively. , true are the sets of enriched GO terms in the higher functional alignment score (0 FA 1) indicates better functional alignment between predicted and ground-truth communities Consistency Ratio (CR). Compares the average within-cluster GO term similarity in the predicted network to that in the ground-truth network: CR ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 Jaccard(T pred ) Jaccard(T true max , ) (11) where is the number of predicted clusters, pred denotes the set of GO terms associated with proteins in the ith predicted cluster, and Jaccard(Ti) is the average pairwise Jaccard similarity between the GO term sets of all protein pairs within cluster i. For each predicted cluster i, the denominator uses the best-matching ground-truth cluster j. consistency ratio (0 CR 1) close to 1 indicates stronger preservation of functional coherence within clusters. D.2.3 Essential Protein Justification Essential proteins are critical for an organisms survival and often occupy central positions in PPI networks [91, 92]. reliable model should differentiate essential from non-essential proteins based on node degree or centrality in the reconstructed network [93]. In this task, we use the Human test subgraph (2,018 proteins) as the evaluation set and annotate essential and non-essential proteins using CCDS [94] and UniProt [37]. Specifically, we select 100 essential and 100 non-essential proteins whose centrality scores [91] differ significantly, with value less than 104 (See Fig. 4(a)). Trained models are used to reconstruct the Human test graph, and performance is assessed using the following metrics: Precision@K (P@K). Measures the proportion of essential proteins among the top nodes ranked by network centrality scores: Precision@K = TopK Essential (12) where TopK denotes the set of top-K nodes ranked by centrality in the reconstructed graph, and Essential is the set of ground-truth essential proteins. We set = 100 in this study. Distribution Overlap (DO). Quantifies the distribution overlap between the centrality score of essential and non-essential proteins using the area under the minimum of their probability density functions (PDFs): Distribution Overlap = (cid:90) min (pessential(x), pnon-essential(x)) dx (13) where pess(x) and pnon(x) are the estimated PDFs of the centrality scores for essential and nonessential proteins, respectively. Lower values of distribution overlap indicate better separability between essential and non-essential proteins, while higher P@K reflects stronger prioritization of biologically important proteins."
        },
        {
            "title": "E Baseline Models",
            "content": "This section introduces the baseline models evaluated in our benchmark. E.1 Sequence Similarity-based Method SPRINT [52]. SPRINT is high-throughput, alignment-based method that identifies PPIs by searching for local sequence similarities between query proteins and known interacting pairs. It uses spaced seed hashing mechanism to locate short, conserved motifs and then filters out nonspecific matches to improve precision. By avoiding supervised learning, it remains model-free and does not require negative sampling, making it computationally efficient to full proteome predictions. We access the codebase via https://github.com/lucian-ilie/SPRINT. E.2 Naive Sequence-based Method PIPR [48]. PIPR employs Siamese residual RCNN to learn complex interaction features directly from protein sequences. Each protein is embedded using property-aware amino acid encoding, followed by convolutional and recurrent layers to extract both short-term motifs and long-range dependencies. PIPR is trained end-to-end to predict whether two sequences interact, making it effective for pairwise classification, though limited in capturing higher-level network structures. The source code is provided in https://github.com/muhaochen/seq_ppi. D-SCRIPT [17]. D-SCRIPT bridges the gap between sequence and structure by predicting physical interactions via inferred inter-residue contact maps. It uses pretrained structure-aware embeddings, which are then projected into contact space using convolutional scoring module. key innovation is its regularization mechanism that enforces structural plausibility, even without access to actual 3D structures. The prediction of the contacts between proteins enables its interpretability. We use the recommended implementation method in https://d-script.readthedocs.io/en/stable/ usage.html. Topsy-Turvy [56]. Topsy-Turvy integrates bottom-up sequence modeling with top-down global network patterns. During training, it incorporates graph-derived supervision to infuse sequence embeddings with knowledge of network-level interactions. The model retains only sequence input at test time, allowing practical use in non-model organisms. We use the recommended implementation method in https://d-script.readthedocs.io/en/stable/usage.html. E.3 Protein Language Model-based Method PPITrans [19]. PPITrans leverages protein language models (PLMs) for generalizable PPI prediction, using ProtT5 [62] to encode contextual sequence embeddings. transformer encoder is applied to model pairwise interactions from these embeddings. The model demonstrates strong cross-species performance, suggesting that PLM-derived representations capture semantic and functional signals relevant to interaction propensity and are transferable across evolutionary distances. We follow the codebase in https://github.com/LtECoD/PPITrans. TUnA [64]. TUnA extends PLM-based PPI prediction by incorporating uncertainty estimation. It uses ESM-2 embeddings [25] as input to transformer layers and employs spectral-normalized neural Gaussian process as the output layer to quantify prediction confidence. This design enables TUnA to provide calibrated interaction scores, which are particularly valuable for prioritizing highconfidence predictions in large-scale or experimental screening settings. Implementation is followed by https://github.com/Wang-lab-UCSD/TUnA. PLM-interact [63]. PLM-interact jointly encodes protein pairs by concatenating sequences and feeding them into ESM-2 [25], enabling attention mechanisms to directly model inter-sequence dependencies. This paired encoding strategy captures interaction-specific features, such as coevolutionary signals and cross-residue contacts, allowing the model to effectively learn the language of interactions. The official implementation is provided in https://github.com/liudan111/ PLM-interact. E.4 Structure-based Method Struct2Graph [65]. Struct2Graph models PPIs using graph neural networks (GNNs) over residuelevel graphs constructed from 3D protein structures. It employs dual GNN encoder with shared weights and mutual attention mechanism to highlight interfacial regions between proteins. By capturing spatial geometry and residue connectivity, the model effectively identifies structural determinants of binding. key limitation, however, is its reliance on experimentally resolved structures from the Protein Data Bank (PDB), which constrains its applicability to proteins with unknown 3D conformations. The source code is provided in https://github.com/baranwa2/Struct2Graph. TAGPPI [66]. TAGPPI introduces structure-aware learning by combining predicted contact maps with sequence features. It uses residueresidue graphs as input to GNN and supplements them with 1D CNN over the raw sequence. The fusion of these modalities enables TAGPPI to benefit from structural topology even when only sequence input is available. The model outperforms prior sequence-only methods and shows robustness across different organisms, validating the utility of predicted structures for downstream functional tasks. The source code is given in https://github. com/xzenglab/TAGPPI. RF2-Lite [70]. RF2-Lite is lightweight derivative of RoseTTAFold2 [98] tailored for highthroughput PPI screening. It uses paired multiple sequence alignments (MSAs) and reduced multitrack architecture to predict inter-chain distance maps efficiently. Although less accurate than full-fledged structure predictors, RF2-Lite achieves strong performance with dramatically lower inference time, making it suitable for proteome-scale predictions. We follow the recommended implementation in https://github.com/SNU-CSSB/RF2-Lite. Chai-1 [99]. Chai-1 is state-of-the-art, multi-modal foundation model developed by the Chai Discovery team for molecular structure prediction. It is highly effective at predicting the structures of proteins, small molecules, DNA, RNA, and covalently modified compounds. Unlike other structure prediction models that rely heavily on multiple sequence alignments (MSAs), Chai-1 can perform effectively in single-sequence mode while maintaining high accuracy. standout feature of Chai-1 is its ability to incorporate experimental data, such as epitope conditioning, to enhance prediction accuracy, particularly in antibody-antigen interactions. This integration can significantly improve performance, making it valuable tool in antibody engineering and drug discovery. We use the source code provided in https://github.com/chaidiscovery/chai-lab."
        },
        {
            "title": "F Experimental Settings",
            "content": "We follow the recommended hyperparameters provided in each models official codebase, as these settings have demonstrated strong performance on their respective benchmarks. We assume these configurations to be robust and generalizable across diverse evaluation scenarios. Specifically, PLM-interact (35M and 650M variants) is used in inference mode with publicly released pre-trained weights. Due to computational constraints, RF2-Lite is evaluated on only 10% of the test graphs (a graph is randomly sampled for each node size to evaluation). All other baseline models are trained from scratch. In addition, we conduct case study of Chai-1 on just three Human graphs (node sizes range from 20 to 60). For the SPRINT, which does not need GPUs for inference, the experiments are done on Kunpeng-920, 64-core platform. For the model trained from scratch, NVIDIA A100-PCIE-40GB is used with the same CPU support. Additionally, for the RF2-Lite and Chai-1, the experiments are conducted on NVIDIA-A800-SXM4-80GB. Since RF2-Lite requires MSAs as input, we use Uniref90 [97] as the search database with GPU-accelerated version of MMseq2 [105]. Given that Chai-1 performs well without the use of MSAs or templates, and that generating MSAs is computationally intensive, we opt to use single-sequence inputs exclusively."
        },
        {
            "title": "G Experimental Results",
            "content": "We present detailed results for all evaluated models across both topology-oriented and functionoriented tasks. The following sections offer comprehensive comparison of model performance, highlighting their ability to preserve structural properties and capture biologically meaningful patterns. 26 For topology-oriented tasks, we additionally report standard classification metrics for all baseline models to provide complementary insights. The test dataset used for classification follows the evaluation protocol established in prior work [46]. Additionally, we include case study on Chia-1 [99], state-of-the-art protein structure prediction model, to evaluate its potential in PPI prediction. Finally, we conduct scaling-law analysis to investigate how model size correlates with graph-level performance, offering insights into the trade-offs between model complexity and predictive fidelity. G.1 Topology-oriented Task Table 7: Graph-level results of intra-species (Human) PPI network construction task. We use three color scales of blue to denote the first, second, and third best performance. RD closer to 1 is better. Sampling Method BFS DFS RW Category Model Seq. Sim. SPRINT Naive Sequence PIPR D-SCRIPT Topsy-Turvy PLM PPITrans TUnA PLM-interact (35M) PLM-interact (650M) Structure Struct2Graph TAGPPI RF2-Lite GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral 0.227 0.209 0.215 0.183 0.362 0.342 0.383 0.396 0.27 0.283 0. 1.61 4.39 1.13 1.74 3.39 2.99 2.29 1.64 5.58 4.56 0.524 10.9 81.7 15.4 17. 52.2 47.8 24.4 30.9 95.1 70.8 43.8 12.5 40.5 18.8 11.4 29.1 30.2 16.0 14.8 50.5 36.2 31. 11.2 36.2 29.7 14.6 26.3 22.9 12.8 16.5 39.2 29.5 34.9 0.145 0.0749 0.123 0. 0.314 0.289 0.322 0.350 0.132 0.143 0.243 5.91 12.3 6.40 3.22 4.54 4.44 2.85 2.03 10.8 9.94 1. 316 518 302 180 418 416 209 236 508 491 226 207 335 237 270 272 78.2 63.6 321 309 39.3 53.8 77.0 67.0 65.9 46.6 46.0 36.3 42.2 67.6 65.3 61. 0.178 0.196 0.209 0.232 0.449 0.450 0.429 0.491 0.237 0.256 0.171 1.32 5.75 5.82 4. 2.57 2.23 2.47 1.76 5.83 4.87 0.514 17.9 165 134 104 71.9 55.4 52.6 50.3 155 127 81. 26 107 64.8 64.8 46.1 42.5 22.6 21.2 103 74.2 73.9 15.1 41.1 52 36. 19.6 13.7 13.6 15.4 38.3 28.6 60.9 Avg. Rank 3 11 8 5 5 4 2 10 9 7 Table 8: Binary classification results of intra-species (Human) test set. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequence",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI Accuracy Precision Recall F1 AUPR Avg. Rank 0.545 0.559 0.505 0. 0.674 0.647 0.640 0.671 0.515 0.545 0.607 0.557 0.597 0.596 0.657 0.676 0.653 0.735 0.511 0. 0.257 0.581 0.104 0.120 0.738 0.563 0.597 0.534 0.696 0.672 0.361 0.568 0.144 0. 0.694 0.614 0.623 0.619 0.589 0.596 0.527 0.565 0.539 0.541 0.769 0.720 0.706 0.707 0.507 0. 7 5 10 9 1 1 4 3 8 6 G.1.1 Intra-species PPI Network Construction Tab. 7 and Tab. 8 report the performance of all models on graph-level and binary classification metrics, respectively. detailed analysis of the graph-level results is provided in Sec. 4.3.1. Overall, the model rankings based on binary classification metrics are broadly consistent with those based on graph-level metrics, with PLM-based approaches outperforming other model types. Notably, PPITrans and TUnA achieve the highest classification scores, while PLM-interact demonstrates superior performance in reconstructing the overall PPI network topology. Table 9: Graph-level results of cross-species (Human to Arath) PPI network construction task. We use three color scales of blue to denote the first, second, and third best performance. RD closer to 1 is better. Sampling Method BFS DFS RW Category Model Seq. Sim. SPRINT Naive Sequence PIPR D-SCRIPT Topsy-Turvy PLM PPITrans TUnA PLM-interact (35M) PLM-interact (650M) Structure Struct2Graph TAGPPI RF2-Lite GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral 0.200 0.211 0.167 0. 0.330 0.310 0.304 0.364 0.241 0.258 0.104 1.27 5.23 1.31 1.91 4.97 4.47 2.50 1.59 7.10 5.89 0. 10.5 101 21.6 20.6 59.6 62.9 23.8 25.8 118 87.7 49.8 15.7 67.4 27.0 20. 47.7 45.3 13.5 13.7 76.4 55.5 87.2 21.0 33.9 37.5 15.6 32.5 26.1 20.2 19.2 37.7 29.5 52. 0.149 0.0886 0.111 0.153 0.250 0.242 0.227 0.276 0.130 0.145 0.161 2.06 11.9 8.09 2. 6.62 6.41 4.26 2.38 14.2 12.8 1.08 9.11 80.7 55.6 13.0 62.0 68.6 35.4 37.0 83.9 82.0 7. 108 306 241 123 252 248 124 108 349 328 167 40.5 64.9 67.1 61. 54.9 48.6 34.6 32.7 69.8 64.0 43.9 0.204 0.847 0.219 0.247 0.267 0.423 0.382 0.339 0. 0.242 0.265 0.184 5.47 6.27 3.90 2.79 2.91 1.78 1.26 5.38 4.24 0.442 11.8 123 118 60. 45.6 58.7 13.5 22.7 109 79.0 30.0 20.8 81.1 57.0 33.0 28.9 30.2 7.49 12.6 60.9 45.1 51. 28.2 35.7 53.3 30.6 28.8 22.6 16.3 19.2 30.9 22.4 43.8 Avg. Rank 10 8 4 6 5 2 1 11 8 7 27 Table 10: Binary classification results of cross-species (Human to Arath) test set. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequene",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI Accuracy Precision Recall F1 AUPR Avg. Rank 0.552 0.579 0.509 0. 0.748 0.702 0.621 0.661 0.507 0.522 0.702 0.568 0.734 0.696 0.757 0.758 0.706 0.811 0.505 0. 0.181 0.659 0.118 0.118 0.731 0.593 0.413 0.420 0.671 0.655 0.287 0.610 0.155 0. 0.744 0.665 0.521 0.554 0.576 0.578 0.537 0.586 0.570 0.556 0.827 0.780 0.677 0.757 0.503 0. 7 4 9 10 1 2 5 3 7 6 Table 11: Graph-level results of cross-species (Human to Yeast) PPI network construction task. We use three color scales of blue to denote the first, second, and third best performance. RD closer to 1 is better. Sampling Method BFS DFS RW Category Model Seq. Sim. SPRINT Naive Sequence PIPR D-SCRIPT Topsy-Turvy PLM PPITrans TUnA PLM-interact (35M) PLM-interact (650M) Structure Struct2Graph TAGPPI RF2-Lite GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral 0.242 0.196 0.168 0.142 0.348 0.332 0.314 0.354 0.256 0.279 0.136 1.04 4.35 1.14 1. 4.43 3.60 2.34 1.90 5.89 4.82 0.361 12.8 90.0 14.2 11.6 55.4 57.7 24.0 25.8 115 91.7 71. 20.2 49.9 26.5 14.2 35.1 37.8 12.5 12.7 68.4 58.8 91.0 16.5 30.2 34.8 10. 30.8 22.6 15.5 14.1 37.1 28.8 73.7 0.171 0.0957 0.106 0.130 0.200 0.219 0.189 0.258 0.139 0.154 0. 1.72 8.96 5.28 2.58 8.15 5.47 3.95 3.04 11.6 10.1 0.699 52.1 531 221 397 414 202 232 546 519 158 130 278 189 101 257 225 119 85.8 337 311 31.6 50.6 49.7 28.5 49.0 36.1 29.4 27.9 55.8 51.2 54.1 0.256 0.173 0.202 0. 0.339 0.323 0.305 0.365 0.211 0.230 0.138 1.65 6.38 8.04 5.12 4.44 4.07 2.82 2.10 6.87 5.67 0. 19.8 180 173 100.0 95.3 119 49.7 51.3 178 151 89.1 42.1 92.2 90.3 50. 50.1 56.0 21.3 19.4 97.7 78.6 127 18.2 38.4 59.1 35.0 35.0 26.3 17.4 13.8 41.1 31.6 69. Avg. Rank 2 11 7 4 6 5 3 1 9 8 10 Table 12: Binary classification results of cross-species (Human to Yeast) test set. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequene",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI Accuracy Precision Recall F1 AUPR Avg. Rank 0.554 0.552 0.516 0. 0.729 0.689 0.615 0.660 0.511 0.528 0.620 0.551 0.554 0.529 0.700 0.688 0.655 0.743 0.508 0. 0.280 0.558 0.167 0.168 0.806 0.690 0.484 0.490 0.709 0.696 0.386 0.555 0.256 0. 0.749 0.690 0.557 0.590 0.592 0.596 0.534 0.563 0.523 0.506 0.815 0.766 0.667 0.725 0.506 0. 7 5 9 10 1 2 4 3 8 5 Table 13: Graph-level results of cross-species (Human to Ecoli) PPI network construction task. We use three color scales of blue to denote the first, second, and third best performance. RD closer to 1 is better. Sampling Method BFS DFS RW Category Model Seq. Sim. SPRINT Naive Sequence PIPR D-SCRIPT Topsy-Turvy PLM Structure PPITrans TUnA PLM-interact (35M) PLM-interact (650M) Struct2Graph TAGPPI RF2-Lite GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral 0.147 0.573 0.172 0.159 0.220 0.298 0.271 0.251 0.267 0.210 0.237 0. 5.27 1.78 5.54 4.94 5.80 3.68 4.88 5.22 4.88 0.395 51.6 144 20.6 132 105 127 83.8 144 136 116 37.5 46.3 30.5 43.6 40.8 51.2 26.7 38.5 54.1 54.1 86.7 41. 45.2 40.2 39.0 39.4 43.2 24.9 33.6 37.7 35.0 78.9 0.125 0.860 0.0814 0.108 0. 0.158 0.144 0.151 0.152 0.0994 0.110 0.197 11.6 10.1 3.26 9.27 10.9 5.68 7.65 11.0 10.7 0.585 780 624 312 711 664 459 589 771 756 273 116 359 286 126 347 345 218 371 368 252 55.2 63.1 58.8 50.1 54.8 56.9 37.8 44.1 57.6 56.6 77.0 0. 0.765 100.0 57.9 0.134 0.145 0.160 0.243 0.204 0.201 0.215 0.155 0.176 0. 8.57 10.3 10.8 5.65 8.17 4.85 6.31 7.15 6.81 0.388 437 410 422 327 392 273 326 393 383 129 119 152 92.0 125 67.5 89.5 108 105 131 44.1 58.0 75.6 70.4 45.5 57.0 33.2 42. 48.0 45.6 91.7 Avg. Rank 2 11 7 8 4 8 1 3 10 6 28 Table 14: Binary classification results of cross-species (Human to Ecoli) test set. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequene",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI"
        },
        {
            "title": "Accuracy Precision Recall",
            "content": "F1 AUPR Avg. Rank 0.521 0.573 0.509 0.507 0.617 0.624 0.580 0.609 0.500 0. 0.747 0.573 0.703 0.538 0.591 0.598 0.593 0.599 0.500 0.514 0.063 0.568 0.0782 0. 0.820 0.756 0.507 0.660 0.543 0.581 0.116 0.571 0.120 0.219 0.682 0.667 0.547 0.628 0.520 0. 0.516 0.578 0.548 0.539 0.718 0.675 0.610 0.638 0.500 0.508 7 5 6 1 1 4 3 10 7 Figure 7: Correlation between binary classification metrics and graph-level topology metrics. Each subplot shows the Pearson and Spearman correlation between classification metric and graph-level metric. Lastly, second-order polynomial regression is fit to the data to capture non-linear trends. G.1.2 Cross-species PPI network Construction We report detailed experimental results for both graph-level and binary classification metrics in the cross-species evaluation setting. Specifically, 29 Tab. 9 and Tab. 10 present the results on Arath, Tab. 11 and Tab. 12 report the performance on Yeast, and Tab. 13 and Tab. 14 summarize the results on Ecoli. These tables collectively illustrate how well models generalize to phylogenetically diverse species beyond the human training data. G.1.3 Correlation Between Binary Classification Scores and Network Metrics To investigate whether standard classification metrics reflect the structural quality of reconstructed PPI networks, we analyze their correlation with five topology-aware metrics across all evaluated models. As shown in Fig. 7, classification metrics such as recall, F1 score, and AUPR exhibit strong positive correlations with graph-level metrics like graph similarity (e.g., Pearson = 0.901 between AUPR and graph similarity). In contrast, accuracy and precision show weak correlations with certain topology metrics, particularly clustering coefficient and spectral distance, suggesting that high pairwise prediction scores may not always align with biologically meaningful network structures. These results underscore the importance of evaluating PPI prediction models beyond binary classification, as traditional metrics alone may fail to reflect structural coherence and biological relevance. Figure 8: Effect of subgraph node size on graph-level metrics. The error bars indicate the 95% confidence intervals. G.1.4 Impact of Subgraph Size on Topology-Oriented Performance In the test subgraphs, as previously described, the node size ranges from 20 to 200. To investigate the impact of node size on model performance, we visualize the results in bar plot, as shown in Fig. 8. We select PLM-interact (650M), the best-performing model overall, to assess how topological fidelity changes with subgraph scale. As node size increases, we observe gradual decline in 30 Graph Similarity, suggesting that reconstructing global structures becomes more challenging in larger subgraphs. Conversely, Relative Density, Clustering Coefficient (MMD), Degree Distribution (MMD), and Spectral (MMD) generally exhibit increasing trends, which indicate degraded alignment with the ground truth in these structural properties. These results suggest that larger PPI networks may exhibit more complex topological patterns, posing greater challenges for faithful reconstruction. G.2 Function-oriented Task Table 15: Results for protein complex pathway prediction task. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequene",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI RF2-Lite Pathway Recall Pathway Precision Pathway Connectivity Avg. Rank 0.526 0.160 0.225 0.240 0.583 0.526 0.550 0.549 0.413 0.487 0. 0.856 0.588 0.466 0.613 0.863 0.864 0.847 0.862 0.742 0.781 0.874 0.752 0.323 0.316 0. 0.818 0.794 0.880 0.887 0.608 0.713 0.135 6 10 11 9 1 3 3 2 8 7 G.2.1 Protein Complex Pathway Prediction As shown in Tab. 15, PLM-based models outperform others, with PPITrans achieving the best overall performance. PLM-interact (650M) ranks second, excelling in pathway connectivity. While RF2-Lite shows strong precision, its low connectivity limits overall ranking. Among non-PLM methods, only SPRINT performs competitively, indicating that PLMs better capture complex-level functional organization. Table 16: Results for GO enrichment analysis task. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Category",
            "content": "GO:BP GO:MF GO:CC"
        },
        {
            "title": "Category Model",
            "content": "Seq. Sim."
        },
        {
            "title": "Naive\nSequene",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI FA CR FA CR FA CR FA CR 0.174 0.468 0. 0.660 0.254 0.838 0.174 0.655 0.153 0.125 0. 0.183 0.160 0.264 0.337 0.150 0.180 0.480 0.462 0.333 0.632 0.404 0.684 0.789 0.469 0.468 0.145 0.113 0. 0.236 0.223 0.322 0.359 0.106 0.111 0.656 0.668 0.652 0.717 0.595 0.713 0.773 0.653 0.653 0.247 0.286 0. 0.330 0.395 0.419 0.408 0.207 0.212 0.827 0.904 0.919 0.919 0.868 0.959 0.970 0.826 0.826 0.182 0.175 0. 0.250 0.259 0.335 0.368 0.154 0.168 0.654 0.678 0.635 0.756 0.622 0.785 0.844 0.649 0.649 Avg. Rank 7 4 4 7 3 6 2 1 10 9 G.2.2 GO Enrichment Analysis In addition to the average scores across the three GO ontologies, Biological Process (BP), Molecular Function (MF), and Cellular Component (CC), we report detailed results for each category in Tab. 16. PLM-interact (650M) consistently achieves the highest performance across all GO categories, demonstrating strong functional coherence in its predicted networks. PLM-interact (35M) and PPITrans also perform competitively. Nevertheless, the low function alignment scores observed across all models suggest that current PPI prediction methods still struggle to capture fine-grained functional modules, indicating room for further improvement. 31 Figure 9: Network centrality distributions of essential and non-essential proteins. G.2.3 Essential Protein Justification The more detailed results analysis for the essential protein justification task is given in Tab. 17. Again, the PLM-interact series achieves the best performance Nevertheless, it should be noted that all baseline methods exhibit relatively large distribution overlap between essential and nonessential proteins in the reconstructed PPI networks. This reflects the limited ability of current PPI prediction models to preserve functional properties such as node centrality, which may hinder their effectiveness in supporting downstream biological applications, including drug discovery and disease gene prioritization. In Fig. 9, we visualize the network centrality distributions for essential and non-essential proteins across all baseline models and compare them to the ground truth. The results 32 Table 17: Results for essential protein justification task. We use three color scales of blue to denote the first, second, and third best performance."
        },
        {
            "title": "Model",
            "content": "Precision@100 Distribution Overlap Avg. Rank Seq. Sim."
        },
        {
            "title": "Naive\nSequence",
            "content": "PIPR D-SCRIPT Topsy-Turvy"
        },
        {
            "title": "PLM",
            "content": "PPITrans TUnA PLM-interact (35M) PLM-interact (650M)"
        },
        {
            "title": "Structure",
            "content": "Struct2Graph TAGPPI 0.54 0.52 0.47 0.43 0.48 0.57 0.8 0.77 0.45 0.43 0. 0.834 0.629 0.755 0.886 0.849 0.423 0.440 0.841 0.828 3 5 4 7 9 5 1 9 8 further highlight the need to improve models ability to preserve centrality signals to accurately identify essential proteins. G.3 Case Study We aim to assess the effectiveness of Chai-1 in reconstructing PPI networks. Owing to computational constraints, we select three representative graphs from the Human species, with node sizes ranging from 20 to 60. The detailed experimental setup is provided in Apx. F, and the corresponding results are summarized in Tab. 18. The results reveal three key observations: (1) Chai-1 fails to achieve satisfactory performance in reconstructing PPI networks, as evidenced by GS score of only 0.263 on graph with 60 nodes; (2) the performance decreases as the graph size increasesfor instance, the GS score drops from 0.425 at 20 nodes to 0.263 at 60 nodeshighlighting the challenge posed by larger topological scales; and (3) Chai-1 typically overpredicts interactions, resulting in high false positive rates, as reflected by an RD score of 6.14 on the 60-node graph, which is consistent with prior findings [70]. Furthermore, we visualize the predictions of Chai-1 in Fig. 10 to support our observations: while the ground-truth PPI networks are generally sparse, Chai-1s predictions are considerably denser. Table 18: Graph-eval results of the Chai-1 on three graphs. RD closer to 1 is better."
        },
        {
            "title": "Node Size",
            "content": "20 40 60 Category Model GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral GS RD Deg. Clus. Spectral"
        },
        {
            "title": "Structure",
            "content": "Chai-1 0.425 3.33 0.522 0.338 0. 0.346 4.380 0.449 0.384 0.563 0. 6.14 0.787 0.666 0.747 G.4 Scaling Analysis Fig. 11 illustrates the graph similarity score against the size of the PPI prediction models. Overall, larger models tend to achieve better network reconstruction performance. The best model, PLMinteract (650M), reaches graph similarity score of 0.41. Some structure-based models, such as Struct2Graph and TAGPPI, have comparable sizes to PLM-based models but fall much behind, achieving only half of the performance. This highlights that biologically-informed protein representations learned by PLMs play crucial role in accurate PPI network reconstruction. Nevertheless, the performance gains from increasing model size are relatively modest, suggesting that sheer model capacity alone is insufficient. Further improvements may require enhanced training objectives, or integration of complementary biological priors."
        },
        {
            "title": "H Broader Societary Impacts",
            "content": "This work introduces PRING, comprehensive benchmark for PPI prediction, designed to advance evaluation from pairwise classification toward biologically grounded, network-level assessment. By offering unified suite of topology-oriented and function-oriented tasks across multiple model organisms, PRING facilitates rigorous evaluation of models ability to reconstruct the structural topology of PPI networks, preserve functional coherence within biological modules, identify essential 33 Figure 10: Visualization of Chai-1 predictions on three subgraphs with node sizes between 20 and 60. Figure 11: Scaling analysis of graph similarity. proteins, and recover meaningful functional pathways. This enables more faithful modeling of cellular systems and supports applications in systems biology, disease mechanism discovery, and therapeutic target identification. Moreover, our empirical analysis highlights the limitations of current computational approaches, revealing gap between predictive accuracy and biological utility. Bridging this gap is essential for the responsible deployment of AI models in biomedical research and for accelerating foundational discoveries in life sciences. Nevertheless, the potential risks associated with such dual-use scenarios are non-negligible. As PPI prediction models become more accurate and scalable, partly enabled by benchmarks like PRING, they may inadvertently lower the barrier for malicious actors to rationally design harmful biological agents. For example, enhanced understanding of host-pathogen interaction networks could be exploited to 34 engineer synthetic pathogens that selectively disrupt immune functions or hijack critical cellular pathways [106, 107]. These scenarios, though speculative, underscore the need for vigilance in how such tools are disseminated and applied. Moving forward, it is essential to develop community norms and safeguards that promote transparency, ethical use, and oversight. This includes clear documentation of limitations, appropriate licensing, and collaboration with biosecurity experts to ensure that scientific progress does not come at the expense of public safety."
        }
    ],
    "affiliations": [
        "Fudan University",
        "National University of Singapore",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "The Chinese University of Hong Kong",
        "Westlake University",
        "Xian Jiaotong University"
    ]
}