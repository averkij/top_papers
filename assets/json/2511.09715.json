{
    "paper_title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control",
    "authors": [
        "Arman Zarei",
        "Samyadeep Basu",
        "Mobina Pournemat",
        "Sayan Nag",
        "Ryan Rossi",
        "Soheil Feizi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control."
        },
        {
            "title": "Start",
            "content": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control Arman Zarei1, Samyadeep Basu2, Mobina Pournemat1, Sayan Nag2, Ryan Rossi2, Soheil Feizi1 1University of Maryland 2Adobe Research 5 2 0 2 2 ] . [ 1 5 1 7 9 0 . 1 1 5 2 : r Figure 1. SliderEdit produces continuous edit trajectories in state-of-the-art instruction-based image editing models. Our method provides fine-grained and disentangled control over the intensity of edit attributes described in an instruction, allowing continuous transitions between editing strengths. Despite its effectiveness, SliderEdit is extremely lightweight and can be trained efficiently to transform state-of-the-art instruction-based image editing model into continuously controllable editing framework."
        },
        {
            "title": "Abstract",
            "content": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from multi-instruction prompt. However, these models apply each instruction in the prompt with fixed strength, limiting the users ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, framework for continuous image editing with fine-grained, interpretable instruction control. Given multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced sliderbased attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns single set of lowrank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-ImageEdit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.1 1. Introduction Recent advances in large-scale diffusion [16, 30, 33] and flow-matching models [9, 27] have revolutionized image synthesis, enabling unprecedented photorealism and seman1Project page is available at: https://armanzarei.github.io/SliderEdit tic fidelity. Building on these foundations, instruction-based image editing has emerged as powerful paradigm, allowing users to modify images through natural language commands [4, 24, 40, 47]. The latest state-of-the-art models, such as FLUX-Kontext [24] and Qwen-Image-Edit [40], can perform wide spectrum of manipulations, from global scene and style transformations to highly localized, finegrained edits, all within unified text-driven framework. Despite these advances, current instruction-based editing models remain inherently discrete: they apply edits in an all-or-nothing manner, offering limited control over how strongly each instruction is expressed. For example, given an image of dragon and multi-instruction prompt such as change the skin color to gold and make it exhale fire, existing models generate single fixed outcome for given prompt. While doing multiple generations may yield different variations, it does not allow systematic adjustment of individual edit strengths, such as turning the skin slightly gold versus bright metallic gold, or adding small flame versus large burst of fire. This lack of fine-grained, continuous control limits both user flexibility and interpretabilitythe two key properties for truly interactive image editing. To address this gap, we propose SliderEdit, framework for continuous image editing with fine-grained instruction control. Our goal is to extend state-of-the-art instructionbased editing models into systems that support continuous, disentangled, and interpretable control over the effects of individual editing instructions. Specifically, given multiinstruction prompt, SliderEdit assigns each instruction its own slider, allowing smooth adjustment of its influence between suppression, full application, and amplification (See Fig. 1). These sliders provide intuitive and flexible control over complex multi-instruction edits, operating seamlessly without any re-training or per-instruction fine-tuning. Our key insight is that the latent representations of modern multimodal diffusion transformers (MMDiTs) encode instruction semantics within localized token embeddings. By identifying and selectively modulating these tokens, we can gain fine-grained control over how individual instructions affect the output. Building on this observation, SliderEdit employs small set of learnable low-rank adaptation matrices that act directly on instruction-relevant token embeddings. These adapters are trained using novel and lightweight objective, the Partial Prompt Suppression (PPS) loss, which teaches the model how to suppress or neutralize the visual effect of specific instruction. The loss simply requires that the models output, when given the full prompt, matches the output produced when the target instruction is removed, making it intuitive, interpretable, and easy to optimize. Once trained, these low-rank adapters naturally yield continuous sliders by smoothly scaling their learned weights, enabling interpretable adjustment of each instructions influence. For single-instruction edits, we further extend this idea by applying the adapter across all image and text tokens, resulting in smoother edit trajectories. SliderEdit integrates seamlessly with existing state-ofthe-art instruction-based image editing models such as FLUX-Kontext and Qwen-Image-Edit, requiring only minimal additional training. Our approach provides unified framework for continuous and compositional control across diverse editing scenariosfrom subtle attribute adjustments and stylistic refinements to complex, multi-object scene manipulations. Through both quantitative evaluation and qualitative analyses, we show that SliderEdit delivers superior edit controllability and semantic disentanglement. In summary, our main contributions are: We are the first to explore and propose framework for continuous instruction-based image editing, enabling smooth, fine-grained, and interpretable modulation of edit intensity for individual instructions. We propose Partial Prompt Suppression loss, which enables efficient training of instruction-aware adapters that learn disentangled, continuous control over edit strengths. We demonstrate seamless integration of our framework with state-of-the-art foundation image editing models, achieving substantial improvements in edit consistency and user controllability. 2. Related Works 2.1. Image Editing Image editing methods have advanced rapidly in recent Early approaches built on diffusion priors enyears. abled flexible editing by perturbing and denoising input images [28]. Subsequent methods [2, 14, 20, 22, 38] formulated editing as steering the diffusion trajectory through optimization or conditioning while preserving image fidelity. With the emergence of instruction-based editing [4, 13, 26, 37, 47], models began to directly interpret natural language commands, allowing intuitive user control. More recently, large foundation models for instruction-based image editing [24, 40] have achieved remarkable versatility, performing both local and global modifications within unified architecture. Despite their impressive capabilities, these models lack fine-grained controllability, i.e., the ability to continuously adjust the strength of individual edits. Our work addresses this limitation through framework that enables continuous and interpretable instruction-level control. 2.2. Continuous Attribute Slider growing body of work in image generative modeling has explored continuous attribute control over generated images. Before diffusion models, much of this effort focused on learning structured and manipulable latent spaces in GANs and VAEs [1, 17, 19, 21, 35]. These approaches discovered semantically meaningful directions in the la2 tent space that correspond to interpretable visual attributes. With the emergence of text-to-image diffusion models, recent works [3, 6, 7, 10, 11, 14, 41] have extended continuous attribute control through per-attribute sliders or semantic embedding directions. Methods such as Concept Sliders [10] and Baumann et al. [3] train per-attribute LoRAs or editing directions within the text embedding space to achieve smooth attribute manipulation. While all these approaches mark significant progress toward controllable generation, they face notable limitations: many require training new LoRA or embedding direction per attribute, suffer from attribute entanglement, or degrade with multiple edits. They also primarily target text-to-image generation, offering limited or indirect applicability to real-image editing. In contrast, our method introduces novel and unified framework that generalizes slider-based continuous control to instruction-based image editing. It eliminates the need for per-attribute retraining, supports multiple simultaneous edits, and remains robust across diverse editing scenarios and unseen attributes, while achieving significantly better performance on real-image editing tasks. 3. SliderEdit: Continuous Image Editing In this section, we address the problem of enabling finegrained control over individual editing instructions in multi-instruction prompt for image editing. Formally, given prompt = {P1, ..., PK}, where each Pi denotes distinct edit instruction (e.g., make her laugh, make her hair curly; see Fig. 2, top row), our goal is to allow the user to modulate the strength of each instruction independently. To this end, we aim to associate each instruction Pi with corresponding scaling factor βi [0, 1], allowing users to continuously control the strength of that specific editranging from fully suppressing it (βi = 0) to fully applying it (βi = 1), or even exaggerating it when βi > 1. In Section 3.1, we present the background of the MMDiT architecture and describe how text and image tokens are processed. Section 3.2 then examines how individual instructions Pi influence the generation process by tracing their effect through the models internal representations. This interpretability analysis provides key insights into where and how control can be applied. Building on this, Section 3.3 introduces our method for modulating each instructions strength via continuous scaling mechanism, enabling fine-grained control over multi-instruction prompts. 3.1. MMDiT Architecture Recent image editing models such as FLUX-Kontext and Qwen-Image-Edit are built upon the MM-DiT architecture, which features dual-branch structure: one for latent image tokens and one for text embeddings. Specifically, the latent image tokens x1, . . . , xN represent both the noisy vectors in the VAE latent space and the encoded latents of the con3 Instruction-token embedding interpolation for Figure 2. strength control. Interpolating between instruction and nulltoken embeddings produces intermediate edit strengths, demonstrating the potential for achieving fine-grained control through direct manipulation of intermediate instruction embeddings. ditioning source image, while the text tokens y1, . . . , yT are obtained by encoding the prompt using pretrained language model (e.g., T5). τ , 1, . . . , 1, . . . , τ +1 = <pad>, . . . , The prompt is first tokenized into τ , then padded with the special <pad> token to reach fixed length : {y <pad>}. These are passed through the T5 encoder to produce final text token embeddings y1, . . . , yT . The image tokens and text embeddings are then jointly processed by each MM-DiT block, where they interact through shared attention layers that enable cross-modal information exchange between visual and textual representations. = 3.2. Instruction-Level Interpretability Analysis Building on the previous sections description of token interactions in MMDiT, we next investigate how individual instruction tokens affect generation. Specifically, we analyze the subset {yu, yu+1, . . . , yu} corresponding to an edit instruction Ptarget. These embeddings carry the semantic signal responsible for the target edit, and we test whether their influence is localized or diffused through the network via targeted interventions. Specifically, within each attention block at layer ℓ, we intervene on the target instruction embeddings {yℓ u}, which represent the target instruction tokens input to that block (i.e., the embeddings after processing by the preceding layers). We linearly interpolate u+1, ..., yℓ u, yℓ Figure 3. Overview of the SliderEdit training pipeline. Learnable low-rank matrices are applied to the intermediate token embeddings corresponding to the target edit instruction. These adapters are trained using the Partial Prompt Suppression (PPS) loss, which encourages the model to suppress or neutralize the visual effect of the selected instruction tokens. them with the padding token embedding yℓ <pad>: (1 β) yℓ yℓ + β yℓ <pad>, for {u, . . . , u}. The interpolation coefficient β [0, 1] determines how much of the instructions information is preserved. Setting β = 1 effectively removes the instruction by replacing its embeddings with that of the padding token (i.e., no information), while β = 0 leaves the instruction fully intact. Figure 2 illustrates the resulting generations. The bottom row corresponds to β = 1, where the edit is entirely removed, while the middle row shows an intermediate, manually chosen β that partially applies the edit. These results demonstrate that the intermediate token embeddings corresponding to Ptarget are highly localized and show strong potential for achieving fine-grained control by directly manipulating their embeddings. While this analysis shows potential for controlling edits via simple embedding interpolation, this approach provides only limited and discontinuous modulation. To achieve stronger and smoother control, we propose robust method in the next section. 3.3. Fine-Grained Control of Edit Instructions Given an input Building on our interpretability findings, we introduce mechanism that enables continuous and independent control over each edit instruction in multi-instruction prompt. image Xorig and prompt = {P1, ..., PK} containing edit instructions, base image editing model produces an edited output P1,...,PK where all edits are applied simultaneously. Our objective is to learn flexible adapter Mθ(Pi) capable of suppressing or modulating specific instruction Pi within P. When this adapter is activated, the model should generate P1,...,Pi1,Pi+1,...,PK , effectively removing the influence of Pi while keeping other edits intact. editted editted Partial Prompt Suppression Loss. To train Mθ, we propose the Partial Prompt Suppression (PPS) objective. Using the frozen base model ϵ(Z, X, ), where denotes the noisy latents, the original image latents, and the text prompt, we first perform forward pass with the prompt excluding the i-th instruction Pi. We then require that the adapted model ϵMθ(Pi), when given the full prompt, produces an equivalent denoising direction: LPPS = ϵMθ(Pi)(Z, Xorig, P) ϵ(Z, Xorig, {Pi}) Intuitively, this objective teaches the adapter to neutralize the representation of the tokens corresponding to Pi throughout the model so that their visual effect disappears. In addition to PPS, we introduce simplified variant, Simplified Partial Prompt Suppression (SPPS). SPPS treats each edit prompt as single instruction (i.e., = P1) and applies the same suppression objective directly to P1 (See Figure 8). Despite its simplicity, SPPS yields highly robust and generalizable adapters, even for multi-instruction editing scenarios. Algorithm 1 outlines the overall training procedure of SliderEdit. Additional details on SPPS and its comparison with PPS are provided in Appendix 7.1. Selective Token LoRA. We instantiate Mθ as Selective Token LoRA (STLoRA)a lightweight, token-aware adapter. STLoRA learns low-rank updates for selected linear projections in the model but applies them only to the embeddings of target tokens corresponding to the suppressed instruction Pi. Formally, consider linear projection at layer ℓ where tokens (either image or text) are transformed as = ℓz. STLoRA introduces trainable low-rank matrices Aℓ and Bℓ with ℓ = BℓAℓ, updating only the selected target tokens: target = (W ℓ + ℓ)ztarget, others = ℓzothers. 4 Algorithm 1 Training SliderEdit Require: ϵ(Z, X, ): Image Editing Model, Mθ: Trainable Adapter, {(X (i) 1: for each training step do 2: orig, (i))}: Dataset Xorig, = {P1, . . . , PK} Sample Data ε (0, I), [0, 1] if use LSPPS then = {P Consider whole as single-instruction prompt 1} end if (1 t)ε + tXorig Pi Random target instruction from to suppress ϵ(Z, Xorig, {Pi}) ˆv ϵMθ(Pi)(Z, Xorig, P) LPPS = ˆv v2 Update θ via gradient descent on LPPS 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for Algorithm 2 ℓ θ (STLoRA / GSTLoRA) Require: ℓ: Base Linear Projection, {Aℓ, Bℓ}: LowRank Matrices, mode {STLoRA, GSTLoRA} Input: {x1, . . . , xN }: Image Tokens, {y1, . . . , yT }: Text Tokens, Pi: Target Instruction 1: ℓ = BℓAℓ 2: if mode = GSTLoRA then yi (W + )yi 3: xi (W + )xi 4: 5: else if mode = STLoRA then 6: 7: 8: 9: 10: end if 11: return {x1, . . . , xN }, {y1, . . . , yT } TokenIndices(Pi) yi (W + )yi yi yi xi xi yi {y1, . . . , yT } xi {x1, . . . , xN } Indices in {1, . . . , } (cid:55) Pi yi yi {y1, . . . , yT } xi {x1, . . . , xN } This selectivity ensures the adapter modifies only target token embeddings. Figure 3 illustrates the SliderEdit training pipeline, and Figure 8 shows the SPPS variant. Continuous Control via Scaling STLoRA Once trained, the LoRA adapter naturally supports continuous control through its scaling parameter [10, 18, 34]. We denote α θ as the adapter with scaled updates αWℓ for each layer. By varying α within predefined range [αmin, αmax], we obtain smooth continuum of effectsfrom complete suppression (α = 1) to full application (α = 0), and even exaggerated edits for α < 0. Note that the scaling parameter αi follows an inverse range compared to βi defined earlier. The two scales can be related through α = 1 β. Globally Selective Token LoRA While STLoRA effectively handles both singleand multi-instruction prompts by selectively modulating tokens corresponding to each instruction Pi, we introduce Globally Selective Token LoRA 5 (GSTLoRA) for the single-instruction setting. In this variant, all token embeddings (both text and image) are included in the adaptation, allowing LoRA updates to be applied globally across the representation space. This design provides stronger control and often yields higher-fidelity edits when manipulating single instruction, as the update can leverage global context rather than being limited to subset of intermediate text token embeddings. Algorithm 2 outlines the operation of STLoRA and GSTLoRA adapters. 4. Experiments We conduct comprehensive quantitative and qualitative evaluations of SliderEdit, showing that it performs robustly across wide range of instruction edits. In addition, we compare various baselines and SliderEdit variants, demonstrating that our method achieves superior results, offering continuous and precise control over edits. 4.1. Implementation details We use FLUX-Kontext and Qwen-Image-Edit as our base models. All models are trained with the ℓSPPS loss for simplicity and generalization, while LPPS provides stronger multi-instruction control for STLoRA (see Appendix 7.1). We set the LoRA rank to 16, keeping the adapters lightweight and efficient. Training uses small subset (1k8k samples) of the GPT-Image-Edit dataset [39]. Both STLoRA models are trained for 1,000 iterations, converging around 400 but extended for consistency. GSTLoRA on FLUX-Kontext is trained for 300 iterations. Overall, the training process is computationally very lightweight and data-efficient. Further details are provided in Appendix 8.1. 4.2. Qualitative Results In this section, we qualitatively evaluate the results of SliderEdit variants, demonstrating their effectiveness across diverse scenarios and editing capabilities. Figure 4 presents qualitative examples of GSTLoRA applied to the FLUX-Kontext model. As shown, our method produces smooth and continuous edit trajectories, enabling fine-grained control over the strength of edits. It effectively handles both local edits (e.g., adding makeup or modifying cars age) and global edits (e.g., changing the season or adjusting scene lighting). Additional examples, such as edits involving camera view or angle changes, as well as applications in text editing and face editing, are provided in Figures 14, 9, 15, and 16 in the Appendix. Figures 6 and 17 illustrate qualitative results of STLoRA for instructions containing two edit directions. The resulting 2D intermediate space exhibits smooth and continuous variations, allowing users to precisely control edit strengths along each direction to obtain desired outputs. Figures 1 and 10 provide additional cases with three edit directions. Figure 4. Qualitative Samples of GSTLoRA. Demonstrates smooth, continuous control over the strength of both local and global edits. To further explore the versatility of SliderEdit, we examine its performance on advanced tasks supported by stateof-the-art editing models. One such task is zero-shot personalization. Figure 5 shows an example where STLoRA is integrated with Qwen-Image-Edit to perform multi-subject personalization, followed by instruction-based scene editing. Our approach provides users with flexible, fine-grained controlby adjusting sliders, one can generate coherent series of images that naturally evolve, resembling narrative. This demonstrates the potential of SliderEdit as powerful tool for storytelling and creative content generation. 4.3. Quantitative Results In this section, we quantitatively evaluate the performance of STLoRA and GSTLoRA against multiple baselines. We assess their ability to achieve continuous, extrapolative, and disentangled control through quantitative metrics, providing an objective analysis of the smoothness and independence of the edit trajectories generated by each method. 4.3.1. Evaluation Set For quantitative evaluation, we construct facial editing benchmark with subjects of diverse genders, ages, and ethnicities, and define edit directions (e.g., make the hair curly, make the hair long). Original images are chosen so that target attributes are absent (e.g., straight, short hair). We evaluate each model under editing configurations containing γ instructions, sampling γ instructions from the available to form (cid:0)M (cid:1) prompts. For each instruction, the edit strength α varies within [αmin, αmax] across δ steps, yielding γ-dimensional edit space of δγ images per prompt. This structured space enables quantitative analysis of continuity, extrapolation, and disentanglement. γ 6 Figure 5. Controllable zero-shot multi-subject personalization with STLoRA. STLoRA enables smooth adjustment of each instructions strength to generate coherent, evolving image sequences, supporting story-like visual editing. (Best viewed from top-left to top-right, then bottom-right to bottom-left) 4.3.2. Metrics We employ several quantitative metrics to evaluate different aspects of the editing behavior, including continuity, extrapolation, and disentanglement. For each instruction edit, the model generates sequence of images at varying edit strengths, which are then analyzed using the following metrics. To measure how strongly each edit is reflected in the generated image, we use vision-language models. For each instruction (e.g., make the person laugh), we define corresponding descriptive prompt (e.g., person smiling) and compute imagetext similarity in the embedding space of VLMs such as CLIP [32], SigLIP [45], and BLIP [25]. This score measures how well the edit is executed. Extrapolation. Extrapolation measures the models ability to apply edits beyond the standard range, which is particularly useful when amplifying attributes such as facial expressions. We define the extrapolation score as the maximum VLMs similarity value, which indicates the strongest expression of the target attribute achieved by the model. Continuity. Given similarity scores s1, . . . , sδ for increasing α values, we expect them to vary smoothly and uniformly between min(si) and max(si). We quantify this using chi-squared statistic comparing the observed and expected counts of si across bins, where higher (χ2 agg/dof)1 indicates smoother edit trajectories. For 2D and 3D edit spaces, we apply the same test to assess the uniformity across the grids. For more details, refer to Appendix 8.2.1 Disentanglement. We evaluate how well the model applies an edit without altering unrelated factors such as idenFigure 6. Qualitative results of STLoRA on 2-instruction edit. The 2D grid shows smooth, continuous transitions, allowing precise and disentangled control over each instructions strength. tity or background. Identity preservation is measured via cosine distance in the ArcFace [8] embedding space, where lower values indicate better consistency. We further compute perceptual distances between edited and original images using LPIPS [46] (AlexNet [23], VGG [36]) and DINOv2 [5, 31], capturing both low-level perceptual and highlevel semantic changes to assess overall disentanglement. 4.3.3. Baselines We consider different baselines depending on the number of edit instructions γ used in the prompt. For the case of single-instruction setting (γ = 1), we compare GSTLoRA (Ours) and STLoRA (Ours) with Explicit CFG and Implicit CFG, all implemented on top of the FLUX-Kontext model, as well as prior methods ConceptSlider [10] and Continuous Attribute Control [3]. FLUXKontext is guidance-distilled model that internally approximates the effect of classifier-free guidance, allowing implicit control over edit strength but offering limited flexibility. To enable explicit control, we reconfigure it to perform guidance externally during inference. Concept-Slider and Continuous Attribute Control provide fine-grained attribute manipulation but rely on inversion techniques [12, 29], making them less effective for direct image editing. For multi-instruction edits (γ > 1), Explicit CFG, Implicit CFG, and GSTLoRA cannot independently control each edit direction, whereas STLoRA enables disentangled, per-instruction control. Since Concept-Slider and Continuous Attribute Control already perform poorly in singleinstruction settings, we omit them from this scenario. We evaluate STLoRA on both FLUX-Kontext and Qwen-Image7 Extrapolation Model Concept Slider [10] Cont. Attr. Control [3] Implicit CFG Explicit CFG [15] SliderEditSTLoRA SliderEditGSTLoRA CLIP 0.2581 0.2380 0.2607 0.2731 0.2632 0.2648 SigLIP BLIP 0.4680 0.0666 0.2082 0.0484 0.4658 0.0667 0.5717 0.0739 0.4762 0.0669 0.4862 0.0673 CLIP 0.1803 0.1891 0.1547 0.1993 0.2538 0.2998 Continuity Disentanglement SigLIP BLIP LPIPSalex LPIPSvgg DiNO 0.2140 0.2174 0.2071 0.1852 0.1973 0.2167 0.1405 0.2149 0.1906 0.1718 0.2465 0.2263 0.1071 0.1902 0.2495 0.1868 0.3062 0. 0.2495 0.2485 0.2611 0.3085 0.2345 0.2330 0.1719 0.1114 0.1513 0.1803 0.1830 0.2227 ID 0.7091 0.5519 0.2748 0.3415 0.2550 0.2675 Table 1. Quantitative results for single-instruction edits (γ = 1). SliderEdit yields smoother trajectories and better identity preservation. γ Model 2 3 FLUXKontext QwenImage-Edit FLUXKontext QwenImage-Edit FLUXKontext QwenImage-Edit Extrap.Avg Cont.Avg Dis.Avg 0.1912 0.2187 0.2509 0.3088 0.2762 0.3630 0.2709 0.2319 0.2409 0.2813 0.3691 0.4345 0.2630 0.3214 0.2776 0.3160 0.2970 0. Table 2. Quantitative results for multi-instruction edits. Both models show comparable performance in continuity. FLUX better preserves identity, while Qwen performs better in extrapolation. Edit. For more details on baselines, refer to Appendix 8.2.2. 4.3.4. Results Table 1 presents the quantitative comparison for singleinstruction prompts (γ = 1) using δ = 15 across different baselines and metrics, including continuity, extrapolation, and disentanglement. For fair comparison, continuity is calculated based on normalized scores across all methods. As shown, GSTLoRA achieves the highest continuity while maintaining strong disentanglement and satisfactory extrapolation performance. Notably, although one might expect Explicit CFG to perform comparably, both STLoRA and GSTLoRA significantly outperform it, demonstrating superior smoothness and control in edit strength. Figure 7 provides qualitative and quantitative examples for representative case. GSTLoRA produces remarkably smooth and continuous edit trajectory, in contrast to Implicit and Explicit CFG, which exhibit abrupt transitions and inconsistent edit intensities. This behavior is well captured by our quantitative metricson the left, the aggregated average similarity score across normalized VLM metrics increases gradually for GSTLoRA, whereas both CFG variants show sudden jumps. In terms of disentanglement, GSTLoRA also achieves lower identity drift and more stable visual consistency compared to the other methods. Refer to Appendix 8.2 for more comparison and other baselines. Table 2 presents quantitative results for multi-instruction prompts with γ 1, 2, 3 and using δ = 7, comparing FLUX-Kontext and Qwen-Image-Edit. Both models perform strongly: for single-instruction edits, STLoRA on FLUX achieves better performance in terms of continuity, whereas in the twoand three-instruction settings, Qwen demonstrates stronger results. Moreover, FLUX better preFigure 7. Qualitative and quantitative comparison of GSTLoRA with CFG baselines. GSTLoRA shows smooth edit trajectories with gradual similarity changes, unlike Implicit and Explicit CFG, which exhibit abrupt transitions and greater identity drift. serves identity and disentanglement, whereas Qwen performs better in extrapolation. However, as observed across all configurations, there consistently exists trade-off between continuity, extrapolation, and disentanglement. 5. Conclusion We introduced SliderEdit, unified framework for continuous, fine-grained instruction control in instruction-based image editing models. By training lightweight low-rank adapters with novel loss to disentangle and modulate instruction effects, SliderEdit enables smooth, interpretable control over edit strength. Integrated with state-of-theart models like FLUX-Kontext and Qwen-Image-Edit, it achieves superior controllability, visual coherence, and flexibility, laying the foundation for interactive, instructiondriven editing with continuous and compositional control."
        },
        {
            "title": "Acknowledgement",
            "content": "This project was supported in part by grant from an NSF CAREER AWARD 1942230, the ONR PECASE grant N00014-25-1-2378, AROs Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), MURI grant 14262683, DARPA AIQ DARPA AIQ grant HR00112590066 and an award from meta 31459300001."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Peihao Zhu, Niloy J. Mitra, and Peter Wonka. Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. ACM Transactions on Graphics, 40(3):121, 2021. 2 [2] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78777888, 2025. 2 [3] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Melvin Sevi, Vincent Tao Hu, and Bjorn Ommer. Continuous, subject-specific attribute control in t2i models by identifying semantic directions, 2024. 3, 7, 8, 13 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 2 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 7, 13 [6] Pin-Yen Chiu, I-Sheng Fang, and Jun-Cheng Chen. Text slider: Efficient and plug-and-play continuous concept control for image/video synthesis via lora adapters, 2025. 3 [7] Yusuf Dalva, Kavana Venkatesh, and Pinar Yanardag. Fluxspace: Disentangled semantic editing in rectified flow transformers, 2024. [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 7, 13 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 12 [10] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models, 2023. 3, 5, 7, 8, 13 [11] Rohit Gandikota, Zongze Wu, Richard Zhang, David Bau, Eli Shechtman, and Nick Kolkin. Sliderspace: Decomposing the visual capabilities of diffusion models, 2025. 3 [12] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. In European Conference on Computer Vision, pages 395413. Springer, 2024. 7, 14 [13] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing by attention modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69866996, 2024. 2 [14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations. 2, 3 [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 8 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [17] Xianxu Hou, Linlin Shen, Ke Sun, and Guoping Qiu. Deep feature consistent variational autoencoder, 2024. 2 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [19] Erik Harkonen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan controls, 2020. [20] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In The Twelfth International Conference on Learning Representations. 2 [21] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019. 2 [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 2 [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2012. 7, 13 [24] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 2 [25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 9 rectified flow for inversion and editing. In Forty-second International Conference on Machine Learning. 2 [39] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 5, 12 [40] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2 [41] Yuqi Yang, Dongliang Chang, Yuanchen Fang, Yi-Zhe SonG, Zhanyu Ma, and Jun Guo. Controllable-continuous color editing in diffusion model via color mapping, 2025. 3 [42] Arman Zarei, Keivan Rezaei, Samyadeep Basu, Mehrdad Saberi, Mazda Moayeri, Priyatham Kattakinda, and Soheil Feizi. Improving compositional attribute binding in textto-image generative models via enhanced text embeddings. arXiv preprint arXiv:2406.07844, 2024. 12 [43] Arman Zarei, Keivan Rezaei, Samyadeep Basu, Mehrdad Saberi, Mazda Moayeri, Priyatham Kattakinda, and Soheil Feizi. Understanding and mitigating compositional issues in text-to-image generative models. arXiv e-prints, pages arXiv2406, 2024. [44] Arman Zarei, Samyadeep Basu, Keivan Rezaei, Zihao Lin, Sayan Nag, and Soheil Feizi. Localizing knowledge in diffusion transformers. arXiv preprint arXiv:2505.18832, 2025. 12 [45] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 7 [46] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7, 13 [47] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. In Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025. 2 [26] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62546263, 2024. 2 [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations. 2 [29] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 7, 14 [30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 1 [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7, 13 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 7 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [34] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: In Any subject in any style by effectively merging loras. European Conference on Computer Vision, pages 422438. Springer, 2024. 5 [35] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing, 2020. 2 [36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 7, 13 [37] Kunal Swami, Raghu Chittersu, Pranav Adlinge, Rajeev Irny, Shashavali Doodekula, and Alok Shukla. Promptartisan: Multi-instruction image editing in single pass with complete attention control. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2 [38] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Simplified Partial Prompt Suppression (SPPS). SPPS applies the same suppression objective as PPS but treats the entire edit prompt as single instruction. During training, second (bottom-row) forward pass is performed to obtain neutralized imageeither using an empty prompt () or neutral textual instruction (e.g., keep the image the same). This simple formulation effectively teaches the adapter to suppress undesired edit effects and generalizes well to multi-instruction editing scenarios. 6. Related Works 6.1. Diffusion Models and Flow Matching Diffusion models belong to class of generative models based on stochastic differential equations (SDE). The core idea is to gradually corrupt data by adding noise through stochastic forward process until the original data distribution becomes simple Gaussian distribution. This process can be described as: dx = (x, t)dt + g(t)dWt, where (x, t) denotes the drift term, g(t) represents the diffusion coefficient, and dWt is the Wiener process (an infinitesimal step of Brownian motion, representing small random Gaussian perturbation). The model then learns the reverse process, which reconstructs the original data distribution from pure noise. Mathematically, this reverse-time SDE is written as: dx = [f (x, t) g2(t)x log pt(x)]dt + g(t)dWt, where log pt(x) is the score function, representing the gradient of the log-density of the data distribution at time t. Intuitively, the score function tells the model in which direction to move each noisy sample to recover the data distribution. In practice, diffusion models are trained to approximate this score function using neural network sθ(x, t). Training minimizes the score matching loss, defined as: EtU (0,T ),xpt(x)[λ(t)x log pt(x) sθ(x, t)2], Figure 9. Qualitative results of GSTLoRA on text editing. where λ(t) is time-dependent weighting function. Once trained, the model can sample new data by simulating the learned reverse process starting from Gaussian noise. Flow matching methods are closely related to diffusion models, designed for training Continuous Normalizing Flows. The key idea is to learn deterministic transformation that maps an initial noise distribution to the target data distribution by integrating an ordinary differential equation (ODE). The evolution of sample over time is governed by time-dependent vector field vθ(x, t), defined as: dx dt = vθ(x, t), where vθ(x, t) is neural network parameterizing the vector field to be learned. Training involves aligning this learned field with predefined target vector field vt(x), which describes how samples should flow from noise to data at each 11 time step. This is achieved by minimizing the flow matching loss: EtU (0,T ),xpt(x)[vθ(x, t) vt(x)2], where pt(x) represents intermediate distributions along the transformation path from the initial to the final data distribution. Unlike diffusion models, which rely on stochastic SDE trajectories involving random noise, flow matching employs deterministic ODE trajectories. This eliminates the stochasticity in sampling and generally leads to faster and more efficient training and inference. As result, flow matching can be viewed as computationally efficient deterministic counterpart to diffusion models. 7. SliderEdit: Continuous Image Editing 7.1. Simplified Partial Prompt Suppression Loss While the main Partial Prompt Suppression (PPS) objective requires selectivel suppressing an individual instruction Pi within composite prompt P, the Simplified PPS (SPPS) variant adopts more streamlined approach that reduces this complexity bit while maintaining strong generalization. In SPPS, each training sample is treated as singleinstruction editing instance, i.e., = {P1}. The model learns to suppress the visual influence of this sole instruction by minimizing the difference between the denoising prediction of the adapted model when conditioned on P1 and that of the frozen base model when the prompt is removed entirely (or replaced with prompt that acts as null instruction, e.g., keep the image the same). Formally, the loss follows the same structure as LPPS: LSPPS = ϵMθ(P1)(Z, Xorig, P1) ϵ(Z, Xorig, }) This encourages the adapter to learn how to neutralize the edit induced by P1, thereby isolating its corresponding representation within the model. Figure 8 visualizes the SPPS training pipeline. Despite its simplicity, SPPS offers several practiIt removes the need to parse multical advantages. instruction prompts or identify token-level boundaries between sub-instructions, allowing efficient training on general instruction-based editing datasets, including those containing only single-instruction pairs. Moreover, the adapters trained with SPPS exhibit strong robustness and compositional generalization, performing effectively even when applied to multi-instruction edits at inference time. However, PPS provides finer-grained supervision, leading to more disentangled and well-localized adaptations across different instruction dimensions, which results in better control when handling complex multi-instruction edits. 8. Experiments 8.1. Implementation Details We use FLUX-Kontext and Qwen-Image-Edit1 as our base models. All models are trained with the ℓSPPS loss, chosen for its simplicity, efficiency, and strong generalization. We observe that LPPS provides more robust and disentangled control for multi-instruction setups when used with STLoRA (see Appendix 8.3.1). Training is performed on small subset (1k8k samples) of the GPT-Image-Edit-1.5M dataset [39]. For STLoRA, we train both base models for 1,000 iterations with batch size of 8, observing early convergence around iterations 400500 but continuing to 1,000 for consistency. For GSTLoRA, we train FLUX-Kontext for 300 iterations with batch size of 4. We employ the AdamW optimizer with learning rate of 1 104, no warm-up, and train across all diffusion timesteps. All experiments are conducted on single NVIDIA H100-SXM GPU using mixed-precision (bfloat16) training with gradient checkpointing for memory efficiency. The LoRA modules have rank of 16 and zero dropout, and are applied to the Q, K, , and output projections of the attention layers, as well as to the two additional linear projections in each transformer block. These settings provide stable and memoryefficient training setup, enabling rapid convergence across all models. Overall, our training is computationally highly lightweight and data-efficient. Furthermore, consistent with prior observations in [9, 44], we found that training adapters on only subset of transformer blocks can achieve performance comparable to training all blocks. Also, following insights from [42, 43], applying adapters at every denoising timestep may not be necessary for effective editing. We leave comprehensive investigation of these efficiencyoriented design choices for future work. 8.2. Quantitative Results 8.2.1. Metrics Continuity. Given sequence of similarity scores {s1, . . . , sδ} corresponding to increasing α values, we expect these scores to change smoothly and approximately uniformly between min(si) and max(si). To quantify this, we compute chi-squared statistic, χ2 = δ (cid:88) i=1 (Oi E)2 , where Oi denotes the observed count in each bin (number of similarity scores sj falling within the i-th bin), and denotes the expected count per bin under uniform distribution (E = 1). We report (χ2 agg/dof)1 (dof :degrees of freedom) as our continuity metriclarger values indicate 1We adopt Qwen-Image-Edit-2509, an updated version with improved performance and stronger identity preservation. 12 Figure 10. Qualitative results of STLoRA on 3-instruction edit. The model demonstrates smooth and continuous control over the strength of each instruction in disentangled manner. Disentanglement. To evaluate disentanglement, we measure how well the model isolates the intended edit without affecting unrelated aspects, such as identity or First, we assess identity preservation usbackground. ing cosine distance in the identity embedding space obtained from ArcFace [8], where lower distances indicate stronger identity consistency. To capture more general visual changes, we compute feature distances between edited images Ii with the origin image using multiple perceptual metrics: LPIPS [46] (using both AlexNet [23] and VGG [36] backbones) and DINOv2 [5, 31]. While LPIPS focuses on low-level perceptual similarity, DINO captures higherlevel semantic consistency, allowing us to evaluate both appearance-level and structural disentanglement. 8.2.2. Baselines We consider different baselines depending on the number of edit instructions γ used in the prompt. For the case of single-instruction setting (γ = 1), we compare GSTLoRA (Ours) and STLoRA (Ours) with Explicit CFG and Implicit CFG, all implemented on top of the FLUX-Kontext model, as well as Concept-Slider [10] and Continuous Attribute Control [3]. Implicit CFG refers to the classifier-free guidance (CFG) mechanism applied in an implicit manner. FLUX-Kontext is guidance-distilled model, meaning that at inference time it does not explicitly Figure 11. Qualitative results of STLoRA on 2-instruction edit for text editing. higher continuity and smoother edit trajectories. For 2D and 3D edit spaces, we apply an analogous chi-squared test to evaluate the uniformity of the sample distribution across the corresponding grids. 13 Figure 12. Qualitative Comparison between PPS and SPPS. PPS produces more disentangled and smoother interpolation space in multi-instruction editing scenarios, offering finer control over individual instruction directions compared to SPPS. perform CFG as: (cid:1), ϵCFG = ϵuncond + s(cid:0)ϵcond ϵuncond where ϵcond and ϵuncond denote the conditional and unconditional predictions, respectively, and is the guidance scale. Instead, the model internally learns to approximate the effect of given s, allowing us to vary this parameter to implicitly control guidance strength. However, as observed in our experiments, this implicit scaling provides only limited control over the edit intensity. To enable explicit guidance, we first set the models internal (implicit) guidance scale to = 1, effectively recovering the base (unguided) model. We then apply explicit CFG during inference using ϵ = ϵuncond+w(cid:0)ϵcondϵuncond (cid:1), where is the external CFG scale. This requires two forward passes through the modelone with the conditioning prompt and one without. Concept-Slider and Continuous Attribute Control enable fine-grained attribute manipulation in text-to-image models. While they can be adapted to image editing via inversion methods [12, 29], their performance in this setting is comparatively limited. For cases involving multiple edit instructions (γ > 1), Explicit CFG, Implicit CFG, and GSTLoRA cannot independently control individual edit directions. This limitation highlights the advantage of STLoRA, which enables disentangled, per-instruction control in multi-instruction editing scenarios. As Concept-Slider and Continuous Attribute Control show limited effectiveness even for singleinstruction edits (γ = 1), we omit them from this setting. We evaluate STLoRA using both FLUX-Kontext and QwenImage-Edit models. 8.3. Qualitative Results We provide additional qualitative results to further illustrate the capabilities of SliderEdit and its variants across diverse range of editing tasks. Figure 14 showcases diverse examples generated using GSTLoRA, demonstrating smooth and continuous control over both local and global edits. The model effectively interpolates between different edit strengths, producing coherent intermediate images without abrupt transitions. To further evaluate its capability in fine-grained manipulation, Figures 15 and 16 present qualitative results on face-editing tasks. The model can accurately and continuously adjust facial attributes such as hair length, curliness, makeup, skin tone, hair color, and age, as well as facial expressions including smiling, anger, and surprise. In addition, Figure 9 demonstrates GSTLoRAs versatility in text editing. The model enables continuous adjustment of textual attributes such as font color, style, and weight. Figures 17 and 10 illustrate qualitative results of STLoRA on multi-instruction editing tasks. In the 2-instruction setting, the model produces smooth and interpretable 2D in14 Figure 13. Qualitative Comparison with Baselines. While SliderEdit (GSTLoRA variant here) and Explicit Guidance produce highquality edits, Concept-Slider and Continuous Attribute Control perform poorly on real image editing, as they are primarily designed for text-to-image generation and rely on indirect inversion-based adaptation. terpolation space, where each axis corresponds to distinct instruction direction. Extending this to 3-instruction scenarios (Figure 10), STLoRA maintains disentangled control, allowing continuous modulation of each instruction independently. We further demonstrate STLoRAs capability on text editing tasks in Figure 11, where the model learns disentangled control over multiple text attributes (e.g., font style and color). Overall, these results highlight the flexibility and generality of the proposed framework across domains, showing that both GSTLoRA and STLoRA enable smooth, continuous, and disentangled control over diverse editing operations. 8.3.1. PPS vs SPPS We compare the Partial Prompt Suppression (PPS) and Simplified PPS (SPPS) objectives to assess their effect on disentanglement and control quality. As illustrated in Figure 12, both objectives enable smooth and continuous interpolation along edit directions. However, PPS produces more disentangled latent space, allowing finer and more independent control over each instruction, while SPPS serves as simpler yet effective alternative that achieves comparable results in most cases. Some degree of attribute entanglement persists across all models, including the underlying base model. For instance, even when using the base instruction-based editing model, modifying persons skin tone can unintentionally affect correlated features such as hair color or lighting. This behavior arises from inherent attribute coupling in the generative model itself, rather than from limitations introduced by our sliders. 8.3.2. Comparison with other baselines As shown quantitatively in Table 1 and discussed in Section 4, Concept-Slider and Continuous Attribute Control perform poorly on real image editing tasks due to their indirect adaptation from text-to-image generation. Here, we provide qualitative examples in Figure 13 for visual comparison. While SliderEdit (GSTLoRA variant in this case ) and Explicit Guidance produce smooth, coherent, and faithful edits aligned with the input instructions, ConceptSlider and Continuous Attribute Control often fail to maintain image fidelity or accurately follow the target modification. These qualitative results further confirm the quantitative findings, demonstrating that SliderEdit enables both fine-grained control and high-quality real image editing. 15 Figure 14. Qualitative Samples of GSTLoRA. The model emonstrates smooth, continuous control over the strength of both local and global edits. Figure 15. Qualitative results of GSTLoRA on face editing 17 Figure 16. Qualitative results of GSTLoRA on face editing 18 Figure 17. Qualitative results of STLoRA on 2-instruction edits. The model demonstrates smooth, continuous control over the strength of both directions."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Maryland"
    ]
}