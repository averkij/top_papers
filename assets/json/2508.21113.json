{
    "paper_title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning",
    "authors": [
        "Jie Jiang",
        "Qi Yang",
        "Bolin Ni",
        "Shiming Xiang",
        "Han Hu",
        "Houwen Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 1 1 1 2 . 8 0 5 2 : r -4B: INCENTIVIZING GENERAL-PURPOSE AUTOTHINKING CAPABILITY IN MLLMS VIA BI-MODE ANNEALING AND REINFORCE LEARNING 2Institute of Automation, CAS 1Tencent Hunyuan Team https://github.com/yannqi/R-4B https://huggingface.co/YannQi/R-4B"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bimode Policy Optimization (BPO) to improve the models accuracy in determining whether to activate the thinking process. Specifically, we first train the model on carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost. Figure 1: Comparison between R-4B-RL and frontier open-source MLLMs, including non-thinking MLLMs (e.g., InternVL3-8B, Qwen2.5-VL 7B), thinking MLLMs (e.g., Kimi-VL-A3B-Thinking2506) and auto-thinking MLLMs (e.g., Keye-VL-8B), on different benchmarks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal Large Language Models (MLLMs) have made significant progress in recent years, particularly through the integration of explicit, step-by-step thinking processes (Guo et al., 2025b; Yue et al., 2025; Yang et al., 2025b). These models employ structured token blocks to distinguish between the exploratory thinking process and the precise answer generation. Specifically, exploratory thinking is incorporated into the <think> </think> block. It involves detailed step-by-step deduction and leverages reflection to explore alternatives or correct past reasoning. In contrast, precise answer generation is dedicated to producing concise and clear results. As shown in Figure 2, this thinking capability has substantially improved performance on complex problems such as mathematical reasoning and scientific diagram interpretation (Yue et al., 2023; Lu et al., 2024; Guo et al., 2025c). However, the default always-thinking behavior leads to unnecessary computational overhead on simple problems, such as query like What is the name of the dish?. This motivates more intelligent auto-thinking paradigm, i.e., the model can automatically decide whether to enable thinking based on the complexity of the problem. Figure 2: Non-thinking and thinking mode response examples (left); Auto-thinking triggering rates across multiple benchmarks (right). Previous explorations into auto-thinking have shown promise but come with their own limitations. For instance, some models like Qwen3 (Yang et al., 2025a) require users to manually enable the thinking mode within one model. To further automate the thinking process, other methods (Lou et al., 2025; Zhan et al., 2025) achieve auto-thinking by relying on manually curated data or complex reward functions during RL. However, training for these methods is dependent on carefully tuned reward strategies, and they are restricted to text-only modalities. More recent work such as Keye-VL (Yang et al., 2025b) attempts multimodal auto-thinking for the first time by constructing data with explicit complexity analysis to trigger thinking. While effective, it requires manually constructing training data with complex analysis, which makes it imprecise. Moreover, it introduces an additional token cost during inference due to the additional complexity analysis. These challenges demand smarter, more computationally efficient solutions for auto-thinking. In this study, we introduce R-4B, MLLM designed for general-purpose auto-thinking. It can autonomously switch between complex reasoning and direct responses directly based on the users query. This capability is enabled by novel training paradigm for content-aware auto-thinking. Considering that the foundation for auto-thinking is built upon model mastering two distinct modes for general-purpose application, i.e., thinking and non-thinking, we first propose bi-mode annealing, which is designed to train model that is inherently capable of both thinking and non-thinking modes in general domains. To support this annealing, we develop bi-mode data curation strategy to carefully construct general reasoning-intensive and direct-answer datasets. This strategy separates: (a) reasoning-intensive examples requiring reasoning (e.g., diagram analysis, logical deduction), and (b) non-reasoning examples demanding direct factual responses. Both types are formatted in unified instruction-following structure without extra complexity analysis. Then, we perform bimode annealing by mixing these datasets and obtain R-4B-Base. This lays solid foundation for the models subsequent auto-thinking training in general-purpose domains. After the annealing stage, R-4B possesses both thinking and non-thinking abilities. However, with regard to auto-thinking, it exhibits preference for non-thinking even for complex queries, due to the uneven distribution of reasoning and non-reasoning data. This highlights lack of judgment in 2 Figure 3: Framework of heuristic-driven strategy for bi-mode data curation. mode selection. Therefore, to further incentivize auto-thinking, we propose Bi-mode Policy Optimization (BPO), reinforcement learning algorithm tailored for auto-thinking. Unlike existing RL methods (Zhang et al., 2025; Lou et al., 2025; Yang et al., 2025b) that require complex reward functions, extensive data dependency, or are prone to hyperparameter sensitivity, BPO leverages simple, rule-based mathematical reward. We find that the proposed approach possesses promising generalization, achieving auto-thinking in other topics as well. Specifically, the core of BPO lies in the proposed bi-mode rollouts, which incorporate both thinking and non-thinking response trajectories simultaneously. By forcing the inclusion of thinking and non-thinking modes, the model is prevented from favoring certain mode during RL training. This mechanism enables the model to learn an adaptive policy for optimal thinking strategy selection, leading to R-4B-RL with enhanced adaptive thinking and improved performance across modes. Experiments demonstrate that R-4B-RL achieves excellent performance in auto-thinking mode, achieving an optimal balance between maximizing performance and minimizing computational overhead. In comprehensive evaluations across multiple public benchmarks, R-4B-RL outperforms Qwen2.5-VL-7B on nearly all tasks. Furthermore, as shown in Figure 1, R-4B-RL surpasses the performance of the significantly larger Kimi-VL-Thinking-2506 (3B activated, 16B total parameters) on reasoning-intensive benchmarks. To promote further advancement in MLLMs, we open-source R-4B-RL, which sets new state of the art among models of comparable scale."
        },
        {
            "title": "2 THE DESIGN OF BI-MODE ANNEALING",
            "content": "2.1 HEURISTIC-DRIVEN STRATEGY FOR BI-MODE DATA CURATION The goal of bi-mode annealing is to develop model that can master two distinct response modes: thinking and non-thinking. To achieve this, we propose general-purpose bi-mode data curation strategy, which systematically partitions data into reasoning and non-reasoning data without laborious manual annotation. This strategy leverages powerful existing MLLM, Qwen2.5-32B-VL (Bai et al., 2025), to serve as consistent annotator, partitioning data into two distinct categories: reasoning data and non-reasoning data. As illustrated in Figure 3, our methodology employs two distinct heuristics tailored to the type of the query: (i) Difficulty-based heuristic (for subjective queries): For queries where correctness is not easily verifiable (e.g., creative or open-ended questions), we leverage prompt engineering with an existing MLLM to assess whether they require reasoning process based on their inherent difficulty. Queries deemed complex are labeled as reasoning-intensive. (ii) Performance-based heuristic (for objective queries): For queries with verifiable answers (e.g., math or multiple choice questions), we introduce model-based offline hard mining strategy to systematically identify difficult samples. Specifically, we generate multiple responses for each query (N = 8); if all attempts fail (hard samples), the query is classified as reasoning-intensive. Conversely, if the model can answer correctly (easy samples), the query is marked as suitable for direct answer. Subsequently, for the identified reasoning samples, we first employ the multimodal reasoning model (Guo et al., 2025b) to extract the reasoning context. Then, to ensure the quality of the generated thinking processes, we filter out invalid samples through consistency verification, keyword filtering and duplicate detection. The resulting data proportions are shown in Figure 4. This unified data curation strategy ensures consistent annotation, eliminating inconsistencies inherent in manual labeling while adaptively addressing both question types through automated decision protocols. 3 Topic Category Non-reasoning Items Reasoning Items Total Proportion General Math/K12 Code Chart OCR Grounding Caption Knowledge Text-Only Total 1,351,060 1,908,486 643,323 1,351,060 1,366,849 280,740 1,166,676 928,190 1,875,174 1,365,693 1,821,412 161,085 1,088,858 225,146 148,710 133,741 359,778 190,787 2,716,753 4,088,776 804,408 2,439,918 1,591,995 429,450 1,300,417 1,287,967 2,065,961 16% 23% 5% 15% 10% 3% 8% 8% 12% 10,871,558 5,495, 16,366,767 100% Table 1: Data distribution across different topics in bi-mode annealing stage."
        },
        {
            "title": "2.2 DATA FORMULATION AND TRAINING PROTOCOL",
            "content": "With the curated bi-mode data, we reorganize the annealing dataset into the following domains: General, Math/K12, Code, Chart, OCR, Grounding, Caption, Knowledge, and Text-Only, as detailed in Table 1. This structured categorization enhances the richness of the data, enabling R-4B-Base to generalize its thinking and non-thinking capabilities across diverse scenarios. the training inDuring bi-mode annealing, stances are formatted according to their designated mode: For queries benefiting from reasoning patterns, responses include the complete reasoning process, formatted as: <think>reasoning steps</think>answer For queries suitable for direct answers, the response format maintains structural consistency but omits the thinking content, using the format: <think> </think>answer Figure 4: Distributions of bi-mode data. Darker regions represent items with thinking mode, while lighter correspond to items without thinking. Notably, consistent use of the <think> tags ensures structural uniformity. In addition, increasing the proportion of reasoning data strengthens both thinking and non-thinking capabilities in general-purpose applications. This lays the foundation for the subsequent policy optimization stage, designed to refine its auto-thinking judgment."
        },
        {
            "title": "3 AUTO-THINKING INCENTIVIZATION VIA BI-MODE POLICY OPTIMIZATION",
            "content": "The bi-mode annealing stage successfully endows R-4B-Base with the dual capabilities of reasoning (thinking) and direct response (non-thinking). However, as illustrated in Figure 5, when operating in an auto-thinking inference setting in R-4B-Base, the model exhibits performance degradation. This phenomenon, which we term \"thinking atrophy\", manifests as tendency towards non-thinking mode, even for complex queries where reasoning is essential. This instruction-following failure indicates that while the model possesses the necessary skills (e.g., thinking and non-thinking capabilities), it lacks the judgment to deploy them appropriately. To bridge this gap, we transition from merely enabling these modes to actively incentivizing the optimal selection between them. Fortunately, reinforcement learning (RL) (Guo et al., 2025a; Yu et al., 2025) presents natural paradigm for this challenge, as it can optimize policy based on outcome-driven rewards, thereby teaching the model to select the most effective thinking strategy. However, using vanilla RL directly will cause the model to develop thinking preferences during training, as mentioned in Section 5. Some methods (Lou et al., 2025; Zhang et al., 2025) aim to train auto-thinking models by proposing 4 Figure 5: Comparison of R-4B-Base and R-4B-RL in auto-thinking mode on OpenCompass Multimodal Reasoning Benchmarks. The R-4B-RL achieves better auto-thinking inference performance. new reward mechanisms in RL to modulate thinking preferences. Despite their effectiveness, these approaches encounter two principal limitations: 1. Complex reward engineering and data dependency: Existing methods often rely on intricate reward functions or manually annotated data that specifies query complexity to balance exploration between thinking and non-thinking modes. This dependency introduces subjectivity, is difficult to scale, and adds significant overhead to the training pipeline. 2. Hyperparameter sensitivity and training instability: The balance between encouraging thinking and non-thinking modes in refined reward-based RL methods is typically controlled by sensitive hyperparameters. Misconfiguration may easily lead to \"mode collapse,\" where the policy converges to suboptimal, single-mode strategy (either always thinking or never thinking), defeating the objective of auto-thinking. To overcome these challenges, we introduce Bi-mode Policy Optimization (BPO), novel yet elegant reinforcement learning framework designed to cultivate robust auto-thinking. Its core principle is to learn an adaptive policy by explicitly contrasting the utility of thinking versus non-thinking pathways for the same input query. Notably, BPO achieves \"a little goes long way\" effect. Instead of requiring complex, hand-crafted reward mechanisms or RL datasets for general domains, it leverages simple, rule-based reward signal derived solely from the mathematical topic. We discovered that this highly specific reward mechanism possesses remarkable universality, effectively promoting auto-thinking in diverse, non-mathematical topics as well. 3.1 THE DESIGN OF BI-MODE POLICY OPTIMIZATION The BPO algorithm is designed to directly optimize the models decision-making process, training it to select the most appropriate mode on per-query basis. As illustrated in Figure 6, BPO builds upon the Group Relative Policy Optimization (GRPO) framework, but introduces critical modification: bi-mode sampling strategy that forces the model to generate and contrast the outcomes from both thinking modes. Specifically, to achieve this, we deterministically generate two distinct response groups for each input prompt: set of thinking responses, denoted as {o1, . . . , og}, and set of non-thinking responses, denoted as {o1, . . . , og}. To ensure the generation of these distinct response types, we control the sampling process by conditioning on special tokens. For example, the special token sequence <thinking token> is appended to the input prompt to trigger thinking responses, whereas the special token sequence <non-thinking token> is used to prompt direct (nonthinking) responses. This design promotes balanced exploration across both modes by ensuring the two groups are of equal size, i.e., Groupthinking = Groupnon-thinking = g. Formally, we propose BPO, which optimizes the policy model πθ by maximizing the following objective function: BPO(θ) = EqP (Q) (cid:34) 1 2g 2g (cid:88) k=1 min (cid:16) A(o R(o k) k) , clip (cid:16) R(o k) , 1ϵ, 1+ϵ (cid:17) (cid:17) A(o k) βDKL (πθ(q) πref(q)) , (1) (cid:35) 5 Figure 6: The Bi-mode Policy Optimization (BPO) framework. For each input query, the policy model is conditioned to generate two distinct groups of responses (thinking and non-thinking). where = (cid:26)ok ok if 0 < if > represents the thinking and non-thinking rollouts respectively. ϵ and β are hyperparameters controlling the clipping range and KL penalty strength, respectively. The policy ratio Rk for each token in the generated responses and the advantage values Ak are computed as in GRPO (Shao et al., 2024). This deterministic grouping mechanism ensures balanced exploration between reasoning and direct answering behaviors. Through this simple yet effective reinforcement learning framework, the R4B-RL model achieves enhanced adaptive thinking capabilities in general-purpose domains, while simultaneously improving performance in both reasoning and non-reasoning generation."
        },
        {
            "title": "4 EVALUATION",
            "content": "This section presents comprehensive evaluation of R-4B, across diverse set of multimodal benchmarks. Specifically, we first detail the evaluation protocols in Section 4.1, ensuring reproducible comparisons. Sections 4.2 and 4.3 explore the models general visual understanding and advanced reasoning skills, respectively. Furthermore, Section 4.4 compares auto-thinking token costs with other modes. We also provide visualization cases in the supplementary material. 4.1 EVALUATION SETTINGS For all benchmark evaluations, we employ greedy decoding with the temperature set to 0 and maximum generation length of 8,192 tokens for R-4B. To ensure fair and consistent comparisons with existing MLLMs, we adopt the widely-used VLM-EvalKit repository for evaluation. For tasks that require scoring-based evaluation, such as open-ended reasoning and complex visual understanding, we utilize Qwen3-32B as the judge model to streamline the process. As indicated in Table 2, we evaluate in three distinct modes: non-thinking (N-T) with extra token <think>nn</think>, thinking (T) with extra token <think>n, and auto-thinking (A-T) with extra token <think>. R-4B-Base and R-4B-RL are evaluated in the thinking and auto-thinking modes, respectively. 4.2 GENERAL CAPABILITIES General visual question answering. As shown in Table 2, R-4B demonstrates exceptional performance on wide array of general visual question answering benchmarks. On MMMUval (Yue et al., 2023), challenging multi-disciplinary benchmark, R-4B-RL achieves state-of-the-art score of 68.1%, outperforming all other models, including Keye-VL-8B (66.8%). While Kimi-VL-A3BThinking leads on MMMU-Pro (Yue et al., 2024), our models remain highly competitive, with R4B-Base (46.7%) and R-4B-RL (46.5%) surpassing several larger counterparts. On MMStar (Chen et al., 2024a), R-4B-RL secures the second-best position with score of 72.1%, closely following the top-performing Keye-VL-8B (72.8%). This strong performance extends to standard MMBench (Liu et al., 2023a) evaluations. On both MMBenchV1.1-ENdev and MMBenchV1.1-CNdev, R-4BRL consistently ranks second with scores of 84.9% and 84.7%, respectively, reaffirming its robust cross-lingual understanding. Notably, on MMVet (Yu et al., 2023), our base model R-4B-Base achieves remarkable score of 85.9%, significantly surpassing all competitors. Furthermore, RCapability Benchmark General Visual QA MMMUval MMMU-Pro MMStar MMBenchV1.1-ENdev MMBenchV1.1-CNdev MMVet HallusionBench VLMs are Blind MMVP VisuLogic RealWorldQA Table & Chart & OCR AI2D CharXiv (DQ) CharXiv (RQ) DocVQAval Visual Perception & Counting OCRBench BLINKval CountBench Math & Reasoning MathVision MathVistaMINI MathVerse-vision OlympiadBench WeMath LogicVista DynaMath Qwen2.5-VL InternVL3 InternVL3.5 Kimi-VL-A3B Keye-VL -4B (T) -Thinking (T) -8B (A-T) -7B (N-T) -8B (N-T) 58.6 34.7 64.1 82.1 81.3 69.7 55.7 37.4 73.3 20.0 68.2 83.9 73.9 42.5 95.5 89.7 56.4 74. 26.2 66.8 41.2 19.4 37.7 44.5 20.1 62.7 45.6 68.7 84.7 83.6 82.8 49.4 36.8 79.3 26.1 70.6 85.2 73.6 37.6 89.4 88.0 55.5 80.0 28.8 70.7 32.4 25.9 38.5 43.6 23.9 66.6 - 65.0 - - 76.6 44.8 - - - 66. 83.9 71.1 39.6 92.4 81.5 58.1 - 26.2 77.1 61.7 - 50.1 56.4 35.7 64.0 49.2 70.4 82.6 80.7 81.9 57.2 60.8 80.3 25.0 66.1 82.7 75.4 47.7 69.0 86.2 56.2 91. 56.8 80.1 57.4 33.9 47.0 51.0 27.1 66.8 47.5 72.8 89.7 89.8 65.5 57.3 61.0 79.0 21.1 66.3 85.8 74.5 40.0 86.3 85.3 52.5 75.4 42.4 75.2 40.8 45.2 58.6 50.6 35.3 R-4B-Base R-4B-RL (T) 63.2 46.7 70.8 81.9 83.2 85.9 53.9 47.0 79.3 22.5 70.5 84.8 82.8 55.4 89.6 82.8 54.8 92.6 45.7 76.8 65.0 47.0 54.1 58.8 36.3 (A-T) 68.1 46.5 73.1 84.9 84.7 81.9 58.9 52.3 80.7 25.1 69.1 86.2 82.9 56.8 91.0 83.6 56.3 90.2 47.8 78.0 64.9 49.6 52.8 59.1 39.5 Table 2: Performance comparison of multimodal large language models across diverse benchmarks. The best and second-best results are highlighted in bold and underlined, respectively. We denote thinking, non-thinking, and auto-thinking modes as T, N-T, and A-T. Notably, Keye-VL-8B and R-4B-RL are evaluated in A-T mode. 4B is good at tasks with visual illusions and logical fallacies. It achieves the top score of 58.9 on HallusionBench (Guan et al., 2023) and 80.7% on MMVP (Tong et al., 2024), setting new standard on both benchmarks. On the logic-intensive VisuLogic (Xu et al., 2025) benchmark, R-4B-RL attains competitive second-place score of 25.1%. Similarly, on RealWorldQA, R-4BBase is the second-best performer with 70.5%. These results collectively underscore R-4Bs superior general visual understanding and reasoning capabilities. Document and chart understanding. R-4B demonstrates outstanding proficiency in interpreting structured visual content like documents, diagrams, and charts. On AI2D (Kembhavi et al., 2016), diagram understanding benchmark, R-4B-RL achieves the highest score of 86.2%, showcasing its advanced spatial and semantic comprehension. The models dominance is particularly evident in chart analysis. On CharXiv (Wang et al., 2024b), R-4B-RL not only leads in Descriptive Questions (DQ) with score of 82.9% but also establishes significant lead in Reasoning Questions (RQ) with score of 56.8%. This is substantial improvement of over 9 percentage points compared to the Kimi-VL-A3B-Thinking (47.7%), highlighting its superior ability to reason over complex chart semantics. On DocVQAval (Mathew et al., 2021), R-4B-RL obtains strong score of 91.0%. This performance confirms R-4Bs exceptionally robust capabilities in both precise text extraction and document structure interpretation. Visual perception and counting. In tasks requiring fine-grained visual perception, R-4B delivers highly competitive results. On BLINKval (Fu et al., 2024), benchmark for referring expression comprehension, R-4B-RL scores 56.3%, effectively tying for the top position with Qwen2.5-VL (56.4%) and outperforming other models. The most impressive result in this category comes from CountBench (Paiss et al., 2023), where our base model, R-4B-Base, sets the highest score with 92.6%, demonstrating exceptional object counting abilities. R-4B-RL also performs strongly with score of 90.2%. On OCRBench (Liu et al., 2023b), R-4B-RL achieves robust score of 83.6%, proving its solid text recognition capabilities in diverse scenes. 7 Figure 7: Comparison of average output tokens per query across non-thinking, auto-thinking, and thinking modes on different benchmarks. The auto-thinking mode achieves trade-off between efficiency and performance."
        },
        {
            "title": "4.3 COMPLEX REASONING CAPABILITIES",
            "content": "R-4B exhibits exceptional strength in complex mathematical and logical reasoning tasks. Notably, our models secure the top two positions on several challenging math-focused evaluations. On MathVerse-vision (Zhang et al., 2024), R-4B-Base (65.0%) and R-4B-RL (64.9%) dramatically outperform all prior models, with the next-best competitor lagging at 57.4%. similar dominance is observed on OlympiadBench (He et al., 2024), where R-4B-RL (49.6%) and R-4B-Base (47.0%) take the lead. The trend continues on LogicVista (Xiao et al., 2024) (R-4B-RL: 59.1%, R-4B-Base: 58.8%) and DynaMath (Zou et al., 2024) (R-4B-RL: 39.5%, R-4B-Base: 36.3%), where our models again claim the top two spots, underscoring their advanced logical deduction skills. On other prominent math benchmarks, R-4B remains top contender. It achieves the secondbest scores on MathVision (Wang et al., 2024a) (47.8%) and MathVistaMINI (Lu et al., 2024) (78.0%), trailing only Kimi-VL-A3B-Thinking but significantly surpassing other baselines. These results demonstrate that our advanced training strategies empower R-4B to compete with larger MLLMs and R-4B-RL sets new standard for 4B MLLMs. 4.4 TOKEN CONSUMPTION ACROSS DIFFERENT MODES As illustrated in Figure 7, we analyze the average output tokens per query of R-4B-RL, to verify the effectiveness of our BPO method. This analysis provides empirical evidence for the generalization capability of the auto-thinking learned via BPO. Although trained with reward signal exclusively from the mathematical domain, R-4B-RL demonstrates an understanding of task complexity across diverse set of general-purpose benchmarks. For simpler tasks such as those in OCRBench, the auto-thinking mode generates only 66 tokens, volume comparable to the non-thinking mode (57 tokens) and significantly lower than the full thinking mode (394 tokens). Besides, its performance of 83.6% matches the non-thinking mode (83.6%) and even surpasses the thinking mode (82.6%). This indicates that for straightforward queries, our model conserves computational resources while maintaining performance. Meanwhile, when confronted with reasoning-intensive benchmarks like MathVista and WeMath, the auto-thinking mode dynamically increases its token output to 996 and 1278, respectively. These figures closely approach the token counts of the thinking mode (1136 and 1382 tokens). At the same time, this enables our model to achieve performance of 78.0% on MathVista and 52.8% on WeMath, which are substantially higher than the non-thinking modes scores (71.5% and 46.6%) and competitive with the thinking mode (79.7% and 55.8%). In conclusion, these findings validate that our BPO is not merely thinking switch but truly intelligent and generalizable policy. It effectively discerns task complexity across varied domains, striking an optimal trade-off between performance and efficiency."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Training data ablation in annealing stage. We performed an ablation study to determine the optimal bi-mode annealing strategy, by focusing on different data compositions. As detailed in Table 3, results demonstrate that mixed-data approach combined with thinking mode is markedly superior. This strategy achieves an average performance of 69.5%, surpassing models trained on only reasoning data (+4.1%) and two-stage curriculum (+2.6%). In addition, while the Only-R 8 Strategy Data Mode MMMU MMStar AI2D OCRBench MathVista MathVision LogicVista Average 16.3M 5.5M N-T Non-R Only-R Non-R 10.8M 5.5M Mixed-R Mixed-R 16.3M 16.3M N-T 60.4 62.8 63.8 56.3 64.6 70.4 65.7 68.7 67.5 73.1 83.2 78.9 80.4 84.5 84. 83.3 82.1 82.7 82.7 82.8 71.1 73.6 74.9 65.5 76.8 33.2 41.9 43.7 29.9 45.7 49.0 52.6 54.4 47.7 58.8 64.4 65.4 66.9 62.0 69.5 *Note: Non-R Reasoning strategy resulting model possesses reasoning capabilities only. Table 3: Ablation study on training strategies during the annealing stage. We compare four data strategies: using only non-reasoning data (Non-R), only reasoning data (Only-R), two-stage curriculum (Non-R R), and mix of both reasoning data and non-reasoning data (Mixed-R). Performance is evaluated in either non-thinking (N-T) or thinking (T) inference mode. Figure 8: Average thinking triggering rate (%) across training steps for non-reasoning benchmarks, and reasoning benchmarks. Figure 9: Average accuracy (%) across RL training steps for non-reasoning benchmarks, and reasoning benchmarks. strategy enhances performance on specific complex tasks (e.g., +8.7% on MathVision), it fails to generalize tasks, leading to lower overall score. The Mixed-R strategy, however, successfully balances specialized reasoning with general capabilities. This suggests that co-training on both data types prevents catastrophic forgetting of general skills while effectively instilling complex reasoning abilities. Thinking triggering rate during reinforcement learning. To dissect the learning process of BPO, we tracked the evolution of the thinking triggering rate across benchmarks with distinct complexity profiles. As illustrated in Figure 8, on reasoning-intensive benchmarks (e.g., MathVision, MathVista), the model rapidly learns to activate its thinking mode, with the trigger rate showing steep initial climb before stabilizing at high level. Conversely, on non-reasoning benchmarks (e.g., OCRBench, HallusionBench), the trigger rate shows only marginal and slow increase. This differential learning processing is the direct consequence of our BPO mechanism. The model quickly discovers that applying the thinking mode to reasoning tasks yields high reward signal, thus reinforcing this behavior. Simultaneously, it learns that thinking on simpler, non-reasoning tasks offers few rewards. The benefit of our RL policy is reflected in the performance gains. As shown in Figure 9, BPO achieves substantial +10.3% improvement on reasoning tasks, gain that far surpasses its impact on non-reasoning tasks. This directly validates BPOs effectiveness in mitigating \"thinking atrophy\" by incentivizing the appropriate auto-thinking. Comparison between R-4B-Base and R-4B-RL. Table 4 presents comprehensive comparison between R-4B-Base and R-4B-RL across multiple reasoning benchmarks under non-thinking, thinking, and auto-thinking modes. The results demonstrate that RL consistently enhances performance in both non-thinking and thinking capabilities. In non-thinking mode, where models generate answers directly without explicit reasoning, R-4B-RL achieves substantial improvement over R-4B-Base, raising the average score from 42.0% to 49.9%, outperforming the base model on all benchmarks. This significant gain underscores the effectiveness of RL for direct response generation, thereby strengthening non-reasoning capabilities. Besides, in thinking mode, R-4B-RL retains its edge, pushing the average from 56.1% to 58.1%, showing greater benefits from reasoning. Overall, R4B-RL consistently outperforms R-4B-Base in both modes, confirming our RL method boosts both thinking and non-thinking performance. 9 Model Mode MathVista MathVision MathVerse DynaMath WeMath LogicVista Average R-4B-Base N-T R-4B-RL N-T R-4B-Base A-T A-T R-4B-RL R-4B-Base R-4B-RL 65.5 71.5 65.7 78.0 76.8 79.7 29.9 35.0 30.8 47.8 45.7 47. 48.4 61.5 48.9 64.9 65.0 67.1 28.5 31.7 30.0 39.5 36.3 39.5 32.0 46.6 34.3 52.8 54.2 55.8 47.7 52.8 49.4 59.1 58.8 58.6 42.0 49.9 43.2 57.0 56.1 58.1 Table 4: Performance comparison of R-4B-Base and R-4B-RL models across reasoning benchmarks. Thinking preference dilemma of vanilla GRPO. As shown in Figure 10, we compare the average items per rollout for vanilla GRPO, considering both thinking and nonthinking scenarios, across the reinforcement learning training steps. For GRPO, the nonthinking items increase initially but then drop and remain at low average correct items per rollout, which falls into the thinking preference dilemma. Overall, BPO demonstrates better performance in both thinking and non-thinking modes across the training steps, indicating that BPO is more effective in handling adaptive reasoning in the RL training stage compared to GRPO and avoid the preference dilemma."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Figure 10: The average thinking and non-thinking items across training steps on vanilla GRPO. Recently, existing methods to enhance the efficiency of LLMs have centered on manually enabling the thinking mode in model responses (Yang et al., 2025a). To automate this process, some methods incorporate RL with extra thinking preference rewards to encourage models to think on their own (Tu et al., 2025; Zhang et al., 2025). Other approaches further fine-tune models on datasets containing both thinking and non-thinking responses, which are obtained from prompt engineering and existing reasoning models (Lou et al., 2025; Zhan et al., 2025). In the multimodal domain, Keye-VL (Yang et al., 2025b) addresses auto-thinking using Mix-Mode RL. Inspired by the above methods, by introducing bi-mode annealing and BPO strategy, R-4B enables efficient, content-dependent autothinking by single model, filling crucial gap towards efficient and auto-thinking MLLMs."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduce R-4B, multimodal large language model designed to address the critical trade-off between complex reasoning and inference efficiency. R-4B features an \"auto-thinking\" mechanism that dynamically switches between thinking and direct answering modes. Our method first equips R-4B-Base with thinking and direct answering capabilities through bi-mode annealing stage. Subsequently, we employ bi-mode policy optimization, reinforcement learning approach that utilizes hybrid mixed-policy rollouts to mitigate \"thinking collapse\" and learn an optimal policy for mode selection in general domains. R-4B-RL achieves state-of-the-art performance on reasoning-intensive benchmarks, outperforming comparable models and matching the larger KimiVL-A3B-Thinking-2506. This research demonstrates practical and effective pathway toward developing more intelligent and resource-efficient MLLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large visionlanguage models. arXiv:2310.14566, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: primary assessment for visual reasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025c. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pp. 235251. Springer, 2016. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023a. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: On the hidden mystery of ocr in large multimodal models. arXiv:2305.07895, 2023b. Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model. arXiv preprint arXiv:2502.04328, 2025. 11 Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. Adacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning. arXiv preprint arXiv:2505.11896, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. In Proceedings of the IEEE/CVF International Conference on Teaching clip to count to ten. Computer Vision, pp. 31703180, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao. Learning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl. arXiv preprint arXiv:2505.10832, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv:2402.14804, 2024a. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024b. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025b. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 12 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv:2311.16502, 2023. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. Zizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, et al. Kat-v1: Kwai-autothink technical report. arXiv preprint arXiv:2507.08297, 2025. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        },
        {
            "title": "A CONTRIBUTIONS",
            "content": "Project Lead Jie Jiang1 Core Contributors Qi Yang1,2, Bolin Ni1 Supervisors Shiming Xiang2, Han Hu1, Houwen Peng1 PRE-TRAINING STAGE B.1 R-4B MODEL ARCHITECTURE Similar to other MLLMs (Li et al., 2024; Zhu et al., 2025; Bai et al., 2025), R-4B consists of three core components for foundational multimodal understanding. The visual encoder is initialized with SigLIP2-So400m (Tschannen et al., 2025) enhanced with the AnyRes strategy (Liu et al., 2024), enabling it to flexibly process images of any resolution. For language comprehension and generation, we utilize the Qwen3-4B Large Language Model (LLM) (Yang et al., 2025a), chosen for its strong reasoning capabilities and computational efficiency. To bridge these two modalities, randomly initialized Multi-Layer Perceptron (MLP) projector maps the visual features into the LLMs latent space, ensuring coherent multimodal interaction. Furthermore, to establish robust non-thinking multimodal understanding capabilities, we employ similar three-stage pre-training strategy (Li et al., 2024; Chen et al., 2024b) with 1. MLP warmup, 2. vision-language alignment, and 3. joint multimodal pre-training, producing base vision-language model with comprehensive non-thinking multimodal understanding. More details are provided in supplementary material. B.2 PRE-TRAINING SETTINGS Our framework implements three-stage pre-training paradigm, summarized in Table I. Stages Purpose Batch Size Scheduler Type MLP Learning Rate ViT Learning Rate LLM Learning Rate Packed Sequence Length Trainable Components Stage MLP Warmup 512 Cosine 1e-3 - - 8192 MLP Stage 2 Stage 3 Vision-Language Alignment Joint Multimodal Pre-training 2048 Cosine 4e-5, 4e-6 4e-5, 4e-6 - 16384 ViT, MLP 1024 Constant 4e-6 4e-6 4e-5 16384 Stage 1 Bi-Mode Annealing 1024 Cosine 4e-6 4e-6 4e-5,4e-6 16384 ViT, MLP, LLM ViT, MLP, LLM Table I: Overview of R-4B-Base pre-training and annealing stages, including MLP warmup, visionlanguage alignment, joint multimodal pre-training, and bi-mode annealing. Stage 1: MLP warmup We begin by freezing the parameters of both the ViT and the LLM, while initializing randomly-initialized two-layer MLP projection module. This projector is trained using image-caption pairs to establish initial cross-modal alignment. This stage enables stable gradient propagation in subsequent stages and mitigates instability caused by poorly aligned representations. Stage 2: Vision-language alignment In this stage, the ViT backbone is unfrozen while the LLM remains frozen, and training proceeds using interleaved multimodal data. The inclusion of diverse visual content in these batches systematically improves the visual encoders ability to handle different visual domains. 14 Stage 3: Joint multimodal pre-training This stage enables full-parameter optimization across the entire architecture. We expand the training regimen to incorporate 145 billion tokens spanning diverse modalities and tasks, including OCR interpretation, visual grounding, mathematical reasoning, and structured data (tables/charts). Additionally, we implement non-thinking loss masking strategy. In this strategy, <think> </think> tags are appended before response generation, and their corresponding loss contributions are masked. This strategy effectively preserves Qwen3s (Yang et al., 2025a) specialized reasoning capabilities during joint multimodal pre-training. B.3 NON-REASONING DATA DISTRIBUTION FOR PRE-TRAINING STAGE To enhance R-4B-Bases capabilities, we employed diverse categories of data across distinct training stages. The pre-training stages primarily utilized non-reasoning data to improve multimodal understanding and visual perceptual abilities. Stage 1: We trained the model using 808K image captioning samples sourced from the LAION datasets (Liu et al., 2025). The primary objective was to establish vision-language connections. Stage 2: This stage focuses on refining the visual module and enhancing image understanding. We utilized large corpus (25 million items) comprising Visual OCR (31.8%), Knowledge (33.7%), Captioning (26.3%), and Math (8.2%) data. Stage 3: To boost overall multimodal understanding, we introduced 13.3% text data alongside higher proportions of quality Math and K-12 data (22.9%). This stage included approximately 37 million items."
        },
        {
            "title": "C CASE STUDY",
            "content": "Case: Object Counting User Question: How many tomatoes are there? Thinking: (Non-thinking) Model Answer: There are five tomatoes visible in the image. They are arranged in cluster, with four of them lying flat and one slightly elevated. The tomatoes appear to be fresh and ripe, with vibrant red color. They are positioned in the foreground of the image, likely on wooden surface, and are surrounded by other fruits and jars of preserved produce. Figure I: An example demonstrating R-4B-RL solves object counting problem. 15 Case: Image Understanding User Question: What is funny about this image? Thinking: (Non-thinking) Model Answer: The humor in this image comes from the rhinoceros painting pictures that all prominently feature its horn, reflecting the caption \"You see the world as you are!\" The rhinos unique perspective in art mirrors our personal views, suggesting that our individual experiences shape how we see the world. Figure II: An example demonstrating R-4B-RL solves image understanding problem. Case: Optical Character Recognition User Question: Who is the author of this book? Thinking: (Non-thinking) Model Answer: The author of this book is Steven N. Meyers. This information is clearly stated on the cover of the book, which features the title \"flower spirits\" and the subtitle \"Radiographs of Nature by Steven N. Meyers.\" The book appears to be 16-month weekly engagement planner covering the period from September 2014 to December 2015. Figure III: An example demonstrating R-4B-RL solves OCR tasks. 17 Case: Sphere Path Counting User Question: There are three great circles on sphere that intersect each other in right angle. Starting in point little bug moves along the great circles in the direction indicated. At crossings it turns alternately to the right or left. How many quarter circles does it crawl along until it is back in point S? Thinking (Part): Got it, lets try to figure out this problem step by step. First, lets recall what great circle is: circle on sphere whose center is the same as the spheres center, so its the largest possible circle on the sphere. The problem says there are three great circles intersecting at right angles. So, imagine sphere with three great circles, like the x, y, axes in coordinate system, each pair intersecting at 90 degrees. Now, the bug starts at point and moves along these great circles, turning right or left at each intersection. We need to find how many quarter circles it crawls before returning to S. Lets visualize the path. Lets ... Model Answer: To solve the problem, we analyze the bugs movement along the great circles and the geometric properties of the sphere: Step 1: Understand Great Circles and Intersections Three great circles intersecting at right angles form coordinate - like system (analogous to the x, y, and axes of 3D coordinate system). Each great circle is the largest circle on the sphere, and their intersections create closed - loop path for the bug. Step 2: Track the Bugs Movement The bug starts at point and moves along great circles, turning right or left at each intersection. Each segment of the bugs path is **quarter circle** (since the great circles intersect at right angles, and the bugs turns alternate, each segment corresponds to 90 arc, or quarter circle). To return to S, the bugs path must form closed loop. By visualizing the path (and using the diagrams structure, where the bugs path forms closed loop with 6 quarter - circle segments), we count the number of quarter circles in this loop. Step 3: Count the Quarter Circles The bugs path consists of 6 quarter - circle segments. Each segment is 90 arc, and the total rotation after 6 segments is 6 90 = 540, but this is not necessary for the countwe only need the number of quarter circles in the closed loop. Thus, the number of quarter circles the bug crawls along until it returns to is 6 . Figure IV: An example demonstrating R-4B-RL solves complex geometry problem. Case: Abstract Reasoning User Question: What diagram replaces the question mark? Thinking (Part): Got it, lets try to figure out this problem step by step. First, we need to identify which operation the green circle (the \"m\" symbol) represents. Looking at the top part of the image, there are four examples: 1. First row left: \"Each symbol moves one place to the right\", so thats shift right. 2. First row right: \"Changes shading on 1st and 3rd symbol\", so shading the first and third. 3. Second row left: \"Reflects all symbols in axis\", reflection over X-axis. 4. Second row right: \"Replaces all squares with stars\", replace squares with stars. Now, the green circle in the bottom part is the \"m\" symbol, which we need to match to one of these operations. Lets check each example: The first example (top left) has the \"m\" symbol, and the operation is ... Model Answer: To solve this problem, we first identify the operation associated with the green symbol by analyzing the top - row examples: 1. Identify the operation for the symbol: - The first example in the top - left shows the symbol, and the description is Each symbol moves one place to the right. - Lets verify with the input [L, +, ]: After the operation, it becomes [, L, +]. This means each symbol shifts one position to the right (the rightmost symbol moves to the leftmost position). 2. Apply the operation to the input diagram [, , ]: - For the input [, , ], we perform the right - shift: the rightmost symbol () moves to the leftmost position, the first symbol () moves to the second position, and the second symbol () moves to the third position. - After the shift, the new order is [, , ]. 3. Match with the options: - Option is [, , ], which matches our result. - Options B, C, and do not match the right - shift operation (B involves shading, involves replacing stars, and has an incorrect star position). Thus, the diagram that replaces the question mark is . Figure V: An example demonstrating R-4B-RL solves logic reasoning tasks. 19 Case: Pie Chart Description User Question: The pie charts below show units of electricity production by fuel source in Australia and France in 1980 and 2000. Summarise the information by selecting and reporting the main features, and make comparisons where relevant. You should write at least 150 words. Thinking (Part): Got it, lets tackle this task. First, need to analyze the pie charts for Australia and France in 1980 and 2000. Lets start by identifying the fuel sources: Natural Gas, Oil, Coal, Nuclear Power, Hydro Power. First, Australias 1980: Total 100 units. Coal is 50 (half), ... Model Answer: The pie charts depict the units of electricity produced by different fuel sources in Australia and France during 1980 and 2000. Overall, coal was the primary fuel in Australia, while nuclear power dominated electricity generation in France. Over the two decades, coal and nuclear power saw significant increases, whereas natural gas and oil production declined, and hydro power remained minor contributor in both countries. In Australia, coal was the largest source of electricity in 1980, accounting for 50 units (50% of total production). Hydro power and natural gas each contributed 20 units (20% each), while oil provided 10 units (10%). By 2000, coals share surged to 130 units (76% of the total 170 units), reflecting substantial increase. Hydro power also rose to 36 units (21%), but natural gas and oil production plummeted to just 2 units each (1% of the total). In France, the fuel mix in 1980 was more balanced, with natural gas, oil, and coal each contributing 25 units (28% each), nuclear power at 15 units (17%), and hydro power at 5 units (5%). By 2000, nuclear power became the dominant source, supplying 126 units (70% of the total 180 units). In contrast, natural gas and oil production dropped to 2 units each (1%), while coal and hydro power remained relatively low at 25 units (14%) and 2 units (1%) respectively. In summary, Australias electricity generation became increasingly coal - reliant, while France shifted towards nuclear power. Both countries experienced decline in natural gas and oil usage, with hydro power playing negligible role throughout the period. Figure VI: An example demonstrating R-4B-RL solves pie chart description problem."
        }
    ],
    "affiliations": [
        "Institute of Automation, CAS",
        "Tencent Hunyuan Team"
    ]
}