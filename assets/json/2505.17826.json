{
    "paper_title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models",
    "authors": [
        "Xuchen Pan",
        "Yanxi Chen",
        "Yushuo Chen",
        "Yuchang Sun",
        "Daoyuan Chen",
        "Wenhao Zhang",
        "Yuexiang Xie",
        "Yilun Huang",
        "Yilei Zhang",
        "Dawei Gao",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework."
        },
        {
            "title": "Start",
            "content": ": General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou"
        },
        {
            "title": "Alibaba Group",
            "content": "Trinity-RFT is general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework. GitHub: https://github.com/modelscope/Trinity-RFT Documents: https://modelscope.github.io/Trinity-RFT Note: Trinity-RFT is currently under active development. This technical report corresponds to commit id f17db3d (May 23, 2025) of the GitHub repository, and will be continuously updated as the codebase evolves. Comments, suggestions and contributions are welcome! 5 2 0 2 3 2 ] . [ 1 6 2 8 7 1 . 5 0 5 2 : r Figure 1: The design of Trinity-RFT. Equal contribution. Corresponding author. {chenyanxi.cyx, panxuchen.pxc, yaliang.li, bolin.ding}@alibaba-inc.com"
        },
        {
            "title": "1 Vision of Trinity-RFT",
            "content": "Reinforcement learning (RL) has achieved remarkable success in the development of large language models (LLMs). Examples include aligning LLMs with human preferences via reinforcement learning from human feedback (RLHF) [18], and training long-CoT reasoning models via RL with rule-based rewards [3, 28]. However, such approaches are limited in their abilities to handle dynamic, real-world, and continuous learning. Trinity-RFT envisions future where AI agents learn by interacting directly with environments, collecting delayed or complex reward signals, and continuously refining their behavior through RL based on the collected experiences [26]. For example, imagine an AI scientist who designs an experiment, executes it, waits for feedback (while working on other tasks concurrently), and iteratively updates itself based on true environmental rewards when the experiment is finally finished. Trinity-RFT offers path into this future by providing various useful features."
        },
        {
            "title": "2 Key Features",
            "content": "Trinity-RFT is general-purpose, flexible, scalable and user-friendly RL framework that can be adapted for diverse experimental or real-world scenarios. The following features of Trinity-RFT make it promising solution for realizing the aforementioned vision: Unified RFT modes & algorithm support. Trinity-RFT unifies and generalizes existing RFT methodologies into flexible and configurable framework, supporting synchronous/asynchronous, onpolicy/off-policy, and online/offline training, as well as hybrid modes that seamlessly combine them into single learning process (e.g., incorporating expert trajectories to accelerate an online RL process [15, 36]). This is made possible partly by our decoupled design that allows rollout and training to be executed separately and scaled up independently on different machines, which will soon be introduced in the next section. Agent-environment interaction as first-class citizen. Trinity-RFT allows delayed rewards in multi-step/time-lagged feedback loops, and handles long-tailed latencies and environment/agent failures gracefully, which ensures efficiency and robustness in real-world scenarios with complex agentenvironment interaction. Systematic data pipelines optimized for RFT. These include converting raw datasets to task sets for RFT, cleaning/filtering/prioritizing experiences stored in the buffer, incorporating priors and additional reward signals from various sources into the collected experiences, synthesizing data for augmenting tasks and experiences, offering user interfaces for human-in-the-loop, among others."
        },
        {
            "title": "3 Design and Implementations",
            "content": "The overall design of Trinity-RFT exhibits trinity consisting of (1) RFT-core, (2) agent-environment interaction, and (3) data pipelines, which are illustrated in Figure 1 and elaborated in this section."
        },
        {
            "title": "3.1 RFT-Core",
            "content": "RFT-core is the component of Trinity-RFT, highlighted at the center of Figure 1, where the core RFT process happens. Its design also exhibits trinity, consisting of the explorer, trainer, and buffer. The explorer, powered by rollout model, takes task as input and solves it by executing workflow that specifies the logic of agent-environment interaction, thereby collecting experiences (including rollout trajectories, rewards, and other useful information) to be stored in the buffer. The buffer stores experiences that can be generated by the explorer or by other sources, such as human It also assists with fetching training samples for the trainer, and can be integrated with experts. advanced sampling strategies and post-processing operations. 2 The trainer, backed by policy model, samples batches of experiences from the buffer and updates the policy model via RL algorithms. Our implementations allow the explorer and trainer to be deployed on separate machines. This decoupled design of RFT-core enables Trinity-RFT to support diverse RFT modes with great flexibility, for example: In synchronous mode, the explorer and trainer get launched simultaneously, work in close coordination, and synchronize their model weights once every sync_interval training steps. Within each synchronization period, the explorer continuously generates sync_interval batches of rollout experiences and stores them in the buffer, which are then retrieved and utilized by the trainer for updating the policy model. If sync_interval = 1, this is an on-policy RL process, whereas if sync_interval > 1, it becomes off-policy (akin to the mode adopted in [28]) and can be accelerated by pipeline parallelism between the explorer and trainer. This mode can be activated by setting the configuration parameter mode to both. In fully asynchronous mode, the explorer and trainer are mostly decoupled and can scale up independently. The explorer continuously generates rollout experiences and stores them in the buffer, while the trainer continuously samples experiences from the buffer and uses them for training the policy model. External experiences, e.g., those generated by expert models or humans, can be continuously incorporated into the buffer as well. The explorer synchronizes its model weights with the trainer once in while, keeping the distribution of rollout experiences up to date. This mode can be activated by setting mode to explore/train and launching the explorer and trainer separately on different machines. Trinity-RFT supports benchmark mode that allows the user to evaluate one or multiple checkpoints on arbitrary benchmarks, after the RFT training process has finished. To activate this mode, the user simply needs to set mode to bench and specify the paths for the evaluation datasets in the configurations. This mode can be particularly useful for experimental purposes; for example, the user might want to try out different RFT techniques or configurations quickly (with limited evaluation on hold-out data) during training, identify which RFT trials have achieved stable convergence and high rewards, and then conduct more thorough evaluations only for the checkpoints of these successful trials. In certain scenarios, the user would like to train the policy model without further exploration, using experiences that have already been collected and stored in the buffer. This train-only mode can be activated by setting the mode parameter to train and launching the trainer alone. Offline methods like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) [20] can be regarded as special cases of such scenarios, both of which are natively supported by Trinity-RFT. For another example, consider an online RFT process that expands over long period of time, where the explorer alone is launched during the daytime for serving human users and collecting experiences, while the trainer alone is launched at night for updating the policy model (which will be thoroughly validated and evaluated before it can be actually deployed as the rollout model for the next day). Another benefit brought by the decoupled design is that explorers and trainers can operate across separate devices (e.g., multiple explorers on edge devices, one trainer on high-end GPU cluster) and scale up independently. Indeed, we anticipate that for successful RL in real-world scenarios, distributed collection of massive rollout experiences could be one promising solution to resolving data scarcity. Remark 1. We take system-algorithm co-design perspective in the development of Trinity-RFT, and aim to unify and generalize diverse RFT methodologies in this framework, including synchronous/asynchronous, on-policy/off-policy, and online/offline modes. RFT-core provides the necessary infrastructure for achieving this goal. While the main text of this technical report focuses on the system perspective, interested readers may refer to Appendix for further discussions on the algorithm perspective of off-policy/asynchronous RL."
        },
        {
            "title": "3.2 Agent-Environment Interaction",
            "content": "Numerous challenges arise when one tries to build an RFT framework that can efficiently and robustly handle real-world interaction between the LLM-powered agent and the environment. These include longtailed latencies, agent/environment failures, and lagged reward signals, among others. 3 Trinity-RFT regards agent-environment interaction as first-class citizen and incorporates various solutions to tackle these challenges, for example: Built upon vLLM [10], our implementations of the LLM inference engine in Trinity-RFT support asynchronous and concurrent generation of rollout trajectories for multiple tasks. This helps mitigate the straggler effect caused by the long-tailed latencies in rollout generation and agent-environment interaction, thereby accelerating the RFT process. Load balancing among multiple LLM inference engines within one RFT training course is also taken care of, and would be one direction for further optimizing the utilization of computational resources. Trinity-RFT incorporates various timeout/retry/skip mechanisms for fault tolerance and robustness, which ensure that continuous rollout generation would not be interrupted or blocked by individual failures in certain rounds of agent-environment interaction. This is crucial for stable and efficient learning in real-world scenarios, e.g., when the agent interacts with large number of MCP services [12] that differ vastly in quality and availability. Trinity-RFT is built to provide native support for asynchronous RFT modes, which allow great flexibility in the paces of the explorer and trainer. This can boost the overall efficiency of the RFT process, compared to synchronous modes where the slower one among the explorer and trainer can block the progress of the other and cause waste of computational resources. For lagged reward signals, the trinity design of RFT-core offers natural solution. As soon as the rollout trajectory (without reward values) has been generated, it is saved into the experience buffer, but marked as not ready for training. The explorer is now free from this task and may continue to collect experiences for other tasks. When the reward signals from the environment finally arrive, they are written to the buffer, and the corresponding experience is now marked as ready for training. To adapt Trinity-RFT to new downstream scenario, the user mainly needs to define and register customized workflow (by inheriting the base class Workflow or MultiTurnWorkflow) where the logic of agentenvironment interaction is implemented. Detailed examples will be demonstrated in Sections 4.1 and 4.2."
        },
        {
            "title": "3.3 Data Pipelines",
            "content": "The data pipelines in Trinity-RFT aim to address fundamental challenges in RFT scenarios, such as managing heterogeneous data dynamics across interaction workflows, enabling delayed reward integration, and facilitating continuous data curation. Our solutions center on three core aspects: end-to-end data transformation, active data-reward co-shaping, and human-in-the-loop curation, each corresponding to key requirements identified in our development of RFT-core (Section 3.1). 3.3.1 End-to-end Data Transformation To support the diverse RFT modes (e.g., synchronous or asynchronous) in Trinity-RFT, we establish service-oriented data pipeline architecture as illustrated in Figure 2. It decouples data pipeline logic from procedure control to enable flexible RL-oriented data transformations with two key modules: The Formatter Module unifies disparate data sources into RFT-compatible formats, providing convenient conversion between raw inputs (e.g., meta-prompts, domain-specific corpora, and QA pairs with tagged rewards) and structured RFT representations (e.g., via RftDatasetModel). For efficient RFT workloads, we utilize PostgreSQL-based persistent storage supporting both ExperienceModel for prioritized trajectories and DPODataModel for preference pairs, which is particularly beneficial for asynchronous RFT mode and distributed environments. Full schema details appear in Appendix B.1. The Controller Module manages the complete data pipeline lifecycle through distributed server initialization, declarative configuration, and automated dataset persistence. It implements dynamic control mechanisms for asynchronous scenarios and protection against resource exhaustion, with configurable termination conditions based on compute quota or data quantity. This modular design enables Trinity-RFT to handle data transformations flexibly while maintaining consistency across different RFT modes. More service utilities are detailed in Appendix B.2. Figure 2: An illustration of data flow in Trinity-RFT. The Formatter-Controller duality mirrors the explorer-trainer decoupling in RFT-core, enabling parallel data ingestion and model updating. This design also allows Trinity-RFT to handle delayed rewards through version-controlled experience updates while maintaining low-latency sampling for the trainer. 3.3.2 Active Data-Reward Co-Shaping To enable efficient and effective learning from complex and delayed reward signals, Trinity-RFT introduces fruitful mechanisms that actively shape both data and reward throughout the RFT process. Trinity-RFT provides the Processor Module to support convenient and extensive data processing capabilities tailored to RFT, through dedicated bridge classes that link Trinity-RFT to over 100 operators from the Data-Juicer system [2]. This module offers efficient integration with the experience buffer, and provides composable building blocks for experience cleaning (e.g., length filters, duplication removal), safety alignment (e.g., toxicity detection, ethics checks), and preference data synthesis (e.g., critique-conditioned augmentation). Users can specify processing chains using either natural language or explicit configurations. These are accessible to both RFT users familiar with Data-Juicer and those who are not, thanks to transparency of the underlying system details. By treating Data-Juicer as modular data processing operator pool rather than central dependency, Trinity-RFT provides RL-specific abstractions and coherence, while benefiting from well-established data tools. RL-oriented data shaping. RL is particularly useful in scenarios where optimization goals cannot be clearly defined, leading users to express vague or high-level requirements. In response, Trinity-RFT automates the translation of these objectives into optimized data processing pipelines. It reduces problem complexity through structured hierarchy: Quality Difficulty Diversity Quantity, with auto-selection of appropriate operators from Data-Juicer. Moreover, Trinity-RFT provides many composable and extensible interfaces for data shaping, aiming to seamlessly inject user-defined inductive biases. For instance, DataCleaner (a sub-module of Processor) supports multi-way cleaning with statistical filtering (e.g., entropy-based diversity [11]) and rule-based validation (e.g., boxed answer verification in math dataset [7]), while DataSynthesizer enables both autonomous generation and hybrid augmentation with controlled feedback metrics [35]. Active data optimization. As agents interact with environments, experiences collected and stored in the buffer can become large-scale and highly diverse. Using the right experiences for RL training is crucial for cost-effective and stable learning process. To this end, Trinity-RFT allows users to flexibly apply multi-dimensional utility scoring to prioritize samples (e.g., 0.4 * difficulty + 0.3 * diversity + 0.1 * quality - 0.2 * frequency). Moreover, we introduce the DataActiveIterator module to enable (1) version-controlled experience reuse through ExperienceModel; (2) cross-task data lineage tracking via rich relationships derived from sample_id and rollout-model_id; and (3) incorporating human annotations into the rewards. These features will be demonstrated through concrete examples in Section 4 and Appendix B.3."
        },
        {
            "title": "3.3.3 Human-AI Collaboration",
            "content": "In scenarios where human feedback is irreplaceable, Trinity-RFT establishes bi-directional human-AI collaboration loop that provides first-class support for human annotations, based on Label Studio [30] and Data-Juicers HumanOPs. Multi-stage annotation. Trinity-RFT implements configurable procedures combining automatic pre-screening and human verification. Typical stages include preference annotation (comparative assessment of model responses), quality auditing (human verification of automated cleaning/synthesis results), and cold-start bootstrapping (initial dataset curation through expert demonstrations). Native asynchronism support. As the collection of human feedback is generally slower than AI/- model feedback, we provide dedicated capabilities to handle both synchronous and asynchronous feedback modes, with configurable timeout and polling parameters. The feedback collaboration is based on an event-driven design, with automatic task creation upon data state changes, configurable notifications via email/Slack/webhook, and an atomic transaction model for annotation batches. Customization. Different applications may involve humans in heterogeneous ways. We thus prioritize flexibility in both the interaction-interface and service levels. Examples include rich built-in interfaces that can be extended in visualized style with XML-like tags provided by Label Studio, fine-grained quality scoring for reward shaping, free-form feedback attachment for dataset shaping, among others. Moreover, for easy deployment, we provide local Label Studio instance management with automatic environment setup via Docker/pip; optimized SDK interactions with batch request coalescing; unified logging across annotation tools and ML services; and concurrent annotation campaigns through priority-based task routing, while maintaining full data lineage preserved via LineageTracker. The decoupled design of Trinity-RFT, or the presence of an experience buffer in particular, enables human feedback to participate in RL loops without breaking the asynchronous execution model. For instance, human-verified samples can be prioritized for training while fresh experiences are being collected, which is critical capability for real-world deployment scenarios with mixed feedback sources. Further details for human-AI collaboration in Trinity-RFT will be illustrated in Section 4.4."
        },
        {
            "title": "3.4 Performance Optimizations and User-Friendliness",
            "content": "Trinity-RFT has made numerous efforts in pursuit of efficiency, for example: For multi-turn conversations and ReAct-style workflows [37], Trinity-RFT supports concatenating multiple rounds of agent-environment interaction compactly into single sequence, with proper masking that indicates which tokens need to be incorporated into the training objective of RL algorithms. For the synchronous RFT mode, Trinity-RFT accelerates the RFT process via pipeline parallelism between the explorer and trainer. Trinity-RFT utilizes Ray [14] for distributed runtime, and NCCL communication primitives [16] (whenever feasible) for model weight synchronization between the explorer and trainer. In addition, Trinity-RFT has been designed with user-friendliness as top priority, for example: We include monitor (built upon Wandb [32] and TensorBoard [29]) that allows the user to conveniently track the progress of an RFT process, both quantitatively (e.g., via learning curves for rewards and other metrics) and qualitatively (e.g., via concrete examples of rollout trajectories generated at different RL steps). See Figure 3 for an example snapshot of the monitor. We also implement configuration manager, as shown in Figure 4, that allows the user to create configuration files conveniently via front-end interface. We provide Trinity-Studio, an all-in-one unified UI that allows the user to configure and run data inspection, data processing, RFT learning process, etc., all by clicking the mouse and filling forms, without writing any code. More functions will be integrated in the future. An example for developing with Trinity-Studio will be introduced in Section 4.5. Figure 3: snapshot of the monitor implemented in Trinity-RFT. (a) The beginner mode. (b) The expert mode. Figure 4: Snapshots of the configuration manager."
        },
        {
            "title": "4 Examples and Applications",
            "content": "Through some concrete examples, we demonstrate how to use Trinity-RFT for different purposes, and also exemplifies some concepts introduced in previous sections. Additional step-by-step tutorials can be found on the documentation website1, or the /examples folder of the GitHub repository2."
        },
        {
            "title": "4.1 Single-Turn Workflow",
            "content": "In simple yet common scenario, user of Trinity-RFT would like to train an LLM for completing singleturn tasks, where the LLM generates one response to each input query. For this purpose, the user mainly needs to (1) define and register single-turn workflow class (by inheriting the base class Workflow) tailored to the targeted tasks, and (2) specify the dataset of tasks (for training and/or evaluation) and the initial LLM, both compatible with HuggingFace [9] and ModelScope [13] formats. Listing 1 gives minimal example for implementing single-turn workflow. Suppose that each task is specified by <question, answer> tuple. The run() method of ExampleWorkflow calls the LLM once to generate response for the question, calculates its reward, and returns an Experience instance that consists of the response itself, the reward value, and the log-probabilities of next-token prediction by the rollout model (which is necessary for certain RL algorithms, such as PPO [22] and GRPO [23]). Some built-in workflows have been implemented in Trinity-RFT, e.g., the MathWorkflow class for math-related tasks. Trinity-RFT also allows more flexible and advanced usage by changing just few configuration parameters, e.g., running supervised fine-tuning (SFT) as warmup stage before the RFT process, or choosing between the synchronous and asynchronous modes of RFT (as described in Section 3.1). 1 @WORKFLOWS.register_module(\"example_workflow\") 2 class ExampleWorkflow(Workflow): 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 def __init__(self, model: ModelWrapper, task: Task, **kwargs): super().__init__(model, task, **kwargs) self.question = task.raw_task.get(\"question\") self.answer = task.raw_task.get(\"answer\") def calculate_reward(self, response: str, truth: str) -> float: return 1.0 if response == truth else 0.0 def run(self) -> List[Experience]: response = self.model.chat( [ { } \"role\": \"user\", \"content\": f\"Question:n{self.question}\", ], n=self.task.rollout_args.repeat_times, temperature=self.task.rollout_args.temperature, ) reward: float = self.calculate_reward(response.response_text, self.answer) return [ Experience( tokens=response.tokens, prompt_length=response.prompt_length, reward=reward, logprobs=response.logprobs, ) ] Listing 1: minimal example for implementing customized workflow. 1https://modelscope.github.io/Trinity-RFT 2https://github.com/modelscope/Trinity-RFT"
        },
        {
            "title": "4.2 Multi-Turn Workflow",
            "content": "In more advanced cases, the user would like to train an LLM-powered agent that solves multi-turn tasks by repeatedly interacting with the environment. With Trinity-RFT, achieving this is mostly as simple as in the single-turn case, except that the user needs to define and register multi-turn workflow class (by inheriting the base class MultiTurnWorkflow), where the logic of agent-environment interaction is implemented. Listing 2 provides one such example for ALFWorld [25]. For training efficiency, the process_messages_to_experience() method concatenates multiple rounds of agent-environment interactions compactly into an Experience instance consisting of single token sequence with proper masking, which can readily be consumed by standard RL algorithms like PPO and GRPO. 1 @WORKFLOWS.register_module(\"alfworld_workflow\") 2 class AlfworldWorkflow(MultiTurnWorkflow): 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 \"\"\"A workflow for the ALFWorld task.\"\"\" # ... def generate_env_inference_samples(self, env, rollout_num) -> List[Experience]: print(\"Generating env inference samples...\") experience_list = [] for in range(rollout_num): observation, info = env.reset() final_reward = -0.1 memory = [] memory.append({\"role\": \"system\", \"content\": AlfWORLD_SYSTEM_PROMPT}) for in range(self.max_env_steps): format_obs = format_observation(observation) memory = memory + [{\"role\": \"user\", \"content\": format_obs}] response_text = self.get_model_response_text(memory) memory.append({\"role\": \"assistant\", \"content\": response_text}) action = parse_action(response_text) observation, reward, done, info = env.step(action) if done: final_reward = reward break experience = self.process_messages_to_experience( memory, final_reward, {\"env_rounds\": r, \"env_done\": 1 if done else 0} ) experience_list.append(experience) # Close the env to save CPU memory env.close() return experience_list def run(self) -> List[Experience]: # ... game_file_path = self.task_desc rollout_n = self.repeat_times # ... env = create_environment(game_file_path) return self.generate_env_inference_samples(env, rollout_n) Listing 2: An implementation of multi-turn workflow for ALFWorld [25]."
        },
        {
            "title": "4.3 Dataset Curation",
            "content": "We present an example to showcase the end-to-end data transformation (Section 3.3.1) and data-reward co-shaping (Section 3.3.2) capabilities of Trinity-RFT in mathematical reasoning task. As shown in Listings 3 and 4, the user configures dataset paths and field mappings through centralized DataConfig, 9 then specifies natural-language processing instructions via dj_process_desc. The system will automatically translate these requirements into executable pipelines through three phases: (1) Processor applies LLMpowered difficulty scoring using Qwen-Max; (2) Controller optimizes quality-diversity tradeoffs through adaptive filtering thresholds and thereby prioritizes high-value training samples; (3) Formatter converts raw arithmetic problems into an RL-ready task set with specified reward functions and workflows. This procedure exemplifies our active data optimization capability (Section 3.3.2) by combining automated quality assessments with configurable curation policies. Users can extend this pattern to many other domains through customization and adaptation: for example, users can define their own difficulty metrics and calculation rules in the parameters of Data-Juicers OPs. Besides, users can flexibly utilize another Synthesizer module and extend new Controller policies to support seamless transitions between different domain-specific reward shaping and data optimization priors. 1 # Core dataset configuration 2 class DataConfig: 4 5 6 7 8 10 11 12 13 14 16 17 18 \"\"\"Centralized dataset configuration management\"\"\" def __init__(self): self.data = { \"dataset_path\": \"/path/to/gsm8k\", \"format_config\": { \"prompt_key\": \"question\", \"response_key\": \"answer\" }, \"dj_process_desc\": \"Compute difficulty scores for math questions\", \"agent_model_name\": \"qwen-max\" } def get_metadata(self) -> dict: \"\"\"Retrieve configuration metadata\"\"\" return self.data Listing 3: Core dataset configuration, which includes processing instructions as metadata. 1 # Typical procedure that transforms raw dataset through multi-stage pipeline 2 my_dataset = RftDataset(data_config) # Raw dataset auto-loading 3 4 my_dataset.format([ 5 6 ]) BoxedMathAnswerFormatter(config), RLHFFormatter(config) # Domain-specific formatting # RLHF format conversion 8 9 my_dataset.to_taskset( 10 12 reward_fn=AccuracyReward, workflow=MathWorkflow # Reward shaping specification # Execution environment binding ) Listing 4: Dataset transformation with domain-specific formatting and RLHF conversion."
        },
        {
            "title": "4.4 Human-AI Collaborative Annotation",
            "content": "This example demonstrates the human-in-the-loop capability in Trinity-RFT for preference modeling. As illustrated in Listing 5 and Figure 5, our framework integrates Label Studios annotation interface with asynchronous data pipelines through four coordinated stages: (1) task generation: auto-creating annotation batches from model rollouts; (2) interactive labeling: providing UI for side-by-side response comparison; (3) quality control: enforcing inter-annotator agreement thresholds; and (4) versioned storage: tracking preference lineage in pre-defined fields like those in DPODataModel. This pipeline reflects Trinity-RFTs bi-directional collaboration feature (Section 3.3.3), backed by timeoutaware task polling and support of atomic batch commit. It enables hybrid procedures where initial AI pre10 screening can reduce human workload in production deployments. Annotation activities can scale across distributed teams through event-driven task routing. The systems flexibility benefits rapid adaptation to diverse annotation protocols, allowing developers to implement custom labeling interfaces through XMLbased templates or integrate third-party annotation services via unified SDK endpoints. This capability underpins advanced use cases such as safety red-teaming datasets and online instruction tuning scenarios where human judgment remains irreplaceable for quality-critical decisions, particularly in human-centric sociocultural contexts where data quality, difficulty, and reward signals are difficult to verify logically. 1 # Human annotation configuration 2 class HumanAnnotationConfig: 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 \"\"\"Preference annotation pipeline configuration\"\"\" def __init__(self): self.process = [ { } ] \"human_preference_annotation_mapper\": { \"wait_for_annotations\": True, \"timeout\": 3600, \"prompt_key\": \"prompt\", \"answer1_key\": \"answer1\", \"chosen_key\": \"chosen\" # Block until annotations complete # Maximum wait time in seconds # Source field for prompts # First candidate response # Selected response key } def get_pipeline(self) -> List[Dict]: \"\"\"Get annotation processing pipeline\"\"\" return self.process Listing 5: Configuration for human preference annotation. Figure 5: An interactive interface for human preference annotation."
        },
        {
            "title": "4.5 Trinity-Studio\nTrinity-Studio provides visual interaction for the core capabilities of Trinity-RFT, designed to bridge the\ngap between system complexity and user accessibility. As shown in Figure 6a, its three integrated modules,\n“Training Portal”, pgAdmin”, and “Label Studio”, form a cohesive interface that directly supports the active\ndata-reward co-shaping and human-AI collaboration capabilities established in Section 3.3.",
            "content": "Training Portal (Figure 6b) implements configuration-to-execution procedures through declarative YAML editing with live validation and auto completion. This design aligns with Trinity-RFTs decoupled architecture, enabling users to optionally specify dataset paths, processing and training parameters, meanwhile ensuring schema-aware validation against pre-defined data schema constraints (Section B.1). The live validation mechanism prevents misconfigurations that could disrupt the explorertrainer coordination described in Section 3.1. Furthermore, the integration of runtime metrics with tools like Wandb/TensorBoard directly helps the active data optimization feature by surfacing signals such as difficulty distribution drifts and diversity metrics mentioned in Section 4.3. This transparency ensures that users can monitor how data curation strategies impact RFT performance in real time. pgAdmin (Figure 6c) reflects Trinity-RFTs end-to-end data transformation capabilities by providing visual panel for PostgreSQL-based storage (Section 3.3.1). This design benefits the versioned data lineage requirements of RFT, particularly for scenarios involving asynchronous training (Section 3.3.2). With intuitive SQL query builders, users can easily adjust schema, audit training experiences and human annotation batches with fine-grained precision. This capability is valuable for rapid validation of active learning policies by cross-referencing training outcomes with metadata (e.g., difficulty scores and staleness in asynchronous mode). Label Studio page (Figure 6d) operationalizes Trinity-RFTs bi-directional human-AI collaboration capability (Section 3.3.3). Utilizing the provided task polling and atomic batch commit mechanisms, users can annotate the data or experiences directly, allowing an asynchronous way to involve human feedback and to dynamically influence data curation. By unifying these capabilities in single UI, Trinity-Studio reduces the cognitive load of managing complex RFT procedures. For example, researcher tuning math reasoning task could use the Training Portal to adjust difficulty scoring parameters, view the resulting distribution shifts in the pgAdmin module, and then validate human annotators preferences in the Label Studio page. This end-to-end visibility is critical for debugging and iterating on RFT strategies, particularly when balancing tradeoffs between quality, diversity, and reward shaping. Moreover, this visual operation complements the programmatic APIs of Trinity-RFT while maintaining full compatibility with CLI procedures. We implement Trinity-Studio with Singe-Spa framework [27]. The modular architecture enables custom view development through JavaScript plugins and flexible extensions for general-purpose usage."
        },
        {
            "title": "5 Conclusion",
            "content": "This report has provided an overview of Trinity-RFT, general-purpose, flexible, scalable and user-friendly framework for reinforcement fine-tuning of large language models. Trinity-RFT offers path into the era of experience [26], by supporting applications in diverse scenarios with complex agent-environment interaction, and serving as unified platform for exploring advanced RL paradigms."
        },
        {
            "title": "Acknowledgements",
            "content": "Trinity-RFT is built upon many excellent open-source projects, including but not limited to: verl [24] and PyTorchs FSDP [5] for LLM training; vLLM [10] for LLM inference; Data-Juicer [2] for data processing functionalities; AgentScope [6] for agentic workflow; and Ray [14] for distributed systems. We have also drawn inspiration from RL/RFT projects like OpenRLHF [8], TRL [31], ChatLearn [1], and TinyZero [19]. 12 (a) Trinity-Studio dashboard. (b) Start training on the Training Portal page. (c) Manage data on the pgAdmin page. (d) Process data on the Label Studio page. Figure 6: Snapshots of Trinity-Studio."
        },
        {
            "title": "References",
            "content": "[1] ChatLearn. https://github.com/alibaba/ChatLearn. [2] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. Data-juicer: one-stop data processing system for large language models. In International Conference on Management of Data, 2024. [3] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, 2025. [4] Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, and Matthieu Geist. Contrastive policy gradient: Aligning LLMs on sequence-level scores in supervised-friendly fashion. In EMNLP, 2024. [5] Pytorch FSDP. https://pytorch.org/docs/stable/fsdp.html. [6] Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Ze Yu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. Agentscope: flexible yet robust multi-agent platform. arXiv, 2024. [7] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. [8] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. OpenRLHF: An easy-to-use, scalable and high-performance RLHF framework. arXiv, 2024. [9] Huggingface. https://huggingface.co/. [10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [11] Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, and Ying Shen. Diversity as reward: Fine-tuning LLMs on mixture of domain-undetermined data. arXiv preprint arXiv:2502.04380, 2025. [12] Model context protocol servers. https://github.com/modelcontextprotocol/servers. [13] Modelscope. https://www.modelscope.cn/home. [14] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: distributed framework for emerging ai applications. arXiv, 2018. [15] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In NIPS, 2017. [16] Nccl. https://github.com/NVIDIA/nccl. [17] Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Faster, more efficient RLHF through off-policy asynchronous learning. In The Thirteenth International Conference on Learning Representations, 2025. [18] Long Ouyang, Pamela Mishkin, Jeff Wu, Mar, Jacob Hilton, Amanda Askell, and Paul Christiano. Training language models to follow instructions with human feedback. arXiv, 2022. [19] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. TinyZero. https://github.com/Jiayi-Pan/TinyZero, 2025. [20] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [21] Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, Aliaksei Severyn, Jonathan Mallinson, Lior Shani, Gil Shamir, Rishabh Joshi, Tianqi Liu, Remi Munos, and Bilal Piot. Offline regularised reinforcement learning for large language models alignment. arXiv, 2024. [22] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv, 2017. [23] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. [24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. HybridFlow: flexible and efficient rlhf framework. arXiv, 2024. [25] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. [26] David Silver and Richard S. Sutton. Welcome to the era of experience. https://storage.googleapis. com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf, 2025. [27] javascript framework for front-end microservices, 2025. 15 [28] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with LLMs. arXiv, 2025. [29] TensorBoard. https://www.tensorflow.org/tensorboard. [30] Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. bel Studio: https://github.com/HumanSignal/label-studio. Data labeling software, 2020-2025. Open source software available Lafrom [31] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [32] Weights & Biases. https://wandb.ai/home. [33] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye HAO, Jun Wang, and Kun Shao. DistRL: An asynchronous distributed reinforcement learning framework for on-device control agent. In The Thirteenth International Conference on Learning Representations, 2025. [34] Tianbing Xu. Training large language models to reason via EM policy gradient. arXiv, 2025. [35] Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, and Ying Shen. Mindgym: What matters in question synthesis for thinking-centric fine-tuning? arXiv preprint arXiv:2503.09499, 2025. [36] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv, 2025. [37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. 16 Off-Policy Reinforcement Learning Algorithms For off-policy learning, one might simply adopt offline methods like supervised fine-tuning and direct preference optimization (DPO) [20], or their iterative variants where (some of) the training samples are generated online by the policy model itself and filtered by their reward values, e.g., rejection-sampling fine-tuning and iterative DPO. There also exist more advanced RL algorithms with dedicated design for off-policy or asynchronous scenarios [15, 21, 28, 4, 17, 33, 34, 36]. In the rest of this section, we present some of our findings about off-policy RL (with focus on the bandit setting) through the lens of online policy mirror descent (OPMD). At the end of this section, we arrive at the surprising conclusion that the standard policy gradient, weighted by coefficient and using the group mean reward as the baseline, can be feasible direction for updating the policy even in off-policy settings (while the standard theory of policy gradient only holds for on-policy settings). This has been validated empirically in our exploratory experiments during the development of Trinity-RFT. A.1 OPMD: Kimis Version Below is recap of the OPMD variant proposed in the technical report of Kimi k1.5 [28]. Analysis. For specific task/query and reference policy πref, consider the following objective for training policy πθ at particular iteration of the RL process: max θ J(θ; x, πref) := Eyπθ (x)[r(x, y)] τ DKL (cid:0)πθ(x)πref(x)(cid:1). Note that πref can be changing during the RL process. In [28], πref is set to πθt when updating the policy from θt to θt+1. The optimal policy π for this objective satisfies the following: for any response y, at the t-th iteration, i.e., π(yx) = πref(yx)er(x,y)/τ where := Z(x, πref) = πref(yx)er(x,y)/τ , πref(yx)er(x,y)/τ dy = Eyπref(x)[er(x,y)/τ ]. (1) (2) (cid:90) Taking logarithm of both sides of Eq. (1), we see that the optimal policy π must satisfy the following consistency condition: r(x, y) τ log τ (cid:0) log π(yx) log πref(yx)(cid:1) = 0. Algorithm. Based on the above analysis, [28] proposes the following OPMD variant. For query x, first sample rollouts y1, . . . , yK πref(x) from the reference policy, then define surrogate loss as follows: (cid:98)J(θ; x, πref) := (cid:88) (cid:16) r(x, yi) τ log (cid:98)Z τ (cid:0) log πθ(yix) log πref(yix)(cid:1)(cid:17) , i[K] where τ log (cid:98)Z := τ log (cid:16) 1 (cid:88) er(x,yi)/τ (cid:17) . i[K] Although this is an off-policy method (since the rollout policy πref is different from the policy πθ being updated), it is still limited because the rollouts have to be sampled from the particular policy πref = πθt for the t-th iteration of the RL process, as mentioned earlier. The reason for this limitation is the need of estimating = Z(x, πref) using samples from πref(x). A.2 Pairwise OPMD Analysis. To eliminate the term, we note that Eq. (1) is equivalent to the following: y1 and y2, π(y1x) π(y2x) = πref(y1x) πref(y2x) (cid:0)r(x,y1)r(x,y2)(cid:1)/τ . Taking logarithm of both sides, we have log π(y1x) log π(y2x) = log πref(y1x) log πref(y2x) + r(x, y1) r(x, y2) τ , or equivalently, r(x, y1) τ (cid:0) log π(y1x) log πref(y1x)(cid:1) = r(x, y2) τ (cid:0) log π(y2x) log πref(y2x)(cid:1). Note that this holds true for pair of arbitrary responses y1 and y2. Algorithm. For query and arbitrary responses y1, . . . , yK, we define the following surrogate loss: (cid:98)J(θ; x, πref) := (cid:88) (cid:0)ai aj (cid:1) , 1i<jK where ai := r(x, yi) τ (cid:0) log πθ(yix) log πref(yix)(cid:1), [K]. Here πref can be any reference policy for KL regularization, regardless of how y1, . . . , yK were sampled. While this is fully off-policy RL method, it has its own limitation: to run this algorithm, we should make sure that multiple (at least 2) rollouts for the same task are included within one micro-batch (whose size is typically much smaller that of batch or mini-batch), which adds to infrastructure complexity. Remark 2. In the special case of = 2, the above method, termed as pairwise OPMD, turns out to be the same as contrastive policy gradient proposed in [4], albeit with simpler and more intuitive derivation. A.3 OPMD: an Embarrassingly Simple Variant Analysis. Consider the t-th iteration of the RL process, i.e., updating from θt to θt+1, and use πref = πθt as the reference policy. For specific task/query x, recall from Section A.1 the original objective: max θ J(θ; x, πθt) := Eyπθ (x)[r(x, y)] τ DKL (cid:0)πθ(x)πθt(x)(cid:1). We leverage the analysis in Section A.2, and take closer look at the following pairwise loss for ai and aj, normalized by 1/(1 + τ )2 to make the loss scale invariant to the hyperparameter τ : (ai aj)2 (1 + τ )2 = 1 (1 + τ )2 (cid:20)(cid:16) r(x, yi) r(x, yj) (cid:17) (cid:16)(cid:0) log πθ(yix) log πθt(yix)(cid:1) (cid:0) log πθ(yjx) log πθt(yjx)(cid:1)(cid:17)(cid:21)2 . τ The trick here is that, if we only intend to take one gradient step of this loss at θ = θt, then the value of (log πθ(yix) log πθt(yix)) (log πθ(yjx) log πθt (yjx)) is simply zero. As result, θ (ai aj)2 (1 + τ )2 (cid:12) (cid:12) (cid:12)θt = (cid:16) 2τ (1 + τ )2 r(x, yi) r(x, yj) (cid:17)(cid:16) θ log πθ(yix)θt θ log πθ(yjx)θt (cid:17) , and thus (cid:88) θ 1i<jK (cid:88) = 1i<jK (ai aj)2 (1 + τ )2 (cid:12) (cid:12) (cid:12)θt (cid:16) 2τ (1 + τ )2 r(x, yi) r(x, yj) (cid:17)(cid:16) θ log πθ(yix)θt θ log πθ(yjx)θt (cid:17) (cid:88) = 1i<jK 2τ (1 + τ )2 (cid:18) (cid:0)r(x, yi) r(x, yj)(cid:1)θ log πθ(yix)θt + (cid:0)r(x, yj) r(x, yi)(cid:1)θ log πθ(yjx)θt (cid:19) 18 = = 2τ (1 + τ )2 2τ (1 + τ )2 (cid:88) (cid:88) (cid:0)r(x, yi) r(x, yj)(cid:1)θ log πθ(yix)θt 1jK (cid:0)r(x, yi) r(x)(cid:1)θ log πθ(yix)θt, 1iK (cid:88) 1iK where r(x) := 1 (cid:80) j[K] r(x, yj) in the last line. Algorithm. To this end, we update from θt to θt+1 by taking one gradient step of the following surrogate loss, where we simplify the constant factor from 2τ /(1 + τ )2 to 1/(1 + τ ) and also drop the factor: min θ (cid:98)J(θ; x) := 1 1 + τ (cid:88) 1iK (cid:0)r(x, yi) r(x)(cid:1) log πθ(yix). This is exactly the standard policy gradient using the group mean reward as the baseline, but derived differently with an off-policy interpretation. The hyperparameter τ 0 controls the size of each policy update. larger τ is effectively the same as smaller learning rate. As heuristic, we simply add regularization term (denoted by g) to the above objective when additional regularization with respect to fixed policy, e.g., SFT model πsft, is desired: min θ (cid:98)J(θ; x) := 1 1 + τ (cid:88) 1iK (cid:0)r(x, yi) r(x)(cid:1) log πθ(yix) + β g(cid:0)πθ, πsft; x, y1, . . . , yK (cid:1)."
        },
        {
            "title": "B Additional Details for Data Pipelines",
            "content": "B."
        },
        {
            "title": "Illustration of Typical Dataset Classes and Schemas",
            "content": "The dataset shaping process described in Section 3.3 is supported by PostgreSQL persistence with comprehensive schema designs; one example can be found in Listing 6. Key model classes include RftDatasetModel (core dataset storage with lineage tracking), ExperienceModel (which stores serialized experiences with priority scoring), DPODataModel and SFTDataModel (specialized storage for DPO and SFT, respectively). 5 4 3 1 -- RftDataset table structure 2 CREATE TABLE rft_dataset ( id SERIAL PRIMARY KEY, consumed_cnt INTEGER DEFAULT 0, prompt TEXT, response TEXT, reward FLOAT, quality_score FLOAT DEFAULT 0.0, difficulty_score FLOAT DEFAULT 0.0, diversity_score FLOAT DEFAULT 0.0, priority FLOAT DEFAULT 0.0 6 9 8 10 11 12 ); -- Unique record identifier -- Usage counter for sampling -- Input instruction -- Model-generated response -- Training signal value -- Quality assessment metric -- Complexity estimation -- Response uniqueness measure -- Sampling preference weight Listing 6: Schema definition for RftDataset PostgreSQL table. B."
        },
        {
            "title": "Illustration of Data Processing Service",
            "content": "The architecture presented in Section 3.3 integrates Data-Juicer capabilities through service-oriented design. Typically, the DJ-service lifecycle covers three stages: initialization via scripts/start_servers.py; configuration through dj_config_path or natural language (dj_process_desc); and execution handling dataset loading, cleaning, priority scoring, and database export. Moreover, we provide DJExecutor to bridge Data-Juicer capabilities through config translation and execution sandboxing, and dynamic termination via _check_clean_targets() with melt protection. B.3 An Example of Active Data Optimization Algorithms The DataActiveIterator serves as the core engine for data-model co-evolution with multi-dimensional utility scoring. It provides extensible interfaces for flexible training prior injection, e.g., as demonstrated in Algorithm 1. Algorithm 1 Multi-Dimensional Sample Selection Require: Dataset D, weights wq, wd, wk, wf Ensure: Selected subset 1: (q, d, k) DataCleaner.Process(D) 2: Ui wqqi + wddi + wkki wf fi 3: {si Ui > θmin} 4: Top-K(D, K) 5: ExperienceCache.Update(D) 6: LineageTracker.Record(D) 7: return # Compute quality (q), diversity (d), difficulty (k) # Combine q, d, k, frequency (f ) # Threshold filtering # Priority selection # Update version-controlled cache # Track data provenance"
        }
    ],
    "affiliations": [
        "alibaba-inc.com"
    ]
}